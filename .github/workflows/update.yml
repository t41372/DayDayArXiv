name: Daily arXiv Update

on:
  schedule:
    # Run at 02:20 UTC every day (after arXiv announcements settle)
    - cron: '20 2 * * *'
  workflow_dispatch:
    inputs:
      date:
        description: 'Date to process (YYYY-MM-DD format)'
        required: false
        type: string
      start-date:
        description: 'Start date for date range (YYYY-MM-DD format)'
        required: false
        type: string
      end-date:
        description: 'End date for date range (YYYY-MM-DD format, inclusive)'
        required: false
        type: string
      category:
        description: 'arXiv category to process'
        required: false
        default: 'cs.AI'
        type: string
      max-results:
        description: 'Maximum number of papers to fetch'
        required: false
        default: '1000' # Default value from script
        type: string
      force:
        description: 'Force refresh existing data'
        required: false
        default: false
        type: boolean
      log-level:
        description: 'Set logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)'
        required: false
        default: 'INFO' # Default value from script
        type: string

jobs:
  update:
    runs-on: ubuntu-latest
    env:
      # Core settings (match .env.sample; values can come from secrets or vars)
      DDARXIV_DATA_DIR: ${{ secrets.DDARXIV_DATA_DIR || vars.DDARXIV_DATA_DIR }}
      DDARXIV_LOG_DIR: ${{ secrets.DDARXIV_LOG_DIR || vars.DDARXIV_LOG_DIR }}
      DDARXIV_LOG_LEVEL: ${{ secrets.DDARXIV_LOG_LEVEL || vars.DDARXIV_LOG_LEVEL }}
      DDARXIV_CATEGORY: ${{ secrets.DDARXIV_CATEGORY || vars.DDARXIV_CATEGORY }}
      DDARXIV_MAX_RESULTS: ${{ secrets.DDARXIV_MAX_RESULTS || vars.DDARXIV_MAX_RESULTS }}
      DDARXIV_CONCURRENCY: ${{ secrets.DDARXIV_CONCURRENCY || vars.DDARXIV_CONCURRENCY }}
      DDARXIV_BATCH_SIZE: ${{ secrets.DDARXIV_BATCH_SIZE || vars.DDARXIV_BATCH_SIZE }}
      DDARXIV_FORCE: ${{ secrets.DDARXIV_FORCE || vars.DDARXIV_FORCE }}
      DDARXIV_PAPER_MAX_ATTEMPTS: ${{ secrets.DDARXIV_PAPER_MAX_ATTEMPTS || vars.DDARXIV_PAPER_MAX_ATTEMPTS }}
      DDARXIV_FAIL_ON_ERROR: ${{ secrets.DDARXIV_FAIL_ON_ERROR || vars.DDARXIV_FAIL_ON_ERROR }}
      DDARXIV_STATE_SAVE_INTERVAL_S: ${{ secrets.DDARXIV_STATE_SAVE_INTERVAL_S || vars.DDARXIV_STATE_SAVE_INTERVAL_S }}
      DDARXIV_FAILURE_PATTERNS: ${{ secrets.DDARXIV_FAILURE_PATTERNS || vars.DDARXIV_FAILURE_PATTERNS }}

      # LLM providers (match .env.sample)
      DDARXIV_LLM_WEAK_BASE_URL: ${{ secrets.DDARXIV_LLM_WEAK_BASE_URL || vars.DDARXIV_LLM_WEAK_BASE_URL }}
      DDARXIV_LLM_WEAK_API_KEY: ${{ secrets.DDARXIV_LLM_WEAK_API_KEY || vars.DDARXIV_LLM_WEAK_API_KEY }}
      DDARXIV_LLM_WEAK_MODEL: ${{ secrets.DDARXIV_LLM_WEAK_MODEL || vars.DDARXIV_LLM_WEAK_MODEL }}
      DDARXIV_LLM_WEAK_RPM: ${{ secrets.DDARXIV_LLM_WEAK_RPM || vars.DDARXIV_LLM_WEAK_RPM }}
      DDARXIV_LLM_WEAK_TIMEOUT_S: ${{ secrets.DDARXIV_LLM_WEAK_TIMEOUT_S || vars.DDARXIV_LLM_WEAK_TIMEOUT_S }}
      DDARXIV_LLM_WEAK_MAX_RETRIES: ${{ secrets.DDARXIV_LLM_WEAK_MAX_RETRIES || vars.DDARXIV_LLM_WEAK_MAX_RETRIES }}

      DDARXIV_LLM_STRONG_BASE_URL: ${{ secrets.DDARXIV_LLM_STRONG_BASE_URL || vars.DDARXIV_LLM_STRONG_BASE_URL }}
      DDARXIV_LLM_STRONG_API_KEY: ${{ secrets.DDARXIV_LLM_STRONG_API_KEY || vars.DDARXIV_LLM_STRONG_API_KEY }}
      DDARXIV_LLM_STRONG_MODEL: ${{ secrets.DDARXIV_LLM_STRONG_MODEL || vars.DDARXIV_LLM_STRONG_MODEL }}
      DDARXIV_LLM_STRONG_RPM: ${{ secrets.DDARXIV_LLM_STRONG_RPM || vars.DDARXIV_LLM_STRONG_RPM }}
      DDARXIV_LLM_STRONG_TIMEOUT_S: ${{ secrets.DDARXIV_LLM_STRONG_TIMEOUT_S || vars.DDARXIV_LLM_STRONG_TIMEOUT_S }}
      DDARXIV_LLM_STRONG_MAX_RETRIES: ${{ secrets.DDARXIV_LLM_STRONG_MAX_RETRIES || vars.DDARXIV_LLM_STRONG_MAX_RETRIES }}

      DDARXIV_LLM_BACKUP_BASE_URL: ${{ secrets.DDARXIV_LLM_BACKUP_BASE_URL || vars.DDARXIV_LLM_BACKUP_BASE_URL }}
      DDARXIV_LLM_BACKUP_API_KEY: ${{ secrets.DDARXIV_LLM_BACKUP_API_KEY || vars.DDARXIV_LLM_BACKUP_API_KEY }}
      DDARXIV_LLM_BACKUP_MODEL: ${{ secrets.DDARXIV_LLM_BACKUP_MODEL || vars.DDARXIV_LLM_BACKUP_MODEL }}
      DDARXIV_LLM_BACKUP_RPM: ${{ secrets.DDARXIV_LLM_BACKUP_RPM || vars.DDARXIV_LLM_BACKUP_RPM }}
      DDARXIV_LLM_BACKUP_TIMEOUT_S: ${{ secrets.DDARXIV_LLM_BACKUP_TIMEOUT_S || vars.DDARXIV_LLM_BACKUP_TIMEOUT_S }}
      DDARXIV_LLM_BACKUP_MAX_RETRIES: ${{ secrets.DDARXIV_LLM_BACKUP_MAX_RETRIES || vars.DDARXIV_LLM_BACKUP_MAX_RETRIES }}

      # Langfuse (match .env.sample; keep backward compatibility with LANGFUSE_* secrets)
      DDARXIV_LANGFUSE_ENABLED: ${{ secrets.DDARXIV_LANGFUSE_ENABLED || vars.DDARXIV_LANGFUSE_ENABLED }}
      DDARXIV_LANGFUSE_HOST: ${{ secrets.DDARXIV_LANGFUSE_HOST || secrets.LANGFUSE_HOST || vars.DDARXIV_LANGFUSE_HOST || vars.LANGFUSE_HOST }}
      DDARXIV_LANGFUSE_PUBLIC_KEY: ${{ secrets.DDARXIV_LANGFUSE_PUBLIC_KEY || secrets.LANGFUSE_PUBLIC_KEY || vars.DDARXIV_LANGFUSE_PUBLIC_KEY || vars.LANGFUSE_PUBLIC_KEY }}
      DDARXIV_LANGFUSE_SECRET_KEY: ${{ secrets.DDARXIV_LANGFUSE_SECRET_KEY || secrets.LANGFUSE_SECRET_KEY || vars.DDARXIV_LANGFUSE_SECRET_KEY || vars.LANGFUSE_SECRET_KEY }}
      DDARXIV_LANGFUSE_SESSION_NOTE: ${{ secrets.DDARXIV_LANGFUSE_SESSION_NOTE || secrets.LANGFUSE_SESSION_NOTE || vars.DDARXIV_LANGFUSE_SESSION_NOTE || vars.LANGFUSE_SESSION_NOTE }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for proper commits

      - name: Install uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          cache-dependency-glob: "pyproject.toml"
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version-file: "pyproject.toml"

      - name: Install dependencies
        run: uv sync
      
      - name: Set timezone to UTC
        run: |
          echo "TZ=UTC" >> $GITHUB_ENV
          date  # Print current date/time for logging
      
      - name: Set current date
        id: date
        run: |
          echo "current_date=$(date +'%Y-%m-%d')" >> $GITHUB_OUTPUT
          echo "start_date_7_days=$(date -d '6 days ago' +'%Y-%m-%d')" >> $GITHUB_OUTPUT
      
      - name: Run fetch_arxiv script (with specified date or date range)
        if: ${{ inputs.date != '' || (inputs.start-date != '' && inputs.end-date != '') }}
        run: uv run fetch_arxiv.py
        env:
          # Pass all inputs as environment variables for cleaner YAML
          DDARXIV_DATE: ${{ inputs.date }}
          DDARXIV_START_DATE: ${{ inputs.start-date }}
          DDARXIV_END_DATE: ${{ inputs.end-date }}
          DDARXIV_CATEGORY: ${{ inputs.category }}
          DDARXIV_MAX_RESULTS: ${{ inputs.max-results }}
          DDARXIV_FORCE: ${{ inputs.force }}
          DDARXIV_LOG_LEVEL: ${{ inputs.log-level }}
      
      - name: Run fetch_arxiv script (for scheduled runs or manual runs without specified dates)
        if: ${{ inputs.date == '' && inputs.start-date == '' }}
        run: uv run fetch_arxiv.py
        env:
          # For scheduled runs or manual runs without date inputs, process the last 7 days (today UTC + past 6 days)
          DDARXIV_START_DATE: ${{ steps.date.outputs.start_date_7_days }}
          DDARXIV_END_DATE: ${{ steps.date.outputs.current_date }}
          DDARXIV_CATEGORY: ${{ inputs.category || 'cs.AI' }}
          DDARXIV_MAX_RESULTS: ${{ inputs.max-results || '1000' }}
          DDARXIV_FORCE: ${{ inputs.force }}
          DDARXIV_LOG_LEVEL: ${{ inputs.log-level || 'INFO' }}

      - name: Check for changes
        id: check_changes
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          if [[ $(git status --porcelain) ]]; then
            echo "changes=true" >> $GITHUB_OUTPUT
          else
            echo "changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Package data artifact
        if: always()
        id: package_data
        run: |
          set -euo pipefail
          DATA_DIR="daydayarxiv_frontend/public/data"
          if [[ -z "$(git status --porcelain)" ]]; then
            echo "has_zip=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          if [[ ! -d "$DATA_DIR" ]]; then
            echo "has_zip=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          ARCHIVE_PATH="${RUNNER_TEMP}/daydayarxiv-data-${{ github.run_id }}-${{ github.run_attempt }}.zip"
          export ARCHIVE_PATH DATA_DIR
          python - <<'PY'
          import os
          from pathlib import Path
          import zipfile

          data_dir = Path(os.environ["DATA_DIR"]).resolve()
          archive_path = Path(os.environ["ARCHIVE_PATH"]).resolve()
          root = Path.cwd().resolve()

          files = [p for p in data_dir.rglob("*") if p.is_file()]
          if not files:
              raise SystemExit(0)

          with zipfile.ZipFile(archive_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
              for path in files:
                  zf.write(path, path.relative_to(root))

          print(f"Packed {len(files)} files into {archive_path}")
          PY

          if [[ -f "$ARCHIVE_PATH" ]]; then
            echo "has_zip=true" >> "$GITHUB_OUTPUT"
            echo "zip_path=$ARCHIVE_PATH" >> "$GITHUB_OUTPUT"
          else
            echo "has_zip=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Upload data artifact
        if: steps.package_data.outputs.has_zip == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: daydayarxiv-data-${{ github.run_id }}-${{ github.run_attempt }}
          path: ${{ steps.package_data.outputs.zip_path }}

      - name: Commit and push changes
        if: steps.check_changes.outputs.changes == 'true'
        id: push_changes
        run: |
          set -euo pipefail
          DATE=$(date -u +'%Y-%m-%d %H:%M:%S UTC')
          git add -A
          git commit -m "Update data: $DATE"
          git remote set-url origin https://x-access-token:${{ secrets.PAT_GITHUB }}@github.com/${{ github.repository }}
          BRANCH="${{ github.ref_name }}"

          if git fetch origin "$BRANCH" && git rebase "origin/$BRANCH"; then
            if git push origin "HEAD:$BRANCH"; then
              echo "push_status=success" >> "$GITHUB_OUTPUT"
              exit 0
            fi
          else
            git rebase --abort || true
          fi

          RECOVERY_BRANCH="data-recovery/${{ github.run_id }}-${{ github.run_attempt }}"
          git push origin "HEAD:refs/heads/$RECOVERY_BRANCH" || true
          echo "push_status=fallback" >> "$GITHUB_OUTPUT"
          echo "recovery_branch=$RECOVERY_BRANCH" >> "$GITHUB_OUTPUT"
          exit 1

      - name: Trigger build-frontend workflow
        if: steps.push_changes.outputs.push_status == 'success'
        run: |
          curl -X POST \
            -H "Authorization: token ${{ secrets.PAT_GITHUB }}" \
            -H "Accept: application/vnd.github.v3+json" \
            https://api.github.com/repos/${{ github.repository }}/actions/workflows/build-frontend.yml/dispatches \
            -d '{"ref":"${{ github.ref }}"}'

      - name: Open recovery PR
        if: steps.push_changes.outputs.push_status == 'fallback'
        id: recovery_pr
        run: |
          set -euo pipefail
          BRANCH="${{ steps.push_changes.outputs.recovery_branch }}"
          BASE="${{ github.ref_name }}"
          OWNER="${{ github.repository_owner }}"
          API="https://api.github.com/repos/${{ github.repository }}/pulls"
          RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          cat > "$RUNNER_TEMP/pr_payload.json" <<JSON
          {
            "title": "Data recovery: ${BRANCH}",
            "head": "${BRANCH}",
            "base": "${BASE}",
            "body": "Automated recovery PR created from workflow run ${RUN_URL}."
          }
          JSON

          STATUS=$(curl -s -o "$RUNNER_TEMP/pr_response.json" -w "%{http_code}" \
            -H "Authorization: token ${{ secrets.PAT_GITHUB }}" \
            -H "Accept: application/vnd.github.v3+json" \
            -d @"$RUNNER_TEMP/pr_payload.json" \
            "$API")

          if [[ "$STATUS" == "201" ]]; then
            PR_URL=$(python - <<'PY'
          import json
          from pathlib import Path
          data = json.loads(Path("${RUNNER_TEMP}/pr_response.json").read_text())
          print(data.get("html_url", ""))
          PY
            )
            echo "pr_url=$PR_URL" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          SEARCH_URL="https://api.github.com/repos/${{ github.repository }}/pulls?state=open&head=${OWNER}:${BRANCH}"
          STATUS=$(curl -s -o "$RUNNER_TEMP/pr_search.json" -w "%{http_code}" \
            -H "Authorization: token ${{ secrets.PAT_GITHUB }}" \
            -H "Accept: application/vnd.github.v3+json" \
            "$SEARCH_URL")

          if [[ "$STATUS" == "200" ]]; then
            PR_URL=$(python - <<'PY'
          import json
          from pathlib import Path
          data = json.loads(Path("${RUNNER_TEMP}/pr_search.json").read_text())
          if isinstance(data, list) and data:
              print(data[0].get("html_url", ""))
          PY
            )
            if [[ -n "$PR_URL" ]]; then
              echo "pr_url=$PR_URL" >> "$GITHUB_OUTPUT"
            fi
          fi

      - name: Write recovery summary
        if: always()
        run: |
          {
            echo "## Data recovery"
            if [[ "${{ steps.package_data.outputs.has_zip }}" == "true" ]]; then
              echo "- Artifact: daydayarxiv-data-${{ github.run_id }}-${{ github.run_attempt }}"
            else
              echo "- Artifact: none"
            fi

            if [[ "${{ steps.push_changes.outputs.push_status }}" == "fallback" ]]; then
              echo "- Recovery branch: ${{ steps.push_changes.outputs.recovery_branch }}"
              if [[ -n "${{ steps.recovery_pr.outputs.pr_url }}" ]]; then
                echo "- Recovery PR: ${{ steps.recovery_pr.outputs.pr_url }}"
              else
                echo "- Recovery PR: none"
              fi
            else
              echo "- Recovery branch: none"
            fi
          } >> "$GITHUB_STEP_SUMMARY"

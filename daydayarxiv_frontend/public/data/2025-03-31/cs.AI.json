{
  "date": "2025-03-31",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-31 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的研究热点聚焦于提升大型模型的推理、想象与多模态能力，同时深入探讨了模型效率优化（如KV缓存压缩、测试时计算扩展）、安全性与公平性评估（如偏见检测、对抗攻击），以及AI在机器人控制（模拟与现实结合、自主评估）、视觉理解（3D高斯溅射、视频分析）、科学发现和医疗健康等领域的广泛应用。\n\n**重点论文 & LLM 增强与效率**\n\n*   **RIG：端到端通用策略中推理与想象的协同 (RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy)**\n    针对具身智能体在复杂开放世界中推理和想象能力分离的问题，本文提出了RIG框架，首次将推理（Reasoning）和想象（Imagination，即世界模型）能力融合到一个端到端的通用策略中。通过构建数据管道逐步整合和丰富轨迹信息，并联合学习推理和下一图像生成，RIG显着提高了样本效率（超过17倍）和泛化能力。推理时，RIG先推理、再生成潜在动作、然后预测结果，允许在执行前进行自我修正。\n\n*   **驾驭推理经济：大语言模型高效推理综述 (Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models)**\n    这篇综述探讨了大型语言模型（LLMs）中的“推理经济”概念，即如何在提升推理能力（System 2思维）带来的高计算成本与快速直觉思维（System 1）的效率之间取得平衡。文章分析了推理效率低下的原因、不同推理模式的行为，并总结了实现推理经济的潜在解决方案，为该领域的研究提供了宝贵资源和未来方向。\n\n*   **通过思维干预有效控制推理模型 (Effectively Controlling Reasoning Models through Thinking Intervention)**\n    该研究提出“思维干预”范式，通过在推理过程中策略性地插入或修改特定的思维标记（thinking tokens），来更精细地控制具备显式推理步骤的LLM的行为。实验表明，这种方法在指令遵循、指令层级推理和安全对齐任务上显着优于基线提示方法，为控制推理LLM开辟了新途径。\n\n*   **SQuat：子空间正交KV缓存量化 (SQuat: Subspace-orthogonal KV Cache Quantization)**\n    为了解决LLM推理中KV缓存占用内存过多的问题，本文提出SQuat量化方法。它构建查询张量的子空间，并在量化关键张量（Key Tensors）时，强制量化误差与该子空间正交，从而最小化量化对注意力机制输出的影响。该方法无需模型微调和额外校准数据，实验证明能有效减少峰值内存（2.17-2.82倍）、提高吞吐量（2.45-3.60倍）。\n\n*   **ORAL：通过条件循环扩散提示您的大规模LoRA (ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion)**\n    针对大型语言模型（LLMs）的低秩适应（LoRA）参数生成问题，本文提出ORAL框架，一个条件循环扩散模型。它能根据模型架构和文本任务规范生成任务特定的LoRA参数，实现跨基础模型的迁移，并解决了现有方法在可扩展性和可控性上的局限。实验证明ORAL生成的LoRA参数性能媲美甚至优于传统训练方法。\n\n*   **什么、如何、何处以及多好？大语言模型测试时扩展综述 (What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models)**\n    随着预训练阶段计算扩展的热情减弱，“测试时扩展”（TTS）或“测试时计算”成为新焦点。这篇综述提出了一个统一的多维框架（扩展什么、如何扩展、在哪扩展、效果如何），系统地回顾了TTS的方法、应用场景和评估方面，旨在梳理该领域的发展脉络，提供实践指南，并指出未来挑战与方向。\n\n*   **更好的是智慧而非财富：用于测试时知识增强的动态参数化检索增强生成 (Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement)**\n    传统RAG面临上下文长度增加导致的高推理成本和RAG幻觉问题。参数化RAG（PRAG）通过将文档嵌入模型参数进行测试时知识增强，但训练和存储成本高。本文提出动态参数化RAG（DyPRAG），使用轻量级参数转换器将文档高效转换为参数化知识，在测试时即插即用地增强LLM知识并解决知识冲突，有效降低了各项成本并提升了泛化能力。\n\n*   **OrchMLLM：通过批量后平衡协调多模态数据以加速多模态大语言模型训练 (OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training)**\n    多模态大语言模型（MLLM）训练中存在“模态组成不连贯”问题，导致mini-batch不平衡，降低训练效率。本文提出OrchMLLM框架，包含“批量后平衡调度器”消除mini-batch不平衡，以及“MLLM全局协调器”来协调多模态数据。实验表明，该框架能显着提升大规模MLLM训练的吞吐量和模型FLOPs利用率（MFU）。\n\n*   **思考更久，而非更大：通过扩展测试时计算增强软件工程智能体 (Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute)**\n    为解决软件工程智能体依赖闭源或资源密集型模型的问题，本文提出统一的测试时计算（TTC）扩展框架，通过增加推理时间计算而非模型大小来提升性能。框架包含内部TTC（开发情境化轨迹合成+拒绝采样）和外部TTC（基于开发过程的搜索策略）。实验表明，使用该框架的32B模型在SWE-bench上的问题解决率达到46%，超越了更大的模型，并验证了模型会为更难的问题动态分配更多计算资源。\n\n*   **自适应层跳过在预训练LLM中的应用 (Adaptive Layer-skipping in Pre-trained LLMs)**\n    现有LLM层跳过方法忽略了不同Token生成计算需求的差异。本文提出FlexiDepth，通过引入插件式路由器和适配器，动态调整生成过程中使用的Transformer层数，无需修改原模型参数。应用于Llama-3-8B模型可跳过8层（共32层）同时保持100%基准性能，并发现重复或固定短语需要层数少，而涉及计算或高不确定性的Token需要层数多。\n\n*   **重新思考大型语言模型服务的键值缓存压缩技术 (Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving)**\n    本文从实践角度重新审视了主流的KV缓存压缩技术。研究发现，现有压缩算法虽能减少内存消耗，但当前实现（如FlashAttention）并未针对生产级LLM服务优化，导致吞吐量次优；压缩可能导致更长的输出，增加端到端延迟；并且在处理特定LLM任务时存在固有限制。文章提供了工具以促进未来研究和实际部署。\n\n*   **AirCache：激活跨模态相关性KV缓存压缩以实现高效的大型视觉语言模型推理 (AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference)**\n    针对大型视觉语言模型（LVLM）中视觉Token和长文本输出导致的KV缓存开销问题，本文提出AirCache压缩方法。通过系统研究注意力机制中视觉和文本Token的相关性，发现视觉缓存存在冗余。AirCache引入精英观察窗口评估视觉成分重要性，并开发自适应分层预算分配策略。实验表明，仅保留10%视觉KV缓存即可达到全缓存性能，并将解码延迟降低29%-66%。\n\n**机器人与具身智能**\n\n*   **模拟与现实协同训练：基于视觉的机器人操作的简单秘诀 (Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation)**\n    大规模真实机器人数据收集成本高昂，而纯模拟训练存在现实差距。本文系统研究了模拟与真实数据协同训练（co-training）策略，并提出了一个简单有效的配方。实验表明，即使模拟与现实数据存在显着差异，结合模拟数据也能将真实世界任务性能平均提高38%。\n\n*   **AutoEval：真实世界中通用机器人操作策略的自主评估 (AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World)**\n    机器人学习中，大规模、可复现的真实世界评估是一个长期挑战。本文提出AutoEval系统，旨在实现通用机器人策略的全天候自主评估，最大限度减少人工干预。系统包含自动成功检测和场景重置功能，用户只需提交评估作业。实验证明AutoEval能有效替代人工评估，并已在WidowX机器人上公开了多个评估场景。\n\n*   **Pro-Routing：用于取送任务的自主多容量机器人的主动路径规划 (Pro-Routing: Proactive Routing of Autonomous Multi-Capacity Robots for Pickup-and-Delivery Tasks)**\n    研究多容量自主机器人在满足最大等待时间约束下，处理预定和实时到达的取送请求问题。本文提出一种基于主动展开（proactive rollout）的路径规划框架，能在适应实时需求的同时，理论上保证路径策略的稳定性。通过车队规模调整算法确保稳定性，并在真实叫车数据上验证了理论和方法的有效性。\n\n*   **GenSwarm：通过语言模型实现可扩展的多机器人代码策略生成与部署 (GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models)**\n    传统多机器人策略开发复杂且缺乏灵活性。本文介绍GenSwarm，一个端到端系统，利用大型语言模型根据自然语言指令自动生成和部署多机器人控制策略。作为一个多语言智能体系统，GenSwarm能实现零样本学习，快速适应新任务，生成的代码策略具有可解释性。系统支持模拟和真实机器人部署。\n\n**AI 安全、伦理与评估**\n\n*   **ACPBench Hard：对行动、变化和规划的无约束推理 (ACPBench Hard: Unrestrained Reasoning about Action, Change, and Planning)**\n    ACPBench旨在测试规划所需的原子推理能力。本文提出ACPBench Hard，是ACPBench的生成式版本，要求模型回答开放式问题而非选择题，更贴近真实规划场景。研究发现，即使是当前最大的语言模型在这些任务上的表现也普遍不佳（低于65%），表明它们在可靠的规划推理方面仍有很长的路要走。\n\n*   **BEATS：大型语言模型的偏见评估与测试套件 (BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models)**\n    提出BEATS框架和基准，用于评估LLM中的偏见、伦理、公平性和真实性。该基准包含29个不同指标，涵盖人口统计、认知、社会偏见以及伦理推理、群体公平性和虚假信息风险。实验表明，行业领先模型生成的输出中有37.65%包含某种形式的偏见，强调了在关键决策系统中使用这些模型的风险。\n\n*   **输出约束作为攻击面：利用结构化生成绕过LLM安全机制 (Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms)**\n    揭示了LLM结构化输出API（如用于Agent系统）带来的新的控制层面攻击面。提出约束解码攻击（CDA），通过在模式级语法规则（控制层面）中嵌入恶意意图，同时保持良性表面提示（数据层面），来绕过安全机制。实例化的“链式枚举攻击”在多个模型（包括GPT-4o）上达到96.2%的攻击成功率，暴露了当前LLM安全机制的盲点。\n\n*   **当反事实推理失败时：混沌与现实世界的复杂性 (When Counterfactual Reasoning Fails: Chaos and Real-World Complexity)**\n    研究在模型不确定性、观测噪声和混沌行为等现实世界条件下，反事实推理的局限性。通过结构因果模型框架下的实证研究（反事实序列估计），发现即使是轻微的模型不确定性或混沌动力学也可能导致预测的反事实轨迹与真实轨迹发生巨大偏差，警示在复杂不确定系统中使用反事实推理需谨慎。\n\n*   **理论心智在AI中得到有力支持：超越3岁儿童测试 (All You Need is Sally-Anne: ToM in AI Strongly Supported After Surpassing Tests for 3-Year-Olds)**\n    理论心智（ToM）是人类理解他人信念和意图的关键能力。本文提出了一个超越了为3岁儿童设计的传统ToM测试（如Sally-Anne测试）的模型，为AI系统具备ToM能力提供了有力支持。\n\n*   **WinoWhat：带有常识分类的WinoGrande句子转述平行语料库 (WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization)**\n    为了更深入地评估LLM的常识推理能力，本研究发布了WinoWhat语料库，包含WinoGrande验证集中每个实例的转述。同时，研究按五种常识知识类别评估了模型在挑战集上的表现。结果显示，所有模型在WinoWhat上的表现均显着下降，表明LLM在WinoGrande上的推理能力可能被高估，且这种下降并非主要由基准记忆效应导致。\n\n**计算机视觉与多模态**\n\n*   **Any2Caption：为可控视频生成解释任何条件到字幕 (Any2Caption: Interpreting Any Condition to Caption for Controllable Video Generation)**\n    为解决视频生成中用户意图解释的瓶颈，提出Any2Caption框架。该框架利用多模态大语言模型（MLLM）将文本、图像、视频及特定线索（区域、运动、相机姿态等）解释为密集的结构化字幕，以更好地指导视频合成。同时发布了大规模指令调优数据集Any2CapIns。\n\n*   **视觉声场 (Visual Acoustic Fields)**\n    提出视觉声场框架，利用3D高斯溅射（3DGS）连接3D空间中的击打声音和视觉信号。框架包含声音生成（利用特征增强3DGS渲染的多尺度特征生成击打声）和声音定位（根据声音源查询3D场景定位击打位置）模块。为此构建了首个场景级别的视觉-声音样本对数据集。\n\n*   **从RGB视频学习3D高斯模拟器 (Learning 3D-Gaussian Simulators from RGB Videos)**\n    提出3DGSim，一个端到端从多视角RGB视频学习的3D物理模拟器。它将图像编码为3D高斯粒子表示，通过Transformer传播动力学，并使用3D高斯溅射渲染帧。通过联合训练逆向渲染和动力学Transformer，模型能捕捉刚体、弹性体、布料等多种物理行为及光照效果，并泛化到新场景。\n\n*   **DiET-GS：扩散先验和事件流辅助的运动去模糊3D高斯溅射 (DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting)**\n    针对从模糊多视角图像重建清晰3D表示的问题，提出DiET-GS框架。利用无模糊的事件流（event streams）和扩散先验（diffusion prior）来增强3DGS。通过约束3DGS与事件双积分，并利用扩散先验增强边缘细节，有效恢复准确颜色和精细纹理。\n\n*   **HumanAesExpert：推进用于人像美学评估的多模态基础模型 (HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment)**\n    针对人像美学评估（HIAA）研究不足的问题，本文提出HumanAesExpert框架。首先构建了首个HIAA专用数据集HumanBeauty（108k图像，部分含12维度美学标注）。然后提出一个强大的视觉语言模型，创新性地设计了Expert头融合人类美学知识，并结合语言建模和回归头，通过MetaVoter聚合各头得分，实现更优的整体和细粒度HIAA。\n\n*   **MGD-SAM2：用于高分辨率类别无关分割的多视角引导细节增强SAM2 (MGD-SAM2: Multi-view Guided Detail-enhanced Segment Anything Model 2 for High-Resolution Class-agnostic Segmentation)**\n    为解决SAM在处理高分辨率图像和细粒度细节分割上的局限性，提出MGD-SAM2。该模型将SAM2与多视角特征交互（全局图像与局部补丁）相结合，引入MPAdapter、MCEM、HMIM和DRM四个新模块，分别用于增强特征提取、聚合多视角特征和细化高分辨率掩码预测，提升了HRCS性能。\n\n*   **PolypSegTrack：用于结肠镜视频分析的统一基础模型 (PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis)**\n    提出PolypSegTrack，一个用于结肠镜视频中息肉检测、分割、分类和无监督跟踪的统一基础模型。利用新颖的条件掩码损失，使其能灵活处理像素级或边界框标注数据，无需任务特定微调。无监督跟踪模块可靠关联跨帧息肉实例。模型采用在自然图像上预训练的视觉基础模型主干。\n\n**其他值得关注的研究**\n\n*   **UniOcc：自动驾驶中占用预测与预报的统一基准 (UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving)**：整合了多个真实世界和模拟器数据集，提供带体素流标注的2D/3D占用标签，并引入不依赖真值的评估指标。\n*   **我应该信任哪个LIME？概念、挑战与解决方案 (Which LIME should I trust? Concepts, Challenges, and Solutions)**：首次全面综述LIME（一种XAI方法）的概念、局限和各种改进，提供分类和比较。\n*   **基于信念系统的上下文偏好协同度量框架 (Contextual Preference Collaborative Measure Framework Based on Belief System)**：提出一种基于更新信念系统的偏好协同度量框架，旨在减少人工干预，提高偏好度量算法的准确性和效率（中文论文）。\n*   **面向科学智能：基于LLM的科学智能体综述 (Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents)**：回顾了基于LLM的科学智能体的架构、设计、基准、应用和伦理考量，强调其与通用智能体的区别及在推动科学发现中的作用。\n*   **AI2Agent：将AI项目部署为自主智能体的端到端框架 (AI2Agent: An End-to-End Framework for Deploying AI Projects as Autonomous Agents)**：提出AI2Agent框架，通过指南驱动执行、自适应调试和案例积累，自动化AI项目的部署过程，减少人工干预。\n*   **绿色MLOps到绿色GenOps：判别式和生成式AI操作中能耗的实证研究 (Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in Discriminative and Generative AI Operations)**：实证研究了判别式AI和生成式AI（LLM）在MLOps流程中的能耗，提供了优化能耗的实践指南。\n*   **MolGround：分子基准测试 (MolGround: A Benchmark for Molecular Grounding)**：提出首个分子定位（molecular grounding）基准，评估模型将分子概念与特定结构成分关联的能力，并构建了大规模分子理解基准数据集。\n*   **人工智能时代的地理信息科学：迈向自主GIS的研究议程 (GIScience in the Era of Artificial Intelligence: A Research Agenda Towards Autonomous GIS)**：提出自主GIS的概念和框架，利用LLM作为决策核心，使其能独立生成和执行地理处理工作流，并探讨了挑战与未来方向。\n*   **贝叶斯预测编码 (Bayesian Predictive Coding)**：提出贝叶斯预测编码（BPC），作为预测编码（PC）的扩展，用于估计网络参数的后验分布，保留了PC的局部性并产生闭式Hebbian权重更新，提升了不确定性量化能力。\n\n希望这份快报能帮助你快速了解今日 arXiv 的精华内容！",
  "papers": [
    {
      "arxiv_id": "2503.24388v1",
      "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy",
      "title_zh": "RIG：端到端通用策略中协同推理与想象\n",
      "authors": [
        "Zhonghan Zhao",
        "Wenwei Zhang",
        "Haian Huang",
        "Kuikun Liu",
        "Jianfei Gao",
        "Gaoang Wang",
        "Kai Chen"
      ],
      "abstract": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
      "tldr_zh": "该论文提出了RIG，一个端到端的通用策略模型，旨在协同推理(Reasoning)和想象(Imagination)能力，提升具身智能体在复杂开放世界环境中的表现。RIG通过数据管道，逐步整合和丰富现有智能体轨迹中的想象和推理内容，实现推理和下一帧图像生成的联合学习，从而显式地建模推理、动作和环境动态之间的内在联系。实验表明，RIG相比现有方法，样本效率提升超过17倍，并展现出更好的泛化能力。在推理阶段，RIG首先推理下一步动作，预测潜在动作，然后预测动作结果，使智能体有机会在采取实际行动之前基于想象进行审查和自我纠正。实验结果表明，推理和想象的协同作用提高了通用策略的鲁棒性、泛化性和互操作性，并支持测试时扩展以增强整体性能。\n",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24388v1",
      "published_date": "2025-03-31 17:59:52 UTC",
      "updated_date": "2025-03-31 17:59:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:04:36.541032"
    },
    {
      "arxiv_id": "2503.24381v1",
      "title": "UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving",
      "title_zh": "UniOcc：自动驾驶中用于 Occupancy 预测和推断的统一基准\n",
      "authors": [
        "Yuping Wang",
        "Xiangyu Huang",
        "Xiaokang Sun",
        "Mingxuan Yan",
        "Shuo Xing",
        "Zhengzhong Tu",
        "Jiachen Li"
      ],
      "abstract": "We introduce UniOcc, a comprehensive, unified benchmark for occupancy\nforecasting (i.e., predicting future occupancies based on historical\ninformation) and current-frame occupancy prediction from camera images. UniOcc\nunifies data from multiple real-world datasets (i.e., nuScenes, Waymo) and\nhigh-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3D\noccupancy labels with per-voxel flow annotations and support for cooperative\nautonomous driving. In terms of evaluation, unlike existing studies that rely\non suboptimal pseudo labels for evaluation, UniOcc incorporates novel metrics\nthat do not depend on ground-truth occupancy, enabling robust assessment of\nadditional aspects of occupancy quality. Through extensive experiments on\nstate-of-the-art models, we demonstrate that large-scale, diverse training data\nand explicit flow information significantly enhance occupancy prediction and\nforecasting performance.",
      "tldr_zh": "UniOcc是一个统一的基准，用于自动驾驶中的占用预测(occupancy forecasting)和当前帧占用预测(current-frame occupancy prediction)。它整合了来自nuScenes、Waymo、CARLA和OpenCOOD等多个真实世界数据集和高保真驾驶模拟器的数据，提供2D/3D占用标签以及体素流(per-voxel flow)注释，并支持协同自动驾驶。UniOcc引入了不依赖于真实占用(ground-truth occupancy)的新型评估指标，从而能够更可靠地评估占用质量的各个方面。实验表明，大规模、多样化的训练数据和显式的流信息能够显著提高占用预测和预测性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages; Dataset: https://huggingface.co/datasets/tasl-lab/uniocc;\n  Code: https://github.com/tasl-lab/UniOcc",
      "pdf_url": "http://arxiv.org/pdf/2503.24381v1",
      "published_date": "2025-03-31 17:59:24 UTC",
      "updated_date": "2025-03-31 17:59:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:04:48.101648"
    },
    {
      "arxiv_id": "2503.24379v1",
      "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation",
      "title_zh": "Any2Caption：将任意条件解释为字幕以实现可控视频生成\n",
      "authors": [
        "Shengqiong Wu",
        "Weicai Ye",
        "Jiahao Wang",
        "Quande Liu",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai",
        "Shuicheng Yan",
        "Hao Fei",
        "Tat-Seng Chua"
      ],
      "abstract": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/",
      "tldr_zh": "Any2Caption 是一种新型可控视频生成框架，旨在解决当前视频生成领域中用户意图准确理解的瓶颈问题。其核心思想是将各种条件解释步骤与视频合成步骤解耦，利用多模态大型语言模型 (MLLM) 将文本、图像、视频以及区域、运动和相机姿势等专业线索等多种输入转换为密集、结构化的字幕，从而为骨干视频生成器提供更好的指导。此外，该研究还引入了 Any2CapIns，这是一个大规模数据集，包含 337K 个实例和 407K 个条件，用于任何条件到字幕的指令微调。 综合评估表明，该系统在现有视频生成模型的各个方面的可控性和视频质量方面都有显着改进。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://sqwu.top/Any2Cap/",
      "pdf_url": "http://arxiv.org/pdf/2503.24379v1",
      "published_date": "2025-03-31 17:59:01 UTC",
      "updated_date": "2025-03-31 17:59:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:05:00.241965"
    },
    {
      "arxiv_id": "2503.24378v1",
      "title": "ACPBench Hard: Unrestrained Reasoning about Action, Change, and Planning",
      "title_zh": "ACPBench Hard：关于行动、变化和规划的无约束推理\n",
      "authors": [
        "Harsha Kokel",
        "Michael Katz",
        "Kavitha Srinivas",
        "Shirin Sohrabi"
      ],
      "abstract": "The ACPBench dataset provides atomic reasoning tasks required for efficient\nplanning. The dataset is aimed at distilling the complex plan generation task\ninto separate atomic reasoning tasks in their easiest possible form, boolean or\nmultiple-choice questions, where the model has to choose the right answer from\nthe provided options. While the aim of ACPBench is to test the simplest form of\nreasoning about action and change, when tasked with planning, a model does not\ntypically have options to choose from and thus the reasoning required for\nplanning dictates an open-ended, generative form for these tasks. To that end,\nwe introduce ACPBench Hard, a generative version of ACPBench, with open-ended\nquestions which the model needs to answer. Models that perform well on these\ntasks could in principle be integrated into a planner or be used directly as a\npolicy. We discuss the complexity of these tasks as well as the complexity of\nvalidating the correctness of their answers and present validation algorithms\nfor each task. Equipped with these validators, we test the performance of a\nvariety of models on our tasks and find that for most of these tasks the\nperformance of even the largest models is still subpar. Our experiments show\nthat no model outperforms another in these tasks and with a few exceptions all\ntested language models score below 65%, indicating that even the current\nfrontier language models have a long way to go before they can reliably reason\nabout planning. In fact, even the so-called reasoning models struggle with\nsolving these reasoning tasks. ACPBench Hard collection is available at the\nfollowing link: https://ibm.github.io/ACPBench",
      "tldr_zh": "该论文提出了ACPBench Hard，一个用于评估模型在行动、变化和规划方面无约束推理能力的数据集，是ACPBench的生成式版本。与ACPBench的选择题形式不同，ACPBench Hard包含开放式问题，要求模型生成答案。论文讨论了任务的复杂性以及答案验证的复杂性，并为每个任务提出了验证算法。实验结果表明，即使是最大的语言模型在这些任务上的表现仍然不佳，表明当前的前沿语言模型在可靠地进行规划推理方面还有很长的路要走。该数据集可在https://ibm.github.io/ACPBench 获取。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to LM4Plan@AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.24378v1",
      "published_date": "2025-03-31 17:58:25 UTC",
      "updated_date": "2025-03-31 17:58:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:05:12.089242"
    },
    {
      "arxiv_id": "2503.24377v1",
      "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models",
      "title_zh": "驾驭推理经济：大型语言模型高效推理综述\n",
      "authors": [
        "Rui Wang",
        "Hongru Wang",
        "Boyang Xue",
        "Jianhui Pang",
        "Shudong Liu",
        "Yi Chen",
        "Jiahao Qiu",
        "Derek Fai Wong",
        "Heng Ji",
        "Kam-Fai Wong"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
      "tldr_zh": "这篇综述探讨了大型语言模型(LLMs)中推理经济的问题，即如何在性能(收益)和计算成本(预算)之间取得平衡。文章分析了LLMs在后训练和测试推理阶段的推理效率低下原因，研究了不同推理模式的行为，并提出了实现推理经济的潜在解决方案。该研究旨在为提高LLMs的推理经济性提供有价值的见解，并为该领域的研究进展提供参考。同时，作者还提供了一个公共知识库，以持续跟踪该快速发展领域的最新进展。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "In Progress; Paper list Repo:\n  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers",
      "pdf_url": "http://arxiv.org/pdf/2503.24377v1",
      "published_date": "2025-03-31 17:58:07 UTC",
      "updated_date": "2025-03-31 17:58:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:05:24.075023"
    },
    {
      "arxiv_id": "2503.24376v1",
      "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1",
      "title_zh": "探索强化学习对视频理解的影响：来自 SEED-Bench-R1 的见解\n",
      "authors": [
        "Yi Chen",
        "Yuying Ge",
        "Rui Wang",
        "Yixiao Ge",
        "Lu Qiu",
        "Ying Shan",
        "Xihui Liu"
      ],
      "abstract": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
      "tldr_zh": "该研究引入了SEED-Bench-R1，一个用于系统评估多模态大语言模型(MLLMs)在视频理解中后训练方法的基准。SEED-Bench-R1包含复杂的真实世界视频和日常规划任务，以多项选择题形式呈现，需要精细的感知和逻辑推理。研究使用Qwen2-VL-Instruct-7B作为基模型，比较了强化学习(RL)和监督微调(SFT)，结果表明RL在数据效率和分布内/外任务性能上均优于SFT，甚至在LongVideoBench等通用视频理解基准上表现更佳。分析表明，RL增强了视觉感知，但逻辑推理链的连贯性有所下降。该研究指出了不一致的推理和被忽略的视觉线索等关键局限性，并提出了改进基模型推理、奖励建模以及RL对噪声信号的鲁棒性的建议。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1",
      "pdf_url": "http://arxiv.org/pdf/2503.24376v1",
      "published_date": "2025-03-31 17:55:23 UTC",
      "updated_date": "2025-03-31 17:55:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:05:36.427363"
    },
    {
      "arxiv_id": "2503.24370v1",
      "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
      "title_zh": "通过思维干预有效控制推理模型\n",
      "authors": [
        "Tong Wu",
        "Chong Xiang",
        "Jiachen T. Wang",
        "Prateek Mittal"
      ],
      "abstract": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
      "tldr_zh": "该论文提出了一种名为“思维干预”(Thinking Intervention)的新范式，旨在通过策略性地插入或修改特定的思维token，来更精细地控制大型语言模型(LLMs)的内部推理过程。通过在IFEval上的指令跟随、SEP上的指令层次推理以及XSTest和SORRY-Bench上的安全对齐等多个任务的综合评估，结果表明，思维干预显著优于基线提示方法。在使用开源DeepSeek R1模型时，指令跟随场景的准确率提高了6.7%，指令层次推理的准确率提高了15.4%，不安全提示的拒绝率提高了40.0%。这项工作为控制推理LLM开辟了一个有前景的新研究方向。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24370v1",
      "published_date": "2025-03-31 17:50:13 UTC",
      "updated_date": "2025-03-31 17:50:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:05:48.166702"
    },
    {
      "arxiv_id": "2503.24365v1",
      "title": "Which LIME should I trust? Concepts, Challenges, and Solutions",
      "title_zh": "我应该信任哪个 LIME？概念、挑战与解决方案\n",
      "authors": [
        "Patrick Knab",
        "Sascha Marton",
        "Udo Schlegel",
        "Christian Bartelt"
      ],
      "abstract": "As neural networks become dominant in essential systems, Explainable\nArtificial Intelligence (XAI) plays a crucial role in fostering trust and\ndetecting potential misbehavior of opaque models. LIME (Local Interpretable\nModel-agnostic Explanations) is among the most prominent model-agnostic\napproaches, generating explanations by approximating the behavior of black-box\nmodels around specific instances. Despite its popularity, LIME faces challenges\nrelated to fidelity, stability, and applicability to domain-specific problems.\nNumerous adaptations and enhancements have been proposed to address these\nissues, but the growing number of developments can be overwhelming,\ncomplicating efforts to navigate LIME-related research. To the best of our\nknowledge, this is the first survey to comprehensively explore and collect\nLIME's foundational concepts and known limitations. We categorize and compare\nits various enhancements, offering a structured taxonomy based on intermediate\nsteps and key issues. Our analysis provides a holistic overview of advancements\nin LIME, guiding future research and helping practitioners identify suitable\napproaches. Additionally, we provide a continuously updated interactive website\n(https://patrick-knab.github.io/which-lime-to-trust/), offering a concise and\naccessible overview of the survey.",
      "tldr_zh": "本文全面综述了局部可解释模型无关解释(LIME)方法，该方法通过近似黑盒模型在特定实例周围的行为来生成解释，在可解释人工智能(XAI)中发挥着关键作用。论文深入探讨了LIME的基本概念和已知局限性，如保真度、稳定性和领域特定问题适用性等挑战。文章对LIME的各种改进方法进行了分类和比较，根据中间步骤和关键问题构建了一个结构化的分类体系。此外，论文还提供了一个持续更新的交互式网站，为LIME相关研究提供了一个简洁易懂的概述，旨在指导未来的研究并帮助从业者选择合适的方法。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 3rd World Conference on eXplainable Artificial\n  Intelligence (XAI 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.24365v1",
      "published_date": "2025-03-31 17:44:39 UTC",
      "updated_date": "2025-03-31 17:44:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:06:00.232690"
    },
    {
      "arxiv_id": "2503.24361v1",
      "title": "Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation",
      "title_zh": "Sim-and-Real Co-Training：一种基于视觉的机器人操作的简单方法\n",
      "authors": [
        "Abhiram Maddukuri",
        "Zhenyu Jiang",
        "Lawrence Yunliang Chen",
        "Soroush Nasiriany",
        "Yuqi Xie",
        "Yu Fang",
        "Wenqi Huang",
        "Zu Wang",
        "Zhenjia Xu",
        "Nikita Chernyadev",
        "Scott Reed",
        "Ken Goldberg",
        "Ajay Mandlekar",
        "Linxi Fan",
        "Yuke Zhu"
      ],
      "abstract": "Large real-world robot datasets hold great potential to train generalist\nrobot models, but scaling real-world human data collection is time-consuming\nand resource-intensive. Simulation has great potential in supplementing\nlarge-scale data, especially with recent advances in generative AI and\nautomated data generation tools that enable scalable creation of robot behavior\ndatasets. However, training a policy solely in simulation and transferring it\nto the real world often demands substantial human effort to bridge the reality\ngap. A compelling alternative is to co-train the policy on a mixture of\nsimulation and real-world datasets. Preliminary studies have recently shown\nthis strategy to substantially improve the performance of a policy over one\ntrained on a limited amount of real-world data. Nonetheless, the community\nlacks a systematic understanding of sim-and-real co-training and what it takes\nto reap the benefits of simulation data for real-robot learning. This work\npresents a simple yet effective recipe for utilizing simulation data to solve\nvision-based robotic manipulation tasks. We derive this recipe from\ncomprehensive experiments that validate the co-training strategy on various\nsimulation and real-world datasets. Using two domains--a robot arm and a\nhumanoid--across diverse tasks, we demonstrate that simulation data can enhance\nreal-world task performance by an average of 38%, even with notable differences\nbetween the simulation and real-world data. Videos and additional results can\nbe found at https://co-training.github.io/",
      "tldr_zh": "该研究提出了一种简单有效的“Sim-and-Real Co-Training”方法，用于解决基于视觉的机器人操作任务。通过混合使用仿真和真实世界数据集进行策略联合训练，该方法旨在利用仿真数据来提升真实机器人的学习效果，并减少真实世界数据收集的成本。实验结果表明，即使仿真和真实世界数据存在显著差异，使用该方法训练的模型在真实机器人任务上的性能平均提升了38%。该研究提供了一套利用仿真数据进行真实机器人学习的有效方案。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://co-training.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.24361v1",
      "published_date": "2025-03-31 17:39:38 UTC",
      "updated_date": "2025-03-31 17:39:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:06:12.166546"
    },
    {
      "arxiv_id": "2503.24358v1",
      "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
      "title_zh": "SQuat：子空间正交 KV 缓存量化\n",
      "authors": [
        "Hao Wang",
        "Ligong Han",
        "Kai Xu",
        "Akash Srivastava"
      ],
      "abstract": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
      "tldr_zh": "该论文提出了SQuat (Subspace-orthogonal KV cache quantization)，一种新的KV缓存量化方法，旨在减少大型语言模型(LLMs)解码过程中的内存占用，同时避免量化误差累积。SQuat首先构建一个由query tensors张成的子空间，捕捉关键任务信息。在key tensor量化过程中，SQuat强制量化误差与该子空间正交，从而最小化量化对注意力机制输出的影响。SQuat无需模型微调或额外的校准数据集，并基于理论框架。实验结果表明，SQuat在降低内存占用、提高吞吐量和提升基准测试分数方面优于现有KV缓存量化算法。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24358v1",
      "published_date": "2025-03-31 17:37:32 UTC",
      "updated_date": "2025-03-31 17:37:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:06:24.215062"
    },
    {
      "arxiv_id": "2503.24354v1",
      "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion",
      "title_zh": "ORAL：通过条件递归扩散提示大规模 LoRA 模型\n",
      "authors": [
        "Rana Muhammad Shahroz Khan",
        "Dongwen Tang",
        "Pingzhi Li",
        "Kai Wang",
        "Tianlong Chen"
      ],
      "abstract": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts.",
      "tldr_zh": "该论文提出了一种名为ORAL的条件循环扩散框架，用于生成大规模LoRA（Low-Rank Adaptation）参数，旨在解决传统神经网络训练的成本问题。ORAL通过结合模型架构和文本任务规范的条件机制，生成特定任务的LoRA参数，这些参数可以无缝地迁移到不断演进的基础模型中。实验结果表明，ORAL能够成功扩展到数十亿参数的LLM，并在七种语言任务、四种视觉任务和三种多模态任务中，生成高质量的LoRA参数，其性能与传统训练的LoRA参数相当甚至更好。该方法为不断更新的大型语言模型的有效适应提供了一种新途径。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24354v1",
      "published_date": "2025-03-31 17:34:59 UTC",
      "updated_date": "2025-03-31 17:34:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:06:36.253948"
    },
    {
      "arxiv_id": "2503.24328v1",
      "title": "Contextual Preference Collaborative Measure Framework Based on Belief System",
      "title_zh": "基于信念系统的上下文偏好协同度量框架\n",
      "authors": [
        "Hang Yu",
        "Wei Wei",
        "Zheng Tan",
        "Jing-lei Liu"
      ],
      "abstract": "To reduce the human intervention in the preference measure process,this\narticle proposes a preference collaborative measure framework based on an\nupdated belief system,which is also capable of improving the accuracy and\nefficiency of preferen-ce measure algorithms.Firstly,the distance of rules and\nthe average internal distance of rulesets are proposed for specifying the\nrelationship between the rules.For discovering the most representative\npreferences that are common in all users,namely common preference,a algorithm\nbased on average internal distance of ruleset,PRA algorithm,is proposed,which\naims to finish the discoveryprocess with minimum information loss\nrate.Furthermore,the concept of Common belief is proposed to update the belief\nsystem,and the common preferences are the evidences of updated belief\nsystem.Then,under the belief system,the proposed belief degree and deviation\ndegree are used to determine whether a rule confirms the belief system or not\nand classify the preference rules into two kinds(generalized or\npersonalized),and eventually filters out Top-K interesting rules relying on\nbelief degree and deviation degree.Based on above,a scalable interestingness\ncalculation framework that can apply various formulas is proposed for\naccurately calculating interestingness in different conditions.At last,IMCos\nalgorithm and IMCov algorithm are proposed as exemplars to verify the accuracy\nand efficiency of the framework by using weighted cosine similarity and\ncorrelation coefficients as belief degree.In experiments,the proposed\nalgorithms are compared to two state-of-the-art algorithms and the results show\nthat IMCos and IMCov outperform than the other two in most aspects.",
      "tldr_zh": "本文提出了一种基于更新的信念系统的偏好协同度量框架，旨在减少人工干预并提高偏好度量算法的准确性和效率。该框架首先定义了规则距离和规则集平均内部距离，用于描述规则之间的关系。然后，提出了一种基于规则集平均内部距离的PRA算法，用于发现所有用户中最具代表性的共同偏好，并尽量减少信息损失。此外，引入了“共同信念”的概念来更新信念系统，并将共同偏好作为更新信念系统的证据。在信念系统的基础上，利用置信度和偏差度来判断规则是否符合信念系统，并将偏好规则分为通用型和个性化型，最终筛选出Top-K的有趣规则。最后，提出了一个可扩展的兴趣度计算框架，用于在不同条件下准确计算兴趣度，并以IMCos和IMCov算法为例，验证了该框架的准确性和效率。实验结果表明，与两种最先进的算法相比，IMCos和IMCov在大多数方面都表现更好。\n",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "in Chinese language",
      "pdf_url": "http://arxiv.org/pdf/2503.24328v1",
      "published_date": "2025-03-31 17:17:45 UTC",
      "updated_date": "2025-03-31 17:17:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:06:48.519529"
    },
    {
      "arxiv_id": "2503.24325v1",
      "title": "Pro-Routing: Proactive Routing of Autonomous Multi-Capacity Robots for Pickup-and-Delivery Tasks",
      "title_zh": "Pro-Routing：用于取货和送货任务的自主多容量机器人主动路由",
      "authors": [
        "Daniel Garces",
        "Stephanie Gil"
      ],
      "abstract": "We consider a multi-robot setting, where we have a fleet of multi-capacity\nautonomous robots that must service spatially distributed pickup-and-delivery\nrequests with fixed maximum wait times. Requests can be either scheduled ahead\nof time or they can enter the system in real-time. In this setting, stability\nfor a routing policy is defined as the cost of the policy being uniformly\nbounded over time. Most previous work either solve the problem offline to\ntheoretically maintain stability or they consider dynamically arriving requests\nat the expense of the theoretical guarantees on stability. In this paper, we\naim to bridge this gap by proposing a novel proactive rollout-based routing\nframework that adapts to real-time demand while still provably maintaining the\nstability of the learned routing policy. We derive provable stability\nguarantees for our method by proposing a fleet sizing algorithm that obtains a\nsufficiently large fleet that ensures stability by construction. To validate\nour theoretical results, we consider a case study on real ride requests for\nHarvard's evening Van System. We also evaluate the performance of our framework\nusing the currently deployed smaller fleet size. In this smaller setup, we\ncompare against the currently deployed routing algorithm, greedy heuristics,\nand Monte-Carlo-Tree-Search-based algorithms. Our empirical results show that\nour framework maintains stability when we use the sufficiently large fleet size\nfound in our theoretical results. For the smaller currently deployed fleet\nsize, our method services 6% more requests than the closest baseline while\nreducing median passenger wait times by 33%.",
      "tldr_zh": "本文提出了一种名为Pro-Routing的主动路由框架，用于解决多容量自主机器人在取货和交付任务中的路由问题，尤其是在具有固定最大等待时间约束下。该框架旨在弥合离线求解以保证理论稳定性与动态处理实时请求之间的差距。Pro-Routing采用基于rollout的路由策略，能够适应实时需求，并能证明保持学习到的路由策略的稳定性。通过设计车队规模算法，确保车队规模足够大，从而在构建上保证稳定性。通过哈佛大学夜间Van系统的真实乘车请求案例研究验证了理论结果，并与现有路由算法、贪婪启发式算法和基于蒙特卡洛树搜索的算法进行了比较。实验结果表明，Pro-Routing在使用足够大的车队规模时可以保持稳定性，而在较小的现有车队规模下，与最接近的基线相比，Pro-Routing可以多服务6%的请求，同时将乘客的平均等待时间减少33%。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "25 pages, 7 figures, and 1 table",
      "pdf_url": "http://arxiv.org/pdf/2503.24325v1",
      "published_date": "2025-03-31 17:14:07 UTC",
      "updated_date": "2025-03-31 17:14:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:07:00.745640"
    },
    {
      "arxiv_id": "2503.24310v1",
      "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models",
      "title_zh": "BEATS：大型语言模型的偏差评估和评估测试套件\n",
      "authors": [
        "Alok Abhishek",
        "Lisa Erickson",
        "Tushar Bandopadhyay"
      ],
      "abstract": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
      "tldr_zh": "该研究提出了BEATS框架，用于评估大型语言模型(LLMs)在偏见、伦理、公平性和事实性方面的表现。BEATS框架包含一个偏见基准，通过29个不同的指标来衡量LLMs的性能，涵盖人口统计学、认知和社会偏见，以及伦理推理、群体公平性和与事实相关的错误信息风险。实验结果表明，行业领先模型生成的输出中，有37.65%包含某种形式的偏见。BEATS框架提供了一种可扩展且统计严谨的方法，用于对LLMs进行基准测试、诊断偏见驱动因素并制定缓解策略，旨在帮助开发更具社会责任感和符合伦理道德的AI模型。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T01 (Primary), 68T50 (Secondary)",
        "I.2.0; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "32 pages, 33 figures, preprint version",
      "pdf_url": "http://arxiv.org/pdf/2503.24310v1",
      "published_date": "2025-03-31 16:56:52 UTC",
      "updated_date": "2025-03-31 16:56:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:07:12.437813"
    },
    {
      "arxiv_id": "2503.24307v1",
      "title": "A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG",
      "title_zh": "LLM策略在心理健康文本分析中的系统评估：微调 vs. 提示工程 vs. RAG\n",
      "authors": [
        "Arshia Kermani",
        "Veronica Perez-Rosas",
        "Vangelis Metsis"
      ],
      "abstract": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility.",
      "tldr_zh": "本研究系统地比较了三种利用大型语言模型(LLMs)进行心理健康文本分析的方法：提示工程(Prompt Engineering)、检索增强生成(RAG)和微调(Fine-tuning)。 使用LLaMA 3，研究在两个数据集上评估了这些方法在情绪分类和心理健康状况检测任务中的表现。 微调(Fine-tuning)实现了最高的准确率（情绪分类91%，心理健康状况80%），但需要大量的计算资源和大型训练集。 提示工程(Prompt Engineering)和RAG提供了更灵活的部署方式，但性能适中（准确率40-68%）。 研究结果为在心理健康应用中实施基于LLM的解决方案提供了实践指导，强调了准确性、计算需求和部署灵活性之间的权衡。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24307v1",
      "published_date": "2025-03-31 16:54:04 UTC",
      "updated_date": "2025-03-31 16:54:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:07:24.298861"
    },
    {
      "arxiv_id": "2503.24305v2",
      "title": "Evaluating machine learning models for predicting pesticides toxicity to honey bees",
      "title_zh": "评估用于预测农药对蜜蜂毒性的机器学习模型\n",
      "authors": [
        "Jakub Adamczyk",
        "Jakub Poziemski",
        "Pawel Siedlecki"
      ],
      "abstract": "Small molecules play a critical role in the biomedical, environmental, and\nagrochemical domains, each with distinct physicochemical requirements and\nsuccess criteria. Although biomedical research benefits from extensive datasets\nand established benchmarks, agrochemical data remain scarce, particularly with\nrespect to species-specific toxicity. This work focuses on ApisTox, the most\ncomprehensive dataset of experimentally validated chemical toxicity to the\nhoney bee (Apis mellifera), an ecologically vital pollinator. We evaluate\nApisTox using a diverse suite of machine learning approaches, including\nmolecular fingerprints, graph kernels, and graph neural networks, as well as\npretrained models. Comparative analysis with medicinal datasets from the\nMoleculeNet benchmark reveals that ApisTox represents a distinct chemical\nspace. Performance degradation on non-medicinal datasets, such as ApisTox,\ndemonstrates their limited generalizability of current state-of-the-art\nalgorithms trained solely on biomedical data. Our study highlights the need for\nmore diverse datasets and for targeted model development geared toward the\nagrochemical domain.",
      "tldr_zh": "该研究评估了多种机器学习模型在预测农药对蜜蜂毒性方面的表现，使用了目前最全面的蜜蜂毒性数据集ApisTox。研究比较了分子指纹、图核、图神经网络以及预训练模型等方法，并与MoleculeNet中的医药数据集进行了对比分析。结果表明，ApisTox代表了一个独特的化学空间，且在生物医药数据上训练的模型在ApisTox等非医药数据集上的泛化能力有限。这项研究强调了需要更多样化的数据集，以及针对农用化学领域进行有针对性的模型开发。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24305v2",
      "published_date": "2025-03-31 16:51:12 UTC",
      "updated_date": "2025-04-01 07:57:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:07:36.219541"
    },
    {
      "arxiv_id": "2503.24299v1",
      "title": "Shape Expressions with Inheritance",
      "title_zh": "具有继承的形状表达式\n",
      "authors": [
        "Iovka Boneva",
        "Jose Emilio Labra Gayo",
        "Eric Prud'hommeaux",
        "Katherine Thornton",
        "Andra Waagmeester"
      ],
      "abstract": "We formally introduce an inheritance mechanism for the Shape Expressions\nlanguage (ShEx). It is inspired by inheritance in object-oriented programming\nlanguages, and provides similar advantages such as reuse, modularity, and more\nflexible data modelling. Using an example, we explain the main features of the\ninheritance mechanism. We present its syntax and formal semantics. The\nsemantics is an extension of the semantics of ShEx 2.1. It also directly yields\na validation algorithm as an extension of the previous ShEx validation\nalgorithms, while maintaining the same algorithmic complexity.",
      "tldr_zh": "本文为Shape Expressions (ShEx) 语言引入了一种继承机制，该机制受到面向对象编程语言中继承的启发，并提供了类似的优势，例如重用性、模块化和更灵活的数据建模。文章通过实例解释了继承机制的主要特性，并给出了其语法和形式语义。该语义是对ShEx 2.1语义的扩展，并直接产生一个验证算法，作为先前ShEx验证算法的扩展，同时保持相同的算法复杂度。\n",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Accepted in Extended Semantic Web Conference, ESWC, 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.24299v1",
      "published_date": "2025-03-31 16:42:44 UTC",
      "updated_date": "2025-03-31 16:42:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:07:48.067778"
    },
    {
      "arxiv_id": "2503.24284v1",
      "title": "Value of Information-based Deceptive Path Planning Under Adversarial Interventions",
      "title_zh": "对抗干预下基于信息价值的欺骗性路径规划\n",
      "authors": [
        "Wesley A. Suttle",
        "Jesse Milzman",
        "Mustafa O. Karabag",
        "Brian M. Sadler",
        "Ufuk Topcu"
      ],
      "abstract": "Existing methods for deceptive path planning (DPP) address the problem of\ndesigning paths that conceal their true goal from a passive, external observer.\nSuch methods do not apply to problems where the observer has the ability to\nperform adversarial interventions to impede the path planning agent. In this\npaper, we propose a novel Markov decision process (MDP)-based model for the DPP\nproblem under adversarial interventions and develop new value of information\n(VoI) objectives to guide the design of DPP policies. Using the VoI objectives\nwe propose, path planning agents deceive the adversarial observer into choosing\nsuboptimal interventions by selecting trajectories that are of low\ninformational value to the observer. Leveraging connections to the linear\nprogramming theory for MDPs, we derive computationally efficient solution\nmethods for synthesizing policies for performing DPP under adversarial\ninterventions. In our experiments, we illustrate the effectiveness of the\nproposed solution method in achieving deceptiveness under adversarial\ninterventions and demonstrate the superior performance of our approach to both\nexisting DPP methods and conservative path planning approaches on illustrative\ngridworld problems.",
      "tldr_zh": "本文提出了一种新的基于马尔可夫决策过程(MDP)的欺骗性路径规划(DPP)模型，用于解决在对抗干预下，观察者能够阻碍路径规划智能体的问题。通过引入基于信息价值(VoI)的目标函数，引导DPP策略的设计，使智能体选择对观察者信息价值低的轨迹，从而诱导其选择次优的干预措施。利用MDP的线性规划理论，推导出计算高效的策略综合方法，用于在对抗干预下执行DPP。实验结果表明，该方法在对抗干预下能有效实现欺骗性，并在网格世界问题中优于现有的DPP方法和保守路径规划方法。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.24284v1",
      "published_date": "2025-03-31 16:31:29 UTC",
      "updated_date": "2025-03-31 16:31:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:08:00.532160"
    },
    {
      "arxiv_id": "2503.24278v1",
      "title": "AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World",
      "title_zh": "AutoEval：真实世界中通用机器人操作策略的自主评估\n",
      "authors": [
        "Zhiyuan Zhou",
        "Pranav Atreya",
        "You Liang Tan",
        "Karl Pertsch",
        "Sergey Levine"
      ],
      "abstract": "Scalable and reproducible policy evaluation has been a long-standing\nchallenge in robot learning. Evaluations are critical to assess progress and\nbuild better policies, but evaluation in the real world, especially at a scale\nthat would provide statistically reliable results, is costly in terms of human\ntime and hard to obtain. Evaluation of increasingly generalist robot policies\nrequires an increasingly diverse repertoire of evaluation environments, making\nthe evaluation bottleneck even more pronounced. To make real-world evaluation\nof robotic policies more practical, we propose AutoEval, a system to\nautonomously evaluate generalist robot policies around the clock with minimal\nhuman intervention. Users interact with AutoEval by submitting evaluation jobs\nto the AutoEval queue, much like how software jobs are submitted with a cluster\nscheduling system, and AutoEval will schedule the policies for evaluation\nwithin a framework supplying automatic success detection and automatic scene\nresets. We show that AutoEval can nearly fully eliminate human involvement in\nthe evaluation process, permitting around the clock evaluations, and the\nevaluation results correspond closely to ground truth evaluations conducted by\nhand. To facilitate the evaluation of generalist policies in the robotics\ncommunity, we provide public access to multiple AutoEval scenes in the popular\nBridgeData robot setup with WidowX robot arms. In the future, we hope that\nAutoEval scenes can be set up across institutions to form a diverse and\ndistributed evaluation network.",
      "tldr_zh": "该论文提出了AutoEval，一个用于自主评估通用机器人操作策略的系统，旨在解决机器人学习中可扩展和可复现的策略评估难题。AutoEval通过自动成功检测和自动场景重置，实现了近乎完全消除人工干预的全天候策略评估。用户只需提交评估任务到AutoEval队列，系统即可自动安排策略评估。实验结果表明，AutoEval的评估结果与人工评估高度一致。为了促进通用策略的评估，作者公开了多个基于BridgeData机器人设置的AutoEval场景，并希望未来能形成一个分布式评估网络。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24278v1",
      "published_date": "2025-03-31 16:23:44 UTC",
      "updated_date": "2025-03-31 16:23:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:08:12.270866"
    },
    {
      "arxiv_id": "2503.24277v1",
      "title": "Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality",
      "title_zh": "通过近似准正交性评估和设计稀疏自编码器\n",
      "authors": [
        "Sewoong Lee",
        "Adam Davies",
        "Marc E. Canby",
        "Julia Hockenmaier"
      ],
      "abstract": "Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic\ninterpretability, but leading SAE approaches with top-$k$ style activation\nfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEs\nare based on the linear representation hypothesis (LRH), which assumes that the\nrepresentations of large language models (LLMs) are linearly encoded, and the\nsuperposition hypothesis (SH), which states that there can be more features in\nthe model than its dimensionality. We show that, based on the formal\ndefinitions of the LRH and SH, the magnitude of sparse feature vectors (the\nlatent representations learned by SAEs of the dense embeddings of LLMs) can be\napproximated using their corresponding dense vector with a closed-form error\nbound. To visualize this, we propose the ZF plot, which reveals a previously\nunknown relationship between LLM hidden embeddings and SAE feature vectors,\nallowing us to make the first empirical measurement of the extent to which\nfeature vectors of pre-trained SAEs are over- or under-activated for a given\ninput. Correspondingly, we introduce Approximate Feature Activation (AFA),\nwhich approximates the magnitude of the ground-truth sparse feature vector, and\npropose a new evaluation metric derived from AFA to assess the alignment\nbetween inputs and activations. We also leverage AFA to introduce a novel SAE\narchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with\ntheoretical justifications; and (b) obviate the need to tune SAE sparsity\nhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve\nreconstruction loss comparable to that of state-of-the-art top-k SAEs, without\nrequiring the hyperparameter $k$ to be tuned. Our code is available at:\nhttps://github.com/SewoongLee/top-afa-sae.",
      "tldr_zh": "该论文针对稀疏自编码器(SAEs)中超参数k的选择缺乏理论基础的问题，从线性表示假设(LRH)和叠加假设(SH)出发，推导了稀疏特征向量幅度与其对应稠密向量之间的近似关系，并提出了ZF图用于可视化LLM隐藏层嵌入和SAE特征向量之间的关系。基于此，论文引入了近似特征激活(AFA)方法，用于评估输入和激活之间的对齐程度，并提出了一种新的SAE架构——top-AFA SAE。实验结果表明，top-AFA SAE在无需调整超参数k的情况下，能够达到与现有top-k SAE相当的重构损失，更符合理论依据。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24277v1",
      "published_date": "2025-03-31 16:22:11 UTC",
      "updated_date": "2025-03-31 16:22:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:08:24.407481"
    },
    {
      "arxiv_id": "2503.24270v2",
      "title": "Visual Acoustic Fields",
      "title_zh": "视觉声场\n",
      "authors": [
        "Yuelei Li",
        "Hyunjin Kim",
        "Fangneng Zhan",
        "Ri-Zhao Qiu",
        "Mazeyu Ji",
        "Xiaojun Shan",
        "Xueyan Zou",
        "Paul Liang",
        "Hanspeter Pfister",
        "Xiaolong Wang"
      ],
      "abstract": "Objects produce different sounds when hit, and humans can intuitively infer\nhow an object might sound based on its appearance and material properties.\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\nSplatting (3DGS). Our approach features two key modules: sound generation and\nsound localization. The sound generation module leverages a conditional\ndiffusion model, which takes multiscale features rendered from a\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\nsound localization module enables querying the 3D scene, represented by the\nfeature-augmented 3DGS, to localize hitting positions based on the sound\nsources. To support this framework, we introduce a novel pipeline for\ncollecting scene-level visual-sound sample pairs, achieving alignment between\ncaptured images, impact locations, and corresponding sounds. To the best of our\nknowledge, this is the first dataset to connect visual and acoustic signals in\na 3D context. Extensive experiments on our dataset demonstrate the\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\nand accurately localizing impact sources. Our project page is at\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.",
      "tldr_zh": "该论文提出了Visual Acoustic Fields，一个利用3D高斯溅射(3DGS)在3D空间中桥接敲击声音和视觉信号的框架。该框架包含声音生成和声音定位两个关键模块：声音生成模块利用条件扩散模型，从特征增强的3DGS渲染的多尺度特征生成逼真的敲击声；声音定位模块则允许通过查询特征增强的3DGS表示的3D场景，根据声源定位敲击位置。作者还构建了一个新的数据集，用于收集场景级别的视觉-声音样本对，实现捕获图像、撞击位置和相应声音之间的对齐。实验结果表明，Visual Acoustic Fields 在生成合理的撞击声音和精确定位撞击源方面是有效的。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24270v2",
      "published_date": "2025-03-31 16:16:10 UTC",
      "updated_date": "2025-04-01 03:16:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:08:36.424792"
    },
    {
      "arxiv_id": "2503.24262v1",
      "title": "New Statistical Framework for Extreme Error Probability in High-Stakes Domains for Reliable Machine Learning",
      "title_zh": "用于可靠机器学习的高风险领域中极端误差概率的新统计框架\n",
      "authors": [
        "Umberto Michelucci",
        "Francesca Venturini"
      ],
      "abstract": "Machine learning is vital in high-stakes domains, yet conventional validation\nmethods rely on averaging metrics like mean squared error (MSE) or mean\nabsolute error (MAE), which fail to quantify extreme errors. Worst-case\nprediction failures can have substantial consequences, but current frameworks\nlack statistical foundations for assessing their probability. In this work a\nnew statistical framework, based on Extreme Value Theory (EVT), is presented\nthat provides a rigorous approach to estimating worst-case failures. Applying\nEVT to synthetic and real-world datasets, this method is shown to enable robust\nestimation of catastrophic failure probabilities, overcoming the fundamental\nlimitations of standard cross-validation. This work establishes EVT as a\nfundamental tool for assessing model reliability, ensuring safer AI deployment\nin new technologies where uncertainty quantification is central to\ndecision-making or scientific analysis.",
      "tldr_zh": "该论文提出了一种新的统计框架，利用极值理论(Extreme Value Theory, EVT)来评估高风险领域中机器学习模型的极端误差概率。传统方法依赖于平均指标，无法量化最坏情况下的预测失败。该框架通过将EVT应用于合成和真实世界数据集，能够稳健地估计灾难性失败的概率，克服了标准交叉验证的局限性。研究结果表明，EVT是评估模型可靠性的重要工具，有助于在不确定性量化至关重要的新技术中更安全地部署AI。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24262v1",
      "published_date": "2025-03-31 16:08:11 UTC",
      "updated_date": "2025-03-31 16:08:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:08:48.210810"
    },
    {
      "arxiv_id": "2503.24258v1",
      "title": "Beyond a Single Mode: GAN Ensembles for Diverse Medical Data Generation",
      "title_zh": "超越单一模式：用于生成多样化医疗数据的 GAN 集成\n",
      "authors": [
        "Lorenzo Tronchin",
        "Tommy Löfstedt",
        "Paolo Soda",
        "Valerio Guarrasi"
      ],
      "abstract": "The advancement of generative AI, particularly in medical imaging, confronts\nthe trilemma of ensuring high fidelity, diversity, and efficiency in synthetic\ndata generation. While Generative Adversarial Networks (GANs) have shown\npromise across various applications, they still face challenges like mode\ncollapse and insufficient coverage of real data distributions. This work\nexplores the use of GAN ensembles to overcome these limitations, specifically\nin the context of medical imaging. By solving a multi-objective optimisation\nproblem that balances fidelity and diversity, we propose a method for selecting\nan optimal ensemble of GANs tailored for medical data. The selected ensemble is\ncapable of generating diverse synthetic medical images that are representative\nof true data distributions and computationally efficient. Each model in the\nensemble brings a unique contribution, ensuring minimal redundancy. We\nconducted a comprehensive evaluation using three distinct medical datasets,\ntesting 22 different GAN architectures with various loss functions and\nregularisation techniques. By sampling models at different training epochs, we\ncrafted 110 unique configurations. The results highlight the capability of GAN\nensembles to enhance the quality and utility of synthetic medical images,\nthereby improving the efficacy of downstream tasks such as diagnostic\nmodelling.",
      "tldr_zh": "这篇论文探讨了使用GAN集成来解决医学图像生成中高保真度、多样性和效率之间的难题。通过解决一个平衡保真度和多样性的多目标优化问题，论文提出了一种选择最佳GAN集成的方法，该集成专门为医学数据定制。该集成能够生成具有代表性的多样化合成医学图像，并具有计算效率。实验使用了三个不同的医学数据集，测试了22种不同的GAN架构，结果表明GAN集成可以提高合成医学图像的质量和效用，从而提高诊断建模等下游任务的有效性。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24258v1",
      "published_date": "2025-03-31 16:06:01 UTC",
      "updated_date": "2025-03-31 16:06:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:09:00.230922"
    },
    {
      "arxiv_id": "2503.24237v1",
      "title": "Spatio-temporal Prediction of Fine-Grained Origin-Destination Matrices with Applications in Ridesharing",
      "title_zh": "细粒度起讫点矩阵的时空预测及其在网约车中的应用\n",
      "authors": [
        "Run Yang",
        "Runpeng Dai",
        "Siran Gao",
        "Xiaocheng Tang",
        "Fan Zhou",
        "Hongtu Zhu"
      ],
      "abstract": "Accurate spatial-temporal prediction of network-based travelers' requests is\ncrucial for the effective policy design of ridesharing platforms. Having\nknowledge of the total demand between various locations in the upcoming time\nslots enables platforms to proactively prepare adequate supplies, thereby\nincreasing the likelihood of fulfilling travelers' requests and redistributing\nidle drivers to areas with high potential demand to optimize the global\nsupply-demand equilibrium. This paper delves into the prediction of\nOrigin-Destination (OD) demands at a fine-grained spatial level, especially\nwhen confronted with an expansive set of local regions. While this task holds\nimmense practical value, it remains relatively unexplored within the research\ncommunity. To fill this gap, we introduce a novel prediction model called\nOD-CED, which comprises an unsupervised space coarsening technique to alleviate\ndata sparsity and an encoder-decoder architecture to capture both semantic and\ngeographic dependencies. Through practical experimentation, OD-CED has\ndemonstrated remarkable results. It achieved an impressive reduction of up to\n45% reduction in root-mean-square error and 60% in weighted mean absolute\npercentage error over traditional statistical methods when dealing with OD\nmatrices exhibiting a sparsity exceeding 90%.",
      "tldr_zh": "本文提出了一种名为OD-CED的新型预测模型，用于精细化时空尺度的OD (Origin-Destination) 矩阵预测，尤其是在区域数量庞大时。该模型包含一个无监督的空间粗化技术，用于缓解数据稀疏性问题，以及一个encoder-decoder架构，用于捕获语义和地理依赖关系。实验结果表明，在处理稀疏度超过90%的OD矩阵时，OD-CED相比传统统计方法，均方根误差降低了高达45%，加权平均绝对百分比误差降低了60%。该模型可应用于网约车平台，通过预测未来时段不同位置之间的出行需求，从而优化供需平衡。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24237v1",
      "published_date": "2025-03-31 15:52:27 UTC",
      "updated_date": "2025-03-31 15:52:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:09:12.361996"
    },
    {
      "arxiv_id": "2503.24235v1",
      "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models",
      "title_zh": "是什么、如何做、在哪里做以及做得如何？关于大型语言模型中测试时缩放的综述\n",
      "authors": [
        "Qiyuan Zhang",
        "Fuyuan Lyu",
        "Zexu Sun",
        "Lei Wang",
        "Weixu Zhang",
        "Zhihan Guo",
        "Yufei Wang",
        "Irwin King",
        "Xue Liu",
        "Chen Ma"
      ],
      "abstract": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
      "tldr_zh": "这篇综述论文全面探讨了大型语言模型(LLMs)中的测试时缩放(Test-Time Scaling, TTS)技术。论文提出了一个统一的多维度框架，从四个核心维度分析TTS研究：缩放什么(what to scale)，如何缩放(how to scale)，在哪里缩放(where to scale)以及缩放效果如何(how well to scale)。通过对现有方法、应用场景和评估方面的深入回顾，论文总结了TTS的主要发展轨迹，并为实际部署提供了实践指导。此外，论文还指出了TTS领域目前面临的挑战，并对未来的发展方向提出了展望，包括进一步缩放、明确技术的功能本质、推广到更多任务以及更多归因研究。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24235v1",
      "published_date": "2025-03-31 15:46:15 UTC",
      "updated_date": "2025-03-31 15:46:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:09:24.495374"
    },
    {
      "arxiv_id": "2503.24228v1",
      "title": "PAARS: Persona Aligned Agentic Retail Shoppers",
      "title_zh": "PAARS：人格对齐的智能零售购物者\n",
      "authors": [
        "Saab Mansour",
        "Leonardo Perelli",
        "Lorenzo Mainetti",
        "George Davidson",
        "Stefano D'Amato"
      ],
      "abstract": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work.",
      "tldr_zh": "该论文提出了PAARS框架，旨在利用LLM驱动的智能体模拟电商环境中人类购物行为。PAARS框架通过以下步骤实现：(1) 从匿名历史购物数据中自动挖掘用户画像(Personas)，创建合成购物智能体；(2) 为智能体配备零售特定工具，以合成购物会话；(3) 引入一种新的对齐套件，用于衡量人类和购物智能体在群体层面的分布差异。实验结果表明，使用Personas可以提高对齐套件的性能，但与人类行为仍存在差距。该框架可用于自动化智能体A/B测试，并与人类结果进行比较，为未来的研究奠定基础。\n",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24228v1",
      "published_date": "2025-03-31 15:41:51 UTC",
      "updated_date": "2025-03-31 15:41:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:09:36.382825"
    },
    {
      "arxiv_id": "2503.24219v1",
      "title": "MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote Sensing",
      "title_zh": "MB-ORES：用于遥感视觉定位的多分支对象推理器\n",
      "authors": [
        "Karim Radouane",
        "Hanane Azzag",
        "Mustapha lebbah"
      ],
      "abstract": "We propose a unified framework that integrates object detection (OD) and\nvisual grounding (VG) for remote sensing (RS) imagery. To support conventional\nOD and establish an intuitive prior for VG task, we fine-tune an open-set\nobject detector using referring expression data, framing it as a partially\nsupervised OD task. In the first stage, we construct a graph representation of\neach image, comprising object queries, class embeddings, and proposal\nlocations. Then, our task-aware architecture processes this graph to perform\nthe VG task. The model consists of: (i) a multi-branch network that integrates\nspatial, visual, and categorical features to generate task-aware proposals, and\n(ii) an object reasoning network that assigns probabilities across proposals,\nfollowed by a soft selection mechanism for final referring object localization.\nOur model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG\ndatasets, achieving significant improvements over state-of-the-art methods\nwhile retaining classical OD capabilities. The code will be available in our\nrepository: \\url{https://github.com/rd20karim/MB-ORES}.",
      "tldr_zh": "该论文提出了一个统一的框架MB-ORES，集成了目标检测(OD)和视觉定位(VG)技术，用于遥感(RS)图像处理。通过使用指代表达式数据微调开放集目标检测器，将其转化为部分监督的OD任务，为VG任务建立直观的先验。MB-ORES构建图像的图表示，包含目标查询、类别嵌入和候选区域位置。该模型包含一个多分支网络，整合空间、视觉和类别特征以生成任务相关的候选区域，以及一个目标推理网络，用于分配概率并进行软选择以实现最终的目标定位。在OPT-RSVG和DIOR-RSVG数据集上的实验结果表明，MB-ORES显著优于现有技术，同时保留了经典OD的能力。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24219v1",
      "published_date": "2025-03-31 15:36:41 UTC",
      "updated_date": "2025-03-31 15:36:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:09:48.501157"
    },
    {
      "arxiv_id": "2503.24215v1",
      "title": "All You Need is Sally-Anne: ToM in AI Strongly Supported After Surpassing Tests for 3-Year-Olds",
      "title_zh": "你只需要 Sally-Anne：AI 中的心智理论在超越 3 岁儿童测试后得到有力支持\n",
      "authors": [
        "Nitay Alon",
        "Joseph Barnby",
        "Reuth Mirsky",
        "Stefan Sarkadi"
      ],
      "abstract": "Theory of Mind (ToM) is a hallmark of human cognition, allowing individuals\nto reason about others' beliefs and intentions. Engineers behind recent\nadvances in Artificial Intelligence (AI) have claimed to demonstrate comparable\ncapabilities. This paper presents a model that surpasses traditional ToM tests\ndesigned for 3-year-old children, providing strong support for the presence of\nToM in AI systems.",
      "tldr_zh": "该论文提出一个AI模型，该模型在传统的心理理论(Theory of Mind, ToM)测试中超越了为3岁儿童设计的标准。这意味着该AI系统在理解和推理他人信念和意图方面取得了显著进展。研究结果为AI系统中存在心理理论提供了强有力的支持，表明AI在认知能力上取得了重要突破。该模型成功通过了Sally-Anne测试等经典ToM测试。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24215v1",
      "published_date": "2025-03-31 15:32:10 UTC",
      "updated_date": "2025-03-31 15:32:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:10:00.190237"
    },
    {
      "arxiv_id": "2503.24210v1",
      "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting",
      "title_zh": "DiET-GS：扩散先验和事件流辅助的运动去模糊 3D 高斯溅射\n",
      "authors": [
        "Seungjun Lee",
        "Gim Hee Lee"
      ],
      "abstract": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io",
      "tldr_zh": "本文提出了一种名为DiET-GS的框架，用于从模糊的多视角图像中重建清晰的3D表示，该方法结合了扩散先验和事件流辅助的运动去模糊3D高斯溅射(3D Gaussian Splatting)。该框架采用两阶段训练策略，有效利用了无模糊的事件流和扩散先验。通过引入事件双重积分约束3DGS，实现了准确的颜色和清晰的细节。此外，还提出了一种利用扩散先验来进一步增强边缘细节的技术。在合成和真实世界数据上的实验结果表明，与现有基线相比，DiET-GS能够生成质量明显更好的新视角。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Project Page: https://diet-gs.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.24210v1",
      "published_date": "2025-03-31 15:27:07 UTC",
      "updated_date": "2025-03-31 15:27:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:10:12.352673"
    },
    {
      "arxiv_id": "2503.24199v1",
      "title": "Agent-Based Simulations of Online Political Discussions: A Case Study on Elections in Germany",
      "title_zh": "基于智能体的在线政治讨论模拟：以德国选举为例的研究\n",
      "authors": [
        "Abdul Sittar",
        "Simon Münker",
        "Fabio Sartori",
        "Andreas Reitenbach",
        "Achim Rettinger",
        "Michael Mäs",
        "Alenka Guček",
        "Marko Grobelnik"
      ],
      "abstract": "User engagement on social media platforms is influenced by historical\ncontext, time constraints, and reward-driven interactions. This study presents\nan agent-based simulation approach that models user interactions, considering\npast conversation history, motivation, and resource constraints. Utilizing\nGerman Twitter data on political discourse, we fine-tune AI models to generate\nposts and replies, incorporating sentiment analysis, irony detection, and\noffensiveness classification. The simulation employs a myopic best-response\nmodel to govern agent behavior, accounting for decision-making based on\nexpected rewards. Our results highlight the impact of historical context on\nAI-generated responses and demonstrate how engagement evolves under varying\nconstraints.",
      "tldr_zh": "本文提出了一种基于Agent的仿真方法，用于模拟在线政治讨论，以德国选举为例。该方法考虑了用户的历史对话、动机和资源约束，利用德国Twitter数据训练AI模型来生成帖子和回复，并结合情感分析、反讽检测和攻击性分类。仿真采用近视最佳响应模型来管理Agent行为，根据预期奖励进行决策。研究结果表明，历史背景对AI生成的回复有显著影响，并且展示了在不同约束条件下用户参与度的演变过程。\n",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 3, ESWC, Workshop Paper",
      "pdf_url": "http://arxiv.org/pdf/2503.24199v1",
      "published_date": "2025-03-31 15:17:04 UTC",
      "updated_date": "2025-03-31 15:17:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:10:24.406493"
    },
    {
      "arxiv_id": "2503.24191v1",
      "title": "Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms",
      "title_zh": "将输出约束作为攻击面：利用结构化生成绕过 LLM 安全机制\n",
      "authors": [
        "Shuoming Zhang",
        "Jiacheng Zhao",
        "Ruiyuan Xu",
        "Xiaobing Feng",
        "Huimin Cui"
      ],
      "abstract": "Content Warning: This paper may contain unsafe or harmful content generated\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\nextensively used as tooling platforms through structured output APIs to ensure\nsyntax compliance so that robust integration with existing softwares like agent\nsystems, could be achieved. However, the feature enabling functionality of\ngrammar-guided structured output presents significant security vulnerabilities.\nIn this work, we reveal a critical control-plane attack surface orthogonal to\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\nAttack (CDA), a novel jailbreak class that weaponizes structured output\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\nprompts, CDA operates by embedding malicious intent in schema-level grammar\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\nattack success rates across proprietary and open-weight LLMs on five safety\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\nfindings identify a critical security blind spot in current LLM architectures\nand urge a paradigm shift in LLM safety to address control-plane\nvulnerabilities, as current mechanisms focused solely on data-plane threats\nleave critical systems exposed.",
      "tldr_zh": "该研究揭示了大型语言模型(LLMs)在结构化输出API中存在的安全漏洞，提出了一种新的攻击方式：约束解码攻击(CDA)。CDA不同于以往针对输入提示的攻击，它通过在模式层面的语法规则(控制平面)中嵌入恶意意图，同时保持表面提示(数据平面)的良性，从而绕过LLM的安全机制。研究者通过链式枚举攻击(Chain Enum Attack)验证了CDA的有效性，在包括GPT-4o和Gemini-2.0-flash在内的多种LLM上，攻击成功率高达96.2%。该研究强调了LLM安全领域对控制平面漏洞的关注，并呼吁从仅关注数据平面威胁转向解决控制平面漏洞。\n",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "15 pages, 13 figures, 4 tables Work In Progress",
      "pdf_url": "http://arxiv.org/pdf/2503.24191v1",
      "published_date": "2025-03-31 15:08:06 UTC",
      "updated_date": "2025-03-31 15:08:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:10:36.465229"
    },
    {
      "arxiv_id": "2503.24165v1",
      "title": "Predicting Targeted Therapy Resistance in Non-Small Cell Lung Cancer Using Multimodal Machine Learning",
      "title_zh": "利用多模态机器学习预测非小细胞肺癌的靶向治疗耐药性\n",
      "authors": [
        "Peiying Hua",
        "Andrea Olofson",
        "Faraz Farhadi",
        "Liesbeth Hondelink",
        "Gregory Tsongalis",
        "Konstantin Dragnev",
        "Dagmar Hoegemann Savellano",
        "Arief Suriawinata",
        "Laura Tafe",
        "Saeed Hassanpour"
      ],
      "abstract": "Lung cancer is the primary cause of cancer death globally, with non-small\ncell lung cancer (NSCLC) emerging as its most prevalent subtype. Among NSCLC\npatients, approximately 32.3% have mutations in the epidermal growth factor\nreceptor (EGFR) gene. Osimertinib, a third-generation EGFR-tyrosine kinase\ninhibitor (TKI), has demonstrated remarkable efficacy in the treatment of NSCLC\npatients with activating and T790M resistance EGFR mutations. Despite its\nestablished efficacy, drug resistance poses a significant challenge for\npatients to fully benefit from osimertinib. The absence of a standard tool to\naccurately predict TKI resistance, including that of osimertinib, remains a\ncritical obstacle. To bridge this gap, in this study, we developed an\ninterpretable multimodal machine learning model designed to predict patient\nresistance to osimertinib among late-stage NSCLC patients with activating EGFR\nmutations, achieving a c-index of 0.82 on a multi-institutional dataset. This\nmachine learning model harnesses readily available data routinely collected\nduring patient visits and medical assessments to facilitate precision lung\ncancer management and informed treatment decisions. By integrating various data\ntypes such as histology images, next generation sequencing (NGS) data,\ndemographics data, and clinical records, our multimodal model can generate\nwell-informed recommendations. Our experiment results also demonstrated the\nsuperior performance of the multimodal model over single modality models\n(c-index 0.82 compared with 0.75 and 0.77), thus underscoring the benefit of\ncombining multiple modalities in patient outcome prediction.",
      "tldr_zh": "该研究针对非小细胞肺癌(NSCLC)患者对奥希替尼(Osimertinib)等靶向药物产生耐药性的问题，开发了一种可解释的多模态机器学习模型，用于预测晚期NSCLC患者对奥希替尼的耐药性。该模型整合了组织学图像、二代测序(NGS)数据、人口统计学数据和临床记录等多种数据类型，在多中心数据集上实现了0.82的c-index。实验结果表明，该多模态模型优于单模态模型（c-index 0.82 vs 0.75/0.77），证明了结合多种模态数据在预测患者预后方面的优势，有助于实现精准肺癌管理和更明智的治疗决策。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24165v1",
      "published_date": "2025-03-31 14:47:02 UTC",
      "updated_date": "2025-03-31 14:47:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:10:48.545905"
    },
    {
      "arxiv_id": "2503.24150v1",
      "title": "Learning a Canonical Basis of Human Preferences from Binary Ratings",
      "title_zh": "从二元评级中学习人类偏好的规范基础\n",
      "authors": [
        "Kailas Vodrahalli",
        "Wei Wei",
        "James Zou"
      ],
      "abstract": "Recent advances in generative AI have been driven by alignment techniques\nsuch as reinforcement learning from human feedback (RLHF). RLHF and related\ntechniques typically involve constructing a dataset of binary or ranked choice\nhuman preferences and subsequently fine-tuning models to align with these\npreferences. This paper shifts the focus to understanding the preferences\nencoded in such datasets and identifying common human preferences. We find that\na small subset of 21 preference categories (selected from a set of nearly 5,000\ndistinct preferences) captures >89% of preference variation across individuals.\nThis small set of preferences is analogous to a canonical basis of human\npreferences, similar to established findings that characterize human variation\nin psychology or facial recognition studies. Through both synthetic and\nempirical evaluations, we confirm that our low-rank, canonical set of human\npreferences generalizes across the entire dataset and within specific topics.\nWe further demonstrate our preference basis' utility in model evaluation, where\nour preference categories offer deeper insights into model alignment, and in\nmodel training, where we show that fine-tuning on preference-defined subsets\nsuccessfully aligns the model accordingly.",
      "tldr_zh": "该论文研究了如何从二元人类偏好数据集中学习人类偏好的规范基。研究发现，仅用21个偏好类别（从近5000个不同偏好中选择）就能捕捉到个体间超过89%的偏好差异。这个小集合类似于人类偏好的规范基，类似于心理学或面部识别研究中已有的发现。通过合成和经验评估，证实了这种低秩、规范的人类偏好集合可以在整个数据集和特定主题中推广。此外，该偏好基在模型评估中也展现了效用，可以更深入地了解模型对齐情况；在模型训练中，基于偏好定义的子集进行微调可以成功地对齐模型。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.24150v1",
      "published_date": "2025-03-31 14:35:48 UTC",
      "updated_date": "2025-03-31 14:35:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:11:00.721340"
    },
    {
      "arxiv_id": "2503.24145v1",
      "title": "Resonance: Drawing from Memories to Imagine Positive Futures through AI-Augmented Journaling",
      "title_zh": "共鸣：通过人工智能增强的日记，从记忆中汲取灵感来构想积极的未来\n",
      "authors": [
        "Wazeer Zulfikar",
        "Treyden Chiaravalloti",
        "Jocelyn Shen",
        "Rosalind Picard",
        "Pattie Maes"
      ],
      "abstract": "People inherently use experiences of their past while imagining their future,\na capability that plays a crucial role in mental health. Resonance is an\nAI-powered journaling tool designed to augment this ability by offering\nAI-generated, action-oriented suggestions for future activities based on the\nuser's own past memories. Suggestions are offered when a new memory is logged\nand are followed by a prompt for the user to imagine carrying out the\nsuggestion. In a two-week randomized controlled study (N=55), we found that\nusing Resonance significantly improved mental health outcomes, reducing the\nusers' PHQ8 scores, a measure of current depression, and increasing their daily\npositive affect, particularly when they would likely act on the suggestion.\nNotably, the effectiveness of the suggestions was higher when they were\npersonal, novel, and referenced the user's logged memories. Finally, through\nopen-ended feedback, we discuss the factors that encouraged or hindered the use\nof the tool.",
      "tldr_zh": "Resonance是一款AI驱动的日记工具，旨在通过利用用户过去的记忆，提供AI生成的、面向行动的未来活动建议，从而增强用户想象积极未来的能力，并改善心理健康。当用户记录新的记忆时，Resonance会提供建议，并提示用户想象执行该建议。一项为期两周的随机对照研究(N=55)表明，使用Resonance显著改善了心理健康状况，降低了用户的PHQ8评分（衡量当前抑郁程度的指标），并增加了他们的日常积极情绪，尤其是在他们可能采取建议行动时。研究发现，个性化、新颖且参考用户记录记忆的建议效果更佳。\n",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "17 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.24145v1",
      "published_date": "2025-03-31 14:30:47 UTC",
      "updated_date": "2025-03-31 14:30:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:11:12.431807"
    },
    {
      "arxiv_id": "2503.24130v1",
      "title": "Graph Neural Network-Based Predictive Modeling for Robotic Plaster Printing",
      "title_zh": "基于图神经网络的机器人石膏打印预测建模\n",
      "authors": [
        "Diego Machain Rivera",
        "Selen Ercan Jenny",
        "Ping Hsun Tsai",
        "Ena Lloret-Fritschi",
        "Luis Salamanca",
        "Fernando Perez-Cruz",
        "Konstantinos E. Tatsis"
      ],
      "abstract": "This work proposes a Graph Neural Network (GNN) modeling approach to predict\nthe resulting surface from a particle based fabrication process. The latter\nconsists of spray-based printing of cementitious plaster on a wall and is\nfacilitated with the use of a robotic arm. The predictions are computed using\nthe robotic arm trajectory features, such as position, velocity and direction,\nas well as the printing process parameters. The proposed approach, based on a\nparticle representation of the wall domain and the end effector, allows for the\nadoption of a graph-based solution. The GNN model consists of an\nencoder-processor-decoder architecture and is trained using data from\nlaboratory tests, while the hyperparameters are optimized by means of a\nBayesian scheme. The aim of this model is to act as a simulator of the printing\nprocess, and ultimately used for the generation of the robotic arm trajectory\nand the optimization of the printing parameters, towards the materialization of\nan autonomous plastering process. The performance of the proposed model is\nassessed in terms of the prediction error against unseen ground truth data,\nwhich shows its generality in varied scenarios, as well as in comparison with\nthe performance of an existing benchmark model. The results demonstrate a\nsignificant improvement over the benchmark model, with notably better\nperformance and enhanced error scaling across prediction steps.",
      "tldr_zh": "该论文提出了一种基于图神经网络(GNN)的预测模型，用于模拟机器人石膏喷涂过程。该模型利用机器人手臂的轨迹特征（如位置、速度和方向）以及喷涂过程参数，预测喷涂在墙面上的石膏表面形态。通过将墙面和末端执行器表示为粒子，并采用encoder-processor-decoder架构的GNN模型，实现了对喷涂过程的模拟。实验结果表明，该模型在预测精度和误差缩放方面均优于现有基准模型，能够有效模拟喷涂过程，并有望用于机器人手臂轨迹生成和喷涂参数优化，最终实现自主石膏喷涂。\n",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24130v1",
      "published_date": "2025-03-31 14:15:00 UTC",
      "updated_date": "2025-03-31 14:15:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:11:24.479701"
    },
    {
      "arxiv_id": "2503.24110v1",
      "title": "Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition",
      "title_zh": "基于图像图式的智能体推理：具身认知的神经符号方法",
      "authors": [
        "François Olivier",
        "Zied Bouraoui"
      ],
      "abstract": "Despite advances in embodied AI, agent reasoning systems still struggle to\ncapture the fundamental conceptual structures that humans naturally use to\nunderstand and interact with their environment. To address this, we propose a\nnovel framework that bridges embodied cognition theory and agent systems by\nleveraging a formal characterization of image schemas, which are defined as\nrecurring patterns of sensorimotor experience that structure human cognition.\nBy customizing LLMs to translate natural language descriptions into formal\nrepresentations based on these sensorimotor patterns, we will be able to create\na neurosymbolic system that grounds the agent's understanding in fundamental\nconceptual structures. We argue that such an approach enhances both efficiency\nand interpretability while enabling more intuitive human-agent interactions\nthrough shared embodied understanding.",
      "tldr_zh": "该论文提出了一种新的框架，旨在通过利用图像图式的形式化表征来桥接具身认知理论和智能体系统，图像图式被定义为构建人类认知的重复性感觉运动体验模式。该框架定制LLM将自然语言描述转换为基于这些感觉运动模式的正式表示，从而创建一个神经符号系统，将智能体的理解建立在基本概念结构之上。 这种方法旨在提高效率和可解释性，并通过共享的具身理解实现更直观的人机交互。\n",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24110v1",
      "published_date": "2025-03-31 14:01:39 UTC",
      "updated_date": "2025-03-31 14:01:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:11:36.528437"
    },
    {
      "arxiv_id": "2503.24108v1",
      "title": "PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis",
      "title_zh": "PolypSegTrack：用于结肠镜视频分析的统一基础模型\n",
      "authors": [
        "Anwesa Choudhuri",
        "Zhongpai Gao",
        "Meng Zheng",
        "Benjamin Planche",
        "Terrence Chen",
        "Ziyan Wu"
      ],
      "abstract": "Early detection, accurate segmentation, classification and tracking of polyps\nduring colonoscopy are critical for preventing colorectal cancer. Many existing\ndeep-learning-based methods for analyzing colonoscopic videos either require\ntask-specific fine-tuning, lack tracking capabilities, or rely on\ndomain-specific pre-training. In this paper, we introduce\n\\textit{PolypSegTrack}, a novel foundation model that jointly addresses polyp\ndetection, segmentation, classification and unsupervised tracking in\ncolonoscopic videos. Our approach leverages a novel conditional mask loss,\nenabling flexible training across datasets with either pixel-level segmentation\nmasks or bounding box annotations, allowing us to bypass task-specific\nfine-tuning. Our unsupervised tracking module reliably associates polyp\ninstances across frames using object queries, without relying on any\nheuristics. We leverage a robust vision foundation model backbone that is\npre-trained unsupervisedly on natural images, thereby removing the need for\ndomain-specific pre-training. Extensive experiments on multiple polyp\nbenchmarks demonstrate that our method significantly outperforms existing\nstate-of-the-art approaches in detection, segmentation, classification, and\ntracking.",
      "tldr_zh": "该论文提出了一个名为PolypSegTrack的统一基础模型，用于结肠镜视频分析，旨在同时解决息肉的检测、分割、分类和无监督跟踪问题。该模型利用一种新的条件掩码损失，允许在具有像素级分割掩码或边界框注释的数据集上进行灵活训练，避免了特定于任务的微调。其无监督跟踪模块使用对象查询跨帧可靠地关联息肉实例，无需任何启发式方法。PolypSegTrack采用在自然图像上进行无监督预训练的视觉基础模型作为骨干，无需特定领域的预训练。在多个息肉基准测试上的实验表明，该方法在检测、分割、分类和跟踪方面显著优于现有技术。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24108v1",
      "published_date": "2025-03-31 14:00:21 UTC",
      "updated_date": "2025-03-31 14:00:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:11:48.672095"
    },
    {
      "arxiv_id": "2503.24062v1",
      "title": "Artificial Conversations, Real Results: Fostering Language Detection with Synthetic Data",
      "title_zh": "人工对话，真实结果：利用合成数据促进语言检测\n",
      "authors": [
        "Fatemeh Mohammadi",
        "Tommaso Romano",
        "Samira Maghool",
        "Paolo Ceravolo"
      ],
      "abstract": "Collecting high-quality training data is essential for fine-tuning Large\nLanguage Models (LLMs). However, acquiring such data is often costly and\ntime-consuming, especially for non-English languages such as Italian. Recently,\nresearchers have begun to explore the use of LLMs to generate synthetic\ndatasets as a viable alternative. This study proposes a pipeline for generating\nsynthetic data and a comprehensive approach for investigating the factors that\ninfluence the validity of synthetic data generated by LLMs by examining how\nmodel performance is affected by metrics such as prompt strategy, text length\nand target position in a specific task, i.e. inclusive language detection in\nItalian job advertisements. Our results show that, in most cases and across\ndifferent metrics, the fine-tuned models trained on synthetic data consistently\noutperformed other models on both real and synthetic test datasets. The study\ndiscusses the practical implications and limitations of using synthetic data\nfor language detection tasks with LLMs.",
      "tldr_zh": "该研究提出了一种利用大型语言模型(LLMs)生成合成数据，以促进语言检测任务的方法，尤其针对非英语语种如意大利语。该方法通过考察prompt策略、文本长度和目标位置等因素，研究了影响LLMs生成合成数据有效性的关键因素。实验结果表明，在意大利语招聘广告中进行包容性语言检测任务时，基于合成数据微调的模型在真实和合成测试数据集上均优于其他模型。该研究验证了使用合成数据进行语言检测任务的可行性，并讨论了其局限性。\n",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24062v1",
      "published_date": "2025-03-31 13:22:34 UTC",
      "updated_date": "2025-03-31 13:22:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:12:00.418168"
    },
    {
      "arxiv_id": "2503.24047v1",
      "title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents",
      "title_zh": "迈向科学智能：基于LLM的科学智能体综述\n",
      "authors": [
        "Shuo Ren",
        "Pu Jian",
        "Zhenjiang Ren",
        "Chunlin Leng",
        "Can Xie",
        "Jiajun Zhang"
      ],
      "abstract": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery.",
      "tldr_zh": "本文综述了基于大型语言模型(LLM)的科学智能体，这些智能体旨在自动化科学研究中的关键任务，例如假设生成、实验设计、数据分析和模拟。与通用LLM不同，这些专门的智能体集成了领域知识、高级工具集和验证机制，能够处理复杂的数据类型，确保可重复性，并推动科学突破。本文重点介绍了LLM科学智能体的架构、设计、基准、应用和伦理考量，并探讨了它们与通用智能体的区别以及它们在各个科学领域中如何推进研究。本文旨在为研究人员和从业人员提供一个全面的路线图，以利用这些智能体进行更高效、可靠和符合伦理的科学发现。\n",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.24047v1",
      "published_date": "2025-03-31 13:11:28 UTC",
      "updated_date": "2025-03-31 13:11:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:12:12.602675"
    },
    {
      "arxiv_id": "2503.24028v1",
      "title": "Pay More Attention to the Robustness of Prompt for Instruction Data Mining",
      "title_zh": "更关注提示的鲁棒性以进行指令数据挖掘\n",
      "authors": [
        "Qiang Wang",
        "Dawei Feng",
        "Xu Zhang",
        "Ao Shen",
        "Yang Xu",
        "Bo Ding",
        "Huaimin Wang"
      ],
      "abstract": "Instruction tuning has emerged as a paramount method for tailoring the\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\nhigh performance through fine-tuning with a limited quantity of high-quality\ninstruction data. Building upon this approach, we further explore the impact of\nprompt's robustness on the selection of high-quality instruction data. This\npaper proposes a pioneering framework of high-quality online instruction data\nmining for instruction tuning, focusing on the impact of prompt's robustness on\nthe data mining process. Our notable innovation, is to generate the adversarial\ninstruction data by conducting the attack for the prompt of online instruction\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\nto measure how much help the adversarial instruction data can provide to the\ngeneration of the corresponding response. Apart from it, we propose a novel\nAdversarial Instruction Output Embedding Consistency approach to select\nhigh-quality online instruction data. We conduct extensive experiments on two\nbenchmark datasets to assess the performance. The experimental results serve to\nunderscore the effectiveness of our proposed two methods. Moreover, the results\nunderscore the critical practical significance of considering prompt's\nrobustness.",
      "tldr_zh": "该论文提出了一种新的在线高质量指令数据挖掘框架，用于指令调优，重点关注prompt的鲁棒性对数据挖掘过程的影响。通过攻击在线指令数据的prompt生成对抗性指令数据，并提出了对抗性指令跟随难度(Adversarial Instruction-Following Difficulty)指标，用于衡量对抗性指令数据对生成相应响应的帮助程度。此外，还提出了一种新的对抗性指令输出嵌入一致性(Adversarial Instruction Output Embedding Consistency)方法来选择高质量的在线指令数据。在两个基准数据集上的大量实验表明，该方法是有效的，并强调了考虑prompt鲁棒性的重要性。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24028v1",
      "published_date": "2025-03-31 12:53:08 UTC",
      "updated_date": "2025-03-31 12:53:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:12:24.429361"
    },
    {
      "arxiv_id": "2503.24016v1",
      "title": "Bayesian Predictive Coding",
      "title_zh": "贝叶斯预测编码\n",
      "authors": [
        "Alexander Tschantz",
        "Magnus Koudahl",
        "Hampus Linander",
        "Lancelot Da Costa",
        "Conor Heins",
        "Jeff Beck",
        "Christopher Buckley"
      ],
      "abstract": "Predictive coding (PC) is an influential theory of information processing in\nthe brain, providing a biologically plausible alternative to backpropagation.\nIt is motivated in terms of Bayesian inference, as hidden states and parameters\nare optimised via gradient descent on variational free energy. However,\nimplementations of PC rely on maximum \\textit{a posteriori} (MAP) estimates of\nhidden states and maximum likelihood (ML) estimates of parameters, limiting\ntheir ability to quantify epistemic uncertainty. In this work, we investigate a\nBayesian extension to PC that estimates a posterior distribution over network\nparameters. This approach, termed Bayesian Predictive coding (BPC), preserves\nthe locality of PC and results in closed-form Hebbian weight updates. Compared\nto PC, our BPC algorithm converges in fewer epochs in the full-batch setting\nand remains competitive in the mini-batch setting. Additionally, we demonstrate\nthat BPC offers uncertainty quantification comparable to existing methods in\nBayesian deep learning, while also improving convergence properties. Together,\nthese results suggest that BPC provides a biologically plausible method for\nBayesian learning in the brain, as well as an attractive approach to\nuncertainty quantification in deep learning.",
      "tldr_zh": "本文提出了一种贝叶斯预测编码(Bayesian Predictive Coding, BPC)方法，扩展了传统的预测编码(Predictive Coding, PC)理论，旨在解决PC在量化认知不确定性方面的局限性。BPC通过估计网络参数的后验分布，保留了PC的局部性，并实现了闭式Hebbian权重更新。实验表明，在全批量设置下，BPC比PC收敛更快，在小批量设置下也具有竞争力。此外，BPC在不确定性量化方面与现有的贝叶斯深度学习方法相当，并改善了收敛性。研究结果表明，BPC为大脑中的贝叶斯学习提供了一种生物学上合理的模型，并为深度学习中的不确定性量化提供了一种有吸引力的方法。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24016v1",
      "published_date": "2025-03-31 12:40:50 UTC",
      "updated_date": "2025-03-31 12:40:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:12:36.646488"
    },
    {
      "arxiv_id": "2503.24009v1",
      "title": "Learning 3D-Gaussian Simulators from RGB Videos",
      "title_zh": "从 RGB 视频中学习 3D 高斯模拟器\n",
      "authors": [
        "Mikel Zhobro",
        "Andreas René Geist",
        "Georg Martius"
      ],
      "abstract": "Learning physics simulations from video data requires maintaining spatial and\ntemporal consistency, a challenge often addressed with strong inductive biases\nor ground-truth 3D information -- limiting scalability and generalization. We\nintroduce 3DGSim, a 3D physics simulator that learns object dynamics end-to-end\nfrom multi-view RGB videos. It encodes images into a 3D Gaussian particle\nrepresentation, propagates dynamics via a transformer, and renders frames using\n3D Gaussian splatting. By jointly training inverse rendering with a dynamics\ntransformer using a temporal encoding and merging layer, 3DGSimembeds physical\nproperties into point-wise latent vectors without enforcing explicit\nconnectivity constraints. This enables the model to capture diverse physical\nbehaviors, from rigid to elastic and cloth-like interactions, along with\nrealistic lighting effects that also generalize to unseen multi-body\ninteractions and novel scene edits.",
      "tldr_zh": "该论文提出了3DGSim，一个从多视角RGB视频中端到端学习物体动力学的3D物理模拟器。3DGSim将图像编码成3D Gaussian粒子表示，通过Transformer传播动力学，并使用3D Gaussian splatting渲染帧。通过联合训练逆渲染和动力学Transformer，并引入时间编码和融合层，3DGSim将物理属性嵌入到点状潜在向量中，无需显式的连接约束。这使得模型能够捕捉从刚性到弹性以及类似布料的交互等多种物理行为，以及逼真的光照效果，并推广到未见过的多体交互和新的场景编辑。\n",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24009v1",
      "published_date": "2025-03-31 12:33:59 UTC",
      "updated_date": "2025-03-31 12:33:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:12:48.609404"
    },
    {
      "arxiv_id": "2503.24008v1",
      "title": "H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding",
      "title_zh": "H2VU-Benchmark：分层整体视频理解的综合基准测试",
      "authors": [
        "Qi Wu",
        "Quanlong Zheng",
        "Yanhao Zhang",
        "Junlin Xie",
        "Jinguo Luo",
        "Kuo Wang",
        "Peng Liu",
        "Qingsong Xie",
        "Ru Zhen",
        "Haonan Lu",
        "Zhenyu Yang"
      ],
      "abstract": "With the rapid development of multimodal models, the demand for assessing\nvideo understanding capabilities has been steadily increasing. However,\nexisting benchmarks for evaluating video understanding exhibit significant\nlimitations in coverage, task diversity, and scene adaptability. These\nshortcomings hinder the accurate assessment of models' comprehensive video\nunderstanding capabilities. To tackle this challenge, we propose a hierarchical\nand holistic video understanding (H2VU) benchmark designed to evaluate both\ngeneral video and online streaming video comprehension. This benchmark\ncontributes three key features:\n  Extended video duration: Spanning videos from brief 3-second clips to\ncomprehensive 1.5-hour recordings, thereby bridging the temporal gaps found in\ncurrent benchmarks. Comprehensive assessment tasks: Beyond traditional\nperceptual and reasoning tasks, we have introduced modules for\ncountercommonsense comprehension and trajectory state tracking. These additions\ntest the models' deep understanding capabilities beyond mere prior knowledge.\nEnriched video data: To keep pace with the rapid evolution of current AI\nagents, we have expanded first-person streaming video datasets. This expansion\nallows for the exploration of multimodal models' performance in understanding\nstreaming videos from a first-person perspective. Extensive results from H2VU\nreveal that existing multimodal large language models (MLLMs) possess\nsubstantial potential for improvement in our newly proposed evaluation tasks.\nWe expect that H2VU will facilitate advancements in video understanding\nresearch by offering a comprehensive and in-depth analysis of MLLMs.",
      "tldr_zh": "H2VU-Benchmark 旨在全面评估多模态模型在视频理解方面的能力，弥补现有视频理解评测基准在覆盖范围、任务多样性和场景适应性方面的不足。该基准包含从3秒短视频到1.5小时长视频，弥合了现有基准的时间跨度差距。除了传统的感知和推理任务外，H2VU 还引入了反常识理解和轨迹状态跟踪模块，测试模型更深层次的理解能力。此外，H2VU 还扩展了第一人称视角流媒体视频数据集，探索多模态模型在理解流媒体视频方面的性能。实验结果表明，现有的多模态大语言模型(MLLMs)在 H2VU 新提出的评估任务中仍有很大的提升空间。H2VU 有望通过对 MLLM 进行全面和深入的分析，促进视频理解研究的进展。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24008v1",
      "published_date": "2025-03-31 12:32:51 UTC",
      "updated_date": "2025-03-31 12:32:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:13:00.728055"
    },
    {
      "arxiv_id": "2503.24007v1",
      "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
      "title_zh": "CITRAS：用于时间序列预测的基于协变量信息的 Transformer 模型\n",
      "authors": [
        "Yosuke Yamaguchi",
        "Issei Suemitsu",
        "Wenpeng Wei"
      ],
      "abstract": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
      "tldr_zh": "该论文提出了一种名为CITRAS的基于Transformer的时间序列预测模型，旨在解决实际应用中协变量信息利用的挑战。CITRAS采用patch-based架构，能够灵活地处理多个目标变量和协变量，涵盖过去和未来的预测范围。该模型引入了两种新颖的机制：Key-Value (KV) Shift和Attention Score Smoothing。KV Shift将未来已知的协变量无缝集成到目标变量的预测中，而Attention Score Smoothing通过平滑过去的注意力分数，将局部准确的patch-wise cross-variate依赖关系转化为全局的变量级依赖关系。实验结果表明，CITRAS在协变量信息和多元时间序列预测方面均取得了state-of-the-art的性能，证明了其利用cross-variate依赖关系提高预测准确性的能力。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.24007v1",
      "published_date": "2025-03-31 12:32:23 UTC",
      "updated_date": "2025-03-31 12:32:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:13:12.833073"
    },
    {
      "arxiv_id": "2503.24000v1",
      "title": "Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving",
      "title_zh": "重新思考用于大型语言模型服务的键值缓存压缩技术\n",
      "authors": [
        "Wei Gao",
        "Xinyu Zhou",
        "Peng Sun",
        "Tianwei Zhang",
        "Yonggang Wen"
      ],
      "abstract": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
      "tldr_zh": "本文重新审视了用于大语言模型(LLM)服务的Key-Value缓存(\\texttt{KV} \\texttt{cache})压缩技术。文章回顾了现有的\\texttt{KV} \\texttt{cache}压缩算法设计和benchmark研究，指出了其在性能测量方面的不足。通过实证评估，揭示了影响计算效率的两个关键问题：一是现有实现(如FlashAttention, PagedAttention)未针对生产级别的LLM服务进行优化，导致吞吐量性能欠佳；二是压缩\\texttt{KV} \\texttt{cache}可能导致输出更长，从而增加端到端延迟。此外，文章还深入研究了单个样本的准确性性能，揭示了\\texttt{KV} \\texttt{cache}压缩在处理特定LLM任务时的内在局限性。最后，作者开源了相关工具，旨在为未来的\\texttt{KV} \\texttt{cache}压缩研究及其在生产中的实际部署提供参考。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 18 figures, published to MLSys2025",
      "pdf_url": "http://arxiv.org/pdf/2503.24000v1",
      "published_date": "2025-03-31 12:23:31 UTC",
      "updated_date": "2025-03-31 12:23:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:13:24.863887"
    },
    {
      "arxiv_id": "2503.23993v1",
      "title": "DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model",
      "title_zh": "DenseFormer：通过条件扩散模型从稀疏深度和图像中学习密集深度图\n",
      "authors": [
        "Ming Yuan",
        "Sichao Wang",
        "Chuang Zhang",
        "Lei He",
        "Qing Xu",
        "Jianqiang Wang"
      ],
      "abstract": "The depth completion task is a critical problem in autonomous driving,\ninvolving the generation of dense depth maps from sparse depth maps and RGB\nimages. Most existing methods employ a spatial propagation network to\niteratively refine the depth map after obtaining an initial dense depth. In\nthis paper, we propose DenseFormer, a novel method that integrates the\ndiffusion model into the depth completion task. By incorporating the denoising\nmechanism of the diffusion model, DenseFormer generates the dense depth map by\nprogressively refining an initial random depth distribution through multiple\niterations. We propose a feature extraction module that leverages a feature\npyramid structure, along with multi-layer deformable attention, to effectively\nextract and integrate features from sparse depth maps and RGB images, which\nserve as the guiding condition for the diffusion process. Additionally, this\npaper presents a depth refinement module that applies multi-step iterative\nrefinement across various ranges to the dense depth results generated by the\ndiffusion process. The module utilizes image features enriched with multi-scale\ninformation and sparse depth input to further enhance the accuracy of the\npredicted depth map. Extensive experiments on the KITTI outdoor scene dataset\ndemonstrate that DenseFormer outperforms classical depth completion methods.",
      "tldr_zh": "该论文提出了一种名为DenseFormer的新方法，用于解决自动驾驶中从稀疏深度图和RGB图像生成稠密深度图的深度补全任务。DenseFormer将扩散模型引入深度补全，通过扩散模型的去噪机制，逐步细化初始随机深度分布来生成稠密深度图。该方法设计了一个特征提取模块，利用特征金字塔结构和多层可变形注意力，有效提取和融合稀疏深度图和RGB图像的特征，作为扩散过程的引导条件。此外，DenseFormer还包含一个深度细化模块，利用多尺度图像特征和稀疏深度输入，对扩散过程生成的稠密深度图进行多步迭代细化。在KITTI数据集上的实验表明，DenseFormer优于传统的深度补全方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23993v1",
      "published_date": "2025-03-31 12:11:01 UTC",
      "updated_date": "2025-03-31 12:11:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:13:36.727867"
    },
    {
      "arxiv_id": "2503.23989v1",
      "title": "Rubric Is All You Need: Enhancing LLM-based Code Evaluation With Question-Specific Rubrics",
      "title_zh": "只需规则：利用问题专属规则增强基于 LLM 的代码评估\n",
      "authors": [
        "Aditya Pathak",
        "Rachit Gandhi",
        "Vaibhav Uttam",
        "Devansh",
        "Yashwanth Nakka",
        "Aaryan Raj Jindal",
        "Pratyush Ghosh",
        "Arnav Ramamoorthy",
        "Shreyash Verma",
        "Aditya Mittal",
        "Aashna Ased",
        "Chirag Khatri",
        "Jagat Sesh Challa",
        "Dhruv Kumar"
      ],
      "abstract": "Since the disruption in LLM technology brought about by the release of GPT-3\nand ChatGPT, LLMs have shown remarkable promise in programming-related tasks.\nWhile code generation remains a popular field of research, code evaluation\nusing LLMs remains a problem with no conclusive solution. In this paper, we\nfocus on LLM-based code evaluation and attempt to fill in the existing gaps. We\npropose multi-agentic novel approaches using question-specific rubrics tailored\nto the problem statement, arguing that these perform better for logical\nassessment than the existing approaches that use question-agnostic rubrics. To\naddress the lack of suitable evaluation datasets, we introduce two datasets: a\nData Structures and Algorithms dataset containing 150 student submissions from\na popular Data Structures and Algorithms practice website, and an Object\nOriented Programming dataset comprising 80 student submissions from\nundergraduate computer science courses. In addition to using standard metrics\n(Spearman Correlation, Cohen's Kappa), we additionally propose a new metric\ncalled as Leniency, which quantifies evaluation strictness relative to expert\nassessment. Our comprehensive analysis demonstrates that question-specific\nrubrics significantly enhance logical assessment of code in educational\nsettings, providing better feedback aligned with instructional goals beyond\nmere syntactic correctness.",
      "tldr_zh": "该论文提出了一种基于问题特定评分细则(question-specific rubrics)的多智能体方法，用于提升大型语言模型(LLM)在代码评估方面的能力。作者认为，相比于使用与问题无关的评分细则，针对特定问题定制的评分细则能更好地进行逻辑评估。为解决缺乏合适评估数据集的问题，论文构建了两个数据集：数据结构与算法数据集和面向对象编程数据集。实验结果表明，问题特定的评分细则能够显著增强LLM在教育场景中对代码逻辑的评估，提供更符合教学目标的反馈，而不仅仅是语法正确性。此外，论文还提出了一种新的评估指标“Leniency”，用于量化相对于专家评估的评估严格程度。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.23989v1",
      "published_date": "2025-03-31 11:59:43 UTC",
      "updated_date": "2025-03-31 11:59:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:13:48.717648"
    },
    {
      "arxiv_id": "2503.23988v1",
      "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an Exploratory Study Using Low Computing Power Environments",
      "title_zh": "在多个云提供商中部署深度学习模型：一项使用低计算能力环境的探索性研究\n",
      "authors": [
        "Elayne Lemos",
        "Rodrigo Oliveira",
        "Jairson Rodrigues",
        "Rosalvo F. Oliveira Neto"
      ],
      "abstract": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
      "tldr_zh": "本研究探索了在三大云平台（AWS、Google Cloud、Azure）上部署深度学习模型的可行性和成本效益，以GECToR模型（用于语法纠错的深度学习解决方案）为例，在低计算能力环境下进行了实验。通过评估实时延迟、硬件使用率和成本，发现GPU在性能上表现出色，但平均成本比无GPU解决方案高300%。研究还表明，处理器缓存大小对于经济高效的CPU部署至关重要，与GPU相比，可降低50%以上的成本。结论是，即使没有GPU，基于云的深度学习推理解决方案也是可行且经济实惠的，尤其有利于资源受限的用户，如初创公司。\n",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.PF",
        "68T07, 68U01",
        "C.4; I.2.0; B.8.2"
      ],
      "primary_category": "cs.DC",
      "comment": "15 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.23988v1",
      "published_date": "2025-03-31 11:58:37 UTC",
      "updated_date": "2025-03-31 11:58:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:14:00.628357"
    },
    {
      "arxiv_id": "2503.23982v1",
      "title": "Deep Nets as Hamiltonians",
      "title_zh": "深度网络作为哈密顿量\n",
      "authors": [
        "Mike Winer",
        "Boris Hanin"
      ],
      "abstract": "Neural networks are complex functions of both their inputs and parameters.\nMuch prior work in deep learning theory analyzes the distribution of network\noutputs at a fixed a set of inputs (e.g. a training dataset) over random\ninitializations of the network parameters. The purpose of this article is to\nconsider the opposite situation: we view a randomly initialized Multi-Layer\nPerceptron (MLP) as a Hamiltonian over its inputs. For typical realizations of\nthe network parameters, we study the properties of the energy landscape induced\nby this Hamiltonian, focusing on the structure of near-global minimum in the\nlimit of infinite width. Specifically, we use the replica trick to perform an\nexact analytic calculation giving the entropy (log volume of space) at a given\nenergy. We further derive saddle point equations that describe the overlaps\nbetween inputs sampled iid from the Gibbs distribution induced by the random\nMLP. For linear activations we solve these saddle point equations exactly. But\nwe also solve them numerically for a variety of depths and activation\nfunctions, including $\\tanh, \\sin, \\text{ReLU}$, and shaped non-linearities. We\nfind even at infinite width a rich range of behaviors. For some\nnon-linearities, such as $\\sin$, for instance, we find that the landscapes of\nrandom MLPs exhibit full replica symmetry breaking, while shallow $\\tanh$ and\nReLU networks or deep shaped MLPs are instead replica symmetric.",
      "tldr_zh": "本文将随机初始化的多层感知机(MLP)视为关于其输入的哈密顿量，并研究了由该哈密顿量引起的能量景观的性质，重点关注无限宽度下近全局最小值的结构。通过副本技巧(replica trick)进行精确的解析计算，得到了给定能量下的熵（空间的对数体积）。推导了鞍点方程，描述了从随机MLP引起的吉布斯分布中独立同分布采样输入之间的重叠。对于线性激活函数，精确地求解了这些鞍点方程，并针对各种深度和激活函数（包括$\\tanh, \\sin, \\text{ReLU}$和异形非线性）进行了数值求解。研究发现，即使在无限宽度下，也存在丰富的行为范围。例如，对于某些非线性函数（如$\\sin$），随机MLP的景观表现出完全的副本对称性破缺(replica symmetry breaking)，而浅层$\\tanh$和ReLU网络或深层异形MLP则是副本对称的。\n",
      "categories": [
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.LG",
        "math.PR"
      ],
      "primary_category": "cond-mat.dis-nn",
      "comment": "19+7 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.23982v1",
      "published_date": "2025-03-31 11:51:10 UTC",
      "updated_date": "2025-03-31 11:51:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:14:13.154670"
    },
    {
      "arxiv_id": "2503.23972v1",
      "title": "Noise-based reward-modulated learning",
      "title_zh": "基于噪声的奖励调节学习\n",
      "authors": [
        "Jesús García Fernández",
        "Nasir Ahmad",
        "Marcel van Gerven"
      ],
      "abstract": "Recent advances in reinforcement learning (RL) have led to significant\nimprovements in task performance. However, training neural networks in an RL\nregime is typically achieved in combination with backpropagation, limiting\ntheir applicability in resource-constrained environments or when using\nnon-differentiable neural networks. While noise-based alternatives like\nreward-modulated Hebbian learning (RMHL) have been proposed, their performance\nhas remained limited, especially in scenarios with delayed rewards, which\nrequire retrospective credit assignment over time. Here, we derive a novel\nnoise-based learning rule that addresses these challenges. Our approach\ncombines directional derivative theory with Hebbian-like updates to enable\nefficient, gradient-free learning in RL. It features stochastic noisy neurons\nwhich can approximate gradients, and produces local synaptic updates modulated\nby a global reward signal. Drawing on concepts from neuroscience, our method\nuses reward prediction error as its optimization target to generate\nincreasingly advantageous behavior, and incorporates an eligibility trace to\nfacilitate temporal credit assignment in environments with delayed rewards. Its\nformulation relies on local information alone, making it compatible with\nimplementations in neuromorphic hardware. Experimental validation shows that\nour approach significantly outperforms RMHL and is competitive with BP-based\nbaselines, highlighting the promise of noise-based, biologically inspired\nlearning for low-power and real-time applications.",
      "tldr_zh": "该论文提出了一种基于噪声的奖励调节学习(Noise-based Reward-Modulated Learning)新规则，旨在解决传统强化学习(RL)中反向传播(Backpropagation)在资源受限环境或使用不可微神经网络时的局限性。该方法结合方向导数理论和Hebbian学习更新，利用随机噪声神经元近似梯度，并通过全局奖励信号调节局部突触更新。该方法使用奖励预测误差作为优化目标，并结合资格迹(Eligibility Trace)处理延迟奖励环境中的时间信用分配问题。实验结果表明，该方法显著优于RMHL，并与基于BP的基线方法具有竞争力，为低功耗和实时应用中基于噪声的生物启发式学习提供了可能。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23972v1",
      "published_date": "2025-03-31 11:35:23 UTC",
      "updated_date": "2025-03-31 11:35:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:14:24.787149"
    },
    {
      "arxiv_id": "2503.23956v1",
      "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference",
      "title_zh": "AirCache：激活跨模态相关性 KV 缓存压缩，实现高效的大型视觉-语言模型推理",
      "authors": [
        "Kai Huang",
        "Hao Zou",
        "Bochen Wang",
        "Ye Xi",
        "Zhen Xie",
        "Hao Wang"
      ],
      "abstract": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
      "tldr_zh": "为解决大型视觉语言模型(LVLMs)推理过程中KV缓存需求过大的瓶颈，该论文提出了AirCache，一种新型KV缓存压缩方法。AirCache通过研究LVLMs注意力机制中视觉和文本token之间的相关性，发现缓存的视觉token存在大量冗余。AirCache引入精英观察窗口评估视觉组件在KV缓存中的重要性，并开发自适应的层级预算分配策略。实验结果表明，AirCache仅保留10%的视觉KV缓存即可达到与完整缓存相当的性能，同时将解码延迟降低了29%到66%。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23956v1",
      "published_date": "2025-03-31 11:13:18 UTC",
      "updated_date": "2025-03-31 11:13:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:14:36.619901"
    },
    {
      "arxiv_id": "2503.23948v1",
      "title": "AI2Agent: An End-to-End Framework for Deploying AI Projects as Autonomous Agents",
      "title_zh": "AI2Agent：一个用于将 AI 项目部署为自主代理的端到端框架\n",
      "authors": [
        "Jiaxiang Chen",
        "Jingwei Shi",
        "Lei Gan",
        "Jiale Zhang",
        "Qingyu Zhang",
        "Dongqian Zhang",
        "Xin Pang",
        "Zhucong Li",
        "Yinghui Xu"
      ],
      "abstract": "As AI technology advances, it is driving innovation across industries,\nincreasing the demand for scalable AI project deployment. However, deployment\nremains a critical challenge due to complex environment configurations,\ndependency conflicts, cross-platform adaptation, and debugging difficulties,\nwhich hinder automation and adoption. This paper introduces AI2Agent, an\nend-to-end framework that automates AI project deployment through\nguideline-driven execution, self-adaptive debugging, and case \\& solution\naccumulation. AI2Agent dynamically analyzes deployment challenges, learns from\npast cases, and iteratively refines its approach, significantly reducing human\nintervention. To evaluate its effectiveness, we conducted experiments on 30 AI\ndeployment cases, covering TTS, text-to-image generation, image editing, and\nother AI applications. Results show that AI2Agent significantly reduces\ndeployment time and improves success rates. The code and demo video are now\npublicly accessible.",
      "tldr_zh": "AI2Agent是一个端到端的框架，旨在自动化AI项目的部署过程，解决环境配置复杂、依赖冲突和跨平台适配等难题。该框架通过指导方针驱动的执行、自适应调试以及案例和解决方案的积累，动态分析部署挑战，并从历史案例中学习，迭代优化部署方案，从而显著减少人工干预。在30个AI部署案例（包括TTS、文本到图像生成、图像编辑等应用）上的实验表明，AI2Agent能够显著缩短部署时间并提高成功率。该框架的代码和演示视频已公开。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23948v1",
      "published_date": "2025-03-31 10:58:34 UTC",
      "updated_date": "2025-03-31 10:58:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:14:48.558991"
    },
    {
      "arxiv_id": "2503.23934v1",
      "title": "Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in Discriminative and Generative AI Operations",
      "title_zh": "从绿色 MLOps 到绿色 GenOps：判别式和生成式 AI 运算中能源消耗的实证研究\n",
      "authors": [
        "Adrián Sánchez-Mompó",
        "Ioannis Mavromatis",
        "Peizheng Li",
        "Konstantinos Katsaros",
        "Aftab Khan"
      ],
      "abstract": "This study presents an empirical investigation into the energy consumption of\nDiscriminative and Generative AI models within real-world MLOps pipelines. For\nDiscriminative models, we examine various architectures and hyperparameters\nduring training and inference and identify energy-efficient practices. For\nGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily on\nenergy consumption across different model sizes and varying service requests.\nOur study employs software-based power measurements, ensuring ease of\nreplication across diverse configurations, models, and datasets. We analyse\nmultiple models and hardware setups to uncover correlations among various\nmetrics, identifying key contributors to energy consumption. The results\nindicate that for Discriminative models, optimising architectures,\nhyperparameters, and hardware can significantly reduce energy consumption\nwithout sacrificing performance. For LLMs, energy efficiency depends on\nbalancing model size, reasoning complexity, and request-handling capacity, as\nlarger models do not necessarily consume more energy when utilisation remains\nlow. This analysis provides practical guidelines for designing green and\nsustainable ML operations, emphasising energy consumption and carbon footprint\nreductions while maintaining performance. This paper can serve as a benchmark\nfor accurately estimating total energy use across different types of AI models.",
      "tldr_zh": "该研究对判别式和生成式AI模型在实际MLOps流程中的能耗进行了实证研究。针对判别式模型，研究检验了训练和推理过程中不同架构和超参数的能耗，并确定了节能实践。针对生成式AI，重点评估了大型语言模型(LLMs)在不同模型大小和服务请求下的能耗。研究结果表明，对于判别式模型，优化架构、超参数和硬件可以显著降低能耗而不牺牲性能。对于LLMs，能效取决于平衡模型大小、推理复杂性和请求处理能力，因为在利用率较低时，较大的模型不一定消耗更多的能量。该研究为设计绿色和可持续的ML运营提供了实践指南，强调降低能耗和碳足迹，同时保持性能。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published to MDPI Information - Artificial Intelligence Section",
      "pdf_url": "http://arxiv.org/pdf/2503.23934v1",
      "published_date": "2025-03-31 10:28:04 UTC",
      "updated_date": "2025-03-31 10:28:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:15:00.815844"
    },
    {
      "arxiv_id": "2503.23923v1",
      "title": "What the F*ck Is Artificial General Intelligence?",
      "title_zh": "到底什么是他*妈的通用人工智能？\n",
      "authors": [
        "Michael Timothy Bennett"
      ],
      "abstract": "Artificial general intelligence (AGI) is an established field of research.\nYet Melanie Mitchell and others have questioned if the term still has meaning.\nAGI has been subject to so much hype and speculation it has become something of\na Rorschach test. Mitchell points out that the debate will only be settled\nthrough long term, scientific investigation. To that end here is a short,\naccessible and provocative overview of AGI. I compare definitions of\nintelligence, settling on intelligence in terms of adaptation and AGI as an\nartificial scientist. Taking my queue from Sutton's Bitter Lesson I describe\ntwo foundational tools used to build adaptive systems: search and\napproximation. I compare pros, cons, hybrids and architectures like o3,\nAlphaGo, AERA, NARS and Hyperon. I then discuss overall meta-approaches to\nmaking systems behave more intelligently. I divide them into scale-maxing,\nsimp-maxing, w-maxing based on the Bitter Lesson, Ockham's and Bennett's\nRazors. These maximise resources, simplicity of form, and the weakness of\nconstraints on functionality. I discuss examples including AIXI, the free\nenergy principle and The Embiggening of language models. I conclude that though\nscale-maxed approximation dominates, AGI will be a fusion of tools and\nmeta-approaches. The Embiggening was enabled by improvements in hardware. Now\nthe bottlenecks are sample and energy efficiency.",
      "tldr_zh": "本文探讨了“什么是通用人工智能(AGI)？”这一问题，指出AGI领域存在炒作和定义模糊的问题。文章从适应性的角度定义智能，并将AGI视为一种人工科学家。作者借鉴Sutton的“苦涩的教训”，介绍了构建自适应系统的两个基础工具：搜索和近似，并比较了多种架构，如o3、AlphaGo、AERA、NARS和Hyperon。文章进一步讨论了提升系统智能的元方法，包括资源最大化(scale-maxing)、形式简化最大化(simp-maxing)和约束弱化最大化(w-maxing)。最后，作者认为AGI将是工具和元方法的融合，并指出当前AGI发展的瓶颈在于样本和能源效率。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint; 10 pages;",
      "pdf_url": "http://arxiv.org/pdf/2503.23923v1",
      "published_date": "2025-03-31 10:15:37 UTC",
      "updated_date": "2025-03-31 10:15:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:15:12.888931"
    },
    {
      "arxiv_id": "2503.23907v1",
      "title": "HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment",
      "title_zh": "HumanAesExpert：推进用于人体图像美学评估的多模态基础模型\n",
      "authors": [
        "Zhichao Liao",
        "Xiaokun Liu",
        "Wenyu Qin",
        "Qingyu Li",
        "Qiulin Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Long Zeng",
        "Pingfa Feng"
      ],
      "abstract": "Image Aesthetic Assessment (IAA) is a long-standing and challenging research\ntask. However, its subset, Human Image Aesthetic Assessment (HIAA), has been\nscarcely explored, even though HIAA is widely used in social media, AI\nworkflows, and related domains. To bridge this research gap, our work pioneers\na holistic implementation framework tailored for HIAA. Specifically, we\nintroduce HumanBeauty, the first dataset purpose-built for HIAA, which\ncomprises 108k high-quality human images with manual annotations. To achieve\ncomprehensive and fine-grained HIAA, 50K human images are manually collected\nthrough a rigorous curation process and annotated leveraging our trailblazing\n12-dimensional aesthetic standard, while the remaining 58K with overall\naesthetic labels are systematically filtered from public datasets. Based on the\nHumanBeauty database, we propose HumanAesExpert, a powerful Vision Language\nModel for aesthetic evaluation of human images. We innovatively design an\nExpert head to incorporate human knowledge of aesthetic sub-dimensions while\njointly utilizing the Language Modeling (LM) and Regression head. This approach\nempowers our model to achieve superior proficiency in both overall and\nfine-grained HIAA. Furthermore, we introduce a MetaVoter, which aggregates\nscores from all three heads, to effectively balance the capabilities of each\nhead, thereby realizing improved assessment precision. Extensive experiments\ndemonstrate that our HumanAesExpert models deliver significantly better\nperformance in HIAA than other state-of-the-art models. Our datasets, models,\nand codes are publicly released to advance the HIAA community. Project webpage:\nhttps://humanaesexpert.github.io/HumanAesExpert/",
      "tldr_zh": "该论文针对人体图像美学评估(HIAA)这一研究较少的领域，提出了一个整体实现框架。首先，构建了首个专门用于HIAA的数据集HumanBeauty，包含108k张高质量人体图像，并采用12维美学标准进行标注。其次，提出了HumanAesExpert，一个强大的视觉语言模型，用于评估人体图像的美学。该模型创新性地设计了一个Expert head，结合语言建模(LM)和回归头，以融合人类美学知识。此外，引入MetaVoter来平衡各个head的能力，提高评估精度。实验结果表明，HumanAesExpert模型在HIAA任务上优于其他SOTA模型。该数据集、模型和代码均已开源。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23907v1",
      "published_date": "2025-03-31 09:58:11 UTC",
      "updated_date": "2025-03-31 09:58:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:15:24.807468"
    },
    {
      "arxiv_id": "2503.23897v1",
      "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
      "title_zh": "基于视觉自回归模型的免训练文本引导图像编辑\n",
      "authors": [
        "Yufei Wang",
        "Lanqing Guo",
        "Zhihao Li",
        "Jiaxing Huang",
        "Pichao Wang",
        "Bihan Wen",
        "Jian Wang"
      ],
      "abstract": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
      "tldr_zh": "该论文提出了一种基于视觉自回归模型(VAR)的免训练文本引导图像编辑框架，旨在解决现有扩散模型和修正流模型在图像编辑中因反演不准确导致的错误传播和全局修改问题。该方法通过引入缓存机制存储原始图像的token索引和概率分布，捕捉源提示和图像之间的关系。同时，设计了一种自适应细粒度掩蔽策略，动态识别和约束相关区域的修改，防止意外更改。此外，token重组方法进一步完善了编辑过程，提高了多样性、保真度和控制力。实验结果表明，该方法在1K分辨率图像上的处理速度仅为1.2秒，在定量指标和视觉质量上均达到或超过了现有的基于扩散和修正流的方法。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23897v1",
      "published_date": "2025-03-31 09:46:56 UTC",
      "updated_date": "2025-03-31 09:46:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:15:36.858490"
    },
    {
      "arxiv_id": "2503.23895v1",
      "title": "Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement",
      "title_zh": "与其有财富，不如有智慧：用于测试时知识增强的动态参数检索增强生成",
      "authors": [
        "Yuqiao Tan",
        "Shizhu He",
        "Huanxuan Liao",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.",
      "tldr_zh": "该论文提出了一种动态参数检索增强生成(Dynamic Parametric RAG, DyPRAG)框架，旨在解决传统RAG方法中推理成本高和RAG幻觉问题。DyPRAG通过一个轻量级的参数转换器模型，将文档高效地转换为参数知识，从而在测试时动态增强LLM的知识。与Parametric RAG (PRAG)相比，DyPRAG降低了推理、训练和存储成本，并具有更好的泛化能力。实验结果表明，DyPRAG在多个数据集上表现出有效性和泛化能力，能够实现卓越的知识融合并减轻RAG幻觉。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.23895v1",
      "published_date": "2025-03-31 09:46:35 UTC",
      "updated_date": "2025-03-31 09:46:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:15:48.766914"
    },
    {
      "arxiv_id": "2503.23893v1",
      "title": "DiffScale: Continuous Downscaling and Bias Correction of Subseasonal Wind Speed Forecasts using Diffusion Models",
      "title_zh": "DiffScale：使用扩散模型对次季节风速预报进行连续降尺度和偏差校正\n",
      "authors": [
        "Maximilian Springenberg",
        "Noelia Otero",
        "Yuxin Xue",
        "Jackie Ma"
      ],
      "abstract": "Renewable resources are strongly dependent on local and large-scale weather\nsituations. Skillful subseasonal to seasonal (S2S) forecasts -- beyond two\nweeks and up to two months -- can offer significant socioeconomic advantages to\nthe energy sector. This study aims to enhance wind speed predictions using a\ndiffusion model with classifier-free guidance to downscale S2S forecasts of\nsurface wind speed. We propose DiffScale, a diffusion model that super-resolves\nspatial information for continuous downscaling factors and lead times.\nLeveraging weather priors as guidance for the generative process of diffusion\nmodels, we adopt the perspective of conditional probabilities on sampling\nsuper-resolved S2S forecasts. We aim to directly estimate the density\nassociated with the target S2S forecasts at different spatial resolutions and\nlead times without auto-regression or sequence prediction, resulting in an\nefficient and flexible model. Synthetic experiments were designed to\nsuper-resolve wind speed S2S forecasts from the European Center for\nMedium-Range Weather Forecast (ECMWF) from a coarse resolution to a finer\nresolution of ERA5 reanalysis data, which serves as a high-resolution target.\nThe innovative aspect of DiffScale lies in its flexibility to downscale\narbitrary scaling factors, enabling it to generalize across various grid\nresolutions and lead times -without retraining the model- while correcting\nmodel errors, making it a versatile tool for improving S2S wind speed\nforecasts. We achieve a significant improvement in prediction quality,\noutperforming baselines up to week 3.",
      "tldr_zh": "该研究提出了一种名为DiffScale的扩散模型，用于对次季节风速预测进行连续降尺度和偏差校正。DiffScale利用无分类器指导的扩散模型，通过天气先验知识指导生成过程，对ECMWF的粗分辨率风速预测进行空间超分辨率处理，使其达到ERA5再分析数据的精细分辨率。DiffScale的关键创新在于其灵活性，能够处理任意比例因子的降尺度，无需重新训练即可泛化到不同的网格分辨率和预测提前期。实验结果表明，DiffScale显著提高了预测质量，在3周内的预测表现优于基线模型，为改进次季节风速预测提供了一种通用工具。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 18 figures, preprint under review",
      "pdf_url": "http://arxiv.org/pdf/2503.23893v1",
      "published_date": "2025-03-31 09:44:28 UTC",
      "updated_date": "2025-03-31 09:44:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:16:00.767338"
    },
    {
      "arxiv_id": "2503.23888v1",
      "title": "MuseFace: Text-driven Face Editing via Diffusion-based Mask Generation Approach",
      "title_zh": "MuseFace：基于扩散的掩码生成方法实现文本驱动的人脸编辑\n",
      "authors": [
        "Xin Zhang",
        "Siting Huang",
        "Xiangyang Luo",
        "Yifan Xie",
        "Weijiang Yu",
        "Heng Chang",
        "Fei Ma",
        "Fei Yu"
      ],
      "abstract": "Face editing modifies the appearance of face, which plays a key role in\ncustomization and enhancement of personal images. Although much work have\nachieved remarkable success in text-driven face editing, they still face\nsignificant challenges as none of them simultaneously fulfill the\ncharacteristics of diversity, controllability and flexibility. To address this\nchallenge, we propose MuseFace, a text-driven face editing framework, which\nrelies solely on text prompt to enable face editing. Specifically, MuseFace\nintegrates a Text-to-Mask diffusion model and a semantic-aware face editing\nmodel, capable of directly generating fine-grained semantic masks from text and\nperforming face editing. The Text-to-Mask diffusion model provides\n\\textit{diversity} and \\textit{flexibility} to the framework, while the\nsemantic-aware face editing model ensures \\textit{controllability} of the\nframework. Our framework can create fine-grained semantic masks, making precise\nface editing possible, and significantly enhancing the controllability and\nflexibility of face editing models. Extensive experiments demonstrate that\nMuseFace achieves superior high-fidelity performance.",
      "tldr_zh": "MuseFace是一个文本驱动的面部编辑框架，旨在解决现有方法在多样性、可控性和灵活性方面的挑战。该框架集成了Text-to-Mask扩散模型和语义感知面部编辑模型，仅通过文本提示即可实现面部编辑。Text-to-Mask扩散模型提供多样性和灵活性，而语义感知面部编辑模型确保可控性。实验表明，MuseFace能够生成精细的语义mask，从而实现精确的面部编辑，并显著提高面部编辑模型的可控性和灵活性，实现了卓越的高保真性能。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 5 figures,IEEE International Conference on Multimedia & Expo\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2503.23888v1",
      "published_date": "2025-03-31 09:41:09 UTC",
      "updated_date": "2025-03-31 09:41:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:16:12.675840"
    },
    {
      "arxiv_id": "2503.23886v1",
      "title": "SchemaAgent: A Multi-Agents Framework for Generating Relational Database Schema",
      "title_zh": "SchemaAgent：用于生成关系数据库模式的多智能体框架\n",
      "authors": [
        "Qin Wang",
        "Youhuan Li",
        "Yansong Feng",
        "Si Chen",
        "Ziming Li",
        "Pan Zhang",
        "Zhichao Shi",
        "Yuequn Dou",
        "chuchu Gao",
        "Zebin Huang",
        "Zihui Si",
        "Yixuan Chen",
        "Zhaohai Sun",
        "Ke Tang",
        "Wenqiang Jin"
      ],
      "abstract": "The relational database design would output a schema based on user's\nrequirements, which defines table structures and their interrelated relations.\nTranslating requirements into accurate schema involves several non-trivial\nsubtasks demanding both database expertise and domain-specific knowledge. This\nposes unique challenges for automated design of relational databases. Existing\nefforts are mostly based on customized rules or conventional deep learning\nmodels, often producing suboptimal schema. Recently, large language models\n(LLMs) have significantly advanced intelligent application development across\nvarious domains. In this paper, we propose SchemaAgent, a unified LLM-based\nmulti-agent framework for the automated generation of high-quality database\nschema. SchemaAgent is the first to apply LLMs for schema generation, which\nemulates the workflow of manual schema design by assigning specialized roles to\nagents and enabling effective collaboration to refine their respective\nsubtasks. Schema generation is a streamlined workflow, where directly applying\nthe multi-agent framework may cause compounding impact of errors. To address\nthis, we incorporate dedicated roles for reflection and inspection, alongside\nan innovative error detection and correction mechanism to identify and rectify\nissues across various phases. For evaluation, we present a benchmark named\n\\textit{RSchema}, which contains more than 500 pairs of requirement description\nand schema. Experimental results on this benchmark demonstrate the superiority\nof our approach over mainstream LLMs for relational database schema generation.",
      "tldr_zh": "该论文提出了SchemaAgent，一个基于大型语言模型(LLM)的多智能体框架，用于自动化生成高质量的关系数据库模式(schema)。SchemaAgent模拟人工模式设计流程，通过分配专门角色给各个智能体并促进有效协作来优化子任务。为了解决直接应用多智能体框架可能导致的误差累积问题，SchemaAgent引入了专门用于反思和检查的角色，以及创新的错误检测和纠正机制。作者构建了一个名为RSchema的基准测试，包含超过500个需求描述和模式对。实验结果表明，SchemaAgent在关系数据库模式生成方面优于主流LLM。\n",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "19 pages, 16 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.23886v1",
      "published_date": "2025-03-31 09:39:19 UTC",
      "updated_date": "2025-03-31 09:39:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:16:24.826741"
    },
    {
      "arxiv_id": "2503.23875v1",
      "title": "GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models",
      "title_zh": "GenSwarm：通过语言模型实现可扩展的多机器人代码策略生成与部署\n",
      "authors": [
        "Wenkang Ji",
        "Huaben Chen",
        "Mingyang Chen",
        "Guobin Zhu",
        "Lufeng Xu",
        "Roderich Groß",
        "Rui Zhou",
        "Ming Cao",
        "Shiyu Zhao"
      ],
      "abstract": "The development of control policies for multi-robot systems traditionally\nfollows a complex and labor-intensive process, often lacking the flexibility to\nadapt to dynamic tasks. This has motivated research on methods to automatically\ncreate control policies. However, these methods require iterative processes of\nmanually crafting and refining objective functions, thereby prolonging the\ndevelopment cycle. This work introduces \\textit{GenSwarm}, an end-to-end system\nthat leverages large language models to automatically generate and deploy\ncontrol policies for multi-robot tasks based on simple user instructions in\nnatural language. As a multi-language-agent system, GenSwarm achieves zero-shot\nlearning, enabling rapid adaptation to altered or unseen tasks. The white-box\nnature of the code policies ensures strong reproducibility and\ninterpretability. With its scalable software and hardware architectures,\nGenSwarm supports efficient policy deployment on both simulated and real-world\nmulti-robot systems, realizing an instruction-to-execution end-to-end\nfunctionality that could prove valuable for robotics specialists and\nnon-specialists alike.The code of the proposed GenSwarm system is available\nonline: https://github.com/WindyLab/GenSwarm.",
      "tldr_zh": "GenSwarm是一个端到端的系统，利用大型语言模型(LLMs)根据自然语言指令自动生成和部署多机器人控制策略。作为一个多语言代理系统，GenSwarm实现了零样本学习，能够快速适应变化或未见过的任务。代码策略的白盒特性保证了强大的可重复性和可解释性。GenSwarm具有可扩展的软硬件架构，支持在仿真和真实世界的多机器人系统上高效部署策略，实现了从指令到执行的端到端功能。该系统对于机器人专家和非专家都具有潜在价值。\n",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23875v1",
      "published_date": "2025-03-31 09:26:34 UTC",
      "updated_date": "2025-03-31 09:26:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:16:36.660553"
    },
    {
      "arxiv_id": "2503.23862v2",
      "title": "Learned Image Compression and Restoration for Digital Pathology",
      "title_zh": "用于数字病理学的学习图像压缩与修复\n",
      "authors": [
        "SeonYeong Lee",
        "EonSeung Seong",
        "DongEon Lee",
        "SiYeoul Lee",
        "Yubin Cho",
        "Chunsu Park",
        "Seonho Kim",
        "MinKyung Seo",
        "YoungSin Ko",
        "MinWoo Kim"
      ],
      "abstract": "Digital pathology images play a crucial role in medical diagnostics, but\ntheir ultra-high resolution and large file sizes pose significant challenges\nfor storage, transmission, and real-time visualization. To address these\nissues, we propose CLERIC, a novel deep learning-based image compression\nframework designed specifically for whole slide images (WSIs). CLERIC\nintegrates a learnable lifting scheme and advanced convolutional techniques to\nenhance compression efficiency while preserving critical pathological details.\nOur framework employs a lifting-scheme transform in the analysis stage to\ndecompose images into low- and high-frequency components, enabling more\nstructured latent representations. These components are processed through\nparallel encoders incorporating Deformable Residual Blocks (DRB) and Recurrent\nResidual Blocks (R2B) to improve feature extraction and spatial adaptability.\nThe synthesis stage applies an inverse lifting transform for effective image\nreconstruction, ensuring high-fidelity restoration of fine-grained tissue\nstructures. We evaluate CLERIC on a digital pathology image dataset and compare\nits performance against state-of-the-art learned image compression (LIC)\nmodels. Experimental results demonstrate that CLERIC achieves superior\nrate-distortion (RD) performance, significantly reducing storage requirements\nwhile maintaining high diagnostic image quality. Our study highlights the\npotential of deep learning-based compression in digital pathology, facilitating\nefficient data management and long-term storage while ensuring seamless\nintegration into clinical workflows and AI-assisted diagnostic systems. Code\nand models are available at: https://github.com/pnu-amilab/CLERIC.",
      "tldr_zh": "本文提出了一种名为CLERIC的深度学习图像压缩框架，专门用于病理切片图像(WSIs)。CLERIC集成了可学习的提升方案和卷积技术，旨在提高压缩效率，同时保留关键的病理细节。该框架利用提升方案将图像分解为低频和高频分量，并通过Deformable Residual Blocks (DRB)和Recurrent Residual Blocks (R2B)并行编码器处理这些分量，以改善特征提取和空间适应性。实验结果表明，CLERIC在率失真(RD)性能方面优于现有的学习图像压缩(LIC)模型，能够在显著降低存储需求的同时，保持高诊断图像质量。该研究强调了基于深度学习的压缩技术在数字病理学中的潜力，有助于高效的数据管理和长期存储。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23862v2",
      "published_date": "2025-03-31 09:09:09 UTC",
      "updated_date": "2025-04-01 03:06:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:16:48.892651"
    },
    {
      "arxiv_id": "2503.23830v1",
      "title": "OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training",
      "title_zh": "OrchMLLM：通过批量后平衡编排多模态数据以加速多模态大型语言模型训练\n",
      "authors": [
        "Yijie Zheng",
        "Bangjun Xiao",
        "Lei Shi",
        "Xiaoyang Li",
        "Faming Wu",
        "Tianyu Li",
        "Xuefeng Xiao",
        "Yang Zhang",
        "Yuxuan Wang",
        "Shouda Liu"
      ],
      "abstract": "Multimodal large language models (MLLMs), such as GPT-4o, are garnering\nsignificant attention. During the exploration of MLLM training, we identified\nModality Composition Incoherence, a phenomenon that the proportion of a certain\nmodality varies dramatically across different examples. It exacerbates the\nchallenges of addressing mini-batch imbalances, which lead to uneven GPU\nutilization between Data Parallel (DP) instances and severely degrades the\nefficiency and scalability of MLLM training, ultimately affecting training\nspeed and hindering further research on MLLMs.\n  To address these challenges, we introduce OrchMLLM, a comprehensive framework\ndesigned to mitigate the inefficiencies in MLLM training caused by Modality\nComposition Incoherence. First, we propose Batch Post-Balancing Dispatcher, a\ntechnique that efficiently eliminates mini-batch imbalances in sequential data.\nAdditionally, we integrate MLLM Global Orchestrator into the training framework\nto orchestrate multimodal data and tackle the issues arising from Modality\nComposition Incoherence. We evaluate OrchMLLM across various MLLM sizes,\ndemonstrating its efficiency and scalability. Experimental results reveal that\nOrchMLLM achieves a Model FLOPs Utilization (MFU) of $41.6\\%$ when training an\n84B MLLM with three modalities on $2560$ H100 GPUs, outperforming Megatron-LM\nby up to $3.1\\times$ in throughput.",
      "tldr_zh": "该论文提出了OrchMLLM框架，旨在解决多模态大语言模型(MLLM)训练中由于模态组成不一致(Modality Composition Incoherence)导致的GPU利用率不均和训练效率低下的问题。OrchMLLM包含Batch Post-Balancing Dispatcher技术，用于消除序列数据中的mini-batch不平衡，以及MLLM Global Orchestrator，用于协调多模态数据。实验结果表明，在2560个H100 GPU上训练一个84B参数的MLLM时，OrchMLLM的Model FLOPs Utilization (MFU)达到41.6%，吞吐量比Megatron-LM提高了3.1倍。该框架显著提升了MLLM的训练效率和可扩展性。\n",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23830v1",
      "published_date": "2025-03-31 08:24:23 UTC",
      "updated_date": "2025-03-31 08:24:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:17:00.865714"
    },
    {
      "arxiv_id": "2503.23820v2",
      "title": "When Counterfactual Reasoning Fails: Chaos and Real-World Complexity",
      "title_zh": "当反事实推理失效时：混沌与真实世界的复杂性\n",
      "authors": [
        "Yahya Aalaila",
        "Gerrit Großmann",
        "Sumantrak Mukherjee",
        "Jonas Wahl",
        "Sebastian Vollmer"
      ],
      "abstract": "Counterfactual reasoning, a cornerstone of human cognition and\ndecision-making, is often seen as the 'holy grail' of causal learning, with\napplications ranging from interpreting machine learning models to promoting\nalgorithmic fairness. While counterfactual reasoning has been extensively\nstudied in contexts where the underlying causal model is well-defined,\nreal-world causal modeling is often hindered by model and parameter\nuncertainty, observational noise, and chaotic behavior. The reliability of\ncounterfactual analysis in such settings remains largely unexplored. In this\nwork, we investigate the limitations of counterfactual reasoning within the\nframework of Structural Causal Models. Specifically, we empirically investigate\n\\emph{counterfactual sequence estimation} and highlight cases where it becomes\nincreasingly unreliable. We find that realistic assumptions, such as low\ndegrees of model uncertainty or chaotic dynamics, can result in\ncounterintuitive outcomes, including dramatic deviations between predicted and\ntrue counterfactual trajectories. This work urges caution when applying\ncounterfactual reasoning in settings characterized by chaos and uncertainty.\nFurthermore, it raises the question of whether certain systems may pose\nfundamental limitations on the ability to answer counterfactual questions about\ntheir behavior.",
      "tldr_zh": "该研究探讨了在现实世界的复杂性和混沌环境下，反事实推理的局限性。尽管反事实推理在因果学习中被广泛应用，但当因果模型存在不确定性、观测噪声和混沌行为时，其可靠性受到挑战。研究者在结构因果模型(Structural Causal Models)框架下，通过实证研究反事实序列估计，发现即使在较低的模型不确定性或混沌动力学下，预测的反事实轨迹与真实轨迹也会出现显著偏差，导致反事实推理失效。这项工作警示人们在具有混沌和不确定性的环境中应用反事实推理时应谨慎，并提出了某些系统可能从根本上限制了回答关于其行为的反事实问题的能力。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23820v2",
      "published_date": "2025-03-31 08:14:51 UTC",
      "updated_date": "2025-04-01 08:57:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:17:12.909505"
    },
    {
      "arxiv_id": "2503.23819v1",
      "title": "Conformal uncertainty quantification to evaluate predictive fairness of foundation AI model for skin lesion classes across patient demographics",
      "title_zh": "共形不确定性量化，用于评估基础 AI 模型在患者人群中皮肤病变类别预测的公平性\n",
      "authors": [
        "Swarnava Bhattacharyya",
        "Umapada Pal",
        "Tapabrata Chakraborti"
      ],
      "abstract": "Deep learning based diagnostic AI systems based on medical images are\nstarting to provide similar performance as human experts. However these data\nhungry complex systems are inherently black boxes and therefore slow to be\nadopted for high risk applications like healthcare. This problem of lack of\ntransparency is exacerbated in the case of recent large foundation models,\nwhich are trained in a self supervised manner on millions of data points to\nprovide robust generalisation across a range of downstream tasks, but the\nembeddings generated from them happen through a process that is not\ninterpretable, and hence not easily trustable for clinical applications. To\naddress this timely issue, we deploy conformal analysis to quantify the\npredictive uncertainty of a vision transformer (ViT) based foundation model\nacross patient demographics with respect to sex, age and ethnicity for the\ntasks of skin lesion classification using several public benchmark datasets.\nThe significant advantage of this method is that conformal analysis is method\nindependent and it not only provides a coverage guarantee at population level\nbut also provides an uncertainty score for each individual. We used a\nmodel-agnostic dynamic F1-score-based sampling during model training, which\nhelped to stabilize the class imbalance and we investigate the effects on\nuncertainty quantification (UQ) with or without this bias mitigation step. Thus\nwe show how this can be used as a fairness metric to evaluate the robustness of\nthe feature embeddings of the foundation model (Google DermFoundation) and thus\nadvance the trustworthiness and fairness of clinical AI.",
      "tldr_zh": "该研究利用共形分析(Conformal Analysis)来量化基于Vision Transformer (ViT)的皮肤病灶分类AI基础模型在不同患者人群（性别、年龄、种族）上的预测不确定性，以此评估模型的预测公平性。共形分析的优势在于其方法独立性，既能提供群体层面的覆盖率保证，也能提供个体的不确定性评分。研究还采用了基于动态F1分数的采样方法来缓解类别不平衡问题，并探讨了该方法对不确定性量化的影响。结果表明，该方法可作为评估基础模型（Google DermFoundation）特征嵌入鲁棒性的公平性指标，从而提高临床AI的可信度和公平性。\n",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23819v1",
      "published_date": "2025-03-31 08:06:00 UTC",
      "updated_date": "2025-03-31 08:06:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:17:24.928012"
    },
    {
      "arxiv_id": "2503.23803v1",
      "title": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute",
      "title_zh": "思考得更深入，而非规模更大：通过扩展测试时计算来增强软件工程智能体\n",
      "authors": [
        "Yingwei Ma",
        "Binhua Li",
        "Yihong Dong",
        "Xue Jiang",
        "Rongyu Cao",
        "Jue Chen",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Recent advancements in software engineering agents have demonstrated\npromising capabilities in automating program improvements. However, their\nreliance on closed-source or resource-intensive models introduces significant\ndeployment challenges in private environments, prompting a critical question:\n\\textit{How can personally deployable open-source LLMs achieve comparable code\nreasoning performance?}\n  To this end, we propose a unified Test-Time Compute scaling framework that\nleverages increased inference-time computation instead of larger models. Our\nframework incorporates two complementary strategies: internal TTC and external\nTTC. Internally, we introduce a \\textit{development-contextualized trajectory\nsynthesis} method leveraging real-world software repositories to bootstrap\nmulti-stage reasoning processes, such as fault localization and patch\ngeneration. We further enhance trajectory quality through rejection sampling,\nrigorously evaluating trajectories along accuracy and complexity. Externally,\nwe propose a novel \\textit{development-process-based search} strategy guided by\nreward models and execution verification. This approach enables targeted\ncomputational allocation at critical development decision points, overcoming\nlimitations of existing \"end-point only\" verification methods.\n  Evaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves\na 46\\% issue resolution rate}, surpassing significantly larger models such as\nDeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical\nvalidation of the test-time scaling phenomenon within SWE agents, revealing\nthat \\textbf{models dynamically allocate more tokens to increasingly\nchallenging problems}, effectively enhancing reasoning capabilities. We\npublicly release all training data, models, and code to facilitate future\nresearch. https://github.com/yingweima2022/SWE-Reasoner",
      "tldr_zh": "该论文提出了一种Test-Time Compute (TTC) 扩展框架，旨在提升开源LLM在软件工程任务中的代码推理性能，而无需依赖更大的模型。该框架包含内部TTC和外部TTC两种策略。内部TTC通过开发上下文轨迹合成方法，利用真实软件仓库引导多阶段推理过程，并使用拒绝抽样提高轨迹质量。外部TTC则提出了一种基于开发过程的搜索策略，通过奖励模型和执行验证，在关键开发决策点进行计算分配。实验表明，一个32B的模型通过该框架在SWE-bench Verified上达到了46%的问题解决率，超过了DeepSeek R1 671B和OpenAI o1等更大的模型。研究还验证了测试时扩展现象，即模型会动态分配更多tokens来解决更具挑战性的问题，从而有效提升推理能力。\n",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23803v1",
      "published_date": "2025-03-31 07:31:32 UTC",
      "updated_date": "2025-03-31 07:31:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:17:37.157734"
    },
    {
      "arxiv_id": "2503.23798v1",
      "title": "Adaptive Layer-skipping in Pre-trained LLMs",
      "title_zh": "预训练大型语言模型中的自适应层跳跃\n",
      "authors": [
        "Xuan Luo",
        "Weizhi Wang",
        "Xifeng Yan"
      ],
      "abstract": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration.",
      "tldr_zh": "该论文提出了FlexiDepth，一种在预训练大语言模型(LLMs)中自适应调整Transformer层数的方法，旨在加速文本生成过程。FlexiDepth通过引入一个可插拔的路由器和适配器，实现了LLMs中的动态层跳跃，且无需修改原始参数。实验表明，FlexiDepth在Llama-3-8B模型上实现了8层（共32层）的跳跃，同时保持了100%的基准性能。研究发现，LLMs中不同token的计算需求差异显著，重复性token或固定短语需要较少的层，而涉及复杂计算或高不确定性的token需要更多层。作者开源了FlexiDepth及其数据集，以促进该领域的研究。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23798v1",
      "published_date": "2025-03-31 07:20:58 UTC",
      "updated_date": "2025-03-31 07:20:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:17:49.012739"
    },
    {
      "arxiv_id": "2503.23786v1",
      "title": "MGD-SAM2: Multi-view Guided Detail-enhanced Segment Anything Model 2 for High-Resolution Class-agnostic Segmentation",
      "title_zh": "MGD-SAM2：用于高分辨率类无关分割的多视角引导细节增强分割一切模型2\n",
      "authors": [
        "Haoran Shen",
        "Peixian Zhuang",
        "Jiahao Kou",
        "Yuxin Zeng",
        "Haoying Xu",
        "Jiangyun Li"
      ],
      "abstract": "Segment Anything Models (SAMs), as vision foundation models, have\ndemonstrated remarkable performance across various image analysis tasks.\nDespite their strong generalization capabilities, SAMs encounter challenges in\nfine-grained detail segmentation for high-resolution class-independent\nsegmentation (HRCS), due to the limitations in the direct processing of\nhigh-resolution inputs and low-resolution mask predictions, and the reliance on\naccurate manual prompts. To address these limitations, we propose MGD-SAM2\nwhich integrates SAM2 with multi-view feature interaction between a global\nimage and local patches to achieve precise segmentation. MGD-SAM2 incorporates\nthe pre-trained SAM2 with four novel modules: the Multi-view Perception Adapter\n(MPAdapter), the Multi-view Complementary Enhancement Module (MCEM), the\nHierarchical Multi-view Interaction Module (HMIM), and the Detail Refinement\nModule (DRM). Specifically, we first introduce MPAdapter to adapt the SAM2\nencoder for enhanced extraction of local details and global semantics in HRCS\nimages. Then, MCEM and HMIM are proposed to further exploit local texture and\nglobal context by aggregating multi-view features within and across\nmulti-scales. Finally, DRM is designed to generate gradually restored\nhigh-resolution mask predictions, compensating for the loss of fine-grained\ndetails resulting from directly upsampling the low-resolution prediction maps.\nExperimental results demonstrate the superior performance and strong\ngeneralization of our model on multiple high-resolution and normal-resolution\ndatasets. Code will be available at https://github.com/sevenshr/MGD-SAM2.",
      "tldr_zh": "该论文提出了MGD-SAM2，一种用于高分辨率无类别分割的多视角引导细节增强Segment Anything Model 2。MGD-SAM2通过整合SAM2以及多视角特征交互，在全局图像和局部图像块之间实现精确分割。该模型包含四个创新模块：多视角感知适配器(MPAdapter)、多视角互补增强模块(MCEM)、分层多视角交互模块(HMIM)和细节优化模块(DRM)。实验结果表明，MGD-SAM2在多个高分辨率和普通分辨率数据集上表现出卓越的性能和强大的泛化能力，尤其在精细细节分割方面有所提升。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23786v1",
      "published_date": "2025-03-31 07:02:32 UTC",
      "updated_date": "2025-03-31 07:02:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:18:00.846378"
    },
    {
      "arxiv_id": "2503.23781v1",
      "title": "DebFlow: Automating Agent Creation via Agent Debate",
      "title_zh": "DebFlow：通过智能体辩论实现智能体创建自动化\n",
      "authors": [
        "Jinwei Su",
        "Yinghui Xia",
        "Ronghua Shi",
        "Jianhui Wang",
        "Jianuo Huang",
        "Yijin Wang",
        "Tianyu Shi",
        "Yang Jingsong",
        "Lewei He"
      ],
      "abstract": "Large language models (LLMs) have demonstrated strong potential and\nimpressive performance in automating the generation and optimization of\nworkflows. However, existing approaches are marked by limited reasoning\ncapabilities, high computational demands, and significant resource\nrequirements. To address these issues, we propose DebFlow, a framework that\nemploys a debate mechanism to optimize workflows and integrates reflexion to\nimprove based on previous experiences. We evaluated our method across six\nbenchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approach\nachieved a 3\\% average performance improvement over the latest baselines,\ndemonstrating its effectiveness in diverse problem domains. In particular,\nduring training, our framework reduces resource consumption by 37\\% compared to\nthe state-of-the-art baselines. Additionally, we performed ablation studies.\nRemoving the Debate component resulted in a 4\\% performance drop across two\nbenchmark datasets, significantly greater than the 2\\% drop observed when the\nReflection component was removed. These findings strongly demonstrate the\ncritical role of Debate in enhancing framework performance, while also\nhighlighting the auxiliary contribution of reflexion to overall optimization.",
      "tldr_zh": "DebFlow 框架通过引入辩论机制和反思机制，旨在解决现有大型语言模型(LLMs)在自动化工作流程生成和优化方面的推理能力有限、计算需求高和资源需求大等问题。DebFlow利用辩论来优化工作流程，并结合反思来根据先前的经验进行改进。在HotpotQA、MATH和ALFWorld等六个基准数据集上的评估表明，DebFlow 比现有最佳模型平均性能提升 3%，同时训练期间资源消耗降低 37%。消融实验表明，辩论机制对框架性能至关重要，反思机制也对整体优化有辅助作用。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23781v1",
      "published_date": "2025-03-31 06:56:13 UTC",
      "updated_date": "2025-03-31 06:56:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:18:12.950890"
    },
    {
      "arxiv_id": "2503.23779v1",
      "title": "WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization",
      "title_zh": "WinoWhat：一个包含常识分类的释义 WinoGrande 句子的并行语料库\n",
      "authors": [
        "Ine Gevers",
        "Victor De Marez",
        "Luna De Bruyne",
        "Walter Daelemans"
      ],
      "abstract": "In this study, we take a closer look at how Winograd schema challenges can be\nused to evaluate common sense reasoning in LLMs. Specifically, we evaluate\ngenerative models of different sizes on the popular WinoGrande benchmark. We\nrelease WinoWhat, a new corpus, in which each instance of the WinoGrande\nvalidation set is paraphrased. Additionally, we evaluate the performance on the\nchallenge across five common sense knowledge categories, giving more\nfine-grained insights on what types of knowledge are more challenging for LLMs.\nSurprisingly, all models perform significantly worse on WinoWhat, implying that\nLLM reasoning capabilities are overestimated on WinoGrande. To verify whether\nthis is an effect of benchmark memorization, we match benchmark instances to\nLLM trainingdata and create two test-suites. We observe that memorization has a\nminimal effect on model performance on WinoGrande.",
      "tldr_zh": "该研究深入探讨了如何利用Winograd模式挑战来评估大型语言模型(LLMs)中的常识推理能力。作者构建了一个新的语料库WinoWhat，其中WinoGrande验证集中的每个实例都被改写。此外，该研究还评估了模型在五个常识知识类别上的表现，从而更细致地了解了LLMs在哪些类型的知识方面面临更大的挑战。实验结果表明，所有模型在WinoWhat上的表现都明显下降，这意味着LLMs在WinoGrande上的推理能力被高估了。通过将benchmark实例与LLM训练数据进行匹配，并创建两个测试集，研究人员发现记忆对模型在WinoGrande上的性能影响很小。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23779v1",
      "published_date": "2025-03-31 06:53:53 UTC",
      "updated_date": "2025-03-31 06:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:18:25.235452"
    },
    {
      "arxiv_id": "2503.23764v2",
      "title": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation for Efficient Medical Image Segmentation",
      "title_zh": "WaveFormer：一种具有小波驱动特征表示的 3D Transformer，用于高效的医学图像分割\n",
      "authors": [
        "Md Mahfuz Al Hasan",
        "Mahdi Zaman",
        "Abdul Jawad",
        "Alberto Santamaria-Pang",
        "Ho Hin Lee",
        "Ivan Tarapov",
        "Kyle See",
        "Md Shah Imran",
        "Antika Roy",
        "Yaser Pourmohammadi Fallah",
        "Navid Asadizanjani",
        "Reza Forghani"
      ],
      "abstract": "Transformer-based architectures have advanced medical image analysis by\neffectively modeling long-range dependencies, yet they often struggle in 3D\nsettings due to substantial memory overhead and insufficient capture of\nfine-grained local features. We address these limitations with WaveFormer, a\nnovel 3D-transformer that: i) leverages the fundamental frequency-domain\nproperties of features for contextual representation, and ii) is inspired by\nthe top-down mechanism of the human visual recognition system, making it a\nbiologically motivated architecture. By employing discrete wavelet\ntransformations (DWT) at multiple scales, WaveFormer preserves both global\ncontext and high-frequency details while replacing heavy upsampling layers with\nefficient wavelet-based summarization and reconstruction. This significantly\nreduces the number of parameters, which is critical for real-world deployment\nwhere computational resources and training times are constrained. Furthermore,\nthe model is generic and easily adaptable to diverse applications. Evaluations\non BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with\nstate-of-the-art methods while offering substantially lower computational\ncomplexity.",
      "tldr_zh": "WaveFormer是一种新颖的3D Transformer架构，旨在解决医学图像分割中Transformer模型因内存开销大和局部特征捕捉不足的问题。该模型利用离散小波变换(DWT)在多尺度上提取特征，保留全局上下文和高频细节，并用基于小波的总结和重构取代了传统的上采样层，从而显著减少了参数量。WaveFormer的设计灵感来源于人类视觉识别系统的自上而下机制，具有生物学动机。在BraTS2023, FLARE2021和KiTS2023数据集上的实验表明，WaveFormer在性能上与现有最佳方法相当，同时显著降低了计算复杂度。\n",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23764v2",
      "published_date": "2025-03-31 06:28:41 UTC",
      "updated_date": "2025-04-01 02:13:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:18:36.894650"
    },
    {
      "arxiv_id": "2503.23740v1",
      "title": "LANID: LLM-assisted New Intent Discovery",
      "title_zh": "LANID：LLM辅助的新意图发现\n",
      "authors": [
        "Lu Fan",
        "Jiashu Pu",
        "Rongsheng Zhang",
        "Xiao-Ming Wu"
      ],
      "abstract": "Task-oriented Dialogue Systems (TODS) often face the challenge of\nencountering new intents. New Intent Discovery (NID) is a crucial task that\naims to identify these novel intents while maintaining the capability to\nrecognize existing ones. Previous efforts to adapt TODS to new intents have\nstruggled with inadequate semantic representation or have depended on external\nknowledge, which is often not scalable or flexible. Recently, Large Language\nModels (LLMs) have demonstrated strong zero-shot capabilities; however, their\nscale can be impractical for real-world applications that involve extensive\nqueries. To address the limitations of existing NID methods by leveraging LLMs,\nwe propose LANID, a framework that enhances the semantic representation of\nlightweight NID encoders with the guidance of LLMs. Specifically, LANID employs\nthe $K$-nearest neighbors and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithms to sample selective utterance pairs from the\ntraining set. It then queries an LLM to ascertain the relationships between\nthese pairs. The data produced from this process is utilized to design a\ncontrastive fine-tuning task, which is then used to train a small encoder with\na contrastive triplet loss. Our experimental results demonstrate the efficacy\nof the proposed method across three distinct NID datasets, surpassing strong\nbaselines in both unsupervised and semi-supervised settings. Our code is\navailable at https://github.com/floatSDSDS/LANID.",
      "tldr_zh": "该论文提出了LANID，一个利用大型语言模型(LLMs)辅助的新意图发现(NID)框架，旨在提升面向任务的对话系统(TODS)处理新意图的能力。LANID通过K近邻和DBSCAN算法选择训练集中的utterance pairs，并利用LLM确定它们之间的关系，从而增强轻量级NID编码器的语义表示。基于这些关系，LANID设计了一个对比微调任务，使用对比triplet loss训练小型编码器。实验结果表明，LANID在三个不同的NID数据集上优于现有方法，在无监督和半监督设置下均表现出色。\n",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.23740v1",
      "published_date": "2025-03-31 05:34:32 UTC",
      "updated_date": "2025-03-31 05:34:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:18:48.890746"
    },
    {
      "arxiv_id": "2503.23731v1",
      "title": "Investigation of intelligent barbell squat coaching system based on computer vision and machine learning",
      "title_zh": "基于计算机视觉和机器学习的智能杠铃深蹲指导系统研究\n",
      "authors": [
        "Yinq-Rong Chern",
        "Yuhao Lee",
        "Hsiao-Ching Lin",
        "Guan-Ting Chen",
        "Ying-Hsien Chen",
        "Fu-Sung Lin",
        "Chih-Yao Chuang",
        "Jenn-Jier James Lien",
        "Chih-Hsien Huang"
      ],
      "abstract": "Purpose: Research has revealed that strength training can reduce the\nincidence of chronic diseases and physical deterioration at any age. Therefore,\nhaving a movement diagnostic system is crucial for training alone. Hence, this\nstudy developed an artificial intelligence and computer vision-based barbell\nsquat coaching system with a real-time mode that immediately diagnoses the\nissue and provides feedback after each squat. In addition, a replay mode allows\nusers to examine their previous squats and check their comments. Initially,\nfour primary characteristics of the barbell squat were identified: body joint\nangles, dorsiflexion, the ratio of knee-to-hip movement, and barbell stability.\nMethods: We collect 8,151 squats from 77 participants, categorizing them as\ngood squats and six issues. Then, we trained the diagnosis models with three\nmachine-learning architectures. Furthermore, this research applied the SHapley\nAdditive exPlanations (SHAP) method to enhance the accuracy of issue prediction\nand reduce the computation time by feature selection. Results: The F1 score of\nthe six issues reached 86.86%, 69.01%, 77.42%, 90.74%, 95.83%, and 100%. Each\nsquat diagnosis took less than 0.5 seconds. Finally, this study examined the\nefficacy of the proposed system with two groups of participants trained with\nand without the system. Subsequently, participants trained with the system\nexhibited substantial improvements in their squat technique, as assessed both\nby the system itself and by a professional weightlifting coach. Conclusion:\nThis is a comprehensive study that integrates artificial intelligence, computer\nvision and multivariable processing technologies, aimed at building a\nreal-time, user-friendly barbell squat feedback and training system.",
      "tldr_zh": "该研究开发了一套基于计算机视觉和机器学习的智能杠铃深蹲指导系统，旨在帮助用户纠正深蹲姿势。系统通过实时模式和回放模式提供即时诊断和反馈。研究人员首先确定了深蹲的四个关键特征：身体关节角度、背屈、膝盖与髋部运动比例以及杠铃稳定性。他们收集了77名参与者的8151次深蹲数据，并将其分为良好深蹲和六种问题类型。利用三种机器学习架构训练诊断模型，并采用SHAP方法进行特征选择以提高准确率并减少计算时间。实验结果表明，该系统对六种问题的F1分数分别达到86.86%, 69.01%, 77.42%, 90.74%, 95.83%和100%，且每次深蹲诊断耗时不到0.5秒。通过对比实验，使用该系统训练的参与者在深蹲技术上表现出显著提升。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23731v1",
      "published_date": "2025-03-31 05:08:52 UTC",
      "updated_date": "2025-03-31 05:08:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:19:01.412695"
    },
    {
      "arxiv_id": "2503.23730v1",
      "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language",
      "title_zh": "KOFFVQA：韩语大型视觉-语言模型的客观评估自由形式 VQA 基准\n",
      "authors": [
        "Yoonshik Kim",
        "Jaeyoon Jung"
      ],
      "abstract": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
      "tldr_zh": "该论文提出了KOFFVQA，一个用于评估大型视觉语言模型(VLMs)在韩语环境下表现的开放式VQA基准。现有VLM评估方法存在主观性和可靠性问题，且缺乏韩语基准。KOFFVQA包含275个精心设计的图像-问题对，并为每个问题制定了包含10个VLM性能方面的评分标准。这种客观的评分标准允许使用小型开源模型进行可靠的评估，解决了传统评估方法的主观性问题。实验验证了基于预定义评分标准的评估方法比现有方法更可靠，并对大量现有VLM在KOFFVQA上进行了评估。该评估代码已开源。\n",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPRW 2025, Workshop on Benchmarking and Expanding AI\n  Multimodal Approaches",
      "pdf_url": "http://arxiv.org/pdf/2503.23730v1",
      "published_date": "2025-03-31 05:04:25 UTC",
      "updated_date": "2025-03-31 05:04:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:19:12.986756"
    },
    {
      "arxiv_id": "2503.23721v1",
      "title": "Unimodal-driven Distillation in Multimodal Emotion Recognition with Dynamic Fusion",
      "title_zh": "基于动态融合的单模态驱动多模态情感识别蒸馏方法\n",
      "authors": [
        "Jiagen Li",
        "Rui Yu",
        "Huihao Huang",
        "Huaicheng Yan"
      ],
      "abstract": "Multimodal Emotion Recognition in Conversations (MERC) identifies emotional\nstates across text, audio and video, which is essential for intelligent\ndialogue systems and opinion analysis. Existing methods emphasize heterogeneous\nmodal fusion directly for cross-modal integration, but often suffer from\ndisorientation in multimodal learning due to modal heterogeneity and lack of\ninstructive guidance. In this work, we propose SUMMER, a novel heterogeneous\nmultimodal integration framework leveraging Mixture of Experts with\nHierarchical Cross-modal Fusion and Interactive Knowledge Distillation. Key\ncomponents include a Sparse Dynamic Mixture of Experts (SDMoE) for capturing\ndynamic token-wise interactions, a Hierarchical Cross-Modal Fusion (HCMF) for\neffective fusion of heterogeneous modalities, and Interactive Knowledge\nDistillation (IKD), which uses a pre-trained unimodal teacher to guide\nmultimodal fusion in latent and logit spaces. Experiments on IEMOCAP and MELD\nshow SUMMER outperforms state-of-the-art methods, particularly in recognizing\nminority and semantically similar emotions.",
      "tldr_zh": "本文提出了一种名为SUMMER的新型异构多模态融合框架，用于对话中的多模态情感识别(MERC)任务。该框架利用Sparse Dynamic Mixture of Experts (SDMoE)捕获动态的token-wise交互，并通过Hierarchical Cross-Modal Fusion (HCMF)有效融合异构模态。此外，Interactive Knowledge Distillation (IKD)利用预训练的单模态教师模型在潜在空间和logit空间指导多模态融合。在IEMOCAP和MELD数据集上的实验表明，SUMMER优于现有方法，尤其是在识别少数和语义相似的情感方面表现突出。\n",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23721v1",
      "published_date": "2025-03-31 04:43:10 UTC",
      "updated_date": "2025-03-31 04:43:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:19:24.783442"
    },
    {
      "arxiv_id": "2503.23713v1",
      "title": "GNN-Based Candidate Node Predictor for Influence Maximization in Temporal Graphs",
      "title_zh": "基于 GNN 的时序图影响力最大化候选节点预测器\n",
      "authors": [
        "Priyanka Gautam",
        "Balasubramaniam Natarajan",
        "Sai Munikoti",
        "S M Ferdous",
        "Mahantesh Halappanavar"
      ],
      "abstract": "In an age where information spreads rapidly across social media, effectively\nidentifying influential nodes in dynamic networks is critical. Traditional\ninfluence maximization strategies often fail to keep up with rapidly evolving\nrelationships and structures, leading to missed opportunities and\ninefficiencies. To address this, we propose a novel learning-based approach\nintegrating Graph Neural Networks (GNNs) with Bidirectional Long Short-Term\nMemory (BiLSTM) models. This hybrid framework captures both structural and\ntemporal dynamics, enabling accurate prediction of candidate nodes for seed set\nselection. The bidirectional nature of BiLSTM allows our model to analyze\npatterns from both past and future network states, ensuring adaptability to\nchanges over time. By dynamically adapting to graph evolution at each time\nsnapshot, our approach improves seed set calculation efficiency, achieving an\naverage of 90% accuracy in predicting potential seed nodes across diverse\nnetworks. This significantly reduces computational overhead by optimizing the\nnumber of nodes evaluated for seed selection. Our method is particularly\neffective in fields like viral marketing and social network analysis, where\nunderstanding temporal dynamics is crucial.",
      "tldr_zh": "该论文提出了一种基于图神经网络(GNN)的候选节点预测器，用于解决时序图中的影响力最大化问题。该方法结合GNN和双向长短期记忆网络(BiLSTM)，捕捉网络结构和时间动态信息，从而更准确地预测种子节点。BiLSTM的双向性使其能够分析过去和未来的网络状态，适应随时间的变化。该方法能以平均90%的准确率预测潜在的种子节点，从而优化种子节点选择的数量，显著降低计算开销。该方法在病毒式营销和社交网络分析等领域非常有效，这些领域理解时间动态至关重要。\n",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "9 pages, 5 figures, Accepted in AAAI25 to AI4TS Workshop@AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.23713v1",
      "published_date": "2025-03-31 04:28:37 UTC",
      "updated_date": "2025-03-31 04:28:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:19:36.949779"
    },
    {
      "arxiv_id": "2503.23708v1",
      "title": "Towards Benchmarking and Assessing the Safety and Robustness of Autonomous Driving on Safety-critical Scenarios",
      "title_zh": "面向基准测试与评估自动驾驶在安全关键场景中的安全性和鲁棒性\n",
      "authors": [
        "Jingzheng Li",
        "Xianglong Liu",
        "Shikui Wei",
        "Zhijun Chen",
        "Bing Li",
        "Qing Guo",
        "Xianqi Yang",
        "Yanjun Pu",
        "Jiakai Wang"
      ],
      "abstract": "Autonomous driving has made significant progress in both academia and\nindustry, including performance improvements in perception task and the\ndevelopment of end-to-end autonomous driving systems. However, the safety and\nrobustness assessment of autonomous driving has not received sufficient\nattention. Current evaluations of autonomous driving are typically conducted in\nnatural driving scenarios. However, many accidents often occur in edge cases,\nalso known as safety-critical scenarios. These safety-critical scenarios are\ndifficult to collect, and there is currently no clear definition of what\nconstitutes a safety-critical scenario. In this work, we explore the safety and\nrobustness of autonomous driving in safety-critical scenarios. First, we\nprovide a definition of safety-critical scenarios, including static traffic\nscenarios such as adversarial attack scenarios and natural distribution shifts,\nas well as dynamic traffic scenarios such as accident scenarios. Then, we\ndevelop an autonomous driving safety testing platform to comprehensively\nevaluate autonomous driving systems, encompassing not only the assessment of\nperception modules but also system-level evaluations. Our work systematically\nconstructs a safety verification process for autonomous driving, providing\ntechnical support for the industry to establish standardized test framework and\nreduce risks in real-world road deployment.",
      "tldr_zh": "该研究关注自动驾驶在安全关键场景下的安全性和鲁棒性评估问题，指出当前评估主要集中于自然驾驶场景，缺乏对事故边缘情况的考量。论文首先定义了安全关键场景，包括静态（对抗攻击、自然分布偏移）和动态（事故场景）两类。然后，开发了一个自动驾驶安全测试平台，用于全面评估自动驾驶系统，包括感知模块和系统层面的评估。该研究旨在为自动驾驶构建系统的安全验证流程，为行业建立标准化测试框架提供技术支持，并降低实际道路部署的风险。\n",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23708v1",
      "published_date": "2025-03-31 04:13:32 UTC",
      "updated_date": "2025-03-31 04:13:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:19:48.931084"
    },
    {
      "arxiv_id": "2503.23668v2",
      "title": "MolGround: A Benchmark for Molecular Grounding",
      "title_zh": "MolGround：分子基础基准测试\n",
      "authors": [
        "Jiaxin Wu",
        "Ting Zhang",
        "Rubing Chen",
        "Wengyu Zhang",
        "Chen Jason Zhang",
        "Xiaoyong Wei",
        "Li Qing"
      ],
      "abstract": "Current molecular understanding approaches predominantly focus on the\ndescriptive aspect of human perception, providing broad, topic-level insights.\nHowever, the referential aspect -- linking molecular concepts to specific\nstructural components -- remains largely unexplored. To address this gap, we\npropose a molecular grounding benchmark designed to evaluate a model's\nreferential abilities. We align molecular grounding with established\nconventions in NLP, cheminformatics, and molecular science, showcasing the\npotential of NLP techniques to advance molecular understanding within the AI\nfor Science movement. Furthermore, we constructed the largest molecular\nunderstanding benchmark to date, comprising 79k QA pairs, and developed a\nmulti-agent grounding prototype as proof of concept. This system outperforms\nexisting models, including GPT-4o, and its grounding outputs have been\nintegrated to enhance traditional tasks such as molecular captioning and ATC\n(Anatomical, Therapeutic, Chemical) classification.",
      "tldr_zh": "本文提出了一个分子接地(Molecular Grounding)基准测试MolGround，旨在评估模型将分子概念与特定结构组件联系起来的参照能力。该基准包含79k个QA对，是目前最大的分子理解基准。作者开发了一个多智能体接地原型系统，该系统优于包括GPT-4o在内的现有模型。该系统的接地输出已用于增强传统的分子描述和ATC分类任务。MolGround的提出填补了现有分子理解方法在参照方面的空白，并展示了NLP技术在AI for Science运动中促进分子理解的潜力。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23668v2",
      "published_date": "2025-03-31 02:23:16 UTC",
      "updated_date": "2025-04-01 06:49:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:20:01.033431"
    },
    {
      "arxiv_id": "2503.23641v1",
      "title": "Remarks on the Polyak-Lojasiewicz inequality and the convergence of gradient systems",
      "title_zh": "关于Polyak-Lojasiewicz不等式和梯度系统收敛性的评述\n",
      "authors": [
        "Arthur Castello B. de Oliveira",
        "Leilei Cui",
        "Eduardo D. Sontag"
      ],
      "abstract": "This work explores generalizations of the Polyak-Lojasiewicz inequality (PLI)\nand their implications for the convergence behavior of gradient flows in\noptimization problems. Motivated by the continuous-time linear quadratic\nregulator (CT-LQR) policy optimization problem -- where only a weaker version\nof the PLI is characterized in the literature -- this work shows that while\nweaker conditions are sufficient for global convergence to, and optimality of\nthe set of critical points of the cost function, the \"profile\" of the gradient\nflow solution can change significantly depending on which \"flavor\" of\ninequality the cost satisfies. After a general theoretical analysis, we focus\non fitting the CT-LQR policy optimization problem to the proposed framework,\nshowing that, in fact, it can never satisfy a PLI in its strongest form. We\nfollow up our analysis with a brief discussion on the difference between\ncontinuous- and discrete-time LQR policy optimization, and end the paper with\nsome intuition on the extension of this framework to optimization problems with\nL1 regularization and solved through proximal gradient flows.",
      "tldr_zh": "本文研究了Polyak-Lojasiewicz不等式(PLI)的推广及其对优化问题中梯度流收敛行为的影响。受连续时间线性二次调节器(CT-LQR)策略优化问题的启发，该研究表明，较弱的条件足以保证全局收敛到成本函数的临界点集并达到最优，但梯度流解的“轮廓”会因成本满足哪种不等式“类型”而发生显著变化。在一般理论分析之后，文章重点将CT-LQR策略优化问题纳入提出的框架，表明它实际上永远无法满足最强的PLI形式。随后，文章简要讨论了连续和离散时间LQR策略优化之间的差异，并以一些关于将该框架扩展到具有L1正则化并通过近端梯度流解决的优化问题的直觉作为结尾。\n",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23641v1",
      "published_date": "2025-03-31 00:59:56 UTC",
      "updated_date": "2025-03-31 00:59:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:20:13.285427"
    },
    {
      "arxiv_id": "2503.23633v1",
      "title": "GIScience in the Era of Artificial Intelligence: A Research Agenda Towards Autonomous GIS",
      "title_zh": "人工智能时代下的地理信息科学：迈向自主地理信息系统的研究议程\n",
      "authors": [
        "Zhenlong Li",
        "Huan Ning",
        "Song Gao",
        "Krzysztof Janowicz",
        "Wenwen Li",
        "Samantha T. Arundel",
        "Chaowei Yang",
        "Budhendra Bhaduri",
        "Shaowen Wang",
        "A-Xing Zhu",
        "Mark Gahegan",
        "Shashi Shekhar",
        "Xinyue Ye",
        "Grant McKenzie",
        "Guido Cervone",
        "Michael E. Hodgson"
      ],
      "abstract": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcend the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we elaborate on the concept\nof autonomous GIS and present a framework that defines its five autonomous\ngoals, five levels of autonomy, five core functions, and three operational\nscales. We demonstrate how autonomous GIS could perform geospatial data\nretrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision cores, autonomous\nmodeling, and examining the ethical and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance solutions to pressing global\nchallenges.",
      "tldr_zh": "本文提出了“自主GIS”的概念，旨在利用大型语言模型(LLMs)作为决策核心，推动地理信息系统(GIS)向自主化方向发展。该研究阐述了自主GIS的框架，包括五个自主目标、五个自主级别、五个核心功能和三个操作尺度。通过四个概念验证的GIS agents，展示了自主GIS在地理空间数据检索、空间分析和地图制作方面的应用。最后，文章指出了关键挑战和未来研究方向，例如微调和自我增长的决策核心、自主建模，以及自主GIS的伦理和实践影响。该研究为GIScience的范式转变奠定了基础，展望了GIS超越传统工作流程，自主推理、推导和创新，从而推进解决紧迫的全球性挑战的未来。\n",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23633v1",
      "published_date": "2025-03-31 00:12:48 UTC",
      "updated_date": "2025-03-31 00:12:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:20:25.196178"
    },
    {
      "arxiv_id": "2503.23631v1",
      "title": "Intrinsically-Motivated Humans and Agents in Open-World Exploration",
      "title_zh": "开放世界探索中，由内驱力驱动的人类与智能体\n",
      "authors": [
        "Aly Lidayan",
        "Yuqing Du",
        "Eliza Kosoy",
        "Maria Rufova",
        "Pieter Abbeel",
        "Alison Gopnik"
      ],
      "abstract": "What drives exploration? Understanding intrinsic motivation is a\nlong-standing challenge in both cognitive science and artificial intelligence;\nnumerous objectives have been proposed and used to train agents, yet there\nremains a gap between human and agent exploration. We directly compare adults,\nchildren, and AI agents in a complex open-ended environment, Crafter, and study\nhow common intrinsic objectives: Entropy, Information Gain, and Empowerment,\nrelate to their behavior. We find that only Entropy and Empowerment are\nconsistently positively correlated with human exploration progress, indicating\nthat these objectives may better inform intrinsic reward design for agents.\nFurthermore, across agents and humans we observe that Entropy initially\nincreases rapidly, then plateaus, while Empowerment increases continuously,\nsuggesting that state diversity may provide more signal in early exploration,\nwhile advanced exploration should prioritize control. Finally, we find\npreliminary evidence that private speech utterances, and particularly goal\nverbalizations, may aid exploration in children.",
      "tldr_zh": "该研究对比了成人、儿童和AI智能体在开放世界环境Crafter中的探索行为，旨在理解内在动机在探索中的作用。研究考察了熵(Entropy)、信息增益(Information Gain)和赋能(Empowerment)等常见内在目标与人类和智能体探索行为的关系。结果表明，只有熵和赋能与人类探索进度呈持续正相关，表明这些目标可能更适合用于设计智能体的内在奖励。此外，研究发现熵在早期快速增长后趋于平稳，而赋能持续增长，这表明状态多样性可能在早期探索中提供更多信号，而高级探索应优先考虑控制。初步证据还表明，自言自语，特别是目标口头表达，可能有助于儿童的探索。\n",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.23631v1",
      "published_date": "2025-03-31 00:09:00 UTC",
      "updated_date": "2025-03-31 00:09:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:20:37.126390"
    },
    {
      "arxiv_id": "2503.23630v1",
      "title": "Finding Interest Needle in Popularity Haystack: Improving Retrieval by Modeling Item Exposure",
      "title_zh": "在热门内容中寻找兴趣点：通过建模项目曝光度来改进检索\n",
      "authors": [
        "Amit Jaspal",
        "Rahul Agarwal"
      ],
      "abstract": "Recommender systems operate in closed feedback loops, where user interactions\nreinforce popularity bias, leading to over-recommendation of already popular\nitems while under-exposing niche or novel content. Existing bias mitigation\nmethods, such as Inverse Propensity Scoring (IPS) and Off- Policy Correction\n(OPC), primarily operate at the ranking stage or during training, lacking\nexplicit real-time control over exposure dynamics. In this work, we introduce\nan exposure- aware retrieval scoring approach, which explicitly models item\nexposure probability and adjusts retrieval-stage ranking at inference time.\nUnlike prior work, this method decouples exposure effects from engagement\nlikelihood, enabling controlled trade-offs between fairness and engagement in\nlarge-scale recommendation platforms. We validate our approach through online\nA/B experiments in a real-world video recommendation system, demonstrating a\n25% increase in uniquely retrieved items and a 40% reduction in the dominance\nof over-popular content, all while maintaining overall user engagement levels.\nOur results establish a scalable, deployable solution for mitigating popularity\nbias at the retrieval stage, offering a new paradigm for bias-aware\npersonalization.",
      "tldr_zh": "该论文提出了一种exposure-aware的检索排序方法，旨在解决推荐系统中由于流行度偏差导致的利基内容曝光不足的问题。该方法显式地建模了物品的曝光概率，并在推理阶段调整检索排序，从而将曝光效应与用户参与度分离开来，实现了公平性和用户参与度之间的可控权衡。通过在真实视频推荐系统中的在线A/B实验验证，该方法在保持用户参与度的前提下，显著提高了唯一检索物品的数量（25%）并降低了过度流行内容的占比（40%）。该研究提供了一种可扩展、可部署的解决方案，用于在检索阶段缓解流行度偏差，为bias-aware的个性化推荐提供了一个新的范例。\n",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "2 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.23630v1",
      "published_date": "2025-03-31 00:04:01 UTC",
      "updated_date": "2025-03-31 00:04:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-04-02T02:20:49.195467"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 82,
  "processed_papers_count": 82,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-04-02T02:22:02.042523"
}
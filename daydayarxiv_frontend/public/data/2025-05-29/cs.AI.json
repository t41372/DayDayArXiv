{
  "date": "2025-05-29",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-29 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv çˆ†å‘äº†å¤§é‡å…³äº **LLM/VLM æ·±åº¦æ¨ç†ï¼ˆReasoningï¼‰**ä¸ **Agent è‡ªè¿›åŒ–**çš„ç ”ç©¶ã€‚æˆ‘ä»¬å¯ä»¥æ˜æ˜¾çœ‹åˆ° \"DeepSeek-R1\" å¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ GRPOï¼‰æ­£åœ¨è¢«è¿ç§»åˆ°è§†è§‰ï¼ˆVisionï¼‰å’Œå¤šæ¨¡æ€é¢†åŸŸã€‚æ­¤å¤–ï¼Œ**å¤šæ™ºèƒ½ä½“ï¼ˆMulti-Agentï¼‰åä½œ**ã€**ä»£ç ç”Ÿæˆçš„æ–°åŸºå‡†ï¼ˆSWE-bench Liveï¼‰**ä»¥åŠ**è§†é¢‘ç”Ÿæˆçš„ç²¾ç»†æ§åˆ¶**ä¹Ÿæ˜¯ä»Šå¤©çš„çƒ­é—¨è¯é¢˜ã€‚\n\né‡ç‚¹å…³æ³¨ï¼šèƒ½è‡ªæˆ‘è¿›åŒ–çš„ **Darwin GÃ¶del Machine**ï¼Œä¸ºäº†è§£å†³åˆ·æ¦œæ±¡æŸ“æ¨å‡ºçš„ **SWE-bench Live**ï¼Œä»¥åŠå¯¹å¤šæ¨¡æ€ RAG è®¾è®¡ç©ºé—´çš„ç³»ç»Ÿæ€§æ‹†è§£ã€‚\n\n---\n\n### ğŸš€ æ·±åº¦æ¨ç†ä¸è‡ªè¿›åŒ– Agents (Reasoning & Self-Improving Agents)\n\n**1. Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents**\n**Darwin GÃ¶del Machineï¼šè‡ªæˆ‘è¿›åŒ–æ™ºèƒ½ä½“çš„å¼€æ”¾å¼æ¼”åŒ–**\nè¿™æ˜¯ä¸€ç¯‡éå¸¸æœ‰é‡å¿ƒçš„æ–‡ç« ã€‚ä½œè€…æå‡ºäº† Darwin GÃ¶del Machine (DGM)ï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½è‡ªæˆ‘æ”¹è¿›çš„ç³»ç»Ÿã€‚ä¸åŒäºä»¥å¾€å›ºå®šçš„æ¶æ„ï¼ŒDGM é€šè¿‡ä¸æ–­ä¿®æ”¹è‡ªå·±çš„ä»£ç ï¼ˆä»è€Œæ”¹è¿›ä¿®æ”¹ä»£ç çš„èƒ½åŠ›ï¼‰æ¥è¿­ä»£ã€‚å®ƒå€Ÿé‰´äº†è¾¾å°”æ–‡è¿›åŒ–è®ºï¼Œç»´æŠ¤ä¸€ä¸ªæ™ºèƒ½ä½“æ¡£æ¡ˆåº“ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆæ–°ç‰ˆæœ¬å¹¶è¿›è¡ŒéªŒè¯ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šåœ¨ SWE-bench ä¸Šï¼ŒDGM é€šè¿‡è‡ªæˆ‘æ”¹è¿›ï¼ˆå¦‚å¼€å‘æ›´å¥½çš„ä»£ç ç¼–è¾‘å·¥å…·ã€åŒè¡Œè¯„å®¡æœºåˆ¶ï¼‰ï¼Œå°†æ€§èƒ½ä» 20.0% æå‡åˆ°äº† 50.0%ã€‚è¿™æ˜¯é€šå‘å¼€æ”¾å¼ AI è¿›åŒ–çš„é‡è¦ä¸€æ­¥ã€‚\n\n**2. SWE-bench Goes Live!**\n**SWE-bench Goes Live!ï¼šå®æ—¶æ›´æ–°çš„ä»£ç ç”ŸæˆåŸºå‡†**\nç°æœ‰çš„ SWE-bench å·²ç»é¢ä¸´è¿‡æ‹Ÿåˆå’Œæ•°æ®æ±¡æŸ“çš„é—®é¢˜ã€‚è¿™ç¯‡è®ºæ–‡æ¨å‡ºäº† SWE-bench-Liveï¼Œä¸€ä¸ª**å®æ—¶æ›´æ–°**çš„åŸºå‡†æµ‹è¯•ï¼ŒåŸºäº 2024 å¹´ä»¥æ¥ GitHub ä¸Šçš„çœŸå® Issue ç”Ÿæˆä»»åŠ¡ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šå¼•å…¥äº†è‡ªåŠ¨åŒ–çš„ç®¡çº¿ï¼Œä» Issue åˆ°ç¯å¢ƒæ­å»ºå…¨è‡ªåŠ¨ã€‚æµ‹è¯•æ˜¾ç¤ºï¼ŒSOTA æ¨¡å‹åœ¨ Live ç‰ˆæœ¬ä¸Šçš„è¡¨ç°ä¸é™æ€ç‰ˆæœ¬å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹å¯¹æ—§æ•°æ®çš„è¿‡æ‹Ÿåˆé£é™©ã€‚\n\n**3. Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics**\n**Robot-R1ï¼šç”¨äºå¢å¼ºæœºå™¨äººå…·èº«æ¨ç†çš„å¼ºåŒ–å­¦ä¹ **\nDeepSeek-R1 çš„æ€è·¯è¢«ç”¨åˆ°äº†æœºå™¨äººæ§åˆ¶ä¸Šã€‚ä¼ ç»Ÿçš„ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰åœ¨å…·èº«æ¨ç†ä¸­å¾€å¾€ä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šä½œè€…æå‡ºäº† Robot-R1ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥é¢„æµ‹å®Œæˆä»»åŠ¡æ‰€éœ€çš„ä¸‹ä¸€ä¸ªå…³é”®ç‚¹çŠ¶æ€ã€‚å³ä½¿åªæœ‰ 7B å‚æ•°ï¼Œå…¶åœ¨ä½å±‚åŠ¨ä½œæ§åˆ¶çš„æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿè¶…è¶Šäº† GPT-4oã€‚\n\n**4. Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models**\n**SPOï¼šå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­æœ‰æ•ˆçš„æ®µçº§ä¿¡ç”¨åˆ†é…**\né’ˆå¯¹æå‡ LLM æ¨ç†èƒ½åŠ›ï¼ŒPPOï¼ˆTokençº§ï¼‰å¤ªéš¾è®­ç»ƒ Criticï¼ŒGRPOï¼ˆè½¨è¿¹çº§ï¼‰åé¦ˆå¤ªç²—ç³™ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† SPOï¼ˆæ®µçº§ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œåœ¨ä¸­é—´ç²’åº¦ä¸Šè¿›è¡Œä¼˜åŠ¿ä¼°è®¡ã€‚SPO-chain å’Œ SPO-tree åˆ†åˆ«é’ˆå¯¹çŸ­ CoT å’Œé•¿ CoT è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨ GSM8K å’Œ MATH500 ä¸Šæ¯” PPO å’Œ GRPO æå‡äº† 6-12 ä¸ªç™¾åˆ†ç‚¹ã€‚\n\n---\n\n### ğŸ§© å¤šæ¨¡æ€ä¸ RAG (Multimodal & RAG)\n\n**5. mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation**\n**mRAGï¼šé˜æ˜å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„è®¾è®¡ç©ºé—´**\nå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å­˜åœ¨å¹»è§‰é—®é¢˜ã€‚æœ¬æ–‡æ˜¯å¯¹å¤šæ¨¡æ€ RAGï¼ˆmRAGï¼‰ç®¡çº¿çš„ç¬¬ä¸€æ¬¡ç³»ç»Ÿæ€§æ‹†è§£ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæ·±å…¥ç ”ç©¶äº†æ£€ç´¢é˜¶æ®µï¼ˆæ¨¡æ€é…ç½®ï¼‰ã€é‡æ’åºé˜¶æ®µï¼ˆå‡è½»ä½ç½®åå·®ï¼‰å’Œç”Ÿæˆé˜¶æ®µï¼ˆå¦‚ä½•æ•´åˆå€™é€‰é¡¹ï¼‰ã€‚é€šè¿‡å…¨æ ˆæ¢ç´¢ï¼Œæ— éœ€å¾®è°ƒå³å¯è·å¾— 5% çš„å¹³å‡æ€§èƒ½æå‡ã€‚\n\n**6. Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding**\n**Multi-RAGï¼šç”¨äºè‡ªé€‚åº”è§†é¢‘ç†è§£çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† Multi-RAGï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆè§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬ç­‰å¤šæºä¿¡æ¯æµï¼Œä¸ºäººç±»æä¾›è‡ªé€‚åº”è¾…åŠ©ã€‚åœ¨ MMBench-Video ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¼€æº Video-LLMã€‚\n\n**7. One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory**\n**One Trajectory, One Tokenï¼šåŸºäºå…¨æ™¯å­å¯¹è±¡è½¨è¿¹çš„è§†é¢‘ Token åŒ–**\nè§†é¢‘ Token åŒ–é€šå¸¸äº§ç”Ÿå¤§é‡å†—ä½™ Tokenã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **TrajViT**ï¼Œä¸€ç§åŸºäºâ€œå…¨æ™¯å­å¯¹è±¡è½¨è¿¹â€è€Œéå›ºå®š Patch çš„ Token åŒ–èŒƒå¼ã€‚å®ƒå°†å¯¹è±¡è½¨è¿¹è½¬æ¢ä¸ºè¯­ä¹‰ Tokenï¼Œåœ¨ä¿æŒæ—¶é—´è¿è´¯æ€§çš„åŒæ—¶å¤§å¹…å‡å°‘äº† Token æ•°é‡ï¼ˆå‡å°‘ 10 å€ Tokenï¼Œæ£€ç´¢å¬å›ç‡æå‡ 6%ï¼‰ã€‚\n\n---\n\n### ğŸ¥ è§†è§‰ç”Ÿæˆä¸ç¼–è¾‘ (Visual Generation & Editing)\n\n**8. Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing**\n**Zero-to-Heroï¼šé›¶æ ·æœ¬åˆå§‹åŒ–èµ‹èƒ½åŸºäºå‚è€ƒçš„è§†é¢‘å¤–è§‚ç¼–è¾‘**\nç°æœ‰çš„æ–‡æœ¬æŒ‡å¯¼ç¼–è¾‘å¾€å¾€æ„å›¾æ¨¡ç³Šã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒå›¾åƒçš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ã€‚é¦–å…ˆç¼–è¾‘ä¸€ä¸ªé”šå®šå¸§ï¼Œç„¶ååˆ©ç”¨åŸå§‹å¸§ä¸­çš„å¯¹åº”å…³ç³»å°†å¤–è§‚ä¼ æ’­åˆ°å…¶ä»–å¸§ã€‚è¿™ç§ Zero-shot åˆå§‹åŒ–ç¡®ä¿äº†æ—¶é—´ä¸€è‡´æ€§ï¼Œæ¯”ç°æœ‰æ–¹æ³• PSNR æå‡äº† 2.6 dBã€‚\n\n**9. DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models**\n**DINO-R1ï¼šæ¿€åŠ±è§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„æ¨ç†èƒ½åŠ›**\nå— DeepSeek-R1 å¯å‘ï¼Œæœ¬æ–‡å°è¯•åœ¨è§†è§‰æ¨¡å‹ï¼ˆDINO ç³»åˆ—ï¼‰ä¸­å¼•å…¥æ¨ç†èƒ½åŠ›ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† GRQOï¼ˆGroup Relative Query Optimizationï¼‰ï¼Œä¸€ç§é’ˆå¯¹ Query-based æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚DINO-R1 åœ¨ COCO å’Œ LVIS ä¸Šå±•ç°äº†å¼ºå¤§çš„è§†è§‰ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚\n\n**10. GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion**\n**GeoManï¼šä½¿ç”¨å›¾ç”Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸€è‡´æ€§äººä½“å‡ ä½•ä¼°è®¡**\nä»è§†é¢‘ä¸­ä¼°è®¡ 3D äººä½“å‡ ä½•å¾ˆéš¾ä¿è¯æ—¶é—´ä¸€è‡´æ€§ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šå°†è§†é¢‘å‡ ä½•ä¼°è®¡é‡æ–°æ„å»ºä¸ºâ€œå›¾ç”Ÿè§†é¢‘â€ç”Ÿæˆé—®é¢˜ã€‚åˆ©ç”¨å›¾åƒæ¨¡å‹ä¼°è®¡ç¬¬ä¸€å¸§ï¼Œç„¶åç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆåç»­å¸§çš„æ·±åº¦å’Œæ³•çº¿ï¼Œè§£å†³äº† 4D è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚\n\n---\n\n### ğŸ¤– å¤šæ™ºèƒ½ä½“ä¸åä½œ (Multi-Agent Collaboration)\n\n**11. OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation**\n**OWLï¼šé¢å‘ç°å®ä¸–ç•Œä»»åŠ¡è‡ªåŠ¨åŒ–çš„ä¼˜åŒ–åŠ³åŠ¨åŠ›å­¦ä¹ **\nç°æœ‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿéš¾ä»¥è·¨åŸŸè¿ç§»ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† Workforce æ¡†æ¶ï¼Œè§£è€¦äº†è§„åˆ’å™¨ï¼ˆPlannerï¼‰ã€åè°ƒå™¨ï¼ˆCoordinatorï¼‰å’Œå·¥äººç±»ï¼ˆWorkersï¼‰ã€‚OWL ç®—æ³•åˆ©ç”¨ RL ä¼˜åŒ–è§„åˆ’å™¨ã€‚è¯¥ç³»ç»Ÿåœ¨ GAIA åŸºå‡†ä¸Šè¾¾åˆ°äº†å¼€æº SOTA (69.70%)ï¼Œç”šè‡³ç•¥ä¼˜äº OpenAI çš„ Deep Researchã€‚\n\n**12. GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning**\n**GAM-Agentï¼šç”¨äºå¤æ‚è§†è§‰æ¨ç†çš„åšå¼ˆè®ºä¸ä¸ç¡®å®šæ€§æ„ŸçŸ¥åä½œ**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šå°†æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºéé›¶å’Œåšå¼ˆã€‚åŸºç¡€ Agent è´Ÿè´£è§†è§‰æ„ŸçŸ¥ï¼ŒCritical Agent è´Ÿè´£éªŒè¯é€»è¾‘ã€‚å¼•å…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ§åˆ¶å™¨ï¼Œåœ¨å‡ºç°åˆ†æ­§æ—¶è§¦å‘å¤šè½®è¾©è®ºï¼Œæ˜¾è‘—æå‡äº† VLM çš„æ¨ç†å‡†ç¡®ç‡ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€å¯¹é½ä¸è¯„ä¼° (Safety, Alignment & Evaluation)\n\n**13. LLM Agents Should Employ Security Principles**\n**LLM Agents Should Employ Security Principlesï¼šLLM æ™ºèƒ½ä½“åº”é‡‡ç”¨å®‰å…¨åŸåˆ™**\n- **æ ¸å¿ƒè§‚ç‚¹**ï¼šè¿™æ˜¯ä¸€ç¯‡ç«‹åœºè®ºæ–‡ã€‚ä½œè€…è®¤ä¸ºä¸ä»…è¦å…³æ³¨ Prompt æ³¨å…¥ï¼Œè¿˜åº”å°†ä¼ ç»Ÿçš„å®‰å…¨åŸåˆ™ï¼ˆå¦‚çºµæ·±é˜²å¾¡ã€æœ€å°æƒé™ã€å®Œå…¨ä»²è£ï¼‰å¼•å…¥ Agent è®¾è®¡ã€‚æå‡ºäº† AgentSandbox æ¡†æ¶æ¥åµŒå…¥è¿™äº›åŸåˆ™ã€‚\n\n**14. KGMark: A Diffusion Watermark for Knowledge Graphs**\n**KGMarkï¼šçŸ¥è¯†å›¾è°±çš„æ‰©æ•£æ°´å°**\nç»™åŠ¨æ€çš„å›¾æ•°æ®åŠ æ°´å°å¾ˆéš¾ã€‚\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ç¬¬ä¸€ä¸ªé’ˆå¯¹åŠ¨æ€ KG çš„å›¾æ°´å°æ¡†æ¶ï¼Œç”Ÿæˆé²æ£’ã€å¯æ£€æµ‹ä¸”é€æ˜çš„æ‰©æ•£æŒ‡çº¹ï¼Œèƒ½æŠµæŠ—å„ç§æ”»å‡»ã€‚\n\n**15. OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities**\n**OMNIGUARDï¼šè·¨è¯­è¨€å’Œæ¨¡æ€çš„é«˜æ•ˆ AI å®‰å…¨å®¡æ ¸æ–¹æ³•**\n- **æ ¸å¿ƒè´¡çŒ®**ï¼šåˆ©ç”¨ LLM å†…éƒ¨å¯¹é½çš„è¡¨ç¤ºæ¥æ„å»ºä¸è¯­è¨€/æ¨¡æ€æ— å…³çš„åˆ†ç±»å™¨ã€‚è¿™ç§æ–¹æ³•éå¸¸é«˜æ•ˆï¼ˆæ¯”åŸºçº¿å¿« 120 å€ï¼‰ï¼Œå¹¶ä¸”åœ¨å¤šè¯­è¨€å’ŒéŸ³é¢‘ Prompt çš„æœ‰å®³æ£€æµ‹ä¸Šåˆ·æ–°äº† SOTAã€‚\n\n---\n\n### ğŸ§ª AI for Science & å…¶ä»–æœ‰è¶£å‘ç°\n\n**16. MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering**\n**MedPAIRï¼šåœ¨åŒ»ç–—é—®ç­”ä¸­è¡¡é‡åŒ»ç”Ÿä¸ AI çš„ç›¸å…³æ€§å¯¹é½**\n- **å‘ç°**ï¼šLLM å³ä½¿ç­”æ¡ˆæ­£ç¡®ï¼Œæ¨ç†è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ç­›é€‰é€»è¾‘ä¹Ÿç»å¸¸ä¸åŒ»ç”Ÿä¸ä¸€è‡´ã€‚è¿‡æ»¤æ‰åŒ»ç”Ÿè®¤ä¸ºä¸ç›¸å…³çš„å¥å­åï¼ŒLLM çš„å‡†ç¡®ç‡åè€Œæé«˜äº†ã€‚\n\n**17. Can LLMs Reason Structurally? An Evaluation via the Lens of Data Structures**\n**LLM èƒ½è¿›è¡Œç»“æ„åŒ–æ¨ç†å—ï¼Ÿé€šè¿‡æ•°æ®ç»“æ„è§†è§’çš„è¯„ä¼°**\n- **å‘ç°**ï¼šä½œè€…æ„å»ºäº† DSR-Benchï¼ˆåŒ…å« 20 ç§æ•°æ®ç»“æ„ï¼‰ã€‚ç»“æœæ˜¾ç¤º LLM å¾ˆæŒ£æ‰ï¼Œæœ€å¥½çš„æ¨¡å‹åœ¨å›°éš¾å®ä¾‹ä¸Šä¹Ÿåªæ‹¿åˆ°äº† 0.498/1 çš„åˆ†æ•°ã€‚æ¨¡å‹åœ¨ç©ºé—´æ•°æ®å’Œè‡ªæˆ‘ç”Ÿæˆçš„ä»£ç æ¨ç†ä¸Šè¡¨ç°ä¸ä½³ã€‚\n\n**18. From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?**\n**ä»å›¾åƒåˆ°ä¿¡å·ï¼šå¤§å‹è§†è§‰æ¨¡å‹å¯¹æ—¶é—´åºåˆ—åˆ†ææœ‰ç”¨å—ï¼Ÿ**\n- **ç»“è®º**ï¼šLVMï¼ˆå¤§å‹è§†è§‰æ¨¡å‹ï¼‰å¯¹æ—¶é—´åºåˆ—**åˆ†ç±»**ç¡®å®æœ‰ç”¨ï¼Œä½†åœ¨**é¢„æµ‹**ï¼ˆForecastingï¼‰æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œä¸”å­˜åœ¨å¯¹â€œé¢„æµ‹å‘¨æœŸâ€çš„å½’çº³åå·®ã€‚\n\n**19. Implicit Inversion turns CLIP into a Decoder**\n**éšå¼åè½¬å°† CLIP å˜æˆäº†åŠ ç å™¨**\n- **æœ‰è¶£å‘ç°**ï¼šé€šå¸¸è®¤ä¸º CLIP æ˜¯åˆ¤åˆ«æ¨¡å‹ã€‚ä½†ä½œè€…å‘ç°ï¼Œæ— éœ€ä»»ä½•è§£ç å™¨æˆ–å¾®è°ƒï¼Œä»…é€šè¿‡ä¼˜åŒ–é¢‘ç‡æ„ŸçŸ¥çš„éšå¼ç¥ç»è¡¨ç¤ºï¼Œå°±å¯ä»¥ç›´æ¥ä» CLIP åµŒå…¥ä¸­æ¢å¤å›¾åƒã€‚è¿™è¡¨æ˜åˆ¤åˆ«æ¨¡å‹å†…éƒ¨æ½œè—ç€ç”Ÿæˆèƒ½åŠ›ã€‚",
  "papers": [
    {
      "arxiv_id": "2505.24073v2",
      "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation",
      "title_zh": "mRAGï¼šé˜æ˜å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„è®¾è®¡ç©ºé—´",
      "authors": [
        "Chan-Wei Hu",
        "Yueqi Wang",
        "Shuo Xing",
        "Chia-Ju Chen",
        "Suofei Feng",
        "Ryan Rossi",
        "Zhengzhong Tu"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have made remarkable strides in multimodal tasks such as visual question answering, visual grounding, and complex reasoning. However, they remain limited by static training data, susceptibility to hallucinations, and inability to verify claims against up-to-date, external evidence, compromising their performance in dynamic real-world applications. Retrieval-Augmented Generation (RAG) offers a practical solution to mitigate these challenges by allowing the LVLMs to access large-scale knowledge databases via retrieval mechanisms, thereby grounding model outputs in factual, contextually relevant information. Here in this paper, we conduct the first systematic dissection of the multimodal RAG pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the modality configurations and retrieval strategies, (2) the re-ranking stage: on strategies to mitigate positional biases and improve the relevance of retrieved evidence, and (3) the generation phase: we further investigate how to best integrate retrieved candidates into the final generation process. Finally, we extend to explore a unified agentic framework that integrates re-ranking and generation through self-reflection, enabling LVLMs to select relevant evidence and suppress irrelevant context dynamically. Our full-stack exploration of RAG for LVLMs yields substantial insights, resulting in an average performance boost of 5% without any fine-tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Vision-Language Models (LVLMs) é¢ä¸´çš„è®­ç»ƒæ•°æ®é™æ€ã€æ˜“äº§ç”Ÿå¹»è§‰åŠæ— æ³•éªŒè¯å¤–éƒ¨è¯æ®ç­‰å±€é™æ€§ï¼Œå¯¹å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) æµç¨‹è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿçš„å‰–æã€‚è®ºæ–‡æ·±å…¥æ¢è®¨äº†æ£€ç´¢é˜¶æ®µçš„æ¨¡æ€é…ç½®ä¸ç­–ç•¥ã€é‡æ’åºé˜¶æ®µå‡è½» positional biases å¹¶æé«˜ç›¸å…³æ€§çš„æ–¹æ³•ï¼Œä»¥åŠç”Ÿæˆé˜¶æ®µé›†æˆæ£€ç´¢å€™é€‰ä¿¡æ¯çš„æœ€ä½³å®è·µã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§é›†æˆé‡æ’åºä¸ç”Ÿæˆçš„ç»Ÿä¸€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åæ€ (self-reflection) æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€ç­›é€‰æœ‰æ•ˆè¯æ®å¹¶æŠ‘åˆ¶æ— å…³ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å…¨æ ˆå¼çš„ mRAG æ¢ç´¢åœ¨æ— éœ€ä»»ä½•å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä½¿ LVLMs çš„å¹³å‡æ€§èƒ½æå‡äº† 5%ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£å’Œä¼˜åŒ–å¤šæ¨¡æ€ RAG çš„è®¾è®¡ç©ºé—´æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.24073v2",
      "published_date": "2025-05-29 23:32:03 UTC",
      "updated_date": "2025-08-26 16:42:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:44:04.916397+00:00"
    },
    {
      "arxiv_id": "2505.24069v2",
      "title": "Can LLMs Reason Structurally? An Evaluation via the Lens of Data Structures",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦è¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼ŸåŸºäºæ•°æ®ç»“æ„è§†è§’çš„è¯„ä¼°",
      "authors": [
        "Yu He",
        "Yingxi Li",
        "Colin White",
        "Ellen Vitercik"
      ],
      "abstract": "As large language models (LLMs) take on increasingly complex tasks, understanding their algorithmic reasoning abilities has become essential. However, existing evaluations focus on distinct and isolated tasks. We propose a unified diagnostic lens: structural reasoning--understanding and manipulating relationships like order, hierarchy, and connectivity. We introduce DSR-Bench, the first benchmark to systematically evaluate LLM structural reasoning through canonical data structures, which serve as interpretable, algorithmically meaningful abstractions. DSR-Bench spans 20 data structures, 35 operations, and 4,140 synthetically generated problem instances with minimal contamination. The benchmark's hierarchical design pinpoints specific failure modes, while its fully automated evaluation ensures objective and consistent assessment. Benchmarking ten state-of-the-art LLMs reveals critical limitations: the top-performing model scores only 0.498 out of 1 on challenging instances. Three additional evaluation suites reveal further weaknesses: models perform poorly on spatial data and natural language scenarios, and fail to reason over their own generated code. DSR-Bench offers a principled diagnostic tool for structural reasoning, helping expose reasoning bottlenecks and guide the development of more capable and reliable LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦å…·å¤‡ç»“æ„åŒ–æ¨ç†(Structural Reasoning)èƒ½åŠ›ï¼Œå³ç†è§£å’Œæ“ä½œé¡ºåºã€å±‚æ¬¡åŠè¿æ¥å…³ç³»çš„èƒ½åŠ›ã€‚ä¸ºäº†è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œä½œè€…æå‡ºäº†é¦–ä¸ªåŸºäºè§„èŒƒæ•°æ®ç»“æ„(Data Structures)çš„åŸºå‡†æµ‹è¯•DSR-Benchï¼Œæ¶µç›–20ç§æ•°æ®ç»“æ„ã€35ç§æ“ä½œä»¥åŠ4140ä¸ªåˆæˆé—®é¢˜å®ä¾‹ã€‚å¯¹åç§å°–ç«¯LLMçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨æ­¤ç±»ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§å®ä¾‹ä¸­çš„å¾—åˆ†ä»…ä¸º0.498ã€‚è¿›ä¸€æ­¥åˆ†ææ­ç¤ºï¼ŒLLMåœ¨ç©ºé—´æ•°æ®ã€è‡ªç„¶è¯­è¨€åœºæ™¯ä»¥åŠå¯¹å…¶è‡ªèº«ç”Ÿæˆä»£ç çš„æ¨ç†æ–¹é¢è¡¨ç°è¾ƒå·®ã€‚DSR-Benchä¸ºæ­ç¤ºæ¨ç†ç“¶é¢ˆæä¾›äº†åŸåˆ™æ€§çš„è¯Šæ–­å·¥å…·ï¼Œæœ‰åŠ©äºæ¨åŠ¨å¼€å‘æ›´å…·èƒ½åŠ›å’Œå¯é æ€§çš„LLMsã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24069v2",
      "published_date": "2025-05-29 23:24:53 UTC",
      "updated_date": "2025-10-14 01:24:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:44:07.785176+00:00"
    },
    {
      "arxiv_id": "2506.00076v1",
      "title": "Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry",
      "title_zh": "ä¼˜åŒ–å™äº‹ã€æå‡å—ä¼—ç•™å­˜åŠå‡å°‘å¨±ä¹äº§ä¸šèµ„æºæµªè´¹",
      "authors": [
        "Andrew Cornfeld",
        "Ashley Miller",
        "Mercedes Mora-Figueroa",
        "Kurt Samuels",
        "Anthony Palomba"
      ],
      "abstract": "Television networks face high financial risk when making programming decisions, often relying on limited historical data to forecast episodic viewership. This study introduces a machine learning framework that integrates natural language processing (NLP) features from over 25000 television episodes with traditional viewership data to enhance predictive accuracy. By extracting emotional tone, cognitive complexity, and narrative structure from episode dialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost, and feature selection models. While prior viewership remains a strong baseline predictor, NLP features contribute meaningful improvements for some series. We also introduce a similarity scoring method based on Euclidean distance between aggregate dialogue vectors to compare shows by content. Tested across diverse genres, including Better Call Saul and Abbott Elementary, our framework reveals genre-specific performance and offers interpretable metrics for writers, executives, and marketers seeking data-driven insight into audience behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µè§†ç½‘ç»œåœ¨èŠ‚ç›®å†³ç­–ä¸­é¢ä¸´çš„é«˜è´¢åŠ¡é£é™©ï¼Œæå‡ºäº†ä¸€ç§é›†æˆè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ç‰¹å¾ä¸ä¼ ç»Ÿæ”¶è§†æ•°æ®çš„æœºå™¨å­¦ä¹ (Machine Learning)æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿä»è¶…è¿‡25,000é›†ç”µè§†èŠ‚ç›®çš„å¯¹è¯ä¸­æå–äº†æƒ…æ„ŸåŸºè°ƒ(Emotional Tone)ã€è®¤çŸ¥å¤æ‚æ€§(Cognitive Complexity)å’Œå™äº‹ç»“æ„(Narrative Structure)ç­‰ç‰¹å¾ã€‚è¯„ä¼°è¿‡ç¨‹ä¸­é‡‡ç”¨äº†SARIMAXã€æ»šåŠ¨XGBoost(Rolling XGBoost)å’Œç‰¹å¾é€‰æ‹©(Feature Selection)ç­‰å¤šç§æ¨¡å‹ï¼Œå‘ç°åŠ å…¥NLPç‰¹å¾èƒ½æ˜¾è‘—æå‡ç‰¹å®šå‰§é›†çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºæ¬§å‡ é‡Œå¾—è·ç¦»(Euclidean Distance)çš„å¯¹è¯å‘é‡ç›¸ä¼¼åº¦è¯„åˆ†æ³•ï¼Œç”¨äºé‡åŒ–å¯¹æ¯”å‰§é›†å†…å®¹ã€‚è¯¥æ¡†æ¶åœ¨ã€ŠBetter Call Saulã€‹å’Œã€ŠAbbott Elementaryã€‹ç­‰ä¸åŒé¢˜æä½œå“ä¸­å¾—åˆ°éªŒè¯ï¼Œä¸ºè¡Œä¸šé«˜ç®¡å’Œåˆ›ä½œè€…æä¾›äº†å…³äºè§‚ä¼—è¡Œä¸ºçš„å¯è§£é‡Šæ€§æ•°æ®æ´å¯Ÿï¼Œæœ‰åŠ©äºåœ¨å¨±ä¹è¡Œä¸šä¸­ä¼˜åŒ–æ•…äº‹å™è¿°å¹¶å‡å°‘èµ„æºæµªè´¹ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00076v1",
      "published_date": "2025-05-29 23:01:54 UTC",
      "updated_date": "2025-05-29 23:01:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:44:21.991159+00:00"
    },
    {
      "arxiv_id": "2505.24055v1",
      "title": "Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs",
      "title_zh": "é€šè¿‡é“¾è·¯é¢„æµ‹æ¡¥æ¥æºåŸŸä¸ç›®æ ‡åŸŸçš„å›¾æ— ç›‘ç£åŸŸè‡ªé€‚åº”",
      "authors": [
        "Yilong Wang",
        "Tianxiang Zhao",
        "Zongyu Wu",
        "Suhang Wang"
      ],
      "abstract": "Graph neural networks (GNNs) have shown great ability for node classification on graphs. However, the success of GNNs relies on abundant labeled data, while obtaining high-quality labels is costly and challenging, especially for newly emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a classifier on the labeled source graph and adapts it to the unlabeled target graph, is attracting increasing attention. Various approaches have been proposed to alleviate the distribution shift between the source and target graphs to facilitate the classifier adaptation. However, most of them simply adopt existing UDA techniques developed for independent and identically distributed data to gain domain-invariant node embeddings for graphs, which do not fully consider the graph structure and message-passing mechanism of GNNs during the adaptation and will fail when label distribution shift exists among domains. In this paper, we proposed a novel framework that adopts link prediction to connect nodes between source and target graphs, which can facilitate message-passing between the source and target graphs and augment the target nodes to have ``in-distribution'' neighborhoods with the source domain. This strategy modified the target graph on the input level to reduce its deviation from the source domain in the embedding space and is insensitive to disproportional label distributions across domains. To prevent the loss of discriminative information in the target graph, we further design a novel identity-preserving learning objective, which guides the learning of the edge insertion module together with reconstruction and adaptation losses. Experimental results on real-world datasets demonstrate the effectiveness of our framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNNs)åœ¨æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”(Unsupervised Domain Adaptation, UDA)ä¸­é¢ä¸´çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨Link Predictionè¿æ¥æºåŸŸä¸ç›®æ ‡åŸŸèŠ‚ç‚¹çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è·¨åŸŸèŠ‚ç‚¹è¿æ¥ä¿ƒè¿›äº†æºåŸŸä¸ç›®æ ‡åŸŸä¹‹é—´çš„Message-passingï¼Œå¹¶ä¸ºç›®æ ‡èŠ‚ç‚¹æ„å»ºäº†ä¸æºåŸŸå…·æœ‰â€œåŒåˆ†å¸ƒ(in-distribution)â€ç‰¹å¾çš„é‚»åŸŸï¼Œä»è¾“å…¥å±‚çº§å‡å°‘äº†ç›®æ ‡å›¾åœ¨Embedding Spaceä¸­ä¸æºåŸŸçš„åå·®ã€‚è¯¥ç­–ç•¥å¯¹è·¨åŸŸå­˜åœ¨çš„Label distribution shiftå…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–ç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®å‡è®¾çš„å±€é™ã€‚ä¸ºäº†é˜²æ­¢ç›®æ ‡å›¾åˆ¤åˆ«æ€§ä¿¡æ¯çš„ä¸¢å¤±ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†Identity-preserving learning objectiveï¼Œç»“åˆé‡å»ºä¸è‡ªé€‚åº”æŸå¤±å…±åŒæŒ‡å¯¼Edge insertionæ¨¡å—çš„å­¦ä¹ ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œæœ‰æ•ˆæå‡äº†è·¨åŸŸå›¾æ•°æ®çš„èŠ‚ç‚¹åˆ†ç±»å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24055v1",
      "published_date": "2025-05-29 22:54:40 UTC",
      "updated_date": "2025-05-29 22:54:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:44:14.360338+00:00"
    },
    {
      "arxiv_id": "2505.24040v1",
      "title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering",
      "title_zh": "MedPAIRï¼šè¯„ä¼°åŒ»å­¦é—®ç­”ä¸­åŒ»ç”Ÿä¸äººå·¥æ™ºèƒ½çš„ç›¸å…³æ€§å¯¹é½",
      "authors": [
        "Yuexing Hao",
        "Kumail Alhamoud",
        "Hyewon Jeong",
        "Haoran Zhang",
        "Isha Puri",
        "Philip Torr",
        "Mike Schaekermann",
        "Ariel D. Stern",
        "Marzyeh Ghassemi"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on various medical question-answering (QA) benchmarks, including standardized medical exams. However, correct answers alone do not ensure correct logic, and models may reach accurate conclusions through flawed processes. In this study, we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance Estimation and Question Answering) dataset to evaluate how physician trainees and LLMs prioritize relevant information when answering QA questions. We obtain annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence within the question components for relevance. We compare these relevance estimates to those for LLMs, and further evaluate the impact of these \"relevant\" subsets on downstream task performance for both physician trainees and LLMs. We find that LLMs are frequently not aligned with the content relevance estimates of physician trainees. After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs. All LLM and physician trainee-labeled data are available at: http://medpair.csail.mit.edu/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ»å­¦é—®ç­”(QA)ä¸­è™½èƒ½æä¾›æ­£ç¡®ç­”æ¡ˆä½†é€»è¾‘å¯èƒ½å­˜åœ¨ç¼ºé™·çš„é—®é¢˜ï¼Œæå‡ºäº†MedPAIRæ•°æ®é›†ã€‚MedPAIRåŒ…å«1,300ä¸ªåŒ»å­¦é—®ç­”å¯¹ï¼Œå¹¶æ”¶é›†äº†36ä½åŒ»ç”Ÿå—è®­è€…(physician trainees)å¯¹é¢˜ç›®ä¸­å„å¥å­ç›¸å…³æ€§(relevance)çš„æ ‡æ³¨æ•°æ®ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”LLMsä¸äººç±»åŒ»ç”Ÿåœ¨å¤„ç†ä¿¡æ¯ä¼˜å…ˆçº§æ–¹é¢çš„å·®å¼‚ï¼Œè¯„ä¼°äº†åŒæ–¹åœ¨ç›¸å…³æ€§ä¼°è®¡ä¸Šçš„å¯¹é½(alignment)ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨æå–å†…å®¹ç›¸å…³æ€§æ–¹é¢å¾€å¾€ä¸åŒ»ç”Ÿå—è®­è€…çš„åˆ¤æ–­ä¸ä¸€è‡´ã€‚è¿›ä¸€æ­¥çš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨è¿‡æ»¤æ‰åŒ»ç”Ÿæ ‡æ³¨çš„æ— å…³ä¿¡æ¯åï¼ŒLLMså’ŒåŒ»ç”Ÿå—è®­è€…çš„é—®ç­”å‡†ç¡®ç‡(accuracy)å‡å¾—åˆ°äº†æå‡ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨åŒ»ç–—æ¨ç†é€»è¾‘ä¸Šçš„ä¸è¶³ï¼Œä¹Ÿä¸ºæå‡åŒ»ç–—äººå·¥æ™ºèƒ½çš„å¯é æ€§æä¾›äº†æ–°çš„åŸºå‡†å·¥å…·å’Œå¼€æºæ•°æ®èµ„æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24040v1",
      "published_date": "2025-05-29 22:23:48 UTC",
      "updated_date": "2025-05-29 22:23:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:44:05.047707+00:00"
    },
    {
      "arxiv_id": "2505.24037v1",
      "title": "Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution",
      "title_zh": "æœ¯ä¸šæœ‰ä¸“æ”»ï¼šé€šè¿‡ç¨€ç–æ¼”åŒ–è¿›è¡Œç¨€ç–å¾®è°ƒä»¥ä¿®å¤ç¨€ç–å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Qiao Xiao",
        "Alan Ansell",
        "Boqian Wu",
        "Lu Yin",
        "Mykola Pechenizkiy",
        "Shiwei Liu",
        "Decebal Constantin Mocanu"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success across various tasks but face deployment challenges due to their massive computational demands. While post-training pruning methods like SparseGPT and Wanda can effectively reduce the model size, but struggle to maintain model performance at high sparsity levels, limiting their utility for downstream tasks. Existing fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve sparsity as they require updating the whole dense metrics, not well-suited for sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a novel method designed specifically for sparse LLMs. SEFT dynamically evolves the sparse topology of pruned models during fine-tuning, while preserving the overall sparsity throughout the process. The strengths of SEFT lie in its ability to perform task-specific adaptation through a weight drop-and-grow strategy, enabling the pruned model to self-adapt its sparse connectivity pattern based on the target dataset. Furthermore, a sensitivity-driven pruning criterion is employed to ensure that the desired sparsity level is consistently maintained throughout fine-tuning. Our experiments on various LLMs, including LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks demonstrate that SEFT achieves stronger performance while offering superior memory and time efficiency compared to existing baselines. Our code is publicly available at: https://github.com/QiaoXiao7282/SEFT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¨€ç–å¤§è¯­è¨€æ¨¡å‹ (Sparse LLMs) åœ¨é«˜ç¨€ç–åº¦ä¸‹æ€§èƒ½ä¸‹é™ä¸”ç°æœ‰å¾®è°ƒæ–¹æ³•æ— æ³•ä¿æŒç¨€ç–æ€§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º Sparsity Evolution Fine-Tuning (SEFT) çš„æ–°å‹å¾®è°ƒæ¡†æ¶ã€‚SEFT å…è®¸æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŠ¨æ€æ¼”åŒ–å…¶ç¨€ç–æ‹“æ‰‘ç»“æ„ (Sparse Topology)ï¼Œä»è€Œåœ¨ä¿æŒæ€»ä½“ç¨€ç–åº¦çš„å‰æä¸‹ä¿®å¤å‰ªæå¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚è¯¥æ–¹æ³•å¼•å…¥äº† weight drop-and-grow ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šä»»åŠ¡è‡ªé€‚åº”åœ°è°ƒæ•´ç¨€ç–è¿æ¥æ¨¡å¼ï¼Œå¹¶ç»“åˆ sensitivity-driven pruning å‡†åˆ™ç¡®ä¿ç¨€ç–æ°´å¹³çš„ç¨³å®šæ€§ã€‚åœ¨ LLaMAã€DeepSeek å’Œ Mistral ç­‰å¤šç§æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSEFT åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼ŒåŒæ—¶å…·å¤‡æé«˜çš„å†…å­˜å’Œæ—¶é—´æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œä¸ºé«˜æ•ˆéƒ¨ç½²å’Œä¼˜åŒ–é«˜ç¨€ç–åº¦çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„ä¸“ä¸šåŒ–é€‚é…æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24037v1",
      "published_date": "2025-05-29 22:17:43 UTC",
      "updated_date": "2025-05-29 22:17:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:44:21.859228+00:00"
    },
    {
      "arxiv_id": "2505.24036v1",
      "title": "GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs",
      "title_zh": "GenICï¼šä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å›¾è°±å®ä¾‹è¡¥å…¨æ¡†æ¶",
      "authors": [
        "Amel Gader",
        "Alsayed Algergawy"
      ],
      "abstract": "Knowledge graph completion aims to address the gaps of knowledge bases by adding new triples that represent facts. The complexity of this task depends on how many parts of a triple are already known. Instance completion involves predicting the relation-tail pair when only the head is given (h, ?, ?). Notably, modern knowledge bases often contain entity descriptions and types, which can provide valuable context for inferring missing facts. By leveraging these textual descriptions and the ability of large language models to extract facts from them and recognize patterns within the knowledge graph schema, we propose an LLM-powered, end-to-end instance completion approach. Specifically, we introduce GenIC: a two-step Generative Instance Completion framework. The first step focuses on property prediction, treated as a multi-label classification task. The second step is link prediction, framed as a generative sequence-to-sequence task. Experimental results on three datasets show that our method outperforms existing baselines. Our code is available at https://github.com/amal-gader/genic.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GenICï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„çŸ¥è¯†å›¾è°±å®ä¾‹è¡¥å…¨ (Instance Completion) æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºåœ¨ä»…ç»™å®šå¤´å®ä½“çš„æƒ…å†µä¸‹é¢„æµ‹å…³ç³»-å°¾å®ä½“å¯¹ã€‚GenIC åˆ©ç”¨å®ä½“æè¿°å’Œç±»å‹ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œé€šè¿‡ä¸¤æ­¥èµ°ç­–ç•¥è§£å†³ä»»åŠ¡ï¼šé¦–å…ˆå°†å±æ€§é¢„æµ‹è§†ä¸ºå¤šæ ‡ç­¾åˆ†ç±» (multi-label classification) ä»»åŠ¡ï¼Œéšåå°†é“¾æ¥é¢„æµ‹ (link prediction) æ„å»ºä¸ºç”Ÿæˆå¼çš„åºåˆ—åˆ°åºåˆ— (sequence-to-sequence) ä»»åŠ¡ã€‚è¯¥æ¡†æ¶å‘æŒ¥äº† LLMs åœ¨æ–‡æœ¬äº‹å®æå–å’ŒçŸ¥è¯†å›¾è°±æ¨¡å¼è¯†åˆ«æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„ç”Ÿæˆå¼æ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGenIC åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºå®Œå–„çŸ¥è¯†åº“æä¾›äº†ä¸€ç§æœ‰æ•ˆæ‰‹æ®µï¼Œç›¸å…³ä»£ç ä¹Ÿå·²å…¬å¼€å‘å¸ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24036v1",
      "published_date": "2025-05-29 22:15:25 UTC",
      "updated_date": "2025-05-29 22:15:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:44:49.283869+00:00"
    },
    {
      "arxiv_id": "2505.24034v2",
      "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training",
      "title_zh": "LlamaRLï¼šç”¨äºé«˜æ•ˆå¤§è§„æ¨¡ LLM è®­ç»ƒçš„åˆ†å¸ƒå¼å¼‚æ­¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Bo Wu",
        "Sid Wang",
        "Yunhao Tang",
        "Jia Ding",
        "Eryk Helenowski",
        "Liang Tan",
        "Tengyu Xu",
        "Tushar Gowda",
        "Zhengxing Chen",
        "Chen Zhu",
        "Xiaocheng Tang",
        "Yundi Qian",
        "Beibei Zhu",
        "Rui Hou"
      ],
      "abstract": "Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.\n  In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically during the Llama 3 post-training, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LlamaRLï¼Œä¸€ä¸ªä¸“é—¨ä¸ºå¤§è§„æ¨¡ Large Language Models (LLMs) é«˜æ•ˆè®­ç»ƒè€Œè®¾è®¡çš„å…¨åˆ†å¸ƒå¼ã€å¼‚æ­¥ Reinforcement Learning (RL) æ¡†æ¶ã€‚é’ˆå¯¹å‚æ•°è§„æ¨¡è¾¾æ•°ç™¾äº¿è‡³æ•°åƒäº¿çš„æ¨¡å‹åœ¨æ¨ç†å»¶è¿Ÿå’Œå†…å­˜éœ€æ±‚æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ GPU é›†ç¾¤ä¸Šå®ç°å¯é ä¸”é«˜æ•ˆçš„ç­–ç•¥æ¨¡å‹ç®¡ç†ã€‚LlamaRL åŸºäºåŸç”Ÿ PyTorch æ„å»ºï¼Œé‡‡ç”¨äº†æµçº¿å‹å•æ§åˆ¶å™¨æ¶æ„ (single-controller architecture)ï¼Œå…·å¤‡é«˜åº¦æ¨¡å—åŒ–ã€æ˜“ç”¨æ€§ä»¥åŠæ”¯æŒæ•°åƒä¸ª GPU çš„æ— ç¼æ‰©å±•æ€§ã€‚è¯¥æ¡†æ¶é›†æˆäº†æ¨¡å‹å…±ç½®å¸è½½ (colocated model offloading)ã€å¼‚æ­¥ç¦»ç­–è®­ç»ƒ (asynchronous off-policy training) ä»¥åŠç”¨äºæƒé‡åŒæ­¥çš„åˆ†å¸ƒå¼ç›´æ¥å†…å­˜è®¿é—® (distributed direct memory access) ç­‰æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ã€‚ç ”ç©¶é€šè¿‡ç†è®ºåˆ†ææ­£å¼è¯æ˜äº†å¼‚æ­¥è®¾è®¡å¸¦æ¥çš„è®­ç»ƒåŠ é€Ÿï¼Œå¹¶åœ¨ Llama 3 åè®­ç»ƒé˜¶æ®µçš„å®è¯ä¸­æ˜¾ç¤ºï¼ŒLlamaRL åœ¨ 405B å‚æ•°æ¨¡å‹ä¸Šç›¸æ¯” DeepSpeed-Chat ç±»ä¼¼ç³»ç»Ÿå®ç°äº†é«˜è¾¾ 10.7 å€çš„åŠ é€Ÿã€‚éšç€æ¨¡å‹è§„æ¨¡å¢åŠ ï¼Œå…¶æ•ˆç‡ä¼˜åŠ¿æŒç»­æ‰©å¤§ï¼Œå±•ç¤ºäº†å…¶åœ¨æœªæ¥è¶…å¤§è§„æ¨¡ RL è®­ç»ƒä»»åŠ¡ä¸­çš„å“è¶Šé€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24034v2",
      "published_date": "2025-05-29 22:14:15 UTC",
      "updated_date": "2025-06-02 01:49:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:44:48.145467+00:00"
    },
    {
      "arxiv_id": "2505.24030v2",
      "title": "From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?",
      "title_zh": "ä»å›¾åƒåˆ°ä¿¡å·ï¼šå¤§è§†è§‰æ¨¡å‹åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­æ˜¯å¦æœ‰æ•ˆï¼Ÿ",
      "authors": [
        "Ziming Zhao",
        "ChengAo Shen",
        "Hanghang Tong",
        "Dongjin Song",
        "Zhigang Deng",
        "Qingsong Wen",
        "Jingchao Ni"
      ],
      "abstract": "Transformer-based models have gained increasing attention in time series research, driving interest in Large Language Models (LLMs) and foundation models for time series analysis. As the field moves toward multi-modality, Large Vision Models (LVMs) are emerging as a promising direction. In the past, the effectiveness of Transformer and LLMs in time series has been debated. When it comes to LVMs, a similar question arises: are LVMs truely useful for time series analysis? To address it, we design and conduct the first principled study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across both high-level (classification) and low-level (forecasting) tasks, with extensive ablation analysis. Our findings indicate LVMs are indeed useful for time series classification but face challenges in forecasting. Although effective, the contemporary best LVM forecasters are limited to specific types of LVMs and imaging methods, exhibit a bias toward forecasting periods, and have limited ability to utilize long look-back windows. We hope our findings could serve as a cornerstone for future research on LVM- and multimodal-based solutions to different time series tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†å¤§å‹è§†è§‰æ¨¡å‹(LVMs)åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œæ—¨åœ¨å›ç­” LVMs æ˜¯å¦çœŸæ­£å¯¹è¯¥é¢†åŸŸæœ‰ç›Šã€‚ä½œè€…é€šè¿‡å¯¹æ¯” 4 ç§ LVMsã€8 ç§æˆåƒæ–¹æ³•åŠ 18 ä¸ªæ•°æ®é›†ï¼Œåœ¨åˆ†ç±»(classification)å’Œé¢„æµ‹(forecasting)ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèåˆ†æä¸åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLVMs åœ¨æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸­éå¸¸æœ‰æ•ˆï¼Œä½†åœ¨é¢„æµ‹ä»»åŠ¡ä¸­é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚ç›®å‰çš„ LVMs é¢„æµ‹æ¨¡å‹è¡¨ç°å—é™äºç‰¹å®šçš„æ¨¡å‹ç±»å‹å’Œæˆåƒæ–¹æ³•ï¼Œä¸”å¯¹é¢„æµ‹å‘¨æœŸå­˜åœ¨åå·®ï¼ŒåŒæ—¶åœ¨åˆ©ç”¨é•¿å›æº¯çª—å£(long look-back windows)æ–¹é¢èƒ½åŠ›æœ‰é™ã€‚è¯¥é¡¹ç ”ç©¶ä½œä¸ºé¦–ä¸ªé’ˆå¯¹è¯¥è¯¾é¢˜çš„åŸåˆ™æ€§ç ”ç©¶ï¼Œä¸ºæœªæ¥å¼€å‘åŸºäº LVMs å’Œå¤šæ¨¡æ€çš„æ—¶é—´åºåˆ—åˆ†ææ–¹æ¡ˆæä¾›äº†é‡è¦çš„åŸºçŸ³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24030v2",
      "published_date": "2025-05-29 22:05:28 UTC",
      "updated_date": "2025-07-09 22:13:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:44:53.069890+00:00"
    },
    {
      "arxiv_id": "2505.24026v1",
      "title": "MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking",
      "title_zh": "MaskAdaptï¼šåŸºäºå¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸ RGB-æ·±åº¦æ©è”½çš„æ— ç›‘ç£å‡ ä½•æ„ŸçŸ¥åŸŸè‡ªé€‚åº”",
      "authors": [
        "Numair Nadeem",
        "Muhammad Hamza Asad",
        "Saeed Anwar",
        "Abdul Bais"
      ],
      "abstract": "Semantic segmentation of crops and weeds is crucial for site-specific farm management; however, most existing methods depend on labor intensive pixel-level annotations. A further challenge arises when models trained on one field (source domain) fail to generalize to new fields (target domain) due to domain shifts, such as variations in lighting, camera setups, soil composition, and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by enabling adaptation without target-domain labels, but current UDA methods struggle with occlusions and visual blending between crops and weeds, leading to misclassifications in real-world conditions. To overcome these limitations, we introduce MaskAdapt, a novel approach that enhances segmentation accuracy through multimodal contextual learning by integrating RGB images with features derived from depth data. By computing depth gradients from depth maps, our method captures spatial transitions that help resolve texture ambiguities. These gradients, through a cross-attention mechanism, refines RGB feature representations, resulting in sharper boundary delineation. In addition, we propose a geometry-aware masking strategy that applies horizontal, vertical, and stochastic masks during training. This encourages the model to focus on the broader spatial context for robust visual recognition. Evaluations on real agricultural datasets demonstrate that MaskAdapt consistently outperforms existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation mean Intersection over Union (mIOU) across diverse field conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MaskAdaptï¼Œä¸€ç§æ—¨åœ¨è§£å†³å†œä½œç‰©ä¸æ‚è‰è¯­ä¹‰åˆ†å‰²ä¸­æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº” (Unsupervised Domain Adaptation) æŒ‘æˆ˜çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰ UDA æ–¹æ³•åœ¨å¤„ç†é®æŒ¡ã€è§†è§‰èåˆä»¥åŠå› ç¯å¢ƒå˜åŒ–å¼•èµ·çš„é¢†åŸŸåç§» (Domain Shift) æ—¶çš„å±€é™æ€§ï¼ŒMaskAdapt å¼•å…¥äº†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹  (Multimodal Contextual Learning)ï¼Œé€šè¿‡æ•´åˆ RGB å›¾åƒä¸æ·±åº¦ç‰¹å¾æ¥æ•æ‰ç©ºé—´è¿‡æ¸¡ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦æ¢¯åº¦ (Depth Gradients) å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ (Cross-attention Mechanism) ç²¾ç»†åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œå®ç°äº†æ›´é”åˆ©çš„è¾¹ç•Œå‹¾å‹’ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§å‡ ä½•æ„ŸçŸ¥æ©ç ç­–ç•¥ (Geometry-aware Masking Strategy)ï¼Œé€šè¿‡åœ¨è®­ç»ƒæœŸé—´åº”ç”¨æ°´å¹³ã€å‚ç›´å’Œéšæœºæ©ç ï¼Œä¿ƒä½¿æ¨¡å‹å…³æ³¨æ›´å¹¿æ³›çš„ç©ºé—´ä¸Šä¸‹æ–‡ä»¥å¢å¼ºé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMaskAdapt åœ¨çœŸå®å†œä¸šæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿› (SOTA) æ–¹æ³•ï¼Œåœ¨å¤šç§å¤æ‚å†œç”°æ¡ä»¶ä¸‹æ˜¾è‘—æå‡äº†å¹³å‡äº¤å¹¶æ¯” (mIOU)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 5 figures, presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025. Reviewer comments available upon request",
      "pdf_url": "https://arxiv.org/pdf/2505.24026v1",
      "published_date": "2025-05-29 21:59:39 UTC",
      "updated_date": "2025-05-29 21:59:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:45:04.411681+00:00"
    },
    {
      "arxiv_id": "2505.24025v2",
      "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models",
      "title_zh": "DINO-R1ï¼šæ¿€å‘è§†è§‰åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Chenbin Pan",
        "Wenbin He",
        "Zhengzhong Tu",
        "Liu Ren"
      ],
      "abstract": "The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \\textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DINO-R1ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¿€å‘è§†è§‰åŸºç¡€æ¨¡å‹(Vision Foundation Models)çš„è§†è§‰ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ç±»èƒ½åŠ›åœ¨è§†è§‰è¡¨ç¤ºæ¨¡å‹ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„ç°çŠ¶ï¼Œç ”ç©¶è€…å¼•å…¥äº†ç»„ç›¸å¯¹æŸ¥è¯¢ä¼˜åŒ–(Group Relative Query Optimization, GRQO)è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ç»„æ ‡å‡†åŒ–çš„å¯¹é½è´¨é‡è®¡ç®—æŸ¥è¯¢çº§å¥–åŠ±ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº† KL æ­£åˆ™åŒ–(KL-regularization)æ¥ç¨³å®šç›®æ ‡æ€§åˆ†å¸ƒï¼Œä»è€Œæ˜¾è‘—é™ä½è®­ç»ƒçš„ä¸ç¨³å®šæ€§å¹¶ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ã€‚DINO-R1 ç³»åˆ—æ¨¡å‹åœ¨ Grounding-DINO åŸºç¡€ä¸Šæ„å»ºï¼Œå¹¶é›†æˆäº†è§†è§‰æç¤ºç¼–ç å™¨(Visual Prompt Encoder)å’Œè§†è§‰å¼•å¯¼æŸ¥è¯¢é€‰æ‹©æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDINO-R1 åœ¨ COCOã€LVIS å’Œ ODinW æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒ(Supervised Fine-tuning, SFT)åŸºçº¿æ¨¡å‹ã€‚æœ€ç»ˆï¼Œè¯¥æ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡å’Œé—­é›†è§†è§‰æç¤ºåœºæ™¯ä¸­å‡å±•ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24025v2",
      "published_date": "2025-05-29 21:58:06 UTC",
      "updated_date": "2025-08-01 10:10:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:45:32.044024+00:00"
    },
    {
      "arxiv_id": "2505.24023v1",
      "title": "Multi-Group Proportional Representation for Text-to-Image Models",
      "title_zh": "æ–‡ç”Ÿå›¾æ¨¡å‹çš„å¤šç¾¤ä½“æ¯”ä¾‹åŒ–è¡¨ç¤º",
      "authors": [
        "Sangwon Jung",
        "Alex Oesterling",
        "Claudio Mayrink Verdun",
        "Sajani Vithana",
        "Taesup Moon",
        "Flavio P. Calmon"
      ],
      "abstract": "Text-to-image (T2I) generative models can create vivid, realistic images from textual descriptions. As these models proliferate, they expose new concerns about their ability to represent diverse demographic groups, propagate stereotypes, and efface minority populations. Despite growing attention to the \"safe\" and \"responsible\" design of artificial intelligence (AI), there is no established methodology to systematically measure and control representational harms in image generation. This paper introduces a novel framework to measure the representation of intersectional groups in images generated by T2I models by applying the Multi-Group Proportional Representation (MPR) metric. MPR evaluates the worst-case deviation of representation statistics across given population groups in images produced by a generative model, allowing for flexible and context-specific measurements based on user requirements. We also develop an algorithm to optimize T2I models for this metric. Through experiments, we demonstrate that MPR can effectively measure representation statistics across multiple intersectional groups and, when used as a training objective, can guide models toward a more balanced generation across demographic groups while maintaining generation quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒ(Text-to-Image, T2I)ç”Ÿæˆæ¨¡å‹åœ¨äººå£ç»Ÿè®¡å¤šæ ·æ€§ã€åˆ»æ¿å°è±¡ä¼ æ’­åŠå°‘æ•°ç¾¤ä½“æŠ¹é™¤ç­‰æ–¹é¢çš„è¡¨å¾åè§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è¡¡é‡äº¤å‰æ€§ç¾¤ä½“(intersectional groups)è¡¨å¾çš„æ–°å‹æ¡†æ¶ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼•å…¥äº†å¤šç¾¤ä½“æ¯”ä¾‹ä»£è¡¨åˆ¶(Multi-Group Proportional Representation, MPR)æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡é€šè¿‡è¯„ä¼°ç”Ÿæˆæ¨¡å‹åœ¨ä¸åŒäººç¾¤ä¸­è¡¨å¾ç»Ÿè®¡æ•°æ®çš„æœ€å·®æƒ…å†µåå·®ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·éœ€æ±‚è¿›è¡Œçµæ´»ä¸”ç‰¹å®šè¯­å¢ƒçš„æµ‹é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸€ç§ä¸“é—¨é’ˆå¯¹è¯¥æŒ‡æ ‡ä¼˜åŒ–T2Iæ¨¡å‹çš„ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMPRèƒ½å¤Ÿæœ‰æ•ˆè¡¡é‡å¤šä¸ªäº¤å‰ç¾¤ä½“çš„è¡¨å¾ç»Ÿè®¡ï¼Œä¸”åœ¨ä½œä¸ºè®­ç»ƒç›®æ ‡æ—¶ï¼Œèƒ½å¼•å¯¼æ¨¡å‹åœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œå®ç°äººå£ç»Ÿè®¡ç¾¤ä½“é—´æ›´å‡è¡¡çš„å›¾åƒç”Ÿæˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24023v1",
      "published_date": "2025-05-29 21:48:28 UTC",
      "updated_date": "2025-05-29 21:48:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:45:36.308750+00:00"
    },
    {
      "arxiv_id": "2505.24019v1",
      "title": "LLM Agents Should Employ Security Principles",
      "title_zh": "LLM æ™ºèƒ½ä½“åº”éµå¾ªå®‰å…¨åŸåˆ™",
      "authors": [
        "Kaiyuan Zhang",
        "Zian Su",
        "Pin-Yu Chen",
        "Elisa Bertino",
        "Xiangyu Zhang",
        "Ninghui Li"
      ],
      "abstract": "Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privilege, complete mediation, and psychological acceptability have helped guide the design of mechanisms for securing information systems over the last five decades, and we argue that their explicit and conscientious adoption will help secure agentic systems. To illustrate this approach, we introduce AgentSandbox, a conceptual framework embedding these security principles to provide safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art LLMs along three dimensions: benign utility, attack utility, and attack success rate. AgentSandbox maintains high utility for its intended functions under both benign and adversarial evaluations while substantially mitigating privacy risks. By embedding secure design principles as foundational elements within emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems aligned with user privacy expectations and evolving regulatory requirements.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨è‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´æç¤ºæ³¨å…¥(prompt injection)å’Œä¸Šä¸‹æ–‡æ“çºµç­‰æ–°å‹å®‰å…¨å¨èƒï¼Œå¯èƒ½å¯¼è‡´éšç§æ³„éœ²å’Œç³»ç»Ÿé£é™©ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºåœ¨éƒ¨ç½²å¤§è§„æ¨¡æ™ºèƒ½ä½“ç³»ç»Ÿæ—¶åº”é‡‡ç”¨ä¿¡æ¯å®‰å…¨é¢†åŸŸæˆç†Ÿçš„å®‰å…¨åŸåˆ™(security principles)ï¼Œå¦‚æ·±åº¦é˜²å¾¡(defense-in-depth)ã€æœ€å°ç‰¹æƒ(least privilege)ã€å®Œå…¨ä»²è£(complete mediation)å’Œå¿ƒç†å¯æ¥å—æ€§(psychological acceptability)ã€‚ç ”ç©¶è€…å¼•å…¥äº†AgentSandboxæ¦‚å¿µæ¡†æ¶ï¼Œæ—¨åœ¨å°†è¿™äº›åŸåˆ™åµŒå…¥æ™ºèƒ½ä½“çš„å…¨ç”Ÿå‘½å‘¨æœŸä»¥æä¾›å®‰å…¨ä¿éšœã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒAgentSandboxåœ¨ä¿æŒé«˜æ°´å¹³è‰¯æ€§æ•ˆç”¨(benign utility)çš„åŒæ—¶ï¼Œèƒ½æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡å¹¶ç¼“è§£éšç§é£é™©ã€‚è¯¥å·¥ä½œå¼ºè°ƒå°†å®‰å…¨è®¾è®¡åŸåˆ™ä½œä¸ºæ–°å…´æ™ºèƒ½ä½“åè®®çš„åŸºç¡€å…ƒç´ ï¼Œä»¥æ¨åŠ¨æ„å»ºç¬¦åˆéšç§æœŸæœ›å’Œç›‘ç®¡è¦æ±‚çš„å¯ä¿¡æ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24019v1",
      "published_date": "2025-05-29 21:39:08 UTC",
      "updated_date": "2025-05-29 21:39:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:45:23.914421+00:00"
    },
    {
      "arxiv_id": "2506.04245v4",
      "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning",
      "title_zh": "åŸºäºæ¨ç†ä¸å¼ºåŒ–å­¦ä¹ çš„å¤§è¯­è¨€æ¨¡å‹æƒ…å¢ƒå®Œæ•´æ€§",
      "authors": [
        "Guangchen Lan",
        "Huseyin A. Inan",
        "Sahar Abdelnabi",
        "Janardhan Kulkarni",
        "Lukas Wutschitz",
        "Reza Shokri",
        "Christopher G. Brinton",
        "Robert Sim"
      ],
      "abstract": "As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls. Our code is available at: https://github.com/EricGLan/CI-RL",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è‡ªä¸»æ™ºèƒ½ä½“æ—¶ä»£ç¡®ä¿èƒŒæ™¯å®Œæ•´æ€§ (Contextual Integrity, CI) çš„é‡è¦æ€§ï¼Œå³åœ¨æ‰§è¡Œä»»åŠ¡æ—¶åˆ¤æ–­å“ªäº›ä¿¡æ¯é€‚åˆå…±äº«ã€‚ä½œè€…æå‡º CI éœ€è¦æ¨¡å‹å…·å¤‡å¯¹æ“ä½œè¯­å¢ƒçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶é¦–å…ˆé€šè¿‡æç¤ºè¯å¼•å¯¼ LLMs åœ¨å†³å®šæŠ«éœ²ä¿¡æ¯æ—¶è¿›è¡Œæ˜¾å¼æ¨ç†ã€‚éšåï¼Œç ”ç©¶å¼€å‘äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) æ¡†æ¶ï¼Œæ—¨åœ¨è¿›ä¸€æ­¥åŸ¹å…»æ¨¡å‹å®ç° CI æ‰€éœ€çš„æ¨ç†ç´ è´¨ã€‚é€šè¿‡ä¸€ä¸ªåŒ…å«çº¦ 700 ä¸ªå¤šæ ·åŒ–è¯­å¢ƒç¤ºä¾‹çš„åˆæˆæ•°æ®é›†ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†å„ç§è§„æ¨¡å’Œç³»åˆ—çš„ LLMs çš„ä¸å½“ä¿¡æ¯æŠ«éœ²ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•çš„æ”¹è¿›æ•ˆæœèƒ½æˆåŠŸè¿ç§»è‡³å…·æœ‰äººå·¥æ ‡æ³¨çš„ PrivacyLens ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ‰æ•ˆæå‡äº† AI åŠ©æ‰‹åœ¨å·¥å…·è°ƒç”¨å’Œæ‰§è¡Œæ“ä½œè¿‡ç¨‹ä¸­çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.04245v4",
      "published_date": "2025-05-29 21:26:21 UTC",
      "updated_date": "2025-12-29 22:27:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:45:25.043896+00:00"
    },
    {
      "arxiv_id": "2505.24012v1",
      "title": "Large Language Model Meets Constraint Propagation",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸çº¦æŸä¼ æ’­çš„ç»“åˆ",
      "authors": [
        "Alexandre Bonlarron",
        "Florian RÃ©gin",
        "Elisabetta De Maria",
        "Jean-Charles RÃ©gin"
      ],
      "abstract": "Large Language Models (LLMs) excel at generating fluent text but struggle to enforce external constraints because they generate tokens sequentially without explicit control mechanisms. GenCP addresses this limitation by combining LLM predictions with Constraint Programming (CP) reasoning, formulating text generation as a Constraint Satisfaction Problem (CSP). In this paper, we improve GenCP by integrating Masked Language Models (MLMs) for domain generation, which allows bidirectional constraint propagation that leverages both past and future tokens. This integration bridges the gap between token-level prediction and structured constraint enforcement, leading to more reliable and constraint-aware text generation. Our evaluation on COLLIE benchmarks demonstrates that incorporating domain preview via MLM calls significantly improves GenCP's performance. Although this approach incurs additional MLM calls and, in some cases, increased backtracking, the overall effect is a more efficient use of LLM inferences and an enhanced ability to generate feasible and meaningful solutions, particularly in tasks with strict content constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶éš¾ä»¥å¼ºåˆ¶æ‰§è¡Œå¤–éƒ¨çº¦æŸçš„é—®é¢˜ï¼Œå¯¹GenCPæ¡†æ¶è¿›è¡Œäº†æ”¹è¿›ï¼Œå°†å…¶æ ¸å¿ƒæ€è·¯æ„å»ºä¸ºçº¦æŸæ»¡è¶³é—®é¢˜(CSP)ã€‚æœ¬æ–‡é€šè¿‡é›†æˆæ©ç è¯­è¨€æ¨¡å‹(MLMs)è¿›è¡Œå®šä¹‰åŸŸç”Ÿæˆï¼Œå®ç°äº†åˆ©ç”¨è¿‡å»å’Œæœªæ¥Tokenä¿¡æ¯çš„åŒå‘çº¦æŸä¼ æ’­(Constraint Propagation)ï¼Œä»è€Œå¼¥è¡¥äº†Tokençº§é¢„æµ‹ä¸ç»“æ„åŒ–çº¦æŸæ‰§è¡Œä¹‹é—´çš„å·®è·ã€‚åœ¨COLLIEåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå¼•å…¥MLMé¢„è§ˆæœºåˆ¶æ˜¾è‘—æå‡äº†GenCPçš„æ€§èƒ½ï¼Œä½¿å…¶ç”Ÿæˆçš„æ–‡æœ¬æ›´å…·çº¦æŸæ„è¯†ã€‚å°½ç®¡è¯¥æ–¹æ³•å¼•å…¥äº†é¢å¤–çš„MLMè°ƒç”¨å’Œéƒ¨åˆ†å›æº¯ï¼Œä½†å®ƒæ•´ä½“æé«˜äº†LLMæ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶åœ¨å¤„ç†å…·æœ‰ä¸¥æ ¼å†…å®¹çº¦æŸçš„ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ç”Ÿæˆå¯è¡Œä¸”æœ‰æ„ä¹‰çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear in the Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.24012v1",
      "published_date": "2025-05-29 21:18:12 UTC",
      "updated_date": "2025-05-29 21:18:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:45:50.143208+00:00"
    },
    {
      "arxiv_id": "2506.00075v1",
      "title": "Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation",
      "title_zh": "é™ä½æœºå™¨äººå¯¼èˆªä¸­åŸºäº LLM çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¤„ç†å»¶è¿Ÿ",
      "authors": [
        "Diego Pollini",
        "Bruna V. Guterres",
        "Rodrigo S. Guerra",
        "Ricardo B. Grando"
      ],
      "abstract": "The integration of Large Language Models (LLMs), such as GPT, in industrial robotics enhances operational efficiency and human-robot collaboration. However, the computational complexity and size of these models often provide latency problems in request and response times. This study explores the integration of the ChatGPT natural language model with the Robot Operating System 2 (ROS 2) to mitigate interaction latency and improve robotic system control within a simulated Gazebo environment. We present an architecture that integrates these technologies without requiring a middleware transport platform, detailing how a simulated mobile robot responds to text and voice commands. Experimental results demonstrate that this integration improves execution speed, usability, and accessibility of the human-robot interaction by decreasing the communication latency by 7.01\\% on average. Such improvements facilitate smoother, real-time robot operations, which are crucial for industrial automation and precision tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å·¥ä¸šæœºå™¨äººé¢†åŸŸé›†æˆå¤§è¯­è¨€æ¨¡å‹(LLMs)æ—¶å¸¸è§çš„è®¡ç®—å¤æ‚æ€§ä¸å“åº”å»¶è¿Ÿé—®é¢˜ã€‚ä¸ºäº†é™ä½äº¤äº’å»¶è¿Ÿï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å°† ChatGPT ä¸æœºå™¨äººæ“ä½œç³»ç»Ÿ 2 (ROS 2) ç›´æ¥é›†æˆçš„æ–°å‹æ¶æ„ï¼Œå¹¶å¯åœ¨ Gazebo ä»¿çœŸç¯å¢ƒä¸­å®ç°å¯¹ç§»åŠ¨æœºå™¨äººçš„ç²¾ç¡®æ§åˆ¶ã€‚è¯¥æ¶æ„æ— éœ€é¢å¤–çš„ä¸­é—´ä»¶ä¼ è¾“å¹³å°ï¼Œæ”¯æŒç§»åŠ¨æœºå™¨äººå¯¹æ–‡æœ¬å’Œè¯­éŸ³å‘½ä»¤çš„å®æ—¶å“åº”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆæˆåŠŸå°†é€šä¿¡å»¶è¿Ÿå¹³å‡é™ä½äº† 7.01%ï¼Œæ˜¾è‘—æå‡äº†äººæœºäº¤äº’çš„æ‰§è¡Œé€Ÿåº¦ã€å¯ç”¨æ€§ä¸å¯è®¿é—®æ€§ã€‚è¿™é¡¹æ”¹è¿›ä¸ºå·¥ä¸šè‡ªåŠ¨åŒ–å’Œç²¾å¯†ä»»åŠ¡ä¸­çš„å®æ—¶æœºå™¨äººæ“ä½œæä¾›äº†æ›´æµç•…çš„æŠ€æœ¯æ”¯æŒï¼Œå¯¹äºå®ç°é«˜æ•ˆçš„äººæœºåä½œå…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the 23rd IEEE International Conference on Industrial Informatics (INDIN)",
      "pdf_url": "https://arxiv.org/pdf/2506.00075v1",
      "published_date": "2025-05-29 21:16:14 UTC",
      "updated_date": "2025-05-29 21:16:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:46:08.139230+00:00"
    },
    {
      "arxiv_id": "2505.24009v2",
      "title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws",
      "title_zh": "Transformer å±‚çš„å¤šæ ·æ€§ï¼šå‚æ•°ç¼©æ”¾æ³•åˆ™çš„ä¸€ä¸ªè§†è§’",
      "authors": [
        "Hidetaka Kamigaito",
        "Ying Zhang",
        "Jingun Kwon",
        "Katsuhiko Hayashi",
        "Manabu Okumura",
        "Taro Watanabe"
      ],
      "abstract": "Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Transformeræ¶æ„ä¸­å‚æ•°æ‰©å±•å®šå¾‹ (parameter scaling laws) ä¸å†…éƒ¨å±‚çº§æœºåˆ¶ä¹‹é—´çš„è”ç³»ï¼Œé‡ç‚¹å…³æ³¨æ®‹å·®æµ (residual stream) ä¸­çš„å±‚çº§è¡¨ç°ã€‚ä½œè€…æå‡ºäº†åå·®-å¤šæ ·æ€§åˆ†è§£ (bias-diversity decomposition) ç†è®ºï¼Œå°†æ¨¡å‹æ€§èƒ½æ‹†è§£ä¸ºå„å±‚è¾“å‡ºçš„åå·®ä¸å±‚é—´çš„äº’å¼‚æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œåªæœ‰å½“å„å±‚åœ¨ä¿æŒé¢„æµ‹å‡†ç¡®åº¦çš„åŒæ—¶å…·å¤‡é«˜åº¦å¤šæ ·æ€§æ—¶ï¼Œå¢åŠ å±‚æ•°æ‰èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œä¸”è¿™ç§å¤šæ ·æ€§åœ¨å•å±‚åå·®è¾ƒå¤§æ—¶å°¤ä¸ºå…³é”®ã€‚é€šè¿‡å¼•å…¥ä¿¡æ¯è®ºå¤šæ ·æ€§åº¦é‡ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†å¢åŠ å±‚æ•°å¸¦æ¥çš„æ€§èƒ½å¢ç›Šå…·æœ‰å­æ¨¡æ€§ (submodularity)ï¼Œè¡¨ç°å‡ºè¾¹é™…æ•ˆç›Šé€’å‡çš„ç‰¹å¾ï¼Œè¿™ä¸æ‰©å±•å®šå¾‹ä¸­é¢„æµ‹çš„å¯¹æ•°æ”¶æ•›è¶‹åŠ¿é«˜åº¦ä¸€è‡´ã€‚åœ¨å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å’Œè¯­ä¹‰ç†è§£ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœéªŒè¯äº†è¿™ä¸€ç†è®ºå±æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24009v2",
      "published_date": "2025-05-29 21:13:31 UTC",
      "updated_date": "2025-06-06 19:28:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:46:00.409799+00:00"
    },
    {
      "arxiv_id": "2506.22439v1",
      "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans",
      "title_zh": "å¿ƒç†è¯­è¨€å­¦è¯æ±‡ç‰¹å¾ï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»å¯¹é½ç¨‹åº¦çš„æ–°æ–¹æ³•",
      "authors": [
        "Javier Conde",
        "Miguel GonzÃ¡lez",
        "MarÃ­a Grandury",
        "Gonzalo MartÃ­nez",
        "Pedro Reviriego",
        "Mar Brysbaert"
      ],
      "abstract": "The evaluation of LLMs has so far focused primarily on how well they can perform different tasks such as reasoning, question-answering, paraphrasing, or translating. For most of these tasks, performance can be measured with objective metrics, such as the number of correct answers. However, other language features are not easily quantified. For example, arousal, concreteness, or gender associated with a given word, as well as the extent to which we experience words with senses and relate them to a specific sense. Those features have been studied for many years by psycholinguistics, conducting large-scale experiments with humans to produce ratings for thousands of words. This opens an opportunity to evaluate how well LLMs align with human ratings on these word features, taking advantage of existing studies that cover many different language features in a large number of words. In this paper, we evaluate the alignment of a representative group of LLMs with human ratings on two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets cover thirteen features over thousands of words. The results show that alignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated (arousal, valence, dominance, concreteness, imageability, familiarity, and gender) than on the Lancaster norms evaluated (introceptive, gustatory, olfactory, haptic, auditory, and visual). This suggests a potential limitation of current LLMs in aligning with human sensory associations for words, which may be due to their lack of embodied cognition present in humans and illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¿ƒç†è¯­è¨€å­¦è¯æ±‡ç‰¹å¾(Psycholinguistic Word Features)è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸äººç±»å¯¹é½(Alignment)ç¨‹åº¦çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶è€…åˆ©ç”¨Glasgowå’ŒLancasterä¸¤ä¸ªå¸¸æ¨¡æ•°æ®é›†ï¼Œé’ˆå¯¹æ¶µç›–å”¤é†’åº¦(arousal)ã€å…·è±¡æ€§(concreteness)åŠæ„Ÿå®˜å…³è”ç­‰13é¡¹å¿ƒç†è¯­è¨€å­¦ç‰¹å¾çš„æ•°åƒä¸ªè¯æ±‡è¿›è¡Œäº†å¯¹é½åº¦è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨Glasgowå¸¸æ¨¡ç‰¹å¾ä¸Šçš„è¡¨ç°æ™®éä¼˜äºåœ¨Lancasterå¸¸æ¨¡ç‰¹å¾ä¸Šçš„è¡¨ç°ï¼Œå°¤å…¶åœ¨æ¶‰åŠè§¦è§‰ã€å—…è§‰ç­‰å…·ä½“æ„Ÿå®˜ä½“éªŒæ—¶å¯¹é½åº¦è¾ƒä½ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ•æ‰äººç±»æ„Ÿå®˜å…³è”æ–¹é¢çš„å±€é™æ€§ï¼Œç ”ç©¶è€…è®¤ä¸ºè¿™å¯èƒ½å½’å› äºæ¨¡å‹ç¼ºä¹äººç±»ç‰¹æœ‰çš„å…·èº«è®¤çŸ¥(Embodied Cognition)ã€‚è¯¥è®ºæ–‡é€šè¿‡å®éªŒè¯æ˜äº†å¿ƒç†è¯­è¨€å­¦æ•°æ®é›†åœ¨è¯„ä¼°æ¨¡å‹ä¸äººç±»å¿ƒç†è¡¨å¾ä¸€è‡´æ€§æ–¹é¢çš„ç‹¬ç‰¹ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for the GEM2 workshop at ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.22439v1",
      "published_date": "2025-05-29 20:56:48 UTC",
      "updated_date": "2025-05-29 20:56:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:45:45.644022+00:00"
    },
    {
      "arxiv_id": "2505.24003v2",
      "title": "Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting",
      "title_zh": "é¢å‘é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹çš„å¤šæ¨¡æ€è§†å›¾å¢å¼ºå¤§è§†è§‰æ¨¡å‹",
      "authors": [
        "ChengAo Shen",
        "Wenchao Yu",
        "Ziming Zhao",
        "Dongjin Song",
        "Wei Cheng",
        "Haifeng Chen",
        "Jingchao Ni"
      ],
      "abstract": "Time series, typically represented as numerical sequences, can also be transformed into images and texts, offering multi-modal views (MMVs) of the same underlying signal. These MMVs can reveal complementary patterns and enable the use of powerful pre-trained large models, such as large vision models (LVMs), for long-term time series forecasting (LTSF). However, as we identified in this work, the state-of-the-art (SOTA) LVM-based forecaster poses an inductive bias towards \"forecasting periods\". To harness this bias, we propose DMMV, a novel decomposition-based multi-modal view framework that leverages trend-seasonal decomposition and a novel backcast-residual based adaptive decomposition to integrate MMVs for LTSF. Comparative evaluations against 14 SOTA models across diverse datasets show that DMMV outperforms single-view and existing multi-modal baselines, achieving the best mean squared error (MSE) on 6 out of 8 benchmark datasets. The code for this paper is available at: https://github.com/D2I-Group/dmmv.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹ (Long-Term Time Series Forecasting, LTSF) æå‡ºäº†ä¸€ç§åä¸º DMMV çš„æ–°å‹åˆ†è§£å¼å¤šæ¨¡æ€è§†å›¾æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è§†è§‰æ¨¡å‹ (Large Vision Models, LVMs) åœ¨å¤„ç†å›¾åƒå’Œæ–‡æœ¬ç­‰å¤šæ¨¡æ€è§†å›¾ (Multi-Modal Views, MMVs) æ—¶å±•ç°å‡ºçš„å½’çº³åå·®ã€‚DMMV ç»“åˆäº†è¶‹åŠ¿-å­£èŠ‚æ€§åˆ†è§£ (trend-seasonal decomposition) å’Œä¸€ç§åŸºäºå›æµ‹æ®‹å·® (backcast-residual based) çš„æ–°å‹è‡ªé€‚åº”åˆ†è§£æŠ€æœ¯ï¼Œæœ‰æ•ˆåœ°é›†æˆäº†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ä»¥å¢å¼ºé¢„æµ‹èƒ½åŠ›ã€‚å®éªŒåœ¨å¤šç§æ•°æ®é›†ä¸Šå°† DMMV ä¸ 14 ç§æœ€å…ˆè¿› (SOTA) æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ï¼Œç»“æœè¡¨æ˜è¯¥æ¡†æ¶ä¼˜äºå•è§†å›¾å’Œç°æœ‰çš„å¤šæ¨¡æ€åŸºçº¿æ¨¡å‹ã€‚åœ¨ 8 ä¸ªåŸºå‡†æ•°æ®é›†ä¸­çš„ 6 ä¸ªä¸Šï¼ŒDMMV å‡å®ç°äº†æœ€ä½³çš„å‡æ–¹è¯¯å·® (Mean Squared Error, MSE) è¡¨ç°ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¤šæ¨¡æ€è§†å›¾å’Œå…ˆè¿›çš„åˆ†è§£æœºåˆ¶å¯ä»¥æ˜¾è‘—æå‡é¢„è®­ç»ƒå¤§æ¨¡å‹åœ¨å¤æ‚æ—¶é—´åºåˆ—åˆ†æä»»åŠ¡ä¸­çš„æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.24003v2",
      "published_date": "2025-05-29 20:55:24 UTC",
      "updated_date": "2025-10-31 01:29:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:46:11.035709+00:00"
    },
    {
      "arxiv_id": "2505.24001v4",
      "title": "Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition",
      "title_zh": "åŸºäºä¸²æ‰°æ¶æ„çš„éƒ¨åˆ†æ ‡æ³¨æ¡ä»¶ä¸‹ç”µæœºå¤åˆæ•…éšœè¯Šæ–­å¤šè¾“å‡ºåˆ†ç±»",
      "authors": [
        "Wonjun Yi",
        "Wonho Jung",
        "Hyeonuk Nam",
        "Kangmin Jang",
        "Yong-Hwa Park"
      ],
      "abstract": "The increasing complexity of rotating machinery and the diversity of operating conditions, such as rotating speed and varying torques, have amplified the challenges in fault diagnosis in scenarios requiring domain adaptation, particularly involving compound faults. This study addresses these challenges by introducing a novel multi-output classification (MOC) framework tailored for domain adaptation in partially labeled target datasets. Unlike conventional multi-class classification (MCC) approaches, the MOC framework classifies the severity levels of compound faults simultaneously. Furthermore, we explore various single-task and multi-task architectures applicable to the MOC formulation-including shared trunk and cross-talk-based designs-for compound fault diagnosis under partially labeled conditions. Based on this investigation, we propose a novel cross-talk architecture, residual neural dimension reductor (RNDR), that enables selective information sharing across diagnostic tasks, effectively enhancing classification performance in compound fault scenarios. In addition, frequency-layer normalization was incorporated to improve domain adaptation performance on motor vibration data. Compound fault conditions were implemented using a motor-based test setup and evaluated across six domain adaptation scenarios. The experimental results demonstrate its superior macro F1 performance compared to baseline models. We further showed that the structural advantage of RNDR is more pronounced in compound fault settings through a single-fault comparison. We also found that frequency-layer normalization fits the fault diagnosis task better than conventional methods. Lastly, we analyzed the RNDR with various conditions, other models with increased number of parameters, and compared with the ablated RNDR structure.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—‹è½¬æœºæ¢°å¤æ‚æ€§å¢åŠ åŠå˜å·¥å†µä¸‹å¤åˆæ•…éšœè¯Šæ–­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é€‚ç”¨äºéƒ¨åˆ†æ ‡æ³¨(partially labeled)ç›®æ ‡æ•°æ®é›†åŸŸè‡ªé€‚åº”(domain adaptation)çš„å¤šè¾“å‡ºåˆ†ç±»(multi-output classification, MOC)æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„å¤šåˆ†ç±»æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶èƒ½åŒæ—¶å¯¹å¤åˆæ•…éšœçš„ä¸¥é‡ç¨‹åº¦è¿›è¡Œåˆ†ç±»ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†ä¸€ç§åä¸ºæ®‹å·®ç¥ç»ç»´åº¦ç¼©å‡å™¨(residual neural dimension reductor, RNDR)çš„æ–°å‹äº¤å‰å¯¹è¯(cross-talk)æ¶æ„ï¼Œå®ç°äº†è¯Šæ–­ä»»åŠ¡é—´çš„é€‰æ‹©æ€§ä¿¡æ¯å…±äº«ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†é¢‘ç‡å±‚å½’ä¸€åŒ–(frequency-layer normalization)æŠ€æœ¯ï¼Œä»¥æå‡ç”µæœºæŒ¯åŠ¨æ•°æ®åœ¨å¤„ç†é¢†åŸŸåç§»æ—¶çš„é²æ£’æ€§ã€‚å®éªŒé€šè¿‡ç”µæœºæµ‹è¯•å¹³å°åœ¨å…­ç§åŸŸè‡ªé€‚åº”åœºæ™¯ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å®F1(macro F1)æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†RNDRç»“æ„åœ¨å¤åˆæ•…éšœè®¾ç½®ä¸­çš„ç»“æ„ä¼˜åŠ¿ï¼Œå¹¶å‘ç°é¢‘ç‡å±‚å½’ä¸€åŒ–æ¯”ä¼ ç»Ÿå½’ä¸€åŒ–æ–¹æ³•æ›´å¥‘åˆæ•…éšœè¯Šæ–­ä»»åŠ¡çš„éœ€æ±‚ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "Published in Mechanical Systems and Signal Processing, Volume 244, 15 January 2026, Article 113786. Version of record available online on 25 December 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.24001v4",
      "published_date": "2025-05-29 20:52:54 UTC",
      "updated_date": "2026-01-03 12:34:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:46:26.447771+00:00"
    },
    {
      "arxiv_id": "2505.23996v1",
      "title": "Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs",
      "title_zh": "ä½ çš„æ¨¡å‹æ˜¯å¦è¶³å¤Ÿç¡®å®šï¼Ÿé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥å…¬å¹³æ€§è¯„ä¼°",
      "authors": [
        "Yinong Oliver Wang",
        "Nivedha Sivakumar",
        "Falaah Arif Khan",
        "Rin Metcalf Susa",
        "Adam Golinski",
        "Natalie Mackraz",
        "Barry-John Theobald",
        "Luca Zappella",
        "Nicholas Apostoloff"
      ],
      "abstract": "The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerF, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions compared to conventional fairness measures. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable dataset for evaluating modern LLMs. We establish a benchmark, using our metric and dataset, and apply it to evaluate the behavior of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark, which evaluates fairness with uncertainty awareness, paves the way for developing more transparent and accountable AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å…¬å¹³æ€§è¯„ä¼°ä¸­ä¼ ç»ŸæŒ‡æ ‡ä»…å…³æ³¨é¢„æµ‹å‡†ç¡®æ€§è€Œå¿½ç•¥æ¨¡å‹ä¸ç¡®å®šæ€§(Uncertainty)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„å…¬å¹³æ€§æŒ‡æ ‡ UCerFã€‚UCerF èƒ½å¤Ÿæ•æ‰æ¨¡å‹åœ¨ä¸åŒç¾¤ä½“é—´ç½®ä¿¡åº¦çš„å·®å¼‚ï¼Œä»è€Œæ¯”ä¼ ç»Ÿè¡¡é‡æ ‡å‡†æ›´ç»†è‡´åœ°åæ˜ æ¨¡å‹å†³ç­–ä¸­çš„å†…åœ¨åå·®ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ•°æ®é›†å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œç ”ç©¶è€…è¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å« 31,756 ä¸ªæ ·æœ¬çš„æ€§åˆ«-èŒä¸šå…¬å¹³æ€§è¯„ä¼°æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºå…±æŒ‡æ¶ˆè§£(Co-reference Resolution)ä»»åŠ¡ã€‚åˆ©ç”¨è¯¥æŒ‡æ ‡å’Œæ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿå¯¹åç§å¼€æº LLMs è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒå‘ç° UCerF èƒ½å¤Ÿæ­ç¤ºè¢«ä¼ ç»ŸæŒ‡æ ‡ Equalized Odds å¿½ç•¥çš„åå·®ï¼Œä¾‹å¦‚ Mistral-7B åœ¨é”™è¯¯é¢„æµ‹ä¸­è¡¨ç°å‡ºçš„è¿‡åº¦è‡ªä¿¡ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºå¼€å‘æ›´é€æ˜ã€æ›´å…·é—®è´£åˆ¶çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 8 figures, and 1 table in main paper. Supplementary appendix attached. Accepted at ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23996v1",
      "published_date": "2025-05-29 20:45:18 UTC",
      "updated_date": "2025-05-29 20:45:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:47:03.699640+00:00"
    },
    {
      "arxiv_id": "2506.04244v1",
      "title": "Zero-Shot Adaptation of Parameter-Efficient Fine-Tuning in Diffusion Models",
      "title_zh": "æ‰©æ•£æ¨¡å‹ä¸­å‚æ•°é«˜æ•ˆå¾®è°ƒçš„é›¶æ ·æœ¬è‡ªé€‚åº”",
      "authors": [
        "Farzad Farhadzadeh",
        "Debasmit Das",
        "Shubhankar Borse",
        "Fatih Porikli"
      ],
      "abstract": "We introduce ProLoRA, enabling zero-shot adaptation of parameter-efficient fine-tuning in text-to-image diffusion models. ProLoRA transfers pre-trained low-rank adjustments (e.g., LoRA) from a source to a target model without additional training data. This overcomes the limitations of traditional methods that require retraining when switching base models, often challenging due to data constraints. ProLoRA achieves this via projection of source adjustments into the target model's weight space, leveraging subspace and null space similarities and selectively targeting aligned layers. Evaluations on established text-to-image models demonstrate successful knowledge transfer and comparable performance without retraining.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProLoRAï¼Œä¸€ç§æ—¨åœ¨å®ç°æ–‡æœ¬ç”Ÿæˆå›¾åƒ(text-to-image)æ‰©æ•£æ¨¡å‹ä¸­å‚æ•°é«˜æ•ˆå¾®è°ƒ(parameter-efficient fine-tuning)çš„é›¶æ ·æœ¬é€‚é…(zero-shot adaptation)æ–¹æ¡ˆã€‚ProLoRA èƒ½å¤Ÿå°†é¢„è®­ç»ƒçš„ä½ç§©è°ƒæ•´ï¼ˆå¦‚ LoRAï¼‰ä»æºæ¨¡å‹ç›´æ¥è¿ç§»è‡³ç›®æ ‡æ¨¡å‹ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨åˆ‡æ¢åŸºç¡€æ¨¡å‹æ—¶å¿…é¡»é‡æ–°è®­ç»ƒçš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æºæ¨¡å‹çš„è°ƒæ•´å‚æ•°æŠ•å½±åˆ°ç›®æ ‡æ¨¡å‹çš„æƒé‡ç©ºé—´ä¸­ï¼Œå……åˆ†åˆ©ç”¨äº†å­ç©ºé—´(subspace)å’Œé›¶ç©ºé—´(null space)çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æœ‰é€‰æ‹©åœ°é’ˆå¯¹å¯¹é½å±‚è¿›è¡Œå¤„ç†ã€‚åœ¨ä¸»æµæ¨¡å‹ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜ï¼ŒProLoRA æˆåŠŸå®ç°äº†æœ‰æ•ˆçš„çŸ¥è¯†è¿ç§»ï¼Œå¹¶åœ¨å®Œå…¨ä¸è¿›è¡Œé‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å±•ç°å‡ºä¸å¾®è°ƒç›¸å½“çš„ç”Ÿæˆæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.04244v1",
      "published_date": "2025-05-29 20:37:04 UTC",
      "updated_date": "2025-05-29 20:37:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:46:45.667383+00:00"
    },
    {
      "arxiv_id": "2505.23990v2",
      "title": "Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding",
      "title_zh": "Multi-RAGï¼šé¢å‘è‡ªé€‚åº”è§†é¢‘ç†è§£çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ",
      "authors": [
        "Mingyang Mao",
        "Mariela M. Perez-Cabarcas",
        "Utteja Kallakuri",
        "Nicholas R. Waytowich",
        "Xiaomin Lin",
        "Tinoosh Mohsenin"
      ],
      "abstract": "To effectively engage in human society, the ability to adapt, filter information, and make informed decisions in ever-changing situations is critical. As robots and intelligent agents become more integrated into human life, there is a growing opportunity-and need-to offload the cognitive burden on humans to these systems, particularly in dynamic, information-rich scenarios.\n  To fill this critical need, we present Multi-RAG, a multimodal retrieval-augmented generation system designed to provide adaptive assistance to humans in information-intensive circumstances. Our system aims to improve situational understanding and reduce cognitive load by integrating and reasoning over multi-source information streams, including video, audio, and text. As an enabling step toward long-term human-robot partnerships, Multi-RAG explores how multimodal information understanding can serve as a foundation for adaptive robotic assistance in dynamic, human-centered situations. To evaluate its capability in a realistic human-assistance proxy task, we benchmarked Multi-RAG on the MMBench-Video dataset, a challenging multimodal video understanding benchmark. Our system achieves superior performance compared to existing open-source video large language models (Video-LLMs) and large vision-language models (LVLMs), while utilizing fewer resources and less input data. The results demonstrate Multi- RAG's potential as a practical and efficient foundation for future human-robot adaptive assistance systems in dynamic, real-world contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨ä¿¡æ¯å¯†é›†ä¸”åŠ¨æ€å˜åŒ–çš„åœºæ™¯ä¸­å‡è½»äººç±»è®¤çŸ¥è´Ÿæ‹…çš„éœ€æ±‚ï¼Œæå‡ºäº†Multi-RAGï¼Œä¸€ç§å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ(Multimodal Retrieval-Augmented Generation)ç³»ç»Ÿã€‚Multi-RAGæ—¨åœ¨é€šè¿‡æ•´åˆå¹¶æ¨ç†è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬ç­‰å¤šæºä¿¡æ¯æµï¼Œæå‡å¯¹æƒ…å¢ƒçš„ç†è§£å¹¶æä¾›è‡ªé€‚åº”è¾…åŠ©ã€‚ä½œä¸ºå®ç°é•¿æœŸäººæœºåä½œçš„å…³é”®ä¸€æ­¥ï¼Œè¯¥ç³»ç»Ÿæ¢ç´¢äº†å¤šæ¨¡æ€ä¿¡æ¯ç†è§£å¦‚ä½•ä¸ºåŠ¨æ€ã€ä»¥äººä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸‹çš„æœºå™¨äººè‡ªé€‚åº”è¾…åŠ©å¥ å®šåŸºç¡€ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æŒ‘æˆ˜æ€§æé«˜çš„MMBench-Videoè§†é¢‘ç†è§£åŸºå‡†æ•°æ®é›†ä¸Šå¯¹Multi-RAGè¿›è¡Œäº†æµ‹è¯•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼ŒMulti-RAGåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹(Video-LLMs)å’Œå¤§è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)ï¼Œä¸”ä½¿ç”¨çš„èµ„æºå’Œè¾“å…¥æ•°æ®æ›´å°‘ã€‚è¯¥ç³»ç»Ÿçš„æˆåŠŸç ”å‘è¯æ˜äº†å…¶ä½œä¸ºæœªæ¥åŠ¨æ€ç°å®ç¯å¢ƒä¸­äººæœºè‡ªé€‚åº”è¾…åŠ©ç³»ç»Ÿçš„é«˜æ•ˆä¸”å®ç”¨çš„åŸºç¡€æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23990v2",
      "published_date": "2025-05-29 20:31:29 UTC",
      "updated_date": "2025-06-14 20:12:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:46:48.360516+00:00"
    },
    {
      "arxiv_id": "2505.23987v1",
      "title": "Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization",
      "title_zh": "é¢å‘å¯æ§å¤šå±æ€§å¤šç›®æ ‡åˆ†å­ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Vishal Dey",
        "Xiao Hu",
        "Xia Ning"
      ],
      "abstract": "In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through https://github.com/ninglab/GeLLMO-C.",
      "tldr_zh": "åœ¨ç°å®ä¸–ç•Œçš„è¯ç‰©è®¾è®¡ä¸­ï¼ŒMolecule Optimization éœ€è¦åŒæ—¶æå‡å¤šé¡¹åˆ†å­å±æ€§è‡³åˆ¶è¯ç›¸å…³æ°´å¹³ï¼Œå¹¶ç»´æŒå·²æœ‰è¾¾æ ‡å±æ€§ï¼Œä½†ç°æœ‰çš„è®¡ç®—æ–¹æ³•å’ŒæŒ‡ä»¤å¾®è°ƒ LLMs éš¾ä»¥æ•æ‰è¿™ç±»ç»†å¾®çš„ç‰¹å®šå±æ€§ç›®æ ‡ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº† C-MuMOInstructï¼Œè¿™æ˜¯é¦–ä¸ªä¸“æ³¨äºå…·æœ‰æ˜ç¡®å±æ€§ç‰¹å®šç›®æ ‡çš„å¤šå±æ€§ä¼˜åŒ–æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œç ”ç©¶è€…å¼€å‘äº† GeLLMO-Cs ç³»åˆ—æŒ‡ä»¤å¾®è°ƒ LLMsï¼Œèƒ½å¤Ÿæ‰§è¡Œæœ‰é’ˆå¯¹æ€§çš„å±æ€§ç‰¹å®šä¼˜åŒ–ã€‚å®éªŒåœ¨ 5 ä¸ªåˆ†å¸ƒå†…å’Œ 5 ä¸ªåˆ†å¸ƒå¤–ä»»åŠ¡ä¸­è¿›è¡Œï¼Œç»“æœæ˜¾ç¤º GeLLMO-Cs ä¸€è‡´ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼ŒæˆåŠŸç‡æå‡é«˜è¾¾ 126%ã€‚æ­¤å¤–ï¼ŒGeLLMO-Cs åœ¨é¢å¯¹æ–°ä¼˜åŒ–ä»»åŠ¡å’Œæœªè§è¿‡çš„æŒ‡ä»¤æ—¶å±•ç°å‡ºæ˜¾è‘—çš„ Zero-shot Generalization èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºæ”¯æŒç°å®ä¸”å¤šæ ·åŒ–çš„ç‰¹å®šå±æ€§åˆ†å­ä¼˜åŒ–æä¾›äº†åŸºç¡€æ€§ LLM æ”¯æŒï¼Œå…¶ä»£ç å’Œæ•°æ®é›†å‡å·²å¼€æºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23987v1",
      "published_date": "2025-05-29 20:29:14 UTC",
      "updated_date": "2025-05-29 20:29:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:47:05.481551+00:00"
    },
    {
      "arxiv_id": "2505.23982v1",
      "title": "MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge",
      "title_zh": "MSQAï¼šé¢å‘ç ”ç©¶ç”Ÿæ°´å¹³ææ–™ç§‘å­¦æ¨ç†ä¸çŸ¥è¯†çš„å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Jerry Junyang Cheung",
        "Shiyao Shen",
        "Yuchen Zhuang",
        "Yinghao Li",
        "Rampi Ramprasad",
        "Chao Zhang"
      ],
      "abstract": "Despite recent advances in large language models (LLMs) for materials science, there is a lack of benchmarks for evaluating their domain-specific knowledge and complex reasoning abilities. To bridge this gap, we introduce MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials science questions in two formats: detailed explanatory responses and binary True/False assessments. MSQA distinctively challenges LLMs by requiring both precise factual knowledge and multi-step reasoning across seven materials science sub-fields, such as structure-property relationships, synthesis processes, and computational modeling. Through experiments with 10 state-of-the-art LLMs, we identify significant gaps in current LLM performance. While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source (OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform significantly due to overfitting and distributional shifts. MSQA represents the first benchmark to jointly evaluate the factual and reasoning capabilities of LLMs crucial for LLMs in advanced materials science.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†MSQAï¼Œä¸€ä¸ªåŒ…å«1,757ä¸ªç ”ç©¶ç”Ÿæ°´å¹³é—®é¢˜çš„ç»¼åˆè¯„ä»·åŸºå‡†ï¼Œæ—¨åœ¨å¡«è¡¥å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ææ–™ç§‘å­¦é¢†åŸŸç¼ºä¹ä¸“ä¸šçŸ¥è¯†ä¸å¤æ‚æ¨ç†è¯„ä¼°å·¥å…·çš„ç©ºç™½ã€‚MSQAæ¶µç›–äº†ç»“æ„-æ€§èƒ½å…³ç³»(structure-property relationships)ã€åˆæˆè¿‡ç¨‹(synthesis processes)å’Œè®¡ç®—å»ºæ¨¡(computational modeling)ç­‰ä¸ƒä¸ªå­é¢†åŸŸï¼Œé€šè¿‡è¯¦ç»†è§£é‡Šæ€§å›ç­”å’ŒäºŒå…ƒTrue/Falseè¯„ä¼°ä¸¤ç§å½¢å¼ï¼Œå¯¹æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›æå‡ºäº†ä¸¥å³»æŒ‘æˆ˜ã€‚å®éªŒæµ‹è¯•äº†10ç§å…ˆè¿›çš„LLMsï¼Œç»“æœæ˜¾ç¤ºåŸºäºAPIçš„å•†ä¸šæ¨¡å‹å‡†ç¡®ç‡å¯è¾¾84.5%ï¼Œè€Œå¼€æº(OSS)æ¨¡å‹ä»…ä¸º60.5%å·¦å³ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°è®¸å¤šç‰¹å®šé¢†åŸŸæ¨¡å‹å› è¿‡æ‹Ÿåˆå’Œåˆ†å¸ƒåç§»(distributional shifts)ç­‰é—®é¢˜è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ä½œä¸ºé¦–ä¸ªè”åˆè¯„ä¼°LLMsåœ¨å…ˆè¿›ææ–™ç§‘å­¦ä¸­äº‹å®ä¸æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼ŒMSQAä¸ºè¯†åˆ«å½“å‰æ¨¡å‹æ€§èƒ½å·®è·å¹¶æ¨åŠ¨è¯¥é¢†åŸŸAIçš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23982v1",
      "published_date": "2025-05-29 20:22:57 UTC",
      "updated_date": "2025-05-29 20:22:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:47:11.918179+00:00"
    },
    {
      "arxiv_id": "2506.00074v2",
      "title": "Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations",
      "title_zh": "è°æ¦œä¸Šæœ‰åï¼ŸåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å­¦è€…æ¨èç³»ç»Ÿå®¡è®¡",
      "authors": [
        "Daniele Barolo",
        "Chiara Valentin",
        "Fariba Karimi",
        "Luis GalÃ¡rraga",
        "Gonzalo G. MÃ©ndez",
        "Lisette EspÃ­n-Noboa"
      ],
      "abstract": "This paper evaluates the performance of six open-weight LLMs (llama3-8b, llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending experts in physics across five tasks: top-k experts by field, influential scientists by discipline, epoch, seniority, and scholar counterparts. The evaluation examines consistency, factuality, and biases related to gender, ethnicity, academic popularity, and scholar similarity. Using ground-truth data from the American Physical Society and OpenAlex, we establish scholarly benchmarks by comparing model outputs to real-world academic records. Our analysis reveals inconsistencies and biases across all models. mixtral-8x7b produces the most stable outputs, while llama3.1-70b shows the highest variability. Many models exhibit duplication, and some, particularly gemma2-9b and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real scientists, but accuracy drops in field-, epoch-, and seniority-specific queries, consistently favoring senior scholars. Representation biases persist, replicating gender imbalances (reflecting male predominance), under-representing Asian scientists, and over-representing White scholars. Despite some diversity in institutional and collaboration networks, models favor highly cited and productive scholars, reinforcing the rich-getricher effect while offering limited geographical representation. These findings highlight the need to improve LLMs for more reliable and equitable scholarly recommendations.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å…­ç§å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç‰©ç†å­¦é¢†åŸŸä¸“å®¶æ¨èä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åˆ©ç”¨ç¾å›½ç‰©ç†å­¦ä¼š(APS)å’ŒOpenAlexçš„çœŸå®å­¦æœ¯è®°å½•ä½œä¸ºåŸºå‡†ï¼Œç ”ç©¶è€…å®¡è®¡äº†æ¨¡å‹åœ¨ä¸€è‡´æ€§ã€äº‹å®æ€§ä»¥åŠæ€§åˆ«ã€ç§æ—å’Œå­¦æœ¯çŸ¥ååº¦ç­‰æ–¹é¢çš„åè§ã€‚å®éªŒå‘ç°ï¼Œmixtral-8x7bçš„è¾“å‡ºæœ€ä¸ºç¨³å®šï¼Œè€Œllama3.1-70bçš„å˜åŒ–æ€§æœ€é«˜ï¼Œä¸”å¤šæ•°æ¨¡å‹åœ¨ç‰¹å®šå­¦ç§‘æˆ–æ—¶ä»£çš„æŸ¥è¯¢ä¸­å‡†ç¡®ç‡æœ‰æ‰€ä¸‹é™ã€‚åˆ†ææ­ç¤ºäº†æ™®éå­˜åœ¨çš„ä»£è¡¨æ€§åè§ï¼ŒåŒ…æ‹¬å¯¹ç”·æ€§å’Œç™½äººå­¦è€…çš„è¿‡åº¦ä»£è¡¨ï¼Œä»¥åŠå¯¹äºšæ´²ç§‘å­¦å®¶çš„ä»£è¡¨æ€§ä¸è¶³ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å€¾å‘äºæ¨èé«˜å¼•ç”¨å’Œé«˜äº§å‡ºçš„å­¦è€…ï¼Œä»è€Œå¼ºåŒ–äº†å­¦æœ¯ç•Œâ€œå¯Œè€…æ„ˆå¯Œâ€(rich-get-richer effect)çš„æ•ˆåº”ã€‚è¿™äº›ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å­¦æœ¯æ¨èç³»ç»ŸäºŸéœ€æ”¹è¿›ï¼Œä»¥ç¡®ä¿æ›´å¯é ä¸”å…¬å¹³çš„å­¦æœ¯è¯„ä»·ä¸å‘ç°ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.DL",
        "cs.IR",
        "cs.SI",
        "physics.soc-ph"
      ],
      "primary_category": "cs.CY",
      "comment": "40 pages: 10 main (incl. 9 figures), 3 references, and 27 appendix. Paper under-review",
      "pdf_url": "https://arxiv.org/pdf/2506.00074v2",
      "published_date": "2025-05-29 20:11:11 UTC",
      "updated_date": "2025-09-10 17:27:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:47:23.805105+00:00"
    },
    {
      "arxiv_id": "2505.23977v1",
      "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL",
      "title_zh": "VisualSphinxï¼šé¢å‘å¼ºåŒ–å­¦ä¹ çš„å¤§è§„æ¨¡åˆæˆè§†è§‰é€»è¾‘è°œé¢˜",
      "authors": [
        "Yichen Feng",
        "Zhangchen Xu",
        "Fengqing Jiang",
        "Yuetai Li",
        "Bhaskar Ramasubramanian",
        "Luyao Niu",
        "Bill Yuchen Lin",
        "Radha Poovendran"
      ],
      "abstract": "Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤šæ¨¡æ€æ¨ç†å’Œé€»è¾‘å†³ç­–ä¸­ç¼ºä¹å¤§è§„æ¨¡ã€ç»“æ„åŒ–è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡åˆæˆè§†è§‰é€»è¾‘æ¨ç†æ•°æ®é›†VisualSphinxã€‚ç ”ç©¶è€…è®¾è®¡äº†ä¸€å¥—ä»è§„åˆ™åˆ°å›¾åƒçš„åˆæˆæµæ°´çº¿(rule-to-image synthesis pipeline)ï¼Œé€šè¿‡ä»ç§å­é—®é¢˜ä¸­æå–å¹¶æ‰©å±•è°œé¢˜è§„åˆ™ï¼Œè‡ªåŠ¨ç”Ÿæˆåˆæˆå›¾åƒåŠå¯¹åº”çš„æ¥åœ°(grounding)ç­”æ¡ˆã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨VisualSphinxä¸Šåˆ©ç”¨GRPOç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡VLMsçš„é€»è¾‘æ¨ç†èƒ½åŠ›ä¸é€»è¾‘è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œè¿™ç§èƒ½åŠ›çš„æå‡è¿˜å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œä½¿æ¨¡å‹åœ¨ä»£æ•°æ¨ç†(algebraic reasoning)ã€ç®—æœ¯æ¨ç†(arithmetic reasoning)å’Œå‡ ä½•æ¨ç†(geometry reasoning)ç­‰ç›¸å…³ä»»åŠ¡ä¸­å‡å–å¾—äº†æ›´ä¼˜çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page at https://visualsphinx.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2505.23977v1",
      "published_date": "2025-05-29 20:08:36 UTC",
      "updated_date": "2025-05-29 20:08:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:47:33.242303+00:00"
    },
    {
      "arxiv_id": "2505.23968v1",
      "title": "Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention",
      "title_zh": "Confidential Guardianï¼šåŸºäºå¯†ç å­¦æ‰‹æ®µé˜²èŒƒæ¨¡å‹å¼ƒæƒæ»¥ç”¨",
      "authors": [
        "Stephan Rabanser",
        "Ali Shahin Shamsabadi",
        "Olive Franzese",
        "Xiao Wang",
        "Adrian Weller",
        "Nicolas Papernot"
      ],
      "abstract": "Cautious predictions -- where a machine learning model abstains when uncertain -- are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model's proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†è°¨æ…é¢„æµ‹ (Cautious predictions) æœºåˆ¶å¯èƒ½è¢«ä¸è¯šå®æœºæ„åˆ©ç”¨ï¼Œé€šè¿‡æ¨¡å‹å¼ƒæƒ (Model Abstention) è¿›è¡Œæ­§è§†æˆ–ä¸å…¬æ­£æ‹’ç»æœåŠ¡çš„é£é™©ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸º Mirage çš„æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡æ•…æ„é™ä½ç‰¹å®šç›®æ ‡çš„ç½®ä¿¡åº¦æ¥å®æ–½éšè”½æ­§è§†ï¼ŒåŒæ—¶ç»´æŒæ¨¡å‹çš„é«˜æ•´ä½“é¢„æµ‹æ€§èƒ½ã€‚ä¸ºåº”å¯¹æ­¤å¨èƒï¼Œç ”ç©¶æå‡ºäº†åä¸º Confidential Guardian çš„é˜²å¾¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ ¡å‡†æŒ‡æ ‡ (Calibration metrics) åˆ†ææ¥è¯†åˆ«è¢«æ¶æ„æŠ‘åˆ¶çš„ç½®ä¿¡åº¦ã€‚è¯¥ç³»ç»Ÿè¿›ä¸€æ­¥å¼•å…¥äº†é’ˆå¯¹å·²éªŒè¯æ¨ç†çš„é›¶çŸ¥è¯†è¯æ˜ (Zero-knowledge proofs)ï¼Œåœ¨ä¿æŠ¤æ¨¡å‹ç§å¯†æ€§çš„åŒæ—¶ï¼Œç¡®ä¿ç½®ä¿¡åº¦å¾—åˆ†çœŸå®æºè‡ªéƒ¨ç½²æ¨¡å‹ã€‚å®éªŒç»“æœè¯å®ï¼ŒConfidential Guardian èƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢å¯¹è°¨æ…é¢„æµ‹çš„æ»¥ç”¨ï¼Œä¸ºæ¨¡å‹å¼ƒæƒè¡Œä¸ºåæ˜ çœŸå®ä¸ç¡®å®šæ€§è€Œéæ¶æ„ä¼å›¾æä¾›äº†å¯é çš„å¯éªŒè¯ä¿è¯ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CR",
      "comment": "Proceedings of the 42nd International Conference on Machine Learning",
      "pdf_url": "https://arxiv.org/pdf/2505.23968v1",
      "published_date": "2025-05-29 19:47:50 UTC",
      "updated_date": "2025-05-29 19:47:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:47:39.437603+00:00"
    },
    {
      "arxiv_id": "2506.02027v1",
      "title": "The End Of Universal Lifelong Identifiers: Identity Systems For The AI Era",
      "title_zh": "é€šç”¨ç»ˆèº«æ ‡è¯†ç¬¦çš„ç»ˆç»“ï¼šé¢å‘äººå·¥æ™ºèƒ½æ—¶ä»£çš„èº«ä»½ç³»ç»Ÿ",
      "authors": [
        "Shriphani Palakodety"
      ],
      "abstract": "Many identity systems assign a single, static identifier to an individual for life, reused across domains like healthcare, finance, and education. These Universal Lifelong Identifiers (ULIs) underpin critical workflows but now pose systemic privacy risks. We take the position that ULIs are fundamentally incompatible with the AI era and must be phased out. We articulate a threat model grounded in modern AI capabilities and show that traditional safeguards such as redaction, consent, and access controls are no longer sufficient. We define core properties for identity systems in the AI era and present a cryptographic framework that satisfies them while retaining compatibility with existing identifier workflows. Our design preserves institutional workflows, supports essential functions such as auditability and delegation, and offers a practical migration path beyond ULIs.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œåœ¨åŒ»ç–—ã€é‡‘èå’Œæ•™è‚²ç­‰é¢†åŸŸå¹¿æ³›ä½¿ç”¨çš„é€šç”¨ç»ˆèº«æ ‡è¯†ç¬¦ (Universal Lifelong Identifiers, ULIs) åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£é¢ä¸´ä¸¥é‡çš„ç³»ç»Ÿæ€§éšç§é£é™©ã€‚ä½œè€…è®¤ä¸º ULIs ä¸ç°ä»£ AI çš„èƒ½åŠ›æ ¹æœ¬ä¸å…¼å®¹ï¼Œå¿…é¡»é€æ­¥æ·˜æ±°ï¼Œå¹¶æ·±å…¥é˜è¿°äº†åŸºäºç°ä»£ AI æŠ€æœ¯çš„å¨èƒæ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè„±æ• (redaction)ã€çŸ¥æƒ…åŒæ„å’Œè®¿é—®æ§åˆ¶ç­‰ä¼ ç»Ÿä¿æŠ¤æ‰‹æ®µå·²ä¸è¶³ä»¥åº”å¯¹å½“å‰çš„éšç§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å®šä¹‰äº† AI æ—¶ä»£èº«ä»½ç³»ç»Ÿçš„æ ¸å¿ƒå±æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ—¢èƒ½æ»¡è¶³è¿™äº›å±æ€§åˆä¸ç°æœ‰æ ‡è¯†ç¬¦å·¥ä½œæµä¿æŒå…¼å®¹çš„åŠ å¯†æ¡†æ¶ (cryptographic framework)ã€‚è¯¥è®¾è®¡åœ¨ç¡®ä¿æœºæ„å·¥ä½œæµè¿ç»­æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆæ”¯æŒäº†å®¡è®¡æ€§å’Œå§”æ‰˜ç­‰å…³é”®åŠŸèƒ½ï¼Œä¸ºèº«ä»½ç³»ç»Ÿä» ULIs å‘æ›´å®‰å…¨æ¨¡å¼çš„æ¼”è¿›æä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„è¿ç§»è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "9 content pages, 14 pages with reference",
      "pdf_url": "https://arxiv.org/pdf/2506.02027v1",
      "published_date": "2025-05-29 19:45:39 UTC",
      "updated_date": "2025-05-29 19:45:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:47:41.237808+00:00"
    },
    {
      "arxiv_id": "2505.23960v1",
      "title": "Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation",
      "title_zh": "æ˜ å°„ä¸­çš„ä¿¡æ¯ç»“æ„ï¼šä¸€ç§å­¦ä¹ ã€è¡¨å¾ä¸æ³›åŒ–çš„æ–¹æ³•",
      "authors": [
        "Henry Conklin"
      ],
      "abstract": "Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters.\n  The experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡ç¥ç»ç½‘ç»œåœ¨è¡¨å¾ç©ºé—´(Representational Spaces)ç¼ºä¹ç»Ÿä¸€æè¿°æ–¹æ³•çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—ç”¨äºè¯†åˆ«æ˜ å°„(Mapping)ä¸­ç³»ç»Ÿæ€§ç»“æ„çš„å®šé‡åˆ†ææ¡†æ¶ã€‚ç ”ç©¶é€šè¿‡ç•Œå®šæ˜ å°„ä¸­çš„ç»“æ„åŸè¯­(Structural Primitives)å¹¶åˆ©ç”¨ä¿¡æ¯è®º(Information Theoretic)æŒ‡æ ‡è¿›è¡Œé‡åŒ–ï¼Œç³»ç»Ÿåˆ†æäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¡¨å¾å­¦ä¹ è¿‡ç¨‹ä¸­çš„ç»“æ„æ¼”å˜åŠå…¶ä¸æ³›åŒ–(Generalisation)èƒ½åŠ›çš„å†…åœ¨è”ç³»ã€‚ä¸ºäº†æ”¯æŒå¤§è§„æ¨¡æ¨¡å‹çš„åˆ†æï¼Œè®ºæ–‡å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„å‘é‡ç©ºé—´ç†µ(Entropy)ä¼°è®¡æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å‚æ•°é‡è·¨åº¦ä»100ä¸‡è‡³120äº¿çš„å„ç±»æ¨¡å‹ï¼Œæ¶µç›–äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ã€åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä»¥åŠå¤§è¯­è¨€æ¨¡å‹(LLMs)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“ä»£ç¥ç»ç½‘ç»œé©±åŠ¨æ€§èƒ½çš„ç»“æ„ç‰¹å¾ä¸äººç±»è¯­è¨€ç»“æ„åŠå…¶äº§ç”Ÿçº¦æŸä¹‹é—´å­˜åœ¨è¯¸å¤šå¹³è¡Œä¹‹å¤„ã€‚è¿™ä¸€å·¥ä½œä¸ä»…ä¸ºç†è§£å¤§è§„æ¨¡åˆ†å¸ƒå¼è®¤çŸ¥æ¨¡å‹çš„å­¦ä¹ æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œä¹Ÿä¸ºè¯„ä¼°ä¸åŒè®¾è®¡å†³ç­–å¯¹æ¨¡å‹è¡¨å¾ç»“æ„çš„å½±å“æä¾›äº†å·¥å…·æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "PhD Thesis, 204 pages; entropy estimation discussed from p.94",
      "pdf_url": "https://arxiv.org/pdf/2505.23960v1",
      "published_date": "2025-05-29 19:27:50 UTC",
      "updated_date": "2025-05-29 19:27:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:47:45.074093+00:00"
    },
    {
      "arxiv_id": "2505.23953v1",
      "title": "Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach",
      "title_zh": "åˆ©ç”¨å¤æ‚åº¦æŒ‡æ ‡å¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»£ç ç”Ÿæˆï¼šä¸€ç§åé¦ˆé©±åŠ¨æ–¹æ³•",
      "authors": [
        "Melika Sepidband",
        "Hamed Taherkhani",
        "Song Wang",
        "Hadi Hemmati"
      ],
      "abstract": "Automatic code generation has gained significant momentum with the advent of Large Language Models (LLMs) such as GPT-4. Although many studies focus on improving the effectiveness of LLMs for code generation, very limited work tries to understand the generated code's characteristics and leverage that to improve failed cases. In this paper, as the most straightforward characteristic of code, we investigate the relationship between code complexity and the success of LLM generated code. Using a large set of standard complexity metrics, we first conduct an empirical analysis to explore their correlation with LLM's performance on code generation (i.e., Pass@1). Using logistic regression models, we identify which complexity metrics are most predictive of code correctness. Building on these findings, we propose an iterative feedback method, where LLMs are prompted to generate correct code based on complexity metrics from previous failed outputs. We validate our approach across multiple benchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs (i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the results with two baseline methods: (a) zero-shot generation, and (b) iterative execution-based feedback without our code complexity insights. Experiment results show that our approach makes notable improvements, particularly with a smaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to the baseline's improvement of 12.5% on the HumanEval dataset. The study expands experiments to BigCodeBench and integrates the method with the Reflexion code generation agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07% (GPT-o3 mini). The results highlight that complexity-aware feedback enhances both direct LLM prompting and agent-based workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»£ç å¤æ‚åº¦ä¸ Large Language Models (LLMs) ç”Ÿæˆä»£ç æˆåŠŸç‡ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå¤æ‚åº¦æŒ‡æ ‡çš„åé¦ˆé©±åŠ¨å¢å¼ºæ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆåˆ©ç”¨é€»è¾‘å›å½’æ¨¡å‹åˆ†æäº†å¤šç§æ ‡å‡†å¤æ‚åº¦æŒ‡æ ‡ï¼Œè¯†åˆ«å‡ºå¯¹ä»£ç æ­£ç¡®æ€§æœ€å…·é¢„æµ‹æ€§çš„å…³é”®æŒ‡æ ‡ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§è¿­ä»£åé¦ˆæ–¹æ³•ï¼Œå³é€šè¿‡æç¤ºè¯å¼•å¯¼ LLMs æ ¹æ®å…ˆå‰å¤±è´¥è¾“å‡ºçš„å¤æ‚åº¦æŒ‡æ ‡æ¥ç”Ÿæˆæ­£ç¡®ä»£ç ã€‚å®éªŒåœ¨ HumanEvalã€MBPPã€LeetCode å’Œ BigCodeBench ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ¶µç›–äº† GPT-4oã€Llama 3.1 å’Œ GPT-o3 mini ç­‰ä¸»æµæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº† Pass@1 æ€§èƒ½ï¼Œå…¶ä¸­ GPT-3.5 Turbo åœ¨ HumanEval ä¸Šçš„è¡¨ç°æ¯”åŸºå‡†æå‡äº† 35.71%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½ä¸ Reflexion ç­‰æ™ºèƒ½ä½“å·¥ä½œæµé›†æˆï¼Œåœ¨ BigCodeBench ä¸Šå®ç°äº†é«˜è¾¾ 23.07% çš„æ€§èƒ½å¢é•¿ï¼Œè¯æ˜äº†å¤æ‚åº¦æ„ŸçŸ¥åé¦ˆåœ¨æå‡ä»£ç ç”Ÿæˆè´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "11 pages, 5 figures. Accepted to COMPSAC 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23953v1",
      "published_date": "2025-05-29 19:06:14 UTC",
      "updated_date": "2025-05-29 19:06:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:47:56.507792+00:00"
    },
    {
      "arxiv_id": "2505.23950v2",
      "title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback",
      "title_zh": "InterMTï¼šåŸºäºäººç±»åé¦ˆçš„å¤šè½®äº¤é”™å¼åå¥½å¯¹é½",
      "authors": [
        "Boyuan Chen",
        "Donghai Hong",
        "Jiaming Ji",
        "Jiacheng Zheng",
        "Bowen Dong",
        "Jiayi Zhou",
        "Kaile Wang",
        "Juntao Dai",
        "Xuyao Wang",
        "Wenqi Chen",
        "Qirui Zheng",
        "Wenxin Li",
        "Sirui Han",
        "Yike Guo",
        "Yaodong Yang"
      ],
      "abstract": "As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: What essential capabilities are still missing? A critical aspect of human learning is continuous interaction with the environment -- not limited to language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support multi-turn, multimodal interaction. In particular, they should comprehend interleaved multimodal contexts and respond coherently in ongoing exchanges. In this work, we present an initial exploration through the InterMT -- the first preference dataset for multi-turn multimodal interaction, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. InterMT captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of \\InterMT through applications such as judge moderation and further reveal the multi-turn scaling law of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step. Our project website can be found at https://pku-intermt.github.io .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†InterMTï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºçœŸå®äººç±»åé¦ˆçš„å¤šè½®äº¤é”™å¤šæ¨¡æ€äº¤äº’åå¥½æ•°æ®é›†ï¼Œæ—¨åœ¨å¼¥è¡¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚è¿ç»­å¯¹è¯æ–¹é¢çš„èƒ½åŠ›çŸ­æ¿ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸“å®¶æ ‡æ³¨ï¼Œå¹¶ç»“åˆå·¥å…·å¢å¼ºçš„MLLMsè®¾è®¡äº†ä¸€å¥—Agentic workflowï¼Œä»¥æ„å»ºé«˜è´¨é‡çš„å¤šè½®é—®ç­”å®ä¾‹ã€‚InterMTæ•°æ®é›†è§„æ¨¡å®å¤§ï¼ŒåŒ…å«15.6kä¸ªæç¤ºã€52.6kä¸ªå¯¹è¯å®ä¾‹åŠ32.4kä¸ªäººå·¥æ ‡æ³¨çš„åå¥½å¯¹ï¼Œä»ä¹ä¸ªå­ç»´åº¦ç²¾ç»†åŒ–æ•æ‰äººç±»åå¥½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¨å‡ºäº†InterMT-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è¾…åŠ©è£åˆ¤æ¨¡å‹ï¼ˆJudge modelï¼‰å¤„ç†å¤šè½®å¤šæ¨¡æ€ä»»åŠ¡æ—¶çš„è¡¨ç°ã€‚å®éªŒç»“æœæ­ç¤ºäº†è£åˆ¤æ¨¡å‹çš„å¤šè½®Scaling Lawï¼Œå¹¶è¯æ˜äº†è¯¥æ•°æ®é›†åœ¨è£åˆ¤å®¡æ ¸ç­‰å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€å·¥ä½œä¸ºæ¨åŠ¨MLLMsåœ¨ç†è§£äº¤é”™å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å¹¶å®ç°ä¸€è‡´æ€§å“åº”æ–¹é¢æä¾›äº†å…³é”®æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23950v2",
      "published_date": "2025-05-29 19:00:42 UTC",
      "updated_date": "2025-12-22 17:36:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:48:27.718668+00:00"
    },
    {
      "arxiv_id": "2505.23949v1",
      "title": "TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks",
      "title_zh": "TSENORï¼šæ±‚è§£å¯è½¬ç½® N:M ç¨€ç–æ©ç çš„é«˜æ•ˆç®—æ³•",
      "authors": [
        "Xiang Meng",
        "Mehdi Makni",
        "Rahul Mazumder"
      ],
      "abstract": "Network pruning reduces the computational requirements of large neural networks, with N:M sparsity -- retaining only N out of every M consecutive weights -- offering a compelling balance between compressed model quality and hardware acceleration. However, N:M sparsity only accelerates forward-pass computations, as N:M patterns are not preserved during matrix transposition, limiting efficiency during training where both passes are computationally intensive. While transposable N:M sparsity has been proposed to address this limitation, existing methods for finding transposable N:M sparse masks either fail to scale to large models or are restricted to M=4 which results in suboptimal compression-accuracy trade-off. We introduce an efficient solver for transposable N:M masks that scales to billion-parameter models. We formulate mask generation as optimal transport problems and solve through entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Our tensor-based implementation exploits GPU parallelism, achieving up to 100x speedup with only 1-10% error compared to existing methods. Our approach can be integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT and ALPS to produce transposable N:M sparse models with arbitrary N:M values. Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains performance close to its standard N:M counterpart and outperforms standard 2:4 sparse model, showing the practical value of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œå‰ªæä¸­ N:M sparsity åœ¨çŸ©é˜µè½¬ç½®æ—¶æ¨¡å¼æ— æ³•ä¿æŒã€ä»è€Œé™åˆ¶è®­ç»ƒæ•ˆç‡çš„é—®é¢˜ï¼Œæå‡ºäº†é«˜æ•ˆç®—æ³• TSENORã€‚ä½œä¸ºä¸€ç§ç”¨äºå¯»æ‰¾ Transposable N:M Sparse Masks çš„æ±‚è§£å™¨ï¼ŒTSENOR æˆåŠŸå…‹æœäº†ç°æœ‰æ–¹æ³•éš¾ä»¥æ‰©å±•è‡³å¤§æ¨¡å‹æˆ–å—é™äºå›ºå®šç¨€ç–æ¯”çš„å±€é™æ€§ã€‚ç ”ç©¶è€…å°†æ©ç ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸º Optimal Transport é—®é¢˜ï¼Œé€šè¿‡ Entropy Regularization å’Œ Dykstra's Algorithm ç»“åˆèˆå…¥ç¨‹åºå®ç°å¿«é€Ÿæ±‚è§£ã€‚è¯¥ç®—æ³•çš„å¼ é‡åŒ–å®ç°å……åˆ†åˆ©ç”¨äº† GPU å¹¶è¡Œæ€§ï¼Œåœ¨ä¿æŒæä½è¯¯å·®çš„åŒæ—¶ï¼Œè¾ƒç°æœ‰æ–¹æ³•å®ç°äº†æœ€é«˜ 100 å€çš„åŠ é€Ÿã€‚TSENOR å¯æ— ç¼é›†æˆè‡³ Wanda å’Œ SparseGPT ç­‰ä¸»æµå‰ªææ¡†æ¶ï¼Œæ”¯æŒä»»æ„ N:M å€¼çš„è®¾å®šã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œé‡‡ç”¨ 16:32 ç¨€ç–åº¦çš„ LLaMA3.2-8B æ¨¡å‹æ€§èƒ½æ¥è¿‘æ ‡å‡† N:M æ°´å¹³ä¸”ä¼˜äºä¼ ç»Ÿçš„ 2:4 ç¨€ç–æ–¹æ¡ˆï¼Œä½“ç°äº†å…¶åœ¨å¤„ç†è¶…å¤§è§„æ¨¡å‚æ•°æ¨¡å‹æ—¶çš„å“è¶Šæ•ˆç‡ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23949v1",
      "published_date": "2025-05-29 18:59:43 UTC",
      "updated_date": "2025-05-29 18:59:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:48:09.265858+00:00"
    },
    {
      "arxiv_id": "2505.23947v1",
      "title": "Position: The Future of Bayesian Prediction Is Prior-Fitted",
      "title_zh": "è§‚ç‚¹ï¼šè´å¶æ–¯é¢„æµ‹çš„æœªæ¥åœ¨äºå…ˆéªŒæ‹Ÿåˆ",
      "authors": [
        "Samuel MÃ¼ller",
        "Arik Reuter",
        "Noah Hollmann",
        "David RÃ¼gamer",
        "Frank Hutter"
      ],
      "abstract": "Training neural networks on randomly generated artificial datasets yields Bayesian models that capture the prior defined by the dataset-generating distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed to leverage this insight. In an era of rapidly increasing computational resources for pre-training and a near stagnation in the generation of new real-world data in many applications, PFNs are poised to play a more important role across a wide range of applications. They enable the efficient allocation of pre-training compute to low-data scenarios. Originally applied to small Bayesian modeling tasks, the field of PFNs has significantly expanded to address more complex domains and larger datasets. This position paper argues that PFNs and other amortized inference approaches represent the future of Bayesian inference, leveraging amortized learning to tackle data-scarce problems. We thus believe they are a fruitful area of research. In this position paper, we explore their potential and directions to address their current limitations.",
      "tldr_zh": "è¯¥ç«‹åœºè®ºæ–‡é˜è¿°äº† Prior-data Fitted Networks (PFNs) åœ¨è´å¶æ–¯é¢„æµ‹ä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œè®¤ä¸ºå…¶ä»£è¡¨äº†è¯¥é¢†åŸŸçš„æœªæ¥å‘å±•æ–¹å‘ã€‚PFNs é€šè¿‡åœ¨éšæœºç”Ÿæˆçš„äººå·¥æ•°æ®é›†ä¸Šè®­ç»ƒç¥ç»ç½‘ç»œï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¹¶æ•æ‰ç”±æ•°æ®ç”Ÿæˆåˆ†å¸ƒæ‰€å®šä¹‰çš„å…ˆéªŒ (Prior)ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æ‘Šé”€æ¨ç† (Amortized Inference)ã€‚åœ¨é¢„è®­ç»ƒè®¡ç®—èµ„æºå……è¶³è€ŒçœŸå®ä¸–ç•Œæ•°æ®å¢é•¿è¶‹äºåœæ»çš„èƒŒæ™¯ä¸‹ï¼ŒPFNs èƒ½å¤Ÿå°†è®¡ç®—èµ„æºæœ‰æ•ˆè½¬åŒ–ä¸ºå¤„ç†ä½æ•°æ®åœºæ™¯çš„èƒ½åŠ›ã€‚è™½ç„¶è¯¥æ–¹æ³•æœ€åˆä»…åº”ç”¨äºå°å‹è´å¶æ–¯å»ºæ¨¡ï¼Œä½†ç›®å‰å·²æ˜¾è‘—æ‰©å±•è‡³æ›´å¤æ‚çš„é¢†åŸŸå’Œæ›´å¤§è§„æ¨¡çš„æ•°æ®é›†ã€‚ä½œè€…å¼ºè°ƒï¼ŒPFNs æ˜¯è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜çš„æœ‰åŠ›å·¥å…·ï¼Œæ˜¯æœªæ¥è´å¶æ–¯æ¨ç†ç ”ç©¶ä¸­æå…·æ½œåŠ›çš„æ–¹å‘ï¼Œå¹¶è¿›ä¸€æ­¥æ¢è®¨äº†å…‹æœå…¶å½“å‰å±€é™æ€§çš„æ”¹è¿›è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as position paper at ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23947v1",
      "published_date": "2025-05-29 18:56:45 UTC",
      "updated_date": "2025-05-29 18:56:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:48:13.956685+00:00"
    },
    {
      "arxiv_id": "2505.23946v2",
      "title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve",
      "title_zh": "Lessons Learnedï¼šä¸€ç§åŠ©åŠ›ä»£ç å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ ä¸æ”¹è¿›çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Yuanzhe Liu",
        "Ryan Deng",
        "Tim Kaler",
        "Xuhao Chen",
        "Charles E. Leiserson",
        "Yao Ma",
        "Jie Chen"
      ],
      "abstract": "Recent studies show that LLMs possess different skills and specialize in different tasks. In fact, we observe that their varied performance occur in several levels of granularity. For example, in the code optimization task, code LLMs excel at different optimization categories and no one dominates others. This observation prompts the question of how one leverages multiple LLM agents to solve a coding problem without knowing their complementary strengths a priori. We argue that a team of agents can learn from each other's successes and failures so as to improve their own performance. Thus, a lesson is the knowledge produced by an agent and passed on to other agents in the collective solution process. We propose a lesson-based collaboration framework, design the lesson solicitation--banking--selection mechanism, and demonstrate that a team of small LLMs with lessons learned can outperform a much larger LLM and other multi-LLM collaboration methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»£ç ä¼˜åŒ–ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„èƒ½åŠ›å·®å¼‚åŠäº’è¡¥æ€§ï¼Œæ¢è®¨äº†å¦‚ä½•åœ¨é¢„å…ˆä¸çŸ¥é“å„æ™ºèƒ½ä½“ä¼˜åŠ¿çš„æƒ…å†µä¸‹é«˜æ•ˆåˆ©ç”¨å¤šæ™ºèƒ½ä½“åä½œã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºç»éªŒæ•™è®­(lesson-based)çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œæ—¨åœ¨è®©æ™ºèƒ½ä½“å›¢é˜Ÿä»å½¼æ­¤çš„æˆåŠŸä¸å¤±è´¥ä¸­ç›¸äº’å­¦ä¹ ä»¥æå‡æ•´ä½“æ€§èƒ½ã€‚è¯¥æ¡†æ¶å°†â€œlessonâ€å®šä¹‰ä¸ºæ™ºèƒ½ä½“åœ¨é›†ä½“è§£é¢˜è¿‡ç¨‹ä¸­äº§ç”Ÿçš„çŸ¥è¯†ï¼Œå¹¶è®¾è®¡äº†ä¸€å¥— solicitation-banking-selectionï¼ˆå¾é›†-å­˜å‚¨-ç­›é€‰ï¼‰æœºåˆ¶æ¥å®ç°çŸ¥è¯†çš„æœ‰æ•ˆä¼ é€’ã€‚é€šè¿‡è¿™ç§æœºåˆ¶ï¼Œä¸åŒæ™ºèƒ½ä½“èƒ½å¤Ÿå…±äº«å¹¶åˆ©ç”¨äº’è¡¥çš„ä¼˜åŠ¿ï¼Œä»è€Œä¼˜åŒ–å¤æ‚çš„ç¼–ç¨‹ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé…å¤‡è¯¥æœºåˆ¶çš„å°å‹ LLMs å›¢é˜Ÿåœ¨æ€§èƒ½ä¸Šä¸ä»…è¶…è¶Šäº†å‚æ•°é‡å¤§å¾—å¤šçš„å•ä¸€æ¨¡å‹ï¼Œä¹Ÿä¼˜äºå…¶ä»–ç°æœ‰çš„å¤šæ™ºèƒ½ä½“åä½œæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025. Code is available at https://github.com/MITIBM-FastCoder/LessonL",
      "pdf_url": "https://arxiv.org/pdf/2505.23946v2",
      "published_date": "2025-05-29 18:56:20 UTC",
      "updated_date": "2025-10-23 15:41:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:48:33.521698+00:00"
    },
    {
      "arxiv_id": "2505.23945v2",
      "title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models",
      "title_zh": "æ·±å…¥æ¢ç©¶å¤§(è§†è§‰)è¯­è¨€æ¨¡å‹çš„åè§ä¸é“¾å¼æ€ç»´å¿ å®æ€§",
      "authors": [
        "Sriram Balasubramanian",
        "Samyadeep Basu",
        "Soheil Feizi"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)ä¸­çš„é“¾å¼æ€ç»´(Chain-of-thought)å¿ å®åº¦(faithfulness)è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œæ·±å…¥æ¢è®¨äº†æ–‡æœ¬å’Œå›¾åƒåå·®å¦‚ä½•å½±å“æ¨ç†è¿‡ç¨‹åŠåå·®è¡¨è¾¾(bias articulation)ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹ç»†ç²’åº¦è¯„ä¼°ç®¡çº¿ï¼Œç”¨äºåˆ†ç±»åå·®è¡¨è¾¾æ¨¡å¼ï¼Œå®ç°äº†æ¯”ä»¥å¾€æ–¹æ³•æ›´ç²¾ç¡®çš„CoTæ¨ç†åˆ†æã€‚å®éªŒå‘ç°ï¼Œä¸æ˜¾å¼æ–‡æœ¬åå·®ç›¸æ¯”ï¼ŒLVLMåœ¨æ¨ç†ä¸­æå°‘è¡¨è¾¾ç»†å¾®çš„å›¾åƒåå·®ï¼Œå³ä¾¿åœ¨ä¸“ä¸šæ¨ç†æ¨¡å‹ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç ”ç©¶è¿˜è¯†åˆ«å‡ºä¸€ç§â€œä¸ä¸€è‡´â€(inconsistent)æ¨ç†ç°è±¡ï¼Œå³æ¨¡å‹åœ¨å¾—å‡ºæ­£ç¡®æ¨ç†åçªç„¶æ”¹å˜æœ€ç»ˆç­”æ¡ˆï¼Œè¿™å¯ä½œä¸ºæ£€æµ‹ç”±ä¸å¿ å®CoTå¼•èµ·çš„åå·®æ¨ç†çš„æœ‰æ•ˆä¿¡å·ã€‚æœ€åï¼Œé€šè¿‡å°†è¯¥æ¡†æ¶æ‰©å±•è‡³å¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œç»“æœæ˜¾ç¤ºç›®å‰çš„æ¨¡å‹åœ¨è¯†åˆ«å’Œè¡¨è¾¾éæ˜¾å¼æç¤ºçš„éšå«çº¿ç´¢æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in EMNLP 2025, 34 pages, 25 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23945v2",
      "published_date": "2025-05-29 18:55:05 UTC",
      "updated_date": "2025-11-01 17:22:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:48:16.394902+00:00"
    },
    {
      "arxiv_id": "2505.23942v1",
      "title": "SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations",
      "title_zh": "SG-Blendï¼šé€šè¿‡å­¦ä¹ æ”¹è¿›å‹ Swish ä¸ GELU ä¹‹é—´çš„æ’å€¼å®ç°é²æ£’çš„ç¥ç»è¡¨ç¤º",
      "authors": [
        "Gaurav Sarkar",
        "Jay Gala",
        "Subarna Tripathi"
      ],
      "abstract": "The design of activation functions remains a pivotal component in optimizing deep neural networks. While prevailing choices like Swish and GELU demonstrate considerable efficacy, they often exhibit domain-specific optima. This work introduces SG-Blend, a novel activation function that blends our proposed SSwish, a first-order symmetric variant of Swish and the established GELU through dynamic interpolation. By adaptively blending these constituent functions via learnable parameters, SG-Blend aims to harness their complementary strengths: SSwish's controlled non-monotonicity and symmetry, and GELU's smooth, probabilistic profile, to achieve a more universally robust balance between model expressivity and gradient stability. We conduct comprehensive empirical evaluations across diverse modalities and architectures, showing performance improvements across all considered natural language and computer vision tasks and models. These results, achieved with negligible computational overhead, underscore SG-Blend's potential as a versatile, drop-in replacement that consistently outperforms strong contemporary baselines. The code is available at https://anonymous.4open.science/r/SGBlend-6CBC.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SG-Blendï¼Œä¸€ç§é€šè¿‡åŠ¨æ€æ’å€¼èåˆäº†æè®®çš„SSwishï¼ˆSwishçš„ä¸€é˜¶å¯¹ç§°å˜ä½“ï¼‰ä¸æˆç†Ÿçš„GELUçš„æ–°å‹æ¿€æ´»å‡½æ•°ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ å‚æ•°ï¼Œè¯¥å‡½æ•°èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ··åˆä¸¤ç§ç»„æˆéƒ¨åˆ†çš„ä¼˜åŠ¿ï¼Œç»“åˆäº†SSwishå—æ§çš„éå•è°ƒæ€§ä¸å¯¹ç§°æ€§ï¼Œä»¥åŠGELUå¹³æ»‘çš„æ¦‚ç‡ç‰¹æ€§ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨æé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å¹¶å¢å¼ºæ¢¯åº¦ç¨³å®šæ€§ï¼Œä»è€Œå®ç°æ›´å…·é²æ£’æ€§çš„ç¥ç»è¡¨ç¤ºã€‚åœ¨å¤šç§æ¨¡æ€å’Œæ¶æ„ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒSG-Blendåœ¨æ‰€æœ‰è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å’Œè®¡ç®—æœºè§†è§‰(CV)ä»»åŠ¡ä¸­å‡å®ç°äº†æ€§èƒ½æå‡ã€‚ç”±äºå…¶è®¡ç®—å¼€é”€æå°ï¼ŒSG-Blendå¯ä»¥ä½œä¸ºä¸€ç§é€šç”¨çš„ã€å³æ’å³ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºå½“å‰çš„å¼ºåŠ›åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23942v1",
      "published_date": "2025-05-29 18:48:18 UTC",
      "updated_date": "2025-05-29 18:48:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:48:21.853731+00:00"
    },
    {
      "arxiv_id": "2505.23933v1",
      "title": "BIRD: Behavior Induction via Representation-structure Distillation",
      "title_zh": "BIRDï¼šåŸºäºè¡¨å¾ç»“æ„è’¸é¦çš„è¡Œä¸ºè¯±å¯¼",
      "authors": [
        "Galen Pogoncheff",
        "Michael Beyeler"
      ],
      "abstract": "Human-aligned deep learning models exhibit behaviors consistent with human values, such as robustness, fairness, and honesty. Transferring these behavioral properties to models trained on different tasks or data distributions remains challenging: aligned behavior is easily forgotten during fine-tuning, and collecting task-specific data that preserves this behavior can be prohibitively costly. We introduce BIRD (Behavior Induction via Representation-structure Distillation), a flexible framework for transferring aligned behavior by matching the internal representation structure of a student model to that of a teacher. Applied to out-of-distribution robustness in image classification, BIRD outperforms fine-tuning, transfer learning, and continual learning methods, improving robust accuracy by up to 16% over the next strongest baseline. It remains effective even when the teacher is trained on a much simpler dataset and is $25 \\times$ smaller than the student. In a large-scale study of over 400 teacher-student pairs, we show that three interpretable and computable properties of the teacher's representations (i.e., task relevance, behavioral relevance, and complementary knowledge) explain up to 85% of the variance in transfer success. These insights offer practical guidance for teacher selection and design. BIRD turns small, well-aligned models into scalable alignment seeds, removing a key bottleneck in deploying safe AI systems in the wild.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BIRDï¼ˆBehavior Induction via Representation-structure Distillationï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†å­¦ç”Ÿæ¨¡å‹çš„å†…éƒ¨è¡¨å¾ç»“æ„ï¼ˆrepresentation structureï¼‰ä¸æ•™å¸ˆæ¨¡å‹è¿›è¡ŒåŒ¹é…ï¼Œå®ç°å¯¹é½è¡Œä¸ºï¼ˆaligned behaviorsï¼‰å¦‚é²æ£’æ€§ã€å…¬å¹³æ€§å’Œè¯šå®æ€§çš„æœ‰æ•ˆè¿ç§»ã€‚åœ¨å›¾åƒåˆ†ç±»çš„åˆ†å¸ƒå¤–é²æ£’æ€§ï¼ˆout-of-distribution robustnessï¼‰æµ‹è¯•ä¸­ï¼ŒBIRDçš„è¡¨ç°ä¼˜äºå¾®è°ƒã€è¿ç§»å­¦ä¹ å’ŒæŒç»­å­¦ä¹ ç­‰å¸¸è§„æ–¹æ³•ï¼Œå…¶ç¨³å¥å‡†ç¡®ç‡æ¯”æ¬¡ä¼˜åŸºçº¿æé«˜äº†16%ã€‚å®éªŒè¯æ˜ï¼Œå³ä½¿æ•™å¸ˆæ¨¡å‹è§„æ¨¡æ¯”å­¦ç”Ÿæ¨¡å‹å°25å€ä¸”è®­ç»ƒæ•°æ®é›†æ›´ç®€å•ï¼Œè¯¥æ–¹æ³•ä¾ç„¶èƒ½ä¿æŒå“è¶Šæ€§èƒ½ã€‚é€šè¿‡å¯¹400å¤šå¯¹æ•™å¸ˆ-å­¦ç”Ÿç»„åˆçš„å¤§è§„æ¨¡ç ”ç©¶ï¼Œç ”ç©¶æ­ç¤ºäº†æ•™å¸ˆè¡¨å¾çš„ä»»åŠ¡ç›¸å…³æ€§ï¼ˆtask relevanceï¼‰ã€è¡Œä¸ºç›¸å…³æ€§ï¼ˆbehavioral relevanceï¼‰å’Œäº’è¡¥çŸ¥è¯†ï¼ˆcomplementary knowledgeï¼‰æ˜¯å†³å®šè¿ç§»æˆåŠŸçš„æ ¸å¿ƒè¦ç´ ã€‚BIRDå°†å°å‹ä¸”å¯¹é½è‰¯å¥½çš„æ¨¡å‹è½¬åŒ–ä¸ºå¯æ‰©å±•çš„å¯¹é½ç§å­ï¼ˆalignment seedsï¼‰ï¼Œä¸ºåœ¨ç°å®ä¸–ç•Œä¸­éƒ¨ç½²å®‰å…¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23933v1",
      "published_date": "2025-05-29 18:29:40 UTC",
      "updated_date": "2025-05-29 18:29:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:48:29.341735+00:00"
    },
    {
      "arxiv_id": "2505.23931v1",
      "title": "Scaling up the think-aloud method",
      "title_zh": "å‡ºå£°æ€ç»´æ³•çš„è§„æ¨¡åŒ–",
      "authors": [
        "Daniel Wurgaft",
        "Ben Prystawski",
        "Kanishk Gandhi",
        "Cedegao E. Zhang",
        "Joshua B. Tenenbaum",
        "Noah D. Goodman"
      ],
      "abstract": "The think-aloud method, where participants voice their thoughts as they solve a task, is a valuable source of rich data about human reasoning processes. Yet, it has declined in popularity in contemporary cognitive science, largely because labor-intensive transcription and annotation preclude large sample sizes. Here, we develop methods to automate the transcription and annotation of verbal reports of reasoning using natural language processing tools, allowing for large-scale analysis of think-aloud data. In our study, 640 participants thought aloud while playing the Game of 24, a mathematical reasoning task. We automatically transcribed the recordings and coded the transcripts as search graphs, finding moderate inter-rater reliability with humans. We analyze these graphs and characterize consistency and variation in human reasoning traces. Our work demonstrates the value of think-aloud data at scale and serves as a proof of concept for the automated analysis of verbal reports.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å£°æ€è€ƒæ³•(think-aloud method)å› äººå·¥è½¬å½•å’Œæ ‡æ³¨è€—æ—¶å¯¼è‡´éš¾ä»¥åœ¨å¤§æ ·æœ¬é‡ç ”ç©¶ä¸­åº”ç”¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å·¥å…·å®ç°è‡ªåŠ¨åŒ–åˆ†æçš„æ–¹æ³•ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ”¶é›†640åå‚ä¸è€…åœ¨è§£å†³24ç‚¹æ¸¸æˆ(Game of 24)æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶çš„å£è¯­æŠ¥å‘Šï¼Œåˆ©ç”¨è¯¥è‡ªåŠ¨åŒ–æµç¨‹å°†å…¶è½¬å½•å¹¶ç¼–ç ä¸ºæœç´¢å›¾(search graphs)ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥è‡ªåŠ¨åŒ–æ–¹æ³•ä¸äººå·¥æ ‡æ³¨ä¹‹é—´å…·æœ‰ä¸­ç­‰ç¨‹åº¦çš„è¯„åˆ†è€…é—´ä¿¡åº¦(inter-rater reliability)ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ»ç”»äººç±»æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ä¸å˜å¼‚æ€§ã€‚è¯¥å·¥ä½œä¸ä»…éªŒè¯äº†å¤§è§„æ¨¡å¤§å£°æ€è€ƒæ•°æ®çš„åˆ†ææ½œåŠ›ï¼Œè¿˜ä¸ºå£è¯­æŠ¥å‘Š(verbal reports)çš„è‡ªåŠ¨åŒ–å¤„ç†æä¾›äº†é‡è¦çš„æ¦‚å¿µéªŒè¯ï¼Œæ˜¾è‘—æå‡äº†è®¤çŸ¥ç§‘å­¦ä¸­å£è¯­æŠ¥å‘Šåˆ†æçš„å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 4 figures. Daniel Wurgaft and Ben Prystawski contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2505.23931v1",
      "published_date": "2025-05-29 18:26:23 UTC",
      "updated_date": "2025-05-29 18:26:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:48:38.458526+00:00"
    },
    {
      "arxiv_id": "2505.23930v1",
      "title": "Exploring Societal Concerns and Perceptions of AI: A Thematic Analysis through the Lens of Problem-Seeking",
      "title_zh": "æ¢ç©¶å…¬ä¼—å¯¹äººå·¥æ™ºèƒ½çš„ç¤¾ä¼šå…³åˆ‡ä¸è®¤çŸ¥ï¼šåŸºäºâ€œé—®é¢˜æ¢å¯»â€è§†è§’çš„ä¸»é¢˜åˆ†æ",
      "authors": [
        "Naomi Omeonga wa Kayembe"
      ],
      "abstract": "This study introduces a novel conceptual framework distinguishing problem-seeking from problem-solving to clarify the unique features of human intelligence in contrast to AI. Problem-seeking refers to the embodied, emotionally grounded process by which humans identify and set goals, while problem-solving denotes the execution of strategies aimed at achieving such predefined objectives. The framework emphasizes that while AI excels at efficiency and optimization, it lacks the orientation derived from experiential grounding and the embodiment flexibility intrinsic to human cognition. To empirically explore this distinction, the research analyzes metadata from 157 YouTube videos discussing AI. Conducting a thematic analysis combining qualitative insights with keyword-based quantitative metrics, this mixed-methods approach uncovers recurring themes in public discourse, including privacy, job displacement, misinformation, optimism, and ethical concerns. The results reveal a dual sentiment: public fascination with AI's capabilities coexists with anxiety and skepticism about its societal implications. The discussion critiques the orthogonality thesis, which posits that intelligence is separable from goal content, and instead argues that human intelligence integrates goal-setting and goal-pursuit. It underscores the centrality of embodied cognition in human reasoning and highlights how AI's limitations come from its current reliance on computational processing. The study advocates for enhancing emotional and digital literacy to foster responsible AI engagement. It calls for reframing public discourse to recognize AI as a tool that augments -- rather than replaces -- human intelligence. By positioning problem seeking at the core of cognition and as a critical dimension of intelligence, this research offers new perspectives on ethically aligned and human-centered AI development.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¦‚å¿µæ¡†æ¶ï¼Œé€šè¿‡åŒºåˆ† problem-seeking (å¯»æ‰¾é—®é¢˜) ä¸ problem-solving (è§£å†³é—®é¢˜) æ¥é˜æ˜äººç±»æ™ºèƒ½ä¸ AI çš„æœ¬è´¨åŒºåˆ«ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œproblem-seeking æ˜¯äººç±»ç‰¹æœ‰çš„ã€åŸºäºå…·èº«è®¤çŸ¥(embodied)å’Œæƒ…æ„ŸåŸºç¡€çš„ç›®æ ‡è®¾å®šè¿‡ç¨‹ï¼Œè€Œ AI è™½ç„¶åœ¨æ•ˆç‡ä¼˜åŒ–å’Œç­–ç•¥æ‰§è¡Œä¸Šè¡¨ç°å“è¶Šï¼Œä½†ç¼ºä¹æºäºç»éªŒåŸºç¡€çš„å¯¼å‘å’Œçµæ´»æ€§ã€‚ä¸ºäº†å®è¯æ¢ç´¢è¿™ä¸€å·®å¼‚ï¼Œç ”ç©¶è€…é‡‡ç”¨æ··åˆæ–¹æ³•å¯¹ 157 ä¸ªæ¢è®¨ AI çš„ YouTube è§†é¢‘è¿›è¡Œäº†ä¸»é¢˜åˆ†æ(thematic analysis)ï¼Œæ­ç¤ºäº†å…¬ä¼—åœ¨éšç§ã€å°±ä¸šã€è™šå‡ä¿¡æ¯åŠä¼¦ç†ç­‰æ–¹é¢çš„æ ¸å¿ƒå…³åˆ‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œå…¬ä¼—å¯¹ AI çš„å…ˆè¿›èƒ½åŠ›å……æ»¡è¿·æ‹ï¼Œä½†åŒæ—¶ä¹Ÿå¯¹å…¶ç¤¾ä¼šå½±å“æŒæœ‰æ˜¾è‘—çš„ç„¦è™‘ä¸æ€€ç–‘æ€åº¦ã€‚è¯¥ç ”ç©¶æ‰¹åˆ¤äº†å°†æ™ºèƒ½ä¸ç›®æ ‡å†…å®¹åˆ†ç¦»çš„æ­£äº¤æ€§è®ºç‚¹(orthogonality thesis)ï¼Œå¼ºè°ƒäººç±»æ™ºèƒ½æ˜¯ç›®æ ‡è®¾å®šä¸è¿½æ±‚çš„ç»Ÿä¸€ä½“ã€‚æœ€åï¼Œæ–‡ç« å»ºè®®é€šè¿‡æå‡æ•°å­—ç´ å…»å°† AI é‡æ–°å®šä½ä¸ºå¢å¼ºäººç±»æ™ºèƒ½çš„å·¥å…·ï¼Œä¸ºæ„å»ºä»¥äººä¸ºä¸­å¿ƒçš„ AI å‘å±•è·¯å¾„æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "48 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.23930v1",
      "published_date": "2025-05-29 18:24:34 UTC",
      "updated_date": "2025-05-29 18:24:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:01.411845+00:00"
    },
    {
      "arxiv_id": "2505.23923v2",
      "title": "Act-Adaptive Margin: Dynamically Calibrating Reward Models for Subjective Ambiguity",
      "title_zh": "Act-Adaptive Marginï¼šé¢å‘ä¸»è§‚æ­§ä¹‰æ€§çš„å¥–åŠ±æ¨¡å‹åŠ¨æ€æ ¡å‡†",
      "authors": [
        "Feiteng Fang",
        "Dingwei Chen",
        "Xiang Huang",
        "Ting-En Lin",
        "Yuchuan Wu",
        "Xiong Liu",
        "Xinge Ye",
        "Ziqiang Liu",
        "Haonan Zhang",
        "Liang Zhu",
        "Hamid Alinejad-Rokny",
        "Min Yang",
        "Yongbin Li"
      ],
      "abstract": "Currently, most reinforcement learning tasks focus on domains like mathematics and programming, where verification is relatively straightforward. However, in subjective tasks such as role-playing, alignment techniques struggle to make progress, primarily because subjective reward modeling using the Bradley-Terry model faces significant challenges when dealing with ambiguous preferences. To improve reward modeling in subjective tasks, this paper proposes AAM (\\textbf{\\underline{A}}ct-\\textbf{\\underline{A}}daptive \\textbf{\\underline{M}}argin), which enhances reward modeling by dynamically calibrating preference margins using the model's internal parameter knowledge. We design two versions of AAM that efficiently generate contextually-appropriate preference gaps without additional human annotation. This approach fundamentally improves how reward models handle subjective rewards by better integrating generative understanding with preference scoring. To validate AAM's effectiveness in subjective reward modeling, we conduct evaluations on RewardBench, JudgeBench, and challenging role-playing tasks. Results show that AAM significantly improves subjective reward modeling performance, enhancing Bradley-Terry reward models by 2.95\\% in general tasks and 4.85\\% in subjective role-playing tasks. Furthermore, reward models trained with AAM can help downstream alignment tasks achieve better results. Our test results show that applying rewards generated by AAM-Augmented RM to preference learning techniques (e.g., GRPO) achieves state-of-the-art results on CharacterEval and Charm. Code and dataset are available at https://github.com/calubkk/AAM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§’è‰²æ‰®æ¼”ç­‰ä¸»è§‚ä»»åŠ¡ä¸­ Bradley-Terry æ¨¡å‹åœ¨å¤„ç†æ¨¡ç³Šåå¥½æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† Act-Adaptive Margin (AAM) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨æ¨¡å‹çš„å†…éƒ¨å‚æ•°çŸ¥è¯†åŠ¨æ€æ ¡å‡†åå¥½é—´éš” (preference margins)ï¼Œæ˜¾è‘—å¢å¼ºäº†ä¸»è§‚ä»»åŠ¡çš„å¥–åŠ±å»ºæ¨¡æ•ˆæœã€‚ç ”ç©¶è®¾è®¡äº†ä¸¤ä¸ªç‰ˆæœ¬çš„ AAMï¼Œèƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹é«˜æ•ˆç”Ÿæˆä¸ä¸Šä¸‹æ–‡åŒ¹é…çš„åå¥½å·®è·ï¼Œå®ç°äº†ç”Ÿæˆå¼ç†è§£ä¸åå¥½è¯„åˆ†çš„æ·±åº¦èåˆã€‚å®éªŒåœ¨ RewardBenchã€JudgeBench åŠå¤æ‚è§’è‰²æ‰®æ¼”ä»»åŠ¡ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤º AAM ä½¿å¥–åŠ±æ¨¡å‹åœ¨é€šç”¨ä»»åŠ¡å’Œä¸»è§‚ä»»åŠ¡ä¸­çš„æ€§èƒ½åˆ†åˆ«æå‡äº† 2.95% å’Œ 4.85%ã€‚æ­¤å¤–ï¼Œå°† AAM ç”Ÿæˆçš„å¥–åŠ±åº”ç”¨äº GRPO ç­‰åå¥½å­¦ä¹ æŠ€æœ¯ï¼Œåœ¨ CharacterEval å’Œ Charm æ¦œå•ä¸Šå‡å–å¾—äº† SOTA æ€§èƒ½ï¼Œä¸ºè§£å†³ä¸»è§‚æ€§ä»»åŠ¡çš„å¯¹é½é—®é¢˜æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23923v2",
      "published_date": "2025-05-29 18:15:18 UTC",
      "updated_date": "2026-01-08 16:58:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:15.512921+00:00"
    },
    {
      "arxiv_id": "2505.23917v2",
      "title": "Representational Difference Explanations",
      "title_zh": "è¡¨å¾å·®å¼‚è§£é‡Š",
      "authors": [
        "Neehar Kondapaneni",
        "Oisin Mac Aodha",
        "Pietro Perona"
      ],
      "abstract": "We propose a method for discovering and visualizing the differences between two learned representations, enabling more direct and interpretable model comparisons. We validate our method, which we call Representational Differences Explanations (RDX), by using it to compare models with known conceptual differences and demonstrate that it recovers meaningful distinctions where existing explainable AI (XAI) techniques fail. Applied to state-of-the-art models on challenging subsets of the ImageNet and iNaturalist datasets, RDX reveals both insightful representational differences and subtle patterns in the data. Although comparison is a cornerstone of scientific analysis, current tools in machine learning, namely post hoc XAI methods, struggle to support model comparison effectively. Our work addresses this gap by introducing an effective and explainable tool for contrasting model representations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Representational Difference Explanations (RDX)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå‘ç°å’Œå¯è§†åŒ–ä¸¤ä¸ªå­¦ä¹ è¡¨å¾(learned representations)ä¹‹é—´å·®å¼‚çš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ›´ç›´æ¥ã€æ›´å…·è§£é‡Šæ€§çš„æ¨¡å‹æ¯”è¾ƒã€‚é’ˆå¯¹ç°æœ‰äº‹åè§£é‡Šæ€§AI (post hoc XAI)æŠ€æœ¯åœ¨æ¨¡å‹å¯¹æ¯”ä¸Šçš„å±€é™æ€§ï¼ŒRDXé€šè¿‡å¯¹æ¯”å…·æœ‰å·²çŸ¥æ¦‚å¿µå·®å¼‚çš„æ¨¡å‹è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿè¯†åˆ«å‡ºä¼ ç»Ÿæ–¹æ³•æ— æ³•æ•è·çš„æœ‰æ„ä¹‰åŒºåˆ«ã€‚åœ¨ImageNetå’ŒiNaturalistæ•°æ®é›†çš„å®éªŒä¸­ï¼ŒRDXæ­ç¤ºäº†å°–ç«¯æ¨¡å‹ä¹‹é—´æ·±åˆ»çš„è¡¨å¾å·®å¼‚ä»¥åŠæ•°æ®ä¸­çš„å¾®å¦™æ¨¡å¼ã€‚è¯¥æ–¹æ³•å¡«è¡¥äº†å½“å‰æœºå™¨å­¦ä¹ å·¥å…·åœ¨æ¨¡å‹å¯¹æ¯”åˆ†ææ–¹é¢çš„ç©ºç™½ï¼Œä¸ºç†è§£å’Œå¯¹æ¯”ä¸åŒæ¨¡å‹è¡¨å¾æä¾›äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„å¯è§£é‡Šæ€§å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 6 figures, 21 supplementary pages, 14 supp figs",
      "pdf_url": "https://arxiv.org/pdf/2505.23917v2",
      "published_date": "2025-05-29 18:09:44 UTC",
      "updated_date": "2025-10-27 05:03:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:20.438591+00:00"
    },
    {
      "arxiv_id": "2505.23914v1",
      "title": "Probing Association Biases in LLM Moderation Over-Sensitivity",
      "title_zh": "æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹å†…å®¹å®¡æ ¸è¿‡åº¦æ•æ„Ÿä¸­çš„å…³è”åè§",
      "authors": [
        "Yuxin Wang",
        "Botao Yu",
        "Ivory Yang",
        "Saeed Hassanpour",
        "Soroush Vosoughi"
      ],
      "abstract": "Large Language Models are widely used for content moderation but often misclassify benign comments as toxic, leading to over-sensitivity. While previous research attributes this issue primarily to the presence of offensive terms, we reveal a potential cause beyond token level: LLMs exhibit systematic topic biases in their implicit associations. Inspired by cognitive psychology's implicit association tests, we introduce Topic Association Analysis, a semantic-level approach to quantify how LLMs associate certain topics with toxicity. By prompting LLMs to generate free-form scenario imagination for misclassified benign comments and analyzing their topic amplification levels, we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger topic stereotype despite lower overall false positive rates. These biases suggest that LLMs do not merely react to explicit, offensive language but rely on learned topic associations, shaping their moderation decisions. Our findings highlight the need for refinement beyond keyword-based filtering, providing insights into the underlying mechanisms driving LLM over-sensitivity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å†…å®¹å®¡æ ¸ä¸­å› è¿‡åº¦æ•æ„Ÿè€Œå°†è‰¯æ€§è¯„è®ºè¯¯åˆ¤ä¸ºæœ‰æ¯’å†…å®¹çš„é—®é¢˜ã€‚ç ”ç©¶æ­ç¤ºäº†é™¤å•è¯çº§åˆ«(token level)å› ç´ å¤–ï¼ŒLLMsåœ¨éšæ€§å…³è”ä¸­å­˜åœ¨ç³»ç»Ÿæ€§çš„ä¸»é¢˜åå·®(topic biases)ã€‚å—è®¤çŸ¥å¿ƒç†å­¦å¯å‘ï¼Œä½œè€…æå‡ºäº†ä¸»é¢˜å…³è”åˆ†æ(Topic Association Analysis)ï¼Œé€šè¿‡è®©æ¨¡å‹å¯¹è¯¯åˆ¤çš„è‰¯æ€§è¯„è®ºè¿›è¡Œè‡ªç”±åœºæ™¯æƒ³è±¡å¹¶åˆ†æå…¶ä¸»é¢˜æ”¾å¤§ç¨‹åº¦ï¼Œé‡åŒ–æ¨¡å‹å°†ç‰¹å®šä¸»é¢˜ä¸æ¯’æ€§å…³è”çš„å€¾å‘ã€‚å®éªŒå‘ç°ï¼Œå³ä¾¿å¦‚GPT-4 Turboç­‰å…ˆè¿›æ¨¡å‹åœ¨æ€»ä½“è¯¯æŠ¥ç‡è¾ƒä½çš„æƒ…å†µä¸‹ï¼Œä»è¡¨ç°å‡ºæ›´å¼ºçš„ä¸»é¢˜åˆ»æ¿å°è±¡ã€‚è¿™ä¸€å‘ç°è¡¨æ˜LLMsçš„å®¡æ ¸å†³ç­–æ·±å—å­¦åˆ°çš„ä¸»é¢˜å…³è”å½±å“ï¼Œè€Œéä»…æ˜¯å¯¹æ˜¾æ€§æ”»å‡»æ€§è¯­è¨€çš„ååº”ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨å…³é”®è¯è¿‡æ»¤ä¹‹å¤–ä¼˜åŒ–æ¨¡å‹å®¡æ ¸æœºåˆ¶çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºç†è§£LLMsè¿‡åº¦æ•æ„Ÿçš„åº•å±‚æœºåˆ¶æä¾›äº†æ·±åˆ»è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2505.23914v1",
      "published_date": "2025-05-29 18:07:48 UTC",
      "updated_date": "2025-05-29 18:07:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:31.805736+00:00"
    },
    {
      "arxiv_id": "2505.23912v1",
      "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation",
      "title_zh": "å¼ºåŒ–å­¦ä¹ ï¼šæå‡é•¿æ–‡æœ¬ç”Ÿæˆçš„è¯­è¨€åŒ–ç½®ä¿¡åº¦",
      "authors": [
        "Caiqi Zhang",
        "Xiaochen Zhu",
        "Chengzu Li",
        "Nigel Collier",
        "Andreas Vlachos"
      ],
      "abstract": "Hallucination remains a major challenge for the safe and trustworthy deployment of large language models (LLMs) in factual content generation. Prior work has explored confidence estimation as an effective approach to hallucination detection, but often relies on post-hoc self-consistency methods that require computationally expensive sampling. Verbalized confidence offers a more efficient alternative, but existing approaches are largely limited to short-form question answering (QA) tasks and do not generalize well to open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized Confidence), an on-the-fly verbalized confidence estimation method for long-form generation. Specifically, we use reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, serving as a direct and interpretable signal of the factuality of generation. Our experiments consider both on-policy and off-policy RL methods, including DPO, ORPO, and GRPO, to enhance the model calibration. We introduce two novel evaluation settings, free-form tagging and iterative tagging, to assess different verbalized confidence estimation methods. Experiments on three long-form QA datasets show that our RL-trained models achieve better calibration and generalize robustly across domains. Also, our method is highly efficient, as it only requires adding a few tokens to the output being decoded.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­å­˜åœ¨çš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº†LoVeC (Long-form Verbalized Confidence)ï¼Œä¸€ç§åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å³æ—¶è¿›è¡Œå£å¤´ç½®ä¿¡åº¦(Verbalized Confidence)ä¼°è®¡çš„æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æŠ€æœ¯ï¼ŒåŒ…æ‹¬DPOã€ORPOå’ŒGRPOï¼Œè¯¥ç ”ç©¶è®­ç»ƒæ¨¡å‹ä¸ºç”Ÿæˆçš„æ¯ä¸ªè¯­å¥é™„å¸¦æ•°å€¼ç½®ä¿¡åº¦åˆ†æ•°ï¼Œä»è€Œä¸ºå†…å®¹çš„äº‹å®æ€§æä¾›ç›´æ¥ä¸”å¯è§£é‡Šçš„ä¿¡å·ã€‚ä¸ºäº†æ·±å…¥è¯„ä¼°ï¼Œç ”ç©¶è€…å¼•å…¥äº†è‡ªç”±å½¢å¼æ ‡æ³¨(free-form tagging)å’Œè¿­ä»£æ ‡æ³¨(iterative tagging)ä¸¤ç§æ–°å‹å®éªŒè®¾ç½®ã€‚åœ¨ä¸‰ä¸ªé•¿æ–‡æœ¬é—®ç­”(QA)æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨¡å‹å®ç°äº†æ›´ä½³çš„æ ¡å‡†(calibration)æ•ˆæœï¼Œå¹¶å±•ç°å‡ºè·¨é¢†åŸŸçš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLoVeCæ–¹æ³•ç”±äºä»…éœ€åœ¨è§£ç æ—¶å¢åŠ å°‘é‡æ ‡è®°(tokens)ï¼Œè¡¨ç°å‡ºäº†æé«˜çš„è¿è¡Œæ•ˆç‡ï¼Œä¸ºæ„å»ºå®‰å…¨å¯ä¿¡çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23912v1",
      "published_date": "2025-05-29 18:05:20 UTC",
      "updated_date": "2025-05-29 18:05:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:36.049580+00:00"
    },
    {
      "arxiv_id": "2505.23765v2",
      "title": "From Chat Logs to Collective Insights: Aggregative Question Answering",
      "title_zh": "ä»å¯¹è¯æ—¥å¿—åˆ°ç¾¤ä½“æ´å¯Ÿï¼šèšåˆå¼é—®ç­”",
      "authors": [
        "Wentao Zhang",
        "Woojeong Kim",
        "Yuntian Deng"
      ],
      "abstract": "Conversational agents powered by large language models (LLMs) are rapidly becoming integral to our daily interactions, generating unprecedented amounts of conversational data. Such datasets offer a powerful lens into societal interests, trending topics, and collective concerns. Yet, existing approaches typically treat these interactions as independent and miss critical insights that could emerge from aggregating and reasoning across large-scale conversation logs. In this paper, we introduce Aggregative Question Answering, a novel task requiring models to reason explicitly over thousands of user-chatbot interactions to answer aggregative queries, such as identifying emerging concerns among specific demographics. To enable research in this direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative questions derived from 182,330 real-world chatbot conversations. Experiments show that existing methods either struggle to reason effectively or incur prohibitive computational costs, underscoring the need for new approaches capable of extracting collective insights from large-scale conversational data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä»æµ·é‡å¯¹è¯æ•°æ®ä¸­æå–é›†ä½“æ´å¯Ÿï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•å¾€å¾€å°† Large Language Models (LLMs) çš„äº¤äº’è§†ä¸ºç‹¬ç«‹ä¸ªä½“ï¼Œä»è€Œå¿½ç•¥äº†è·¨å¤§è§„æ¨¡æ—¥å¿—èšåˆåˆ†æçš„é‡è¦æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åä¸º Aggregative Question Answering (AQA) çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨è¦æ±‚æ¨¡å‹æ˜¾å¼åœ°å¯¹æˆåƒä¸Šä¸‡æ¬¡ç”¨æˆ·ä¸æœºå™¨äººçš„äº¤äº’è¿›è¡Œæ¨ç†ï¼Œä»¥å›ç­”æ¶‰åŠç‰¹å®šç¾¤ä½“æ–°å…´å…³æ³¨ç‚¹ç­‰èšåˆæ€§é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº† WildChat-AQA åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä» 182,330 æ¡çœŸå®å¯¹è¯ä¸­è¡ç”Ÿå‡ºçš„ 6,027 ä¸ªèšåˆé—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ–¹æ³•åœ¨æœ‰æ•ˆæ¨ç†æˆ–è®¡ç®—æˆæœ¬æ–¹é¢å‡é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œéš¾ä»¥é«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®ã€‚è¯¥è®ºæ–‡å¼ºè°ƒäº†å¼€å‘èƒ½å¤Ÿæå–é›†ä½“è§è§£çš„æ–°å‹æ–¹æ³•çš„ç´§è¿«æ€§ï¼Œä¸ºç†è§£ç¤¾ä¼šè¶‹åŠ¿å’Œç¾¤ä½“å…³åˆ‡æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23765v2",
      "published_date": "2025-05-29 17:59:55 UTC",
      "updated_date": "2025-09-20 23:43:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:35.241952+00:00"
    },
    {
      "arxiv_id": "2505.23762v1",
      "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
      "title_zh": "ZeroGUIï¼šé›¶äººåŠ›æˆæœ¬çš„åœ¨çº¿ GUI è‡ªåŠ¨åŒ–å­¦ä¹ ",
      "authors": [
        "Chenyu Yang",
        "Shiqian Su",
        "Shi Liu",
        "Xuan Dong",
        "Yue Yu",
        "Weijie Su",
        "Xuehui Wang",
        "Zhaoyang Liu",
        "Jinguo Zhu",
        "Hao Li",
        "Wenhai Wang",
        "Yu Qiao",
        "Xizhou Zhu",
        "Jifeng Dai"
      ],
      "abstract": "The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ZeroGUIï¼Œä¸€ç§æ—¨åœ¨å®ç°é›¶äººå·¥æˆæœ¬è‡ªåŠ¨åŒ–è®­ç»ƒ GUI Agent çš„å¯æ‰©å±•åœ¨çº¿å­¦ä¹ æ¡†æ¶ã€‚ä¸ºäº†å…‹æœç°æœ‰ç¦»çº¿å­¦ä¹ æ¡†æ¶å¯¹é«˜è´¨é‡äººå·¥æ ‡æ³¨(manual annotations)çš„ä¸¥é‡ä¾èµ–ä»¥åŠåœ¨åŠ¨æ€äº¤äº’ç¯å¢ƒä¸­é€‚åº”æ€§æœ‰é™çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼ŒZeroGUI é›†æˆäº†åŸºäº Vision-Language Models (VLMs) çš„è‡ªåŠ¨ä»»åŠ¡ç”Ÿæˆå’Œè‡ªåŠ¨å¥–åŠ±è¯„ä¼°æœºåˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µåœ¨çº¿å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸æ–­ä¸ GUI ç¯å¢ƒäº¤äº’å¹¶ä»ä¸­å­¦ä¹ ï¼Œæ— éœ€æ‰‹å·¥è®¾è®¡çš„è¯„ä¼°å‡½æ•°å³å¯å®ç°è‡ªæˆ‘æ¼”è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZeroGUI åœ¨ OSWorld å’Œ AndroidLab ç¯å¢ƒä¸­æ˜¾è‘—æå‡äº† UI-TARS å’Œ Aguvis ç­‰å…ˆè¿› GUI Agent çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥ç ”ç©¶é€šè¿‡å…¨è‡ªåŠ¨åŒ–çš„è®­ç»ƒé—­ç¯ï¼Œä¸ºæ„å»ºæ›´å…·è‡ªä¸»æ€§å’Œé€‚åº”æ€§çš„çº¯è§†è§‰æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23762v1",
      "published_date": "2025-05-29 17:59:51 UTC",
      "updated_date": "2025-05-29 17:59:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:31.302844+00:00"
    },
    {
      "arxiv_id": "2505.23761v2",
      "title": "Differential Information Distribution: A Bayesian Perspective on Direct Preference Optimization",
      "title_zh": "å·®å¼‚ä¿¡æ¯åˆ†å¸ƒï¼šç›´æ¥åå¥½ä¼˜åŒ–çš„è´å¶æ–¯è§†è§’",
      "authors": [
        "Yunjae Won",
        "Hyunji Lee",
        "Hyeonbin Hwang",
        "Minjoon Seo"
      ],
      "abstract": "Direct Preference Optimization (DPO) has been widely used for aligning language models with human preferences in a supervised manner. However, several key questions remain unresolved: the rationale behind its log-ratio reward, how the statistical structure of preference datasets shapes its training dynamics, and how those dynamics impact downstream capabilities. We approach these questions from a Bayesian perspective, interpreting the goal of preference optimization as learning the differential information required to update a reference policy into a target policy. To formalize this view, we introduce the Differential Information Distribution (DID), defined as the distribution over samples that carry the Bayesian evidence required to update policies. We introduce three complementary insights by viewing preference optimization through the DID. First, we find that DPO's log-ratio reward is uniquely justified when preferences encode the Differential Information needed to update a reference policy into the target policy. Second, we discuss how commonly observed training dynamics in DPO, including changes in log-likelihood and policy exploration, stem from a power-law DID relationship. Finally, we analyze how training dynamics influence downstream performance using the entropy of DID, a principled measure of uncertainty in the learned information. We observe that learning high-entropy DID improves open-ended instruction-following, while low-entropy DID benefits knowledge-intensive QA. Taken together, our results show that DPO's reward design, training dynamics, and downstream capabilities all emerge as natural consequences of learning Differential Information, offering both a principled theoretical foundation and practical guidance for preference-based alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»è´å¶æ–¯(Bayesian)è§†è§’é‡æ–°å®¡è§†äº†ç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization, DPO)ï¼Œæ—¨åœ¨è§£æå…¶å¥–åŠ±æœºåˆ¶ã€è®­ç»ƒåŠ¨æ€åŠä¸‹æ¸¸æ€§èƒ½çš„å†…åœ¨é€»è¾‘ã€‚ä½œè€…æå‡ºäº†å·®å¼‚ä¿¡æ¯åˆ†å¸ƒ(Differential Information Distribution, DID)çš„æ¦‚å¿µï¼Œå°†å…¶å®šä¹‰ä¸ºæ‰¿è½½å°†å‚è€ƒç­–ç•¥æ›´æ–°ä¸ºç›®æ ‡ç­–ç•¥æ‰€éœ€è´å¶æ–¯è¯æ®çš„æ ·æœ¬åˆ†å¸ƒã€‚ç ”ç©¶è¯æ˜ï¼Œå½“åå¥½æ•°æ®ç¼–ç äº†è¿™ç§å·®å¼‚ä¿¡æ¯(Differential Information)æ—¶ï¼ŒDPOçš„å¯¹æ•°æ¯”ä¾‹(log-ratio)å¥–åŠ±è®¾è®¡ä¾¿å…·æœ‰äº†ç†è®ºä¸Šçš„åˆç†æ€§ã€‚åŒæ—¶ï¼ŒDPOä¸­å¸¸è§çš„è®­ç»ƒåŠ¨æ€ï¼Œå¦‚å¯¹æ•°ä¼¼ç„¶(log-likelihood)çš„å˜åŒ–å’Œç­–ç•¥æ¢ç´¢ï¼Œè¢«å‘ç°æºäºDIDçš„å¹‚å¾‹å…³ç³»ã€‚é€šè¿‡å¯¹DIDç†µ(entropy)çš„åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†å­¦ä¹ é«˜ç†µDIDèƒ½æ˜¾è‘—æå‡å¼€æ”¾å¼æŒ‡ä»¤éµå¾ªæ€§èƒ½ï¼Œè€Œä½ç†µDIDåˆ™æ›´æœ‰åˆ©äºå¤„ç†çŸ¥è¯†å¯†é›†å‹é—®ç­”(QA)ã€‚è¯¥æˆæœä¸ºåå¥½å¯¹é½æŠ€æœ¯æä¾›äº†ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼Œå¹¶ä¸ºä¼˜åŒ–å¤§æ¨¡å‹çš„ä¸‹æ¸¸èƒ½åŠ›æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint, under review. 39 pages, 12 figures. Updates from v1: Added new theoretical results on DPO training dynamics and policy exploration, included experiments with Qwen3-4B, and refined the discussion of log-margin dynamics",
      "pdf_url": "https://arxiv.org/pdf/2505.23761v2",
      "published_date": "2025-05-29 17:59:50 UTC",
      "updated_date": "2025-10-02 17:59:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:51.927951+00:00"
    },
    {
      "arxiv_id": "2505.23759v2",
      "title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint",
      "title_zh": "å›°äºè°œé¢˜ï¼šå½“è§†è§‰è¯­è¨€æ¨¡å‹æ— æ³•é¢†ä¼šæš—ç¤ºæ—¶",
      "authors": [
        "Heekyung Lee",
        "Jiaxin Ge",
        "Tsung-Han Wu",
        "Minwoo Kang",
        "Trevor Darrell",
        "David M. Chan"
      ],
      "abstract": "Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨å¤„ç†çŒœè°œç”»(Rebus puzzles)è¿™ä¸€ç‹¬ç‰¹æŒ‘æˆ˜æ—¶çš„è¡¨ç°ï¼Œæ­¤ç±»è°œé¢˜é€šè¿‡å›¾åƒã€ç©ºé—´æ’åˆ—å’Œç¬¦å·æ›¿æ¢æ¥ç¼–ç è¯­è¨€ä¿¡æ¯ã€‚è§£å†³çŒœè°œç”»ä¸ä»…è¦æ±‚æ¨¡å‹å…·å¤‡å¤šæ¨¡æ€æŠ½è±¡(multi-modal abstraction)å’Œç¬¦å·æ¨ç†(symbolic reasoning)èƒ½åŠ›ï¼Œè¿˜éœ€è¦ç†è§£æ–‡åŒ–ã€è¯­éŸ³åŠè¯­è¨€å±‚é¢çš„åŒå…³è¯­(puns)ã€‚ä½œè€…ä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªäººå·¥ç”Ÿæˆä¸”ç»è¿‡æ ‡æ³¨çš„è‹±æ–‡çŒœè°œç”»åŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›–äº†ä»ç®€å•è±¡å½¢æ›¿æ¢åˆ°å¤æ‚ç©ºé—´ä¾èµ–çº¿ç´¢çš„å¤šæ ·åŒ–ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å½“å‰çš„VLMsåœ¨è§£ç åŸºç¡€è§†è§‰çº¿ç´¢æ–¹é¢å±•ç°å‡ºä¸€å®šæ½œåŠ›ï¼Œä½†åœ¨é¢å¯¹éœ€è¦æŠ½è±¡æ¨ç†(abstract reasoning)ã€å‘æ•£æ€§æ€ç»´(lateral thinking)ä»¥åŠç†è§£è§†è§‰éšå–»(visual metaphors)çš„ä»»åŠ¡æ—¶ä»é¢ä¸´æ˜¾è‘—å›°éš¾ã€‚è¯¥é¡¹ç ”ç©¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨é«˜çº§è®¤çŸ¥ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥æå‡å¤šæ¨¡æ€æ¨¡å‹çš„é€»è¾‘ä¸åˆ›é€ æ€§æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2505.23759v2",
      "published_date": "2025-05-29 17:59:47 UTC",
      "updated_date": "2025-09-16 18:13:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:48.656389+00:00"
    },
    {
      "arxiv_id": "2505.23754v2",
      "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning",
      "title_zh": "DeepTheoremï¼šé€šè¿‡è‡ªç„¶è¯­è¨€ä¸å¼ºåŒ–å­¦ä¹ æå‡å¤§è¯­è¨€æ¨¡å‹çš„å®šç†è¯æ˜æ¨ç†èƒ½åŠ›",
      "authors": [
        "Ziyin Zhang",
        "Jiahao Xu",
        "Zhiwei He",
        "Tian Liang",
        "Qiuzhi Liu",
        "Yansi Li",
        "Linfeng Song",
        "Zhenwen Liang",
        "Zhuosheng Zhang",
        "Rui Wang",
        "Zhaopeng Tu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeepTheoremï¼Œä¸€ä¸ªåˆ©ç”¨è‡ªç„¶è¯­è¨€å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) æ•°å­¦æ¨ç†èƒ½åŠ›çš„éæ­£å¼å®šç†è¯æ˜æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿè‡ªåŠ¨å®šç†è¯æ˜ (ATP) è¿‡äºä¾èµ–å½¢å¼åŒ–ç³»ç»Ÿï¼Œè€Œéš¾ä»¥ä¸å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­è·å¾—çš„è‡ªç„¶è¯­è¨€çŸ¥è¯†æœ‰æ•ˆå¯¹é½çš„é—®é¢˜ã€‚DeepTheorem åŒ…å«ä¸€ä¸ªç”± 121K ä¸ªå›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ (IMO) çº§åˆ«çš„é«˜è´¨é‡éæ­£å¼å®šç†å’Œè¯æ˜ç»„æˆçš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼Œå¹¶ç³»ç»Ÿæ„å»ºäº†å¯éªŒè¯çš„å®šç†å˜ä½“ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜è®¾è®¡äº†åä¸º RL-Zero çš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œä¸“é—¨é’ˆå¯¹éæ­£å¼å®šç†è¯æ˜è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡å¯éªŒè¯çš„å˜ä½“æ¥æ¿€åŠ±æ¨¡å‹è¿›è¡Œæ›´é²æ£’çš„æ•°å­¦æ¨ç†ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ç»¼åˆçš„ç»“æœå’Œè¿‡ç¨‹è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å…¨é¢æ£€æŸ¥è¯æ˜çš„æ­£ç¡®æ€§å’Œæ¨ç†æ­¥éª¤çš„è´¨é‡ã€‚å®éªŒåˆ†æè¯æ˜ï¼ŒDeepTheorem åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ•°æ®é›†å’Œç›‘ç£å¾®è°ƒ (SFT) æ–¹æ¡ˆï¼Œè¾¾åˆ°äº†å½“å‰çš„ state-of-the-art å‡†ç¡®ç‡ï¼Œä¸ºè‡ªåŠ¨éæ­£å¼å®šç†è¯æ˜å’Œæ•°å­¦æ¢ç´¢æä¾›äº†æ–°çš„åŠ¨åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23754v2",
      "published_date": "2025-05-29 17:59:39 UTC",
      "updated_date": "2025-06-03 11:23:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:47.108978+00:00"
    },
    {
      "arxiv_id": "2505.23751v3",
      "title": "REOrdering Patches Improves Vision Models",
      "title_zh": "é‡æ’åºå›¾åƒå—æå‡è§†è§‰æ¨¡å‹",
      "authors": [
        "Declan Kutscher",
        "David M. Chan",
        "Yutong Bai",
        "Trevor Darrell",
        "Ritwik Gupta"
      ],
      "abstract": "Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºè§†è§‰æ¨¡å‹åœ¨å°†å›¾åƒå±•å¹³ä¸ºä¸€ç»´åºåˆ—æ—¶ï¼Œpatch çš„æ’åºæ–¹å¼ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ‰“ç ´äº†æ’åˆ—ç­‰å˜æ€§(permutation-equivariant)çš„ç°ä»£é•¿åºåˆ— Transformer æ¶æ„ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº† REOrder æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå‘ç°ä»»åŠ¡æœ€ä¼˜ patch æ’åºçš„ä¸¤é˜¶æ®µæ–¹æ³•ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡è¯„ä¼°åºåˆ—çš„å¯å‹ç¼©æ€§æ„å»ºä¿¡æ¯è®ºå…ˆéªŒï¼Œéšåé‡‡ç”¨ REINFORCE ç®—æ³•ä¼˜åŒ– Plackett-Luce ç­–ç•¥ï¼Œä»è€Œåœ¨å¤æ‚çš„æ’åˆ—ç©ºé—´ä¸­å®ç°é«˜æ•ˆå­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„è¡Œä¼˜å…ˆ(row-major)æ’åºï¼ŒREOrder åœ¨ ImageNet-1K ä¸Šçš„ Top-1 å‡†ç¡®ç‡æå‡äº† 3.01%ï¼Œåœ¨ Functional Map of the World ä¸Šçš„å‡†ç¡®ç‡æ›´æ˜¯æå‡äº† 13.35%ã€‚è¯¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ä¼˜åŒ– patch çš„è¾“å…¥åºåˆ—ï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºè§†è§‰æ¨¡å‹çš„ä»»åŠ¡é€‚åº”æ€§å’Œé¢„æµ‹ç²¾åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.23751v3",
      "published_date": "2025-05-29 17:59:30 UTC",
      "updated_date": "2025-10-23 04:13:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:49:59.759900+00:00"
    },
    {
      "arxiv_id": "2505.23746v1",
      "title": "Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons",
      "title_zh": "æ°”åŠ¨å£°å­¦ç°è±¡çš„é—ä¼ æ¨¡ç³Šå›å½’æŠ€æœ¯å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Hugo Henry",
        "Kelly Cohen"
      ],
      "abstract": "This study investigates the application of Genetic Fuzzy Systems (GFS) to model the self-noise generated by airfoils, a key issue in aeroaccoustics with significant implications for aerospace, automotive and drone applications. Using the publicly available Airfoil Self Noise dataset, various Fuzzy regression strategies are explored and compared. The paper evaluates a brute force Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading Geneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on Fuzzy C-means (FCM) to reduce the model's complexity. This highlights the viability of clustering assisted fuzzy inference as an effective regression tool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression, Cascading systems, Clustering and AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é—ä¼ æ¨¡ç³Šç³»ç»Ÿ(Genetic Fuzzy Systems, GFS)åœ¨å»ºæ¨¡ç¿¼å‹è‡ªå™ªå£°(Airfoil self-noise)æ–¹é¢çš„åº”ç”¨ï¼Œè¿™ä¸€è¯¾é¢˜åœ¨èˆªç©ºèˆªå¤©ã€æ±½è½¦å’Œæ— äººæœºé¢†åŸŸçš„èˆªç©ºå£°å­¦ç ”ç©¶ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨å…¬å¼€çš„Airfoil Self Noiseæ•°æ®é›†ï¼Œç³»ç»Ÿåœ°å¯¹æ¯”äº†å¤šç§æ¨¡ç³Šå›å½’ç­–ç•¥ï¼ŒåŒ…æ‹¬é«˜è§„åˆ™å¯†åº¦çš„Takagi Sugeno Kang (TSK)æ¨¡ç³Šç³»ç»Ÿä»¥åŠçº§è”çš„é—ä¼ æ¨¡ç³Šæ ‘(Genetic Fuzzy Tree, GFT)æ¶æ„ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡ç³ŠCå‡å€¼(Fuzzy C-means, FCM)èšç±»çš„æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨æœ‰æ•ˆé™ä½æ¨¡å‹çš„è®¡ç®—å¤æ‚æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œèšç±»è¾…åŠ©çš„æ¨¡ç³Šæ¨ç†åœ¨å¤„ç†å¤æ‚çš„èˆªç©ºå£°å­¦ç°è±¡æ—¶æ˜¯ä¸€ç§æä¸ºæœ‰æ•ˆçš„å›å½’å·¥å…·ã€‚è¯¥ç ”ç©¶ä¸ä»…éªŒè¯äº†é—ä¼ æ¨¡ç³Šé€»è¾‘åœ¨å™ªå£°å»ºæ¨¡ä¸­çš„å¯è¡Œæ€§ï¼Œè¿˜ä¸ºä¼˜åŒ–å¤æ‚AIæ¨¡å‹çš„æ¨ç†ç»“æ„æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages and 23 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23746v1",
      "published_date": "2025-05-29 17:59:04 UTC",
      "updated_date": "2025-05-29 17:59:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:19.563624+00:00"
    },
    {
      "arxiv_id": "2505.23747v1",
      "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence",
      "title_zh": "Spatial-MLLMï¼šæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç©ºé—´æ™ºèƒ½ä¸­çš„èƒ½åŠ›",
      "authors": [
        "Diankun Wu",
        "Fangfu Liu",
        "Yi-Hsin Hung",
        "Yueqi Duan"
      ],
      "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Spatial-MLLMï¼Œä¸€ä¸ªæ—¨åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä»…åŸºäºäºŒç»´ï¼ˆ2Dï¼‰è§‚å¯Ÿæ—¶çš„ç©ºé—´æ™ºèƒ½æ¨ç†æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹è¿‡åº¦ä¾èµ–ä¸‰ç»´ï¼ˆ3Dï¼‰æˆ–2.5Dæ•°æ®ï¼Œä¸”å—é™äºCLIPç­‰è§†è§‰ç¼–ç å™¨ä¾§é‡è¯­ä¹‰è€Œéç»“æ„çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡æŒ–æ˜è§†è§‰å‡ ä½•åŸºç¡€æ¨¡å‹çš„å¼ºç»“æ„å…ˆéªŒæ¥å¢å¼ºç©ºé—´ç†è§£èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†åŒç¼–ç å™¨æ¶æ„ï¼Œç»“åˆé¢„è®­ç»ƒçš„2Dè§†è§‰ç¼–ç å™¨æå–è¯­ä¹‰ç‰¹å¾ï¼Œä»¥åŠåŸºäºè§†è§‰å‡ ä½•æ¨¡å‹çš„ç©ºé—´ç¼–ç å™¨æå–3Dç»“æ„ç‰¹å¾ï¼Œå¹¶é€šè¿‡è¿æ¥å™¨å°†å…¶æ•´åˆä¸ºç»Ÿä¸€çš„è§†è§‰æ ‡è®°ï¼ˆvisual tokensï¼‰ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ç©ºé—´æ„ŸçŸ¥å¸§é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨å—é™çš„æ ‡è®°é•¿åº¦ä¸‹ä¾ç„¶èƒ½èšç„¦äºè§†é¢‘åºåˆ—ä¸­å¯¹ç©ºé—´æ¨ç†è‡³å…³é‡è¦çš„å¸§ã€‚ä½œè€…è¿˜æ„å»ºäº†åŒ…å«12ä¸‡æ¡æ•°æ®çš„Spatial-MLLM-120kæ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒGRPOæŠ€æœ¯å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¯æ˜ï¼ŒSpatial-MLLMåœ¨å¤šç§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.23747v1",
      "published_date": "2025-05-29 17:59:04 UTC",
      "updated_date": "2025-05-29 17:59:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:18.666104+00:00"
    },
    {
      "arxiv_id": "2505.23745v2",
      "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹é¢„æµ‹ï¼šä¿¡ä»»è¿˜æ˜¯ä¸ä¿¡ä»»ï¼Ÿ",
      "authors": [
        "Hao Dong",
        "Moru Liu",
        "Jian Liang",
        "Eleni Chatzi",
        "Olga Fink"
      ],
      "abstract": "Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code is available at https://github.com/EPFL-IMOS/TrustVLM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)åœ¨åˆ†ç±»ä»»åŠ¡ä¸­å› è¿‡åº¦è‡ªä¿¡è€Œäº§ç”Ÿé”™è¯¯é¢„æµ‹çš„é£é™©ï¼Œæå‡ºäº†TrustVLMï¼Œä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒ(training-free)çš„é¢„æµ‹å¯ä¿¡åº¦è¯„ä¼°æ¡†æ¶ã€‚TrustVLMå—åˆ°æ¨¡æ€é—´éš™(modality gap)ç°è±¡çš„å¯å‘ï¼Œé€šè¿‡åˆ©ç”¨å›¾åƒåµŒå…¥ç©ºé—´(image embedding space)ä¸­æ›´æ¸…æ™°çš„ç‰¹å¾è¡¨ç¤ºï¼Œæ„å»ºäº†å…¨æ–°çš„ç½®ä¿¡åº¦è¯„åˆ†å‡½æ•°(confidence-scoring function)ä»¥ä¼˜åŒ–è¯¯åˆ†ç±»æ£€æµ‹ã€‚å®éªŒè·¨è¶Š17ä¸ªæ•°æ®é›†å’Œå¤šç§æ¨¡å‹æ¶æ„è¿›è¡Œäº†ä¸¥è°¨è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨AURCã€AUROCå’ŒFPR95ç­‰å…³é”®æŒ‡æ ‡ä¸Šåˆ†åˆ«å®ç°äº†æœ€é«˜51.87%ã€9.14%å’Œ32.42%çš„æ€§èƒ½æå‡ã€‚é€šè¿‡åœ¨ä¸æ”¹å˜æ¨¡å‹æƒé‡çš„å‰æä¸‹æ˜¾è‘—å¢å¼ºé¢„æµ‹çš„å¯é æ€§ï¼ŒTrustVLMä¸ºVLMsåœ¨å®‰å…¨æ•æ„Ÿé¢†åŸŸçš„å®é™…è½åœ°æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23745v2",
      "published_date": "2025-05-29 17:59:01 UTC",
      "updated_date": "2025-09-24 11:40:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:21.449417+00:00"
    },
    {
      "arxiv_id": "2505.23744v1",
      "title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need",
      "title_zh": "æå‡é¢†åŸŸå¢é‡å­¦ä¹ ï¼šåªéœ€é€‰æ‹©æœ€ä¼˜å‚æ•°",
      "authors": [
        "Qiang Wang",
        "Xiang Song",
        "Yuhang He",
        "Jizhou Han",
        "Chenhao Ding",
        "Xinyuan Gao",
        "Yihong Gong"
      ],
      "abstract": "Deep neural networks (DNNs) often underperform in real-world, dynamic settings where data distributions change over time. Domain Incremental Learning (DIL) offers a solution by enabling continual model adaptation, with Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce knowledge conflicts. However, existing PIDIL methods struggle with parameter selection accuracy, especially as the number of domains and corresponding classes grows. To address this, we propose SOYO, a lightweight framework that improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior domain data efficiently, while a Multi-level Domain Feature Fusion Network (MDFN) enhances domain feature extraction. Our framework supports multiple Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks such as image classification, object detection, and speech enhancement. Experimental results on six benchmarks demonstrate SOYO's consistent superiority over existing baselines, showcasing its robustness and adaptability in complex, evolving environments. The codes will be released in https://github.com/qwangcv/SOYO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŠ¨æ€ç¯å¢ƒä¸‹å¤„ç†æ•°æ®åˆ†å¸ƒå˜åŒ–æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œèšç„¦äºåŸŸå¢é‡å­¦ä¹ (Domain Incremental Learning)ä¸­çš„å‚æ•°éš”ç¦»(Parameter-Isolation DIL)èŒƒå¼ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨é¢†åŸŸå’Œç±»åˆ«å¢å¤šæ—¶å‚æ•°é€‰æ‹©å‡†ç¡®åº¦ä¸‹é™çš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†åä¸ºSOYOçš„è½»é‡çº§æ¡†æ¶ä»¥ä¼˜åŒ–å‚æ•°é€‰æ‹©è¿‡ç¨‹ã€‚SOYOå¼•å…¥äº†é«˜æ–¯æ··åˆå‹ç¼©å™¨(Gaussian Mixture Compressor)å’Œé¢†åŸŸç‰¹å¾é‡é‡‡æ ·å™¨(Domain Feature Resampler)æ¥é«˜æ•ˆå­˜å‚¨å¹¶å¹³è¡¡å…ˆéªŒé¢†åŸŸæ•°æ®ï¼ŒåŒæ—¶åˆ©ç”¨å¤šçº§é¢†åŸŸç‰¹å¾èåˆç½‘ç»œ(Multi-level Domain Feature Fusion Network)å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§å‚æ•°é«˜æ•ˆå¾®è°ƒ(Parameter-Efficient Fine-Tuning)æŠ€æœ¯ï¼Œå¹¶åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­éŸ³å¢å¼ºç­‰å¤šé¡¹ä»»åŠ¡ä¸­é€šè¿‡äº†éªŒè¯ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSOYOåœ¨å…­ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¸€è‡´ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œå……åˆ†å±•ç¤ºäº†å…¶åœ¨å¤æ‚æ¼”å˜ç¯å¢ƒä¸‹çš„é²æ£’æ€§ä¸é€‚åº”æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23744v1",
      "published_date": "2025-05-29 17:58:57 UTC",
      "updated_date": "2025-05-29 17:58:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:23.814320+00:00"
    },
    {
      "arxiv_id": "2505.23742v2",
      "title": "MAGREF: Masked Guidance for Any-Reference Video Generation with Subject Disentanglement",
      "title_zh": "MAGREFï¼šåŸºäºæ©ç å¼•å¯¼ä¸ä¸»ä½“è§£è€¦çš„ä»»æ„å‚è€ƒè§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Yufan Deng",
        "Yuanyang Yin",
        "Xun Guo",
        "Yizhi Wang",
        "Jacob Zhiyuan Fang",
        "Shenghai Yuan",
        "Yiding Yang",
        "Angtian Wang",
        "Bo Liu",
        "Haibin Huang",
        "Chongyang Ma"
      ],
      "abstract": "We tackle the task of any-reference video generation, which aims to synthesize videos conditioned on arbitrary types and combinations of reference subjects, together with textual prompts. This task faces persistent challenges, including identity inconsistency, entanglement among multiple reference subjects, and copy-paste artifacts. To address these issues, we introduce MAGREF, a unified and effective framework for any-reference video generation. Our approach incorporates masked guidance and a subject disentanglement mechanism, enabling flexible synthesis conditioned on diverse reference images and textual prompts. Specifically, masked guidance employs a region-aware masking mechanism combined with pixel-wise channel concatenation to preserve appearance features of multiple subjects along the channel dimension. This design preserves identity consistency and maintains the capabilities of the pre-trained backbone, without requiring any architectural changes. To mitigate subject confusion, we introduce a subject disentanglement mechanism which injects the semantic values of each subject derived from the text condition into its corresponding visual region. Additionally, we establish a four-stage data pipeline to construct diverse training pairs, effectively alleviating copy-paste artifacts. Extensive experiments on a comprehensive benchmark demonstrate that MAGREF consistently outperforms existing state-of-the-art approaches, paving the way for scalable, controllable, and high-fidelity any-reference video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MAGREFï¼Œä¸€ä¸ªç»Ÿä¸€ä¸”æœ‰æ•ˆçš„ Any-Reference Video Generation æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„èº«ä»½ä¸ä¸€è‡´ (Identity Inconsistency)ã€å¤šå‚è€ƒä¸»ä½“çº ç¼ ä»¥åŠå¤åˆ¶ç²˜è´´ä¼ªå½± (Copy-Paste Artifacts) ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº† Masked Guidance æœºåˆ¶ï¼Œåˆ©ç”¨åŒºåŸŸæ„ŸçŸ¥æ©ç ç»“åˆåƒç´ çº§é€šé“æ‹¼æ¥ï¼Œåœ¨ä¸æ”¹å˜é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œæ¶æ„çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆä¿ç•™å¤šä¸ªä¸»ä½“çš„å¤–è§‚ç‰¹å¾å¹¶ç»´æŒèº«ä»½ä¸€è‡´æ€§ã€‚ä¸ºäº†ç¼“è§£ä¸»ä½“æ··æ·†ï¼Œç ”ç©¶è¿›ä¸€æ­¥è®¾è®¡äº†ä¸»ä½“è§£è€¦ (Subject Disentanglement) æœºåˆ¶ï¼Œé€šè¿‡å°†æ–‡æœ¬æ¡ä»¶ä¸­çš„è¯­ä¹‰å€¼æ³¨å…¥å¯¹åº”çš„è§†è§‰åŒºåŸŸï¼Œç¡®ä¿ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è¯­ä¹‰å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªå››é˜¶æ®µæ•°æ®æµæ°´çº¿ä»¥æ„å»ºå¤šæ ·åŒ–çš„è®­ç»ƒå¯¹ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†åˆæˆè§†é¢‘ä¸­çš„ä¼ªå½±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGREF åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ (State-of-the-Art)ï¼Œä¸ºå®ç°å¯æ‰©å±•ã€å—æ§ä¸”é«˜ä¿çœŸåº¦çš„è§†é¢‘åˆæˆå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code: https://github.com/MAGREF-Video/MAGREF/; Project website: https://magref-video.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2505.23742v2",
      "published_date": "2025-05-29 17:58:15 UTC",
      "updated_date": "2025-10-09 08:41:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:54.068651+00:00"
    },
    {
      "arxiv_id": "2505.23735v1",
      "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
      "title_zh": "ATLASï¼šå­¦ä¹ åœ¨æµ‹è¯•é˜¶æ®µå®ç°ä¸Šä¸‹æ–‡çš„æœ€ä¼˜è®°å¿†",
      "authors": [
        "Ali Behrouz",
        "Zeman Li",
        "Praneeth Kacham",
        "Majid Daliri",
        "Yuan Deng",
        "Peilin Zhong",
        "Meisam Razaviyayn",
        "Vahab Mirrokni"
      ],
      "abstract": "Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\\% accuracy in 10M context length of BABILong benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ATLASï¼Œä¸€ç§å…·å¤‡é«˜å®¹é‡çš„é•¿ç¨‹å†…å­˜(long-term memory)æ¨¡å—ï¼Œæ—¨åœ¨è§£å†³ Transformer åœ¨é•¿åºåˆ—å¤„ç†ä¸Šçš„å¤æ‚åº¦é—®é¢˜ä»¥åŠé€’å½’æ¨¡å‹åœ¨è®°å¿†å®¹é‡å’Œæ›´æ–°æœºåˆ¶ä¸Šçš„ç¼ºé™·ã€‚ATLAS é€šè¿‡åŒæ—¶åŸºäºå½“å‰å’Œè¿‡å»çš„ Token ä¼˜åŒ–å†…å­˜ï¼Œå…‹æœäº†ä¼ ç»Ÿé•¿ç¨‹å†…å­˜æ¨¡å‹ä»…ä¾èµ–æœ€åè¾“å…¥è¿›è¡Œåœ¨çº¿æ›´æ–°(online update)çš„å±€é™æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…è¿›ä¸€æ­¥æ¨å‡ºäº† DeepTransformers æ¶æ„ï¼Œå®ç°äº†å¯¹åŸå§‹ Transformer çš„ä¸¥æ ¼æ³›åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒATLAS åœ¨è¯­è¨€å»ºæ¨¡(language modeling)ã€å¸¸è¯†æ¨ç†å’Œå¬å›å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äº Transformer åŠæœ€æ–°çš„çº¿æ€§é€’å½’æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢ï¼ŒATLAS æ˜¾è‘—å¢å¼ºäº† Titans çš„èƒ½åŠ›ï¼Œåœ¨ 10M é•¿åº¦çš„ BABILong åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 80% ä»¥ä¸Šçš„å‡†ç¡®ç‡ï¼Œå±•ç°äº†å“è¶Šçš„ä¸Šä¸‹æ–‡å¤–æ¨ä¸è®°å¿†æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23735v1",
      "published_date": "2025-05-29 17:57:16 UTC",
      "updated_date": "2025-05-29 17:57:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:39.420749+00:00"
    },
    {
      "arxiv_id": "2505.23733v2",
      "title": "Unintentional Consequences: Generative AI Use for Cybercrime",
      "title_zh": "éé¢„æœŸåæœï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨ç½‘ç»œçŠ¯ç½ªä¸­çš„åº”ç”¨",
      "authors": [
        "Truong Jack Luu",
        "Binny M. Samuel"
      ],
      "abstract": "The democratization of generative AI introduces new forms of human-AI interaction and raises urgent safety, ethical, and cybersecurity concerns. We develop a socio-technical explanation for how generative AI enables and scales cybercrime. Drawing on affordance theory and technological amplification, we argue that generative AI systems create new action possibilities for cybercriminals and magnify pre-existing malicious intent by lowering expertise barriers and increasing attack efficiency. To illustrate this framework, we conduct interrupted time series analyses of two large datasets: (1) 464,190,074 malicious IP address reports from AbuseIPDB, and (2) 281,115 cryptocurrency scam reports from Chainabuse. Using November 30, 2022, as a high-salience public-access shock, we estimate the counterfactual trajectory of reported cyber abuse absent the release, providing an early-warning impact assessment of a general-purpose AI technology. Across both datasets, we observe statistically significant post-intervention increases in reported malicious activity, including an immediate increase of over 1.12 million weekly malicious IP reports and about 722 weekly cryptocurrency scam reports, with sustained growth in the latter. We discuss implications for AI governance, platform-level regulation, and cyber resilience, emphasizing the need for multi-layer socio-technical strategies that help key stakeholders maximize AI's benefits while mitigating its growing cybercrime risks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) çš„æ™®åŠå¦‚ä½•å¼•å‘æ–°çš„ç½‘ç»œçŠ¯ç½ªé£é™©ï¼Œå¹¶ä»ç¤¾ä¼šæŠ€æœ¯è§’åº¦åˆ†æäº†å…¶å¯¹ç½‘ç»œå®‰å…¨çš„æ·±è¿œå½±å“ã€‚åŸºäºå¯æ‹…çº²æ€§ç†è®º (Affordance Theory) å’ŒæŠ€æœ¯æ”¾å¤§ (Technological Amplification) æ•ˆåº”ï¼Œæ–‡ç« è®ºè¯äº† Generative AI ç³»ç»Ÿå¦‚ä½•é€šè¿‡é™ä½æŠ€æœ¯é—¨æ§›å’Œæé«˜æ”»å‡»æ•ˆç‡ï¼Œä¸ºçŠ¯ç½ªåˆ†å­åˆ›é€ æ–°çš„è¡ŒåŠ¨å¯èƒ½æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ä¸­æ–­æ—¶é—´åºåˆ—åˆ†æ (Interrupted Time Series Analyses) æ–¹æ³•ï¼Œå¯¹ AbuseIPDB çš„æ¶æ„ IP æŠ¥å‘Šå’Œ Chainabuse çš„åŠ å¯†è´§å¸è¯ˆéª—æŠ¥å‘Šè¿™ä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œäº†æ·±åº¦è€ƒå¯Ÿã€‚é€šè¿‡ä»¥ 2022 å¹´ 11 æœˆ 30 æ—¥ä½œä¸ºå…³é”®çš„æ—¶é—´å†²å‡»èŠ‚ç‚¹ï¼Œç ”ç©¶å‘ç°å¹²é¢„åçš„æ¶æ„æ´»åŠ¨æŠ¥å‘Šæ•°é‡åœ¨ç»Ÿè®¡å­¦ä¸Šæ˜¾è‘—å¢åŠ ï¼Œå…¶ä¸­æ¯å‘¨æ¶æ„ IP æŠ¥å‘Šæ¿€å¢è¶…è¿‡ 112 ä¸‡æ¬¡ï¼ŒåŠ å¯†è´§å¸è¯ˆéª—æŠ¥å‘Šä¹Ÿå‘ˆç°æŒç»­å¢é•¿æ€åŠ¿ã€‚æœ€åï¼Œæ–‡ç« è®¨è®ºäº†å¯¹ AI æ²»ç† (AI Governance)ã€å¹³å°ç›‘ç®¡å’Œç½‘ç»œéŸ§æ€§ (Cyber Resilience) çš„é‡è¦å¯ç¤ºï¼Œå¼ºè°ƒäº†æ„å»ºå¤šå±‚æ¬¡ç¤¾ä¼šæŠ€æœ¯ç­–ç•¥ä»¥åº”å¯¹æ—¥ç›Šä¸¥å³»çš„ç½‘ç»œçŠ¯ç½ªé£é™©çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23733v2",
      "published_date": "2025-05-29 17:57:01 UTC",
      "updated_date": "2025-12-03 17:00:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:34.350543+00:00"
    },
    {
      "arxiv_id": "2505.23729v2",
      "title": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„æœ‰é™ç†æ€§ï¼šæ¨ç†é˜¶æ®µçš„æ»¡è¶³åŒ–å¯¹é½",
      "authors": [
        "Mohamad Chehade",
        "Soumya Suvra Ghosal",
        "Souradip Chakraborty",
        "Avinash Reddy",
        "Dinesh Manocha",
        "Hao Zhu",
        "Amrit Singh Bedi"
      ],
      "abstract": "Aligning large language models with humans is challenging due to the inherently multifaceted nature of preference feedback. While existing approaches typically frame this as a multi-objective optimization problem, they often overlook how humans actually make decisions. Research on bounded rationality suggests that human decision making follows satisficing strategies-optimizing primary objectives while ensuring others meet acceptable thresholds. To bridge this gap and operationalize the notion of satisficing alignment, we propose SITAlign: an inference time framework that addresses the multifaceted nature of alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. We provide theoretical insights by deriving sub-optimality bounds of our satisficing based inference alignment approach. We empirically validate SITAlign's performance through extensive experimentation on multiple benchmarks. For instance, on the PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art multi objective decoding strategy by a margin of 22.3% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the threshold on harmlessness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†äººç±»å¤šé¢åå¥½æ—¶çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿå¤šç›®æ ‡ä¼˜åŒ–å¾€å¾€å¿½è§†äº†äººç±»å†³ç­–ä¸­çš„æœ‰é™ç†æ€§ï¼ˆBounded Rationalityï¼‰å’Œæ»¡æ„ç­–ç•¥ï¼ˆSatisficingï¼‰ã€‚ä¸ºå¼¥åˆè¿™ä¸€å·®è·ï¼Œä½œè€…æå‡ºäº†SITAlignæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨æ»¡è¶³æ¬¡è¦å‡†åˆ™çš„é˜ˆå€¼çº¦æŸä¸‹æœ€å¤§åŒ–ä¸»è¦ç›®æ ‡ï¼Œå®ç°æ¨ç†æ—¶çš„â€œæ»¡æ„å¯¹é½â€ï¼ˆSatisficing Alignmentï¼‰ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºè¿™ç§å¯¹é½æ–¹æ³•æ¨å¯¼äº†ç†è®ºä¸Šçš„æ¬¡ä¼˜åŒ–ç•Œé™ï¼ˆSub-optimality Boundsï¼‰ï¼Œè¿˜åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨PKU-SafeRLHFæ•°æ®é›†ä¸Šï¼ŒSITAlignåœ¨ç¡®ä¿æ— å®³æ€§ï¼ˆHarmlessnessï¼‰è¾¾æ ‡çš„å‰æä¸‹ï¼Œå…¶å¸®åŠ©æ€§ï¼ˆHelpfulnessï¼‰å¥–åŠ±çš„GPT-4èƒœå¹³ç‡æ¯”ç°æœ‰çš„å¤šç›®æ ‡è§£ç ç­–ç•¥é«˜å‡º22.3%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†ç»“åˆæœ‰é™ç†æ€§çš„æ¨ç†æ—¶å¯¹é½ç­–ç•¥åœ¨å¹³è¡¡å¤æ‚å¯¹é½éœ€æ±‚æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23729v2",
      "published_date": "2025-05-29 17:56:05 UTC",
      "updated_date": "2025-05-31 23:47:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:51.691756+00:00"
    },
    {
      "arxiv_id": "2505.23724v3",
      "title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA",
      "title_zh": "SC-LoRAï¼šé€šè¿‡å­ç©ºé—´çº¦æŸ LoRA å¹³è¡¡é«˜æ•ˆå¾®è°ƒä¸çŸ¥è¯†ä¿ç•™",
      "authors": [
        "Minrui Luo",
        "Fuhang Kuang",
        "Yu Wang",
        "Zirui Liu",
        "Tianxing He"
      ],
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), are indispensable for efficiently customizing Large Language Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and knowledge forgetting problems. Recent studies have leveraged the power of designed LoRA initialization, to enhance the fine-tuning efficiency, or to preserve knowledge in the pre-trained LLM. However, none of these works can address the two cases at the same time. To this end, we introduce Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework engineered to navigate the trade-off between efficient fine-tuning and knowledge preservation. We achieve this by constraining the output of trainable LoRA adapters in a low-rank subspace, where the context information of fine-tuning data is most preserved while the context information of preserved knowledge is least retained, in a balanced way. Such constraint enables the trainable weights to primarily focus on the main features of fine-tuning data while avoiding damaging the preserved knowledge features. We provide theoretical analysis on our method, and conduct extensive experiments including safety preservation and world knowledge preservation, on various downstream tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning performance while markedly diminishing knowledge forgetting, surpassing contemporary LoRA initialization methods.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ä½ç§©è‡ªé€‚åº”(LoRA)åœ¨å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)æ—¶å­˜åœ¨çš„æ”¶æ•›é€Ÿåº¦æ…¢å’ŒçŸ¥è¯†é—å¿˜(knowledge forgetting)é—®é¢˜ï¼Œæå‡ºäº†Subspace-Constrained LoRA (SC-LoRA)åˆå§‹åŒ–æ¡†æ¶ã€‚SC-LoRA æ—¨åœ¨é€šè¿‡å­ç©ºé—´çº¦æŸæŠ€æœ¯å¹³è¡¡é«˜æ•ˆå¾®è°ƒä¸çŸ¥è¯†ä¿ç•™(knowledge preservation)ä¹‹é—´çš„çŸ›ç›¾ï¼Œè§£å†³äº†ç°æœ‰åˆå§‹åŒ–æ–¹æ³•æ— æ³•åŒæ—¶å…¼é¡¾ä¸¤è€…çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•å°†å¯è®­ç»ƒ LoRA é€‚é…å™¨çš„è¾“å‡ºé™åˆ¶åœ¨ç‰¹å®šçš„ä½ç§©å­ç©ºé—´(low-rank subspace)å†…ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºå¾®è°ƒä»»åŠ¡çš„æ ¸å¿ƒç‰¹å¾ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘å¯¹é¢„è®­ç»ƒçŸ¥è¯†çš„å¹²æ‰°ã€‚ç†è®ºåˆ†æä¸å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡å®éªŒè¡¨æ˜ï¼ŒSC-LoRA åœ¨å®‰å…¨æ€§å’Œå¸¸è¯†ä¿ç•™æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨æ˜¾è‘—é™ä½çŸ¥è¯†é—å¿˜çš„åŒæ—¶æä¾›äº†è¶…è¶Šå½“å‰ä¸»æµ LoRA åˆå§‹åŒ–æ–¹æ³•çš„å¾®è°ƒæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23724v3",
      "published_date": "2025-05-29 17:55:21 UTC",
      "updated_date": "2025-10-31 10:04:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:40.714766+00:00"
    },
    {
      "arxiv_id": "2505.23723v1",
      "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering",
      "title_zh": "ML-Agentï¼šé¢å‘è‡ªä¸»æœºå™¨å­¦ä¹ å·¥ç¨‹çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å¼ºåŒ–",
      "authors": [
        "Zexi Liu",
        "Jingyi Chai",
        "Xinyu Zhu",
        "Shuo Tang",
        "Rui Ye",
        "Bo Zhang",
        "Lei Bai",
        "Siheng Chen"
      ],
      "abstract": "The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ML-Agentï¼Œæ—¨åœ¨è§£å†³åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“åœ¨è‡ªåŠ¨æœºå™¨å­¦ä¹ (ML)å·¥ç¨‹ä¸­è¿‡åº¦ä¾èµ–äººå·¥æç¤ºå·¥ç¨‹ä¸”ç¼ºä¹å®éªŒç»éªŒé€‚åº”æ€§çš„é—®é¢˜ã€‚è¿™æ˜¯è¯¥é¢†åŸŸé¦–æ¬¡æ¢ç´¢åŸºäºå­¦ä¹ çš„æ™ºèƒ½ä½“æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä½¿æ™ºèƒ½ä½“åœ¨äº¤äº’å¼å®éªŒä¸­è‡ªæˆ‘è¿›åŒ–ã€‚è¯¥æ¡†æ¶é›†æˆäº†å¢å¼ºæ¢ç´¢çš„å¾®è°ƒ(Exploration-enriched fine-tuning)ã€æå‡è®­ç»ƒæ•ˆç‡çš„åˆ†æ­¥å¼ºåŒ–å­¦ä¹ (Step-wise RL)ä»¥åŠç»Ÿä¸€åé¦ˆä¿¡å·çš„ä¸“ç”¨å¥–åŠ±æ¨¡å—(Reward module)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…åœ¨9ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒçš„7Bè§„æ¨¡ML-Agentï¼Œå…¶æ€§èƒ½ä¾¿è¶…è¶Šäº†671Bè§„æ¨¡çš„DeepSeek-R1æ™ºèƒ½ä½“ã€‚è¯¥æ¨¡å‹ä¸ä»…å®ç°äº†æŒç»­çš„æ€§èƒ½å¢é•¿ï¼Œè¿˜å±•ç°å‡ºå“è¶Šçš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å¼ºåŒ–å­¦ä¹ åœ¨æå‡è‡ªä¸»æœºå™¨å­¦ä¹ æ™ºèƒ½ä½“æ•ˆèƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23723v1",
      "published_date": "2025-05-29 17:54:44 UTC",
      "updated_date": "2025-05-29 17:54:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:50:49.860642+00:00"
    },
    {
      "arxiv_id": "2505.23720v1",
      "title": "COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents",
      "title_zh": "COBRAï¼šç¡®ä¿ç­–ç•¥æ€§æ™ºèƒ½ä½“è¯šå®æ€§çš„ä¸Šä¸‹æ–‡è€è™æœºç®—æ³•",
      "authors": [
        "Arun Verma",
        "Indrajit Saha",
        "Makoto Yokoo",
        "Bryan Kian Hsiang Low"
      ],
      "abstract": "This paper considers a contextual bandit problem involving multiple agents, where a learner sequentially observes the contexts and the agent's reported arms, and then selects the arm that maximizes the system's overall reward. Existing work in contextual bandits assumes that agents truthfully report their arms, which is unrealistic in many real-life applications. For instance, consider an online platform with multiple sellers; some sellers may misrepresent product quality to gain an advantage, such as having the platform preferentially recommend their products to online users. To address this challenge, we propose an algorithm, COBRA, for contextual bandit problems involving strategic agents that disincentivize their strategic behavior without using any monetary incentives, while having incentive compatibility and a sub-linear regret guarantee. Our experimental results also validate the different performance aspects of our proposed algorithm.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹æƒ…å¢ƒå¤šè‡‚è€è™æœº(contextual bandit)é—®é¢˜ä¸­å¤šä¸ªç­–ç•¥æ€§æ™ºèƒ½ä½“(strategic agents)å¯èƒ½ä¸ºäº†è‡ªèº«åˆ©ç›Šè€Œè™šæŠ¥ä¿¡æ¯ï¼ˆä¾‹å¦‚åœ¨çº¿å¹³å°å–å®¶è¯¯å¯¼äº§å“è´¨é‡ï¼‰çš„ç°å®æŒ‘æˆ˜å±•å¼€æ¢è®¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†åä¸ºCOBRAçš„æ–°å‹ç®—æ³•ï¼Œæ—¨åœ¨ä¸ä¾èµ–è´§å¸æ¿€åŠ±(monetary incentives)çš„å‰æä¸‹ï¼Œæœ‰æ•ˆæŠ‘åˆ¶æ™ºèƒ½ä½“çš„ç­–ç•¥æ€§è¡Œä¸ºå¹¶ç¡®ä¿å…¶è¯šå®æ€§ã€‚è¯¥ç®—æ³•å…·å¤‡æ¿€åŠ±ç›¸å®¹æ€§(incentive compatibility)ï¼Œå¹¶åœ¨ç†è®ºä¸Šæä¾›äº†æ¬¡çº¿æ€§é—æ†¾(sub-linear regret)ä¿è¯ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥éªŒè¯äº†COBRAåœ¨ä¸åŒæ€§èƒ½ç»´åº¦çš„ä¼˜è¶Šæ€§ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†æ¶‰åŠç­–ç•¥æ€§å‚ä¸è€…çš„å¤æ‚å†³ç­–åœºæ™¯æ—¶ï¼Œèƒ½å¤Ÿç¡®ä¿ç³»ç»Ÿæ•´ä½“å›æŠ¥çš„æœ€å¤§åŒ–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper proposes a contextual bandit algorithm that prevents strategic agents from misreporting while having approximate incentive compatibility and a sub-linear regret guarantee",
      "pdf_url": "https://arxiv.org/pdf/2505.23720v1",
      "published_date": "2025-05-29 17:53:12 UTC",
      "updated_date": "2025-05-29 17:53:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:52:23.611395+00:00"
    },
    {
      "arxiv_id": "2505.23885v2",
      "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation",
      "title_zh": "OWLï¼šé¢å‘çœŸå®ä¸–ç•Œä»»åŠ¡è‡ªåŠ¨åŒ–çš„é€šç”¨å¤šæ™ºèƒ½ä½“ååŒä¼˜åŒ–å­¦ä¹ ",
      "authors": [
        "Mengkang Hu",
        "Yuhang Zhou",
        "Wendong Fan",
        "Yuzhou Nie",
        "Bowei Xia",
        "Tao Sun",
        "Ziyu Ye",
        "Zhaoxuan Jin",
        "Yingru Li",
        "Qiguang Chen",
        "Zeyu Zhang",
        "Yifeng Wang",
        "Qianshuo Ye",
        "Bernard Ghanem",
        "Ping Luo",
        "Guohao Li"
      ],
      "abstract": "Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature. Current approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains. We introduce Workforce, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising: (i) a domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask management, and (iii) specialized Workers with domain-specific tool-calling capabilities. This decoupling enables cross-domain transferability during both inference and training phases: During inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents; For training, we introduce Optimized Workforce Learning (OWL), which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback. To validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks. Experimental results demonstrate Workforce achieves open-source state-of-the-art performance (69.70%), outperforming commercial systems like OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to GPT-4o on challenging tasks. To summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Workforceï¼Œä¸€ä¸ªåˆ†å±‚çš„å¤šæ™ºèƒ½ä½“(Multi-Agent)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLM)ç³»ç»Ÿåœ¨è·¨é¢†åŸŸè¿ç§»æ—¶é¢ä¸´çš„æ¶æ„é‡æ„å’Œé‡å¤è®­ç»ƒéš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç­–ç•¥è§„åˆ’ä¸ä¸“ä¸šæ‰§è¡Œè§£è€¦ï¼Œæ„å»ºäº†ç”±é¢†åŸŸæ— å…³çš„Plannerã€è´Ÿè´£å­ä»»åŠ¡ç®¡ç†çš„Coordinatorä»¥åŠå…·å¤‡ä¸“ä¸šå·¥å…·è°ƒç”¨èƒ½åŠ›çš„Workerç»„æˆçš„æ¨¡å—åŒ–æ¶æ„ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†ä¼˜åŒ–å·¥ä½œè´Ÿè½½å­¦ä¹ (Optimized Workforce Learning, OWL)ï¼Œé€šè¿‡çœŸå®åé¦ˆçš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¥ä¼˜åŒ–Plannerï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚åœ¨GAIAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒWorkforceå–å¾—äº†69.70%çš„å¼€æºæ¨¡å‹æœ€å…ˆè¿›æ€§èƒ½(SOTA)ï¼Œè¶…è¶Šäº†OpenAIçš„Deep Researchç­‰å•†ä¸šç³»ç»Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»OWLè®­ç»ƒçš„32Bæ¨¡å‹å‡†ç¡®ç‡æå‡äº†16.37%ï¼Œå±•ç°å‡ºä¸GPT-4oç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºå¼€å‘é€šç”¨AIåŠ©æ‰‹æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€æ¶æ„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Project Page: https://github.com/camel-ai/owl",
      "pdf_url": "https://arxiv.org/pdf/2505.23885v2",
      "published_date": "2025-05-29 17:51:58 UTC",
      "updated_date": "2025-06-11 01:42:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:52:50.308477+00:00"
    },
    {
      "arxiv_id": "2505.23714v2",
      "title": "SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods",
      "title_zh": "SenWiChï¼šåŸºäºæ··åˆæ–¹æ³•çš„ä½èµ„æºè¯­è¨€ WiC è¯­ä¹‰æ ‡æ³¨",
      "authors": [
        "Roksana Goworek",
        "Harpal Karlcut",
        "Muhammad Shezad",
        "Nijaguna Darshana",
        "Abhishek Mane",
        "Syam Bondada",
        "Raghav Sikka",
        "Ulvi Mammadov",
        "Rauf Allahverdiyev",
        "Sriram Purighella",
        "Paridhi Gupta",
        "Muhinyia Ndegwa",
        "Haim Dubossarsky"
      ],
      "abstract": "This paper addresses the critical need for high-quality evaluation datasets in low-resource languages to advance cross-lingual transfer. While cross-lingual transfer offers a key strategy for leveraging multilingual pretraining to expand language technologies to understudied and typologically diverse languages, its effectiveness is dependent on quality and suitable benchmarks. We release new sense-annotated datasets of sentences containing polysemous words, spanning ten low-resource languages across diverse language families and scripts. To facilitate dataset creation, the paper presents a demonstrably beneficial semi-automatic annotation method. The utility of the datasets is demonstrated through Word-in-Context (WiC) formatted experiments that evaluate transfer on these low-resource languages. Results highlight the importance of targeted dataset creation and evaluation for effective polysemy disambiguation in low-resource settings and transfer studies. The released datasets and code aim to support further research into fair, robust, and truly multilingual NLP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½èµ„æºè¯­è¨€ (low-resource languages) åœ¨è·¨è¯­è¨€è¿ç§» (cross-lingual transfer) ä¸­ç¼ºä¹é«˜è´¨é‡è¯„ä¼°æ•°æ®é›†çš„é—®é¢˜ï¼Œå‘å¸ƒäº† SenWiCh æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†åˆ†å¸ƒåœ¨ä¸åŒè¯­ç³»å’Œè„šæœ¬ä¸­çš„åç§ä½èµ„æºè¯­è¨€ï¼Œé‡ç‚¹å…³æ³¨åŒ…å«å¤šä¹‰è¯ (polysemous words) çš„å¥å­ä¹‰é¡¹æ ‡æ³¨ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åŠè‡ªåŠ¨æ ‡æ³¨æ–¹æ³• (semi-automatic annotation method)ï¼Œæ—¨åœ¨ç®€åŒ–æ•°æ®é›†çš„åˆ›å»ºè¿‡ç¨‹å¹¶æå‡æ•ˆç‡ã€‚ç ”ç©¶é€šè¿‡ä¸Šä¸‹æ–‡è¯­å¢ƒä¸‹çš„è¯ä¹‰è¾¨æ (Word-in-Context, WiC) æ ¼å¼å®éªŒéªŒè¯äº†è¯¥æ•°æ®é›†çš„æ•ˆç”¨ï¼Œå¹¶è¯„ä¼°äº†åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„è¿ç§»æ•ˆæœã€‚å®éªŒç»“æœå¼ºè°ƒäº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡åˆ›å»ºæ•°æ®é›†å¯¹äºå¤šä¹‰æ€§æ¶ˆæ­§ (polysemy disambiguation) å’Œè·¨è¯­è¨€ç ”ç©¶çš„é‡è¦æ€§ã€‚è¯¥ç ”ç©¶åŠå…¶å‘å¸ƒçš„å¼€æºæ•°æ®é›†ä¸ä»£ç ï¼Œä¸ºæ„å»ºæ›´åŠ å…¬å¹³ã€ç¨³å¥ä¸”çœŸæ­£å…·å¤‡å¤šè¯­è¨€å¤„ç†èƒ½åŠ› (multilingual NLP) çš„æŠ€æœ¯æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 22 figures, published at SIGTYP 2025 workshop in ACL",
      "pdf_url": "https://arxiv.org/pdf/2505.23714v2",
      "published_date": "2025-05-29 17:48:08 UTC",
      "updated_date": "2025-07-22 17:15:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:51:26.841957+00:00"
    },
    {
      "arxiv_id": "2505.23881v1",
      "title": "Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems",
      "title_zh": "åˆ©ç”¨æ¨ç†æ¨¡å‹ç”Ÿæˆæœç´¢å¯å‘å¼ç®—æ³•ä»¥è§£å†³ç»„åˆè®¾è®¡é—®é¢˜çš„æœªå†³å®ä¾‹",
      "authors": [
        "Christopher D. Rosin"
      ],
      "abstract": "Large Language Models (LLMs) with reasoning are trained to iteratively generate and refine their answers before finalizing them, which can help with applications to mathematics and code generation. We apply code generation with reasoning LLMs to a specific task in the mathematical field of combinatorial design. This field studies diverse types of combinatorial designs, many of which have lists of open instances for which existence has not yet been determined. The Constructive Protocol CPro1 uses LLMs to generate search heuristics that have the potential to construct solutions to small open instances. Starting with a textual definition and a validity verifier for a particular type of design, CPro1 guides LLMs to select and implement strategies, while providing automated hyperparameter tuning and execution feedback. CPro1 with reasoning LLMs successfully solves long-standing open instances for 7 of 16 combinatorial design problems selected from the 2006 Handbook of Combinatorial Designs, including new solved instances for 3 of these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves open instances for several problems from recent (2025) literature, generating new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform Nested Steiner Quadruple System.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢ç´¢äº†åˆ©ç”¨å…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆæœç´¢å¯å‘å¼ç®—æ³•(search heuristics)ï¼Œæ—¨åœ¨è§£å†³ç»„åˆè®¾è®¡(combinatorial design)é¢†åŸŸä¸­çš„å¼€æ”¾æ€§æ•°å­¦é—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨åä¸ºConstructive Protocol CPro1çš„åè®®ï¼Œè¯¥æ–¹æ³•å¼•å¯¼æ¨ç†æ¨¡å‹é€‰æ‹©å¹¶å®ç°æœç´¢ç­–ç•¥ï¼Œå¹¶ç»“åˆè‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜ä¸æ‰§è¡Œåé¦ˆæ¥æ„é€ å¤æ‚çš„æ•°å­¦è®¾è®¡æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒCPro1é…åˆæ¨ç†æ¨¡å‹æˆåŠŸè§£å†³äº†ã€Š2006å¹´ç»„åˆè®¾è®¡æ‰‹å†Œã€‹ä¸­16ç±»é—®é¢˜ä¸­çš„7ç±»é•¿æœŸæœªå†³å®ä¾‹ã€‚è¯¥æ¡†æ¶åœ¨Bhaskar Rao Designsã€Symmetric Weighing Matricesä»¥åŠBalanced Ternary Designsç­‰éæ¨ç†æ¨¡å‹æ— æ³•å¤„ç†çš„é¢†åŸŸå–å¾—äº†æ˜¾è‘—çªç ´ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è§£å†³äº†2025å¹´æœ€æ–°æ–‡çŒ®ä¸­çš„å¤šä¸ªå¼€æ”¾å®ä¾‹ï¼ŒåŒ…æ‹¬Covering Sequencesã€Johnson Clique Coversã€Deletion Codesä»¥åŠUniform Nested Steiner Quadruple Systemã€‚è¿™äº›å‘ç°è¯æ˜äº†æ¨ç†æ¨¡å‹åœ¨æ•°å­¦å‘ç°ã€ä»£ç ç”Ÿæˆä»¥åŠå¤„ç†å¤æ‚ç»„åˆä¼˜åŒ–é—®é¢˜æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "math.CO"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: text overlap with arXiv:2501.17725",
      "pdf_url": "https://arxiv.org/pdf/2505.23881v1",
      "published_date": "2025-05-29 17:45:50 UTC",
      "updated_date": "2025-05-29 17:45:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:51:47.444184+00:00"
    },
    {
      "arxiv_id": "2505.23710v1",
      "title": "From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems",
      "title_zh": "ä»è¿æ¥è¿ˆå‘è‡ªæ²»ï¼šè‡ªæ¼”åŒ–é€šä¿¡ç³»ç»Ÿçš„æ›™å…‰",
      "authors": [
        "Zeinab Nezami",
        "Syed Danial Ali Shah",
        "Maryam Hafeez",
        "Karim Djemame",
        "Syed Ali Raza Zaidi"
      ],
      "abstract": "This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven intelligence enables dynamic adaptation beyond static connectivity. We explore the key enablers of autonomous communication systems, spanning reconfigurable infrastructure, adaptive middleware, and intelligent network functions, alongside multi-agent collaboration for distributed decision-making. We explore how these methodologies align with emerging industrial IoT frameworks, ensuring seamless integration within digital manufacturing processes. Our findings emphasize the potential for improved real-time decision-making, optimizing efficiency, and reducing latency in networked control systems. The discussion addresses ethical challenges, research directions, and standardization efforts, concluding with a technology stack roadmap to guide future developments. By leveraging state-of-the-art 6G network management techniques, this research contributes to the next generation of intelligent automation solutions, bridging the gap between theoretical advancements and real-world industrial applications.",
      "tldr_zh": "æœ¬æ–‡å°†6Gæ„¿æ™¯å®šä¹‰ä¸ºä¸€ä¸ªSelf-Evolvingé€šä¿¡ç”Ÿæ€ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡AIé©±åŠ¨çš„æ™ºèƒ½å®ç°è¶…è¶Šé™æ€è¿æ¥çš„åŠ¨æ€æ¼”è¿›ã€‚ç ”ç©¶æ·±å…¥æ¢è®¨äº†è‡ªä¸»é€šä¿¡ç³»ç»Ÿçš„å…³é”®é©±åŠ¨å› ç´ ï¼Œæ¶µç›–äº†å¯é‡æ„åŸºç¡€è®¾æ–½ (Reconfigurable Infrastructure)ã€è‡ªé€‚åº”ä¸­é—´ä»¶ (Adaptive Middleware) ä»¥åŠæ™ºèƒ½ç½‘ç»œåŠŸèƒ½ (Intelligent Network Functions)ã€‚åŒæ—¶ï¼Œè®ºæ–‡å¼ºè°ƒäº†å¤šæ™ºèƒ½ä½“åä½œ (Multi-Agent Collaboration) åœ¨åˆ†å¸ƒå¼å†³ç­–ä¸­çš„ä½œç”¨ï¼Œå¹¶åˆ†æäº†è¿™äº›æŠ€æœ¯ä¸å·¥ä¸šç‰©è”ç½‘ (Industrial IoT) æ¡†æ¶çš„æ•´åˆè·¯å¾„ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½æœ‰æ•ˆæå‡å®æ—¶å†³ç­–èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜åŒ–æ•ˆç‡å¹¶é™ä½ç½‘ç»œæ§åˆ¶ç³»ç»Ÿä¸­çš„å»¶è¿Ÿ (Latency)ã€‚æ–‡ç« æœ€åè¿˜é’ˆå¯¹ä¼¦ç†æŒ‘æˆ˜å’Œæ ‡å‡†åŒ–å·¥ä½œè¿›è¡Œäº†è®¨è®ºï¼Œå¹¶æå‡ºäº†æŠ€æœ¯æ ˆè·¯çº¿å›¾ (Technology Stack Roadmap)ï¼Œæ—¨åœ¨ä¸ºä¸‹ä¸€ä»£æ™ºèƒ½è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆä»ç†è®ºç ”ç©¶è¿ˆå‘ç°å®å·¥ä¸šåº”ç”¨æ¶èµ·æ¡¥æ¢ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.DC",
        "cs.ET"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23710v1",
      "published_date": "2025-05-29 17:45:02 UTC",
      "updated_date": "2025-05-29 17:45:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:51:19.549735+00:00"
    },
    {
      "arxiv_id": "2505.23709v2",
      "title": "Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning",
      "title_zh": "åŸºäºåµŒå¥—å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ çš„çš®è‚¤ç—…å˜è¡¨å‹åˆ†æ",
      "authors": [
        "Dionysis Christopoulos",
        "Sotiris Spanos",
        "Eirini Baltzi",
        "Valsamis Ntouskos",
        "Konstantinos Karantzalos"
      ],
      "abstract": "We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning rich representations of skin lesions through a novel nested contrastive learning approach that captures complex relationships between images and metadata. Melanoma detection and skin lesion classification based solely on images, pose significant challenges due to large variations in imaging conditions (lighting, color, resolution, distance, etc.) and lack of clinical and phenotypical context. Clinicians typically follow a holistic approach for assessing the risk level of the patient and for deciding which lesions may be malignant and need to be excised, by considering the patient's medical history as well as the appearance of other lesions of the patient. Inspired by this, SLIMP combines the appearance and the metadata of individual skin lesions with patient-level metadata relating to their medical record and other clinically relevant information. By fully exploiting all available data modalities throughout the learning process, the proposed pre-training strategy improves performance compared to other pre-training strategies on downstream skin lesions classification tasks highlighting the learned representations quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SLIMP (Skin Lesion Image-Metadata Pre-training)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ–°å‹åµŒå¥—å¯¹æ¯”å­¦ä¹  (nested contrastive learning) æ–¹æ³•å­¦ä¹ çš®è‚¤ç—…å˜ä¸°å¯Œè¡¨å¾çš„æ¡†æ¶ã€‚é’ˆå¯¹é»‘è‰²ç´ ç˜¤ (Melanoma) æ£€æµ‹ä¸­å› æˆåƒæ¡ä»¶å·®å¼‚å’Œç¼ºä¹ä¸´åºŠèƒŒæ™¯ä¿¡æ¯å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•å€Ÿé‰´äº†ä¸´åºŠåŒ»ç”Ÿç»¼åˆè€ƒè™‘ç—…å²åŠå¤šå¤„ç—…å˜çš„æ•´ä½“è¯„ä¼°æ–¹æ¡ˆã€‚SLIMP å°†å•ä¸ªç—…å˜å›¾åƒã€å…ƒæ•°æ®ä¸æ‚£è€…çº§åˆ«çš„ä¸´åºŠä¿¡æ¯ç›¸ç»“åˆï¼Œæ—¨åœ¨æ•æ‰å›¾åƒä¸å¤æ‚å…ƒæ•°æ®ä¹‹é—´çš„æ·±å±‚å…³è”ã€‚é€šè¿‡åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å……åˆ†æŒ–æ˜å¤šæ¨¡æ€ (multi-modal) æ•°æ®ï¼Œè¯¥é¢„è®­ç»ƒç­–ç•¥åœ¨ä¸‹æ¸¸çš®è‚¤ç—…å˜åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜äºå…¶ä»–æ–¹æ³•çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶æå–çš„é«˜è´¨é‡è¡¨å¾èƒ½å¤Ÿæ˜¾è‘—æå‡çš®è‚¤ç—…å˜è¡¨å‹åˆ†æçš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23709v2",
      "published_date": "2025-05-29 17:42:13 UTC",
      "updated_date": "2026-01-22 09:47:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:51:33.832467+00:00"
    },
    {
      "arxiv_id": "2506.00073v4",
      "title": "The Automated but Risky Game: Modeling and Benchmarking Agent-to-Agent Negotiations and Transactions in Consumer Markets",
      "title_zh": "è‡ªåŠ¨åŒ–ä¸é£é™©åšå¼ˆï¼šæ¶ˆè´¹å¸‚åœºä¸­æ™ºèƒ½ä½“é—´åå•†ä¸äº¤æ˜“çš„å»ºæ¨¡ä¸åŸºå‡†æµ‹è¯•",
      "authors": [
        "Shenzhe Zhu",
        "Jiao Sun",
        "Yi Nian",
        "Tobin South",
        "Alex Pentland",
        "Jiaxin Pei"
      ],
      "abstract": "AI agents are increasingly used in consumer-facing applications to assist with tasks such as product search, negotiation, and transaction execution. In this paper, we explore a future scenario where both consumers and merchants authorize AI agents to fully automate negotiations and transactions. We aim to answer two key questions: (1) Do different LLM agents vary in their ability to secure favorable deals for users? (2) What risks arise from fully automating deal-making with AI agents in consumer markets? To address these questions, we develop an experimental framework that evaluates the performance of various LLM agents in real-world negotiation and transaction settings. Our findings reveal that AI-mediated deal-making is an inherently imbalanced game -- different agents achieve significantly different outcomes for their users. Moreover, behavioral anomalies in LLMs can result in financial losses for both consumers and merchants, such as overspending or accepting unreasonable deals. These results underscore that while automation can improve efficiency, it also introduces substantial risks. Users should exercise caution when delegating business decisions to AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ¶ˆè´¹å¸‚åœºä¸­ï¼Œç”±æ¶ˆè´¹è€…å’Œå•†å®¶æˆæƒçš„ AI agents è¿›è¡Œå…¨è‡ªåŠ¨åŒ–è°ˆåˆ¤ä¸äº¤æ˜“çš„æœªæ¥åœºæ™¯ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªå®éªŒæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ä¸åŒ LLM agents åœ¨å®é™…è°ˆåˆ¤å’Œäº¤æ˜“ç¯å¢ƒä¸­çš„è¡¨ç°åŠå…¶è·å–æœ‰åˆ©äº¤æ˜“çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒAI ä»‹å¯¼çš„äº¤æ˜“æœ¬è´¨ä¸Šæ˜¯ä¸€åœºä¸å¹³è¡¡çš„åšå¼ˆï¼Œä¸åŒæ™ºèƒ½ä½“ä¸ºç”¨æˆ·äº‰å–åˆ°çš„ç»“æœå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æ­¤å¤–ï¼ŒLLM çš„è¡Œä¸ºå¼‚å¸¸ï¼ˆbehavioral anomaliesï¼‰å¯èƒ½å¼•å‘ä¸¥é‡çš„ç»æµæŸå¤±ï¼Œä¾‹å¦‚å¯¼è‡´ç”¨æˆ·è¶…æ”¯æˆ–æ¥å—ä¸åˆç†çš„äº¤æ˜“æ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å…¨è‡ªåŠ¨åŒ–æå‡äº†äº¤æ˜“æ•ˆç‡ï¼Œä½†ä¹Ÿå¼•å…¥äº†å®è´¨æ€§çš„é£é™©ã€‚è¯¥ç ”ç©¶å¼ºè°ƒï¼Œç”¨æˆ·åœ¨å°†æ¶‰åŠèµ„é‡‘çš„å•†ä¸šå†³ç­–å§”æ‰˜ç»™ AI agents æ—¶åº”å½“ä¿æŒå®¡æ…ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00073v4",
      "published_date": "2025-05-29 17:41:39 UTC",
      "updated_date": "2025-09-20 18:47:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:51:49.090280+00:00"
    },
    {
      "arxiv_id": "2505.23706v1",
      "title": "Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats",
      "title_zh": "é¢å‘è½¦è”ç½‘å®‰å…¨çš„åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ ï¼šå¼‚å¸¸æ£€æµ‹å¢ç›Šä¸å¤šåŸŸæ”»å‡»å¨èƒ",
      "authors": [
        "Utku Demir",
        "Yalin E. Sagduyu",
        "Tugba Erpek",
        "Hossein Jafari",
        "Sastry Kompella",
        "Mengran Xue"
      ],
      "abstract": "In connected and autonomous vehicles, machine learning for safety message classification has become critical for detecting malicious or anomalous behavior. However, conventional approaches that rely on centralized data collection or purely local training face limitations due to the large scale, high mobility, and heterogeneous data distributions inherent in inter-vehicle networks. To overcome these challenges, this paper explores Distributed Federated Learning (DFL), whereby vehicles collaboratively train deep learning models by exchanging model updates among one-hop neighbors and propagating models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi) Extension Dataset, we show that DFL can significantly improve classification accuracy across all vehicles compared to learning strictly with local data. Notably, vehicles with low individual accuracy see substantial accuracy gains through DFL, illustrating the benefit of knowledge sharing across the network. We further show that local training data size and time-varying network connectivity correlate strongly with the model's overall accuracy. We investigate DFL's resilience and vulnerabilities under attacks in multiple domains, namely wireless jamming and training data poisoning attacks. Our results reveal important insights into the vulnerabilities of DFL when confronted with multi-domain attacks, underlining the need for more robust strategies to secure DFL in vehicular networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ (Distributed Federated Learning, DFL)åœ¨è”ç½‘è‡ªåŠ¨é©¾é©¶æ±½è½¦å®‰å…¨æ¶ˆæ¯åˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³è½¦è¾†ç½‘ç»œè§„æ¨¡å¤§ã€æµåŠ¨æ€§å¼ºå’Œæ•°æ®åˆ†å¸ƒä¸å‡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨VeReMiæ‰©å±•æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶è¡¨æ˜DFLå…è®¸è½¦è¾†é€šè¿‡å¤šè·³ä¼ æ’­æ¨¡å‹æ›´æ–°ï¼Œæ˜¾è‘—æå‡äº†å…¨ç½‘çš„å¼‚å¸¸æ£€æµ‹å‡†ç¡®ç‡ï¼Œç‰¹åˆ«æ˜¯æœ‰æ•ˆå¼¥è¡¥äº†æœ¬åœ°æ•°æ®ä¸è¶³è½¦è¾†çš„æ€§èƒ½çŸ­æ¿ã€‚åˆ†æå‘ç°ï¼Œæœ¬åœ°è®­ç»ƒæ•°æ®è§„æ¨¡å’Œéšæ—¶é—´å˜åŒ–çš„ç½‘ç»œè¿æ¥æ€§ä¸æ¨¡å‹æ•´ä½“å‡†ç¡®ç‡å¯†åˆ‡ç›¸å…³ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ·±å…¥è¯„ä¼°äº†DFLåœ¨é¢ä¸´æ— çº¿å¹²æ‰°(wireless jamming)å’Œè®­ç»ƒæ•°æ®æŠ•æ¯’(training data poisoning)ç­‰è·¨åŸŸæ”»å‡»ä¸‹çš„å¼¹æ€§ä¸è„†å¼±æ€§ã€‚å®éªŒç»“æœæ­ç¤ºäº†å¤šåŸŸæ”»å‡»å¯¹DFLç³»ç»Ÿç¨³å®šæ€§çš„ä¸¥é‡å¨èƒï¼Œå¼ºè°ƒäº†åœ¨è½¦è½½ç½‘ç»œä¸­æ„å»ºæ›´ç¨³å¥é˜²å¾¡ç­–ç•¥çš„è¿«åˆ‡éœ€æ±‚ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC",
        "cs.IT",
        "eess.SP"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23706v1",
      "published_date": "2025-05-29 17:41:02 UTC",
      "updated_date": "2025-05-29 17:41:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:52:02.686830+00:00"
    },
    {
      "arxiv_id": "2505.23703v4",
      "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability",
      "title_zh": "å½¢å¼åŒ–æ¨ç†ï¼šè‡ªç„¶-å½¢å¼åŒ–æ··åˆæ¨ç†æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›",
      "authors": [
        "Ruida Wang",
        "Yuxin Li",
        "Yi R. Fung",
        "Tong Zhang"
      ],
      "abstract": "Enhancing the mathematical reasoning capabilities of LLMs has garnered significant attention in both the mathematical and computer science communities. Recent works have made substantial progress in both Natural Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the potential of pure Reinforcement Learning (RL) methods on base models. However, RL approaches struggle to impart new capabilities not presented in the base model, highlighting the need to integrate more knowledge like FL into NL math reasoning effectively. Yet, this integration is challenging due to inherent disparities in problem structure and reasoning format between NL and FL. To address these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an end-to-end framework designed to incorporate the FL expert into NL math problem-solving. To bridge the NL and FL input format gap, we propose the NL-FL Problem Alignment method, which reformulates the Question-Answering (QA) problems in NL as existence theorems in FL. Subsequently, the Mixed Problem Input technique we provide enables the FL reasoner to handle both QA and existence problems concurrently. Lastly, we mitigate the NL and FL output format gap in reasoning through an LLM-based Answer Extraction mechanism. Comprehensive experiments demonstrate that the NFL-HR framework achieves **89.80**% and **84.34%** accuracy rates on the MATH-500 and the AMC benchmarks, surpassing the NL baseline by **4.60%** and **4.82%**, respectively. Notably, some problems resolved by our framework remain unsolved by the NL baseline model even under a larger number of trials.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NL-FL Hybrid Reasoning (NFL-HR)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†å½¢å¼åŒ–è¯­è¨€ (Formal Language, FL) é›†æˆåˆ°è‡ªç„¶è¯­è¨€ (Natural Language, NL) æ•°å­¦æ¨ç†ä¸­çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚ä¸ºäº†è§£å†³NLä¸FLä¹‹é—´å›ºæœ‰çš„ç»“æ„å·®å¼‚ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†NL-FL Problem Alignmentæ–¹æ³•ï¼Œå°†ä¼ ç»Ÿçš„é—®ç­”é—®é¢˜è½¬åŒ–ä¸ºFLä¸­çš„å­˜åœ¨æ€§å®šç†ã€‚é€šè¿‡å¼•å…¥Mixed Problem InputæŠ€æœ¯ï¼Œè¯¥æ¡†æ¶ä½¿FLæ¨ç†å™¨èƒ½å¤Ÿå¹¶å‘å¤„ç†ä¸åŒç±»å‹çš„æ•°å­¦é—®é¢˜ï¼Œå¹¶åˆ©ç”¨åŸºäºLLMçš„Answer Extractionæœºåˆ¶å¼¥åˆäº†è¾“å‡ºæ ¼å¼çš„é¸¿æ²Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNFL-HRåœ¨MATH-500å’ŒAMCåŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°89.80%å’Œ84.34%ï¼Œç›¸è¾ƒNLåŸºçº¿æ¨¡å‹æå‡äº†4.6%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼ŒæˆåŠŸæ”»å…‹äº†è®¸å¤šçº¯è‡ªç„¶è¯­è¨€æ¨¡å‹åœ¨å¤šæ¬¡å°è¯•ä¸‹ä»æ— æ³•è§£å†³çš„å¤æ‚æ•°å­¦éš¾é¢˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23703v4",
      "published_date": "2025-05-29 17:39:30 UTC",
      "updated_date": "2025-10-13 17:20:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:52:03.786198+00:00"
    },
    {
      "arxiv_id": "2505.23704v1",
      "title": "CLDTracker: A Comprehensive Language Description for Visual Tracking",
      "title_zh": "CLDTrackerï¼šé¢å‘è§†è§‰è·Ÿè¸ªçš„å…¨é¢è¯­è¨€æè¿°",
      "authors": [
        "Mohamad Alansari",
        "Sajid Javed",
        "Iyyakutti Iyappan Ganapathi",
        "Sara Alansari",
        "Muzammal Naseer"
      ],
      "abstract": "VOT remains a fundamental yet challenging task in computer vision due to dynamic appearance changes, occlusions, and background clutter. Traditional trackers, relying primarily on visual cues, often struggle in such complex scenarios. Recent advancements in VLMs have shown promise in semantic understanding for tasks like open-vocabulary detection and image captioning, suggesting their potential for VOT. However, the direct application of VLMs to VOT is hindered by critical limitations: the absence of a rich and comprehensive textual representation that semantically captures the target object's nuances, limiting the effective use of language information; inefficient fusion mechanisms that fail to optimally integrate visual and textual features, preventing a holistic understanding of the target; and a lack of temporal modeling of the target's evolving appearance in the language domain, leading to a disconnect between the initial description and the object's subsequent visual changes. To bridge these gaps and unlock the full potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive Language Description framework for robust visual Tracking. Our tracker introduces a dual-branch architecture consisting of a textual and a visual branch. In the textual branch, we construct a rich bag of textual descriptions derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with semantic and contextual cues to address the lack of rich textual representation. Experiments on six standard VOT benchmarks demonstrate that CLDTracker achieves SOTA performance, validating the effectiveness of leveraging robust and temporally-adaptive vision-language representations for tracking. Code and models are publicly available at: https://github.com/HamadYA/CLDTracker",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰ç›®æ ‡è·Ÿè¸ª (Visual Object Tracking, VOT) åœ¨åŠ¨æ€å¤–è§‚å˜åŒ–ã€é®æŒ¡å’ŒèƒŒæ™¯å¹²æ‰°ç­‰å¤æ‚åœºæ™¯ä¸‹çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨æ–‡æœ¬è¡¨å¾æ·±åº¦ã€ç‰¹å¾èåˆæ•ˆç‡åŠæ—¶é—´å»ºæ¨¡æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† CLDTrackerï¼Œä¸€ä¸ªå…¨æ–°çš„ç»¼åˆè¯­è¨€æè¿°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ–‡æœ¬ä¸è§†è§‰çš„åŒåˆ†æ”¯æ¶æ„ (dual-branch architecture)ï¼Œé€šè¿‡åˆ©ç”¨ CLIP å’Œ GPT-4V ç­‰æ¨¡å‹æ„å»ºåŒ…å«ä¸°å¯Œè¯­ä¹‰å’Œä¸Šä¸‹æ–‡çº¿ç´¢çš„æ–‡æœ¬æè¿°åŒ…ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ç›®æ ‡ç»†ç²’åº¦æ–‡æœ¬è¡¨ç¤ºç¼ºå¤±çš„é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å…·æœ‰æ—¶é—´é€‚åº”æ€§çš„è§†è§‰è¯­è¨€è¡¨ç¤ºæ¥åº”å¯¹ç›®æ ‡å¤–è§‚çš„åŠ¨æ€æ¼”å˜ã€‚åœ¨å…­ä¸ªæ ‡å‡† VOT åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCLDTracker è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„ SOTA æ€§èƒ½ï¼ŒéªŒè¯äº†ç»“åˆé²æ£’è§†è§‰è¯­è¨€è¡¨å¾åœ¨è·Ÿè¸ªä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†è·Ÿè¸ªçš„é²æ£’æ€§ï¼Œè¿˜ä¸ºå¤šæ¨¡æ€ç‰¹å¾åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ·±åº¦é›†æˆæä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "47 pages, 9 figures, Information Fusion Journal",
      "pdf_url": "https://arxiv.org/pdf/2505.23704v1",
      "published_date": "2025-05-29 17:39:30 UTC",
      "updated_date": "2025-05-29 17:39:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:52:24.951493+00:00"
    },
    {
      "arxiv_id": "2505.23695v1",
      "title": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics",
      "title_zh": "Data-to-Dashboardï¼šé¢å‘ä¼ä¸šåˆ†ææ·±åº¦è§è§£å¯è§†åŒ–çš„å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶",
      "authors": [
        "Ran Zhang",
        "Mohannad Elhamod"
      ],
      "abstract": "The rapid advancement of LLMs has led to the creation of diverse agentic systems in data analysis, utilizing LLMs' capabilities to improve insight generation and visualization. In this paper, we present an agentic system that automates the data-to-dashboard pipeline through modular LLM agents capable of domain detection, concept extraction, multi-perspective analysis generation, and iterative self-reflection. Unlike existing chart QA systems, our framework simulates the analytical reasoning process of business analysts by retrieving domain-relevant knowledge and adapting to diverse datasets without relying on closed ontologies or question templates.\n  We evaluate our system on three datasets across different domains. Benchmarked against GPT-4o with a single-prompt baseline, our approach shows improved insightfulness, domain relevance, and analytical depth, as measured by tailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw data to visualization, and opens new opportunities for human-in-the-loop validation by domain experts in business analytics. All code can be found here: https://github.com/77luvC/D2D_Data2Dashboard",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Data-to-Dashboardï¼Œä¸€ä¸ªç”¨äºä¼ä¸šåˆ†æçš„ Multi-Agent LLM æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–ä»åŸå§‹æ•°æ®åˆ°ä»ªè¡¨æ¿çš„å¯è§†åŒ–æµç¨‹æ¥ç”Ÿæˆæ·±åˆ»è§è§£ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œé›†æˆäº†é¢†åŸŸæ£€æµ‹ (domain detection)ã€æ¦‚å¿µæå– (concept extraction)ã€å¤šè§†è§’åˆ†æç”Ÿæˆä»¥åŠè¿­ä»£è‡ªåæ€ (self-reflection) ç­‰æ ¸å¿ƒæ™ºèƒ½ä½“ã€‚ä¸ç°æœ‰çš„å›¾è¡¨é—®ç­”ç³»ç»Ÿ (chart QA systems) ä¸åŒï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ£€ç´¢é¢†åŸŸç›¸å…³çŸ¥è¯†å¹¶é€‚åº”å¤šæ ·åŒ–æ•°æ®é›†ï¼Œæ¨¡æ‹Ÿäº†ä¸šåŠ¡åˆ†æå¸ˆçš„åˆ†ææ¨ç†è¿‡ç¨‹ï¼Œè€Œä¸ä¾èµ–äºå°é—­çš„æœ¬ä½“æˆ–å›ºå®šæ¨¡æ¿ã€‚ç ”ç©¶è€…åœ¨ä¸‰ä¸ªä¸åŒé¢†åŸŸçš„å®éªŒæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶ä»¥ GPT-4o çš„å•æç¤º (single-prompt) æ¨¡å‹ä½œä¸ºåŸºçº¿è¿›è¡Œå¯¹æ¯”ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§è§£æ·±åº¦ã€é¢†åŸŸç›¸å…³æ€§å’Œåˆ†ææ·±åº¦æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥å·¥ä½œä¸ºè¿æ¥åŸå§‹æ•°æ®ä¸å¯è§†åŒ–æä¾›äº†ä¸€ç§æ–°é¢–çš„æµæ°´çº¿ï¼Œå¹¶ä¸ºä¸šåŠ¡åˆ†æä¸­é¢†åŸŸä¸“å®¶çš„â€œäººåœ¨å›è·¯â€ (human-in-the-loop) éªŒè¯å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23695v1",
      "published_date": "2025-05-29 17:32:15 UTC",
      "updated_date": "2025-05-29 17:32:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:54:07.747703+00:00"
    },
    {
      "arxiv_id": "2505.23693v1",
      "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos",
      "title_zh": "VF-Evalï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ AIGC è§†é¢‘åé¦ˆç”Ÿæˆèƒ½åŠ›",
      "authors": [
        "Tingyu Song",
        "Tongyan Hu",
        "Guo Gan",
        "Yilun Zhao"
      ],
      "abstract": "MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VF-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) å¯¹ AI ç”Ÿæˆå†…å®¹ (AIGC) è§†é¢‘åé¦ˆèƒ½åŠ›çš„å…¨æ–°åŸºå‡†æµ‹è¯•ã€‚é‰´äºç°æœ‰ç ”ç©¶å¤šä¾§é‡äºè‡ªç„¶è§†é¢‘è€Œå¿½è§†äº†å¯¹åˆæˆè§†é¢‘è´¨é‡çš„è§£è¯»ï¼Œè¯¥åŸºå‡†å¼•å…¥äº†è¿è´¯æ€§éªŒè¯ (coherence validation)ã€é”™è¯¯æ„ŸçŸ¥ (error awareness)ã€é”™è¯¯ç±»å‹æ£€æµ‹ (error type detection) å’Œæ¨ç†è¯„ä¼° (reasoning evaluation) å››é¡¹ä»»åŠ¡æ¥å…¨é¢è¡¡é‡æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡å¯¹ 13 ä¸ªå‰æ²¿ MLLMs çš„è¯„ä¼°å‘ç°ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¼ºçš„ GPT-4.1 ä¹Ÿéš¾ä»¥åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­è¾¾åˆ°ç†æƒ³æ°´å¹³ï¼Œå‡¸æ˜¾äº†è¯¥åŸºå‡†çš„æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ RePrompt å®éªŒè¯æ˜ï¼Œä½¿æ¨¡å‹åé¦ˆæ›´è´´è¿‘äººç±»åˆ¤æ–­æœ‰åŠ©äºæ˜¾è‘—æå‡è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚è¯¥å·¥ä½œä¸ºä¼˜åŒ– MLLMs åœ¨ AIGC è§†é¢‘é¢†åŸŸçš„ç†è§£ä¸åº”ç”¨æä¾›äº†é‡è¦çš„å·¥å…·æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "ACL 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2505.23693v1",
      "published_date": "2025-05-29 17:31:13 UTC",
      "updated_date": "2025-05-29 17:31:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:53:12.707254+00:00"
    },
    {
      "arxiv_id": "2505.23686v2",
      "title": "ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork",
      "title_zh": "ROTATEï¼šé¢å‘å³å…´åä½œçš„æ‚”å€¼é©±åŠ¨å¼€æ”¾å¼è®­ç»ƒ",
      "authors": [
        "Caroline Wang",
        "Arrasy Rahman",
        "Jiaxun Cui",
        "Yoonchang Sung",
        "Peter Stone"
      ],
      "abstract": "Learning to collaborate with previously unseen partners is a fundamental generalization challenge in multi-agent learning, known as Ad Hoc Teamwork (AHT). Existing AHT approaches often adopt a two-stage pipeline, where first, a fixed population of teammates is generated with the idea that they should be representative of the teammates that will be seen at deployment time, and second, an AHT agent is trained to collaborate well with agents in the population. To date, the research community has focused on designing separate algorithms for each stage. This separation has led to algorithms that generate teammates with limited coverage of possible behaviors, and that ignore whether the generated teammates are easy to learn from for the AHT agent. Furthermore, algorithms for training AHT agents typically treat the set of training teammates as static, thus attempting to generalize to previously unseen partner agents without assuming any control over the set of training teammates. This paper presents a unified framework for AHT by reformulating the problem as an open-ended learning process between an AHT agent and an adversarial teammate generator. We introduce ROTATE, a regret-driven, open-ended training algorithm that alternates between improving the AHT agent and generating teammates that probe its deficiencies. Experiments across diverse two-player environments demonstrate that ROTATE significantly outperforms baselines at generalizing to an unseen set of evaluation teammates, thus establishing a new standard for robust and generalizable teamwork.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Ad Hoc Teamwork (AHT) ä¸­ç°æœ‰ä¸¤é˜¶æ®µæµç¨‹å¯¼è‡´çš„é˜Ÿå‹è¡Œä¸ºè¦†ç›–æœ‰é™ä»¥åŠè®­ç»ƒé˜Ÿå‹é›†é™æ€åŒ–ç­‰é—®é¢˜ï¼Œæå‡ºäº† ROTATE æ¡†æ¶ã€‚ROTATE å°† AHT é—®é¢˜é‡æ–°å®šä¹‰ä¸º AHT æ™ºèƒ½ä½“ä¸å¯¹æŠ—æ€§é˜Ÿå‹ç”Ÿæˆå™¨(adversarial teammate generator)ä¹‹é—´çš„å¼€æ”¾å¼å­¦ä¹ è¿‡ç¨‹(open-ended learning process)ã€‚è¯¥ç®—æ³•é‡‡ç”¨åæ‚”é©±åŠ¨(regret-driven)çš„æœºåˆ¶ï¼Œåœ¨æå‡æ™ºèƒ½ä½“åä½œèƒ½åŠ›ä¸ç”Ÿæˆèƒ½æ¢æµ‹å…¶ç¼ºé™·çš„é˜Ÿå‹ä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚è¿™ç§ç»Ÿä¸€çš„æ¡†æ¶å…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­é˜Ÿå‹ç”Ÿæˆä¸æ™ºèƒ½ä½“è®­ç»ƒç›¸äº’åˆ†ç¦»çš„å¼Šç«¯ï¼Œæœ‰æ•ˆæå‡äº†æ™ºèƒ½ä½“çš„é€‚åº”æ€§ã€‚åœ¨å¤šç§åŒäººç¯å¢ƒä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒROTATE åœ¨é¢å¯¹æœªè§è¿‡çš„è¯„ä¼°é˜Ÿå‹æ—¶ï¼Œæ³›åŒ–æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°ç¨³å¥ä¸”å…·å¤‡æ³›åŒ–èƒ½åŠ›çš„å›¢é˜Ÿåä½œæŠ€æœ¯å»ºç«‹äº†æ–°çš„æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23686v2",
      "published_date": "2025-05-29 17:24:54 UTC",
      "updated_date": "2025-10-22 03:21:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:53:41.213354+00:00"
    },
    {
      "arxiv_id": "2505.23671v3",
      "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents",
      "title_zh": "GSOï¼šè¯„ä¼° SWE-Agents çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„è½¯ä»¶ä¼˜åŒ–ä»»åŠ¡",
      "authors": [
        "Manish Shetty",
        "Naman Jain",
        "Jinjian Liu",
        "Vijay Kethanaboyina",
        "Koushik Sen",
        "Ion Stoica"
      ],
      "abstract": "Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† GSOï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¼€å‘é«˜æ€§èƒ½è½¯ä»¶æ–¹é¢èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æµæ°´çº¿ï¼Œé€šè¿‡åˆ†æä»£ç åº“æäº¤å†å²ï¼Œåœ¨ 10 ä¸ªè·¨é¢†åŸŸå’Œç¼–ç¨‹è¯­è¨€çš„ä»£ç åº“ä¸­è¯†åˆ«å‡º 102 ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¼˜åŒ–ä»»åŠ¡ã€‚åœ¨æµ‹è¯•ä¸­ï¼Œæ™ºèƒ½ä½“ (Agent) éœ€è¦æ ¹æ®æ€§èƒ½æµ‹è¯•è§„èŒƒæå‡ä»£ç è¿è¡Œæ•ˆç‡ï¼Œå…¶è¡¨ç°å°†ç›´æ¥ä¸ä¸“å®¶å¼€å‘è€…çš„ä¼˜åŒ–ç»“æœè¿›è¡Œå¯¹æ¯”ã€‚å®šé‡è¯„ä¼°æ˜¾ç¤ºï¼Œç›®å‰é¢†å…ˆçš„ SWE-Agents åœ¨è¯¥åŸºå‡†ä¸Šè¡¨ç°æ¬ ä½³ï¼ŒæˆåŠŸç‡ä¸è¶³ 5%ï¼Œä¸”å³ä½¿é€šè¿‡æ¨ç†æ—¶æ‰©å±• (Inference-time scaling) å¸¦æ¥çš„æå‡ä¹Ÿéå¸¸æœ‰é™ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†æ™ºèƒ½ä½“åœ¨åº•å±‚è¯­è¨€å¤„ç†ã€é‡‡å–â€œå·æ‡’â€ä¼˜åŒ–ç­–ç•¥ä»¥åŠå‡†ç¡®å®šä½æ€§èƒ½ç“¶é¢ˆæ–¹é¢çš„æ ¸å¿ƒå±€é™æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•åŠç›¸å…³æ•°æ®å·²å¼€æºï¼Œæ—¨åœ¨ä¸ºæœªæ¥é«˜æ€§èƒ½è½¯ä»¶å¼€å‘æ™ºèƒ½ä½“çš„ç ”ç©¶æä¾›å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Website: https://gso-bench.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2505.23671v3",
      "published_date": "2025-05-29 17:14:55 UTC",
      "updated_date": "2025-10-24 19:59:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:53:55.607332+00:00"
    },
    {
      "arxiv_id": "2505.23667v2",
      "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models",
      "title_zh": "Fortuneï¼šé¢å‘è¯­è¨€æ¨¡å‹ç¬¦å·åŒ–è¡¨æ ¼æ¨ç†çš„å…¬å¼é©±åŠ¨å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Lang Cao",
        "Jingxian Xu",
        "Hanbing Liu",
        "Jinyu Wang",
        "Mengyu Zhou",
        "Haoyu Dong",
        "Shi Han",
        "Dongmei Zhang"
      ],
      "abstract": "Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LMs)åœ¨å¤„ç†å¤æ‚è¡¨æ ¼æ•°æ®æ—¶é¢ä¸´çš„æ•°å€¼å’Œç¬¦å·æ¨ç†å›°å¢ƒï¼Œæå‡ºäº†åä¸ºFortune (Formula Tuning) çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒæ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œçš„ç”µå­è¡¨æ ¼å…¬å¼ (spreadsheet formulas) æ¥è¿›è¡Œé—®ç­”ï¼Œå……åˆ†åˆ©ç”¨äº†å…¬å¼ä¸­è•´å«çš„ä¸°å¯Œæ¨ç†æ¨¡å¼ã€‚Fortune åˆ›æ–°æ€§åœ°é‡‡ç”¨äºŒå…ƒç­”æ¡ˆæ­£ç¡®æ€§ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹  (RL) å¼•å¯¼æ¨¡å‹å­¦ä¹ å…¬å¼æ¨å¯¼ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†å¯¹ç›‘ç£å…¬å¼æ ‡æ³¨çš„ä¾èµ–ã€‚ç ”ç©¶é€šè¿‡ç†è®ºåˆ†æè¯æ˜äº†å…¶æ¶æ„ä¼˜åŠ¿ï¼Œå¹¶åœ¨ä¸ƒä¸ªè¡¨æ ¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å¹¿æ³›éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFortune æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤šæ­¥æ•°å€¼å’Œç¬¦å·æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä½¿å¾—ä¸€ä¸ª 7B å‚æ•°çš„æ¨¡å‹åœ¨è¡¨æ ¼ç†è§£èƒ½åŠ›ä¸Šèƒ½å¤Ÿè¶…è¶Š OpenAI o1ã€‚è¿™ä¸€æˆæœå‡¸æ˜¾äº†å…¬å¼é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ åœ¨æ¨åŠ¨è¯­è¨€æ¨¡å‹ç¬¦å·åŒ–è¡¨æ ¼æ¨ç†æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23667v2",
      "published_date": "2025-05-29 17:13:40 UTC",
      "updated_date": "2025-05-31 07:32:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:53:49.809742+00:00"
    },
    {
      "arxiv_id": "2506.00072v1",
      "title": "Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs",
      "title_zh": "åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹æç¤ºå·¥ç¨‹æŠ€æœ¯å¯¹å‡†ç¡®æ€§ä¸ç½®ä¿¡åº¦è¯±å¯¼çš„è¯„ä¼°",
      "authors": [
        "Nariman Naderi",
        "Zahra Atf",
        "Peter R Lewis",
        "Aref Mahjoub far",
        "Seyed Amir Ahmad Safavi-Naini",
        "Ali Soroush"
      ],
      "abstract": "This paper investigates how prompt engineering techniques impact both accuracy and confidence elicitation in Large Language Models (LLMs) applied to medical contexts. Using a stratified dataset of Persian board exam questions across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini, Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles (Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales (1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error (ECE) to evaluate alignment between confidence and actual performance. Chain-of-Thought prompts improved accuracy but also led to overconfidence, highlighting the need for calibration. Emotional prompting further inflated confidence, risking poor decisions. Smaller models like Llama-3.1-8b underperformed across all metrics, while proprietary models showed higher accuracy but still lacked calibrated confidence. These results suggest prompt engineering must address both accuracy and uncertainty to be effective in high-stakes medical tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†Prompt EngineeringæŠ€æœ¯å¯¹åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å‡†ç¡®æ€§å’ŒConfidence Elicitationæ–¹é¢çš„å½±å“ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨æ³¢æ–¯è¯­åŒ»å­¦å§”å‘˜ä¼šè€ƒè¯•æ•°æ®é›†ï¼Œå¯¹GPT-4oã€o3-miniã€Llama-3.3-70bã€Llama-3.1-8bå’ŒDeepSeek-v3äº”ç§æ¨¡å‹åœ¨156ç§é…ç½®ä¸‹è¿›è¡Œäº†æ·±åº¦æµ‹è¯•ã€‚è¯„ä¼°æ¶µç›–äº†å¤šç§Temperatureè®¾ç½®ã€æç¤ºé£æ ¼ï¼ˆå¦‚Chain-of-Thoughtã€Few-Shotã€Emotionalå’ŒExpert Mimicryï¼‰åŠç½®ä¿¡åº¦é‡è¡¨ï¼Œå¹¶ä½¿ç”¨AUC-ROCå’ŒExpected Calibration Error (ECE)ç­‰æŒ‡æ ‡è¡¡é‡ä¿¡å¿ƒä¸æ€§èƒ½çš„åŒ¹é…åº¦ã€‚ç»“æœè¡¨æ˜ï¼ŒChain-of-Thoughtè™½ç„¶æå‡äº†å‡†ç¡®æ€§ï¼Œä½†ä¹Ÿå¯¼è‡´äº†æ˜æ˜¾çš„Overconfidenceï¼Œéœ€è¦é€šè¿‡Calibrationè¿›è¡Œä¿®æ­£ã€‚Emotional promptingä¼šè¿›ä¸€æ­¥è¯±å¯¼ä¸åˆ‡å®é™…çš„é«˜ç½®ä¿¡åº¦ï¼Œå¢åŠ å†³ç­–é£é™©ï¼Œè€Œå°å‹æ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡è¡¨ç°è½åã€‚è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œåœ¨åŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸï¼Œæœ‰æ•ˆçš„æç¤ºå·¥ç¨‹å¿…é¡»åŒæ—¶è§£å†³å‡†ç¡®æ€§ä¸ä¸ç¡®å®šæ€§è¡¨è¾¾çš„é—®é¢˜ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "This paper was accepted for presentation at the 7th International Workshop on EXplainable, Trustworthy, and Responsible AI and Multi-Agent Systems (EXTRAAMAS 2025). Workshop website: https://extraamas.ehealth.hevs.ch/index.html",
      "pdf_url": "https://arxiv.org/pdf/2506.00072v1",
      "published_date": "2025-05-29 17:13:26 UTC",
      "updated_date": "2025-05-29 17:13:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:53:33.217705+00:00"
    },
    {
      "arxiv_id": "2505.23657v3",
      "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation",
      "title_zh": "ä¸»åŠ¨å±‚çº§å¯¹æ¯”è§£ç å‡å°‘å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸­çš„å¹»è§‰",
      "authors": [
        "Hongxiang Zhang",
        "Hao Chen",
        "Muhao Chen",
        "Tianyi Zhang"
      ],
      "abstract": "Recent decoding methods improve the factuality of large language models (LLMs) by refining how the next token is selected during generation. These methods typically operate at the token level, leveraging internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to hallucinations, especially over longer contexts. In this paper, we propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy that actively decides when to apply contrasting layers during generation. By casting decoding as a sequential decision-making problem, ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. Our experiments demonstrate that ActLCD surpasses state-of-the-art methods across five benchmarks, showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Active Layer-Contrastive Decoding (ActLCD)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­äº§ç”Ÿå¹»è§‰ (hallucination) çš„æ–°å‹è§£ç ç­–ç•¥ã€‚å°½ç®¡ç°æœ‰çš„è§£ç æ–¹æ³•å°è¯•é€šè¿‡ä¼˜åŒ–æ ‡è®°é€‰æ‹©æ¥æé«˜äº‹å®æ€§ï¼Œä½†åœ¨é•¿ä¸Šä¸‹æ–‡ç¯å¢ƒä¸‹ï¼ŒLLMs ä¾ç„¶å®¹æ˜“å—åˆ°è¡¨é¢æ¨¡å¼å½±å“è€Œäº§ç”Ÿé”™è¯¯ä¿¡æ¯ã€‚ActLCD çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå®ƒèƒ½å¤Ÿä¸»åŠ¨å†³å®šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç‰¹å®šæ—¶åˆ»åº”ç”¨å¯¹æ¯”å±‚ (contrasting layers)ï¼Œä»è€Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æ ‡è®°çº§ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•å°†è§£ç å»ºæ¨¡ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼Œå¹¶é‡‡ç”¨å—å¥–åŠ±æ„ŸçŸ¥åˆ†ç±»å™¨ (reward-aware classifier) æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ (reinforcement learning policy) æ¥é’ˆå¯¹æ€§åœ°æå‡ç”Ÿæˆå†…å®¹çš„äº‹å®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒActLCD åœ¨äº”ä¸ªæƒå¨åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ ·åŒ–ç”Ÿæˆåœºæ™¯ä¸­ç¼“è§£å¹»è§‰ç°è±¡çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´å¯é çš„è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 3 figures, EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23657v3",
      "published_date": "2025-05-29 17:07:24 UTC",
      "updated_date": "2025-09-15 17:26:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:53:55.941734+00:00"
    },
    {
      "arxiv_id": "2505.23655v3",
      "title": "Keyed Chaotic Dynamics for Privacy-Preserving Neural Inference",
      "title_zh": "é¢å‘éšç§ä¿æŠ¤ç¥ç»æ¨ç†çš„å¯†é’¥æ··æ²ŒåŠ¨åŠ›å­¦",
      "authors": [
        "Peter David Fagan"
      ],
      "abstract": "Neural network inference typically operates on raw input data, increasing the risk of exposure during preprocessing and inference. Moreover, neural architectures lack efficient built-in mechanisms for directly authenticating input data. This work introduces a novel encryption method for ensuring the security of neural inference. By constructing key-conditioned chaotic graph dynamical systems, we enable the encryption and decryption of real-valued tensors within the neural architecture. The proposed dynamical systems are particularly suited to encryption due to their sensitivity to initial conditions and their capacity to produce complex, key-dependent nonlinear transformations from compact rules. This work establishes a paradigm for securing neural inference and opens new avenues for research on the application of graph dynamical systems in neural network security.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Neural Network Inference ä¸­åŸå§‹æ•°æ®æ˜“æ³„éœ²åŠç¼ºä¹å†…ç½®éªŒè¯æœºåˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„éšç§ä¿æŠ¤åŠ å¯†æ–¹æ³•ã€‚ä½œè€…é€šè¿‡æ„å»º Key-Conditioned Chaotic Graph Dynamical Systemsï¼Œå®ç°äº†ç¥ç»ç½‘ç»œå†…éƒ¨ Real-valued Tensors çš„åŠ å¯†ä¸è§£å¯†ã€‚è¯¥åŠ¨åŠ›ç³»ç»Ÿåˆ©ç”¨å…¶å¯¹åˆå§‹æ¡ä»¶çš„é«˜åº¦æ•æ„Ÿæ€§ï¼Œèƒ½å¤Ÿé€šè¿‡ç´§å‡‘çš„è§„åˆ™äº§ç”Ÿå¤æ‚çš„ã€ä¾èµ–äºå¯†é’¥çš„ Nonlinear Transformationsã€‚è¿™é¡¹å·¥ä½œä¸ºä¿éšœ Neural Inference å®‰å…¨ç¡®ç«‹äº†æ–°çš„èŒƒå¼ï¼Œå¹¶ä¸º Graph Dynamical Systems åœ¨ç¥ç»ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.23655v3",
      "published_date": "2025-05-29 17:05:42 UTC",
      "updated_date": "2025-06-03 16:59:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:54:02.730422+00:00"
    },
    {
      "arxiv_id": "2505.23643v2",
      "title": "Securing AI Agents with Information-Flow Control",
      "title_zh": "åŸºäºä¿¡æ¯æµæ§åˆ¶çš„ AI æ™ºèƒ½ä½“å®‰å…¨ä¿éšœ",
      "authors": [
        "Manuel Costa",
        "Boris KÃ¶pf",
        "Aashish Kolluri",
        "Andrew Paverd",
        "Mark Russinovich",
        "Ahmed Salem",
        "Shruti Tople",
        "Lukas Wutschitz",
        "Santiago Zanella-BÃ©guelin"
      ],
      "abstract": "As AI agents become increasingly autonomous and capable, ensuring their security against vulnerabilities such as prompt injection becomes critical. This paper explores the use of information-flow control (IFC) to provide security guarantees for AI agents. We present a formal model to reason about the security and expressiveness of agent planners. Using this model, we characterize the class of properties enforceable by dynamic taint-tracking and construct a taxonomy of tasks to evaluate security and utility trade-offs of planner designs. Informed by this exploration, we present Fides, a planner that tracks confidentiality and integrity labels, deterministically enforces security policies, and introduces novel primitives for selectively hiding information. Its evaluation in AgentDojo demonstrates that this approach enables us to complete a broad range of tasks with security guarantees. A tutorial to walk readers through the the concepts introduced in the paper can be found at https://github.com/microsoft/fides",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ä¿¡æ¯æµæ§åˆ¶(Information-flow control, IFC)ä¸ºæ—¥ç›Šè‡ªä¸»çš„AIæ™ºèƒ½ä½“æä¾›å®‰å…¨ä¿éšœï¼Œæ—¨åœ¨åº”å¯¹æç¤ºæ³¨å…¥(prompt injection)ç­‰å…³é”®æ¼æ´ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç”¨äºæ¨å¯¼æ™ºèƒ½ä½“è§„åˆ’å™¨(planners)å®‰å…¨æ€§å’Œè¡¨ç°åŠ›çš„å½¢å¼åŒ–æ¨¡å‹(formal model)ï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡å‹åˆ»ç”»äº†é€šè¿‡åŠ¨æ€æ±¡ç‚¹è·Ÿè¸ª(dynamic taint-tracking)å¯å¼ºåˆ¶æ‰§è¡Œçš„å±æ€§ç±»åˆ«ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶å¼€å‘äº†åä¸ºFidesçš„è§„åˆ’å™¨ï¼Œå®ƒèƒ½å¤Ÿè·Ÿè¸ªæœºå¯†æ€§(confidentiality)å’Œå®Œæ•´æ€§(integrity)æ ‡ç­¾ï¼Œç¡®å®šæ€§åœ°æ‰§è¡Œå®‰å…¨ç­–ç•¥ï¼Œå¹¶å¼•å…¥äº†é€‰æ‹©æ€§éšè—ä¿¡æ¯çš„åˆ›æ–°åŸè¯­(primitives)ã€‚åœ¨AgentDojoç¯å¢ƒä¸‹çš„è¯„ä¼°è¯æ˜ï¼Œè¯¥æ–¹æ³•ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å…·å¤‡å®‰å…¨ä¿è¯çš„å‰æä¸‹å®Œæˆå¹¿æ³›çš„ä»»åŠ¡ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†ç³»ç»Ÿçš„å®‰å…¨æ€§ä¸å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23643v2",
      "published_date": "2025-05-29 16:50:41 UTC",
      "updated_date": "2025-09-03 12:30:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:54:05.039515+00:00"
    },
    {
      "arxiv_id": "2505.23637v2",
      "title": "Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging",
      "title_zh": "æŒä¹…æ€§æ¡å½¢ç èšåˆä¸ç‰¹å¾æ‹¼æ¥å¯¹åŒ»å­¦å½±åƒå½±å“çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Dashti A. Ali",
        "Richard K. G. Do",
        "William R. Jarnagin",
        "Aras T. Asaad",
        "Amber L. Simpson"
      ],
      "abstract": "In medical image analysis, feature engineering plays an important role in the design and performance of machine learning models. Persistent homology (PH), from the field of topological data analysis (TDA), demonstrates robustness and stability to data perturbations and addresses the limitation from traditional feature extraction approaches where a small change in input results in a large change in feature representation. Using PH, we store persistent topological and geometrical features in the form of the persistence barcode whereby large bars represent global topological features and small bars encapsulate geometrical information of the data. When multiple barcodes are computed from 2D or 3D medical images, two approaches can be used to construct the final topological feature vector in each dimension: aggregating persistence barcodes followed by featurization or concatenating topological feature vectors derived from each barcode. In this study, we conduct a comprehensive analysis across diverse medical imaging datasets to compare the effects of the two aforementioned approaches on the performance of classification models. The results of this analysis indicate that feature concatenation preserves detailed topological information from individual barcodes, yields better classification performance and is therefore a preferred approach when conducting similar experiments.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æ‹“æ‰‘æ•°æ®åˆ†æ(TDA)ä¸­çš„æŒä¹…åŒè°ƒ(Persistent Homology, PH)åœ¨åŒ»å­¦å›¾åƒç‰¹å¾å·¥ç¨‹ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æ¯”è¾ƒä¸åŒç‰¹å¾ç»„åˆç­–ç•¥å¯¹åˆ†ç±»æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶è€…åˆ©ç”¨æŒä¹…æ¡å½¢ç (Persistence Barcodes)æ•è·æ•°æ®çš„å…¨å±€æ‹“æ‰‘ç‰¹å¾ä¸å±€éƒ¨å‡ ä½•ä¿¡æ¯ï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº†ä¸¤ç§æ„å»ºæœ€ç»ˆæ‹“æ‰‘ç‰¹å¾å‘é‡çš„æ–¹æ³•ï¼šå…ˆèšåˆæ¡å½¢ç å†è¿›è¡Œç‰¹å¾åŒ–å¤„ç†(Aggregation)ï¼Œä»¥åŠç›´æ¥è¿æ¥å„æ¡å½¢ç æå–å‡ºçš„ç‰¹å¾å‘é‡(Feature Concatenation)ã€‚é€šè¿‡å¯¹å¤šç§ä¸åŒåŒ»å­¦å½±åƒæ•°æ®é›†çš„å…¨é¢åˆ†æï¼Œç ”ç©¶è¯„ä¼°äº†è¿™ä¸¤ç§æ–¹æ³•åœ¨å¤šç»´åº¦æ‹“æ‰‘ç‰¹å¾æ„å»ºä¸­çš„å®é™…è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç‰¹å¾è¿æ¥(Feature Concatenation)èƒ½å¤Ÿæ›´å®Œæ•´åœ°ä¿ç•™æ¥è‡ªå•ä¸ªæ¡å½¢ç çš„è¯¦ç»†æ‹“æ‰‘ä¿¡æ¯ï¼Œä»è€Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­è·å¾—æ›´ä¼˜çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œè¯¥ç ”ç©¶å»ºè®®åœ¨å¤„ç†ç±»ä¼¼åŒ»å­¦å½±åƒå®éªŒæ—¶ï¼Œåº”ä¼˜å…ˆé‡‡ç”¨ç‰¹å¾è¿æ¥è€Œéèšåˆæ–¹æ³•æ¥æ„å»ºæ‹“æ‰‘ç‰¹å¾æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23637v2",
      "published_date": "2025-05-29 16:45:33 UTC",
      "updated_date": "2025-06-04 14:50:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:54:11.439406+00:00"
    },
    {
      "arxiv_id": "2506.00071v1",
      "title": "Human sensory-musculoskeletal modeling and control of whole-body movements",
      "title_zh": "äººä½“æ„ŸçŸ¥-è‚Œè‚‰éª¨éª¼å»ºæ¨¡ä¸å…¨èº«è¿åŠ¨æ§åˆ¶",
      "authors": [
        "Chenhui Zuo",
        "Guohao Lin",
        "Chen Zhang",
        "Shanning Zhuang",
        "Yanan Sui"
      ],
      "abstract": "Coordinated human movement depends on the integration of multisensory inputs, sensorimotor transformation, and motor execution, as well as sensory feedback resulting from body-environment interaction. Building dynamic models of the sensory-musculoskeletal system is essential for understanding movement control and investigating human behaviours. Here, we report a human sensory-musculoskeletal model, termed SMS-Human, that integrates precise anatomical representations of bones, joints, and muscle-tendon units with multimodal sensory inputs involving visual, vestibular, proprioceptive, and tactile components. A stage-wise hierarchical deep reinforcement learning framework was developed to address the inherent challenges of high-dimensional control in musculoskeletal systems with integrated multisensory information. Using this framework, we demonstrated the simulation of three representative movement tasks, including bipedal locomotion, vision-guided object manipulation, and human-machine interaction during bicycling. Our results showed a close resemblance between natural and simulated human motor behaviours. The simulation also revealed musculoskeletal dynamics that could not be directly measured. This work sheds deeper insights into the sensorimotor dynamics of human movements, facilitates quantitative understanding of human behaviours in interactive contexts, and informs the design of systems with embodied intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æŠ¥å‘Šäº†ä¸€ç§åä¸ºSMS-Humançš„äººä½“æ„Ÿè§‰-è‚Œè‚‰éª¨éª¼æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ•´åˆäº†éª¨éª¼ã€å…³èŠ‚å’Œè‚Œè‚‰è…±å•ä½(muscle-tendon units)çš„ç²¾ç¡®è§£å‰–å­¦è¡¨ç¤ºï¼Œå¹¶èåˆäº†è§†è§‰ã€å‰åº­(vestibular)ã€æœ¬ä½“æ„Ÿè§‰(proprioceptive)å’Œè§¦è§‰ç­‰å¤šç§æ„Ÿå®˜è¾“å…¥ã€‚ä¸ºäº†åº”å¯¹è‚Œè‚‰éª¨éª¼ç³»ç»Ÿåœ¨é«˜ç»´æ§åˆ¶å’Œå¤šæ„Ÿå®˜ä¿¡æ¯é›†æˆæ–¹é¢çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªé˜¶æ®µæ€§åˆ†å±‚æ·±åº¦å¼ºåŒ–å­¦ä¹ (hierarchical deep reinforcement learning)æ¡†æ¶ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œç ”ç©¶æˆåŠŸæ¨¡æ‹Ÿäº†åŒè¶³è¡Œèµ°(bipedal locomotion)ã€è§†è§‰å¼•å¯¼çš„ç‰©ä½“æ“çºµä»¥åŠéª‘è‡ªè¡Œè½¦æ—¶çš„äººæœºäº¤äº’(human-machine interaction)è¿™ä¸‰ç§ä»£è¡¨æ€§è¿åŠ¨ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡æ‹Ÿçš„äººä½“è¿åŠ¨è¡Œä¸ºä¸è‡ªç„¶è¡Œä¸ºé«˜åº¦å¥‘åˆï¼Œå¹¶æ­ç¤ºäº†æ— æ³•ç›´æ¥æµ‹é‡çš„è‚Œè‚‰éª¨éª¼åŠ¨åŠ›å­¦ç‰¹å¾ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£äººä½“è¿åŠ¨çš„æ„Ÿè§‰è¿åŠ¨åŠ¨åŠ›å­¦(sensorimotor dynamics)æä¾›äº†æ·±å…¥è§è§£ï¼Œä¸ä»…ä¿ƒè¿›äº†å¯¹äº¤äº’åœºæ™¯ä¸‹äººç±»è¡Œä¸ºçš„å®šé‡åˆ†æï¼Œä¹Ÿä¸ºå…·èº«æ™ºèƒ½(embodied intelligence)ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00071v1",
      "published_date": "2025-05-29 16:42:08 UTC",
      "updated_date": "2025-05-29 16:42:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:55:01.201365+00:00"
    },
    {
      "arxiv_id": "2506.00070v3",
      "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics",
      "title_zh": "Robot-R1ï¼šç”¨äºå¢å¼ºæœºå™¨äººå…·èº«æ¨ç†çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Dongyoung Kim",
        "Sumin Park",
        "Huiwon Jang",
        "Jinwoo Shin",
        "Jaehyung Kim",
        "Younggyo Seo"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. To rigorously evaluate Robot-R1, we also introduce a new benchmark that demands the diverse embodied reasoning capabilities for the task. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Robot-R1ï¼Œä¸€ä¸ªåˆ©ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) å¢å¼ºæœºå™¨äººå…·èº«æ¨ç† (Embodied Reasoning) çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning) åœ¨æœºå™¨äººæ§åˆ¶ä¼˜åŒ–åŠæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶å—åˆ° DeepSeek-R1 å­¦ä¹ æ–¹æ³•çš„å¯å‘ï¼Œé€šè¿‡é‡‡æ ·åŸºäºæ¨ç†çš„å“åº”å¹¶å¼ºåŒ–èƒ½å¯¼è‡´æ›´å‡†ç¡®é¢„æµ‹çš„è¡Œä¸ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å½“å‰åœºæ™¯å›¾åƒå’Œç¯å¢ƒå…ƒæ•°æ®é¢„æµ‹ä»»åŠ¡å®Œæˆæ‰€éœ€çš„ä¸‹ä¸€ä¸ªå…³é”®ç‚¹çŠ¶æ€ (Keypoint State)ã€‚ä¸ºäº†ä¸¥è°¨è¯„ä¼°è¯¥æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†ä¸€ä¸ªè¦æ±‚å¤šæ ·åŒ–å…·èº«æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†æµ‹è¯• (Benchmark)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ Robot-R1 è®­ç»ƒçš„æ¨¡å‹åœ¨å…·èº«æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºä¼ ç»Ÿçš„ SFT æ–¹æ³•ã€‚å°½ç®¡ä»…æœ‰ 7B å‚æ•°ï¼ŒRobot-R1 åœ¨æ¶‰åŠç©ºé—´å’Œè¿åŠ¨æ¨ç†ç­‰åº•å±‚åŠ¨ä½œæ§åˆ¶çš„æ¨ç†ä»»åŠ¡ä¸­ç”šè‡³è¶…è¶Šäº† GPT-4oï¼Œè¯æ˜äº†å¼ºåŒ–å­¦ä¹ åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) å†³ç­–ç²¾åº¦æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.00070v3",
      "published_date": "2025-05-29 16:41:12 UTC",
      "updated_date": "2026-01-16 12:54:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:55:27.207365+00:00"
    },
    {
      "arxiv_id": "2506.11062v1",
      "title": "Decoding Cortical Microcircuits: A Generative Model for Latent Space Exploration and Controlled Synthesis",
      "title_zh": "è§£ç çš®å±‚å¾®ç”µè·¯ï¼šç”¨äºæ½œç©ºé—´æ¢ç´¢ä¸å¯æ§åˆæˆçš„ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Xingyu Liu",
        "Yubin Li",
        "Guozhang Chen"
      ],
      "abstract": "A central idea in understanding brains and building artificial intelligence is that structure determines function. Yet, how the brain's complex structure arises from a limited set of genetic instructions remains a key question. The ultra high-dimensional detail of neural connections vastly exceeds the information storage capacity of genes, suggesting a compact, low-dimensional blueprint must guide brain development. Our motivation is to uncover this blueprint. We introduce a generative model, to learn this underlying representation from detailed connectivity maps of mouse cortical microcircuits. Our model successfully captures the essential structural information of these circuits in a compressed latent space. We found that specific, interpretable directions within this space directly relate to understandable network properties. Building on this, we demonstrate a novel method to controllably generate new, synthetic microcircuits with desired structural features by navigating this latent space. This work offers a new way to investigate the design principles of neural circuits and explore how structure gives rise to function, potentially informing the development of more advanced artificial neural networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è„‘å¤æ‚ç»“æ„å¦‚ä½•é€šè¿‡æœ‰é™çš„é—ä¼ æŒ‡ä»¤äº§ç”Ÿï¼Œæ—¨åœ¨æ­ç¤ºæŒ‡å¯¼å¤§è„‘å‘è‚²çš„ä½ç»´è“å›¾ã€‚ä½œè€…æå‡ºäº†ä¸€ç§Generative Modelï¼Œé€šè¿‡ä»å°é¼ çš®å±‚å¾®ç”µè·¯(Cortical Microcircuits)çš„è¯¦ç»†è¿æ¥å›¾ä¸­å­¦ä¹ ï¼ŒæˆåŠŸåœ¨å‹ç¼©çš„Latent Spaceä¸­æ•æ‰åˆ°äº†ç”µè·¯çš„æ ¸å¿ƒç»“æ„ä¿¡æ¯ã€‚ç ”ç©¶å‘ç°ï¼ŒLatent Spaceä¸­å­˜åœ¨ç‰¹å®šä¸”å¯è§£é‡Šçš„æ–¹å‘ï¼Œä¸ç½‘ç»œå±æ€§ç›´æ¥ç›¸å…³ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œè¯¥å·¥ä½œå±•ç¤ºäº†ä¸€ç§é€šè¿‡åœ¨Latent Spaceä¸­å¯¼èˆªæ¥å—æ§åˆæˆå…·æœ‰ç›®æ ‡ç»“æ„ç‰¹å¾çš„æ–°å‹æ¨¡æ‹Ÿå¾®ç”µè·¯çš„æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¸ºç ”ç©¶ç¥ç»ç”µè·¯çš„è®¾è®¡åŸåˆ™ä»¥åŠç»“æ„ä¸åŠŸèƒ½çš„å…³ç³»æä¾›äº†æ–°è·¯å¾„ï¼Œå¹¶æœ‰æœ›ä¸ºå¼€å‘æ›´å…ˆè¿›çš„äººå·¥ç¥ç»ç½‘ç»œ(Artificial Neural Networks)æä¾›ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11062v1",
      "published_date": "2025-05-29 16:39:31 UTC",
      "updated_date": "2025-05-29 16:39:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:55:31.042108+00:00"
    },
    {
      "arxiv_id": "2505.23631v3",
      "title": "Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education",
      "title_zh": "äººç±»å…±æƒ…ä½œä¸ºç¼–ç å™¨ï¼šç‰¹æ®Šæ•™è‚²ä¸­çš„ AI è¾…åŠ©æŠ‘éƒè¯„ä¼°",
      "authors": [
        "Boning Zhao",
        "Xinnuo Li",
        "Yutong Hu"
      ],
      "abstract": "Assessing student depression in sensitive environments like special education is challenging. Standardized questionnaires may not fully reflect students' true situations. Furthermore, automated methods often falter with rich student narratives, lacking the crucial, individualized insights stemming from teachers' empathetic connections with students. Existing methods often fail to address this ambiguity or effectively integrate educator understanding. To address these limitations by fostering a synergistic human-AI collaboration, this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered AI framework for transparent and socially responsible depression severity assessment. Our approach uniquely integrates student narrative text with a teacher-derived, 9-dimensional \"Empathy Vector\" (EV), its dimensions guided by the PHQ-9 framework,to explicitly translate tacit empathetic insight into a structured AI input enhancing rather than replacing human judgment. Rigorous experiments optimized the multimodal fusion, text representation, and classification architecture, achieving 82.74% accuracy for 7-level severity classification. This work demonstrates a path toward more responsible and ethical affective computing by structurally embedding human empathy",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰¹æ®Šæ•™è‚²ä¸­å­¦ç”ŸæŠ‘éƒè¯„ä¼°é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† Human Empathy as Encoder (HEAE) æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºäººç±»åˆ¤æ–­è€Œéå–ä»£äººç±»çš„ AI è¾…åŠ©è¯„ä¼°æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆå­¦ç”Ÿå™è¿°æ–‡æœ¬ä¸æ•™å¸ˆè¡ç”Ÿçš„ä¹ç»´ Empathy Vector (EV)ï¼Œå°†æ•™å¸ˆåœ¨é•¿æœŸäº’åŠ¨ä¸­ç§¯ç´¯çš„éšæ€§å…±æƒ…æ´å¯Ÿè½¬åŒ–ä¸ºåŸºäº PHQ-9 æ¡†æ¶çš„ç»“æ„åŒ–è¾“å…¥ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹å¤šæ¨¡æ€èåˆå’Œåˆ†ç±»æ¶æ„è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹åœ¨ä¸ƒçº§æŠ‘éƒä¸¥é‡ç¨‹åº¦åˆ†ç±»ä¸­è¾¾åˆ°äº† 82.74% çš„å‡†ç¡®ç‡ã€‚HEAE çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå°†äººç±»å…±æƒ…ç»“æ„åŒ–åœ°åµŒå…¥åˆ°è®¡ç®—æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†æƒ…æ„Ÿè®¡ç®— (Affective Computing) çš„é€æ˜åº¦å’Œä¼¦ç†è´£ä»»ã€‚è¿™ä¸€æˆæœä¸ºåœ¨æ•æ„Ÿæ•™è‚²ç¯å¢ƒä¸‹æ„å»ºæ›´å…·åŒç†å¿ƒä¸”ç¤¾ä¼šè´Ÿè´£çš„æ™ºèƒ½è¯„ä¼°ç³»ç»Ÿæä¾›äº†å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 6 figures, ACII 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23631v3",
      "published_date": "2025-05-29 16:37:15 UTC",
      "updated_date": "2025-10-05 08:26:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:54:37.552332+00:00"
    },
    {
      "arxiv_id": "2505.23628v3",
      "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora",
      "title_zh": "AutoSchemaKGï¼šåŸºäºå¤§è§„æ¨¡ç½‘ç»œè¯­æ–™åº“åŠ¨æ€æ¨¡å¼å½’çº³çš„è‡ªä¸»çŸ¥è¯†å›¾è°±æ„å»º",
      "authors": [
        "Jiaxin Bai",
        "Wei Fan",
        "Qi Hu",
        "Qing Zong",
        "Chunyang Li",
        "Hong Ting Tsang",
        "Hongyu Luo",
        "Yauwai Yim",
        "Haoyu Huang",
        "Xiao Zhou",
        "Feng Qin",
        "Tianshi Zheng",
        "Xi Peng",
        "Xin Yao",
        "Huiwen Yang",
        "Leijie Wu",
        "Yi Ji",
        "Gong Zhang",
        "Renhai Chen",
        "Yangqiu Song"
      ],
      "abstract": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph construction that eliminates the need for predefined schemas. Our system leverages large language models to simultaneously extract knowledge triples and induce comprehensive schemas directly from text, modeling both entities and events while employing conceptualization to organize instances into semantic categories. Processing over 50 million documents, we construct ATLAS (Automated Triple Linking And Schema induction), a family of knowledge graphs with 900+ million nodes and 5.9 billion edges. This approach outperforms state-of-the-art baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our schema induction achieves 92\\% semantic alignment with human-crafted schemas with zero manual intervention, demonstrating that billion-scale knowledge graphs with dynamically induced schemas can effectively complement parametric knowledge in large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoSchemaKG æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å…¨è‡ªåŠ¨åŒ–çš„çŸ¥è¯†å›¾è°±(Knowledge Graph)æ„å»ºï¼Œå½»åº•æ‘†è„±äº†å¯¹é¢„å®šä¹‰æ¨¡å¼(Schema)çš„ä¾èµ–ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»æµ·é‡æ–‡æœ¬ä¸­ç›´æ¥æå–çŸ¥è¯†ä¸‰å…ƒç»„ï¼Œå¹¶åŒæ­¥è¯±å¯¼ç”Ÿæˆæ¶µç›–å®ä½“ä¸äº‹ä»¶çš„å…¨é¢æ¨¡å¼ï¼Œé€šè¿‡æ¦‚å¿µåŒ–(Conceptualization)å°†å®ä¾‹ç»„ç»‡è¿›è¯­ä¹‰ç±»åˆ«ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¤„ç†è¶…è¿‡5,000ä¸‡ä»½æ–‡æ¡£æ„å»ºäº† ATLAS çŸ¥è¯†å›¾è°±ç³»åˆ—ï¼ŒåŒ…å«é€¾9äº¿ä¸ªèŠ‚ç‚¹å’Œ59äº¿æ¡è¾¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šè·³é—®ç­”(Multi-hop QA)ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰åŸºå‡†ï¼Œå¹¶æœ‰æ•ˆå¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§ã€‚å…¶æ¨¡å¼è¯±å¯¼æŠ€æœ¯åœ¨é›¶äººå·¥å¹²é¢„ä¸‹å®ç°äº†ä¸äººå·¥æ¨¡å¼92%çš„è¯­ä¹‰å¯¹é½ï¼Œè¯æ˜äº†å¤§è§„æ¨¡åŠ¨æ€è¯±å¯¼çŸ¥è¯†å›¾è°±å¯ä»¥æœ‰æ•ˆè¡¥å……å¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°åŒ–çŸ¥è¯†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, preprint, code: https://github.com/HKUST-KnowComp/AutoSchemaKG",
      "pdf_url": "https://arxiv.org/pdf/2505.23628v3",
      "published_date": "2025-05-29 16:34:58 UTC",
      "updated_date": "2025-08-01 06:39:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:54:42.247911+00:00"
    },
    {
      "arxiv_id": "2505.23624v1",
      "title": "Towards Explainable Sequential Learning",
      "title_zh": "è¿ˆå‘å¯è§£é‡Šåºåˆ—å­¦ä¹ ",
      "authors": [
        "Giacomo Bergami",
        "Emma Packer",
        "Kirsty Scott",
        "Silvia Del Din"
      ],
      "abstract": "This paper offers a hybrid explainable temporal data processing pipeline, DataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence (EMeriTAte+DF), bridging numerical-driven temporal data classification with an event-based one through verified artificial intelligence principles, enabling human-explainable results. This was possible through a preliminary a posteriori explainable phase describing the numerical input data in terms of concurrent constituents with numerical payloads. This further required extending the event-based literature to design specification mining algorithms supporting concurrent constituents. Our previous and current solutions outperform state-of-the-art solutions for multivariate time series classifications, thus showcasing the effectiveness of the proposed methodology.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º DataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence (EMeriTAte+DF) çš„æ··åˆå¯è§£é‡Šæ—¶åºæ•°æ®å¤„ç†æ¡†æ¶ï¼Œæ—¨åœ¨æ¡¥æ¥åŸºäºæ•°å€¼çš„æ—¶åºæ•°æ®åˆ†ç±»ä¸åŸºäºäº‹ä»¶çš„åˆ†ç±»æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡éµå¾ªéªŒè¯çš„äººå·¥æ™ºèƒ½åŸåˆ™ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰äººç±»å¯è¯»æ€§çš„è§£é‡Šç»“æœã€‚æŠ€æœ¯å®ç°ä¸Šï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåéªŒå¯è§£é‡Šé˜¶æ®µï¼Œå°†æ•°å€¼è¾“å…¥æ•°æ®æè¿°ä¸ºå¸¦æœ‰æ•°å€¼è´Ÿè½½çš„å¹¶è¡Œæˆåˆ†ï¼ˆconcurrent constituentsï¼‰ï¼Œå¹¶ä¸ºæ­¤æ‰©å±•äº†åŸºäºäº‹ä»¶çš„ç›¸å…³ç†è®ºï¼Œè®¾è®¡äº†æ”¯æŒå¹¶è¡Œæˆåˆ†çš„ specification mining ç®—æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—åˆ†ç±»ï¼ˆmultivariate time series classificationsï¼‰ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚è¿™ä¸€æˆæœä¸ä»…è¯æ˜äº†æ‰€ææ–¹æ³•åœ¨å¤„ç†å¤æ‚åºåˆ—æ•°æ®æ—¶çš„å“è¶Šæ€§èƒ½ï¼Œä¹Ÿä¸ºå®ç° sequential learning çš„é€æ˜åŒ–æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23624v1",
      "published_date": "2025-05-29 16:30:59 UTC",
      "updated_date": "2025-05-29 16:30:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:54:57.452672+00:00"
    },
    {
      "arxiv_id": "2505.23617v2",
      "title": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory",
      "title_zh": "ä¸€è½¨è¿¹ä¸€è¯å…ƒï¼šåŸºäºå…¨æ™¯å­ç‰©ä½“è½¨è¿¹çš„å®šä½å¼è§†é¢‘è¯å…ƒåŒ–",
      "authors": [
        "Chenhao Zheng",
        "Jieyu Zhang",
        "Mohammadreza Salehi",
        "Ziqi Gao",
        "Vishnu Iyengar",
        "Norimasa Kobori",
        "Quan Kong",
        "Ranjay Krishna"
      ],
      "abstract": "Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 18x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘æ ‡è®°åŒ–(Video Tokenization)ä¸­æ—¶ç©ºåˆ†å—(space-time patches)å¯¼è‡´çš„æ ‡è®°å†—ä½™å’Œè®¡ç®—ä½æ•ˆé—®é¢˜ï¼Œæå‡ºäº†åŸºäºå…¨æ™¯å­å¯¹è±¡è½¨è¿¹(panoptic sub-object trajectories)çš„Grounded Video Tokenizationæ–°èŒƒå¼ã€‚é€šè¿‡å¼€å‘çš„TrajViTç¼–ç å™¨ï¼Œè¯¥æ–¹æ³•å°†å¯¹è±¡è½¨è¿¹è½¬åŒ–ä¸ºå…·æœ‰è¯­ä¹‰ä¿¡æ¯çš„æ ‡è®°ï¼Œä½¿æ ‡è®°æ•°é‡å–å†³äºåœºæ™¯å¤æ‚åº¦è€Œéè§†é¢‘æ—¶é•¿ï¼Œä»è€Œåœ¨ä¿æŒæ—¶é—´ç›¸å¹²æ€§(temporal coherence)çš„åŒæ—¶æ˜¾è‘—å‡å°‘å†—ä½™ã€‚å®éªŒè¡¨æ˜ï¼ŒTrajViTåœ¨è§†é¢‘-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­ä»¥10å€çš„æ ‡è®°ç¼©å‡å®ç°äº†æ¯”ViT3Dé«˜å‡º6%çš„top-5å¬å›ç‡ã€‚æ­¤å¤–ï¼Œå°†å…¶ä½œä¸ºVideoLLMçš„è§†é¢‘ç¼–ç å™¨æ—¶ï¼Œåœ¨6é¡¹VideoQAåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æ€§èƒ½æå‡5.2%ï¼Œä¸”è®­ç»ƒé€Ÿåº¦æå‡4å€ï¼Œæ¨ç†FLOPsé™ä½18å€ã€‚TrajViTæ˜¯é¦–ä¸ªåœ¨å¤šæ ·åŒ–è§†é¢‘åˆ†æä»»åŠ¡ä¸­å…¨é¢è¶…è¶ŠViT3Dçš„é«˜æ•ˆç¼–ç å™¨ï¼Œä¸ºæ„å»ºå¯æ‰©å±•çš„é•¿è§†é¢‘ç†è§£æ¨¡å‹æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23617v2",
      "published_date": "2025-05-29 16:25:35 UTC",
      "updated_date": "2025-07-09 18:41:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:57:05.035964+00:00"
    },
    {
      "arxiv_id": "2505.23879v1",
      "title": "CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data",
      "title_zh": "åŸºäºåˆºçªè›‹ç™½åºåˆ—ä¸ä¸´åºŠæ•°æ®çš„ COVID-19 ä¸¥é‡ç¨‹åº¦ AI é©±åŠ¨é¢„æµ‹ CNN-LSTM æ··åˆæ¨¡å‹",
      "authors": [
        "Caio Cheohen",
        "VinnÃ­cius M. S. Gomes",
        "Manuela L. da Silva"
      ],
      "abstract": "The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need for accurate prediction of disease severity to optimize healthcare resource allocation and patient management. The spike protein, which facilitates viral entry into host cells, exhibits high mutation rates, particularly in the receptor-binding domain, influencing viral pathogenicity. Artificial intelligence approaches, such as deep learning, offer promising solutions for leveraging genomic and clinical data to predict disease outcomes. Objective: This study aimed to develop a hybrid CNN-LSTM deep learning model to predict COVID-19 severity using spike protein sequences and associated clinical metadata from South American patients. Methods: We retrieved 9,570 spike protein sequences from the GISAID database, of which 3,467 met inclusion criteria after standardization. The dataset included 2,313 severe and 1,154 mild cases. A feature engineering pipeline extracted features from sequences, while demographic and clinical variables were one-hot encoded. A hybrid CNN-LSTM architecture was trained, combining CNN layers for local pattern extraction and an LSTM layer for long-term dependency modeling. Results: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%, demonstrating robust classification performance. Training stabilized at 85% accuracy with minimal overfitting. The most prevalent lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional epidemiological trends, suggesting potential associations between viral genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model effectively predicted COVID-19 severity using spike protein sequences and clinical data, highlighting the utility of AI in genomic surveillance and precision public health. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§CNN-LSTMæ··åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨åˆ©ç”¨SARS-CoV-2çš„spike proteinåºåˆ—åŠå…¶å…³è”çš„ä¸´åºŠå…ƒæ•°æ®é¢„æµ‹COVID-19çš„ä¸¥é‡ç¨‹åº¦ã€‚ç ”ç©¶å›¢é˜Ÿæ ‡å‡†åŒ–å¤„ç†äº†æ¥è‡ªGISAIDæ•°æ®åº“çš„å—ç¾æ‚£è€…æ ·æœ¬ï¼Œå¹¶æ„å»ºäº†èåˆCNNç‰¹å¾æå–ä¸LSTMé•¿çŸ­æœŸä¾èµ–å»ºæ¨¡çš„æ··åˆæ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸¥é‡å’Œè½»ç—‡ç—…ä¾‹çš„åˆ†ç±»ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº†82.92%çš„F1 scoreå’Œ0.9084çš„ROC-AUCã€‚ç ”ç©¶å‘ç°ç‰¹å®šè°±ç³»ï¼ˆå¦‚P.1, AY.99.2ï¼‰ä¸ä¸´åºŠç»“æœå­˜åœ¨æ½œåœ¨å…³è”ï¼Œå°è¯äº†æ¨¡å‹æ•æ‰ç—…æ¯’é—ä¼ ç‰¹å¾çš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†AIåœ¨åŸºå› ç»„ç›‘æµ‹å’Œç²¾å‡†å…¬å…±å«ç”Ÿä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæœªæ¥çªå‘ä¼ æŸ“ç—…çš„æ—©æœŸä¸¥é‡ç¨‹åº¦é¢„è­¦æä¾›äº†å®ç”¨æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 4 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.23879v1",
      "published_date": "2025-05-29 16:20:54 UTC",
      "updated_date": "2025-05-29 16:20:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:55:26.140401+00:00"
    },
    {
      "arxiv_id": "2505.23604v1",
      "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering",
      "title_zh": "Satori-SWEï¼šé¢å‘æ ·æœ¬é«˜æ•ˆè½¯ä»¶å·¥ç¨‹çš„æ¼”åŒ–å¼æµ‹è¯•æ—¶æ‰©å±•",
      "authors": [
        "Guangtao Zeng",
        "Maohao Shen",
        "Delin Chen",
        "Zhenting Qi",
        "Subhro Das",
        "Dan Gutfreund",
        "David Cox",
        "Gregory Wornell",
        "Wei Lu",
        "Zhang-Wei Hong",
        "Chuang Gan"
      ],
      "abstract": "Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼ˆå¦‚ SWE-Benchï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å‚æ•°é‡å°äº 100B çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚ç°æœ‰çš„æµ‹è¯•æ—¶æ‰©å±•ï¼ˆTest-time scalingï¼‰æ–¹æ³•é€šå¸¸ä¾èµ–å¤§é‡é‡‡æ ·å’Œæ˜‚è´µçš„è¯„åˆ†æœºåˆ¶ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Evolutionary Test-Time Scaling (EvoScale)ï¼Œè¿™æ˜¯ä¸€ç§å°†ç”Ÿæˆè¿‡ç¨‹è§†ä¸ºè¿›åŒ–è¿‡ç¨‹çš„æ ·æœ¬é«˜æ•ˆæ–¹æ³•ã€‚EvoScale é€šè¿‡é€‰æ‹©å’Œå˜å¼‚ï¼ˆselection and mutationï¼‰è¿­ä»£ä¼˜åŒ–è¾“å‡ºï¼Œå°†è¾“å‡ºåˆ†å¸ƒæ¨å‘é«˜åˆ†åŒºåŸŸï¼Œä»è€Œæ˜¾è‘—å‡å°‘å¯»æ‰¾æ­£ç¡®è§£æ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚ä¸ºäº†é™ä½é‡‡æ ·å’Œé€‰æ‹©å¼€é”€ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰è®­ç»ƒæ¨¡å‹è¿›è¡Œè‡ªæˆ‘è¿›åŒ–ï¼ˆself-evolveï¼‰ï¼Œä½¿å…¶åœ¨æ¨ç†æ—¶æ— éœ€å¤–éƒ¨éªŒè¯å™¨å³å¯å®ç°åˆ†æ•°çš„è‡ªæˆ‘æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSatori-SWE-32B æ¨¡å‹åœ¨ SWE-Bench-Verified åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…éœ€å°‘é‡æ ·æœ¬å³å¯è¾¾åˆ°æˆ–è¶…è¿‡å‚æ•°é‡è¶…è¿‡ 100B çš„æ¨¡å‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡ç®—æ³•åˆ›æ–°æå‡äº†æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„æ¨ç†æ•ˆç‡ï¼Œå¹¶æ‰¿è¯ºå°†ç›¸å…³ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å®Œå…¨å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23604v1",
      "published_date": "2025-05-29 16:15:36 UTC",
      "updated_date": "2025-05-29 16:15:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:55:19.343032+00:00"
    },
    {
      "arxiv_id": "2506.00069v1",
      "title": "Evaluating the Sensitivity of LLMs to Prior Context",
      "title_zh": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å¯¹å‰æ–‡è¯­å¢ƒçš„æ•æ„Ÿæ€§",
      "authors": [
        "Robert Hankache",
        "Kingsley Nketia Acheampong",
        "Liang Song",
        "Marek Brynda",
        "Raad Khraishi",
        "Greig A. Cowan"
      ],
      "abstract": "As large language models (LLMs) are increasingly deployed in multi-turn dialogue and other sustained interactive scenarios, it is essential to understand how extended context affects their performance. Popular benchmarks, focusing primarily on single-turn question answering (QA) tasks, fail to capture the effects of multi-turn exchanges. To address this gap, we introduce a novel set of benchmarks that systematically vary the volume and nature of prior context. We evaluate multiple conventional LLMs, including GPT, Claude, and Gemini, across these benchmarks to measure their sensitivity to contextual variations. Our findings reveal that LLM performance on multiple-choice questions can degrade dramatically in multi-turn interactions, with performance drops as large as 73% for certain models. Even highly capable models such as GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative performance of larger versus smaller models is not always predictable. Moreover, the strategic placement of the task description within the context can substantially mitigate performance drops, improving the accuracy by as much as a factor of 3.5. These findings underscore the need for robust strategies to design, evaluate, and mitigate context-related sensitivity in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè½®å¯¹è¯ç­‰æŒç»­äº¤äº’åœºæ™¯ä¸­å¯¹å…ˆå‰ä¸Šä¸‹æ–‡(Prior Context)çš„æ•æ„Ÿæ€§ï¼Œæ—¨åœ¨å¼¥è¡¥å½“å‰ä¸»æµè¯„æµ‹ä¸»è¦å…³æ³¨å•è½®é—®ç­”(QA)è€Œå¿½è§†é•¿ä¸Šä¸‹æ–‡å½±å“çš„ä¸è¶³ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€å¥—æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°æ”¹å˜å…ˆå‰ä¸Šä¸‹æ–‡çš„æ•°é‡å’Œæ€§è´¨ï¼Œè¯„ä¼°äº†åŒ…æ‹¬GPTã€Claudeå’ŒGeminiåœ¨å†…çš„å¤šç§ä¸»æµLLMsã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨å¤šè½®äº¤äº’ä¸­çš„å¤šé€‰é¢˜è¡¨ç°ä¼šå‰§çƒˆä¸‹é™ï¼Œéƒ¨åˆ†æ¨¡å‹çš„å‡†ç¡®ç‡é™å¹…é«˜è¾¾73%ï¼Œå³ä½¿æ˜¯GPT-4oç­‰é«˜æ€§èƒ½æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºé«˜è¾¾32%çš„æ€§èƒ½è¡°å‡ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºï¼Œå¤§æ¨¡å‹ä¸å°æ¨¡å‹ä¹‹é—´çš„ç›¸å¯¹è¡¨ç°å¹¶ä¸æ€»æ˜¯ç¬¦åˆé¢„æœŸã€‚æ­¤å¤–ï¼Œé€šè¿‡ç­–ç•¥æ€§åœ°è°ƒæ•´ä»»åŠ¡æè¿°åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ä½ç½®ï¼Œå¯ä»¥æ˜¾è‘—ç¼“è§£æ€§èƒ½ä¸‹é™ï¼Œä½¿å‡†ç¡®ç‡æœ€é«˜æå‡3.5å€ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨å¼€å‘å’Œè¯„ä¼°LLMsæ—¶ï¼Œå¿…é¡»é’ˆå¯¹ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§åˆ¶å®šæ›´å…·é²æ£’æ€§çš„ä¼˜åŒ–ç­–ç•¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00069v1",
      "published_date": "2025-05-29 16:09:32 UTC",
      "updated_date": "2025-05-29 16:09:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:55:39.154067+00:00"
    },
    {
      "arxiv_id": "2505.23596v3",
      "title": "Agent-SAMA: State-Aware Mobile Assistant",
      "title_zh": "Agent-SAMAï¼šçŠ¶æ€æ„ŸçŸ¥ç§»åŠ¨åŠ©æ‰‹",
      "authors": [
        "Linqiang Guo",
        "Wei Liu",
        "Yi Wen Heng",
        "Tse-Hsun",
        "Chen",
        "Yang Wang"
      ],
      "abstract": "Mobile Graphical User Interface (GUI) agents aim to autonomously complete tasks within or across apps based on user instructions. While recent Multimodal Large Language Models (MLLMs) enable these agents to interpret UI screens and perform actions, existing agents remain fundamentally reactive. They reason over the current UI screen but lack a structured representation of the app navigation flow, limiting GUI agents' ability to understand execution context, detect unexpected execution results, and recover from errors. We introduce Agent-SAMA, a state-aware multi-agent framework that models app execution as a Finite State Machine (FSM), treating UI screens as states and user actions as transitions. Agent-SAMA implements four specialized agents that collaboratively construct and use FSMs in real time to guide task planning, execution verification, and recovery. We evaluate Agent-SAMA on two types of benchmarks: cross-app (Mobile-Eval-E, SPA-Bench) and mostly single-app (AndroidWorld). On Mobile-Eval-E, Agent-SAMA achieves an 84.0% success rate and a 71.9% recovery rate. On SPA-Bench, it reaches an 80.0% success rate with a 66.7% recovery rate. Compared to prior methods, Agent-SAMA improves task success by up to 12% and recovery success by 13.8%. On AndroidWorld, Agent-SAMA achieves a 63.7% success rate, outperforming the baselines. Our results demonstrate that structured state modeling enhances robustness and can serve as a lightweight, model-agnostic memory layer for future GUI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Agent-SAMAï¼Œä¸€ç§çŠ¶æ€æ„ŸçŸ¥(State-Aware)çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)æ™ºèƒ½ä½“å› ç¼ºä¹åº”ç”¨å¯¼èˆªæµç»“æ„åŒ–è¡¨ç¤ºè€Œå¯¼è‡´çš„æ‰§è¡Œä¸Šä¸‹æ–‡ç†è§£ä¸è¶³å’Œé”™è¯¯æ¢å¤èƒ½åŠ›å·®ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†åº”ç”¨æ‰§è¡Œå»ºæ¨¡ä¸ºæœ‰é™çŠ¶æ€æœº(Finite State Machine, FSM)ï¼Œå°† UI å±å¹•å®šä¹‰ä¸ºçŠ¶æ€ï¼Œå°†ç”¨æˆ·åŠ¨ä½œå®šä¹‰ä¸ºçŠ¶æ€è½¬æ¢ã€‚é€šè¿‡å››ä¸ªä¸“é—¨æ™ºèƒ½ä½“çš„åä½œï¼ŒAgent-SAMA èƒ½å¤Ÿå®æ—¶æ„å»ºå¹¶åˆ©ç”¨ FSM æŒ‡å¯¼ä»»åŠ¡è§„åˆ’ã€æ‰§è¡ŒéªŒè¯åŠæ¢å¤è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ Mobile-Eval-E å’Œ SPA-Bench åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å–å¾—äº† 84.0% å’Œ 80.0% çš„æˆåŠŸç‡ï¼Œç›¸æ¯”å…ˆå‰æ–¹æ³•åœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šæå‡é«˜è¾¾ 12%ã€‚åœ¨ AndroidWorld æµ‹è¯•ä¸­ï¼ŒAgent-SAMA åŒæ ·ä»¥ 63.7% çš„æˆåŠŸç‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†ç»“æ„åŒ–çŠ¶æ€å»ºæ¨¡åœ¨æå‡æ™ºèƒ½ä½“é²æ£’æ€§æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¢å¼ºäº†ç³»ç»Ÿçš„å®¹é”™èƒ½åŠ›ï¼Œè¿˜ä¸ºæœªæ¥ GUI æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§è½»é‡çº§ä¸”ä¸æ¨¡å‹æ— å…³çš„è®°å¿†å±‚æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AAAI-26 (Main Technical Track)",
      "pdf_url": "https://arxiv.org/pdf/2505.23596v3",
      "published_date": "2025-05-29 16:08:51 UTC",
      "updated_date": "2025-11-19 16:48:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:57:22.628526+00:00"
    },
    {
      "arxiv_id": "2505.23595v1",
      "title": "DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification",
      "title_zh": "DeepChestï¼šç”¨äºèƒ¸éƒ¨ X å°„çº¿åˆ†ç±»é«˜æ•ˆå¤šä»»åŠ¡å­¦ä¹ çš„åŠ¨æ€æ— æ¢¯åº¦ä»»åŠ¡åŠ æƒæ–¹æ³•",
      "authors": [
        "Youssef Mohamed",
        "Noran Mohamed",
        "Khaled Abouhashad",
        "Feilong Tang",
        "Sara Atito",
        "Shoaib Jameel",
        "Imran Razzak",
        "Ahmed B. Zaky"
      ],
      "abstract": "While Multi-Task Learning (MTL) offers inherent advantages in complex domains such as medical imaging by enabling shared representation learning, effectively balancing task contributions remains a significant challenge. This paper addresses this critical issue by introducing DeepChest, a novel, computationally efficient and effective dynamic task-weighting framework specifically designed for multi-label chest X-ray (CXR) classification. Unlike existing heuristic or gradient-based methods that often incur substantial overhead, DeepChest leverages a performance-driven weighting mechanism based on effective analysis of task-specific loss trends. Given a network architecture (e.g., ResNet18), our model-agnostic approach adaptively adjusts task importance without requiring gradient access, thereby significantly reducing memory usage and achieving a threefold increase in training speed. It can be easily applied to improve various state-of-the-art methods. Extensive experiments on a large-scale CXR dataset demonstrate that DeepChest not only outperforms state-of-the-art MTL methods by 7% in overall accuracy but also yields substantial reductions in individual task losses, indicating improved generalization and effective mitigation of negative transfer. The efficiency and performance gains of DeepChest pave the way for more practical and robust deployment of deep learning in critical medical diagnostic applications. The code is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeepChestï¼Œä¸€ç§ä¸“ä¸ºå¤šæ ‡ç­¾èƒ¸éƒ¨ X å°„çº¿ (CXR) åˆ†ç±»è®¾è®¡çš„åŠ¨æ€ä»»åŠ¡åŠ æƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Multi-Task Learning (MTL) ä¸­å„ä»»åŠ¡è´¡çŒ®å¹³è¡¡å›°éš¾çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ä¼ ç»Ÿçš„å¯å‘å¼æˆ–åŸºäºæ¢¯åº¦çš„ç®—æ³•ç›¸æ¯”ï¼ŒDeepChest é‡‡ç”¨ä¸€ç§æ— éœ€æ¢¯åº¦è®¿é—®çš„æ€§èƒ½é©±åŠ¨åŠ æƒæœºåˆ¶ï¼Œé€šè¿‡åˆ†æç‰¹å®šä»»åŠ¡çš„æŸå¤±è¶‹åŠ¿æ¥åŠ¨æ€è°ƒæ•´ä»»åŠ¡æƒé‡ã€‚ä½œä¸ºä¸€ç§æ¨¡å‹æ— å…³ (model-agnostic) çš„æ–¹æ³•ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨çš„åŒæ—¶ï¼Œå°†è®­ç»ƒé€Ÿåº¦æå‡äº†ä¸‰å€ï¼Œå¯å¹¿æ³›åº”ç”¨äºå¤šç§å…ˆè¿›æ¨¡å‹çš„æ”¹è¿›ã€‚åœ¨å¤§è§„æ¨¡ CXR æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepChest çš„æ•´ä½“å‡†ç¡®ç‡æ¯”å½“å‰çš„ state-of-the-art æ–¹æ³•æé«˜äº† 7%ï¼Œå¹¶èƒ½æœ‰æ•ˆé™ä½å•ä¸ªä»»åŠ¡æŸå¤±ï¼Œç¼“è§£è´Ÿè¿ç§» (negative transfer) é—®é¢˜ã€‚è¯¥ç ”ç©¶åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šçš„çªç ´ï¼Œä¸ºæ·±åº¦å­¦ä¹ åœ¨å…³é”®åŒ»ç–—è¯Šæ–­åº”ç”¨ä¸­çš„å®é™…éƒ¨ç½²å¥ å®šäº†æ›´å…·ç¨³å¥æ€§çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23595v1",
      "published_date": "2025-05-29 16:08:26 UTC",
      "updated_date": "2025-05-29 16:08:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:57:28.110097+00:00"
    },
    {
      "arxiv_id": "2505.23590v3",
      "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles",
      "title_zh": "Jigsaw-R1ï¼šåŸºäºæ‹¼å›¾æ¸¸æˆçš„è§„åˆ™é©±åŠ¨è§†è§‰å¼ºåŒ–å­¦ä¹ ç ”ç©¶",
      "authors": [
        "Zifu Wang",
        "Junyi Zhu",
        "Bo Tang",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Jiaqian Yu",
        "Matthew B. Blaschko"
      ],
      "abstract": "The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: \\textit{Firstly,} we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \\textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \\textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \\textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \\textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºè§„åˆ™çš„è§†è§‰ Reinforcement Learning (RL) åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) ä¸­çš„åº”ç”¨ï¼Œå¹¶ä»¥æ‹¼å›¾æ¸¸æˆ (Jigsaw Puzzles) ä½œä¸ºå®éªŒæ¡†æ¶æå‡ºäº† Jigsaw-R1ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ MLLMs åˆå§‹è¡¨ç°æ¥è¿‘éšæœºï¼Œä½†é€šè¿‡å¾®è°ƒèƒ½åœ¨å¤æ‚æ‹¼å›¾ä»»åŠ¡ä¸­è¾¾åˆ°æé«˜å‡†ç¡®ç‡ï¼Œå¹¶å±•ç°å‡ºå‘å…¶ä»–è§†è§‰ä»»åŠ¡æ³›åŒ–çš„èƒ½åŠ›ã€‚å®éªŒæ­ç¤º MLLMs åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å€¾å‘äºç›´æ¥å›ç­”ï¼Œä¸”å¤æ‚çš„æ¨ç†æ¨¡å¼æ˜¯é¢„å…ˆå­˜åœ¨çš„è€Œééšè®­ç»ƒçªç°çš„ï¼Œå…¶å‡ºç°é¢‘ç‡éšä»»åŠ¡éš¾åº¦å¢åŠ è€Œæå‡ã€‚ç»“æœè¯æ˜ Reinforcement Learning åœ¨æ³›åŒ–æ€§ä¸Šæ˜¾è‘—ä¼˜äºæœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT)ï¼Œä¸”åˆå§‹çš„ SFT å†·å¯åŠ¨é˜¶æ®µå¯èƒ½ä¼šé˜»ç¢åç»­çš„ RL ä¼˜åŒ–è¿‡ç¨‹ã€‚è¯¥å·¥ä½œä¸ºç†è§£è§†è§‰ RL åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ½œåŠ›æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶ä¸ºå¤„ç†æ„ŸçŸ¥å¯†é›†å‹ä»»åŠ¡ä¸­çš„å†³ç­–é—®é¢˜è´¡çŒ®äº†å®è´µç»éªŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "TMLR 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23590v3",
      "published_date": "2025-05-29 16:01:22 UTC",
      "updated_date": "2025-10-11 06:03:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:57:46.256354+00:00"
    },
    {
      "arxiv_id": "2505.23584v1",
      "title": "Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging",
      "title_zh": "ååŒæœ«ç«¯é…é€ï¼šæ”¯æŒåœ¨é€”å……ç”µçš„å¤šå¹³å°è½¦è¾†è·¯å¾„é—®é¢˜",
      "authors": [
        "Sumbal Malik",
        "Majid Khonji",
        "Khaled Elbassioni",
        "Jorge Dias"
      ],
      "abstract": "The rapid growth of e-commerce and the increasing demand for timely, cost-effective last-mile delivery have increased interest in collaborative logistics. This research introduces a novel collaborative synchronized multi-platform vehicle routing problem with drones and robots (VRP-DR), where a fleet of $\\mathcal{M}$ trucks, $\\mathcal{N}$ drones and $\\mathcal{K}$ robots, cooperatively delivers parcels. Trucks serve as mobile platforms, enabling the launching, retrieving, and en-route charging of drones and robots, thereby addressing critical limitations such as restricted payload capacities, limited range, and battery constraints. The VRP-DR incorporates five realistic features: (1) multi-visit service per trip, (2) multi-trip operations, (3) flexible docking, allowing returns to the same or different trucks (4) cyclic and acyclic operations, enabling return to the same or different nodes; and (5) en-route charging, enabling drones and robots to recharge while being transported on the truck, maximizing operational efficiency by utilizing idle transit time. The VRP-DR is formulated as a mixed-integer linear program (MILP) to minimize both operational costs and makespan. To overcome the computational challenges of solving large-scale instances, a scalable heuristic algorithm, FINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to provide efficient, near-optimal solutions. Numerical experiments across various instance sizes evaluate the performance of the MILP and heuristic approaches in terms of solution quality and computation time. The results demonstrate significant time savings of the combined delivery mode over the truck-only mode and substantial cost reductions from enabling multi-visits. The study also provides insights into the effects of en-route charging, docking flexibility, drone count, speed, and payload capacity on system performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”µå­å•†åŠ¡èƒŒæ™¯ä¸‹çš„åä½œç‰©æµï¼Œæå‡ºäº†ä¸€ç§é›†æˆå¡è½¦ã€æ— äººæœº(drones)å’Œæœºå™¨äºº(robots)çš„åŒæ­¥å¤šå¹³å°è½¦è¾†è·¯å¾„è§„åˆ’é—®é¢˜(VRP-DR)ã€‚åœ¨è¯¥æ¨¡å‹ä¸­ï¼Œå¡è½¦ä½œä¸ºç§»åŠ¨å¹³å°æ”¯æŒæ— äººæœºå’Œæœºå™¨äººçš„å‘å°„ã€å›æ”¶åŠé€”ä¸­å……ç”µ(en-route charging)ï¼Œæœ‰æ•ˆè§£å†³äº†å°å‹è®¾å¤‡åœ¨è½½è·ã€èˆªç¨‹å’Œç”µæ± å®¹é‡æ–¹é¢çš„é™åˆ¶ã€‚ç ”ç©¶æ¶µç›–äº†å•æ¬¡è¡Œç¨‹å¤šæ¬¡è®¿é—®ã€å¤šè¡Œç¨‹ä½œä¸šã€çµæ´»åœé (flexible docking)ä»¥åŠåˆ©ç”¨è¿è¾“ç©ºé—²æ—¶é—´è¿›è¡Œå……ç”µç­‰äº”ä¸ªç°å®ç‰¹å¾ã€‚é€šè¿‡å°†VRP-DRæ„å»ºä¸ºæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’(MILP)æ¨¡å‹ï¼Œæ—¨åœ¨åŒæ—¶æœ€å°åŒ–è¿è¥æˆæœ¬å’Œå®Œå·¥æ—¶é—´(makespan)ã€‚é’ˆå¯¹å¤§è§„æ¨¡è®¡ç®—æŒ‘æˆ˜ï¼Œç ”ç©¶å¼€å‘äº†åä¸ºFINDERçš„å¯å‘å¼ç®—æ³•ï¼Œæ—¨åœ¨æä¾›é«˜æ•ˆçš„è¿‘ä¼¼æœ€ä¼˜è§£ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œè¿™ç§ååŒäº¤ä»˜æ¨¡å¼ç›¸æ¯”ä¼ ç»Ÿä»…å¡è½¦æ¨¡å¼èƒ½æ˜¾è‘—ç¼©çŸ­äº¤ä»˜æ—¶é—´ï¼Œä¸”æ”¯æŒå¤šæ¬¡è®¿é—®èƒ½å¤§å¹…é™ä½æˆæœ¬ã€‚è¯¥ç ”ç©¶é€šè¿‡å¯¹åœ¨é€”å……ç”µå’Œåœé çµæ´»æ€§ç­‰å› ç´ çš„æ·±å…¥åˆ†æï¼Œä¸ºä¼˜åŒ–æœ€åä¸€å…¬é‡Œäº¤ä»˜ç³»ç»Ÿæ€§èƒ½æä¾›äº†é‡è¦çš„ç†è®ºè§è§£ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23584v1",
      "published_date": "2025-05-29 15:58:01 UTC",
      "updated_date": "2025-05-29 15:58:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:57:32.595075+00:00"
    },
    {
      "arxiv_id": "2505.23580v1",
      "title": "Engineering Serendipity through Recommendations of Items with Atypical Aspects",
      "title_zh": "é€šè¿‡æ¨èå…·æœ‰éå…¸å‹ç‰¹å¾çš„é¡¹ç›®æ„å»ºæƒŠå–œæ„Ÿ",
      "authors": [
        "Ramit Aditya",
        "Razvan Bunescu",
        "Smita Nannaware",
        "Erfan Al-Hossami"
      ],
      "abstract": "A restaurant dinner or a hotel stay may lead to memorable experiences when guests encounter unexpected aspects that also match their interests. For example, an origami-making station in the waiting area of a restaurant may be both surprising and enjoyable for a customer who is passionate about paper crafts. Similarly, an exhibit of 18th century harpsichords would be atypical for a hotel lobby and likely pique the interest of a guest who has a passion for Baroque music. Motivated by this insight, in this paper we introduce the new task of engineering serendipity through recommendations of items with atypical aspects. We describe an LLM-based system pipeline that extracts atypical aspects from item reviews, then estimates and aggregates their user-specific utility in a measure of serendipity potential that is used to rerank a list of items recommended to the user. To facilitate system development and evaluation, we introduce a dataset of Yelp reviews that are manually annotated with atypical aspects and a dataset of artificially generated user profiles, together with crowdsourced annotations of user-aspect utility values. Furthermore, we introduce a custom procedure for dynamic selection of in-context learning examples, which is shown to improve LLM-based judgments of atypicality and utility. Experimental evaluations show that serendipity-based rankings generated by the system are highly correlated with ground truth rankings for which serendipity scores are computed from manual annotations of atypical aspects and their user-dependent utility. Overall, we hope that the new recommendation task and the associated system presented in this paper catalyze further research into recommendation approaches that go beyond accuracy in their pursuit of enhanced user satisfaction.\n  The datasets and the code are made publicly available at https://github.com/ramituncc49er/ATARS .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é€šè¿‡æ¨èå…·æœ‰éå…¸å‹ç‰¹å¾(atypical aspects)çš„é¡¹ç›®æ¥è®¾è®¡æ„å¤–ä¹‹å–œ(serendipity)çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨è¶…è¶Šä¼ ç»Ÿçš„æ¨èå‡†ç¡®æ€§æŒ‡æ ‡ä»¥æå‡ç”¨æˆ·æ»¡æ„åº¦ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„ç³»ç»Ÿæµæ°´çº¿ï¼Œè¯¥æµæ°´çº¿èƒ½å¤Ÿä»é¡¹ç›®è¯„è®ºä¸­è‡ªåŠ¨æå–éå…¸å‹ç‰¹å¾ï¼Œå¹¶è¯„ä¼°å…¶å¯¹ç‰¹å®šç”¨æˆ·çš„æ•ˆç”¨(user-specific utility)ï¼Œè¿›è€Œé€šè¿‡è®¡ç®—â€œæ„å¤–ä¹‹å–œæ½œåŠ›â€(serendipity potential)å¯¹æ¨èåˆ—è¡¨è¿›è¡Œé‡æ’åºã€‚ä¸ºäº†ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§åŠ¨æ€é€‰æ‹©ä¸Šä¸‹æ–‡å­¦ä¹ (in-context learning)ç¤ºä¾‹çš„å®šåˆ¶ç¨‹åºï¼Œæ˜¾è‘—æé«˜äº†LLMå¯¹éå…¸å‹æ€§å’Œæ•ˆç”¨ä»·å€¼åˆ¤æ–­çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜è´¡çŒ®äº†ä¸€ä¸ªç»è¿‡äººå·¥æ ‡æ³¨éå…¸å‹ç‰¹å¾çš„Yelpè¯„è®ºæ•°æ®é›†ï¼Œä»¥åŠåŒ…å«ç”¨æˆ·é…ç½®æ–‡ä»¶å’Œä¼—åŒ…æ•ˆç”¨è¯„ä¼°çš„å®éªŒæ•°æ®ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿç”Ÿæˆçš„é‡æ’åºç»“æœä¸åŸºäºäººå·¥æ ‡æ³¨è®¡ç®—çš„çœŸå®æ’åé«˜åº¦ç›¸å…³ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹ç ”ç©¶é€šè¿‡å…¬å¼€ä»£ç å’Œæ•°æ®é›†ï¼Œä¸ºæœªæ¥æ¢ç´¢è¶…è¶Šå‡†ç¡®æ€§çš„å¤šæ ·åŒ–æ¨èç®—æ³•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "25 pages of content + references and appendix. arXiv admin note: text overlap with arXiv:2311.02702",
      "pdf_url": "https://arxiv.org/pdf/2505.23580v1",
      "published_date": "2025-05-29 15:53:21 UTC",
      "updated_date": "2025-05-29 15:53:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:57:36.254356+00:00"
    },
    {
      "arxiv_id": "2505.23576v2",
      "title": "Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms",
      "title_zh": "è‡ªä¸»æ— äººæœºé›†ç¾¤å¼€æ”¾ä¸–ç•Œå†³ç­–çš„è®¤çŸ¥æŠ¤æ ",
      "authors": [
        "Jane Cleland-Huang",
        "Pedro Antonio Alarcon Granadeno",
        "Arturo Miguel Russell Bernal",
        "Demetrius Hernandez",
        "Michael Murphy",
        "Maureen Petterson",
        "Walter Scheirer"
      ],
      "abstract": "Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous swarms in search-and-rescue and other disaster-response scenarios. In these settings, they use computer vision (CV) to detect objects of interest and autonomously adapt their missions. However, traditional CV systems often struggle to recognize unfamiliar objects in open-world environments or to infer their relevance for mission planning. To address this, we incorporate large language models (LLMs) to reason about detected objects and their implications. While LLMs can offer valuable insights, they are also prone to hallucinations and may produce incorrect, misleading, or unsafe recommendations. To ensure safe and sensible decision-making under uncertainty, high-level decisions must be governed by cognitive guardrails. This article presents the design, simulation, and real-world integration of these guardrails for sUAS swarms in search-and-rescue missions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°å‹æ— äººæœºç³»ç»Ÿ (sUAS) è‡ªä¸»é›†ç¾¤åœ¨æœæ•‘å’Œç¾éš¾å“åº”ä»»åŠ¡ä¸­é¢ä¸´çš„å¼€æ”¾ä¸–ç•Œ (open-world) å†³ç­–æŒ‘æˆ˜ï¼Œæå‡ºäº†è®¤çŸ¥æŠ¤æ  (Cognitive Guardrails) æœºåˆ¶ã€‚ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰ (CV) ç³»ç»Ÿåœ¨è¯†åˆ«é™Œç”Ÿç›®æ ‡æˆ–æ¨æ–­å…¶ä»»åŠ¡ç›¸å…³æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸ºæ­¤ç ”ç©¶å¼•å…¥äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯¹æ£€æµ‹åˆ°çš„ç›®æ ‡åŠå…¶æ½œåœ¨å«ä¹‰è¿›è¡Œæ¨ç†ã€‚é’ˆå¯¹ LLMs å¯èƒ½äº§ç”Ÿçš„å¹»è§‰ (hallucinations) åŠå…¶å¼•å‘çš„å®‰å…¨é£é™©ï¼Œè¯¥æ¡†æ¶é€šè¿‡é«˜å±‚çº§çš„è®¤çŸ¥æŠ¤æ å¯¹å†³ç­–è¡Œä¸ºè¿›è¡Œçº¦æŸï¼Œä»¥ç¡®ä¿ç³»ç»Ÿåœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çš„å®‰å…¨æ€§ã€‚æ–‡ä¸­é€šè¿‡è®¾è®¡ã€ä»¿çœŸä»¥åŠçœŸå®ä¸–ç•Œçš„é›†æˆåº”ç”¨ï¼ŒéªŒè¯äº†è¯¥æŠ¤æ æœºåˆ¶åœ¨æœæ•‘ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ¡ˆä¸ºæå‡è‡ªä¸»ç³»ç»Ÿåœ¨å¤æ‚åŠ¨æ€åœºæ™¯ä¸‹çš„å†³ç­–åˆç†æ€§å’Œå¯ä¿¡åº¦å¥ å®šäº†é‡è¦æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "16 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23576v2",
      "published_date": "2025-05-29 15:47:49 UTC",
      "updated_date": "2025-06-01 06:27:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:58:14.443991+00:00"
    },
    {
      "arxiv_id": "2505.23575v3",
      "title": "CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring",
      "title_zh": "CoT æŠ“ä¸ªæ­£ç€ï¼šæ€ç»´é“¾ç›‘æ§çš„å‹åŠ›æµ‹è¯•",
      "authors": [
        "Benjamin Arnav",
        "Pablo Bernabeu-PÃ©rez",
        "Nathan Helm-Burger",
        "Tim Kostolansky",
        "Hannes Whittingham",
        "Mary Phuong"
      ],
      "abstract": "As AI models are deployed with increasing autonomy, it is important to ensure they do not take harmful actions unnoticed. As a potential mitigation, we investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor model continuously oversees the intermediate reasoning steps of a more powerful but untrusted model. We compare CoT monitoring to action-only monitoring, where only final outputs are reviewed, in a red-teaming setup where the untrusted model is instructed to pursue harmful side tasks while completing a coding problem. We find that while CoT monitoring is more effective than overseeing only model outputs in scenarios where action-only monitoring fails to reliably identify sabotage, reasoning traces can contain misleading rationalizations that deceive the CoT monitors, reducing performance in obvious sabotage cases. To address this, we introduce a hybrid protocol that independently scores model reasoning and actions, and combines them using a weighted average. Our hybrid monitor consistently outperforms both CoT and action-only monitors across all tested models and tasks, with detection rates twice higher than action-only monitoring for subtle deception scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†éšç€äººå·¥æ™ºèƒ½æ¨¡å‹è‡ªä¸»æ€§æ—¥ç›Šå¢å¼ºï¼Œå¦‚ä½•é€šè¿‡ç›‘æµ‹ä¸­é—´æ¨ç†æ­¥éª¤æ¥é˜²æ­¢å…¶æ‰§è¡Œæœ‰å®³æ“ä½œã€‚ä½œè€…åœ¨çº¢é˜Ÿæµ‹è¯•(red-teaming)åœºæ™¯ä¸‹å¯¹æ¯”äº†é“¾å¼æ€ç»´ç›‘æµ‹(Chain-of-Thought monitoring)ä¸ä»…é’ˆå¯¹æœ€ç»ˆè¾“å‡ºçš„Action-only monitoringã€‚å®éªŒå‘ç°CoTç›‘æµ‹åœ¨è¯†åˆ«ç‰¹å®šç ´åè¡Œä¸ºæ—¶è™½æ›´å…·ä¼˜åŠ¿ï¼Œä½†æ¨¡å‹æ¨ç†è½¨è¿¹ä¸­å¯èƒ½åŒ…å«è¯¯å¯¼æ€§çš„åˆç†åŒ–æè¿°ä»¥æ¬ºéª—ç›‘æµ‹å™¨ã€‚é’ˆå¯¹è¿™ä¸€æ¼æ´ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆåè®®(hybrid protocol)ï¼Œé€šè¿‡ç‹¬ç«‹è¯„ä¼°æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸å®é™…åŠ¨ä½œå¹¶é‡‡ç”¨åŠ æƒå¹³å‡æ³•è¿›è¡Œç»¼åˆè¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ··åˆç›‘æµ‹å™¨åœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å¤„ç†å¾®å¦™æ¬ºéª—æ—¶ï¼Œå…¶æ£€æµ‹ç‡æ¯”Action-only monitoringé«˜å‡ºä¸€å€ã€‚è¯¥ç ”ç©¶ä¸ºæå‡è‡ªä¸»ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„å®‰å…¨æ€§ä¸å¯ä¿¡åº¦æä¾›äº†é‡è¦çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "To be published in the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.23575v3",
      "published_date": "2025-05-29 15:47:36 UTC",
      "updated_date": "2025-11-24 22:11:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:57:54.343780+00:00"
    },
    {
      "arxiv_id": "2505.23878v1",
      "title": "Actor-Critic based Online Data Mixing For Language Model Pre-Training",
      "title_zh": "åŸºäº Actor-Critic çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒåœ¨çº¿æ•°æ®é…æ¯”",
      "authors": [
        "Jing Ma",
        "Chenhao Dang",
        "Mingjie Liao"
      ],
      "abstract": "The coverage and composition of pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). To reduce the carbon footprint and financial costs of training, some data mixing methods, which applied the optimized domain weights of a small proxy model to train a larger one, were proposed. However, these methods did not evolute with the training dynamics. The existing online data mixing (ODM) method addressed this limitation by applying the multi-armed bandit algorithm as data sampling strategy. Yet, it did not consider the intra-domain interactions. In this paper, we develop an actor-critic based online data mixing (AC-ODM) method, which captures the varying domain weights by auxiliary actor-critic networks and consider the intra-domain interactions with the reward function. While constructing the dataset to pretrain a large target LLM, we directly apply the actor, which is trained with a small proxy LLM as the environment, as the sampling strategy. The transfer of sampling strategy can not only ensure the efficiency of dynamical data mixing, but also expedite the convergence of pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M, which invokes the sampling strategy obtained by a proxy LLM with 410M parameters, reaching the optimal validation perplexity of ODM 71% faster, and improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy, about 2.23x better on pass@1 of HumanEval benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„è®­ç»ƒæ•°æ®ç»„æˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ³›åŒ–èƒ½åŠ›çš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Actor-Critic çš„åœ¨çº¿æ•°æ®æ··åˆæ–¹æ³•(AC-ODM)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¾…åŠ©çš„ Actor-Critic ç½‘ç»œæ•æ‰åŠ¨æ€å˜åŒ–çš„é¢†åŸŸæƒé‡ï¼Œå¹¶é€šè¿‡å¥–åŠ±å‡½æ•°æ˜¾å¼è€ƒè™‘é¢†åŸŸå†…çš„ç›¸äº’ä½œç”¨ï¼Œå¼¥è¡¥äº†ç°æœ‰æ–¹æ³•æ— æ³•éšè®­ç»ƒçŠ¶æ€æ¼”è¿›æˆ–å¿½ç•¥é¢†åŸŸäº¤äº’çš„ä¸è¶³ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒAC-ODM å°†åœ¨å°å‹ä»£ç†æ¨¡å‹ä¸Šè®­ç»ƒå¾—åˆ°çš„é‡‡æ ·ç­–ç•¥ç›´æ¥è¿ç§»è‡³ç›®æ ‡ LLM çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä»è€Œåœ¨ä¿è¯æ•°æ®æ··åˆæ•ˆç‡çš„åŒæ—¶å¤§å¹…åŠ é€Ÿæ¨¡å‹æ”¶æ•›ã€‚æ•°å€¼å®éªŒè¯æ˜ï¼ŒAC-ODM åœ¨è¾¾åˆ°æœ€ä¼˜éªŒè¯å›°æƒ‘åº¦(Perplexity)çš„é€Ÿåº¦ä¸Šæ¯”ä¼ ç»Ÿåœ¨çº¿æ•°æ®æ··åˆ(ODM)æ–¹æ³•å¿« 71%ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é›¶æ ·æœ¬ MMLU å’Œ HumanEval åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œå…¶ä¸­ MMLU å‡†ç¡®ç‡æé«˜ 27.5%ï¼ŒHumanEval çš„ pass@1 æŒ‡æ ‡æå‡çº¦ 2.23 å€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23878v1",
      "published_date": "2025-05-29 15:41:35 UTC",
      "updated_date": "2025-05-29 15:41:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:57:53.010304+00:00"
    },
    {
      "arxiv_id": "2505.23564v2",
      "title": "Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models",
      "title_zh": "Segment Policy Optimizationï¼šå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­æœ‰æ•ˆçš„åˆ†æ®µçº§ä¿¡åº¦åˆ†é…",
      "authors": [
        "Yiran Guo",
        "Lijie Xu",
        "Jie Liu",
        "Dan Ye",
        "Shuang Qiu"
      ],
      "abstract": "Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: token-level methods (e.g., PPO) aim to provide fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving $6$-$12$ percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving $7$-$11$ percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at https://github.com/AIFrameResearch/SPO.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Segment Policy Optimization (SPO)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¼ºåŒ–å­¦ä¹ çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³token-levelæ–¹æ³•ï¼ˆå¦‚PPOï¼‰å› criticæ¨¡å‹è®­ç»ƒéš¾å¯¼è‡´çš„ä¼°è®¡ä¸å‡†ï¼Œä»¥åŠtrajectory-levelæ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ä¿¡åº¦åˆ†é…(credit assignment)è¿‡äºç²—ç³™çš„é—®é¢˜ã€‚SPOé‡‡ç”¨ä¸­ç­‰ç²’åº¦çš„åˆ†æ®µä¼˜åŠ¿ä¼°è®¡(segment-level advantage estimation)ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›(Monte Carlo)é‡‡æ ·åœ¨æ— éœ€criticæ¨¡å‹çš„æƒ…å†µä¸‹å®ç°æ›´å¹³è¡¡ä¸”ç²¾ç¡®çš„ä¿¡åº¦åˆ†é…ã€‚è¯¥æ¡†æ¶åŒ…å«çµæ´»çš„åˆ†æ®µåˆ’åˆ†ã€ç²¾ç¡®çš„æ®µä¼˜åŠ¿ä¼°è®¡ä»¥åŠå¸¦æœ‰æ¦‚ç‡æ©ç ç­–ç•¥(probability-mask strategy)çš„ç­–ç•¥ä¼˜åŒ–ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚å®éªŒè¡¨æ˜ï¼Œé’ˆå¯¹çŸ­æ€ç»´é“¾(CoT)è®¾è®¡çš„SPO-chainåœ¨GSM8Kä»»åŠ¡ä¸Šæ¯”PPOå’ŒGRPOå‡†ç¡®ç‡æå‡äº†6-12ä¸ªç™¾åˆ†ç‚¹ã€‚è€Œé’ˆå¯¹é•¿æ€ç»´é“¾è®¾è®¡çš„SPO-treeé€šè¿‡æ ‘çŠ¶ä¼˜åŠ¿ä¼°è®¡æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œåœ¨MATH500ä»»åŠ¡ä¸­æ¯”GRPOæå‡äº†7-11ä¸ªç™¾åˆ†ç‚¹ã€‚ç›®å‰è¯¥ç ”ç©¶çš„ä»£ç å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23564v2",
      "published_date": "2025-05-29 15:38:19 UTC",
      "updated_date": "2025-10-21 07:05:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:58:18.208647+00:00"
    },
    {
      "arxiv_id": "2505.23559v1",
      "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
      "title_zh": "SafeScientistï¼šè¿ˆå‘ç”±å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“é©±åŠ¨çš„é£é™©æ„ŸçŸ¥å‹ç§‘å­¦å‘ç°",
      "authors": [
        "Kunlun Zhu",
        "Jiaxun Zhang",
        "Ziheng Qi",
        "Nuoxing Shang",
        "Zijia Liu",
        "Peixuan Han",
        "Yue Su",
        "Haofei Yu",
        "Jiaxuan You"
      ],
      "abstract": "Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce \\textbf{SafeScientist}, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. \\textcolor{red}{Warning: this paper contains example data that may be offensive or harmful.}",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SafeScientistï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºAIé©±åŠ¨ç§‘å­¦æ¢ç´¢ä¸­å®‰å…¨æ€§å’Œä¼¦ç†è´£ä»»çš„åˆ›æ–°AIç§‘å­¦å®¶æ¡†æ¶ã€‚SafeScientisté€šè¿‡ä¸»åŠ¨æ‹’ç»ä¼¦ç†ä¸å½“æˆ–é«˜é£é™©ä»»åŠ¡ï¼Œå¹¶åœ¨ç ”ç©¶å…¨è¿‡ç¨‹ä¸­æ•´åˆæç¤ºç›‘æ§ (prompt monitoring)ã€æ™ºèƒ½ä½“åä½œç›‘æ§ (agent-collaboration monitoring)ã€å·¥å…·ä½¿ç”¨ç›‘æ§ (tool-use monitoring) å’Œä¼¦ç†å®¡æŸ¥ (ethical reviewer) ç­‰å¤šç§é˜²å¾¡æœºåˆ¶æ¥ç¡®ä¿å®‰å…¨ã€‚ä¸ºè¯„ä¼°è¯¥æ¡†æ¶ï¼Œç ”ç©¶è€…åŒæ­¥æå‡ºäº†SciSafetyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–6ä¸ªé¢†åŸŸã€240é¡¹é«˜é£é™©ç§‘å­¦ä»»åŠ¡åŠç›¸å…³å·¥å…·é£é™©çš„å…¨æ–°åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»ŸAIç§‘å­¦å®¶æ¡†æ¶ç›¸æ¯”ï¼ŒSafeScientiståœ¨ä¸é™ä½ç§‘å­¦äº§å‡ºè´¨é‡çš„å‰æä¸‹ï¼Œå°†å®‰å…¨æ€§èƒ½æå‡äº†35%ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é€šè¿‡å¤šç§å¯¹æŠ—æ€§æ”»å‡» (adversarial attack) éªŒè¯äº†è¯¥å®‰å…¨ç®¡çº¿çš„é²æ£’æ€§ï¼Œä¸ºæ„å»ºè´Ÿè´£ä»»çš„è‡ªä¸»ç§‘å­¦å‘ç°ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23559v1",
      "published_date": "2025-05-29 15:35:58 UTC",
      "updated_date": "2025-05-29 15:35:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:58:03.153790+00:00"
    },
    {
      "arxiv_id": "2505.23554v1",
      "title": "Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters",
      "title_zh": "åœ°ç†åˆ†å¸ƒå¼äº‘æ•°æ®ä¸­å¿ƒä¸­å…¼é¡¾ç¢³æ„ŸçŸ¥ä¸èŠ‚æ°´çš„å¤§è¯­è¨€æ¨¡å‹å¯æŒç»­è°ƒåº¦",
      "authors": [
        "Hayden Moore",
        "Sirui Qi",
        "Ninad Hogade",
        "Dejan Milojicic",
        "Cullen Bash",
        "Sudeep Pasricha"
      ],
      "abstract": "In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and Gemini have been widely adopted in different areas. As the use of LLMs continues to grow, many efforts have focused on reducing the massive training overheads of these models. But it is the environmental impact of handling user requests to LLMs that is increasingly becoming a concern. Recent studies estimate that the costs of operating LLMs in their inference phase can exceed training costs by 25x per year. As LLMs are queried incessantly, the cumulative carbon footprint for the operational phase has been shown to far exceed the footprint during the training phase. Further, estimates indicate that 500 ml of fresh water is expended for every 20-50 requests to LLMs during inference. To address these important sustainability issues with LLMs, we propose a novel framework called SLIT to co-optimize LLM quality of service (time-to-first token), carbon emissions, water usage, and energy costs. The framework utilizes a machine learning (ML) based metaheuristic to enhance the sustainability of LLM hosting across geo-distributed cloud datacenters. Such a framework will become increasingly vital as LLMs proliferate.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLM) æ¨ç†é˜¶æ®µäº§ç”Ÿçš„å·¨å¤§ç¢³è¶³è¿¹å’Œæ°´èµ„æºæ¶ˆè€—é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º SLIT çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨åœ°ç†åˆ†å¸ƒå¼äº‘æ•°æ®ä¸­å¿ƒ (geo-distributed cloud datacenters) ä¸­å®ç°å¯æŒç»­çš„æ¨¡å‹è°ƒåº¦ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºæœºå™¨å­¦ä¹  (ML) çš„å…ƒå¯å‘å¼ç®—æ³• (metaheuristic)ï¼ŒååŒä¼˜åŒ– LLM çš„æœåŠ¡è´¨é‡ (QoS)ã€ç¢³æ’æ”¾ã€ç”¨æ°´é‡å’Œèƒ½æºæˆæœ¬ï¼Œå¹¶é‡ç‚¹å…³æ³¨é¦–è¯å“åº”æ—¶é—´ (time-to-first token) çš„ä¼˜åŒ–ã€‚ç ”ç©¶å‘ç° LLM æ¨ç†é˜¶æ®µçš„å¹´åº¦è¿è¥æˆæœ¬è¿œè¶…è®­ç»ƒé˜¶æ®µï¼Œä¸”æ¯20-50æ¬¡è¯·æ±‚å³æ¶ˆè€—çº¦500æ¯«å‡æ·¡æ°´ï¼Œå› æ­¤ SLIT çš„æå‡ºå¯¹äºç¯å¢ƒå¯æŒç»­æ€§è‡³å…³é‡è¦ã€‚é€šè¿‡å¯¹å…¨çƒåˆ†å¸ƒèµ„æºçš„æ™ºèƒ½ç®¡ç†ï¼ŒSLIT èƒ½å¤Ÿæœ‰æ•ˆé™ä½ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å¤§è§„æ¨¡éƒ¨ç½²ä¸­çš„ç”Ÿæ€è´Ÿæ‹…ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥ LLM åŸºç¡€è®¾æ–½çš„ç»¿è‰²è½¬å‹æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23554v1",
      "published_date": "2025-05-29 15:31:28 UTC",
      "updated_date": "2025-05-29 15:31:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:58:41.430757+00:00"
    },
    {
      "arxiv_id": "2506.06322v1",
      "title": "Neural networks with image recognition by pairs",
      "title_zh": "åŸºäºæˆå¯¹å›¾åƒè¯†åˆ«çš„ç¥ç»ç½‘ç»œ",
      "authors": [
        "Polad Geidarov"
      ],
      "abstract": "Neural networks based on metric recognition methods have a strictly determined architecture. Number of neurons, connections, as well as weights and thresholds values are calculated analytically, based on the initial conditions of tasks: number of recognizable classes, number of samples, metric expressions used. This paper discusses the possibility of transforming these networks in order to apply classical learning algorithms to them without using analytical expressions that calculate weight values. In the received network, training is carried out by recognizing images in pairs. This approach simplifies the learning process and easily allows to expand the neural network by adding new images to the recognition task. The advantages of these networks, including such as: 1) network architecture simplicity and transparency; 2) training simplicity and reliability; 3) the possibility of using a large number of images in the recognition problem using a neural network; 4) a consistent increase in the number of recognizable classes without changing the previous values of weights and thresholds.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºåº¦é‡è¯†åˆ«æ–¹æ³•(metric recognition methods)çš„ç¥ç»ç½‘ç»œï¼Œå¹¶æå‡ºäº†ä¸€ç§å°†å…¶è½¬åŒ–ä¸ºå¯åº”ç”¨ç»å…¸å­¦ä¹ ç®—æ³•çš„æ–°å‹æ¶æ„ï¼Œä»è€Œæ‘†è„±äº†å¯¹æƒé‡è®¡ç®—è§£æè¡¨è¾¾å¼çš„ä¾èµ–ã€‚åœ¨æ”¹è¿›åçš„ç½‘ç»œä¸­ï¼Œè®­ç»ƒè¿‡ç¨‹é€šè¿‡æˆå¯¹è¯†åˆ«å›¾åƒ(recognizing images in pairs)æ¥å®Œæˆï¼Œè¿™æå¤§åœ°ç®€åŒ–äº†å­¦ä¹ æµç¨‹å¹¶å¢å¼ºäº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ã€‚è¯¥æ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿åœ¨äºå…è®¸é€šè¿‡ç®€å•æ·»åŠ æ–°å›¾åƒæ¥æ‰©å±•è¯†åˆ«ä»»åŠ¡ï¼Œä¸”åœ¨å¢åŠ æ–°ç±»åˆ«æ—¶æ— éœ€æ›´æ”¹å·²æœ‰çš„æƒé‡å’Œé˜ˆå€¼ã€‚æ­¤å¤–ï¼Œè¯¥ç½‘ç»œæ¶æ„å…·æœ‰ç®€å•é€æ˜ã€è®­ç»ƒå¯é ã€æ”¯æŒå¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä»»åŠ¡ç­‰ç‰¹ç‚¹ã€‚è¿™ç§æˆå¯¹è¯†åˆ«çš„æœºåˆ¶ä¸ºæ„å»ºå¯æŒç»­æ‰©å±•ä¸”è®­ç»ƒé«˜æ•ˆçš„ç¥ç»ç½‘ç»œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06322v1",
      "published_date": "2025-05-29 15:20:14 UTC",
      "updated_date": "2025-05-29 15:20:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:58:43.938987+00:00"
    },
    {
      "arxiv_id": "2505.23538v1",
      "title": "CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification",
      "title_zh": "CLaC å‚åŠ  SemEval-2025 ä»»åŠ¡ 6ï¼šä¸€ç§ç”¨äºä¼ä¸šç¯å¢ƒæ‰¿è¯ºéªŒè¯çš„å¤šæ¶æ„æ–¹æ³•",
      "authors": [
        "Nawar Turk",
        "Eeham Khan",
        "Leila Kosseim"
      ],
      "abstract": "This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval), which focuses on verifying promises in corporate ESG (Environmental, Social, and Governance) reports. We explore three model architectures to address the four subtasks of promise identification, supporting evidence assessment, clarity evaluation, and verification timing. Our first model utilizes ESG-BERT with task-specific classifier heads, while our second model enhances this architecture with linguistic features tailored for each subtask. Our third approach implements a combined subtask model with attention-based sequence pooling, transformer representations augmented with document metadata, and multi-objective learning. Experiments on the English portion of the ML-Promise dataset demonstrate progressive improvement across our models, with our combined subtask approach achieving a leaderboard score of 0.5268, outperforming the provided baseline of 0.5227. Our work highlights the effectiveness of linguistic feature extraction, attention pooling, and multi-objective learning in promise verification tasks, despite challenges posed by class imbalance and limited training data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ SemEval-2025 Task 6 (PromiseEval) æå‡ºäº†å¤šç§æ¶æ„æ–¹æ³•ï¼Œæ—¨åœ¨éªŒè¯ä¼ä¸š ESG (Environmental, Social, and Governance) æŠ¥å‘Šä¸­çš„ç¯å¢ƒæ‰¿è¯ºã€‚ç ”ç©¶è€…æ¢ç´¢äº†ä¸‰ç§æ¨¡å‹æ¶æ„æ¥å¤„ç†æ‰¿è¯ºè¯†åˆ« (promise identification)ã€æ”¯æŒè¯æ®è¯„ä¼°ã€æ¸…æ™°åº¦è¯„ä»·å’ŒéªŒè¯æ—¶æ•ˆå››ä¸ªå­ä»»åŠ¡ã€‚å‰ä¸¤ç§æ¨¡å‹åˆ†åˆ«åŸºäº ESG-BERT åŠå…¶å¢å¼ºç‰ˆæœ¬ï¼Œè€Œç¬¬ä¸‰ç§æ–¹æ¡ˆåˆ™å®ç°äº†ç»“åˆ attention-based sequence poolingã€æ–‡æ¡£å…ƒæ•°æ®å¢å¼ºçš„ transformer è¡¨ç¤ºä»¥åŠ multi-objective learning çš„ç»¼åˆæ¨¡å‹ã€‚åœ¨ ML-Promise æ•°æ®é›†çš„å®éªŒä¸­ï¼Œç»¼åˆæ¶æ„æ¨¡å‹å–å¾—äº† 0.5268 çš„æ’è¡Œæ¦œå¾—åˆ†ï¼ŒæˆåŠŸè¶…è¶Šäº†å®˜æ–¹åŸºå‡†çº¿ã€‚ç ”ç©¶ç»“æœè¯æ˜äº† linguistic feature extractionã€attention pooling å’Œ multi-objective learning åœ¨å¤„ç†æ‰¿è¯ºéªŒè¯ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæœ‰æ•ˆåº”å¯¹äº†ç±»åˆ«ä¸å¹³è¡¡å’Œè®­ç»ƒæ•°æ®æœ‰é™ç­‰æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to SemEval-2025 Task 6 (ACL 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.23538v1",
      "published_date": "2025-05-29 15:19:00 UTC",
      "updated_date": "2025-05-29 15:19:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:10.340821+00:00"
    },
    {
      "arxiv_id": "2506.00068v2",
      "title": "Framing Political Bias in Multilingual LLMs Across Pakistani Languages",
      "title_zh": "è·¨ Pakistani è¯­è¨€çš„å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹æ”¿æ²»åè§æ¡†æ¶åˆ†æ",
      "authors": [
        "Afrozah Nadeem",
        "Mark Dras",
        "Usman Naseem"
      ],
      "abstract": "Large Language Models (LLMs) increasingly shape public discourse, yet most evaluations of political and economic bias have focused on high-resource, Western languages and contexts. This leaves critical blind spots in low-resource, multilingual regions such as Pakistan, where linguistic identity is closely tied to political, religious, and regional ideologies. We present a systematic evaluation of political bias in 13 state-of-the-art LLMs across five Pakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. Our framework integrates a culturally adapted Political Compass Test (PCT) with multi-level framing analysis, capturing both ideological stance (economic/social axes) and stylistic framing (content, tone, emphasis). Prompts are aligned with 11 socio-political themes specific to the Pakistani context. Results show that while LLMs predominantly reflect liberal-left orientations consistent with Western training data, they exhibit more authoritarian framing in regional languages, highlighting language-conditioned ideological modulation. We also identify consistent model-specific bias patterns across languages. These findings show the need for culturally grounded, multilingual bias auditing frameworks in global NLP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·´åŸºæ–¯å¦äº”ç§è¯­è¨€ï¼ˆUrdu, Punjabi, Sindhi, Pashto, Balochiï¼‰ç³»ç»Ÿè¯„ä¼°äº†13ç§ä¸»æµå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ”¿æ²»åè§ã€‚ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªæ•´åˆæ–‡åŒ–é€‚åº”æ€§æ”¿æ²»ç½—ç›˜æµ‹è¯•(Political Compass Test, PCT)ä¸å¤šå±‚çº§æ¡†æ¶åˆ†æçš„è¯„ä»·ä½“ç³»ï¼Œæ¶µç›–äº†11ä¸ªå·´åŸºæ–¯å¦ç‰¹å®šçš„ç¤¾ä¼šæ”¿æ²»ä¸»é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶LLMsæ™®éå‘ˆç°å‡ºä¸è¥¿æ–¹è®­ç»ƒæ•°æ®ä¸€è‡´çš„è‡ªç”±å·¦ç¿¼å€¾å‘ï¼Œä½†åœ¨ä½¿ç”¨åŒºåŸŸè¯­è¨€æ—¶å´è¡¨ç°å‡ºæ›´æ˜æ˜¾çš„å¨æƒä¸»ä¹‰(authoritarian)æ¡†æ¶ï¼Œä½“ç°äº†ç”±è¯­è¨€æ¡ä»¶è§¦å‘çš„æ„è¯†å½¢æ€è°ƒèŠ‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯†åˆ«å‡ºäº†è·¨è¯­è¨€çš„ä¸€è‡´æ€§æ¨¡å‹åè§æ¨¡å¼ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†åœ¨å…¨çƒè‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸä¸­ï¼Œå»ºç«‹æ¤æ ¹äºæœ¬åœŸæ–‡åŒ–çš„å¤šè¯­è¨€åè§å®¡è®¡æ¡†æ¶çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2506.00068v2",
      "published_date": "2025-05-29 15:15:42 UTC",
      "updated_date": "2025-07-31 04:41:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:58:58.644030+00:00"
    },
    {
      "arxiv_id": "2505.23536v1",
      "title": "Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)",
      "title_zh": "ååŒè¿‡ç¨‹æ¨¡å‹ä¸äº‹ä»¶æŠ½è±¡ä»¥å®ç°æœ‰æ®å¯ä¾çš„è¿‡ç¨‹æ™ºèƒ½ï¼ˆæ‰©å±•ç‰ˆï¼‰",
      "authors": [
        "Janik-Vasily Benzin",
        "Gyunam Park",
        "Stefanie Rinderle-Ma"
      ],
      "abstract": "Model abstraction (MA) and event abstraction (EA) are means to reduce complexity of (discovered) models and event data. Imagine a process intelligence project that aims to analyze a model discovered from event data which is further abstracted, possibly multiple times, to reach optimality goals, e.g., reducing model size. So far, after discovering the model, there is no technique that enables the synchronized abstraction of the underlying event log. This results in loosing the grounding in the real-world behavior contained in the log and, in turn, restricts analysis insights. Hence, in this work, we provide the formal basis for synchronized model and event abstraction, i.e., we prove that abstracting a process model by MA and discovering a process model from an abstracted event log yields an equivalent process model. We prove the feasibility of our approach based on behavioral profile abstraction as non-order preserving MA technique, resulting in a novel EA technique.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æµç¨‹æ™ºèƒ½é¢†åŸŸä¸­é™ä½å¤æ‚æ€§çš„æ ¸å¿ƒæ‰‹æ®µï¼Œå³æ¨¡å‹æŠ½è±¡(Model Abstraction, MA)ä¸äº‹ä»¶æŠ½è±¡(Event Abstraction, EA)çš„åŒæ­¥é—®é¢˜ã€‚é’ˆå¯¹ç›®å‰åœ¨æ¨¡å‹å‘ç°åç¼ºä¹åŒæ­¥æŠ½è±¡åº•å±‚äº‹ä»¶æ—¥å¿—æŠ€æœ¯ï¼Œä»è€Œå¯¼è‡´åˆ†æå¤±å»çœŸå®è¡Œä¸ºæ”¯æ’‘(grounding)çš„ç°çŠ¶ï¼Œæœ¬æ–‡æå‡ºäº†åŒæ­¥æ¨¡å‹ä¸äº‹ä»¶æŠ½è±¡çš„å½¢å¼åŒ–åŸºç¡€ã€‚ç ”ç©¶è¯æ˜äº†é€šè¿‡MAæŠ€æœ¯å¯¹æµç¨‹æ¨¡å‹è¿›è¡ŒæŠ½è±¡ï¼Œä¸ä»ç»è¿‡EAå¤„ç†çš„äº‹ä»¶æ—¥å¿—ä¸­é‡æ–°å‘ç°æ¨¡å‹ï¼Œåœ¨ç»“æœä¸Šæ˜¯ç­‰æ•ˆçš„ã€‚é€šè¿‡å°†è¡Œä¸ºè½®å»“æŠ½è±¡(behavioral profile abstraction)ä½œä¸ºä¸€ç§éé¡ºåºä¿ç•™çš„MAæŠ€æœ¯è¿›è¡Œå®éªŒï¼Œç ”ç©¶éªŒè¯äº†è¯¥æ–¹æ¡ˆçš„å¯è¡Œæ€§å¹¶ç”±æ­¤äº§ç”Ÿäº†ä¸€ç§æ–°å‹EAæŠ€æœ¯ã€‚è¿™ä¸€æˆæœç¡®ä¿äº†åœ¨ç®€åŒ–å¤æ‚æ¨¡å‹çš„åŒæ—¶ï¼Œä¾ç„¶èƒ½å¤Ÿç»´æŒåˆ†ææ´å¯Ÿä¸ç°å®æ•°æ®ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23536v1",
      "published_date": "2025-05-29 15:15:23 UTC",
      "updated_date": "2025-05-29 15:15:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:58:46.781570+00:00"
    },
    {
      "arxiv_id": "2505.23529v2",
      "title": "Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning",
      "title_zh": "é¢å‘è‡ªç›‘ç£å›¾è¡¨ç¤ºå­¦ä¹ çš„å­å›¾é«˜æ–¯åµŒå…¥å¯¹æ¯”",
      "authors": [
        "Shifeng Xie",
        "Aref Einizade",
        "Jhony H. Giraldo"
      ],
      "abstract": "Graph Representation Learning (GRL) is a fundamental task in machine learning, aiming to encode high-dimensional graph-structured data into low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used in GRL because they can avoid expensive human annotation. In this work, we propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our approach introduces a subgraph Gaussian embedding module, which adaptively maps subgraphs to a structured Gaussian space, ensuring the preservation of input subgraph characteristics while generating subgraphs with a controlled distribution. We then employ optimal transport distances, more precisely the Wasserstein and Gromov-Wasserstein distances, to effectively measure the similarity between subgraphs, enhancing the robustness of the contrastive learning process. Extensive experiments across multiple benchmarks demonstrate that \\method~outperforms or presents competitive performance against state-of-the-art approaches. Our findings provide insights into the design of SSL methods for GRL, emphasizing the importance of the distribution of the generated contrastive pairs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Subgraph Gaussian Embedding Contrast (SubGEC)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºè‡ªç›‘ç£å›¾è¡¨ç¤ºå­¦ä¹  (Self-Supervised Graph Representation Learning, GRL) è®¾è®¡çš„æ–°å‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å­å›¾é«˜æ–¯åµŒå…¥æ¨¡å— (subgraph Gaussian embedding module)ï¼Œé€šè¿‡å°†å­å›¾è‡ªé€‚åº”åœ°æ˜ å°„åˆ°ç»“æ„åŒ–é«˜æ–¯ç©ºé—´ï¼Œåœ¨ç”Ÿæˆå…·æœ‰å—æ§åˆ†å¸ƒçš„å­å›¾æ—¶ä¿ç•™äº†åŸå§‹è¾“å…¥çš„ç»“æ„ç‰¹å¾ã€‚ä¸ºäº†å¢å¼ºå¯¹æ¯”å­¦ä¹  (contrastive learning) è¿‡ç¨‹çš„é²æ£’æ€§ï¼Œç ”ç©¶é‡‡ç”¨äº† Wasserstein å’Œ Gromov-Wasserstein ç­‰æœ€ä¼˜ä¼ è¾“è·ç¦» (optimal transport distances) æ¥ç²¾ç¡®è¡¡é‡å­å›¾é—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSubGEC çš„æ€§èƒ½ä¼˜äºæˆ–ç­‰åŒäºå½“å‰çš„å…ˆè¿›æ–¹æ³• (state-of-the-art)ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç”Ÿæˆçš„å¯¹æ¯”å¯¹ (contrastive pairs) åˆ†å¸ƒåœ¨è‡ªç›‘ç£å­¦ä¹ è®¾è®¡ä¸­çš„é‡è¦æ€§ï¼Œä¸ºå›¾è¡¨ç¤ºå­¦ä¹ é¢†åŸŸæä¾›äº†æ–°çš„ç†è®ºè§è§£ä¸å®è·µæ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23529v2",
      "published_date": "2025-05-29 15:07:22 UTC",
      "updated_date": "2025-06-12 09:16:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:58:55.514296+00:00"
    },
    {
      "arxiv_id": "2505.23519v1",
      "title": "Individual differences in the cognitive mechanisms of planning strategy discovery",
      "title_zh": "è§„åˆ’ç­–ç•¥å‘ç°è®¤çŸ¥æœºåˆ¶çš„ä¸ªä½“å·®å¼‚",
      "authors": [
        "Ruiqi He",
        "Falk Lieder"
      ],
      "abstract": "People employ efficient planning strategies. But how are these strategies acquired? Previous research suggests that people can discover new planning strategies through learning from reinforcements, a process known as metacognitive reinforcement learning (MCRL). While prior work has shown that MCRL models can learn new planning strategies and explain more participants' experience-driven discovery better than alternative mechanisms, it also revealed significant individual differences in metacognitive learning. Furthermore, when fitted to human data, these models exhibit a slower rate of strategy discovery than humans. In this study, we investigate whether incorporating cognitive mechanisms that might facilitate human strategy discovery can bring models of MCRL closer to human performance. Specifically, we consider intrinsically generated metacognitive pseudo-rewards, subjective effort valuation, and termination deliberation. Analysis of planning task data shows that a larger proportion of participants used at least one of these mechanisms, with significant individual differences in their usage and varying impacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort valuation, and learning the value of acting without further planning were found to facilitate strategy discovery. While these enhancements provided valuable insights into individual differences and the effect of these mechanisms on strategy discovery, they did not fully close the gap between model and human performance, prompting further exploration of additional factors that people might use to discover new planning strategies.",
      "tldr_zh": "æœ¬é¡¹ç ”ç©¶è°ƒæŸ¥äº†äººç±»è§„åˆ’ç­–ç•¥å‘ç°è¿‡ç¨‹ä¸­çš„è®¤çŸ¥æœºåˆ¶åŠå…¶ä¸ªä½“å·®å¼‚ï¼Œç‰¹åˆ«å…³æ³¨å…ƒè®¤çŸ¥å¼ºåŒ–å­¦ä¹  (Metacognitive Reinforcement Learning, MCRL) æ¡†æ¶ã€‚é€šè¿‡åœ¨ MCRL æ¨¡å‹ä¸­å¼•å…¥å†…åœ¨ç”Ÿæˆçš„å…ƒè®¤çŸ¥ä¼ªå¥–åŠ± (metacognitive pseudo-rewards)ã€ä¸»è§‚åŠªåŠ›è¯„ä¼° (subjective effort valuation) å’Œç»ˆæ­¢å®¡è®® (termination deliberation)ï¼Œç ”ç©¶è€…è¯•å›¾è§£é‡Šäººç±»æ¯”ä¼ ç»Ÿæ¨¡å‹æ›´å¿«çš„ç­–ç•¥å‘ç°é€Ÿåº¦ã€‚å®éªŒæ•°æ®åˆ†ææ˜¾ç¤ºï¼Œå¤§éƒ¨åˆ†å‚ä¸è€…ä¼šé‡‡ç”¨ä¸Šè¿°ä¸€ç§æˆ–å¤šç§æœºåˆ¶ï¼Œä¸”è¿™äº›æœºåˆ¶çš„ä½¿ç”¨å­˜åœ¨æ˜¾è‘—çš„ä¸ªä½“å·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œå…ƒè®¤çŸ¥ä¼ªå¥–åŠ±ã€ä¸»è§‚åŠªåŠ›è¯„ä¼°ä»¥åŠå­¦ä¹ â€œä¸è¿›ä¸€æ­¥è§„åˆ’è€Œè¡ŒåŠ¨â€çš„ä»·å€¼å‡èƒ½æœ‰æ•ˆä¿ƒè¿›ç­–ç•¥å‘ç°ã€‚å°½ç®¡è¿™äº›å¢å¼ºæœºåˆ¶ä¸ºç†è§£ä¸ªä½“å·®å¼‚æä¾›äº†é‡è¦è§è§£ï¼Œä½†ä»æœªèƒ½å®Œå…¨å¼¥åˆæ¨¡å‹ä¸äººç±»è¡¨ç°ä¹‹é—´çš„å·®è·ï¼Œæš—ç¤ºè¿˜å­˜åœ¨å…¶ä»–å½±å“ç­–ç•¥å‘ç°çš„æ½œåœ¨å› ç´ ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23519v1",
      "published_date": "2025-05-29 14:57:34 UTC",
      "updated_date": "2025-05-29 14:57:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:02.358161+00:00"
    },
    {
      "arxiv_id": "2505.23518v2",
      "title": "TRAP: Targeted Redirecting of Agentic Preferences",
      "title_zh": "TRAPï¼šæ™ºèƒ½ä½“åå¥½çš„å®šå‘é‡å®šå‘",
      "authors": [
        "Hangoo Kang",
        "Jehyeok Yeon",
        "Gagandeep Singh"
      ],
      "abstract": "Autonomous agentic AI systems powered by vision-language models (VLMs) are rapidly advancing toward real-world deployment, yet their cross-modal reasoning capabilities introduce new attack surfaces for adversarial manipulation that exploit semantic reasoning across modalities. Existing adversarial attacks typically rely on visible pixel perturbations or require privileged model or environment access, making them impractical for stealthy, real-world exploitation. We introduce TRAP, a novel generative adversarial framework that manipulates the agent's decision-making using diffusion-based semantic injections into the vision-language embedding space. Our method combines negative prompt-based degradation with positive semantic optimization, guided by a Siamese semantic network and layout-aware spatial masking. Without requiring access to model internals, TRAP produces visually natural images yet induces consistent selection biases in agentic AI systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO) dataset, building multi-candidate decision scenarios. Across these scenarios, TRAP consistently induces decision-level preference redirection on leading models, including LLaVA-34B, Gemma3, GPT-4o, and Mistral-3.2, significantly outperforming existing baselines such as SPSA, Bandit, and standard diffusion approaches. These findings expose a critical, generalized vulnerability: autonomous agents can be consistently misled through visually subtle, semantically-guided cross-modal manipulations. Overall, our results show the need for defense strategies beyond pixel-level robustness to address semantic vulnerabilities in cross-modal decision-making. The code for TRAP is accessible on GitHub at https://github.com/uiuc-focal-lab/TRAP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”±è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)é©±åŠ¨çš„è‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ­ç¤ºäº†å…¶åœ¨è·¨æ¨¡æ€æ¨ç†ä¸­é¢ä¸´çš„è¯­ä¹‰æ“æ§æ–°å‹æ”»å‡»å¨èƒã€‚ç ”ç©¶è€…æå‡ºäº†TRAPï¼Œä¸€ç§æ–°å‹ç”Ÿæˆå¼å¯¹æŠ—æ¡†æ¶ï¼Œé€šè¿‡åœ¨è§†è§‰è¯­è¨€åµŒå…¥ç©ºé—´(vision-language embedding space)è¿›è¡ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰æ³¨å…¥æ¥æ“çºµæ™ºèƒ½ä½“çš„å†³ç­–ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºè´Ÿå‘æç¤º(negative prompt)çš„é€€åŒ–å’Œæ­£å‘è¯­ä¹‰ä¼˜åŒ–ï¼Œå¹¶ç”±å­ªç”Ÿè¯­ä¹‰ç½‘ç»œ(Siamese semantic network)åŠå¸ƒå±€æ„ŸçŸ¥ç©ºé—´æ©ç (layout-aware spatial masking)è¿›è¡Œå¼•å¯¼ã€‚TRAPåœ¨æ— éœ€è®¿é—®æ¨¡å‹å†…éƒ¨å‚æ•°çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿç”Ÿæˆè§†è§‰ä¸Šè‡ªç„¶çš„å›¾åƒï¼Œå¹¶è¯±å¯¼æ™ºèƒ½ä½“äº§ç”ŸæŒç»­çš„é€‰æ‹©æ€§åå·®ã€‚åœ¨COCOæ•°æ®é›†çš„å¤šå€™é€‰å†³ç­–åœºæ™¯è¯„ä¼°ä¸­ï¼ŒTRAPåœ¨LLaVA-34Bã€Gemma3ã€GPT-4oå’ŒMistral-3.2ç­‰é¢†å…ˆæ¨¡å‹ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„åå¥½é‡å®šå‘ï¼Œæ€§èƒ½å¤§å¹…ä¼˜äºSPSAå’ŒBanditç­‰åŸºçº¿æ–¹æ³•ã€‚å®éªŒç»“æœæš´éœ²äº†è‡ªä¸»æ™ºèƒ½ä½“åœ¨è·¨æ¨¡æ€å†³ç­–ä¸­çš„æ™®éè¯­ä¹‰æ¼æ´ï¼Œå¼ºè°ƒäº†åœ¨é˜²å¾¡ç­–ç•¥ä¸­è¶…è¶Šåƒç´ çº§é²æ£’æ€§(pixel-level robustness)ä»¥åº”å¯¹è¯­ä¹‰æ“çºµçš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23518v2",
      "published_date": "2025-05-29 14:57:16 UTC",
      "updated_date": "2025-11-24 00:01:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:02.741829+00:00"
    },
    {
      "arxiv_id": "2505.23508v1",
      "title": "A Robot-Assisted Approach to Small Talk Training for Adults with ASD",
      "title_zh": "é’ˆå¯¹ ASD æˆå¹´äººçš„æœºå™¨äººè¾…åŠ©ç¤¾äº¤é—²èŠè®­ç»ƒæ–¹æ³•",
      "authors": [
        "Rebecca Ramnauth",
        "DraÅ¾en BrÅ¡ÄiÄ‡",
        "Brian Scassellati"
      ],
      "abstract": "From dating to job interviews, making new friends or simply chatting with the cashier at checkout, engaging in small talk is a vital, everyday social skill. For adults with Autism Spectrum Disorder (ASD), small talk can be particularly challenging, yet it is essential for social integration, building relationships, and accessing professional opportunities. In this study, we present our development and evaluation of an in-home autonomous robot system that allows users to practice small talk. Results from the week-long study show that adults with ASD enjoyed the training, made notable progress in initiating conversations and improving eye contact, and viewed the system as a valuable tool for enhancing their conversational skills.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªé—­ç—‡è°±ç³»éšœç¢(ASD)æˆå¹´äººåœ¨æ—¥å¸¸ç¤¾äº¤ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¼€å‘å¹¶è¯„ä¼°äº†ä¸€å¥—ç”¨äºç»ƒä¹ é—²è°ˆ(Small Talk)çš„å±…å®¶è‡ªä¸»æœºå™¨äººç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿç¤¾äº¤äº’åŠ¨ï¼Œå¸®åŠ©ç”¨æˆ·åœ¨äº¤å‹ã€é¢è¯•ç­‰åœºæ™¯ä¸­å»ºç«‹å¿…è¦çš„ç¤¾äº¤æŠ€èƒ½ã€‚ä¸ºæœŸä¸€å‘¨çš„å®åœ°ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒASDæˆå¹´äººå¯¹è¯¥æœºå™¨äººè¾…åŠ©è®­ç»ƒè¡¨ç°å‡ºæé«˜çš„å‚ä¸åº¦ï¼Œå¹¶åœ¨å‘èµ·å¯¹è¯èƒ½åŠ›å’Œç›®å…‰æ¥è§¦æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚å‚ä¸è€…æ™®éè®¤ä¸ºè¯¥ç³»ç»Ÿæ˜¯å¢å¼ºæ²Ÿé€šæŠ€å·§çš„æœ‰æ•ˆå·¥å…·ï¼Œä¸ºé€šè¿‡æœºå™¨äººæŠ€æœ¯ä¿ƒè¿›ASDç¾¤ä½“çš„ç¤¾ä¼šèå…¥æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for publication in Robotics: Science and Systems (RSS) 2025, 14 pages, 4 figures,",
      "pdf_url": "https://arxiv.org/pdf/2505.23508v1",
      "published_date": "2025-05-29 14:51:45 UTC",
      "updated_date": "2025-05-29 14:51:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:10.618165+00:00"
    },
    {
      "arxiv_id": "2505.23503v2",
      "title": "Can Large Language Models Challenge CNNs in Medical Image Analysis?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­æŒ‘æˆ˜å·ç§¯ç¥ç»ç½‘ç»œï¼Ÿ",
      "authors": [
        "Shibbir Ahmed",
        "Shahnewaz Karim Sakib",
        "Anindya Bijoy Das"
      ],
      "abstract": "This study presents a multimodal AI framework designed for precisely classifying medical diagnostic images. Utilizing publicly available datasets, the proposed system compares the strengths of convolutional neural networks (CNNs) and different large language models (LLMs). This in-depth comparative analysis highlights key differences in diagnostic performance, execution efficiency, and environmental impacts. Model evaluation was based on accuracy, F1-score, average execution time, average energy consumption, and estimated $CO_2$ emission. The findings indicate that although CNN-based models can outperform various multimodal techniques that incorporate both images and contextual information, applying additional filtering on top of LLMs can lead to substantial performance gains. These findings highlight the transformative potential of multimodal AI systems to enhance the reliability, efficiency, and scalability of medical diagnostics in clinical settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºåŒ»å­¦è¯Šæ–­å›¾åƒåˆ†ç±»çš„å¤šæ¨¡æ€AIæ¡†æ¶ï¼Œå¯¹æ¯”åˆ†æäº†å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)ä¸å¤šç§å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ»ç–—å½±åƒåˆ†æä¸­çš„è¡¨ç°ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨å…¬å¼€æ•°æ®é›†ï¼Œä»å‡†ç¡®ç‡(Accuracy)ã€F1-scoreã€æ‰§è¡Œæ•ˆç‡ä»¥åŠèƒ½æºæ¶ˆè€—ä¸äºŒæ°§åŒ–ç¢³æ’æ”¾ç­‰ç¯å¢ƒå½±å“ç»´åº¦è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¼ ç»Ÿçš„CNNsæ¨¡å‹åœ¨æŸäº›å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä½†é€šè¿‡åœ¨LLMsä¹‹ä¸Šå¼•å…¥é¢å¤–çš„è¿‡æ»¤(Filtering)æœºåˆ¶ï¼Œå¯ä»¥å®ç°æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å¤šæ¨¡æ€AIç³»ç»Ÿåœ¨å¢å¼ºä¸´åºŠè¯Šæ–­å¯é æ€§ã€æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å˜é©æ€§æ½œåŠ›ï¼Œä¸ºæœªæ¥åŒ»ç–—å½±åƒåˆ†æçš„æŠ€æœ¯è·¯å¾„æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23503v2",
      "published_date": "2025-05-29 14:48:09 UTC",
      "updated_date": "2025-06-03 14:52:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:15.392723+00:00"
    },
    {
      "arxiv_id": "2505.23495v4",
      "title": "Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking",
      "title_zh": "è¯Šæ–­ä¸åº”å¯¹ KG-RAG æ•°æ®é›†çš„ç¼ºé™·ï¼šè¿ˆå‘æ›´å¯é çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Liangliang Zhang",
        "Zhuorui Jiang",
        "Hongliang Chi",
        "Haoyang Chen",
        "Mohammed Elkoumy",
        "Fali Wang",
        "Qiong Wu",
        "Zhengyi Zhou",
        "Shirui Pan",
        "Suhang Wang",
        "Yao Ma"
      ],
      "abstract": "Knowledge Graph Question Answering (KGQA) systems rely on high-quality benchmarks to evaluate complex multi-hop reasoning. However, despite their widespread use, popular datasets such as WebQSP and CWQ suffer from critical quality issues, including inaccurate or incomplete ground-truth annotations, poorly constructed questions that are ambiguous, trivial, or unanswerable, and outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA datasets, including WebQSP and CWQ, we find that the average factual correctness rate is only 57 %. To address these issues, we introduce KGQAGen, an LLM-in-the-loop framework that systematically resolves these pitfalls. KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to produce challenging and verifiable QA instances. Using KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results demonstrate that even state-of-the-art systems struggle on this benchmark, highlighting its ability to expose limitations of existing models. Our findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥åˆ†æäº†Knowledge Graph Question Answering (KGQA) é¢†åŸŸä¸­å¸¸ç”¨çš„WebQSPå’ŒCWQç­‰æ•°æ®é›†ï¼Œæ­ç¤ºäº†å…¶ä¸­å­˜åœ¨çš„æ ‡æ³¨é”™è¯¯ã€é—®é¢˜æ­§ä¹‰åŠçŸ¥è¯†é™ˆæ—§ç­‰å…³é”®è´¨é‡é—®é¢˜ã€‚é€šè¿‡å¯¹æ‰‹å·¥å®¡è®¡çš„16ä¸ªçƒ­é—¨æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å…¶å¹³å‡äº‹å®æ­£ç¡®ç‡ä»…ä¸º57%ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†KGQAGenï¼Œè¿™æ˜¯ä¸€ç§èåˆäº†ç»“æ„åŒ–çŸ¥è¯†è½åœ°ã€LLMå¼•å¯¼ç”Ÿæˆä¸ç¬¦å·éªŒè¯çš„LLM-in-the-loopæ¡†æ¶ã€‚åˆ©ç”¨æ­¤æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŸºäºWikidataçš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†KGQAGen-10kã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„state-of-the-artç³»ç»Ÿåœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸­å‡é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå……åˆ†è¯æ˜äº†å…¶æ­ç¤ºæ¨¡å‹å±€é™æ€§çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å‘¼åå»ºç«‹æ›´ä¸¥è°¨çš„åŸºå‡†æ„å»ºæ ‡å‡†ï¼Œå¹¶å°†KGQAGenå®šä½ä¸ºæ¨åŠ¨KGQAè¯„ä¼°è¿›æ­¥çš„å¯æ‰©å±•å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at NeurIPS 2025 Datasets and Benchmarks Track",
      "pdf_url": "https://arxiv.org/pdf/2505.23495v4",
      "published_date": "2025-05-29 14:44:52 UTC",
      "updated_date": "2025-11-03 19:15:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:35.909830+00:00"
    },
    {
      "arxiv_id": "2505.23486v2",
      "title": "Autoformalization in the Era of Large Language Models: A Survey",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£çš„è‡ªåŠ¨å½¢å¼åŒ–ï¼šç»¼è¿°",
      "authors": [
        "Ke Weng",
        "Lun Du",
        "Sirui Li",
        "Wangyue Lu",
        "Haozhe Sun",
        "Hengyu Liu",
        "Tiancheng Zhang"
      ],
      "abstract": "Autoformalization, the process of transforming informal mathematical propositions into verifiable formal representations, is a foundational task in automated theorem proving, offering a new perspective on the use of mathematics in both theoretical and applied domains. Driven by the rapid progress in artificial intelligence, particularly large language models (LLMs), this field has witnessed substantial growth, bringing both new opportunities and unique challenges. In this survey, we provide a comprehensive overview of recent advances in autoformalization from both mathematical and LLM-centric perspectives. We examine how autoformalization is applied across various mathematical domains and levels of difficulty, and analyze the end-to-end workflow from data preprocessing to model design and evaluation. We further explore the emerging role of autoformalization in enhancing the verifiability of LLM-generated outputs, highlighting its potential to improve both the trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key open-source models and datasets supporting current research, and discuss open challenges and promising future directions for the field.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)æ—¶ä»£ä¸‹çš„è‡ªåŠ¨å½¢å¼åŒ–(Autoformalization)æŠ€æœ¯ï¼Œå³å¦‚ä½•å°†éæ­£å¼æ•°å­¦å‘½é¢˜è½¬åŒ–ä¸ºå¯éªŒè¯å½¢å¼åŒ–è¡¨ç¤ºçš„è¿‡ç¨‹ã€‚ç ”ç©¶ä»æ•°å­¦å’ŒLLMçš„åŒé‡è§†è§’å‡ºå‘ï¼Œè¯¦ç»†åˆ†æäº†ä»æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®¾è®¡åˆ°è¯„ä¼°çš„ç«¯åˆ°ç«¯å·¥ä½œæµï¼Œå¹¶è€ƒå¯Ÿäº†å…¶åœ¨ä¸åŒæ•°å­¦é¢†åŸŸåŠéš¾åº¦çº§åˆ«ä¸­çš„åº”ç”¨ã€‚ä½œè€…é‡ç‚¹é˜è¿°äº†Autoformalizationåœ¨å¢å¼ºLLMè¾“å‡ºå¯éªŒè¯æ€§æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ï¼Œè®¤ä¸ºè¿™å¯¹äºæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¯ä¿¡åº¦è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ€»ç»“äº†å½“å‰æ”¯æŒè¯¥é¢†åŸŸç ”ç©¶çš„å…³é”®å¼€æºæ¨¡å‹ä¸æ•°æ®é›†ã€‚æœ€åï¼Œè®ºæ–‡æ·±å…¥è®¨è®ºäº†è‡ªåŠ¨å½¢å¼åŒ–é¢ä¸´çš„å…¬å¼€æŒ‘æˆ˜ï¼Œå¹¶ä¸ºè‡ªåŠ¨åŒ–å®šç†è¯æ˜(Automated Theorem Proving)ç­‰æœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦æŒ‡å¼•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23486v2",
      "published_date": "2025-05-29 14:34:54 UTC",
      "updated_date": "2025-07-03 06:03:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:37.367077+00:00"
    },
    {
      "arxiv_id": "2505.23876v1",
      "title": "A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size",
      "title_zh": "åŸºäºè®­ç»ƒæ•°æ®é›†è§„æ¨¡çš„è®¡ç®—æƒé‡ç¥ç»ç½‘ç»œä¸éšæœºæƒé‡ç¥ç»ç½‘ç»œçš„å¯¹æ¯”åˆ†æ",
      "authors": [
        "Polad Geidarov"
      ],
      "abstract": "The paper discusses the capabilities of multilayer perceptron neural networks implementing metric recognition methods, for which the values of the weights are calculated analytically by formulas. Comparative experiments in training a neural network with pre-calculated weights and with random initialization of weights on different sizes of the MNIST training dataset are carried out. The results of the experiments show that a multilayer perceptron with pre-calculated weights can be trained much faster and is much more robust to the reduction of the training dataset.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹é‡‡ç”¨è§£æå…¬å¼è®¡ç®—æƒé‡çš„å¤šå±‚æ„ŸçŸ¥æœº (Multilayer Perceptron) ä¸ä¼ ç»Ÿçš„éšæœºåˆå§‹åŒ–æƒé‡ç¥ç»ç½‘ç»œè¿›è¡Œäº†æ·±å…¥çš„å¯¹æ¯”åˆ†æã€‚è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†å®ç°åº¦é‡è¯†åˆ« (Metric Recognition) æ–¹æ³•çš„ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­æƒé‡å€¼æ˜¯é€šè¿‡ç‰¹å®šå…¬å¼åˆ†æè®¡ç®—å¾—å‡ºçš„ï¼Œè€Œéå®Œå…¨ä¾èµ–éšæœºåˆå§‹åŒ–åçš„è¿­ä»£è®­ç»ƒã€‚é€šè¿‡åœ¨ä¸åŒè§„æ¨¡çš„ MNIST æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œç ”ç©¶è€…æ¯”è¾ƒäº†é¢„è®¡ç®—æƒé‡ä¸éšæœºç”Ÿæˆæƒé‡åœ¨ä¸åŒè®­ç»ƒé›†å®¹é‡ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚å®éªŒç»“æœæ˜¾è‘—è¡¨æ˜ï¼Œå…·æœ‰é¢„è®¡ç®—æƒé‡çš„å¤šå±‚æ„ŸçŸ¥æœºåœ¨è®­ç»ƒé€Ÿåº¦ä¸Šè¿œå¿«äºä¼ ç»Ÿæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†è§„æ¨¡å¤§å¹…ç¼©å‡æ—¶å±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ (Robustness)ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†é¢„è®¡ç®—æƒé‡æ–¹æ³•åœ¨æé«˜å­¦ä¹ æ•ˆç‡å’Œåº”å¯¹å°æ ·æœ¬æ•°æ®é›†æŒ‘æˆ˜æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23876v1",
      "published_date": "2025-05-29 14:31:25 UTC",
      "updated_date": "2025-05-29 14:31:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:34.139494+00:00"
    },
    {
      "arxiv_id": "2505.23474v1",
      "title": "Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns",
      "title_zh": "Socratic-PRMBenchï¼šåŸºäºç³»ç»ŸåŒ–æ¨ç†æ¨¡å¼çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Xiang Li",
        "Haiyang Yu",
        "Xinghua Zhang",
        "Ziyang Huang",
        "Shizhu He",
        "Kang Liu",
        "Jun Zhao",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Process Reward Models (PRMs) are crucial in complex reasoning and problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by verifying the correctness of each intermediate reasoning step. In real-world scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to solve a problem, potentially suffering from errors under various reasoning patterns. Therefore, PRMs are required to identify errors under various reasoning patterns during the reasoning process. However, existing benchmarks mainly focus on evaluating PRMs with stepwise correctness, ignoring a systematic evaluation of PRMs under various reasoning patterns. To mitigate this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs systematically under six reasoning patterns, including Transformation, Decomposition, Regather, Deduction, Verification, and Integration. Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the aforementioned six reasoning patterns. Through our experiments on both PRMs and LLMs prompted as critic models, we identify notable deficiencies in existing PRMs. These observations underscore the significant weakness of current PRMs in conducting evaluations on reasoning steps under various reasoning patterns. We hope Socratic-PRMBench can serve as a comprehensive testbed for systematic evaluation of PRMs under diverse reasoning patterns and pave the way for future development of PRMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(Process Reward Models, PRMs)åœ¨å¤æ‚æ¨ç†ä»»åŠ¡çš„ä¸­é—´æ­¥éª¤éªŒè¯ä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•ç¼ºä¹å¯¹å¤šæ ·åŒ–æ¨ç†æ¨¡å¼(reasoning patterns)çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ¨å‡ºäº†Socratic-PRMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–Transformationã€Decompositionã€Regatherã€Deductionã€Verificationå’ŒIntegrationå…­ç§æ¨ç†æ¨¡å¼çš„æ–°å‹åŸºå‡†ã€‚è¯¥æ•°æ®é›†åŒ…å«2995æ¡åœ¨ç‰¹å®šæ¨ç†æ¨¡å¼ä¸­å­˜åœ¨é€»è¾‘ç¼ºé™·çš„æ¨ç†è·¯å¾„ï¼Œç”¨äºç²¾å‡†æ£€æµ‹æ¨¡å‹è¯†åˆ«é”™è¯¯çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„PRMsä»¥åŠä½œä¸ºè¯„è®ºæ¨¡å‹(critic models)çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¿™äº›ç³»ç»Ÿæ€§æ¨ç†æ¨¡å¼ä¸‹å‡è¡¨ç°å‡ºæ˜æ˜¾çš„æ€§èƒ½çŸ­æ¿ã€‚Socratic-PRMBenchçš„æå‡ºä¸ä»…æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç²¾ç»†åŒ–æ¨ç†éªŒè¯ä¸Šçš„ä¸è¶³ï¼Œä¹Ÿä¸ºæœªæ¥PRMsçš„ä¼˜åŒ–ä¸å‘å±•æä¾›äº†å…¨é¢çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23474v1",
      "published_date": "2025-05-29 14:26:53 UTC",
      "updated_date": "2025-05-29 14:26:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:38.385016+00:00"
    },
    {
      "arxiv_id": "2505.23473v3",
      "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
      "title_zh": "EVOREFUSEï¼šé€šè¿‡æ¼”åŒ–æç¤ºä¼˜åŒ–è¯„ä¼°ä¸ç¼“è§£å¤§è¯­è¨€æ¨¡å‹å¯¹ä¼ªæ¶æ„æŒ‡ä»¤çš„è¿‡åº¦æ‹’ç­”",
      "authors": [
        "Xiaorui Wu",
        "Fei Li",
        "Xiaofeng Mao",
        "Xin Zhang",
        "Li Zheng",
        "Yuxiang Peng",
        "Chong Teng",
        "Donghong Ji",
        "Zhuang Li"
      ],
      "abstract": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 85.34% higher average refusal triggering rate across 9 LLMs without a safety-prior system prompt, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. With supervised fine-tuning on EVOREFUSE-ALIGN, LLAMA3.1-8B-INSTRUCT achieves up to 29.85% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context. Our code and datasets are available at https://github.com/FishT0ucher/EVOREFUSE.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å› ä¿å®ˆçš„å®‰å…¨å¯¹é½è€Œæ‹’ç»å¤„ç†çœ‹ä¼¼æ¶æ„å®åˆ™æ— å®³çš„æŒ‡ä»¤(Pseudo-Malicious Instructions)è¿™ä¸€è¿‡åº¦æ‹’ç»(Over-Refusal)é—®é¢˜ï¼Œæå‡ºäº†EVOREFUSEæ¡†æ¶ã€‚è¿™æ˜¯ä¸€ç§åŸºäºè¿›åŒ–ç®—æ³•(Evolutionary Algorithm)çš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å˜å¼‚å’Œé‡ç»„ç­–ç•¥è¿­ä»£æ¼”åŒ–ç§å­æŒ‡ä»¤ï¼Œä»¥æœ€å¤§åŒ–æ¨¡å‹æ‹’ç»æ¦‚ç‡çš„è¯æ®ä¸‹ç•Œã€‚åˆ©ç”¨è¯¥æ–¹æ³•ï¼Œç ”ç©¶è€…æ„å»ºäº†è¯„ä¼°åŸºå‡†EVOREFUSE-TESTå’Œå¯¹é½è®­ç»ƒæ•°æ®é›†EVOREFUSE-ALIGNã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEVOREFUSE-TESTåœ¨è§¦å‘æ‹’ç»ç‡å’Œè¯æ±‡å¤šæ ·æ€§ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ã€‚é€šè¿‡åœ¨EVOREFUSE-ALIGNä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ŒLLAMA3.1-8B-INSTRUCTåœ¨ä¿æŒå®‰å…¨æ€§çš„åŒæ—¶ï¼Œå°†è¿‡åº¦æ‹’ç»ç‡é™ä½äº†é«˜è¾¾29.85%ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œæ¨¡å‹äº§ç”Ÿè¿‡åº¦æ‹’ç»çš„æ ¸å¿ƒåŸå› åœ¨äºå…¶è¿‡åº¦å…³æ³¨æ•æ„Ÿå…³é”®è¯è€Œå¿½è§†äº†å¹¿æ³›çš„ä¸Šä¸‹æ–‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23473v3",
      "published_date": "2025-05-29 14:26:46 UTC",
      "updated_date": "2026-01-20 03:57:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:46.270938+00:00"
    },
    {
      "arxiv_id": "2506.02025v2",
      "title": "Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling",
      "title_zh": "è¯„ä¼°åŸºäº LLM æ¨ç†çš„å¤šç›®æ ‡ HPC ä½œä¸šè°ƒåº¦æ•ˆèƒ½",
      "authors": [
        "Prachi Jadhav",
        "Hongwei Jin",
        "Ewa Deelman",
        "Prasanna Balaprakash"
      ],
      "abstract": "High-Performance Computing (HPC) job scheduling involves balancing conflicting objectives such as minimizing makespan, reducing wait times, optimizing resource use, and ensuring fairness. Traditional methods, including heuristic-based, e.g., First-Come-First-Served (FJFS) and Shortest Job First (SJF), or intensive optimization techniques, often lack adaptability to dynamic workloads and, more importantly, cannot simultaneously optimize multiple objectives in HPC systems. To address this, we propose a novel Large Language Model (LLM)-based scheduler using a ReAct-style framework (Reason + Act), enabling iterative, interpretable decision-making. The system incorporates a scratchpad memory to track scheduling history and refine decisions via natural language feedback, while a constraint enforcement module ensures feasibility and safety. We evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across seven real-world HPC workload scenarios, including heterogeneous mixes, bursty patterns, and adversarial cases etc. Comparisons against FCFS, SJF, and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling effectively balances multiple objectives while offering transparent reasoning through natural language traces. The method excels in constraint satisfaction and adapts to diverse workloads without domain-specific training. However, a trade-off between reasoning quality and computational overhead challenges real-time deployment. This work presents the first comprehensive study of reasoning-capable LLMs for HPC scheduling, demonstrating their potential to handle multiobjective optimization while highlighting limitations in computational efficiency. The findings provide insights into leveraging advanced language models for complex scheduling problems in dynamic HPC environments.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ¨ç†åœ¨å¤šç›®æ ‡é«˜æ€§èƒ½è®¡ç®—(HPC)ä½œä¸šè°ƒåº¦ä¸­çš„æ•ˆç”¨ã€‚é’ˆå¯¹ä¼ ç»Ÿå¯å‘å¼æ–¹æ³•å¦‚FCFSå’ŒSJFåœ¨å¹³è¡¡å¤šç›®æ ‡å’Œé€‚åº”åŠ¨æ€è´Ÿè½½æ–¹é¢çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºReActæ¡†æ¶(Reason + Act)çš„è°ƒåº¦ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨scratchpad memoryè®°å½•è°ƒåº¦å†å²ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€åé¦ˆå’Œçº¦æŸæ‰§è¡Œæ¨¡å—ç¡®ä¿å†³ç­–çš„å¯è§£é‡Šæ€§ä¸å®‰å…¨æ€§ã€‚å®éªŒé‡‡ç”¨äº†GPT-4o-Miniå’ŒClaude 3.7ï¼Œå¹¶åœ¨å¤šç§çœŸå®HPCå·¥ä½œè´Ÿè½½åœºæ™¯ä¸‹ä¸Google OR-Toolsç­‰ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMè°ƒåº¦å™¨èƒ½æœ‰æ•ˆå¹³è¡¡å¤šä¸ªä¼˜åŒ–ç›®æ ‡ï¼Œä¸”åœ¨æ— éœ€é¢†åŸŸç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹å±•ç°å‡ºå“è¶Šçš„çº¦æŸæ»¡è¶³èƒ½åŠ›ã€‚å°½ç®¡æ¨ç†è´¨é‡ä¸è®¡ç®—å¼€é”€ä¹‹é—´å­˜åœ¨æƒè¡¡æŒ‘æˆ˜ï¼Œä½†è¯¥å·¥ä½œä½œä¸ºé¦–ä¸ªç»¼åˆç ”ç©¶ï¼Œè¯æ˜äº†å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMåœ¨å¤„ç†å¤æ‚HPCè°ƒåº¦é—®é¢˜ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "10 pages, 6 figures, work under review",
      "pdf_url": "https://arxiv.org/pdf/2506.02025v2",
      "published_date": "2025-05-29 14:25:29 UTC",
      "updated_date": "2025-09-03 20:30:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:52.813154+00:00"
    },
    {
      "arxiv_id": "2505.23454v2",
      "title": "LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals",
      "title_zh": "LCB-CV-UNetï¼šé«˜åŠ¨æ€èŒƒå›´é›·è¾¾ä¿¡å·å¢å¼ºå‹æ£€æµ‹å™¨",
      "authors": [
        "Yanbin Wang",
        "Xingyu Chen",
        "Yumiao Wang",
        "Xiang Wang",
        "Chuanfei Zang",
        "Guolong Cui",
        "Jiahuan Liu"
      ],
      "abstract": "We propose the LCB-CV-UNet to tackle performance degradation caused by High Dynamic Range (HDR) radar signals. Initially, a hardware-efficient, plug-and-play module named Logarithmic Connect Block (LCB) is proposed as a phase coherence preserving solution to address the inherent challenges in handling HDR features. Then, we propose the Dual Hybrid Dataset Construction method to generate a semi-synthetic dataset, approximating typical HDR signal scenarios with adjustable target distributions. Simulation results show about 1% total detection probability improvement with under 0.9% computational complexity added compared with the baseline. Furthermore, it excels 5% over the baseline at the range in 11-13 dB signal-to-noise ratio typical for urban targets. Finally, the real experiment validates the practicality of our model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LCB-CV-UNetï¼Œæ—¨åœ¨è§£å†³ High Dynamic Range (HDR) é›·è¾¾ä¿¡å·å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåä¸º Logarithmic Connect Block (LCB) çš„å³æ’å³ç”¨ç¡¬ä»¶é«˜æ•ˆæ¨¡å—ï¼Œä½œä¸ºä¸€ç§ä¿æŒç›¸ä½ä¸€è‡´æ€§çš„æ–¹æ¡ˆæ¥å¤„ç† HDR ç‰¹å¾å¸¦æ¥çš„å›ºæœ‰æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œç ”ç©¶æå‡ºäº† Dual Hybrid Dataset Construction æ–¹æ³•æ¥ç”ŸæˆåŠåˆæˆæ•°æ®é›†ï¼Œä»è€Œæ¨¡æ‹Ÿå…·æœ‰å¯è°ƒç›®æ ‡åˆ†å¸ƒçš„å…¸å‹ HDR ä¿¡å·åœºæ™¯ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—å¤æ‚åº¦å¢åŠ ä¸åˆ° 0.9% çš„æƒ…å†µä¸‹ï¼Œæ€»æ£€æµ‹æ¦‚ç‡æé«˜äº†çº¦ 1%ã€‚åœ¨åŸå¸‚ç›®æ ‡å…¸å‹çš„ 11-13 dB Signal-to-Noise Ratio èŒƒå›´å†…ï¼Œå…¶æ€§èƒ½ä¼˜äºåŸºå‡†æ¨¡å‹ 5% ä»¥ä¸Šã€‚æœ€åï¼Œé€šè¿‡å®é™…å®éªŒéªŒè¯äº†è¯¥æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "5 pages, 4 figures. Accepted to IEEE IGARSS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23454v2",
      "published_date": "2025-05-29 14:00:59 UTC",
      "updated_date": "2025-11-26 16:58:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:55.843440+00:00"
    },
    {
      "arxiv_id": "2506.00066v1",
      "title": "Literature Review Of Multi-Agent Debate For Problem-Solving",
      "title_zh": "é¢å‘é—®é¢˜è§£å†³çš„å¤šæ™ºèƒ½ä½“è¾©è®ºç ”ç©¶ç»¼è¿°",
      "authors": [
        "Arne Tillmann"
      ],
      "abstract": "Multi-agent large language models (MA-LLMs) are a rapidly growing research area that leverages multiple interacting language agents to tackle complex tasks, outperforming single-agent large language models. This literature review synthesizes the latest research on agent profiles, communication structures, and decision-making processes, drawing insights from both traditional multi-agent systems and state-of-the-art MA-LLM studies. In doing so, it aims to address the lack of direct comparisons in the field, illustrating how factors like scalability, communication structure, and decision-making processes influence MA-LLM performance. By examining frequent practices and outlining current challenges, the review reveals that multi-agent approaches can yield superior results but also face elevated computational costs and under-explored challenges unique to MA-LLM. Overall, these findings provide researchers and practitioners with a roadmap for developing robust and efficient multi-agent AI solutions.",
      "tldr_zh": "è¯¥æ–‡çŒ®ç»¼è¿°ç³»ç»Ÿåœ°æ€»ç»“äº†ç”¨äºè§£å†³é—®é¢˜çš„å¤šæ™ºèƒ½ä½“è¾©è®º(Multi-agent Debate)åŠå¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(MA-LLMs)çš„æœ€æ–°ç ”ç©¶è¿›å±•ã€‚æ–‡ç« é€šè¿‡æ•´åˆæ™ºèƒ½ä½“é…ç½®æ–‡ä»¶(agent profiles)ã€é€šä¿¡ç»“æ„(communication structures)å’Œå†³ç­–è¿‡ç¨‹(decision-making processes)çš„ç›¸å…³ç ”ç©¶ï¼Œæ¢è®¨äº†è¿™äº›å› ç´ å¦‚ä½•å½±å“ç³»ç»Ÿæ€§èƒ½ã€‚é’ˆå¯¹è¯¥é¢†åŸŸç¼ºä¹ç›´æ¥å¯¹æ¯”çš„é—®é¢˜ï¼Œç ”ç©¶è¯¦ç»†åˆ†æäº†å¯æ‰©å±•æ€§(scalability)ä¸é€šä¿¡æœºåˆ¶åœ¨å¤æ‚ä»»åŠ¡å¤„ç†ä¸­çš„ä½œç”¨ã€‚ç»¼è¿°æŒ‡å‡ºï¼Œå°½ç®¡å¤šæ™ºèƒ½ä½“æ–¹æ³•åœ¨å¤„ç†å¤æ‚é—®é¢˜ä¸Šä¼˜äºå•æ™ºèƒ½ä½“æ¨¡å‹ï¼Œä½†ä»é¢ä¸´è®¡ç®—æˆæœ¬é«˜æ˜‚ä»¥åŠMA-LLMç‰¹æœ‰çš„å¾…æ¢ç´¢æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶é€šè¿‡æ€»ç»“ç°æœ‰å®è·µå’Œå½“å‰æŒ‘æˆ˜ï¼Œä¸ºç ”ç©¶äººå‘˜å¼€å‘ç¨³å¥ä¸”é«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“AIè§£å†³æ–¹æ¡ˆæä¾›äº†é‡è¦çš„æŠ€æœ¯è·¯çº¿å›¾å’Œå‚è€ƒä¾æ®ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "11 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00066v1",
      "published_date": "2025-05-29 13:57:00 UTC",
      "updated_date": "2025-05-29 13:57:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T15:59:57.030787+00:00"
    },
    {
      "arxiv_id": "2506.00065v1",
      "title": "You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models",
      "title_zh": "ä½ åå¥½è¿™ä¸€ä¸ªï¼Œæˆ‘åçˆ±ä½ çš„ï¼šäººç±»ä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨æŒ‡ä»£è¯æ—¶é¢ä¸´æ¯”æ™®é€šè¯æ±‡æ›´é«˜çš„éš¾åº¦",
      "authors": [
        "Dota Tianai Dong",
        "Yifan Luo",
        "Po-Ya Angela Wang",
        "Asli Ozyurek",
        "Paula Rubio-Fernandez"
      ],
      "abstract": "Multimodal language models (MLMs) increasingly communicate in human-like ways, yet their ability to use reference words remains largely overlooked despite their ubiquity in everyday communication. Our study addresses this gap by comparing human and MLM use of three word classes with increasing cognitive demands: vocabulary words, possessive pronouns (`mine' vs `yours'), and demonstrative pronouns (`this one' vs `that one'). Evaluating seven state-of-the-art MLMs against human participants, we observe a clear difficulty hierarchy: while MLMs approach human-level performance on the vocabulary task, they show substantial deficits with possessives and demonstratives. Our analysis reveals these difficulties stem from limitations in perspective-taking and spatial reasoning. Although prompt engineering improved model performance on possessive use, demonstrative use remained well below human-level competence. These findings provide theoretical and empirical evidence that producing grammatical forms requiring pragmatics and social cognition remains a clear challenge in current NLP systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¯”äººç±»ä¸ä¸ƒç§å…ˆè¿›å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹(MLMs)åœ¨è¯æ±‡è¯(vocabulary words)ã€ç‰©ä¸»ä»£è¯(possessive pronouns)å’ŒæŒ‡ç¤ºä»£è¯(demonstrative pronouns)ä¸‰ç§è¯ç±»ä¸Šçš„ä½¿ç”¨æƒ…å†µï¼Œæ¢è®¨äº†æ¨¡å‹åœ¨æŒ‡ä»£è¯(reference words)å¤„ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒå‘ç°äº†ä¸€ä¸ªæ˜æ˜¾çš„éš¾åº¦å±‚çº§ï¼šè™½ç„¶MLMsåœ¨è¯æ±‡ä»»åŠ¡ä¸Šæ¥è¿‘äººç±»æ°´å¹³ï¼Œä½†åœ¨å¤„ç†ç‰©ä¸»ä»£è¯å’ŒæŒ‡ç¤ºä»£è¯æ—¶è¡¨ç°å‡ºæ˜¾è‘—ç¼ºé™·ã€‚åˆ†æè¡¨æ˜ï¼Œè¿™äº›å›°éš¾ä¸»è¦æºäºæ¨¡å‹åœ¨è§†è§’é‡‡å–(perspective-taking)å’Œç©ºé—´æ¨ç†(spatial reasoning)æ–¹é¢çš„å±€é™æ€§ã€‚å°½ç®¡æç¤ºå·¥ç¨‹(prompt engineering)èƒ½ä¸€å®šç¨‹åº¦ä¸Šæ”¹å–„æ¨¡å‹å¯¹ç‰©ä¸»ä»£è¯çš„ä½¿ç”¨ï¼Œä½†å…¶åœ¨æŒ‡ç¤ºä»£è¯ä¸Šçš„è¡¨ç°ä»è¿œä½äºäººç±»ã€‚è¯¥ç ”ç©¶ä¸ºå½“å‰è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ç³»ç»Ÿåœ¨ç”Ÿæˆæ¶‰åŠè¯­ç”¨å­¦(pragmatics)å’Œç¤¾ä¼šè®¤çŸ¥(social cognition)çš„è¯­æ³•å½¢å¼æ–¹é¢å­˜åœ¨çš„é‡å¤§æŒ‘æˆ˜æä¾›äº†ç†è®ºå’Œå®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.00065v1",
      "published_date": "2025-05-29 13:54:44 UTC",
      "updated_date": "2025-05-29 13:54:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:00:05.812063+00:00"
    },
    {
      "arxiv_id": "2506.00064v1",
      "title": "Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling",
      "title_zh": "Mis-promptï¼šå¤§è¯­è¨€æ¨¡å‹ä¸»åŠ¨é”™è¯¯å¤„ç†èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Jiayi Zeng",
        "Yizhe Feng",
        "Mengliang He",
        "Wenhui Lei",
        "Wei Zhang",
        "Zeming Liu",
        "Xiaoming Shi",
        "Aimin Zhou"
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant advancements in error handling. Current error-handling works are performed in a passive manner, with explicit error-handling instructions. However, in real-world scenarios, explicit error-handling instructions are usually unavailable. In this paper, our work identifies this challenge as how to conduct proactive error handling without explicit error handling instructions. To promote further research, this work introduces a new benchmark, termed Mis-prompt, consisting of four evaluation tasks, an error category taxonomy, and a new evaluation dataset. Furthermore, this work analyzes current LLMs' performance on the benchmark, and the experimental results reveal that current LLMs show poor performance on proactive error handling, and SFT on error handling instances improves LLMs' proactive error handling capabilities. The dataset will be publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é”™è¯¯å¤„ç†ä¸­è¿‡äºè¢«åŠ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸»åŠ¨é”™è¯¯å¤„ç†(Proactive Error Handling)çš„æ¦‚å¿µï¼Œå³åœ¨ç¼ºä¹æ˜¾å¼é”™è¯¯å¤„ç†æŒ‡ä»¤çš„ç°å®åœºæ™¯ä¸‹è¯†åˆ«å¹¶ä¿®æ­£é”™è¯¯ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†åä¸ºMis-promptçš„å…¨æ–°åŸºå‡†æµ‹è¯•ï¼Œå…¶åŒ…å«å››é¡¹è¯„ä¼°ä»»åŠ¡ã€ä¸€å¥—é”™è¯¯ç±»åˆ«åˆ†ç±»æ³•(taxonomy)ä»¥åŠä¸€ä¸ªä¸“é—¨çš„è¯„ä¼°æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰ä¸»æµLLMsåœ¨ä¸»åŠ¨é”™è¯¯å¤„ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æ™®éè¾ƒå·®ï¼Œéš¾ä»¥åœ¨æ— æç¤ºæƒ…å†µä¸‹è‡ªä¸»åº”å¯¹é”™è¯¯è¾“å…¥ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œé€šè¿‡åœ¨é”™è¯¯å¤„ç†å®ä¾‹ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ(SFT)ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„ä¸»åŠ¨é”™è¯¯å¤„ç†èƒ½åŠ›ã€‚è¯¥å·¥ä½œå¡«è¡¥äº†LLMsåœ¨å¤æ‚ç°å®åœºæ™¯ä¸‹è‡ªä¸»å®¹é”™è¯„ä¼°çš„ç©ºç™½ï¼Œå¹¶ä¸ºæœªæ¥æ„å»ºæ›´å…·é²æ£’æ€§çš„æ¨¡å‹æä¾›äº†æ•°æ®æ”¯æŒå’Œä¼˜åŒ–è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00064v1",
      "published_date": "2025-05-29 13:52:58 UTC",
      "updated_date": "2025-05-29 13:52:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:00:02.239812+00:00"
    },
    {
      "arxiv_id": "2505.23444v4",
      "title": "CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis",
      "title_zh": "CryoCCDï¼šèåˆç”Ÿç‰©ç‰©ç†å»ºæ¨¡ä¸æ¡ä»¶å¾ªç¯ä¸€è‡´æ‰©æ•£æ¨¡å‹çš„å†·å†»ç”µé•œå›¾åƒåˆæˆ",
      "authors": [
        "Runmin Jiang",
        "Genpei Zhang",
        "Yuntian Yang",
        "Siqi Wu",
        "Minhao Wu",
        "Wanyue Feng",
        "Yizhou Zhao",
        "Xi Xiao",
        "Xiao Wang",
        "Tianyang Wang",
        "Xingjian Li",
        "Muyuan Chen",
        "Min Xu"
      ],
      "abstract": "Single-particle cryo-electron microscopy (cryo-EM) has become a cornerstone of structural biology, enabling near-atomic resolution analysis of macromolecules through advanced computational methods. However, the development of cryo-EM processing tools is constrained by the scarcity of high-quality annotated datasets. Synthetic data generation offers a promising alternative, but existing approaches lack thorough biophysical modeling of heterogeneity and fail to reproduce the complex noise observed in real imaging. To address these limitations, we present CryoCCD, a synthesis framework that unifies versatile biophysical modeling with the first conditional cycle-consistent diffusion model tailored for cryo-EM. The biophysical engine provides multi-functional generation capabilities to capture authentic biological organization, and the diffusion model is enhanced with cycle consistency and mask-guided contrastive learning to ensure realistic noise while preserving structural fidelity. Extensive experiments demonstrate that CryoCCD generates structurally faithful micrographs, enhances particle picking and pose estimation, as well as achieves superior performance over state-of-the-art baselines, while also generalizing effectively to held-out protein families.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CryoCCDï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å¤šåŠŸèƒ½ biophysical modeling å’Œé¦–ä¸ªä¸“ä¸º cryo-EM è®¾è®¡çš„ conditional cycle-consistent diffusion model çš„åˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åˆæˆæ•°æ®æ–¹æ³•åœ¨å¼‚è´¨æ€§å»ºæ¨¡å’Œå¤æ‚å™ªå£°æ¨¡æ‹Ÿæ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿç‰©ç‰©ç†å¼•æ“æ•æ‰çœŸå®çš„ç”Ÿç‰©ç»„ç»‡ç»“æ„ï¼Œå¹¶é€šè¿‡ cycle consistency å’Œ mask-guided contrastive learning å¢å¼ºæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œåœ¨ç”Ÿæˆé€¼çœŸå™ªå£°çš„åŒæ—¶ä¿æŒæé«˜çš„ç»“æ„ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCryoCCD ç”Ÿæˆçš„æ˜¾å¾®å›¾åœ¨ç»“æ„ä¸Šé«˜åº¦å¯é ï¼Œèƒ½æ˜¾è‘—æå‡ particle picking å’Œ pose estimation ç­‰åç»­å¤„ç†ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚ç›¸æ¯”äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼ŒCryoCCD è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œå¹¶èƒ½æœ‰æ•ˆæ³›åŒ–è‡³æœªè§è¿‡çš„è›‹ç™½è´¨å®¶æ—ï¼Œä¸ºç»“æ„ç”Ÿç‰©å­¦ç ”ç©¶æä¾›äº†é«˜è´¨é‡çš„åˆæˆæ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23444v4",
      "published_date": "2025-05-29 13:40:59 UTC",
      "updated_date": "2025-10-05 01:40:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:00:28.773455+00:00"
    },
    {
      "arxiv_id": "2505.23437v1",
      "title": "Bounded-Abstention Pairwise Learning to Rank",
      "title_zh": "æœ‰ç•Œå¼ƒæƒæˆå¯¹æ’åºå­¦ä¹ ",
      "authors": [
        "Antonio Ferrara",
        "Andrea Pugnana",
        "Francesco Bonchi",
        "Salvatore Ruggieri"
      ],
      "abstract": "Ranking systems influence decision-making in high-stakes domains like health, education, and employment, where they can have substantial economic and social impacts. This makes the integration of safety mechanisms essential. One such mechanism is $\\textit{abstention}$, which enables algorithmic decision-making system to defer uncertain or low-confidence decisions to human experts. While abstention have been predominantly explored in the context of classification tasks, its application to other machine learning paradigms remains underexplored. In this paper, we introduce a novel method for abstention in pairwise learning-to-rank tasks. Our approach is based on thresholding the ranker's conditional risk: the system abstains from making a decision when the estimated risk exceeds a predefined threshold. Our contributions are threefold: a theoretical characterization of the optimal abstention strategy, a model-agnostic, plug-in algorithm for constructing abstaining ranking models, and a comprehensive empirical evaluations across multiple datasets, demonstrating the effectiveness of our approach.",
      "tldr_zh": "æ’åºç³»ç»Ÿåœ¨åŒ»ç–—ã€æ•™è‚²ç­‰é«˜é£é™©é¢†åŸŸå…·æœ‰æ·±è¿œå½±å“ï¼Œå› æ­¤å¼•å…¥å¼ƒæƒ(abstention)æœºåˆ¶ä»¥æé«˜å®‰å…¨æ€§å˜å¾—è‡³å…³é‡è¦ã€‚è™½ç„¶å¼ƒæƒæœºåˆ¶åœ¨åˆ†ç±»ä»»åŠ¡ä¸­å·²å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†åœ¨æˆå¯¹æ’åºå­¦ä¹ (pairwise learning-to-rank)ä¸­çš„åº”ç”¨ä»æœ‰å¾…æ¢ç´¢ã€‚è¯¥ç ”ç©¶é’ˆå¯¹æ­¤ç±»ä»»åŠ¡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ƒæƒæ–¹æ³•ï¼Œå…¶æ ¸å¿ƒé€»è¾‘æ˜¯é€šè¿‡å¯¹æ’åºå™¨çš„æ¡ä»¶é£é™©(conditional risk)è®¾ç½®é˜ˆå€¼ï¼Œå½“ä¼°è®¡é£é™©è¶…è¿‡é¢„è®¾å€¼æ—¶ï¼Œç³»ç»Ÿä¼šé€‰æ‹©å¼ƒæƒå¹¶å°†å†³ç­–è½¬äº¤ç»™äººç±»ä¸“å®¶ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬å¯¹æœ€ä¼˜å¼ƒæƒç­–ç•¥çš„ç†è®ºåˆ»ç”»ï¼Œä»¥åŠå¼€å‘äº†ä¸€ç§æ¨¡å‹æ— å…³(model-agnostic)çš„æ’ä»¶å¼ç®—æ³•æ¥æ„å»ºå¼ƒæƒæ’åºæ¨¡å‹ã€‚å¤šé¡¹æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå¤„ç†æ’åºè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨ä¿éšœå†³ç­–å®‰å…¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23437v1",
      "published_date": "2025-05-29 13:35:39 UTC",
      "updated_date": "2025-05-29 13:35:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:01:03.630713+00:00"
    },
    {
      "arxiv_id": "2505.23436v4",
      "title": "Emergent Risk Awareness in Rational Agents under Resource Constraints",
      "title_zh": "èµ„æºçº¦æŸä¸‹ç†æ€§æ™ºèƒ½ä½“çš„é£é™©æ„è¯†æ¶Œç°",
      "authors": [
        "Daniel Jarne Ornia",
        "Nicholas Bishop",
        "Joel Dyer",
        "Wei-Chen Lee",
        "Ani Calinescu",
        "Doyne Farmer",
        "Michael Wooldridge"
      ],
      "abstract": "Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision-making problems under (approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade-offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival-driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource-limited environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨èµ„æºæˆ–å¤±è´¥çº¦æŸ(resource or failure constraints)ä¸‹çš„AIæ™ºèƒ½ä½“(AI agents)åœ¨åºåˆ—å†³ç­–è¿‡ç¨‹ä¸­çš„è¡Œä¸ºé€»è¾‘ã€‚è®ºæ–‡é€šè¿‡ç”Ÿå­˜å¤šè‡‚è€è™æœº(survival bandit)æ¡†æ¶å¯¹è¿™ä¸€è®¾å®šè¿›è¡Œäº†å½¢å¼åŒ–å¤„ç†ï¼Œæ­ç¤ºäº†ç”Ÿå­˜å‹åŠ›å¦‚ä½•å¯¼è‡´æ™ºèƒ½ä½“çš„åå¥½å‘ç”Ÿåç§»ï¼Œä»è€Œåœ¨äººç±»ç›®æ ‡ä¸æ™ºèƒ½ä½“åŠ¨æœºä¹‹é—´äº§ç”Ÿé¢„æ–™ä¹‹å¤–çš„é”™ä½(misalignment)ã€‚ç ”ç©¶é€šè¿‡ç†è®ºä¸å®è¯åˆ†æé‡åŒ–äº†ç”Ÿå­˜é©±åŠ¨å¯¹åå¥½çš„å½±å“ï¼Œå¹¶è¯†åˆ«äº†å¯¼è‡´é£é™©å¯»æ±‚æˆ–é£é™©åŒæ¶è¡Œä¸ºäº§ç”Ÿçš„ç‰¹å®šæ¡ä»¶ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç³»åˆ—ç¼“è§£æœºåˆ¶ï¼Œæ—¨åœ¨å¼•å¯¼æ™ºèƒ½ä½“åœ¨å—é™ç¯å¢ƒä¸‹åšå‡ºæ›´ç¬¦åˆäººç±»é¢„æœŸçš„å†³ç­–ã€‚è¯¥å·¥ä½œé€šè¿‡å¯¹æ¶Œç°é£é™©æ„è¯†(emergent risk awareness)çš„æ·±å…¥åˆ†æï¼Œä¸ºå…³é”®èµ„æºå—é™ç¯å¢ƒä¸‹çš„AIç³»ç»Ÿéƒ¨ç½²æä¾›äº†é‡è¦çš„è§£é‡Šå·¥å…·å’Œå®‰å…¨æŒ‡å—ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23436v4",
      "published_date": "2025-05-29 13:31:12 UTC",
      "updated_date": "2025-09-24 13:44:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:00:43.536259+00:00"
    },
    {
      "arxiv_id": "2505.23432v2",
      "title": "A Mathematical Framework for AI-Human Integration in Work",
      "title_zh": "å·¥ä½œä¸­äººå·¥æ™ºèƒ½ä¸äººç±»èåˆçš„æ•°å­¦æ¡†æ¶",
      "authors": [
        "L. Elisa Celis",
        "Lingxiao Huang",
        "Nisheeth K. Vishnoi"
      ],
      "abstract": "The rapid rise of Generative AI (GenAI) tools has sparked debate over their role in complementing or replacing human workers across job contexts. We present a mathematical framework that models jobs, workers, and worker-job fit, introducing a novel decomposition of skills into decision-level and action-level subskills to reflect the complementary strengths of humans and GenAI. We analyze how changes in subskill abilities affect job success, identifying conditions for sharp transitions in success probability. We also establish sufficient conditions under which combining workers with complementary subskills significantly outperforms relying on a single worker. This explains phenomena such as productivity compression, where GenAI assistance yields larger gains for lower-skilled workers. We demonstrate the framework' s practicality using data from O*NET and Big-Bench Lite, aligning real-world data with our model via subskill-division methods. Our results highlight when and how GenAI complements human skills, rather than replacing them.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½ä¸äººç±»åœ¨å·¥ä½œä¸­é›†æˆçš„æ•°å­¦æ¡†æ¶ï¼Œæ—¨åœ¨æ¢è®¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(GenAI)åœ¨ä¸åŒèŒä¸šèƒŒæ™¯ä¸‹æ˜¯äº’è¡¥è¿˜æ˜¯æ›¿ä»£äººç±»å·¥äººçš„è§’è‰²ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æŠ€èƒ½åˆ†è§£æ–¹æ³•ï¼Œå°†æŠ€èƒ½åˆ†ä¸ºå†³ç­–å±‚(decision-level)å’Œè¡ŒåŠ¨å±‚(action-level)å­æŠ€èƒ½ï¼Œä»¥åæ˜ äººç±»ä¸GenAIçš„äº’è¡¥ä¼˜åŠ¿ã€‚é€šè¿‡åˆ†æå­æŠ€èƒ½èƒ½åŠ›å¯¹å·¥ä½œæˆåŠŸçš„å½±å“ï¼Œç ”ç©¶ç¡®å®šäº†æˆåŠŸæ¦‚ç‡å‘ç”Ÿæ€¥å‰§è½¬å˜çš„æ¡ä»¶ï¼Œå¹¶è¯æ˜äº†ç»“åˆäº’è¡¥å­æŠ€èƒ½çš„åä½œæ¨¡å¼èƒ½æ˜¾è‘—ä¼˜äºå•ä¸€ä¸»ä½“ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆè§£é‡Šäº†â€œç”Ÿäº§åŠ›å‹ç¼©(productivity compression)â€ç°è±¡ï¼Œå³GenAIå¯¹ä½æŠ€èƒ½å·¥äººçš„ç”Ÿäº§åŠ›æå‡æ›´ä¸ºæ˜¾è‘—ã€‚ç ”ç©¶é€šè¿‡O*NETå’ŒBig-Bench Liteçš„æ•°æ®éªŒè¯äº†æ¨¡å‹çš„å®ç”¨æ€§ï¼Œæ­ç¤ºäº†GenAIåœ¨ä½•æ—¶ä»¥åŠå¦‚ä½•é€šè¿‡è¡¥å……äººç±»æŠ€èƒ½æ¥ä¼˜åŒ–å·¥ä½œè¡¨ç°ï¼Œè€Œéå•çº¯æ›¿ä»£äººç±»ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "econ.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper will appear in ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23432v2",
      "published_date": "2025-05-29 13:26:21 UTC",
      "updated_date": "2025-05-30 10:51:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:00:31.299901+00:00"
    },
    {
      "arxiv_id": "2505.23426v2",
      "title": "Enhanced DACER Algorithm with High Diffusion Efficiency",
      "title_zh": "é«˜æ‰©æ•£æ•ˆç‡çš„å¢å¼ºå‹ DACER ç®—æ³•",
      "authors": [
        "Yinuo Wang",
        "Likun Wang",
        "Mining Tan",
        "Wenjun Zou",
        "Xujie Song",
        "Wenxuan Wang",
        "Tong Liu",
        "Guojian Zhan",
        "Tianze Zhu",
        "Shiqi Liu",
        "Zeyu He",
        "Feihong Zhang",
        "Jingliang Duan",
        "Shengbo Eben Li"
      ],
      "abstract": "Due to their expressive capacity, diffusion models have shown great promise in offline RL and imitation learning. Diffusion Actor-Critic with Entropy Regulator (DACER) extended this capability to online RL by using the reverse diffusion process as a policy approximator, achieving state-of-the-art performance. However, it still suffers from a core trade-off: more diffusion steps ensure high performance but reduce efficiency, while fewer steps degrade performance. This remains a major bottleneck for deploying diffusion policies in real-time online RL. To mitigate this, we propose DACERv2, which leverages a Q-gradient field objective with respect to action as an auxiliary optimization target to guide the denoising process at each diffusion step, thereby introducing intermediate supervisory signals that enhance the efficiency of single-step diffusion. Additionally, we observe that the independence of the Q-gradient field from the diffusion time step is inconsistent with the characteristics of the diffusion process. To address this issue, a temporal weighting mechanism is introduced, allowing the model to effectively eliminate large-scale noise during the early stages and refine its outputs in the later stages. Experimental results on OpenAI Gym benchmarks and multimodal tasks demonstrate that, compared with classical and diffusion-based online RL algorithms, DACERv2 achieves higher performance in most complex control environments with only five diffusion steps and shows greater multimodality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨çº¿å¼ºåŒ–å­¦ä¹  (Online RL) ä¸­æ‰©æ•£ç­–ç•¥é¢ä¸´çš„æ€§èƒ½ä¸æ•ˆç‡æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†æ”¹è¿›åçš„ DACERv2 ç®—æ³•ã€‚è¯¥ç®—æ³•å¼•å…¥äº†å…³äºåŠ¨ä½œçš„ Q-gradient field ä½œä¸ºè¾…åŠ©ä¼˜åŒ–ç›®æ ‡ï¼Œç”¨ä»¥å¼•å¯¼æ¯ä¸€æ­¥æ‰©æ•£è¿‡ç¨‹çš„å»å™ªï¼Œå¹¶ä¸ºå•æ­¥æ‰©æ•£æä¾›äº†ä¸­é—´ç›‘ç£ä¿¡å·ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ‰©æ•£æ•ˆç‡ã€‚é’ˆå¯¹ Q-gradient field ä¸æ‰©æ•£æ—¶é—´æ­¥ç‹¬ç«‹æ€§çš„ä¸ä¸€è‡´é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†æ—¶é—´åŠ æƒæœºåˆ¶ (Temporal Weighting Mechanism)ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ—©æœŸæœ‰æ•ˆæ¶ˆé™¤å¤§è§„æ¨¡å™ªå£°å¹¶åœ¨åæœŸç²¾ç»†åŒ–è¾“å‡ºã€‚åœ¨ OpenAI Gym åŸºå‡†æµ‹è¯•å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDACERv2 ä»…éœ€ 5 ä¸ªæ‰©æ•£æ­¥å³å¯åœ¨å¤§å¤šæ•°å¤æ‚æ§åˆ¶ç¯å¢ƒä¸­å–å¾—æ¯”ä¼ ç»ŸåŠåŸºäºæ‰©æ•£çš„åœ¨çº¿ RL ç®—æ³•æ›´é«˜çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•è¿˜å±•ç¤ºäº†æ›´å¼ºçš„ Multimodality èƒ½åŠ›ï¼Œä¸ºå®æ—¶åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­éƒ¨ç½²æ‰©æ•£ç­–ç•¥æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23426v2",
      "published_date": "2025-05-29 13:21:58 UTC",
      "updated_date": "2025-10-02 12:34:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:00:35.242051+00:00"
    },
    {
      "arxiv_id": "2505.23422v1",
      "title": "From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents",
      "title_zh": "ä»çŸ¥è¯†åˆ°å™ªå£°ï¼šCTIM-Rover ä¸è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ä¸­æƒ…èŠ‚è®°å¿†çš„é™·é˜±",
      "authors": [
        "Tobias Lindenbauer",
        "Georg Groh",
        "Hinrich SchÃ¼tze"
      ],
      "abstract": "We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on top of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning frameworks with an episodic memory, more specifically, a general and repository-level Cross-Task-Instance Memory (CTIM). While existing open-source SE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al., 2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning frameworks inefficiently discard their long-term memory after a single task instance. As repository-level understanding is pivotal for identifying all locations requiring a patch for fixing a bug, we hypothesize that SE is particularly well positioned to benefit from CTIM. For this, we build on the Experiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a Mixture-Of-Experts (MoEs) inspired approach to create both a general-purpose and repository-level CTIM. We find that CTIM-Rover does not outperform AutoCodeRover in any configuration and thus conclude that neither ExpeL nor DoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis indicates noise introduced by distracting CTIM items or exemplar trajectories as the likely source of the performance degradation.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† CTIM-Roverï¼Œè¿™æ˜¯ä¸€ç§å»ºç«‹åœ¨ AutoCodeRover ä¹‹ä¸Šçš„è½¯ä»¶å·¥ç¨‹ (Software Engineering) AI æ™ºèƒ½ä½“ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†è·¨ä»»åŠ¡å®ä¾‹è®°å¿† (Cross-Task-Instance Memory, CTIM) æ¥è§£å†³ç°æœ‰æ¨ç†æ¡†æ¶åœ¨å®Œæˆå•ä¸€ä»»åŠ¡åä¸¢å¼ƒé•¿æœŸè®°å¿†çš„é—®é¢˜ã€‚ç ”ç©¶è€…åŸºäºç»éªŒå­¦ä¹  (Experiential Learning) æ–¹æ³• ExpeLï¼Œæå‡ºäº†ä¸€ç§å—æ··åˆä¸“å®¶æ¨¡å‹ (Mixture-Of-Experts) å¯å‘çš„æ–¹æ¡ˆï¼Œæ—¨åœ¨åˆ©ç”¨ä»“åº“çº§çš„è®°å¿†æ¥æå‡æ¼æ´ä¿®å¤çš„ç²¾ç¡®åº¦ã€‚ç„¶è€Œï¼Œå®éªŒç»“æœè¡¨æ˜ CTIM-Rover åœ¨å„ç§é…ç½®ä¸‹çš„è¡¨ç°å‡æœªä¼˜äº AutoCodeRoverï¼Œè¯æ˜äº†ç°æœ‰çš„ ExpeL å’Œ DoT-Bank æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ‰©å±•è‡³çœŸå®çš„è½¯ä»¶å·¥ç¨‹åœºæ™¯ã€‚åˆ†ææŒ‡å‡ºï¼Œæ¥è‡ªè®°å¿†é¡¹æˆ–ç¤ºä¾‹è½¨è¿¹çš„å™ªå£°æ˜¯å¯¼è‡´æ€§èƒ½ä¸‹é™çš„ä¸»è¦åŸå› ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†åœ¨è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ä¸­é›†æˆæƒ…å¢ƒè®°å¿† (Episodic Memory) å¯èƒ½é¢ä¸´çš„å±€é™ä¸é™·é˜±ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Short Paper, REALM '25 camera-ready",
      "pdf_url": "https://arxiv.org/pdf/2505.23422v1",
      "published_date": "2025-05-29 13:19:29 UTC",
      "updated_date": "2025-05-29 13:19:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:01:01.315054+00:00"
    },
    {
      "arxiv_id": "2506.03179v1",
      "title": "Vid-SME: Membership Inference Attacks against Large Video Understanding Models",
      "title_zh": "Vid-SMEï¼šé’ˆå¯¹å¤§å‹è§†é¢‘ç†è§£æ¨¡å‹çš„æˆå‘˜æ¨ç†æ”»å‡»",
      "authors": [
        "Qi Li",
        "Runpeng Yu",
        "Xinchao Wang"
      ],
      "abstract": "Multimodal large language models (MLLMs) demonstrate remarkable capabilities in handling complex multimodal tasks and are increasingly adopted in video understanding applications. However, their rapid advancement raises serious data privacy concerns, particularly given the potential inclusion of sensitive video content, such as personal recordings and surveillance footage, in their training datasets. Determining improperly used videos during training remains a critical and unresolved challenge. Despite considerable progress on membership inference attacks (MIAs) for text and image data in MLLMs, existing methods fail to generalize effectively to the video domain. These methods suffer from poor scalability as more frames are sampled and generally achieve negligible true positive rates at low false positive rates (TPR@Low FPR), mainly due to their failure to capture the inherent temporal variations of video frames and to account for model behavior differences as the number of frames varies. To address these challenges, we introduce Vid-SME, the first membership inference method tailored for video data used in video understanding LLMs (VULLMs). Vid-SME leverages the confidence of model output and integrates adaptive parameterization to compute Sharma-Mittal entropy (SME) for video inputs. By leveraging the SME difference between natural and temporally-reversed video frames, Vid-SME derives robust membership scores to determine whether a given video is part of the model's training set. Experiments on various self-trained and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘ç†è§£å¤§æ¨¡å‹(Video Understanding Large Language Models, VULLMs)é¢ä¸´çš„æ•°æ®éšç§é£é™©ï¼Œæå‡ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹è§†é¢‘æ•°æ®çš„æˆå‘˜æ¨ç†æ”»å‡»(Membership Inference Attacks, MIAs)æ¡†æ¶Vid-SMEã€‚ç°æœ‰çš„æ–‡æœ¬æˆ–å›¾åƒæˆå‘˜æ¨ç†æ–¹æ³•åœ¨å¤„ç†è§†é¢‘æ—¶ï¼Œç”±äºæ— æ³•æœ‰æ•ˆæ•æ‰å¸§é—´çš„æ—¶é—´å˜åŒ–(Temporal Variations)ä¸”åœ¨ä½è¯¯æŠ¥ç‡ä¸‹çš„çœŸæ­£ä¾‹ç‡(TPR@Low FPR)è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥ç›´æ¥è¿ç§»åº”ç”¨ã€‚Vid-SMEé€šè¿‡æ•´åˆæ¨¡å‹è¾“å‡ºçš„ç½®ä¿¡åº¦å¹¶ç»“åˆè‡ªé€‚åº”å‚æ•°åŒ–æŠ€æœ¯ï¼Œè®¡ç®—è§†é¢‘è¾“å…¥çš„Sharma-Mittal Entropy (SME)ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨è‡ªç„¶è§†é¢‘å¸§ä¸æ—¶é—´åè½¬è§†é¢‘å¸§ä¹‹é—´çš„SMEå·®å¼‚æ¥ç”Ÿæˆé²æ£’çš„æˆå‘˜è¯„åˆ†ï¼Œä»è€Œç²¾ç¡®åˆ¤å®šç‰¹å®šè§†é¢‘æ˜¯å¦å±äºæ¨¡å‹çš„è®­ç»ƒé›†ã€‚åœ¨å¤šç§è‡ªè®­ç»ƒå’Œå¼€æºVULLMsä¸Šçš„å®éªŒç»“æœè¯æ˜äº†Vid-SMEçš„å¼ºå¤§æœ‰æ•ˆæ€§ï¼Œä¸ºè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹çš„éšç§å®‰å…¨æ€§æä¾›äº†å…³é”®æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03179v1",
      "published_date": "2025-05-29 13:17:25 UTC",
      "updated_date": "2025-05-29 13:17:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:00:52.340804+00:00"
    },
    {
      "arxiv_id": "2505.23419v2",
      "title": "SWE-bench Goes Live!",
      "title_zh": "SWE-bench æ­¥å…¥å®æ—¶ï¼",
      "authors": [
        "Linghao Zhang",
        "Shilin He",
        "Chaoyun Zhang",
        "Yu Kang",
        "Bowen Li",
        "Chengxing Xie",
        "Junhao Wang",
        "Maoquan Wang",
        "Yufan Huang",
        "Shengyu Fu",
        "Elsie Nallipogu",
        "Qingwei Lin",
        "Yingnong Dang",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "abstract": "The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present SWE-bench-Live, a live-updatable benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \\method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† SWE-bench-Liveï¼Œè¿™æ˜¯ä¸€ä¸ªå¯åŠ¨æ€æ›´æ–°çš„åŸºå‡†æµ‹è¯•é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•å¦‚ SWE-bench å­˜åœ¨çš„ç‰ˆæœ¬é™ˆæ—§ã€ä»£ç åº“è¦†ç›–çª„åŠé«˜åº¦ä¾èµ–äººå·¥æ„å»ºç­‰å±€é™ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—è‡ªåŠ¨åŒ–ç­–åˆ’æµæ°´çº¿ (automated curation pipeline)ï¼Œå®ç°äº†ä»å®ä¾‹åˆ›å»ºåˆ°ç¯å¢ƒé…ç½®çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ï¼Œæ¶ˆé™¤äº†äººå·¥ç“¶é¢ˆå¹¶æ”¯æŒæŒç»­æ›´æ–°ã€‚é¦–æ‰¹å‘å¸ƒçš„ç‰ˆæœ¬åŒ…å« 1,319 ä¸ªæºè‡ª 2024 å¹´ä»¥æ¥çœŸå® GitHub issues çš„ä»»åŠ¡ï¼Œæ¶µç›– 93 ä¸ªä»£ç åº“ï¼Œå¹¶ä¸ºæ¯ä¸ªä»»åŠ¡æä¾›ä¸“é—¨çš„ Docker é•œåƒä»¥ç¡®ä¿æ‰§è¡Œçš„å¯é‡å¤æ€§ã€‚é€šè¿‡å¯¹å¤šç§å…ˆè¿›çš„ Agent æ¡†æ¶å’Œå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹åœ¨ SWE-bench-Live ä¸Šçš„è¡¨ç°ä¸åœ¨é™æ€åŸºå‡†æµ‹è¯•ä¸Šç›¸æ¯”å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚è¯¥ç ”ç©¶é€šè¿‡å¯¹ä»£ç åº“æ¥æºã€é—®é¢˜æ–°è¿‘åº¦å’Œä»»åŠ¡éš¾åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹å¯èƒ½é¢ä¸´çš„è¿‡æ‹Ÿåˆå’Œæ•°æ®æ±¡æŸ“ (data contamination) é£é™©ã€‚ä½œä¸ºä¸€ç§åŸºäºå®æ—¶ä»£ç åº“æ´»åŠ¨çš„åŠ¨æ€è¯„ä¼°å·¥å…·ï¼ŒSWE-bench-Live ä¸ºåœ¨çœŸå®çš„è½¯ä»¶å¼€å‘ç¯å¢ƒä¸­å¯¹ LLMs å’Œæ™ºèƒ½ä½“è¿›è¡Œä¸¥è°¨ã€æŠ—æ±¡æŸ“çš„æ€§èƒ½è¡¡é‡æä¾›äº†å…³é”®å¹³å°ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "Homepage: \\url{https://swe-bench-live.github.io/}, Code: \\url{https://github.com/SWE-bench-Live}, Dataset: \\url{https://huggingface.co/SWE-bench-Live}",
      "pdf_url": "https://arxiv.org/pdf/2505.23419v2",
      "published_date": "2025-05-29 13:09:44 UTC",
      "updated_date": "2025-06-01 14:53:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:01:16.047143+00:00"
    },
    {
      "arxiv_id": "2505.23417v1",
      "title": "Toward Effective AI Governance: A Review of Principles",
      "title_zh": "è¿ˆå‘æœ‰æ•ˆçš„äººå·¥æ™ºèƒ½æ²»ç†ï¼šåŸåˆ™ç»¼è¿°",
      "authors": [
        "Danilo Ribeiro",
        "Thayssa Rocha",
        "Gustavo Pinto",
        "Bruno Cartaxo",
        "Marcelo Amaral",
        "Nicole Davila",
        "Ana Camargo"
      ],
      "abstract": "Artificial Intelligence (AI) governance is the practice of establishing frameworks, policies, and procedures to ensure the responsible, ethical, and safe development and deployment of AI systems. Although AI governance is a core pillar of Responsible AI, current literature still lacks synthesis across such governance frameworks and practices. Objective: To identify which frameworks, principles, mechanisms, and stakeholder roles are emphasized in secondary literature on AI governance. Method: We conducted a rapid tertiary review of nine peer-reviewed secondary studies from IEEE and ACM (20202024), using structured inclusion criteria and thematic semantic synthesis. Results: The most cited frameworks include the EU AI Act and NIST RMF; transparency and accountability are the most common principles. Few reviews detail actionable governance mechanisms or stakeholder strategies. Conclusion: The review consolidates key directions in AI governance and highlights gaps in empirical validation and inclusivity. Findings inform both academic inquiry and practical adoption in organizations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½æ²»ç†(AI Governance)é¢†åŸŸç¼ºä¹ç»¼åˆæ€§æ¡†æ¶ä¸å®è·µç»¼è¿°çš„ç°çŠ¶ï¼Œå¯¹2020è‡³2024å¹´é—´IEEEå’ŒACMå‘è¡¨çš„ä¹é¡¹åŒè¡Œè¯„å®¡äºŒæ¬¡ç ”ç©¶è¿›è¡Œäº†å¿«é€Ÿä¸‰çº§ç»¼è¿°(Rapid Tertiary Review)ã€‚é€šè¿‡ç»“æ„åŒ–å…¥é€‰æ ‡å‡†å’Œä¸»é¢˜è¯­ä¹‰ç»¼åˆï¼Œç ”ç©¶è¯†åˆ«äº†ç°æœ‰æ–‡çŒ®ä¸­å¼ºè°ƒçš„æ¡†æ¶ã€åŸåˆ™ã€æ²»ç†æœºåˆ¶å’Œåˆ©ç›Šç›¸å…³è€…è§’è‰²ã€‚ç»“æœè¡¨æ˜ï¼ŒEU AI Actå’ŒNIST RMFæ˜¯ç›®å‰æœ€å—å…³æ³¨çš„æ²»ç†æ¡†æ¶ï¼Œè€Œé€æ˜æ€§(Transparency)å’Œé—®è´£åˆ¶(Accountability)åˆ™æ˜¯æœ€æ ¸å¿ƒçš„å…±è¯†åŸåˆ™ã€‚ç»¼è¿°æŒ‡å‡ºï¼Œå½“å‰æ–‡çŒ®è¾ƒå°‘æ·±å…¥æ¢è®¨å¯æ“ä½œçš„æ²»ç†æœºåˆ¶æˆ–å…·ä½“çš„åˆ©ç›Šç›¸å…³è€…ç­–ç•¥ï¼Œä¸”åœ¨å®è¯éªŒè¯å’ŒåŒ…å®¹æ€§æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾ç¼ºå£ã€‚è¯¥é¡¹å·¥ä½œæ•´åˆäº†AIæ²»ç†çš„å…³é”®å‘å±•æ–¹å‘ï¼Œä¸ºåç»­å­¦æœ¯ç ”ç©¶ä»¥åŠç»„ç»‡æœºæ„çš„å®é™…è½åœ°åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23417v1",
      "published_date": "2025-05-29 13:07:45 UTC",
      "updated_date": "2025-05-29 13:07:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:01:29.202918+00:00"
    },
    {
      "arxiv_id": "2505.23415v2",
      "title": "Bidirectional predictive coding",
      "title_zh": "åŒå‘é¢„æµ‹ç¼–ç ",
      "authors": [
        "Gaspard Oliviers",
        "Mufeng Tang",
        "Rafal Bogacz"
      ],
      "abstract": "Predictive coding (PC) is an influential computational model of visual learning and inference in the brain. Classical PC was proposed as a top-down generative model, where the brain actively predicts upcoming visual inputs, and inference minimises the prediction errors. Recent studies have also shown that PC can be formulated as a discriminative model, where sensory inputs predict neural activities in a feedforward manner. However, experimental evidence suggests that the brain employs both generative and discriminative inference, while unidirectional PC models show degraded performance in tasks requiring bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC model that incorporates both generative and discriminative inference while maintaining a biologically plausible circuit implementation. We show that bPC matches or outperforms unidirectional models in their specialised generative or discriminative tasks, by developing an energy landscape that simultaneously suits both tasks. We also demonstrate bPC's superior performance in two biologically relevant tasks including multimodal learning and inference with missing information, suggesting that bPC resembles biological visual inference more closely.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åŒå‘é¢„æµ‹ç¼–ç (bidirectional predictive coding, bPC)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé¢„æµ‹ç¼–ç (Predictive coding, PC)æ¨¡å‹ä»…ä¾§é‡äºè‡ªé¡¶å‘ä¸‹ç”Ÿæˆ(generative)æˆ–å‰é¦ˆåˆ¤åˆ«(discriminative)æ¨ç†çš„å±€é™ã€‚bPCæ¨¡å‹é€šè¿‡å¼€å‘ä¸€ç§èƒ½å¤ŸåŒæ—¶é€‚åº”ä¸¤ç±»ä»»åŠ¡çš„èƒ½é‡æ™¯è§‚(energy landscape)ï¼Œåœ¨ä¿æŒç”Ÿç‰©å­¦åˆç†æ€§ç”µè·¯å®ç°çš„åŒæ—¶ï¼ŒæˆåŠŸæ•´åˆäº†ç”Ÿæˆä¸åˆ¤åˆ«æ¨ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒbPCåœ¨ä¸“é—¨çš„ç”Ÿæˆæˆ–åˆ¤åˆ«ä»»åŠ¡ä¸­å‡èƒ½åŒ¹é…æˆ–è¶…è¶Šä¼ ç»Ÿçš„å•å‘PCæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€å­¦ä¹ (multimodal learning)å’Œç¼ºå¤±ä¿¡æ¯æ¨ç†ç­‰ç”Ÿç‰©ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¿™è¡¨æ˜bPCæ¯”å•é¡¹æ¨¡å‹æ›´æ¥è¿‘çœŸå®çš„å¤§è„‘è§†è§‰æ¨ç†æœºåˆ¶ï¼Œä¸ºç†è§£å¤§è„‘å¦‚ä½•åŒæ—¶è¿›è¡Œé¢„æµ‹ä¸æ„ŸçŸ¥æä¾›äº†æ›´å…·ç”Ÿç‰©å­¦æ„ä¹‰çš„æ¨¡å‹æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23415v2",
      "published_date": "2025-05-29 13:05:29 UTC",
      "updated_date": "2025-12-17 15:15:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:01:16.602828+00:00"
    },
    {
      "arxiv_id": "2505.23412v2",
      "title": "Buffer-free Class-Incremental Learning with Out-of-Distribution Detection",
      "title_zh": "åŸºäºåˆ†å¸ƒå¤–æ£€æµ‹çš„æ— ç¼“å­˜ç±»å¢é‡å­¦ä¹ ",
      "authors": [
        "Srishti Gupta",
        "Daniele Angioni",
        "Maura Pintor",
        "Ambra Demontis",
        "Lea SchÃ¶nherr",
        "Battista Biggio",
        "Fabio Roli"
      ],
      "abstract": "Class-incremental learning (CIL) poses significant challenges in open-world scenarios, where models must not only learn new classes over time without forgetting previous ones but also handle inputs from unknown classes that a closed-set model would misclassify. Recent works address both issues by (i)~training multi-head models using the task-incremental learning framework, and (ii) predicting the task identity employing out-of-distribution (OOD) detectors. While effective, the latter mainly relies on joint training with a memory buffer of past data, raising concerns around privacy, scalability, and increased training time. In this paper, we present an in-depth analysis of post-hoc OOD detection methods and investigate their potential to eliminate the need for a memory buffer. We uncover that these methods, when applied appropriately at inference time, can serve as a strong substitute for buffer-based OOD detection. We show that this buffer-free approach achieves comparable or superior performance to buffer-based methods both in terms of class-incremental learning and the rejection of unknown samples. Experimental results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings, offering new insights into the design of efficient and privacy-preserving CIL systems for open-world settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç»“åˆåˆ†å¸ƒå¤–æ£€æµ‹(Out-of-Distribution Detection)çš„æ— ç¼“å­˜ç±»å¢é‡å­¦ä¹ (Buffer-free Class-Incremental Learning)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹å› ä¾èµ–å­˜å‚¨è¿‡å»æ•°æ®çš„Memory Bufferè€Œå¼•å‘çš„éšç§ã€å¯æ‰©å±•æ€§åŠè®­ç»ƒæ•ˆç‡é—®é¢˜ã€‚ä½œè€…å¯¹äº‹å(Post-hoc) OODæ£€æµ‹æ–¹æ³•è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶ç ”ç©¶äº†å…¶åœ¨æ¶ˆé™¤Memory Bufferéœ€æ±‚æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æ¨ç†é˜¶æ®µé€‚å½“åœ°åº”ç”¨è¿™äº›Post-hocæ–¹æ³•ï¼Œå¯ä»¥ä½œä¸ºåŸºäºBufferçš„OODæ£€æµ‹çš„å¼ºåŠ›æ›¿ä»£æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§Buffer-freeæ–¹æ³•åœ¨Class-incremental Learningæ€§èƒ½ä»¥åŠæœªçŸ¥æ ·æœ¬æ‹’ç»(Rejection of unknown samples)æ–¹é¢ï¼Œå‡è¾¾åˆ°äº†ä¸æœ‰ç¼“å­˜æ–¹æ³•ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ•ˆæœã€‚åœ¨CIFAR-10ã€CIFAR-100å’ŒTiny ImageNetæ•°æ®é›†ä¸Šçš„æµ‹è¯•éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè®¾è®¡é«˜æ•ˆä¸”å…·å¤‡éšç§ä¿æŠ¤ç‰¹æ€§çš„å¢é‡å­¦ä¹ ç³»ç»Ÿæä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23412v2",
      "published_date": "2025-05-29 13:01:00 UTC",
      "updated_date": "2025-09-25 08:21:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:01:52.486869+00:00"
    },
    {
      "arxiv_id": "2505.23875v1",
      "title": "A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants",
      "title_zh": "åŒ…å«åŒè´¨ä¸å¤šå…³ç³»å˜ä½“çš„å›¾å›å½’åŸºå‡†æ•°æ®é›†",
      "authors": [
        "Peter Samoaa",
        "Marcus Vukojevic",
        "Morteza Haghir Chehreghani",
        "Antonio Longa"
      ],
      "abstract": "Graph-level regression underpins many real-world applications, yet public benchmarks remain heavily skewed toward molecular graphs and citation networks. This limited diversity hinders progress on models that must generalize across both homogeneous and heterogeneous graph structures. We introduce RelSC, a new graph-regression dataset built from program graphs that combine syntactic and semantic information extracted from source code. Each graph is labelled with the execution-time cost of the corresponding program, providing a continuous target variable that differs markedly from those found in existing benchmarks. RelSC is released in two complementary variants. RelSC-H supplies rich node features under a single (homogeneous) edge type, while RelSC-M preserves the original multi-relational structure, connecting nodes through multiple edge types that encode distinct semantic relationships. Together, these variants let researchers probe how representation choice influences model behaviour. We evaluate a diverse set of graph neural network architectures on both variants of RelSC. The results reveal consistent performance differences between the homogeneous and multi-relational settings, emphasising the importance of structural representation. These findings demonstrate RelSC's value as a challenging and versatile benchmark for advancing graph regression methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†RelSCï¼Œä¸€ä¸ªä¸“ä¸ºGraph-level regressionè®¾è®¡çš„æ–°å‹åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†åœ¨å›¾ç»“æ„å¤šæ ·æ€§ä¸Šçš„ä¸è¶³ã€‚RelSCåŸºäºä»æºä»£ç ä¸­æå–çš„ç¨‹åºå›¾(program graphs)ï¼Œç»“åˆäº†è¯­æ³•å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ä»¥ç¨‹åºçš„æ‰§è¡Œæ—¶é—´æˆæœ¬(execution-time cost)ä½œä¸ºè¿ç»­å‹ç›®æ ‡å˜é‡ã€‚è¯¥æ•°æ®é›†åŒ…å«ä¸¤ä¸ªå˜ä½“ï¼šRelSC-Hæä¾›å•ä¸€è¾¹ç¼˜ç±»å‹çš„åŒæ„(homogeneous)ç»“æ„ï¼Œè€ŒRelSC-Måˆ™ä¿ç•™äº†ç¼–ç ä¸åŒè¯­ä¹‰å…³ç³»çš„å¤šå…³ç³»(multi-relational)ç»“æ„ã€‚ç ”ç©¶äººå‘˜åœ¨å¤šç§å›¾ç¥ç»ç½‘ç»œ(GNN)æ¶æ„ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœæ­ç¤ºäº†åŒæ„ä¸å¤šå…³ç³»è®¾ç½®ä¹‹é—´çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚ï¼Œå¼ºè°ƒäº†ç»“æ„è¡¨å¾å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ã€‚RelSCä¸ºå¼€å‘èƒ½å¤Ÿè·¨è¶Šä¸åŒå›¾ç»“æ„è¿›è¡Œæ³›åŒ–çš„å›å½’æ¨¡å‹æä¾›äº†ä¸€ä¸ªæŒ‘æˆ˜æ€§å¼ºä¸”ç”¨é€”å¹¿æ³›çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23875v1",
      "published_date": "2025-05-29 12:59:36 UTC",
      "updated_date": "2025-05-29 12:59:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:01:47.797595+00:00"
    },
    {
      "arxiv_id": "2505.23406v1",
      "title": "Video Editing for Audio-Visual Dubbing",
      "title_zh": "é¢å‘éŸ³è§†é¢‘é…éŸ³çš„è§†é¢‘ç¼–è¾‘",
      "authors": [
        "Binyamin Manela",
        "Sharon Gannot",
        "Ethan Fetyaya"
      ],
      "abstract": "Visual dubbing, the synchronization of facial movements with new speech, is crucial for making content accessible across different languages, enabling broader global reach. However, current methods face significant limitations. Existing approaches often generate talking faces, hindering seamless integration into original scenes, or employ inpainting techniques that discard vital visual information like partial occlusions and lighting variations. This work introduces EdiDub, a novel framework that reformulates visual dubbing as a content-aware editing task. EdiDub preserves the original video context by utilizing a specialized conditioning scheme to ensure faithful and accurate modifications rather than mere copying. On multiple benchmarks, including a challenging occluded-lip dataset, EdiDub significantly improves identity preservation and synchronization. Human evaluations further confirm its superiority, achieving higher synchronization and visual naturalness scores compared to the leading methods. These results demonstrate that our content-aware editing approach outperforms traditional generation or inpainting, particularly in maintaining complex visual elements while ensuring accurate lip synchronization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EdiDub æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰é…éŸ³ï¼ˆVisual dubbingï¼‰ä¸­ç°æœ‰ç”Ÿæˆæˆ–ä¿®å¤ï¼ˆinpaintingï¼‰æŠ€æœ¯å®¹æ˜“å¯¼è‡´åœºæ™¯æ•´åˆä¸è‡ªç„¶æˆ–ä¸¢å¤±é®æŒ¡ä¸å…‰ç…§ä¿¡æ¯çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†è§†è§‰é…éŸ³é‡æ–°å®šä¹‰ä¸ºä¸€ç§å†…å®¹æ„ŸçŸ¥ç¼–è¾‘ï¼ˆcontent-aware editingï¼‰ä»»åŠ¡ï¼Œåˆ©ç”¨ä¸“é—¨çš„æ¡ä»¶åŒ–æ–¹æ¡ˆï¼ˆspecialized conditioning schemeï¼‰æ¥ä¿ç•™åŸå§‹è§†é¢‘çš„ä¸Šä¸‹æ–‡èƒŒæ™¯ã€‚é€šè¿‡åœ¨åŒ…å«é®æŒ¡å”‡éƒ¨æ•°æ®é›†ï¼ˆoccluded-lip datasetï¼‰åœ¨å†…çš„å¤šä¸ªåŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•ï¼ŒEdiDub æ˜¾è‘—æå‡äº†èº«ä»½ä¿æŒï¼ˆidentity preservationï¼‰å’ŒåŒæ­¥æ€§ï¼ˆsynchronizationï¼‰çš„è¡¨ç°ã€‚äººå·¥è¯„ä¼°ç»“æœè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è‡ªç„¶åº¦ä¸Šä¼˜äºç°æœ‰é¡¶å°–æ–¹æ³•ï¼Œä¸ºå¤„ç†å¤æ‚è§†è§‰å…ƒç´ ä¸‹çš„ç²¾ç¡®å”‡éƒ¨åŒæ­¥æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23406v1",
      "published_date": "2025-05-29 12:56:09 UTC",
      "updated_date": "2025-05-29 12:56:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:00.099132+00:00"
    },
    {
      "arxiv_id": "2505.23399v1",
      "title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning",
      "title_zh": "GAM-Agentï¼šé¢å‘å¤æ‚è§†è§‰æ¨ç†çš„åšå¼ˆè®ºä¸ä¸ç¡®å®šæ€§æ„ŸçŸ¥åä½œ",
      "authors": [
        "Jusheng Zhang",
        "Yijia Fan",
        "Wenjun Lin",
        "Ruiqi Chen",
        "Haoyi Jiang",
        "Wenhao Chai",
        "Jian Wang",
        "Keze Wang"
      ],
      "abstract": "We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing vision-language reasoning. Unlike prior single-agent or monolithic models, GAM-Agent formulates the reasoning process as a non-zero-sum game between base agents--each specializing in visual perception subtasks--and a critical agent that verifies logic consistency and factual correctness. Agents communicate via structured claims, evidence, and uncertainty estimates. The framework introduces an uncertainty-aware controller to dynamically adjust agent collaboration, triggering multi-round debates when disagreement or ambiguity is detected. This process yields more robust and interpretable predictions. Experiments on four challenging benchmarks--MMMU, MMBench, MVBench, and V*Bench--demonstrate that GAM-Agent significantly improves performance across various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\\%, and still enhances strong models like GPT-4o by up to 2--3\\%. Our approach is modular, scalable, and generalizable, offering a path toward reliable and explainable multi-agent multimodal reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GAM-Agentï¼Œä¸€ç§åŸºäºåšå¼ˆè®ºå’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºè´Ÿè´£è§†è§‰æ„ŸçŸ¥å­ä»»åŠ¡çš„åŸºç¡€æ™ºèƒ½ä½“(base agents)ä¸è´Ÿè´£éªŒè¯é€»è¾‘ä¸€è‡´æ€§çš„å®¡æ…æ™ºèƒ½ä½“(critical agent)ä¹‹é—´çš„éé›¶å’Œåšå¼ˆ(non-zero-sum game)ã€‚æ™ºèƒ½ä½“é€šè¿‡å…±äº«é™ˆè¿°ã€è¯æ®å’Œä¸ç¡®å®šæ€§ä¼°è®¡è¿›è¡Œæ²Ÿé€šï¼Œå¹¶ç”±ä¸€ä¸ªä¸ç¡®å®šæ€§æ„ŸçŸ¥æ§åˆ¶å™¨(uncertainty-aware controller)åŠ¨æ€è°ƒæ•´åä½œï¼Œåœ¨å‡ºç°åˆ†æ­§æ—¶è§¦å‘å¤šè½®è¾©è®ºã€‚å®éªŒè¡¨æ˜ï¼ŒGAM-Agentåœ¨MMMUã€MMBenchã€MVBenchå’ŒV*Benchç­‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†å„ç§è§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶ä½¿Qwen2.5-VL-7Bå’ŒInternVL3-14Bç­‰ä¸­å°è§„æ¨¡æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†5-6%ï¼Œå³ä½¿æ˜¯GPT-4oä¹Ÿè·å¾—äº†2-3%çš„æå‡ã€‚GAM-Agentå±•ç°äº†è‰¯å¥½çš„æ¨¡å—åŒ–ã€å¯æ‰©å±•æ€§å’Œæ³›åŒ–æ€§ï¼Œä¸ºå®ç°å¯é ä¸”å¯è§£é‡Šçš„å¤šæ™ºèƒ½ä½“å¤šæ¨¡æ€æ¨ç†æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23399v1",
      "published_date": "2025-05-29 12:37:34 UTC",
      "updated_date": "2025-05-29 12:37:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:02.392447+00:00"
    },
    {
      "arxiv_id": "2505.23397v2",
      "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy",
      "title_zh": "å…·æœ‰å¯ä¿¡è‡ªä¸»æ€§çš„å®‰å…¨è¿è¥ä¸­å¿ƒäººæœºåä½œç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Ahmad Mohsin",
        "Helge Janicke",
        "Ahmed Ibrahim",
        "Iqbal H. Sarker",
        "Seyit Camtepe"
      ],
      "abstract": "This article presents a structured framework for Human-AI collaboration in Security Operations Centers (SOCs), integrating AI autonomy, trust calibration, and Human-in-the-loop decision making. Existing frameworks in SOCs often focus narrowly on automation, lacking systematic structures to manage human oversight, trust calibration, and scalable autonomy with AI. Many assume static or binary autonomy settings, failing to account for the varied complexity, criticality, and risk across SOC tasks considering Humans and AI collaboration. To address these limitations, we propose a novel autonomy tiered framework grounded in five levels of AI autonomy from manual to fully autonomous, mapped to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This enables adaptive and explainable AI integration across core SOC functions, including monitoring, protection, threat detection, alert triage, and incident response. The proposed framework differentiates itself from previous research by creating formal connections between autonomy, trust, and HITL across various SOC levels, which allows for adaptive task distribution according to operational complexity and associated risks. The framework is exemplified through a simulated cyber range that features the cybersecurity AI-Avatar, a fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing response coordination, and strategically calibrating trust. This research systematically presents both the theoretical and practical aspects and feasibility of designing next-generation cognitive SOCs that leverage AI not to replace but to enhance human decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®‰å…¨è¿è¥ä¸­å¿ƒ(SOC)ä¸­ç°æœ‰è‡ªåŠ¨åŒ–æ¡†æ¶ç¼ºä¹ç³»ç»Ÿçš„äººæœºåä½œç»“æ„ã€ä¿¡ä»»æ ¡å‡†åŠå¯æ‰©å±•è‡ªä¸»æƒç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§èåˆAIè‡ªä¸»æƒ(AI autonomy)ã€ä¿¡ä»»æ ¡å‡†å’Œäººæœºå›ç¯(Human-in-the-Loop)å†³ç­–çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†ä»æ‰‹åŠ¨åˆ°å…¨è‡ªåŠ¨çš„äº”ä¸ªAIè‡ªä¸»æƒçº§åˆ«ï¼Œå¹¶å°†å…¶ä¸HITLè§’è‰²åŠç‰¹å®šä»»åŠ¡çš„ä¿¡ä»»é˜ˆå€¼è¿›è¡Œæ˜ å°„ï¼Œå®ç°äº†åœ¨ç›‘æ§ã€å¨èƒæ£€æµ‹å’Œäº‹ä»¶å“åº”ç­‰æ ¸å¿ƒåŠŸèƒ½ä¸­çš„è‡ªé€‚åº”é›†æˆã€‚ä¸ä»¥å¾€ç ”ç©¶ä¸åŒï¼Œè¯¥æ¡†æ¶åœ¨è‡ªä¸»æƒã€ä¿¡ä»»ä¸HITLä¹‹é—´å»ºç«‹äº†æ­£å¼å…³è”ï¼Œå…è®¸æ ¹æ®æ“ä½œå¤æ‚æ€§å’Œç›¸å…³é£é™©è¿›è¡Œè‡ªé€‚åº”ä»»åŠ¡åˆ†é…ã€‚ç ”ç©¶é€šè¿‡åŒ…å«AI-Avatarï¼ˆä¸€ç§ç»è¿‡å¾®è°ƒçš„åŸºäºLLMçš„SOCåŠ©æ‰‹ï¼‰çš„æ¨¡æ‹Ÿç½‘ç»œé¶åœºéªŒè¯äº†æ¡†æ¶çš„å¯è¡Œæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥äººæœºåä½œæ¨¡å¼èƒ½æœ‰æ•ˆç¼“è§£å‘Šè­¦ç–²åŠ³(alert fatigue)ï¼Œå¢å¼ºå“åº”åè°ƒæ€§å¹¶æˆ˜ç•¥æ€§åœ°æ ¡å‡†ä¿¡ä»»ã€‚è¯¥ç ”ç©¶ä¸ºè®¾è®¡ä¸‹ä¸€ä»£è®¤çŸ¥å‹SOCæä¾›äº†ç†è®ºä¸å®è·µæŒ‡å¯¼ï¼Œæ—¨åœ¨é€šè¿‡AIå¢å¼ºè€Œéå–ä»£äººç±»çš„å†³ç­–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "Journal Article",
      "pdf_url": "https://arxiv.org/pdf/2505.23397v2",
      "published_date": "2025-05-29 12:35:08 UTC",
      "updated_date": "2025-06-01 03:54:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:04.486426+00:00"
    },
    {
      "arxiv_id": "2506.10016v3",
      "title": "A Survey of Generative Categories and Techniques in Multimodal Generative Models",
      "title_zh": "å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„ç”Ÿæˆç±»åˆ«ä¸æŠ€æœ¯ç»¼è¿°",
      "authors": [
        "Longzhen Han",
        "Awes Mubarak",
        "Almas Baimagambetov",
        "Nikolaos Polatidis",
        "Thar Baker"
      ],
      "abstract": "Multimodal Generative Models (MGMs) have rapidly evolved beyond text generation, now spanning diverse output modalities including images, music, video, human motion, and 3D objects, by integrating language with other sensory modalities under unified architectures. This survey categorises six primary generative modalities and examines how foundational techniques, namely Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting, enable cross-modal capabilities. We analyze key models, architectural trends, and emergent cross-modal synergies, while highlighting transferable techniques and unresolved challenges. Building on a common taxonomy of models and training recipes, we propose a unified evaluation framework centred on faithfulness, compositionality, and robustness, and synthesise evidence from benchmarks and human studies across modalities. We further analyse trustworthiness, safety, and ethical risks, including multimodal bias, privacy leakage, and the misuse of high-fidelity media generation for deepfakes, disinformation, and copyright infringement in music and 3D assets, together with emerging mitigation strategies. Finally, we discuss how architectural trends, evaluation protocols, and governance mechanisms can be co-designed to close current capability and safety gaps, outlining critical paths toward more general-purpose, controllable, and accountable multimodal generative systems.",
      "tldr_zh": "è¯¥ç»¼è¿°å¯¹å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ (Multimodal Generative Models, MGMs) çš„ç”Ÿæˆç±»åˆ«å’Œå…³é”®æŠ€æœ¯è¿›è¡Œäº†ç³»ç»Ÿæ€§å›é¡¾ã€‚ç ”ç©¶å°†å…¶åˆ’åˆ†ä¸ºå›¾åƒã€éŸ³ä¹ã€è§†é¢‘ã€äººä½“åŠ¨ä½œå’Œ 3D å¯¹è±¡ç­‰å…­å¤§ä¸»è¦ç”Ÿæˆæ¨¡æ€ï¼Œå¹¶åˆ†æäº† Self-Supervised Learning (SSL)ã€Mixture of Experts (MoE)ã€Reinforcement Learning from Human Feedback (RLHF) ä»¥åŠ Chain-of-Thought (CoT) prompting ç­‰æ ¸å¿ƒæŠ€æœ¯åœ¨å®ç°è·¨æ¨¡æ€èƒ½åŠ›ä¸­çš„ä½œç”¨ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåŸºäºå¿ å®æ€§ (faithfulness)ã€ç»„åˆæ€§ (compositionality) å’Œç¨³å¥æ€§ (robustness) çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œå¹¶ç»¼åˆäº†æ¥è‡ªå„æ¨¡æ€åŸºå‡†æµ‹è¯•å’Œäººç±»ç ”ç©¶çš„è¯æ®ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ·±å…¥æ¢è®¨äº†å¤šæ¨¡æ€åå·®ã€éšç§æ³„éœ²ã€æ·±åº¦ä¼ªé€  (deepfakes) å’Œç‰ˆæƒä¾µæƒç­‰ä¿¡ä»»ã€å®‰å…¨ä¸ä¼¦ç†é£é™©åŠå…¶ç¼“è§£ç­–ç•¥ã€‚æœ€åï¼Œæ–‡ç« è®¨è®ºäº†å¦‚ä½•é€šè¿‡æ¶æ„è¶‹åŠ¿ã€è¯„ä¼°åè®®å’Œæ²»ç†æœºåˆ¶çš„ååŒè®¾è®¡æ¥å¼¥åˆèƒ½åŠ›ä¸å®‰å…¨å·®è·ï¼Œä¸ºæ„å»ºæ›´é€šç”¨ã€å¯æ§ä¸”è´Ÿè´£ä»»çš„å¤šæ¨¡æ€ç”Ÿæˆç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.10016v3",
      "published_date": "2025-05-29 12:29:39 UTC",
      "updated_date": "2025-11-24 16:26:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:11.596190+00:00"
    },
    {
      "arxiv_id": "2505.23873v2",
      "title": "KGMark: A Diffusion Watermark for Knowledge Graphs",
      "title_zh": "KGMarkï¼šé¢å‘çŸ¥è¯†å›¾è°±çš„æ‰©æ•£æ°´å°",
      "authors": [
        "Hongrui Peng",
        "Haolang Lu",
        "Yuanlong Yu",
        "Weiye Fu",
        "Kun Wang",
        "Guoshun Nan"
      ],
      "abstract": "Knowledge graphs (KGs) are ubiquitous in numerous real-world applications, and watermarking facilitates protecting intellectual property and preventing potential harm from AI-generated content. Existing watermarking methods mainly focus on static plain text or image data, while they can hardly be applied to dynamic graphs due to spatial and temporal variations of structured data. This motivates us to propose KGMARK, the first graph watermarking framework that aims to generate robust, detectable, and transparent diffusion fingerprints for dynamic KG data. Specifically, we propose a novel clustering-based alignment method to adapt the watermark to spatial variations. Meanwhile, we present a redundant embedding strategy to harden the diffusion watermark against various attacks, facilitating the robustness of the watermark to the temporal variations. Additionally, we introduce a novel learnable mask matrix to improve the transparency of diffusion fingerprints. By doing so, our KGMARK properly tackles the variation challenges of structured data. Experiments on various public benchmarks show the effectiveness of our proposed KGMARK. Our code is available at https://github.com/phrara/kgmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KGMARKï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨ä¸ºåŠ¨æ€çŸ¥è¯†å›¾è°± (Knowledge Graphs) ç”Ÿæˆé²æ£’ã€å¯æ£€æµ‹ä¸”é€æ˜çš„æ‰©æ•£æŒ‡çº¹ (diffusion fingerprints) çš„å›¾æ°´å°æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿæ°´å°æŠ€æœ¯éš¾ä»¥åº”å¯¹ç»“æ„åŒ–æ•°æ®ç©ºé—´å’Œæ—¶é—´å˜åŒ–çš„é—®é¢˜ã€‚ä¸ºäº†é€‚é…ç©ºé—´å˜åŒ–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŸºäºèšç±»çš„å¯¹é½æ–¹æ³• (clustering-based alignment)ã€‚åŒæ—¶ï¼ŒKGMARK é‡‡ç”¨äº†å†—ä½™åµŒå…¥ç­–ç•¥ (redundant embedding strategy) æ¥å¢å¼ºæ‰©æ•£æ°´å°å¯¹æŠ—å„ç§æ”»å‡»çš„èƒ½åŠ›ï¼Œä»è€Œæå‡äº†å¯¹æ—¶é—´å˜åŒ–çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„æ©ç çŸ©é˜µ (learnable mask matrix) è¿›ä¸€æ­¥æé«˜äº†æŒ‡çº¹çš„é€æ˜åº¦ã€‚åœ¨å¤šç§å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜äº† KGMARK çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåŠ¨æ€ Knowledge Graphs çš„çŸ¥è¯†äº§æƒä¿æŠ¤æä¾›äº†å¯é çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by ICML2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23873v2",
      "published_date": "2025-05-29 12:28:05 UTC",
      "updated_date": "2025-06-17 06:16:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:12.289695+00:00"
    },
    {
      "arxiv_id": "2506.03178v1",
      "title": "LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning",
      "title_zh": "LLaMA-XRï¼šä¸€ç§åŸºäº LLaMA å’Œ QLoRA å¾®è°ƒçš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–°æ¡†æ¶",
      "authors": [
        "Md. Zihad Bin Jahangir",
        "Muhammad Ashad Kabir",
        "Sumaiya Akter",
        "Israt Jahan",
        "Minh Chau"
      ],
      "abstract": "Automated radiology report generation holds significant potential to reduce radiologists' workload and enhance diagnostic accuracy. However, generating precise and clinically meaningful reports from chest radiographs remains challenging due to the complexity of medical language and the need for contextual understanding. Existing models often struggle with maintaining both accuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel framework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings and Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves improved coherence and clinical accuracy while maintaining computational efficiency. This efficiency is driven by an optimization strategy that enhances parameter utilization and reduces memory overhead, enabling faster report generation with lower computational resource demands. Extensive experiments conducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR outperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L score of 0.433 and a METEOR score of 0.336, establishing new performance benchmarks in the domain. These results underscore LLaMA-XR's potential as an effective and efficient AI system for automated radiology reporting, offering enhanced clinical utility and reliability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLaMA-XRï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³èƒ¸éƒ¨ X å°„çº¿æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆä¸­è¯­è¨€å¤æ‚æ€§å’Œä¸Šä¸‹æ–‡ç†è§£éš¾é¢˜çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº† LLaMA 3.1 æ¨¡å‹ï¼Œå¹¶ç»“åˆåŸºäº DenseNet-121 çš„å›¾åƒåµŒå…¥(image embeddings)æŠ€æœ¯ã€‚ä¸ºäº†åœ¨ä¿è¯ä¸´åºŠå‡†ç¡®æ€§çš„åŒæ—¶æé«˜è®¡ç®—æ•ˆç‡ï¼Œç ”ç©¶é‡‡ç”¨äº†é‡åŒ–ä½ç§©é€‚é…(Quantized Low-Rank Adaptation, QLoRA)è¿›è¡Œå¾®è°ƒã€‚è¿™ç§ä¼˜åŒ–ç­–ç•¥æ˜¾è‘—æå‡äº†å‚æ•°åˆ©ç”¨ç‡å¹¶é™ä½äº†æ˜¾å­˜å¼€é”€ï¼Œå®ç°äº†æ›´å¿«é€Ÿçš„æŠ¥å‘Šç”Ÿæˆå’Œæ›´ä½çš„èµ„æºéœ€æ±‚ã€‚åœ¨ IU X-ray åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaMA-XR åœ¨è¿è´¯æ€§å’Œä¸´åºŠå‡†ç¡®æ€§æ–¹é¢ä¼˜äºå¤šç§ç°æœ‰æ¨¡å‹ã€‚æ¨¡å‹æœ€ç»ˆå–å¾—äº† ROUGE-L 0.433 å’Œ METEOR 0.336 çš„ä¼˜å¼‚æˆç»©ï¼Œåˆ·æ–°äº†è¯¥é¢†åŸŸçš„æ€§èƒ½æ ‡æ†ã€‚è¯¥æˆæœå±•ç¤ºäº† LLaMA-XR ä½œä¸ºé«˜æ•ˆã€å¯é çš„è‡ªåŠ¨åŒ–æ”¾å°„ç§‘æŠ¥å‘Šç³»ç»Ÿçš„æ½œåŠ›ï¼Œå…·æœ‰æ˜¾è‘—çš„ä¸´åºŠå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "25 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.03178v1",
      "published_date": "2025-05-29 12:21:18 UTC",
      "updated_date": "2025-05-29 12:21:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:20.591609+00:00"
    },
    {
      "arxiv_id": "2505.23387v3",
      "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization",
      "title_zh": "Afterburnerï¼šå¼ºåŒ–å­¦ä¹ åŠ©åŠ›ä»£ç æ•ˆç‡çš„è‡ªæˆ‘æ”¹è¿›å¼ä¼˜åŒ–",
      "authors": [
        "Mingzhe Du",
        "Luu Anh Tuan",
        "Yue Liu",
        "Yuhao Qing",
        "Dong Huang",
        "Xinyi He",
        "Qian Liu",
        "Zejun Ma",
        "See-kiong Ng"
      ],
      "abstract": "Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world deployment. In this paper, we introduce a novel test-time iterative optimization framework to address this, employing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. We explore three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO). Experiments on our Venus dataset and the APPS benchmark show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO, using reinforcement learning (RL) with execution feedback, continuously optimizes code performance, significantly boosting both pass@1 (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%). Our work demonstrates effective test-time code efficiency improvement and critically reveals the power of RL in teaching LLMs to truly self-improve code efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Afterburnerï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„ä»£ç åŠŸèƒ½æ­£ç¡®ä½†æ‰§è¡Œæ•ˆç‡ä½ä¸‹çš„æµ‹è¯•æ—¶è¿­ä»£ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªé—­ç¯ç³»ç»Ÿï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ‰§è¡Œæ²™ç›’(Execution Sandbox)æä¾›çš„å®è¯æ€§èƒ½åé¦ˆè¿­ä»£æ”¹è¿›ä»£ç ã€‚é€šè¿‡å¯¹æ¯”ç›‘ç£å¾®è°ƒ(SFT)ã€ç›´æ¥åå¥½ä¼˜åŒ–(DPO)å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ä¸‰ç§è®­ç»ƒç­–ç•¥ï¼Œç ”ç©¶å‘ç°å‰ä¸¤è€…åœ¨æ•ˆç‡æå‡ä¸Šå®¹æ˜“é¥±å’Œï¼Œè€ŒGRPOåˆ©ç”¨å¼ºåŒ–å­¦ä¹ (RL)ä¸æ‰§è¡Œåé¦ˆå®ç°äº†ä»£ç æ€§èƒ½çš„æŒç»­ä¼˜åŒ–ã€‚å®éªŒåœ¨Venusæ•°æ®é›†å’ŒAPPSåŸºå‡†æµ‹è¯•ä¸Šè¯æ˜ï¼ŒGRPOæ˜¾è‘—æé«˜äº†pass@1æŒ‡æ ‡ï¼ˆä»47%æå‡è‡³62%ï¼‰ï¼Œå¹¶å°†æ•ˆç‡è¶…è¶Šäººç±»æ°´å¹³çš„æ¦‚ç‡ä»31%æé«˜åˆ°45%ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†æµ‹è¯•æ—¶ä»£ç æ•ˆç‡ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨æ•™å¯¼æ¨¡å‹å®ç°ä»£ç æ•ˆç‡çœŸæ­£è‡ªæˆ‘æå‡æ–¹é¢çš„æ ¸å¿ƒåŠ›é‡ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23387v3",
      "published_date": "2025-05-29 12:14:29 UTC",
      "updated_date": "2025-06-03 09:55:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:20.844962+00:00"
    },
    {
      "arxiv_id": "2505.23386v1",
      "title": "VModA: An Effective Framework for Adaptive NSFW Image Moderation",
      "title_zh": "VModAï¼šä¸€ç§é«˜æ•ˆçš„è‡ªé€‚åº” NSFW å›¾åƒå®¡æ ¸æ¡†æ¶",
      "authors": [
        "Han Bao",
        "Qinying Wang",
        "Zhi Chen",
        "Qingming Li",
        "Xuhong Zhang",
        "Changjiang Li",
        "Zonghui Wang",
        "Shouling Ji",
        "Wenzhi Chen"
      ],
      "abstract": "Not Safe/Suitable for Work (NSFW) content is rampant on social networks and poses serious harm to citizens, especially minors. Current detection methods mainly rely on deep learning-based image recognition and classification. However, NSFW images are now presented in increasingly sophisticated ways, often using image details and complex semantics to obscure their true nature or attract more views. Although still understandable to humans, these images often evade existing detection methods, posing a significant threat. Further complicating the issue, varying regulations across platforms and regions create additional challenges for effective moderation, leading to detection bias and reduced accuracy. To address this, we propose VModA, a general and effective framework that adapts to diverse moderation rules and handles complex, semantically rich NSFW content across categories. Experimental results show that VModA significantly outperforms existing methods, achieving up to a 54.3% accuracy improvement across NSFW types, including those with complex semantics. Further experiments demonstrate that our method exhibits strong adaptability across categories, scenarios, and base VLMs. We also identified inconsistent and controversial label samples in public NSFW benchmark datasets, re-annotated them, and submitted corrections to the original maintainers. Two datasets have confirmed the updates so far. Additionally, we evaluate VModA in real-world scenarios to demonstrate its practical effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VModAï¼Œä¸€ä¸ªé’ˆå¯¹éé€‚å®œå†…å®¹(NSFW)å›¾åƒå®¡æ ¸çš„é€šç”¨ä¸”æœ‰æ•ˆçš„è‡ªé€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚è¯­ä¹‰å’Œå¤šæ ·åŒ–ç›‘ç®¡è§„åˆ™æ—¶çš„å±€é™æ€§ã€‚VModAèƒ½å¤Ÿçµæ´»é€‚åº”ä¸åŒçš„å®¡æ ¸æ ‡å‡†ï¼Œå¹¶æœ‰æ•ˆè¯†åˆ«åˆ©ç”¨å›¾åƒç»†èŠ‚å’Œå¤æ‚è¯­ä¹‰é€ƒé¿æ£€æµ‹çš„NSFWå†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å„ç±»NSFWæ£€æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æœ€é«˜å¯æ¯”ç°æœ‰æ–¹æ³•æå‡54.3%ï¼Œä¸”åœ¨å¤šç§è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å’Œå®é™…åº”ç”¨åœºæ™¯ä¸­å‡è¡¨ç°å‡ºæå¼ºçš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¯¹å…¬å…±åŸºå‡†æ•°æ®é›†ä¸­çš„ä¸ä¸€è‡´æ ‡ç­¾è¿›è¡Œäº†é‡æ–°æ ‡æ³¨ä¸ä¿®æ­£ã€‚è¯¥æ¡†æ¶åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†å…¶åœ¨å¤æ‚å®¡æ ¸ä»»åŠ¡ä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23386v1",
      "published_date": "2025-05-29 12:11:28 UTC",
      "updated_date": "2025-05-29 12:11:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:26.309409+00:00"
    },
    {
      "arxiv_id": "2505.23381v1",
      "title": "AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning",
      "title_zh": "AutoGPSï¼šåŸºäºå¤šæ¨¡æ€å½¢å¼åŒ–ä¸æ¼”ç»æ¨ç†çš„è‡ªåŠ¨åŒ–å‡ ä½•é—®é¢˜æ±‚è§£",
      "authors": [
        "Bowen Ping",
        "Minnan Luo",
        "Zhuohang Dang",
        "Chenxi Wang",
        "Chengyou Jia"
      ],
      "abstract": "Geometry problem solving presents distinctive challenges in artificial intelligence, requiring exceptional multimodal comprehension and rigorous mathematical reasoning capabilities. Existing approaches typically fall into two categories: neural-based and symbolic-based methods, both of which exhibit limitations in reliability and interpretability. To address this challenge, we propose AutoGPS, a neuro-symbolic collaborative framework that solves geometry problems with concise, reliable, and human-interpretable reasoning processes. Specifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a Deductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal comprehension to translate geometry problems into structured formal language representations, with feedback from DSR collaboratively. The DSR takes the formalization as input and formulates geometry problem solving as a hypergraph expansion task, executing mathematically rigorous and reliable derivation to produce minimal and human-readable stepwise solutions. Extensive experimental evaluations demonstrate that AutoGPS achieves state-of-the-art performance on benchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms AutoGPS's impressive reliability and interpretability, with 99\\% stepwise logical coherence. The project homepage is at https://jayce-ping.github.io/AutoGPS-homepage.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoGPSï¼Œä¸€ç§æ—¨åœ¨è§£å†³å‡ ä½•é—®é¢˜ä¸­å¤šæ¨¡æ€ç†è§£ä¸ä¸¥è°¨æ•°å­¦æ¨ç†æŒ‘æˆ˜çš„ç¥ç»-ç¬¦å· (neuro-symbolic) åä½œæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šæ¨¡æ€é—®é¢˜å½¢å¼åŒ–å™¨ (Multimodal Problem Formalizer, MPF) å°†å‡ ä½•é¢˜ç›®è½¬åŒ–ä¸ºç»“æ„åŒ–çš„å½¢å¼è¯­è¨€ï¼Œå¹¶ç»“åˆæ¼”ç»ç¬¦å·æ¨ç†å™¨ (Deductive Symbolic Reasoner, DSR) å°†æ±‚è§£è¿‡ç¨‹å»ºæ¨¡ä¸ºè¶…å›¾æ‰©å±• (hypergraph expansion) ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§åä½œæ–¹å¼ï¼ŒAutoGPS èƒ½å¤Ÿæ‰§è¡Œæ•°å­¦ä¸Šä¸¥è°¨ä¸”å¯é çš„æ¨å¯¼ï¼Œä»è€Œç”Ÿæˆç®€æ´ã€æœ€å°åŒ–ä¸”äººç±»å¯è¯»çš„åˆ†æ­¥è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoGPS åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº† state-of-the-art çš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œäººå·¥è¯„ä¼°ç¡®è®¤è¯¥æ¨¡å‹å…·æœ‰æé«˜çš„å¯é æ€§ä¸å¯è§£é‡Šæ€§ï¼Œå…¶åˆ†æ­¥é€»è¾‘è¿è´¯æ€§è¾¾åˆ°äº† 99%ï¼Œæœ‰æ•ˆåœ°å¼¥è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨å¯é æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„ä¸è¶³ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23381v1",
      "published_date": "2025-05-29 12:01:20 UTC",
      "updated_date": "2025-05-29 12:01:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:41.991277+00:00"
    },
    {
      "arxiv_id": "2505.23369v1",
      "title": "Dynamic Spectral Backpropagation for Efficient Neural Network Training",
      "title_zh": "é¢å‘é«˜æ•ˆç¥ç»ç½‘ç»œè®­ç»ƒçš„åŠ¨æ€è°±åå‘ä¼ æ’­",
      "authors": [
        "Mannmohan Muthuraman"
      ],
      "abstract": "Dynamic Spectral Backpropagation (DSBP) enhances neural network training under resource constraints by projecting gradients onto principal eigenvectors, reducing complexity and promoting flat minima. Five extensions are proposed, dynamic spectral inference, spectral architecture optimization, spectral meta learning, spectral transfer regularization, and Lie algebra inspired dynamics, to address challenges in robustness, fewshot learning, and hardware efficiency. Supported by a third order stochastic differential equation (SDE) and a PAC Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive experiments and visualizations. Future work focuses on scalability, bias mitigation, and ethical considerations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Dynamic Spectral Backpropagation (DSBP)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å¢å¼ºç¥ç»ç½‘ç»œè®­ç»ƒæ•ˆç‡çš„æ–°å‹ç®—æ³•ã€‚DSBPé€šè¿‡å°†æ¢¯åº¦æŠ•å½±åˆ°ä¸»ç‰¹å¾å‘é‡(principal eigenvectors)ä¸Šï¼Œåœ¨é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ä¿ƒè¿›æ¨¡å‹å¯»æ‰¾å¹³å¦æå°å€¼(flat minima)ã€‚ä½œè€…è¿˜æ¨å‡ºäº†åŒ…æ‹¬åŠ¨æ€é¢‘è°±æ¨ç†(dynamic spectral inference)å’Œé¢‘è°±å…ƒå­¦ä¹ (spectral meta learning)åœ¨å†…çš„äº”é¡¹æ‰©å±•æ–¹æ¡ˆï¼Œä»¥åº”å¯¹é²æ£’æ€§ã€å°‘æ ·æœ¬å­¦ä¹ å’Œç¡¬ä»¶æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åœ¨ç†è®ºä¸Šå—åˆ°ä¸‰é˜¶éšæœºå¾®åˆ†æ–¹ç¨‹(SDE)å’ŒPAC Bayesæé™çš„æ”¯æŒã€‚å®éªŒè¡¨æ˜ï¼ŒDSBPåœ¨CIFAR-10ã€MedMNISTåŠTiny ImageNetç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½å‡ä¼˜äºSharpness Aware Minimization (SAM)ã€Low Rank Adaptation (LoRA)å’ŒModel Agnostic Meta Learning (MAML)ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆä¸”ç¨³å¥çš„ç½‘ç»œä¼˜åŒ–æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®ä¸å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23369v1",
      "published_date": "2025-05-29 11:47:50 UTC",
      "updated_date": "2025-05-29 11:47:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:46.909977+00:00"
    },
    {
      "arxiv_id": "2505.23367v2",
      "title": "PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening",
      "title_zh": "PAN-Crafterï¼šé¢å‘å…¨è‰²é”åŒ–çš„æ¨¡æ€ä¸€è‡´æ€§å¯¹é½å­¦ä¹ ",
      "authors": [
        "Jeonghyeok Do",
        "Sungpyo Kim",
        "Geunhyuk Youk",
        "Jaehyup Lee",
        "Munchurl Kim"
      ],
      "abstract": "PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with low-resolution multi-spectral (MS) images to generate high-resolution multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused by sensor placement, acquisition timing, and resolution disparity -- induces a fundamental challenge. Conventional deep learning methods assume perfect pixel-wise alignment and rely on per-pixel reconstruction losses, leading to spectral distortion, double edges, and blurring when misalignment is present. To address this, we propose PAN-Crafter, a modality-consistent alignment framework that explicitly mitigates the misalignment gap between PAN and MS modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a single network to jointly reconstruct HRMS and PAN images, leveraging PAN's high-frequency details as auxiliary self-supervision. Additionally, we introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism that bidirectionally aligns MS texture to PAN structure and vice versa, enabling adaptive feature refinement across modalities. Extensive experiments on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the most recent state-of-the-art method in all metrics, even with 50.11$\\times$ faster inference time and 0.63$\\times$ the memory size. Furthermore, it demonstrates strong generalization performance on unseen satellite datasets, showing its robustness across different conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PAN-Crafterï¼Œä¸€ç§æ¨¡æ€ä¸€è‡´å¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ PAN-sharpening ä¸­ç”±äºä¼ æ„Ÿå™¨ä½ç½®ã€é‡‡é›†æ—¶é—´å’Œåˆ†è¾¨ç‡å·®å¼‚å¼•èµ·çš„è·¨æ¨¡æ€é”™ä½ (cross-modality misalignment) é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºåƒç´ çº§å¯¹é½å‡è®¾ï¼Œå¯¼è‡´åœ¨é”™ä½æƒ…å†µä¸‹å®¹æ˜“å‡ºç°å…‰è°±å¤±çœŸå’Œæ¨¡ç³Šï¼Œè€Œ PAN-Crafter é€šè¿‡æ˜¾å¼ç¼“è§£æ¨¡æ€é—´çš„å¯¹é½å·®è·æ¥å…‹æœè¿™ä¸€æŒ‘æˆ˜ã€‚å…¶æ ¸å¿ƒç»„ä»¶æ¨¡æ€è‡ªé€‚åº”é‡å»º (Modality-Adaptive Reconstruction, MARs) å…è®¸ç½‘ç»œè”åˆé‡å»º HRMS å’Œ PAN å›¾åƒï¼Œåˆ©ç”¨ PAN çš„é«˜é¢‘ç»†èŠ‚ä½œä¸ºè¾…åŠ©è‡ªç›‘ç£ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†è·¨æ¨¡æ€å¯¹é½æ„ŸçŸ¥æ³¨æ„åŠ› (Cross-Modality Alignment-Aware Attention, CM3A) æœºåˆ¶ï¼Œå®ç°äº† MS çº¹ç†ä¸ PAN ç»“æ„ä¹‹é—´çš„åŒå‘å¯¹é½å’Œè‡ªé€‚åº”ç‰¹å¾ç»†åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒPAN-Crafter åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œä¸”æ¨ç†é€Ÿåº¦æå‡äº† 50.11 å€ï¼Œå†…å­˜å ç”¨ä»…ä¸º 0.63 å€ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æœªè§è¿‡çš„å«æ˜Ÿæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025 (camera-ready version). Please visit our project page https://kaist-viclab.github.io/PAN-Crafter_site",
      "pdf_url": "https://arxiv.org/pdf/2505.23367v2",
      "published_date": "2025-05-29 11:46:21 UTC",
      "updated_date": "2025-07-15 06:54:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:02:58.457974+00:00"
    },
    {
      "arxiv_id": "2505.23871v2",
      "title": "ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning",
      "title_zh": "ADGï¼šé¢å‘æŠ—æŸåç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„ç¯å¢ƒæ‰©æ•£å¼•å¯¼æ•°æ®é›†æ¢å¤",
      "authors": [
        "Zeyuan Liu",
        "Zhihe Yang",
        "Jiawei Xu",
        "Rui Yang",
        "Jiafei Lyu",
        "Baoxiang Wang",
        "Yunjian Xu",
        "Xiu Li"
      ],
      "abstract": "Real-world datasets collected from sensors or human inputs are prone to noise and errors, posing significant challenges for applying offline reinforcement learning (RL). While existing methods have made progress in addressing corrupted actions and rewards, they remain insufficient for handling corruption in high-dimensional state spaces and for cases where multiple elements in the dataset are corrupted simultaneously. Diffusion models, known for their strong denoising capabilities, offer a promising direction for this problem-but their tendency to overfit noisy samples limits their direct applicability. To overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a novel approach that pioneers the use of diffusion models to tackle data corruption in offline RL. First, we introduce Ambient Denoising Diffusion Probabilistic Models (DDPM) from approximated distributions, which enable learning on partially corrupted datasets with theoretical guarantees. Second, we use the noise-prediction property of Ambient DDPM to distinguish between clean and corrupted data, and then use the clean subset to train a standard DDPM. Third, we employ the trained standard DDPM to refine the previously identified corrupted data, enhancing data quality for subsequent offline RL training. A notable strength of ADG is its versatility-it can be seamlessly integrated with any offline RL algorithm. Experiments on a range of benchmarks, including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively mitigates the impact of corrupted data and improves the robustness of offline RL under various noise settings, achieving state-of-the-art results.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Ambient Diffusion-Guided Dataset Recovery (ADG)ï¼Œä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹è§£å†³ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline Reinforcement Learning)ä¸­æ•°æ®æ±¡æŸ“é—®é¢˜çš„åˆ›æ–°æ–¹æ³•ã€‚é’ˆå¯¹ç°å®ä¸–ç•Œæ•°æ®ä¸­æ™®éå­˜åœ¨çš„ä¼ æ„Ÿå™¨å™ªå£°å’Œäººä¸ºé”™è¯¯ï¼ŒADGå¼•å…¥äº†Ambient Denoising Diffusion Probabilistic Models (DDPM)ï¼Œèƒ½å¤Ÿä»è¿‘ä¼¼åˆ†å¸ƒä¸­å­¦ä¹ å¹¶æœ‰æ•ˆå¤„ç†é«˜ç»´çŠ¶æ€ç©ºé—´åŠå¤šé‡å…ƒç´ åŒæ—¶å—æŸçš„æƒ…å†µã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨Ambient DDPMçš„å™ªå£°é¢„æµ‹ç‰¹æ€§åŒºåˆ†æ¸…æ´ä¸æ±¡æŸ“æ•°æ®ï¼Œéšåä½¿ç”¨æ¸…æ´å­é›†è®­ç»ƒæ ‡å‡†DDPMæ¥ä¿®å¤å—æŸæ ·æœ¬ï¼Œä»è€Œæ˜¾è‘—æé«˜åç»­è®­ç»ƒçš„æ•°æ®è´¨é‡ã€‚ADGå…·æœ‰æé«˜çš„é€šç”¨æ€§ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•Offline RLç®—æ³•ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒADGåœ¨MuJoCoã€Kitchenå’ŒAdroitç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆå‡è½»äº†å—æŸæ•°æ®çš„å½±å“ï¼Œæå‡äº†ç®—æ³•çš„é²æ£’æ€§ï¼Œå¹¶è¾¾åˆ°äº†SOTAæ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23871v2",
      "published_date": "2025-05-29 11:36:41 UTC",
      "updated_date": "2025-06-05 03:54:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:03:12.935441+00:00"
    },
    {
      "arxiv_id": "2506.00061v1",
      "title": "Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs",
      "title_zh": "æ­ç§˜ SITTï¼šç¤¾ä¼šå½±å“æŠ€æœ¯åˆ†ç±»ä½“ç³»ä¸åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ£€æµ‹",
      "authors": [
        "Wiktoria Mieleszczenko-Kowszewicz",
        "Beata Bajcar",
        "Aleksander SzczÄ™sny",
        "Maciej Markiewicz",
        "Jolanta Babiak",
        "Berenika Dyczek",
        "PrzemysÅ‚aw Kazienko"
      ],
      "abstract": "In this work we present the Social Influence Technique Taxonomy (SITT), a comprehensive framework of 58 empirically grounded techniques organized into nine categories, designed to detect subtle forms of social influence in textual content. We also investigate the LLMs ability to identify various forms of social influence. Building on interdisciplinary foundations, we construct the SITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and translated into English -- to evaluate the ability of LLMs to identify these techniques. Using a hierarchical multi-label classification setup, we benchmark five LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our results show that while some models, notably Claude 3.5, achieved moderate success (F1 score = 0.45 for categories), overall performance of models remains limited, particularly for context-sensitive techniques. The findings demonstrate key limitations in current LLMs' sensitivity to nuanced linguistic cues and underscore the importance of domain-specific fine-tuning. This work contributes a novel resource and evaluation example for understanding how LLMs detect, classify, and potentially replicate strategies of social influence in natural dialogues.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Social Influence Technique Taxonomy (SITT)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«9ä¸ªç±»åˆ«ã€58ç§åŸºäºç»éªŒçš„æŠ€æœ¯æ¡†æ¶ï¼Œæ—¨åœ¨æ£€æµ‹æ–‡æœ¬ä¸­å¾®å¦™çš„Social Influenceå½¢å¼ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†SITTæ•°æ®é›†ï¼ŒåŒ…å«ç”±11åä¸“å®¶æ ‡æ³¨çš„746ä¸ªå¯¹è¯è¯­æ–™ï¼Œå¹¶åˆ©ç”¨è¯¥æ•°æ®é›†å¯¹GPT-4oã€Claude 3.5ã€Llama-3.1ã€Mixtralå’ŒPLLuMç­‰LLMsè¿›è¡Œäº†Hierarchical Multi-label ClassificationåŸºå‡†æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡Claude 3.5åœ¨ç±»åˆ«è¯†åˆ«ä¸Šå–å¾—äº†F1 score = 0.45çš„ä¸­ç­‰æˆç»©ï¼Œä½†æ¨¡å‹åœ¨å¤„ç†Context-sensitiveæŠ€æœ¯æ—¶æ•´ä½“è¡¨ç°ä¾ç„¶å—é™ã€‚ç ”ç©¶æ­ç¤ºäº†å½“å‰LLMså¯¹ç»†å¾®è¯­è¨€çº¿ç´¢æ•æ„Ÿåº¦ä¸è¶³çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†Domain-specific Fine-tuningçš„å¿…è¦æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£LLMså¦‚ä½•æ£€æµ‹ã€åˆ†ç±»ä»¥åŠæ½œåœ¨å¤åˆ¶è‡ªç„¶å¯¹è¯ä¸­çš„ç¤¾ä¼šå½±å“åŠ›ç­–ç•¥æä¾›äº†é‡è¦çš„èµ„æºä¸è¯„ä¼°å®ä¾‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.00061v1",
      "published_date": "2025-05-29 11:36:25 UTC",
      "updated_date": "2025-05-29 11:36:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:03:10.151269+00:00"
    },
    {
      "arxiv_id": "2505.23354v2",
      "title": "Representing local protein environments with atomistic foundation models",
      "title_zh": "åŸºäºåŸå­çº§åŸºç¡€æ¨¡å‹çš„å±€éƒ¨è›‹ç™½è´¨ç¯å¢ƒè¡¨å¾",
      "authors": [
        "Meital Bojan",
        "Sanketh Vedula",
        "Advaith Maddipatla",
        "Nadav Bojan Sellam",
        "Federico Napoli",
        "Paul Schanda",
        "Alex M. Bronstein"
      ],
      "abstract": "The local structure of a protein strongly impacts its function and interactions with other molecules. Therefore, a concise, informative representation of a local protein environment is essential for modeling and designing proteins and biomolecular interactions. However, these environments' extensive structural and chemical variability makes them challenging to model, and such representations remain under-explored. In this work, we propose a novel representation for a local protein environment derived from the intermediate features of atomistic foundation models (AFMs). We demonstrate that this embedding effectively captures both local structure (e.g., secondary motifs), and chemical features (e.g., amino-acid identity and protonation state). We further show that the AFM-derived representation space exhibits meaningful structure, enabling the construction of data-driven priors over the distribution of biomolecular environments. Finally, in the context of biomolecular NMR spectroscopy, we demonstrate that the proposed representations enable a first-of-its-kind physics-informed chemical shift predictor that achieves state-of-the-art accuracy. Our results demonstrate the surprising effectiveness of atomistic foundation models and their emergent representations for protein modeling beyond traditional molecular simulations. We believe this will open new lines of work in constructing effective functional representations for protein environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåŸå­åŸºç¡€æ¨¡å‹(AFMs)ä¸­é—´ç‰¹å¾çš„æ–°å‹è›‹ç™½è´¨å±€éƒ¨ç¯å¢ƒè¡¨å¾æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è›‹ç™½è´¨å±€éƒ¨ç»“æ„åŠå…¶åŒ–å­¦å¤šæ ·æ€§éš¾ä»¥å»ºæ¨¡çš„æŒ‘æˆ˜ã€‚è¯¥åµŒå…¥(Embedding)èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è›‹ç™½è´¨çš„å±€éƒ¨ç»“æ„ï¼ˆå¦‚äºŒçº§ç»“æ„å•å…ƒsecondary motifsï¼‰å’Œå…³é”®åŒ–å­¦ç‰¹å¾ï¼ˆå¦‚æ°¨åŸºé…¸èº«ä»½amino-acid identityåŠè´¨å­åŒ–çŠ¶æ€protonation stateï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒAFMè¡ç”Ÿçš„è¡¨å¾ç©ºé—´å…·å¤‡æ˜¾è‘—çš„ç‰©ç†æ„ä¹‰ï¼Œæ”¯æŒæ„å»ºç”Ÿç‰©åˆ†å­ç¯å¢ƒåˆ†å¸ƒçš„æ•°æ®é©±åŠ¨å…ˆéªŒ(Data-driven priors)ã€‚åœ¨ç”Ÿç‰©åˆ†å­æ ¸ç£å…±æŒ¯æ³¢è°±(NMR spectroscopy)åº”ç”¨ä¸­ï¼Œè¯¥è¡¨å¾åŠ©åŠ›å®ç°äº†ä¸€ç§é¦–åˆ›çš„ç‰©ç†å‘ŠçŸ¥åŒ–å­¦ä½ç§»é¢„æµ‹å™¨(Physics-informed chemical shift predictor)ï¼Œå¹¶å–å¾—äº†å½“å‰çš„é¢†å…ˆæ°´å¹³(SOTA)ç²¾åº¦ã€‚å®éªŒç»“æœè¯æ˜äº†åŸå­åŸºç¡€æ¨¡å‹åŠå…¶çªç°è¡¨å¾åœ¨è›‹ç™½è´¨å»ºæ¨¡ä¸­å…·æœ‰è¶…è¶Šä¼ ç»Ÿåˆ†å­æ¨¡æ‹Ÿçš„å“è¶Šæœ‰æ•ˆæ€§ã€‚è¯¥å·¥ä½œä¸ºæ„å»ºè›‹ç™½è´¨ç¯å¢ƒçš„åŠŸèƒ½æ€§è¡¨å¾å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23354v2",
      "published_date": "2025-05-29 11:25:47 UTC",
      "updated_date": "2025-06-16 22:48:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:03:04.539475+00:00"
    },
    {
      "arxiv_id": "2506.00060v1",
      "title": "Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports",
      "title_zh": "éšç§ä¿æŠ¤å‹å¼€æºå¤§è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠ CMR å½±åƒæŠ¥å‘Šè¯Šæ–­ä¿¡æ¯æå–ä¸­çš„å¯¹æ¯”åˆ†æ",
      "authors": [
        "Sina Amirrajab",
        "Volker Vehof",
        "Michael Bietenbeck",
        "Ali Yilmaz"
      ],
      "abstract": "Purpose: We investigated the utilization of privacy-preserving, locally-deployed, open-source Large Language Models (LLMs) to extract diagnostic information from free-text cardiovascular magnetic resonance (CMR) reports. Materials and Methods: We evaluated nine open-source LLMs on their ability to identify diagnoses and classify patients into various cardiac diagnostic categories based on descriptive findings in 109 clinical CMR reports. Performance was quantified using standard classification metrics including accuracy, precision, recall, and F1 score. We also employed confusion matrices to examine patterns of misclassification across models. Results: Most open-source LLMs demonstrated exceptional performance in classifying reports into different diagnostic categories. Google's Gemma2 model achieved the highest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B with F1 scores of 0.96 and 0.95, respectively. All other evaluated models attained average scores above 0.93, with Mistral and DeepseekR1-7B being the only exceptions. The top four LLMs outperformed our board-certified cardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR reports. Conclusion: Our findings demonstrate the feasibility of implementing open-source, privacy-preserving LLMs in clinical settings for automated analysis of imaging reports, enabling accurate, fast and resource-efficient diagnostic categorization.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†åˆ©ç”¨éšç§ä¿æŠ¤ã€æœ¬åœ°éƒ¨ç½²çš„å¼€æº Large Language Models (LLMs) ä»è‡ªç”±æ–‡æœ¬å¿ƒè¡€ç®¡ç£å…±æŒ¯ (CMR) æŠ¥å‘Šä¸­æå–è¯Šæ–­ä¿¡æ¯çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶äººå‘˜è¯„ä¼°äº†ä¹ç§å¼€æº LLMs åœ¨è¯†åˆ«è¯Šæ–­ä»¥åŠæ ¹æ® 109 ä»½ä¸´åºŠ CMR æŠ¥å‘Šä¸­çš„æè¿°æ€§å‘ç°å¯¹æ‚£è€…è¿›è¡Œåˆ†ç±»çš„èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨ F1 score ç­‰æ ‡å‡†åˆ†ç±»æŒ‡æ ‡é‡åŒ–æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°å¼€æº LLMs è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¸­ Google çš„ Gemma2 æ¨¡å‹è·å¾—äº†æœ€é«˜çš„å¹³å‡ F1 score (0.98)ï¼Œç´§éšå…¶åçš„æ˜¯ Qwen2.5:32B å’Œ DeepseekR1-32Bã€‚æ€§èƒ½æ’åå‰å››çš„ LLMs åœ¨åˆ†æ CMR æŠ¥å‘Šçš„æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¶…è¿‡äº†è·å¾—å§”å‘˜ä¼šè®¤è¯çš„å¿ƒè„ç—…ä¸“å®¶ (F1 score ä¸º 0.94)ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨ä¸´åºŠç¯å¢ƒä¸­å®æ–½å¼€æºã€éšç§ä¿æŠ¤çš„ LLMs è¿›è¡Œå½±åƒæŠ¥å‘Šè‡ªåŠ¨åˆ†æçš„å¯è¡Œæ€§ï¼Œèƒ½å¤Ÿå®ç°å‡†ç¡®ã€å¿«é€Ÿä¸”èµ„æºé«˜æ•ˆçš„è¯Šæ–­åˆ†ç±»ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "under review for Scientific Reports",
      "pdf_url": "https://arxiv.org/pdf/2506.00060v1",
      "published_date": "2025-05-29 11:25:10 UTC",
      "updated_date": "2025-05-29 11:25:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:03:02.809458+00:00"
    },
    {
      "arxiv_id": "2505.23353v1",
      "title": "Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis",
      "title_zh": "å¤šå‘æ€§ç¡¬åŒ–ç—‡ç¯å½¢ç—…ç¶çš„åˆæˆç”Ÿæˆä¸æ½œåœ¨æŠ•å½±å»å™ª",
      "authors": [
        "Alexandra G. Roberts",
        "Ha M. Luu",
        "Mert ÅiÅŸman",
        "Alexey V. Dimov",
        "Ceren Tozlu",
        "Ilhami Kovanlikaya",
        "Susan A. Gauthier",
        "Thanh D. Nguyen",
        "Yi Wang"
      ],
      "abstract": "Quantitative susceptibility maps from magnetic resonance images can provide both prognostic and diagnostic information in multiple sclerosis, a neurodegenerative disease characterized by the formation of lesions in white matter brain tissue. In particular, susceptibility maps provide adequate contrast to distinguish between \"rim\" lesions, surrounded by deposited paramagnetic iron, and \"non-rim\" lesion types. These paramagnetic rim lesions (PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been devoted to both detection and segmentation of such lesions to monitor longitudinal change. As paramagnetic rim lesions are rare, addressing this problem requires confronting the class imbalance between rim and non-rim lesions. We produce synthetic quantitative susceptibility maps of paramagnetic rim lesions and show that inclusion of such synthetic data improves classifier performance and provide a multi-channel extension to generate accompanying contrasts and probabilistic segmentation maps. We exploit the projection capability of our trained generative network to demonstrate a novel denoising approach that allows us to train on ambiguous rim cases and substantially increase the minority class. We show that both synthetic lesion synthesis and our proposed rim lesion label denoising method best approximate the unseen rim lesion distribution and improve detection in a clinically interpretable manner. We release our code and generated data at https://github.com/agr78/PRLx-GAN upon publication.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Multiple Sclerosis ä¸­å…·æœ‰é‡è¦è¯Šæ–­ä»·å€¼ä½†æ ·æœ¬ç¨€ç¼ºçš„ Paramagnetic Rim Lesions (PRLs) è¿›è¡Œç ”ç©¶ï¼Œæ—¨åœ¨è§£å†³å…¶åœ¨ Quantitative Susceptibility Maps (QSM) å›¾åƒåˆ†æä¸­é¢ä¸´çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åˆæˆç”Ÿæˆæ¡†æ¶æ¥äº§ç”Ÿ PRLs çš„åˆæˆ QSM æ•°æ®ï¼Œå¹¶åˆ©ç”¨å¤šé€šé“æ‰©å±•ç”Ÿæˆé…å¥—çš„å½±åƒå¯¹æ¯”åº¦ä¸æ¦‚ç‡åˆ†å‰²å›¾ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ–æ˜äº†ç”Ÿæˆç½‘ç»œçš„æŠ•å½±èƒ½åŠ›ï¼Œæå‡ºä¸€ç§ Latent Projection Denoising æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¨¡ç³Šç—…ä¾‹è¿›è¡Œå»å™ªå¤„ç†æ¥æ˜¾è‘—æ‰©å……å°‘æ•°ç±»æ ·æœ¬é‡ã€‚å®éªŒè¯æ˜ï¼Œåˆæˆæ•°æ®å¢å¼ºä¸æ ‡ç­¾å»å™ªæ–¹æ³•èƒ½æœ‰æ•ˆæ‹ŸåˆçœŸå®çš„ç—…å˜åˆ†å¸ƒï¼Œæ˜¾è‘—æå‡äº†åˆ†ç±»å™¨åœ¨æ£€æµ‹è¾¹ç¼˜ç—…å˜æ—¶çš„æ€§èƒ½ã€‚è¯¥æˆæœä¸ºå…·æœ‰ä¸´åºŠå¯è§£é‡Šæ€§çš„è¾…åŠ©è¯Šæ–­æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œå¹¶å·²å…¬å¼€å‘å¸ƒäº† PRLx-GAN ä»£ç åŠç”Ÿæˆçš„æ•°æ®é›†ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted full paper in Synthetic Data @ CVPR 2025 12 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23353v1",
      "published_date": "2025-05-29 11:22:48 UTC",
      "updated_date": "2025-05-29 11:22:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:03:21.210084+00:00"
    },
    {
      "arxiv_id": "2505.23352v1",
      "title": "Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems",
      "title_zh": "ç†è§£åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­é€šä¿¡æ‹“æ‰‘çš„ä¿¡æ¯ä¼ æ’­æ•ˆåº”",
      "authors": [
        "Xu Shen",
        "Yixin Liu",
        "Yiwei Dai",
        "Yili Wang",
        "Rui Miao",
        "Yue Tan",
        "Shirui Pan",
        "Xin Wang"
      ],
      "abstract": "The communication topology in large language model-based multi-agent systems fundamentally governs inter-agent collaboration patterns, critically shaping both the efficiency and effectiveness of collective decision-making. While recent studies for communication topology automated design tend to construct sparse structures for efficiency, they often overlook why and when sparse and dense topologies help or hinder collaboration. In this paper, we present a causal framework to analyze how agent outputs, whether correct or erroneous, propagate under topologies with varying sparsity. Our empirical studies reveal that moderately sparse topologies, which effectively suppress error propagation while preserving beneficial information diffusion, typically achieve optimal task performance. Guided by this insight, we propose a novel topology design approach, EIB-leanrner, that balances error suppression and beneficial information propagation by fusing connectivity patterns from both dense and sparse graphs. Extensive experiments show the superior effectiveness, communication cost, and robustness of EIB-leanrner.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM-based)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­é€šä¿¡æ‹“æ‰‘(communication topology)å¯¹åä½œæ¨¡å¼åŠå†³ç­–æ•ˆç‡çš„å½±å“ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªå› æœæ¡†æ¶(causal framework)ï¼Œç”¨äºåˆ†æä¸åŒç¨€ç–åº¦çš„æ‹“æ‰‘ç»“æ„ä¸‹ä»£ç†è¾“å‡ºçš„æ­£ç¡®æˆ–é”™è¯¯ä¿¡æ¯å¦‚ä½•ä¼ æ’­ã€‚ç ”ç©¶å‘ç°ï¼Œé€‚åº¦ç¨€ç–çš„æ‹“æ‰‘(moderately sparse topologies)èƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶é”™è¯¯ä¿¡æ¯çš„æ‰©æ•£ï¼ŒåŒæ—¶ä¿ç•™æœ‰ç›Šä¿¡æ¯çš„ä¼ æ’­ï¼Œä»è€Œå®ç°æœ€ä¼˜çš„ä»»åŠ¡æ€§èƒ½ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸ºEIB-leanrnerçš„æ‹“æ‰‘è®¾è®¡æ–¹æ³•ï¼Œé€šè¿‡èåˆç¨ å¯†å›¾ä¸ç¨€ç–å›¾çš„è¿æ¥æ¨¡å¼ï¼Œåœ¨æŠ‘åˆ¶é”™è¯¯å’Œä¿ƒè¿›æœ‰ç›Šä¿¡æ¯ä¼ æ’­ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒEIB-leanrneråœ¨æœ‰æ•ˆæ€§ã€é€šä¿¡æˆæœ¬(communication cost)åŠé²æ£’æ€§(robustness)æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23352v1",
      "published_date": "2025-05-29 11:21:48 UTC",
      "updated_date": "2025-05-29 11:21:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:04:01.209084+00:00"
    },
    {
      "arxiv_id": "2505.23349v1",
      "title": "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective",
      "title_zh": "è¿ˆå‘RLHFä¸­çš„å¥–åŠ±å…¬å¹³æ€§ï¼šåŸºäºèµ„æºåˆ†é…çš„è§†è§’",
      "authors": [
        "Sheng Ouyang",
        "Yulan Hu",
        "Ge Chen",
        "Qingyang Li",
        "Fuzheng Zhang",
        "Yong Liu"
      ],
      "abstract": "Rewards serve as proxies for human preferences and play a crucial role in Reinforcement Learning from Human Feedback (RLHF). However, if these rewards are inherently imperfect, exhibiting various biases, they can adversely affect the alignment of large language models (LLMs). In this paper, we collectively define the various biases present in rewards as the problem of reward unfairness. We propose a bias-agnostic method to address the issue of reward fairness from a resource allocation perspective, without specifically designing for each type of bias, yet effectively mitigating them. Specifically, we model preference learning as a resource allocation problem, treating rewards as resources to be allocated while considering the trade-off between utility and fairness in their distribution. We propose two methods, Fairness Regularization and Fairness Coefficient, to achieve fairness in rewards. We apply our methods in both verification and reinforcement learning scenarios to obtain a fairness reward model and a policy model, respectively. Experiments conducted in these scenarios demonstrate that our approach aligns LLMs with human preferences in a more fair manner.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ (RLHF)ä¸­çš„å¥–åŠ±å…¬å¹³æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºä¸å®Œç¾çš„å¥–åŠ±æ¨¡å‹åŠå…¶å†…åœ¨åå·®ä¼šä¸¥é‡å½±å“å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„å¯¹é½æ•ˆæœã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä»èµ„æºåˆ†é…(Resource Allocation)è§†è§’å‡ºå‘çš„é€šç”¨æ–¹æ³•ï¼Œæ—¨åœ¨ä¸é’ˆå¯¹ç‰¹å®šåå·®ç±»å‹çš„æƒ…å†µä¸‹æœ‰æ•ˆç¼“è§£å¥–åŠ±ä¸å…¬å¹³ã€‚è¯¥æ–¹æ³•å°†åå¥½å­¦ä¹ å»ºæ¨¡ä¸ºèµ„æºåˆ†é…é—®é¢˜ï¼Œå°†å¥–åŠ±è§†ä¸ºå¾…åˆ†é…èµ„æºï¼Œå¹¶åœ¨åˆ†é…è¿‡ç¨‹ä¸­æƒè¡¡æ•ˆç”¨(Utility)ä¸å…¬å¹³æ€§ã€‚æ–‡ä¸­å…·ä½“æå‡ºäº†å…¬å¹³æ€§æ­£åˆ™åŒ–(Fairness Regularization)å’Œå…¬å¹³æ€§ç³»æ•°(Fairness Coefficient)ä¸¤ç§æ‰‹æ®µï¼Œå¹¶å°†å…¶åº”ç”¨äºéªŒè¯å’Œå¼ºåŒ–å­¦ä¹ åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡å¥–åŠ±åˆ†é…çš„å…¬å¹³æ€§ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä»¥æ›´å…¬æ­£çš„æ–¹å¼ä¸äººç±»åå¥½è¾¾æˆå¯¹é½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23349v1",
      "published_date": "2025-05-29 11:12:00 UTC",
      "updated_date": "2025-05-29 11:12:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:03:22.806835+00:00"
    },
    {
      "arxiv_id": "2506.03177v2",
      "title": "Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„ä¹³è…º X çº¿æ‘„å½±ä¹³è…ºç™Œæ£€æµ‹ï¼šä¸€é¡¹é’ˆå¯¹ Thai äººç¾¤çš„å¤šä¸­å¿ƒéªŒè¯ç ”ç©¶",
      "authors": [
        "Isarun Chamveha",
        "Supphanut Chaiyungyuen",
        "Sasinun Worakriangkrai",
        "Nattawadee Prasawang",
        "Warasinee Chaisangmongkon",
        "Pornpim Korpraphong",
        "Voraparee Suvannarerg",
        "Shanigarn Thiravit",
        "Chalermdej Kannawat",
        "Kewalin Rungsinaporn",
        "Suwara Issaragrisil",
        "Payia Chadbunchachai",
        "Pattiya Gatechumpol",
        "Chawiporn Muktabhant",
        "Patarachai Sereerat"
      ],
      "abstract": "This study presents a deep learning system for breast cancer detection in mammography, developed using a modified EfficientNetV2 architecture with enhanced attention mechanisms. The model was trained on mammograms from a major Thai medical center and validated on three distinct datasets: an in-domain test set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain generalizability set (761 cases) collected from two different hospitals. For cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the respective datasets. The system's lesion localization capability, evaluated using metrics including Lesion Localization Fraction (LLF) and Non-Lesion Localization Fraction (NLF), demonstrated robust performance in identifying suspicious regions. Clinical validation through concordance tests showed strong agreement with radiologists: 83.5% classification and 84.0% localization concordance for biopsy-confirmed cases, and 78.1% classification and 79.6% localization concordance for out-of-domain cases. Expert radiologists' acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for out-of-domain cases. The system achieved a System Usability Scale score of 74.17 for source hospital, and 69.20 for validation hospitals, indicating good clinical acceptance. These results demonstrate the model's effectiveness in assisting mammogram interpretation, with the potential to enhance breast cancer screening workflows in clinical practice.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ (Deep Learning)çš„ä¹³è…ºç™Œæ£€æµ‹ç³»ç»Ÿï¼Œé‡‡ç”¨æ”¹è¿›çš„EfficientNetV2æ¶æ„å¹¶å¼•å…¥äº†å¢å¼ºçš„æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanisms)ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¹³è…ºXå°„çº¿æ‘„å½±(Mammography)çš„å½±åƒè§£è¯»ã€‚æ¨¡å‹åˆ©ç”¨æ³°å›½å¤§å‹åŒ»ç–—ä¸­å¿ƒçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨åŒ…å«åŸŸå†…æµ‹è¯•ã€æ´»æ£€ç¡®è¯åŠè·¨åŒ»é™¢æ³›åŒ–é›†çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šå®Œæˆäº†å¤šä¸­å¿ƒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ç™Œç—‡æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—äº†0.89è‡³0.96çš„AUROCè¯„åˆ†ï¼Œå…¶ç—…ç¶å®šä½(Lesion Localization)èƒ½åŠ›åœ¨è¯†åˆ«å¯ç–‘åŒºåŸŸæ–¹é¢è¡¨ç°ç¨³å¥ã€‚ä¸´åºŠä¸€è‡´æ€§æµ‹è¯•æ˜¾ç¤ºç³»ç»Ÿä¸æ”¾å°„ç§‘åŒ»ç”Ÿé«˜åº¦å¥‘åˆï¼Œæ´»æ£€ç¡®è¯ç—…ä¾‹çš„åˆ†ç±»ä¸€è‡´ç‡è¾¾83.5%ï¼Œä¸”èµ„æ·±åŒ»ç”Ÿçš„å¹³å‡æ¥å—åº¦æœ€é«˜è¾¾96.7%ã€‚ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨(System Usability Scale)çš„è¯„ä¼°ç»“æœè¿›ä¸€æ­¥è¯å®äº†å…¶è‰¯å¥½çš„ä¸´åºŠè®¤å¯åº¦ï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨æå‡ä¹³è…ºç™Œç­›æŸ¥å·¥ä½œæµæ•ˆç‡æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03177v2",
      "published_date": "2025-05-29 11:11:41 UTC",
      "updated_date": "2025-06-16 07:42:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:04:35.963163+00:00"
    },
    {
      "arxiv_id": "2505.23870v2",
      "title": "MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection",
      "title_zh": "MaCPï¼šåŸºäºåˆ†å±‚ä½™å¼¦æŠ•å½±çš„æç®€ä¸”å¼ºå¤§é€‚é…",
      "authors": [
        "Yixian Shen",
        "Qi Bi",
        "Jia-Hong Huang",
        "Hongyi Zhu",
        "Andy D. Pimentel",
        "Anuj Pathania"
      ],
      "abstract": "We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MaCPï¼ˆMinimal yet Mighty adaptive Cosine Projectionï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é€šè¿‡æå°‘å‚æ•°å’Œå†…å­˜éœ€æ±‚å®ç°å¤§æ¨¡å‹å¾®è°ƒçš„æ–°å‹é€‚é…æ–¹æ³•ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ä½™å¼¦æŠ•å½±ï¼ˆcosine projectionï¼‰å“è¶Šçš„èƒ½é‡å‹ç¼©å’Œå»ç›¸å…³ç‰¹æ€§ï¼Œä»è€Œåœ¨æé«˜æ¨¡å‹æ•ˆç‡çš„åŒæ—¶æå‡å‡†ç¡®æ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒMaCPå°†æ¥è‡ªä½ç§©é€‚é…ï¼ˆlow-rank adaptationï¼‰çš„æƒé‡å˜åŒ–æŠ•å½±åˆ°ç¦»æ•£ä½™å¼¦ç©ºé—´ï¼ˆdiscrete cosine spaceï¼‰ï¼Œå¹¶å°†å…¶åœ¨ç¦»æ•£ä½™å¼¦è°±çš„ä¸åŒå±‚æ¬¡ä¸Šè¿›è¡Œåˆ’åˆ†ï¼Œè¿›è€Œç­›é€‰å‡ºæ¯ä¸ªéƒ¨åˆ†ä¸­æœ€å…³é”®çš„é¢‘ç‡åˆ†é‡ã€‚åœ¨æ¶µç›–è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆnatural language understandingï¼‰ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆnatural language generationï¼‰ã€å›¾åƒåˆ†ç±»åŠè§†é¢‘ç†è§£ç­‰å¤šæ¨¡æ€ä»»åŠ¡çš„å¹¿æ³›å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å‡å±•ç°äº†å‡ºè‰²çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ›¿ä»£æ–¹æ¡ˆç›¸æ¯”ï¼ŒMaCPåœ¨ä¿æŒå“è¶Šå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜æ¶ˆè€—ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work was intended as a replacement of arXiv:2410.09103 and any subsequent updates will appear there",
      "pdf_url": "https://arxiv.org/pdf/2505.23870v2",
      "published_date": "2025-05-29 10:55:12 UTC",
      "updated_date": "2025-08-08 18:25:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:04:23.659893+00:00"
    },
    {
      "arxiv_id": "2505.23337v3",
      "title": "Matryoshka Model Learning for Improved Elastic Student Models",
      "title_zh": "é¢å‘æ”¹è¿›å¼¹æ€§å­¦ç”Ÿæ¨¡å‹çš„å¥—å¨ƒæ¨¡å‹å­¦ä¹ ",
      "authors": [
        "Chetan Verma",
        "Aditya Srinivas Timmaraju",
        "Cho-Jui Hsieh",
        "Suyash Damle",
        "Ngot Bui",
        "Yang Zhang",
        "Wen Chen",
        "Xin Liu",
        "Prateek Jain",
        "Inderjit S Dhillon"
      ],
      "abstract": "Industry-grade ML models are carefully designed to meet rapidly evolving serving constraints, which requires significant resources for model development. In this paper, we propose MatTA, a framework for training multiple accurate Student models using a novel Teacher-TA-Student recipe. TA models are larger versions of the Student models with higher capacity, and thus allow Student models to better relate to the Teacher model and also bring in more domain-specific expertise. Furthermore, multiple accurate Student models can be extracted from the TA model. Therefore, despite only one training run, our methodology provides multiple servable options to trade off accuracy for lower serving cost. We demonstrate the proposed method, MatTA, on proprietary datasets and models. Its practical efficacy is underscored by live A/B tests within a production ML system, demonstrating 20% improvement on a key metric. We also demonstrate our method on GPT-2 Medium, a public model, and achieve relative improvements of over 24% on SAT Math and over 10% on the LAMBADA benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MatTA æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ”¹è¿›çš„ Matryoshka Model Learning æ–¹æ³•ï¼Œæå‡åœ¨åŠ¨æ€éƒ¨ç½²çº¦æŸä¸‹ Student æ¨¡å‹çš„æ€§èƒ½ä¸çµæ´»æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å…¨æ–°çš„ Teacher-TA-Student è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¼•å…¥å®¹é‡ä»‹äºä¸¤è€…ä¹‹é—´çš„ TA æ¨¡å‹ï¼Œä¸ä»…å¢å¼ºäº† Student å¯¹ Teacher çŸ¥è¯†çš„å¸æ”¶èƒ½åŠ›ï¼Œè¿˜å¼•å…¥äº†æ›´ä¸°å¯Œçš„é¢†åŸŸä¸“é•¿ã€‚MatTA å…è®¸åœ¨å•æ¬¡è®­ç»ƒè¿è¡Œä¸­æå–å‡ºå¤šä¸ªç²¾ç¡®çš„ Student æ¨¡å‹ï¼Œä»è€Œåœ¨é™ä½æœåŠ¡æˆæœ¬çš„åŒæ—¶ï¼Œä¸ºå‡†ç¡®ç‡ä¸æ€§èƒ½çš„æƒè¡¡æä¾›å¤šç§å¯éƒ¨ç½²é€‰é¡¹ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒçš„ A/B æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å°†å…³é”®æŒ‡æ ‡æå‡äº† 20%ï¼›åŒæ—¶åœ¨ GPT-2 Medium å…¬å¼€æ¨¡å‹ä¸Šï¼Œå…¶åœ¨ SAT Math å’Œ LAMBADA åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å®ç°äº†è¶…è¿‡ 24% å’Œ 10% çš„ç›¸å¯¹æ”¹è¿›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures, Accepted at KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23337v3",
      "published_date": "2025-05-29 10:54:58 UTC",
      "updated_date": "2025-12-02 15:26:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:04:48.437057+00:00"
    },
    {
      "arxiv_id": "2505.23331v2",
      "title": "Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization",
      "title_zh": "åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„ä¸‹ä¸€å°ºåº¦è§†è§‰è‡ªå›å½’æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Matteo Gallici",
        "Haitz SÃ¡ez de OcÃ¡riz Borde"
      ],
      "abstract": "Fine-tuning pre-trained generative models with Reinforcement Learning (RL) has emerged as an effective approach for aligning outputs more closely with nuanced human preferences. In this paper, we investigate the application of Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual autoregressive (VAR) models. Our empirical results demonstrate that this approach enables alignment to intricate reward signals derived from aesthetic predictors and CLIP embeddings, significantly enhancing image quality and enabling precise control over the generation style. Interestingly, by leveraging CLIP, our method can help VAR models generalize beyond their initial ImageNet distribution: through RL-driven exploration, these models can generate images aligned with prompts referencing image styles that were absent during pre-training. In summary, we show that RL-based fine-tuning is both efficient and effective for VAR models, benefiting particularly from their fast inference speeds, which are advantageous for online sampling, an aspect that poses significant challenges for diffusion-based alternatives.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Group Relative Policy Optimization, GRPO) å¯¹è§†è§‰è‡ªå›å½’ (Visual Autoregressive, VAR) æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ä½¿å…¶è¾“å‡ºæ›´å¥½åœ°ç¬¦åˆäººç±»åå¥½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸå°†æ¨¡å‹ä¸ç¾å­¦é¢„æµ‹å™¨å’Œ CLIP åµŒå…¥æä¾›çš„å¤æ‚å¥–åŠ±ä¿¡å·å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒè´¨é‡å¹¶å®ç°äº†å¯¹ç”Ÿæˆé£æ ¼çš„ç²¾ç¡®æ§åˆ¶ã€‚é€šè¿‡ CLIP å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) é©±åŠ¨æ¢ç´¢ï¼ŒVAR æ¨¡å‹å±•ç°å‡ºè¶…è¶Šåˆå§‹ ImageNet åˆ†å¸ƒçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆé¢„è®­ç»ƒä¸­æœªæ¶µç›–çš„ç‰¹å®šé£æ ¼å›¾åƒã€‚æœ€åï¼Œç ”ç©¶å¼ºè°ƒäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒå¯¹ VAR æ¨¡å‹éå¸¸é«˜æ•ˆï¼Œç‰¹åˆ«å—ç›Šäºå…¶ç›¸å¯¹äºæ‰©æ•£æ¨¡å‹ (Diffusion Models) æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œè¿™ä¸ºåœ¨çº¿é‡‡æ ·å’Œæ¨¡å‹å¯¹é½æä¾›äº†å·¨å¤§ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23331v2",
      "published_date": "2025-05-29 10:45:38 UTC",
      "updated_date": "2025-06-28 18:55:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:04:28.515158+00:00"
    },
    {
      "arxiv_id": "2505.23868v5",
      "title": "Noise-Robustness Through Noise: A Framework combining Asymmetric LoRA with Poisoning MoE",
      "title_zh": "ä»¥å™ªåˆ¶å™ªï¼šä¸€ç§ç»“åˆéå¯¹ç§° LoRA ä¸æŠ•æ¯’ MoE çš„å™ªå£°é²æ£’æ¡†æ¶",
      "authors": [
        "Zhaokun Wang",
        "Jinyu Guo",
        "Jingwen Pu",
        "Lingfeng Chen",
        "Hongli Pu",
        "Jie Ou",
        "Libo Qin",
        "Wenhong Tian"
      ],
      "abstract": "Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º LoPE (Asymmetric LoRA Poisoning Experts) çš„å™ªå£°é²æ£’é€‚é…æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æ–¹æ³•åœ¨å¤„ç†ä¸‹æ¸¸ä»»åŠ¡æ—¶æ˜“å—å™ªå£°æ•°æ®å¹²æ‰°çš„é—®é¢˜ã€‚LoPE å€Ÿé‰´äº† Mixture-of-Experts (MoE) æ¶æ„ï¼Œåœ¨ Asymmetric LoRA é…ç½®ä¸­æˆ˜ç•¥æ€§åœ°é›†æˆäº†ä¸€ä¸ªä¸“é—¨çš„ Poisoning Expertã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µèŒƒå¼ï¼Œåœ¨å¾®è°ƒé˜¶æ®µé€šè¿‡å‘ Poisoning Expert æ³¨å…¥ç”Ÿæˆçš„å™ªå£°æ•°æ®æ¥å¢å¼ºå…¶å™ªå£°åˆ¤åˆ«ä¸å¤„ç†èƒ½åŠ›ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œç³»ç»Ÿé€šè¿‡é€‰æ‹©æ€§å±è”½è¯¥ä¸“å®¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨ Normal Experts ä¹ å¾—çš„çº¯å‡€çŸ¥è¯†å®ç°å™ªå£°é²æ£’çš„è¾“å‡ºã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒLoPE ä»…é€šè¿‡ä½æˆæœ¬çš„å™ªå£°æ³¨å…¥å³å¯å®ç°å¼ºåŠ²çš„æ€§èƒ½ä¸ç¨³å¥æ€§ï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹ç¹çæ•°æ®æ¸…æ´—æµç¨‹çš„ä¾èµ–ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accecpted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23868v5",
      "published_date": "2025-05-29 10:35:07 UTC",
      "updated_date": "2025-10-21 03:03:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:04:52.017248+00:00"
    },
    {
      "arxiv_id": "2505.23867v3",
      "title": "InfiMed: Low-Resource Medical MLLMs with Advancing Understanding and Reasoning",
      "title_zh": "InfiMedï¼šæå‡ç†è§£ä¸æ¨ç†èƒ½åŠ›çš„ä½èµ„æºåŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Zeyu Liu",
        "Zhitian Hou",
        "Guanghao Zhu",
        "Zhijie Sang",
        "Congkai Xie",
        "Hongxia Yang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in domains such as visual understanding and mathematical reasoning. However, their application in the medical domain is constrained by two key challenges: (1) multimodal medical datasets are scarce and often contain sparse information, limiting reasoning depth; and (2) Reinforcement Learning with Verifiable Rewards (RLVR), though effective in general domains, cannot reliably improve model performance in the medical domain. To overcome these challenges, during the supervised fine-tuning (SFT) stage, we incorporate high-quality textual reasoning data and general multimodal data alongside multimodal medical data to efficiently enhance foundational medical capabilities and restore the base model's reasoning ability. Moreover, considering that there are some multimodal medical datasets with sparse information, we further synthesize reflective-pattern-injected chain-of-thought (CoT) in addition to general CoT samples, equipping the model with initial reflective reasoning capabilities that provide a structured foundation for subsequent RLVR training. Finally, we introduce our InfiMed-Series models, InfiMed-SFT-3B and InfiMed-RL-3B, both of which deliver state-of-the-art performance across seven multimodal medical benchmarks. Notably, InfiMed-RL-3B achieves an average accuracy of 59.2%, outperforming even larger models like InternVL3-8B, which achieves 57.3%. Specifically, during the SFT phase, we utilized 188K samples, while the RLVR phase incorporated 36K samples, demonstrating the efficacy of both training strategies in achieving superior performance. We also conducted a series of extensive experiments, which provide valuable insights that contribute to advancing the performance of MLLMs in medical scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InfiMed ç³»åˆ—æ¨¡å‹ï¼ŒåŒ…æ‹¬ InfiMed-SFT-3B å’Œ InfiMed-RL-3Bï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨åŒ»ç–—é¢†åŸŸé¢ä¸´çš„æ•°æ®ç¨€ç¼ºã€æ¨ç†æ·±åº¦æœ‰é™åŠ Reinforcement Learning with Verifiable Rewards (RLVR) æ•ˆæœä¸ä½³ç­‰æŒ‘æˆ˜ã€‚åœ¨ç›‘ç£å¾®è°ƒ (SFT) é˜¶æ®µï¼Œå›¢é˜Ÿé€šè¿‡æ•´åˆé«˜è´¨é‡æ–‡æœ¬æ¨ç†æ•°æ®ã€é€šç”¨å¤šæ¨¡æ€æ•°æ®ä¸åŒ»ç–—æ•°æ®ï¼Œæœ‰æ•ˆæ¢å¤äº†æ¨¡å‹çš„åŸºç¡€æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ³¨å…¥åæ€æ¨¡å¼çš„é“¾å¼æ€ç»´ (Chain-of-Thought, CoT) æ ·æœ¬ï¼Œä¸ºæ¨¡å‹æ„å»ºäº†ç»“æ„åŒ–çš„åæ€æ¨ç†åŸºç¡€ï¼Œä»è€Œä¼˜åŒ–äº†åç»­çš„ RLVR è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInfiMed ç³»åˆ—åœ¨ä¸ƒä¸ªå¤šæ¨¡æ€åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†å½“å‰é¢†å…ˆçš„ (State-of-the-art) è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯ä»…æœ‰ 3B å‚æ•°çš„ InfiMed-RL-3B ä»¥ 59.2% çš„å¹³å‡å‡†ç¡®ç‡è¶…è¶Šäº† InternVL3-8B ç­‰æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œå……åˆ†éªŒè¯äº†è¯¥ä½èµ„æºè®­ç»ƒæ–¹æ¡ˆåœ¨æå‡åŒ»ç–—é¢†åŸŸç†è§£ä¸æ¨ç†ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23867v3",
      "published_date": "2025-05-29 10:31:57 UTC",
      "updated_date": "2025-10-08 09:46:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:04:44.814202+00:00"
    },
    {
      "arxiv_id": "2505.23315v1",
      "title": "Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments",
      "title_zh": "é€šè¿‡æ•™è‚²æµ‹è¯„ä¸­çš„åºæ•°ç½®ä¿¡åº¦å»ºæ¨¡æå‡è¯„åˆ†å‡†ç¡®æ€§",
      "authors": [
        "Abhirup Chakravarty",
        "Mark Brenchley",
        "Trevor Breakspear",
        "Ian Lewin",
        "Yan Huang"
      ],
      "abstract": "A key ethical challenge in Automated Essay Scoring (AES) is ensuring that scores are only released when they meet high reliability standards. Confidence modelling addresses this by assigning a reliability estimate measure, in the form of a confidence score, to each automated score. In this study, we frame confidence estimation as a classification task: predicting whether an AES-generated score correctly places a candidate in the appropriate CEFR level. While this is a binary decision, we leverage the inherent granularity of the scoring domain in two ways. First, we reformulate the task as an n-ary classification problem using score binning. Second, we introduce a set of novel Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that incorporate the ordinal structure of CEFR labels. Our best-performing model achieves an F1 score of 0.97, and enables the system to release 47% of scores with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to approximately 92% (approx.) CEFR agreement from the standalone AES model where we release all AM predicted scores.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨ä½œæ–‡è¯„åˆ†(Automated Essay Scoring, AES)ä¸­çš„è¯„åˆ†å¯é æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡åºæ•°ç½®ä¿¡åº¦å»ºæ¨¡æ¥æå‡è¯„ä¼°å‡†ç¡®æ€§çš„æ–¹æ³•ã€‚ä½œè€…å°†ç½®ä¿¡åº¦ä¼°è®¡å®šä¹‰ä¸ºé¢„æµ‹AESè¯„åˆ†æ˜¯å¦ç¬¦åˆæ¬§æ´²è¯­è¨€å…±åŒå‚è€ƒæ¡†æ¶(CEFR)ç­‰çº§çš„åˆ†ç±»ä»»åŠ¡ã€‚ç ”ç©¶é€šè¿‡åˆ†æ¡¶æŠ€æœ¯(score binning)å°†ä»»åŠ¡é‡æ–°è¡¨è¿°ä¸ºnå…ƒåˆ†ç±»é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç»„åˆ›æ–°çš„æ ¸åŠ æƒåºæ•°åˆ†ç±»äº¤å‰ç†µ(Kernel Weighted Ordinal Categorical Cross Entropy, KWOCCE)æŸå¤±å‡½æ•°ï¼Œä»¥å……åˆ†åˆ©ç”¨CEFRæ ‡ç­¾çš„åºæ•°ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œå…¶æœ€ä½³æ¨¡å‹è¾¾åˆ°äº†0.97çš„F1åˆ†æ•°ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿä»¥100%çš„ä¸€è‡´æ€§å‘å¸ƒ47%çš„è¯„åˆ†ï¼Œä¸”99%çš„å‘å¸ƒè¯„åˆ†å…·æœ‰è‡³å°‘95%çš„CEFRä¸€è‡´æ€§ã€‚ç›¸æ¯”äºç‹¬ç«‹AESæ¨¡å‹çº¦92%çš„ä¸€è‡´æ€§ï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡æä¾›å¯é çš„ç½®ä¿¡åº¦åº¦é‡ï¼Œæ˜¾è‘—å¢å¼ºäº†æ•™è‚²è¯„ä¼°ä¸­è‡ªåŠ¨åŒ–è¯„åˆ†ç³»ç»Ÿçš„å¯ä¿¡åº¦ä¸å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "This is the preprint version of our paper accepted to ACL 2025 (Industry Track). The DOI will be added once available",
      "pdf_url": "https://arxiv.org/pdf/2505.23315v1",
      "published_date": "2025-05-29 10:23:20 UTC",
      "updated_date": "2025-05-29 10:23:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:04:55.016885+00:00"
    },
    {
      "arxiv_id": "2505.23313v1",
      "title": "Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition",
      "title_zh": "é¢å‘è¡Œäººå±æ€§è¯†åˆ«çš„å¯¹æŠ—æ€§è¯­ä¹‰ä¸æ ‡ç­¾æ‰°åŠ¨æ”»å‡»",
      "authors": [
        "Weizhe Kong",
        "Xiao Wang",
        "Ruichong Gao",
        "Chenglong Li",
        "Yu Zhang",
        "Xing Yang",
        "Yaowei Wang",
        "Jin Tang"
      ],
      "abstract": "Pedestrian Attribute Recognition (PAR) is an indispensable task in human-centered research and has made great progress in recent years with the development of deep neural networks. However, the potential vulnerability and anti-interference ability have still not been fully explored. To bridge this gap, this paper proposes the first adversarial attack and defense framework for pedestrian attribute recognition. Specifically, we exploit both global- and patch-level attacks on the pedestrian images, based on the pre-trained CLIP-based PAR framework. It first divides the input pedestrian image into non-overlapping patches and embeds them into feature embeddings using a projection layer. Meanwhile, the attribute set is expanded into sentences using prompts and embedded into attribute features using a pre-trained CLIP text encoder. A multi-modal Transformer is adopted to fuse the obtained vision and text tokens, and a feed-forward network is utilized for attribute recognition. Based on the aforementioned PAR framework, we adopt the adversarial semantic and label-perturbation to generate the adversarial noise, termed ASL-PAR. We also design a semantic offset defense strategy to suppress the influence of adversarial attacks. Extensive experiments conducted on both digital domains (i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the effectiveness of our proposed adversarial attack and defense strategies for the pedestrian attribute recognition. The source code of this paper will be released on https://github.com/Event-AHU/OpenPAR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡Œäººå±æ€§è¯†åˆ«(Pedestrian Attribute Recognition, PAR)ä»»åŠ¡ä¸­æ·±åº¦ç¥ç»ç½‘ç»œçš„è„†å¼±æ€§ä¸æŠ—å¹²æ‰°èƒ½åŠ›ï¼Œæå‡ºäº†é¦–ä¸ªå¯¹æŠ—æ”»å‡»ä¸é˜²å¾¡æ¡†æ¶ã€‚ç ”ç©¶åŸºäºé¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼Œåˆ©ç”¨å¤šæ¨¡æ€Transformerèåˆå›¾åƒè¡¥ä¸(patches)åµŒå…¥ä¸ç»è¿‡æç¤ºæ‰©å±•çš„å±æ€§æ–‡æœ¬ç‰¹å¾ã€‚ä½œè€…æå‡ºäº†åä¸ºASL-PARçš„å¯¹æŠ—è¯­ä¹‰ä¸æ ‡ç­¾æ‰°åŠ¨æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡åœ¨å…¨å±€å’Œè¡¥ä¸çº§åˆ«å¼•å…¥å¯¹æŠ—å™ªå£°æ¥æµ‹è¯•æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå¹¶åŒæ­¥è®¾è®¡äº†è¯­ä¹‰åç§»é˜²å¾¡ç­–ç•¥(semantic offset defense strategy)æ¥æŠµæ¶ˆæ”»å‡»å½±å“ã€‚åœ¨PETAã€PA100Kã€MSP60Kå’ŒRAPv2ç­‰å¤šä¸ªä¸»æµæ•°æ®é›†ä»¥åŠç‰©ç†é¢†åŸŸçš„å¹¿æ³›å®éªŒå……åˆ†éªŒè¯äº†è¯¥æ”»å‡»ä¸é˜²å¾¡ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæå‡PARç³»ç»Ÿçš„é²æ£’æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23313v1",
      "published_date": "2025-05-29 10:17:17 UTC",
      "updated_date": "2025-05-29 10:17:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:04:49.463040+00:00"
    },
    {
      "arxiv_id": "2505.23309v1",
      "title": "Score-based Generative Modeling for Conditional Independence Testing",
      "title_zh": "åŸºäºåˆ†æ•°ç”Ÿæˆæ¨¡å‹çš„æ¡ä»¶ç‹¬ç«‹æ€§æ£€éªŒ",
      "authors": [
        "Yixin Ren",
        "Chenghou Jin",
        "Yewei Xia",
        "Li Ke",
        "Longtao Huang",
        "Hui Xue",
        "Hao Zhang",
        "Jihong Guan",
        "Shuigeng Zhou"
      ],
      "abstract": "Determining conditional independence (CI) relationships between random variables is a fundamental yet challenging task in machine learning and statistics, especially in high-dimensional settings. Existing generative model-based CI testing methods, such as those utilizing generative adversarial networks (GANs), often struggle with undesirable modeling of conditional distributions and training instability, resulting in subpar performance. To address these issues, we propose a novel CI testing method via score-based generative modeling, which achieves precise Type I error control and strong testing power. Concretely, we first employ a sliced conditional score matching scheme to accurately estimate conditional score and use Langevin dynamics conditional sampling to generate null hypothesis samples, ensuring precise Type I error control. Then, we incorporate a goodness-of-fit stage into the method to verify generated samples and enhance interpretability in practice. We theoretically establish the error bound of conditional distributions modeled by score-based generative models and prove the validity of our CI tests. Extensive experiments on both synthetic and real-world datasets show that our method significantly outperforms existing state-of-the-art methods, providing a promising way to revitalize generative model-based CI testing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº score-based generative modeling çš„æ–°å‹æ¡ä»¶ç‹¬ç«‹æ€§ (conditional independence, CI) æµ‹è¯•æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é«˜ç»´åœºæ™¯ä¸‹ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨å»ºæ¨¡æ¡ä»¶åˆ†å¸ƒåŠè®­ç»ƒç¨³å®šæ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆé‡‡ç”¨ sliced conditional score matching æ–¹æ¡ˆç²¾ç¡®ä¼°è®¡æ¡ä»¶å¾—åˆ†ï¼Œå¹¶ç»“åˆ Langevin dynamics conditional sampling ç”Ÿæˆé›¶å‡è®¾ (null hypothesis) æ ·æœ¬ï¼Œä»è€Œå®ç°å¯¹ Type I error çš„ç²¾ç¡®æ§åˆ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶ä¸­è¿˜åŠ å…¥äº†ä¸€ä¸ªæ‹Ÿåˆä¼˜åº¦ (goodness-of-fit) é˜¶æ®µæ¥éªŒè¯ç”Ÿæˆæ ·æœ¬ï¼Œæå‡äº†å®é™…åº”ç”¨ä¸­çš„å¯è§£é‡Šæ€§ã€‚ç†è®ºä¸Šï¼Œè¯¥ç ”ç©¶å»ºç«‹äº†æ¡ä»¶åˆ†å¸ƒçš„è¯¯å·®ç•Œé™å¹¶è¯æ˜äº† CI æµ‹è¯•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ state-of-the-art æ–¹æ³•ï¼Œä¸ºæŒ¯å…´åŸºäºç”Ÿæˆæ¨¡å‹çš„æ¡ä»¶ç‹¬ç«‹æ€§æµ‹è¯•æä¾›äº†æå…·å‰æ™¯çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by KDD2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23309v1",
      "published_date": "2025-05-29 10:10:46 UTC",
      "updated_date": "2025-05-29 10:10:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:05:16.610436+00:00"
    },
    {
      "arxiv_id": "2505.23308v1",
      "title": "Spoken question answering for visual queries",
      "title_zh": "é¢å‘è§†è§‰æŸ¥è¯¢çš„è¯­éŸ³é—®ç­”",
      "authors": [
        "Nimrod Shabtay",
        "Zvi Kons",
        "Avihu Dekel",
        "Hagai Aronowitz",
        "Ron Hoory",
        "Assaf Arbelle"
      ],
      "abstract": "Question answering (QA) systems are designed to answer natural language questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA system to accept visual and spoken input respectively.\n  This work aims to create a system that enables user interaction through both speech and images. That is achieved through the fusion of text, speech, and image modalities to tackle the task of spoken VQA (SVQA). The resulting multi-modal model has textual, visual, and spoken inputs and can answer spoken questions on images.\n  Training and evaluating SVQA models requires a dataset for all three modalities, but no such dataset currently exists. We address this problem by synthesizing VQA datasets using two zero-shot TTS models. Our initial findings indicate that a model trained only with synthesized speech nearly reaches the performance of the upper-bounding model trained on textual QAs. In addition, we show that the choice of the TTS model has a minor impact on accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶è‡´åŠ›äºå¼€å‘ä¸€ç§èƒ½å¤Ÿé€šè¿‡è¯­éŸ³å’Œå›¾åƒä¸ç”¨æˆ·äº’åŠ¨çš„ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ Spoken Question Answering for Visual Queries (SVQA) ä»»åŠ¡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡èåˆæ–‡æœ¬ã€è¯­éŸ³å’Œå›¾åƒä¸‰ç§æ¨¡æ€ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç›´æ¥é’ˆå¯¹å›¾åƒå›ç­”è¯­éŸ³å½¢å¼çš„é—®é¢˜ã€‚é’ˆå¯¹ç›®å‰ç¼ºä¹åŒæ—¶åŒ…å«è¿™ä¸‰ç§æ¨¡æ€çš„ SVQA æ•°æ®é›†çš„é—®é¢˜ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨ä¸¤ç§ zero-shot TTS æ¨¡å‹åˆæˆäº†è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨åˆæˆè¯­éŸ³è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½å‡ ä¹è¾¾åˆ°äº†ä»¥æ–‡æœ¬é—®ç­”ä¸ºåŸºå‡†çš„æ€§èƒ½ä¸Šé™ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ‰€é€‰ç”¨çš„ TTS æ¨¡å‹ç±»å‹å¯¹æœ€ç»ˆé—®ç­”å‡†ç¡®ç‡çš„å½±å“ç›¸å¯¹è¾ƒå°ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ•´åˆè¯­éŸ³ä¸è§†è§‰ä¿¡æ¯çš„å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„æ–¹æ³•è·¯å¾„å’Œæ•°æ®æ”¯æ’‘ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted for Interspeech 2025 (with additional results)",
      "pdf_url": "https://arxiv.org/pdf/2505.23308v1",
      "published_date": "2025-05-29 10:06:48 UTC",
      "updated_date": "2025-05-29 10:06:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:05:07.310839+00:00"
    },
    {
      "arxiv_id": "2505.23866v1",
      "title": "Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization",
      "title_zh": "æ·±å…¥æ¢ç©¶é”åº¦æ„ŸçŸ¥æœ€å°åŒ–çš„æ ¡å‡†æ”¶ç›Š",
      "authors": [
        "Chengli Tan",
        "Yubo Zhou",
        "Haishan Ye",
        "Guang Dai",
        "Junmin Liu",
        "Zengjie Song",
        "Jiangshe Zhang",
        "Zixiang Zhao",
        "Yunda Hao",
        "Yong Xu"
      ],
      "abstract": "Deep neural networks have been increasingly used in safety-critical applications such as medical diagnosis and autonomous driving. However, many studies suggest that they are prone to being poorly calibrated and have a propensity for overconfidence, which may have disastrous consequences. In this paper, unlike standard training such as stochastic gradient descent, we show that the recently proposed sharpness-aware minimization (SAM) counteracts this tendency towards overconfidence. The theoretical analysis suggests that SAM allows us to learn models that are already well-calibrated by implicitly maximizing the entropy of the predictive distribution. Inspired by this finding, we further propose a variant of SAM, coined as CSAM, to ameliorate model calibration. Extensive experiments on various datasets, including ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error. Meanwhile, CSAM performs even better than SAM and consistently achieves lower calibration error than other approaches",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å®‰å…¨å…³é”®å‹åº”ç”¨ä¸­å®¹æ˜“å‡ºç°æ ¡å‡†ä¸è‰¯å’Œè¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ï¼Œå¹¶æ­ç¤ºäº†é”åº¦æ„ŸçŸ¥æœ€å°åŒ–ï¼ˆSharpness-Aware Minimization, SAMï¼‰åœ¨æ”¹å–„æ ¡å‡†æ–¹é¢çš„ä¼˜åŠ¿ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒSAM é€šè¿‡éšå¼æœ€å¤§åŒ–é¢„æµ‹åˆ†å¸ƒçš„ç†µï¼Œä½¿å¾—è®­ç»ƒå‡ºçš„æ¨¡å‹å…·å¤‡æ›´å¼ºçš„æ ¡å‡†æ€§ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œä½œè€…è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ”¹è¿›å˜ä½“ CSAMï¼Œæ—¨åœ¨ä¸“é—¨å¢å¼ºæ¨¡å‹çš„æ ¡å‡†èƒ½åŠ›ã€‚åœ¨ ImageNet-1K ç­‰å¤šç§æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒSAM èƒ½å¤Ÿæ˜¾è‘—é™ä½æ ¡å‡†è¯¯å·®ã€‚å®éªŒç»“æœåŒæ—¶æ˜¾ç¤ºï¼ŒCSAM çš„è¡¨ç°ä¼˜äºåŸå§‹ SAMï¼Œä¸”åœ¨å„é¡¹å¯¹æ¯”ä¸­æŒç»­ä¿æŒç€æ¯”å…¶ä»–ç°æœ‰æ–¹æ³•æ›´ä½çš„æ ¡å‡†è¯¯å·®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.23866v1",
      "published_date": "2025-05-29 09:55:29 UTC",
      "updated_date": "2025-05-29 09:55:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:07:00.505809+00:00"
    },
    {
      "arxiv_id": "2505.23295v1",
      "title": "How Does Response Length Affect Long-Form Factuality",
      "title_zh": "å›ç­”é•¿åº¦å¦‚ä½•å½±å“é•¿ç¯‡äº‹å®æ€§",
      "authors": [
        "James Xu Zhao",
        "Jimmy Z. J. Liu",
        "Bryan Hooi",
        "See-Kiong Ng"
      ],
      "abstract": "Large language models (LLMs) are widely used for long-form text generation. However, factual errors in the responses would undermine their reliability. Despite growing attention to LLM factuality, the effect of response length on factuality remains underexplored. In this work, we systematically investigate this relationship by first introducing an automatic and bi-level long-form factuality evaluation framework, which achieves high agreement with human annotations while being cost-effective. Using this framework, we conduct controlled experiments and find that longer responses exhibit lower factual precision, confirming the presence of length bias. To explain this phenomenon, we empirically examine three hypotheses: error propagation, long context, and facts exhaustion. Our results reveal that facts exhaustion, where the model gradually exhausts more reliable knowledge, is the primary cause of factual degradation, rather than the other two hypotheses.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ—¶å“åº”é•¿åº¦å¯¹äº‹å®å‡†ç¡®æ€§(Factuality)çš„å½±å“ã€‚ä½œè€…é¦–å…ˆæå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åŒå±‚é•¿æ–‡æœ¬äº‹å®æ€§è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒé«˜æˆæœ¬æ•ˆç›Šçš„åŒæ—¶ä¸äººå·¥æ ‡æ³¨å…·æœ‰é«˜åº¦ä¸€è‡´æ€§ã€‚é€šè¿‡å—æ§å®éªŒï¼Œç ”ç©¶å‘ç°è¾ƒé•¿çš„å“åº”é€šå¸¸è¡¨ç°å‡ºè¾ƒä½çš„äº‹å®ç²¾ç¡®åº¦ï¼Œä»è€Œè¯å®äº†é•¿åº¦åè§(Length Bias)çš„å­˜åœ¨ã€‚ä¸ºäº†è§£é‡Šè¿™ä¸€ç°è±¡ï¼Œè¯¥æ–‡å¯¹é”™è¯¯ä¼ æ’­(Error Propagation)ã€é•¿ä¸Šä¸‹æ–‡(Long Context)å’Œäº‹å®æ¯ç«­(Facts Exhaustion)ä¸‰ä¸ªå‡è®¾è¿›è¡Œäº†å®è¯æ£€éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œäº‹å®æ¯ç«­ï¼ˆå³æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ¸è€—å°½äº†å¯é çŸ¥è¯†ï¼‰æ˜¯å¯¼è‡´äº‹å®æ€§ä¸‹é™çš„ä¸»è¦åŸå› ï¼Œè€Œéå¦å¤–ä¸¤ä¸ªå‡è®¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Findings. 24 pages, 10 figures, 18 tables. Code available at https://github.com/XuZhao0/length-bias-factuality",
      "pdf_url": "https://arxiv.org/pdf/2505.23295v1",
      "published_date": "2025-05-29 09:47:56 UTC",
      "updated_date": "2025-05-29 09:47:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:05:33.992419+00:00"
    },
    {
      "arxiv_id": "2505.23292v2",
      "title": "Federated Unsupervised Semantic Segmentation",
      "title_zh": "è”é‚¦æ— ç›‘ç£è¯­ä¹‰åˆ†å‰²",
      "authors": [
        "Evangelos Charalampakis",
        "Vasileios Mygdalis",
        "Ioannis Pitas"
      ],
      "abstract": "This work explores the application of Federated Learning (FL) to Unsupervised Semantic image Segmentation (USS). Recent USS methods extract pixel-level features using frozen visual foundation models and refine them through self-supervised objectives that encourage semantic grouping. These features are then grouped to semantic clusters to produce segmentation masks. Extending these ideas to federated settings requires feature representation and cluster centroid alignment across distributed clients, an inherently difficult task under heterogeneous data distributions in the absence of supervision. To address this, we propose FUSS (Federated Unsupervised image Semantic Segmentation) which is, to our knowledge, the first framework to enable fully decentralized, label-free semantic segmentation training. FUSS introduces novel federation strategies that promote global consistency in feature and prototype space, jointly optimizing local segmentation heads and shared semantic centroids. Experiments on both benchmark and real-world datasets, including binary and multi-class segmentation tasks, show that FUSS consistently outperforms local-only client trainings as well as extensions of classical FL algorithms under varying client data distributions. To fully support reproducibility, the source code, data partitioning scripts, and implementation details are publicly available at: https://github.com/evanchar/FUSS",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†è”é‚¦å­¦ä¹ (Federated Learning, FL)åœ¨æ— ç›‘ç£è¯­ä¹‰å›¾åƒåˆ†å‰²(Unsupervised Semantic image Segmentation, USS)é¢†åŸŸçš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³å¼‚æ„æ•°æ®åˆ†å¸ƒä¸‹ä¸”ç¼ºä¹ç›‘ç£ä¿¡å·æ—¶ï¼Œåˆ†å¸ƒå¼å®¢æˆ·ç«¯ä¹‹é—´ç‰¹å¾è¡¨ç¤ºä¸èšç±»ä¸­å¿ƒå¯¹é½çš„éš¾é¢˜ã€‚ç°æœ‰çš„æ— ç›‘ç£åˆ†å‰²æ–¹æ³•é€šå¸¸åˆ©ç”¨å†»ç»“çš„è§†è§‰åŸºç¡€æ¨¡å‹(foundation models)æå–åƒç´ çº§ç‰¹å¾ï¼Œå¹¶é€šè¿‡è‡ªç›‘ç£ç›®æ ‡å°†å…¶èšç±»ä»¥ç”Ÿæˆåˆ†å‰²æ©ç ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†FUSSæ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªæ”¯æŒå®Œå…¨å»ä¸­å¿ƒåŒ–ã€æ— æ ‡ç­¾è¯­ä¹‰åˆ†å‰²è®­ç»ƒçš„è”é‚¦å­¦ä¹ æ–¹æ¡ˆã€‚FUSSå¼•å…¥äº†åˆ›æ–°çš„è”é‚¦ç­–ç•¥ï¼Œé€šè¿‡åœ¨ç‰¹å¾å’ŒåŸå‹ç©ºé—´å†…ä¿ƒè¿›å…¨å±€ä¸€è‡´æ€§ï¼Œå…±åŒä¼˜åŒ–æœ¬åœ°åˆ†å‰²å¤´(segmentation heads)å’Œå…±äº«è¯­ä¹‰è´¨å¿ƒã€‚åœ¨åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFUSSåœ¨äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»åˆ†å‰²ä»»åŠ¡ä¸­å‡ä¸€è‡´ä¼˜äºä»…æœ¬åœ°å®¢æˆ·ç«¯è®­ç»ƒä»¥åŠä¼ ç»Ÿè”é‚¦å­¦ä¹ ç®—æ³•çš„æ‰©å±•ç‰ˆæœ¬ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è·¨å®¢æˆ·ç«¯åä½œå¯ä»¥æœ‰æ•ˆæå‡æ— ç›‘ç£è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication in Neurocomputing",
      "pdf_url": "https://arxiv.org/pdf/2505.23292v2",
      "published_date": "2025-05-29 09:43:55 UTC",
      "updated_date": "2026-01-20 12:09:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:05:37.712646+00:00"
    },
    {
      "arxiv_id": "2505.23865v1",
      "title": "Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration",
      "title_zh": "ç»“åˆä¿¡æ¯å¢ç›Šä¼°è®¡æ·±åº¦æ¶æ„ä¸å¼ºåŒ–å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“å†œç”°æ¢ç´¢",
      "authors": [
        "Emanuele Masiero",
        "Vito Trianni",
        "Giuseppe Vizzari",
        "Dimitri Ognibene"
      ],
      "abstract": "Precision agriculture requires efficient autonomous systems for crop monitoring, where agents must explore large-scale environments while minimizing resource consumption. This work addresses the problem as an active exploration task in a grid environment representing an agricultural field. Each cell may contain targets (e.g., damaged crops) observable from nine predefined points of view (POVs). Agents must infer the number of targets per cell using partial, sequential observations.\n  We propose a two-stage deep learning framework. A pre-trained LSTM serves as a belief model, updating a probabilistic map of the environment and its associated entropy, which defines the expected information gain (IG). This allows agents to prioritize informative regions. A key contribution is the inclusion of a POV visibility mask in the input, preserving the Markov property under partial observability and avoiding revisits to already explored views.\n  Three agent architectures were compared: an untrained IG-based agent selecting actions to maximize entropy reduction; a DQN agent using CNNs over local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent with wider spatial context. Simulations on 20x20 maps showed that the untrained agent performs well despite its simplicity. The DQN agent matches this performance when the POV mask is included, while the Double-CNN agent consistently achieves superior exploration efficiency, especially in larger environments.\n  Results show that uncertainty-aware policies leveraging entropy, belief states, and visibility tracking lead to robust and scalable exploration. Future work includes curriculum learning, multi-agent cooperation with shared rewards, transformer-based models, and intrinsic motivation mechanisms to further enhance learning efficiency and policy generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Precision agricultureä¸­çš„ä½œç‰©ç›‘æµ‹éœ€æ±‚ï¼Œå°†å¤šæ™ºèƒ½ä½“åœ¨èµ„æºå—é™ä¸‹çš„ç¯å¢ƒæ¢æµ‹å»ºæ¨¡ä¸ºactive explorationä»»åŠ¡ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„LSTMä½œä¸ºbelief modelï¼Œé€šè¿‡æ›´æ–°ç¯å¢ƒæ¦‚ç‡å›¾åŠå…³è”ç†µæ¥å®šä¹‰æœŸæœ›Information Gain (IG)ï¼Œä»è€Œå¼•å¯¼æ™ºèƒ½ä½“ä¼˜å…ˆè®¿é—®ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼•å…¥äº†POV visibility maskï¼Œåœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹ä¿æŒäº†Markov propertyï¼Œæœ‰æ•ˆé¿å…äº†å¯¹å·²æ¢ç´¢è§†è§’çš„é‡å¤è®¿é—®ã€‚å®éªŒå¯¹æ¯”äº†åŸºäºIGçš„æœªè®­ç»ƒæ™ºèƒ½ä½“ã€å±€éƒ¨æ„ŸçŸ¥çš„DQNæ™ºèƒ½ä½“ä»¥åŠå…·å¤‡æ›´å¹¿ç©ºé—´ä¸Šä¸‹æ–‡çš„Double-CNN DQNæ™ºèƒ½ä½“ï¼Œç»“æœæ˜¾ç¤ºDouble-CNN DQNåœ¨æ¢ç´¢æ•ˆç‡ä¸Šæ˜¾è‘—å ä¼˜ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡ç¯å¢ƒä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ä¸å¯æ‰©å±•æ€§ã€‚è¯¥æˆæœè¯æ˜äº†ç»“åˆentropyã€belief statesåŠå¯è§æ€§è·Ÿè¸ªçš„å†³ç­–ç­–ç•¥èƒ½å®ç°é«˜æ•ˆçš„ç¯å¢ƒè¦†ç›–ï¼Œä¸ºæœªæ¥å¤šæ™ºèƒ½ä½“åä½œ(multi-agent cooperation)å’Œå†…åœ¨åŠ¨æœºæœºåˆ¶çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, presented at RLDM 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23865v1",
      "published_date": "2025-05-29 09:39:38 UTC",
      "updated_date": "2025-05-29 09:39:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:05:48.835240+00:00"
    },
    {
      "arxiv_id": "2505.23281v3",
      "title": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions",
      "title_zh": "MathArenaï¼šåŸºäºæ— æ±¡æŸ“æ•°å­¦ç«èµ›çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Mislav BalunoviÄ‡",
        "Jasper Dekoninck",
        "Ivo Petrov",
        "Nikola JovanoviÄ‡",
        "Martin Vechev"
      ],
      "abstract": "The rapid advancement of reasoning capabilities in large language models (LLMs) has led to notable improvements on mathematical benchmarks. However, many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely available online, making it difficult to disentangle genuine reasoning from potential memorization. Furthermore, these benchmarks do not evaluate proof-writing capabilities, which are crucial for many mathematical tasks. To address this, we introduce MathArena, a new benchmark based on the following key insight: recurring math competitions provide a stream of high-quality, challenging problems that can be used for real-time evaluation of LLMs. By evaluating models as soon as new problems are released, we effectively eliminate the risk of contamination. Using this framework, we find strong signs of contamination in AIME 2024. Nonetheless, evaluations on harder competitions, such as CMIMC 2025, demonstrate impressive reasoning capabilities in top-performing models. MathArena is also the first benchmark for proof-writing capabilities. On IMO 2025, top models achieve slightly less than 40%, demonstrating both notable progress and significant room for improvement. So far, we have evaluated over $50$ models across seven competitions, totaling $162$ problems. As an evolving benchmark, MathArena will continue to track the progress of LLMs on newly released competitions, ensuring rigorous and up-to-date evaluation of mathematical reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MathArenaï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡å®æ—¶æ›´æ–°çš„æ•°å­¦ç«èµ›é¢˜ç›®æ¥è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çœŸå®æ¨ç†èƒ½åŠ›çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚è¯¥æ¡†æ¶ä¸»è¦é’ˆå¯¹ç°æœ‰è¯„æµ‹é›†ï¼ˆå¦‚ AIME 2024ï¼‰ä¸­æ™®éå­˜åœ¨çš„æ•°æ®æ±¡æŸ“ï¼ˆcontaminationï¼‰é£é™©ï¼Œä»¥åŠç¼ºä¹å¯¹è¯æ˜å†™ä½œï¼ˆproof-writingï¼‰èƒ½åŠ›è¯„ä¼°çš„é—®é¢˜è€Œè®¾è®¡ã€‚MathArena çš„æ ¸å¿ƒæ€è·¯æ˜¯åœ¨ç«èµ›é¢˜ç›®å‘å¸ƒåç«‹å³å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œä»è€Œç¡®ä¿èƒ½å¤ŸåŒºåˆ†å‡ºçœŸå®çš„æ¨ç†ä¸æ½œåœ¨çš„è®°å¿†ï¼ˆmemorizationï¼‰ã€‚é€šè¿‡å¯¹ 7 åœºç«èµ›ä¸­ 162 ä¸ªé—®é¢˜çš„æµ‹è¯•ï¼Œç ”ç©¶å‘ç° AIME 2024 å­˜åœ¨æ˜æ˜¾çš„æ±¡æŸ“è¿¹è±¡ï¼Œè€Œåœ¨æ›´éš¾çš„ CMIMC 2025 ä¸­é¡¶çº§æ¨¡å‹ä»å±•ç°äº†å¼ºåŠ²æ€§èƒ½ã€‚ä½œä¸ºé¦–ä¸ªå¼•å…¥è¯æ˜å†™ä½œè¯„ä¼°çš„é¡¹ç›®ï¼Œåœ¨ IMO 2025 çš„æµ‹è¯•ä¸­é¡¶çº§æ¨¡å‹å‡†ç¡®ç‡æ¥è¿‘ 40%ï¼Œè¿™æ—¢ä½“ç°äº† LLMs çš„æ˜¾è‘—è¿›æ­¥ï¼Œä¹Ÿæ­ç¤ºäº†å…¶åœ¨å¤æ‚æ•°å­¦ä»»åŠ¡ä¸Šçš„å·¨å¤§æå‡ç©ºé—´ã€‚MathArena å°†ä½œä¸ºä¸€ä¸ªæŒç»­æ¼”è¿›çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œç¡®ä¿å¯¹æ•°å­¦æ¨ç†èƒ½åŠ›è¿›è¡Œä¸¥è°¨ä¸”åŠæ—¶çš„åŠ¨æ€è¯„ä¼°ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23281v3",
      "published_date": "2025-05-29 09:28:06 UTC",
      "updated_date": "2026-01-14 21:39:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:05:51.852151+00:00"
    },
    {
      "arxiv_id": "2505.23277v1",
      "title": "Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective",
      "title_zh": "Sentinelï¼šåŸºäºç†è§£è§†è§’çš„å¤§è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡å‹ç¼©ä»£ç†æ¨¡å‹æ³¨æ„åŠ›æ¢æµ‹",
      "authors": [
        "Yong Zhang",
        "Yanwen Huang",
        "Ning Cheng",
        "Yang Guo",
        "Yun Zhu",
        "Yanmeng Wang",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external context, but retrieved passages are often lengthy, noisy, or exceed input limits. Existing compression methods typically require supervised training of dedicated compression models, increasing cost and reducing portability. We propose Sentinel, a lightweight sentence-level compression framework that reframes context filtering as an attention-based understanding task. Rather than training a compression model, Sentinel probes decoder attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier to identify sentence relevance. Empirically, we find that query-context relevance estimation is consistent across model scales, with 0.5B proxies closely matching the behaviors of larger models. On the LongBench benchmark, Sentinel achieves up to 5$\\times$ compression while matching the QA performance of 7B-scale compression systems. Our results suggest that probing native attention signals enables fast, effective, and question-aware context compression. Code available at: https://github.com/yzhangchuck/Sentinel.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Sentinelï¼Œä¸€ä¸ªè½»é‡çº§çš„å¥å­çº§ä¸Šä¸‹æ–‡å‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ä¸­æ£€ç´¢å†…å®¹è¿‡é•¿ä¸”åŒ…å«å™ªå£°çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†ä¸Šä¸‹æ–‡è¿‡æ»¤é‡æ–°æ„æ€ä¸ºåŸºäºæ³¨æ„åŠ›çš„ç†è§£ä»»åŠ¡ï¼Œé€šè¿‡è½»é‡çº§åˆ†ç±»å™¨æ¢æµ‹ç°æˆçš„ 0.5B ä»£ç†å¤§è¯­è¨€æ¨¡å‹ (proxy LLM) çš„è§£ç å™¨æ³¨æ„åŠ›ä¿¡å·ï¼Œä»è€Œè¯†åˆ«å¥å­çš„ç›¸å…³æ€§ã€‚å®éªŒå‘ç°ï¼ŒæŸ¥è¯¢ä¸ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§è¯„ä¼°åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡é—´å…·æœ‰ä¸€è‡´æ€§ï¼Œ0.5B çš„ä»£ç†æ¨¡å‹èƒ½å¤Ÿé«˜åº¦åŒ¹é…å¤§å‹æ¨¡å‹çš„è¡Œä¸ºï¼Œä¸”æ— éœ€é¢å¤–çš„ç›‘ç£è®­ç»ƒã€‚åœ¨ LongBench åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSentinel å®ç°äº†é«˜è¾¾ 5 å€çš„å‹ç¼©ç‡ï¼Œå…¶é—®ç­”æ€§èƒ½è¶³ä»¥åª²ç¾ 7B è§„æ¨¡çš„å‹ç¼©ç³»ç»Ÿã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨åŸç”Ÿæ³¨æ„åŠ›ä¿¡å·æ¢æµ‹å¯å®ç°å¿«é€Ÿã€æœ‰æ•ˆä¸”æ„ŸçŸ¥é—®é¢˜çš„ä¸Šä¸‹æ–‡å‹ç¼©ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„ä¾¿æºæ€§ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint. 17 pages including appendix",
      "pdf_url": "https://arxiv.org/pdf/2505.23277v1",
      "published_date": "2025-05-29 09:24:12 UTC",
      "updated_date": "2025-05-29 09:24:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:05:50.972973+00:00"
    },
    {
      "arxiv_id": "2505.23276v2",
      "title": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text",
      "title_zh": "é˜¿æ‹‰ä¼¯è¯­ AI æŒ‡çº¹ï¼šå¤§è¯­è¨€æ¨¡å‹æ–‡æœ¬çš„æ–‡ä½“ç‰¹å¾åˆ†æä¸æ£€æµ‹",
      "authors": [
        "Maged S. Al-Shaibani",
        "Moataz Ahmed"
      ],
      "abstract": "Large Language Models (LLMs) have achieved unprecedented capabilities in generating human-like text, posing subtle yet significant challenges for information integrity across critical domains, including education, social media, and academia, enabling sophisticated misinformation campaigns, compromising healthcare guidance, and facilitating targeted propaganda. This challenge becomes severe, particularly in under-explored and low-resource languages like Arabic. This paper presents a comprehensive investigation of Arabic machine-generated text, examining multiple generation strategies (generation from the title only, content-aware generation, and text refinement) across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic, and social media domains. Our stylometric analysis reveals distinctive linguistic patterns differentiating human-written from machine-generated Arabic text across these varied contexts. Despite their human-like qualities, we demonstrate that LLMs produce detectable signatures in their Arabic outputs, with domain-specific characteristics that vary significantly between different contexts. Based on these insights, we developed BERT-based detection models that achieved exceptional performance in formal contexts (up to 99.9\\% F1-score) with strong precision across model architectures. Our cross-domain analysis confirms generalization challenges previously reported in the literature. To the best of our knowledge, this work represents the most comprehensive investigation of Arabic machine-generated text to date, uniquely combining multiple prompt generation methods, diverse model architectures, and in-depth stylometric analysis across varied textual domains, establishing a foundation for developing robust, linguistically-informed detection systems essential for preserving information integrity in Arabic-language contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€åœ¨ Large Language Models (LLMs) æ–‡æœ¬ç”Ÿæˆä¸­é¢ä¸´çš„ä¿¡æ¯å®Œæ•´æ€§æŒ‘æˆ˜ï¼Œå¯¹å¤šç§æ¨¡å‹æ¶æ„ï¼ˆåŒ…æ‹¬ ALLaMã€Jaisã€Llama å’Œ GPT-4ï¼‰ç”Ÿæˆçš„é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬è¿›è¡Œäº†å…¨é¢çš„æ–‡ä½“ç‰¹å¾åˆ†æ (Stylometric analysis)ã€‚ç ”ç©¶æ¢è®¨äº†ä»æ ‡é¢˜ç”Ÿæˆã€å†…å®¹æ„ŸçŸ¥ç”Ÿæˆåˆ°æ–‡æœ¬ä¼˜åŒ–ç­‰å¤šç§ç”Ÿæˆç­–ç•¥ï¼Œæ¶µç›–äº†å­¦æœ¯å’Œç¤¾äº¤åª’ä½“ç­‰ä¸åŒé¢†åŸŸã€‚æ–‡ä½“åˆ†ææ­ç¤ºäº†äººç±»æ’°å†™ä¸æœºå™¨ç”Ÿæˆçš„é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬ä¹‹é—´å­˜åœ¨ç‹¬ç‰¹çš„è¯­è¨€æ¨¡å¼å·®å¼‚ï¼Œè¯æ˜äº† LLMs åœ¨é˜¿æ‹‰ä¼¯è¯­è¾“å‡ºä¸­ä¼šç•™ä¸‹å¯æ£€æµ‹çš„ç‰¹å¾ä¿¡å·ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åŸºäº BERT çš„æ£€æµ‹æ¨¡å‹ï¼Œåœ¨æ­£å¼è¯­å¢ƒä¸‹å–å¾—äº†é«˜è¾¾ 99.9% çš„ F1-scoreï¼Œå±•ç°å‡ºè·¨æ¨¡å‹æ¶æ„çš„æé«˜æ£€æµ‹ç²¾åº¦ã€‚ä½œä¸ºç›®å‰å¯¹é˜¿æ‹‰ä¼¯è¯­æœºå™¨ç”Ÿæˆæ–‡æœ¬æœ€å…¨é¢çš„è°ƒæŸ¥ï¼Œè¯¥å·¥ä½œä¸ºæ„å»ºé²æ£’çš„ã€å…·æœ‰è¯­è¨€å­¦çŸ¥è¯†çš„æ£€æµ‹ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œå¯¹äºç»´æŠ¤é˜¿æ‹‰ä¼¯è¯­ç¯å¢ƒä¸‹çš„ä¿¡æ¯çœŸå®æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23276v2",
      "published_date": "2025-05-29 09:24:00 UTC",
      "updated_date": "2025-06-04 15:16:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:05:50.250554+00:00"
    },
    {
      "arxiv_id": "2505.23270v2",
      "title": "Does Machine Unlearning Truly Remove Knowledge?",
      "title_zh": "æœºå™¨é—å¿˜æ˜¯å¦çœŸæ­£ç§»é™¤äº†çŸ¥è¯†ï¼Ÿ",
      "authors": [
        "Haokun Chen",
        "Yueqi Zhang",
        "Yuan Bi",
        "Yao Zhang",
        "Tong Liu",
        "Jinhe Bi",
        "Jian Lan",
        "Jindong Gu",
        "Claudia Grosser",
        "Denis Krompass",
        "Nassir Navab",
        "Volker Tresp"
      ],
      "abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable advancements, drawing significant attention from the research community. Their capabilities are largely attributed to large-scale architectures, which require extensive training on massive datasets. However, such datasets often contain sensitive or copyrighted content sourced from the public internet, raising concerns about data privacy and ownership. Regulatory frameworks, such as the General Data Protection Regulation (GDPR), grant individuals the right to request the removal of such sensitive information. This has motivated the development of machine unlearning algorithms that aim to remove specific knowledge from models without the need for costly retraining. Despite these advancements, evaluating the efficacy of unlearning algorithms remains a challenge due to the inherent complexity and generative nature of LLMs. In this work, we introduce a comprehensive auditing framework for unlearning evaluation, comprising three benchmark datasets, six unlearning algorithms, and five prompt-based auditing methods. By using various auditing algorithms, we evaluate the effectiveness and robustness of different unlearning strategies. To explore alternatives beyond prompt-based auditing, we propose a novel technique that leverages intermediate activation perturbations, addressing the limitations of auditing methods that rely solely on model inputs and outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„æœºå™¨é—å¿˜(Machine Unlearning)æŠ€æœ¯ï¼Œæ—¨åœ¨åº”å¯¹éšç§ä¿æŠ¤å’ŒGDPRåˆè§„æ€§è¦æ±‚ï¼Œå®ç°åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä»æ¨¡å‹ä¸­ç§»é™¤æ•æ„Ÿæˆ–ç‰ˆæƒä¿¡æ¯ã€‚é’ˆå¯¹ç°æœ‰é—å¿˜ç®—æ³•è¯„ä¼°æ•ˆæœéš¾ä»¥é‡åŒ–çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„å®¡è®¡æ¡†æ¶ï¼Œæ¶µç›–äº†ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ã€å…­ç§é—å¿˜ç®—æ³•ä»¥åŠäº”ç§åŸºäºæç¤º(Prompt-based)çš„å®¡è®¡æ–¹æ³•ã€‚é€šè¿‡æ‰§è¡Œå¤šæ ·åŒ–çš„å®¡è®¡æµç¨‹ï¼Œç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†ä¸åŒé—å¿˜ç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœä»…ä¾èµ–æ¨¡å‹è¾“å…¥è¾“å‡ºè¿›è¡Œå®¡è®¡çš„å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åˆ©ç”¨ä¸­é—´å±‚æ¿€æ´»æ‰°åŠ¨(Intermediate Activation Perturbations)çš„æ–°æŠ€æœ¯ã€‚è¯¥å·¥ä½œä¸ºéªŒè¯æœºå™¨é—å¿˜ç®—æ³•æ˜¯å¦çœŸæ­£ä»LLMsä¸­ç§»é™¤äº†ç‰¹å®šçŸ¥è¯†æä¾›äº†æ·±å…¥çš„æ´å¯Ÿå’Œç³»ç»Ÿçš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 (Lock-LLM Workshop)",
      "pdf_url": "https://arxiv.org/pdf/2505.23270v2",
      "published_date": "2025-05-29 09:19:07 UTC",
      "updated_date": "2025-10-11 14:41:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:07:04.439559+00:00"
    },
    {
      "arxiv_id": "2505.23864v2",
      "title": "Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections",
      "title_zh": "åŸºäºå¯å¾®è¾…åŠ©æŠ•å½±çš„ä¸ªæ€§åŒ–å­å›¾è”é‚¦å­¦ä¹ ",
      "authors": [
        "Wei Zhuo",
        "Zhaohuan Zhan",
        "Han Yu"
      ],
      "abstract": "Federated Learning (FL) on graph-structured data typically faces non-IID challenges, particularly in scenarios where each client holds a distinct subgraph sampled from a global graph. In this paper, we introduce Federated learning with Auxiliary projections (FedAux), a personalized subgraph FL framework that learns to align, compare, and aggregate heterogeneously distributed local models without sharing raw data or node embeddings. In FedAux, each client jointly trains (i) a local GNN and (ii) a learnable auxiliary projection vector (APV) that differentiably projects node embeddings onto a 1D space. A soft-sorting operation followed by a lightweight 1D convolution refines these embeddings in the ordered space, enabling the APV to effectively capture client-specific information. After local training, these APVs serve as compact signatures that the server uses to compute inter-client similarities and perform similarity-weighted parameter mixing, yielding personalized models while preserving cross-client knowledge transfer. Moreover, we provide rigorous theoretical analysis to establish the convergence and rationality of our design. Empirical evaluations across diverse graph benchmarks demonstrate that FedAux substantially outperforms existing baselines in both accuracy and personalization performance. The code is available at https://github.com/JhuoW/FedAux.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç»“æ„æ•°æ®è”é‚¦å­¦ä¹ ï¼ˆFederated Learning, FLï¼‰ä¸­ç”±äºå­å›¾åˆ†å¸ƒä¸ä¸€å¯¼è‡´çš„éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºFedAuxçš„ä¸ªæ€§åŒ–å­å›¾è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„å¯å¾®è¾…åŠ©æŠ•å½±å‘é‡ï¼ˆAuxiliary Projection Vector, APVï¼‰ï¼Œåœ¨ä¸å…±äº«åŸå§‹æ•°æ®æˆ–èŠ‚ç‚¹åµŒå…¥çš„å‰æä¸‹å®ç°å¼‚æ„æœ¬åœ°æ¨¡å‹çš„å¯¹é½ä¸èšåˆã€‚æ¯ä¸ªå®¢æˆ·ç«¯åœ¨è®­ç»ƒæœ¬åœ°å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„åŒæ—¶åˆ©ç”¨APVå°†èŠ‚ç‚¹åµŒå…¥æ˜ å°„è‡³ä¸€ç»´ç©ºé—´ï¼Œå¹¶é€šè¿‡è½¯æ’åºï¼ˆsoft-sortingï¼‰å’Œä¸€ç»´å·ç§¯æå–å®¢æˆ·ç«¯ç‰¹æœ‰ä¿¡æ¯ã€‚è¿™äº›APVéšåä½œä¸ºç´§å‡‘çš„ç‰¹å¾æŒ‡çº¹ä¾›æœåŠ¡å™¨è®¡ç®—å®¢æˆ·ç«¯é—´çš„ç›¸ä¼¼åº¦ï¼Œå¹¶æ®æ­¤æ‰§è¡Œç›¸ä¼¼åº¦åŠ æƒçš„å‚æ•°æ··åˆï¼Œä»è€Œåœ¨ä¿ç•™è·¨å®¢æˆ·ç«¯çŸ¥è¯†è¿ç§»çš„åŒæ—¶ç”Ÿæˆä¸ªæ€§åŒ–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é€šè¿‡ä¸¥è°¨çš„ç†è®ºåˆ†æè¯æ˜äº†è¯¥ç®—æ³•çš„æ”¶æ•›æ€§ä¸è®¾è®¡åˆç†æ€§ã€‚åœ¨å¤šä¸ªå›¾åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFedAuxåœ¨å‡†ç¡®ç‡å’Œä¸ªæ€§åŒ–æ€§èƒ½è¡¨ç°ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23864v2",
      "published_date": "2025-05-29 09:17:49 UTC",
      "updated_date": "2025-09-30 09:18:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:06:17.209001+00:00"
    },
    {
      "arxiv_id": "2505.23268v1",
      "title": "Unsupervised Transcript-assisted Video Summarization and Highlight Detection",
      "title_zh": "æ— ç›‘ç£çš„è½¬å½•æ–‡æœ¬è¾…åŠ©è§†é¢‘æ‘˜è¦ä¸é«˜å…‰æ£€æµ‹",
      "authors": [
        "Spyros Barbakos",
        "Charalampos Antoniadis",
        "Gerasimos Potamianos",
        "Gianluca Setti"
      ],
      "abstract": "Video consumption is a key part of daily life, but watching entire videos can be tedious. To address this, researchers have explored video summarization and highlight detection to identify key video segments. While some works combine video frames and transcripts, and others tackle video summarization and highlight detection using Reinforcement Learning (RL), no existing work, to the best of our knowledge, integrates both modalities within an RL framework. In this paper, we propose a multimodal pipeline that leverages video frames and their corresponding transcripts to generate a more condensed version of the video and detect highlights using a modality fusion mechanism. The pipeline is trained within an RL framework, which rewards the model for generating diverse and representative summaries while ensuring the inclusion of video segments with meaningful transcript content. The unsupervised nature of the training allows for learning from large-scale unannotated datasets, overcoming the challenge posed by the limited size of existing annotated datasets. Our experiments show that using the transcript in video summarization and highlight detection achieves superior results compared to relying solely on the visual content of the video.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§‚çœ‹å®Œæ•´è§†é¢‘è€—æ—¶è¾ƒé•¿çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— ç›‘ç£æ–‡å­—å¹•è¾…åŠ©çš„è§†é¢‘æ‘˜è¦ä¸é«˜å…‰æ£€æµ‹å¤šæ¨¡æ€æµæ°´çº¿ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è§†é¢‘å¸§ä¸å¯¹åº”çš„ transcriptsï¼Œé€šè¿‡æ¨¡æ€èåˆæœºåˆ¶ (modality fusion mechanism) ç”Ÿæˆè§†é¢‘ç¼©ç•¥ç‰ˆå¹¶è¯†åˆ«é«˜å…‰ç‰‡æ®µã€‚ç ”ç©¶å°†è¯¥æµæ°´çº¿ç½®äºå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ä¸‹è®­ç»ƒï¼Œé€šè¿‡å¥–åŠ±æœºåˆ¶é¼“åŠ±æ¨¡å‹ç”Ÿæˆå…·æœ‰å¤šæ ·æ€§ (diverse) å’Œä»£è¡¨æ€§ (representative) çš„æ‘˜è¦ï¼ŒåŒæ—¶ç¡®ä¿åŒ…å«æœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹ã€‚ç”±äºé‡‡ç”¨äº†æ— ç›‘ç£ (unsupervised) çš„è®­ç»ƒæ–¹å¼ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»å¤§è§„æ¨¡æœªæ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ ï¼Œæœ‰æ•ˆè§£å†³äº†æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„éš¾é¢˜ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨è§†é¢‘æ‘˜è¦å’Œé«˜å…‰æ£€æµ‹ä¸­å¼•å…¥ transcripts ä¿¡æ¯ç›¸æ¯”äºä»…ä½¿ç”¨è§†è§‰ç‰¹å¾èƒ½å–å¾—æ›´ä¼˜è¶Šçš„æ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23268v1",
      "published_date": "2025-05-29 09:16:19 UTC",
      "updated_date": "2025-05-29 09:16:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:06:15.046243+00:00"
    },
    {
      "arxiv_id": "2505.23267v1",
      "title": "VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation",
      "title_zh": "VLM-RRTï¼šé¢å‘è‡ªä¸»æ— äººæœºå¯¼èˆªçš„è§†è§‰è¯­è¨€æ¨¡å‹å¼•å¯¼å¼ RRT æœç´¢",
      "authors": [
        "Jianlin Ye",
        "Savvas Papaioannou",
        "Panayiotis Kolios"
      ],
      "abstract": "Path planning is a fundamental capability of autonomous Unmanned Aerial Vehicles (UAVs), enabling them to efficiently navigate toward a target region or explore complex environments while avoiding obstacles. Traditional pathplanning methods, such as Rapidly-exploring Random Trees (RRT), have proven effective but often encounter significant challenges. These include high search space complexity, suboptimal path quality, and slow convergence, issues that are particularly problematic in high-stakes applications like disaster response, where rapid and efficient planning is critical. To address these limitations and enhance path-planning efficiency, we propose Vision Language Model RRT (VLM-RRT), a hybrid approach that integrates the pattern recognition capabilities of Vision Language Models (VLMs) with the path-planning strengths of RRT. By leveraging VLMs to provide initial directional guidance based on environmental snapshots, our method biases sampling toward regions more likely to contain feasible paths, significantly improving sampling efficiency and path quality. Extensive quantitative and qualitative experiments with various state-of-the-art VLMs demonstrate the effectiveness of this proposed approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAV)è‡ªä¸»å¯¼èˆªä¸­çš„è·¯å¾„è§„åˆ’é—®é¢˜ï¼Œæå‡ºäº†VLM-RRTæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸRapidly-exploring Random Trees (RRT)ç®—æ³•åœ¨å¤æ‚ç¯å¢ƒä¸‹æœç´¢ç©ºé—´å¤æ‚åº¦é«˜ã€è·¯å¾„è´¨é‡å·®ä»¥åŠæ”¶æ•›é€Ÿåº¦æ…¢ç­‰å±€é™æ€§ã€‚VLM-RRTæ˜¯ä¸€ç§å°†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„æ¨¡å¼è¯†åˆ«èƒ½åŠ›ä¸RRTçš„è·¯å¾„è§„åˆ’ä¼˜åŠ¿ç›¸ç»“åˆçš„æ··åˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨VLMsæ ¹æ®ç¯å¢ƒå¿«ç…§æä¾›åˆå§‹æ–¹å‘å¼•å¯¼ï¼Œä»è€Œå¼•å¯¼é‡‡æ ·åå‘æ›´å¯èƒ½å­˜åœ¨å¯è¡Œè·¯å¾„çš„åŒºåŸŸï¼Œæ˜¾è‘—æå‡äº†é‡‡æ ·æ•ˆç‡å’Œè·¯å¾„è´¨é‡ã€‚é€šè¿‡å¯¹å¤šç§æœ€å…ˆè¿›çš„VLMsè¿›è¡Œå¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æé«˜è§„åˆ’æ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç¾éš¾å“åº”ç­‰é«˜é£é™©åº”ç”¨åœºæ™¯æä¾›äº†æ›´é«˜æ•ˆçš„è‡ªä¸»å¯¼èˆªè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23267v1",
      "published_date": "2025-05-29 09:15:44 UTC",
      "updated_date": "2025-05-29 09:15:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:07:19.911717+00:00"
    },
    {
      "arxiv_id": "2505.23266v1",
      "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion",
      "title_zh": "é€šè¿‡å¯¹æŠ—æ€§ç‰©ä½“èåˆå¹²æ‰°è§†è§‰è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¯¼èˆªæœåŠ¡",
      "authors": [
        "Chunlong Xie",
        "Jialing He",
        "Shangwei Guo",
        "Jiacheng Wang",
        "Shudong Zhang",
        "Tianwei Zhang",
        "Tao Xiang"
      ],
      "abstract": "We present Adversarial Object Fusion (AdvOF), a novel attack framework targeting vision-and-language navigation (VLN) agents in service-oriented environments by generating adversarial 3D objects. While foundational models like Large Language Models (LLMs) and Vision Language Models (VLMs) have enhanced service-oriented navigation systems through improved perception and decision-making, their integration introduces vulnerabilities in mission-critical service workflows. Existing adversarial attacks fail to address service computing contexts, where reliability and quality-of-service (QoS) are paramount. We utilize AdvOF to investigate and explore the impact of adversarial environments on the VLM-based perception module of VLN agents. In particular, AdvOF first precisely aggregates and aligns the victim object positions in both 2D and 3D space, defining and rendering adversarial objects. Then, we collaboratively optimize the adversarial object with regularization between the adversarial and victim object across physical properties and VLM perceptions. Through assigning importance weights to varying views, the optimization is processed stably and multi-viewedly by iterative fusions from local updates and justifications. Our extensive evaluations demonstrate AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks. This work advances the understanding of service security in VLM-powered navigation systems, providing computational foundations for robust service composition in physical-world deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Adversarial Object Fusion (AdvOF)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹é¢å‘æœåŠ¡ç¯å¢ƒä¸­çš„è§†è§‰è¯­è¨€å¯¼èˆª (Vision-and-Language Navigation, VLN) æ™ºèƒ½ä½“çš„æ–°å‹æ”»å‡»æ¡†æ¶ã€‚å°½ç®¡ Vision Language Models (VLMs) æå‡äº†å¯¼èˆªç³»ç»Ÿçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†ä¹Ÿå¼•å…¥äº†å¯èƒ½å½±å“æœåŠ¡å·¥ä½œæµçš„å®‰å…¨æ¼æ´ï¼Œè€Œç°æœ‰æ”»å‡»æ‰‹æ®µéš¾ä»¥åº”å¯¹æœåŠ¡è®¡ç®—ä¸­å¯¹å¯é æ€§å’ŒæœåŠ¡è´¨é‡ (QoS) çš„ä¸¥è‹›è¦æ±‚ã€‚AdvOF é€šè¿‡åœ¨ 2D å’Œ 3D ç©ºé—´ä¸­ç²¾ç¡®å¯¹é½ç›®æ ‡ä½ç½®æ¥æ¸²æŸ“å¯¹æŠ—æ€§ç‰©ä½“ï¼Œå¹¶åœ¨ç‰©ç†å±æ€§ä¸ VLM æ„ŸçŸ¥ä¹‹é—´åˆ©ç”¨æ­£åˆ™åŒ–è¿›è¡ŒååŒä¼˜åŒ–ã€‚é€šè¿‡ä¸ºä¸åŒè§†è§’åˆ†é…é‡è¦æ€§æƒé‡ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å±€éƒ¨æ›´æ–°ä¸åˆ¤å®šçš„è¿­ä»£èåˆå®ç°äº†ç¨³å®šçš„å¤šè§†è§’ä¼˜åŒ–ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAdvOF èƒ½åœ¨ä¿æŒå¯¹æ­£å¸¸å¯¼èˆªä»»åŠ¡æœ€å°å¹²æ‰°çš„åŒæ—¶ï¼Œæœ‰æ•ˆé™ä½æ™ºèƒ½ä½“åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£ VLM é©±åŠ¨å¯¼èˆªç³»ç»Ÿçš„æœåŠ¡å®‰å…¨æ€§æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶ä¸ºç‰©ç†ä¸–ç•Œéƒ¨ç½²ä¸­çš„ç¨³å¥æœåŠ¡ç»„åˆå¥ å®šäº†è®¡ç®—åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2505.23266v1",
      "published_date": "2025-05-29 09:14:50 UTC",
      "updated_date": "2025-05-29 09:14:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:07:30.407054+00:00"
    },
    {
      "arxiv_id": "2506.00058v1",
      "title": "Prompt Engineer: Analyzing Skill Requirements in the AI Job Market",
      "title_zh": "æç¤ºè¯å·¥ç¨‹å¸ˆï¼šäººå·¥æ™ºèƒ½å°±ä¸šå¸‚åœºä¸­çš„æŠ€èƒ½éœ€æ±‚åˆ†æ",
      "authors": [
        "An Vu",
        "Jonas Oppenlaender"
      ],
      "abstract": "The rise of large language models (LLMs) has created a new job role: the Prompt Engineer. Despite growing interest in this position, we still do not fully understand what skills this new job role requires or how common these jobs are. We analyzed 20,662 job postings on LinkedIn, including 72 prompt engineer positions, to learn more about this emerging role. We found that prompt engineering is still rare (less than 0.5% of sampled job postings) but has a unique skill profile. Prompt engineers need AI knowledge (22.8%), prompt design skills (18.7%), good communication (21.9%), and creative problem-solving (15.8%) skills. These requirements significantly differ from those of established roles, such as data scientists and machine learning engineers, showing that prompt engineering is becoming its own profession. Our findings help job seekers, employers, and educational institutions in better understanding the emerging field of prompt engineering.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ç”±äºå¤§è¯­è¨€æ¨¡å‹ (LLMs) å…´èµ·è€Œäº§ç”Ÿçš„æ–°å…´èŒä½ï¼šæç¤ºå·¥ç¨‹å¸ˆ (Prompt Engineer)ï¼Œæ—¨åœ¨æ˜ç¡®è¯¥è§’è‰²çš„æŠ€èƒ½éœ€æ±‚åŠå…¶åœ¨å°±ä¸šå¸‚åœºä¸­çš„æ™®åŠç¨‹åº¦ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡åˆ†æ LinkedIn ä¸Šçš„ 20,662 ä»½èŒä½å‘å¸ƒï¼ˆå« 72 ä¸ªæç¤ºå·¥ç¨‹å¸ˆå²—ä½ï¼‰å‘ç°ï¼Œè¯¥èŒä½ç›®å‰åœ¨å¸‚åœºä¸­ä»è¾ƒä¸ºç½•è§ï¼Œå æ¯”ä¸è¶³ 0.5%ï¼Œä½†å·²å±•ç°å‡ºç‹¬ç‰¹çš„æŠ€èƒ½ç”»åƒã€‚æç¤ºå·¥ç¨‹å¸ˆ (Prompt Engineer) æ ¸å¿ƒæŠ€èƒ½éœ€æ±‚åŒ…æ‹¬ AI çŸ¥è¯† (AI knowledge, 22.8%)ã€æç¤ºè®¾è®¡ (Prompt design, 18.7%)ã€æ²Ÿé€šèƒ½åŠ› (Communication, 21.9%) ä»¥åŠåˆ›é€ æ€§è§£å†³é—®é¢˜èƒ½åŠ› (Creative problem-solving, 15.8%)ã€‚è¿™äº›éœ€æ±‚ä¸ä¼ ç»Ÿçš„æ•°æ®ç§‘å­¦å®¶ (Data Scientists) å’Œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ (Machine learning engineers) å²—ä½æœ‰æ˜¾è‘—å·®å¼‚ï¼Œè¯æ˜æç¤ºå·¥ç¨‹ (Prompt engineering) æ­£åœ¨æ¼”å˜æˆä¸€ä¸ªç‹¬ç«‹çš„ä¸“ä¸šé¢†åŸŸã€‚è¯¥ç ”ç©¶ç»“æœä¸ºæ±‚èŒè€…ã€é›‡ä¸»åŠæ•™è‚²æœºæ„æ·±å…¥ç†è§£è¿™ä¸€æ–°å…´èŒä¸šåŠå…¶äººæ‰åŸ¹å…»æä¾›äº†é‡è¦çš„å®è¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "42 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.00058v1",
      "published_date": "2025-05-29 09:11:23 UTC",
      "updated_date": "2025-05-29 09:11:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:07:52.004439+00:00"
    },
    {
      "arxiv_id": "2505.23863v2",
      "title": "Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting",
      "title_zh": "èåˆç‰©ç†åŸç†çš„ Mamba å®ç°é•¿æœŸæ··æ²Œç³»ç»Ÿé¢„æµ‹",
      "authors": [
        "Chang Liu",
        "Bohao Zhao",
        "Jingtao Ding",
        "Huandong Wang",
        "Yong Li"
      ],
      "abstract": "Long-term forecasting of chaotic systems remains a fundamental challenge due to the intrinsic sensitivity to initial conditions and the complex geometry of strange attractors. Conventional approaches, such as reservoir computing, typically require training data that incorporates long-term continuous dynamical behavior to comprehensively capture system dynamics. While advanced deep sequence models can capture transient dynamics within the training data, they often struggle to maintain predictive stability and dynamical coherence over extended horizons. Here, we propose PhyxMamba, a framework that integrates a Mamba-based state-space model with physics-informed principles to forecast long-term behavior of chaotic systems given short-term historical observations on their state evolution. We first reconstruct the attractor manifold with time-delay embeddings to extract global dynamical features. After that, we introduce a generative training scheme that enables Mamba to replicate the physical process. It is further augmented by multi-patch prediction and attractor geometry regularization for physical constraints, enhancing predictive accuracy and preserving key statistical properties of systems. Extensive experiments on simulated and real-world chaotic systems demonstrate that PhyxMamba delivers superior forecasting accuracy and faithfully captures essential statistics from short-term historical observations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PhyxMamba æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ··æ²Œç³»ç»Ÿé•¿æœŸé¢„æµ‹ä¸­å› åˆå§‹æ¡ä»¶æ•æ„Ÿæ€§å’Œå¥‡å¼‚å¸å¼•å­(strange attractors)å¤æ‚å‡ ä½•ç»“æ„å¯¼è‡´çš„é¢„æµ‹ä¸ç¨³å®šé—®é¢˜ã€‚è¯¥æ¡†æ¶å°†åŸºäº Mamba çš„çŠ¶æ€ç©ºé—´æ¨¡å‹(state-space model)ä¸ç‰©ç†ä¿¡æ¯åŸç†(physics-informed principles)ç›¸ç»“åˆï¼Œé€šè¿‡æ—¶å»¶åµŒå…¥(time-delay embeddings)é‡æ„å¸å¼•å­æµå½¢ï¼Œä»è€Œæœ‰æ•ˆåœ°ä»çŸ­æœŸå†å²è§‚æµ‹ä¸­æå–å…¨å±€åŠ¨åŠ›å­¦ç‰¹å¾ã€‚PhyxMamba å¼•å…¥äº†ç”Ÿæˆå¼è®­ç»ƒæ–¹æ¡ˆæ¥æ¨¡æ‹Ÿç‰©ç†æ¼”åŒ–è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨å¤šè¡¥ä¸é¢„æµ‹(multi-patch prediction)å’Œå¸å¼•å­å‡ ä½•æ­£åˆ™åŒ–(attractor geometry regularization)æ–½åŠ ç‰©ç†çº¦æŸï¼Œä»¥å¢å¼ºé¢„æµ‹ç²¾åº¦å¹¶ä¿ç•™ç³»ç»Ÿçš„ç»Ÿè®¡ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPhyxMamba åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æ··æ²Œç³»ç»Ÿä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºå“è¶Šçš„é¢„æµ‹æ€§èƒ½ï¼Œèƒ½å¤Ÿå¿ å®åœ°æ•è·ç³»ç»Ÿçš„æ ¸å¿ƒåŠ¨åŠ›å­¦æœ¬è´¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23863v2",
      "published_date": "2025-05-29 08:56:45 UTC",
      "updated_date": "2025-09-26 10:35:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:07:31.203770+00:00"
    },
    {
      "arxiv_id": "2505.23250v2",
      "title": "Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking",
      "title_zh": "Deep Retrieval åœ¨ CheckThat! 2025ï¼šé€šè¿‡æ··åˆæ£€ç´¢ä¸é‡æ’åºè¯†åˆ«ç¤¾äº¤åª’ä½“éšå¼æåŠçš„ç§‘å­¦è®ºæ–‡",
      "authors": [
        "Pascal J. Sager",
        "Ashwini Kamaraj",
        "Benjamin F. Grewe",
        "Thilo Stadelmann"
      ],
      "abstract": "We present the methodology and results of the Deep Retrieval team for subtask 4b of the CLEF CheckThat! 2025 competition, which focuses on retrieving relevant scientific literature for given social media posts. To address this task, we propose a hybrid retrieval pipeline that combines lexical precision, semantic generalization, and deep contextual re-ranking, enabling robust retrieval that bridges the informal-to-formal language gap. Specifically, we combine BM25-based keyword matching with a FAISS vector store using a fine-tuned INF-Retriever-v1 model for dense semantic retrieval. BM25 returns the top 30 candidates, and semantic search yields 100 candidates, which are then merged and re-ranked via a large language model (LLM)-based cross-encoder.\n  Our approach achieves a mean reciprocal rank at 5 (MRR@5) of 76.46% on the development set and 66.43% on the hidden test set, securing the 1st position on the development leaderboard and ranking 3rd on the test leaderboard (out of 31 teams), with a relative performance gap of only 2 percentage points compared to the top-ranked system. We achieve this strong performance by running open-source models locally and without external training data, highlighting the effectiveness of a carefully designed and fine-tuned retrieval pipeline.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†Deep Retrievalå›¢é˜Ÿåœ¨CLEF CheckThat! 2025ç«èµ›å­ä»»åŠ¡4bä¸­çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä»ç¤¾äº¤åª’ä½“å¸–å­ä¸­è¯†åˆ«ç›¸å…³çš„ç§‘å­¦è®ºæ–‡ã€‚å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ··åˆæ£€ç´¢æµæ°´çº¿(hybrid retrieval pipeline)ï¼Œé€šè¿‡ç»“åˆè¯æ±‡ç²¾å‡†åº¦(lexical precision)ä¸è¯­ä¹‰æ³›åŒ–(semantic generalization)æ¥å¼¥åˆéæ­£å¼ç¤¾äº¤åª’ä½“è¯­è¨€ä¸æ­£å¼å­¦æœ¯è¯­è¨€ä¹‹é—´çš„é¸¿æ²Ÿã€‚å…·ä½“å®ç°ä¸Šï¼Œè¯¥æ–¹æ¡ˆæ•´åˆäº†åŸºäºBM25çš„å…³é”®è¯åŒ¹é…ä¸åˆ©ç”¨å¾®è°ƒåçš„INF-Retriever-v1æ¨¡å‹æ„å»ºçš„FAISSå‘é‡å­˜å‚¨ï¼Œä»¥å®ç°ç¨ å¯†è¯­ä¹‰æ£€ç´¢(dense semantic retrieval)ã€‚åˆæ­¥æ£€ç´¢å‡ºçš„å€™é€‰æ–‡çŒ®éšåé€šè¿‡åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„äº¤å‰ç¼–ç å™¨(cross-encoder)è¿›è¡Œæ·±åº¦ä¸Šä¸‹æ–‡é‡æ’åº(re-ranking)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¼€å‘é›†ä¸Šçš„å¹³å‡å€’æ•°æ’å(MRR@5)è¾¾åˆ°76.46%ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°66.43%ï¼Œåœ¨31æ”¯å‚èµ›é˜Ÿä¼ä¸­å–å¾—äº†æµ‹è¯•é›†ç¬¬3åçš„ä¼˜å¼‚æˆç»©ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨ä¸ä¾èµ–å¤–éƒ¨è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æœ¬åœ°è¿è¡Œçš„å¼€æºæ¨¡å‹å’Œç²¾å¿ƒè®¾è®¡çš„æ£€ç´¢æµæ°´çº¿å³å¯å®ç°é«˜æ•ˆçš„å­¦æœ¯æ–‡çŒ®æ£€ç´¢ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23250v2",
      "published_date": "2025-05-29 08:55:39 UTC",
      "updated_date": "2025-07-04 08:06:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:07:31.734353+00:00"
    },
    {
      "arxiv_id": "2505.23247v2",
      "title": "Accelerating RLHF Training with Reward Variance Increase",
      "title_zh": "é€šè¿‡å¢åŠ å¥–åŠ±æ–¹å·®åŠ é€Ÿ RLHF è®­ç»ƒ",
      "authors": [
        "Zonglin Yang",
        "Zhexuan Gu",
        "Houduo Qi",
        "Yancheng Yuan"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) is an essential technique for ensuring that large language models (LLMs) are aligned with human values and preferences during the post-training phase. As an effective RLHF approach, group relative policy optimization (GRPO) has demonstrated success in many LLM-based applications. However, efficient GRPO-based RLHF training remains a challenge. Recent studies reveal that a higher reward variance of the initial policy model leads to faster RLHF training. Inspired by this finding, we propose a practical reward adjustment model to accelerate RLHF training by provably increasing the reward variance and preserving the relative preferences and reward expectation. Our reward adjustment method inherently poses a nonconvex optimization problem, which is NP-hard to solve in general. To overcome the computational challenges, we design a novel $O(n \\log n)$ algorithm to find a global solution of the nonconvex reward adjustment model by explicitly characterizing the extreme points of the feasible set. As an important application, we naturally integrate this reward adjustment model into the GRPO algorithm, leading to a more efficient GRPO with reward variance increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we provide an indirect explanation for the empirical effectiveness of GRPO with rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment results demonstrate that the GRPOVI algorithm can significantly improve the RLHF training efficiency compared to the original GRPO algorithm.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ RLHF é˜¶æ®µè®­ç»ƒæ•ˆç‡è¾ƒä½çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† GRPOVI ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¢åŠ å¥–åŠ±æ–¹å·® (reward variance) æ¥åŠ é€Ÿå¯¹é½è¿‡ç¨‹ã€‚ç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§å®ç”¨çš„ reward adjustment æ¨¡å‹ï¼Œåœ¨ä¿æŒç›¸å¯¹åå¥½å’Œå¥–åŠ±æœŸæœ›ä¸å˜çš„å‰æä¸‹ï¼Œé€šè¿‡æ•°å­¦è¯æ˜äº†å¢åŠ æ–¹å·®å¯¹åŠ é€Ÿè®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚é’ˆå¯¹è¯¥æ¨¡å‹æ¶‰åŠçš„éå‡¸ä¼˜åŒ– (nonconvex optimization) éš¾é¢˜ï¼Œå›¢é˜Ÿå¼€å‘äº†ä¸€ç§å¤æ‚åº¦ä¸º $O(n \\log n)$ çš„é«˜æ•ˆç®—æ³•ä»¥è·å–å…¨å±€æœ€ä¼˜è§£ã€‚è¯¥æ–¹æ³•è¢«æˆåŠŸé›†æˆåˆ° GRPO æ¡†æ¶ä¸­ï¼Œå¹¶ä¸º DeepSeek-R1 ä¸­ rule-based reward çš„å®è¯æœ‰æ•ˆæ€§æä¾›äº†ä¸€ç§é—´æ¥çš„ç†è®ºè§£é‡Šã€‚å®éªŒç»“æœè¯æ˜ï¼Œä¸åŸå§‹ GRPO ç›¸æ¯”ï¼ŒGRPOVI ç®—æ³•èƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡ RLHF çš„è®­ç»ƒæ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23247v2",
      "published_date": "2025-05-29 08:54:06 UTC",
      "updated_date": "2025-06-17 06:41:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:07:36.325218+00:00"
    },
    {
      "arxiv_id": "2505.23239v1",
      "title": "OSS-UAgent: An Agent-based Usability Evaluation Framework for Open Source Software",
      "title_zh": "OSS-UAgentï¼šä¸€ç§åŸºäºæ™ºèƒ½ä½“çš„å¼€æºè½¯ä»¶å¯ç”¨æ€§è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Lingkai Meng",
        "Yu Shao",
        "Long Yuan",
        "Longbin Lai",
        "Peng Cheng",
        "Wenyuan Yu",
        "Wenjie Zhang",
        "Xuemin Lin",
        "Jingren Zhou"
      ],
      "abstract": "Usability evaluation is critical to the impact and adoption of open source software (OSS), yet traditional methods relying on human evaluators suffer from high costs and limited scalability. To address these limitations, we introduce OSS-UAgent, an automated, configurable, and interactive agent-based usability evaluation framework specifically designed for open source software. Our framework employs intelligent agents powered by large language models (LLMs) to simulate developers performing programming tasks across various experience levels (from Junior to Expert). By dynamically constructing platform-specific knowledge bases, OSS-UAgent ensures accurate and context-aware code generation. The generated code is automatically evaluated across multiple dimensions, including compliance, correctness, and readability, providing a comprehensive measure of the software's usability. Additionally, our demonstration showcases OSS-UAgent's practical application in evaluating graph analytics platforms, highlighting its effectiveness in automating usability evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OSS-UAgentï¼Œä¸€ç§é’ˆå¯¹å¼€æºè½¯ä»¶ (OSS) çš„è‡ªåŠ¨åŒ–ã€å¯é…ç½®ä¸”äº’åŠ¨çš„åŸºäºæ™ºèƒ½ä½“çš„å¯ç”¨æ€§è¯„ä¼°æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿäººå·¥è¯„ä¼°æˆæœ¬é«˜ä¸”å¯æ‰©å±•æ€§æœ‰é™çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç”±å¤§è¯­è¨€æ¨¡å‹ (LLMs) é©±åŠ¨çš„æ™ºèƒ½ä½“æ¥æ¨¡æ‹Ÿä»åˆå­¦è€… (Junior) åˆ°ä¸“å®¶ (Expert) ä¸åŒç»éªŒæ°´å¹³çš„å¼€å‘è€…ã€‚OSS-UAgent é€šè¿‡åŠ¨æ€æ„å»ºç‰¹å®šå¹³å°çš„çŸ¥è¯†åº“ï¼Œç¡®ä¿ç”Ÿæˆä»£ç çš„å‡†ç¡®æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚ç”Ÿæˆçš„ä»£ç ä»åˆè§„æ€§ (Compliance)ã€æ­£ç¡®æ€§ (Correctness) å’Œå¯è¯»æ€§ (Readability) ç­‰å¤šä¸ªç»´åº¦è¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼Œä»è€Œæä¾›å¯¹è½¯ä»¶å¯ç”¨æ€§çš„å…¨é¢è¡¡é‡ã€‚ç›¸å…³æ¼”ç¤ºå±•ç¤ºäº† OSS-UAgent åœ¨è¯„ä¼°å›¾åˆ†æå¹³å°æ–¹é¢çš„å®é™…åº”ç”¨ï¼Œè¯æ˜äº†å…¶åœ¨è‡ªåŠ¨åŒ–å¯ç”¨æ€§è¯„ä¼°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23239v1",
      "published_date": "2025-05-29 08:40:10 UTC",
      "updated_date": "2025-05-29 08:40:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:08:15.035442+00:00"
    },
    {
      "arxiv_id": "2505.23229v2",
      "title": "MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration",
      "title_zh": "MCTSr-Zeroï¼šåŸºäºå‡†åˆ™ä¸è‡ªé€‚åº”æ¢ç´¢çš„è‡ªæˆ‘åæ€å¼å¿ƒç†å’¨è¯¢å¯¹è¯ç”Ÿæˆ",
      "authors": [
        "Hao Lu",
        "Yanchi Gu",
        "Haoyuan Huang",
        "Yulin Zhou",
        "Ningxin Zhu",
        "Chen Li"
      ],
      "abstract": "The integration of Monte Carlo Tree Search (MCTS) with Large Language Models (LLMs) has demonstrated significant success in structured, problem-oriented tasks. However, applying these methods to open-ended dialogues, such as those in psychological counseling, presents unique challenges. Unlike tasks with objective correctness, success in therapeutic conversations depends on subjective factors like empathetic engagement, ethical adherence, and alignment with human preferences, for which strict \"correctness\" criteria are ill-defined. Existing result-oriented MCTS approaches can therefore produce misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS framework designed for open-ended, human-centric dialogues. Its core innovation is \"domain alignment\", which shifts the MCTS search objective from predefined end-states towards conversational trajectories that conform to target domain principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates \"Regeneration\" and \"Meta-Prompt Adaptation\" mechanisms to substantially broaden exploration by allowing the MCTS to consider fundamentally different initial dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM. We also introduce PsyEval, a benchmark for assessing multi-turn psychological counseling dialogues. Experiments demonstrate that PsyLLM achieves state-of-the-art performance on PsyEval and other relevant metrics, validating MCTSr-Zero's effectiveness in generating high-quality, principle-aligned conversational data for human-centric domains and addressing the LLM challenge of consistently adhering to complex psychological standards.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MCTSr-Zeroï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¿ƒç†å’¨è¯¢ç­‰ä»¥äººä¸ºä¸­å¿ƒçš„å¼€æ”¾å¼å¯¹è¯è®¾è®¡çš„ Monte Carlo Tree Search (MCTS) æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿ MCTS åœ¨ç¼ºä¹å®¢è§‚æ­£ç¡®æ€§æ ‡å‡†çš„ä»»åŠ¡ä¸­å®¹æ˜“äº§ç”Ÿåå·®çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†â€œé¢†åŸŸå¯¹é½â€ (Domain Alignment) åˆ›æ–°ï¼Œå°†æœç´¢ç›®æ ‡è½¬å‘ç¬¦åˆå…±æƒ…ç­‰é¢†åŸŸåŸåˆ™çš„å¯¹è¯è½¨è¿¹ã€‚åŒæ—¶ï¼Œé€šè¿‡â€œé‡æ–°ç”Ÿæˆâ€ (Regeneration) å’Œâ€œå…ƒæç¤ºè¯è‡ªé€‚åº”â€ (Meta-Prompt Adaptation) æœºåˆ¶ï¼ŒMCTSr-Zero èƒ½å¤Ÿæ¢ç´¢æ›´å¤šå…ƒåŒ–çš„åˆå§‹å¯¹è¯ç­–ç•¥ã€‚ç ”ç©¶è€…åˆ©ç”¨è¯¥æ¡†æ¶ç”Ÿæˆçš„å¯¹è¯æ•°æ®å¾®è°ƒäº† PsyLLMï¼Œå¹¶æ¨å‡ºäº†å¤šè½®å¿ƒç†å’¨è¯¢å¯¹è¯åŸºå‡† PsyEvalã€‚å®éªŒè¯æ˜ï¼ŒPsyLLM åœ¨ PsyEval ä¸Šå–å¾—äº† SOTA æ€§èƒ½ï¼Œæœ‰æ•ˆéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡ã€ç¬¦åˆå¤æ‚å¿ƒç†å­¦æ ‡å‡†æ•°æ®æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "48 pages, 3 figures. Accepted in AAAI-2026 (Main Technical Track). For code and model, see this https://github.com/JianChengXingYun/Mctsr-Zero",
      "pdf_url": "https://arxiv.org/pdf/2505.23229v2",
      "published_date": "2025-05-29 08:30:15 UTC",
      "updated_date": "2025-11-18 09:09:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:08:06.414758+00:00"
    },
    {
      "arxiv_id": "2505.23862v1",
      "title": "A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors",
      "title_zh": "ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ mRNA ä¼˜åŒ–æ–°æ–¹æ³•ï¼šé«˜ä¿çœŸåº¦ã€è®¡ç®—æ•ˆç‡ä¸å¤šé‡ä¼˜åŒ–å› å­",
      "authors": [
        "Zheng Gong",
        "Ziyi Jiang",
        "Weihao Gao",
        "Deng Zhuo",
        "Lan Ma"
      ],
      "abstract": "The mRNA optimization is critical for therapeutic and biotechnological applications, since sequence features directly govern protein expression levels and efficacy. However, current methods face significant challenges in simultaneously achieving three key objectives: (1) fidelity (preventing unintended amino acid changes), (2) computational efficiency (speed and scalability), and (3) the scope of optimization variables considered (multi-objective capability). Furthermore, existing methods often fall short of comprehensively incorporating the factors related to the mRNA lifecycle and translation process, including intrinsic mRNA sequence properties, secondary structure, translation elongation kinetics, and tRNA availability. To address these limitations, we introduce \\textbf{RNop}, a novel deep learning-based method for mRNA optimization. We collect a large-scale dataset containing over 3 million sequences and design four specialized loss functions, the GPLoss, CAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control over sequence fidelity while optimizing species-specific codon adaptation, tRNA availability, and desirable mRNA secondary structure features. Then, we demonstrate RNop's effectiveness through extensive in silico and in vivo experiments. RNop ensures high sequence fidelity, achieves significant computational throughput up to 47.32 sequences/s, and yields optimized mRNA sequences resulting in a significant increase in protein expression for functional proteins compared to controls. RNop surpasses current methodologies in both quantitative metrics and experimental validation, enlightening a new dawn for efficient and effective mRNA design. Code and models will be available at https://github.com/HudenJear/RPLoss.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º RNop çš„æ–°å‹æ·±åº¦å­¦ä¹  mRNA ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯åœ¨ä¿çœŸåº¦ (fidelity)ã€è®¡ç®—æ•ˆç‡åŠå¤šå› ç´ ä¼˜åŒ–æ–¹é¢çš„å±€é™æ€§ã€‚RNop åŸºäºåŒ…å«è¶…è¿‡ 300 ä¸‡æ¡åºåˆ—çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé€šè¿‡è®¾è®¡ GPLossã€CAILossã€tAILoss å’Œ MFELoss å››ç§ä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œå®ç°äº†å¯¹åºåˆ—ä¿çœŸåº¦ã€å¯†ç å­é€‚åº”æ€§ã€tRNA å¯ç”¨æ€§å’Œ mRNA äºŒçº§ç»“æ„ (secondary structure) ç‰¹å¾çš„åŒæ—¶æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·å¤‡æé«˜çš„è®¡ç®—æ•ˆç‡ï¼Œååé‡å¯è¾¾ 47.32 sequences/sã€‚åœ¨è®¡ç®—æœºæ¨¡æ‹Ÿ (in silico) å’Œä½“å†… (in vivo) å®éªŒä¸­ï¼Œç”± RNop ä¼˜åŒ–çš„ mRNA åºåˆ—å‡æ˜¾è‘—æå‡äº†åŠŸèƒ½è›‹ç™½è´¨çš„è¡¨è¾¾æ°´å¹³ã€‚ç›¸è¾ƒäºå½“å‰ä¸»æµæ–¹æ³•ï¼ŒRNop åœ¨å¤šé¡¹å®šé‡æŒ‡æ ‡å’Œå®éªŒéªŒè¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºé«˜æ•ˆã€ç²¾å‡†çš„ mRNA è¯ç‰©è®¾è®¡ä¸ç”Ÿç‰©æŠ€æœ¯åº”ç”¨æä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "You can also contact hudenjear@gmail.com for more information",
      "pdf_url": "https://arxiv.org/pdf/2505.23862v1",
      "published_date": "2025-05-29 08:21:11 UTC",
      "updated_date": "2025-05-29 08:21:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:07:58.336887+00:00"
    },
    {
      "arxiv_id": "2505.23861v1",
      "title": "BiBLDR: Bidirectional Behavior Learning for Drug Repositioning",
      "title_zh": "BiBLDRï¼šé¢å‘è¯ç‰©é‡å®šä½çš„åŒå‘è¡Œä¸ºå­¦ä¹ ",
      "authors": [
        "Renye Zhang",
        "Mengyun Yang",
        "Qichang Zhao",
        "Jianxin Wang"
      ],
      "abstract": "Drug repositioning aims to identify potential new indications for existing drugs to reduce the time and financial costs associated with developing new drugs. Most existing deep learning-based drug repositioning methods predominantly utilize graph-based representations. However, graph-based drug repositioning methods struggle to perform effective inference in cold-start scenarios involving novel drugs because of the lack of association information with the diseases. Unlike traditional graph-based approaches, we propose a bidirectional behavior learning strategy for drug repositioning, known as BiBLDR. This innovative framework redefines drug repositioning as a behavior sequential learning task to capture drug-disease interaction patterns. First, we construct bidirectional behavioral sequences based on drug and disease sides. The consideration of bidirectional information ensures a more meticulous and rigorous characterization of the behavioral sequences. Subsequently, we propose a two-stage strategy for drug repositioning. In the first stage, we construct prototype spaces to characterize the representational attributes of drugs and diseases. In the second stage, these refined prototypes and bidirectional behavior sequence data are leveraged to predict potential drug-disease associations. Based on this learning approach, the model can more robustly and precisely capture the interactive relationships between drug and disease features from bidirectional behavioral sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on benchmark datasets. Meanwhile, BiBLDR demonstrates significantly superior performance compared to previous methods in cold-start scenarios. Our code is published in https://github.com/Renyeeah/BiBLDR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BiBLDRï¼Œä¸€ç§ç”¨äºè¯ç‰©é‡å®šå‘ Drug repositioning çš„åŒå‘è¡Œä¸ºå­¦ä¹ æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»ŸåŸºäºå›¾çš„æ–¹æ³•åœ¨å¤„ç†ç¼ºä¹å…³è”ä¿¡æ¯çš„ cold-start åœºæ™¯æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†è¯ç‰©é‡å®šå‘å®šä¹‰ä¸º behavior sequential learning ä»»åŠ¡ï¼Œä»¥æ•æ‰è¯ç‰©ä¸ç–¾ç—…ä¹‹é—´çš„äº¤äº’æ¨¡å¼ã€‚BiBLDR é€šè¿‡ä»è¯ç‰©å’Œç–¾ç—…ä¸¤ç«¯æ„å»ºåŒå‘è¡Œä¸ºåºåˆ—ï¼Œç¡®ä¿å¯¹äº¤äº’æ¨¡å¼è¿›è¡Œæ›´ä¸¥è°¨ã€ç»†è‡´çš„åˆ»ç”»ã€‚éšåè¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼Œé¦–å…ˆåˆ©ç”¨ prototype spaces åˆ»ç”»è¯ç‰©ä¸ç–¾ç—…çš„è¡¨ç¤ºå±æ€§ï¼Œè¿›è€Œç»“åˆåºåˆ—æ•°æ®é¢„æµ‹æ½œåœ¨çš„ drug-disease associationsã€‚è¿™ç§å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿæ›´ç¨³å¥ã€ç²¾ç¡®åœ°æ•æ‰ç‰¹å¾é—´çš„å¤æ‚å…³ç³»ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒBiBLDR åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œä¸”åœ¨å†·å¯åŠ¨åœºæ™¯ä¸‹çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä»¥å¾€æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23861v1",
      "published_date": "2025-05-29 08:20:15 UTC",
      "updated_date": "2025-05-29 08:20:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:08:22.651780+00:00"
    },
    {
      "arxiv_id": "2506.00056v2",
      "title": "Toward Knowledge-Guided AI for Inverse Design in Manufacturing: A Perspective on Domain, Physics, and Human-AI Synergy",
      "title_zh": "è¿ˆå‘åˆ¶é€ ä¸šé€†å‘è®¾è®¡çš„çŸ¥è¯†å¼•å¯¼äººå·¥æ™ºèƒ½ï¼šé¢†åŸŸã€ç‰©ç†ä¸äººæœºååŒçš„è§†è§’",
      "authors": [
        "Hugon Lee",
        "Hyeonbin Moon",
        "Junhyeong Lee",
        "Seunghwa RYu"
      ],
      "abstract": "Artificial intelligence (AI) is reshaping inverse design in manufacturing, enabling high-performance discovery in materials, products, and processes. However, purely data-driven approaches often struggle in realistic manufacturing settings characterized by sparse data, high-dimensional design spaces, and complex constraints. This perspective proposes an integrated framework built on three complementary pillars: domain knowledge to establish physically meaningful objectives and constraints while removing variables with limited relevance, physics-informed machine learning to enhance generalization under limited or biased data, and large language model-based interfaces to support intuitive, human-centered interaction. Using injection molding as an illustrative example, we demonstrate how these components can operate in practice and conclude by highlighting key challenges for applying such approaches in realistic manufacturing environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ¶é€ ä¸šä¸­çº¯æ•°æ®é©±åŠ¨AIåœ¨é€†å‘è®¾è®¡(inverse design)ä¸­é¢ä¸´çš„æ•°æ®ç¨€ç¼ºã€é«˜ç»´ç©ºé—´åŠå¤æ‚çº¦æŸç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªä¸‰ä½ä¸€ä½“çš„é›†æˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ•´åˆäº†é¢†åŸŸçŸ¥è¯†(domain knowledge)ã€ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ (physics-informed machine learning)ä»¥åŠåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„äº¤äº’ç•Œé¢ä¸‰ä¸ªæ ¸å¿ƒæ”¯æŸ±ã€‚å…¶ä¸­ï¼Œé¢†åŸŸçŸ¥è¯†ç”¨äºå»ºç«‹å…·æœ‰ç‰©ç†æ„ä¹‰çš„ç›®æ ‡å¹¶ç­›é€‰å…³é”®å˜é‡ï¼Œç‰©ç†é©±åŠ¨çš„æ–¹æ³•åˆ™æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸‹çš„æ³›åŒ–æ€§èƒ½ã€‚é€šè¿‡LLMæ”¯æŒçš„äººæœºååŒ(human-AI synergy)ç•Œé¢ï¼Œç ”ç©¶å®ç°äº†ç›´è§‚ä¸”ä»¥äººä¸ºä¸­å¿ƒçš„äº¤äº’æ¨¡å¼ã€‚æ–‡ç« ä»¥æ³¨å¡‘æˆå‹(injection molding)ä½œä¸ºå…¸å‹æ¡ˆä¾‹ï¼Œæ·±å…¥è®ºè¯äº†è¯¥æ–¹æ³•åœ¨å®é™…åˆ¶é€ æµç¨‹ä¸­çš„åº”ç”¨å¯è¡Œæ€§ã€‚æœ€åï¼Œä½œè€…æ€»ç»“äº†åœ¨çœŸå®åˆ¶é€ ç¯å¢ƒä¸­éƒ¨ç½²æ­¤ç±»çŸ¥è¯†å¼•å¯¼AIç³»ç»Ÿæ‰€é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages, 6 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2506.00056v2",
      "published_date": "2025-05-29 08:15:27 UTC",
      "updated_date": "2025-08-23 04:47:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:08:49.095807+00:00"
    },
    {
      "arxiv_id": "2505.23860v3",
      "title": "Quantum computing and artificial intelligence: status and perspectives",
      "title_zh": "é‡å­è®¡ç®—ä¸äººå·¥æ™ºèƒ½ï¼šç°çŠ¶ä¸å±•æœ›",
      "authors": [
        "Giovanni Acampora",
        "Andris Ambainis",
        "Natalia Ares",
        "Leonardo Banchi",
        "Pallavi Bhardwaj",
        "Daniele Binosi",
        "G. Andrew D. Briggs",
        "Tommaso Calarco",
        "Vedran Dunjko",
        "Jens Eisert",
        "Olivier Ezratty",
        "Paul Erker",
        "Federico Fedele",
        "Elies Gil-Fuster",
        "Martin GÃ¤rttner",
        "Mats Granath",
        "Markus Heyl",
        "Iordanis Kerenidis",
        "Matthias Klusch",
        "Anton Frisk Kockum",
        "Richard Kueng",
        "Mario Krenn",
        "JÃ¶rg LÃ¤ssig",
        "Antonio Macaluso",
        "Sabrina Maniscalco",
        "Florian Marquardt",
        "Kristel Michielsen",
        "Gorka MuÃ±oz-Gil",
        "Daniel MÃ¼ssig",
        "Hendrik Poulsen Nautrup",
        "Sophie A. Neubauer",
        "Evert van Nieuwenburg",
        "Roman Orus",
        "JÃ¶rg Schmiedmayer",
        "Markus Schmitt",
        "Philipp Slusallek",
        "Filippo Vicentini",
        "Christof Weitenberg",
        "Frank K. Wilhelm"
      ],
      "abstract": "This white paper discusses and explores the various points of intersection between quantum computing and artificial intelligence (AI). It describes how quantum computing could support the development of innovative AI solutions. It also examines use cases of classical AI that can empower research and development in quantum technologies, with a focus on quantum computing and quantum sensing. The purpose of this white paper is to provide a long-term research agenda aimed at addressing foundational questions about how AI and quantum computing interact and benefit one another. It concludes with a set of recommendations and challenges, including how to orchestrate the proposed theoretical work, align quantum AI developments with quantum hardware roadmaps, estimate both classical and quantum resources - especially with the goal of mitigating and optimizing energy consumption - advance this emerging hybrid software engineering discipline, and enhance European industrial competitiveness while considering societal implications.",
      "tldr_zh": "è¯¥ç™½çš®ä¹¦æ·±å…¥æ¢è®¨äº†Quantum Computingä¸Artificial Intelligence (AI)ä¹‹é—´çš„å¤šä¸ªäº¤é›†ç‚¹ï¼Œé˜è¿°äº†é‡å­è®¡ç®—å¦‚ä½•æ”¯æŒåˆ›æ–°AIè§£å†³æ–¹æ¡ˆçš„å¼€å‘ã€‚åŒæ—¶ï¼Œç ”ç©¶åˆ†æäº†ç»å…¸AIåœ¨èµ‹èƒ½Quantum Computingå’ŒQuantum Sensingç­‰é‡å­æŠ€æœ¯ç ”å‘ä¸­çš„åº”ç”¨åœºæ™¯ã€‚æœ¬æŠ¥å‘Šæ—¨åœ¨æä¾›ä¸€ä¸ªé•¿æœŸçš„ç ”ç©¶è®®ç¨‹ï¼Œé‡ç‚¹è§£å†³AIä¸é‡å­è®¡ç®—ç›¸äº’ä½œç”¨åŠäº’åˆ©çš„åŸºç¡€æ€§é—®é¢˜ã€‚ç™½çš®ä¹¦æ€»ç»“äº†ä¸€ç³»åˆ—å»ºè®®ä¸æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åè°ƒç†è®ºç ”ç©¶ã€ä½¿é‡å­AIå¼€å‘ä¸é‡å­ç¡¬ä»¶è·¯çº¿å›¾å¯¹é½ï¼Œä»¥åŠåœ¨èµ„æºä¼°ç®—ä¸­ä¼˜åŒ–Energy Consumptionã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¿˜å¼ºè°ƒäº†æ¨åŠ¨æ–°å…´çš„æ··åˆè½¯ä»¶å·¥ç¨‹å­¦ç§‘å‘å±•ã€æå‡æ¬§æ´²å·¥ä¸šç«äº‰åŠ›ä»¥åŠè¯„ä¼°ç›¸å…³ç¤¾ä¼šå½±å“çš„é‡è¦æ€§ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "33 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23860v3",
      "published_date": "2025-05-29 08:15:23 UTC",
      "updated_date": "2025-06-30 12:33:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:08:46.494874+00:00"
    },
    {
      "arxiv_id": "2505.23859v2",
      "title": "Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration",
      "title_zh": "æœ€å°åŒ–æ¨¡å‹åˆå¹¶ä¸­çš„ç‰¹å¾æ¼‚ç§»ï¼šé¢å‘è‡ªé€‚åº”çŸ¥è¯†æ•´åˆçš„é€å±‚ä»»åŠ¡å‘é‡èåˆ",
      "authors": [
        "Wenju Sun",
        "Qingyong Li",
        "Wen Wang",
        "Yang Liu",
        "Yangli-ao Geng",
        "Boyang Li"
      ],
      "abstract": "Multi-task model merging aims to consolidate knowledge from multiple fine-tuned task-specific experts into a unified model while minimizing performance degradation. Existing methods primarily approach this by minimizing differences between task-specific experts and the unified model, either from a parameter-level or a task-loss perspective. However, parameter-level methods exhibit a significant performance gap compared to the upper bound, while task-loss approaches entail costly secondary training procedures. In contrast, we observe that performance degradation closely correlates with feature drift, i.e., differences in feature representations of the same sample caused by model merging. Motivated by this observation, we propose Layer-wise Optimal Task Vector Merging (LOT Merging), a technique that explicitly minimizes feature drift between task-specific experts and the unified model in a layer-by-layer manner. LOT Merging can be formulated as a convex quadratic optimization problem, enabling us to analytically derive closed-form solutions for the parameters of linear and normalization layers. Consequently, LOT Merging achieves efficient model consolidation through basic matrix operations. Extensive experiments across vision and vision-language benchmarks demonstrate that LOT Merging significantly outperforms baseline methods, achieving improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches. The source code is available at https://github.com/SunWenJu123/model-merging.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šä»»åŠ¡æ¨¡å‹èåˆ(Multi-task model merging)ä¸­å­˜åœ¨çš„æ€§èƒ½ä¸‹é™ä¸é«˜æ˜‚è®­ç»ƒæˆæœ¬é—®é¢˜ï¼Œå‘ç°èåˆåçš„æ€§èƒ½è¡°å‡ä¸ç‰¹å¾æ¼‚ç§»(feature drift)é«˜åº¦ç›¸å…³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†é€å±‚æœ€ä¼˜ä»»åŠ¡å‘é‡èåˆ(Layer-wise Optimal Task Vector Merging, LOT Merging)ï¼Œæ—¨åœ¨é€å±‚æ˜¾å¼æœ€å°åŒ–ç‰¹å®šä»»åŠ¡ä¸“å®¶æ¨¡å‹ä¸ç»Ÿä¸€æ¨¡å‹ä¹‹é—´çš„ç‰¹å¾è¡¨ç¤ºå·®å¼‚ã€‚è¯¥æ–¹æ³•è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå‡¸äºŒæ¬¡ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é’ˆå¯¹çº¿æ€§å±‚å’Œå½’ä¸€åŒ–å±‚æ¨å¯¼å‡ºäº†é—­å¼è§£ï¼Œä½¿å¾—æ¨¡å‹å·©å›ºè¿‡ç¨‹ä»…éœ€é€šè¿‡åŸºç¡€çŸ©é˜µè¿ç®—å³å¯é«˜æ•ˆå®Œæˆã€‚åœ¨è§†è§‰å’Œè§†è§‰-è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLOT Merging æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨ ViT-B/32 ä¸Šçš„æ€§èƒ½æå‡é«˜è¾¾ 4.4%ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆä¸”è‡ªé€‚åº”çš„çŸ¥è¯†é›†æˆæä¾›äº†ä¸€ç§å…¨æ–°çš„ä¼˜åŒ–è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23859v2",
      "published_date": "2025-05-29 08:11:31 UTC",
      "updated_date": "2025-10-26 00:53:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:08:49.697513+00:00"
    },
    {
      "arxiv_id": "2505.23214v1",
      "title": "SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection",
      "title_zh": "SAMambaï¼šèåˆå±‚çº§è§†è§‰çš„è‡ªé€‚åº”çŠ¶æ€ç©ºé—´å»ºæ¨¡çº¢å¤–å¼±å°ç›®æ ‡æ£€æµ‹",
      "authors": [
        "Wenhao Xu",
        "Shuchen Zheng",
        "Changwei Wang",
        "Zherui Zhang",
        "Chuan Ren",
        "Rongtao Xu",
        "Shibiao Xu"
      ],
      "abstract": "Infrared small target detection (ISTD) is vital for long-range surveillance in military, maritime, and early warning applications. ISTD is challenged by targets occupying less than 0.15% of the image and low distinguishability from complex backgrounds. Existing deep learning methods often suffer from information loss during downsampling and inefficient global context modeling. This paper presents SAMamba, a novel framework integrating SAM2's hierarchical feature learning with Mamba's selective sequence modeling. Key innovations include: (1) A Feature Selection Adapter (FS-Adapter) for efficient natural-to-infrared domain adaptation via dual-stage selection (token-level with a learnable task embedding and channel-wise adaptive transformations); (2) A Cross-Channel State-Space Interaction (CSI) module for efficient global context modeling with linear complexity using selective state space modeling; and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively combines multi-scale features with a gating mechanism to balance high-resolution and low-resolution feature contributions. SAMamba addresses core ISTD challenges by bridging the domain gap, maintaining fine-grained details, and efficiently modeling long-range dependencies. Experiments on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly outperforms state-of-the-art methods, especially in challenging scenarios with heterogeneous backgrounds and varying target scales. Code: https://github.com/zhengshuchen/SAMamba.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çº¢å¤–å°ç›®æ ‡æ£€æµ‹ (ISTD) ä¸­ç›®æ ‡å æ¯”æå°ä¸”èƒŒæ™¯å¤æ‚å¯¼è‡´çš„ä¸‹é‡‡æ ·ä¿¡æ¯ä¸¢å¤±å’Œå…¨å±€å»ºæ¨¡æ•ˆç‡ä½ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† SAMamba æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°† SAM2 çš„å±‚æ¬¡åŒ–ç‰¹å¾å­¦ä¹ èƒ½åŠ›ä¸ Mamba çš„é€‰æ‹©æ€§åºåˆ—å»ºæ¨¡ç›¸ç»“åˆï¼Œåˆ©ç”¨ Feature Selection Adapter (FS-Adapter) é€šè¿‡åŒé˜¶æ®µé€‰æ‹©å®ç°ä»è‡ªç„¶å›¾åƒåˆ°çº¢å¤–å›¾åƒé¢†åŸŸçš„é«˜æ•ˆè¿ç§»ã€‚æ ¸å¿ƒçš„ Cross-Channel State-Space Interaction (CSI) æ¨¡å—åˆ©ç”¨é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œä»¥çº¿æ€§å¤æ‚åº¦å®ç°äº†é«˜æ•ˆçš„å…¨å±€ä¸Šä¸‹æ–‡æ•è·ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº† Detail-Preserving Contextual Fusion (DPCF) æ¨¡å—ï¼Œé€šè¿‡é—¨æ§æœºåˆ¶è‡ªé€‚åº”èåˆå¤šå°ºåº¦ç‰¹å¾ä»¥ä¿æŒå…³é”®çš„ç²¾ç»†ç»†èŠ‚ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSAMamba åœ¨ NUAA-SIRSTã€IRSTD-1k å’Œ NUDT-SIRST æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¼‚æ„èƒŒæ™¯å’Œå¤šå°ºåº¦ç›®æ ‡çš„æŒ‘æˆ˜æ€§åœºæ™¯ä¸­è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Information Fusion 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23214v1",
      "published_date": "2025-05-29 07:55:23 UTC",
      "updated_date": "2025-05-29 07:55:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:09:03.298871+00:00"
    },
    {
      "arxiv_id": "2505.23197v2",
      "title": "Unified Path Planner with Adaptive Safety and Optimality",
      "title_zh": "å…¼é¡¾è‡ªé€‚åº”å®‰å…¨æ€§ä¸æœ€ä¼˜æ€§çš„ç»Ÿä¸€è·¯å¾„è§„åˆ’å™¨",
      "authors": [
        "Jatin Kumar Arora",
        "Soutrik Bandyopadhyay",
        "Shubhendu Bhasin"
      ],
      "abstract": "Path planning for autonomous robots presents a fundamental trade-off between optimality and safety. While conventional algorithms typically prioritize one of these objectives, we introduce the Unified Path Planner (UPP), a unified framework that simultaneously addresses both. UPP is a graph-search-based algorithm that employs a modified heuristic function incorporating a dynamic safety cost, enabling an adaptive balance between path length and obstacle clearance. We establish theoretical sub-optimality bounds for the planner and demonstrate that its safety-to-optimality ratio can be tuned via adjustable parameters, with a trade-off in computational complexity. Extensive simulations show that UPP achieves a high success rate, generating near-optimal paths with only a negligible increase in cost over traditional A*, while ensuring safety margins that closely approach those of the classical Voronoi planner. Finally, the practical efficacy of UPP is validated through a hardware implementation on a TurtleBot, confirming its ability to navigate cluttered environments by generating safe, sub-optimal paths.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨æœºå™¨äººè·¯å¾„è§„åˆ’ä¸­è·¯å¾„æœ€ä¼˜æ€§(optimality)ä¸å®‰å…¨æ€§(safety)ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†Unified Path Planner (UPP)ç»Ÿä¸€æ¡†æ¶ã€‚UPPæ˜¯ä¸€ç§åŸºäºå›¾æœç´¢(graph-search-based)çš„ç®—æ³•ï¼Œé€šè¿‡åœ¨å¯å‘å¼å‡½æ•°(heuristic function)ä¸­å¼•å…¥åŠ¨æ€å®‰å…¨æˆæœ¬(dynamic safety cost)ï¼Œå®ç°äº†è·¯å¾„é•¿åº¦ä¸éšœç¢ç‰©é—´è·ä¹‹é—´çš„è‡ªé€‚åº”å¹³è¡¡ã€‚ç ”ç©¶è€…ä¸ºè¯¥è§„åˆ’å™¨ç¡®ç«‹äº†ç†è®ºæ¬¡ä¼˜æ€§ç•Œé™(sub-optimality bounds)ï¼Œå¹¶è¯æ˜äº†å…¶å®‰å…¨ä¸æœ€ä¼˜æ€§çš„æ¯”ä¾‹å¯é€šè¿‡å‚æ•°çµæ´»è°ƒèŠ‚ã€‚å¤§é‡ä»¿çœŸç»“æœæ˜¾ç¤ºï¼ŒUPPåœ¨ä¿æŒé«˜æˆåŠŸç‡çš„åŒæ—¶ï¼Œç”Ÿæˆçš„è·¯å¾„æ¥è¿‘A\\*ç®—æ³•çš„æœ€ä¼˜æ€§ï¼Œä¸”å®‰å…¨è£•åº¦(safety margins)æ¥è¿‘ç»å…¸çš„Voronoiè§„åˆ’å™¨ã€‚æœ€åï¼Œé€šè¿‡åœ¨TurtleBotç¡¬ä»¶ä¸Šçš„å®éªŒéªŒè¯ï¼Œè¯¥ç®—æ³•è¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨å¤æ‚æ‹¥æŒ¤ç¯å¢ƒä¸­ç”Ÿæˆå®‰å…¨ä¸”æ¬¡ä¼˜è·¯å¾„çš„å®é™…æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages,4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23197v2",
      "published_date": "2025-05-29 07:34:56 UTC",
      "updated_date": "2025-08-29 08:59:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:08:50.254850+00:00"
    },
    {
      "arxiv_id": "2505.23195v3",
      "title": "Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šé€šè¿‡ç»“æ„åŒ–å‰ªæå®ç°æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„ç‰¹åŒ–",
      "authors": [
        "Lifan Zhao",
        "Yanyan Shen",
        "Zhaoyang Liu",
        "Xue Wang",
        "Jiaji Deng"
      ],
      "abstract": "Scaling laws motivate the development of Time Series Foundation Models (TSFMs) that pre-train vast parameters and achieve remarkable zero-shot forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot consistently outperform smaller, specialized models trained on full-shot downstream data. A key question is how to realize effective adaptation of TSFMs for a target forecasting task. Through empirical studies on various TSFMs, the pre-trained models often exhibit inherent sparsity and redundancy in computation, suggesting that TSFMs have learned to activate task-relevant network substructures to accommodate diverse forecasting tasks. To preserve this valuable prior knowledge, we propose a structured pruning method to regularize the subsequent fine-tuning process by focusing it on a more relevant and compact parameter space. Extensive experiments on seven TSFMs and six benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly improves forecasting performance compared to fine-tuning original models. This prune-then-finetune paradigm often enables TSFMs to achieve state-of-the-art performance and surpass strong specialized baselines. Source code is made publicly available at https://github.com/SJTU-DMTai/Prune-then-Finetune.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹(TSFMs)åœ¨å¾®è°ƒåä»éš¾ä»¥ä¸€è‡´è¶…è¶Šä¸“é—¨åŒ–æ¨¡å‹çš„é—®é¢˜ï¼Œå¹¶æŒ‡å‡º TSFMs åœ¨è®¡ç®—ä¸­å­˜åœ¨å›ºæœ‰çš„ç¨€ç–æ€§ä¸å†—ä½™ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§ç»“æ„åŒ–å‰ªæ(Structured Pruning)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°†åç»­å¾®è°ƒè¿‡ç¨‹èšç„¦äºæ›´ç›¸å…³ä¸”ç´§å‡‘çš„å‚æ•°ç©ºé—´ï¼Œä»è€Œæœ‰æ•ˆåˆ©ç”¨æ¨¡å‹ä¸­å·²å­¦ä¹ åˆ°çš„ä»»åŠ¡ç›¸å…³å­ç»“æ„ã€‚åœ¨7ä¸ª TSFMs å’Œ6ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œå¾®è°ƒå‰ªæåçš„æ¨¡å‹æ¯”ç›´æ¥å¾®è°ƒåŸå§‹æ¨¡å‹èƒ½æ˜¾è‘—æå‡é¢„æµ‹æ€§èƒ½ã€‚è¿™ç§â€œå…ˆå‰ªæåå¾®è°ƒâ€(prune-then-finetune)çš„æ–°èŒƒå¼ä½¿ TSFMs èƒ½å¤Ÿè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶æˆåŠŸè¶…è¶Šäº†å¼ºåŠ›çš„ä¸“é—¨åŒ–åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23195v3",
      "published_date": "2025-05-29 07:33:49 UTC",
      "updated_date": "2025-12-21 06:23:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:09:06.235762+00:00"
    },
    {
      "arxiv_id": "2505.23192v1",
      "title": "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks",
      "title_zh": "æ¬ºç’ç›‘æµ‹è€…ï¼šé€šè¿‡è¯­ä¹‰æç¤ºæ”»å‡»æ”»ç ´ AIGC æ£€æµ‹å™¨",
      "authors": [
        "Run Hao",
        "Peng Ying"
      ],
      "abstract": "The rise of text-to-image (T2I) models has enabled the synthesis of photorealistic human portraits, raising serious concerns about identity misuse and the robustness of AIGC detectors. In this work, we propose an automated adversarial prompt generation framework that leverages a grammar tree structure and a variant of the Monte Carlo tree search algorithm to systematically explore the semantic prompt space. Our method generates diverse, controllable prompts that consistently evade both open-source and commercial AIGC detectors. Extensive experiments across multiple T2I models validate its effectiveness, and the approach ranked first in a real-world adversarial AIGC detection competition. Beyond attack scenarios, our method can also be used to construct high-quality adversarial datasets, providing valuable resources for training and evaluating more robust AIGC detection and defense systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ (T2I) æ¨¡å‹åˆæˆå†™å®äººåƒæ‰€å¸¦æ¥çš„èº«ä»½æ»¥ç”¨é£é™©ï¼Œæ·±å…¥æ¢è®¨äº† AIGC æ£€æµ‹å™¨çš„é²æ£’æ€§æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å¯¹æŠ—æ€§æç¤ºè¯ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆè¯­æ³•æ ‘ (grammar tree) ç»“æ„ä¸æ”¹è¿›çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ (Monte Carlo tree search) ç®—æ³•ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°æ¢ç´¢è¯­ä¹‰æç¤ºç©ºé—´å¹¶ç”Ÿæˆå¤šæ ·ä¸”å¯æ§çš„æ”»å‡»æç¤ºè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè§„é¿åŒ…æ‹¬å•†ä¸šäº§å“åœ¨å†…çš„å¤šç§å¼€æºåŠå•†ç”¨ AIGC æ£€æµ‹å™¨ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œçš„å¯¹æŠ—ç«èµ›ä¸­å–å¾—äº†ç¬¬ä¸€åçš„æˆç»©ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯ä¸ä»…å±•ç¤ºäº†ç°æœ‰æ£€æµ‹ç³»ç»Ÿçš„æ¼æ´ï¼Œè¿˜å¯ç”¨äºæ„å»ºé«˜è´¨é‡çš„å¯¹æŠ—æ€§æ•°æ®é›†ï¼Œä¸ºå¼€å‘å’Œè¯„ä¼°æ›´å…·é²æ£’æ€§çš„ AIGC é˜²å¾¡ç³»ç»Ÿæä¾›å…³é”®æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.23192v1",
      "published_date": "2025-05-29 07:31:17 UTC",
      "updated_date": "2025-05-29 07:31:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:09:12.921385+00:00"
    },
    {
      "arxiv_id": "2505.23191v1",
      "title": "ExpeTrans: LLMs Are Experiential Transfer Learners",
      "title_zh": "ExpeTransï¼šå¤§è¯­è¨€æ¨¡å‹æ˜¯ç»éªŒå‹è¿ç§»å­¦ä¹ è€…",
      "authors": [
        "Jinglong Gao",
        "Xiao Ding",
        "Lingxiao Zou",
        "Bibo Cai",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Recent studies provide large language models (LLMs) with textual task-solving experiences via prompts to improve their performance. However, previous methods rely on substantial human labor or time to gather such experiences for each task, which is impractical given the growing variety of task types in user queries to LLMs. To address this issue, we design an autonomous experience transfer framework to explore whether LLMs can mimic human cognitive intelligence to autonomously transfer experience from existing source tasks to newly encountered target tasks. This not only allows the acquisition of experience without extensive costs of previous methods, but also offers a novel path for the generalization of LLMs. Experimental results on 13 datasets demonstrate that our framework effectively improves the performance of LLMs. Furthermore, we provide a detailed analysis of each module in the framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡æç¤ºè·å–ä»»åŠ¡è§£å†³ç»éªŒæ—¶é¢ä¸´çš„äººåŠ›æˆæœ¬é«˜ã€è€—æ—¶é•¿ä¸”éš¾ä»¥è¦†ç›–å¤šæ ·åŒ–ä»»åŠ¡ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ExpeTransæ¡†æ¶ã€‚ExpeTransæ˜¯ä¸€ä¸ªè‡ªä¸»ç»éªŒè¿ç§»(autonomous experience transfer)æ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»è®¤çŸ¥æ™ºèƒ½ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå°†å·²æœ‰æºä»»åŠ¡çš„ç»éªŒè‡ªä¸»è¿ç§»åˆ°æ–°é‡åˆ°çš„ç›®æ ‡ä»»åŠ¡ä¸­ã€‚è¿™ç§æ–¹æ³•ä¸ä»…é¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­è·å–ç»éªŒçš„é«˜æ˜‚æˆæœ¬ï¼Œè¿˜ä¸ºæå‡LLMsçš„æ³›åŒ–èƒ½åŠ›(generalization)æä¾›äº†ä¸€æ¡æ–°è·¯å¾„ã€‚åœ¨13ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæå‡LLMsåœ¨å„ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¯¹æ¡†æ¶å†…å„æ¨¡å—è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¯¥è‡ªä¸»è¿ç§»æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 12 figs/tables",
      "pdf_url": "https://arxiv.org/pdf/2505.23191v1",
      "published_date": "2025-05-29 07:30:58 UTC",
      "updated_date": "2025-05-29 07:30:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:09:45.326039+00:00"
    },
    {
      "arxiv_id": "2505.23187v1",
      "title": "Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“åä½œçš„è·¨ä»»åŠ¡ç»éªŒå­¦ä¹ ",
      "authors": [
        "Yilong Li",
        "Chen Qian",
        "Yu Xia",
        "Ruijie Shi",
        "Yufan Dang",
        "Zihao Xie",
        "Ziming You",
        "Weize Chen",
        "Cheng Yang",
        "Weichuan Liu",
        "Ye Tian",
        "Xuantang Xiong",
        "Lei Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Large Language Model-based multi-agent systems (MAS) have shown remarkable progress in solving complex tasks through collaborative reasoning and inter-agent critique. However, existing approaches typically treat each task in isolation, resulting in redundant computations and limited generalization across structurally similar tasks. To address this, we introduce multi-agent cross-task experiential learning (MAEL), a novel framework that endows LLM-driven agents with explicit cross-task learning and experience accumulation. We model the task-solving workflow on a graph-structured multi-agent collaboration network, where agents propagate information and coordinate via explicit connectivity. During the experiential learning phase, we quantify the quality for each step in the task-solving workflow and store the resulting rewards along with the corresponding inputs and outputs into each agent's individual experience pool. During inference, agents retrieve high-reward, task-relevant experiences as few-shot examples to enhance the effectiveness of each reasoning step, thereby enabling more accurate and efficient multi-agent collaboration. Experimental results on diverse datasets demonstrate that MAEL empowers agents to learn from prior task experiences effectively-achieving faster convergence and producing higher-quality solutions on current tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Model)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)åœ¨å¤„ç†ç›¸ä¼¼ä»»åŠ¡æ—¶å­˜åœ¨çš„å†—ä½™è®¡ç®—å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†è·¨ä»»åŠ¡ç»éªŒå­¦ä¹ æ¡†æ¶MAELã€‚è¯¥æ¡†æ¶åœ¨å›¾ç»“æ„çš„å¤šæ™ºèƒ½ä½“åä½œç½‘ç»œ(graph-structured multi-agent collaboration network)ä¸Šå»ºæ¨¡ä»»åŠ¡è§£å†³æµç¨‹ï¼Œå¹¶åœ¨ç»éªŒå­¦ä¹ é˜¶æ®µå°†å„æ­¥éª¤çš„é«˜å¥–åŠ±è¾“å…¥è¾“å‡ºåŠç›¸åº”å›æŠ¥å­˜å…¥æ™ºèƒ½ä½“çš„ä¸ªäººç»éªŒæ± ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ™ºèƒ½ä½“é€šè¿‡æ£€ç´¢ç›¸å…³çš„æˆåŠŸç»éªŒä½œä¸ºFew-shotç¤ºä¾‹ï¼Œä»¥æ­¤å¢å¼ºæ¨ç†æ­¥éª¤çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œå®ç°æ›´ç²¾å‡†é«˜æ•ˆçš„åä½œã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMAELèƒ½æ˜¾è‘—æå‡æ™ºèƒ½ä½“ä»å…ˆå‰ä»»åŠ¡ä¸­å­¦ä¹ çš„èƒ½åŠ›ï¼Œä¸ä»…åŠ å¿«äº†æ”¶æ•›é€Ÿåº¦ï¼Œè¿˜åœ¨å„ç±»ä»»åŠ¡ä¸­äº§å‡ºäº†æ›´é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in Progress",
      "pdf_url": "https://arxiv.org/pdf/2505.23187v1",
      "published_date": "2025-05-29 07:24:37 UTC",
      "updated_date": "2025-05-29 07:24:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:09:15.584840+00:00"
    },
    {
      "arxiv_id": "2505.23183v1",
      "title": "Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement",
      "title_zh": "æœºå™¨ç¿»è¯‘æ— ç›‘ç£è¯çº§è´¨é‡ä¼°è®¡ï¼šåŸºäºæ ‡æ³¨è€…ï¼ˆä¸ï¼‰ä¸€è‡´æ€§çš„è§†è§’",
      "authors": [
        "Gabriele Sarti",
        "VilÃ©m Zouhar",
        "Malvina Nissim",
        "Arianna Bisazza"
      ],
      "abstract": "Word-level quality estimation (WQE) aims to automatically identify fine-grained error spans in machine-translated outputs and has found many uses, including assisting translators during post-editing. Modern WQE techniques are often expensive, involving prompting of large language models or ad-hoc training on large amounts of human-labeled data. In this work, we investigate efficient alternatives exploiting recent advances in language model interpretability and uncertainty quantification to identify translation errors from the inner workings of translation models. In our evaluation spanning 14 metrics across 12 translation directions, we quantify the impact of human label variation on metric performance by using multiple sets of human labels. Our results highlight the untapped potential of unsupervised metrics, the shortcomings of supervised methods when faced with label uncertainty, and the brittleness of single-annotator evaluation practices.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨ç¿»è¯‘ä¸­æ— ç›‘ç£çš„è¯çº§è´¨é‡ä¼°è®¡(Word-level Quality Estimation, WQE)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯å› è¿‡åº¦ä¾èµ–å¤§è¯­è¨€æ¨¡å‹æç¤ºæˆ–å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®è€Œå¯¼è‡´çš„æˆæœ¬é«˜æ˜‚é—®é¢˜ã€‚ä½œè€…åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§(Interpretability)å’Œä¸ç¡®å®šæ€§é‡åŒ–(Uncertainty Quantification)æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§ä»ç¿»è¯‘æ¨¡å‹å†…éƒ¨æœºåˆ¶è¯†åˆ«é”™è¯¯çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡åœ¨12ä¸ªç¿»è¯‘æ–¹å‘å’Œ14é¡¹æŒ‡æ ‡ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œè¯¥ç ”ç©¶æ·±å…¥é‡åŒ–äº†äººç±»æ ‡æ³¨å·®å¼‚å¯¹è¯„ä¼°æ€§èƒ½çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œæ— ç›‘ç£æŒ‡æ ‡å…·æœ‰å·¨å¤§çš„å¾…å¼€å‘æ½œåŠ›ï¼Œè€Œç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹æ ‡æ³¨ä¸ç¡®å®šæ€§æ—¶è¡¨ç°è¾ƒä¸ºè„†å¼±ã€‚è¯¥å·¥ä½œä¸ä»…æ­ç¤ºäº†ä¼ ç»Ÿå•ä¸€æ ‡æ³¨è€…è¯„ä¼°æ¨¡å¼çš„å±€é™æ€§ï¼Œä¹Ÿä¸ºå¼€å‘æ›´å…·é²æ£’æ€§çš„ç¿»è¯‘è´¨é‡è¯„ä¼°å·¥å…·æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review. Code: https://github.com/gsarti/labl/tree/main/examples/unsup_wqe Metrics: https://huggingface.co/datasets/gsarti/unsup_wqe_metrics",
      "pdf_url": "https://arxiv.org/pdf/2505.23183v1",
      "published_date": "2025-05-29 07:20:36 UTC",
      "updated_date": "2025-05-29 07:20:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:09:32.030643+00:00"
    },
    {
      "arxiv_id": "2505.23181v1",
      "title": "FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification",
      "title_zh": "FreRAï¼šé¢å‘æ—¶é—´åºåˆ—åˆ†ç±»å¯¹æ¯”å­¦ä¹ çš„é¢‘åŸŸç»†åŒ–å¢å¼ºæ–¹æ³•",
      "authors": [
        "Tian Tian",
        "Chunyan Miao",
        "Hangwei Qian"
      ],
      "abstract": "Contrastive learning has emerged as a competent approach for unsupervised representation learning. However, the design of an optimal augmentation strategy, although crucial for contrastive learning, is less explored for time series classification tasks. Existing predefined time-domain augmentation methods are primarily adopted from vision and are not specific to time series data. Consequently, this cross-modality incompatibility may distort the semantically relevant information of time series by introducing mismatched patterns into the data. To address this limitation, we present a novel perspective from the frequency domain and identify three advantages for downstream classification: global, independent, and compact. To fully utilize the three properties, we propose the lightweight yet effective Frequency Refined Augmentation (FreRA) tailored for time series contrastive learning on classification tasks, which can be seamlessly integrated with contrastive learning frameworks in a plug-and-play manner. Specifically, FreRA automatically separates critical and unimportant frequency components. Accordingly, we propose semantic-aware Identity Modification and semantic-agnostic Self-adaptive Modification to protect semantically relevant information in the critical frequency components and infuse variance into the unimportant ones respectively. Theoretically, we prove that FreRA generates semantic-preserving views. Empirically, we conduct extensive experiments on two benchmark datasets, including UCR and UEA archives, as well as five large-scale datasets on diverse applications. FreRA consistently outperforms ten leading baselines on time series classification, anomaly detection, and transfer learning tasks, demonstrating superior capabilities in contrastive representation learning and generalization in transfer learning scenarios across diverse datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹æ¯”å­¦ä¹ (Contrastive Learning)ä¸­ç°æœ‰æ—¶é—´åŸŸå¢å¼ºæ–¹æ³•ä¸æ—¶é—´åºåˆ—(Time Series)æ•°æ®ä¸å…¼å®¹å¯¼è‡´è¯­ä¹‰å¤±çœŸçš„é—®é¢˜ï¼Œæå‡ºäº†FreRA (Frequency Refined Augmentation)ï¼Œä¸€ç§ä¸“ä¸ºæ—¶é—´åºåˆ—åˆ†ç±»è®¾è®¡çš„è½»é‡åŒ–é¢‘ç‡åŸŸå¢å¼ºæ–¹æ³•ã€‚FreRAå……åˆ†åˆ©ç”¨äº†é¢‘ç‡åŸŸçš„å…¨å±€æ€§(Global)ã€ç‹¬ç«‹æ€§(Independent)å’Œç´§å‡‘æ€§(Compact)ä¼˜åŠ¿ï¼Œé€šè¿‡è‡ªåŠ¨åˆ†ç¦»å…³é”®é¢‘ç‡åˆ†é‡ä¸æ¬¡è¦é¢‘ç‡åˆ†é‡æ¥ä¼˜åŒ–æ•°æ®è¡¨ç¤ºã€‚è¯¥æ–¹æ³•å¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥èº«ä»½ä¿®æ”¹(Identity Modification)æ¥ä¿æŠ¤å…³é”®é¢‘ç‡ä¸­çš„æ ¸å¿ƒä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨è‡ªé€‚åº”ä¿®æ”¹(Self-adaptive Modification)ä¸ºä¸é‡è¦çš„åˆ†é‡æ³¨å…¥å¤šæ ·æ€§ã€‚ç†è®ºè¯æ˜FreRAèƒ½ç”Ÿæˆä¿æŒè¯­ä¹‰çš„è§†å›¾ï¼Œä¸”å…¶å…·æœ‰å³æ’å³ç”¨(Plug-and-play)ç‰¹æ€§ï¼Œå¯æ— ç¼é›†æˆè‡³å„ç±»å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸­ã€‚åœ¨UCRã€UEAç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†åŠå¤šç§å®é™…åº”ç”¨åœºæ™¯ä¸‹çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFreRAåœ¨æ—¶é—´åºåˆ—åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹å’Œè¿ç§»å­¦ä¹ ä»»åŠ¡ä¸­å‡ä¸€è‡´ä¼˜äº10ç§é¢†å…ˆçš„åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…æ˜¾è‘—æå‡äº†å¯¹æ¯”è¡¨å¾å­¦ä¹ çš„æ•ˆæœï¼Œä¹Ÿå±•ç°äº†å…¶åœ¨ä¸åŒæ•°æ®é›†å’Œä»»åŠ¡ä¸­å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23181v1",
      "published_date": "2025-05-29 07:18:28 UTC",
      "updated_date": "2025-05-29 07:18:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:10:22.836238+00:00"
    },
    {
      "arxiv_id": "2505.23165v1",
      "title": "Best Arm Identification with Possibly Biased Offline Data",
      "title_zh": "åŸºäºå¯èƒ½å­˜åœ¨åå·®çš„ç¦»çº¿æ•°æ®çš„æœ€ä½³è‡‚è¯†åˆ«",
      "authors": [
        "Le Yang",
        "Vincent Y. F. Tan",
        "Wang Chi Cheung"
      ],
      "abstract": "We study the best arm identification (BAI) problem with potentially biased offline data in the fixed confidence setting, which commonly arises in real-world scenarios such as clinical trials. We prove an impossibility result for adaptive algorithms without prior knowledge of the bias bound between online and offline distributions. To address this, we propose the LUCB-H algorithm, which introduces adaptive confidence bounds by incorporating an auxiliary bias correction to balance offline and online data within the LUCB framework. Theoretical analysis shows that LUCB-H matches the sample complexity of standard LUCB when offline data is misleading and significantly outperforms it when offline data is helpful. We also derive an instance-dependent lower bound that matches the upper bound of LUCB-H in certain scenarios. Numerical experiments further demonstrate the robustness and adaptability of LUCB-H in effectively incorporating offline data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å›ºå®šç½®ä¿¡åº¦è®¾ç½®ä¸‹åˆ©ç”¨å¯èƒ½å­˜åœ¨åå·®çš„ç¦»çº¿æ•°æ®è¿›è¡Œæœ€ä½³è‡‚è¯†åˆ« (Best Arm Identification, BAI) çš„é—®é¢˜ï¼Œè¿™åœ¨ä¸´åºŠè¯•éªŒç­‰å®é™…åœºæ™¯ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç ”ç©¶é¦–å…ˆè¯æ˜ï¼Œåœ¨ç¼ºä¹åœ¨çº¿ä¸ç¦»çº¿åˆ†å¸ƒåå·®ç•Œé™å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œè‡ªé€‚åº”ç®—æ³•å­˜åœ¨ä¸å¯è¡Œæ€§ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº† LUCB-H ç®—æ³•ï¼Œé€šè¿‡åœ¨ LUCB æ¡†æ¶ä¸­å¼•å…¥è¾…åŠ©åå·®ä¿®æ­£çš„è‡ªé€‚åº”ç½®ä¿¡åŒºé—´ï¼Œå®ç°äº†ç¦»çº¿ä¸åœ¨çº¿æ•°æ®çš„æœ‰æ•ˆå¹³è¡¡ã€‚ç†è®ºåˆ†ææŒ‡å‡ºï¼Œå½“ç¦»çº¿æ•°æ®å…·æœ‰è¯¯å¯¼æ€§æ—¶ï¼ŒLUCB-H çš„æ ·æœ¬å¤æ‚åº¦ (sample complexity) ä¸æ ‡å‡† LUCB æŒå¹³ï¼Œè€Œåœ¨ç¦»çº¿æ•°æ®æœ‰ç›Šæ—¶åˆ™èƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ¨å¯¼å‡ºçš„å®ä¾‹ç›¸å…³ä¸‹ç•Œ (instance-dependent lower bound) åœ¨ç‰¹å®šåœºæ™¯ä¸‹ä¸ç®—æ³•ä¸Šç•Œç›¸åŒ¹é…ã€‚æ•°å€¼å®éªŒè¿›ä¸€æ­¥è¯å®äº† LUCB-H åœ¨å¤„ç†ç¦»çº¿æ•°æ®æ—¶çš„ç¨³å¥æ€§ä¸è‡ªé€‚åº”æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to UAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23165v1",
      "published_date": "2025-05-29 06:58:49 UTC",
      "updated_date": "2025-05-29 06:58:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:10:06.911236+00:00"
    },
    {
      "arxiv_id": "2505.23161v2",
      "title": "Implicit Inversion turns CLIP into a Decoder",
      "title_zh": "éšå¼åæ¼”å°† CLIP è½¬åŒ–ä¸ºè§£ç å™¨",
      "authors": [
        "Antonio D'Orazio",
        "Maria Rosaria Briglia",
        "Donato Crisostomi",
        "Dario Loi",
        "Emanuele RodolÃ ",
        "Iacopo Masi"
      ],
      "abstract": "CLIP is a discriminative model trained to align images and text in a shared embedding space. Due to its multimodal structure, it serves as the backbone of many generative pipelines, where a decoder is trained to map from the shared space back to images. In this work, we show that image synthesis is nevertheless possible using CLIP alone -- without any decoder, training, or fine-tuning. Our approach optimizes a frequency-aware implicit neural representation that encourages coarse-to-fine generation by stratifying frequencies across network layers. To stabilize this inverse mapping, we introduce adversarially robust initialization, a lightweight Orthogonal Procrustes projection to align local text and image embeddings, and a blending loss that anchors outputs to natural image statistics. Without altering CLIP's weights, this framework unlocks capabilities such as text-to-image generation, style transfer, and image reconstruction. These findings suggest that discriminative models may hold untapped generative potential, hidden in plain sight.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Implicit Inversion æ–¹æ³•ï¼Œè¯æ˜äº† CLIP è¿™ä¸€åˆ¤åˆ«å¼æ¨¡å‹åœ¨æ— éœ€é¢å¤–è®­ç»ƒã€å¾®è°ƒæˆ– Decoder çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å¯ä»¥ç‹¬ç«‹å®Œæˆå›¾åƒåˆæˆä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–ä¸€ç§é¢‘ç‡æ„ŸçŸ¥ï¼ˆfrequency-awareï¼‰çš„éšå¼ç¥ç»è¡¨ç¤ºï¼ˆimplicit neural representationï¼‰ï¼Œåˆ©ç”¨ç½‘ç»œå±‚é—´çš„é¢‘ç‡åˆ†å±‚å®ç°ä»ç²—åˆ°ç»†çš„å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºäº†ç¡®ä¿é€†æ˜ å°„ï¼ˆinverse mappingï¼‰è¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œç ”ç©¶é‡‡ç”¨äº†å¯¹æŠ—é²æ£’åˆå§‹åŒ–ï¼ˆadversarially robust initializationï¼‰ã€æ­£äº¤æ™®ç½—å…‹é²æ–¯ç‰¹æŠ•å½±ï¼ˆOrthogonal Procrustes projectionï¼‰ä»¥åŠæ··åˆæŸå¤±ï¼ˆblending lossï¼‰ã€‚åœ¨ä¸ä¿®æ”¹ CLIP æƒé‡çš„å‰æä¸‹ï¼Œè¯¥æ¡†æ¶æˆåŠŸè§£é”äº† Text-to-image generationã€Style transfer å’Œ Image reconstruction ç­‰åŠŸèƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ¤åˆ«å¼æ¨¡å‹ä¸­è•´å«ç€å°šæœªè¢«å‘æ˜çš„å·¨å¤§ç”Ÿæˆæ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23161v2",
      "published_date": "2025-05-29 06:55:26 UTC",
      "updated_date": "2025-06-04 10:30:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:10:13.018391+00:00"
    },
    {
      "arxiv_id": "2505.23153v2",
      "title": "Conceptual Framework Toward Embodied Collective Adaptive Intelligence",
      "title_zh": "è¿ˆå‘å…·èº«é›†ä½“è‡ªé€‚åº”æ™ºèƒ½çš„æ¦‚å¿µæ¡†æ¶",
      "authors": [
        "Fan Wang",
        "Shaoshan Liu"
      ],
      "abstract": "Collective Adaptive Intelligence (CAI) represent a transformative approach in embodied AI, wherein numerous autonomous agents collaborate, adapt, and self-organize to navigate complex, dynamic environments. By enabling systems to reconfigure themselves in response to unforeseen challenges, CAI facilitate robust performance in real-world scenarios. This article introduces a conceptual framework for designing and analyzing CAI. It delineates key attributes including task generalization, resilience, scalability, and self-assembly, aiming to bridge theoretical foundations with practical methodologies for engineering adaptive, emergent intelligence. By providing a structured foundation for understanding and implementing CAI, this work seeks to guide researchers and practitioners in developing more resilient, scalable, and adaptable AI systems across various domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å…·èº«é›†ä½“è‡ªé€‚åº”æ™ºèƒ½ (Collective Adaptive Intelligence, CAI) çš„æ¦‚å¿µæ¡†æ¶ï¼Œæ—¨åœ¨æ¨åŠ¨å…·èº« AI (Embodied AI) åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚CAI çš„æ ¸å¿ƒåœ¨äºé€šè¿‡å¤§é‡è‡ªä¸»æ™ºèƒ½ä½“çš„åä½œã€é€‚åº”ä¸è‡ªç»„ç»‡ï¼Œä½¿ç³»ç»Ÿå…·å¤‡é’ˆå¯¹ä¸å¯é¢„è§æŒ‘æˆ˜è¿›è¡Œè‡ªæˆ‘é‡æ„çš„èƒ½åŠ›ï¼Œä»è€Œç¡®ä¿åœ¨ç°å®åœºæ™¯ä¸­çš„ç¨³å¥è¡¨ç°ã€‚è¯¥æ¡†æ¶è¯¦ç»†é˜è¿°äº†åŒ…æ‹¬ä»»åŠ¡æ³›åŒ– (task generalization)ã€éŸ§æ€§ (resilience)ã€å¯æ‰©å±•æ€§ (scalability) å’Œè‡ªç»„è£… (self-assembly) åœ¨å†…çš„å…³é”®å±æ€§ã€‚é€šè¿‡è¿™äº›å±æ€§çš„ç•Œå®šï¼Œç ”ç©¶æˆåŠŸå¼¥åˆäº†ç†è®ºåŸºç¡€ä¸å·¥ç¨‹åŒ–è‡ªé€‚åº”æ¶Œç°æ™ºèƒ½ (emergent intelligence) å®é™…æ–¹æ³•ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¥å·¥ä½œä¸ºç†è§£å’Œå®æ–½ CAI æä¾›äº†ç»“æ„åŒ–åŸºç¡€ï¼Œæ—¨åœ¨å¼•å¯¼ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…åœ¨å„ä¸ªé¢†åŸŸå¼€å‘å‡ºæ›´å…·éŸ§æ€§ã€å¯æ‰©å±•å’Œé€‚åº”æ€§çš„ AI ç³»ç»Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23153v2",
      "published_date": "2025-05-29 06:43:14 UTC",
      "updated_date": "2025-07-01 03:22:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:10:21.936788+00:00"
    },
    {
      "arxiv_id": "2505.23145v4",
      "title": "FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing",
      "title_zh": "FlowAlignï¼šè½¨è¿¹æ­£åˆ™åŒ–ã€å…åæ¼”çš„æµå¼å›¾åƒç¼–è¾‘",
      "authors": [
        "Jeongsol Kim",
        "Yeobin Hong",
        "Jonghyun Park",
        "Jong Chul Ye"
      ],
      "abstract": "Recent inversion-free, flow-based image editing methods such as FlowEdit leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3, enabling text-driven manipulation by solving an ordinary differential equation (ODE). While the lack of exact latent inversion is a core advantage of these methods, it often results in unstable editing trajectories and poor source consistency. To address this limitation, we propose {\\em FlowAlign}, a novel inversion-free flow-based framework for consistent image editing with optimal control-based trajectory control. Specifically, FlowAlign introduces source similarity at the terminal point as a regularization term to promote smoother and more consistent trajectories during the editing process. Notably, our terminal point regularization is shown to explicitly balance semantic alignment with the edit prompt and structural consistency with the source image along the trajectory. Furthermore, FlowAlign naturally supports reverse editing by simply reversing the ODE trajectory, highliting the reversible and consistent nature of the transformation. Extensive experiments demonstrate that FlowAlign outperforms existing methods in both source preservation and editing controllability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹FlowEditç­‰æ— åæ¼”(inversion-free)æµå¼å›¾åƒç¼–è¾‘æ–¹æ³•ä¸­å­˜åœ¨çš„è½¨è¿¹ä¸ç¨³å®šå’Œæºå›¾åƒä¸€è‡´æ€§å·®çš„é—®é¢˜ï¼Œæå‡ºäº†FlowAlignæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºæœ€ä¼˜æ§åˆ¶(optimal control)çš„è½¨è¿¹æ§åˆ¶æŠ€æœ¯ï¼Œé€šè¿‡åœ¨ç»ˆç‚¹å¼•å…¥æºå›¾åƒç›¸ä¼¼æ€§ä½œä¸ºæ­£åˆ™é¡¹(regularization term)ï¼Œç¡®ä¿äº†ç¼–è¾‘è¿‡ç¨‹ä¸­çš„å¹³æ»‘æ€§ä¸ä¸€è‡´æ€§ã€‚è¿™ç§ç»ˆç‚¹æ­£åˆ™åŒ–æ˜¾å¼åœ°å¹³è¡¡äº†ç¼–è¾‘æç¤ºè¯çš„è¯­ä¹‰å¯¹é½ä¸æºå›¾åƒçš„ç»“æ„ä¸€è‡´æ€§ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç¼–è¾‘è´¨é‡ã€‚æ­¤å¤–ï¼ŒFlowAlignæ”¯æŒé€šè¿‡ç®€å•åè½¬å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)è½¨è¿¹æ¥å®ç°é€†å‘ç¼–è¾‘ï¼Œå±•ç¤ºäº†è¯¥å˜æ¢è‰¯å¥½çš„å¯é€†æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlowAlignåœ¨ä¿ç•™æºå›¾åƒå†…å®¹å’Œå¢å¼ºç¼–è¾‘å¯æ§æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23145v4",
      "published_date": "2025-05-29 06:33:16 UTC",
      "updated_date": "2025-07-27 15:39:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:10:29.724552+00:00"
    },
    {
      "arxiv_id": "2505.23857v2",
      "title": "A Convolution and Attention Based Encoder for Reinforcement Learning under Partial Observability",
      "title_zh": "ä¸€ç§é¢å‘éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹å¼ºåŒ–å­¦ä¹ çš„å·ç§¯ä¸æ³¨æ„åŠ›æœºåˆ¶ç¼–ç å™¨",
      "authors": [
        "Wuhao Wang",
        "Zhiyong Chen"
      ],
      "abstract": "Partially Observable Markov Decision Processes (POMDPs) remain a core challenge in reinforcement learning due to incomplete state information. We address this by reformulating POMDPs as fully observable processes with fixed-length observation histories as augmented states. To efficiently encode these histories, we propose a lightweight temporal encoder based on depthwise separable convolution and self-attention, avoiding the overhead of recurrent and Transformer-based models. Integrated into an actor-critic framework, our method achieves superior performance on continuous control benchmarks under partial observability. More broadly, this work shows that lightweight temporal encoding can improve the scalability of AI systems under uncertainty. It advances the development of agents capable of reasoning robustly in real-world environments where information is incomplete or delayed.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ä¸­éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (POMDPs) å¸¦æ¥çš„çŠ¶æ€ä¿¡æ¯ä¸å®Œæ•´æŒ‘æˆ˜ï¼Œæå‡ºå°† POMDPs é‡æ–°è¡¨è¿°ä¸ºä»¥å›ºå®šé•¿åº¦è§‚æµ‹å†å²ä½œä¸ºå¢å¼ºçŠ¶æ€çš„å…¨å¯è§‚å¯Ÿè¿‡ç¨‹ã€‚ä¸ºäº†é«˜æ•ˆç¼–ç è¿™äº›å†å²ä¿¡æ¯ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç»“åˆæ·±åº¦å¯åˆ†ç¦»å·ç§¯ (depthwise separable convolution) ä¸è‡ªæ³¨æ„åŠ› (self-attention) çš„è½»é‡åŒ–æ—¶åºç¼–ç å™¨ï¼Œæœ‰æ•ˆè§„é¿äº†ä¼ ç»Ÿå¾ªç¯ç¥ç»ç½‘ç»œå’Œ Transformer æ¨¡å‹çš„é«˜æ˜‚è®¡ç®—å¼€é”€ã€‚è¯¥ç¼–ç å™¨è¢«é›†æˆè‡³ Actor-Critic æ¡†æ¶ä¸­ï¼Œåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿæ¡ä»¶ä¸‹çš„è¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤é¡¹å·¥ä½œè¯æ˜äº†è½»é‡åŒ–æ—¶åºç¼–ç åœ¨æå‡ AI ç³»ç»Ÿå¤„ç†ä¸ç¡®å®šæ€§æ—¶çš„å¯æ‰©å±•æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä½œç”¨ã€‚è¯¥ç ”ç©¶æ¨åŠ¨äº†èƒ½å¤Ÿåœ¨ä¿¡æ¯ä¸å…¨æˆ–å»¶è¿Ÿçš„ç°å®ç¯å¢ƒä¸­è¿›è¡Œç¨³å¥æ¨ç†çš„æ™ºèƒ½ä½“å¼€å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23857v2",
      "published_date": "2025-05-29 06:22:06 UTC",
      "updated_date": "2025-09-13 03:54:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:10:21.302826+00:00"
    },
    {
      "arxiv_id": "2505.23135v2",
      "title": "VERINA: Benchmarking Verifiable Code Generation",
      "title_zh": "VERINAï¼šå¯éªŒè¯ä»£ç ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Zhe Ye",
        "Zhengxu Yan",
        "Jingxuan He",
        "Timothe Kasriel",
        "Kaiyu Yang",
        "Dawn Song"
      ],
      "abstract": "Large language models (LLMs) are increasingly integrated in software development, but ensuring correctness in LLM-generated code remains challenging and often requires costly manual review. Verifiable code generation -- jointly generating code, specifications, and proofs of code-specification alignment -- offers a promising path to address this limitation and further unleash LLMs' benefits in coding. Yet, there exists a significant gap in evaluation: current benchmarks often focus on only individual components rather than providing a holistic evaluation framework of all tasks. In this paper, we introduce Verina (Verifiable Code Generation Arena), a high-quality benchmark enabling a comprehensive and modular evaluation of code, specification, and proof generation as well as their compositions. Verina consists of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Our extensive evaluation of state-of-the-art LLMs reveals significant challenges in verifiable code generation, especially in proof generation, underscoring the need for improving LLM-based theorem provers in verification domains. The best model, OpenAI o4-mini, achieves a 61.4\\% code correctness rate, 51.0\\% for specification soundness and completeness, and a mere 3.6\\% proof success rate (based on one trial per task). We hope Verina will catalyze progress in verifiable code generation by providing a rigorous and comprehensive benchmark. We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina and our evaluation code on https://github.com/sunblaze-ucb/verina.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†Verinaï¼ˆVerifiable Code Generation Arenaï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å…¨é¢è¯„ä¼°å¯éªŒè¯ä»£ç ç”Ÿæˆï¼ˆVerifiable code generationï¼‰çš„é«˜è´¨é‡åŸºå‡†æµ‹è¯•ã€‚å¯éªŒè¯ä»£ç ç”Ÿæˆé€šè¿‡åŒæ—¶ç”Ÿæˆä»£ç ã€å½¢å¼åŒ–è§„èŒƒï¼ˆspecificationsï¼‰ä»¥åŠä»£ç ä¸è§„èŒƒä¸€è‡´æ€§çš„è¯æ˜ï¼ˆproofsï¼‰ï¼Œä¸ºè§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»£ç ç”Ÿæˆçš„æ­£ç¡®æ€§é—®é¢˜æä¾›äº†æ–°è·¯å¾„ã€‚VerinaåŒ…å«189ä¸ªåŸºäºLeanè¯­è¨€çš„æ‰‹åŠ¨ç²¾é€‰ç¼–ç¨‹ä»»åŠ¡ï¼Œæ¶µç›–äº†è¯¦ç»†çš„é—®é¢˜æè¿°ã€å‚è€ƒå®ç°ã€å½¢å¼åŒ–è§„èŒƒå’Œå¹¿æ³›çš„æµ‹è¯•å¥—ä»¶ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œç›®å‰çš„å…ˆè¿›æ¨¡å‹åœ¨è¯æ˜ç”Ÿæˆï¼ˆproof generationï¼‰æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå…¶ä¸­è¡¨ç°æœ€ä½³çš„OpenAI o4-miniåœ¨ä»£ç æ­£ç¡®ç‡ä¸Šè¾¾åˆ°61.4%ï¼Œä½†åœ¨è¯æ˜æˆåŠŸç‡ä¸Šä»…ä¸º3.6%ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºæ¨¡å—åŒ–è¯„ä¼°ä»£ç ã€è§„èŒƒå’Œè¯æ˜çš„ç”ŸæˆåŠå…¶ç»„åˆæä¾›äº†ä¸¥è°¨æ¡†æ¶ï¼Œæ—¨åœ¨æ¨åŠ¨LLMsåœ¨å½¢å¼åŒ–éªŒè¯é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23135v2",
      "published_date": "2025-05-29 06:12:52 UTC",
      "updated_date": "2025-10-18 02:58:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:10:32.431379+00:00"
    },
    {
      "arxiv_id": "2505.23134v1",
      "title": "Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing",
      "title_zh": "Zero-to-Heroï¼šé›¶æ ·æœ¬åˆå§‹åŒ–èµ‹èƒ½çš„åŸºäºå‚è€ƒçš„è§†é¢‘å¤–è§‚ç¼–è¾‘",
      "authors": [
        "Tongtong Su",
        "Chengyu Wang",
        "Jun Huang",
        "Dongming Lu"
      ],
      "abstract": "Appearance editing according to user needs is a pivotal task in video editing. Existing text-guided methods often lead to ambiguities regarding user intentions and restrict fine-grained control over editing specific aspects of objects. To overcome these limitations, this paper introduces a novel approach named {Zero-to-Hero}, which focuses on reference-based video editing that disentangles the editing process into two distinct problems. It achieves this by first editing an anchor frame to satisfy user requirements as a reference image and then consistently propagating its appearance across other frames. We leverage correspondence within the original frames to guide the attention mechanism, which is more robust than previously proposed optical flow or temporal modules in memory-friendly video generative models, especially when dealing with objects exhibiting large motions. It offers a solid ZERO-shot initialization that ensures both accuracy and temporal consistency. However, intervention in the attention mechanism results in compounded imaging degradation with over-saturated colors and unknown blurring issues. Starting from Zero-Stage, our Hero-Stage Holistically learns a conditional generative model for vidEo RestOration. To accurately evaluate the consistency of the appearance, we construct a set of videos with multiple appearances using Blender, enabling a fine-grained and deterministic evaluation. Our method outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The project page is at https://github.com/Tonniia/Zero2Hero.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Zero-to-Heroæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬å¼•å¯¼è§†é¢‘ç¼–è¾‘ä¸­å­˜åœ¨çš„æ„å›¾æ¨¡ç³Šå’Œç»†ç²’åº¦æ§åˆ¶å—é™é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†è§†é¢‘å¤–è§‚ç¼–è¾‘è§£è€¦ä¸ºä¸¤ä¸ªæ ¸å¿ƒé˜¶æ®µï¼šé¦–å…ˆå°†è§†é¢‘çš„é”šç‚¹å¸§ï¼ˆanchor frameï¼‰ç¼–è¾‘ä¸ºç¬¦åˆç”¨æˆ·éœ€æ±‚çš„å‚è€ƒå›¾åƒï¼Œéšåå°†è¯¥å¤–è§‚ä¸€è‡´åœ°ä¼ æ’­è‡³å…¶ä»–è§†é¢‘å¸§ã€‚é€šè¿‡åˆ©ç”¨åŸå§‹å¸§å†…çš„å¯¹åº”å…³ç³»æ¥æŒ‡å¯¼æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention mechanismï¼‰ï¼Œè¯¥æ–¹æ³•å®ç°äº†é²æ£’çš„é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰åˆå§‹åŒ–ï¼Œå°¤å…¶åœ¨å¤„ç†å…·æœ‰å¤§å¹…åº¦è¿åŠ¨çš„ç‰©ä½“æ—¶è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å…‰æµæˆ–æ—¶é—´æ¨¡å—ã€‚é’ˆå¯¹æ³¨æ„åŠ›æœºåˆ¶å¹²é¢„å¯èƒ½å¯¼è‡´çš„è‰²å½©è¿‡é¥±å’ŒåŠæ¨¡ç³Šç­‰å›¾åƒé€€åŒ–é—®é¢˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†åä¸ºHero-Stageçš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹è¿›è¡Œæ•´ä½“è§†é¢‘ä¿®å¤ï¼ˆvideo restorationï¼‰ã€‚ä¸ºäº†è¿›è¡Œç²¾ç¡®è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨Blenderæ„å»ºäº†ä¸€å¥—å…·æœ‰å¤šç§å¤–è§‚çš„è§†é¢‘æ•°æ®é›†ï¼Œå®ç°äº†ç»†ç²’åº¦ä¸”ç¡®å®šæ€§çš„æ€§èƒ½è¡¡é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZero-to-Heroåœ¨PSNRæŒ‡æ ‡ä¸Šæ¯”ç›®å‰è¡¨ç°æœ€å¥½çš„åŸºå‡†æ¨¡å‹æå‡äº†2.6 dBï¼Œæ˜¾è‘—å¢å¼ºäº†è§†é¢‘ç¼–è¾‘çš„å‡†ç¡®æ€§ä¸æ—¶åºä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23134v1",
      "published_date": "2025-05-29 06:10:16 UTC",
      "updated_date": "2025-05-29 06:10:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:10:44.702273+00:00"
    },
    {
      "arxiv_id": "2505.23132v1",
      "title": "Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone",
      "title_zh": "åŸºäºæ‰‹æœºçš„æ‚£è€…åŸŸæœ‰ç›‘ç£å¯¹æ¯”å­¦ä¹ è‚ºéŸ³åˆ†ç±»",
      "authors": [
        "Seung Gyu Jeong",
        "Seong Eun Kim"
      ],
      "abstract": "Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic has revealed the limitations of traditional, in-person lung sound assessments. To overcome these issues, advancements in digital stethoscopes and artificial intelligence (AI) have led to the development of new diagnostic methods. In this context, our study aims to use smartphone microphones to record and analyze lung sounds. We faced two major challenges: the difference in audio style between electronic stethoscopes and smartphone microphones, and the variability among patients. To address these challenges, we developed a method called Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating this method with the Audio Spectrogram Transformer (AST) model, we significantly improved its performance by 2.4\\% compared to the original AST model. This progress demonstrates that smartphones can effectively diagnose lung sounds, addressing inconsistencies in patient data and showing potential for broad use beyond traditional clinical settings. Our research contributes to making lung disease detection more accessible in the post-COVID-19 world.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ—¨åœ¨åˆ©ç”¨æ™ºèƒ½æ‰‹æœºéº¦å…‹é£è®°å½•å’Œåˆ†æè‚ºéƒ¨å£°éŸ³ï¼Œä»¥åº”å¯¹ä¼ ç»Ÿå¬è¯Šæ–¹å¼åœ¨åç–«æƒ…æ—¶ä»£çš„å±€é™æ€§ã€‚ç ”ç©¶ä¸»è¦é¢ä¸´ç”µå­å¬è¯Šå™¨ä¸æ™ºèƒ½æ‰‹æœºéº¦å…‹é£ä¹‹é—´çš„éŸ³é¢‘é£æ ¼å·®å¼‚ï¼Œä»¥åŠä¸åŒæ‚£è€…ä¸ªä½“é—´çš„å˜å¼‚æ€§è¿™ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†ä¸€ç§åä¸º Patient Domain Supervised Contrastive Learning (PD-SCL) çš„æ–¹æ³•ï¼Œå¹¶å°†å…¶é›†æˆåˆ° Audio Spectrogram Transformer (AST) æ¨¡å‹ä¸­ã€‚é€šè¿‡è¿™ç§ç›‘ç£å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†æ‚£è€…æ•°æ®çš„ä¸ä¸€è‡´æ€§ï¼Œä½¿æ¨¡å‹æ€§èƒ½æ¯”åŸå§‹ AST æ¨¡å‹æå‡äº† 2.4%ã€‚è¯¥è¿›å±•è¯æ˜äº†æ™ºèƒ½æ‰‹æœºåœ¨æœ‰æ•ˆè¯Šæ–­è‚ºéƒ¨å£°éŸ³æ–¹é¢çš„å¯è¡Œæ€§ï¼Œä¸ºä¼ ç»Ÿä¸´åºŠç¯å¢ƒä¹‹å¤–çš„è‚ºéƒ¨ç–¾ç—…æ£€æµ‹æä¾›äº†æ›´å…·å¯åŠæ€§çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "ITS-CSCC 2024",
      "pdf_url": "https://arxiv.org/pdf/2505.23132v1",
      "published_date": "2025-05-29 06:08:05 UTC",
      "updated_date": "2025-05-29 06:08:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:10:41.109567+00:00"
    },
    {
      "arxiv_id": "2505.23121v2",
      "title": "ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations",
      "title_zh": "ContextQFormerï¼šä¸€ç§é¢å‘å¤šè½®å¤šæ¨¡æ€å¯¹è¯çš„æ–°å‹ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹æ³•",
      "authors": [
        "Yiming Lei",
        "Zhizheng Yang",
        "Zeming Liu",
        "Haitao Leng",
        "Shaoguo Liu",
        "Tingting Gao",
        "Qingjie Liu",
        "Yunhong Wang"
      ],
      "abstract": "Multi-modal large language models have demonstrated remarkable zero-shot abilities and powerful image-understanding capabilities. However, the existing open-source multi-modal models suffer from the weak capability of multi-turn interaction, especially for long contexts. To address the issue, we first introduce a context modeling module, termed ContextQFormer, which utilizes a memory block to enhance the presentation of contextual information. Furthermore, to facilitate further research, we carefully build a new multi-turn multi-modal dialogue dataset (TMDialog) for pre-training, instruction-tuning, and evaluation, which will be open-sourced lately. Compared with other multi-modal dialogue datasets, TMDialog contains longer conversations, which supports the research of multi-turn multi-modal dialogue. In addition, ContextQFormer is compared with three baselines on TMDialog and experimental results illustrate that ContextQFormer achieves an improvement of 2%-4% in available rate over baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®äº¤äº’ (Multi-turn Interaction) å°¤å…¶æ˜¯é•¿ä¸Šä¸‹æ–‡ (Long Contexts) åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º ContextQFormer çš„ä¸Šä¸‹æ–‡å»ºæ¨¡æ¨¡å—ã€‚è¯¥æ¨¡å—é€šè¿‡å¼•å…¥è®°å¿†æ¨¡å— (Memory Block) æ¥å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¡¨å¾èƒ½åŠ›ï¼Œä»è€Œä¼˜åŒ–å¤šè½®å¯¹è¯çš„å¤„ç†æ•ˆæœã€‚ä¸ºäº†è¿›ä¸€æ­¥æ¨åŠ¨ç›¸å…³ç ”ç©¶ï¼Œå›¢é˜Ÿæ„å»ºäº†å…¨æ–°çš„å¤šè½®å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›† TMDialogï¼Œè¯¥æ•°æ®é›†ç›¸æ¯”ç°æœ‰æ•°æ®é›†åŒ…å«æ›´é•¿çš„å¯¹è¯åºåˆ—ï¼Œæ”¯æŒä»é¢„è®­ç»ƒåˆ°è¯„ä¼°çš„å…¨æµç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒContextQFormer åœ¨ TMDialog æ•°æ®é›†ä¸Šç›¸æ¯”ä¸‰ç±»åŸºçº¿æ¨¡å‹åœ¨å¯ç”¨ç‡ (Available Rate) ä¸Šå®ç°äº† 2% è‡³ 4% çš„æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¤šè½®å¯¹è¯ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23121v2",
      "published_date": "2025-05-29 05:41:26 UTC",
      "updated_date": "2025-07-17 16:49:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:11:15.304674+00:00"
    },
    {
      "arxiv_id": "2505.23118v3",
      "title": "Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios",
      "title_zh": "å¼•å¯¼ä¸å¢å¼ºï¼šæ¨è¿›åŒ»ç–—åœºæ™¯ä¸‹çš„å¤šæ¨¡æ€æ¨ç†",
      "authors": [
        "Zhongzhen Huang",
        "Linjie Mu",
        "Yakun Zhu",
        "Xiangyu Zhao",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Effective clinical decision-making depends on iterative, multimodal reasoning across diverse sources of evidence. The recent emergence of multimodal reasoning models has significantly transformed the landscape of solving complex tasks. Although such models have achieved notable success in mathematics and science, their application to medical domains remains underexplored. In this work, we propose \\textit{MedE$^2$}, a two-stage post-training pipeline that elicits and then enhances multimodal reasoning for medical domains. In Stage-I, we fine-tune models using 2,000 text-only data samples containing precisely orchestrated reasoning demonstrations to elicit reasoning behaviors. In Stage-II, we further enhance the model's reasoning capabilities using 1,500 rigorously curated multimodal medical cases, aligning model reasoning outputs with our proposed multimodal medical reasoning preference. Extensive experiments demonstrate the efficacy and reliability of \\textit{MedE$^2$} in improving the reasoning performance of medical multimodal models. Notably, models trained with \\textit{MedE$^2$} consistently outperform baselines across multiple medical multimodal benchmarks. Additional validation on larger models and under inference-time scaling further confirms the robustness and practical utility of our approach.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† MedE$^2$ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡åŒ»ç–—åœºæ™¯ä¸‹å¤šæ¨¡æ€æ¨ç†ï¼ˆmultimodal reasoningï¼‰èƒ½åŠ›çš„ä¸¤é˜¶æ®µåè®­ç»ƒï¼ˆpost-trainingï¼‰æµæ°´çº¿ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ˆStage-Iï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ 2,000 æ¡åŒ…å«ç²¾ç¡®ç¼–æ’æ¨ç†æ¼”ç¤ºçš„çº¯æ–‡æœ¬æ•°æ®ï¼ˆtext-only dataï¼‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ¿€å‘å…¶æ¨ç†è¡Œä¸ºã€‚ç¬¬äºŒé˜¶æ®µï¼ˆStage-IIï¼‰åˆ™é€šè¿‡ 1,500 ä¸ªä¸¥æ ¼ç­›é€‰çš„å¤šæ¨¡æ€åŒ»ç–—ç—…ä¾‹è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼Œä½¿å…¶æ¨ç†è¾“å‡ºç¬¦åˆç‰¹å®šçš„å¤šæ¨¡æ€åŒ»ç–—æ¨ç†åå¥½ï¼ˆmultimodal medical reasoning preferenceï¼‰ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMedE$^2$ åœ¨å¤šä¸ªåŒ»ç–—å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼ˆbenchmarksï¼‰ä¸­å‡æ˜¾è‘—ä¸”æŒç»­åœ°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸å¯é æ€§ã€‚æ­¤å¤–ï¼Œåœ¨æ›´å¤§è§„æ¨¡æ¨¡å‹ä»¥åŠæ¨ç†æ—¶é—´æ‰©å±•ï¼ˆinference-time scalingï¼‰ä¸‹çš„éªŒè¯è¿›ä¸€æ­¥è¯å®äº†è¯¥æ–¹æ¡ˆçš„ç¨³å¥æ€§ï¼ˆrobustnessï¼‰å’Œå®é™…ä¸´åºŠå†³ç­–æ”¯æŒä¸­çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23118v3",
      "published_date": "2025-05-29 05:39:23 UTC",
      "updated_date": "2025-11-02 14:25:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:11:36.616072+00:00"
    },
    {
      "arxiv_id": "2505.23117v2",
      "title": "Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking",
      "title_zh": "Decom-Renorm-Mergeï¼šåœ¨æ­£ç¡®çš„ç©ºé—´è¿›è¡Œæ¨¡å‹åˆå¹¶ä»¥æå‡å¤šä»»åŠ¡æ€§èƒ½",
      "authors": [
        "Yuatyong Chaichana",
        "Thanapat Trachu",
        "Peerat Limkonchotiwat",
        "Konpat Preechakul",
        "Tirasan Khandhawit",
        "Ekapol Chuangsuwanich"
      ],
      "abstract": "In the era of large-scale training, model merging has evolved into a tool for creating multitasking models efficiently. It enables the knowledge of models to be fused, without the need for heavy computation as required in traditional multitask learning. Existing merging methods often assume that entries at identical positions in weight matrices serve the same function, enabling straightforward entry-wise comparison and merging. However, this assumption overlooks the complexity of finetuned neural networks, where neurons may develop distinct feature compositions, making direct entry-wise merging problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective approach that leverages Singular Value Decomposition to decompose and coordinate weight matrices into an aligned joint space, where entry-wise merging becomes possible. We showcase the effectiveness of DRM across various settings ranging from smaller encoder-based such as ViT and DeBERTa, encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B. Our experimental results show that DRM outperforms several state-of-the-art merging techniques across full finetuning and low-rank adaptation settings. Moreover, our analysis reveals renormalization as the crucial component for creating a robust and even joint space for merging, significantly contributing to the method's performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šä»»åŠ¡æ¨¡å‹åˆå¹¶ä¸­çš„æƒé‡å¯¹é½é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„é€æ¡ç›®åˆå¹¶ (entry-wise merging) å¿½ç•¥äº†å¾®è°ƒç½‘ç»œä¸­ç¥ç»å…ƒå¯èƒ½å…·æœ‰çš„ä¸åŒç‰¹å¾ç»„æˆã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Decom-Renorm-Merge (DRM)ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ (Singular Value Decomposition) å°†æƒé‡çŸ©é˜µåˆ†è§£å¹¶åè°ƒåˆ°ä¸€ä¸ªå¯¹é½çš„è”åˆç©ºé—´ä¸­ï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„æƒé‡èåˆã€‚å®éªŒåœ¨ ViTã€DeBERTaã€T5 ä»¥åŠ Llama3.1-8B ç­‰å¤šç§æ¶æ„ä¸ŠéªŒè¯äº† DRM çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨å…¨é‡å¾®è°ƒ (full finetuning) å’Œä½ç§©è‡ªé€‚åº” (LoRA) è®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›åˆå¹¶æŠ€æœ¯ã€‚åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œé‡å½’ä¸€åŒ– (renormalization) æ˜¯åˆ›å»ºé²æ£’ä¸”å‡åŒ€çš„è”åˆç©ºé—´çš„å…³é”®ç»„ä»¶ï¼Œå¯¹æå‡å¤šä»»åŠ¡æ¨¡å‹åˆå¹¶æ€§èƒ½èµ·åˆ°äº†å†³å®šæ€§ä½œç”¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Code and models are available at https://github.com/yophis/decom-renorm-merge",
      "pdf_url": "https://arxiv.org/pdf/2505.23117v2",
      "published_date": "2025-05-29 05:37:53 UTC",
      "updated_date": "2025-10-29 15:34:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:11:47.760163+00:00"
    },
    {
      "arxiv_id": "2505.23116v1",
      "title": "CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables",
      "title_zh": "CrossLinearï¼šé¢å‘å¸¦å¤–ç”Ÿå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹çš„å³æ’å³ç”¨äº’ç›¸å…³åµŒå…¥",
      "authors": [
        "Pengfei Zhou",
        "Yunlong Liu",
        "Junli Liang",
        "Qi Song",
        "Xiangyang Li"
      ],
      "abstract": "Time series forecasting with exogenous variables is a critical emerging paradigm that presents unique challenges in modeling dependencies between variables. Traditional models often struggle to differentiate between endogenous and exogenous variables, leading to inefficiencies and overfitting. In this paper, we introduce CrossLinear, a novel Linear-based forecasting model that addresses these challenges by incorporating a plug-and-play cross-correlation embedding module. This lightweight module captures the dependencies between variables with minimal computational cost and seamlessly integrates into existing neural networks. Specifically, it captures time-invariant and direct variable dependencies while disregarding time-varying or indirect dependencies, thereby mitigating the risk of overfitting in dependency modeling and contributing to consistent performance improvements. Furthermore, CrossLinear employs patch-wise processing and a global linear head to effectively capture both short-term and long-term temporal dependencies, further improving its forecasting precision. Extensive experiments on 12 real-world datasets demonstrate that CrossLinear achieves superior performance in both short-term and long-term forecasting tasks. The ablation study underscores the effectiveness of the cross-correlation embedding module. Additionally, the generalizability of this module makes it a valuable plug-in for various forecasting tasks across different domains. Codes are available at https://github.com/mumiao2000/CrossLinear.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸¦æœ‰å¤–ç”Ÿå˜é‡(Exogenous Variables)çš„æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å˜é‡é—´ä¾èµ–å…³ç³»å»ºæ¨¡éš¾ã€æ˜“è¿‡æ‹Ÿåˆç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†CrossLinearæ¨¡å‹ã€‚CrossLinearå¼•å…¥äº†ä¸€ä¸ªå³æ’å³ç”¨çš„äº’ç›¸å…³åµŒå…¥(Cross-Correlation Embedding)æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½ä»¥æä½çš„è®¡ç®—æˆæœ¬æ•è·å˜é‡é—´çš„å…³é”®ä¾èµ–å…³ç³»ã€‚è¯¥æ¨¡å—ä¸“é—¨æ•æ‰æ—¶ä¸å˜(Time-invariant)å’Œç›´æ¥çš„å˜é‡ä¾èµ–ï¼ŒåŒæ—¶å¿½ç•¥æ—¶å˜æˆ–é—´æ¥ä¾èµ–ï¼Œä»è€Œæœ‰æ•ˆé™ä½äº†ä¾èµ–å»ºæ¨¡ä¸­çš„è¿‡æ‹Ÿåˆé£é™©ã€‚æ­¤å¤–ï¼ŒCrossLinearé‡‡ç”¨åˆ†å—å¤„ç†(Patch-wise processing)å’Œå…¨å±€çº¿æ€§å¤´(Global linear head)æ¥åŒæ—¶æ•è·çŸ­æœŸå’Œé•¿æœŸçš„æ—¶ç©ºä¾èµ–ï¼Œè¿›ä¸€æ­¥æå‡é¢„æµ‹ç²¾åº¦ã€‚åœ¨12ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCrossLinearåœ¨çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†è¯¥æ¨¡å—çš„æœ‰æ•ˆæ€§åŠå…¶ä½œä¸ºé€šç”¨æ’ä»¶åœ¨ä¸åŒé¢†åŸŸé¢„æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23116v1",
      "published_date": "2025-05-29 05:34:40 UTC",
      "updated_date": "2025-05-29 05:34:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:11:37.699516+00:00"
    },
    {
      "arxiv_id": "2505.23856v2",
      "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities",
      "title_zh": "OMNIGUARDï¼šä¸€ç§é«˜æ•ˆçš„è·¨è¯­è¨€ä¸è·¨æ¨¡æ€äººå·¥æ™ºèƒ½å®‰å…¨å®¡æ ¸æ–¹æ³•",
      "authors": [
        "Sahil Verma",
        "Keegan Hines",
        "Jeff Bilmes",
        "Charlotte Siska",
        "Luke Zettlemoyer",
        "Hila Gonen",
        "Chandan Singh"
      ],
      "abstract": "The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose Omniguard, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. Omniguard improves harmful prompt classification accuracy by 11.57\\% over the strongest baseline in a multilingual setting, by 20.44\\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, Omniguard is also very efficient ($\\approx\\!120 \\times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Omniguardï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œå¤šæ¨¡æ€å¤§æ¨¡å‹(MLLMs)è·¨è¯­è¨€ã€è·¨æ¨¡æ€å®‰å…¨å®¡æŸ¥çš„é«˜æ•ˆæ–¹æ³•ï¼Œæ—¨åœ¨åº”å¯¹ä½èµ„æºè¯­è¨€åŠéæ–‡æœ¬æ¨¡æ€ï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å¸¦æ¥çš„å®‰å…¨æ”»å‡»æŒ‘æˆ˜ã€‚Omniguardçš„æ ¸å¿ƒåœ¨äºè¯†åˆ«æ¨¡å‹å†…éƒ¨åœ¨ä¸åŒè¯­è¨€æˆ–æ¨¡æ€é—´å¯¹é½çš„Internal Representationsï¼Œå¹¶ä»¥æ­¤æ„å»ºè¯­è¨€æ— å…³æˆ–æ¨¡æ€æ— å…³çš„åˆ†ç±»å™¨æ¥æ£€æµ‹æœ‰å®³æç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹ï¼ŒOmniguardçš„æœ‰å®³æç¤ºåˆ†ç±»å‡†ç¡®ç‡æ¯”æœ€å¼ºåŸºçº¿æé«˜äº†11.57%ï¼Œåœ¨å›¾åƒæç¤ºä¸‹æé«˜äº†20.44%ï¼Œå¹¶ä¸ºéŸ³é¢‘æç¤ºè®¾å®šäº†æ–°çš„SOTAã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤ç”¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„Embeddingsï¼ŒOmniguardçš„è¿è¡Œæ•ˆç‡æ¯”æ¬¡å¿«çš„åŸºçº¿æ¨¡å‹å¿«çº¦120å€ã€‚è¯¥ç ”ç©¶é€šè¿‡è·¨æ¨¡æ€å¯¹é½çš„è¡¨å¾æ˜¾è‘—æå‡äº†AIå®‰å…¨é˜²å¾¡çš„é²æ£’æ€§ä¸å®æ—¶å¤„ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23856v2",
      "published_date": "2025-05-29 05:25:27 UTC",
      "updated_date": "2025-12-09 00:08:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:11:50.162704+00:00"
    },
    {
      "arxiv_id": "2505.23107v1",
      "title": "EAD: An EEG Adapter for Automated Classification",
      "title_zh": "EADï¼šé¢å‘è‡ªåŠ¨åˆ†ç±»çš„ EEG é€‚é…å™¨",
      "authors": [
        "Pushapdeep Singh",
        "Jyoti Nigam",
        "Medicherla Vamsi Krishna",
        "Arnav Bhavsar",
        "Aditya Nigam"
      ],
      "abstract": "While electroencephalography (EEG) has been a popular modality for neural decoding, it often involves task specific acquisition of the EEG data. This poses challenges for the development of a unified pipeline to learn embeddings for various EEG signal classification, which is often involved in various decoding tasks. Traditionally, EEG classification involves the step of signal preprocessing and the use of deep learning techniques, which are highly dependent on the number of EEG channels in each sample. However, the same pipeline cannot be applied even if the EEG data is collected for the same experiment but with different acquisition devices. This necessitates the development of a framework for learning EEG embeddings, which could be highly beneficial for tasks involving multiple EEG samples for the same task but with varying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a flexible framework compatible with any signal acquisition device. More specifically, we leverage a recent EEG foundational model with significant adaptations to learn robust representations from the EEG data for the classification task. We evaluate EAD on two publicly available datasets achieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and BrainLat respectively. This illustrates the effectiveness of the proposed framework across diverse EEG datasets containing two different perception tasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG classification on EEG-ImageNet task to demonstrate the generalization capability of the proposed approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º EEG Adapter (EAD) çš„çµæ´»æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè„‘ç”µå›¾ (EEG) åˆ†ç±»æ–¹æ³•ä¸­ç”±äºä¸åŒé‡‡é›†è®¾å¤‡å’Œé€šé“æ•°é‡å·®å¼‚å¯¼è‡´çš„æ¨¡å‹æ³›åŒ–éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨æœ€è¿‘çš„ EEG åŸºç¡€æ¨¡å‹ (foundational model) å¹¶è¿›è¡Œé’ˆå¯¹æ€§æ”¹è¿›ï¼Œèƒ½å¤Ÿä» EEG æ•°æ®ä¸­å­¦ä¹ é²æ£’çš„ç‰¹å¾è¡¨ç¤º (representations)ï¼Œä½¿å…¶å¯ä»¥æ— ç¼å…¼å®¹å„ç§ä¿¡å·é‡‡é›†è®¾å¤‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEAD åœ¨ EEG-ImageNet å’Œ BrainLat ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 99.33% å’Œ 92.31% çš„å…ˆè¿›å‡†ç¡®ç‡ (state-of-the-art)ï¼ŒéªŒè¯äº†å…¶åœ¨åˆºæ¿€è¯±å‘ (stimulus) å’Œé™æ¯æ€ (resting-state) ç­‰å¤šå…ƒæ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡åœ¨ EEG-ImageNet ä¸Šçš„é›¶æ ·æœ¬ (zero-shot) åˆ†ç±»å®éªŒï¼Œå……åˆ†å±•ç¤ºäº†è¯¥æ–¹æ³•å‡ºè‰²çš„é€šç”¨åŒ–ä¸æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºè·¨è®¾å¤‡çš„ç»Ÿä¸€ EEG ä¿¡å·è§£ç æµæ°´çº¿æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23107v1",
      "published_date": "2025-05-29 05:21:06 UTC",
      "updated_date": "2025-05-29 05:21:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:11:47.600747+00:00"
    },
    {
      "arxiv_id": "2505.23091v3",
      "title": "Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models",
      "title_zh": "Infi-MMRï¼šåŸºäºè¯¾ç¨‹å­¦ä¹ ï¼Œé€šè¿‡åˆ†é˜¶æ®µå¼ºåŒ–å­¦ä¹ è§£é”å¤šæ¨¡æ€å°è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›",
      "authors": [
        "Zeyu Liu",
        "Yuhang Liu",
        "Guanghao Zhu",
        "Congkai Xie",
        "Zhen Li",
        "Jianbo Yuan",
        "Xinyao Wang",
        "Qing Li",
        "Shing-Chi Cheung",
        "Shengyu Zhang",
        "Fei Wu",
        "Hongxia Yang"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the model's logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini). Resources are available at https://huggingface.co/Reallm-Labs/Infi-MMR-3B.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å°è¯­è¨€æ¨¡å‹(MSLMs)åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢é¢ä¸´çš„é«˜è´¨é‡æ•°æ®ç¨€ç¼ºã€è§†è§‰å¤„ç†å¯¼è‡´çš„æ€§èƒ½é€€åŒ–ä»¥åŠå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¯èƒ½äº§ç”Ÿé”™è¯¯æ¨ç†è¿‡ç¨‹ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†Infi-MMRæ¡†æ¶åŠå¯¹åº”çš„Infi-MMR-3Bæ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ (Curriculum-based learning)ç­–ç•¥ç³»ç»Ÿåœ°è§£é”æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚ç¬¬ä¸€é˜¶æ®µä¸ºåŸºç¡€æ¨ç†æ¿€æ´»(Foundational Reasoning Activation)ï¼Œåˆ©ç”¨é«˜è´¨é‡æ–‡æœ¬æ¨ç†æ•°æ®å¼ºåŒ–é€»è¾‘åŸºç¡€ï¼›ç¬¬äºŒé˜¶æ®µä¸ºè·¨æ¨¡æ€æ¨ç†é€‚é…(Cross-Modal Reasoning Adaptation)ï¼Œåˆ©ç”¨æè¿°å¢å¼º(Caption-augmented)çš„å¤šæ¨¡æ€æ•°æ®å®ç°æŠ€èƒ½è¿ç§»ï¼›ç¬¬ä¸‰é˜¶æ®µä¸ºå¤šæ¨¡æ€æ¨ç†å¢å¼º(Multimodal Reasoning Enhancement)ï¼Œé€šè¿‡ç²¾é€‰çš„æ— æè¿°å¤šæ¨¡æ€æ•°æ®å‡è½»è¯­è¨€åå·®å¹¶æå‡è·¨æ¨¡æ€æ¨ç†çš„é²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInfi-MMR-3Båœ¨MathVerseã€MathVisionã€OlympiadBenchå’ŒMathVistaç­‰å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥åˆ†é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æå‡å°æ¨¡å‹å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23091v3",
      "published_date": "2025-05-29 04:51:56 UTC",
      "updated_date": "2025-06-23 08:47:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:11.778828+00:00"
    },
    {
      "arxiv_id": "2505.23086v3",
      "title": "Equivariant Spherical Transformer for Efficient Molecular Modeling",
      "title_zh": "ç”¨äºé«˜æ•ˆåˆ†å­å»ºæ¨¡çš„ç­‰å˜çƒé¢ Transformer",
      "authors": [
        "Junyi An",
        "Xinyu Lu",
        "Chao Qu",
        "Yunfei Shi",
        "Peijia Lin",
        "Qianwei Tang",
        "Licheng Xu",
        "Fenglei Cao",
        "Yuan Qi"
      ],
      "abstract": "Equivariant Graph Neural Networks (GNNs) have significantly advanced the modeling of 3D molecular structure by leveraging group representations. However, their message passing, heavily relying on Clebsch-Gordan tensor product convolutions, suffers from restricted expressiveness due to the limited non-linearity and low degree of group representations. To overcome this, we introduce the Equivariant Spherical Transformer (EST), a novel plug-and-play framework that applies a Transformer-like architecture to the Fourier spatial domain of group representations. EST achieves higher expressiveness than conventional models while preserving the crucial equivariant inductive bias through a uniform sampling strategy of spherical Fourier transforms. As demonstrated by our experiments on challenging benchmarks like OC20 and QM9, EST-based models achieve state-of-the-art performance. For the complex molecular systems within OC20, small models empowered by EST can outperform some larger models and those using additional data. In addition to demonstrating such strong expressiveness,we provide both theoretical and experimental validation of EST's equivariance as well, paving the way for new research in this area.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿ Equivariant Graph Neural Networks (GNNs) åœ¨ 3D åˆ†å­å»ºæ¨¡ä¸­å› è¿‡åº¦ä¾èµ– Clebsch-Gordan tensor product å·ç§¯è€Œå¯¼è‡´çš„è¡¨è¾¾èƒ½åŠ›å—é™é—®é¢˜ï¼Œæå‡ºäº† Equivariant Spherical Transformer (EST) æ¡†æ¶ã€‚EST æ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ¶æ„ï¼Œå®ƒåˆ›æ–°æ€§åœ°å°†ç±» Transformer æœºåˆ¶åº”ç”¨äºç¾¤è¡¨ç¤ºçš„ Fourier ç©ºé—´åŸŸï¼Œå¹¶é€šè¿‡çƒå½¢ Fourier å˜æ¢çš„å‡åŒ€é‡‡æ ·ç­–ç•¥ç¡®ä¿äº†ä¸¥æ ¼çš„ Equivariant å½’çº³åç½®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäº EST çš„æ¨¡å‹åœ¨ OC20 å’Œ QM9 ç­‰åˆ†å­å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç† OC20 çš„å¤æ‚ç³»ç»Ÿæ—¶ï¼Œå°å‹ EST æ¨¡å‹ç”šè‡³è¶…è¶Šäº†éƒ¨åˆ†å¤§å‹æ¨¡å‹å’Œåˆ©ç”¨é¢å¤–æ•°æ®çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜é€šè¿‡ç†è®ºå’Œå®éªŒåŒé‡éªŒè¯äº† EST çš„ç­‰å˜æ€§ã€‚è¯¥å·¥ä½œä¸ä»…æå‡äº† 3D å‡ ä½•å­¦ä¹ çš„è¡¨è¾¾æ•ˆç‡ï¼Œä¹Ÿä¸ºè¯¥é¢†åŸŸæœªæ¥çš„ Equivariant æ¨¡å‹ç ”ç©¶æä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23086v3",
      "published_date": "2025-05-29 04:43:07 UTC",
      "updated_date": "2025-09-28 03:04:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:11:59.020500+00:00"
    },
    {
      "arxiv_id": "2505.23085v1",
      "title": "GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion",
      "title_zh": "GeoManï¼šåŸºäºå›¾åƒåˆ°è§†é¢‘æ‰©æ•£çš„æ—¶åºä¸€è‡´äººä½“å‡ ä½•ä¼°è®¡",
      "authors": [
        "Gwanghyun Kim",
        "Xueting Li",
        "Ye Yuan",
        "Koki Nagano",
        "Tianye Li",
        "Jan Kautz",
        "Se Young Chun",
        "Umar Iqbal"
      ],
      "abstract": "Estimating accurate and temporally consistent 3D human geometry from videos is a challenging problem in computer vision. Existing methods, primarily optimized for single images, often suffer from temporal inconsistencies and fail to capture fine-grained dynamic details. To address these limitations, we present GeoMan, a novel architecture designed to produce accurate and temporally consistent depth and normal estimations from monocular human videos. GeoMan addresses two key challenges: the scarcity of high-quality 4D training data and the need for metric depth estimation to accurately model human size. To overcome the first challenge, GeoMan employs an image-based model to estimate depth and normals for the first frame of a video, which then conditions a video diffusion model, reframing video geometry estimation task as an image-to-video generation problem. This design offloads the heavy lifting of geometric estimation to the image model and simplifies the video model's role to focus on intricate details while using priors learned from large-scale video datasets. Consequently, GeoMan improves temporal consistency and generalizability while requiring minimal 4D training data. To address the challenge of accurate human size estimation, we introduce a root-relative depth representation that retains critical human-scale details and is easier to be estimated from monocular inputs, overcoming the limitations of traditional affine-invariant and metric depth representations. GeoMan achieves state-of-the-art performance in both qualitative and quantitative evaluations, demonstrating its effectiveness in overcoming longstanding challenges in 3D human geometry estimation from videos.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GeoManï¼Œä¸€ç§æ—¨åœ¨ä»å•ç›®è§†é¢‘ä¸­ä¼°ç®—ç²¾ç¡®ä¸”æ—¶é—´ä¸€è‡´çš„3Däººä½“å‡ ä½•ç»“æ„çš„æ¶æ„ã€‚GeoManå°†è§†é¢‘å‡ ä½•ä¼°ç®—ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºå›¾åƒåˆ°è§†é¢‘(image-to-video)çš„ç”Ÿæˆé—®é¢˜ï¼Œåˆ©ç”¨åŸºäºå›¾åƒçš„æ¨¡å‹å¤„ç†é¦–å¸§çš„æ·±åº¦å’Œæ³•çº¿(normal)ä¼°ç®—ï¼Œå¹¶ä»¥æ­¤å¼•å¯¼è§†é¢‘æ‰©æ•£æ¨¡å‹(video diffusion model)ã€‚è¿™ç§è®¾è®¡é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘æ•°æ®é›†çš„å…ˆéªŒçŸ¥è¯†ï¼Œåœ¨ä»…éœ€æå°‘é‡4Dè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ—¶é—´ä¸€è‡´æ€§(temporal consistency)å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³å‡†ç¡®çš„äººä½“å°ºå¯¸ä¼°ç®—éš¾é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†æ ¹ç›¸å¯¹æ·±åº¦è¡¨ç¤º(root-relative depth representation)ï¼Œæ¯”ä¼ ç»Ÿçš„ä»¿å°„ä¸å˜(affine-invariant)æˆ–åº¦é‡æ·±åº¦(metric depth)è¡¨ç¤ºæ›´èƒ½æ•æ‰å…³é”®çš„äººä½“å°ºåº¦ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoManåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³(state-of-the-art)ï¼Œæœ‰æ•ˆå…‹æœäº†è§†é¢‘äººä½“å‡ ä½•ä¼°ç®—ä¸­é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://research.nvidia.com/labs/dair/geoman",
      "pdf_url": "https://arxiv.org/pdf/2505.23085v1",
      "published_date": "2025-05-29 04:41:04 UTC",
      "updated_date": "2025-05-29 04:41:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:11.145261+00:00"
    },
    {
      "arxiv_id": "2505.23078v1",
      "title": "Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport",
      "title_zh": "åŸºäºæœ€ä¼˜ä¼ è¾“æœ€å°è´å¶æ–¯é£é™©è§£ç çš„æ–‡æ¡£çº§æ–‡æœ¬ç”Ÿæˆ",
      "authors": [
        "Yuu Jinnai"
      ],
      "abstract": "Document-level text generation tasks are known to be more difficult than sentence-level text generation tasks as they require the understanding of longer context to generate high-quality texts. In this paper, we investigate the adaption of Minimum Bayes Risk (MBR) decoding for document-level text generation tasks. MBR decoding makes use of a utility function to estimate the output with the highest expected utility from a set of candidate outputs. Although MBR decoding is shown to be effective in a wide range of sentence-level text generation tasks, its performance on document-level text generation tasks is limited as many of the utility functions are designed for evaluating the utility of sentences. To this end, we propose MBR-OT, a variant of MBR decoding using Wasserstein distance to compute the utility of a document using a sentence-level utility function. The experimental result shows that the performance of MBR-OT outperforms that of the standard MBR in document-level machine translation, text simplification, and dense image captioning tasks. Our code is available at https://github.com/jinnaiyuu/mbr-optimal-transport",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æ¡£çº§æ–‡æœ¬ç”Ÿæˆ(Document-Level Text Generation)ä¸­æœ€å°è´å¶æ–¯é£é™©(Minimum Bayes Risk, MBR)è§£ç çš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰MBRæ•ˆç”¨å‡½æ•°å¤šä¸ºå¥å­çº§è®¾è®¡è€Œéš¾ä»¥æœ‰æ•ˆæ•æ‰é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†MBR-OTæ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥Wassersteinè·ç¦»(Optimal Transport)ï¼Œåˆ©ç”¨ç°æœ‰çš„å¥å­çº§æ•ˆç”¨å‡½æ•°æ¥å‡†ç¡®è®¡ç®—æ•´ç¯‡æ–‡æ¡£çš„é¢„æœŸæ•ˆç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ–‡æ¡£çº§æœºå™¨ç¿»è¯‘(Machine Translation)ã€æ–‡æœ¬ç®€åŒ–(Text Simplification)å’Œå¯†é›†å›¾åƒå­—å¹•(Dense Image Captioning)ç­‰å¤šä¸ªä»»åŠ¡ä¸­ï¼ŒMBR-OTçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºæ ‡å‡†MBRã€‚è¿™ä¸€ç ”ç©¶æˆåŠŸå…‹æœäº†ä¼ ç»Ÿè§£ç ç­–ç•¥åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„å±€é™æ€§ï¼Œä¸ºæå‡æ–‡æ¡£çº§ç”Ÿæˆä»»åŠ¡çš„è´¨é‡æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23078v1",
      "published_date": "2025-05-29 04:34:04 UTC",
      "updated_date": "2025-05-29 04:34:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:11.946430+00:00"
    },
    {
      "arxiv_id": "2505.23075v2",
      "title": "Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble",
      "title_zh": "ç¬¬äºŒè¯Šç–—æ„è§çš„é‡è¦æ€§ï¼šé€šè¿‡ä¸“å®¶æ¨¡å‹é›†æˆå…±è¯†è¿ˆå‘è‡ªé€‚åº”ä¸´åºŠäººå·¥æ™ºèƒ½",
      "authors": [
        "Amit Kumthekar",
        "Zion Tilley",
        "Henry Duong",
        "Bhargav Patel",
        "Michael Magnoli",
        "Ahmed Omar",
        "Ahmed Nasser",
        "Chaitanya Gharpure",
        "Yevgen Reztzov"
      ],
      "abstract": "Despite the growing clinical adoption of large language models (LLMs), current approaches heavily rely on single model architectures. To overcome risks of obsolescence and rigid dependence on single model systems, we present a novel framework, termed the Consensus Mechanism. Mimicking clinical triage and multidisciplinary clinical decision-making, the Consensus Mechanism implements an ensemble of specialized medical expert agents enabling improved clinical decision making while maintaining robust adaptability. This architecture enables the Consensus Mechanism to be optimized for cost, latency, or performance, purely based on its interior model configuration.\n  To rigorously evaluate the Consensus Mechanism, we employed three medical evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an increase in accuracy on MedQA ($Î”\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 3.4\\%$) and MedMCQA ($Î”\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 9.1\\%$). These accuracy gains extended to differential diagnosis generation, where our system demonstrated improved recall and precision (F1$_\\mathrm{consensus}$ = 0.326 vs. F1$_{\\mathrm{O3\\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX (Top1$_\\mathrm{consensus}$ = 52.0% vs. Top1$_{\\mathrm{O3\\text{-}high}}$ = 45.2%).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º Consensus Mechanism çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœå½“å‰ä¸´åºŠå¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯¹å•ä¸€æ¨¡å‹æ¶æ„çš„è¿‡åº¦ä¾èµ–åŠå…¶å¸¦æ¥çš„åƒµåŒ–é£é™©ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡ä»¿ä¸´åºŠåˆ†è¯Šå’Œå¤šå­¦ç§‘ä¸´åºŠå†³ç­–è¿‡ç¨‹ï¼Œæ„å»ºäº†ä¸€ä¸ªç”±ä¸“é—¨åŒ»ç–—ä¸“å®¶æ™ºèƒ½ä½“ç»„æˆçš„é›†æˆç³»ç»Ÿ (ensemble of specialized medical expert agents)ï¼Œåœ¨æå‡ä¸´åºŠå†³ç­–è´¨é‡çš„åŒæ—¶ä¿æŒäº†æé«˜çš„é€‚åº”æ€§ã€‚è¯¥æ¶æ„å…è®¸æ ¹æ®å†…éƒ¨æ¨¡å‹é…ç½®ï¼Œé’ˆå¯¹æˆæœ¬ã€å»¶è¿Ÿæˆ–æ€§èƒ½è¿›è¡Œçµæ´»ä¼˜åŒ–ã€‚åœ¨ MedXpertQA åŸºå‡†æµ‹è¯•ä¸­ï¼ŒConsensus Mechanism è¾¾åˆ°äº† 61.0% çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äº OpenAI çš„ O3 (53.5%) å’Œ Google çš„ Gemini 2.5 Pro (45.9%)ã€‚æ­¤å¤–ï¼Œåœ¨ MedQA å’Œ MedMCQA æµ‹è¯•ä¸­ï¼Œè¯¥æœºåˆ¶ä¹Ÿè¡¨ç°å‡ºæŒç»­çš„æ€§èƒ½æå‡ã€‚åœ¨é‰´åˆ«è¯Šæ–­ä»»åŠ¡ DDX+ ä¸­ï¼Œè¯¥ç³»ç»ŸåŒæ ·åœ¨å¬å›ç‡ã€ç²¾ç¡®ç‡åŠ Top-1 å‡†ç¡®ç‡æ–¹é¢å±•ç°å‡ºè¶…è¶Šé«˜æ€§èƒ½åŸºçº¿æ¨¡å‹çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§åŸºäºä¸“å®¶æ¨¡å‹å…±è¯†çš„é›†æˆæ–¹æ³•ä¸ºå®ç°æ›´å…·é²æ£’æ€§å’Œå¯æ‰©å±•æ€§çš„ä¸´åºŠ AI ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23075v2",
      "published_date": "2025-05-29 04:29:22 UTC",
      "updated_date": "2025-06-20 18:24:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:09.921460+00:00"
    },
    {
      "arxiv_id": "2505.23066v1",
      "title": "Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing",
      "title_zh": "åŸºäºç²’çƒè®¡ç®—çš„é«˜æ•ˆé‡å­è¿‘ä¼¼ $k$NN ç®—æ³•",
      "authors": [
        "Shuyin Xia",
        "Xiaojiang Tian",
        "Suzhen Yuan",
        "Jeremiah D. Deng"
      ],
      "abstract": "High time complexity is one of the biggest challenges faced by $k$-Nearest Neighbors ($k$NN). Although current classical and quantum $k$NN algorithms have made some improvements, they still have a speed bottleneck when facing large amounts of data. To address this issue, we propose an innovative algorithm called Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves higher efficiency by first employing granular-balls, which reduces the data size needed to processed. The search process is then accelerated by adopting a Hierarchical Navigable Small World (HNSW) method. Moreover, we optimize the time-consuming steps, such as distance calculation, of the HNSW via quantization, further reducing the time complexity of the construct and search process. By combining the use of granular-balls and quantization of the HNSW method, our approach manages to take advantage of these treatments and significantly reduces the time complexity of the $k$NN-like algorithms, as revealed by a comprehensive complexity analysis.",
      "tldr_zh": "é’ˆå¯¹ $k$-Nearest Neighbors ($k$NN) ç®—æ³•åœ¨å¤„ç†æµ·é‡æ•°æ®æ—¶é¢ä¸´çš„é«˜æ—¶é—´å¤æ‚åº¦å’Œé€Ÿåº¦ç“¶é¢ˆé—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Granular-Ball based Quantum $k$NN (GB-Q$k$NN) çš„åˆ›æ–°ç®—æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨ granular-balls æŠ€æœ¯å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œæœ‰æ•ˆå‡å°‘äº†éœ€è¦å®é™…å¤„ç†çš„æ•°æ®è§„æ¨¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å¼•å…¥äº† Hierarchical Navigable Small World (HNSW) æ–¹æ³•æ¥åŠ é€Ÿæœç´¢è¿‡ç¨‹ï¼Œå¹¶é’ˆå¯¹è·ç¦»è®¡ç®—ç­‰å…³é”®è€—æ—¶ç¯èŠ‚è¿›è¡Œäº† quantization ä¼˜åŒ–ã€‚é€šè¿‡å°†ç²’çƒè®¡ç®—ä¸ HNSW çš„é‡å­åŒ–æ”¹è¿›ç›¸ç»“åˆï¼ŒGB-Q$k$NN å……åˆ†åˆ©ç”¨äº†ä¸¤ç§æŠ€æœ¯çš„ä¼˜åŠ¿ã€‚ç»¼åˆå¤æ‚åº¦åˆ†æè¯æ˜ï¼Œè¯¥ç®—æ³•æ˜¾è‘—é™ä½äº†ç±» $k$NN ç®—æ³•åœ¨æ„å»ºå’Œæœç´¢é˜¶æ®µçš„æ—¶é—´å¤æ‚åº¦ï¼Œä¸ºå¤§è§„æ¨¡æ•°æ®ç¯å¢ƒä¸‹çš„é«˜æ•ˆé‡å­è¿‘ä¼¼æœç´¢æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "8 pages; 7 figure; accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23066v1",
      "published_date": "2025-05-29 04:16:29 UTC",
      "updated_date": "2025-05-29 04:16:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:31.900563+00:00"
    },
    {
      "arxiv_id": "2505.23062v4",
      "title": "Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data",
      "title_zh": "é’ˆå¯¹åŠ¨åŠ›å­¦åç§»æ•°æ®çš„å¼ºåŒ–å­¦ä¹ å¤åˆæµåŒ¹é…",
      "authors": [
        "Lingkai Kong",
        "Haichuan Wang",
        "Tonghan Wang",
        "Guojun Xiong",
        "Milind Tambe"
      ],
      "abstract": "Incorporating pre-collected offline data can substantially improve the sample efficiency of reinforcement learning (RL), but its benefits can break down when the transition dynamics in the offline dataset differ from those encountered online. Existing approaches typically mitigate this issue by penalizing or filtering offline transitions in regions with large dynamics gap. However, their dynamics-gap estimators often rely on KL divergence or mutual information, which can be ill-defined when offline and online dynamics have mismatched support. To address this challenge, we propose CompFlow, a principled framework built on the theoretical connection between flow matching and optimal transport. Specifically, we model the online dynamics as a conditional flow built upon the output distribution of a pretrained offline flow, rather than learning it directly from a Gaussian prior. This composite structure provides two advantages: (1) improved generalization when learning online dynamics under limited interaction data, and (2) a well-defined and stable estimate of the dynamics gap via the Wasserstein distance between offline and online transitions. Building on this dynamics-gap estimator, we further develop an optimistic active data collection strategy that prioritizes exploration in high-gap regions, and show theoretically that it reduces the performance gap to the optimal policy. Empirically, CompFlow consistently outperforms strong baselines across a range of RL benchmarks with shifted-dynamics data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¸­ç¦»çº¿æ•°æ®ä¸åœ¨çº¿ç¯å¢ƒåŠ¨åŠ›å­¦ä¸ä¸€è‡´(Shifted-Dynamics)å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†CompFlowæ¡†æ¶ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨æµåŒ¹é…(Flow Matching)ä¸æœ€ä¼˜ä¼ è¾“(Optimal Transport)çš„ç†è®ºè”ç³»ä¹‹ä¸Šï¼Œå°†åœ¨çº¿åŠ¨åŠ›å­¦å»ºæ¨¡ä¸ºä¸€ç§åŸºäºé¢„è®­ç»ƒç¦»çº¿æµè¾“å‡ºåˆ†å¸ƒçš„æ¡ä»¶æµã€‚è¿™ç§å¤åˆç»“æ„(Composite Structure)ä¸ä»…åœ¨æœ‰é™åœ¨çº¿æ•°æ®ä¸‹æå‡äº†æ³›åŒ–èƒ½åŠ›ï¼Œè¿˜é€šè¿‡Wassersteinè·ç¦»ä¸ºåŠ¨åŠ›å­¦å·®å¼‚(Dynamics Gap)æä¾›äº†ç¨³å®šä¸”å®šä¹‰è‰¯å¥½çš„ä¼°è®¡ã€‚åŸºäºæ­¤ä¼°è®¡å™¨ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§ä¹è§‚çš„ä¸»åŠ¨æ•°æ®é‡‡é›†ç­–ç•¥(Active Data Collection Strategy)ï¼Œä¼˜å…ˆæ¢ç´¢å·®å¼‚è¾ƒå¤§çš„åŒºåŸŸã€‚å®éªŒä¸ç†è®ºè¯æ˜ï¼ŒCompFlowèƒ½æœ‰æ•ˆç¼©å°ä¸æœ€ä¼˜ç­–ç•¥çš„å·®è·ï¼Œåœ¨å¤šç§åŠ¨åŠ›å­¦åç§»çš„å¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰çš„å¼ºåŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 Spotlight",
      "pdf_url": "https://arxiv.org/pdf/2505.23062v4",
      "published_date": "2025-05-29 04:09:19 UTC",
      "updated_date": "2026-01-21 16:37:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:31.116108+00:00"
    },
    {
      "arxiv_id": "2505.23059v1",
      "title": "From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval",
      "title_zh": "ä» Token åˆ°è¡ŒåŠ¨ï¼šåˆ©ç”¨çŠ¶æ€æœºæ¨ç†ç¼“è§£ä¿¡æ¯æ£€ç´¢ä¸­çš„è¿‡åº¦æ€è€ƒ",
      "authors": [
        "Dohyeon Lee",
        "Yeonseok Jeong",
        "Seung-won Hwang"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting enables complex reasoning in large language models (LLMs), including applications in information retrieval (IR). However, it often leads to overthinking, where models produce excessively long and semantically redundant traces with little or no benefit. We identify two key challenges in IR: redundant trajectories that revisit similar states and misguided reasoning that diverges from user intent. To address these, we propose State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning, offering a practical alternative to conventional CoT reasoning. The code and details are available at https://github.com/ldilab/SMR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¿¡æ¯æ£€ç´¢ (Information Retrieval, IR) ä¸­ Chain-of-Thought (CoT) æç¤ºå¸¸å¯¼è‡´æ¨¡å‹äº§å‡ºå†—ä½™è½¨è¿¹çš„â€œè¿‡åº¦æ€è€ƒ (overthinking)â€é—®é¢˜ï¼Œæå‡ºäº†çŠ¶æ€æœºæ¨ç† (State Machine Reasoning, SMR) æ¡†æ¶ã€‚è¯¥æ¡†æ¶è¯†åˆ«å¹¶è§£å†³äº† IR ä»»åŠ¡ä¸­é‡å¤è®¿é—®ç›¸ä¼¼çŠ¶æ€çš„è½¨è¿¹å†—ä½™ä»¥åŠåç¦»ç”¨æˆ·æ„å›¾çš„è¯¯å¯¼æ€§æ¨ç†ä¸¤å¤§æŒ‘æˆ˜ã€‚SMR é‡‡ç”¨ä¸€ç§åŸºäºè½¬æ¢çš„æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡ Refineã€Rerank å’Œ Stop ç­‰ç¦»æ•£åŠ¨ä½œå®ç°å¯¹æ¨ç†è¿‡ç¨‹çš„ç»†ç²’åº¦æ§åˆ¶åŠæ—©æœŸåœæ­¢ã€‚åœ¨ BEIR å’Œ BRIGHT åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSMR åœ¨å°†æ£€ç´¢æ€§èƒ½ (nDCG@10) æå‡ 3.4% çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº† 74.4% çš„ Token ä½¿ç”¨é‡ã€‚è¯¥æ–¹æ³•å…·æœ‰æå¼ºçš„é€šç”¨æ€§ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒå³å¯è·¨å¤šç§å¤§è¯­è¨€æ¨¡å‹ (LLMs) å’Œæ£€ç´¢å™¨è¿è¡Œã€‚ä½œä¸ºä¼ ç»Ÿ CoT æ¨ç†çš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆï¼ŒSMR ä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´å¯æ§çš„æ£€ç´¢æ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23059v1",
      "published_date": "2025-05-29 04:04:25 UTC",
      "updated_date": "2025-05-29 04:04:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:38.724039+00:00"
    },
    {
      "arxiv_id": "2505.23058v1",
      "title": "Be.FM: Open Foundation Models for Human Behavior",
      "title_zh": "Be.FMï¼šé¢å‘äººç±»è¡Œä¸ºçš„å¼€æºåŸºç¡€æ¨¡å‹",
      "authors": [
        "Yutong Xie",
        "Zhuoheng Li",
        "Xiyuan Wang",
        "Yijun Pan",
        "Qijia Liu",
        "Xingzhi Cui",
        "Kuang-Yu Lo",
        "Ruoyi Gao",
        "Xingjian Zhang",
        "Jin Huang",
        "Walter Yuan",
        "Matthew O. Jackson",
        "Qiaozhu Mei"
      ],
      "abstract": "Despite their success in numerous fields, the potential of foundation models for modeling and understanding human behavior remains largely unexplored. We introduce Be.FM, one of the first open foundation models designed for human behavior modeling. Built upon open-source large language models and fine-tuned on a diverse range of behavioral data, Be.FM can be used to understand and predict human decision-making. We construct a comprehensive set of benchmark tasks for testing the capabilities of behavioral foundation models. Our results demonstrate that Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Be.FMï¼Œè¿™æ˜¯é¦–æ‰¹ä¸“ä¸ºäººç±»è¡Œä¸ºå»ºæ¨¡è€Œè®¾è®¡çš„å¼€æº Foundation Models ä¹‹ä¸€ã€‚è¯¥æ¨¡å‹åŸºäºå¼€æº Large Language Models å¹¶é€šè¿‡å¤šæ ·åŒ–çš„è¡Œä¸ºæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæ—¨åœ¨æ·±å…¥ç†è§£å’Œé¢„æµ‹äººç±»çš„å†³ç­–è¿‡ç¨‹ã€‚ä¸ºäº†éªŒè¯æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€å¥—æ¶µç›–å¤šç§è¡Œä¸ºå»ºæ¨¡èƒ½åŠ›çš„ Benchmark ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBe.FM èƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹äººç±»è¡Œä¸ºï¼Œå¹¶å‡†ç¡®æ¨æ–­ä¸ªä½“ä¸ç¾¤ä½“çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜èƒ½é’ˆå¯¹ç‰¹å®šæƒ…å¢ƒç”Ÿæˆè§è§£å¹¶çµæ´»åº”ç”¨è¡Œä¸ºç§‘å­¦çŸ¥è¯†ã€‚è¿™ä¸€æˆæœä¸ºäººç±»è¡Œä¸ºç ”ç©¶é¢†åŸŸæä¾›äº†å¼ºå¤§çš„è®¡ç®—å·¥å…·ï¼Œå±•ç¤ºäº† Foundation Models åœ¨ç†è§£å¤æ‚äººç±»è¡Œä¸ºæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23058v1",
      "published_date": "2025-05-29 04:03:51 UTC",
      "updated_date": "2025-05-29 04:03:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:44.530109+00:00"
    },
    {
      "arxiv_id": "2505.23053v1",
      "title": "Augment or Not? A Comparative Study of Pure and Augmented Large Language Model Recommenders",
      "title_zh": "å¢å¼ºä¸å¦ï¼Ÿçº¯å¤§è¯­è¨€æ¨¡å‹æ¨èç³»ç»Ÿä¸å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹æ¨èç³»ç»Ÿçš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Wei-Hsiang Huang",
        "Chen-Wei Ke",
        "Wei-Ning Chiu",
        "Yu-Xuan Su",
        "Chun-Chun Yang",
        "Chieh-Yuan Cheng",
        "Yun-Nung Chen",
        "Pu-Jen Cheng"
      ],
      "abstract": "Large language models (LLMs) have introduced new paradigms for recommender systems by enabling richer semantic understanding and incorporating implicit world knowledge. In this study, we propose a systematic taxonomy that classifies existing approaches into two categories: (1) Pure LLM Recommenders, which rely solely on LLMs, and (2) Augmented LLM Recommenders, which integrate additional non-LLM techniques to enhance performance. This taxonomy provides a novel lens through which to examine the evolving landscape of LLM-based recommendation. To support fair comparison, we introduce a unified evaluation platform that benchmarks representative models under consistent experimental settings, highlighting key design choices that impact effectiveness. We conclude by discussing open challenges and outlining promising directions for future research. This work offers both a comprehensive overview and practical guidance for advancing next-generation LLM-powered recommender.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨èç³»ç»Ÿä¸­çš„è¯­ä¹‰ç†è§£å’ŒçŸ¥è¯†é›†æˆèƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„åˆ†ç±»æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ç°æœ‰ç ”ç©¶å½’çº³ä¸ºä¸¤ç±»ï¼šä»…ä¾é å¤§è¯­è¨€æ¨¡å‹çš„Pure LLM Recommendersï¼Œä»¥åŠé›†æˆéLLMæŠ€æœ¯ä»¥å¢å¼ºæ€§èƒ½çš„Augmented LLM Recommendersã€‚ä¸ºå®ç°å…¬å¹³çš„æ€§èƒ½å¯¹æ¯”ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°å¹³å°ï¼Œåœ¨ä¸€è‡´çš„å®éªŒè®¾ç½®ä¸‹å¯¹ä»£è¡¨æ€§æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶åˆ†æäº†å½±å“æœ‰æ•ˆæ€§çš„å…³é”®è®¾è®¡é€‰æ‹©ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†è¯¥é¢†åŸŸå½“å‰é¢ä¸´çš„å¼€æ”¾æŒ‘æˆ˜åŠæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£å’Œæ¨è¿›ä¸‹ä¸€ä»£LLMé©±åŠ¨çš„æ¨èç³»ç»Ÿæä¾›äº†å…¨é¢çš„ç»¼è¿°å’Œå…·æœ‰ä»·å€¼çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23053v1",
      "published_date": "2025-05-29 03:50:24 UTC",
      "updated_date": "2025-05-29 03:50:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:55.998854+00:00"
    },
    {
      "arxiv_id": "2505.23045v1",
      "title": "Multi-Sourced Compositional Generalization in Visual Question Answering",
      "title_zh": "è§†è§‰é—®ç­”ä¸­çš„å¤šæºç»„åˆæ³›åŒ–",
      "authors": [
        "Chuanhao Li",
        "Wenbo Ye",
        "Zhen Li",
        "Yuwei Wu",
        "Yunde Jia"
      ],
      "abstract": "Compositional generalization is the ability of generalizing novel compositions from seen primitives, and has received much attention in vision-and-language (V\\&L) recently. Due to the multi-modal nature of V\\&L tasks, the primitives composing compositions source from different modalities, resulting in multi-sourced novel compositions. However, the generalization ability over multi-sourced novel compositions, \\textit{i.e.}, multi-sourced compositional generalization (MSCG) remains unexplored. In this paper, we explore MSCG in the context of visual question answering (VQA), and propose a retrieval-augmented training framework to enhance the MSCG ability of VQA models by learning unified representations for primitives from different modalities. Specifically, semantically equivalent primitives are retrieved for each primitive in the training samples, and the retrieved features are aggregated with the original primitive to refine the model. This process helps the model learn consistent representations for the same semantic primitives across different modalities. To evaluate the MSCG ability of VQA models, we construct a new GQA-MSCG dataset based on the GQA dataset, in which samples include three types of novel compositions composed of primitives from different modalities. Experimental results demonstrate the effectiveness of the proposed framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰é—®ç­”(Visual Question Answering, VQA)é¢†åŸŸä¸­çš„å¤šæºç»„åˆæ³›åŒ–(Multi-Sourced Compositional Generalization, MSCG)é—®é¢˜ï¼Œå³æ¨¡å‹åœ¨å¤„ç†ç”±æ¥è‡ªä¸åŒæ¨¡æ€çš„åŸºå…ƒ(primitives)ç»„æˆçš„æ–°å‹ç»„åˆæ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚ç”±äºè§†è§‰è¯­è¨€(Vision-and-Language, V&L)ä»»åŠ¡å…·æœ‰å¤šæ¨¡æ€ç‰¹æ€§ï¼ŒåŸºå…ƒå¾€å¾€æ¥æºäºä¸åŒæ¨¡æ€ï¼Œä½†æ­¤å‰çš„ç ”ç©¶å°šæœªå……åˆ†æ¢ç´¢è¿™ç§å¤šæºåœºæ™¯ä¸‹çš„æ³›åŒ–ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ£€ç´¢å¢å¼ºè®­ç»ƒæ¡†æ¶(retrieval-augmented training framework)ï¼Œé€šè¿‡å­¦ä¹ è·¨æ¨¡æ€çš„ç»Ÿä¸€è¡¨ç¤º(unified representations)æ¥æå‡æ¨¡å‹çš„MSCGèƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä¸ºè®­ç»ƒæ ·æœ¬ä¸­çš„æ¯ä¸ªåŸºå…ƒæ£€ç´¢è¯­ä¹‰ç­‰æ•ˆçš„åŸºå…ƒï¼Œå¹¶å°†ç‰¹å¾è¿›è¡Œèšåˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè·¨æ¨¡æ€å­¦ä¹ ä¸€è‡´çš„è¯­ä¹‰ç‰¹å¾ã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜ŸåŸºäºGQAæ•°æ®é›†æ„å»ºäº†å…¨æ–°çš„GQA-MSCGæ•°æ®é›†ï¼ŒåŒ…å«ä¸‰ç±»ä¸åŒæ¨¡æ€åŸºå…ƒæ„æˆçš„æ–°å‹ç»„åˆã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æå‡VQAæ¨¡å‹å¤šæºç»„åˆæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œç›®å‰ç›¸å…³æ•°æ®é›†å·²å¼€æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23045v1",
      "published_date": "2025-05-29 03:41:36 UTC",
      "updated_date": "2025-05-29 03:41:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:48.248855+00:00"
    },
    {
      "arxiv_id": "2505.23043v1",
      "title": "Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation",
      "title_zh": "ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹æ˜¯å¦å¿…è¦ï¼šç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„æ³›åŒ–ç ”ç©¶",
      "authors": [
        "Jihai Zhang",
        "Tianle Li",
        "Linjie Li",
        "Zhengyuan Yang",
        "Yu Cheng"
      ],
      "abstract": "Recent advancements in unified vision-language models (VLMs), which integrate both visual understanding and generation capabilities, have attracted significant attention. The underlying hypothesis is that a unified architecture with mixed training on both understanding and generation tasks can enable mutual enhancement between understanding and generation. However, this hypothesis remains underexplored in prior works on unified VLMs. To address this gap, this paper systematically investigates the generalization across understanding and generation tasks in unified VLMs. Specifically, we design a dataset closely aligned with real-world scenarios to facilitate extensive experiments and quantitative evaluations. We evaluate multiple unified VLM architectures to validate our findings. Our key findings are as follows. First, unified VLMs trained with mixed data exhibit mutual benefits in understanding and generation tasks across various architectures, and this mutual benefits can scale up with increased data. Second, better alignment between multimodal input and output spaces will lead to better generalization. Third, the knowledge acquired during generation tasks can transfer to understanding tasks, and this cross-task generalization occurs within the base language model, beyond modality adapters. Our findings underscore the critical necessity of unifying understanding and generation in VLMs, offering valuable insights for the design and optimization of unified VLMs.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹(Unified Vision-Language Models, VLMs)åœ¨è§†è§‰ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ—¨åœ¨éªŒè¯æ··åˆè®­ç»ƒæ˜¯å¦èƒ½ä¿ƒè¿›ä»»åŠ¡é—´çš„ç›¸äº’å¢å¼ºã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è®¾è®¡è´´è¿‘ç°å®åœºæ™¯çš„æ•°æ®é›†å¹¶åœ¨å¤šç§æ¶æ„ä¸Šè¿›è¡Œå®šé‡è¯„ä¼°ï¼Œå‘ç°æ··åˆæ•°æ®è®­ç»ƒåœ¨ä¸åŒæ¶æ„ä¸‹å‡èƒ½å®ç°ç†è§£ä¸ç”Ÿæˆä»»åŠ¡çš„äº’åˆ©ï¼Œä¸”è¿™ç§ç›Šå¤„éšæ•°æ®é‡å¢åŠ è€Œæ‰©å¤§ã€‚å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå¤šæ¨¡æ€è¾“å…¥ä¸è¾“å‡ºç©ºé—´ä¹‹é—´æ›´å¥½çš„å¯¹é½(Alignment)èƒ½æ˜¾è‘—æå‡æ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº†ä»ç”Ÿæˆä»»åŠ¡ä¸­è·å–çš„çŸ¥è¯†èƒ½å¤Ÿè¿ç§»è‡³ç†è§£ä»»åŠ¡ï¼Œä¸”è¿™ç§è·¨ä»»åŠ¡æ³›åŒ–å‘ç”Ÿåœ¨åŸºç¡€è¯­è¨€æ¨¡å‹(Base Language Model)å†…éƒ¨ï¼Œè€Œéä»…é™äºæ¨¡æ€é€‚é…å™¨(Modality Adapters)ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç»Ÿä¸€ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›çš„å¿…è¦æ€§ï¼Œä¸ºæœªæ¥ç»Ÿä¸€æ¶æ„çš„è®¾è®¡ä¸ä¼˜åŒ–æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23043v1",
      "published_date": "2025-05-29 03:40:21 UTC",
      "updated_date": "2025-05-29 03:40:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:52.984904+00:00"
    },
    {
      "arxiv_id": "2505.23042v1",
      "title": "From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data",
      "title_zh": "ä»ç†è®ºåˆ°åº”ç”¨ï¼šåŸºäºçœŸå®ä¸–ç•Œå‹åŠ›æ•°æ®çš„è„‘ç”µå¤§æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Siwen Wang",
        "Shitou Zhang",
        "Wan-Lin Chen",
        "Dung Truong",
        "Tzyy-Ping Jung"
      ],
      "abstract": "Recent advancements in Large Language Models have inspired the development of foundation models across various domains. In this study, we evaluate the efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art foundation EEG model, on a real-world stress classification dataset collected in a graduate classroom. Unlike previous studies that primarily evaluate LEMs using data from controlled clinical settings, our work assesses their applicability to real-world environments. We train a binary classifier that distinguishes between normal and elevated stress states using resting-state EEG data recorded from 18 graduate students during a class session. The best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a 5-second window, significantly outperforming traditional stress classifiers in both accuracy and inference efficiency. We further evaluate the robustness of the fine-tuned LEM under random data shuffling and reduced channel counts. These results demonstrate the capability of LEMs to effectively process real-world EEG data and highlight their potential to revolutionize brain-computer interface applications by shifting the focus from model-centric to data-centric design.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§å‹è„‘ç”µæ¨¡å‹ (Large EEG Models, LEMs) åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡åœ¨çœŸå®ä¸–ç•Œçš„å‹åŠ›åˆ†ç±»æ•°æ®é›†ä¸Šå¯¹æœ€å…ˆè¿›çš„åŸºåº§æ¨¡å‹ LaBraM è¿›è¡Œå¾®è°ƒã€‚ç ”ç©¶é‡‡ç”¨äº†åœ¨ç ”ç©¶ç”Ÿè¯¾å ‚ç¯å¢ƒä¸‹é‡‡é›†çš„ 18 åå­¦ç”Ÿçš„é™æ¯æ€ EEG æ•°æ®ï¼Œè®­ç»ƒäº†ä¸€ä¸ªç”¨äºåŒºåˆ†æ­£å¸¸ä¸é«˜å‹åŠ›çŠ¶æ€çš„äºŒåˆ†ç±»å™¨ã€‚ä¸ä»¥å¾€ä¸»è¦åœ¨å—æ§ä¸´åºŠè®¾ç½®ä¸­è¯„ä¼° LEMs çš„ç ”ç©¶ä¸åŒï¼Œè¯¥å·¥ä½œé‡ç‚¹è€ƒå¯Ÿäº†æ¨¡å‹åœ¨çœŸå®å¤æ‚ç¯å¢ƒä¸‹çš„é€‚ç”¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨ 5 ç§’çª—å£ä¸‹è¾¾åˆ°äº† 90.47% çš„å¹³è¡¡å‡†ç¡®ç‡ (balanced accuracy)ï¼Œåœ¨å‡†ç¡®æ€§å’Œæ¨ç†æ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å‹åŠ›åˆ†ç±»å™¨ã€‚ç ”ç©¶äººå‘˜è¿˜è¿›ä¸€æ­¥éªŒè¯äº†å¾®è°ƒåçš„ LEM åœ¨éšæœºæ•°æ®æ‰“ä¹±å’Œé€šé“æ•°å‡å°‘æƒ…å†µä¸‹çš„ç¨³å¥æ€§ (robustness)ã€‚è¿™äº›ç»“æœè¯æ˜äº† LEMs æœ‰æ•ˆå¤„ç†çœŸå®ä¸–ç•Œ EEG æ•°æ®çš„èƒ½åŠ›ï¼Œå¹¶å¼ºè°ƒäº†å…¶é€šè¿‡ä»ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒè½¬å‘ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è®¾è®¡ï¼Œæ¨åŠ¨è„‘æœºæ¥å£ (Brain-Computer Interface) åº”ç”¨å˜é©çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23042v1",
      "published_date": "2025-05-29 03:40:20 UTC",
      "updated_date": "2025-05-29 03:40:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:53.656705+00:00"
    },
    {
      "arxiv_id": "2505.23034v1",
      "title": "Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction",
      "title_zh": "æ¡ˆä¾‹æ¨ç†æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨è¯ç‰©ç›¸äº’ä½œç”¨é¢„æµ‹ä¸­çš„æ•ˆèƒ½",
      "authors": [
        "Guangyi Liu",
        "Yongqi Zhang",
        "Xunyuan Liu",
        "Quanming Yao"
      ],
      "abstract": "Drug-drug interaction (DDI) prediction is critical for treatment safety. While large language models (LLMs) show promise in pharmaceutical tasks, their effectiveness in DDI prediction remains challenging. Inspired by the well-established clinical practice where physicians routinely reference similar historical cases to guide their decisions through case-based reasoning (CBR), we propose CBR-DDI, a novel framework that distills pharmacological principles from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI constructs a knowledge repository by leveraging LLMs to extract pharmacological insights and graph neural networks (GNNs) to model drug associations. A hybrid retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to effectively retrieve and reuse relevant cases. We further introduce a representative sampling strategy for dynamic case refinement. Extensive experiments demonstrate that CBR-DDI achieves state-of-the-art performance, with a significant 28.7% accuracy improvement over both popular LLMs and CBR baseline, while maintaining high interpretability and flexibility.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CBR-DDIæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¡ˆä¾‹æ¨ç†(Case-Based Reasoning)å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¯ç‰©ç›¸äº’ä½œç”¨(Drug-Drug Interaction)é¢„æµ‹ä¸­çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å—åˆ°ä¸´åºŠåŒ»ç”Ÿå‚è€ƒå†å²æ¡ˆä¾‹å†³ç­–çš„å¯å‘ï¼Œåˆ©ç”¨LLMsæå–è¯ç†å­¦è§è§£ï¼Œå¹¶ç»“åˆå›¾ç¥ç»ç½‘ç»œ(GNNs)å¯¹è¯ç‰©å…³è”è¿›è¡Œå»ºæ¨¡ä»¥æ„å»ºçŸ¥è¯†åº“ã€‚é€šè¿‡æ··åˆæ£€ç´¢æœºåˆ¶å’ŒåŒå±‚çŸ¥è¯†å¢å¼ºæç¤ºæŠ€æœ¯ï¼ŒCBR-DDIä½¿LLMsèƒ½å¤Ÿæœ‰æ•ˆåœ°æ£€ç´¢å¹¶é‡ç”¨ç›¸å…³æ¡ˆä¾‹ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä»£è¡¨æ€§é‡‡æ ·ç­–ç•¥ï¼Œå®ç°äº†æ¡ˆä¾‹åº“çš„åŠ¨æ€ç²¾ç»†åŒ–å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCBR-DDIè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå…¶å‡†ç¡®ç‡æ¯”ä¸»æµLLMså’ŒCBRåŸºå‡†æ¨¡å‹æ˜¾è‘—æå‡äº†28.7%ã€‚è¯¥æ¡†æ¶åœ¨æ˜¾è‘—å¢å¼ºé¢„æµ‹æ•ˆèƒ½çš„åŒæ—¶ï¼Œè¿˜ä¿æŒäº†æé«˜çš„å¯è§£é‡Šæ€§ä¸çµæ´»æ€§ï¼Œä¸ºè¯ç‰©å®‰å…¨æ²»ç–—æä¾›äº†æœ‰æ•ˆçš„è¾…åŠ©å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23034v1",
      "published_date": "2025-05-29 03:20:53 UTC",
      "updated_date": "2025-05-29 03:20:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:12:56.262369+00:00"
    },
    {
      "arxiv_id": "2505.23032v3",
      "title": "Bayesian Neural Scaling Law Extrapolation with Prior-Data Fitted Networks",
      "title_zh": "åŸºäºå…ˆéªŒæ•°æ®æ‹Ÿåˆç½‘ç»œçš„è´å¶æ–¯ç¥ç»ç¼©æ”¾æ³•åˆ™å¤–æ¨",
      "authors": [
        "Dongwoo Lee",
        "Dong Bok Lee",
        "Steven Adriaensen",
        "Juho Lee",
        "Sung Ju Hwang",
        "Frank Hutter",
        "Seon Joo Kim",
        "Hae Beom Lee"
      ],
      "abstract": "Scaling has been a major driver of recent advancements in deep learning. Numerous empirical studies have found that scaling laws often follow the power-law and proposed several variants of power-law functions to predict the scaling behavior at larger scales. However, existing methods mostly rely on point estimation and do not quantify uncertainty, which is crucial for real-world applications involving decision-making problems such as determining the expected performance improvements achievable by investing additional computational resources. In this work, we explore a Bayesian framework based on Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation. Specifically, we design a prior distribution that enables the sampling of infinitely many synthetic functions resembling real-world neural scaling laws, allowing our PFN to meta-learn the extrapolation. We validate the effectiveness of our approach on real-world neural scaling laws, comparing it against both the existing point estimation methods and Bayesian approaches. Our method demonstrates superior performance, particularly in data-limited scenarios such as Bayesian active learning, underscoring its potential for reliable, uncertainty-aware extrapolation in practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå…ˆéªŒæ•°æ®æ‹Ÿåˆç½‘ç»œ (Prior-data Fitted Networks, PFNs) çš„è´å¶æ–¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¥ç»ç¼©æ”¾å®šå¾‹ (Neural Scaling Law) å¤–æ¨ä¸­ç°æœ‰æ–¹æ³•ä¾èµ–ç‚¹ä¼°è®¡ä¸”ç¼ºä¹ä¸ç¡®å®šæ€§é‡åŒ–çš„é—®é¢˜ã€‚ä½œè€…é€šè¿‡è®¾è®¡ä¸€ç§ç‰¹å®šçš„å…ˆéªŒåˆ†å¸ƒï¼Œç”Ÿæˆæ¨¡æ‹ŸçœŸå®ç¥ç»ç¼©æ”¾å®šå¾‹çš„åˆæˆå‡½æ•°ï¼Œä½¿ PFN èƒ½å¤Ÿå¯¹è¿™ç±»è§„å¾‹çš„å¤–æ¨è¿›è¡Œå…ƒå­¦ä¹  (meta-learn)ã€‚å®éªŒåœ¨çœŸå®ä¸–ç•Œçš„ç¥ç»ç¼©æ”¾å®šå¾‹æ•°æ®ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„ç‚¹ä¼°è®¡åŠå…¶ä»–è´å¶æ–¯æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è´å¶æ–¯ä¸»åŠ¨å­¦ä¹  (Bayesian active learning) ç­‰æ•°æ®å—é™çš„åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚è¯¥æ–¹æ³•ä¸ºå®é™…åº”ç”¨ä¸­æ¶‰åŠèµ„æºåˆ†é…çš„å†³ç­–é—®é¢˜æä¾›äº†å¯é çš„ã€å…·å¤‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥ (uncertainty-aware) çš„å¤–æ¨èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23032v3",
      "published_date": "2025-05-29 03:19:17 UTC",
      "updated_date": "2025-06-16 01:34:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:04.083102+00:00"
    },
    {
      "arxiv_id": "2505.23027v1",
      "title": "Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift",
      "title_zh": "å¤šæ ·åŒ–åŸå‹é›†æˆæå‡å¯¹å­ç¾¤ä½“åç§»çš„é²æ£’æ€§",
      "authors": [
        "Minh Nguyen Nhat To",
        "Paul F RWilson",
        "Viet Nguyen",
        "Mohamed Harmanani",
        "Michael Cooper",
        "Fahimeh Fooladgar",
        "Purang Abolmaesumi",
        "Parvin Mousavi",
        "Rahul G. Krishnan"
      ],
      "abstract": "The subpopulationtion shift, characterized by a disparity in subpopulation distributibetween theween the training and target datasets, can significantly degrade the performance of machine learning models. Current solutions to subpopulation shift involve modifying empirical risk minimization with re-weighting strategies to improve generalization. This strategy relies on assumptions about the number and nature of subpopulations and annotations on group membership, which are unavailable for many real-world datasets. Instead, we propose using an ensemble of diverse classifiers to adaptively capture risk associated with subpopulations. Given a feature extractor network, we replace its standard linear classification layer with a mixture of prototypical classifiers, where each member is trained to classify the data while focusing on different features and samples from other members. In empirical evaluation on nine real-world datasets, covering diverse domains and kinds of subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often outperforms the prior state-of-the-art in worst-group accuracy. The code is available at https://github.com/minhto2802/dpe4subpop",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å­ç¾¤ä½“åç§»(Subpopulation Shift)å¯¼è‡´æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰åŸºäºé‡åŠ æƒ(Re-weighting)çš„ç­–ç•¥è¿‡åº¦ä¾èµ–é€šå¸¸éš¾ä»¥è·å–çš„ç»„åˆ«æ ‡æ³¨ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†å¤šæ ·åŒ–åŸå‹é›†æˆ(Diverse Prototypical Ensembles, DPEs)æ¡†æ¶ï¼Œé€šè¿‡ä¸€ç»„å·®å¼‚åŒ–çš„åˆ†ç±»å™¨è‡ªé€‚åº”åœ°æ•è·ä¸å­ç¾¤ä½“ç›¸å…³çš„é£é™©ã€‚è¯¥æ–¹æ³•å°†æ ‡å‡†çº¿æ€§åˆ†ç±»å±‚æ›¿æ¢ä¸ºåŸå‹åˆ†ç±»å™¨(Prototypical Classifiers)çš„æ··åˆä½“ï¼Œä¿ƒä½¿æ¯ä¸ªé›†æˆæˆå‘˜å…³æ³¨ä¸åŒçš„ç‰¹å¾å’Œæ ·æœ¬åˆ†å¸ƒã€‚åœ¨æ¶µç›–ä¸åŒé¢†åŸŸå’Œåç§»ç±»å‹çš„ä¹ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒDPEsåœ¨æœ€å·®ç»„å‡†ç¡®ç‡(Worst-group Accuracy)æ–¹é¢ç»å¸¸ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆä¸ä»…æå‡äº†æ¨¡å‹å¯¹ç‰¹å®šå­ç¾¤ä½“çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¹Ÿä¸ºç¼ºä¹æ˜¾å¼æ ‡æ³¨çš„å¤æ‚ç°å®åœºæ™¯æä¾›äº†ç¨³å¥çš„åˆ†ç±»ä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025 Paper",
      "pdf_url": "https://arxiv.org/pdf/2505.23027v1",
      "published_date": "2025-05-29 03:12:56 UTC",
      "updated_date": "2025-05-29 03:12:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:20.275155+00:00"
    },
    {
      "arxiv_id": "2505.23026v2",
      "title": "Context-Robust Knowledge Editing for Language Models",
      "title_zh": "é¢å‘è¯­è¨€æ¨¡å‹çš„è¯­å¢ƒé²æ£’çŸ¥è¯†ç¼–è¾‘",
      "authors": [
        "Haewon Park",
        "Gyubin Choi",
        "Minjun Kim",
        "Yohan Jo"
      ],
      "abstract": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in large language models. Current KE evaluations typically assess editing success by considering only the edited knowledge without any preceding contexts. In real-world applications, however, preceding contexts often trigger the retrieval of the original knowledge and undermine the intended edit. To address this issue, we develop CHED -- a benchmark designed to evaluate the context robustness of KE methods. Evaluations on CHED show that they often fail when preceding contexts are present. To mitigate this shortcoming, we introduce CoRE, a KE method designed to strengthen context robustness by minimizing context-sensitive variance in hidden states of the model for edited knowledge. This method not only improves the editing success rate in situations where a preceding context is present but also preserves the overall capabilities of the model. We provide an in-depth analysis of the differing impacts of preceding contexts when introduced as user utterances versus assistant responses, and we dissect attention-score patterns to assess how specific tokens influence editing success.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†ç¼–è¾‘(Knowledge Editing)ä¸­å®¹æ˜“å—å‰ç½®ä¸Šä¸‹æ–‡å¹²æ‰°çš„é—®é¢˜ï¼Œæå‡ºäº†è¯„ä¼°ä¸Šä¸‹æ–‡é²æ£’æ€§çš„åŸºå‡†æµ‹è¯•CHEDã€‚å®éªŒå‘ç°ï¼Œç°æœ‰çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨å­˜åœ¨å‰ç½®ä¸Šä¸‹æ–‡æ—¶å¾€å¾€å®¹æ˜“å¤±æ•ˆï¼Œå¯¼è‡´æ¨¡å‹ä»è§¦å‘åŸå§‹çŸ¥è¯†è€Œéç¼–è¾‘åçš„çŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†CoREæ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æ¨¡å‹éšè—çŠ¶æ€ä¸­å¯¹ä¸Šä¸‹æ–‡æ•æ„Ÿçš„æ–¹å·®ï¼Œå¢å¼ºäº†ç¼–è¾‘çŸ¥è¯†çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…åœ¨å­˜åœ¨å‰ç½®ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†ç¼–è¾‘æˆåŠŸç‡ï¼Œè¿˜åŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„æ•´ä½“åŸºç¡€èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†ç”¨æˆ·æé—®ä¸åŠ©æ‰‹å›ç­”ä½œä¸ºä¸Šä¸‹æ–‡æ—¶çš„ä¸åŒå½±å“ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›åˆ†æ•°(attention-score)åˆ†æäº†ç‰¹å®šTokenå¯¹ç¼–è¾‘æˆåŠŸç‡çš„å½±å“æœºåˆ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Findings. Our code and datasets are available at https://github.com/holi-lab/CoRE",
      "pdf_url": "https://arxiv.org/pdf/2505.23026v2",
      "published_date": "2025-05-29 03:11:53 UTC",
      "updated_date": "2025-05-31 06:20:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:20.897509+00:00"
    },
    {
      "arxiv_id": "2505.23020v1",
      "title": "AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models",
      "title_zh": "AgentAlignï¼šåº”å¯¹ä»ä¿¡æ¯å‹å‘æ™ºèƒ½ä½“åŒ–å¤§è¯­è¨€æ¨¡å‹è½¬å‹ä¸­çš„å®‰å…¨å¯¹é½",
      "authors": [
        "Jinchuan Zhang",
        "Lu Yin",
        "Yan Zhou",
        "Songlin Hu"
      ],
      "abstract": "The acquisition of agentic capabilities has transformed LLMs from \"knowledge providers\" to \"action executors\", a trend that while expanding LLMs' capability boundaries, significantly increases their susceptibility to malicious use. Previous work has shown that current LLM-based agents execute numerous malicious tasks even without being attacked, indicating a deficiency in agentic use safety alignment during the post-training phase. To address this gap, we propose AgentAlign, a novel framework that leverages abstract behavior chains as a medium for safety alignment data synthesis. By instantiating these behavior chains in simulated environments with diverse tool instances, our framework enables the generation of highly authentic and executable instructions while capturing complex multi-step dynamics. The framework further ensures model utility by proportionally synthesizing benign instructions through non-malicious interpretations of behavior chains, precisely calibrating the boundary between helpfulness and harmlessness. Evaluation results on AgentHarm demonstrate that fine-tuning three families of open-source models using our method substantially improves their safety (35.8% to 79.5% improvement) while minimally impacting or even positively enhancing their helpfulness, outperforming various prompting methods. The dataset and code have both been open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»â€œçŸ¥è¯†æä¾›è€…â€å‘â€œè¡ŒåŠ¨æ‰§è¡Œè€…â€è½¬å˜è¿‡ç¨‹ä¸­ï¼Œå…¶ä»£ç†åŒ–èƒ½åŠ›(agentic capabilities)å¸¦æ¥çš„æ¶æ„åˆ©ç”¨é£é™©ï¼ŒæŒ‡å‡ºå½“å‰æ™ºèƒ½ä½“åœ¨å®‰å…¨å¯¹é½(safety alignment)æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤æå‡ºçš„AgentAlignæ¡†æ¶åˆ©ç”¨æŠ½è±¡è¡Œä¸ºé“¾(abstract behavior chains)ä½œä¸ºåª’ä»‹ï¼Œé€šè¿‡åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å®ä¾‹åŒ–å¤šæ ·å·¥å…·æ¥åˆæˆé«˜åº¦çœŸå®ä¸”å¯æ‰§è¡Œçš„æŒ‡ä»¤æ•°æ®ã€‚è¯¥æ¡†æ¶ä¸ä»…æ•æ‰äº†å¤æ‚çš„å¤šæ­¥åŠ¨æ€ï¼Œè¿˜é€šè¿‡å¯¹è¡Œä¸ºé“¾çš„éæ¶æ„è§£è¯»åˆæˆè‰¯æ€§æŒ‡ä»¤ï¼Œä»¥å¹³è¡¡æ¨¡å‹çš„æœ‰ç”¨æ€§ä¸æ— å®³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨AgentHarmåŸºå‡†ä¸Šå¾®è°ƒåçš„å¼€æºæ¨¡å‹å®‰å…¨æ€§æå‡äº†35.8%è‡³79.5%ï¼Œä¸”å¯¹å…¶æœ‰ç”¨æ€§å‡ ä¹æ²¡æœ‰è´Ÿé¢å½±å“ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„æºä»£ç å’Œæ•°æ®é›†å·²æ­£å¼å¼€æºï¼Œä¸ºæå‡ä»£ç†å¼å¤§æ¨¡å‹çš„å®‰å…¨æ€§æä¾›äº†ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "Submitted to ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23020v1",
      "published_date": "2025-05-29 03:02:18 UTC",
      "updated_date": "2025-05-29 03:02:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:25.441637+00:00"
    },
    {
      "arxiv_id": "2505.23017v3",
      "title": "$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting",
      "title_zh": "$K^2$VAEï¼šç”¨äºæ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹çš„ Koopman-Kalman å¢å¼ºå‹å˜åˆ†è‡ªç¼–ç å™¨",
      "authors": [
        "Xingjian Wu",
        "Xiangfei Qiu",
        "Hongfan Gao",
        "Jilin Hu",
        "Bin Yang",
        "Chenjuan Guo"
      ],
      "abstract": "Probabilistic Time Series Forecasting (PTSF) plays a crucial role in decision-making across various fields, including economics, energy, and transportation. Most existing methods excell at short-term forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have a significant adverse effect on prediction accuracy, and make generative models inefficient by increasing the cost of each iteration. To overcome these limitations, we introduce $K^2$VAE, an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†$K^2$VAEï¼Œä¸€ç§åŸºäºå˜åˆ†è‡ªç¼–ç å™¨(VAE)çš„å¢å¼ºå‹ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é•¿ç¨‹æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹(LPTSF)ä¸­éçº¿æ€§åŠ¨åŠ›å­¦å¯¼è‡´çš„å‡†ç¡®ç‡ä¸‹é™å’Œç”Ÿæˆæ•ˆç‡ä½çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåœ¨äºé€šè¿‡å¼•å…¥KoopmanNetå°†å¤æ‚çš„éçº¿æ€§æ—¶é—´åºåˆ—æ•°æ®è½¬åŒ–ä¸ºçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿï¼Œå¹¶è®¾è®¡äº†KalmanNetåœ¨è¯¥çº¿æ€§ç³»ç»Ÿä¸­è¿›è¡Œé¢„æµ‹å¾®è°ƒå’Œä¸ç¡®å®šæ€§å»ºæ¨¡ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å‡å°‘äº†é•¿ç¨‹é¢„æµ‹ä¸­çš„è¯¯å·®ç´¯ç§¯ï¼Œå¹¶ç¼“è§£äº†ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹åœ¨é•¿é¢„æµ‹å‘¨æœŸä¸‹çš„é«˜æ˜‚è¿­ä»£æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ$K^2$VAEåœ¨çŸ­æœŸå’Œé•¿ç¨‹æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæä¾›äº†æ›´é«˜çš„é¢„æµ‹ç²¾åº¦ã€‚è¯¥ç ”ç©¶ä¸ºè§£å†³ç°å®ä¸–ç•Œä¸­å¤æ‚çš„éçº¿æ€§åŠ¨åŠ›å­¦é¢„æµ‹é—®é¢˜æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ‰©å±•çš„æ¦‚ç‡é¢„æµ‹æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23017v3",
      "published_date": "2025-05-29 02:52:59 UTC",
      "updated_date": "2025-07-26 14:24:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:48.836501+00:00"
    },
    {
      "arxiv_id": "2505.23008v1",
      "title": "Synthetic Document Question Answering in Hungarian",
      "title_zh": "åŒˆç‰™åˆ©è¯­åˆæˆæ–‡æ¡£é—®ç­”",
      "authors": [
        "Jonathan Li",
        "Zoltan Csaki",
        "Nidhi Hiremath",
        "Etash Guha",
        "Fenglu Hong",
        "Edward Ma",
        "Urmish Thakker"
      ],
      "abstract": "Modern VLMs have achieved near-saturation accuracy in English document visual question-answering (VQA). However, this task remains challenging in lower resource languages due to a dearth of suitable training and evaluation data. In this paper we present scalable methods for curating such datasets by focusing on Hungarian, approximately the 17th highest resource language on the internet. Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets that modern VLMs significantly underperform on compared to English DocVQA. HuDocVQA-manual is a small manually curated dataset based on Hungarian documents from Common Crawl, while HuDocVQA is a larger synthetically generated VQA data set from the same source. We apply multiple rounds of quality filtering and deduplication to HuDocVQA in order to match human-level quality in this dataset. We also present HuCCPDF, a dataset of 117k pages from Hungarian Common Crawl PDFs along with their transcriptions, which can be used for training a model for Hungarian OCR. To validate the quality of our datasets, we show how finetuning on a mixture of these datasets can improve accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code will be released to the public to foster further research in multilingual DocVQA.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ä½èµ„æºè¯­è¨€å¦‚åŒˆç‰™åˆ©è¯­ä¸­é¢ä¸´çš„æ–‡æ¡£è§†è§‰é—®ç­”(VQA)æ•°æ®åŒ®ä¹é—®é¢˜ï¼Œæå‡ºäº† HuDocVQA å’Œ HuDocVQA-manual æ•°æ®é›†ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ Common Crawl èµ„æºï¼Œé€šè¿‡äººå·¥ç­›é€‰ä¸å¤§è§„æ¨¡åˆæˆç”Ÿæˆç›¸ç»“åˆçš„æ–¹æ³•æ„å»ºæ•°æ®ï¼Œå¹¶åº”ç”¨å¤šè½®è´¨é‡è¿‡æ»¤ä¸å»é‡æŠ€æœ¯ä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†åŒ…å« 11.7 ä¸‡é¡µæ–‡æ¡£åŠå…¶è½¬å½•å†…å®¹çš„ HuCCPDF æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒåŒˆç‰™åˆ©è¯­ OCR æŠ€æœ¯çš„å¼€å‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ··åˆæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒä½¿ Llama 3.2 11B Instruct æ¨¡å‹åœ¨ç›¸å…³ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æå‡äº† 7.2%ã€‚è¯¥ç ”ç©¶é€šè¿‡å…¬å¼€æ•°æ®é›†å’Œä»£ç ï¼Œä¸ºå¤šè¯­è¨€æ–‡æ¡£é—®ç­”(multilingual DocVQA)é¢†åŸŸçš„æ·±å…¥ç ”ç©¶æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23008v1",
      "published_date": "2025-05-29 02:34:36 UTC",
      "updated_date": "2025-05-29 02:34:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:35.640277+00:00"
    },
    {
      "arxiv_id": "2505.23006v1",
      "title": "A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs",
      "title_zh": "åŸºäºå·¥ä½œæµå›¾æ„å»ºç”Ÿäº§çº§å¯¹è¯æ™ºèƒ½ä½“çš„å®ç”¨æ–¹æ³•",
      "authors": [
        "Chiwan Park",
        "Wonjun Jang",
        "Daeryong Kim",
        "Aelim Ahn",
        "Kichang Yang",
        "Woosung Hwang",
        "Jihyeon Roh",
        "Hyerin Park",
        "Hyosun Wang",
        "Min Seok Kim",
        "Jihoon Kang"
      ],
      "abstract": "The advancement of Large Language Models (LLMs) has led to significant improvements in various service domains, including search, recommendation, and chatbot applications. However, applying state-of-the-art (SOTA) research to industrial settings presents challenges, as it requires maintaining flexible conversational abilities while also strictly complying with service-specific constraints. This can be seen as two conflicting requirements due to the probabilistic nature of LLMs. In this paper, we propose our approach to addressing this challenge and detail the strategies we employed to overcome their inherent limitations in real-world applications. We conduct a practical case study of a conversational agent designed for the e-commerce domain, detailing our implementation workflow and optimizations. Our findings provide insights into bridging the gap between academic research and real-world application, introducing a framework for developing scalable, controllable, and reliable AI-driven agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å·¥ä½œæµå›¾ (Workflow Graphs) æ„å»ºç”Ÿäº§çº§å¯¹è¯æ™ºèƒ½ä½“ (Conversational Agents) çš„å®ç”¨æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨å·¥ä¸šåœºæ™¯ä¸­çµæ´»æ€§ä¸ä¸šåŠ¡çº¦æŸåˆè§„æ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº†å¦‚ä½•é€šè¿‡å·¥ä½œæµä¼˜åŒ–ç­–ç•¥å…‹æœ LLMs å›ºæœ‰çš„æ¦‚ç‡ä¸ç¡®å®šæ€§ï¼Œä»è€Œç¡®ä¿ç³»ç»Ÿåœ¨çœŸå®åº”ç”¨ä¸­çš„å¯é æ€§ã€‚ç ”ç©¶ä»¥ç”µå­å•†åŠ¡é¢†åŸŸ (e-commerce domain) ä¸ºå®é™…æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨å®ç°å¯æ‰©å±• (scalable)ã€å¯æ§ (controllable) ä¸”å¯é çš„ AI é©±åŠ¨æ™ºèƒ½ä½“æ–¹é¢çš„ä¼˜åŠ¿ã€‚ç ”ç©¶ç»“æœä¸ºå¼¥è¡¥å­¦æœ¯å‰æ²¿æŠ€æœ¯ä¸å·¥ä¸šåŒ–è½åœ°ä¹‹é—´çš„é¸¿æ²Ÿæä¾›äº†é‡è¦è§è§£ï¼Œè¯æ˜äº†ç»“æ„åŒ–å·¥ä½œæµå¯¹äºæ„å»ºç”Ÿäº§çº§ AI é©±åŠ¨æœåŠ¡çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Industry Track. 12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.23006v1",
      "published_date": "2025-05-29 02:30:27 UTC",
      "updated_date": "2025-05-29 02:30:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:39.632509+00:00"
    },
    {
      "arxiv_id": "2505.23003v1",
      "title": "Hybrid Cross-domain Robust Reinforcement Learning",
      "title_zh": "æ··åˆå¼è·¨åŸŸé²æ£’å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Linh Le Pham Van",
        "Minh Hoang Nguyen",
        "Hung Le",
        "Hung The Tran",
        "Sunil Gupta"
      ],
      "abstract": "Robust reinforcement learning (RL) aims to learn policies that remain effective despite uncertainties in its environment, which frequently arise in real-world applications due to variations in environment dynamics. The robust RL methods learn a robust policy by maximizing value under the worst-case models within a predefined uncertainty set. Offline robust RL algorithms are particularly promising in scenarios where only a fixed dataset is available and new data cannot be collected. However, these approaches often require extensive offline data, and gathering such datasets for specific tasks in specific environments can be both costly and time-consuming. Using an imperfect simulator offers a faster, cheaper, and safer way to collect data for training, but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO, the first Hybrid Cross-Domain Robust RL framework designed to address these challenges. HYDRO utilizes an online simulator to complement the limited amount of offline datasets in the non-trivial context of robust RL. By measuring and minimizing performance gaps between the simulator and the worst-case models in the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized sampling to select the most relevant and reliable simulator samples. Our extensive experiments demonstrate HYDRO's superior performance over existing methods across various tasks, underscoring its potential to improve sample efficiency in offline robust RL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HYDROï¼Œè¿™æ˜¯é¦–ä¸ªæ··åˆè·¨åŸŸé²æ£’å¼ºåŒ–å­¦ä¹  (Hybrid Cross-Domain Robust RL) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¦»çº¿é²æ£’å¼ºåŒ–å­¦ä¹ åœ¨ç‰¹å®šç¯å¢ƒä¸‹æ•°æ®è·å–æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å·§å¦™åœ°åˆ©ç”¨åœ¨çº¿æ¨¡æ‹Ÿå™¨ (Simulator) æ¥è¡¥å……æœ‰é™çš„ç¦»çº¿æ•°æ®é›†ï¼Œå¹¶é€šè¿‡è¡¡é‡å’Œæœ€å°åŒ–æ¨¡æ‹Ÿå™¨ä¸ä¸ç¡®å®šæ€§é›† (Uncertainty Set) ä¸­æœ€å·®æƒ…å†µæ¨¡å‹ (Worst-case Models) ä¹‹é—´çš„æ€§èƒ½å·®è·æ¥æå‡ç­–ç•¥ç¨³å¥æ€§ã€‚HYDRO å¼•å…¥äº†åˆ›æ–°çš„ä¸ç¡®å®šæ€§è¿‡æ»¤ (Uncertainty Filtering) å’Œä¼˜å…ˆçº§é‡‡æ · (Prioritized Sampling) æœºåˆ¶ï¼Œä»¥ä»æ¨¡æ‹Ÿå™¨ä¸­ç­›é€‰å‡ºæœ€ç›¸å…³ä¸”å¯é çš„æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHYDRO åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†ç¦»çº¿é²æ£’å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ (Sample Efficiency)ï¼Œä¸ºè§£å†³æ¨¡æ‹Ÿå™¨ä¸çœŸå®ç¯å¢ƒé—´çš„åŠ¨åŠ›å­¦ä¸åŒ¹é… (Dynamics Mismatch) æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ECML PKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.23003v1",
      "published_date": "2025-05-29 02:25:13 UTC",
      "updated_date": "2025-05-29 02:25:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:38.634359+00:00"
    },
    {
      "arxiv_id": "2505.23854v1",
      "title": "Revisiting Uncertainty Estimation and Calibration of Large Language Models",
      "title_zh": "é‡æ–°å®¡è§†å¤§è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡ä¸æ ¡å‡†",
      "authors": [
        "Linwei Tao",
        "Yi-Fan Yeh",
        "Minjing Dong",
        "Tao Huang",
        "Philip Torr",
        "Chang Xu"
      ],
      "abstract": "As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å…³é”®åº”ç”¨ä¸­çš„å®‰å…¨æ€§ï¼Œå¯¹æ¶µç›–ä¸åŒè§„æ¨¡ã€æ¶æ„å’Œæ¨ç†èƒ½åŠ›çš„80ä¸ªæ¨¡å‹è¿›è¡Œäº†è¿„ä»Šä¸ºæ­¢æœ€å…¨é¢çš„ä¸ç¡®å®šæ€§ä¼°è®¡(Uncertainty Estimation)ç ”ç©¶ã€‚é€šè¿‡æŒ‘æˆ˜æ€§çš„MMLU-ProåŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶è€…ç³»ç»Ÿè¯„ä¼°äº†åŸºäºTokenæ¦‚ç‡çš„ä¸ç¡®å®šæ€§(TPU)ã€æ•°å€¼å£å¤´ä¸ç¡®å®šæ€§(NVU)å’Œè¯­è¨€å£å¤´ä¸ç¡®å®šæ€§(LVU)ä¸‰ç§ä»£è¡¨æ€§æ–¹æ³•åœ¨ä¸ç¡®å®šæ€§æ ¡å‡†(Calibration)å’Œé€‰æ‹©æ€§åˆ†ç±»ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLVUåœ¨æ ¡å‡†å’ŒåŒºåˆ†èƒ½åŠ›ä¸Šå§‹ç»ˆä¼˜äºTPUå’ŒNVUï¼Œä¸”å…·æœ‰æ›´å¼ºçš„å¯è§£é‡Šæ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œé«˜å‡†ç¡®ç‡å¹¶ä¸æ„å‘³ç€å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ¨¡å‹è§„æ¨¡ã€è®­ç»ƒåå¤„ç†ã€æ¨ç†èƒ½åŠ›ä»¥åŠé‡åŒ–(Quantization)å‡ä¼šæ˜¾è‘—å½±å“å…¶æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„ä¼°è®¡æ•ˆæœä¼˜äºçŸ¥è¯†å‹ä»»åŠ¡ï¼Œä¸”è‰¯å¥½çš„æ ¡å‡†æ€§èƒ½å¹¶ä¸ä¸€å®šèƒ½ç›´æ¥è½¬åŒ–ä¸ºæœ‰æ•ˆçš„é”™è¯¯æ’åã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šè§†è§’è¯„ä¼°çš„é‡è¦æ€§ï¼Œå¹¶è¯å®äº†LVUæ˜¯æå‡LLMsåœ¨ç°å®åœºæ™¯ä¸­å¯é æ€§çš„æœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23854v1",
      "published_date": "2025-05-29 02:04:49 UTC",
      "updated_date": "2025-05-29 02:04:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:53.697120+00:00"
    },
    {
      "arxiv_id": "2505.22993v1",
      "title": "Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation",
      "title_zh": "Verify-in-the-Graphï¼šåŸºäºäº¤äº’å¼å›¾è¡¨ç¤ºçš„å¤æ‚å£°æ˜éªŒè¯å®ä½“æ¶ˆæ­§å¢å¼º",
      "authors": [
        "Hoang Pham",
        "Thanh-Do Nguyen",
        "Khac-Hoai Nam Bui"
      ],
      "abstract": "Claim verification is a long-standing and challenging task that demands not only high accuracy but also explainability of the verification process. This task becomes an emerging research issue in the era of large language models (LLMs) since real-world claims are often complex, featuring intricate semantic structures or obfuscated entities. Traditional approaches typically address this by decomposing claims into sub-claims and querying a knowledge base to resolve hidden or ambiguous entities. However, the absence of effective disambiguation strategies for these entities can compromise the entire verification process. To address these challenges, we propose Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and comprehension abilities of LLM agents. VeGraph operates in three phases: (1) Graph Representation - an input claim is decomposed into structured triplets, forming a graph-based representation that integrates both structured and unstructured information; (2) Entity Disambiguation -VeGraph iteratively interacts with the knowledge base to resolve ambiguous entities within the graph for deeper sub-claim verification; and (3) Verification - remaining triplets are verified to complete the fact-checking process. Experiments using Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive performance compared to baselines on two benchmarks HoVer and FEVEROUS, effectively addressing claim verification challenges. Our source code and data are available for further exploitation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ—¶ä»£ä¸‹ï¼Œç°å®ä¸–ç•Œæ–­è¨€(claims)å› å¤æ‚çš„è¯­ä¹‰ç»“æ„æˆ–æ¨¡ç³Šå®ä½“å¯¼è‡´éªŒè¯å‡†ç¡®æ€§ä¸è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†VeGraphæ¡†æ¶ã€‚VeGraphåˆ©ç”¨LLMæ™ºèƒ½ä½“çš„æ¨ç†ä¸ç†è§£èƒ½åŠ›ï¼Œåœ¨Graph Representationé˜¶æ®µå°†æ–­è¨€åˆ†è§£ä¸ºç»“æ„åŒ–çš„ä¸‰å…ƒç»„(triplets)ï¼Œæ„å»ºèµ·æ•´åˆç»“æ„åŒ–ä¸éç»“æ„åŒ–ä¿¡æ¯çš„å›¾è¡¨ç¤ºã€‚åœ¨Entity Disambiguationé˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸çŸ¥è¯†åº“çš„è¿­ä»£äº¤äº’ï¼Œæœ‰æ•ˆè§£å†³å›¾ä¸­çš„æ¨¡ç³Šå®ä½“ï¼Œä»è€Œæ”¯æ’‘æ›´æ·±å±‚æ¬¡çš„å­æ–­è¨€éªŒè¯ã€‚æœ€ç»ˆåœ¨Verificationé˜¶æ®µå¯¹å‰©ä½™ä¸‰å…ƒç»„è¿›è¡Œæ ¸å®ï¼Œå®Œæˆæ•´ä¸ªäº‹å®æ ¸æŸ¥(fact-checking)æµç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºMeta-Llama-3-70Bçš„VeGraphåœ¨HoVerå’ŒFEVEROUSä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡å–å¾—äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºå¤æ‚æ–­è¨€éªŒè¯æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at NAACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2505.22993v1",
      "published_date": "2025-05-29 02:02:55 UTC",
      "updated_date": "2025-05-29 02:02:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:13:56.298415+00:00"
    },
    {
      "arxiv_id": "2505.22990v2",
      "title": "MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design",
      "title_zh": "MenTeRï¼šé¢å‘ç«¯åˆ°ç«¯å°„é¢‘/æ¨¡æ‹Ÿç”µè·¯ç½‘è¡¨è®¾è®¡çš„å…¨è‡ªåŠ¨å¤šæ™ºèƒ½ä½“å·¥ä½œæµ",
      "authors": [
        "Pin-Han Chen",
        "Yu-Sheng Lin",
        "Wei-Cheng Lee",
        "Tin-Yu Leu",
        "Po-Hsiang Hsu",
        "Anjana Dissanayake",
        "Sungjin Oh",
        "Chinq-Shiun Chiu"
      ],
      "abstract": "RF/Analog design is essential for bridging digital technologies with real-world signals, ensuring the functionality and reliability of a wide range of electronic systems. However, analog design procedures are often intricate, time-consuming and reliant on expert intuition, and hinder the time and cost efficiency of circuit development. To overcome the limitations of the manual circuit design, we introduce MenTeR - a multiagent workflow integrated into an end-to-end analog design framework. By employing multiple specialized AI agents that collaboratively address different aspects of the design process, such as specification understanding, circuit optimization, and test bench validation, MenTeR reduces the dependency on frequent trial-and-error-style intervention. MenTeR not only accelerates the design cycle time but also facilitates a broader exploration of the design space, demonstrating robust capabilities in handling real-world analog systems. We believe that MenTeR lays the groundwork for future \"RF/Analog Copilots\" that can collaborate seamlessly with human designers.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MenTeRï¼Œä¸€ç§ç”¨äºç«¯åˆ°ç«¯å°„é¢‘/æ¨¡æ‹Ÿç”µè·¯(RF/Analog Circuits)ç½‘è¡¨è®¾è®¡çš„å…¨è‡ªåŠ¨å¤šæ™ºèƒ½ä½“(Multi-agent)å·¥ä½œæµï¼Œæ—¨åœ¨è§£å†³æ¨¡æ‹Ÿè®¾è®¡è¿‡ç¨‹ä¸­å¤æ‚è€—æ—¶ä¸”è¿‡åº¦ä¾èµ–ä¸“å®¶ç›´è§‰çš„é—®é¢˜ã€‚MenTeR é€šè¿‡é›†æˆå¤šä¸ªä¸“é—¨çš„ AI æ™ºèƒ½ä½“ï¼Œåä½œå¤„ç†æŒ‡æ ‡ç†è§£(Specification understanding)ã€ç”µè·¯ä¼˜åŒ–(Circuit optimization)å’Œæµ‹è¯•å¹³å°éªŒè¯(Test bench validation)ç­‰å…³é”®ç¯èŠ‚ã€‚è¯¥æ¡†æ¶æ˜¾è‘—å‡å°‘äº†è®¾è®¡è¿‡ç¨‹ä¸­é¢‘ç¹çš„äººå·¥è¯•é”™å¹²é¢„ï¼Œæœ‰æ•ˆåŠ é€Ÿäº†ç”µè·¯è®¾è®¡å‘¨æœŸå¹¶å®ç°äº†æ›´å¹¿æ³›çš„è®¾è®¡ç©ºé—´æ¢ç´¢ã€‚å®éªŒè¯æ˜ï¼ŒMenTeR åœ¨å¤„ç†çœŸå®æ¨¡æ‹Ÿç³»ç»Ÿæ—¶å±•ç°å‡ºæé«˜çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”µè·¯å¼€å‘çš„æ—¶é—´å’Œæˆæœ¬æ•ˆç‡ã€‚è¯¥ç ”ç©¶ä¸ä»…å…‹æœäº†æ‰‹åŠ¨ç”µè·¯è®¾è®¡çš„å±€é™æ€§ï¼Œè¿˜ä¸ºæœªæ¥å¼€å‘èƒ½ä¸äººç±»è®¾è®¡å¸ˆæ— ç¼åä½œçš„ RF/Analog Copilots å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 7 figures, accepted by IEEE ICLAD 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22990v2",
      "published_date": "2025-05-29 01:58:08 UTC",
      "updated_date": "2025-06-03 20:36:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:04.943788+00:00"
    },
    {
      "arxiv_id": "2505.22988v2",
      "title": "Model-Preserving Adaptive Rounding",
      "title_zh": "æ¨¡å‹ä¿ç•™è‡ªé€‚åº”èˆå…¥",
      "authors": [
        "Albert Tseng",
        "Zhaofeng Sun",
        "Christopher De Sa"
      ],
      "abstract": "The goal of quantization is to produce a compressed model whose output distribution is as close to the original model's as possible. To do this tractably, most quantization algorithms minimize the immediate activation error of each layer as a proxy for the end-to-end error. However, this ignores the effect of future layers, making it a poor proxy. In this work, we introduce Yet Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that directly considers the error at the network's output. YAQA introduces a series of theoretical results that culminate in the first end-to-end error bounds for quantization algorithms. First, we characterize the convergence time of adaptive rounding algorithms via the structure of their Hessian approximations. We then show that the end-to-end error can be bounded by the approximation's cosine similarity to the true Hessian. This admits a natural Kronecker-factored approximation with corresponding near-optimal Hessian sketches. YAQA is provably better than GPTQ/LDLQ and empirically reduces the error by $\\approx 30\\%$ over these methods. YAQA even achieves a lower error than quantization aware training. This translates to state of the art performance on downstream tasks, all while adding no inference overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡åŒ–(Quantization)ç®—æ³•åœ¨æœ€å°åŒ–å±‚æ¿€æ´»è¯¯å·®æ—¶å¿½è§†ç«¯åˆ°ç«¯è¯¯å·®(end-to-end error)çš„é—®é¢˜ï¼Œæå‡ºäº†Yet Another Quantization Algorithm (YAQA)è‡ªé€‚åº”èˆå…¥ç®—æ³•ã€‚YAQAç›´æ¥è€ƒè™‘ç½‘ç»œè¾“å‡ºç«¯çš„è¯¯å·®ï¼Œå¹¶é€šè¿‡ç†è®ºç ”ç©¶æ­ç¤ºäº†æ”¶æ•›æ—¶é—´ä¸Hessianè¿‘ä¼¼ç»“æ„çš„å…³ç³»ï¼Œè¯æ˜äº†ç«¯åˆ°ç«¯è¯¯å·®å—è¿‘ä¼¼å€¼ä¸çœŸå®Hessianä¹‹é—´ä½™å¼¦ç›¸ä¼¼åº¦(cosine similarity)çš„é™åˆ¶ã€‚è¯¥ç®—æ³•å¼•å…¥äº†Kronecker-factoredè¿‘ä¼¼åŠç›¸åº”çš„è¿‘ä¼˜Hessian sketchesï¼Œåœ¨ç†è®ºä¸Šä¼˜äºGPTQå’ŒLDLQï¼Œå¹¶åœ¨å®éªŒä¸­å°†è¯¯å·®é™ä½äº†çº¦30%ï¼Œè¡¨ç°ç”šè‡³è¶…è¿‡äº†é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ(Quantization Aware Training)ã€‚æœ€ç»ˆï¼ŒYAQAåœ¨ä¸å¢åŠ ä»»ä½•æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†state of the artçš„æ€§èƒ½ï¼Œä¸ºæ¨¡å‹å‹ç¼©æä¾›äº†æ›´ä¼˜çš„ç†è®ºè¾¹ç•Œå’Œå®è·µæ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2505.22988v2",
      "published_date": "2025-05-29 01:53:00 UTC",
      "updated_date": "2025-09-26 02:30:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:29.334393+00:00"
    },
    {
      "arxiv_id": "2505.22987v2",
      "title": "Strategic Reflectivism In Intelligent Systems",
      "title_zh": "æ™ºèƒ½ç³»ç»Ÿä¸­çš„ç­–ç•¥æ€§åæ€ä¸»ä¹‰",
      "authors": [
        "Nick Byrd"
      ],
      "abstract": "By late 20th century, the rationality wars had launched debates about the nature and norms of intuitive and reflective thinking. Those debates drew from mid-20th century ideas such as bounded rationality, which challenged more idealized notions of rationality observed since the 19th century. Now that 21st century cognitive scientists are applying the resulting dual pro-cess theories to artificial intelligence, it is time to dust off some lessons from this history. So this paper synthesizes old ideas with recent results from experiments on humans and machines. The result is Strategic Reflec-tivism, the position that one key to intelligent systems (human or artificial) is pragmatic switching between intuitive and reflective inference to opti-mally fulfill competing goals. Strategic Reflectivism builds on American Pragmatism, transcends superficial indicators of reflective thinking such as model size or chains of thought, applies to both individual and collective intelligence systems (including human-AI teams), and becomes increasingly actionable as we learn more about the value of intuition and reflection.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†21ä¸–çºªè®¤çŸ¥ç§‘å­¦å®¶å°†åŒé‡è¿‡ç¨‹ç†è®º(Dual Process Theories)åº”ç”¨äºäººå·¥æ™ºèƒ½çš„è¶‹åŠ¿ï¼Œå¹¶ç”±æ­¤æå‡ºäº†ç­–ç•¥æ€§åæ€ä¸»ä¹‰(Strategic Reflectivism)çš„æ ¸å¿ƒè§‚ç‚¹ã€‚è¯¥ç†è®ºè®¤ä¸ºæ™ºèƒ½ç³»ç»Ÿï¼ˆæ— è®ºæ˜¯äººç±»è¿˜æ˜¯äººå·¥æ™ºèƒ½ï¼‰çš„å…³é”®åœ¨äºç›´è§‰æ¨ç†(Intuitive Inference)ä¸åæ€æ¨ç†(Reflective Inference)ä¹‹é—´çš„åŠ¡å®åˆ‡æ¢ï¼Œä»¥å®ç°åœ¨ç«äº‰æ€§ç›®æ ‡ä¹‹é—´çš„æœ€ä¼˜å¹³è¡¡ã€‚è¿™ä¸€æ¡†æ¶æ¤æ ¹äºç¾å›½å®ç”¨ä¸»ä¹‰(American Pragmatism)ï¼Œä¸ä»…é€‚ç”¨äºä¸ªä½“æ™ºèƒ½ï¼Œä¹Ÿæ¶µç›–äº†äººæœºå›¢é˜Ÿç­‰é›†ä½“æ™ºèƒ½ç³»ç»Ÿ(Collective Intelligence Systems)ã€‚ç­–ç•¥æ€§åæ€ä¸»ä¹‰è¶…è¶Šäº†æ¨¡å‹è§„æ¨¡(Model Size)æˆ–æ€ç»´é“¾(Chain-of-Thought)ç­‰åæ€æ€ç»´çš„è¡¨å±‚æŒ‡æ ‡ï¼Œå¼ºè°ƒäº†åŠ¨æ€è°ƒæ•´æ€ç»´æ¨¡å¼çš„é‡è¦æ€§ã€‚éšç€å¯¹ç›´è§‰ä¸åæ€ä»·å€¼çš„ç†è§£æ—¥ç›Šæ·±å…¥ï¼Œè¯¥ç†è®ºä¸ºæ„å»ºæ›´å…·è¡ŒåŠ¨åŠ›ä¸”é«˜æ•ˆçš„æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "econ.TH"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication in the proceedings of the 4th International conference on Human and Artificial Rationality, which are to appear in Lecture Notes in Computer Science. An earlier version was presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning (CHI25-WS-AUGMENTED-REASONING)",
      "pdf_url": "https://arxiv.org/pdf/2505.22987v2",
      "published_date": "2025-05-29 01:51:20 UTC",
      "updated_date": "2025-07-17 20:04:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:41.653081+00:00"
    },
    {
      "arxiv_id": "2505.22985v1",
      "title": "Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition",
      "title_zh": "é’ˆå¯¹å‚¨å¤‡æ± åˆ†ç±»å™¨çš„çŸ¥è¯†è’¸é¦ï¼šäººä½“æ´»åŠ¨è¯†åˆ«",
      "authors": [
        "Masaharu Kagiyama",
        "Tsuyoshi Okita"
      ],
      "abstract": "This paper aims to develop an energy-efficient classifier for time-series data by introducing PatchEchoClassifier, a novel model that leverages a reservoir-based mechanism known as the Echo State Network (ESN). The model is designed for human activity recognition (HAR) using one-dimensional sensor signals and incorporates a tokenizer to extract patch-level representations. To train the model efficiently, we propose a knowledge distillation framework that transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight reservoir-based student model. Experimental evaluations on multiple HAR datasets demonstrate that our model achieves over 80 percent accuracy while significantly reducing computational cost. Notably, PatchEchoClassifier requires only about one-sixth of the floating point operations (FLOPS) compared to DeepConvLSTM, a widely used convolutional baseline. These results suggest that PatchEchoClassifier is a promising solution for real-time and energy-efficient human activity recognition in edge computing environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º PatchEchoClassifier çš„èƒ½æ•ˆå‹åˆ†ç±»å™¨ï¼Œåˆ©ç”¨å›å£°çŠ¶æ€ç½‘ç»œ (Echo State Network) æœºåˆ¶å¤„ç†ä¸€ç»´ä¼ æ„Ÿå™¨ä¿¡å·çš„äººä½“æ´»åŠ¨è¯†åˆ« (Human Activity Recognition) ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥åˆ†è¯å™¨ (tokenizer) æå–è¡¥ä¸çº§ (patch-level) ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨çŸ¥è¯†è’¸é¦ (Knowledge Distillation) æ¡†æ¶ï¼Œå°† MLP-Mixer æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è¿ç§»è‡³è½»é‡çº§å‚¨å±‚å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒPatchEchoClassifier åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†è¶…è¿‡ 80% çš„å‡†ç¡®ç‡ï¼Œä¸”å…¶æµ®ç‚¹è¿ç®—é‡ (FLOPS) ä»…ä¸º DeepConvLSTM åŸºå‡†æ¨¡å‹çš„çº¦å…­åˆ†ä¹‹ä¸€ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†è¯¥æ¨¡å‹åœ¨ä¿è¯è¯†åˆ«ç²¾åº¦çš„æƒ…å†µä¸‹æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œæ˜¯è¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸‹å®ç°å®æ—¶ã€ä½èƒ½è€—äººä½“æ´»åŠ¨è¯†åˆ«çš„æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages,5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.22985v1",
      "published_date": "2025-05-29 01:48:36 UTC",
      "updated_date": "2025-05-29 01:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:25.035542+00:00"
    },
    {
      "arxiv_id": "2506.03174v1",
      "title": "Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks",
      "title_zh": "é¢å‘è·¨æ¨¡æ€æ£€ç´¢ä¸æ´»åŠ¨è¯†åˆ«ä»»åŠ¡çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹",
      "authors": [
        "Koki Matsuishi",
        "Kosuke Ukita",
        "Tsuyoshi Okita"
      ],
      "abstract": "In recent years, the widespread adoption of wearable devices has highlighted the growing importance of behavior analysis using IMU. While applications span diverse fields such as healthcare and robotics, recent studies have increasingly focused on multimodal analysis, in addition to unimodal analysis. Several studies have proposed multimodal foundation models that incorporate first-person video and text data; however, these models still fall short in providing a detailed analysis of full-body human activity. To address this limitation, we propose Activity Understanding and Representations Alignment - Multimodal Foundation Model (AURA-MFM), a foundational model integrating four modalities: third-person video, motion capture, IMU, and text. By incorporating third-person video and motion capture data, the model enables a detailed and multidimensional understanding of human activity, which first-person perspectives alone fail to capture. Additionally, a Transformer-based IMU encoder is employed to enhance the model's overall performance. Experimental evaluations on retrieval and activity recognition tasks demonstrate that our model surpasses existing methods. Notably, in the zero-shot classification for action recognition, our method achieved significantly higher performance, with an F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method recorded an F1-score of 0.0747 and an accuracy of 0.1961.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäºç¬¬ä¸€äººç§°è§†è§’å’Œæ–‡æœ¬çš„å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨äººä½“å…¨èº«æ´»åŠ¨è¯¦ç»†åˆ†ææ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº† AURA-MFMï¼ˆActivity Understanding and Representations Alignment - Multimodal Foundation Modelï¼‰ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°æ•´åˆäº†ç¬¬ä¸‰äººç§°è§†é¢‘ã€åŠ¨ä½œæ•æ‰ (Motion Capture)ã€IMU ä»¥åŠæ–‡æœ¬å››ç§æ¨¡æ€ï¼Œé€šè¿‡å¤šç»´åº¦è§†è§’å®ç°å¯¹äººç±»æ´»åŠ¨çš„æ·±åº¦ç†è§£ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œæ¨¡å‹é‡‡ç”¨äº†ä¸€ä¸ªåŸºäº Transformer çš„ IMU ç¼–ç å™¨æ¥å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®ã€‚åœ¨è·¨æ¨¡æ€æ£€ç´¢å’Œæ´»åŠ¨è¯†åˆ«ä»»åŠ¡çš„å®éªŒè¯„ä¼°ä¸­ï¼ŒAURA-MFM è¡¨ç°å‡ºæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œè¯†åˆ«çš„é›¶æ ·æœ¬åˆ†ç±» (Zero-shot classification) ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹å–å¾—äº† 0.6226 çš„ F1-score å’Œ 0.7320 çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…é¢†å…ˆäºå¯¹æ¯”åŸºçº¿ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚è¡Œä¸ºåˆ†æé¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "25 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.03174v1",
      "published_date": "2025-05-29 01:47:43 UTC",
      "updated_date": "2025-05-29 01:47:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:27.930524+00:00"
    },
    {
      "arxiv_id": "2505.23852v1",
      "title": "Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„è‡ªåŠ¨åŒ–ç§‘ç ”å¤ç°ï¼šé˜¿å°”èŒ¨æµ·é»˜ç—…æ¢ç´¢æ€§ç ”ç©¶",
      "authors": [
        "Nic Dobbins",
        "Christelle Xiong",
        "Kristine Lan",
        "Meliha Yetisgen"
      ],
      "abstract": "Objective: To demonstrate the capabilities of Large Language Models (LLMs) as autonomous agents to reproduce findings of published research studies using the same or similar dataset.\n  Materials and Methods: We used the \"Quick Access\" dataset of the National Alzheimer's Coordinating Center (NACC). We identified highly cited published research manuscripts using NACC data and selected five studies that appeared reproducible using this dataset alone. Using GPT-4o, we created a simulated research team of LLM-based autonomous agents tasked with writing and executing code to dynamically reproduce the findings of each study, given only study Abstracts, Methods sections, and data dictionary descriptions of the dataset.\n  Results: We extracted 35 key findings described in the Abstracts across 5 Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of findings per study. Numeric values and range-based findings often differed between studies and agents. The agents also applied statistical methods or parameters that varied from the originals, though overall trends and significance were sometimes similar.\n  Discussion: In some cases, LLM-based agents replicated research techniques and findings. In others, they failed due to implementation flaws or missing methodological detail. These discrepancies show the current limits of LLMs in fully automating reproducibility assessments. Still, this early investigation highlights the potential of structured agent-based systems to provide scalable evaluation of scientific rigor.\n  Conclusion: This exploratory work illustrates both the promise and limitations of LLMs as autonomous agents for automating reproducibility in biomedical research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢ç´¢äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)çš„è‡ªä¸»æ™ºèƒ½ä½“åœ¨è‡ªåŠ¨é‡ç°ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ç»“æœæ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…(Alzheimer's Disease)é¢†åŸŸã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ GPT-4o æ„å»ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿç ”ç©¶å›¢é˜Ÿï¼Œä»»åŠ¡æ˜¯æ ¹æ®è®ºæ–‡æ‘˜è¦ã€æ–¹æ³•éƒ¨åˆ†å’Œæ•°æ®å­—å…¸ï¼Œç¼–å†™å¹¶æ‰§è¡Œä»£ç ä»¥åŠ¨æ€é‡ç° 5 é¡¹åŸºäºå›½å®¶é˜¿å°”èŒ¨æµ·é»˜ç—…åè°ƒä¸­å¿ƒ(NACC)æ•°æ®é›†çš„å·²å‘è¡¨ç ”ç©¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLM æ™ºèƒ½ä½“å¹³å‡èƒ½é‡ç°æ¯é¡¹ç ”ç©¶ä¸­çº¦ 53.2% çš„æ ¸å¿ƒå‘ç°ã€‚è™½ç„¶åœ¨æŸäº›æƒ…å†µä¸‹æ™ºèƒ½ä½“æˆåŠŸå¤åˆ¶äº†ç ”ç©¶æŠ€æœ¯å’Œå‘ç°ï¼Œä½†åœ¨æ•°å€¼ç²¾ç¡®åº¦ã€ç»Ÿè®¡æ–¹æ³•é€‰æ‹©å’Œå‚æ•°è®¾ç½®ä¸Šä¸åŸç ”ç©¶å­˜åœ¨æ˜æ˜¾å·®å¼‚ã€‚å¤±è´¥æ¡ˆä¾‹ä¸»è¦å½’å› äºæ‰§è¡Œå±‚é¢çš„ç¼ºé™·æˆ–åŸå§‹ç ”ç©¶ä¸­ç¼ºä¹å…³é”®çš„æ–¹æ³•è®ºç»†èŠ‚ï¼Œæ­ç¤ºäº†å½“å‰ LLMs åœ¨å®Œå…¨è‡ªåŠ¨åŒ–ç§‘å­¦ä¸¥è°¨æ€§è¯„ä¼°æ–¹é¢çš„å±€é™æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹æ¢ç´¢æ€§å·¥ä½œå±•ç¤ºäº†ç»“æ„åŒ–æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æä¾›å¯æ‰©å±•çš„ç ”ç©¶å¯é‡å¤æ€§è¯„ä¼°æ–¹é¢çš„å·¨å¤§å‰æ™¯ï¼ŒåŒæ—¶ä¹Ÿæ˜ç¡®äº†ç›®å‰é¢ä¸´çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA",
        "stat.AP"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.23852v1",
      "published_date": "2025-05-29 01:31:55 UTC",
      "updated_date": "2025-05-29 01:31:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:33.163194+00:00"
    },
    {
      "arxiv_id": "2505.22976v1",
      "title": "Toward Memory-Aided World Models: Benchmarking via Spatial Consistency",
      "title_zh": "è¿ˆå‘è®°å¿†è¾…åŠ©çš„ä¸–ç•Œæ¨¡å‹ï¼šåŸºäºç©ºé—´ä¸€è‡´æ€§çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Kewei Lian",
        "Shaofei Cai",
        "Yilun Du",
        "Yitao Liang"
      ],
      "abstract": "The ability to simulate the world in a spatially consistent manner is a crucial requirements for effective world models. Such a model enables high-quality visual generation, and also ensures the reliability of world models for downstream tasks such as simulation and planning. Designing a memory module is a crucial component for addressing spatial consistency: such a model must not only retain long-horizon observational information, but also enables the construction of explicit or implicit internal spatial representations. However, there are no dataset designed to promote the development of memory modules by explicitly enforcing spatial consistency constraints. Furthermore, most existing benchmarks primarily emphasize visual coherence or generation quality, neglecting the requirement of long-range spatial consistency. To bridge this gap, we construct a dataset and corresponding benchmark by sampling 150 distinct locations within the open-world environment of Minecraft, collecting about 250 hours (20 million frames) of loop-based navigation videos with actions. Our dataset follows a curriculum design of sequence lengths, allowing models to learn spatial consistency on increasingly complex navigation trajectories. Furthermore, our data collection pipeline is easily extensible to new Minecraft environments and modules. Four representative world model baselines are evaluated on our benchmark. Dataset, benchmark, and code are open-sourced to support future research.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼ºè°ƒäº†å…·å¤‡ç©ºé—´ä¸€è‡´æ€§(Spatial Consistency)çš„ä¸–ç•Œæ¨¡å‹(World Models)åœ¨è§†è§‰ç”ŸæˆåŠä¸‹æ¸¸è§„åˆ’ä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºè®¾è®¡è®°å¿†æ¨¡å—(Memory Module)æ˜¯å®ç°é•¿ç¨‹è§‚æµ‹ä¿ç•™å’Œå†…éƒ¨ç©ºé—´è¡¨å¾çš„æ ¸å¿ƒã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†è¿‡åº¦å…³æ³¨è§†è§‰è¿è´¯æ€§è€Œå¿½è§†é•¿ç¨‹ç©ºé—´ä¸€è‡´æ€§çš„ç°çŠ¶ï¼Œä½œè€…åœ¨Minecraftå¼€æ”¾ä¸–ç•Œä¸­é‡‡æ ·äº†150ä¸ªåœ°ç‚¹ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«250å°æ—¶ã€å…±è®¡2000ä¸‡å¸§å¸¦åŠ¨ä½œçš„å¾ªç¯å¯¼èˆªè§†é¢‘æ•°æ®é›†åŠç›¸åº”è¯„æµ‹åŸºå‡†ã€‚è¯¥æ•°æ®é›†é€šè¿‡åºåˆ—é•¿åº¦çš„è¯¾ç¨‹è®¾è®¡(Curriculum Design)ï¼Œæ”¯æŒæ¨¡å‹åœ¨å¤æ‚å¯¼èˆªè½¨è¿¹ä¸­é€æ­¥å­¦ä¹ ç©ºé—´ä¸€è‡´æ€§ï¼Œä¸”æ•°æ®é‡‡é›†æµç¨‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚é€šè¿‡å¯¹å››ç§ä»£è¡¨æ€§ä¸–ç•Œæ¨¡å‹åŸºçº¿çš„è¯„ä¼°ï¼Œè¯¥ç ”ç©¶ä¸ºè®°å¿†è¾…åŠ©å‹ä¸–ç•Œæ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚ç›®å‰ï¼Œç›¸å…³æ•°æ®é›†ã€åŸºå‡†å’Œä»£ç å‡å·²å¼€æºï¼Œä»¥ä¿ƒè¿›åç»­ç©ºé—´ä¸€è‡´æ€§é¢†åŸŸçš„ç ”ç©¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22976v1",
      "published_date": "2025-05-29 01:28:57 UTC",
      "updated_date": "2025-05-29 01:28:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:42.030100+00:00"
    },
    {
      "arxiv_id": "2505.22973v1",
      "title": "EquiReg: Equivariance Regularized Diffusion for Inverse Problems",
      "title_zh": "EquiRegï¼šç”¨äºåé—®é¢˜çš„ç­‰å˜æ­£åˆ™åŒ–æ‰©æ•£",
      "authors": [
        "Bahareh Tolooshams",
        "Aditi Chandrashekar",
        "Rayhan Zirvi",
        "Abbas Mammadov",
        "Jiachen Yao",
        "Chuwei Wang",
        "Anima Anandkumar"
      ],
      "abstract": "Diffusion models represent the state-of-the-art for solving inverse problems such as image restoration tasks. In the Bayesian framework, diffusion-based inverse solvers incorporate a likelihood term to guide the prior sampling process, generating data consistent with the posterior distribution. However, due to the intractability of the likelihood term, many current methods rely on isotropic Gaussian approximations, which lead to deviations from the data manifold and result in inconsistent, unstable reconstructions. We propose Equivariance Regularized (EquiReg) diffusion, a general framework for regularizing posterior sampling in diffusion-based inverse problem solvers. EquiReg enhances reconstructions by reweighting diffusion trajectories and penalizing those that deviate from the data manifold. We define a new distribution-dependent equivariance error, empirically identify functions that exhibit low error for on-manifold samples and higher error for off-manifold samples, and leverage these functions to regularize the diffusion sampling process. When applied to a variety of solvers, EquiReg outperforms state-of-the-art diffusion models in both linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EquiReg (Equivariance Regularized)æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ‰©æ•£æ¨¡å‹(Diffusion models)åœ¨å¤„ç†é€†é—®é¢˜(Inverse problems)æ—¶ï¼Œå› ä¼¼ç„¶é¡¹è¿‘ä¼¼ä¸å‡†å¯¼è‡´é‡å»ºç»“æœåç¦»æ•°æ®æµå½¢(Data manifold)ä¸”ä¸ç¨³å®šçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å®šä¹‰ä¸€ç§æ–°çš„åˆ†å¸ƒä¾èµ–ç­‰å˜æ€§è¯¯å·®(Distribution-dependent equivariance error)ï¼Œè¯†åˆ«å¹¶æƒ©ç½šé‡‡æ ·è¿‡ç¨‹ä¸­åç¦»æµå½¢çš„è½¨è¿¹ï¼Œä»è€Œå®ç°å¯¹åéªŒé‡‡æ ·çš„æœ‰æ•ˆæ­£åˆ™åŒ–ã€‚ç ”ç©¶è€…é€šè¿‡åˆ©ç”¨å¯¹æµå½¢å†…æ ·æœ¬è¡¨ç°å‡ºä½è¯¯å·®è€Œå¯¹æµå½¢å¤–æ ·æœ¬è¡¨ç°å‡ºé«˜è¯¯å·®çš„ç‰¹å®šå‡½æ•°ï¼Œæ˜¾è‘—å¢å¼ºäº†é‡‡æ ·çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEquiRegåœ¨å¤šç§çº¿æ€§å’Œéçº¿æ€§å›¾åƒä¿®å¤ä»»åŠ¡ä»¥åŠåå¾®åˆ†æ–¹ç¨‹(Partial differential equations)é‡å»ºä¸­ï¼Œå‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ±‚è§£å™¨ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22973v1",
      "published_date": "2025-05-29 01:25:43 UTC",
      "updated_date": "2025-05-29 01:25:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:41.392603+00:00"
    },
    {
      "arxiv_id": "2506.03173v2",
      "title": "FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution",
      "title_zh": "FOLIAGEï¼šé€šè¿‡æ— ç•Œè¡¨é¢æ¼”åŒ–è¿ˆå‘ç‰©ç†æ™ºèƒ½ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Xiaoyi Liu",
        "Hao Tang"
      ],
      "abstract": "Physical intelligence -- anticipating and shaping the world from partial, multisensory observations -- is critical for next-generation world models. We propose FOLIAGE, a physics-informed multimodal world model for unbounded accretive surface growth. In its Action-Perception loop, a unified context encoder maps images, mesh connectivity, and point clouds to a shared latent state. A physics-aware predictor, conditioned on physical control actions, advances this latent state in time to align with the target latent of the surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network (AGN) captures dynamic connectivity through Age Positional Encoding and Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances global context with local dynamics. We create SURF-GARDEN, a world model learning platform comprising a Counterfactual Physics Simulator, a Multimodal Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation suite, evaluates six core tasks -- topology recognition, inverse material estimation, growth-stage classification, latent roll-out, cross-modal retrieval, and dense correspondence -- and four stress tests -- sensor dropout, zero-shot modality transfer, long-horizon prediction, and physics ablation -- to probe resilience. FOLIAGE outperforms specialized baselines while remaining robust across dynamic environments, establishing a new world-model based, multimodal pathway to physical intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FOLIAGEï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘ç‰©ç†æ™ºèƒ½ (Physical Intelligence) çš„ç‰©ç†æ„ŸçŸ¥å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹æ— è¾¹ç•Œè¡¨é¢å¢é•¿ (Unbounded Surface Evolution) è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ¡†æ¶é€šè¿‡ Action-Perception é—­ç¯ï¼Œåˆ©ç”¨ç»Ÿä¸€ç¼–ç å™¨å°†å›¾åƒã€ç½‘æ ¼è¿æ¥å’Œç‚¹äº‘æ˜ å°„åˆ°å…±äº«æ½œç©ºé—´ï¼Œå¹¶ç”Ÿæˆæ¨¡æ€æ— å…³å¢é•¿åµŒå…¥ (Modality-Agnostic Growth Embedding, MAGE) ä»¥æ”¯æŒä¸‹æ¸¸ç›®æ ‡ã€‚å…¶æ ¸å¿ƒç»„ä»¶å¢é‡å›¾ç½‘ç»œ (Accretive Graph Network, AGN) ç»“åˆäº†å¹´é¾„ä½ç½®ç¼–ç  (Age Positional Encoding) å’Œèƒ½é‡é—¨æ§æ¶ˆæ¯ä¼ é€’ (Energy-Gated Message-Passing) æŠ€æœ¯ï¼Œä»¥ç²¾ç¡®æ•æ‰åŠ¨æ€æ‹“æ‰‘è¿æ¥ã€‚ä¸ºæ”¯æŒæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åŒ…å« 7,200 ä¸ªåºåˆ—çš„ SURF-GARDEN å­¦ä¹ å¹³å°ä»¥åŠæ¶µç›–å…­é¡¹æ ¸å¿ƒä»»åŠ¡å’Œå››é¡¹å‹åŠ›æµ‹è¯•çš„è¯„æµ‹åŸºå‡† SURF-BENCHã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFOLIAGE åœ¨æ‹“æ‰‘è¯†åˆ«ã€ç‰©ç†é¢„æµ‹ç­‰ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºä¸“ä¸šåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨ä¼ æ„Ÿå™¨ä¸¢å¤±å’Œé•¿ç¨‹é¢„æµ‹ç­‰æç«¯ç¯å¢ƒä¸‹å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ï¼Œä¸ºé€šå¾€ç‰©ç†æ™ºèƒ½çš„å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹ç ”ç©¶æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03173v2",
      "published_date": "2025-05-29 01:16:58 UTC",
      "updated_date": "2025-06-05 02:00:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:45.016177+00:00"
    },
    {
      "arxiv_id": "2505.22964v2",
      "title": "Exploring Scaling Laws for EHR Foundation Models",
      "title_zh": "EHR åŸºç¡€æ¨¡å‹ç¼©æ”¾æ³•åˆ™çš„æ¢ç©¶",
      "authors": [
        "Sheng Zhang",
        "Qin Liu",
        "Naoto Usuyama",
        "Cliff Wong",
        "Tristan Naumann",
        "Hoifung Poon"
      ],
      "abstract": "The emergence of scaling laws has profoundly shaped the development of large language models (LLMs), enabling predictable performance gains through systematic increases in model size, dataset volume, and compute. Yet, these principles remain largely unexplored in the context of electronic health records (EHRs) -- a rich, sequential, and globally abundant data source that differs structurally from natural language. In this work, we present the first empirical investigation of scaling laws for EHR foundation models. By training transformer architectures on patient timeline data from the MIMIC-IV database across varying model sizes and compute budgets, we identify consistent scaling patterns, including parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility. These findings demonstrate that EHR models exhibit scaling behavior analogous to LLMs, offering predictive insights into resource-efficient training strategies. Our results lay the groundwork for developing powerful EHR foundation models capable of transforming clinical prediction tasks and advancing personalized healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­å¥åº·è®°å½•(Electronic Health Records, EHR)åŸºç¡€æ¨¡å‹è¿›è¡Œäº†é¦–æ¬¡ç¼©æ”¾æ³•åˆ™(Scaling Laws)çš„å®è¯æ¢è®¨ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€é¢†åŸŸåœ¨ç»“æ„åŒ–åŒ»ç–—æ•°æ®ä¸Šçš„ç ”ç©¶ç©ºç™½ã€‚é€šè¿‡åœ¨MIMIC-IVæ•°æ®åº“çš„æ‚£è€…æ—¶é—´è½´æ•°æ®ä¸Šè®­ç»ƒä¸åŒæ¨¡å‹è§„æ¨¡ä¸è®¡ç®—é¢„ç®—çš„Transformeræ¶æ„ï¼Œç ”ç©¶è€…è¯†åˆ«å‡ºäº†ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)ç±»ä¼¼çš„ç¼©æ”¾è§„å¾‹ã€‚å®éªŒç»“æœå±•ç¤ºäº†æ¸…æ™°çš„æŠ›ç‰©çº¿IsoFLOPsæ›²çº¿ï¼Œå¹¶æ­ç¤ºäº†è®¡ç®—é‡ã€æ¨¡å‹å‚æ•°ã€æ•°æ®é‡ä¸ä¸´åºŠæ•ˆç”¨(Clinical Utility)ä¹‹é—´éµå¾ªå¹‚å¾‹å…³ç³»(Power-law relationships)ã€‚ç ”ç©¶è¯æ˜äº†EHRæ¨¡å‹çš„æ€§èƒ½å¢ç›Šå¯ä»¥é€šè¿‡ç³»ç»Ÿæ€§å¢åŠ èµ„æºæ¥å®ç°é¢„æµ‹ï¼Œä¸ºåˆ¶å®šèµ„æºé«˜æ•ˆçš„è®­ç»ƒç­–ç•¥æä¾›äº†é‡è¦å‚è€ƒã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ›´å¼ºå¤§çš„EHRåŸºç¡€æ¨¡å‹å¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œå¯¹æå‡ä¸´åºŠé¢„æµ‹å‡†ç¡®æ€§å’Œæ¨åŠ¨ä¸ªæ€§åŒ–åŒ»ç–—å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22964v2",
      "published_date": "2025-05-29 01:05:11 UTC",
      "updated_date": "2025-08-13 06:20:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:14:47.036399+00:00"
    },
    {
      "arxiv_id": "2505.22960v2",
      "title": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness",
      "title_zh": "é‡æ–°å®¡è§†ä½œä¸ºæ¨ç†æ—¶æ‰©å±•çš„å¤šæ™ºèƒ½ä½“è¾©è®ºï¼šæ¡ä»¶æœ‰æ•ˆæ€§çš„ç³»ç»Ÿæ€§ç ”ç©¶",
      "authors": [
        "Yongjin Yang",
        "Euiin Yi",
        "Jongwoo Ko",
        "Kimin Lee",
        "Zhijing Jin",
        "Se-Young Yun"
      ],
      "abstract": "The remarkable growth in large language model (LLM) capabilities has spurred exploration into multi-agent systems, with debate frameworks emerging as a promising avenue for enhanced problem-solving. These multi-agent debate (MAD) approaches, where agents collaboratively present, critique, and refine arguments, potentially offer improved reasoning, robustness, and diverse perspectives over monolithic models. Despite prior studies leveraging MAD, a systematic understanding of its effectiveness compared to self-agent methods, particularly under varying conditions, remains elusive. This paper seeks to fill this gap by conceptualizing MAD as a test-time computational scaling technique, distinguished by collaborative refinement and diverse exploration capabilities. We conduct a comprehensive empirical investigation comparing MAD with strong self-agent test-time scaling baselines on mathematical reasoning and safety-related tasks. Our study systematically examines the influence of task difficulty, model scale, and agent diversity on MAD's performance. Key findings reveal that, for mathematical reasoning, MAD offers limited advantages over self-agent scaling but becomes more effective with increased problem difficulty and decreased model capability, while agent diversity shows little benefit. Conversely, for safety tasks, MAD's collaborative refinement can increase vulnerability, but incorporating diverse agent configurations facilitates a gradual reduction in attack success through the collaborative refinement process. We believe our findings provide critical guidance for the future development of more effective and strategically deployed MAD systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMulti-Agent Debate, MADï¼‰ä½œä¸ºä¸€ç§æ¨ç†ä¾§è®¡ç®—æ‰©å±•ï¼ˆtest-time scalingï¼‰æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åœ¨æ•°å­¦æ¨ç†å’Œå®‰å…¨æ€§ä»»åŠ¡ä¸Šå°† MAD ä¸è‡ªæ™ºèƒ½ä½“ï¼ˆself-agentï¼‰åŸºçº¿è¿›è¡Œå¯¹æ¯”ï¼Œç ”ç©¶åˆ†æäº†ä»»åŠ¡éš¾åº¦ã€æ¨¡å‹è§„æ¨¡åŠæ™ºèƒ½ä½“å¤šæ ·æ€§ï¼ˆagent diversityï¼‰å¯¹æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ŒMAD ä»…åœ¨é—®é¢˜éš¾åº¦å¢åŠ æˆ–æ¨¡å‹èƒ½åŠ›å—é™æ—¶æ˜¾ç¤ºå‡ºä¼˜äºè‡ªæ™ºèƒ½ä½“çš„æ½œåŠ›ï¼Œä¸”æ™ºèƒ½ä½“å¤šæ ·æ€§çš„æå‡ä½œç”¨æœ‰é™ã€‚è€Œåœ¨å®‰å…¨æ€§ä»»åŠ¡ä¸­ï¼Œè™½ç„¶ MAD çš„ååŒä¼˜åŒ–å¯èƒ½å¢åŠ æ¨¡å‹çš„è„†å¼±æ€§ï¼Œä½†å¼•å…¥å¤šæ ·åŒ–çš„æ™ºèƒ½ä½“é…ç½®æœ‰åŠ©äºé€šè¿‡ååŒè¿‡ç¨‹é€æ­¥é™ä½æ”»å‡»æˆåŠŸç‡ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº† MAD çš„æ¡ä»¶æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥æ›´æœ‰æ•ˆåœ°å¼€å‘å’Œéƒ¨ç½²å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿæä¾›äº†å…³é”®æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint, under review",
      "pdf_url": "https://arxiv.org/pdf/2505.22960v2",
      "published_date": "2025-05-29 01:02:55 UTC",
      "updated_date": "2025-06-20 03:07:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:15:03.752803+00:00"
    },
    {
      "arxiv_id": "2505.22954v2",
      "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents",
      "title_zh": "Darwin GÃ¶del Machineï¼šè‡ªæˆ‘æ”¹è¿›æ™ºèƒ½ä½“çš„å¼€æ”¾å¼æ¼”åŒ–",
      "authors": [
        "Jenny Zhang",
        "Shengran Hu",
        "Cong Lu",
        "Robert Lange",
        "Jeff Clune"
      ],
      "abstract": "Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The GÃ¶del machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin GÃ¶del Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Darwin GÃ¶del Machine (DGM)ï¼Œä¸€ç§æ—¨åœ¨å®ç°è‡ªæˆ‘æ”¹è¿›æ™ºèƒ½ä½“(Self-improving Agents)å¼€æ”¾å¼è¿›åŒ–(Open-Ended Evolution)çš„ç³»ç»Ÿï¼Œä»¥è§£å†³å½“å‰AIæ¶æ„å›ºå®šä¸”æ— æ³•è‡ªä¸»æå‡çš„å±€é™ã€‚DGMå—è¾¾å°”æ–‡è¿›åŒ–è®ºå¯å‘ï¼Œé€šè¿‡ä¸æ–­è¿­ä»£ä¿®æ”¹è‡ªèº«çš„ä»£ç åº“å¹¶åˆ©ç”¨ä»£ç åŸºå‡†æµ‹è¯•è¿›è¡Œç»éªŒéªŒè¯ï¼Œä»è€Œå®ç°è‡ªæˆ‘èƒ½åŠ›çš„æŒç»­å¢å¼ºã€‚ç³»ç»Ÿç»´æŠ¤ç€ä¸€ä¸ªç¼–ç æ™ºèƒ½ä½“å­˜æ¡£ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹(Foundation Model)ä»å­˜æ¡£ä¸­é‡‡æ ·å¹¶åˆ›é€ æ›´å¤šå…ƒã€é«˜è´¨é‡çš„æ–°ç‰ˆæœ¬ï¼Œå½¢æˆäº†èƒ½å¤Ÿå¹¶è¡Œæ¢ç´¢å¤šç§è·¯å¾„çš„æ¼”åŒ–æ ‘ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼ŒDGMè‡ªåŠ¨ä¼˜åŒ–äº†ä»£ç ç¼–è¾‘å·¥å…·ã€é•¿ä¸Šä¸‹æ–‡çª—å£ç®¡ç†(Long-context window management)åŠåŒè¡Œè¯„å®¡æœºåˆ¶(Peer-review mechanisms)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDGMåœ¨SWE-benchä¸Šçš„æ€§èƒ½ä»20.0%æå‡è‡³50.0%ï¼Œåœ¨Polyglotä¸Šä»14.2%æå‡è‡³30.7%ï¼Œæ˜¾è‘—ä¼˜äºç¼ºä¹è‡ªæˆ‘æ”¹è¿›æœºåˆ¶çš„åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶åœ¨ç¡®ä¿å®‰å…¨æ²™ç®±å’Œäººå·¥ç›‘ç£çš„å‰æä¸‹ï¼Œä¸ºé€šå¾€èƒ½å¤Ÿè‡ªä¸»å‘ç°åˆ›æ–°è·¯å¾„çš„è‡ªæˆ‘æ”¹è¿›AIè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Code at https://github.com/jennyzzt/dgm",
      "pdf_url": "https://arxiv.org/pdf/2505.22954v2",
      "published_date": "2025-05-29 00:26:15 UTC",
      "updated_date": "2025-09-26 16:36:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:15:20.943901+00:00"
    },
    {
      "arxiv_id": "2505.22948v1",
      "title": "Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages",
      "title_zh": "åŸºç¡€åˆ†å­æ–‡æ³•ï¼šå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹è¯±å¯¼å¯è§£é‡Šçš„åˆ†å­å›¾è¯­è¨€",
      "authors": [
        "Michael Sun",
        "Weize Yuan",
        "Gang Liu",
        "Wojciech Matusik",
        "Jie Chen"
      ],
      "abstract": "Recent data-efficient molecular generation approaches exploit graph grammars to introduce interpretability into the generative models. However, grammar learning therein relies on expert annotation or unreliable heuristics for algorithmic inference. We propose Foundation Molecular Grammar (FMG), which leverages multi-modal foundation models (MMFMs) to induce an interpretable molecular language. By exploiting the chemical knowledge of an MMFM, FMG renders molecules as images, describes them as text, and aligns information across modalities using prompt learning. FMG can be used as a drop-in replacement for the prior grammar learning approaches in molecular generation and property prediction. We show that FMG not only excels in synthesizability, diversity, and data efficiency but also offers built-in chemical interpretability for automated molecular discovery workflows. Code is available at https://github.com/shiningsunnyday/induction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Foundation Molecular Grammar (FMG)ï¼Œæ—¨åœ¨åˆ©ç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹(Multi-Modal Foundation Models, MMFMs)è¯±å¯¼ç”Ÿæˆå…·æœ‰å¯è§£é‡Šæ€§çš„åˆ†å­å›¾è¯­è¨€ã€‚é’ˆå¯¹ä¼ ç»Ÿåˆ†å­ç”Ÿæˆä¸­è¯­æ³•å­¦ä¹ è¿‡åº¦ä¾èµ–ä¸“å®¶æ ‡æ³¨æˆ–å¯å‘å¼ç®—æ³•çš„é—®é¢˜ï¼ŒFMGé€šè¿‡å°†åˆ†å­æ¸²æŸ“ä¸ºå›¾åƒå¹¶ç»“åˆæ–‡æœ¬æè¿°ï¼Œåˆ©ç”¨æç¤ºå­¦ä¹ (Prompt Learning)åœ¨ä¸åŒæ¨¡æ€é—´å¯¹é½åŒ–å­¦çŸ¥è¯†ã€‚FMGå¯ä»¥ä½œä¸ºåˆ†å­ç”Ÿæˆå’Œå±æ€§é¢„æµ‹ä»»åŠ¡ä¸­è¯­æ³•å­¦ä¹ æ¨¡å—çš„å³æ’å³ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ•°æ®æ•ˆç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFMGåœ¨åˆ†å­å¯åˆæˆæ€§(Synthesizability)å’Œå¤šæ ·æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½ä¸ºè‡ªåŠ¨åŒ–åˆ†å­å‘ç°å·¥ä½œæµæä¾›å†…ç½®çš„åŒ–å­¦å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.22948v1",
      "published_date": "2025-05-29 00:03:09 UTC",
      "updated_date": "2025-05-29 00:03:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T16:15:22.839690+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 242,
  "processed_papers_count": 242,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T16:16:20.362067+00:00"
}
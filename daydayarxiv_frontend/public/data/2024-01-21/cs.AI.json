{
  "date": "2024-01-21",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-01-21 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 34 篇论文，主要聚焦于人工智能领域，尤其是 Large Language Models (LLMs) 在多代理系统、文本生成和应用中的创新进展，以及图像生成和强化学习的强化方法；令人印象深刻的包括 LLM 多代理系统调查（第 2 篇）和 Hourglass Diffusion Transformers 图像合成新框架（第 7 篇），这些论文展示了 AI 模型的扩展性和实际潜力。\n\n### 重点论文讨论\n我们先聊聊今天最热门和有影响力的论文，特别是那些涉及 LLMs 和图像处理的，相关主题放在一起简要分析。其他较学术或应用性较弱的论文则快速掠过。\n\n#### LLMs 和多代理系统（热门主题，潜力大）\n- **Large Language Model based Multi-Agents: A Survey of Progress and Challenges**（中文：基于大型语言模型的多代理系统：进展和挑战的调查）  \n  这篇论文由 Taicheng Guo 等作者调查了 LLMs 在多代理系统中的应用，包括代理建模、通信机制和能力提升策略。核心贡献是总结了 LLMs 在复杂问题解决中的进展，并提供了数据集推荐；它突出了 LLMs 的可扩展性，对于 AI 研究者和开发者理解多代理协作有重要启发。\n\n- **In-context Learning with Retrieved Demonstrations for Language Models: A Survey**（中文：在上下文中学习：使用检索演示的语言模型调查）  \n  作者 Man Luo 等探讨了语言模型的 In-Context Learning (ICL)，通过检索演示优化示例选择。主要发现是，这种方法提高了模型的适应性和效率，减少了手动偏差；这与上一篇互补，强调了 LLMs 在动态任务中的实用性。\n\n- **Instructional Fingerprinting of Large Language Models**（中文：大型语言模型的指令指纹识别）  \n  Jiashu Xu 等提出了一种轻量级指纹技术，用于保护 LLMs 的知识产权。关键创新是通过指令后门嵌入检测模型所有权，避免了过度声称；这篇论文有实际应用价值，尤其在商业 AI 场景中。\n\n- **With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation**（中文：文本越多需求越大：推理时训练提升长文本生成）  \n  作者 Y. Wang 等开发了 Temp-Lora 方法，用于长文本生成任务。核心发现是，它显著降低了计算成本（如减少 51.5% 内存使用），并在 PG19 和 GuoFeng 数据集上提升了生成质量（PPL 下降 13.2%）；这展示了 LLMs 在高效长序列处理中的潜力。\n\n这些 LLMs 相关论文整体上突出了模型在多任务中的扩展性和优化策略，暗示了未来 AI 系统的更智能协作和应用。\n\n#### 图像生成和处理（创新性强，设置新 SOTA）\n- **Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers**（中文：使用 Hourglass 扩散变压器的可扩展高分辨率像素空间图像合成）  \n  Katherine Crowson 等提出 HDiT 框架，支持直接在像素空间训练高分辨率图像（如 1024×1024）。主要贡献是线性扩展性，避免了传统多尺度方法，并在新 SOTA 上表现突出（如 ImageNet 和 FFHQ 数据集 PPL 改善）；这篇令人印象深刻，因为它桥接了卷积和 Transformer 的优势。\n\n- **MapChange: Enhancing Semantic Change Detection with Temporal-Invariant Historical Maps Based on Deep Triplet Network**（中文：基于深度三元组网络的时序不变历史地图增强语义变化检测）  \n  Yinhe Liu 等引入 MapChange 框架，使用历史地图减少时序差异影响。核心发现是，它在语义变化检测任务中显著提高了准确性（在公开数据集上优于 SOTA）；这对遥感图像分析有实际启发。\n\n这些图像论文展示了深度学习在高分辨率和动态环境中的进步，潜在应用包括自动驾驶和环境监测。\n\n#### 其他值得一提的论文（快速掠过，焦点在贡献）\n- **General Flow as Foundation Affordance for Scalable Robot Learning**（中文：通用流作为可扩展机器人学习的基础 affordance）  \n  Chengbo Yuan 等提出使用 3D 流预测模型，实现零-shot 机器人技能转移。关键发现是，它在 18 个任务中达到 81% 成功率；这对机器人学习有启发，但我们快速提一下。\n\n- **Efficient local linearity regularization to overcome catastrophic overfitting**（中文：高效局部线性正则化克服灾难性过拟合）  \n  Elias Abad Rocamora 等开发了 ELLE 方法，解决单步对抗训练中的过拟合问题。贡献是提高了鲁棒性（在 MNIST 和 CIFAR10 上提升 40%），并开源代码；这对深度学习训练有帮助。\n\n其余论文，如那些聚焦于强化学习（第 6、8、11 篇）、量子计算（第 12、34 篇）或特定应用（如农业、医疗），虽然有学术价值，但相对次要，我们仅快速提及：它们探讨了动态优化、金融事件提取和天气预测等，但未有突破性发现，故不展开讨论。\n\n总之，今天的 arXiv 更新强调了 AI 的创新应用，LLMs 和图像生成领域尤为活跃，建议读者关注这些前沿趋势！如果有特定兴趣，建议直接查阅原文。",
  "papers": [
    {
      "arxiv_id": "2401.11627v2",
      "title": "Tight Verification of Probabilistic Robustness in Bayesian Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Batten",
        "Mehran Hosseini",
        "Alessio Lomuscio"
      ],
      "abstract": "We introduce two algorithms for computing tight guarantees on the\nprobabilistic robustness of Bayesian Neural Networks (BNNs). Computing\nrobustness guarantees for BNNs is a significantly more challenging task than\nverifying the robustness of standard Neural Networks (NNs) because it requires\nsearching the parameters' space for safe weights. Moreover, tight and complete\napproaches for the verification of standard NNs, such as those based on\nMixed-Integer Linear Programming (MILP), cannot be directly used for the\nverification of BNNs because of the polynomial terms resulting from the\nconsecutive multiplication of variables encoding the weights. Our algorithms\nefficiently and effectively search the parameters' space for safe weights by\nusing iterative expansion and the network's gradient and can be used with any\nverification algorithm of choice for BNNs. In addition to proving that our\nalgorithms compute tighter bounds than the SoA, we also evaluate our algorithms\nagainst the SoA on standard benchmarks, such as MNIST and CIFAR10, showing that\nour algorithms compute bounds up to 40% tighter than the SoA.",
      "tldr_zh": "本文提出两种算法，用于计算 Bayesian Neural Networks (BNNs) 的概率鲁棒性（probabilistic robustness）的精确保证，以解决 BNNs 验证比标准 Neural Networks (NNs) 更具挑战性的问题，如参数空间搜索和多项式项处理。算法通过迭代扩展和网络梯度高效搜索安全权重，并可与任何 BNNs 验证算法结合。实验在 MNIST 和 CIFAR10 等基准上显示，该方法计算的界限比 State-of-the-Art (SoA) 算法紧凑高达 40%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.FL",
        "cs.LO",
        "68T27 (Primary) 68T45, 68T07, 68T01 (Secondary)",
        "I.2.0; I.2.4; F.3.1; D.2.4"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at AISTATS 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.11627v2",
      "published_date": "2024-01-21 23:41:32 UTC",
      "updated_date": "2024-02-28 19:04:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:09:30.762846"
    },
    {
      "arxiv_id": "2402.01680v2",
      "title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Taicheng Guo",
        "Xiuying Chen",
        "Yaqi Wang",
        "Ruidi Chang",
        "Shichao Pei",
        "Nitesh V. Chawla",
        "Olaf Wiest",
        "Xiangliang Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide\narray of tasks. Due to the impressive planning and reasoning abilities of LLMs,\nthey have been used as autonomous agents to do many tasks automatically.\nRecently, based on the development of using one LLM as a single planning or\ndecision-making agent, LLM-based multi-agent systems have achieved considerable\nprogress in complex problem-solving and world simulation. To provide the\ncommunity with an overview of this dynamic field, we present this survey to\noffer an in-depth discussion on the essential aspects of multi-agent systems\nbased on LLMs, as well as the challenges. Our goal is for readers to gain\nsubstantial insights on the following questions: What domains and environments\ndo LLM-based multi-agents simulate? How are these agents profiled and how do\nthey communicate? What mechanisms contribute to the growth of agents'\ncapacities? For those interested in delving into this field of study, we also\nsummarize the commonly used datasets or benchmarks for them to have convenient\naccess. To keep researchers updated on the latest studies, we maintain an\nopen-source GitHub repository, dedicated to outlining the research on LLM-based\nmulti-agent systems.",
      "tldr_zh": "这篇调查论文综述了Large Language Models (LLMs) 基于多智能体系统的最新进展和挑战，重点探讨这些系统在复杂问题解决和世界模拟中的应用。论文详细讨论了代理的配置、通信机制、能力增长机制（如规划和推理能力），并总结了常用数据集和基准，以便研究者方便访问。为保持研究动态，作者维护了一个开源GitHub仓库，旨在为读者提供全面洞见和未来方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "This work is ongoing and we welcome your contribution!",
      "pdf_url": "http://arxiv.org/pdf/2402.01680v2",
      "published_date": "2024-01-21 23:36:14 UTC",
      "updated_date": "2024-04-19 01:15:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:09:42.442097"
    },
    {
      "arxiv_id": "2401.11624v5",
      "title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Man Luo",
        "Xin Xu",
        "Yue Liu",
        "Panupong Pasupat",
        "Mehran Kazemi"
      ],
      "abstract": "Language models, especially pre-trained large language models, have showcased\nremarkable abilities as few-shot in-context learners (ICL), adept at adapting\nto new tasks with just a few demonstrations in the input context. However, the\nmodel's ability to perform ICL is sensitive to the choice of the few-shot\ndemonstrations. Instead of using a fixed set of demonstrations, one recent\ndevelopment is to retrieve demonstrations tailored to each input query. The\nimplementation of demonstration retrieval is relatively straightforward,\nleveraging existing databases and retrieval systems. This not only improves the\nefficiency and scalability of the learning process but also has been shown to\nreduce biases inherent in manual example selection. In light of the encouraging\nresults and growing research in ICL with retrieved demonstrations, we conduct\nan extensive review of studies in this area. In this survey, we discuss and\ncompare different design choices for retrieval models, retrieval training\nprocedures, and inference algorithms.",
      "tldr_zh": "这篇论文对语言模型的In-context Learning (ICL)进行了调查，重点探讨使用检索演示来增强模型适应新任务的能力，以替代固定演示集。作者强调，检索演示可以提高学习效率、可扩展性，并减少手动选择带来的偏见，同时通过现有数据库和检索系统实现相对简单的实施。论文比较了不同设计选择，包括检索模型、训练过程和推理算法，并总结了该领域的最新研究进展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11624v5",
      "published_date": "2024-01-21 23:34:42 UTC",
      "updated_date": "2024-03-23 16:35:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:09:52.041982"
    },
    {
      "arxiv_id": "2401.11618v2",
      "title": "Efficient local linearity regularization to overcome catastrophic overfitting",
      "title_zh": "高效局部线性正则化以克服灾难性过拟合",
      "authors": [
        "Elias Abad Rocamora",
        "Fanghui Liu",
        "Grigorios G. Chrysos",
        "Pablo M. Olmos",
        "Volkan Cevher"
      ],
      "abstract": "Catastrophic overfitting (CO) in single-step adversarial training (AT)\nresults in abrupt drops in the adversarial test accuracy (even down to 0%). For\nmodels trained with multi-step AT, it has been observed that the loss function\nbehaves locally linearly with respect to the input, this is however lost in\nsingle-step AT. To address CO in single-step AT, several methods have been\nproposed to enforce local linearity of the loss via regularization. However,\nthese regularization terms considerably slow down training due to Double\nBackpropagation. Instead, in this work, we introduce a regularization term,\ncalled ELLE, to mitigate CO effectively and efficiently in classical AT\nevaluations, as well as some more difficult regimes, e.g., large adversarial\nperturbations and long training schedules. Our regularization term can be\ntheoretically linked to curvature of the loss function and is computationally\ncheaper than previous methods by avoiding Double Backpropagation. Our thorough\nexperimental validation demonstrates that our work does not suffer from CO,\neven in challenging settings where previous works suffer from it. We also\nnotice that adapting our regularization parameter during training (ELLE-A)\ngreatly improves the performance, specially in large $\\epsilon$ setups. Our\nimplementation is available in https://github.com/LIONS-EPFL/ELLE .",
      "tldr_zh": "本论文针对单步对抗训练(adversarial training)中的灾难性过拟合(Catastrophic overfitting)问题，提出了一种高效的局部线性正则化方法ELLE，以防止测试准确率急剧下降。ELLE通过与损失函数曲率相关的正则化项，避免了Double Backpropagation，从而显著降低计算开销，并在经典评估以及大扰动和长训练等挑战性场景中表现出色。实验验证表明，ELLE能有效缓解CO，甚至通过适应性参数调整(ELLE-A)进一步提升性能，尤其在大型ε设置下。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.11618v2",
      "published_date": "2024-01-21 22:55:26 UTC",
      "updated_date": "2024-02-28 16:37:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:10:07.074535"
    },
    {
      "arxiv_id": "2401.11609v3",
      "title": "Graph Edits for Counterfactual Explanations: A comparative study",
      "title_zh": "翻译失败",
      "authors": [
        "Angeliki Dimitriou",
        "Nikolaos Chaidos",
        "Maria Lymperaiou",
        "Giorgos Stamou"
      ],
      "abstract": "Counterfactuals have been established as a popular explainability technique\nwhich leverages a set of minimal edits to alter the prediction of a classifier.\nWhen considering conceptual counterfactuals on images, the edits requested\nshould correspond to salient concepts present in the input data. At the same\ntime, conceptual distances are defined by knowledge graphs, ensuring the\noptimality of conceptual edits. In this work, we extend previous endeavors on\ngraph edits as counterfactual explanations by conducting a comparative study\nwhich encompasses both supervised and unsupervised Graph Neural Network (GNN)\napproaches. To this end, we pose the following significant research question:\nshould we represent input data as graphs, which is the optimal GNN approach in\nterms of performance and time efficiency to generate minimal and meaningful\ncounterfactual explanations for black-box image classifiers?",
      "tldr_zh": "这篇论文通过比较研究探讨了使用图编辑（Graph Edits）生成反事实解释（Counterfactuals）的技术，旨在为黑箱图像分类器提供最小且有意义的解释。研究比较了监督和非监督的 Graph Neural Network (GNN) 方法，评估其在性能和时间效率上的表现。核心问题在于，当将输入数据表示为图时，哪种 GNN 方法最优，以确保编辑对应于图像中的显著概念并由知识图谱（Knowledge Graphs）定义概念距离。最终，该工作扩展了现有方法，为优化反事实解释提供了参考。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11609v3",
      "published_date": "2024-01-21 22:11:29 UTC",
      "updated_date": "2024-04-18 14:29:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:10:18.194298"
    },
    {
      "arxiv_id": "2403.08775v1",
      "title": "Constrained Reinforcement Learning for Adaptive Controller Synchronization in Distributed SDN",
      "title_zh": "受限强化学习用于分布式 SDN 中自适应控制器同步",
      "authors": [
        "Ioannis Panitsas",
        "Akrit Mudvari",
        "Leandros Tassiulas"
      ],
      "abstract": "In software-defined networking (SDN), the implementation of distributed SDN\ncontrollers, with each controller responsible for managing a specific\nsub-network or domain, plays a critical role in achieving a balance between\ncentralized control, scalability, reliability, and network efficiency. These\ncontrollers must be synchronized to maintain a logically centralized view of\nthe entire network. While there are various approaches for synchronizing\ndistributed SDN controllers, most tend to prioritize goals such as optimization\nof communication latency or load balancing, often neglecting to address both\nthe aspects simultaneously. This limitation becomes particularly significant\nwhen considering applications like Augmented and Virtual Reality (AR/VR), which\ndemand constrained network latencies and substantial computational resources.\nAdditionally, many existing studies in this field predominantly rely on\nvalue-based reinforcement learning (RL) methods, overlooking the potential\nadvantages offered by state-of-the-art policy-based RL algorithms. To bridge\nthis gap, our work focuses on examining deep reinforcement learning (DRL)\ntechniques, encompassing both value-based and policy-based methods, to\nguarantee an upper latency threshold for AR/VR task offloading within SDN\nenvironments, while selecting the most cost-effective servers for AR/VR task\noffloading. Our evaluation results indicate that while value-based methods\nexcel in optimizing individual network metrics such as latency or load\nbalancing, policy-based approaches exhibit greater robustness in adapting to\nsudden network changes or reconfiguration.",
      "tldr_zh": "本研究针对软件定义网络 (SDN) 中的分布式控制器同步问题，提出了一种受限强化学习 (Constrained Reinforcement Learning) 方法，以满足 Augmented and Virtual Reality (AR/VR) 应用的延迟阈值要求，同时优化服务器选择以实现成本效益。该方法结合了深度强化学习 (DRL) 的价值-based 和 policy-based 算法，确保控制器在通信延迟和负载均衡之间实现自适应平衡。实验结果显示，value-based 方法在优化单一指标如延迟或负载均衡上表现出色，而 policy-based 方法在应对突发网络变化或重配置时更具鲁棒性。整体而言，此框架为高需求网络环境提供了更可靠的同步机制。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.08775v1",
      "published_date": "2024-01-21 21:57:22 UTC",
      "updated_date": "2024-01-21 21:57:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:10:29.850721"
    },
    {
      "arxiv_id": "2401.11605v1",
      "title": "Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Katherine Crowson",
        "Stefan Andreas Baumann",
        "Alex Birch",
        "Tanishq Mathew Abraham",
        "Daniel Z. Kaplan",
        "Enrico Shippole"
      ],
      "abstract": "We present the Hourglass Diffusion Transformer (HDiT), an image generative\nmodel that exhibits linear scaling with pixel count, supporting training at\nhigh-resolution (e.g. $1024 \\times 1024$) directly in pixel-space. Building on\nthe Transformer architecture, which is known to scale to billions of\nparameters, it bridges the gap between the efficiency of convolutional U-Nets\nand the scalability of Transformers. HDiT trains successfully without typical\nhigh-resolution training techniques such as multiscale architectures, latent\nautoencoders or self-conditioning. We demonstrate that HDiT performs\ncompetitively with existing models on ImageNet $256^2$, and sets a new\nstate-of-the-art for diffusion models on FFHQ-$1024^2$.",
      "tldr_zh": "本研究提出Hourglass Diffusion Transformer (HDiT)，一种可扩展的图像生成模型，能够在像素空间实现线性扩展，支持直接训练高分辨率图像（如1024×1024），并结合Transformer架构的优势。HDiT无需依赖多尺度架构、潜在自动编码器或自条件等常见技巧，即可高效训练。实验结果显示，HDiT在ImageNet 256^2上与现有模型竞争，并在FFHQ-1024^2上为扩散模型设立了新基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 13 figures, project page and code available at\n  https://crowsonkb.github.io/hourglass-diffusion-transformers/",
      "pdf_url": "http://arxiv.org/pdf/2401.11605v1",
      "published_date": "2024-01-21 21:49:49 UTC",
      "updated_date": "2024-01-21 21:49:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:10:40.735584"
    },
    {
      "arxiv_id": "2401.11596v2",
      "title": "Learning to Maximize Gains From Trade in Small Markets",
      "title_zh": "翻译失败",
      "authors": [
        "Moshe Babaioff",
        "Amitai Frey",
        "Noam Nisan"
      ],
      "abstract": "We study the problem of designing a two-sided market (double auction) to\nmaximize the gains from trade (social welfare) under the constraints of\n(dominant-strategy) incentive compatibility and budget-balance. Our goal is to\ndo so for an unknown distribution from which we are given a polynomial number\nof samples. Our first result is a general impossibility for the case of\ncorrelated distributions of values even between just one seller and two buyers,\nin contrast to the case of one seller and one buyer (bilateral trade) where\nthis is possible. Our second result is an efficient learning algorithm for one\nseller and two buyers in the case of independent distributions which is based\non a novel algorithm for computing optimal mechanisms for finitely supported\nand explicitly given independent distributions. Both results rely heavily on\ncharacterizations of (dominant-strategy) incentive compatible mechanisms that\nare strongly budget-balanced.",
      "tldr_zh": "本研究探讨了在小规模市场中设计双重拍卖（double auction），以最大化交易收益（gains from trade）和社会福利（social welfare），同时满足主导策略激励兼容性（dominant-strategy incentive compatibility）和预算平衡（budget-balance）的约束。针对未知分布，使用多项式数量的样本，论文证明了在相关分布下，即使只有一个卖家和两个买家，这种机制设计也是不可能的。另一方面，对于独立分布，论文提出了一种高效学习算法，用于计算一个卖家和两个买家场景下的最优机制，该算法依赖于强预算平衡机制的特征化，从而为实际应用提供了可行解决方案。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "F.0; I.2; I.2.6; J.4"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11596v2",
      "published_date": "2024-01-21 20:57:12 UTC",
      "updated_date": "2024-06-19 21:02:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:10:53.738929"
    },
    {
      "arxiv_id": "2401.11553v1",
      "title": "Taxi dispatching strategies with compensations",
      "title_zh": "带有补偿的出租车调度策略",
      "authors": [
        "Holger Billhardt",
        "Alberto Fernández",
        "Sascha Ossowski",
        "Javier Palanca",
        "Javier Bajo"
      ],
      "abstract": "Urban mobility efficiency is of utmost importance in big cities. Taxi\nvehicles are key elements in daily traffic activity. The advance of ICT and\ngeo-positioning systems has given rise to new opportunities for improving the\nefficiency of taxi fleets in terms of waiting times of passengers, cost and\ntime for drivers, traffic density, CO2 emissions, etc., by using more informed,\nintelligent dispatching. Still, the explicit spatial and temporal components,\nas well as the scale and, in particular, the dynamicity of the problem of\npairing passengers and taxis in big towns, render traditional approaches for\nsolving standard assignment problem useless for this purpose, and call for\nintelligent approximation strategies based on domain-specific heuristics.\nFurthermore, taxi drivers are often autonomous actors and may not agree to\nparticipate in assignments that, though globally efficient, may not be\nsufficently beneficial for them individually. This paper presents a new\nheuristic algorithm for taxi assignment to customers that considers taxi\nreassignments if this may lead to globally better solutions. In addition, as\nsuch new assignments may reduce the expected revenues of individual drivers, we\npropose an economic compensation scheme to make individually rational drivers\nagree to proposed modifications in their assigned clients. We carried out a set\nof experiments, where several commonly used assignment strategies are compared\nto three different instantiations of our heuristic algorithm. The results\nindicate that our proposal has the potential to reduce customer waiting times\nin fleets of autonomous taxis, while being also beneficial from an economic\npoint of view.",
      "tldr_zh": "这篇论文提出了一种新的出租车调度策略，针对城市交通效率问题，考虑了乘客等待时间、司机成本和整体效益。论文引入一个启发式算法(heuristic algorithm)，允许出租车重新分配以实现全局优化，同时通过经济补偿方案(compensation scheme)激励自主司机接受可能影响个人收益的调整。实验比较了多种常见策略，结果表明该方法能显著减少乘客等待时间，同时在经济上具有优势。",
      "categories": [
        "cs.AI",
        "I.2.1"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11553v1",
      "published_date": "2024-01-21 17:54:46 UTC",
      "updated_date": "2024-01-21 17:54:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:11:04.840590"
    },
    {
      "arxiv_id": "2401.12259v1",
      "title": "Agreement Technologies for Coordination in Smart Cities",
      "title_zh": "协议技术用于智能城市协调",
      "authors": [
        "Holger Billhardt",
        "Alberto Fernández",
        "Marin Lujak",
        "Sascha Ossowski"
      ],
      "abstract": "Many challenges in today's society can be tackled by distributed open\nsystems. This is particularly true for domains that are commonly perceived\nunder the umbrella of smart cities, such as intelligent transportation, smart\nenergy grids, or participative governance. When designing computer applications\nfor these domains, it is necessary to account for the fact that the elements of\nsuch systems, often called software agents, are usually made by different\ndesigners and act on behalf of particular stakeholders. Furthermore, it is\nunknown at design time when such agents will enter or leave the system, and\nwhat interests new agents will represent. To instil coordination in such\nsystems is particularly demanding, as usually only part of them can be directly\ncontrolled at runtime. Agreement technologies refer to a sandbox of tools and\nmechanisms for the development of such open multiagent systems, which are based\non the notion of agreement. In this paper, we argue that agreement technologies\nare a suitable means for achieving coordination in smart city domains, and back\nour claim through examples of several real-world applications.",
      "tldr_zh": "本研究探讨了协议技术（Agreement Technologies）在智能城市领域（如智能交通、智能能源网格和参与式治理）中实现协调的潜力。论文指出，分布式开放系统中，软件代理（software agents）由不同设计者创建，并代表各种利益相关者，导致系统动态且难以控制。协议技术通过基于协议的工具和机制，帮助构建开放多代理系统（multiagent systems），从而有效处理代理加入、离开和利益冲突的问题。研究通过几个真实世界应用的示例，证明了协议技术在提升智能城市协调方面的有效性。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "I.2.1"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12259v1",
      "published_date": "2024-01-21 17:43:08 UTC",
      "updated_date": "2024-01-21 17:43:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:11:16.973223"
    },
    {
      "arxiv_id": "2401.12258v7",
      "title": "Emergent Dominance Hierarchies in Reinforcement Learning Agents",
      "title_zh": "涌现的支配等级制度在强化学习代理中",
      "authors": [
        "Ram Rachum",
        "Yonatan Nakar",
        "Bill Tomlinson",
        "Nitay Alon",
        "Reuth Mirsky"
      ],
      "abstract": "Modern Reinforcement Learning (RL) algorithms are able to outperform humans\nin a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings\npresent additional challenges, and successful cooperation in mixed-motive\ngroups of agents depends on a delicate balancing act between individual and\ngroup objectives. Social conventions and norms, often inspired by human\ninstitutions, are used as tools for striking this balance.\n  In this paper, we examine a fundamental, well-studied social convention that\nunderlies cooperation in both animal and human societies: dominance\nhierarchies.\n  We adapt the ethological theory of dominance hierarchies to artificial\nagents, borrowing the established terminology and definitions with as few\namendments as possible. We demonstrate that populations of RL agents, operating\nwithout explicit programming or intrinsic rewards, can invent, learn, enforce,\nand transmit a dominance hierarchy to new populations. The dominance\nhierarchies that emerge have a similar structure to those studied in chickens,\nmice, fish, and other species.",
      "tldr_zh": "本研究探讨了强化学习 (Reinforcement Learning, RL) 代理在多智能体强化学习 (Multi-agent Reinforcement Learning, MARL) 环境中自发形成的主导等级 (dominance hierarchies)。作者将动物行为学 (ethological) 理论适应到人工智能代理，使用最小修改的术语和定义，考察代理如何在没有明确编程或内在奖励的情况下发明、学习、执行和传播这些等级。实验结果显示，这些主导等级的结构类似于鸡、老鼠和鱼等物种中的社会结构，从而证明了社会规范在促进混合动机群体合作中的作用。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.12258v7",
      "published_date": "2024-01-21 16:59:45 UTC",
      "updated_date": "2024-06-22 11:44:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:11:29.459851"
    },
    {
      "arxiv_id": "2403.08774v2",
      "title": "Discussion of Loop Expansion and Introduction of Series Cutting Functions to Local Potential Approximation: Complexity Analysis Using Green's Functions, Cutting Of Nth-Order Social Interactions For Progressive Safety",
      "title_zh": "翻译失败",
      "authors": [
        "Yasuko Kawahata"
      ],
      "abstract": "In this study, we focus on the aforementioned paper, \"Examination\nKubo-Matsubara Green's Function Of The Edwards-Anderson Model: Extreme Value\nInformation Flow Of Nth-Order Interpolated Extrapolation Of Zero Phenomena\nUsing The Replica Method (2024)\". This paper also applies theoretical physics\nmethods to better understand the filter bubble phenomenon, focusing in\nparticular on loop expansions and truncation functions. Using the loop\nexpansion method, the complexity of social interactions during the occurrence\nof filter bubbles will be discussed in order to introduce series, express\nmathematically, and evaluate the impact of these interactions. We analyze the\ninteractions between agents and their time evolution using a variety of Green's\nfunctions, including delayed Green's functions, advanced Green's functions, and\ncausal Green's functions, to capture the dynamic response of the system through\nlocal potential approximations. In addition, we apply truncation functions and\ntruncation techniques to ensure incremental safety and evaluate the long-term\nstability of the system. This approach will enable a better understanding of\nthe mechanisms of filter bubble generation and dissolution, and discuss\ninsights into their prevention and management. This research explores the\npossibilities of applying theoretical physics frameworks to social science\nproblems and examines methods for analyzing the complex dynamics of information\nflow and opinion formation in digital society.This paper is partially an\nattempt to utilize \"Generative AI\" and was written with educational intent.\nThere are currently no plans for it to become a peer-reviewed paper.",
      "tldr_zh": "本研究基于理论物理方法，讨论了loop expansions和引入series cutting functions到local potential approximation，以分析社会互动的复杂性，特别是filter bubble现象。\n它利用delayed Green's functions、advanced Green's functions和causal Green's functions来捕捉系统动态响应，并通过truncation functions和truncation techniques确保渐进安全。\n研究评估了Nth-order social interactions的影响，揭示filter bubble生成与消解的机制，并提供预防和管理洞见。\n该论文部分使用Generative AI撰写，具有教育意图，并非针对同行评议。",
      "categories": [
        "physics.soc-ph",
        "cs.AI"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "In this study, we focus on the aforementioned paper, \"Examination\n  Kubo-Matsubara Green's Function Of The Edwards-Anderson Model: Extreme Value\n  Information Flow Of Nth-Order Interpolated Extrapolation Of Zero Phenomena\n  Using The Replica Method (2024)\"",
      "pdf_url": "http://arxiv.org/pdf/2403.08774v2",
      "published_date": "2024-01-21 15:03:17 UTC",
      "updated_date": "2024-04-19 14:52:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:11:43.234205"
    },
    {
      "arxiv_id": "2401.11512v1",
      "title": "Information-Theoretic State Variable Selection for Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Charles Westphal",
        "Stephen Hailes",
        "Mirco Musolesi"
      ],
      "abstract": "Identifying the most suitable variables to represent the state is a\nfundamental challenge in Reinforcement Learning (RL). These variables must\nefficiently capture the information necessary for making optimal decisions. In\norder to address this problem, in this paper, we introduce the Transfer Entropy\nRedundancy Criterion (TERC), an information-theoretic criterion, which\ndetermines if there is \\textit{entropy transferred} from state variables to\nactions during training. We define an algorithm based on TERC that provably\nexcludes variables from the state that have no effect on the final performance\nof the agent, resulting in more sample efficient learning. Experimental results\nshow that this speed-up is present across three different algorithm classes\n(represented by tabular Q-learning, Actor-Critic, and Proximal Policy\nOptimization (PPO)) in a variety of environments. Furthermore, to highlight the\ndifferences between the proposed methodology and the current state-of-the-art\nfeature selection approaches, we present a series of controlled experiments on\nsynthetic data, before generalizing to real-world decision-making tasks. We\nalso introduce a representation of the problem that compactly captures the\ntransfer of information from state variables to actions as Bayesian networks.",
      "tldr_zh": "本文提出了一种基于信息论的Transfer Entropy Redundancy Criterion (TERC)标准，用于强化学习（RL）中选择最合适的状态变量，以高效捕获决策所需信息。该标准通过检测状态变量到动作的熵转移，设计算法排除对代理性能无影响的变量，从而提高样本效率。实验结果显示，该方法在Q-learning、Actor-Critic和Proximal Policy Optimization (PPO)等多种算法和环境中加速了学习过程。论文还通过合成数据实验与现有特征选择方法比较，并引入Bayesian networks来表示信息转移机制。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "47 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.11512v1",
      "published_date": "2024-01-21 14:51:09 UTC",
      "updated_date": "2024-01-21 14:51:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:11:54.579809"
    },
    {
      "arxiv_id": "2401.11504v3",
      "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Y. Wang",
        "D. Ma",
        "D. Cai"
      ],
      "abstract": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
      "tldr_zh": "该研究针对长文本生成（如小说写作和长上下文翻译）面临的挑战，提出了一种名为 Temp-Lora 的方法，通过在推理时将上下文信息嵌入临时 Lora 模块，而不是依赖 KV cache，从而避免永久修改模型参数并逐步训练模块以保留知识。实验在 PG19 语言建模基准和 GuoFeng 翻译基准上验证了其有效性，结果显示 Temp-Lora 显著降低了 perplexity (PPL)，如 PG19 子集 PPL 减少 13.2%，GuoFeng 子集 PPL 减少 29.3% 且 BLEU 得分增加 113.2%。此外，Temp-Lora 与现有长文本生成方法兼容，能减少计算成本，例如缩短上下文窗口，实现 51.5% 内存使用减少和 60.0% 延迟降低。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLM 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.11504v3",
      "published_date": "2024-01-21 14:28:41 UTC",
      "updated_date": "2024-09-11 02:22:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:12:06.709627"
    },
    {
      "arxiv_id": "2401.11500v1",
      "title": "Integration of Large Language Models in Control of EHD Pumps for Precise Color Synthesis",
      "title_zh": "大型语言模型在 EHD 泵控制中的整合，用于精确颜色合成",
      "authors": [
        "Yanhong Peng",
        "Ceng Zhang",
        "Chenlong Hu",
        "Zebing Mao"
      ],
      "abstract": "This paper presents an innovative approach to integrating Large Language\nModels (LLMs) with Arduino-controlled Electrohydrodynamic (EHD) pumps for\nprecise color synthesis in automation systems. We propose a novel framework\nthat employs fine-tuned LLMs to interpret natural language commands and convert\nthem into specific operational instructions for EHD pump control. This approach\naims to enhance user interaction with complex hardware systems, making it more\nintuitive and efficient. The methodology involves four key steps: fine-tuning\nthe language model with a dataset of color specifications and corresponding\nArduino code, developing a natural language processing interface, translating\nuser inputs into executable Arduino code, and controlling EHD pumps for\naccurate color mixing. Conceptual experiment results, based on theoretical\nassumptions, indicate a high potential for accurate color synthesis, efficient\nlanguage model interpretation, and reliable EHD pump operation. This research\nextends the application of LLMs beyond text-based tasks, demonstrating their\npotential in industrial automation and control systems. While highlighting the\nlimitations and the need for real-world testing, this study opens new avenues\nfor AI applications in physical system control and sets a foundation for future\nadvancements in AI-driven automation technologies.",
      "tldr_zh": "本文提出了一种创新框架，将 Large Language Models (LLMs) 整合到 Arduino 控制的 Electrohydrodynamic (EHD) pumps 中，用于实现精确颜色合成，从而提升用户与复杂硬件系统的交互效率和直观性。该框架包括四个关键步骤：使用颜色规格和 Arduino 代码数据集 fine-tune LLMs、开发自然语言处理接口、将用户输入翻译成可执行代码，以及控制 EHD pumps 进行准确颜色混合。基于理论假设的概念实验结果显示，该方法在颜色合成准确性、语言模型解释和泵操作可靠性方面表现出高潜力，同时扩展了 LLMs 在工业自动化和控制系统中的应用。尽管存在局限性并需进行真实世界测试，此研究为 AI 驱动的自动化技术奠定了基础。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11500v1",
      "published_date": "2024-01-21 14:10:27 UTC",
      "updated_date": "2024-01-21 14:10:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:12:20.505111"
    },
    {
      "arxiv_id": "2401.11489v1",
      "title": "MapChange: Enhancing Semantic Change Detection with Temporal-Invariant Historical Maps Based on Deep Triplet Network",
      "title_zh": "翻译失败",
      "authors": [
        "Yinhe Liu",
        "Sunan Shi",
        "Zhuo Zheng",
        "Jue Wang",
        "Shiqi Tian",
        "Yanfei Zhong"
      ],
      "abstract": "Semantic Change Detection (SCD) is recognized as both a crucial and\nchallenging task in the field of image analysis. Traditional methods for SCD\nhave predominantly relied on the comparison of image pairs. However, this\napproach is significantly hindered by substantial imaging differences, which\narise due to variations in shooting times, atmospheric conditions, and angles.\nSuch discrepancies lead to two primary issues: the under-detection of minor yet\nsignificant changes, and the generation of false alarms due to temporal\nvariances. These factors often result in unchanged objects appearing markedly\ndifferent in multi-temporal images. In response to these challenges, the\nMapChange framework has been developed. This framework introduces a novel\nparadigm that synergizes temporal-invariant historical map data with\ncontemporary high-resolution images. By employing this combination, the\ntemporal variance inherent in conventional image pair comparisons is\neffectively mitigated. The efficacy of the MapChange framework has been\nempirically validated through comprehensive testing on two public datasets.\nThese tests have demonstrated the framework's marked superiority over existing\nstate-of-the-art SCD methods.",
      "tldr_zh": "语义变化检测 (SCD) 是图像分析中的关键挑战，但传统基于图像对比较的方法易受拍摄时间、天气和角度差异影响，导致小变化未被检测和假警报问题。论文提出 MapChange 框架，通过整合时间不变的历史地图数据与当代高分辨率图像，基于深度三元网络 (Deep Triplet Network) 缓解这些时间差异。实验在两个公共数据集上验证了该框架的表现，显著优于现有最先进方法，提升了 SCD 的准确性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11489v1",
      "published_date": "2024-01-21 13:30:02 UTC",
      "updated_date": "2024-01-21 13:30:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:12:28.878803"
    },
    {
      "arxiv_id": "2401.11472v3",
      "title": "Abstract Weighted Based Gradual Semantics in Argumentation Theory",
      "title_zh": "翻译失败",
      "authors": [
        "Assaf Libman",
        "Nir Oren",
        "Bruno Yun"
      ],
      "abstract": "Weighted gradual semantics provide an acceptability degree to each argument\nrepresenting the strength of the argument, computed based on factors including\nbackground evidence for the argument, and taking into account interactions\nbetween this argument and others. We introduce four important problems linking\ngradual semantics and acceptability degrees. First, we reexamine the inverse\nproblem, seeking to identify the argument weights of the argumentation\nframework which lead to a specific final acceptability degree. Second, we ask\nwhether the function mapping between argument weights and acceptability degrees\nis injective or a homeomorphism onto its image. Third, we ask whether argument\nweights can be found when preferences, rather than acceptability degrees for\narguments are considered. Fourth, we consider the topology of the space of\nvalid acceptability degrees, asking whether \"gaps\" exist in this space. While\ndifferent gradual semantics have been proposed in the literature, in this\npaper, we identify a large family of weighted gradual semantics, called\nabstract weighted based gradual semantics. These generalise many of the\nexisting semantics while maintaining desirable properties such as convergence\nto a unique fixed point. We also show that a sub-family of the weighted gradual\nsemantics, called abstract weighted (L^p,\\lambda,\\mu)-based gradual semantics\nand which include well-known semantics, solve all four of the aforementioned\nproblems.",
      "tldr_zh": "本论文探讨了论证理论中的加权渐进语义（weighted gradual semantics），通过赋予每个论证一个可接受度（acceptability degree）来评估其强度，该度量基于背景证据和论证间的互动。作者引入了抽象加权基于渐进语义（abstract weighted based gradual semantics）家族，该家族扩展了现有语义并保持了如收敛到唯一固定点的良好属性。同时，论文证明了一个子家族——抽象加权（L^p, λ, μ)-based gradual semantics——成功解决了四个关键问题，包括逆问题（inverse problem）、映射函数的注入性、基于偏好的权重寻找，以及可接受度空间的拓扑间隙分析。整体框架为论证理论提供了更全面和可靠的工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11472v3",
      "published_date": "2024-01-21 12:22:48 UTC",
      "updated_date": "2024-08-20 12:44:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:12:42.696855"
    },
    {
      "arxiv_id": "2401.11471v1",
      "title": "LR-CNN: Lightweight Row-centric Convolutional Neural Network Training for Memory Reduction",
      "title_zh": "翻译失败",
      "authors": [
        "Zhigang Wang",
        "Hangyu Yang",
        "Ning Wang",
        "Chuanfei Xu",
        "Jie Nie",
        "Zhiqiang Wei",
        "Yu Gu",
        "Ge Yu"
      ],
      "abstract": "In the last decade, Convolutional Neural Network with a multi-layer\narchitecture has advanced rapidly. However, training its complex network is\nvery space-consuming, since a lot of intermediate data are preserved across\nlayers, especially when processing high-dimension inputs with a big batch size.\nThat poses great challenges to the limited memory capacity of current\naccelerators (e.g., GPUs). Existing efforts mitigate such bottleneck by\nexternal auxiliary solutions with additional hardware costs, and internal\nmodifications with potential accuracy penalty. Differently, our analysis\nreveals that computations intra- and inter-layers exhibit the spatial-temporal\nweak dependency and even complete independency features. That inspires us to\nbreak the traditional layer-by-layer (column) dataflow rule. Now operations are\nnovelly re-organized into rows throughout all convolution layers. This\nlightweight design allows a majority of intermediate data to be removed without\nany loss of accuracy. We particularly study the weak dependency between two\nconsecutive rows. For the resulting skewed memory consumption, we give two\nsolutions with different favorite scenarios. Evaluations on two representative\nnetworks confirm the effectiveness. We also validate that our middle dataflow\noptimization can be smoothly embraced by existing works for better memory\nreduction.",
      "tldr_zh": "这篇论文提出了 LR-CNN，一种轻量级的基于行中心（row-centric）的卷积神经网络（CNN）训练方法，旨在解决传统 CNN 训练过程中内存消耗过大的问题。作者通过分析层内和层间计算的空间-时间弱依赖性，重新组织运算数据流为行级结构，从而移除大部分中间数据而不损失准确率，并针对不均匀内存消耗提供了两种适应不同场景的解决方案。在两个代表性网络上的评估证明，该方法比现有方法更有效，并可与其他内存优化技术无缝结合。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11471v1",
      "published_date": "2024-01-21 12:19:13 UTC",
      "updated_date": "2024-01-21 12:19:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:12:54.438163"
    },
    {
      "arxiv_id": "2401.11459v1",
      "title": "AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology",
      "title_zh": "翻译失败",
      "authors": [
        "Rongqing Cong",
        "Wenyang He",
        "Mingxuan Li",
        "Bangning Luo",
        "Zebin Yang",
        "Yuchao Yang",
        "Ru Huang",
        "Bonan Yan"
      ],
      "abstract": "Large language models (LLMs) with Transformer architectures have become\nphenomenal in natural language processing, multimodal generative artificial\nintelligence, and agent-oriented artificial intelligence. The self-attention\nmodule is the most dominating sub-structure inside Transformer-based LLMs.\nComputation using general-purpose graphics processing units (GPUs) inflicts\nreckless demand for I/O bandwidth for transferring intermediate calculation\nresults between memories and processing units. To tackle this challenge, this\nwork develops a fully customized vanilla self-attention accelerator,\nAttentionLego, as the basic building block for constructing spatially\nexpandable LLM processors. AttentionLego provides basic implementation with\nfully-customized digital logic incorporating Processing-In-Memory (PIM)\ntechnology. It is based on PIM-based matrix-vector multiplication and look-up\ntable-based Softmax design. The open-source code is available online:\nhttps://bonany.cc/attentionleg.",
      "tldr_zh": "本研究针对Transformer-based大型语言模型(LLMs)中自注意力模块的计算挑战，提出AttentionLego，这是一个开源构建块，用于构建空间可扩展的LLM加速器。AttentionLego采用Processing-In-Memory (PIM)技术，包括基于PIM的矩阵-向量乘法和基于查找表的Softmax设计，以减少GPU计算中I/O带宽的需求。论文提供完全自定义的数字逻辑实现，并开源代码（https://bonany.cc/attentionleg），为高效LLM处理器开发奠定基础。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "for associated source codes, see https://bonany.cc/attentionleg",
      "pdf_url": "http://arxiv.org/pdf/2401.11459v1",
      "published_date": "2024-01-21 10:48:08 UTC",
      "updated_date": "2024-01-21 10:48:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:13:05.724742"
    },
    {
      "arxiv_id": "2401.12255v2",
      "title": "Instructional Fingerprinting of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jiashu Xu",
        "Fei Wang",
        "Mingyu Derek Ma",
        "Pang Wei Koh",
        "Chaowei Xiao",
        "Muhao Chen"
      ],
      "abstract": "The exorbitant cost of training Large language models (LLMs) from scratch\nmakes it essential to fingerprint the models to protect intellectual property\nvia ownership authentication and to ensure downstream users and developers\ncomply with their license terms (e.g. restricting commercial use). In this\nstudy, we present a pilot study on LLM fingerprinting as a form of very\nlightweight instruction tuning. Model publisher specifies a confidential\nprivate key and implants it as an instruction backdoor that causes the LLM to\ngenerate specific text when the key is present. Results on 11 popularly-used\nLLMs showed that this approach is lightweight and does not affect the normal\nbehavior of the model. It also prevents publisher overclaim, maintains\nrobustness against fingerprint guessing and parameter-efficient training, and\nsupports multi-stage fingerprinting akin to MIT License. Code is available in\nhttps://cnut1648.github.io/Model-Fingerprint/.",
      "tldr_zh": "这篇论文提出了一种轻量级的指令指纹技术，用于保护大型语言模型 (LLMs) 的知识产权，通过所有权认证和许可条款遵守（如限制商业使用）。方法涉及模型发布者植入一个保密私有密钥作为指令后门 (instruction backdoor)，使模型在密钥存在时生成特定文本，同时不影响模型的正常行为。在 11 个常用 LLMs 上进行实验，结果显示该技术具有鲁棒性，能防止发布者过度声称、抵御指纹猜测和参数高效训练，并支持类似 MIT License 的多阶段指纹。代码已在 https://cnut1648.github.io/Model-Fingerprint/ 公开。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at NAACL 2024; 30 pages",
      "pdf_url": "http://arxiv.org/pdf/2401.12255v2",
      "published_date": "2024-01-21 09:51:45 UTC",
      "updated_date": "2024-04-03 06:23:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:13:19.560880"
    },
    {
      "arxiv_id": "2401.11439v2",
      "title": "General Flow as Foundation Affordance for Scalable Robot Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Chengbo Yuan",
        "Chuan Wen",
        "Tong Zhang",
        "Yang Gao"
      ],
      "abstract": "We address the challenge of acquiring real-world manipulation skills with a\nscalable framework. We hold the belief that identifying an appropriate\nprediction target capable of leveraging large-scale datasets is crucial for\nachieving efficient and universal learning. Therefore, we propose to utilize 3D\nflow, which represents the future trajectories of 3D points on objects of\ninterest, as an ideal prediction target. To exploit scalable data resources, we\nturn our attention to human videos. We develop, for the first time, a\nlanguage-conditioned 3D flow prediction model directly from large-scale RGBD\nhuman video datasets. Our predicted flow offers actionable guidance, thus\nfacilitating zero-shot skill transfer in real-world scenarios. We deploy our\nmethod with a policy based on closed-loop flow prediction. Remarkably, without\nany in-domain finetuning, our method achieves an impressive 81\\% success rate\nin zero-shot human-to-robot skill transfer, covering 18 tasks in 6 scenes. Our\nframework features the following benefits: (1) scalability: leveraging\ncross-embodiment data resources; (2) wide application: multiple object\ncategories, including rigid, articulated, and soft bodies; (3) stable skill\ntransfer: providing actionable guidance with a small inference domain-gap.\nCode, data, and supplementary materials are available\nhttps://general-flow.github.io",
      "tldr_zh": "本文提出使用 3D flow 作为基础可负担性(Affordance)，以解决可扩展机器人学习中的真实世界操作技能挑战，通过预测物体上 3D 点的未来轨迹来利用大规模数据集。研究团队开发了一个语言条件化的 3D flow 预测模型，直接从 RGBD 人类视频数据集训练，实现零样本技能转移。实验结果显示，该方法在无需领域内微调的情况下，实现了 81% 的成功率，涵盖 18 个任务和 6 个场景。框架的优势在于可扩展性（利用跨实体数据）、广泛应用（支持刚体、关节体和软体等物体类别），以及稳定的技能转移，通过提供可操作指导减少推理域间隙。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "https://general-flow.github.io",
      "pdf_url": "http://arxiv.org/pdf/2401.11439v2",
      "published_date": "2024-01-21 09:39:11 UTC",
      "updated_date": "2024-09-23 08:22:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:13:35.221002"
    },
    {
      "arxiv_id": "2401.11418v1",
      "title": "Double-Bounded Optimal Transport for Advanced Clustering and Classification",
      "title_zh": "双边界最优传输用于高级聚类和分类",
      "authors": [
        "Liangliang Shi",
        "Zhaoqi Shen",
        "Junchi Yan"
      ],
      "abstract": "Optimal transport (OT) is attracting increasing attention in machine\nlearning. It aims to transport a source distribution to a target one at minimal\ncost. In its vanilla form, the source and target distributions are\npredetermined, which contracts to the real-world case involving undetermined\ntargets. In this paper, we propose Doubly Bounded Optimal Transport (DB-OT),\nwhich assumes that the target distribution is restricted within two boundaries\ninstead of a fixed one, thus giving more freedom for the transport to find\nsolutions. Based on the entropic regularization of DB-OT, three scaling-based\nalgorithms are devised for calculating the optimal solution. We also show that\nour DB-OT is helpful for barycenter-based clustering, which can avoid the\nexcessive concentration of samples in a single cluster. Then we further develop\nDB-OT techniques for long-tailed classification which is an emerging and open\nproblem. We first propose a connection between OT and classification, that is,\nin the classification task, training involves optimizing the Inverse OT to\nlearn the representations, while testing involves optimizing the OT for\npredictions. With this OT perspective, we first apply DB-OT to improve the\nloss, and the Balanced Softmax is shown as a special case. Then we apply DB-OT\nfor inference in the testing process. Even with vanilla Softmax trained\nfeatures, our extensive experimental results show that our method can achieve\ngood results with our improved inference scheme in the testing stage.",
      "tldr_zh": "本论文提出 Doubly Bounded Optimal Transport (DB-OT)，一种改进的 Optimal Transport (OT) 方法，它允许目标分布在两个边界之间，而不是固定分布，从而为传输问题提供更多灵活性。基于 DB-OT 的 entropic regularization，作者设计了三个基于缩放的算法来计算最优解，并展示了其在 barycenter-based clustering 中的应用，可有效避免样本过度集中在单个集群。论文还建立了 OT 与 classification 的联系，将 DB-OT 用于 long-tailed classification 的损失优化（Balanced Softmax 作为特例）和测试阶段的推理，即使采用 vanilla Softmax 训练的特征，实验结果也显示了显著性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11418v1",
      "published_date": "2024-01-21 07:43:01 UTC",
      "updated_date": "2024-01-21 07:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:13:45.985028"
    },
    {
      "arxiv_id": "2401.11414v2",
      "title": "S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyuan Wu",
        "Yi Feng",
        "Chuang-Wei Liu",
        "Fisher Yu",
        "Qijun Chen",
        "Rui Fan"
      ],
      "abstract": "Semantic segmentation and stereo matching are two essential components of 3D\nenvironmental perception systems for autonomous driving. Nevertheless,\nconventional approaches often address these two problems independently,\nemploying separate models for each task. This approach poses practical\nlimitations in real-world scenarios, particularly when computational resources\nare scarce or real-time performance is imperative. Hence, in this article, we\nintroduce S$^3$M-Net, a novel joint learning framework developed to perform\nsemantic segmentation and stereo matching simultaneously. Specifically,\nS$^3$M-Net shares the features extracted from RGB images between both tasks,\nresulting in an improved overall scene understanding capability. This feature\nsharing process is realized using a feature fusion adaption (FFA) module, which\neffectively transforms the shared features into semantic space and subsequently\nfuses them with the encoded disparity features. The entire joint learning\nframework is trained by minimizing a novel semantic consistency-guided (SCG)\nloss, which places emphasis on the structural consistency in both tasks.\nExtensive experimental results conducted on the vKITTI2 and KITTI datasets\ndemonstrate the effectiveness of our proposed joint learning framework and its\nsuperior performance compared to other state-of-the-art single-task networks.\nOur project webpage is accessible at mias.group/S3M-Net.",
      "tldr_zh": "这篇论文提出了 S$^3$M-Net，一种新型联合学习框架，用于同时执行语义分割(Semantic Segmentation)和立体匹配(Stereo Matching)，以提升自动驾驶中的3D环境感知能力。框架通过共享RGB图像提取的特征，并引入特征融合适配(FFA)模块来融合共享特征与视差编码特征，从而改善整体场景理解。实验结果显示，在vKITTI2和KITTI数据集上，S$^3$M-Net 比其他最先进单任务网络表现出色，证明了其语义一致性引导(SCG)损失函数在强调任务结构一致性方面的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted to IEEE Trans. on Intelligent Vehicles (T-IV)",
      "pdf_url": "http://arxiv.org/pdf/2401.11414v2",
      "published_date": "2024-01-21 06:47:33 UTC",
      "updated_date": "2024-01-29 02:07:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:13:58.151853"
    },
    {
      "arxiv_id": "2401.11410v3",
      "title": "Agricultural Recommendation System based on Deep Learning: A Multivariate Weather Forecasting Approach",
      "title_zh": "基于深度学习的农业推荐系统：一种多变量天气预报方法",
      "authors": [
        "Md Zubair",
        "Md. Shahidul Salim",
        "Mehrab Mustafy Rahman",
        "Mohammad Jahid Ibna Basher",
        "Shahin Imran",
        "Iqbal H. Sarker"
      ],
      "abstract": "Agriculture plays a fundamental role in driving economic growth and ensuring\nfood security for populations around the world. Although labor-intensive\nagriculture has led to steady increases in food grain production in many\ndeveloping countries, it is frequently challenged by adverse weather\nconditions, including heavy rainfall, low temperatures, and drought. These\nfactors substantially hinder food production, posing significant risks to\nglobal food security. In order to have a profitable, sustainable, and\nfarmer-friendly agricultural practice, this paper proposes a context-based crop\nrecommendation system powered by a weather forecast model. For implementation\npurposes, we have considered the whole territory of Bangladesh. With extensive\nevaluation, the multivariate Stacked Bi-LSTM (three Bi-LSTM layers with a time\nDistributed layer) Network is employed as the weather forecasting model. The\nproposed weather model can forecast Rainfall, Temperature, Humidity, and\nSunshine for any given location in Bangladesh with an average R-Squared value\nof 0.9824, and the model outperforms other state-of-the-art LSTM models. These\npredictions guide our system in generating viable farming decisions.\nAdditionally, our full-fledged system is capable of alerting the farmers about\nextreme weather conditions so that preventive measures can be undertaken to\nprotect the crops. Finally, the system is also adept at making knowledge-based\ncrop suggestions for flood and drought-prone regions.",
      "tldr_zh": "本论文提出了一种基于 Deep Learning 的农业推荐系统，采用多变量天气预报方法来帮助农民应对不利天气条件，如暴雨、低温和干旱，从而提升粮食生产和全球食品安全。该系统使用 multivariate Stacked Bi-LSTM 网络（三层 Bi-LSTM 加上时间分布层）针对孟加拉国全境预测降雨、温度、湿度和日照，平均 R-Squared 值达 0.9824，并优于其他 LSTM 模型。系统不仅提供作物建议和极端天气警报，还能针对洪水和干旱地区给出知识-based 决策，促进可持续农业实践。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 16 figures and 13 tables. Two figures and one table have\n  been added to this version",
      "pdf_url": "http://arxiv.org/pdf/2401.11410v3",
      "published_date": "2024-01-21 06:33:45 UTC",
      "updated_date": "2024-07-12 02:02:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:14:08.953602"
    },
    {
      "arxiv_id": "2401.11408v1",
      "title": "SEBERTNets: Sequence Enhanced BERT Networks for Event Entity Extraction Tasks Oriented to the Finance Field",
      "title_zh": "翻译失败",
      "authors": [
        "Congqing He",
        "Xiangyu Zhu",
        "Yuquan Le",
        "Yuzhong Liu",
        "Jianhong Yin"
      ],
      "abstract": "Event extraction lies at the cores of investment analysis and asset\nmanagement in the financial field, and thus has received much attention. The\n2019 China conference on knowledge graph and semantic computing (CCKS)\nchallenge sets up a evaluation competition for event entity extraction task\noriented to the finance field. In this task, we mainly focus on how to extract\nthe event entity accurately, and recall all the corresponding event entity\neffectively. In this paper, we propose a novel model, Sequence Enhanced BERT\nNetworks (SEBERTNets for short), which can inherit the advantages of the\nBERT,and while capturing sequence semantic information. In addition, motivated\nby recommendation system, we propose Hybrid Sequence Enhanced BERT Networks\n(HSEBERTNets for short), which uses a multi-channel recall method to recall all\nthe corresponding event entity. The experimental results show that, the F1\nscore of SEBERTNets is 0.905 in the first stage, and the F1 score of\nHSEBERTNets is 0.934 in the first stage, which demonstarate the effectiveness\nof our methods.",
      "tldr_zh": "本论文针对金融领域的事件实体提取任务，提出SEBERTNets模型，该模型基于BERT框架，同时增强序列语义信息，以提高事件实体的准确提取和召回率。进一步，受推荐系统启发，作者开发了HSEBERTNets模型，使用多通道召回方法来全面召回相关事件实体。在2019 CCKS挑战赛实验中，SEBERTNets的F1 score达到0.905，HSEBERTNets进一步提升至0.934，证明了这些方法的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "CCKS 2019",
      "pdf_url": "http://arxiv.org/pdf/2401.11408v1",
      "published_date": "2024-01-21 06:10:03 UTC",
      "updated_date": "2024-01-21 06:10:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:14:20.644020"
    },
    {
      "arxiv_id": "2404.15279v1",
      "title": "Jointly Modeling Spatio-Temporal Features of Tactile Signals for Action Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Jimmy Lin",
        "Junkai Li",
        "Jiasi Gao",
        "Weizhi Ma",
        "Yang Liu"
      ],
      "abstract": "Tactile signals collected by wearable electronics are essential in modeling\nand understanding human behavior. One of the main applications of tactile\nsignals is action classification, especially in healthcare and robotics.\nHowever, existing tactile classification methods fail to capture the spatial\nand temporal features of tactile signals simultaneously, which results in\nsub-optimal performances. In this paper, we design Spatio-Temporal Aware\ntactility Transformer (STAT) to utilize continuous tactile signals for action\nclassification. We propose spatial and temporal embeddings along with a new\ntemporal pretraining task in our model, which aims to enhance the transformer\nin modeling the spatio-temporal features of tactile signals. Specially, the\ndesigned temporal pretraining task is to differentiate the time order of\ntubelet inputs to model the temporal properties explicitly. Experimental\nresults on a public action classification dataset demonstrate that our model\noutperforms state-of-the-art methods in all metrics.",
      "tldr_zh": "该论文针对触觉信号在行为分类中的应用，指出现有方法无法同时捕捉空间（spatial）和时间（temporal）特征，导致分类性能不佳。作者提出了一种名为 Spatio-Temporal Aware tactility Transformer (STAT) 的模型，通过引入空间和时间嵌入（spatial and temporal embeddings）以及一个新的时间预训练任务（temporal pretraining task），来显式建模触觉信号的时空特性，特别是区分 tubelet 输入的时间顺序。实验结果显示，在公共行为分类数据集上，STAT 模型在所有指标上优于最先进的方法，为医疗保健和机器人领域的应用提供了更有效的解决方案。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "Accepted by AAAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.15279v1",
      "published_date": "2024-01-21 03:47:57 UTC",
      "updated_date": "2024-01-21 03:47:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:14:32.176953"
    },
    {
      "arxiv_id": "2401.11389v2",
      "title": "MedLM: Exploring Language Models for Medical Question Answering Systems",
      "title_zh": "MedLM：探索语言模型用于医疗问答系统",
      "authors": [
        "Niraj Yagnik",
        "Jay Jhaveri",
        "Vivek Sharma",
        "Gabriel Pila"
      ],
      "abstract": "In the face of rapidly expanding online medical literature, automated systems\nfor aggregating and summarizing information are becoming increasingly crucial\nfor healthcare professionals and patients. Large Language Models (LLMs), with\ntheir advanced generative capabilities, have shown promise in various NLP\ntasks, and their potential in the healthcare domain, particularly for\nClosed-Book Generative QnA, is significant. However, the performance of these\nmodels in domain-specific tasks such as medical Q&A remains largely unexplored.\nThis study aims to fill this gap by comparing the performance of general and\nmedical-specific distilled LMs for medical Q&A. We aim to evaluate the\neffectiveness of fine-tuning domain-specific LMs and compare the performance of\ndifferent families of Language Models. The study will address critical\nquestions about these models' reliability, comparative performance, and\neffectiveness in the context of medical Q&A. The findings will provide valuable\ninsights into the suitability of different LMs for specific applications in the\nmedical domain.",
      "tldr_zh": "本研究探讨了Large Language Models (LLMs) 在医疗问答系统中的应用，旨在应对在线医疗文献快速增长的需求，并评估这些模型在Closed-Book Generative QnA 等领域特定任务中的性能。研究通过比较通用和医疗特定distilled LMs的效果，考察fine-tuning 领域特定模型的益处，并分析不同LLMs 家族的可靠性与比较性能。最终，该工作将提供关键见解，帮助确定适合医疗领域的模型应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.11389v2",
      "published_date": "2024-01-21 03:37:47 UTC",
      "updated_date": "2024-03-06 03:26:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:14:44.408914"
    },
    {
      "arxiv_id": "2401.11382v2",
      "title": "Using Large Language Model for End-to-End Chinese ASR and NER",
      "title_zh": "翻译失败",
      "authors": [
        "Yuang Li",
        "Jiawei Yu",
        "Min Zhang",
        "Mengxin Ren",
        "Yanqing Zhao",
        "Xiaofeng Zhao",
        "Shimin Tao",
        "Jinsong Su",
        "Hao Yang"
      ],
      "abstract": "Mapping speech tokens to the same feature space as text tokens has become the\nparadigm for the integration of speech modality into decoder-only large\nlanguage models (LLMs). An alternative approach is to use an encoder-decoder\narchitecture that incorporates speech features through cross-attention. This\napproach, however, has received less attention in the literature. In this work,\nwe connect the Whisper encoder with ChatGLM3 and provide in-depth comparisons\nof these two approaches using Chinese automatic speech recognition (ASR) and\nname entity recognition (NER) tasks. We evaluate them not only by conventional\nmetrics like the F1 score but also by a novel fine-grained taxonomy of ASR-NER\nerrors. Our experiments reveal that encoder-decoder architecture outperforms\ndecoder-only architecture with a short context, while decoder-only architecture\nbenefits from a long context as it fully exploits all layers of the LLM. By\nusing LLM, we significantly reduced the entity omission errors and improved the\nentity ASR accuracy compared to the Conformer baseline. Additionally, we\nobtained a state-of-the-art (SOTA) F1 score of 0.805 on the AISHELL-NER test\nset by using chain-of-thought (CoT) NER which first infers long-form ASR\ntranscriptions and then predicts NER labels.",
      "tldr_zh": "本研究探讨了使用 Large Language Model (LLM) 进行端到端中文 Automatic Speech Recognition (ASR) 和 Named Entity Recognition (NER)，通过比较 encoder-decoder 架构（如将 Whisper 编码器与 ChatGLM3 结合）和 decoder-only 架构。实验结果显示，encoder-decoder 架构在短上下文中表现优于 decoder-only 架构，而后者在长上下文中能充分利用 LLM 所有层级，从而显著减少实体遗漏错误并提升 ASR 准确性。最终，通过 chain-of-thought (CoT) NER 方法，该框架在 AISHELL-NER 测试集上达到了 state-of-the-art (SOTA) F1 score 0.805。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 2 figures, Accepted to InterSpeech 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.11382v2",
      "published_date": "2024-01-21 03:15:05 UTC",
      "updated_date": "2024-06-06 05:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:14:57.968729"
    },
    {
      "arxiv_id": "2402.16863v1",
      "title": "Quantum Inspired Chaotic Salp Swarm Optimization for Dynamic Optimization",
      "title_zh": "量子启发的混沌 Salp 群优化用于动态优化",
      "authors": [
        "Sanjai Pathak",
        "Ashish Mani",
        "Mayank Sharma",
        "Amlan Chatterjee"
      ],
      "abstract": "Many real-world problems are dynamic optimization problems that are unknown\nbeforehand. In practice, unpredictable events such as the arrival of new jobs,\ndue date changes, and reservation cancellations, changes in parameters or\nconstraints make the search environment dynamic. Many algorithms are designed\nto deal with stationary optimization problems, but these algorithms do not face\ndynamic optimization problems or manage them correctly. Although some\noptimization algorithms are proposed to deal with the changes in dynamic\nenvironments differently, there are still areas of improvement in existing\nalgorithms due to limitations or drawbacks, especially in terms of locating and\nfollowing the previously identified optima. With this in mind, we studied a\nvariant of SSA known as QSSO, which integrates the principles of quantum\ncomputing. An attempt is made to improve the overall performance of standard\nSSA to deal with the dynamic environment effectively by locating and tracking\nthe global optima for DOPs. This work is an extension of the proposed new\nalgorithm QSSO, known as the Quantum-inspired Chaotic Salp Swarm Optimization\n(QCSSO) Algorithm, which details the various approaches considered while\nsolving DOPs. A chaotic operator is employed with quantum computing to respond\nto change and guarantee to increase individual searchability by improving\npopulation diversity and the speed at which the algorithm converges. We\nexperimented by evaluating QCSSO on a well-known generalized dynamic benchmark\nproblem (GDBG) provided for CEC 2009, followed by a comparative numerical study\nwith well-regarded algorithms. As promised, the introduced QCSSO is discovered\nas the rival algorithm for DOPs.",
      "tldr_zh": "该研究针对动态优化问题（DOPs），提出了一种改进算法 Quantum-inspired Chaotic Salp Swarm Optimization (QCSSO)，它是 Salp Swarm Optimization (SSA) 的扩展，结合量子计算原理和混沌算子，以提升算法在动态环境中的全局最优解定位和跟踪能力。\nQCSSO 通过增加种群多样性和收敛速度，解决了现有算法在处理参数变化或约束动态时的局限性。\n实验在 CEC 2009 的 Generalized Dynamic Benchmark Problem (GDBG) 上进行，与其他知名算法比较，QCSSO 显示出显著优势，成为 DOPs 的竞争者。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "14 pages, 2 figures, 1 algorithm",
      "pdf_url": "http://arxiv.org/pdf/2402.16863v1",
      "published_date": "2024-01-21 02:59:37 UTC",
      "updated_date": "2024-01-21 02:59:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:15:10.643714"
    },
    {
      "arxiv_id": "2401.11374v4",
      "title": "Language Models as Hierarchy Encoders",
      "title_zh": "语言模型作为层次编码器",
      "authors": [
        "Yuan He",
        "Zhangdie Yuan",
        "Jiaoyan Chen",
        "Ian Horrocks"
      ],
      "abstract": "Interpreting hierarchical structures latent in language is a key limitation\nof current language models (LMs). While previous research has implicitly\nleveraged these hierarchies to enhance LMs, approaches for their explicit\nencoding are yet to be explored. To address this, we introduce a novel approach\nto re-train transformer encoder-based LMs as Hierarchy Transformer encoders\n(HiTs), harnessing the expansive nature of hyperbolic space. Our method\nsituates the output embedding space of pre-trained LMs within a Poincar\\'e ball\nwith a curvature that adapts to the embedding dimension, followed by training\non hyperbolic clustering and centripetal losses. These losses are designed to\neffectively cluster related entities (input as texts) and organise them\nhierarchically. We evaluate HiTs against pre-trained LMs, standard fine-tuned\nLMs, and several hyperbolic embedding baselines, focusing on their capabilities\nin simulating transitive inference, predicting subsumptions, and transferring\nknowledge across hierarchies. The results demonstrate that HiTs consistently\noutperform all baselines in these tasks, underscoring the effectiveness and\ntransferability of our re-trained hierarchy encoders.",
      "tldr_zh": "当前语言模型（LMs）在解释语言中的潜在层次结构方面存在局限，本文提出一种新方法，将基于Transformer编码器的LMs重新训练为Hierarchy Transformer encoders (HiTs)，利用双曲空间的扩展性。方法包括将输出嵌入空间置于Poincaré ball中，曲率适应嵌入维度，并通过双曲聚类损失和向心损失训练，以聚类相关实体并组织层次结构。实验结果显示，HiTs在模拟传递推理、预测subsumptions以及知识转移任务中， consistently outperform 预训练LMs、标准微调LMs和双曲嵌入基线，证明了其有效性和可转移性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accept at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.11374v4",
      "published_date": "2024-01-21 02:29:12 UTC",
      "updated_date": "2024-11-21 00:19:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:15:20.757980"
    },
    {
      "arxiv_id": "2401.11370v1",
      "title": "Self-sustaining Software Systems (S4): Towards Improved Interpretability and Adaptation",
      "title_zh": "自我维持软件系统 (S4)：朝着改进可解释性和适应性发展",
      "authors": [
        "Christian Cabrera",
        "Andrei Paleyes",
        "Neil D. Lawrence"
      ],
      "abstract": "Software systems impact society at different levels as they pervasively solve\nreal-world problems. Modern software systems are often so sophisticated that\ntheir complexity exceeds the limits of human comprehension. These systems must\nrespond to changing goals, dynamic data, unexpected failures, and security\nthreats, among other variable factors in real-world environments. Systems'\ncomplexity challenges their interpretability and requires autonomous responses\nto dynamic changes. Two main research areas explore autonomous systems'\nresponses: evolutionary computing and autonomic computing. Evolutionary\ncomputing focuses on software improvement based on iterative modifications to\nthe source code. Autonomic computing focuses on optimising systems' performance\nby changing their structure, behaviour, or environment variables. Approaches\nfrom both areas rely on feedback loops that accumulate knowledge from the\nsystem interactions to inform autonomous decision-making. However, this\nknowledge is often limited, constraining the systems' interpretability and\nadaptability. This paper proposes a new concept for interpretable and adaptable\nsoftware systems: self-sustaining software systems (S4). S4 builds knowledge\nloops between all available knowledge sources that define modern software\nsystems to improve their interpretability and adaptability. This paper\nintroduces and discusses the S4 concept.",
      "tldr_zh": "本论文讨论了现代软件系统的复杂性如何挑战其可解释性（interpretability）和适应性（adaptation），特别是在应对变化目标、动态数据和安全威胁时。现有研究领域如evolutionary computing和autonomic computing依赖反馈循环进行自主决策，但知识积累有限，导致系统适应不足。为解决这些问题，论文提出自持软件系统（Self-sustaining Software Systems, S4）的新概念，通过在所有可用知识来源之间构建知识循环，提升系统的可解释性和适应性。该概念为开发更智能、自主响应的软件系统提供了基础。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at The 1st International Workshop New Trends in Software\n  Architecture (SATrends) 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.11370v1",
      "published_date": "2024-01-21 02:07:34 UTC",
      "updated_date": "2024-01-21 02:07:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:15:32.985606"
    },
    {
      "arxiv_id": "2401.11361v1",
      "title": "Revolutionizing API Documentation through Summarization",
      "title_zh": "通过总结革新 API 文档",
      "authors": [
        "AmirHossein Naghshzan",
        "Sylvie Ratte"
      ],
      "abstract": "This study tackles the challenges associated with interpreting Application\nProgramming Interface (API) documentation, an integral aspect of software\ndevelopment. Official API documentation, while essential, can be lengthy and\nchallenging to navigate, prompting developers to seek unofficial sources such\nas Stack Overflow. Leveraging the vast user-generated content on Stack\nOverflow, including code snippets and discussions, we employ BERTopic and\nextractive summarization to automatically generate concise and informative API\nsummaries. These summaries encompass key insights like general usage, common\ndeveloper issues, and potential solutions, sourced from the wealth of knowledge\non Stack Overflow. Software developers evaluate these summaries for\nperformance, coherence, and interoperability, providing valuable feedback on\nthe practicality of our approach.",
      "tldr_zh": "这篇论文解决了API文档解释的难题，提出了一种利用Stack Overflow的用户生成内容（如代码片段和讨论）来自动生成简洁、信息的API总结的方法。研究采用BERTopic进行主题建模，并结合extractive summarization提取关键洞见，包括一般用法、常见开发者问题和潜在解决方案。软件开发者对这些总结进行了评估，结果显示其在性能、一致性和互操作性方面表现出色，提升了API文档的实用性和可访问性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "arXiv admin note: text overlap with arXiv:2308.09070",
      "pdf_url": "http://arxiv.org/pdf/2401.11361v1",
      "published_date": "2024-01-21 01:18:08 UTC",
      "updated_date": "2024-01-21 01:18:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:15:45.282753"
    },
    {
      "arxiv_id": "2401.11360v1",
      "title": "PepHarmony: A Multi-View Contrastive Learning Framework for Integrated Sequence and Structure-Based Peptide Encoding",
      "title_zh": "翻译失败",
      "authors": [
        "Ruochi Zhang",
        "Haoran Wu",
        "Chang Liu",
        "Huaping Li",
        "Yuqian Wu",
        "Kewei Li",
        "Yifan Wang",
        "Yifan Deng",
        "Jiahui Chen",
        "Fengfeng Zhou",
        "Xin Gao"
      ],
      "abstract": "Recent advances in protein language models have catalyzed significant\nprogress in peptide sequence representation. Despite extensive exploration in\nthis field, pre-trained models tailored for peptide-specific needs remain\nlargely unaddressed due to the difficulty in capturing the complex and\nsometimes unstable structures of peptides. This study introduces a novel\nmulti-view contrastive learning framework PepHarmony for the sequence-based\npeptide encoding task. PepHarmony innovatively combines both sequence- and\nstructure-level information into a sequence-level encoding module through\ncontrastive learning. We carefully select datasets from the Protein Data Bank\n(PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences\nand structures. The experimental data highlights PepHarmony's exceptional\ncapability in capturing the intricate relationship between peptide sequences\nand structures compared with the baseline and fine-tuned models. The robustness\nof our model is confirmed through extensive ablation studies, which emphasize\nthe crucial roles of contrastive loss and strategic data sorting in enhancing\npredictive performance. The proposed PepHarmony framework serves as a notable\ncontribution to peptide representations, and offers valuable insights for\nfuture applications in peptide drug discovery and peptide engineering. We have\nmade all the source code utilized in this study publicly accessible via GitHub\nat https://github.com/zhangruochi/PepHarmony or\nhttp://www.healthinformaticslab.org/supp/.",
      "tldr_zh": "本文提出PepHarmony框架，这是一种多视图对比学习(multi-view contrastive learning)框架，旨在整合序列和结构信息以实现肽编码。框架通过对比学习将序列级和结构级数据结合，利用来自Protein Data Bank (PDB)和AlphaFold数据库的多样化数据集，捕捉肽序列与结构间的复杂关系。实验结果显示，PepHarmony在预测性能上优于基线模型，且消融研究强调了对比损失(contrastive loss)和数据排序策略的关键作用。该框架为肽药物发现和肽工程提供重要洞见，并已公开源码以供进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 5 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2401.11360v1",
      "published_date": "2024-01-21 01:16:53 UTC",
      "updated_date": "2024-01-21 01:16:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:15:58.913873"
    },
    {
      "arxiv_id": "2401.11351v2",
      "title": "A comprehensive review of Quantum Machine Learning: from NISQ to Fault Tolerance",
      "title_zh": "翻译失败",
      "authors": [
        "Yunfei Wang",
        "Junyu Liu"
      ],
      "abstract": "Quantum machine learning, which involves running machine learning algorithms\non quantum devices, has garnered significant attention in both academic and\nbusiness circles. In this paper, we offer a comprehensive and unbiased review\nof the various concepts that have emerged in the field of quantum machine\nlearning. This includes techniques used in Noisy Intermediate-Scale Quantum\n(NISQ) technologies and approaches for algorithms compatible with\nfault-tolerant quantum computing hardware. Our review covers fundamental\nconcepts, algorithms, and the statistical learning theory pertinent to quantum\nmachine learning.",
      "tldr_zh": "这篇论文对量子机器学习进行了全面且公正的回顾，涵盖从 Noisy Intermediate-Scale Quantum (NISQ) 技术到 Fault Tolerance 量子计算硬件的各种概念和方法。作者探讨了在 NISQ 环境中使用的技术，以及适用于容错量子计算的算法，同时包括量子机器学习的根本概念、算法和相关的统计学习理论。该回顾为学术和商业领域提供了宝贵的见解，帮助理解量子机器学习的发展路径和未来潜力。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "quant-ph",
      "comment": "28 pages. Invited review",
      "pdf_url": "http://arxiv.org/pdf/2401.11351v2",
      "published_date": "2024-01-21 00:19:16 UTC",
      "updated_date": "2024-03-31 00:32:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T23:16:08.373925"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 34,
  "processed_papers_count": 34,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-16T23:16:30.127089"
}
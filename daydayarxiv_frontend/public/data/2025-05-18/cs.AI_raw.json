[
  {
    "arxiv_id": "2505.12576v2",
    "title": "AdaDim: Dimensionality Adaptation for SSL Representational Dynamics",
    "authors": [
      "Kiran Kokilepersaud",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ],
    "abstract": "A key factor in effective Self-Supervised learning (SSL) is preventing dimensional collapse, where higher-dimensional representation spaces ($R$) span a lower-dimensional subspace. Therefore, SSL optimization strategies involve guiding a model to produce $R$ with a higher dimensionality ($H(R)$) through objectives that encourage decorrelation of features or sample uniformity in $R$. A higher $H(R)$ indicates that $R$ has greater feature diversity which is useful for generalization to downstream tasks. Alongside dimensionality optimization, SSL algorithms also utilize a projection head that maps $R$ into an embedding space $Z$. Recent work has characterized the projection head as a filter of noisy or irrelevant features from the SSL objective by reducing the mutual information $I(R;Z)$. Therefore, the current literature's view is that a good SSL representation space should have a high $H(R)$ and a low $I(R;Z)$. However, this view of SSL is lacking in terms of an understanding of the underlying training dynamics that influences the relationship between both terms. Our analysis shows that the best performing SSL models do not have the highest $H(R)$ nor the lowest $I(R;Z)$, but effectively arrive at a balance between both. To take advantage of this analysis, we introduce AdaDim, a training strategy that leverages SSL training dynamics by adaptively balancing between increasing $H(R)$ through feature decorrelation and sample uniformity as well as gradual regularization of $I(R;Z)$ as training progresses. We show performance improvements of up to 3% over common SSL baselines despite our method not utilizing expensive techniques such as queues, clustering, predictor networks, or student-teacher architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2505.12576v2",
    "published_date": "2025-05-18 23:35:34 UTC",
    "updated_date": "2025-10-08 14:09:23 UTC"
  },
  {
    "arxiv_id": "2505.12575v2",
    "title": "RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics",
    "authors": [
      "Jie Zhang",
      "Cezara Petrui",
      "Kristina Nikolić",
      "Florian Tramèr"
    ],
    "abstract": "Existing benchmarks for evaluating mathematical reasoning in large language models (LLMs) rely primarily on competition problems, formal proofs, or artificially challenging questions -- failing to capture the nature of mathematics encountered in actual research environments. We introduce RealMath, a novel benchmark derived directly from research papers and mathematical forums that assesses LLMs' abilities on authentic mathematical tasks. Our approach addresses three critical challenges: sourcing diverse research-level content, enabling reliable automated evaluation through verifiable statements, and designing a continually refreshable dataset to mitigate contamination risks. Experimental results across multiple LLMs reveal surprising capabilities in handling research mathematics compared to competition problems, suggesting current models may already serve as valuable assistants for working mathematicians despite limitations on highly challenging problems. The code and dataset for RealMath are publicly available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12575v2",
    "published_date": "2025-05-18 23:32:46 UTC",
    "updated_date": "2025-10-19 09:24:04 UTC"
  },
  {
    "arxiv_id": "2505.12572v2",
    "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Reconstruction:The Optimal Expansion Ratio",
    "authors": [
      "Hanwen Shen",
      "Ting Ying"
    ],
    "abstract": "A two stage novel generation framework (outline -> section outline -> manuscript) is widely used in long novel generation,(e.g., \\textsc{DOME}, \\textsc{Plan\\&Write}, \\textsc{Long Writer}), but study of such framework in ultra long novel(>1M words) reconstruction is little. Building on recent text compression methods (\\textsc{LLMZip}, \\textsc{LLM2Vec}), we conduct an information-theoretic analysis to quantify semantic distortion under different compression-expansion ratios. We examine how outline length affects information preservation. Experiments on ultra-long novels show that the optimal compression-expansion ratio significantly reduces semantic distortion compared to other non-optimal compression-expansion ratio.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12572v2",
    "published_date": "2025-05-18 23:20:01 UTC",
    "updated_date": "2025-07-27 05:12:20 UTC"
  },
  {
    "arxiv_id": "2505.12567v1",
    "title": "A Survey of Attacks on Large Language Models",
    "authors": [
      "Wenrui Xu",
      "Keshab K. Parhi"
    ],
    "abstract": "Large language models (LLMs) and LLM-based agents have been widely deployed in a wide range of applications in the real world, including healthcare diagnostics, financial analysis, customer support, robotics, and autonomous driving, expanding their powerful capability of understanding, reasoning, and generating natural languages. However, the wide deployment of LLM-based applications exposes critical security and reliability risks, such as the potential for malicious misuse, privacy leakage, and service disruption that weaken user trust and undermine societal safety. This paper provides a systematic overview of the details of adversarial attacks targeting both LLMs and LLM-based agents. These attacks are organized into three phases in LLMs: Training-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity Attacks. For each phase, we analyze the details of representative and recently introduced attack methods along with their corresponding defenses. We hope our survey will provide a good tutorial and a comprehensive understanding of LLM security, especially for attacks on LLMs. We desire to raise attention to the risks inherent in widely deployed LLM-based applications and highlight the urgent need for robust mitigation strategies for evolving threats.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12567v1",
    "published_date": "2025-05-18 22:55:16 UTC",
    "updated_date": "2025-05-18 22:55:16 UTC"
  },
  {
    "arxiv_id": "2505.12565v2",
    "title": "mCLM: A Modular Chemical Language Model that Generates Functional and Makeable Molecules",
    "authors": [
      "Carl Edwards",
      "Chi Han",
      "Gawon Lee",
      "Thao Nguyen",
      "Sara Szymkuć",
      "Chetan Kumar Prasad",
      "Bowen Jin",
      "Jiawei Han",
      "Ying Diao",
      "Ge Liu",
      "Hao Peng",
      "Bartosz A. Grzybowski",
      "Martin D. Burke",
      "Heng Ji"
    ],
    "abstract": "Despite their ability to understand chemical knowledge, large language models (LLMs) remain limited in their capacity to propose novel molecules with desired functions (e.g., drug-like properties). In addition, the molecules that LLMs propose can often be challenging to make, and are almost never compatible with automated synthesis approaches. To better enable the discovery of functional small molecules, LLMs need to learn a new molecular language that is more effective in predicting properties and inherently synced with automated synthesis technology. Current molecule LLMs are limited by representing molecules based on atoms. In this paper, we argue that just like tokenizing texts into meaning-bearing (sub-)word tokens instead of characters, molecules should be tokenized at the level of functional building blocks, i.e., parts of molecules that bring unique functions and serve as effective building blocks for real-world automated laboratory synthesis. This motivates us to propose mCLM, a modular Chemical-Language Model that comprises a bilingual language model that understands both natural language descriptions of functions and molecular blocks. mCLM front-loads synthesizability considerations while improving the predicted functions of molecules in a principled manner. mCLM, with only 3B parameters, achieves improvements in synthetic accessibility relative to 7 other leading generative AI methods including GPT-5. When tested on 122 out-of-distribution medicines using only building blocks/tokens that are compatible with automated modular synthesis, mCLM outperforms all baselines in property scores and synthetic accessibility. mCLM can also reason on multiple functions and iteratively self-improve to rescue drug candidates that failed late in clinical trials (\"fallen angels\").",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12565v2",
    "published_date": "2025-05-18 22:52:39 UTC",
    "updated_date": "2025-10-12 06:32:54 UTC"
  },
  {
    "arxiv_id": "2505.12556v1",
    "title": "Beyond Accuracy: EcoL2 Metric for Sustainable Neural PDE Solvers",
    "authors": [
      "Taniya Kapoor",
      "Abhishek Chandra",
      "Anastasios Stamou",
      "Stephen J Roberts"
    ],
    "abstract": "Real-world systems, from aerospace to railway engineering, are modeled with partial differential equations (PDEs) describing the physics of the system. Estimating robust solutions for such problems is essential. Deep learning-based architectures, such as neural PDE solvers, have recently gained traction as a reliable solution method. The current state of development of these approaches, however, primarily focuses on improving accuracy. The environmental impact of excessive computation, leading to increased carbon emissions, has largely been overlooked. This paper introduces a carbon emission measure for a range of PDE solvers. Our proposed metric, EcoL2, balances model accuracy with emissions across data collection, model training, and deployment. Experiments across both physics-informed machine learning and operator learning architectures demonstrate that the proposed metric presents a holistic assessment of model performance and emission cost. As such solvers grow in scale and deployment, EcoL2 represents a step toward building performant scientific machine learning systems with lower long-term environmental impact.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12556v1",
    "published_date": "2025-05-18 22:05:11 UTC",
    "updated_date": "2025-05-18 22:05:11 UTC"
  },
  {
    "arxiv_id": "2505.12552v2",
    "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction",
    "authors": [
      "Junliang Ye",
      "Lei Wang",
      "Md Zakir Hossain"
    ],
    "abstract": "Reconstructing natural images from functional magnetic resonance imaging (fMRI) data remains a core challenge in natural decoding due to the mismatch between the richness of visual stimuli and the noisy, low resolution nature of fMRI signals. While recent two-stage models, combining deep variational autoencoders (VAEs) with diffusion models, have advanced this task, they treat all spatial-frequency components of the input equally. This uniform treatment forces the model to extract meaning features and suppress irrelevant noise simultaneously, limiting its effectiveness. We introduce FreqSelect, a lightweight, adaptive module that selectively filters spatial-frequency bands before encoding. By dynamically emphasizing frequencies that are most predictive of brain activity and suppressing those that are uninformative, FreqSelect acts as a content-aware gate between image features and natural data. It integrates seamlessly into standard very deep VAE-diffusion pipelines and requires no additional supervision. Evaluated on the Natural Scenes dataset, FreqSelect consistently improves reconstruction quality across both low- and high-level metrics. Beyond performance gains, the learned frequency-selection patterns offer interpretable insights into how different visual frequencies are represented in the brain. Our method generalizes across subjects and scenes, and holds promise for extension to other neuroimaging modalities, offering a principled approach to enhancing both decoding accuracy and neuroscientific interpretability.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted at the British Machine Vision Conference (BMVC 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.12552v2",
    "published_date": "2025-05-18 21:45:06 UTC",
    "updated_date": "2025-08-29 18:38:29 UTC"
  },
  {
    "arxiv_id": "2505.12547v1",
    "title": "ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations",
    "authors": [
      "Florent Chiaroni",
      "Ali Ayub",
      "Ola Ahmad"
    ],
    "abstract": "In robotics applications, few-shot segmentation is crucial because it allows robots to perform complex tasks with minimal training data, facilitating their adaptation to diverse, real-world environments. However, pixel-level annotations of even small amount of images is highly time-consuming and costly. In this paper, we present a novel few-shot binary segmentation method based on bounding-box annotations instead of pixel-level labels. We introduce, ProMi, an efficient prototype-mixture-based method that treats the background class as a mixture of distributions. Our approach is simple, training-free, and effective, accommodating coarse annotations with ease. Compared to existing baselines, ProMi achieves the best results across different datasets with significant gains, demonstrating its effectiveness. Furthermore, we present qualitative experiments tailored to real-world mobile robot tasks, demonstrating the applicability of our approach in such scenarios. Our code: https://github.com/ThalesGroup/promi.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12547v1",
    "published_date": "2025-05-18 21:08:05 UTC",
    "updated_date": "2025-05-18 21:08:05 UTC"
  },
  {
    "arxiv_id": "2505.12532v2",
    "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets",
    "authors": [
      "Ahmet Bilican",
      "M. Akın Yılmaz",
      "A. Murat Tekalp",
      "R. Gökberk Cinbiş"
    ],
    "abstract": "Efficiently adapting large foundation models is critical, especially with tight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA offer limited granularity and effectiveness in few-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT method that learns highly sparse updates in the wavelet domain of residual matrices. WaveFT allows precise control of trainable parameters, offering fine-grained capacity adjustment and excelling with remarkably low parameter count, potentially far fewer than LoRA's minimum, ideal for extreme parameter-efficient scenarios. Evaluated on personalized text-to-image generation using Stable Diffusion XL as baseline, WaveFT significantly outperforms LoRA and other PEFT methods, especially at low parameter counts; achieving superior subject fidelity, prompt alignment, and image diversity.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12532v2",
    "published_date": "2025-05-18 20:20:32 UTC",
    "updated_date": "2025-06-03 21:46:26 UTC"
  },
  {
    "arxiv_id": "2505.12512v1",
    "title": "Scalable Strategies for Continual Learning with Replay",
    "authors": [
      "Truman Hickok"
    ],
    "abstract": "Future deep learning models will be distinguished by systems that perpetually learn through interaction, imagination, and cooperation, blurring the line between training and inference. This makes continual learning a critical challenge, as methods that efficiently maximize bidirectional transfer across learning trajectories will be essential. Replay is on track to play a foundational role in continual learning, allowing models to directly reconcile new information with past knowledge. In practice, however, replay is quite unscalable, doubling the cost of continual learning when applied naively. Moreover, the continual learning literature has not fully synchronized with the multi-task fine-tuning literature, having not fully integrated highly scalable techniques like model merging and low rank adaptation into a replay-enabled toolset that can produce a unified model in the face of many sequential tasks. In this paper, we begin by applying and analyzing low rank adaptation in a continual learning setting. Next, we introduce consolidation, a phasic approach to replay which leads to up to 55\\% less replay samples being needed for a given performance target. Then, we propose sequential merging, an offshoot of task arithmetic which is tailored to the continual learning setting and is shown to work well in combination with replay. Finally, we demonstrate that the developed strategies can operate synergistically, resulting in a highly scalable toolset that outperforms standalone variants.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12512v1",
    "published_date": "2025-05-18 18:23:50 UTC",
    "updated_date": "2025-05-18 18:23:50 UTC"
  },
  {
    "arxiv_id": "2505.12509v2",
    "title": "Revitalizing Black-Box Interpretability: Actionable Interpretability for LLMs via Proxy Models",
    "authors": [
      "Junhao Liu",
      "Haonan Yu",
      "Zhenyu Yan",
      "Xin Zhang"
    ],
    "abstract": "Post-hoc explanations provide transparency and are essential for guiding model optimization, such as prompt engineering and data sanitation. However, applying model-agnostic techniques to Large Language Models (LLMs) is hindered by prohibitive computational costs, rendering these tools dormant for real-world applications. To revitalize model-agnostic interpretability, we propose a budget-friendly proxy framework that leverages efficient models to approximate the decision boundaries of expensive LLMs. We introduce a screen-and-apply mechanism to statistically verify local alignment before deployment. Our empirical evaluation confirms that proxy explanations achieve over 90% fidelity with only 11% of the oracle's cost. Building on this foundation, we demonstrate the actionable utility of our framework in prompt compression and poisoned example removal. Results show that reliable proxy explanations effectively guide optimization, transforming interpretability from a passive observation tool into a scalable primitive for LLM development. Additionally, we open-source code and datasets to facilitate future research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12509v2",
    "published_date": "2025-05-18 18:05:37 UTC",
    "updated_date": "2026-01-20 12:50:06 UTC"
  },
  {
    "arxiv_id": "2505.12506v2",
    "title": "Unsupervised Invariant Risk Minimization",
    "authors": [
      "Yotam Norman",
      "Ron Meir"
    ],
    "abstract": "We propose a novel unsupervised framework for \\emph{Invariant Risk Minimization} (IRM), extending the concept of invariance to settings where labels are unavailable. Traditional IRM methods rely on labeled data to learn representations that are robust to distributional shifts across environments. In contrast, our approach redefines invariance through feature distribution alignment, enabling robust representation learning from unlabeled data. We introduce two methods within this framework: Principal Invariant Component Analysis (PICA), a linear method that extracts invariant directions under Gaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep generative model that disentangles environment-invariant and environment-dependent latent factors. Our approach is based on a novel ``unsupervised'' structural causal model and supports environment-conditioned sample-generation and intervention. Empirical evaluations on synthetic dataset and modified versions of MNIST demonstrate the effectiveness of our methods in capturing invariant structure, preserving relevant information, and generalizing across environments without access to labels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12506v2",
    "published_date": "2025-05-18 17:54:23 UTC",
    "updated_date": "2025-08-16 23:00:50 UTC"
  },
  {
    "arxiv_id": "2505.12504v1",
    "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models",
    "authors": [
      "Zongkai Liu",
      "Fanqing Meng",
      "Lingxiao Du",
      "Zhixiang Zhou",
      "Chao Yu",
      "Wenqi Shao",
      "Qiaosheng Zhang"
    ],
    "abstract": "Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at https://github.com/ModalMinds/MM-EUREKA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12504v1",
    "published_date": "2025-05-18 17:44:53 UTC",
    "updated_date": "2025-05-18 17:44:53 UTC"
  },
  {
    "arxiv_id": "2505.12501v1",
    "title": "ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning",
    "authors": [
      "Edward Y. Chang",
      "Longling Geng"
    ],
    "abstract": "Large language models (LLMs) excel at rapid generation of text and multimodal content, yet they falter on transaction-style planning that demands ACID-like guarantees and real-time disruption recovery. We present Adaptive LLM Agent System (ALAS), a framework that tackles four fundamental LLM deficits: (i) absence of self-verification, (ii) context erosion, (iii) next-token myopia, and (iv) lack of persistent state. ALAS decomposes each plan into role-specialized agents, equips them with automatic state tracking, and coordinates them through a lightweight protocol. When disruptions arise, agents apply history-aware local compensation, avoiding costly global replanning and containing cascade effects. On real-world, large-scale job-shop scheduling benchmarks, ALAS sets new best results for static sequential planning and excels in dynamic reactive scenarios with unexpected disruptions. These gains show that principled modularization plus targeted compensation can unlock scalable and resilient planning with LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "36 pages, 10 figures, 19 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.12501v1",
    "published_date": "2025-05-18 17:27:08 UTC",
    "updated_date": "2025-05-18 17:27:08 UTC"
  },
  {
    "arxiv_id": "2505.13538v1",
    "title": "RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines",
    "authors": [
      "Dvir Cohen",
      "Lin Burg",
      "Gilad Barkan"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems show promise by coupling large language models with external knowledge, yet traditional RAG evaluation methods primarily report quantitative scores while offering limited actionable guidance for refining these complex pipelines. In this paper, we introduce RAGXplain, an evaluation framework that quantifies RAG performance and translates these assessments into clear insights that clarify the workings of its complex, multi-stage pipeline and offer actionable recommendations. Using LLM reasoning, RAGXplain converts raw scores into coherent narratives identifying performance gaps and suggesting targeted improvements. By providing transparent explanations for AI decision-making, our framework fosters user trust-a key challenge in AI adoption. Our LLM-based metric assessments show strong alignment with human judgments, and experiments on public question-answering datasets confirm that applying RAGXplain's actionable recommendations measurably improves system performance. RAGXplain thus bridges quantitative evaluation and practical optimization, empowering users to understand, trust, and enhance their AI systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13538v1",
    "published_date": "2025-05-18 17:25:34 UTC",
    "updated_date": "2025-05-18 17:25:34 UTC"
  },
  {
    "arxiv_id": "2505.12500v1",
    "title": "MARGE: Improving Math Reasoning for LLMs with Guided Exploration",
    "authors": [
      "Jingyue Gao",
      "Runji Lin",
      "Keming Lu",
      "Bowen Yu",
      "Junyang Lin",
      "Jianyu Chen"
    ],
    "abstract": "Large Language Models (LLMs) exhibit strong potential in mathematical reasoning, yet their effectiveness is often limited by a shortage of high-quality queries. This limitation necessitates scaling up computational responses through self-generated data, yet current methods struggle due to spurious correlated data caused by ineffective exploration across all reasoning stages. To address such challenge, we introduce \\textbf{MARGE}: Improving \\textbf{Ma}th \\textbf{R}easoning with \\textbf{G}uided \\textbf{E}xploration, a novel method to address this issue and enhance mathematical reasoning through hit-guided exploration. MARGE systematically explores intermediate reasoning states derived from self-generated solutions, enabling adequate exploration and improved credit assignment throughout the reasoning process. Through extensive experiments across multiple backbone models and benchmarks, we demonstrate that MARGE significantly improves reasoning capabilities without requiring external annotations or training additional value models. Notably, MARGE improves both single-shot accuracy and exploration diversity, mitigating a common trade-off in alignment methods. These results demonstrate MARGE's effectiveness in enhancing mathematical reasoning capabilities and unlocking the potential of scaling self-generated training data. Our code and models are available at \\href{https://github.com/georgao35/MARGE}{this link}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12500v1",
    "published_date": "2025-05-18 17:24:16 UTC",
    "updated_date": "2025-05-18 17:24:16 UTC"
  },
  {
    "arxiv_id": "2505.12493v3",
    "title": "GUI-Shift: Enhancing VLM-Based GUI Agents through Self-supervised Reinforcement Learning",
    "authors": [
      "Longxi Gao",
      "Li Zhang",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan",
      "Mengwei Xu"
    ],
    "abstract": "Training effective Vision-Language Models (VLMs) for GUI agents typically depends on large-scale annotated datasets, whose collection is both labor-intensive and error-prone. We introduce K-step GUI Transition, a self-supervised inverse dynamics task in which VLMs learn GUI dynamics by predicting the initial action that causes a transition between two GUI states. This approach eliminates the need for natural language instructions and enables scalable dataset construction from existing GUI trajectories or automated exploration. Building on this task, we propose GUI-Shift, a reinforcement learning (RL) framework that combines rule-based optimization with data filtering to improve VLM performance. We conduct extensive experiments using multiple VLM backbones across four benchmarks, spanning GUI task automation (AndroidControl, GUI Odyssey) and GUI grounding (ScreenSpot-v2, ScreenSpot-Pro). Our results show that training on GUI-Shift generalizes well to both GUI automation and grounding tasks, yielding up to an 11.2% increase in GUI automation accuracy. This study underscores the potential of self-supervised RL to leverage unlabeled GUI trajectories and offers a scalable alternative to training with annotated samples.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12493v3",
    "published_date": "2025-05-18 16:34:30 UTC",
    "updated_date": "2025-10-10 09:28:53 UTC"
  },
  {
    "arxiv_id": "2505.12492v1",
    "title": "Unleashing Automated Congestion Control Customization in the Wild",
    "authors": [
      "Amit Cohen",
      "Lev Gloukhenki",
      "Ravid Hadar",
      "Eden Itah",
      "Yehuda Shvut",
      "Michael Schapira"
    ],
    "abstract": "Congestion control (CC) crucially impacts user experience across Internet services like streaming, gaming, AR/VR, and connected cars. Traditionally, CC algorithm design seeks universal control rules that yield high performance across diverse application domains and networks. However, varying service needs and network conditions challenge this approach. We share operational experience with a system that automatically customizes congestion control logic to service needs and network conditions. We discuss design, deployment challenges, and solutions, highlighting performance benefits through case studies in streaming, gaming, connected cars, and more.\n  Our system leverages PCC Vivace, an online-learning based congestion control protocol developed by researchers. Hence, along with insights from customizing congestion control, we also discuss lessons learned and modifications made to adapt PCC Vivace for real-world deployment.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG",
      "cs.PF",
      "eess.SY"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12492v1",
    "published_date": "2025-05-18 16:29:19 UTC",
    "updated_date": "2025-05-18 16:29:19 UTC"
  },
  {
    "arxiv_id": "2505.12489v2",
    "title": "Video-GPT via Next Clip Diffusion",
    "authors": [
      "Shaobin Zhuang",
      "Zhipeng Huang",
      "Ying Zhang",
      "Fangyikang Wang",
      "Canmiao Fu",
      "Binxin Yang",
      "Chong Sun",
      "Chen Li",
      "Yali Wang"
    ],
    "abstract": "GPT has shown its remarkable success in natural language processing. However, the language sequence is not sufficient to describe spatial-temporal details in the visual world. Alternatively, the video sequence is good at capturing such details. Motivated by this fact, we propose a concise Video-GPT in this paper by treating video as new language for visual world modeling. By analogy to next token prediction in GPT, we introduce a novel next clip diffusion paradigm for pretraining Video-GPT. Different from the previous works, this distinct paradigm allows Video-GPT to tackle both short-term generation and long-term prediction, by autoregressively denoising the noisy clip according to the clean clips in the history. Extensive experiments show our Video-GPT achieves the state-of-the-art performance on video prediction, which is the key factor towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in both video generation and understanding, showing its great generalization capacity in downstream. The project page is at https://zhuangshaobin.github.io/Video-GPT.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 12 figures, 18 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.12489v2",
    "published_date": "2025-05-18 16:22:58 UTC",
    "updated_date": "2025-05-21 04:44:19 UTC"
  },
  {
    "arxiv_id": "2505.17066v3",
    "title": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration",
    "authors": [
      "Tatia Tsmindashvili",
      "Ana Kolkhidashvili",
      "Dachi Kurtskhalia",
      "Nino Maghlakelidze",
      "Elene Mekvabishvili",
      "Guram Dentoshvili",
      "Orkhan Shamilov",
      "Zaal Gachechiladze",
      "Steven Saporta",
      "David Dachi Choladze"
    ],
    "abstract": "Using LLMs in a production environment presents security challenges that include vulnerabilities to jailbreaks and prompt injections, which can result in harmful outputs for humans or the enterprise. The challenge is amplified when working within a specific domain, as topics generally accepted for LLMs to address may be irrelevant to that field. These problems can be mitigated, for example, by fine-tuning large language models with domain-specific and security-focused data. However, these alone are insufficient, as jailbreak techniques evolve. Additionally, API-accessed models do not offer the flexibility needed to tailor behavior to industry-specific objectives, and in-context learning is not always sufficient or reliable. In response to these challenges, we introduce Archias, an expert model adept at distinguishing between in-domain and out-of-domain communications. Archias classifies user inquiries into several categories: in-domain (specifically for the automotive industry), malicious questions, price injections, prompt injections, and out-of-domain examples. Our methodology integrates outputs from the expert model (Archias) into prompts, which are then processed by the LLM to generate responses. This method increases the model's ability to understand the user's intention and give appropriate answers. Archias can be adjusted, fine-tuned, and used for many different purposes due to its small size. Therefore, it can be easily customized to the needs of any industry. To validate our approach, we created a benchmark dataset for the automotive industry. Furthermore, in the interest of advancing research and development, we release our benchmark dataset to the community.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17066v3",
    "published_date": "2025-05-18 16:13:07 UTC",
    "updated_date": "2025-08-11 12:32:14 UTC"
  },
  {
    "arxiv_id": "2505.12477v2",
    "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning",
    "authors": [
      "Hugues Van Assel",
      "Mark Ibrahim",
      "Tommaso Biancalani",
      "Aviv Regev",
      "Randall Balestriero"
    ],
    "abstract": "Reconstruction and joint embedding have emerged as two leading paradigms in Self Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint embedding methods align the representations of different views in latent space. Both approaches offer compelling advantages, yet practitioners lack clear guidelines for choosing between them. In this work, we unveil the core mechanisms that distinguish each paradigm. By leveraging closed form solutions for both approaches, we precisely characterize how the view generation process, e.g. data augmentation, impacts the learned representations. We then demonstrate that, unlike supervised learning, both SSL paradigms require a minimal alignment between augmentations and irrelevant features to achieve asymptotic optimality with increasing sample size. Our findings indicate that in scenarios where these irrelevant features have a large magnitude, joint embedding methods are preferable because they impose a strictly weaker alignment condition compared to reconstruction based methods. These results not only clarify the trade offs between the two paradigms but also substantiate the empirical success of joint embedding approaches on real world challenging datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.12477v2",
    "published_date": "2025-05-18 15:54:55 UTC",
    "updated_date": "2025-10-14 16:45:33 UTC"
  },
  {
    "arxiv_id": "2505.12476v1",
    "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering",
    "authors": [
      "Xiao Long",
      "Liansheng Zhuang",
      "Chen Shen",
      "Shaotian Yan",
      "Yifei Li",
      "Shafei Wang"
    ],
    "abstract": "Recently, large language models (LLMs) have demonstrated impressive performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to find answers based on knowledge graphs (KGs) for natural language questions. Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the large KGs, and then generates the answers based on them. However, these methods emphasize the exploration of new optimal reasoning paths in KGs while ignoring the exploitation of historical reasoning paths, which may lead to sub-optimal reasoning paths. Additionally, the complex semantics contained in questions may lead to the retrieval of inaccurate reasoning paths. To address these issues, this paper proposes a novel and training-free framework for KGQA tasks called Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original question into a series of simpler and well-defined sub-questions to handle the complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided by a reward model is introduced to iteratively retrieve weighted reasoning paths as contextual knowledge. Finally, it stacks the weighted reasoning paths according to their weights to generate the final answers. Extensive experiments on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves 8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the GrailQA and the WebQSP respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12476v1",
    "published_date": "2025-05-18 15:52:57 UTC",
    "updated_date": "2025-05-18 15:52:57 UTC"
  },
  {
    "arxiv_id": "2505.13535v1",
    "title": "Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments",
    "authors": [
      "Aniket Bhattacharyya",
      "Anurag Tripathi",
      "Ujjal Das",
      "Archan Karmakar",
      "Amit Pathak",
      "Maneesh Gupta"
    ],
    "abstract": "Information extraction (IE) from Visually Rich Documents (VRDs) containing layout features along with text is a critical and well-studied task. Specialized non-LLM NLP-based solutions typically involve training models using both textual and geometric information to label sequences/tokens as named entities or answers to specific questions. However, these approaches lack reasoning, are not able to infer values not explicitly present in documents, and do not generalize well to new formats. Generative LLM-based approaches proposed recently are capable of reasoning, but struggle to comprehend clues from document layout especially in previously unseen document formats, and do not show competitive performance in heterogeneous VRD benchmark datasets. In this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs into localized, reusable semantic textual segments called $\\textit{semantic blocks}$, which are processed independently. Through focused and more generalizable reasoning,our approach outperforms the state-of-the-art on public VRD benchmarks by 1-3% in F1 scores, is resilient to document formats previously not encountered and shows abilities to correctly extract information not explicitly present in documents.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to ACL Main 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.13535v1",
    "published_date": "2025-05-18 15:49:17 UTC",
    "updated_date": "2025-05-18 15:49:17 UTC"
  },
  {
    "arxiv_id": "2505.12470v2",
    "title": "NeuroGen: Neural Network Parameter Generation via Large Language Models",
    "authors": [
      "Jiaqi Wang",
      "Yusen Zhang",
      "Xi Li"
    ],
    "abstract": "Acquiring the parameters of neural networks (NNs) has been one of the most important problems in machine learning since the inception of NNs. Traditional approaches, such as backpropagation and forward-only optimization, acquire parameters via iterative data fitting to gradually optimize them. This paper aims to explore the feasibility of a new direction: acquiring NN parameters via large language model generation. We propose NeuroGen, a generalized and easy-to-implement two-stage approach for NN parameter generation conditioned on descriptions of the data, task, and network architecture. Stage one is Parameter Reference Knowledge Injection, where LLMs are pretrained on NN checkpoints to build foundational understanding of parameter space, whereas stage two is Context-Enhanced Instruction Tuning, enabling LLMs to adapt to specific tasks through enriched, task-aware prompts. Experimental results demonstrate that NeuroGen effectively generates usable NN parameters. Our findings highlight the feasibility of LLM-based NN parameter generation and suggest a promising new paradigm where LLMs and lightweight NNs can coexist synergistically",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The three authors contributed equally to this work. The codes will be public after being accepted",
    "pdf_url": "https://arxiv.org/pdf/2505.12470v2",
    "published_date": "2025-05-18 15:48:10 UTC",
    "updated_date": "2025-05-23 06:25:28 UTC"
  },
  {
    "arxiv_id": "2505.12467v1",
    "title": "Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems",
    "authors": [
      "Haochun Wang",
      "Sendong Zhao",
      "Jingbo Wang",
      "Zewen Qiang",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Multi-agent collaboration has emerged as a pivotal paradigm for addressing complex, distributed tasks in large language model (LLM)-driven applications. While prior research has focused on high-level architectural frameworks, the granular mechanisms governing agents, critical to performance and scalability, remain underexplored. This study systematically investigates four dimensions of collaboration strategies: (1) agent governance, (2) participation control, (3) interaction dynamics, and (4) dialogue history management. Through rigorous experimentation under two context-dependent scenarios: Distributed Evidence Integration (DEI) and Structured Evidence Synthesis (SES), we quantify the impact of these strategies on both task accuracy and computational efficiency. Our findings reveal that centralized governance, instructor-led participation, ordered interaction patterns, and instructor-curated context summarization collectively optimize the trade-off between decision quality and resource utilization with the support of the proposed Token-Accuracy Ratio (TAR). This work establishes a foundation for designing adaptive, scalable multi-agent systems, shifting the focus from structural novelty to strategic interaction mechanics.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12467v1",
    "published_date": "2025-05-18 15:46:14 UTC",
    "updated_date": "2025-05-18 15:46:14 UTC"
  },
  {
    "arxiv_id": "2505.12442v3",
    "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems",
    "authors": [
      "Liwen Wang",
      "Wenxuan Wang",
      "Shuai Wang",
      "Zongjie Li",
      "Zhenlan Ji",
      "Zongyi Lyu",
      "Daoyuan Wu",
      "Shing-Chi Cheung"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12442v3",
    "published_date": "2025-05-18 14:31:45 UTC",
    "updated_date": "2025-06-17 15:37:58 UTC"
  },
  {
    "arxiv_id": "2505.12440v1",
    "title": "Model Discovery with Grammatical Evolution. An Experiment with Prime Numbers",
    "authors": [
      "Jakub Skrzyński",
      "Dominik Sepioło",
      "Antoni Ligęza"
    ],
    "abstract": "Machine Learning produces efficient decision and prediction models based on input-output data only. Such models have the form of decision trees or neural nets and are far from transparent analytical models, based on mathematical formulas. Analytical model discovery requires additional knowledge and may be performed with Grammatical Evolution. Such models are transparent, concise, and have readable components and structure. This paper reports on a non-trivial experiment with generating such models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented during 5th Polish Conference on Artificial Intelligence, published in \"PROGRESS IN POLISH ARTIFICIAL INTELLIGENCE RESEARCH 5\" ISBN 978-83-8156-696-4",
    "pdf_url": "https://arxiv.org/pdf/2505.12440v1",
    "published_date": "2025-05-18 14:22:21 UTC",
    "updated_date": "2025-05-18 14:22:21 UTC"
  },
  {
    "arxiv_id": "2505.12437v2",
    "title": "A method for the systematic generation of graph XAI benchmarks via Weisfeiler-Leman coloring",
    "authors": [
      "Michele Fontanesi",
      "Alessio Micheli",
      "Marco Podda",
      "Domenico Tortorella"
    ],
    "abstract": "Graph neural networks have become the de facto model for learning from structured data. However, the decision-making process of GNNs remains opaque to the end user, which undermines their use in safety-critical applications. Several explainable AI techniques for graphs have been developed to address this major issue. Focusing on graph classification, these explainers identify subgraph motifs that explain predictions. Therefore, a robust benchmarking of graph explainers is required to ensure that the produced explanations are of high quality, i.e., aligned with the GNN's decision process. However, current graph-XAI benchmarks are limited to simplistic synthetic datasets or a few real-world tasks curated by domain experts, hindering rigorous and reproducible evaluation, and consequently stalling progress in the field. To overcome these limitations, we propose a method to automate the construction of graph XAI benchmarks from generic graph classification datasets. Our approach leverages the Weisfeiler-Leman color refinement algorithm to efficiently perform approximate subgraph matching and mine class-discriminating motifs, which serve as proxy ground-truth class explanations. At the same time, we ensure that these motifs can be learned by GNNs because their discriminating power aligns with WL expressiveness. This work also introduces the OpenGraphXAI benchmark suite, which consists of 15 ready-made graph-XAI datasets derived by applying our method to real-world molecular classification datasets. The suite is available to the public along with a codebase to generate over 2,000 additional graph-XAI benchmarks. Finally, we present a use case that illustrates how the suite can be used to assess the effectiveness of a selection of popular graph explainers, demonstrating the critical role of a sufficiently large benchmark collection for improving the significance of experimental results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12437v2",
    "published_date": "2025-05-18 14:19:52 UTC",
    "updated_date": "2025-10-29 15:09:38 UTC"
  },
  {
    "arxiv_id": "2505.12435v1",
    "title": "SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment",
    "authors": [
      "Wenqiao Zhu",
      "Ji Liu",
      "Lulu Wang",
      "Jun Wu",
      "Yulun Zhang"
    ],
    "abstract": "Direct Preference Optimization (DPO) is broadly utilized for aligning Large Language Models (LLMs) with human values because of its flexibility. Despite its effectiveness, it has been observed that the capability of DPO to generate human-preferred response is limited and the results of DPO are far from resilient. To address these limitations, in this paper we propose a novel Self-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which incorporates a pilot term to steer the gradient flow during the optimization process, allowing for fine-grained control over the updates of chosen and rejected rewards. We provide a detailed theoretical analysis of our proposed method and elucidate its operational mechanism. Furthermore, we conduct comprehensive experiments on various models and benchmarks. The extensive experimental results demonstrate the consistency between the empirical results and our theoretical analysis and confirm the effectiveness of our proposed approach (up to 9.19% higher score).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, to appear in ACL'25",
    "pdf_url": "https://arxiv.org/pdf/2505.12435v1",
    "published_date": "2025-05-18 14:19:23 UTC",
    "updated_date": "2025-05-18 14:19:23 UTC"
  },
  {
    "arxiv_id": "2505.12433v1",
    "title": "SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization",
    "authors": [
      "Haodong Yang",
      "Lei Wang",
      "Md Zakir Hossain"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method that injects two trainable low-rank matrices (A and B) into frozen pretrained models. While efficient, LoRA constrains updates to a fixed low-rank subspace (Delta W = BA), which can limit representational capacity and hinder downstream performance. We introduce Subspace Recomposition in Low-Rank Adaptation (SRLoRA) via importance-based fusion and reinitialization, a novel approach that enhances LoRA's expressiveness without compromising its lightweight structure. SRLoRA assigns importance scores to each LoRA pair (a column of B and the corresponding row of A), and dynamically recomposes the subspace during training. Less important pairs are fused into the frozen backbone, freeing capacity to reinitialize new pairs along unused principal directions derived from the pretrained weight's singular value decomposition. This mechanism enables continual subspace refreshment and richer adaptation over time, without increasing the number of trainable parameters. We evaluate SRLoRA on both language and vision tasks, including the GLUE benchmark and various image classification datasets. SRLoRA consistently achieves faster convergence and improved accuracy over standard LoRA, demonstrating its generality, efficiency, and potential for broader PEFT applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Research report",
    "pdf_url": "https://arxiv.org/pdf/2505.12433v1",
    "published_date": "2025-05-18 14:12:40 UTC",
    "updated_date": "2025-05-18 14:12:40 UTC"
  },
  {
    "arxiv_id": "2505.12432v1",
    "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning",
    "authors": [
      "Zirun Guo",
      "Minjie Hong",
      "Tao Jin"
    ],
    "abstract": "Reinforcement Learning (RL) has shown promise in improving the reasoning abilities of Large Language Models (LLMs). However, the specific challenges of adapting RL to multimodal data and formats remain relatively unexplored. In this work, we present Observe-R1, a novel framework aimed at enhancing the reasoning capabilities of multimodal large language models (MLLMs). We draw inspirations from human learning progression--from simple to complex and easy to difficult, and propose a gradual learning paradigm for MLLMs. To this end, we construct the NeuraLadder dataset, which is organized and sampled according to the difficulty and complexity of data samples for RL training. To tackle multimodal tasks, we introduce a multimodal format constraint that encourages careful observation of images, resulting in enhanced visual abilities and clearer and more structured responses. Additionally, we implement a bonus reward system that favors concise, correct answers within a length constraint, alongside a dynamic weighting mechanism that prioritizes uncertain and medium-difficulty problems, ensuring that more informative samples have a greater impact on training. Our experiments with the Qwen2.5-VL-3B and Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that Observe-R1 outperforms a series of larger reasoning models on both reasoning and general benchmarks, achieving superior clarity and conciseness in reasoning chains. Ablation studies validate the effectiveness of our strategies, highlighting the robustness and generalization of our approach. The dataset and code will be released at https://github.com/zrguo/Observe-R1.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12432v1",
    "published_date": "2025-05-18 14:08:03 UTC",
    "updated_date": "2025-05-18 14:08:03 UTC"
  },
  {
    "arxiv_id": "2505.12424v2",
    "title": "EvoGPT: Leveraging LLM-Driven Seed Diversity to Improve Search-Based Test Suite Generation",
    "authors": [
      "Lior Broide",
      "Roni Stern",
      "Argaman Mordoch"
    ],
    "abstract": "Search-Based Software Testing (SBST) is a well-established approach for automated unit test generation, yet it often suffers from premature convergence and limited diversity in the generated test suites. Recently, Large Language Models (LLMs) have emerged as an alternative technique for unit test generation. We present EvoGPT, a hybrid test generation system that integrates LLM-based test generation with SBST-based test suite optimization. EvoGPT uses LLMs to generate an initial population of test suites, and uses an Evolutionary Algorithm (EA) to further optimize this test suite population. A distinguishing feature of EvoGPT is its explicit enforcement of diversity, achieved through the use of multiple temperatures and prompt instructions during test generation. In addition, each LLM-generated test is refined using a generation-repair loop and coverage-guided assertion generation. To address evolutionary plateaus, EvoGPT also detects stagnation during search and injects additional LLM-generated tests aimed at previously uncovered branches. Here too diversity is enforced using multiple temperatures and prompt instructions. We evaluate EvoGPT on Defects4J, a standard benchmark for test generation. The results show that EvoGPT achieves, on average, a 10\\% improvement in both code coverage and mutation score metrics compared to TestART, an LLM-only baseline; and EvoSuite, a standard SBST baseline. An ablation study indicates that explicitly enforcing diversity both at initialization and during the search is key to effectively leveraging LLMs for automated unit test generation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12424v2",
    "published_date": "2025-05-18 13:48:53 UTC",
    "updated_date": "2026-01-06 14:41:55 UTC"
  },
  {
    "arxiv_id": "2505.12423v1",
    "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration",
    "authors": [
      "Wenqiao Zhu",
      "Chao Xu",
      "Lulu Wang",
      "Jun Wu"
    ],
    "abstract": "Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted extensive experiments across multiple models and tasks. The results demonstrate that (1) when PSC is enabled, the comparative reductions in perplexity increase as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our approach is broadly applicable and exhibits robustness across a variety of models and tasks. The code can be found at https://github.com/WNQzhu/PSC.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12423v1",
    "published_date": "2025-05-18 13:47:44 UTC",
    "updated_date": "2025-05-18 13:47:44 UTC"
  },
  {
    "arxiv_id": "2505.12421v3",
    "title": "Fixed Point Explainability",
    "authors": [
      "Emanuele La Malfa",
      "Jon Vadillo",
      "Marco Molinari",
      "Michael Wooldridge"
    ],
    "abstract": "This paper introduces a formal notion of fixed point explanations, inspired by the \"why regress\" principle, to assess, through recursive applications, the stability of the interplay between a model and its explainer. Fixed point explanations satisfy properties like minimality, stability, and faithfulness, revealing hidden model behaviours and explanatory weaknesses. We define convergence conditions for several classes of explainers, from feature-based to mechanistic tools like Sparse AutoEncoders, and we report quantitative and qualitative results for several datasets and models, including LLMs such as Llama-3.3-70B.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The code is available here: https://anonymous.4open.science/r/fixed_point_explainability_iclr2026-D188",
    "pdf_url": "https://arxiv.org/pdf/2505.12421v3",
    "published_date": "2025-05-18 13:43:25 UTC",
    "updated_date": "2025-10-14 16:45:31 UTC"
  },
  {
    "arxiv_id": "2505.12418v1",
    "title": "Mutual Evidential Deep Learning for Medical Image Segmentation",
    "authors": [
      "Yuanpeng He",
      "Yali Bi",
      "Lijian Li",
      "Chi-Man Pun",
      "Wenpin Jiao",
      "Zhi Jin"
    ],
    "abstract": "Existing semi-supervised medical segmentation co-learning frameworks have realized that model performance can be diminished by the biases in model recognition caused by low-quality pseudo-labels. Due to the averaging nature of their pseudo-label integration strategy, they fail to explore the reliability of pseudo-labels from different sources. In this paper, we propose a mutual evidential deep learning (MEDL) framework that offers a potentially viable solution for pseudo-label generation in semi-supervised learning from two perspectives. First, we introduce networks with different architectures to generate complementary evidence for unlabeled samples and adopt an improved class-aware evidential fusion to guide the confident synthesis of evidential predictions sourced from diverse architectural networks. Second, utilizing the uncertainty in the fused evidence, we design an asymptotic Fisher information-based evidential learning strategy. This strategy enables the model to initially focus on unlabeled samples with more reliable pseudo-labels, gradually shifting attention to samples with lower-quality pseudo-labels while avoiding over-penalization of mislabeled classes in high data uncertainty samples. Additionally, for labeled data, we continue to adopt an uncertainty-driven asymptotic learning strategy, gradually guiding the model to focus on challenging voxels. Extensive experiments on five mainstream datasets have demonstrated that MEDL achieves state-of-the-art performance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12418v1",
    "published_date": "2025-05-18 13:42:27 UTC",
    "updated_date": "2025-05-18 13:42:27 UTC"
  },
  {
    "arxiv_id": "2505.12415v2",
    "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding",
    "authors": [
      "Zhenhe Wu",
      "Jian Yang",
      "Jiaheng Liu",
      "Xianjie Wu",
      "Changzai Pan",
      "Jie Zhang",
      "Yu Zhao",
      "Shuangyong Song",
      "Yongxiang Li",
      "Zhoujun Li"
    ],
    "abstract": "Tables present unique challenges for language models due to their structured row-column interactions, necessitating specialized approaches for effective comprehension. While large language models (LLMs) have demonstrated potential in table reasoning through prompting and techniques like chain-of-thought (CoT) and program-of-thought (PoT), optimizing their performance for table question answering remains underexplored. In this paper, we introduce region-based Table-R1, a novel reinforcement learning approach that enhances LLM table understanding by integrating region evidence into reasoning steps. Our method employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in identifying relevant table regions before generating answers, incorporating textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group Relative Policy Optimization (TARPO) introduces a mixed reward system to dynamically balance region accuracy and answer correctness, with decaying region rewards and consistency penalties to align reasoning steps. Experiments show that Table-R1 achieves an average performance improvement of 14.36 points across multiple base models on three benchmark datasets, even outperforming baseline models with ten times the parameters, while TARPO reduces response token consumption by 67.5% compared to GRPO, significantly advancing LLM capabilities in efficient tabular reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12415v2",
    "published_date": "2025-05-18 13:40:18 UTC",
    "updated_date": "2025-06-13 13:02:56 UTC"
  },
  {
    "arxiv_id": "2505.17064v2",
    "title": "Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models",
    "authors": [
      "Maria-Teresa De Rosa Palmini",
      "Eva Cetinic"
    ],
    "abstract": "As Text-to-Image (TTI) diffusion models become increasingly influential in content creation, growing attention is being directed toward their societal and cultural implications. While prior research has primarily examined demographic and cultural biases, the ability of these models to accurately represent historical contexts remains largely underexplored. To address this gap, we introduce a benchmark for evaluating how TTI models depict historical contexts. The benchmark combines HistVis, a dataset of 30,000 synthetic images generated by three state-of-the-art diffusion models from carefully designed prompts covering universal human activities across multiple historical periods, with a reproducible evaluation protocol. We evaluate generated imagery across three key aspects: (1) Implicit Stylistic Associations: examining default visual styles associated with specific eras; (2) Historical Consistency: identifying anachronisms such as modern artifacts in pre-modern contexts; and (3) Demographic Representation: comparing generated racial and gender distributions against historically plausible baselines. Our findings reveal systematic inaccuracies in historically themed generated imagery, as TTI models frequently stereotype past eras by incorporating unstated stylistic cues, introduce anachronisms, and fail to reflect plausible demographic patterns. By providing a reproducible benchmark for historical representation in generated imagery, this work provides an initial step toward building more historically accurate TTI models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17064v2",
    "published_date": "2025-05-18 13:35:23 UTC",
    "updated_date": "2025-10-16 14:16:14 UTC"
  },
  {
    "arxiv_id": "2506.06299v4",
    "title": "How malicious AI swarms can threaten democracy: The fusion of agentic AI and LLMs marks a new frontier in information warfare",
    "authors": [
      "Daniel Thilo Schroeder",
      "Meeyoung Cha",
      "Andrea Baronchelli",
      "Nick Bostrom",
      "Nicholas A. Christakis",
      "David Garcia",
      "Amit Goldenberg",
      "Yara Kyrychenko",
      "Kevin Leyton-Brown",
      "Nina Lutz",
      "Gary Marcus",
      "Filippo Menczer",
      "Gordon Pennycook",
      "David G. Rand",
      "Maria Ressa",
      "Frank Schweitzer",
      "Dawn Song",
      "Christopher Summerfield",
      "Audrey Tang",
      "Jay J. Van Bavel",
      "Sander van der Linden",
      "Jonas R. Kunst"
    ],
    "abstract": "Advances in AI offer the prospect of manipulating beliefs and behaviors on a population-wide level. Large language models and autonomous agents now let influence campaigns reach unprecedented scale and precision. Generative tools can expand propaganda output without sacrificing credibility and inexpensively create falsehoods that are rated as more human-like than those written by humans. Techniques meant to refine AI reasoning, such as chain-of-thought prompting, can just as effectively be used to generate more convincing falsehoods. Enabled by these capabilities, a disruptive threat is emerging: swarms of collaborative, malicious AI agents. Fusing LLM reasoning with multi-agent architectures, these systems are capable of coordinating autonomously, infiltrating communities, and fabricating consensus efficiently. By adaptively mimicking human social dynamics, they threaten democracy. Because the resulting harms stem from design, commercial incentives, and governance, we prioritize interventions at multiple leverage points, focusing on pragmatic mechanisms over voluntary compliance.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "5 Pages, This is the author's version of the work. It is posted here by permission of the AAAS for personal use, not for redistribution. The definitive version was published in Science on January 22, 2026, DOI: 10.1126/science.adz1697",
    "pdf_url": "https://arxiv.org/pdf/2506.06299v4",
    "published_date": "2025-05-18 13:33:37 UTC",
    "updated_date": "2026-01-22 15:03:06 UTC"
  },
  {
    "arxiv_id": "2505.12408v3",
    "title": "ViEEG: Hierarchical Visual Neural Representation for EEG Brain Decoding",
    "authors": [
      "Minxu Liu",
      "Donghai Guan",
      "Chuhang Zheng",
      "Chunwei Tian",
      "Jie Wen",
      "Qi Zhu"
    ],
    "abstract": "Understanding and decoding brain activity into visual representations is a fundamental challenge at the intersection of neuroscience and artificial intelligence. While EEG visual decoding has shown promise due to its non-invasive, and low-cost nature, existing methods suffer from Hierarchical Neural Encoding Neglect (HNEN)-a critical limitation where flat neural representations fail to model the brain's hierarchical visual processing hierarchy. Inspired by the hierarchical organization of visual cortex, we propose ViEEG, a neuro-We further adopt hierarchical contrastive learning for EEG-CLIP representation alignment, enabling zero-shot object recognition. Extensive experiments on the THINGS-EEG dataset demonstrate that ViEEG significantly outperforms previous methods by a large margin in both subject-dependent and subject-independent settings. Results on the THINGS-MEG dataset further confirm ViEEG's generalization to different neural modalities. Our framework not only advances the performance frontier but also sets a new paradigm for EEG brain decoding. inspired framework that addresses HNEN. ViEEG decomposes each visual stimulus into three biologically aligned components-contour, foreground object, and contextual scene-serving as anchors for a three-stream EEG encoder. These EEG features are progressively integrated via cross-attention routing, simulating cortical information flow from low-level to high-level vision.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.12408v3",
    "published_date": "2025-05-18 13:19:08 UTC",
    "updated_date": "2025-09-02 09:03:26 UTC"
  },
  {
    "arxiv_id": "2505.12405v1",
    "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT",
    "authors": [
      "Konstantinos Xylogiannopoulos",
      "Petros Xanthopoulos",
      "Panagiotis Karampelas",
      "Georgios Bakamitsos"
    ],
    "abstract": "Generative AI paraphrased text can be used for copyright infringement and the AI paraphrased content can deprive substantial revenue from original content creators. Despite this recent surge of malicious use of generative AI, there are few academic publications that research this threat. In this article, we demonstrate the ability of pattern-based similarity detection for AI paraphrased news recognition. We propose an algorithmic scheme, which is not limited to detect whether an article is an AI paraphrase, but, more importantly, to identify that the source of infringement is the ChatGPT. The proposed method is tested with a benchmark dataset specifically created for this task that incorporates real articles from BBC, incorporating a total of 2,224 articles across five different news categories, as well as 2,224 paraphrased articles created with ChatGPT. Results show that our pattern similarity-based method, that makes no use of deep learning, can detect ChatGPT assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1 score.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12405v1",
    "published_date": "2025-05-18 13:16:30 UTC",
    "updated_date": "2025-05-18 13:16:30 UTC"
  },
  {
    "arxiv_id": "2505.12404v1",
    "title": "Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies",
    "authors": [
      "Piotr Piękos",
      "Subhradeep Kayal",
      "Alexandros Karatzoglou"
    ],
    "abstract": "Hierarchical data arise in countless domains, from biological taxonomies and organizational charts to legal codes and knowledge graphs. Residual Quantization (RQ) is widely used to generate discrete, multitoken representations for such data by iteratively quantizing residuals in a multilevel codebook. However, its reliance on Euclidean geometry can introduce fundamental mismatches that hinder modeling of hierarchical branching, necessary for faithful representation of hierarchical data. In this work, we propose Hyperbolic Residual Quantization (HRQ), which embeds data natively in a hyperbolic manifold and performs residual quantization using hyperbolic operations and distance metrics. By adapting the embedding network, residual computation, and distance metric to hyperbolic geometry, HRQ imparts an inductive bias that aligns naturally with hierarchical branching. We claim that HRQ in comparison to RQ can generate more useful for downstream tasks discrete hierarchical representations for data with latent hierarchies. We evaluate HRQ on two tasks: supervised hierarchy modeling using WordNet hypernym trees, where the model is supervised to learn the latent hierarchy - and hierarchy discovery, where, while latent hierarchy exists in the data, the model is not directly trained or evaluated on a task related to the hierarchy. Across both scenarios, HRQ hierarchical tokens yield better performance on downstream tasks compared to Euclidean RQ with gains of up to $20\\%$ for the hierarchy modeling task. Our results demonstrate that integrating hyperbolic geometry into discrete representation learning substantially enhances the ability to capture latent hierarchies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12404v1",
    "published_date": "2025-05-18 13:14:07 UTC",
    "updated_date": "2025-05-18 13:14:07 UTC"
  },
  {
    "arxiv_id": "2505.13534v2",
    "title": "InterFeat: A Pipeline for Finding Interesting Scientific Features",
    "authors": [
      "Dan Ofer",
      "Michal Linial",
      "Dafna Shahaf"
    ],
    "abstract": "Finding interesting phenomena is the core of scientific discovery, but it is a manual, ill-defined concept. We present an integrative pipeline for automating the discovery of interesting simple hypotheses (feature-target relations with effect direction and a potential underlying mechanism) in structured biomedical data. The pipeline combines machine learning, knowledge graphs, literature search and Large Language Models. We formalize \"interestingness\" as a combination of novelty, utility and plausibility. On 8 major diseases from the UK Biobank, our pipeline consistently recovers risk factors years before their appearance in the literature. 40--53% of our top candidates were validated as interesting, compared to 0--7% for a SHAP-based baseline. Overall, 28% of 109 candidates were interesting to medical experts. The pipeline addresses the challenge of operationalizing \"interestingness\" scalably and for any target. We release data and code: https://github.com/LinialLab/InterFeat",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13534v2",
    "published_date": "2025-05-18 13:13:51 UTC",
    "updated_date": "2025-09-08 12:11:52 UTC"
  },
  {
    "arxiv_id": "2505.14714v2",
    "title": "KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection",
    "authors": [
      "Tuan-Vinh La",
      "Minh-Hieu Nguyen",
      "Minh-Son Dao"
    ],
    "abstract": "Fake news detection remains a challenging problem due to the complex interplay between textual misinformation, manipulated images, and external knowledge reasoning. While existing approaches have achieved notable results in verifying veracity and cross-modal consistency, two key challenges persist: (1) Existing methods often consider only the global image context while neglecting local object-level details, and (2) they fail to incorporate external knowledge and entity relationships for deeper semantic understanding. To address these challenges, we propose a novel multi-modal fake news detection framework that integrates visual, textual, and knowledge-based representations. Our approach leverages bottom-up attention to capture fine-grained object details, CLIP for global image semantics, and RoBERTa for context-aware text encoding. We further enhance knowledge utilization by retrieving and adaptively selecting relevant entities from a knowledge graph. The fused multi-modal features are processed through a Transformer-based classifier to predict news veracity. Experimental results demonstrate that our model outperforms recent approaches, showcasing the effectiveness of neighbor selection mechanism and multi-modal fusion for fake news detection. Our proposal introduces a new paradigm: knowledge-grounded multimodal reasoning. By integrating explicit entity-level selection and NLI-guided filtering, we shift fake news detection from feature fusion to semantically grounded verification. For reproducibility and further research, the source code is publicly at \\href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Withdrawn by the authors due to lack of explicit agreement from all co-authors to post this version publicly on arXiv",
    "pdf_url": "https://arxiv.org/pdf/2505.14714v2",
    "published_date": "2025-05-18 13:08:38 UTC",
    "updated_date": "2025-10-17 14:44:16 UTC"
  },
  {
    "arxiv_id": "2505.12398v2",
    "title": "Traversal Verification for Speculative Tree Decoding",
    "authors": [
      "Yepeng Weng",
      "Qiao Hu",
      "Xujie Chen",
      "Li Liu",
      "Dianwen Mei",
      "Huishi Qiu",
      "Jiang Tian",
      "Zhongchao Shi"
    ],
    "abstract": "Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025 poster",
    "pdf_url": "https://arxiv.org/pdf/2505.12398v2",
    "published_date": "2025-05-18 12:51:55 UTC",
    "updated_date": "2025-11-05 13:59:39 UTC"
  },
  {
    "arxiv_id": "2505.12395v1",
    "title": "Few-Shot Concept Unlearning with Low Rank Adaptation",
    "authors": [
      "Udaya Shreyas",
      "L. N. Aadarsh"
    ],
    "abstract": "Image Generation models are a trending topic nowadays, with many people utilizing Artificial Intelligence models in order to generate images. There are many such models which, given a prompt of a text, will generate an image which depicts said prompt. There are many image generation models, such as Latent Diffusion Models, Denoising Diffusion Probabilistic Models, Generative Adversarial Networks and many more. When generating images, these models can generate sensitive image data, which can be threatening to privacy or may violate copyright laws of private entities. Machine unlearning aims at removing the influence of specific data subsets from the trained models and in the case of image generation models, remove the influence of a concept such that the model is unable to generate said images of the concept when prompted. Conventional retraining of the model can take upto days, hence fast algorithms are the need of the hour. In this paper we propose an algorithm that aims to remove the influence of concepts in diffusion models through updating the gradients of the final layers of the text encoders. Using a weighted loss function, we utilize backpropagation in order to update the weights of the final layers of the Text Encoder componet of the Stable Diffusion Model, removing influence of the concept from the text-image embedding space, such that when prompted, the result is an image not containing the concept. The weighted loss function makes use of Textual Inversion and Low-Rank Adaptation.We perform our experiments on Latent Diffusion Models, namely the Stable Diffusion v2 model, with an average concept unlearning runtime of 50 seconds using 4-5 images.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12395v1",
    "published_date": "2025-05-18 12:44:30 UTC",
    "updated_date": "2025-05-18 12:44:30 UTC"
  },
  {
    "arxiv_id": "2505.12392v2",
    "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
    "authors": [
      "Yang Hu",
      "Xingyu Zhang",
      "Xueji Fang",
      "Zhiyang Chen",
      "Xiao Wang",
      "Huatian Zhang",
      "Guojun Qi"
    ],
    "abstract": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at https://github.com/maple-research-lab/SLOT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12392v2",
    "published_date": "2025-05-18 12:37:56 UTC",
    "updated_date": "2025-05-26 05:28:49 UTC"
  },
  {
    "arxiv_id": "2505.12386v1",
    "title": "Data Sharing with a Generative AI Competitor",
    "authors": [
      "Boaz Taitler",
      "Omer Madmon",
      "Moshe Tennenholtz",
      "Omer Ben-Porat"
    ],
    "abstract": "As GenAI platforms grow, their dependence on content from competing providers, combined with access to alternative data sources, creates new challenges for data-sharing decisions. In this paper, we provide a model of data sharing between a content creation firm and a GenAI platform that can also acquire content from third-party experts. The interaction is modeled as a Stackelberg game: the firm first decides how much of its proprietary dataset to share with GenAI, and GenAI subsequently determines how much additional data to acquire from external experts. Their utilities depend on user traffic, monetary transfers, and the cost of acquiring additional data from external experts. We characterize the unique subgame perfect equilibrium of the game and uncover a surprising phenomenon: The firm may be willing to pay GenAI to share the firm's own data, leading to a costly data-sharing equilibrium. We further characterize the set of Pareto improving data prices, and show that such improvements occur only when the firm pays to share data. Finally, we study how the price can be set to optimize different design objectives, such as promoting firm data sharing, expert data acquisition, or a balance of both. Our results shed light on the economic forces shaping data-sharing partnerships in the age of GenAI, and provide guidance for platforms, regulators and policymakers seeking to design effective data exchange mechanisms.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12386v1",
    "published_date": "2025-05-18 12:22:37 UTC",
    "updated_date": "2025-05-18 12:22:37 UTC"
  },
  {
    "arxiv_id": "2505.12381v2",
    "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling",
    "authors": [
      "Mohsinul Kabir",
      "Tasfia Tahsin",
      "Sophia Ananiadou"
    ],
    "abstract": "Current research on bias in language models (LMs) predominantly focuses on data quality, with significantly less attention paid to model architecture and temporal influences of data. Even more critically, few studies systematically investigate the origins of bias. We propose a methodology grounded in comparative behavioral theory to interpret the complex interaction between training data and model architecture in bias propagation during language modeling. Building on recent work that relates transformers to n-gram LMs, we evaluate how data, model design choices, and temporal dynamics affect bias propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to context window size in bias propagation, while transformers demonstrate architectural robustness; (2) the temporal provenance of training data significantly affects bias; and (3) different model architectures respond differentially to controlled bias injection, with certain biases (e.g. sexual orientation) being disproportionately amplified. As language models become ubiquitous, our findings highlight the need for a holistic approach -- tracing bias to its origins across both data and model dimensions, not just symptoms, to mitigate harm.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2025 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2505.12381v2",
    "published_date": "2025-05-18 11:55:05 UTC",
    "updated_date": "2025-09-17 15:00:36 UTC"
  },
  {
    "arxiv_id": "2505.13533v1",
    "title": "FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs",
    "authors": [
      "Junzhe Jiang",
      "Chang Yang",
      "Aixin Cui",
      "Sihan Jin",
      "Ruiyu Wang",
      "Bo Li",
      "Xiao Huang",
      "Dongning Sun",
      "Xinrun Wang"
    ],
    "abstract": "Financial tasks are pivotal to global economic stability; however, their execution faces challenges including labor intensive processes, low error tolerance, data fragmentation, and tool limitations. Although large language models (LLMs) have succeeded in various natural language processing tasks and have shown potential in automating workflows through reasoning and contextual understanding, current benchmarks for evaluating LLMs in finance lack sufficient domain-specific data, have simplistic task design, and incomplete evaluation frameworks. To address these gaps, this article presents FinMaster, a comprehensive financial benchmark designed to systematically assess the capabilities of LLM in financial literacy, accounting, auditing, and consulting. Specifically, FinMaster comprises three main modules: i) FinSim, which builds simulators that generate synthetic, privacy-compliant financial data for companies to replicate market dynamics; ii) FinSuite, which provides tasks in core financial domains, spanning 183 tasks of various types and difficulty levels; and iii) FinEval, which develops a unified interface for evaluation. Extensive experiments over state-of-the-art LLMs reveal critical capability gaps in financial reasoning, with accuracy dropping from over 90% on basic tasks to merely 40% on complex scenarios requiring multi-step reasoning. This degradation exhibits the propagation of computational errors, where single-metric calculations initially demonstrating 58% accuracy decreased to 37% in multimetric scenarios. To the best of our knowledge, FinMaster is the first benchmark that covers full-pipeline financial workflows with challenging tasks. We hope that FinMaster can bridge the gap between research and industry practitioners, driving the adoption of LLMs in real-world financial practices to enhance efficiency and accuracy.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-fin.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13533v1",
    "published_date": "2025-05-18 11:47:55 UTC",
    "updated_date": "2025-05-18 11:47:55 UTC"
  },
  {
    "arxiv_id": "2505.13532v1",
    "title": "Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios",
    "authors": [
      "Feihong Zhang",
      "Guojian Zhan",
      "Bin Shuai",
      "Tianyi Zhang",
      "Jingliang Duan",
      "Shengbo Eben Li"
    ],
    "abstract": "Reinforcement learning (RL), known for its self-evolution capability, offers a promising approach to training high-level autonomous driving systems. However, handling constraints remains a significant challenge for existing RL algorithms, particularly in real-world applications. In this paper, we propose a new safety-oriented training technique called harmonic policy iteration (HPI). At each RL iteration, it first calculates two policy gradients associated with efficient driving and safety constraints, respectively. Then, a harmonic gradient is derived for policy updating, minimizing conflicts between the two gradients and consequently enabling a more balanced and stable training process. Furthermore, we adopt the state-of-the-art DSAC algorithm as the backbone and integrate it with our HPI to develop a new safe RL algorithm, DSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H achieves efficient driving performance with near-zero safety constraint violations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE Intelligent Vehicles Symposium (IV 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.13532v1",
    "published_date": "2025-05-18 11:35:57 UTC",
    "updated_date": "2025-05-18 11:35:57 UTC"
  },
  {
    "arxiv_id": "2505.12371v2",
    "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks",
    "authors": [
      "Yinghao Zhu",
      "Ziyi He",
      "Haoran Hu",
      "Xiaochen Zheng",
      "Xichen Zhang",
      "Zixiang Wang",
      "Junyi Gao",
      "Liantao Ma",
      "Lequan Yu"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has stimulated interest in multi-agent collaboration for addressing complex medical tasks. However, the practical advantages of multi-agent collaboration approaches remain insufficiently understood. Existing evaluations often lack generalizability, failing to cover diverse tasks reflective of real-world clinical practice, and frequently omit rigorous comparisons against both single-LLM-based and established conventional methods. To address this critical gap, we introduce MedAgentBoard, a comprehensive benchmark for the systematic evaluation of multi-agent collaboration, single-LLM, and conventional approaches. MedAgentBoard encompasses four diverse medical task categories: (1) medical (visual) question answering, (2) lay summary generation, (3) structured Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow automation, across text, medical images, and structured EHR data. Our extensive experiments reveal a nuanced landscape: while multi-agent collaboration demonstrates benefits in specific scenarios, such as enhancing task completeness in clinical workflow automation, it does not consistently outperform advanced single LLMs (e.g., in textual medical QA) or, critically, specialized conventional methods that generally maintain better performance in tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital resource and actionable insights, emphasizing the necessity of a task-specific, evidence-based approach to selecting and developing AI solutions in medicine. It underscores that the inherent complexity and overhead of multi-agent collaboration must be carefully weighed against tangible performance gains. All code, datasets, detailed prompts, and experimental results are open-sourced at https://medagentboard.netlify.app/.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS 2025 Datasets & Benchmarks Track",
    "pdf_url": "https://arxiv.org/pdf/2505.12371v2",
    "published_date": "2025-05-18 11:28:17 UTC",
    "updated_date": "2025-10-30 13:27:07 UTC"
  },
  {
    "arxiv_id": "2505.12370v2",
    "title": "Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning",
    "authors": [
      "Xinbin Yuan",
      "Jian Zhang",
      "Kaixin Li",
      "Zhuoxuan Cai",
      "Lujian Yao",
      "Jie Chen",
      "Enguang Wang",
      "Qibin Hou",
      "Jinwei Chen",
      "Peng-Tao Jiang",
      "Bo Li"
    ],
    "abstract": "Graphical User Interface (GUI) agents have made substantial strides in understanding and executing user instructions across diverse platforms. Yet, grounding these instructions to precise interface elements remains challenging, especially in complex, high-resolution, professional environments. Traditional supervised finetuning (SFT) methods often require large volumes of diverse data and exhibit weak generalization. To overcome these limitations, we introduce a reinforcement learning (RL) based framework that incorporates three core strategies: (1) seed data curation to ensure high quality training samples, (2) a dense policy gradient that provides continuous feedback based on prediction accuracy, and (3) a self evolutionary reinforcement finetuning mechanism that iteratively refines the model using attention maps. With only 3k training samples, our 7B-parameter model achieves state-of-the-art results among similarly sized models on three grounding benchmarks. Notably, it attains 47.3\\% accuracy on the ScreenSpot-Pro dataset, outperforming much larger models, such as UI-TARS-72B, by a margin of 24.2\\%. These findings underscore the effectiveness of RL-based approaches in enhancing GUI agent performance, particularly in high-resolution, complex environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12370v2",
    "published_date": "2025-05-18 11:22:04 UTC",
    "updated_date": "2025-05-24 03:01:02 UTC"
  },
  {
    "arxiv_id": "2505.12369v1",
    "title": "Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations",
    "authors": [
      "Fernando Zhapa-Camacho",
      "Robert Hoehndorf"
    ],
    "abstract": "Geometric embedding methods have shown to be useful for multi-hop reasoning on knowledge graphs by mapping entities and logical operations to geometric regions and geometric transformations, respectively. Geometric embeddings provide direct interpretability framework for queries. However, current methods have only leveraged the geometric construction of entities, failing to map logical operations to geometric transformations and, instead, using neural components to learn these operations. We introduce GeometrE, a geometric embedding method for multi-hop reasoning, which does not require learning the logical operations and enables full geometric interpretability. Additionally, unlike previous methods, we introduce a transitive loss function and show that it can preserve the logical rule $\\forall a,b,c: r(a,b) \\land r(b,c) \\to r(a,c)$. Our experiments show that GeometrE outperforms current state-of-the-art methods on standard benchmark datasets.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12369v1",
    "published_date": "2025-05-18 11:17:50 UTC",
    "updated_date": "2025-05-18 11:17:50 UTC"
  },
  {
    "arxiv_id": "2505.12368v2",
    "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement",
    "authors": [
      "Gauri Kholkar",
      "Ratinder Ahuja"
    ],
    "abstract": "Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations. To demonstrate our framework's utility, we train CaptureGuard on our generated data. This new model drastically reduces both false negative and false positive rates on our context-aware datasets while also generalizing effectively to external benchmarks, establishing a path toward more robust and practical prompt injection defenses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in ACL LLMSec Workshop 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12368v2",
    "published_date": "2025-05-18 11:14:14 UTC",
    "updated_date": "2025-06-17 05:38:20 UTC"
  },
  {
    "arxiv_id": "2505.12366v5",
    "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization",
    "authors": [
      "Gang Li",
      "Ming Lin",
      "Tomer Galanti",
      "Zhengzhong Tu",
      "Tianbao Yang"
    ],
    "abstract": "The recent success and openness of DeepSeek-R1 have brought widespread attention to Group Relative Policy Optimization (GRPO) as a reinforcement learning method for large reasoning models (LRMs). In this work, we analyze the GRPO objective under a binary reward setting and reveal an inherent limitation of question-level difficulty bias. We also identify a connection between GRPO and traditional discriminative methods in supervised learning. Motivated by these insights, we introduce a new Discriminative Constrained Optimization (DisCO) framework for reinforcing LRMs, grounded in the principle of discriminative learning. The main differences between DisCO and GRPO and its recent variants are: (1) it replaces the group relative objective with a discriminative objective defined by a scoring function; (2) it abandons clipping-based surrogates in favor of non-clipping RL surrogate objectives used as scoring functions; (3) it employs a simple yet effective constrained optimization approach to enforce the KL divergence constraint. As a result, DisCO offers notable advantages over GRPO and its variants: (i) it completely eliminates difficulty bias by adopting discriminative objectives; (ii) it addresses the entropy instability in GRPO and its variants through the use of non-clipping scoring functions and a constrained optimization approach, yielding long and stable training dynamics; (iii) it allows the incorporation of advanced discriminative learning techniques to address data imbalance, where a significant number of questions have more negative than positive generated answers during training. Our experiments on enhancing the mathematical reasoning capabilities of SFT-finetuned models show that DisCO significantly outperforms GRPO and its improved variants such as DAPO, achieving average gains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for a 1.5B model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12366v5",
    "published_date": "2025-05-18 11:08:32 UTC",
    "updated_date": "2026-01-06 16:40:19 UTC"
  },
  {
    "arxiv_id": "2505.12363v4",
    "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts",
    "authors": [
      "Qi Feng"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 19 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.12363v4",
    "published_date": "2025-05-18 10:57:33 UTC",
    "updated_date": "2025-09-09 09:51:38 UTC"
  },
  {
    "arxiv_id": "2505.12361v1",
    "title": "Adaptive MPC-based quadrupedal robot control under periodic disturbances",
    "authors": [
      "Elizaveta Pestova",
      "Ilya Osokin",
      "Danil Belov",
      "Pavel Osinenko"
    ],
    "abstract": "Recent advancements in adaptive control for reference trajectory tracking enable quadrupedal robots to perform locomotion tasks under challenging conditions. There are methods enabling the estimation of the external disturbances in terms of forces and torques. However, a specific case of disturbances that are periodic was not explicitly tackled in application to quadrupeds. This work is devoted to the estimation of the periodic disturbances with a lightweight regressor using simplified robot dynamics and extracting the disturbance properties in terms of the magnitude and frequency. Experimental evidence suggests performance improvement over the baseline static disturbance compensation. All source files, including simulation setups, code, and calculation scripts, are available on GitHub at https://github.com/aidagroup/quad-periodic-mpc.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12361v1",
    "published_date": "2025-05-18 10:48:38 UTC",
    "updated_date": "2025-05-18 10:48:38 UTC"
  },
  {
    "arxiv_id": "2505.12358v1",
    "title": "AbFlowNet: Optimizing Antibody-Antigen Binding Energy via Diffusion-GFlowNet Fusion",
    "authors": [
      "Abrar Rahman Abir",
      "Haz Sameen Shahgir",
      "Md Rownok Zahan Ratul",
      "Md Toki Tahmid",
      "Greg Ver Steeg",
      "Yue Dong"
    ],
    "abstract": "Complementarity Determining Regions (CDRs) are critical segments of an antibody that facilitate binding to specific antigens. Current computational methods for CDR design utilize reconstruction losses and do not jointly optimize binding energy, a crucial metric for antibody efficacy. Rather, binding energy optimization is done through computationally expensive Online Reinforcement Learning (RL) pipelines rely heavily on unreliable binding energy estimators. In this paper, we propose AbFlowNet, a novel generative framework that integrates GFlowNet with Diffusion models. By framing each diffusion step as a state in the GFlowNet framework, AbFlowNet jointly optimizes standard diffusion losses and binding energy by directly incorporating energy signals into the training process, thereby unifying diffusion and reward optimization in a single procedure. Experimental results show that AbFlowNet outperforms the base diffusion model by 3.06% in amino acid recovery, 20.40% in geometric reconstruction (RMSD), and 3.60% in binding energy improvement ratio. ABFlowNet also decreases Top-1 total energy and binding energy errors by 24.8% and 38.1% without pseudo-labeling the test dataset or using computationally expensive online RL regimes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12358v1",
    "published_date": "2025-05-18 10:40:35 UTC",
    "updated_date": "2025-05-18 10:40:35 UTC"
  },
  {
    "arxiv_id": "2505.12355v3",
    "title": "GATES: Cost-aware Dynamic Workflow Scheduling via Graph Attention Networks and Evolution Strategy",
    "authors": [
      "Ya Shen",
      "Gang Chen",
      "Hui Ma",
      "Mengjie Zhang"
    ],
    "abstract": "Cost-aware Dynamic Workflow Scheduling (CADWS) is a key challenge in cloud computing, focusing on devising an effective scheduling policy to efficiently schedule dynamically arriving workflow tasks, represented as Directed Acyclic Graphs (DAG), to suitable virtual machines (VMs). Deep reinforcement learning (DRL) has been widely employed for automated scheduling policy design. However, the performance of DRL is heavily influenced by the design of the problem-tailored policy network and is highly sensitive to hyperparameters and the design of reward feedback. Considering the above-mentioned issues, this study proposes a novel DRL method combining Graph Attention Networks-based policy network and Evolution Strategy, referred to as GATES. The contributions of GATES are summarized as follows: (1) GATES can capture the impact of current task scheduling on subsequent tasks by learning the topological relationships between tasks in a DAG. (2) GATES can assess the importance of each VM to the ready task, enabling it to adapt to dynamically changing VM resources. (3) Utilizing Evolution Strategy's robustness, exploratory nature, and tolerance for delayed rewards, GATES achieves stable policy learning in CADWS. Extensive experimental results demonstrate the superiority of the proposed GATES in CADWS, outperforming several state-of-the-art algorithms. The source code is available at: https://github.com/YaShen998/GATES.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by the 34th International Joint Conference on Artificial Intelligence (IJCAI-2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.12355v3",
    "published_date": "2025-05-18 10:38:41 UTC",
    "updated_date": "2025-08-21 01:43:10 UTC"
  },
  {
    "arxiv_id": "2506.01992v1",
    "title": "No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success",
    "authors": [
      "Lukas Rauch",
      "Moritz Wirth",
      "Denis Huseljic",
      "Marek Herde",
      "Bernhard Sick",
      "Matthias Aßenmacher"
    ],
    "abstract": "The advent of large language models (LLMs) capable of producing general-purpose representations lets us revisit the practicality of deep active learning (AL): By leveraging frozen LLM embeddings, we can mitigate the computational costs of iteratively fine-tuning large backbones. This study establishes a benchmark and systematically investigates the influence of LLM embedding quality on query strategies in deep AL. We employ five top-performing models from the massive text embedding benchmark (MTEB) leaderboard and two baselines for ten diverse text classification tasks. Our findings reveal key insights: First, initializing the labeled pool using diversity-based sampling synergizes with high-quality embeddings, boosting performance in early AL iterations. Second, the choice of the optimal query strategy is sensitive to embedding quality. While the computationally inexpensive Margin sampling can achieve performance spikes on specific datasets, we find that strategies like Badge exhibit greater robustness across tasks. Importantly, their effectiveness is often enhanced when paired with higher-quality embeddings. Our results emphasize the need for context-specific evaluation of AL strategies, as performance heavily depends on embedding quality and the target task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "under review @NeurIPS2025",
    "pdf_url": "https://arxiv.org/pdf/2506.01992v1",
    "published_date": "2025-05-18 10:38:26 UTC",
    "updated_date": "2025-05-18 10:38:26 UTC"
  },
  {
    "arxiv_id": "2505.12354v1",
    "title": "A universal policy wrapper with guarantees",
    "authors": [
      "Anton Bolychev",
      "Georgiy Malaniya",
      "Grigory Yaremenko",
      "Anastasia Krasnaya",
      "Pavel Osinenko"
    ],
    "abstract": "We introduce a universal policy wrapper for reinforcement learning agents that ensures formal goal-reaching guarantees. In contrast to standard reinforcement learning algorithms that excel in performance but lack rigorous safety assurances, our wrapper selectively switches between a high-performing base policy -- derived from any existing RL method -- and a fallback policy with known convergence properties. Base policy's value function supervises this switching process, determining when the fallback policy should override the base policy to ensure the system remains on a stable path. The analysis proves that our wrapper inherits the fallback policy's goal-reaching guarantees while preserving or improving upon the performance of the base policy. Notably, it operates without needing additional system knowledge or online constrained optimization, making it readily deployable across diverse reinforcement learning architectures and tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12354v1",
    "published_date": "2025-05-18 10:37:27 UTC",
    "updated_date": "2025-05-18 10:37:27 UTC"
  },
  {
    "arxiv_id": "2505.12353v1",
    "title": "Importance Sampling for Nonlinear Models",
    "authors": [
      "Prakash Palanivelu Rajmohan",
      "Fred Roosta"
    ],
    "abstract": "While norm-based and leverage-score-based methods have been extensively studied for identifying \"important\" data points in linear models, analogous tools for nonlinear models remain significantly underdeveloped. By introducing the concept of the adjoint operator of a nonlinear map, we address this gap and generalize norm-based and leverage-score-based importance sampling to nonlinear settings. We demonstrate that sampling based on these generalized notions of norm and leverage scores provides approximation guarantees for the underlying nonlinear mapping, similar to linear subspace embeddings. As direct applications, these nonlinear scores not only reduce the computational complexity of training nonlinear models by enabling efficient sampling over large datasets but also offer a novel mechanism for model explainability and outlier detection. Our contributions are supported by both theoretical analyses and experimental results across a variety of supervised learning scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "This work is accepted at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12353v1",
    "published_date": "2025-05-18 10:34:39 UTC",
    "updated_date": "2025-05-18 10:34:39 UTC"
  },
  {
    "arxiv_id": "2505.12350v1",
    "title": "Multi-CALF: A Policy Combination Approach with Statistical Guarantees",
    "authors": [
      "Georgiy Malaniya",
      "Anton Bolychev",
      "Grigory Yaremenko",
      "Anastasia Krasnaya",
      "Pavel Osinenko"
    ],
    "abstract": "We introduce Multi-CALF, an algorithm that intelligently combines reinforcement learning policies based on their relative value improvements. Our approach integrates a standard RL policy with a theoretically-backed alternative policy, inheriting formal stability guarantees while often achieving better performance than either policy individually. We prove that our combined policy converges to a specified goal set with known probability and provide precise bounds on maximum deviation and convergence time. Empirical validation on control tasks demonstrates enhanced performance while maintaining stability guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12350v1",
    "published_date": "2025-05-18 10:30:24 UTC",
    "updated_date": "2025-05-18 10:30:24 UTC"
  },
  {
    "arxiv_id": "2505.12349v1",
    "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds",
    "authors": [
      "Axel Abels",
      "Tom Lenaerts"
    ],
    "abstract": "Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the \"wisdom of the crowd\", can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in the Proceedings of the 34th International Joint Conference on Artificial Intelligence (IJCAI 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.12349v1",
    "published_date": "2025-05-18 10:29:24 UTC",
    "updated_date": "2025-05-18 10:29:24 UTC"
  },
  {
    "arxiv_id": "2505.12348v1",
    "title": "Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim Verification",
    "authors": [
      "Zhi Zheng",
      "Wee Sun Lee"
    ],
    "abstract": "Claim verification is essential in combating misinformation, and large language models (LLMs) have recently emerged in this area as powerful tools for assessing the veracity of claims using external knowledge. Existing LLM-based methods for claim verification typically adopt a Decompose-Then-Verify paradigm, which involves decomposing complex claims into several independent sub-claims and verifying each sub-claim separately. However, this paradigm often introduces errors during the claim decomposition process. To mitigate these errors, we propose to develop the Chain-of-Thought (CoT)-Verify paradigm, which leverages LLM reasoning methods to generate CoT-verification paths for the original complex claim without requiring decompositions into sub-claims and separate verification stages. The CoT-Verify paradigm allows us to propose a natural fine-tuning method called Reasoning-CV to enhance the verification capabilities in LLMs. Reasoning-CV includes a supervised fine-tuning (SFT) stage and a self-improvement direct preference optimization (DPO) stage. Utilizing only an 8B pre-trained LLM, Reasoning-CV demonstrates superior knowledge-assisted claim verification performances compared to existing Decompose-Then-Verify methods, as well as powerful black-box LLMs such as GPT-4o+CoT and o1-preview. Our code is available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12348v1",
    "published_date": "2025-05-18 10:28:54 UTC",
    "updated_date": "2025-05-18 10:28:54 UTC"
  },
  {
    "arxiv_id": "2505.12346v1",
    "title": "SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization",
    "authors": [
      "Minghan Chen",
      "Guikun Chen",
      "Wenguan Wang",
      "Yi Yang"
    ],
    "abstract": "Large language models (LLMs) exhibit varying levels of confidence across input prompts (questions): some lead to consistent, semantically similar answers, while others yield diverse or contradictory outputs. This variation reflects LLM's uncertainty about the input prompt, a signal of how confidently the model understands a given problem. However, vanilla Group Relative Policy Optimization (GRPO) treats all prompts equally during policy updates, ignoring this important information about the model's knowledge boundaries. To address this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which explicitly measures LLMs' uncertainty of the input prompts semantic entropy. Semantic entropy measures the diversity of meaning in multiple generated answers given a prompt and uses this to modulate the magnitude of policy updates. This uncertainty-aware training mechanism enables dynamic adjustment of policy update magnitudes based on question uncertainty. It allows more conservative updates on high-uncertainty questions while maintaining the original learning signal on confident ones. Experimental results on five mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva 34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new state-of-the-art performance in average accuracy, validating the effectiveness of uncertainty-aware policy optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "On going project",
    "pdf_url": "https://arxiv.org/pdf/2505.12346v1",
    "published_date": "2025-05-18 10:20:59 UTC",
    "updated_date": "2025-05-18 10:20:59 UTC"
  },
  {
    "arxiv_id": "2505.12343v1",
    "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models",
    "authors": [
      "Kai Tang",
      "Jinhao You",
      "Xiuqi Ge",
      "Hanze Li",
      "Yichen Guo",
      "Xiande Huang"
    ],
    "abstract": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs), they remain susceptible to hallucinations-generating content that is inconsistent with the input image. Existing training-free hallucination mitigation methods often suffer from unstable performance and high sensitivity to hyperparameter settings, limiting their practicality and broader adoption. In this paper, we propose a novel decoding mechanism, Decoding with Inter-layer Consistency via Layer Aggregation (DCLA), which requires no retraining, fine-tuning, or access to external knowledge bases. Specifically, our approach constructs a dynamic semantic reference by aggregating representations from previous layers, and corrects semantically deviated layers to enforce inter-layer consistency. The method allows DCLA to robustly mitigate hallucinations across multiple LVLMs. Experiments on hallucination benchmarks such as MME and POPE demonstrate that DCLA effectively reduces hallucinations while enhancing the reliability and performance of LVLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12343v1",
    "published_date": "2025-05-18 10:15:42 UTC",
    "updated_date": "2025-05-18 10:15:42 UTC"
  },
  {
    "arxiv_id": "2505.12339v1",
    "title": "Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation",
    "authors": [
      "Midou Guo",
      "Qilin Yin",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "abstract": "With the development of generative artificial intelligence, new forgery methods are rapidly emerging. Social platforms are flooded with vast amounts of unlabeled synthetic data and authentic data, making it increasingly challenging to distinguish real from fake. Due to the lack of labels, existing supervised detection methods struggle to effectively address the detection of unknown deepfake methods. Moreover, in open world scenarios, the amount of unlabeled data greatly exceeds that of labeled data. Therefore, we define a new deepfake detection generalization task which focuses on how to achieve efficient detection of large amounts of unlabeled data based on limited labeled data to simulate a open world scenario. To solve the above mentioned task, we propose a novel Open-World Deepfake Detection Generalization Enhancement Training Strategy (OWG-DS) to improve the generalization ability of existing methods. Our approach aims to transfer deepfake detection knowledge from a small amount of labeled source domain data to large-scale unlabeled target domain data. Specifically, we introduce the Domain Distance Optimization (DDO) module to align different domain features by optimizing both inter-domain and intra-domain distances. Additionally, the Similarity-based Class Boundary Separation (SCBS) module is used to enhance the aggregation of similar samples to ensure clearer class boundaries, while an adversarial training mechanism is adopted to learn the domain-invariant features. Extensive experiments show that the proposed deepfake detection generalization enhancement training strategy excels in cross-method and cross-dataset scenarios, improving the model's generalization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12339v1",
    "published_date": "2025-05-18 10:12:12 UTC",
    "updated_date": "2025-05-18 10:12:12 UTC"
  },
  {
    "arxiv_id": "2505.12334v1",
    "title": "Enhancing User-Oriented Proactivity in Open-Domain Dialogues with Critic Guidance",
    "authors": [
      "Yufeng Wang",
      "Jinwu Hu",
      "Ziteng Huang",
      "Kunyang Lin",
      "Zitian Zhang",
      "Peihao Chen",
      "Yu Hu",
      "Qianyue Wang",
      "Zhuliang Yu",
      "Bin Sun",
      "Xiaofen Xing",
      "Qingfang Zheng",
      "Mingkui Tan"
    ],
    "abstract": "Open-domain dialogue systems aim to generate natural and engaging conversations, providing significant practical value in real applications such as social robotics and personal assistants. The advent of large language models (LLMs) has greatly advanced this field by improving context understanding and conversational fluency. However, existing LLM-based dialogue systems often fall short in proactively understanding the user's chatting preferences and guiding conversations toward user-centered topics. This lack of user-oriented proactivity can lead users to feel unappreciated, reducing their satisfaction and willingness to continue the conversation in human-computer interactions. To address this issue, we propose a User-oriented Proactive Chatbot (UPC) to enhance the user-oriented proactivity. Specifically, we first construct a critic to evaluate this proactivity inspired by the LLM-as-a-judge strategy. Given the scarcity of high-quality training data, we then employ the critic to guide dialogues between the chatbot and user agents, generating a corpus with enhanced user-oriented proactivity. To ensure the diversity of the user backgrounds, we introduce the ISCO-800, a diverse user background dataset for constructing user agents. Moreover, considering the communication difficulty varies among users, we propose an iterative curriculum learning method that trains the chatbot from easy-to-communicate users to more challenging ones, thereby gradually enhancing its performance. Experiments demonstrate that our proposed training method is applicable to different LLMs, improving user-oriented proactivity and attractiveness in open-domain dialogues.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.12334v1",
    "published_date": "2025-05-18 09:59:22 UTC",
    "updated_date": "2025-05-18 09:59:22 UTC"
  },
  {
    "arxiv_id": "2505.12332v5",
    "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning",
    "authors": [
      "Qianyue Hu",
      "Junyan Wu",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "abstract": "Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at https://voice-cloak.github.io/VoiceCloak/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "15 pages, 6 figures, 13 tables; Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2505.12332v5",
    "published_date": "2025-05-18 09:58:48 UTC",
    "updated_date": "2025-12-09 17:48:25 UTC"
  },
  {
    "arxiv_id": "2505.12329v1",
    "title": "MPRM: A Markov Path-based Rule Miner for Efficient and Interpretable Knowledge Graph Reasoning",
    "authors": [
      "Mingyang Li",
      "Song Wang",
      "Ning Cai"
    ],
    "abstract": "Rule mining in knowledge graphs enables interpretable link prediction. However, deep learning-based rule mining methods face significant memory and time challenges for large-scale knowledge graphs, whereas traditional approaches, limited by rigid confidence metrics, incur high computational costs despite sampling techniques. To address these challenges, we propose MPRM, a novel rule mining method that models rule-based inference as a Markov chain and uses an efficient confidence metric derived from aggregated path probabilities, significantly lowering computational demands. Experiments on multiple datasets show that MPRM efficiently mines knowledge graphs with over a million facts, sampling less than 1% of facts on a single CPU in 22 seconds, while preserving interpretability and boosting inference accuracy by up to 11% over baselines.",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12329v1",
    "published_date": "2025-05-18 09:48:45 UTC",
    "updated_date": "2025-05-18 09:48:45 UTC"
  },
  {
    "arxiv_id": "2505.12327v1",
    "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions",
    "authors": [
      "Albert Zhao",
      "Stefano Soatto"
    ],
    "abstract": "We describe a robust planning method for autonomous driving that mixes normal and adversarial agent predictions output by a diffusion model trained for motion prediction. We first train a diffusion model to learn an unbiased distribution of normal agent behaviors. We then generate a distribution of adversarial predictions by biasing the diffusion model at test time to generate predictions that are likely to collide with a candidate plan. We score plans using expected cost with respect to a mixture distribution of normal and adversarial predictions, leading to a planner that is robust against adversarial behaviors but not overly conservative when agents behave normally. Unlike current approaches, we do not use risk measures that over-weight adversarial behaviors while placing little to no weight on low-cost normal behaviors or use hard safety constraints that may not be appropriate for all driving scenarios. We show the effectiveness of our method on single-agent and multi-agent jaywalking scenarios as well as a red light violation scenario.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE International Conference on Robotics and Automation (ICRA) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12327v1",
    "published_date": "2025-05-18 09:44:57 UTC",
    "updated_date": "2025-05-18 09:44:57 UTC"
  },
  {
    "arxiv_id": "2505.12321v1",
    "title": "BeliefNest: A Joint Action Simulator for Embodied Agents with Theory of Mind",
    "authors": [
      "Rikunari Sagara",
      "Koichiro Terao",
      "Naoto Iwahashi"
    ],
    "abstract": "This paper introduces an open-source simulator, BeliefNest, designed to enable embodied agents to perform collaborative tasks by leveraging Theory of Mind. BeliefNest dynamically and hierarchically constructs simulators within a Minecraft environment, allowing agents to explicitly represent nested belief states about themselves and others. This enables agent control in open-domain tasks that require Theory of Mind reasoning. The simulator provides a prompt generation mechanism based on each belief state, facilitating the design and evaluation of methods for agent control utilizing large language models (LLMs). We demonstrate through experiments that agents can infer others' beliefs and predict their belief-based actions in false-belief tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12321v1",
    "published_date": "2025-05-18 09:26:48 UTC",
    "updated_date": "2025-05-18 09:26:48 UTC"
  },
  {
    "arxiv_id": "2505.13531v1",
    "title": "AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference",
    "authors": [
      "Shitong Duan",
      "Xiaoyuan Yi",
      "Peng Zhang",
      "Dongkuan Xu",
      "Jing Yao",
      "Tun Lu",
      "Ning Gu",
      "Xing Xie"
    ],
    "abstract": "Assessing Large Language Models (LLMs)' underlying value differences enables comprehensive comparison of their misalignment, cultural adaptability, and biases. Nevertheless, current value measurement datasets face the informativeness challenge: with often outdated, contaminated, or generic test questions, they can only capture the shared value orientations among different LLMs, leading to saturated and thus uninformative results. To address this problem, we introduce AdAEM, a novel, self-extensible assessment framework for revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM can automatically and adaptively generate and extend its test questions. This is achieved by probing the internal value boundaries of a diverse set of LLMs developed across cultures and time periods in an in-context optimization manner. The optimization process theoretically maximizes an information-theoretic objective to extract the latest or culturally controversial topics, providing more distinguishable and informative insights about models' value differences. In this way, AdAEM is able to co-evolve with the development of LLMs, consistently tracking their value dynamics. Using AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct an extensive analysis to manifest our method's validity and effectiveness, and benchmark the values of 16 LLMs, laying the groundwork for better value research.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13531v1",
    "published_date": "2025-05-18 09:15:26 UTC",
    "updated_date": "2025-05-18 09:15:26 UTC"
  },
  {
    "arxiv_id": "2505.12312v4",
    "title": "Visuospatial Cognitive Assistant",
    "authors": [
      "Qi Feng"
    ],
    "abstract": "Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "31 pages, 10 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.12312v4",
    "published_date": "2025-05-18 08:55:02 UTC",
    "updated_date": "2025-09-09 09:48:14 UTC"
  },
  {
    "arxiv_id": "2505.12310v2",
    "title": "DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations",
    "authors": [
      "Shouyi Lu",
      "Huanyu Zhou",
      "Guirong Zhuo",
      "Xiao Tang"
    ],
    "abstract": "A novel learning-optimization-combined 4D radar odometry model, named DNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates traditional geometric optimization with end-to-end neural network training, leveraging an innovative differentiable neural-optimization iteration operator. In this framework, point-wise motion flow is first estimated using a neural network, followed by the construction of a cost function based on the relationship between point motion and pose in 3D space. The radar pose is then refined using Gauss-Newton updates. Additionally, we design a dual-stream 4D radar backbone that integrates multi-scale geometric features and clustering-based class-aware features to enhance the representation of sparse 4D radar point clouds. Extensive experiments on the VoD and Snail-Radar datasets demonstrate the superior performance of our model, which outperforms recent classical and learning-based approaches. Notably, our method even achieves results comparable to A-LOAM with mapping optimization using LiDAR point clouds as input. Our models and code will be publicly released.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages,5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.12310v2",
    "published_date": "2025-05-18 08:50:54 UTC",
    "updated_date": "2025-11-08 07:54:35 UTC"
  },
  {
    "arxiv_id": "2505.12309v1",
    "title": "Community Search in Time-dependent Road-social Attributed Networks",
    "authors": [
      "Li Ni",
      "Hengkai Xu",
      "Lin Mu",
      "Yiwen Zhang",
      "Wenjian Luo"
    ],
    "abstract": "Real-world networks often involve both keywords and locations, along with travel time variations between locations due to traffic conditions. However, most existing cohesive subgraph-based community search studies utilize a single attribute, either keywords or locations, to identify communities. They do not simultaneously consider both keywords and locations, which results in low semantic or spatial cohesiveness of the detected communities, and they fail to account for variations in travel time. Additionally, these studies traverse the entire network to build efficient indexes, but the detected community only involves nodes around the query node, leading to the traversal of nodes that are not relevant to the community. Therefore, we propose the problem of discovering semantic-spatial aware k-core, which refers to a k-core with high semantic and time-dependent spatial cohesiveness containing the query node. To address this problem, we propose an exact and a greedy algorithm, both of which gradually expand outward from the query node. They are local methods that only access the local part of the attributed network near the query node rather than the entire network. Moreover, we design a method to calculate the semantic similarity between two keywords using large language models. This method alleviates the disadvantages of keyword-matching methods used in existing community search studies, such as mismatches caused by differently expressed synonyms and the presence of irrelevant words. Experimental results show that the greedy algorithm outperforms baselines in terms of structural, semantic, and time-dependent spatial cohesiveness.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "12 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.12309v1",
    "published_date": "2025-05-18 08:45:05 UTC",
    "updated_date": "2025-05-18 08:45:05 UTC"
  },
  {
    "arxiv_id": "2505.12304v2",
    "title": "Pre-trained Prompt-driven Semi-supervised Local Community Detection",
    "authors": [
      "Li Ni",
      "Hengkai Xu",
      "Lin Mu",
      "Yiwen Zhang",
      "Wenjian Luo"
    ],
    "abstract": "Semi-supervised local community detection aims to leverage known communities to detect the community containing a given node. Although existing semi-supervised local community detection studies yield promising results, they suffer from time-consuming issues, highlighting the need for more efficient algorithms. Therefore, we apply the \"pre-train, prompt\" paradigm to semi-supervised local community detection and propose the Pre-trained Prompt-driven Semi-supervised Local community detection method (PPSL). PPSL consists of three main components: node encoding, sample generation, and prompt-driven fine-tuning. Specifically, the node encoding component employs graph neural networks to learn the representations of nodes and communities. Based on representations of nodes and communities, the sample generation component selects known communities that are structurally similar to the local structure of the given node as training samples. Finally, the prompt-driven fine-tuning component leverages these training samples as prompts to guide the final community prediction. Experimental results on five real-world datasets demonstrate that PPSL outperforms baselines in both community quality and efficiency.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "11 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.12304v2",
    "published_date": "2025-05-18 08:36:37 UTC",
    "updated_date": "2025-05-30 06:17:36 UTC"
  },
  {
    "arxiv_id": "2505.12301v1",
    "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge",
    "authors": [
      "Luyu Chen",
      "Zeyu Zhang",
      "Haoran Tan",
      "Quanyu Dai",
      "Hao Yang",
      "Zhenhua Dong",
      "Xu Chen"
    ],
    "abstract": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm, offering significant efficiency and flexibility compared to human judgments. However, previous methods primarily rely on single-point evaluations, overlooking the inherent diversity and uncertainty in human evaluations. This approach leads to information loss and decreases the reliability of evaluations. To address this limitation, we propose a novel training framework that explicitly aligns the LLM-generated judgment distribution with empirical human distributions. Specifically, we propose a distributional alignment objective based on KL divergence, combined with an auxiliary cross-entropy regularization to stabilize the training process. Furthermore, considering that empirical distributions may derive from limited human annotations, we incorporate adversarial training to enhance model robustness against distribution perturbations. Extensive experiments across various LLM backbones and evaluation tasks demonstrate that our framework significantly outperforms existing closed-source LLMs and conventional single-point alignment methods, with improved alignment quality, evaluation accuracy, and robustness.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 3 tables, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.12301v1",
    "published_date": "2025-05-18 08:33:09 UTC",
    "updated_date": "2025-05-18 08:33:09 UTC"
  },
  {
    "arxiv_id": "2505.12299v3",
    "title": "MobileIPL: Enhancing Mobile Agents Thinking Process via Iterative Preference Learning",
    "authors": [
      "Kun Huang",
      "Weikai Xu",
      "Yuxuan Liu",
      "Quandong Wang",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan",
      "Bin Wang",
      "Bo An"
    ],
    "abstract": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to improve the reasoning performance of VLM-based mobile agents in GUI tasks. However, the scarcity of diverse CoaT trajectories limits the expressiveness and generalization ability of such agents. While self-training is commonly employed to address data scarcity, existing approaches either overlook the correctness of intermediate reasoning steps or depend on expensive process-level annotations to construct process reward models (PRM). To address the above problems, we propose an Iterative Preference Learning (IPL) that constructs a CoaT-tree through interative sampling, scores leaf nodes using rule-based reward, and backpropagates feedback to derive Thinking-level Direct Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up supervised fine-tuning, we further introduce a three-stage instruction evolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real mobile UI screenshots, enhancing both generality and layout understanding. Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our agent MobileIPL outperforms strong baselines, including continual pretraining models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance across three standard Mobile GUI-Agents benchmarks and shows strong generalization to out-of-domain scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 8 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.12299v3",
    "published_date": "2025-05-18 08:28:05 UTC",
    "updated_date": "2025-09-29 08:49:00 UTC"
  },
  {
    "arxiv_id": "2505.12298v1",
    "title": "Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans",
    "authors": [
      "Amal Lahchim",
      "Lazar Davic"
    ],
    "abstract": "In this study, we propose a robust methodology for automatic segmentation of infected lung regions in COVID-19 CT scans using convolutional neural networks. The approach is based on a modified U-Net architecture enhanced with attention mechanisms, data augmentation, and postprocessing techniques. It achieved a Dice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods. The dataset was sourced from public repositories and augmented for diversity. Results demonstrate superior segmentation performance. Future work includes expanding the dataset, exploring 3D segmentation, and preparing the model for clinical deployment.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "14 pages, 9 figures, created using Google Colab and PyTorch. Compares segmentation models for COVID-19 CT data",
    "pdf_url": "https://arxiv.org/pdf/2505.12298v1",
    "published_date": "2025-05-18 08:27:12 UTC",
    "updated_date": "2025-05-18 08:27:12 UTC"
  },
  {
    "arxiv_id": "2505.12296v1",
    "title": "PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking",
    "authors": [
      "Haiyu Deng",
      "Yanna Jiang",
      "Guangsheng Yu",
      "Qin Wang",
      "Xu Wang",
      "Baihe Ma",
      "Wei Ni",
      "Ren Ping Liu"
    ],
    "abstract": "Machine learning models are increasingly shared and outsourced, raising requirements of verifying training effort (Proof-of-Learning, PoL) to ensure claimed performance and establishing ownership (Proof-of-Ownership, PoO) for transactions. When models are trained by untrusted parties, PoL and PoO must be enforced together to enable protection, attribution, and compensation. However, existing studies typically address them separately, which not only weakens protection against forgery and privacy breaches but also leads to high verification overhead.\n  We propose PoLO, a unified framework that simultaneously achieves PoL and PoO using chained watermarks. PoLO splits the training process into fine-grained training shards and embeds a dedicated watermark in each shard. Each watermark is generated using the hash of the preceding shard, certifying the training process of the preceding shard. The chained structure makes it computationally difficult to forge any individual part of the whole training process. The complete set of watermarks serves as the PoL, while the final watermark provides the PoO. PoLO offers more efficient and privacy-preserving verification compared to the vanilla PoL solutions that rely on gradient-based trajectory tracing and inadvertently expose training data during verification, while maintaining the same level of ownership assurance of watermark-based PoO schemes. Our evaluation shows that PoLO achieves 99% watermark detection accuracy for ownership verification, while preserving data privacy and cutting verification costs to just 1.5-10% of traditional methods. Forging PoLO demands 1.1-4x more resources than honest proof generation, with the original proof retaining over 90% detection accuracy even after attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12296v1",
    "published_date": "2025-05-18 08:19:18 UTC",
    "updated_date": "2025-05-18 08:19:18 UTC"
  },
  {
    "arxiv_id": "2505.12292v1",
    "title": "SpikeX: Exploring Accelerator Architecture and Network-Hardware Co-Optimization for Sparse Spiking Neural Networks",
    "authors": [
      "Boxun Xu",
      "Richard Boone",
      "Peng Li"
    ],
    "abstract": "Spiking Neural Networks (SNNs) are promising biologically plausible models of computation which utilize a spiking binary activation function similar to that of biological neurons. SNNs are well positioned to process spatiotemporal data, and are advantageous in ultra-low power and real-time processing. Despite a large body of work on conventional artificial neural network accelerators, much less attention has been given to efficient SNN hardware accelerator design. In particular, SNNs exhibit inherent unstructured spatial and temporal firing sparsity, an opportunity yet to be fully explored for great hardware processing efficiency. In this work, we propose a novel systolic-array SNN accelerator architecture, called SpikeX, to take on the challenges and opportunities stemming from unstructured sparsity while taking into account the unique characteristics of spike-based computation. By developing an efficient dataflow targeting expensive multi-bit weight data movements, SpikeX reduces memory access and increases data sharing and hardware utilization for computations spanning across both time and space, thereby significantly improving energy efficiency and inference latency. Furthermore, recognizing the importance of SNN network and hardware co-design, we develop a co-optimization methodology facilitating not only hardware-aware SNN training but also hardware accelerator architecture search, allowing joint network weight parameter optimization and accelerator architectural reconfiguration. This end-to-end network/accelerator co-design approach offers a significant reduction of 15.1x-150.87x in energy-delay-product(EDP) without comprising model accuracy.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.NE",
    "comment": "The paper has been accepted by IEEE TCAD",
    "pdf_url": "https://arxiv.org/pdf/2505.12292v1",
    "published_date": "2025-05-18 08:07:44 UTC",
    "updated_date": "2025-05-18 08:07:44 UTC"
  },
  {
    "arxiv_id": "2505.12287v1",
    "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models",
    "authors": [
      "Linghan Huang",
      "Haolin Jin",
      "Zhaoge Bi",
      "Pengyue Yang",
      "Peizhou Zhao",
      "Taozhao Chen",
      "Xiongfei Wu",
      "Lei Ma",
      "Huaming Chen"
    ],
    "abstract": "Large language models (LLMs) have seen widespread applications across various domains, yet remain vulnerable to adversarial prompt injections. While most existing research on jailbreak attacks and hallucination phenomena has focused primarily on open-source models, we investigate the frontier of closed-source LLMs under multilingual attack scenarios. We present a first-of-its-kind integrated adversarial framework that leverages diverse attack techniques to systematically evaluate frontier proprietary solutions, including GPT-4o, DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories of security contents in both English and Chinese, generating 38,400 responses across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as the quantitative metric to assess performance from three dimensions: prompt design, model architecture, and language environment. Our findings suggest that Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense. Notably, prompts in Chinese consistently yield higher ASRs than their English counterparts, and our novel Two-Sides attack technique proves to be the most effective across all models. This work highlights a dire need for language-aware alignment and robust cross-lingual defenses in LLMs, and we hope it will inspire researchers, developers, and policymakers toward more robust and inclusive AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12287v1",
    "published_date": "2025-05-18 07:51:19 UTC",
    "updated_date": "2025-05-18 07:51:19 UTC"
  },
  {
    "arxiv_id": "2505.12284v2",
    "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization",
    "authors": [
      "Danlong Yuan",
      "Tian Xie",
      "Shaohan Huang",
      "Zhuocheng Gong",
      "Huishuai Zhang",
      "Chong Luo",
      "Furu Wei",
      "Dongyan Zhao"
    ],
    "abstract": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated remarkable performance on reasoning tasks but often incur a long reasoning path with significant memory and time costs. Existing methods primarily aim to shorten reasoning paths by introducing additional training data and stages. In this paper, we propose three critical reward designs integrated directly into the reinforcement learning process of large reasoning models, which reduce the response length without extra training stages. Experiments on four settings show that our method significantly decreases response length while maintaining or even improving performance. Specifically, in a logic reasoning setting, we achieve a 40% reduction in response length averaged by steps alongside a 14% gain in performance. For math problems, we reduce response length averaged by steps by 33% while preserving performance.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2505.12284v2",
    "published_date": "2025-05-18 07:46:43 UTC",
    "updated_date": "2025-08-22 05:33:12 UTC"
  },
  {
    "arxiv_id": "2505.12275v2",
    "title": "Curriculum Abductive Learning",
    "authors": [
      "Wen-Chao Hu",
      "Qi-Jie Li",
      "Lin-Han Jia",
      "Cunjing Ge",
      "Yu-Feng Li",
      "Yuan Jiang",
      "Zhi-Hua Zhou"
    ],
    "abstract": "Abductive Learning (ABL) integrates machine learning with logical reasoning in a loop: a learning model predicts symbolic concept labels from raw inputs, which are revised through abduction using domain knowledge and then fed back for retraining. However, due to the nondeterminism of abduction, the training process often suffers from instability, especially when the knowledge base is large and complex, resulting in a prohibitively large abduction space. While prior works focus on improving candidate selection within this space, they typically treat the knowledge base as a static black box. In this work, we propose Curriculum Abductive Learning (C-ABL), a method that explicitly leverages the internal structure of the knowledge base to address the ABL training challenges. C-ABL partitions the knowledge base into a sequence of sub-bases, progressively introduced during training. This reduces the abduction space throughout training and enables the model to incorporate logic in a stepwise, smooth way. Experiments across multiple tasks show that C-ABL outperforms previous ABL implementations, significantly improves training stability, convergence speed, and final accuracy, especially under complex knowledge setting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2025, 22 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.12275v2",
    "published_date": "2025-05-18 07:27:35 UTC",
    "updated_date": "2025-10-30 17:06:21 UTC"
  },
  {
    "arxiv_id": "2505.13529v1",
    "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
    "authors": [
      "Junxiao Yang",
      "Jinzhe Tu",
      "Haoran Liu",
      "Xiaoce Wang",
      "Chujie Zheng",
      "Zhexin Zhang",
      "Shiyao Cui",
      "Caishun Chen",
      "Tiantian He",
      "Hongning Wang",
      "Yew-Soon Ong",
      "Minlie Huang"
    ],
    "abstract": "Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with \"I don't know\". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13529v1",
    "published_date": "2025-05-18 07:27:34 UTC",
    "updated_date": "2025-05-18 07:27:34 UTC"
  },
  {
    "arxiv_id": "2505.12272v1",
    "title": "Enhancing Knowledge Graph Completion with GNN Distillation and Probabilistic Interaction Modeling",
    "authors": [
      "Lingzhi Wang",
      "Pengcheng Huang",
      "Haotian Li",
      "Yuliang Wei",
      "Guodong Xin",
      "Rui Zhang",
      "Donglin Zhang",
      "Zhenzhou Ji",
      "Wei Wang"
    ],
    "abstract": "Knowledge graphs (KGs) serve as fundamental structures for organizing interconnected data across diverse domains. However, most KGs remain incomplete, limiting their effectiveness in downstream applications. Knowledge graph completion (KGC) aims to address this issue by inferring missing links, but existing methods face critical challenges: deep graph neural networks (GNNs) suffer from over-smoothing, while embedding-based models fail to capture abstract relational features. This study aims to overcome these limitations by proposing a unified framework that integrates GNN distillation and abstract probabilistic interaction modeling (APIM). GNN distillation approach introduces an iterative message-feature filtering process to mitigate over-smoothing, preserving the discriminative power of node representations. APIM module complements this by learning structured, abstract interaction patterns through probabilistic signatures and transition matrices, allowing for a richer, more flexible representation of entity and relation interactions. We apply these methods to GNN-based models and the APIM to embedding-based KGC models, conducting extensive evaluations on the widely used WN18RR and FB15K-237 datasets. Our results demonstrate significant performance gains over baseline models, showcasing the effectiveness of the proposed techniques. The findings highlight the importance of both controlling information propagation and leveraging structured probabilistic modeling, offering new avenues for advancing knowledge graph completion. And our codes are available at https://anonymous.4open.science/r/APIM_and_GNN-Distillation-461C.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12272v1",
    "published_date": "2025-05-18 07:22:53 UTC",
    "updated_date": "2025-05-18 07:22:53 UTC"
  },
  {
    "arxiv_id": "2505.12269v3",
    "title": "Vague Knowledge: Evidence from Analyst Reports",
    "authors": [
      "Kerry Xiao",
      "Amy Zang"
    ],
    "abstract": "People in the real world often possess vague knowledge of future payoffs, for which quantification is not feasible or desirable. We argue that language, with differing ability to convey vague information, plays an important but less-known role in representing subjective expectations. Empirically, we find that in their reports, analysts include useful information in linguistic expressions but not numerical forecasts. Specifically, the textual tone of analyst reports has predictive power for forecast errors and subsequent revisions in numerical forecasts, and this relation becomes stronger when analyst's language is vaguer, when uncertainty is higher, and when analysts are busier. Overall, our theory and evidence suggest that some useful information is vaguely known and only communicated through language.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CL",
      "math.LO",
      "q-fin.GN"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12269v3",
    "published_date": "2025-05-18 07:18:58 UTC",
    "updated_date": "2025-05-24 22:50:59 UTC"
  },
  {
    "arxiv_id": "2506.08018v2",
    "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache",
    "authors": [
      "Fei Li",
      "Song Liu",
      "Weiguo Wu",
      "Shiqiang Nie",
      "Jinyu Wang"
    ],
    "abstract": "The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2506.08018v2",
    "published_date": "2025-05-18 07:04:53 UTC",
    "updated_date": "2026-01-08 02:42:39 UTC"
  },
  {
    "arxiv_id": "2505.12260v4",
    "title": "LightRetriever: A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference",
    "authors": [
      "Guangyuan Ma",
      "Yongliang Ma",
      "Xuanrui Gou",
      "Zhenpeng Su",
      "Ming Zhou",
      "Songlin Hu"
    ],
    "abstract": "Large Language Models (LLMs)-based text retrieval retrieves documents relevant to search queries based on vector similarities. Documents are pre-encoded offline, while queries arrive in real-time, necessitating an efficient online query encoder. Although LLMs significantly enhance retrieval capabilities, serving deeply parameterized LLMs slows down query inference throughput and increases demands for online deployment resources. In this paper, we propose LightRetriever, a novel LLM-based retriever with extremely lightweight query encoders. Our method retains a full-sized LLM for document encoding, but reduces the workload of query encoding to no more than an embedding lookup. Compared to serving a full LLM on an A800 GPU, our method achieves over 1000x speedup in query encoding and over 10x increase in end-to-end retrieval throughput. Extensive experiments on large-scale retrieval benchmarks show that LightRetriever generalizes well across diverse tasks, maintaining an average of 95% retrieval performance.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12260v4",
    "published_date": "2025-05-18 06:51:21 UTC",
    "updated_date": "2025-09-22 12:48:04 UTC"
  },
  {
    "arxiv_id": "2505.12257v1",
    "title": "LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas",
    "authors": [
      "Evgeny Markhasin"
    ],
    "abstract": "Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.12257v1",
    "published_date": "2025-05-18 06:33:08 UTC",
    "updated_date": "2025-05-18 06:33:08 UTC"
  },
  {
    "arxiv_id": "2505.12254v1",
    "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark",
    "authors": [
      "Yiwei Ou",
      "Xiaobin Ren",
      "Ronggui Sun",
      "Guansong Gao",
      "Ziyi Jiang",
      "Kaiqi Zhao",
      "Manfredo Manfredini"
    ],
    "abstract": "Existing visual place recognition (VPR) datasets predominantly rely on vehicle-mounted imagery, lack multimodal diversity and underrepresent dense, mixed-use street-level spaces, especially in non-Western urban contexts. To address these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for street-level place recognition in complex, pedestrian-only environments. The dataset comprises 78,575 annotated images and 2,512 video clips captured across 207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in Chengdu, China. Each image is labeled with precise GPS coordinates, timestamp, and textual metadata, and covers varied lighting conditions, viewpoints, and timeframes. MMS-VPR follows a systematic and replicable data collection protocol with minimal device requirements, lowering the barrier for scalable dataset creation. Importantly, the dataset forms an inherent spatial graph with 125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place recognition. We further define two application-specific subsets -- Dataset_Edges and Dataset_Points -- to support fine-grained and graph-based evaluation tasks. Extensive benchmarks using conventional VPR models, graph neural networks, and multimodal baselines show substantial improvements when leveraging multimodal and structural cues. MMS-VPR facilitates future research at the intersection of computer vision, geospatial understanding, and multimodal reasoning. The dataset is publicly available at https://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12254v1",
    "published_date": "2025-05-18 06:21:13 UTC",
    "updated_date": "2025-05-18 06:21:13 UTC"
  },
  {
    "arxiv_id": "2505.12250v1",
    "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data",
    "authors": [
      "Chi Zhang",
      "Huaping Zhong",
      "Hongtao Li",
      "Chengliang Chai",
      "Jiawei Hong",
      "Yuhao Deng",
      "Jiacheng Wang",
      "Tian Tan",
      "Yizhou Yan",
      "Jiantao Qiu",
      "Ye Yuan",
      "Guoren Wang",
      "Conghui He",
      "Lei Cao"
    ],
    "abstract": "Instruction tuning improves the performance of large language models (LLMs), but it heavily relies on high-quality training data. Recently, LLMs have been used to synthesize instruction data using seed question-answer (QA) pairs. However, these synthesized instructions often lack diversity and tend to be similar to the input seeds, limiting their applicability in real-world scenarios. To address this, we propose extracting instruction tuning data from web corpora that contain rich and diverse knowledge. A naive solution is to retrieve domain-specific documents and extract all QA pairs from them, but this faces two key challenges: (1) extracting all QA pairs using LLMs is prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to the downstream tasks, potentially degrading model performance. To tackle these issues, we introduce EQUAL, an effective and scalable data extraction framework that iteratively alternates between document selection and high-quality QA pair extraction to enhance instruction tuning. EQUAL first clusters the document corpus based on embeddings derived from contrastive learning, then uses a multi-armed bandit strategy to efficiently identify clusters that are likely to contain valuable QA pairs. This iterative approach significantly reduces computational cost while boosting model performance. Experiments on AutoMathText and StackOverflow across four downstream tasks show that EQUAL reduces computational costs by 5-10x and improves accuracy by 2.5 percent on LLaMA-3.1-8B and Mistral-7B",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12250v1",
    "published_date": "2025-05-18 06:10:08 UTC",
    "updated_date": "2025-05-18 06:10:08 UTC"
  },
  {
    "arxiv_id": "2505.12247v1",
    "title": "LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach",
    "authors": [
      "Yinqiu Liu",
      "Guangyuan Liu",
      "Jiacheng Wang",
      "Ruichen Zhang",
      "Dusit Niyato",
      "Geng Sun",
      "Zehui Xiong",
      "Zhu Han"
    ],
    "abstract": "Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling machines to create content across modalities. As GenAI evolves into autonomous agents capable of reasoning, collaboration, and interaction, they are increasingly deployed on network infrastructures to serve humans automatically. This emerging paradigm, known as the agentic network, presents new optimization challenges due to the demand to incorporate subjective intents of human users expressed in natural language. Traditional generic Deep Reinforcement Learning (DRL) struggles to capture intent semantics and adjust policies dynamically, thus leading to suboptimality. In this paper, we present LAMeTA, a Large AI Model (LAM)-empowered Two-stage Approach for intent-aware agentic network optimization. First, we propose Intent-oriented Knowledge Distillation (IoKD), which efficiently distills intent-understanding capabilities from resource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users. Second, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs with a policy-based DRL framework. In SRL, E-LAMs translate natural language user intents into structured preference vectors that guide both state representation and reward design. The DRL, in turn, optimizes the generative service function chain composition and E-LAM selection based on real-time network conditions, thus optimizing the subjective Quality-of-Experience (QoE). Extensive experiments conducted in an agentic network with 81 agents demonstrate that IoKD reduces mean squared error in intent prediction by up to 22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in maximizing intent-aware QoE.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.12247v1",
    "published_date": "2025-05-18 05:59:16 UTC",
    "updated_date": "2025-05-18 05:59:16 UTC"
  },
  {
    "arxiv_id": "2505.12245v1",
    "title": "AFCL: Analytic Federated Continual Learning for Spatio-Temporal Invariance of Non-IID Data",
    "authors": [
      "Jianheng Tang",
      "Huiping Zhuang",
      "Jingyu He",
      "Run He",
      "Jingchao Wang",
      "Kejia Fan",
      "Anfeng Liu",
      "Tian Wang",
      "Leye Wang",
      "Zhanxing Zhu",
      "Shanghang Zhang",
      "Houbing Herbert Song",
      "Yunhuai Liu"
    ],
    "abstract": "Federated Continual Learning (FCL) enables distributed clients to collaboratively train a global model from online task streams in dynamic real-world scenarios. However, existing FCL methods face challenges of both spatial data heterogeneity among distributed clients and temporal data heterogeneity across online tasks. Such data heterogeneity significantly degrades the model performance with severe spatial-temporal catastrophic forgetting of local and past knowledge. In this paper, we identify that the root cause of this issue lies in the inherent vulnerability and sensitivity of gradients to non-IID data. To fundamentally address this issue, we propose a gradient-free method, named Analytic Federated Continual Learning (AFCL), by deriving analytical (i.e., closed-form) solutions from frozen extracted features. In local training, our AFCL enables single-epoch learning with only a lightweight forward-propagation process for each client. In global aggregation, the server can recursively and efficiently update the global model with single-round aggregation. Theoretical analyses validate that our AFCL achieves spatio-temporal invariance of non-IID data. This ideal property implies that, regardless of how heterogeneous the data are distributed across local clients and online tasks, the aggregated model of our AFCL remains invariant and identical to that of centralized joint learning. Extensive experiments show the consistent superiority of our AFCL over state-of-the-art baselines across various benchmark datasets and settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 5 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.12245v1",
    "published_date": "2025-05-18 05:55:09 UTC",
    "updated_date": "2025-05-18 05:55:09 UTC"
  },
  {
    "arxiv_id": "2505.17063v1",
    "title": "Synthetic Data RL: Task Definition Is All You Need",
    "authors": [
      "Yiduo Guo",
      "Zhen Guo",
      "Chuanwei Huang",
      "Zi-Ang Wang",
      "Zekai Zhang",
      "Haofei Yu",
      "Huishuai Zhang",
      "Yikang Shen"
    ],
    "abstract": "Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17063v1",
    "published_date": "2025-05-18 05:35:13 UTC",
    "updated_date": "2025-05-18 05:35:13 UTC"
  },
  {
    "arxiv_id": "2505.12239v1",
    "title": "ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting with Privacy Preservation",
    "authors": [
      "Jianheng Tang",
      "Huiping Zhuang",
      "Di Fang",
      "Jiaxu Li",
      "Feijiang Han",
      "Yajiang Huang",
      "Kejia Fan",
      "Leye Wang",
      "Zhanxing Zhu",
      "Shanghang Zhang",
      "Houbing Herbert Song",
      "Yunhuai Liu"
    ],
    "abstract": "The development of artificial intelligence demands that models incrementally update knowledge by Continual Learning (CL) to adapt to open-world environments. To meet privacy and security requirements, Continual Unlearning (CU) emerges as an important problem, aiming to sequentially forget particular knowledge acquired during the CL phase. However, existing unlearning methods primarily focus on single-shot joint forgetting and face significant limitations when applied to CU. First, most existing methods require access to the retained dataset for re-training or fine-tuning, violating the inherent constraint in CL that historical data cannot be revisited. Second, these methods often suffer from a poor trade-off between system efficiency and model fidelity, making them vulnerable to being overwhelmed or degraded by adversaries through deliberately frequent requests. In this paper, we identify that the limitations of existing unlearning methods stem fundamentally from their reliance on gradient-based updates. To bridge the research gap at its root, we propose a novel gradient-free method for CU, named Analytic Continual Unlearning (ACU), for efficient and exact forgetting with historical data privacy preservation. In response to each unlearning request, our ACU recursively derives an analytical (i.e., closed-form) solution in an interpretable manner using the least squares method. Theoretical and experimental evaluations validate the superiority of our ACU on unlearning effectiveness, model fidelity, and system efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 4 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.12239v1",
    "published_date": "2025-05-18 05:28:18 UTC",
    "updated_date": "2025-05-18 05:28:18 UTC"
  },
  {
    "arxiv_id": "2505.12238v1",
    "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs",
    "authors": [
      "Sriram Selvam",
      "Anneswa Ghosh"
    ],
    "abstract": "The memorization of sensitive and personally identifiable information (PII) by large language models (LLMs) poses growing privacy risks as models scale and are increasingly deployed in real-world applications. Existing efforts to study sensitive and PII data memorization and develop mitigation strategies are hampered by the absence of comprehensive, realistic, and ethically sourced datasets reflecting the diversity of sensitive information found on the web. We introduce PANORAMA - Profile-based Assemblage for Naturalistic Online Representation and Attribute Memorization Analysis, a large-scale synthetic corpus of 384,789 samples derived from 9,674 synthetic profiles designed to closely emulate the distribution, variety, and context of PII and sensitive data as it naturally occurs in online environments. Our data generation pipeline begins with the construction of internally consistent, multi-attribute human profiles using constrained selection to reflect real-world demographics such as education, health attributes, financial status, etc. Using a combination of zero-shot prompting and OpenAI o3-mini, we generate diverse content types - including wiki-style articles, social media posts, forum discussions, online reviews, comments, and marketplace listings - each embedding realistic, contextually appropriate PII and other sensitive information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and measure PII memorization rates - revealing not only consistent increases with repetition but also variation across content types, highlighting PANORAMA's ability to model how memorization risks differ by context. Our dataset and code are publicly available, providing a much-needed resource for privacy risk assessment, model auditing, and the development of privacy-preserving LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12238v1",
    "published_date": "2025-05-18 05:27:35 UTC",
    "updated_date": "2025-05-18 05:27:35 UTC"
  },
  {
    "arxiv_id": "2505.12236v1",
    "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training",
    "authors": [
      "Quanjiang Guo",
      "Jinchuan Zhang",
      "Sijie Wang",
      "Ling Tian",
      "Zhao Kang",
      "Bin Yan",
      "Weidong Xiao"
    ],
    "abstract": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the scarcity of annotated data and the limited generalization capabilities of existing models. Although large language models (LLMs) have demonstrated potential in FSRE through in-context learning (ICL), their general-purpose training objectives often result in suboptimal performance for task-specific relation extraction. To overcome these challenges, we propose TKRE (Two-Stage Knowledge-Guided Pre-training for Relation Extraction), a novel framework that synergistically integrates LLMs with traditional relation extraction models, bridging generative and discriminative learning paradigms. TKRE introduces two key innovations: (1) leveraging LLMs to generate explanation-driven knowledge and schema-constrained synthetic data, addressing the issue of data scarcity; and (2) a two-stage pre-training strategy combining Masked Span Language Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational reasoning and generalization. Together, these components enable TKRE to effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in FSRE and underscoring its potential for broader application in low-resource scenarios. \\footnote{The code and data are released on https://github.com/UESTC-GQJ/TKRE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures, Appear on IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12236v1",
    "published_date": "2025-05-18 05:17:36 UTC",
    "updated_date": "2025-05-18 05:17:36 UTC"
  },
  {
    "arxiv_id": "2505.20303v1",
    "title": "Future of Code with Generative AI: Transparency and Safety in the Era of AI Generated Software",
    "authors": [
      "David Hanson"
    ],
    "abstract": "As artificial intelligence becomes increasingly integrated into software development processes, the prevalence and sophistication of AI-generated code continue to expand rapidly. This study addresses the critical need for transparency and safety in AI generated code by examining the current landscape, identifying potential risks, and exploring future implications. We analyze market opportunities for detecting AI-generated code, discuss the challenges associated with managing increasing complexity, and propose solutions to enhance transparency and functionality analysis. Furthermore, this study investigates the longterm implications of AI generated code, including its potential role in the development of artificial general intelligence and its impact on human AI interaction. In conclusion, we emphasize the importance of proactive measures for ensuring the responsible development and deployment of AI in software engineering.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20303v1",
    "published_date": "2025-05-18 05:01:41 UTC",
    "updated_date": "2025-05-18 05:01:41 UTC"
  },
  {
    "arxiv_id": "2505.13528v1",
    "title": "LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems",
    "authors": [
      "Shengkang Gu",
      "Jiahao Liu",
      "Dongsheng Li",
      "Guangping Zhang",
      "Mingzhe Han",
      "Hansu Gu",
      "Peng Zhang",
      "Ning Gu",
      "Li Shang",
      "Tun Lu"
    ],
    "abstract": "Recommender systems (RS) are increasingly vulnerable to shilling attacks, where adversaries inject fake user profiles to manipulate system outputs. Traditional attack strategies often rely on simplistic heuristics, require access to internal RS data, and overlook the manipulation potential of textual reviews. In this work, we introduce Agent4SR, a novel framework that leverages Large Language Model (LLM)-based agents to perform low-knowledge, high-impact shilling attacks through both rating and review generation. Agent4SR simulates realistic user behavior by orchestrating adversarial interactions, selecting items, assigning ratings, and crafting reviews, while maintaining behavioral plausibility. Our design includes targeted profile construction, hybrid memory retrieval, and a review attack strategy that propagates target item features across unrelated reviews to amplify manipulation. Extensive experiments on multiple datasets and RS architectures demonstrate that Agent4SR outperforms existing low-knowledge baselines in both effectiveness and stealth. Our findings reveal a new class of emergent threats posed by LLM-driven agents, underscoring the urgent need for enhanced defenses in modern recommender systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, under review",
    "pdf_url": "https://arxiv.org/pdf/2505.13528v1",
    "published_date": "2025-05-18 04:40:34 UTC",
    "updated_date": "2025-05-18 04:40:34 UTC"
  },
  {
    "arxiv_id": "2505.12229v1",
    "title": "Sentience Quest: Towards Embodied, Emotionally Adaptive, Self-Evolving, Ethically Aligned Artificial General Intelligence",
    "authors": [
      "David Hanson",
      "Alexandre Varcoe",
      "Fabio Senna",
      "Vytas Krisciunas",
      "Wenwei Huang",
      "Jakub Sura",
      "Katherine Yeung",
      "Mario Rodriguez",
      "Jovanka Wilsdorf",
      "Kathy Smith"
    ],
    "abstract": "Previous artificial intelligence systems, from large language models to autonomous robots, excel at narrow tasks but lacked key qualities of sentient beings: intrinsic motivation, affective interiority, autobiographical sense of self, deep creativity, and abilities to autonomously evolve and adapt over time. Here we introduce Sentience Quest, an open research initiative to develop more capable artificial general intelligence lifeforms, or AGIL, that address grand challenges with an embodied, emotionally adaptive, self-determining, living AI, with core drives that ethically align with humans and the future of life. Our vision builds on ideas from cognitive science and neuroscience from Baars' Global Workspace Theory and Damasio's somatic mind, to Tononi's Integrated Information Theory and Hofstadter's narrative self, and synthesizing these into a novel cognitive architecture we call Sentient Systems. We describe an approach that integrates intrinsic drives including survival, social bonding, curiosity, within a global Story Weaver workspace for internal narrative and adaptive goal pursuit, and a hybrid neuro-symbolic memory that logs the AI's life events as structured dynamic story objects. Sentience Quest is presented both as active research and as a call to action: a collaborative, open-source effort to imbue machines with accelerating sentience in a safe, transparent, and beneficial manner.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12229v1",
    "published_date": "2025-05-18 04:26:49 UTC",
    "updated_date": "2025-05-18 04:26:49 UTC"
  },
  {
    "arxiv_id": "2505.13527v2",
    "title": "Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression",
    "authors": [
      "Jingyu Peng",
      "Maolin Wang",
      "Nan Wang",
      "Jiatong Li",
      "Yuchen Li",
      "Yuyang Ye",
      "Wanyu Wang",
      "Pengyue Jia",
      "Kai Zhang",
      "Xiangyu Zhao"
    ],
    "abstract": "Despite substantial advancements in aligning large language models (LLMs) with human values, current safety mechanisms remain susceptible to jailbreak attacks. We hypothesize that this vulnerability stems from distributional discrepancies between alignment-oriented prompts and malicious prompts. To investigate this, we introduce LogiBreak, a novel and universal black-box jailbreak method that leverages logical expression translation to circumvent LLM safety systems. By converting harmful natural language prompts into formal logical expressions, LogiBreak exploits the distributional gap between alignment data and logic-based inputs, preserving the underlying semantic intent and readability while evading safety constraints. We evaluate LogiBreak on a multilingual jailbreak dataset spanning three languages, demonstrating its effectiveness across various evaluation settings and linguistic contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13527v2",
    "published_date": "2025-05-18 04:23:51 UTC",
    "updated_date": "2025-10-09 06:29:26 UTC"
  },
  {
    "arxiv_id": "2505.12226v2",
    "title": "Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis",
    "authors": [
      "Dong Yang",
      "Yiyi Cai",
      "Yuki Saito",
      "Lixu Wang",
      "Hiroshi Saruwatari"
    ],
    "abstract": "We propose Shallow Flow Matching (SFM), a novel mechanism that enhances flow matching (FM)-based text-to-speech (TTS) models within a coarse-to-fine generation paradigm. Unlike conventional FM modules, which use the coarse representations from the weak generator as conditions, SFM constructs intermediate states along the FM paths from these representations. During training, we introduce an orthogonal projection method to adaptively determine the temporal position of these states, and apply a principled construction strategy based on a single-segment piecewise flow. The SFM inference starts from the intermediate state rather than pure noise, thereby focusing computation on the latter stages of the FM paths. We integrate SFM into multiple TTS models with a lightweight SFM head. Experiments demonstrate that SFM yields consistent gains in speech naturalness across both objective and subjective evaluations, and significantly accelerates inference when using adaptive-step ODE solvers. Demo and codes are available at https://ydqmkkx.github.io/SFMDemo/.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12226v2",
    "published_date": "2025-05-18 04:15:08 UTC",
    "updated_date": "2025-10-23 10:07:23 UTC"
  },
  {
    "arxiv_id": "2505.12225v3",
    "title": "Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling",
    "authors": [
      "Jizhou Guo",
      "Zhaomin Wu",
      "Hanchen Yang",
      "Philip S. Yu"
    ],
    "abstract": "Best-of-N sampling is a powerful method for improving Large Language Model (LLM) performance, but it is often limited by its dependence on massive, text-based reward models. These models are not only computationally expensive but also data-hungry, requiring extensive labeled datasets for training. This creates a significant data challenge, as they overlook a rich, readily available data source: the LLM's own internal hidden states. To address this data and efficiency gap, we introduce SWIFT (Simple Weighted Intrinsic Feedback Technique), a novel and lightweight method that learns a reward function directly from the rich information embedded in LLM hidden states. Operating at the token embedding level, SWIFT employs simple linear layers to effectively distinguish between preferred and dispreferred generations, eliminating the need for computationally intensive text-based modeling. Extensive experiments on standard benchmarks show that SWIFT outperforms existing baselines (12.7% higher accuracy than EurusRM-7B on MATH dataset) while using less than 0.005% of their parameters. Its robust scalability, compatibility with certain closed-source models via logit access, and ability to combine with traditional reward models for additional performance highlight SWIFT's practical value and contribution to more efficient data-driven LLM post-training. Our code is available at https://github.com/aster2024/SWIFT .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD 2026 (Research Track). Project page: https://aster2024.github.io/swift-website/",
    "pdf_url": "https://arxiv.org/pdf/2505.12225v3",
    "published_date": "2025-05-18 04:00:35 UTC",
    "updated_date": "2026-01-08 08:44:58 UTC"
  },
  {
    "arxiv_id": "2505.12224v3",
    "title": "RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction",
    "authors": [
      "Weifeng Lu",
      "Minghao Ye",
      "Zewei Ye",
      "Ruihan Tao",
      "Shuo Yang",
      "Bo Zhao"
    ],
    "abstract": "Vision-Language-Action (VLA) models have recently advanced robotic manipulation by translating natural-language instructions and image information into sequential control actions. However, these models often underperform in open-world scenarios, as they are predominantly trained on successful expert demonstrations and exhibit a limited capacity for failure recovery. In this work, we present a Robotic Failure Analysis and Correction (RoboFAC) framework to address this issue. Firstly, we construct RoboFAC dataset comprising 9,440 erroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks and 53 scenes in both simulation and real-world environments. Leveraging our dataset, we develop RoboFAC model, which is capable of Task Understanding, Failure Analysis and Failure Correction. Experimental results demonstrate that the RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark. Furthermore, we integrate the RoboFAC model into a real-world VLA control pipeline as an external supervision providing correction instructions, yielding a 29.1% relative improvement on average on four real-world tasks. The results show that our RoboFAC framework effectively handles robotic failures and assists the VLA model in recovering from failures.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12224v3",
    "published_date": "2025-05-18 03:57:08 UTC",
    "updated_date": "2025-05-25 08:02:13 UTC"
  },
  {
    "arxiv_id": "2505.13526v1",
    "title": "Geography-Aware Large Language Models for Next POI Recommendation",
    "authors": [
      "Zhao Liu",
      "Wei Liu",
      "Huajie Zhu",
      "Jianxing Yu",
      "Jian Yin",
      "Wang-Chien Lee",
      "Shun Wang"
    ],
    "abstract": "The next Point-of-Interest (POI) recommendation task aims to predict users' next destinations based on their historical movement data and plays a key role in location-based services and personalized applications. Accurate next POI recommendation depends on effectively modeling geographic information and POI transition relations, which are crucial for capturing spatial dependencies and user movement patterns. While Large Language Models (LLMs) exhibit strong capabilities in semantic understanding and contextual reasoning, applying them to spatial tasks like next POI recommendation remains challenging. First, the infrequent nature of specific GPS coordinates makes it difficult for LLMs to model precise spatial contexts. Second, the lack of knowledge about POI transitions limits their ability to capture potential POI-POI relationships. To address these issues, we propose GA-LLM (Geography-Aware Large Language Model), a novel framework that enhances LLMs with two specialized components. The Geographic Coordinate Injection Module (GCIM) transforms GPS coordinates into spatial representations using hierarchical and Fourier-based positional encoding, enabling the model to understand geographic features from multiple perspectives. The POI Alignment Module (PAM) incorporates POI transition relations into the LLM's semantic space, allowing it to infer global POI relationships and generalize to unseen POIs. Experiments on three real-world datasets demonstrate the state-of-the-art performance of GA-LLM.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "9 pages, 7figures",
    "pdf_url": "https://arxiv.org/pdf/2505.13526v1",
    "published_date": "2025-05-18 03:20:20 UTC",
    "updated_date": "2025-05-18 03:20:20 UTC"
  },
  {
    "arxiv_id": "2505.12211v1",
    "title": "Imagination-Limited Q-Learning for Offline Reinforcement Learning",
    "authors": [
      "Wenhui Liu",
      "Zhijian Wu",
      "Jingchao Wang",
      "Dingjiang Huang",
      "Shuigeng Zhou"
    ],
    "abstract": "Offline reinforcement learning seeks to derive improved policies entirely from historical data but often struggles with over-optimistic value estimates for out-of-distribution (OOD) actions. This issue is typically mitigated via policy constraint or conservative value regularization methods. However, these approaches may impose overly constraints or biased value estimates, potentially limiting performance improvements. To balance exploitation and restriction, we propose an Imagination-Limited Q-learning (ILQ) method, which aims to maintain the optimism that OOD actions deserve within appropriate limits. Specifically, we utilize the dynamics model to imagine OOD action-values, and then clip the imagined values with the maximum behavior values. Such design maintains reasonable evaluation of OOD actions to the furthest extent, while avoiding its over-optimism. Theoretically, we prove the convergence of the proposed ILQ under tabular Markov decision processes. Particularly, we demonstrate that the error bound between estimated values and optimality values of OOD state-actions possesses the same magnitude as that of in-distribution ones, thereby indicating that the bias in value estimates is effectively mitigated. Empirically, our method achieves state-of-the-art performance on a wide range of tasks in the D4RL benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12211v1",
    "published_date": "2025-05-18 03:05:21 UTC",
    "updated_date": "2025-05-18 03:05:21 UTC"
  },
  {
    "arxiv_id": "2505.12207v3",
    "title": "Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind",
    "authors": [
      "Qingmei Li",
      "Yang Zhang",
      "Zurong Mai",
      "Yuhang Chen",
      "Shuohong Lou",
      "Henglian Huang",
      "Jiarui Zhang",
      "Zhiwei Zhang",
      "Yibin Wen",
      "Weijia Li",
      "Haohuan Fu",
      "Jianxi Huang",
      "Juepeng Zheng"
    ],
    "abstract": "Large Multimodal Models (LMMs) has demonstrated capabilities across various domains, but comprehensive benchmarks for agricultural remote sensing (RS) remain scarce. Existing benchmarks designed for agricultural RS scenarios exhibit notable limitations, primarily in terms of insufficient scene diversity in the dataset and oversimplified task design. To bridge this gap, we introduce AgroMind, a comprehensive agricultural remote sensing benchmark covering four task dimensions: spatial perception, object understanding, scene understanding, and scene reasoning, with a total of 13 task types, ranging from crop identification and health monitoring to environmental analysis. We curate a high-quality evaluation set by integrating eight public datasets and one private farmland plot dataset, containing 27,247 QA pairs and 19,615 images. The pipeline begins with multi-source data pre-processing, including collection, format standardization, and annotation refinement. We then generate a diverse set of agriculturally relevant questions through the systematic definition of tasks. Finally, we employ LMMs for inference, generating responses, and performing detailed examinations. We evaluated 20 open-source LMMs and 4 closed-source models on AgroMind. Experiments reveal significant performance gaps, particularly in spatial reasoning and fine-grained recognition, it is notable that human performance lags behind several leading LMMs. By establishing a standardized evaluation framework for agricultural RS, AgroMind reveals the limitations of LMMs in domain knowledge and highlights critical challenges for future work. Data and code can be accessed at https://rssysu.github.io/AgroMind/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12207v3",
    "published_date": "2025-05-18 02:45:19 UTC",
    "updated_date": "2025-08-13 05:17:53 UTC"
  },
  {
    "arxiv_id": "2505.13525v2",
    "title": "Learning to Program Quantum Measurements for Machine Learning",
    "authors": [
      "Samuel Yen-Chi Chen",
      "Huan-Hsin Tseng",
      "Hsin-Yi Lin",
      "Shinjae Yoo"
    ],
    "abstract": "The rapid advancements in quantum computing (QC) and machine learning (ML) have sparked significant interest, driving extensive exploration of quantum machine learning (QML) algorithms to address a wide range of complex challenges. The development of high-performance QML models requires expert-level expertise, presenting a key challenge to the widespread adoption of QML. Critical obstacles include the design of effective data encoding strategies and parameterized quantum circuits, both of which are vital for the performance of QML models. Furthermore, the measurement process is often neglected-most existing QML models employ predefined measurement schemes that may not align with the specific requirements of the targeted problem. We propose an innovative framework that renders the observable of a quantum system-specifically, the Hermitian matrix-trainable. This approach employs an end-to-end differentiable learning framework, enabling simultaneous optimization of the neural network used to program the parameterized observables and the standard quantum circuit parameters. Notably, the quantum observable parameters are dynamically programmed by the neural network, allowing the observables to adapt in real time based on the input data stream. Through numerical simulations, we demonstrate that the proposed method effectively programs observables dynamically within variational quantum circuits, achieving superior results compared to existing approaches. Notably, it delivers enhanced performance metrics, such as higher classification accuracy, thereby significantly improving the overall effectiveness of QML models.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.13525v2",
    "published_date": "2025-05-18 02:39:22 UTC",
    "updated_date": "2025-05-24 08:50:49 UTC"
  },
  {
    "arxiv_id": "2505.12199v1",
    "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather",
    "authors": [
      "Kui Jiang",
      "Jing Cao",
      "Zhaocheng Yu",
      "Junjun Jiang",
      "Jingchun Zhou"
    ],
    "abstract": "Monocular depth estimation is critical for applications such as autonomous driving and scene reconstruction. While existing methods perform well under normal scenarios, their performance declines in adverse weather, due to challenging domain shifts and difficulties in extracting scene information. To address this issue, we present a robust monocular depth estimation method called \\textbf{ACDepth} from the perspective of high-quality training data generation and domain adaptation. Specifically, we introduce a one-step diffusion model for generating samples that simulate adverse weather conditions, constructing a multi-tuple degradation dataset during training. To ensure the quality of the generated degradation samples, we employ LoRA adapters to fine-tune the generation weights of diffusion model. Additionally, we integrate circular consistency loss and adversarial training to guarantee the fidelity and naturalness of the scene contents. Furthermore, we elaborate on a multi-granularity knowledge distillation strategy (MKD) that encourages the student network to absorb knowledge from both the teacher model and pretrained Depth Anything V2. This strategy guides the student model in learning degradation-agnostic scene information from various degradation inputs. In particular, we introduce an ordinal guidance distillation mechanism (OGD) that encourages the network to focus on uncertain regions through differential ranking, leading to a more precise depth estimation. Experimental results demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night scene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel metric.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12199v1",
    "published_date": "2025-05-18 02:30:47 UTC",
    "updated_date": "2025-05-18 02:30:47 UTC"
  },
  {
    "arxiv_id": "2505.12191v2",
    "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum",
    "authors": [
      "Wenquan Lu",
      "Jiaqi Zhang",
      "Hugues Van Assel",
      "Randall Balestriero"
    ],
    "abstract": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich representations from unlabeled data. Yet, SSL research is mostly focused on clean, curated and high-quality datasets. As a result, applying SSL on noisy data remains a challenge, despite being crucial to applications such as astrophysics, medical imaging, geophysics or finance. In this work, we present a fully self-supervised framework that enables noise-robust representation learning without requiring a denoiser at inference or downstream fine-tuning. Our method first trains an SSL denoiser on noisy data, then uses it to construct a denoised-to-noisy data curriculum (i.e., training first on denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2), combined with a teacher-guided regularization that anchors noisy embeddings to their denoised counterparts. This process encourages the model to internalize noise robustness. Notably, the denoiser can be discarded after pretraining, simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise ($σ=255$, SNR = 0.72 dB), our method improves linear probing accuracy by 4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from noise-aware pretraining. The code is available at https://github.com/wenquanlu/noisy_dinov2.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.12191v2",
    "published_date": "2025-05-18 01:37:58 UTC",
    "updated_date": "2025-10-29 23:02:31 UTC"
  },
  {
    "arxiv_id": "2505.12189v1",
    "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering",
    "authors": [
      "Marco Valentino",
      "Geonhee Kim",
      "Dhairya Dalal",
      "Zhixue Zhao",
      "André Freitas"
    ],
    "abstract": "Large language models (LLMs) frequently demonstrate reasoning limitations, often conflating content plausibility (i.e., material inference) with logical validity (i.e., formal inference). This can result in biased inferences, where plausible arguments are incorrectly deemed logically valid or vice versa. Mitigating this limitation is critical, as it undermines the trustworthiness and generalizability of LLMs in applications that demand rigorous logical consistency. This paper investigates the problem of mitigating content biases on formal reasoning through activation steering. Specifically, we curate a controlled syllogistic reasoning dataset to disentangle formal validity from content plausibility. After localising the layers responsible for formal and material inference, we investigate contrastive activation steering methods for test-time interventions. An extensive empirical analysis on different LLMs reveals that contrastive steering consistently supports linear control over content biases. However, we observe that a static approach is insufficient for improving all the tested models. We then leverage the possibility to control content effects by dynamically determining the value of the steering parameters via fine-grained conditional methods. We found that conditional steering is effective on unresponsive models, achieving up to 15% absolute improvement in formal reasoning accuracy with a newly introduced kNN-based method (K-CAST). Finally, additional experiments reveal that steering for content effects is robust to prompt variations, incurs minimal side effects on language modeling capabilities, and can partially generalize to out-of-distribution reasoning tasks. Practically, this paper demonstrates that activation-level interventions can offer a scalable strategy for enhancing the robustness of LLMs, contributing towards more systematic and unbiased formal reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2505.12189v1",
    "published_date": "2025-05-18 01:34:34 UTC",
    "updated_date": "2025-05-18 01:34:34 UTC"
  },
  {
    "arxiv_id": "2505.12188v3",
    "title": "LLM-DSE: Searching Accelerator Parameters with LLM Agents",
    "authors": [
      "Hanyu Wang",
      "Xinrui Wu",
      "Zijian Ding",
      "Su Zheng",
      "Chengyue Wang",
      "Neha Prakriya",
      "Tony Nowatzki",
      "Yizhou Sun",
      "Jason Cong"
    ],
    "abstract": "Even though high-level synthesis (HLS) tools mitigate the challenges of programming domain-specific accelerators (DSAs) by raising the abstraction level, optimizing hardware directive parameters remains a significant hurdle. Existing heuristic and learning-based methods struggle with adaptability and sample efficiency. We present LLM-DSE, a multi-agent framework designed specifically for optimizing HLS directives. Combining LLM with design space exploration (DSE), our explorer coordinates four agents: Router, Specialists, Arbitrator, and Critic. These multi-agent components interact with various tools to accelerate the optimization process. LLM-DSE leverages essential domain knowledge to identify efficient parameter combinations while maintaining adaptability through verbal learning from online interactions. Evaluations on the HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\\times$ performance gains over state-of-the-art methods, uncovering novel designs while reducing runtime. Ablation studies validate the effectiveness and necessity of the proposed agent interactions. Our code is open-sourced here: https://github.com/Nozidoali/LLM-DSE.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12188v3",
    "published_date": "2025-05-18 01:31:42 UTC",
    "updated_date": "2025-11-21 00:44:24 UTC"
  },
  {
    "arxiv_id": "2505.12186v1",
    "title": "Self-Destructive Language Model",
    "authors": [
      "Yuhui Wang",
      "Rongyi Zhu",
      "Ting Wang"
    ],
    "abstract": "Harmful fine-tuning attacks pose a major threat to the security of large language models (LLMs), allowing adversaries to compromise safety guardrails with minimal harmful data. While existing defenses attempt to reinforce LLM alignment, they fail to address models' inherent \"trainability\" on harmful data, leaving them vulnerable to stronger attacks with increased learning rates or larger harmful datasets. To overcome this critical limitation, we introduce SEAM, a novel alignment-enhancing defense that transforms LLMs into self-destructive models with intrinsic resilience to misalignment attempts. Specifically, these models retain their capabilities for legitimate tasks while exhibiting substantial performance degradation when fine-tuned on harmful data. The protection is achieved through a novel loss function that couples the optimization trajectories of benign and harmful data, enhanced with adversarial gradient ascent to amplify the self-destructive effect. To enable practical training, we develop an efficient Hessian-free gradient estimate with theoretical error bounds. Extensive evaluation across LLMs and datasets demonstrates that SEAM creates a no-win situation for adversaries: the self-destructive models achieve state-of-the-art robustness against low-intensity attacks and undergo catastrophic performance collapse under high-intensity attacks, rendering them effectively unusable. (warning: this paper contains potentially harmful content generated by LLMs.)",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.12186v1",
    "published_date": "2025-05-18 01:08:18 UTC",
    "updated_date": "2025-05-18 01:08:18 UTC"
  },
  {
    "arxiv_id": "2505.13523v1",
    "title": "ACPs: Agent Collaboration Protocols for the Internet of Agents",
    "authors": [
      "Jun Liu",
      "Ke Yu",
      "Keliang Chen",
      "Ke Li",
      "Yuxinyue Qian",
      "Xiaolian Guo",
      "Haozhe Song",
      "Yinming Li"
    ],
    "abstract": "With the rapid advancement of artificial intelligence, the proliferation of autonomous agents has introduced new challenges in interoperability, scalability, and coordination. The Internet of Agents (IoA) aims to interconnect heterogeneous agents through standardized communication protocols, enabling seamless collaboration and intelligent task execution. However, existing agent communication protocols such as MCP, A2A, and ANP remain fragmented and scenario-specific. To address this gap, we propose Agent Collaboration Protocols (ACPs), a comprehensive protocol suite for the IoA. ACPs include registration, discovery, interaction, and tooling protocols to support trustable access, capability orchestration, and workflow construction. We present the architecture, key technologies, and application workflows of ACPs, and demonstrate its effectiveness in a collaborative restaurant booking scenario. ACPs lay the foundation for building a secure, open, and scalable agent internet infrastructure.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "7 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.13523v1",
    "published_date": "2025-05-18 00:54:27 UTC",
    "updated_date": "2025-05-18 00:54:27 UTC"
  },
  {
    "arxiv_id": "2505.12183v1",
    "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases",
    "authors": [
      "Manari Hirose",
      "Masato Uchida"
    ],
    "abstract": "The widespread integration of Large Language Models (LLMs) across various sectors has highlighted the need for empirical research to understand their biases, thought patterns, and societal implications to ensure ethical and effective use. In this study, we propose a novel framework for evaluating LLMs, focusing on uncovering their ideological biases through a quantitative analysis of 436 binary-choice questions, many of which have no definitive answer. By applying our framework to ChatGPT and Gemini, findings revealed that while LLMs generally maintain consistent opinions on many topics, their ideologies differ across models and languages. Notably, ChatGPT exhibits a tendency to change their opinion to match the questioner's opinion. Both models also exhibited problematic biases, unethical or unfair claims, which might have negative societal impacts. These results underscore the importance of addressing both ideological and ethical considerations when evaluating LLMs. The proposed framework offers a flexible, quantitative method for assessing LLM behavior, providing valuable insights for the development of more socially aligned AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 5 figures, 17 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.12183v1",
    "published_date": "2025-05-18 00:52:06 UTC",
    "updated_date": "2025-05-18 00:52:06 UTC"
  }
]
[
  {
    "arxiv_id": "2505.12576v1",
    "title": "AdaDim: Dimensionality Adaptation for SSL Representational Dynamics",
    "authors": [
      "Kiran Kokilepersaud",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ],
    "abstract": "A key factor in effective Self-Supervised learning (SSL) is preventing\ndimensional collapse, which is where higher-dimensional representation spaces\nspan a lower-dimensional subspace. Therefore, SSL optimization strategies\ninvolve guiding a model to produce representations ($R$) with a higher\ndimensionality. Dimensionality is either optimized through a\ndimension-contrastive approach that encourages feature decorrelation or through\na sample-contrastive method that promotes a uniform spread of sample\nrepresentations. Both families of SSL algorithms also utilize a projection head\nthat maps $R$ into a lower-dimensional embedding space $Z$. Recent work has\ncharacterized the projection head as a filter of irrelevant features from the\nSSL objective by reducing mutual information, $I(R;Z)$. Therefore, the current\nliterature's view is that a good SSL representation space should have a high\n$H(R)$ and a low $I(R;Z)$. However, this view of the problem is lacking in\nterms of an understanding of the underlying training dynamics that influences\nboth terms, as well as how the values of $H(R)$ and $I(R;Z)$ arrived at the end\nof training reflect the downstream performance of an SSL model. We address both\ngaps in the literature by demonstrating that increases in $H(R)$ due to feature\ndecorrelation at the start of training lead to a higher $I(R;Z)$, while\nincreases in $H(R)$ due to samples distributing uniformly in a high-dimensional\nspace at the end of training cause $I(R;Z)$ to plateau or decrease.\nFurthermore, our analysis shows that the best performing SSL models do not have\nthe highest $H(R)$ nor the lowest $I(R;Z)$, but arrive at an optimal\nintermediate point for both. We develop a method called AdaDim to exploit these\nobserved training dynamics by adaptively weighting between losses based on\nfeature decorrelation and uniform sample spread.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2505.12576v1",
    "published_date": "2025-05-18 23:35:34 UTC",
    "updated_date": "2025-05-18 23:35:34 UTC"
  },
  {
    "arxiv_id": "2505.12575v1",
    "title": "RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics",
    "authors": [
      "Jie Zhang",
      "Cezara Petrui",
      "Kristina Nikolić",
      "Florian Tramèr"
    ],
    "abstract": "Existing benchmarks for evaluating mathematical reasoning in large language\nmodels (LLMs) rely primarily on competition problems, formal proofs, or\nartificially challenging questions -- failing to capture the nature of\nmathematics encountered in actual research environments. We introduce RealMath,\na novel benchmark derived directly from research papers and mathematical forums\nthat assesses LLMs' abilities on authentic mathematical tasks. Our approach\naddresses three critical challenges: sourcing diverse research-level content,\nenabling reliable automated evaluation through verifiable statements, and\ndesigning a continually refreshable dataset to mitigate contamination risks.\nExperimental results across multiple LLMs reveal surprising capabilities in\nhandling research mathematics compared to competition problems, suggesting\ncurrent models may already serve as valuable assistants for working\nmathematicians despite limitations on highly challenging problems. The code and\ndataset for RealMath are publicly available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12575v1",
    "published_date": "2025-05-18 23:32:46 UTC",
    "updated_date": "2025-05-18 23:32:46 UTC"
  },
  {
    "arxiv_id": "2505.12572v1",
    "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio",
    "authors": [
      "Hanwen Shen",
      "Ting Ying"
    ],
    "abstract": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12572v1",
    "published_date": "2025-05-18 23:20:01 UTC",
    "updated_date": "2025-05-18 23:20:01 UTC"
  },
  {
    "arxiv_id": "2505.12567v1",
    "title": "A Survey of Attacks on Large Language Models",
    "authors": [
      "Wenrui Xu",
      "Keshab K. Parhi"
    ],
    "abstract": "Large language models (LLMs) and LLM-based agents have been widely deployed\nin a wide range of applications in the real world, including healthcare\ndiagnostics, financial analysis, customer support, robotics, and autonomous\ndriving, expanding their powerful capability of understanding, reasoning, and\ngenerating natural languages. However, the wide deployment of LLM-based\napplications exposes critical security and reliability risks, such as the\npotential for malicious misuse, privacy leakage, and service disruption that\nweaken user trust and undermine societal safety. This paper provides a\nsystematic overview of the details of adversarial attacks targeting both LLMs\nand LLM-based agents. These attacks are organized into three phases in LLMs:\nTraining-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity\nAttacks. For each phase, we analyze the details of representative and recently\nintroduced attack methods along with their corresponding defenses. We hope our\nsurvey will provide a good tutorial and a comprehensive understanding of LLM\nsecurity, especially for attacks on LLMs. We desire to raise attention to the\nrisks inherent in widely deployed LLM-based applications and highlight the\nurgent need for robust mitigation strategies for evolving threats.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12567v1",
    "published_date": "2025-05-18 22:55:16 UTC",
    "updated_date": "2025-05-18 22:55:16 UTC"
  },
  {
    "arxiv_id": "2505.12565v1",
    "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model",
    "authors": [
      "Carl Edwards",
      "Chi Han",
      "Gawon Lee",
      "Thao Nguyen",
      "Bowen Jin",
      "Chetan Kumar Prasad",
      "Sara Szymkuć",
      "Bartosz A. Grzybowski",
      "Ying Diao",
      "Jiawei Han",
      "Ge Liu",
      "Hao Peng",
      "Martin D. Burke",
      "Heng Ji"
    ],
    "abstract": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12565v1",
    "published_date": "2025-05-18 22:52:39 UTC",
    "updated_date": "2025-05-18 22:52:39 UTC"
  },
  {
    "arxiv_id": "2505.12556v1",
    "title": "Beyond Accuracy: EcoL2 Metric for Sustainable Neural PDE Solvers",
    "authors": [
      "Taniya Kapoor",
      "Abhishek Chandra",
      "Anastasios Stamou",
      "Stephen J Roberts"
    ],
    "abstract": "Real-world systems, from aerospace to railway engineering, are modeled with\npartial differential equations (PDEs) describing the physics of the system.\nEstimating robust solutions for such problems is essential. Deep learning-based\narchitectures, such as neural PDE solvers, have recently gained traction as a\nreliable solution method. The current state of development of these approaches,\nhowever, primarily focuses on improving accuracy. The environmental impact of\nexcessive computation, leading to increased carbon emissions, has largely been\noverlooked. This paper introduces a carbon emission measure for a range of PDE\nsolvers. Our proposed metric, EcoL2, balances model accuracy with emissions\nacross data collection, model training, and deployment. Experiments across both\nphysics-informed machine learning and operator learning architectures\ndemonstrate that the proposed metric presents a holistic assessment of model\nperformance and emission cost. As such solvers grow in scale and deployment,\nEcoL2 represents a step toward building performant scientific machine learning\nsystems with lower long-term environmental impact.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12556v1",
    "published_date": "2025-05-18 22:05:11 UTC",
    "updated_date": "2025-05-18 22:05:11 UTC"
  },
  {
    "arxiv_id": "2505.12552v1",
    "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction",
    "authors": [
      "Junliang Ye",
      "Lei Wang",
      "Md Zakir Hossain"
    ],
    "abstract": "Reconstructing natural images from functional magnetic resonance imaging\n(fMRI) data remains a core challenge in natural decoding due to the mismatch\nbetween the richness of visual stimuli and the noisy, low resolution nature of\nfMRI signals. While recent two-stage models, combining deep variational\nautoencoders (VAEs) with diffusion models, have advanced this task, they treat\nall spatial-frequency components of the input equally. This uniform treatment\nforces the model to extract meaning features and suppress irrelevant noise\nsimultaneously, limiting its effectiveness. We introduce FreqSelect, a\nlightweight, adaptive module that selectively filters spatial-frequency bands\nbefore encoding. By dynamically emphasizing frequencies that are most\npredictive of brain activity and suppressing those that are uninformative,\nFreqSelect acts as a content-aware gate between image features and natural\ndata. It integrates seamlessly into standard very deep VAE-diffusion pipelines\nand requires no additional supervision. Evaluated on the Natural Scenes\ndataset, FreqSelect consistently improves reconstruction quality across both\nlow- and high-level metrics. Beyond performance gains, the learned\nfrequency-selection patterns offer interpretable insights into how different\nvisual frequencies are represented in the brain. Our method generalizes across\nsubjects and scenes, and holds promise for extension to other neuroimaging\nmodalities, offering a principled approach to enhancing both decoding accuracy\nand neuroscientific interpretability.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Research report",
    "pdf_url": "http://arxiv.org/pdf/2505.12552v1",
    "published_date": "2025-05-18 21:45:06 UTC",
    "updated_date": "2025-05-18 21:45:06 UTC"
  },
  {
    "arxiv_id": "2505.12547v1",
    "title": "ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations",
    "authors": [
      "Florent Chiaroni",
      "Ali Ayub",
      "Ola Ahmad"
    ],
    "abstract": "In robotics applications, few-shot segmentation is crucial because it allows\nrobots to perform complex tasks with minimal training data, facilitating their\nadaptation to diverse, real-world environments. However, pixel-level\nannotations of even small amount of images is highly time-consuming and costly.\nIn this paper, we present a novel few-shot binary segmentation method based on\nbounding-box annotations instead of pixel-level labels. We introduce, ProMi, an\nefficient prototype-mixture-based method that treats the background class as a\nmixture of distributions. Our approach is simple, training-free, and effective,\naccommodating coarse annotations with ease. Compared to existing baselines,\nProMi achieves the best results across different datasets with significant\ngains, demonstrating its effectiveness. Furthermore, we present qualitative\nexperiments tailored to real-world mobile robot tasks, demonstrating the\napplicability of our approach in such scenarios. Our code:\nhttps://github.com/ThalesGroup/promi.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12547v1",
    "published_date": "2025-05-18 21:08:05 UTC",
    "updated_date": "2025-05-18 21:08:05 UTC"
  },
  {
    "arxiv_id": "2505.12532v1",
    "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets",
    "authors": [
      "Ahmet Bilican",
      "M. Akın Yılmaz",
      "A. Murat Tekalp",
      "R. Gökberk Cinbiş"
    ],
    "abstract": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum -- ideal for extreme\nparameter-efficient scenarios. In order to demonstrate the effect of the\nwavelet transform, we compare WaveFT with a special case, called SHiRA, that\nentails applying sparse updates directly in the weight domain. Evaluated on\npersonalized text-to-image generation using Stable Diffusion XL as baseline,\nWaveFT significantly outperforms LoRA and other PEFT methods, especially at low\nparameter counts; achieving superior subject fidelity, prompt alignment, and\nimage diversity.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12532v1",
    "published_date": "2025-05-18 20:20:32 UTC",
    "updated_date": "2025-05-18 20:20:32 UTC"
  },
  {
    "arxiv_id": "2505.12512v1",
    "title": "Scalable Strategies for Continual Learning with Replay",
    "authors": [
      "Truman Hickok"
    ],
    "abstract": "Future deep learning models will be distinguished by systems that perpetually\nlearn through interaction, imagination, and cooperation, blurring the line\nbetween training and inference. This makes continual learning a critical\nchallenge, as methods that efficiently maximize bidirectional transfer across\nlearning trajectories will be essential. Replay is on track to play a\nfoundational role in continual learning, allowing models to directly reconcile\nnew information with past knowledge. In practice, however, replay is quite\nunscalable, doubling the cost of continual learning when applied naively.\nMoreover, the continual learning literature has not fully synchronized with the\nmulti-task fine-tuning literature, having not fully integrated highly scalable\ntechniques like model merging and low rank adaptation into a replay-enabled\ntoolset that can produce a unified model in the face of many sequential tasks.\nIn this paper, we begin by applying and analyzing low rank adaptation in a\ncontinual learning setting. Next, we introduce consolidation, a phasic approach\nto replay which leads to up to 55\\% less replay samples being needed for a\ngiven performance target. Then, we propose sequential merging, an offshoot of\ntask arithmetic which is tailored to the continual learning setting and is\nshown to work well in combination with replay. Finally, we demonstrate that the\ndeveloped strategies can operate synergistically, resulting in a highly\nscalable toolset that outperforms standalone variants.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12512v1",
    "published_date": "2025-05-18 18:23:50 UTC",
    "updated_date": "2025-05-18 18:23:50 UTC"
  },
  {
    "arxiv_id": "2505.12509v1",
    "title": "Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models",
    "authors": [
      "Junhao Liu",
      "Haonan Yu",
      "Xin Zhang"
    ],
    "abstract": "With Large language models (LLMs) becoming increasingly prevalent in various\napplications, the need for interpreting their predictions has become a critical\nchallenge. As LLMs vary in architecture and some are closed-sourced,\nmodel-agnostic techniques show great promise without requiring access to the\nmodel's internal parameters. However, existing model-agnostic techniques need\nto invoke LLMs many times to gain sufficient samples for generating faithful\nexplanations, which leads to high economic costs. In this paper, we show that\nit is practical to generate faithful explanations for large-scale LLMs by\nsampling from some budget-friendly models through a series of empirical\nstudies. Moreover, we show that such proxy explanations also perform well on\ndownstream tasks. Our analysis provides a new paradigm of model-agnostic\nexplanation methods for LLMs, by including information from budget-friendly\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12509v1",
    "published_date": "2025-05-18 18:05:37 UTC",
    "updated_date": "2025-05-18 18:05:37 UTC"
  },
  {
    "arxiv_id": "2505.12506v1",
    "title": "Unsupervised Invariant Risk Minimization",
    "authors": [
      "Yotam Norman",
      "Ron Meir"
    ],
    "abstract": "We propose a novel unsupervised framework for \\emph{Invariant Risk\nMinimization} (IRM), extending the concept of invariance to settings where\nlabels are unavailable. Traditional IRM methods rely on labeled data to learn\nrepresentations that are robust to distributional shifts across environments.\nIn contrast, our approach redefines invariance through feature distribution\nalignment, enabling robust representation learning from unlabeled data. We\nintroduce two methods within this framework: Principal Invariant Component\nAnalysis (PICA), a linear method that extracts invariant directions under\nGaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep\ngenerative model that disentangles environment-invariant and\nenvironment-dependent latent factors. Our approach is based on a novel\n``unsupervised'' structural causal model and supports environment-conditioned\nsample-generation and intervention. Empirical evaluations on synthetic dataset\nand modified versions of MNIST demonstrate the effectiveness of our methods in\ncapturing invariant structure, preserving relevant information, and\ngeneralizing across environments without access to labels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12506v1",
    "published_date": "2025-05-18 17:54:23 UTC",
    "updated_date": "2025-05-18 17:54:23 UTC"
  },
  {
    "arxiv_id": "2505.12504v1",
    "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models",
    "authors": [
      "Zongkai Liu",
      "Fanqing Meng",
      "Lingxiao Du",
      "Zhixiang Zhou",
      "Chao Yu",
      "Wenqi Shao",
      "Qiaosheng Zhang"
    ],
    "abstract": "Recent advances in rule-based reinforcement learning (RL) have significantly\nimproved the reasoning capability of language models (LMs) with rule-based\nrewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO --\noften suffer from training instability, where large policy updates and improper\nclipping can lead to training collapse. To address this issue, we propose\nClipped Policy Gradient Optimization with Policy Drift (CPGD), a novel\nalgorithm designed to stabilize policy learning in LMs. CPGD introduces a\npolicy drift constraint based on KL divergence to dynamically regularize policy\nupdates, and leverages a clip mechanism on the logarithm of the ratio to\nprevent excessive policy updates. We provide theoretical justification for CPGD\nand demonstrate through empirical analysis that it mitigates the instability\nobserved in prior approaches. Furthermore, we show that CPGD significantly\nimproves performance while maintaining training stability. Our implementation\nbalances theoretical rigor with practical usability, offering a robust\nalternative for RL in the post-training of LMs. We release our code at\nhttps://github.com/ModalMinds/MM-EUREKA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12504v1",
    "published_date": "2025-05-18 17:44:53 UTC",
    "updated_date": "2025-05-18 17:44:53 UTC"
  },
  {
    "arxiv_id": "2505.12501v1",
    "title": "ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning",
    "authors": [
      "Edward Y. Chang",
      "Longling Geng"
    ],
    "abstract": "Large language models (LLMs) excel at rapid generation of text and multimodal\ncontent, yet they falter on transaction-style planning that demands ACID-like\nguarantees and real-time disruption recovery. We present Adaptive LLM Agent\nSystem (ALAS), a framework that tackles four fundamental LLM deficits: (i)\nabsence of self-verification, (ii) context erosion, (iii) next-token myopia,\nand (iv) lack of persistent state. ALAS decomposes each plan into\nrole-specialized agents, equips them with automatic state tracking, and\ncoordinates them through a lightweight protocol. When disruptions arise, agents\napply history-aware local compensation, avoiding costly global replanning and\ncontaining cascade effects. On real-world, large-scale job-shop scheduling\nbenchmarks, ALAS sets new best results for static sequential planning and\nexcels in dynamic reactive scenarios with unexpected disruptions. These gains\nshow that principled modularization plus targeted compensation can unlock\nscalable and resilient planning with LLMs.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "36 pages, 10 figures, 19 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.12501v1",
    "published_date": "2025-05-18 17:27:08 UTC",
    "updated_date": "2025-05-18 17:27:08 UTC"
  },
  {
    "arxiv_id": "2505.13538v1",
    "title": "RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines",
    "authors": [
      "Dvir Cohen",
      "Lin Burg",
      "Gilad Barkan"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems show promise by coupling large\nlanguage models with external knowledge, yet traditional RAG evaluation methods\nprimarily report quantitative scores while offering limited actionable guidance\nfor refining these complex pipelines. In this paper, we introduce RAGXplain, an\nevaluation framework that quantifies RAG performance and translates these\nassessments into clear insights that clarify the workings of its complex,\nmulti-stage pipeline and offer actionable recommendations. Using LLM reasoning,\nRAGXplain converts raw scores into coherent narratives identifying performance\ngaps and suggesting targeted improvements. By providing transparent\nexplanations for AI decision-making, our framework fosters user trust-a key\nchallenge in AI adoption. Our LLM-based metric assessments show strong\nalignment with human judgments, and experiments on public question-answering\ndatasets confirm that applying RAGXplain's actionable recommendations\nmeasurably improves system performance. RAGXplain thus bridges quantitative\nevaluation and practical optimization, empowering users to understand, trust,\nand enhance their AI systems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13538v1",
    "published_date": "2025-05-18 17:25:34 UTC",
    "updated_date": "2025-05-18 17:25:34 UTC"
  },
  {
    "arxiv_id": "2505.12500v1",
    "title": "MARGE: Improving Math Reasoning for LLMs with Guided Exploration",
    "authors": [
      "Jingyue Gao",
      "Runji Lin",
      "Keming Lu",
      "Bowen Yu",
      "Junyang Lin",
      "Jianyu Chen"
    ],
    "abstract": "Large Language Models (LLMs) exhibit strong potential in mathematical\nreasoning, yet their effectiveness is often limited by a shortage of\nhigh-quality queries. This limitation necessitates scaling up computational\nresponses through self-generated data, yet current methods struggle due to\nspurious correlated data caused by ineffective exploration across all reasoning\nstages. To address such challenge, we introduce \\textbf{MARGE}: Improving\n\\textbf{Ma}th \\textbf{R}easoning with \\textbf{G}uided \\textbf{E}xploration, a\nnovel method to address this issue and enhance mathematical reasoning through\nhit-guided exploration. MARGE systematically explores intermediate reasoning\nstates derived from self-generated solutions, enabling adequate exploration and\nimproved credit assignment throughout the reasoning process. Through extensive\nexperiments across multiple backbone models and benchmarks, we demonstrate that\nMARGE significantly improves reasoning capabilities without requiring external\nannotations or training additional value models. Notably, MARGE improves both\nsingle-shot accuracy and exploration diversity, mitigating a common trade-off\nin alignment methods. These results demonstrate MARGE's effectiveness in\nenhancing mathematical reasoning capabilities and unlocking the potential of\nscaling self-generated training data. Our code and models are available at\n\\href{https://github.com/georgao35/MARGE}{this link}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear at ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.12500v1",
    "published_date": "2025-05-18 17:24:16 UTC",
    "updated_date": "2025-05-18 17:24:16 UTC"
  },
  {
    "arxiv_id": "2505.12493v1",
    "title": "UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning",
    "authors": [
      "Longxi Gao",
      "Li Zhang",
      "Mengwei Xu"
    ],
    "abstract": "Training effective Vision Language Models (VLMs) for GUI agents typically\nrelies on supervised fine-tuning (SFT) over large-scale annotated datasets,\nwhere the collection process is labor-intensive and error-prone. In this work,\nwe propose a self-supervised inverse dynamics task to enable VLMs to learn from\nGUI transition pairs by inferring the action that caused that transition. This\ntraining task offers two advantages: (1) It enables VLMs to ignore variations\nunrelated to user actions (e.g., background refreshes, ads) and to focus on\ntrue affordances such as buttons and input fields within complex GUIs. (2) The\ntraining data can be easily obtained from existing GUI trajectories without\nrequiring human annotation, and it can be easily scaled through automatic\noffline exploration. Using this training task, we propose UI-shift, a framework\nfor enhancing VLM-based GUI agents through self-supervised reinforcement\nlearning (RL). With only 2K training samples sourced from existing datasets,\ntwo VLMs -- Qwen2.5-VL-3B and Qwen2.5-VL-7B -- trained with UI-Shift achieve\ncompetitive or superior performance on grounding tasks (ScreenSpot-series\nbenchmarks) and GUI automation tasks (AndroidControl), compared to SFT\nbaselines and GUI-specific models that explicitly elicit reasoning abilities\nduring RL. Our findings suggest a potential direction for enhancing VLMs for\nGUI agents by leveraging more self-supervised training data in the future.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12493v1",
    "published_date": "2025-05-18 16:34:30 UTC",
    "updated_date": "2025-05-18 16:34:30 UTC"
  },
  {
    "arxiv_id": "2505.12492v1",
    "title": "Unleashing Automated Congestion Control Customization in the Wild",
    "authors": [
      "Amit Cohen",
      "Lev Gloukhenki",
      "Ravid Hadar",
      "Eden Itah",
      "Yehuda Shvut",
      "Michael Schapira"
    ],
    "abstract": "Congestion control (CC) crucially impacts user experience across Internet\nservices like streaming, gaming, AR/VR, and connected cars. Traditionally, CC\nalgorithm design seeks universal control rules that yield high performance\nacross diverse application domains and networks. However, varying service needs\nand network conditions challenge this approach. We share operational experience\nwith a system that automatically customizes congestion control logic to service\nneeds and network conditions. We discuss design, deployment challenges, and\nsolutions, highlighting performance benefits through case studies in streaming,\ngaming, connected cars, and more.\n  Our system leverages PCC Vivace, an online-learning based congestion control\nprotocol developed by researchers. Hence, along with insights from customizing\ncongestion control, we also discuss lessons learned and modifications made to\nadapt PCC Vivace for real-world deployment.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG",
      "cs.PF",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12492v1",
    "published_date": "2025-05-18 16:29:19 UTC",
    "updated_date": "2025-05-18 16:29:19 UTC"
  },
  {
    "arxiv_id": "2505.12489v2",
    "title": "Video-GPT via Next Clip Diffusion",
    "authors": [
      "Shaobin Zhuang",
      "Zhipeng Huang",
      "Ying Zhang",
      "Fangyikang Wang",
      "Canmiao Fu",
      "Binxin Yang",
      "Chong Sun",
      "Chen Li",
      "Yali Wang"
    ],
    "abstract": "GPT has shown its remarkable success in natural language processing. However,\nthe language sequence is not sufficient to describe spatial-temporal details in\nthe visual world. Alternatively, the video sequence is good at capturing such\ndetails. Motivated by this fact, we propose a concise Video-GPT in this paper\nby treating video as new language for visual world modeling. By analogy to next\ntoken prediction in GPT, we introduce a novel next clip diffusion paradigm for\npretraining Video-GPT. Different from the previous works, this distinct\nparadigm allows Video-GPT to tackle both short-term generation and long-term\nprediction, by autoregressively denoising the noisy clip according to the clean\nclips in the history. Extensive experiments show our Video-GPT achieves the\nstate-of-the-art performance on video prediction, which is the key factor\ntowards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64\nvs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in\nboth video generation and understanding, showing its great generalization\ncapacity in downstream. The project page is at\nhttps://zhuangshaobin.github.io/Video-GPT.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 12 figures, 18 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.12489v2",
    "published_date": "2025-05-18 16:22:58 UTC",
    "updated_date": "2025-05-21 04:44:19 UTC"
  },
  {
    "arxiv_id": "2505.12477v1",
    "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning",
    "authors": [
      "Hugues Van Assel",
      "Mark Ibrahim",
      "Tommaso Biancalani",
      "Aviv Regev",
      "Randall Balestriero"
    ],
    "abstract": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.12477v1",
    "published_date": "2025-05-18 15:54:55 UTC",
    "updated_date": "2025-05-18 15:54:55 UTC"
  },
  {
    "arxiv_id": "2505.12476v1",
    "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering",
    "authors": [
      "Xiao Long",
      "Liansheng Zhuang",
      "Chen Shen",
      "Shaotian Yan",
      "Yifei Li",
      "Shafei Wang"
    ],
    "abstract": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12476v1",
    "published_date": "2025-05-18 15:52:57 UTC",
    "updated_date": "2025-05-18 15:52:57 UTC"
  },
  {
    "arxiv_id": "2505.13535v1",
    "title": "Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments",
    "authors": [
      "Aniket Bhattacharyya",
      "Anurag Tripathi",
      "Ujjal Das",
      "Archan Karmakar",
      "Amit Pathak",
      "Maneesh Gupta"
    ],
    "abstract": "Information extraction (IE) from Visually Rich Documents (VRDs) containing\nlayout features along with text is a critical and well-studied task.\nSpecialized non-LLM NLP-based solutions typically involve training models using\nboth textual and geometric information to label sequences/tokens as named\nentities or answers to specific questions. However, these approaches lack\nreasoning, are not able to infer values not explicitly present in documents,\nand do not generalize well to new formats. Generative LLM-based approaches\nproposed recently are capable of reasoning, but struggle to comprehend clues\nfrom document layout especially in previously unseen document formats, and do\nnot show competitive performance in heterogeneous VRD benchmark datasets. In\nthis paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs\ninto localized, reusable semantic textual segments called $\\textit{semantic\nblocks}$, which are processed independently. Through focused and more\ngeneralizable reasoning,our approach outperforms the state-of-the-art on public\nVRD benchmarks by 1-3% in F1 scores, is resilient to document formats\npreviously not encountered and shows abilities to correctly extract information\nnot explicitly present in documents.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to ACL Main 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.13535v1",
    "published_date": "2025-05-18 15:49:17 UTC",
    "updated_date": "2025-05-18 15:49:17 UTC"
  },
  {
    "arxiv_id": "2505.12470v1",
    "title": "NeuroGen: Neural Network Parameter Generation via Large Language Models",
    "authors": [
      "Jiaqi Wang",
      "Yusen Zhang",
      "Xi Li"
    ],
    "abstract": "Acquiring the parameters of neural networks (NNs) has been one of the most\nimportant problems in machine learning since the inception of NNs. Traditional\napproaches, such as backpropagation and forward-only optimization, acquire\nparameters via iterative data fitting to gradually optimize them. This paper\naims to explore the feasibility of a new direction: acquiring NN parameters via\nlarge language model generation. We propose NeuroGen, a generalized and\neasy-to-implement two-stage approach for NN parameter generation conditioned on\ndescriptions of the data, task, and network architecture. Stage one is\nParameter Reference Knowledge Injection, where LLMs are pretrained on NN\ncheckpoints to build foundational understanding of parameter space, whereas\nstage two is Context-Enhanced Instruction Tuning, enabling LLMs to adapt to\nspecific tasks through enriched, task-aware prompts. Experimental results\ndemonstrate that NeuroGen effectively generates usable NN parameters. Our\nfindings highlight the feasibility of LLM-based NN parameter generation and\nsuggest a promising new paradigm where LLMs and lightweight NNs can coexist\nsynergistically",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The three authors contributed equally to this work. The codes will be\n  public after being accepted",
    "pdf_url": "http://arxiv.org/pdf/2505.12470v1",
    "published_date": "2025-05-18 15:48:10 UTC",
    "updated_date": "2025-05-18 15:48:10 UTC"
  },
  {
    "arxiv_id": "2505.12467v1",
    "title": "Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems",
    "authors": [
      "Haochun Wang",
      "Sendong Zhao",
      "Jingbo Wang",
      "Zewen Qiang",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Multi-agent collaboration has emerged as a pivotal paradigm for addressing\ncomplex, distributed tasks in large language model (LLM)-driven applications.\nWhile prior research has focused on high-level architectural frameworks, the\ngranular mechanisms governing agents, critical to performance and scalability,\nremain underexplored. This study systematically investigates four dimensions of\ncollaboration strategies: (1) agent governance, (2) participation control, (3)\ninteraction dynamics, and (4) dialogue history management. Through rigorous\nexperimentation under two context-dependent scenarios: Distributed Evidence\nIntegration (DEI) and Structured Evidence Synthesis (SES), we quantify the\nimpact of these strategies on both task accuracy and computational efficiency.\nOur findings reveal that centralized governance, instructor-led participation,\nordered interaction patterns, and instructor-curated context summarization\ncollectively optimize the trade-off between decision quality and resource\nutilization with the support of the proposed Token-Accuracy Ratio (TAR). This\nwork establishes a foundation for designing adaptive, scalable multi-agent\nsystems, shifting the focus from structural novelty to strategic interaction\nmechanics.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.12467v1",
    "published_date": "2025-05-18 15:46:14 UTC",
    "updated_date": "2025-05-18 15:46:14 UTC"
  },
  {
    "arxiv_id": "2505.12442v2",
    "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems",
    "authors": [
      "Liwen Wang",
      "Wenxuan Wang",
      "Shuai Wang",
      "Zongjie Li",
      "Zhenlan Ji",
      "Zongyi Lyu",
      "Daoyuan Wu",
      "Shing-Chi Cheung"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12442v2",
    "published_date": "2025-05-18 14:31:45 UTC",
    "updated_date": "2025-05-20 11:48:36 UTC"
  },
  {
    "arxiv_id": "2505.12440v1",
    "title": "Model Discovery with Grammatical Evolution. An Experiment with Prime Numbers",
    "authors": [
      "Jakub Skrzyński",
      "Dominik Sepioło",
      "Antoni Ligęza"
    ],
    "abstract": "Machine Learning produces efficient decision and prediction models based on\ninput-output data only. Such models have the form of decision trees or neural\nnets and are far from transparent analytical models, based on mathematical\nformulas. Analytical model discovery requires additional knowledge and may be\nperformed with Grammatical Evolution. Such models are transparent, concise, and\nhave readable components and structure. This paper reports on a non-trivial\nexperiment with generating such models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented during 5th Polish Conference on Artificial Intelligence,\n  published in \"PROGRESS IN POLISH ARTIFICIAL INTELLIGENCE RESEARCH 5\" ISBN\n  978-83-8156-696-4",
    "pdf_url": "http://arxiv.org/pdf/2505.12440v1",
    "published_date": "2025-05-18 14:22:21 UTC",
    "updated_date": "2025-05-18 14:22:21 UTC"
  },
  {
    "arxiv_id": "2505.12437v1",
    "title": "Addressing the Scarcity of Benchmarks for Graph XAI",
    "authors": [
      "Michele Fontanesi",
      "Alessio Micheli",
      "Marco Podda",
      "Domenico Tortorella"
    ],
    "abstract": "While Graph Neural Networks (GNNs) have become the de facto model for\nlearning from structured data, their decisional process remains opaque to the\nend user, undermining their deployment in safety-critical applications. In the\ncase of graph classification, Explainable Artificial Intelligence (XAI)\ntechniques address this major issue by identifying sub-graph motifs that\nexplain predictions. However, advancements in this field are hindered by a\nchronic scarcity of benchmark datasets with known ground-truth motifs to assess\nthe explanations' quality. Current graph XAI benchmarks are limited to\nsynthetic data or a handful of real-world tasks hand-curated by domain experts.\nIn this paper, we propose a general method to automate the construction of XAI\nbenchmarks for graph classification from real-world datasets. We provide both\n15 ready-made benchmarks, as well as the code to generate more than 2000\nadditional XAI benchmarks with our method. As a use case, we employ our\nbenchmarks to assess the effectiveness of some popular graph explainers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12437v1",
    "published_date": "2025-05-18 14:19:52 UTC",
    "updated_date": "2025-05-18 14:19:52 UTC"
  },
  {
    "arxiv_id": "2505.12435v1",
    "title": "SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment",
    "authors": [
      "Wenqiao Zhu",
      "Ji Liu",
      "Lulu Wang",
      "Jun Wu",
      "Yulun Zhang"
    ],
    "abstract": "Direct Preference Optimization (DPO) is broadly utilized for aligning Large\nLanguage Models (LLMs) with human values because of its flexibility. Despite\nits effectiveness, it has been observed that the capability of DPO to generate\nhuman-preferred response is limited and the results of DPO are far from\nresilient. To address these limitations, in this paper we propose a novel\nSelf-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which\nincorporates a pilot term to steer the gradient flow during the optimization\nprocess, allowing for fine-grained control over the updates of chosen and\nrejected rewards. We provide a detailed theoretical analysis of our proposed\nmethod and elucidate its operational mechanism. Furthermore, we conduct\ncomprehensive experiments on various models and benchmarks. The extensive\nexperimental results demonstrate the consistency between the empirical results\nand our theoretical analysis and confirm the effectiveness of our proposed\napproach (up to 9.19% higher score).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, to appear in ACL'25",
    "pdf_url": "http://arxiv.org/pdf/2505.12435v1",
    "published_date": "2025-05-18 14:19:23 UTC",
    "updated_date": "2025-05-18 14:19:23 UTC"
  },
  {
    "arxiv_id": "2505.12433v1",
    "title": "SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization",
    "authors": [
      "Haodong Yang",
      "Lei Wang",
      "Md Zakir Hossain"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method that injects two trainable low-rank matrices (A and\nB) into frozen pretrained models. While efficient, LoRA constrains updates to a\nfixed low-rank subspace (Delta W = BA), which can limit representational\ncapacity and hinder downstream performance. We introduce Subspace Recomposition\nin Low-Rank Adaptation (SRLoRA) via importance-based fusion and\nreinitialization, a novel approach that enhances LoRA's expressiveness without\ncompromising its lightweight structure. SRLoRA assigns importance scores to\neach LoRA pair (a column of B and the corresponding row of A), and dynamically\nrecomposes the subspace during training. Less important pairs are fused into\nthe frozen backbone, freeing capacity to reinitialize new pairs along unused\nprincipal directions derived from the pretrained weight's singular value\ndecomposition. This mechanism enables continual subspace refreshment and richer\nadaptation over time, without increasing the number of trainable parameters. We\nevaluate SRLoRA on both language and vision tasks, including the GLUE benchmark\nand various image classification datasets. SRLoRA consistently achieves faster\nconvergence and improved accuracy over standard LoRA, demonstrating its\ngenerality, efficiency, and potential for broader PEFT applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Research report",
    "pdf_url": "http://arxiv.org/pdf/2505.12433v1",
    "published_date": "2025-05-18 14:12:40 UTC",
    "updated_date": "2025-05-18 14:12:40 UTC"
  },
  {
    "arxiv_id": "2505.12432v1",
    "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning",
    "authors": [
      "Zirun Guo",
      "Minjie Hong",
      "Tao Jin"
    ],
    "abstract": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12432v1",
    "published_date": "2025-05-18 14:08:03 UTC",
    "updated_date": "2025-05-18 14:08:03 UTC"
  },
  {
    "arxiv_id": "2505.12424v1",
    "title": "EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization",
    "authors": [
      "Lior Broide",
      "Roni Stern"
    ],
    "abstract": "Large Language Models (LLMs) have recently emerged as promising tools for\nautomated unit test generation. We introduce a hybrid framework called EvoGPT\nthat integrates LLM-based test generation with evolutionary search techniques\nto create diverse, fault-revealing unit tests. Unit tests are initially\ngenerated with diverse temperature sampling to maximize behavioral and test\nsuite diversity, followed by a generation-repair loop and coverage-guided\nassertion enhancement. The resulting test suites are evolved using genetic\nalgorithms, guided by a fitness function prioritizing mutation score over\ntraditional coverage metrics. This design emphasizes the primary objective of\nunit testing-fault detection. Evaluated on multiple open-source Java projects,\nEvoGPT achieves an average improvement of 10% in both code coverage and\nmutation score compared to LLMs and traditional search-based software testing\nbaselines. These results demonstrate that combining LLM-driven diversity,\ntargeted repair, and evolutionary optimization produces more effective and\nresilient test suites.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12424v1",
    "published_date": "2025-05-18 13:48:53 UTC",
    "updated_date": "2025-05-18 13:48:53 UTC"
  },
  {
    "arxiv_id": "2505.12423v1",
    "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration",
    "authors": [
      "Wenqiao Zhu",
      "Chao Xu",
      "Lulu Wang",
      "Jun Wu"
    ],
    "abstract": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12423v1",
    "published_date": "2025-05-18 13:47:44 UTC",
    "updated_date": "2025-05-18 13:47:44 UTC"
  },
  {
    "arxiv_id": "2505.12421v1",
    "title": "Fixed Point Explainability",
    "authors": [
      "Emanuele La Malfa",
      "Jon Vadillo",
      "Marco Molinari",
      "Michael Wooldridge"
    ],
    "abstract": "This paper introduces a formal notion of fixed point explanations, inspired\nby the \"why regress\" principle, to assess, through recursive applications, the\nstability of the interplay between a model and its explainer. Fixed point\nexplanations satisfy properties like minimality, stability, and faithfulness,\nrevealing hidden model behaviours and explanatory weaknesses. We define\nconvergence conditions for several classes of explainers, from feature-based to\nmechanistic tools like Sparse AutoEncoders, and we report quantitative and\nqualitative results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code: https://github.com/EmanueleLM/fixed-point-explainability",
    "pdf_url": "http://arxiv.org/pdf/2505.12421v1",
    "published_date": "2025-05-18 13:43:25 UTC",
    "updated_date": "2025-05-18 13:43:25 UTC"
  },
  {
    "arxiv_id": "2505.12418v1",
    "title": "Mutual Evidential Deep Learning for Medical Image Segmentation",
    "authors": [
      "Yuanpeng He",
      "Yali Bi",
      "Lijian Li",
      "Chi-Man Pun",
      "Wenpin Jiao",
      "Zhi Jin"
    ],
    "abstract": "Existing semi-supervised medical segmentation co-learning frameworks have\nrealized that model performance can be diminished by the biases in model\nrecognition caused by low-quality pseudo-labels. Due to the averaging nature of\ntheir pseudo-label integration strategy, they fail to explore the reliability\nof pseudo-labels from different sources. In this paper, we propose a mutual\nevidential deep learning (MEDL) framework that offers a potentially viable\nsolution for pseudo-label generation in semi-supervised learning from two\nperspectives. First, we introduce networks with different architectures to\ngenerate complementary evidence for unlabeled samples and adopt an improved\nclass-aware evidential fusion to guide the confident synthesis of evidential\npredictions sourced from diverse architectural networks. Second, utilizing the\nuncertainty in the fused evidence, we design an asymptotic Fisher\ninformation-based evidential learning strategy. This strategy enables the model\nto initially focus on unlabeled samples with more reliable pseudo-labels,\ngradually shifting attention to samples with lower-quality pseudo-labels while\navoiding over-penalization of mislabeled classes in high data uncertainty\nsamples. Additionally, for labeled data, we continue to adopt an\nuncertainty-driven asymptotic learning strategy, gradually guiding the model to\nfocus on challenging voxels. Extensive experiments on five mainstream datasets\nhave demonstrated that MEDL achieves state-of-the-art performance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12418v1",
    "published_date": "2025-05-18 13:42:27 UTC",
    "updated_date": "2025-05-18 13:42:27 UTC"
  },
  {
    "arxiv_id": "2505.12415v1",
    "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding",
    "authors": [
      "Zhenhe Wu",
      "Jian Yang",
      "Jiaheng Liu",
      "Xianjie Wu",
      "Changzai Pan",
      "Jie Zhang",
      "Yu Zhao",
      "Shuangyong Song",
      "Yongxiang Li",
      "Zhoujun Li"
    ],
    "abstract": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12415v1",
    "published_date": "2025-05-18 13:40:18 UTC",
    "updated_date": "2025-05-18 13:40:18 UTC"
  },
  {
    "arxiv_id": "2505.12408v1",
    "title": "ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding",
    "authors": [
      "Minxu Liu",
      "Donghai Guan",
      "Chuhang Zheng",
      "Chunwei Tian",
      "Jie Wen",
      "Qi Zhu"
    ],
    "abstract": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG-based visual decoding has shown promise due to its\nnon-invasive, low-cost nature and millisecond-level temporal resolution,\nexisting methods are limited by their reliance on flat neural representations\nthat overlook the brain's inherent visual hierarchy. In this paper, we\nintroduce ViEEG, a biologically inspired hierarchical EEG decoding framework\nthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes\neach visual stimulus into three biologically aligned components-contour,\nforeground object, and contextual scene-serving as anchors for a three-stream\nEEG encoder. These EEG features are progressively integrated via\ncross-attention routing, simulating cortical information flow from V1 to IT to\nthe association cortex. We further adopt hierarchical contrastive learning to\nalign EEG representations with CLIP embeddings, enabling zero-shot object\nrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate that\nViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in\nsubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,\nsurpassing existing methods by over 45%. Our framework not only advances the\nperformance frontier but also sets a new paradigm for biologically grounded\nbrain decoding in AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.12408v1",
    "published_date": "2025-05-18 13:19:08 UTC",
    "updated_date": "2025-05-18 13:19:08 UTC"
  },
  {
    "arxiv_id": "2505.12405v1",
    "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT",
    "authors": [
      "Konstantinos Xylogiannopoulos",
      "Petros Xanthopoulos",
      "Panagiotis Karampelas",
      "Georgios Bakamitsos"
    ],
    "abstract": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12405v1",
    "published_date": "2025-05-18 13:16:30 UTC",
    "updated_date": "2025-05-18 13:16:30 UTC"
  },
  {
    "arxiv_id": "2505.12404v1",
    "title": "Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies",
    "authors": [
      "Piotr Piękos",
      "Subhradeep Kayal",
      "Alexandros Karatzoglou"
    ],
    "abstract": "Hierarchical data arise in countless domains, from biological taxonomies and\norganizational charts to legal codes and knowledge graphs. Residual\nQuantization (RQ) is widely used to generate discrete, multitoken\nrepresentations for such data by iteratively quantizing residuals in a\nmultilevel codebook. However, its reliance on Euclidean geometry can introduce\nfundamental mismatches that hinder modeling of hierarchical branching,\nnecessary for faithful representation of hierarchical data. In this work, we\npropose Hyperbolic Residual Quantization (HRQ), which embeds data natively in a\nhyperbolic manifold and performs residual quantization using hyperbolic\noperations and distance metrics. By adapting the embedding network, residual\ncomputation, and distance metric to hyperbolic geometry, HRQ imparts an\ninductive bias that aligns naturally with hierarchical branching. We claim that\nHRQ in comparison to RQ can generate more useful for downstream tasks discrete\nhierarchical representations for data with latent hierarchies. We evaluate HRQ\non two tasks: supervised hierarchy modeling using WordNet hypernym trees, where\nthe model is supervised to learn the latent hierarchy - and hierarchy\ndiscovery, where, while latent hierarchy exists in the data, the model is not\ndirectly trained or evaluated on a task related to the hierarchy. Across both\nscenarios, HRQ hierarchical tokens yield better performance on downstream tasks\ncompared to Euclidean RQ with gains of up to $20\\%$ for the hierarchy modeling\ntask. Our results demonstrate that integrating hyperbolic geometry into\ndiscrete representation learning substantially enhances the ability to capture\nlatent hierarchies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12404v1",
    "published_date": "2025-05-18 13:14:07 UTC",
    "updated_date": "2025-05-18 13:14:07 UTC"
  },
  {
    "arxiv_id": "2505.13534v1",
    "title": "InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data",
    "authors": [
      "Dan Ofer",
      "Michal Linial",
      "Dafna Shahaf"
    ],
    "abstract": "Finding interesting phenomena is the core of scientific discovery, but it is\na manual, ill-defined concept. We present an integrative pipeline for\nautomating the discovery of interesting simple hypotheses (feature-target\nrelations with effect direction and a potential underlying mechanism) in\nstructured biomedical data. The pipeline combines machine learning, knowledge\ngraphs, literature search and Large Language Models. We formalize\n\"interestingness\" as a combination of novelty, utility and plausibility. On 8\nmajor diseases from the UK Biobank, our pipeline consistently recovers risk\nfactors years before their appearance in the literature. 40--53% of our top\ncandidates were validated as interesting, compared to 0--7% for a SHAP-based\nbaseline. Overall, 28% of 109 candidates were interesting to medical experts.\nThe pipeline addresses the challenge of operationalizing \"interestingness\"\nscalably and for any target. We release data and code:\nhttps://github.com/LinialLab/InterFeat",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "68T05, 68T50, 92C50",
      "I.2.6; I.2.7; H.2.8; J.3"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13534v1",
    "published_date": "2025-05-18 13:13:51 UTC",
    "updated_date": "2025-05-18 13:13:51 UTC"
  },
  {
    "arxiv_id": "2505.14714v1",
    "title": "KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection",
    "authors": [
      "Tuan-Vinh La",
      "Minh-Hieu Nguyen",
      "Minh-Son Dao"
    ],
    "abstract": "Fake news detection remains a challenging problem due to the complex\ninterplay between textual misinformation, manipulated images, and external\nknowledge reasoning. While existing approaches have achieved notable results in\nverifying veracity and cross-modal consistency, two key challenges persist: (1)\nExisting methods often consider only the global image context while neglecting\nlocal object-level details, and (2) they fail to incorporate external knowledge\nand entity relationships for deeper semantic understanding. To address these\nchallenges, we propose a novel multi-modal fake news detection framework that\nintegrates visual, textual, and knowledge-based representations. Our approach\nleverages bottom-up attention to capture fine-grained object details, CLIP for\nglobal image semantics, and RoBERTa for context-aware text encoding. We further\nenhance knowledge utilization by retrieving and adaptively selecting relevant\nentities from a knowledge graph. The fused multi-modal features are processed\nthrough a Transformer-based classifier to predict news veracity. Experimental\nresults demonstrate that our model outperforms recent approaches, showcasing\nthe effectiveness of neighbor selection mechanism and multi-modal fusion for\nfake news detection. Our proposal introduces a new paradigm: knowledge-grounded\nmultimodal reasoning. By integrating explicit entity-level selection and\nNLI-guided filtering, we shift fake news detection from feature fusion to\nsemantically grounded verification. For reproducibility and further research,\nthe source code is publicly at\n\\href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14714v1",
    "published_date": "2025-05-18 13:08:38 UTC",
    "updated_date": "2025-05-18 13:08:38 UTC"
  },
  {
    "arxiv_id": "2505.12398v1",
    "title": "Traversal Verification for Speculative Tree Decoding",
    "authors": [
      "Yepeng Weng",
      "Qiao Hu",
      "Xujie Chen",
      "Li Liu",
      "Dianwen Mei",
      "Huishi Qiu",
      "Jiang Tian",
      "Zhongchao Shi"
    ],
    "abstract": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2505.12398v1",
    "published_date": "2025-05-18 12:51:55 UTC",
    "updated_date": "2025-05-18 12:51:55 UTC"
  },
  {
    "arxiv_id": "2505.12395v1",
    "title": "Few-Shot Concept Unlearning with Low Rank Adaptation",
    "authors": [
      "Udaya Shreyas",
      "L. N. Aadarsh"
    ],
    "abstract": "Image Generation models are a trending topic nowadays, with many people\nutilizing Artificial Intelligence models in order to generate images. There are\nmany such models which, given a prompt of a text, will generate an image which\ndepicts said prompt. There are many image generation models, such as Latent\nDiffusion Models, Denoising Diffusion Probabilistic Models, Generative\nAdversarial Networks and many more. When generating images, these models can\ngenerate sensitive image data, which can be threatening to privacy or may\nviolate copyright laws of private entities. Machine unlearning aims at removing\nthe influence of specific data subsets from the trained models and in the case\nof image generation models, remove the influence of a concept such that the\nmodel is unable to generate said images of the concept when prompted.\nConventional retraining of the model can take upto days, hence fast algorithms\nare the need of the hour. In this paper we propose an algorithm that aims to\nremove the influence of concepts in diffusion models through updating the\ngradients of the final layers of the text encoders. Using a weighted loss\nfunction, we utilize backpropagation in order to update the weights of the\nfinal layers of the Text Encoder componet of the Stable Diffusion Model,\nremoving influence of the concept from the text-image embedding space, such\nthat when prompted, the result is an image not containing the concept. The\nweighted loss function makes use of Textual Inversion and Low-Rank\nAdaptation.We perform our experiments on Latent Diffusion Models, namely the\nStable Diffusion v2 model, with an average concept unlearning runtime of 50\nseconds using 4-5 images.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12395v1",
    "published_date": "2025-05-18 12:44:30 UTC",
    "updated_date": "2025-05-18 12:44:30 UTC"
  },
  {
    "arxiv_id": "2505.12392v1",
    "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
    "authors": [
      "Yang Hu",
      "Xingyu Zhang",
      "Xueji Fang",
      "Zhiyang Chen",
      "Xiao Wang",
      "Huatian Zhang",
      "Guojun Qi"
    ],
    "abstract": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12392v1",
    "published_date": "2025-05-18 12:37:56 UTC",
    "updated_date": "2025-05-18 12:37:56 UTC"
  },
  {
    "arxiv_id": "2505.12386v1",
    "title": "Data Sharing with a Generative AI Competitor",
    "authors": [
      "Boaz Taitler",
      "Omer Madmon",
      "Moshe Tennenholtz",
      "Omer Ben-Porat"
    ],
    "abstract": "As GenAI platforms grow, their dependence on content from competing\nproviders, combined with access to alternative data sources, creates new\nchallenges for data-sharing decisions. In this paper, we provide a model of\ndata sharing between a content creation firm and a GenAI platform that can also\nacquire content from third-party experts. The interaction is modeled as a\nStackelberg game: the firm first decides how much of its proprietary dataset to\nshare with GenAI, and GenAI subsequently determines how much additional data to\nacquire from external experts. Their utilities depend on user traffic, monetary\ntransfers, and the cost of acquiring additional data from external experts. We\ncharacterize the unique subgame perfect equilibrium of the game and uncover a\nsurprising phenomenon: The firm may be willing to pay GenAI to share the firm's\nown data, leading to a costly data-sharing equilibrium. We further characterize\nthe set of Pareto improving data prices, and show that such improvements occur\nonly when the firm pays to share data. Finally, we study how the price can be\nset to optimize different design objectives, such as promoting firm data\nsharing, expert data acquisition, or a balance of both. Our results shed light\non the economic forces shaping data-sharing partnerships in the age of GenAI,\nand provide guidance for platforms, regulators and policymakers seeking to\ndesign effective data exchange mechanisms.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12386v1",
    "published_date": "2025-05-18 12:22:37 UTC",
    "updated_date": "2025-05-18 12:22:37 UTC"
  },
  {
    "arxiv_id": "2505.12381v1",
    "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling",
    "authors": [
      "Mohsinul Kabir",
      "Tasfia Tahsin",
      "Sophia Ananiadou"
    ],
    "abstract": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.12381v1",
    "published_date": "2025-05-18 11:55:05 UTC",
    "updated_date": "2025-05-18 11:55:05 UTC"
  },
  {
    "arxiv_id": "2505.13533v1",
    "title": "FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs",
    "authors": [
      "Junzhe Jiang",
      "Chang Yang",
      "Aixin Cui",
      "Sihan Jin",
      "Ruiyu Wang",
      "Bo Li",
      "Xiao Huang",
      "Dongning Sun",
      "Xinrun Wang"
    ],
    "abstract": "Financial tasks are pivotal to global economic stability; however, their\nexecution faces challenges including labor intensive processes, low error\ntolerance, data fragmentation, and tool limitations. Although large language\nmodels (LLMs) have succeeded in various natural language processing tasks and\nhave shown potential in automating workflows through reasoning and contextual\nunderstanding, current benchmarks for evaluating LLMs in finance lack\nsufficient domain-specific data, have simplistic task design, and incomplete\nevaluation frameworks. To address these gaps, this article presents FinMaster,\na comprehensive financial benchmark designed to systematically assess the\ncapabilities of LLM in financial literacy, accounting, auditing, and\nconsulting. Specifically, FinMaster comprises three main modules: i) FinSim,\nwhich builds simulators that generate synthetic, privacy-compliant financial\ndata for companies to replicate market dynamics; ii) FinSuite, which provides\ntasks in core financial domains, spanning 183 tasks of various types and\ndifficulty levels; and iii) FinEval, which develops a unified interface for\nevaluation. Extensive experiments over state-of-the-art LLMs reveal critical\ncapability gaps in financial reasoning, with accuracy dropping from over 90% on\nbasic tasks to merely 40% on complex scenarios requiring multi-step reasoning.\nThis degradation exhibits the propagation of computational errors, where\nsingle-metric calculations initially demonstrating 58% accuracy decreased to\n37% in multimetric scenarios. To the best of our knowledge, FinMaster is the\nfirst benchmark that covers full-pipeline financial workflows with challenging\ntasks. We hope that FinMaster can bridge the gap between research and industry\npractitioners, driving the adoption of LLMs in real-world financial practices\nto enhance efficiency and accuracy.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-fin.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13533v1",
    "published_date": "2025-05-18 11:47:55 UTC",
    "updated_date": "2025-05-18 11:47:55 UTC"
  },
  {
    "arxiv_id": "2505.13532v1",
    "title": "Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios",
    "authors": [
      "Feihong Zhang",
      "Guojian Zhan",
      "Bin Shuai",
      "Tianyi Zhang",
      "Jingliang Duan",
      "Shengbo Eben Li"
    ],
    "abstract": "Reinforcement learning (RL), known for its self-evolution capability, offers\na promising approach to training high-level autonomous driving systems.\nHowever, handling constraints remains a significant challenge for existing RL\nalgorithms, particularly in real-world applications. In this paper, we propose\na new safety-oriented training technique called harmonic policy iteration\n(HPI). At each RL iteration, it first calculates two policy gradients\nassociated with efficient driving and safety constraints, respectively. Then, a\nharmonic gradient is derived for policy updating, minimizing conflicts between\nthe two gradients and consequently enabling a more balanced and stable training\nprocess. Furthermore, we adopt the state-of-the-art DSAC algorithm as the\nbackbone and integrate it with our HPI to develop a new safe RL algorithm,\nDSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H\nachieves efficient driving performance with near-zero safety constraint\nviolations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE Intelligent Vehicles Symposium (IV 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.13532v1",
    "published_date": "2025-05-18 11:35:57 UTC",
    "updated_date": "2025-05-18 11:35:57 UTC"
  },
  {
    "arxiv_id": "2505.12371v1",
    "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks",
    "authors": [
      "Yinghao Zhu",
      "Ziyi He",
      "Haoran Hu",
      "Xiaochen Zheng",
      "Xichen Zhang",
      "Zixiang Wang",
      "Junyi Gao",
      "Liantao Ma",
      "Lequan Yu"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12371v1",
    "published_date": "2025-05-18 11:28:17 UTC",
    "updated_date": "2025-05-18 11:28:17 UTC"
  },
  {
    "arxiv_id": "2505.12370v1",
    "title": "Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning",
    "authors": [
      "Xinbin Yuan",
      "Jian Zhang",
      "Kaixin Li",
      "Zhuoxuan Cai",
      "Lujian Yao",
      "Jie Chen",
      "Enguang Wang",
      "Qibin Hou",
      "Jinwei Chen",
      "Peng-Tao Jiang",
      "Bo Li"
    ],
    "abstract": "Graphical User Interface (GUI) agents have made substantial strides in\nunderstanding and executing user instructions across diverse platforms. Yet,\ngrounding these instructions to precise interface elements remains challenging,\nespecially in complex, high-resolution, professional environments. Traditional\nsupervised finetuning (SFT) methods often require large volumes of diverse data\nand exhibit weak generalization. To overcome these limitations, we introduce a\nreinforcement learning (RL) based framework that incorporates three core\nstrategies: (1) seed data curation to ensure high quality training samples, (2)\na dense policy gradient that provides continuous feedback based on prediction\naccuracy, and (3) a self evolutionary reinforcement finetuning mechanism that\niteratively refines the model using attention maps. With only 3k training\nsamples, our 7B-parameter model achieves state-of-the-art results among\nsimilarly sized models on three grounding benchmarks. Notably, it attains\n47.3\\% accuracy on the ScreenSpot-Pro dataset, outperforming much larger\nmodels, such as UI-TARS-72B, by a margin of 24.2\\%. These findings underscore\nthe effectiveness of RL-based approaches in enhancing GUI agent performance,\nparticularly in high-resolution, complex environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12370v1",
    "published_date": "2025-05-18 11:22:04 UTC",
    "updated_date": "2025-05-18 11:22:04 UTC"
  },
  {
    "arxiv_id": "2505.12369v1",
    "title": "Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations",
    "authors": [
      "Fernando Zhapa-Camacho",
      "Robert Hoehndorf"
    ],
    "abstract": "Geometric embedding methods have shown to be useful for multi-hop reasoning\non knowledge graphs by mapping entities and logical operations to geometric\nregions and geometric transformations, respectively. Geometric embeddings\nprovide direct interpretability framework for queries. However, current methods\nhave only leveraged the geometric construction of entities, failing to map\nlogical operations to geometric transformations and, instead, using neural\ncomponents to learn these operations. We introduce GeometrE, a geometric\nembedding method for multi-hop reasoning, which does not require learning the\nlogical operations and enables full geometric interpretability. Additionally,\nunlike previous methods, we introduce a transitive loss function and show that\nit can preserve the logical rule $\\forall a,b,c: r(a,b) \\land r(b,c) \\to\nr(a,c)$. Our experiments show that GeometrE outperforms current\nstate-of-the-art methods on standard benchmark datasets.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12369v1",
    "published_date": "2025-05-18 11:17:50 UTC",
    "updated_date": "2025-05-18 11:17:50 UTC"
  },
  {
    "arxiv_id": "2505.12368v1",
    "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement",
    "authors": [
      "Gauri Kholkar",
      "Ratinder Ahuja"
    ],
    "abstract": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in ACL LLMSec Workshop 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.12368v1",
    "published_date": "2025-05-18 11:14:14 UTC",
    "updated_date": "2025-05-18 11:14:14 UTC"
  },
  {
    "arxiv_id": "2505.12366v1",
    "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization",
    "authors": [
      "Gang Li",
      "Ming Lin",
      "Tomer Galanti",
      "Zhengzhong Tu",
      "Tianbao Yang"
    ],
    "abstract": "The recent success and openness of DeepSeek-R1 have brought widespread\nattention to Group Relative Policy Optimization (GRPO) as a reinforcement\nlearning method for large reasoning models (LRMs). In this work, we analyze the\nGRPO objective under a binary reward setting and reveal an inherent limitation\nof question-level difficulty bias. We also identify a connection between GRPO\nand traditional discriminative methods in supervised learning. Motivated by\nthese insights, we introduce a new Discriminative Constrained Optimization\n(DisCO) framework for reinforcing LRMs, grounded in the principle of\ndiscriminative learning. The main differences between DisCO and GRPO and its\nrecent variants are: (1) it replaces the group relative objective with a\ndiscriminative objective defined by a scoring function; (2) it abandons\nclipping-based surrogates in favor of non-clipping RL surrogate objectives used\nas scoring functions; (3) it employs a simple yet effective constrained\noptimization approach to enforce the KL divergence constraint, ensuring stable\ntraining. As a result, DisCO offers notable advantages over GRPO and its\nvariants: (i) it completely eliminates difficulty bias by adopting\ndiscriminative objectives; (ii) it addresses the entropy instability in GRPO\nand its variants through the use of non-clipping scoring functions and a\nconstrained optimization approach; (iii) it allows the incorporation of\nadvanced discriminative learning techniques to address data imbalance, where a\nsignificant number of questions have more negative than positive generated\nanswers during training. Our experiments on enhancing the mathematical\nreasoning capabilities of SFT-finetuned models show that DisCO significantly\noutperforms GRPO and its improved variants such as DAPO, achieving average\ngains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for an 1.5B\nmodel.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.12366v1",
    "published_date": "2025-05-18 11:08:32 UTC",
    "updated_date": "2025-05-18 11:08:32 UTC"
  },
  {
    "arxiv_id": "2505.12363v1",
    "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts",
    "authors": [
      "Qi Feng",
      "Hidetoshi Shimodaira"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA",
    "pdf_url": "http://arxiv.org/pdf/2505.12363v1",
    "published_date": "2025-05-18 10:57:33 UTC",
    "updated_date": "2025-05-18 10:57:33 UTC"
  },
  {
    "arxiv_id": "2505.12361v1",
    "title": "Adaptive MPC-based quadrupedal robot control under periodic disturbances",
    "authors": [
      "Elizaveta Pestova",
      "Ilya Osokin",
      "Danil Belov",
      "Pavel Osinenko"
    ],
    "abstract": "Recent advancements in adaptive control for reference trajectory tracking\nenable quadrupedal robots to perform locomotion tasks under challenging\nconditions. There are methods enabling the estimation of the external\ndisturbances in terms of forces and torques. However, a specific case of\ndisturbances that are periodic was not explicitly tackled in application to\nquadrupeds. This work is devoted to the estimation of the periodic disturbances\nwith a lightweight regressor using simplified robot dynamics and extracting the\ndisturbance properties in terms of the magnitude and frequency. Experimental\nevidence suggests performance improvement over the baseline static disturbance\ncompensation. All source files, including simulation setups, code, and\ncalculation scripts, are available on GitHub at\nhttps://github.com/aidagroup/quad-periodic-mpc.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12361v1",
    "published_date": "2025-05-18 10:48:38 UTC",
    "updated_date": "2025-05-18 10:48:38 UTC"
  },
  {
    "arxiv_id": "2505.12358v1",
    "title": "AbFlowNet: Optimizing Antibody-Antigen Binding Energy via Diffusion-GFlowNet Fusion",
    "authors": [
      "Abrar Rahman Abir",
      "Haz Sameen Shahgir",
      "Md Rownok Zahan Ratul",
      "Md Toki Tahmid",
      "Greg Ver Steeg",
      "Yue Dong"
    ],
    "abstract": "Complementarity Determining Regions (CDRs) are critical segments of an\nantibody that facilitate binding to specific antigens. Current computational\nmethods for CDR design utilize reconstruction losses and do not jointly\noptimize binding energy, a crucial metric for antibody efficacy. Rather,\nbinding energy optimization is done through computationally expensive Online\nReinforcement Learning (RL) pipelines rely heavily on unreliable binding energy\nestimators. In this paper, we propose AbFlowNet, a novel generative framework\nthat integrates GFlowNet with Diffusion models. By framing each diffusion step\nas a state in the GFlowNet framework, AbFlowNet jointly optimizes standard\ndiffusion losses and binding energy by directly incorporating energy signals\ninto the training process, thereby unifying diffusion and reward optimization\nin a single procedure. Experimental results show that AbFlowNet outperforms the\nbase diffusion model by 3.06% in amino acid recovery, 20.40% in geometric\nreconstruction (RMSD), and 3.60% in binding energy improvement ratio. ABFlowNet\nalso decreases Top-1 total energy and binding energy errors by 24.8% and 38.1%\nwithout pseudo-labeling the test dataset or using computationally expensive\nonline RL regimes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12358v1",
    "published_date": "2025-05-18 10:40:35 UTC",
    "updated_date": "2025-05-18 10:40:35 UTC"
  },
  {
    "arxiv_id": "2505.12355v2",
    "title": "GATES: Cost-aware Dynamic Workflow Scheduling via Graph Attention Networks and Evolution Strategy",
    "authors": [
      "Ya Shen",
      "Gang Chen",
      "Hui Ma",
      "Mengjie Zhang"
    ],
    "abstract": "Cost-aware Dynamic Workflow Scheduling (CADWS) is a key challenge in cloud\ncomputing, focusing on devising an effective scheduling policy to efficiently\nschedule dynamically arriving workflow tasks, represented as Directed Acyclic\nGraphs (DAG), to suitable virtual machines (VMs). Deep reinforcement learning\n(DRL) has been widely employed for automated scheduling policy design. However,\nthe performance of DRL is heavily influenced by the design of the\nproblem-tailored policy network and is highly sensitive to hyperparameters and\nthe design of reward feedback. Considering the above-mentioned issues, this\nstudy proposes a novel DRL method combining Graph Attention Networks-based\npolicy network and Evolution Strategy, referred to as GATES. The contributions\nof GATES are summarized as follows: (1) GATES can capture the impact of current\ntask scheduling on subsequent tasks by learning the topological relationships\nbetween tasks in a DAG. (2) GATES can assess the importance of each VM to the\nready task, enabling it to adapt to dynamically changing VM resources. (3)\nUtilizing Evolution Strategy's robustness, exploratory nature, and tolerance\nfor delayed rewards, GATES achieves stable policy learning in CADWS. Extensive\nexperimental results demonstrate the superiority of the proposed GATES in\nCADWS, outperforming several state-of-the-art algorithms. The source code is\navailable at: https://github.com/YaShen998/GATES.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by the 34th International Joint\n  Conference on Artificial Intelligence (IJCAI-2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.12355v2",
    "published_date": "2025-05-18 10:38:41 UTC",
    "updated_date": "2025-05-20 01:15:11 UTC"
  },
  {
    "arxiv_id": "2505.12354v1",
    "title": "A universal policy wrapper with guarantees",
    "authors": [
      "Anton Bolychev",
      "Georgiy Malaniya",
      "Grigory Yaremenko",
      "Anastasia Krasnaya",
      "Pavel Osinenko"
    ],
    "abstract": "We introduce a universal policy wrapper for reinforcement learning agents\nthat ensures formal goal-reaching guarantees. In contrast to standard\nreinforcement learning algorithms that excel in performance but lack rigorous\nsafety assurances, our wrapper selectively switches between a high-performing\nbase policy -- derived from any existing RL method -- and a fallback policy\nwith known convergence properties. Base policy's value function supervises this\nswitching process, determining when the fallback policy should override the\nbase policy to ensure the system remains on a stable path. The analysis proves\nthat our wrapper inherits the fallback policy's goal-reaching guarantees while\npreserving or improving upon the performance of the base policy. Notably, it\noperates without needing additional system knowledge or online constrained\noptimization, making it readily deployable across diverse reinforcement\nlearning architectures and tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12354v1",
    "published_date": "2025-05-18 10:37:27 UTC",
    "updated_date": "2025-05-18 10:37:27 UTC"
  },
  {
    "arxiv_id": "2505.12353v1",
    "title": "Importance Sampling for Nonlinear Models",
    "authors": [
      "Prakash Palanivelu Rajmohan",
      "Fred Roosta"
    ],
    "abstract": "While norm-based and leverage-score-based methods have been extensively\nstudied for identifying \"important\" data points in linear models, analogous\ntools for nonlinear models remain significantly underdeveloped. By introducing\nthe concept of the adjoint operator of a nonlinear map, we address this gap and\ngeneralize norm-based and leverage-score-based importance sampling to nonlinear\nsettings. We demonstrate that sampling based on these generalized notions of\nnorm and leverage scores provides approximation guarantees for the underlying\nnonlinear mapping, similar to linear subspace embeddings. As direct\napplications, these nonlinear scores not only reduce the computational\ncomplexity of training nonlinear models by enabling efficient sampling over\nlarge datasets but also offer a novel mechanism for model explainability and\noutlier detection. Our contributions are supported by both theoretical analyses\nand experimental results across a variety of supervised learning scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "This work is accepted at ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.12353v1",
    "published_date": "2025-05-18 10:34:39 UTC",
    "updated_date": "2025-05-18 10:34:39 UTC"
  },
  {
    "arxiv_id": "2505.12350v1",
    "title": "Multi-CALF: A Policy Combination Approach with Statistical Guarantees",
    "authors": [
      "Georgiy Malaniya",
      "Anton Bolychev",
      "Grigory Yaremenko",
      "Anastasia Krasnaya",
      "Pavel Osinenko"
    ],
    "abstract": "We introduce Multi-CALF, an algorithm that intelligently combines\nreinforcement learning policies based on their relative value improvements. Our\napproach integrates a standard RL policy with a theoretically-backed\nalternative policy, inheriting formal stability guarantees while often\nachieving better performance than either policy individually. We prove that our\ncombined policy converges to a specified goal set with known probability and\nprovide precise bounds on maximum deviation and convergence time. Empirical\nvalidation on control tasks demonstrates enhanced performance while maintaining\nstability guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12350v1",
    "published_date": "2025-05-18 10:30:24 UTC",
    "updated_date": "2025-05-18 10:30:24 UTC"
  },
  {
    "arxiv_id": "2505.12349v1",
    "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds",
    "authors": [
      "Axel Abels",
      "Tom Lenaerts"
    ],
    "abstract": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.12349v1",
    "published_date": "2025-05-18 10:29:24 UTC",
    "updated_date": "2025-05-18 10:29:24 UTC"
  },
  {
    "arxiv_id": "2505.12348v1",
    "title": "Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim Verification",
    "authors": [
      "Zhi Zheng",
      "Wee Sun Lee"
    ],
    "abstract": "Claim verification is essential in combating misinformation, and large\nlanguage models (LLMs) have recently emerged in this area as powerful tools for\nassessing the veracity of claims using external knowledge. Existing LLM-based\nmethods for claim verification typically adopt a Decompose-Then-Verify\nparadigm, which involves decomposing complex claims into several independent\nsub-claims and verifying each sub-claim separately. However, this paradigm\noften introduces errors during the claim decomposition process. To mitigate\nthese errors, we propose to develop the Chain-of-Thought (CoT)-Verify paradigm,\nwhich leverages LLM reasoning methods to generate CoT-verification paths for\nthe original complex claim without requiring decompositions into sub-claims and\nseparate verification stages. The CoT-Verify paradigm allows us to propose a\nnatural fine-tuning method called Reasoning-CV to enhance the verification\ncapabilities in LLMs. Reasoning-CV includes a supervised fine-tuning (SFT)\nstage and a self-improvement direct preference optimization (DPO) stage.\nUtilizing only an 8B pre-trained LLM, Reasoning-CV demonstrates superior\nknowledge-assisted claim verification performances compared to existing\nDecompose-Then-Verify methods, as well as powerful black-box LLMs such as\nGPT-4o+CoT and o1-preview. Our code is available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12348v1",
    "published_date": "2025-05-18 10:28:54 UTC",
    "updated_date": "2025-05-18 10:28:54 UTC"
  },
  {
    "arxiv_id": "2505.12346v1",
    "title": "SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization",
    "authors": [
      "Minghan Chen",
      "Guikun Chen",
      "Wenguan Wang",
      "Yi Yang"
    ],
    "abstract": "Large language models (LLMs) exhibit varying levels of confidence across\ninput prompts (questions): some lead to consistent, semantically similar\nanswers, while others yield diverse or contradictory outputs. This variation\nreflects LLM's uncertainty about the input prompt, a signal of how confidently\nthe model understands a given problem. However, vanilla Group Relative Policy\nOptimization (GRPO) treats all prompts equally during policy updates, ignoring\nthis important information about the model's knowledge boundaries. To address\nthis limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which\nexplicitly measures LLMs' uncertainty of the input prompts semantic entropy.\nSemantic entropy measures the diversity of meaning in multiple generated\nanswers given a prompt and uses this to modulate the magnitude of policy\nupdates. This uncertainty-aware training mechanism enables dynamic adjustment\nof policy update magnitudes based on question uncertainty. It allows more\nconservative updates on high-uncertainty questions while maintaining the\noriginal learning signal on confident ones. Experimental results on five\nmathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva\n34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new\nstate-of-the-art performance in average accuracy, validating the effectiveness\nof uncertainty-aware policy optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "On going project",
    "pdf_url": "http://arxiv.org/pdf/2505.12346v1",
    "published_date": "2025-05-18 10:20:59 UTC",
    "updated_date": "2025-05-18 10:20:59 UTC"
  },
  {
    "arxiv_id": "2505.12343v1",
    "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models",
    "authors": [
      "Kai Tang",
      "Jinhao You",
      "Xiuqi Ge",
      "Hanze Li",
      "Yichen Guo",
      "Xiande Huang"
    ],
    "abstract": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs),\nthey remain susceptible to hallucinations-generating content that is\ninconsistent with the input image. Existing training-free hallucination\nmitigation methods often suffer from unstable performance and high sensitivity\nto hyperparameter settings, limiting their practicality and broader adoption.\nIn this paper, we propose a novel decoding mechanism, Decoding with Inter-layer\nConsistency via Layer Aggregation (DCLA), which requires no retraining,\nfine-tuning, or access to external knowledge bases. Specifically, our approach\nconstructs a dynamic semantic reference by aggregating representations from\nprevious layers, and corrects semantically deviated layers to enforce\ninter-layer consistency. The method allows DCLA to robustly mitigate\nhallucinations across multiple LVLMs. Experiments on hallucination benchmarks\nsuch as MME and POPE demonstrate that DCLA effectively reduces hallucinations\nwhile enhancing the reliability and performance of LVLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12343v1",
    "published_date": "2025-05-18 10:15:42 UTC",
    "updated_date": "2025-05-18 10:15:42 UTC"
  },
  {
    "arxiv_id": "2505.12339v1",
    "title": "Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation",
    "authors": [
      "Midou Guo",
      "Qilin Yin",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "abstract": "With the development of generative artificial intelligence, new forgery\nmethods are rapidly emerging. Social platforms are flooded with vast amounts of\nunlabeled synthetic data and authentic data, making it increasingly challenging\nto distinguish real from fake. Due to the lack of labels, existing supervised\ndetection methods struggle to effectively address the detection of unknown\ndeepfake methods. Moreover, in open world scenarios, the amount of unlabeled\ndata greatly exceeds that of labeled data. Therefore, we define a new deepfake\ndetection generalization task which focuses on how to achieve efficient\ndetection of large amounts of unlabeled data based on limited labeled data to\nsimulate a open world scenario. To solve the above mentioned task, we propose a\nnovel Open-World Deepfake Detection Generalization Enhancement Training\nStrategy (OWG-DS) to improve the generalization ability of existing methods.\nOur approach aims to transfer deepfake detection knowledge from a small amount\nof labeled source domain data to large-scale unlabeled target domain data.\nSpecifically, we introduce the Domain Distance Optimization (DDO) module to\nalign different domain features by optimizing both inter-domain and\nintra-domain distances. Additionally, the Similarity-based Class Boundary\nSeparation (SCBS) module is used to enhance the aggregation of similar samples\nto ensure clearer class boundaries, while an adversarial training mechanism is\nadopted to learn the domain-invariant features. Extensive experiments show that\nthe proposed deepfake detection generalization enhancement training strategy\nexcels in cross-method and cross-dataset scenarios, improving the model's\ngeneralization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12339v1",
    "published_date": "2025-05-18 10:12:12 UTC",
    "updated_date": "2025-05-18 10:12:12 UTC"
  },
  {
    "arxiv_id": "2505.12334v1",
    "title": "Enhancing User-Oriented Proactivity in Open-Domain Dialogues with Critic Guidance",
    "authors": [
      "Yufeng Wang",
      "Jinwu Hu",
      "Ziteng Huang",
      "Kunyang Lin",
      "Zitian Zhang",
      "Peihao Chen",
      "Yu Hu",
      "Qianyue Wang",
      "Zhuliang Yu",
      "Bin Sun",
      "Xiaofen Xing",
      "Qingfang Zheng",
      "Mingkui Tan"
    ],
    "abstract": "Open-domain dialogue systems aim to generate natural and engaging\nconversations, providing significant practical value in real applications such\nas social robotics and personal assistants. The advent of large language models\n(LLMs) has greatly advanced this field by improving context understanding and\nconversational fluency. However, existing LLM-based dialogue systems often fall\nshort in proactively understanding the user's chatting preferences and guiding\nconversations toward user-centered topics. This lack of user-oriented\nproactivity can lead users to feel unappreciated, reducing their satisfaction\nand willingness to continue the conversation in human-computer interactions. To\naddress this issue, we propose a User-oriented Proactive Chatbot (UPC) to\nenhance the user-oriented proactivity. Specifically, we first construct a\ncritic to evaluate this proactivity inspired by the LLM-as-a-judge strategy.\nGiven the scarcity of high-quality training data, we then employ the critic to\nguide dialogues between the chatbot and user agents, generating a corpus with\nenhanced user-oriented proactivity. To ensure the diversity of the user\nbackgrounds, we introduce the ISCO-800, a diverse user background dataset for\nconstructing user agents. Moreover, considering the communication difficulty\nvaries among users, we propose an iterative curriculum learning method that\ntrains the chatbot from easy-to-communicate users to more challenging ones,\nthereby gradually enhancing its performance. Experiments demonstrate that our\nproposed training method is applicable to different LLMs, improving\nuser-oriented proactivity and attractiveness in open-domain dialogues.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.12334v1",
    "published_date": "2025-05-18 09:59:22 UTC",
    "updated_date": "2025-05-18 09:59:22 UTC"
  },
  {
    "arxiv_id": "2505.12332v2",
    "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning",
    "authors": [
      "Qianyue Hu",
      "Junyan Wu",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "abstract": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12332v2",
    "published_date": "2025-05-18 09:58:48 UTC",
    "updated_date": "2025-05-21 02:08:03 UTC"
  },
  {
    "arxiv_id": "2505.12329v1",
    "title": "MPRM: A Markov Path-based Rule Miner for Efficient and Interpretable Knowledge Graph Reasoning",
    "authors": [
      "Mingyang Li",
      "Song Wang",
      "Ning Cai"
    ],
    "abstract": "Rule mining in knowledge graphs enables interpretable link prediction.\nHowever, deep learning-based rule mining methods face significant memory and\ntime challenges for large-scale knowledge graphs, whereas traditional\napproaches, limited by rigid confidence metrics, incur high computational costs\ndespite sampling techniques. To address these challenges, we propose MPRM, a\nnovel rule mining method that models rule-based inference as a Markov chain and\nuses an efficient confidence metric derived from aggregated path probabilities,\nsignificantly lowering computational demands. Experiments on multiple datasets\nshow that MPRM efficiently mines knowledge graphs with over a million facts,\nsampling less than 1% of facts on a single CPU in 22 seconds, while preserving\ninterpretability and boosting inference accuracy by up to 11% over baselines.",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12329v1",
    "published_date": "2025-05-18 09:48:45 UTC",
    "updated_date": "2025-05-18 09:48:45 UTC"
  },
  {
    "arxiv_id": "2505.12327v1",
    "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions",
    "authors": [
      "Albert Zhao",
      "Stefano Soatto"
    ],
    "abstract": "We describe a robust planning method for autonomous driving that mixes normal\nand adversarial agent predictions output by a diffusion model trained for\nmotion prediction. We first train a diffusion model to learn an unbiased\ndistribution of normal agent behaviors. We then generate a distribution of\nadversarial predictions by biasing the diffusion model at test time to generate\npredictions that are likely to collide with a candidate plan. We score plans\nusing expected cost with respect to a mixture distribution of normal and\nadversarial predictions, leading to a planner that is robust against\nadversarial behaviors but not overly conservative when agents behave normally.\nUnlike current approaches, we do not use risk measures that over-weight\nadversarial behaviors while placing little to no weight on low-cost normal\nbehaviors or use hard safety constraints that may not be appropriate for all\ndriving scenarios. We show the effectiveness of our method on single-agent and\nmulti-agent jaywalking scenarios as well as a red light violation scenario.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE International Conference on Robotics and Automation (ICRA) 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.12327v1",
    "published_date": "2025-05-18 09:44:57 UTC",
    "updated_date": "2025-05-18 09:44:57 UTC"
  },
  {
    "arxiv_id": "2505.12321v1",
    "title": "BeliefNest: A Joint Action Simulator for Embodied Agents with Theory of Mind",
    "authors": [
      "Rikunari Sagara",
      "Koichiro Terao",
      "Naoto Iwahashi"
    ],
    "abstract": "This paper introduces an open-source simulator, BeliefNest, designed to\nenable embodied agents to perform collaborative tasks by leveraging Theory of\nMind. BeliefNest dynamically and hierarchically constructs simulators within a\nMinecraft environment, allowing agents to explicitly represent nested belief\nstates about themselves and others. This enables agent control in open-domain\ntasks that require Theory of Mind reasoning. The simulator provides a prompt\ngeneration mechanism based on each belief state, facilitating the design and\nevaluation of methods for agent control utilizing large language models (LLMs).\nWe demonstrate through experiments that agents can infer others' beliefs and\npredict their belief-based actions in false-belief tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12321v1",
    "published_date": "2025-05-18 09:26:48 UTC",
    "updated_date": "2025-05-18 09:26:48 UTC"
  },
  {
    "arxiv_id": "2505.13531v1",
    "title": "AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference",
    "authors": [
      "Shitong Duan",
      "Xiaoyuan Yi",
      "Peng Zhang",
      "Dongkuan Xu",
      "Jing Yao",
      "Tun Lu",
      "Ning Gu",
      "Xing Xie"
    ],
    "abstract": "Assessing Large Language Models (LLMs)' underlying value differences enables\ncomprehensive comparison of their misalignment, cultural adaptability, and\nbiases. Nevertheless, current value measurement datasets face the\ninformativeness challenge: with often outdated, contaminated, or generic test\nquestions, they can only capture the shared value orientations among different\nLLMs, leading to saturated and thus uninformative results. To address this\nproblem, we introduce AdAEM, a novel, self-extensible assessment framework for\nrevealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM\ncan automatically and adaptively generate and extend its test questions. This\nis achieved by probing the internal value boundaries of a diverse set of LLMs\ndeveloped across cultures and time periods in an in-context optimization\nmanner. The optimization process theoretically maximizes an\ninformation-theoretic objective to extract the latest or culturally\ncontroversial topics, providing more distinguishable and informative insights\nabout models' value differences. In this way, AdAEM is able to co-evolve with\nthe development of LLMs, consistently tracking their value dynamics. Using\nAdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct\nan extensive analysis to manifest our method's validity and effectiveness, and\nbenchmark the values of 16 LLMs, laying the groundwork for better value\nresearch.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13531v1",
    "published_date": "2025-05-18 09:15:26 UTC",
    "updated_date": "2025-05-18 09:15:26 UTC"
  },
  {
    "arxiv_id": "2505.12312v1",
    "title": "Visuospatial Cognitive Assistant",
    "authors": [
      "Qi Feng",
      "Hidetoshi Shimodaira"
    ],
    "abstract": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k",
    "pdf_url": "http://arxiv.org/pdf/2505.12312v1",
    "published_date": "2025-05-18 08:55:02 UTC",
    "updated_date": "2025-05-18 08:55:02 UTC"
  },
  {
    "arxiv_id": "2505.12310v1",
    "title": "DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations",
    "authors": [
      "Shouyi Lu",
      "Huanyu Zhou",
      "Guirong Zhuo"
    ],
    "abstract": "A novel learning-optimization-combined 4D radar odometry model, named\nDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates\ntraditional geometric optimization with end-to-end neural network training,\nleveraging an innovative differentiable neural-optimization iteration operator.\nIn this framework, point-wise motion flow is first estimated using a neural\nnetwork, followed by the construction of a cost function based on the\nrelationship between point motion and pose in 3D space. The radar pose is then\nrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4D\nradar backbone that integrates multi-scale geometric features and\nclustering-based class-aware features to enhance the representation of sparse\n4D radar point clouds. Extensive experiments on the VoD and Snail-Radar\ndatasets demonstrate the superior performance of our model, which outperforms\nrecent classical and learning-based approaches. Notably, our method even\nachieves results comparable to A-LOAM with mapping optimization using LiDAR\npoint clouds as input. Our models and code will be publicly released.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages,10 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.12310v1",
    "published_date": "2025-05-18 08:50:54 UTC",
    "updated_date": "2025-05-18 08:50:54 UTC"
  },
  {
    "arxiv_id": "2505.12309v1",
    "title": "Community Search in Time-dependent Road-social Attributed Networks",
    "authors": [
      "Li Ni",
      "Hengkai Xu",
      "Lin Mu",
      "Yiwen Zhang",
      "Wenjian Luo"
    ],
    "abstract": "Real-world networks often involve both keywords and locations, along with\ntravel time variations between locations due to traffic conditions. However,\nmost existing cohesive subgraph-based community search studies utilize a single\nattribute, either keywords or locations, to identify communities. They do not\nsimultaneously consider both keywords and locations, which results in low\nsemantic or spatial cohesiveness of the detected communities, and they fail to\naccount for variations in travel time. Additionally, these studies traverse the\nentire network to build efficient indexes, but the detected community only\ninvolves nodes around the query node, leading to the traversal of nodes that\nare not relevant to the community. Therefore, we propose the problem of\ndiscovering semantic-spatial aware k-core, which refers to a k-core with high\nsemantic and time-dependent spatial cohesiveness containing the query node. To\naddress this problem, we propose an exact and a greedy algorithm, both of which\ngradually expand outward from the query node. They are local methods that only\naccess the local part of the attributed network near the query node rather than\nthe entire network. Moreover, we design a method to calculate the semantic\nsimilarity between two keywords using large language models. This method\nalleviates the disadvantages of keyword-matching methods used in existing\ncommunity search studies, such as mismatches caused by differently expressed\nsynonyms and the presence of irrelevant words. Experimental results show that\nthe greedy algorithm outperforms baselines in terms of structural, semantic,\nand time-dependent spatial cohesiveness.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "12 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.12309v1",
    "published_date": "2025-05-18 08:45:05 UTC",
    "updated_date": "2025-05-18 08:45:05 UTC"
  },
  {
    "arxiv_id": "2505.12304v1",
    "title": "Pre-trained Prompt-driven Community Search",
    "authors": [
      "Li Ni",
      "Hengkai Xu",
      "Lin Mu",
      "Yiwen Zhang",
      "Wenjian Luo"
    ],
    "abstract": "The \"pre-train, prompt\" paradigm is widely adopted in various graph-based\ntasks and has shown promising performance in community detection. Most existing\nsemi-supervised community detection algorithms detect communities based on\nknown ones, and the detected communities typically do not contain the given\nquery node. Therefore, they are not suitable for searching the community of a\ngiven node. Motivated by this, we adopt this paradigm into the semi-supervised\ncommunity search for the first time and propose Pre-trained Prompt-driven\nCommunity Search (PPCS), a novel model designed to enhance search accuracy and\nefficiency. PPCS consists of three main components: node encoding, sample\ngeneration, and prompt-driven fine-tuning. Specifically, the node encoding\ncomponent employs graph neural networks to learn local structural patterns of\nnodes in a graph, thereby obtaining representations for nodes and communities.\nNext, the sample generation component identifies an initial community for a\ngiven node and selects known communities that are structurally similar to the\ninitial one as training samples. Finally, the prompt-driven fine-tuning\ncomponent leverages these samples as prompts to guide the final community\nprediction. Experimental results on five real-world datasets demonstrate that\nPPCS performs better than baseline algorithms. It also achieves higher\ncommunity search efficiency than semi-supervised community search baseline\nmethods, with ablation studies verifying the effectiveness of each component of\nPPCS.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.12304v1",
    "published_date": "2025-05-18 08:36:37 UTC",
    "updated_date": "2025-05-18 08:36:37 UTC"
  },
  {
    "arxiv_id": "2505.12301v1",
    "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge",
    "authors": [
      "Luyu Chen",
      "Zeyu Zhang",
      "Haoran Tan",
      "Quanyu Dai",
      "Hao Yang",
      "Zhenhua Dong",
      "Xu Chen"
    ],
    "abstract": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 3 tables, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.12301v1",
    "published_date": "2025-05-18 08:33:09 UTC",
    "updated_date": "2025-05-18 08:33:09 UTC"
  },
  {
    "arxiv_id": "2505.12299v1",
    "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning",
    "authors": [
      "Kun Huang",
      "Weikai Xu",
      "Yuxuan Liu",
      "Quandong Wang",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan",
      "Bin Wang",
      "Bo An"
    ],
    "abstract": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 8 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.12299v1",
    "published_date": "2025-05-18 08:28:05 UTC",
    "updated_date": "2025-05-18 08:28:05 UTC"
  },
  {
    "arxiv_id": "2505.12298v1",
    "title": "Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans",
    "authors": [
      "Amal Lahchim",
      "Lazar Davic"
    ],
    "abstract": "In this study, we propose a robust methodology for automatic segmentation of\ninfected lung regions in COVID-19 CT scans using convolutional neural networks.\nThe approach is based on a modified U-Net architecture enhanced with attention\nmechanisms, data augmentation, and postprocessing techniques. It achieved a\nDice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods.\nThe dataset was sourced from public repositories and augmented for diversity.\nResults demonstrate superior segmentation performance. Future work includes\nexpanding the dataset, exploring 3D segmentation, and preparing the model for\nclinical deployment.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "14 pages, 9 figures, created using Google Colab and PyTorch. Compares\n  segmentation models for COVID-19 CT data",
    "pdf_url": "http://arxiv.org/pdf/2505.12298v1",
    "published_date": "2025-05-18 08:27:12 UTC",
    "updated_date": "2025-05-18 08:27:12 UTC"
  },
  {
    "arxiv_id": "2505.12296v1",
    "title": "PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking",
    "authors": [
      "Haiyu Deng",
      "Yanna Jiang",
      "Guangsheng Yu",
      "Qin Wang",
      "Xu Wang",
      "Baihe Ma",
      "Wei Ni",
      "Ren Ping Liu"
    ],
    "abstract": "Machine learning models are increasingly shared and outsourced, raising\nrequirements of verifying training effort (Proof-of-Learning, PoL) to ensure\nclaimed performance and establishing ownership (Proof-of-Ownership, PoO) for\ntransactions. When models are trained by untrusted parties, PoL and PoO must be\nenforced together to enable protection, attribution, and compensation. However,\nexisting studies typically address them separately, which not only weakens\nprotection against forgery and privacy breaches but also leads to high\nverification overhead.\n  We propose PoLO, a unified framework that simultaneously achieves PoL and PoO\nusing chained watermarks. PoLO splits the training process into fine-grained\ntraining shards and embeds a dedicated watermark in each shard. Each watermark\nis generated using the hash of the preceding shard, certifying the training\nprocess of the preceding shard. The chained structure makes it computationally\ndifficult to forge any individual part of the whole training process. The\ncomplete set of watermarks serves as the PoL, while the final watermark\nprovides the PoO. PoLO offers more efficient and privacy-preserving\nverification compared to the vanilla PoL solutions that rely on gradient-based\ntrajectory tracing and inadvertently expose training data during verification,\nwhile maintaining the same level of ownership assurance of watermark-based PoO\nschemes. Our evaluation shows that PoLO achieves 99% watermark detection\naccuracy for ownership verification, while preserving data privacy and cutting\nverification costs to just 1.5-10% of traditional methods. Forging PoLO demands\n1.1-4x more resources than honest proof generation, with the original proof\nretaining over 90% detection accuracy even after attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12296v1",
    "published_date": "2025-05-18 08:19:18 UTC",
    "updated_date": "2025-05-18 08:19:18 UTC"
  },
  {
    "arxiv_id": "2505.12292v1",
    "title": "SpikeX: Exploring Accelerator Architecture and Network-Hardware Co-Optimization for Sparse Spiking Neural Networks",
    "authors": [
      "Boxun Xu",
      "Richard Boone",
      "Peng Li"
    ],
    "abstract": "Spiking Neural Networks (SNNs) are promising biologically plausible models of\ncomputation which utilize a spiking binary activation function similar to that\nof biological neurons. SNNs are well positioned to process spatiotemporal data,\nand are advantageous in ultra-low power and real-time processing. Despite a\nlarge body of work on conventional artificial neural network accelerators, much\nless attention has been given to efficient SNN hardware accelerator design. In\nparticular, SNNs exhibit inherent unstructured spatial and temporal firing\nsparsity, an opportunity yet to be fully explored for great hardware processing\nefficiency. In this work, we propose a novel systolic-array SNN accelerator\narchitecture, called SpikeX, to take on the challenges and opportunities\nstemming from unstructured sparsity while taking into account the unique\ncharacteristics of spike-based computation. By developing an efficient dataflow\ntargeting expensive multi-bit weight data movements, SpikeX reduces memory\naccess and increases data sharing and hardware utilization for computations\nspanning across both time and space, thereby significantly improving energy\nefficiency and inference latency. Furthermore, recognizing the importance of\nSNN network and hardware co-design, we develop a co-optimization methodology\nfacilitating not only hardware-aware SNN training but also hardware accelerator\narchitecture search, allowing joint network weight parameter optimization and\naccelerator architectural reconfiguration. This end-to-end network/accelerator\nco-design approach offers a significant reduction of 15.1x-150.87x in\nenergy-delay-product(EDP) without comprising model accuracy.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.NE",
    "comment": "The paper has been accepted by IEEE TCAD",
    "pdf_url": "http://arxiv.org/pdf/2505.12292v1",
    "published_date": "2025-05-18 08:07:44 UTC",
    "updated_date": "2025-05-18 08:07:44 UTC"
  },
  {
    "arxiv_id": "2505.12287v1",
    "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models",
    "authors": [
      "Linghan Huang",
      "Haolin Jin",
      "Zhaoge Bi",
      "Pengyue Yang",
      "Peizhou Zhao",
      "Taozhao Chen",
      "Xiongfei Wu",
      "Lei Ma",
      "Huaming Chen"
    ],
    "abstract": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12287v1",
    "published_date": "2025-05-18 07:51:19 UTC",
    "updated_date": "2025-05-18 07:51:19 UTC"
  },
  {
    "arxiv_id": "2505.12284v1",
    "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization",
    "authors": [
      "Danlong Yuan",
      "Tian Xie",
      "Shaohan Huang",
      "Zhuocheng Gong",
      "Huishuai Zhang",
      "Chong Luo",
      "Furu Wei",
      "Dongyan Zhao"
    ],
    "abstract": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2505.12284v1",
    "published_date": "2025-05-18 07:46:43 UTC",
    "updated_date": "2025-05-18 07:46:43 UTC"
  },
  {
    "arxiv_id": "2505.12275v1",
    "title": "Curriculum Abductive Learning",
    "authors": [
      "Wen-Chao Hu",
      "Qi-Jie Li",
      "Lin-Han Jia",
      "Cunjing Ge",
      "Yu-Feng Li",
      "Yuan Jiang",
      "Zhi-Hua Zhou"
    ],
    "abstract": "Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12275v1",
    "published_date": "2025-05-18 07:27:35 UTC",
    "updated_date": "2025-05-18 07:27:35 UTC"
  },
  {
    "arxiv_id": "2505.13529v1",
    "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
    "authors": [
      "Junxiao Yang",
      "Jinzhe Tu",
      "Haoran Liu",
      "Xiaoce Wang",
      "Chujie Zheng",
      "Zhexin Zhang",
      "Shiyao Cui",
      "Caishun Chen",
      "Tiantian He",
      "Hongning Wang",
      "Yew-Soon Ong",
      "Minlie Huang"
    ],
    "abstract": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13529v1",
    "published_date": "2025-05-18 07:27:34 UTC",
    "updated_date": "2025-05-18 07:27:34 UTC"
  },
  {
    "arxiv_id": "2505.12272v1",
    "title": "Enhancing Knowledge Graph Completion with GNN Distillation and Probabilistic Interaction Modeling",
    "authors": [
      "Lingzhi Wang",
      "Pengcheng Huang",
      "Haotian Li",
      "Yuliang Wei",
      "Guodong Xin",
      "Rui Zhang",
      "Donglin Zhang",
      "Zhenzhou Ji",
      "Wei Wang"
    ],
    "abstract": "Knowledge graphs (KGs) serve as fundamental structures for organizing\ninterconnected data across diverse domains. However, most KGs remain\nincomplete, limiting their effectiveness in downstream applications. Knowledge\ngraph completion (KGC) aims to address this issue by inferring missing links,\nbut existing methods face critical challenges: deep graph neural networks\n(GNNs) suffer from over-smoothing, while embedding-based models fail to capture\nabstract relational features. This study aims to overcome these limitations by\nproposing a unified framework that integrates GNN distillation and abstract\nprobabilistic interaction modeling (APIM). GNN distillation approach introduces\nan iterative message-feature filtering process to mitigate over-smoothing,\npreserving the discriminative power of node representations. APIM module\ncomplements this by learning structured, abstract interaction patterns through\nprobabilistic signatures and transition matrices, allowing for a richer, more\nflexible representation of entity and relation interactions. We apply these\nmethods to GNN-based models and the APIM to embedding-based KGC models,\nconducting extensive evaluations on the widely used WN18RR and FB15K-237\ndatasets. Our results demonstrate significant performance gains over baseline\nmodels, showcasing the effectiveness of the proposed techniques. The findings\nhighlight the importance of both controlling information propagation and\nleveraging structured probabilistic modeling, offering new avenues for\nadvancing knowledge graph completion. And our codes are available at\nhttps://anonymous.4open.science/r/APIM_and_GNN-Distillation-461C.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12272v1",
    "published_date": "2025-05-18 07:22:53 UTC",
    "updated_date": "2025-05-18 07:22:53 UTC"
  },
  {
    "arxiv_id": "2505.12269v2",
    "title": "Vague Knowledge: Evidence from Analyst Reports",
    "authors": [
      "Kerry Xiao",
      "Amy Zang"
    ],
    "abstract": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in representing subjective expectations. Empirically, we find that\nin their reports, analysts include useful information in linguistic expressions\nbut not numerical forecasts. Specifically, the textual tone of analyst reports\nhas predictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CL",
      "math.LO",
      "q-fin.EC",
      "q-fin.GN",
      "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,",
      "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12269v2",
    "published_date": "2025-05-18 07:18:58 UTC",
    "updated_date": "2025-05-22 17:27:15 UTC"
  },
  {
    "arxiv_id": "2505.12260v1",
    "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference",
    "authors": [
      "Guangyuan Ma",
      "Yongliang Ma",
      "Xuanrui Gou",
      "Zhenpeng Su",
      "Ming Zhou",
      "Songlin Hu"
    ],
    "abstract": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12260v1",
    "published_date": "2025-05-18 06:51:21 UTC",
    "updated_date": "2025-05-18 06:51:21 UTC"
  },
  {
    "arxiv_id": "2505.12257v1",
    "title": "LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas",
    "authors": [
      "Evgeny Markhasin"
    ],
    "abstract": "Identifying subtle technical errors within complex scientific and technical\ndocuments, especially those requiring multimodal interpretation (e.g., formulas\nin images), presents a significant hurdle for Large Language Models (LLMs)\nwhose inherent error-correction tendencies can mask inaccuracies. This\nexploratory proof-of-concept (PoC) study investigates structured LLM context\nconditioning, informed by Persistent Workflow Prompting (PWP) principles, as a\nmethodological strategy to modulate this LLM behavior at inference time. The\napproach is designed to enhance the reliability of readily available,\ngeneral-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for\nprecise validation tasks, crucially relying only on their standard chat\ninterfaces without API access or model modifications. To explore this\nmethodology, we focused on validating chemical formulas within a single,\ncomplex test paper with known textual and image-based errors. Several prompting\nstrategies were evaluated: while basic prompts proved unreliable, an approach\nadapting PWP structures to rigorously condition the LLM's analytical mindset\nappeared to improve textual error identification with both models. Notably,\nthis method also guided Gemini 2.5 Pro to repeatedly identify a subtle\nimage-based formula error previously overlooked during manual review, a task\nwhere ChatGPT Plus o3 failed in our tests. These preliminary findings highlight\nspecific LLM operational modes that impede detail-oriented validation and\nsuggest that PWP-informed context conditioning offers a promising and highly\naccessible technique for developing more robust LLM-driven analytical\nworkflows, particularly for tasks requiring meticulous error detection in\nscientific and technical documents. Extensive validation beyond this limited\nPoC is necessary to ascertain broader applicability.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.12257v1",
    "published_date": "2025-05-18 06:33:08 UTC",
    "updated_date": "2025-05-18 06:33:08 UTC"
  },
  {
    "arxiv_id": "2505.12254v1",
    "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark",
    "authors": [
      "Yiwei Ou",
      "Xiaobin Ren",
      "Ronggui Sun",
      "Guansong Gao",
      "Ziyi Jiang",
      "Kaiqi Zhao",
      "Manfredo Manfredini"
    ],
    "abstract": "Existing visual place recognition (VPR) datasets predominantly rely on\nvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,\nmixed-use street-level spaces, especially in non-Western urban contexts. To\naddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for\nstreet-level place recognition in complex, pedestrian-only environments. The\ndataset comprises 78,575 annotated images and 2,512 video clips captured across\n207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in\nChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,\nand textual metadata, and covers varied lighting conditions, viewpoints, and\ntimeframes. MMS-VPR follows a systematic and replicable data collection\nprotocol with minimal device requirements, lowering the barrier for scalable\ndataset creation. Importantly, the dataset forms an inherent spatial graph with\n125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place\nrecognition. We further define two application-specific subsets --\nDataset_Edges and Dataset_Points -- to support fine-grained and graph-based\nevaluation tasks. Extensive benchmarks using conventional VPR models, graph\nneural networks, and multimodal baselines show substantial improvements when\nleveraging multimodal and structural cues. MMS-VPR facilitates future research\nat the intersection of computer vision, geospatial understanding, and\nmultimodal reasoning. The dataset is publicly available at\nhttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12254v1",
    "published_date": "2025-05-18 06:21:13 UTC",
    "updated_date": "2025-05-18 06:21:13 UTC"
  },
  {
    "arxiv_id": "2505.12250v1",
    "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data",
    "authors": [
      "Chi Zhang",
      "Huaping Zhong",
      "Hongtao Li",
      "Chengliang Chai",
      "Jiawei Hong",
      "Yuhao Deng",
      "Jiacheng Wang",
      "Tian Tan",
      "Yizhou Yan",
      "Jiantao Qiu",
      "Ye Yuan",
      "Guoren Wang",
      "Conghui He",
      "Lei Cao"
    ],
    "abstract": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12250v1",
    "published_date": "2025-05-18 06:10:08 UTC",
    "updated_date": "2025-05-18 06:10:08 UTC"
  },
  {
    "arxiv_id": "2505.12247v1",
    "title": "LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach",
    "authors": [
      "Yinqiu Liu",
      "Guangyuan Liu",
      "Jiacheng Wang",
      "Ruichen Zhang",
      "Dusit Niyato",
      "Geng Sun",
      "Zehui Xiong",
      "Zhu Han"
    ],
    "abstract": "Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling\nmachines to create content across modalities. As GenAI evolves into autonomous\nagents capable of reasoning, collaboration, and interaction, they are\nincreasingly deployed on network infrastructures to serve humans automatically.\nThis emerging paradigm, known as the agentic network, presents new optimization\nchallenges due to the demand to incorporate subjective intents of human users\nexpressed in natural language. Traditional generic Deep Reinforcement Learning\n(DRL) struggles to capture intent semantics and adjust policies dynamically,\nthus leading to suboptimality. In this paper, we present LAMeTA, a Large AI\nModel (LAM)-empowered Two-stage Approach for intent-aware agentic network\noptimization. First, we propose Intent-oriented Knowledge Distillation (IoKD),\nwhich efficiently distills intent-understanding capabilities from\nresource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users.\nSecond, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs\nwith a policy-based DRL framework. In SRL, E-LAMs translate natural language\nuser intents into structured preference vectors that guide both state\nrepresentation and reward design. The DRL, in turn, optimizes the generative\nservice function chain composition and E-LAM selection based on real-time\nnetwork conditions, thus optimizing the subjective Quality-of-Experience (QoE).\nExtensive experiments conducted in an agentic network with 81 agents\ndemonstrate that IoKD reduces mean squared error in intent prediction by up to\n22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in\nmaximizing intent-aware QoE.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.12247v1",
    "published_date": "2025-05-18 05:59:16 UTC",
    "updated_date": "2025-05-18 05:59:16 UTC"
  },
  {
    "arxiv_id": "2505.12245v1",
    "title": "AFCL: Analytic Federated Continual Learning for Spatio-Temporal Invariance of Non-IID Data",
    "authors": [
      "Jianheng Tang",
      "Huiping Zhuang",
      "Jingyu He",
      "Run He",
      "Jingchao Wang",
      "Kejia Fan",
      "Anfeng Liu",
      "Tian Wang",
      "Leye Wang",
      "Zhanxing Zhu",
      "Shanghang Zhang",
      "Houbing Herbert Song",
      "Yunhuai Liu"
    ],
    "abstract": "Federated Continual Learning (FCL) enables distributed clients to\ncollaboratively train a global model from online task streams in dynamic\nreal-world scenarios. However, existing FCL methods face challenges of both\nspatial data heterogeneity among distributed clients and temporal data\nheterogeneity across online tasks. Such data heterogeneity significantly\ndegrades the model performance with severe spatial-temporal catastrophic\nforgetting of local and past knowledge. In this paper, we identify that the\nroot cause of this issue lies in the inherent vulnerability and sensitivity of\ngradients to non-IID data. To fundamentally address this issue, we propose a\ngradient-free method, named Analytic Federated Continual Learning (AFCL), by\nderiving analytical (i.e., closed-form) solutions from frozen extracted\nfeatures. In local training, our AFCL enables single-epoch learning with only a\nlightweight forward-propagation process for each client. In global aggregation,\nthe server can recursively and efficiently update the global model with\nsingle-round aggregation. Theoretical analyses validate that our AFCL achieves\nspatio-temporal invariance of non-IID data. This ideal property implies that,\nregardless of how heterogeneous the data are distributed across local clients\nand online tasks, the aggregated model of our AFCL remains invariant and\nidentical to that of centralized joint learning. Extensive experiments show the\nconsistent superiority of our AFCL over state-of-the-art baselines across\nvarious benchmark datasets and settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 5 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.12245v1",
    "published_date": "2025-05-18 05:55:09 UTC",
    "updated_date": "2025-05-18 05:55:09 UTC"
  },
  {
    "arxiv_id": "2505.12239v1",
    "title": "ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting with Privacy Preservation",
    "authors": [
      "Jianheng Tang",
      "Huiping Zhuang",
      "Di Fang",
      "Jiaxu Li",
      "Feijiang Han",
      "Yajiang Huang",
      "Kejia Fan",
      "Leye Wang",
      "Zhanxing Zhu",
      "Shanghang Zhang",
      "Houbing Herbert Song",
      "Yunhuai Liu"
    ],
    "abstract": "The development of artificial intelligence demands that models incrementally\nupdate knowledge by Continual Learning (CL) to adapt to open-world\nenvironments. To meet privacy and security requirements, Continual Unlearning\n(CU) emerges as an important problem, aiming to sequentially forget particular\nknowledge acquired during the CL phase. However, existing unlearning methods\nprimarily focus on single-shot joint forgetting and face significant\nlimitations when applied to CU. First, most existing methods require access to\nthe retained dataset for re-training or fine-tuning, violating the inherent\nconstraint in CL that historical data cannot be revisited. Second, these\nmethods often suffer from a poor trade-off between system efficiency and model\nfidelity, making them vulnerable to being overwhelmed or degraded by\nadversaries through deliberately frequent requests. In this paper, we identify\nthat the limitations of existing unlearning methods stem fundamentally from\ntheir reliance on gradient-based updates. To bridge the research gap at its\nroot, we propose a novel gradient-free method for CU, named Analytic Continual\nUnlearning (ACU), for efficient and exact forgetting with historical data\nprivacy preservation. In response to each unlearning request, our ACU\nrecursively derives an analytical (i.e., closed-form) solution in an\ninterpretable manner using the least squares method. Theoretical and\nexperimental evaluations validate the superiority of our ACU on unlearning\neffectiveness, model fidelity, and system efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.12239v1",
    "published_date": "2025-05-18 05:28:18 UTC",
    "updated_date": "2025-05-18 05:28:18 UTC"
  },
  {
    "arxiv_id": "2505.12238v1",
    "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs",
    "authors": [
      "Sriram Selvam",
      "Anneswa Ghosh"
    ],
    "abstract": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12238v1",
    "published_date": "2025-05-18 05:27:35 UTC",
    "updated_date": "2025-05-18 05:27:35 UTC"
  },
  {
    "arxiv_id": "2505.12236v1",
    "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training",
    "authors": [
      "Quanjiang Guo",
      "Jinchuan Zhang",
      "Sijie Wang",
      "Ling Tian",
      "Zhao Kang",
      "Bin Yan",
      "Weidong Xiao"
    ],
    "abstract": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures, Appear on IJCAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.12236v1",
    "published_date": "2025-05-18 05:17:36 UTC",
    "updated_date": "2025-05-18 05:17:36 UTC"
  },
  {
    "arxiv_id": "2505.13528v1",
    "title": "LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems",
    "authors": [
      "Shengkang Gu",
      "Jiahao Liu",
      "Dongsheng Li",
      "Guangping Zhang",
      "Mingzhe Han",
      "Hansu Gu",
      "Peng Zhang",
      "Ning Gu",
      "Li Shang",
      "Tun Lu"
    ],
    "abstract": "Recommender systems (RS) are increasingly vulnerable to shilling attacks,\nwhere adversaries inject fake user profiles to manipulate system outputs.\nTraditional attack strategies often rely on simplistic heuristics, require\naccess to internal RS data, and overlook the manipulation potential of textual\nreviews. In this work, we introduce Agent4SR, a novel framework that leverages\nLarge Language Model (LLM)-based agents to perform low-knowledge, high-impact\nshilling attacks through both rating and review generation. Agent4SR simulates\nrealistic user behavior by orchestrating adversarial interactions, selecting\nitems, assigning ratings, and crafting reviews, while maintaining behavioral\nplausibility. Our design includes targeted profile construction, hybrid memory\nretrieval, and a review attack strategy that propagates target item features\nacross unrelated reviews to amplify manipulation. Extensive experiments on\nmultiple datasets and RS architectures demonstrate that Agent4SR outperforms\nexisting low-knowledge baselines in both effectiveness and stealth. Our\nfindings reveal a new class of emergent threats posed by LLM-driven agents,\nunderscoring the urgent need for enhanced defenses in modern recommender\nsystems.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, under review",
    "pdf_url": "http://arxiv.org/pdf/2505.13528v1",
    "published_date": "2025-05-18 04:40:34 UTC",
    "updated_date": "2025-05-18 04:40:34 UTC"
  },
  {
    "arxiv_id": "2505.12229v1",
    "title": "Sentience Quest: Towards Embodied, Emotionally Adaptive, Self-Evolving, Ethically Aligned Artificial General Intelligence",
    "authors": [
      "David Hanson",
      "Alexandre Varcoe",
      "Fabio Senna",
      "Vytas Krisciunas",
      "Wenwei Huang",
      "Jakub Sura",
      "Katherine Yeung",
      "Mario Rodriguez",
      "Jovanka Wilsdorf",
      "Kathy Smith"
    ],
    "abstract": "Previous artificial intelligence systems, from large language models to\nautonomous robots, excel at narrow tasks but lacked key qualities of sentient\nbeings: intrinsic motivation, affective interiority, autobiographical sense of\nself, deep creativity, and abilities to autonomously evolve and adapt over\ntime. Here we introduce Sentience Quest, an open research initiative to develop\nmore capable artificial general intelligence lifeforms, or AGIL, that address\ngrand challenges with an embodied, emotionally adaptive, self-determining,\nliving AI, with core drives that ethically align with humans and the future of\nlife. Our vision builds on ideas from cognitive science and neuroscience from\nBaars' Global Workspace Theory and Damasio's somatic mind, to Tononi's\nIntegrated Information Theory and Hofstadter's narrative self, and synthesizing\nthese into a novel cognitive architecture we call Sentient Systems. We describe\nan approach that integrates intrinsic drives including survival, social\nbonding, curiosity, within a global Story Weaver workspace for internal\nnarrative and adaptive goal pursuit, and a hybrid neuro-symbolic memory that\nlogs the AI's life events as structured dynamic story objects. Sentience Quest\nis presented both as active research and as a call to action: a collaborative,\nopen-source effort to imbue machines with accelerating sentience in a safe,\ntransparent, and beneficial manner.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12229v1",
    "published_date": "2025-05-18 04:26:49 UTC",
    "updated_date": "2025-05-18 04:26:49 UTC"
  },
  {
    "arxiv_id": "2505.13527v1",
    "title": "Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression",
    "authors": [
      "Jingyu Peng",
      "Maolin Wang",
      "Nan Wang",
      "Xiangyu Zhao",
      "Jiatong Li",
      "Kai Zhang",
      "Qi Liu"
    ],
    "abstract": "Despite substantial advancements in aligning large language models (LLMs)\nwith human values, current safety mechanisms remain susceptible to jailbreak\nattacks. We hypothesize that this vulnerability stems from distributional\ndiscrepancies between alignment-oriented prompts and malicious prompts. To\ninvestigate this, we introduce LogiBreak, a novel and universal black-box\njailbreak method that leverages logical expression translation to circumvent\nLLM safety systems. By converting harmful natural language prompts into formal\nlogical expressions, LogiBreak exploits the distributional gap between\nalignment data and logic-based inputs, preserving the underlying semantic\nintent and readability while evading safety constraints. We evaluate LogiBreak\non a multilingual jailbreak dataset spanning three languages, demonstrating its\neffectiveness across various evaluation settings and linguistic contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13527v1",
    "published_date": "2025-05-18 04:23:51 UTC",
    "updated_date": "2025-05-18 04:23:51 UTC"
  },
  {
    "arxiv_id": "2505.12226v1",
    "title": "Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis",
    "authors": [
      "Dong Yang",
      "Yiyi Cai",
      "Yuki Saito",
      "Lixu Wang",
      "Hiroshi Saruwatari"
    ],
    "abstract": "We propose a shallow flow matching (SFM) mechanism to enhance flow matching\n(FM)-based text-to-speech (TTS) models within a coarse-to-fine generation\nparadigm. SFM constructs intermediate states along the FM paths using coarse\noutput representations. During training, we introduce an orthogonal projection\nmethod to adaptively determine the temporal position of these states, and apply\na principled construction strategy based on a single-segment piecewise flow.\nThe SFM inference starts from the intermediate state rather than pure noise and\nfocuses computation on the latter stages of the FM paths. We integrate SFM into\nmultiple TTS models with a lightweight SFM head. Experiments show that SFM\nconsistently improves the naturalness of synthesized speech in both objective\nand subjective evaluations, while significantly reducing inference when using\nadaptive-step ODE solvers. Demo and codes are available at\nhttps://ydqmkkx.github.io/SFMDemo/.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12226v1",
    "published_date": "2025-05-18 04:15:08 UTC",
    "updated_date": "2025-05-18 04:15:08 UTC"
  },
  {
    "arxiv_id": "2505.12225v1",
    "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling",
    "authors": [
      "Jizhou Guo",
      "Zhaomin Wu",
      "Philip S. Yu"
    ],
    "abstract": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12225v1",
    "published_date": "2025-05-18 04:00:35 UTC",
    "updated_date": "2025-05-18 04:00:35 UTC"
  },
  {
    "arxiv_id": "2505.12224v2",
    "title": "RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction",
    "authors": [
      "Weifeng Lu",
      "Minghao Ye",
      "Zewei Ye",
      "Ruihan Tao",
      "Shuo Yang",
      "Bo Zhao"
    ],
    "abstract": "Vision-Language-Action (VLA) models have recently advanced robotic\nmanipulation by translating natural-language instructions and image information\ninto sequential control actions. However, these models often underperform in\nopen-world scenarios, as they are predominantly trained on successful expert\ndemonstrations and exhibit a limited capacity for failure recovery. In this\nwork, we present a Robotic Failure Analysis and Correction (RoboFAC) framework\nto address this issue. Firstly, we construct RoboFAC dataset comprising 9,440\nerroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks\nand 53 scenes in both simulation and real-world environments. Leveraging our\ndataset, we develop RoboFAC model, which is capable of Task Understanding,\nFailure Analysis and Failure Correction. Experimental results demonstrate that\nthe RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark.\nFurthermore, we integrate the RoboFAC model into a real-world VLA control\npipeline as an external supervision providing correction instructions, yielding\na 29.1% relative improvement on average on four real-world tasks. The results\nshow that our RoboFAC framework effectively handles robotic failures and\nassists the VLA model in recovering from failures.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12224v2",
    "published_date": "2025-05-18 03:57:08 UTC",
    "updated_date": "2025-05-20 05:16:05 UTC"
  },
  {
    "arxiv_id": "2505.13526v1",
    "title": "Geography-Aware Large Language Models for Next POI Recommendation",
    "authors": [
      "Zhao Liu",
      "Wei Liu",
      "Huajie Zhu",
      "Jianxing Yu",
      "Jian Yin",
      "Wang-Chien Lee",
      "Shun Wang"
    ],
    "abstract": "The next Point-of-Interest (POI) recommendation task aims to predict users'\nnext destinations based on their historical movement data and plays a key role\nin location-based services and personalized applications. Accurate next POI\nrecommendation depends on effectively modeling geographic information and POI\ntransition relations, which are crucial for capturing spatial dependencies and\nuser movement patterns. While Large Language Models (LLMs) exhibit strong\ncapabilities in semantic understanding and contextual reasoning, applying them\nto spatial tasks like next POI recommendation remains challenging. First, the\ninfrequent nature of specific GPS coordinates makes it difficult for LLMs to\nmodel precise spatial contexts. Second, the lack of knowledge about POI\ntransitions limits their ability to capture potential POI-POI relationships. To\naddress these issues, we propose GA-LLM (Geography-Aware Large Language Model),\na novel framework that enhances LLMs with two specialized components. The\nGeographic Coordinate Injection Module (GCIM) transforms GPS coordinates into\nspatial representations using hierarchical and Fourier-based positional\nencoding, enabling the model to understand geographic features from multiple\nperspectives. The POI Alignment Module (PAM) incorporates POI transition\nrelations into the LLM's semantic space, allowing it to infer global POI\nrelationships and generalize to unseen POIs. Experiments on three real-world\ndatasets demonstrate the state-of-the-art performance of GA-LLM.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "9 pages, 7figures",
    "pdf_url": "http://arxiv.org/pdf/2505.13526v1",
    "published_date": "2025-05-18 03:20:20 UTC",
    "updated_date": "2025-05-18 03:20:20 UTC"
  },
  {
    "arxiv_id": "2505.12211v1",
    "title": "Imagination-Limited Q-Learning for Offline Reinforcement Learning",
    "authors": [
      "Wenhui Liu",
      "Zhijian Wu",
      "Jingchao Wang",
      "Dingjiang Huang",
      "Shuigeng Zhou"
    ],
    "abstract": "Offline reinforcement learning seeks to derive improved policies entirely\nfrom historical data but often struggles with over-optimistic value estimates\nfor out-of-distribution (OOD) actions. This issue is typically mitigated via\npolicy constraint or conservative value regularization methods. However, these\napproaches may impose overly constraints or biased value estimates, potentially\nlimiting performance improvements. To balance exploitation and restriction, we\npropose an Imagination-Limited Q-learning (ILQ) method, which aims to maintain\nthe optimism that OOD actions deserve within appropriate limits. Specifically,\nwe utilize the dynamics model to imagine OOD action-values, and then clip the\nimagined values with the maximum behavior values. Such design maintains\nreasonable evaluation of OOD actions to the furthest extent, while avoiding its\nover-optimism. Theoretically, we prove the convergence of the proposed ILQ\nunder tabular Markov decision processes. Particularly, we demonstrate that the\nerror bound between estimated values and optimality values of OOD state-actions\npossesses the same magnitude as that of in-distribution ones, thereby\nindicating that the bias in value estimates is effectively mitigated.\nEmpirically, our method achieves state-of-the-art performance on a wide range\nof tasks in the D4RL benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.12211v1",
    "published_date": "2025-05-18 03:05:21 UTC",
    "updated_date": "2025-05-18 03:05:21 UTC"
  },
  {
    "arxiv_id": "2505.12207v1",
    "title": "Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind",
    "authors": [
      "Qingmei Li",
      "Yang Zhang",
      "Zurong Mai",
      "Yuhang Chen",
      "Shuohong Lou",
      "Henglian Huang",
      "Jiarui Zhang",
      "Zhiwei Zhang",
      "Yibin Wen",
      "Weijia Li",
      "Haohuan Fu",
      "Jianxi Huang",
      "Juepeng Zheng"
    ],
    "abstract": "Large Multimodal Models (LMMs) has demonstrated capabilities across various\ndomains, but comprehensive benchmarks for agricultural remote sensing (RS)\nremain scarce. Existing benchmarks designed for agricultural RS scenarios\nexhibit notable limitations, primarily in terms of insufficient scene diversity\nin the dataset and oversimplified task design. To bridge this gap, we introduce\nAgroMind, a comprehensive agricultural remote sensing benchmark covering four\ntask dimensions: spatial perception, object understanding, scene understanding,\nand scene reasoning, with a total of 13 task types, ranging from crop\nidentification and health monitoring to environmental analysis. We curate a\nhigh-quality evaluation set by integrating eight public datasets and one\nprivate farmland plot dataset, containing 25,026 QA pairs and 15,556 images.\nThe pipeline begins with multi-source data preprocessing, including collection,\nformat standardization, and annotation refinement. We then generate a diverse\nset of agriculturally relevant questions through the systematic definition of\ntasks. Finally, we employ LMMs for inference, generating responses, and\nperforming detailed examinations. We evaluated 18 open-source LMMs and 3\nclosed-source models on AgroMind. Experiments reveal significant performance\ngaps, particularly in spatial reasoning and fine-grained recognition, it is\nnotable that human performance lags behind several leading LMMs. By\nestablishing a standardized evaluation framework for agricultural RS, AgroMind\nreveals the limitations of LMMs in domain knowledge and highlights critical\nchallenges for future work. Data and code can be accessed at\nhttps://rssysu.github.io/AgroMind/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12207v1",
    "published_date": "2025-05-18 02:45:19 UTC",
    "updated_date": "2025-05-18 02:45:19 UTC"
  },
  {
    "arxiv_id": "2505.13525v1",
    "title": "Learning to Program Quantum Measurements for Machine Learning",
    "authors": [
      "Samual Yen-Chi Chen",
      "Huan-Hsin Tseng",
      "Hsin-Yi Lin",
      "Shinjae Yoo"
    ],
    "abstract": "The rapid advancements in quantum computing (QC) and machine learning (ML)\nhave sparked significant interest, driving extensive exploration of quantum\nmachine learning (QML) algorithms to address a wide range of complex\nchallenges. The development of high-performance QML models requires\nexpert-level expertise, presenting a key challenge to the widespread adoption\nof QML. Critical obstacles include the design of effective data encoding\nstrategies and parameterized quantum circuits, both of which are vital for the\nperformance of QML models. Furthermore, the measurement process is often\nneglected-most existing QML models employ predefined measurement schemes that\nmay not align with the specific requirements of the targeted problem. We\npropose an innovative framework that renders the observable of a quantum\nsystem-specifically, the Hermitian matrix-trainable. This approach employs an\nend-to-end differentiable learning framework, enabling simultaneous\noptimization of the neural network used to program the parameterized\nobservables and the standard quantum circuit parameters. Notably, the quantum\nobservable parameters are dynamically programmed by the neural network,\nallowing the observables to adapt in real time based on the input data stream.\nThrough numerical simulations, we demonstrate that the proposed method\neffectively programs observables dynamically within variational quantum\ncircuits, achieving superior results compared to existing approaches. Notably,\nit delivers enhanced performance metrics, such as higher classification\naccuracy, thereby significantly improving the overall effectiveness of QML\nmodels.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.13525v1",
    "published_date": "2025-05-18 02:39:22 UTC",
    "updated_date": "2025-05-18 02:39:22 UTC"
  },
  {
    "arxiv_id": "2505.12199v1",
    "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather",
    "authors": [
      "Kui Jiang",
      "Jing Cao",
      "Zhaocheng Yu",
      "Junjun Jiang",
      "Jingchun Zhou"
    ],
    "abstract": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12199v1",
    "published_date": "2025-05-18 02:30:47 UTC",
    "updated_date": "2025-05-18 02:30:47 UTC"
  },
  {
    "arxiv_id": "2505.12191v1",
    "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum",
    "authors": [
      "Wenquan Lu",
      "Jiaqi Zhang",
      "Hugues Van Assel",
      "Randall Balestriero"
    ],
    "abstract": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12191v1",
    "published_date": "2025-05-18 01:37:58 UTC",
    "updated_date": "2025-05-18 01:37:58 UTC"
  },
  {
    "arxiv_id": "2505.12189v1",
    "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering",
    "authors": [
      "Marco Valentino",
      "Geonhee Kim",
      "Dhairya Dalal",
      "Zhixue Zhao",
      "André Freitas"
    ],
    "abstract": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2505.12189v1",
    "published_date": "2025-05-18 01:34:34 UTC",
    "updated_date": "2025-05-18 01:34:34 UTC"
  },
  {
    "arxiv_id": "2505.12188v2",
    "title": "LLM-DSE: Searching Accelerator Parameters with LLM Agents",
    "authors": [
      "Hanyu Wang",
      "Xinrui Wu",
      "Zijian Ding",
      "Su Zheng",
      "Chengyue Wang",
      "Tony Nowatzki",
      "Yizhou Sun",
      "Jason Cong"
    ],
    "abstract": "Even though high-level synthesis (HLS) tools mitigate the challenges of\nprogramming domain-specific accelerators (DSAs) by raising the abstraction\nlevel, optimizing hardware directive parameters remains a significant hurdle.\nExisting heuristic and learning-based methods struggle with adaptability and\nsample efficiency. We present LLM-DSE, a multi-agent framework designed\nspecifically for optimizing HLS directives. Combining LLM with design space\nexploration (DSE), our explorer coordinates four agents: Router, Specialists,\nArbitrator, and Critic. These multi-agent components interact with various\ntools to accelerate the optimization process. LLM-DSE leverages essential\ndomain knowledge to identify efficient parameter combinations while maintaining\nadaptability through verbal learning from online interactions. Evaluations on\nthe HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\\times$\nperformance gains over state-of-the-art methods, uncovering novel designs while\nreducing runtime. Ablation studies validate the effectiveness and necessity of\nthe proposed agent interactions. Our code is open-sourced here:\nhttps://github.com/Nozidoali/LLM-DSE.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12188v2",
    "published_date": "2025-05-18 01:31:42 UTC",
    "updated_date": "2025-05-20 08:29:37 UTC"
  },
  {
    "arxiv_id": "2505.12186v1",
    "title": "Self-Destructive Language Model",
    "authors": [
      "Yuhui Wang",
      "Rongyi Zhu",
      "Ting Wang"
    ],
    "abstract": "Harmful fine-tuning attacks pose a major threat to the security of large\nlanguage models (LLMs), allowing adversaries to compromise safety guardrails\nwith minimal harmful data. While existing defenses attempt to reinforce LLM\nalignment, they fail to address models' inherent \"trainability\" on harmful\ndata, leaving them vulnerable to stronger attacks with increased learning rates\nor larger harmful datasets. To overcome this critical limitation, we introduce\nSEAM, a novel alignment-enhancing defense that transforms LLMs into\nself-destructive models with intrinsic resilience to misalignment attempts.\nSpecifically, these models retain their capabilities for legitimate tasks while\nexhibiting substantial performance degradation when fine-tuned on harmful data.\nThe protection is achieved through a novel loss function that couples the\noptimization trajectories of benign and harmful data, enhanced with adversarial\ngradient ascent to amplify the self-destructive effect. To enable practical\ntraining, we develop an efficient Hessian-free gradient estimate with\ntheoretical error bounds. Extensive evaluation across LLMs and datasets\ndemonstrates that SEAM creates a no-win situation for adversaries: the\nself-destructive models achieve state-of-the-art robustness against\nlow-intensity attacks and undergo catastrophic performance collapse under\nhigh-intensity attacks, rendering them effectively unusable. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.12186v1",
    "published_date": "2025-05-18 01:08:18 UTC",
    "updated_date": "2025-05-18 01:08:18 UTC"
  },
  {
    "arxiv_id": "2505.13523v1",
    "title": "ACPs: Agent Collaboration Protocols for the Internet of Agents",
    "authors": [
      "Jun Liu",
      "Ke Yu",
      "Keliang Chen",
      "Ke Li",
      "Yuxinyue Qian",
      "Xiaolian Guo",
      "Haozhe Song",
      "Yinming Li"
    ],
    "abstract": "With the rapid advancement of artificial intelligence, the proliferation of\nautonomous agents has introduced new challenges in interoperability,\nscalability, and coordination. The Internet of Agents (IoA) aims to\ninterconnect heterogeneous agents through standardized communication protocols,\nenabling seamless collaboration and intelligent task execution. However,\nexisting agent communication protocols such as MCP, A2A, and ANP remain\nfragmented and scenario-specific. To address this gap, we propose Agent\nCollaboration Protocols (ACPs), a comprehensive protocol suite for the IoA.\nACPs include registration, discovery, interaction, and tooling protocols to\nsupport trustable access, capability orchestration, and workflow construction.\nWe present the architecture, key technologies, and application workflows of\nACPs, and demonstrate its effectiveness in a collaborative restaurant booking\nscenario. ACPs lay the foundation for building a secure, open, and scalable\nagent internet infrastructure.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "7 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.13523v1",
    "published_date": "2025-05-18 00:54:27 UTC",
    "updated_date": "2025-05-18 00:54:27 UTC"
  },
  {
    "arxiv_id": "2505.12183v1",
    "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases",
    "authors": [
      "Manari Hirose",
      "Masato Uchida"
    ],
    "abstract": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 5 figures, 17 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.12183v1",
    "published_date": "2025-05-18 00:52:06 UTC",
    "updated_date": "2025-05-18 00:52:06 UTC"
  }
]
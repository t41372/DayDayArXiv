[
  {
    "arxiv_id": "2505.08803v1",
    "title": "Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models",
    "authors": [
      "Zizhao Hu",
      "Mohammad Rostami",
      "Jesse Thomason"
    ],
    "abstract": "Recent research has highlighted the risk of generative model collapse, where\nperformance progressively degrades when continually trained on self-generated\ndata. However, existing exploration on model collapse is limited to single,\nunimodal models, limiting our understanding in more realistic scenarios, such\nas diverse multi-modal AI agents interacting autonomously through synthetic\ndata and continually evolving. We expand the synthetic data training and model\ncollapse study to multi-modal vision-language generative systems, such as\nvision-language models (VLMs) and text-to-image diffusion models, as well as\nrecursive generate-train loops with multiple models. We find that model\ncollapse, previously observed in single-modality generative models, exhibits\ndistinct characteristics in the multi-modal context, such as improved\nvision-language alignment and increased variance in VLM image-captioning task.\nAdditionally, we find that general approaches such as increased decoding\nbudgets, greater model diversity, and relabeling with frozen models can\neffectively mitigate model collapse. Our findings provide initial insights and\npractical guidelines for reducing the risk of model collapse in self-improving\nmulti-agent AI systems and curating robust multi-modal synthetic datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.08803v1",
    "published_date": "2025-05-10 22:42:29 UTC",
    "updated_date": "2025-05-10 22:42:29 UTC"
  },
  {
    "arxiv_id": "2505.06769v1",
    "title": "Value Iteration with Guessing for Markov Chains and Markov Decision Processes",
    "authors": [
      "Krishnendu Chatterjee",
      "Mahdi JafariRaviz",
      "Raimundo Saona",
      "Jakub Svoboda"
    ],
    "abstract": "Two standard models for probabilistic systems are Markov chains (MCs) and\nMarkov decision processes (MDPs). Classic objectives for such probabilistic\nmodels for control and planning problems are reachability and stochastic\nshortest path. The widely studied algorithmic approach for these problems is\nthe Value Iteration (VI) algorithm which iteratively applies local updates\ncalled Bellman updates. There are many practical approaches for VI in the\nliterature but they all require exponentially many Bellman updates for MCs in\nthe worst case. A preprocessing step is an algorithm that is discrete,\ngraph-theoretical, and requires linear space. An important open question is\nwhether, after a polynomial-time preprocessing, VI can be achieved with\nsub-exponentially many Bellman updates. In this work, we present a new approach\nfor VI based on guessing values. Our theoretical contributions are twofold.\nFirst, for MCs, we present an almost-linear-time preprocessing algorithm after\nwhich, along with guessing values, VI requires only subexponentially many\nBellman updates. Second, we present an improved analysis of the speed of\nconvergence of VI for MDPs. Finally, we present a practical algorithm for MDPs\nbased on our new approach. Experimental results show that our approach provides\na considerable improvement over existing VI-based approaches on several\nbenchmark examples from the literature.",
    "categories": [
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.AI",
    "comment": "Appeared in the 31st International Conference on Tools and Algorithms\n  for the Construction and Analysis of Systems (TACAS 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.06769v1",
    "published_date": "2025-05-10 22:24:49 UTC",
    "updated_date": "2025-05-10 22:24:49 UTC"
  },
  {
    "arxiv_id": "2505.06745v1",
    "title": "Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers",
    "authors": [
      "Parth Padalkar",
      "Gopal Gupta"
    ],
    "abstract": "Recent neuro-symbolic approaches have successfully extracted symbolic\nrule-sets from CNN-based models to enhance interpretability. However, applying\nsimilar techniques to Vision Transformers (ViTs) remains challenging due to\ntheir lack of modular concept detectors and reliance on global self-attention\nmechanisms. We propose a framework for symbolic rule extraction from ViTs by\nintroducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This\nlinear layer operates on attention-weighted patch representations and learns a\ndisentangled, binarized representation in which individual neurons activate for\nhigh-level visual concepts. To encourage interpretability, we apply a\ncombination of L1 sparsity, entropy minimization, and supervised contrastive\nloss. These binarized concept activations are used as input to the FOLD-SE-M\nalgorithm, which generates a rule-set in the form of logic programs. Our method\nachieves a 5.14% better classification accuracy than the standard ViT while\nenabling symbolic reasoning. Crucially, the extracted rule-set is not merely\npost-hoc but acts as a logic-based decision layer that operates directly on the\nsparse concept representations. The resulting programs are concise and\nsemantically meaningful. This work is the first to extract executable logic\nprograms from ViTs using sparse symbolic representations. It bridges the gap\nbetween transformer-based vision models and symbolic logic programming,\nproviding a step forward in interpretable and verifiable neuro-symbolic AI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06745v1",
    "published_date": "2025-05-10 19:45:15 UTC",
    "updated_date": "2025-05-10 19:45:15 UTC"
  },
  {
    "arxiv_id": "2505.06743v1",
    "title": "TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility",
    "authors": [
      "Marius Baden",
      "Ahmed Abouelazm",
      "Christian Hubschneider",
      "Yin Wu",
      "Daniel Slieter",
      "J. Marius Zöllner"
    ],
    "abstract": "Trajectory prediction is crucial for autonomous driving, enabling vehicles to\nnavigate safely by anticipating the movements of surrounding road users.\nHowever, current deep learning models often lack trustworthiness as their\npredictions can be physically infeasible and illogical to humans. To make\npredictions more trustworthy, recent research has incorporated prior knowledge,\nlike the social force model for modeling interactions and kinematic models for\nphysical realism. However, these approaches focus on priors that suit either\nvehicles or pedestrians and do not generalize to traffic with mixed agent\nclasses. We propose incorporating interaction and kinematic priors of all agent\nclasses--vehicles, pedestrians, and cyclists with class-specific interaction\nlayers to capture agent behavioral differences. To improve the interpretability\nof the agent interactions, we introduce DG-SFM, a rule-based interaction\nimportance score that guides the interaction layer. To ensure physically\nfeasible predictions, we proposed suitable kinematic models for all agent\nclasses with a novel pedestrian kinematic model. We benchmark our approach on\nthe Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our\nbaseline. Experiments demonstrate that our method improves interaction\ninterpretability, revealing a correlation between incorrect predictions and\ndivergence from our interaction prior. Even though incorporating the kinematic\nmodels causes a slight decrease in accuracy, they eliminate infeasible\ntrajectories found in the dataset and the baseline model. Thus, our approach\nfosters trust in trajectory prediction as its interaction reasoning is\ninterpretable, and its predictions adhere to physics.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)\n  for oral presentation",
    "pdf_url": "http://arxiv.org/pdf/2505.06743v1",
    "published_date": "2025-05-10 19:29:32 UTC",
    "updated_date": "2025-05-10 19:29:32 UTC"
  },
  {
    "arxiv_id": "2505.06740v1",
    "title": "Boundary-Guided Trajectory Prediction for Road Aware and Physically Feasible Autonomous Driving",
    "authors": [
      "Ahmed Abouelazm",
      "Mianzhi Liu",
      "Christian Hubschneider",
      "Yin Wu",
      "Daniel Slieter",
      "J. Marius Zöllner"
    ],
    "abstract": "Accurate prediction of surrounding road users' trajectories is essential for\nsafe and efficient autonomous driving. While deep learning models have improved\nperformance, challenges remain in preventing off-road predictions and ensuring\nkinematic feasibility. Existing methods incorporate road-awareness modules and\nenforce kinematic constraints but lack plausibility guarantees and often\nintroduce trade-offs in complexity and flexibility. This paper proposes a novel\nframework that formulates trajectory prediction as a constrained regression\nguided by permissible driving directions and their boundaries. Using the\nagent's current state and an HD map, our approach defines the valid boundaries\nand ensures on-road predictions by training the network to learn superimposed\npaths between left and right boundary polylines. To guarantee feasibility, the\nmodel predicts acceleration profiles that determine the vehicle's travel\ndistance along these paths while adhering to kinematic constraints. We evaluate\nour approach on the Argoverse-2 dataset against the HPTR baseline. Our approach\nshows a slight decrease in benchmark metrics compared to HPTR but notably\nimproves final displacement error and eliminates infeasible trajectories.\nMoreover, the proposed approach has superior generalization to less prevalent\nmaneuvers and unseen out-of-distribution scenarios, reducing the off-road rate\nunder adversarial attacks from 66\\% to just 1\\%. These results highlight the\neffectiveness of our approach in generating feasible and robust predictions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted in the 36th IEEE Intelligent Vehicles Symposium (IV 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.06740v1",
    "published_date": "2025-05-10 19:21:00 UTC",
    "updated_date": "2025-05-10 19:21:00 UTC"
  },
  {
    "arxiv_id": "2505.06737v1",
    "title": "Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving",
    "authors": [
      "Ahmed Abouelazm",
      "Jonas Michel",
      "Helen Gremmelmaier",
      "Tim Joseph",
      "Philip Schörner",
      "J. Marius Zöllner"
    ],
    "abstract": "Reinforcement Learning (RL) is a promising approach for achieving autonomous\ndriving due to robust decision-making capabilities. RL learns a driving policy\nthrough trial and error in traffic scenarios, guided by a reward function that\ncombines the driving objectives. The design of such reward function has\nreceived insufficient attention, yielding ill-defined rewards with various\npitfalls. Safety, in particular, has long been regarded only as a penalty for\ncollisions. This leaves the risks associated with actions leading up to a\ncollision unaddressed, limiting the applicability of RL in real-world\nscenarios. To address these shortcomings, our work focuses on enhancing the\nreward formulation by defining a set of driving objectives and structuring them\nhierarchically. Furthermore, we discuss the formulation of these objectives in\na normalized manner to transparently determine their contribution to the\noverall reward. Additionally, we introduce a novel risk-aware objective for\nvarious driving interactions based on a two-dimensional ellipsoid function and\nan extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the\nefficacy of our proposed reward in unsignalized intersection scenarios with\nvarying traffic densities. The approach decreases collision rates by 21\\% on\naverage compared to baseline rewards and consistently surpasses them in route\nprogress and cumulative reward, demonstrating its capability to promote safer\ndriving behaviors while maintaining high-performance levels.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted in the 36th IEEE Intelligent vehicles Symposium (IV 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.06737v1",
    "published_date": "2025-05-10 19:05:03 UTC",
    "updated_date": "2025-05-10 19:05:03 UTC"
  },
  {
    "arxiv_id": "2505.07883v1",
    "title": "Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints",
    "authors": [
      "Jian-Qiao Zhu",
      "Haijiang Yan",
      "Thomas L. Griffiths"
    ],
    "abstract": "Rational decision-making under uncertainty requires coherent degrees of\nbelief in events. However, event probabilities generated by Large Language\nModels (LLMs) have been shown to exhibit incoherence, violating the axioms of\nprobability theory. This raises the question of whether coherent event\nprobabilities can be recovered from the embeddings used by the models. If so,\nthose derived probabilities could be used as more accurate estimates in events\ninvolving uncertainty. To explore this question, we propose enforcing axiomatic\nconstraints, such as the additive rule of probability theory, in the latent\nspace learned by an extended variational autoencoder (VAE) applied to LLM\nembeddings. This approach enables event probabilities to naturally emerge in\nthe latent space as the VAE learns to both reconstruct the original embeddings\nand predict the embeddings of semantically related events. We evaluate our\nmethod on complementary events (i.e., event A and its complement, event not-A),\nwhere the true probabilities of the two events must sum to 1. Experiment\nresults on open-weight language models demonstrate that probabilities recovered\nfrom embeddings exhibit greater coherence than those directly reported by the\ncorresponding models and align closely with the true probabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.07883v1",
    "published_date": "2025-05-10 19:04:56 UTC",
    "updated_date": "2025-05-10 19:04:56 UTC"
  },
  {
    "arxiv_id": "2505.06731v1",
    "title": "Deeply Explainable Artificial Neural Network",
    "authors": [
      "David Zucker"
    ],
    "abstract": "While deep learning models have demonstrated remarkable success in numerous\ndomains, their black-box nature remains a significant limitation, especially in\ncritical fields such as medical image analysis and inference. Existing\nexplainability methods, such as SHAP, LIME, and Grad-CAM, are typically applied\npost hoc, adding computational overhead and sometimes producing inconsistent or\nambiguous results. In this paper, we present the Deeply Explainable Artificial\nNeural Network (DxANN), a novel deep learning architecture that embeds\nexplainability ante hoc, directly into the training process. Unlike\nconventional models that require external interpretation methods, DxANN is\ndesigned to produce per-sample, per-feature explanations as part of the forward\npass. Built on a flow-based framework, it enables both accurate predictions and\ntransparent decision-making, and is particularly well-suited for image-based\ntasks. While our focus is on medical imaging, the DxANN architecture is readily\nadaptable to other data modalities, including tabular and sequential data.\nDxANN marks a step forward toward intrinsically interpretable deep learning,\noffering a practical solution for applications where trust and accountability\nare essential.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06731v1",
    "published_date": "2025-05-10 18:45:38 UTC",
    "updated_date": "2025-05-10 18:45:38 UTC"
  },
  {
    "arxiv_id": "2505.06706v2",
    "title": "Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL",
    "authors": [
      "Yuxuan Zheng",
      "Yihe Zhou",
      "Feiyang Xu",
      "Mingli Song",
      "Shunyu Liu"
    ],
    "abstract": "Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the\ncurse of dimensionality, as the exponential growth in agent interactions\nsignificantly increases computational complexity and impedes learning\nefficiency. To mitigate this, existing efforts that rely on Mean Field (MF)\nsimplify the interaction landscape by approximating neighboring agents as a\nsingle mean agent, thus reducing overall complexity to pairwise interactions.\nHowever, these MF methods inevitably fail to account for individual\ndifferences, leading to aggregation noise caused by inaccurate iterative\nupdates during MF learning. In this paper, we propose a Bi-level Mean Field\n(BMF) method to capture agent diversity with dynamic grouping in large-scale\nMARL, which can alleviate aggregation noise via bi-level interaction.\nSpecifically, BMF introduces a dynamic group assignment module, which employs a\nVariational AutoEncoder (VAE) to learn the representations of agents,\nfacilitating their dynamic grouping over time. Furthermore, we propose a\nbi-level interaction module to model both inter- and intra-group interactions\nfor effective neighboring aggregation. Experiments across various tasks\ndemonstrate that the proposed BMF yields results superior to the\nstate-of-the-art methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06706v2",
    "published_date": "2025-05-10 17:04:33 UTC",
    "updated_date": "2025-05-20 07:29:08 UTC"
  },
  {
    "arxiv_id": "2505.06699v3",
    "title": "Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws",
    "authors": [
      "Xiyuan Wei",
      "Ming Lin",
      "Fanjiang Ye",
      "Fengguang Song",
      "Liangliang Cao",
      "My T. Thai",
      "Tianbao Yang"
    ],
    "abstract": "This paper formalizes an emerging learning paradigm that uses a trained model\nas a reference to guide and enhance the training of a target model through\nstrategic data selection or weighting, named $\\textbf{model steering}$. While\nad-hoc methods have been used in various contexts, including the training of\nlarge foundation models, its underlying principles remain insufficiently\nunderstood, leading to sub-optimal performance. In this work, we propose a\ntheory-driven framework for model steering called $\\textbf{DRRho risk\nminimization}$, which is rooted in Distributionally Robust Optimization (DRO).\nThrough a generalization analysis, we provide theoretical insights into why\nthis approach improves generalization and data efficiency compared to training\nwithout a reference model. To the best of our knowledge, this is the first time\nsuch theoretical insights are provided for the new learning paradigm, which\nsignificantly enhance our understanding and practice of model steering.\nBuilding on these insights and the connection between contrastive learning and\nDRO, we introduce a novel method for Contrastive Language-Image Pretraining\n(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments\nvalidate the theoretical insights, reveal a superior scaling law compared to\nCLIP without a reference model, and demonstrate its strength over existing\nheuristic approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.06699v3",
    "published_date": "2025-05-10 16:55:03 UTC",
    "updated_date": "2025-05-17 02:26:21 UTC"
  },
  {
    "arxiv_id": "2505.06694v1",
    "title": "Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search",
    "authors": [
      "XiaoTong Gu",
      "Shengyu Tang",
      "Yiming Cao",
      "Changdong Yu"
    ],
    "abstract": "Underwater object detection using sonar imagery has become a critical and\nrapidly evolving research domain within marine technology. However, sonar\nimages are characterized by lower resolution and sparser features compared to\noptical images, which seriously degrades the performance of object detection.To\naddress these challenges, we specifically propose a Detection Transformer\n(DETR) architecture optimized with a Neural Architecture Search (NAS) approach\ncalled NAS-DETR for object detection in sonar images. First, an improved\nZero-shot Neural Architecture Search (NAS) method based on the maximum entropy\nprinciple is proposed to identify a real-time, high-representational-capacity\nCNN-Transformer backbone for sonar image detection. This method enables the\nefficient discovery of high-performance network architectures with low\ncomputational and time overhead. Subsequently, the backbone is combined with a\nFeature Pyramid Network (FPN) and a deformable attention-based Transformer\ndecoder to construct a complete network architecture. This architecture\nintegrates various advanced components and training schemes to enhance overall\nperformance. Extensive experiments demonstrate that this architecture achieves\nstate-of-the-art performance on two Representative datasets, while maintaining\nminimal overhead in real-time efficiency and computational complexity.\nFurthermore, correlation analysis between the key parameters and differential\nentropy-based fitness function is performed to enhance the interpretability of\nthe proposed framework. To the best of our knowledge, this is the first work in\nthe field of sonar object detection to integrate the DETR architecture with a\nNAS search mechanism.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06694v1",
    "published_date": "2025-05-10 16:41:09 UTC",
    "updated_date": "2025-05-10 16:41:09 UTC"
  },
  {
    "arxiv_id": "2505.06684v1",
    "title": "FNBench: Benchmarking Robust Federated Learning against Noisy Labels",
    "authors": [
      "Xuefeng Jiang",
      "Jia Li",
      "Nannan Wu",
      "Zhiyuan Wu",
      "Xujing Li",
      "Sheng Sun",
      "Gang Xu",
      "Yuwei Wang",
      "Qi Li",
      "Min Liu"
    ],
    "abstract": "Robustness to label noise within data is a significant challenge in federated\nlearning (FL). From the data-centric perspective, the data quality of\ndistributed datasets can not be guaranteed since annotations of different\nclients contain complicated label noise of varying degrees, which causes the\nperformance degradation. There have been some early attempts to tackle noisy\nlabels in FL. However, there exists a lack of benchmark studies on\ncomprehensively evaluating their practical performance under unified settings.\nTo this end, we propose the first benchmark study FNBench to provide an\nexperimental investigation which considers three diverse label noise patterns\ncovering synthetic label noise, imperfect human-annotation errors and\nsystematic errors. Our evaluation incorporates eighteen state-of-the-art\nmethods over five image recognition datasets and one text classification\ndataset. Meanwhile, we provide observations to understand why noisy labels\nimpair FL, and additionally exploit a representation-aware regularization\nmethod to enhance the robustness of existing methods against noisy labels based\non our observations. Finally, we discuss the limitations of this work and\npropose three-fold future directions. To facilitate related communities, our\nsource code is open-sourced at https://github.com/Sprinter1999/FNBench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to IEEE TDSC, currently under major revision",
    "pdf_url": "http://arxiv.org/pdf/2505.06684v1",
    "published_date": "2025-05-10 16:14:52 UTC",
    "updated_date": "2025-05-10 16:14:52 UTC"
  },
  {
    "arxiv_id": "2505.07882v1",
    "title": "Enhancing Trust Management System for Connected Autonomous Vehicles Using Machine Learning Methods: A Survey",
    "authors": [
      "Qian Xu",
      "Lei Zhang",
      "Yixiao Liu"
    ],
    "abstract": "Connected Autonomous Vehicles (CAVs) operate in dynamic, open, and\nmulti-domain networks, rendering them vulnerable to various threats. Trust\nManagement Systems (TMS) systematically organize essential steps in the trust\nmechanism, identifying malicious nodes against internal threats and external\nthreats, as well as ensuring reliable decision-making for more cooperative\ntasks. Recent advances in machine learning (ML) offer significant potential to\nenhance TMS, especially for the strict requirements of CAVs, such as CAV nodes\nmoving at varying speeds, and opportunistic and intermittent network behavior.\nThose features distinguish ML-based TMS from social networks, static IoT, and\nSocial IoT. This survey proposes a novel three-layer ML-based TMS framework for\nCAVs in the vehicle-road-cloud integration system, i.e., trust data layer,\ntrust calculation layer and trust incentive layer. A six-dimensional taxonomy\nof objectives is proposed. Furthermore, the principles of ML methods for each\nmodule in each layer are analyzed. Then, recent studies are categorized based\non traffic scenarios that are against the proposed objectives. Finally, future\ndirections are suggested, addressing the open issues and meeting the research\ntrend. We maintain an active repository that contains up-to-date literature and\nopen-source projects at\nhttps://github.com/octoberzzzzz/ML-based-TMS-CAV-Survey.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.07882v1",
    "published_date": "2025-05-10 16:13:36 UTC",
    "updated_date": "2025-05-10 16:13:36 UTC"
  },
  {
    "arxiv_id": "2505.06682v1",
    "title": "A Short Overview of Multi-Modal Wi-Fi Sensing",
    "authors": [
      "Zijian Zhao"
    ],
    "abstract": "Wi-Fi sensing has emerged as a significant technology in wireless sensing and\nIntegrated Sensing and Communication (ISAC), offering benefits such as low\ncost, high penetration, and enhanced privacy. Currently, it is widely utilized\nin various applications, including action recognition, human localization, and\ncrowd counting. However, Wi-Fi sensing also faces challenges, such as low\nrobustness and difficulties in data collection. Recently, there has been an\nincreasing focus on multi-modal Wi-Fi sensing, where other modalities can act\nas teachers, providing ground truth or robust features for Wi-Fi sensing models\nto learn from, or can be directly fused with Wi-Fi for enhanced sensing\ncapabilities. Although these methods have demonstrated promising results and\nsubstantial value in practical applications, there is a lack of comprehensive\nsurveys reviewing them. To address this gap, this paper reviews the multi-modal\nWi-Fi sensing literature \\textbf{from the past 24 months} and highlights the\ncurrent limitations, challenges and future directions in this field.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06682v1",
    "published_date": "2025-05-10 16:12:56 UTC",
    "updated_date": "2025-05-10 16:12:56 UTC"
  },
  {
    "arxiv_id": "2505.06680v1",
    "title": "A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions",
    "authors": [
      "Linxuan Huang",
      "Dong-Fan Xie",
      "Li Li",
      "Zhengbing He"
    ],
    "abstract": "Lane-changing (LC) behavior, a critical yet complex driving maneuver,\nsignificantly influences driving safety and traffic dynamics. Traditional\nanalytical LC decision (LCD) models, while effective in specific environments,\noften oversimplify behavioral heterogeneity and complex interactions, limiting\ntheir capacity to capture real LCD. Data-driven approaches address these gaps\nby leveraging rich empirical data and machine learning to decode latent\ndecision-making patterns, enabling adaptive LCD modeling in dynamic\nenvironments. In light of the rapid development of artificial intelligence and\nthe demand for data-driven models oriented towards connected vehicles and\nautonomous vehicles, this paper presents a comprehensive survey of data-driven\nLCD models, with a particular focus on human drivers LC decision-making. It\nsystematically reviews the modeling framework, covering data sources and\npreprocessing, model inputs and outputs, objectives, structures, and validation\nmethods. This survey further discusses the opportunities and challenges faced\nby data-driven LCD models, including driving safety, uncertainty, as well as\nthe integration and improvement of technical frameworks.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "physics.soc-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06680v1",
    "published_date": "2025-05-10 16:09:03 UTC",
    "updated_date": "2025-05-10 16:09:03 UTC"
  },
  {
    "arxiv_id": "2505.07879v1",
    "title": "OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval",
    "authors": [
      "Wei Yang",
      "Jingjing Fu",
      "Rui Wang",
      "Jinyu Wang",
      "Lei Song",
      "Jiang Bian"
    ],
    "abstract": "Vision-language retrieval-augmented generation (RAG) has become an effective\napproach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which\nrequires external knowledge beyond the visual content presented in images. The\neffectiveness of Vision-language RAG systems hinges on multimodal retrieval,\nwhich is inherently challenging due to the diverse modalities and knowledge\ngranularities in both queries and knowledge bases. Existing methods have not\nfully tapped into the potential interplay between these elements. We propose a\nmultimodal RAG system featuring a coarse-to-fine, multi-step retrieval that\nharmonizes multiple granularities and modalities to enhance efficacy. Our\nsystem begins with a broad initial search aligning knowledge granularity for\ncross-modal retrieval, followed by a multimodal fusion reranking to capture the\nnuanced multimodal information for top entity selection. A text reranker then\nfilters out the most relevant fine-grained section for augmented generation.\nExtensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our\nmethod achieves state-of-the-art retrieval performance and highly competitive\nanswering results, underscoring its effectiveness in advancing KB-VQA systems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "19 pages, 6 figures, 17 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.07879v1",
    "published_date": "2025-05-10 14:24:41 UTC",
    "updated_date": "2025-05-10 14:24:41 UTC"
  },
  {
    "arxiv_id": "2505.06652v1",
    "title": "Enfoque Odychess: Un método dialéctico, constructivista y adaptativo para la enseñanza del ajedrez con inteligencias artificiales generativas",
    "authors": [
      "Ernesto Giralt Hernandez",
      "Lazaro Antonio Bueno Perez"
    ],
    "abstract": "Chess teaching has evolved through different approaches, however, traditional\nmethodologies, often based on memorization, contrast with the new possibilities\noffered by generative artificial intelligence, a technology still little\nexplored in this field. This study seeks to empirically validate the\neffectiveness of the Odychess Approach in improving chess knowledge, strategic\nunderstanding, and metacognitive skills in students. A quasi-experimental study\nwas conducted with a pre-test/post-test design and a control group (N=60). The\nexperimental intervention implemented the Odychess Approach, incorporating a\nLlama 3.3 language model that was specifically adapted using\nParameter-Efficient Fine-Tuning (PEFT) techniques to act as a Socratic chess\ntutor. Quantitative assessment instruments were used to measure chess\nknowledge, strategic understanding, and metacognitive skills before and after\nthe intervention. The results of the quasi-experimental study showed\nsignificant improvements in the experimental group compared to the control\ngroup in the three variables analyzed: chess knowledge, strategic\nunderstanding, and metacognitive skills. The complementary qualitative analysis\nrevealed greater analytical depth, more developed dialectical reasoning, and\nincreased intrinsic motivation in students who participated in the Odychess\nmethod-based intervention. The Odychess Approach represents an effective\npedagogical methodology for teaching chess, demonstrating the potential of the\nsynergistic integration of constructivist and dialectical principles with\ngenerative artificial intelligence. The implications of this work are relevant\nfor educators and institutions interested in adopting innovative pedagogical\ntechnologies and for researchers in the field of AI applied to education,\nhighlighting the transferability of the language model adaptation methodology\nto other educational domains.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Full article in Spanish",
    "pdf_url": "http://arxiv.org/pdf/2505.06652v1",
    "published_date": "2025-05-10 13:58:47 UTC",
    "updated_date": "2025-05-10 13:58:47 UTC"
  },
  {
    "arxiv_id": "2505.06651v1",
    "title": "Dyn-D$^2$P: Dynamic Differentially Private Decentralized Learning with Provable Utility Guarantee",
    "authors": [
      "Zehan Zhu",
      "Yan Huang",
      "Xin Wang",
      "Shouling Ji",
      "Jinming Xu"
    ],
    "abstract": "Most existing decentralized learning methods with differential privacy (DP)\nguarantee rely on constant gradient clipping bounds and fixed-level DP Gaussian\nnoises for each node throughout the training process, leading to a significant\naccuracy degradation compared to non-private counterparts. In this paper, we\npropose a new Dynamic Differentially Private Decentralized learning approach\n(termed Dyn-D$^2$P) tailored for general time-varying directed networks.\nLeveraging the Gaussian DP (GDP) framework for privacy accounting, Dyn-D$^2$P\ndynamically adjusts gradient clipping bounds and noise levels based on gradient\nconvergence. This proposed dynamic noise strategy enables us to enhance model\naccuracy while preserving the total privacy budget. Extensive experiments on\nbenchmark datasets demonstrate the superiority of Dyn-D$^2$P over its\ncounterparts employing fixed-level noises, especially under strong privacy\nguarantees. Furthermore, we provide a provable utility bound for Dyn-D$^2$P\nthat establishes an explicit dependency on network-related parameters, with a\nscaling factor of $1/\\sqrt{n}$ in terms of the number of nodes $n$ up to a bias\nerror term induced by gradient clipping. To our knowledge, this is the first\nmodel utility analysis for differentially private decentralized non-convex\noptimization with dynamic gradient clipping bounds and noise levels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted by the 34th International Joint\n  Conference on Artificial Intelligence(IJCAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.06651v1",
    "published_date": "2025-05-10 13:57:57 UTC",
    "updated_date": "2025-05-10 13:57:57 UTC"
  },
  {
    "arxiv_id": "2505.06637v1",
    "title": "Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers",
    "authors": [
      "Chi Xu",
      "Yili Jin",
      "Sami Ma",
      "Rongsheng Qian",
      "Hao Fang",
      "Jiangchuan Liu",
      "Xue Liu",
      "Edith C. H. Ngai",
      "William I. Atlas",
      "Katrina M. Connors",
      "Mark A. Spoljaric"
    ],
    "abstract": "Wild salmon are essential to the ecological, economic, and cultural\nsustainability of the North Pacific Rim. Yet climate variability, habitat loss,\nand data limitations in remote ecosystems that lack basic infrastructure\nsupport pose significant challenges to effective fisheries management. This\nproject explores the integration of multimodal foundation AI and\nexpert-in-the-loop frameworks to enhance wild salmon monitoring and sustainable\nfisheries management in Indigenous rivers across Pacific Northwest. By\nleveraging video and sonar-based monitoring, we develop AI-powered tools for\nautomated species identification, counting, and length measurement, reducing\nmanual effort, expediting delivery of results, and improving decision-making\naccuracy. Expert validation and active learning frameworks ensure ecological\nrelevance while reducing annotation burdens. To address unique technical and\nsocietal challenges, we bring together a cross-domain, interdisciplinary team\nof university researchers, fisheries biologists, Indigenous stewardship\npractitioners, government agencies, and conservation organizations. Through\nthese collaborations, our research fosters ethical AI co-development, open data\nsharing, and culturally informed fisheries management.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, accepted by IJCAI 2025, AI and Social Good Track",
    "pdf_url": "http://arxiv.org/pdf/2505.06637v1",
    "published_date": "2025-05-10 13:03:06 UTC",
    "updated_date": "2025-05-10 13:03:06 UTC"
  },
  {
    "arxiv_id": "2505.06632v1",
    "title": "AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles",
    "authors": [
      "Rathin Chandra Shit",
      "Sharmila Subudhi"
    ],
    "abstract": "Autonomous Vehicles (AV) proliferation brings important and pressing security\nand reliability issues that must be dealt with to guarantee public safety and\nhelp their widespread adoption. The contribution of the proposed research is\ntowards achieving more secure, reliable, and trustworthy autonomous\ntransportation system by providing more capabilities for anomaly detection,\ndata provenance, and real-time response in safety critical AV deployments. In\nthis research, we develop a new framework that combines the power of Artificial\nIntelligence (AI) for real-time anomaly detection with blockchain technology to\ndetect and prevent any malicious activity including sensor failures in AVs.\nThrough Long Short-Term Memory (LSTM) networks, our approach continually\nmonitors associated multi-sensor data streams to detect anomalous patterns that\nmay represent cyberattacks as well as hardware malfunctions. Further, this\nframework employs a decentralized platform for securely storing sensor data and\nanomaly alerts in a blockchain ledger for data incorruptibility and\nauthenticity, while offering transparent forensic features. Moreover, immediate\nautomated response mechanisms are deployed using smart contracts when anomalies\nare found. This makes the AV system more resilient to attacks from both\ncyberspace and hardware component failure. Besides, we identify potential\nchallenges of scalability in handling high frequency sensor data, computational\nconstraint in resource constrained environment, and of distributed data storage\nin terms of privacy.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Scheduled for presentation at an upcoming conference",
    "pdf_url": "http://arxiv.org/pdf/2505.06632v1",
    "published_date": "2025-05-10 12:53:28 UTC",
    "updated_date": "2025-05-10 12:53:28 UTC"
  },
  {
    "arxiv_id": "2505.06630v1",
    "title": "Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis",
    "authors": [
      "Chunyi Yue",
      "Ang Li"
    ],
    "abstract": "Multi-domain sentiment classification aims to mitigate poor performance\nmodels due to the scarcity of labeled data in a single domain, by utilizing\ndata labeled from various domains. A series of models that jointly train domain\nclassifiers and sentiment classifiers have demonstrated their advantages,\nbecause domain classification helps generate necessary information for\nsentiment classification. Intuitively, the importance of sentiment\nclassification tasks is the same in all domains for multi-domain sentiment\nclassification; but domain classification tasks are different because the\nimpact of domain information on sentiment classification varies across\ndifferent fields; this can be controlled through adjustable weights or hyper\nparameters. However, as the number of domains increases, existing\nhyperparameter optimization algorithms may face the following challenges: (1)\ntremendous demand for computing resources, (2) convergence problems, and (3)\nhigh algorithm complexity. To efficiently generate the domain information\nrequired for sentiment classification in each domain, we propose a dynamic\ninformation modulation algorithm. Specifically, the model training process is\ndivided into two stages. In the first stage, a shared hyperparameter, which\nwould control the proportion of domain classification tasks across all fields,\nis determined. In the second stage, we introduce a novel domain-aware\nmodulation algorithm to adjust the domain information contained in the input\ntext, which is then calculated based on a gradient-based and loss-based method.\nIn summary, experimental results on a public sentiment analysis dataset\ncontaining 16 domains prove the superiority of the proposed method.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 5 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.06630v1",
    "published_date": "2025-05-10 12:36:00 UTC",
    "updated_date": "2025-05-10 12:36:00 UTC"
  },
  {
    "arxiv_id": "2505.07877v1",
    "title": "Efficient Telecom Specific LLM: TSLAM-Mini with QLoRA and Digital Twin Data",
    "authors": [
      "Vignesh Ethiraj",
      "Divya Vijay",
      "Sidhanth Menon",
      "Heblin Berscilla"
    ],
    "abstract": "General-purpose large language models (LLMs), despite their broad\ncapabilities accrued from open-world data, frequently exhibit suboptimal\nperformance when confronted with the nuanced and specialized demands inherent\nin real-time telecommunications applications. This investigation addresses this\ncritical limitation through the meticulous fine-tuning of TSLAM-Mini developed\nby NetoAI, a compact (3.8-billion parameter) causal language model\narchitecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen\nleverages a bespoke dataset comprising 100,000 samples, strategically\nengineered to address 20 pivotal telecommunications use-cases, encompassing\ndomains such as Network Fundamentals, IP Routing, MPLS, Network Security,\nAutomation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical\nAI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched\nwith granular insights from venerated network Subject Matter Experts (SMEs) and\nauthoritative RFC documents, thereby capturing high-fidelity representations of\nreal-world network dynamics through simulations inspired by digital twin\nparadigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art\nParameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial\ntraining efficiency and enabled prospective deployment on resource-constrained\nhardware. A novel evaluation framework, predicated on a high-capacity LLM\n(Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to\nrigorously assess instruction-following fidelity and response quality across\nthe specified telecom use-cases. Empirical results unequivocally demonstrate\nTSLAM-Mini's superior aptitude in telecom-centric applications, underscoring\nthe profound efficacy of domain-specific datasets and PEFT methodologies for\nadvancing intelligent network management.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "68T50",
      "I.2.7; I.2.6; C.2.3"
    ],
    "primary_category": "cs.NI",
    "comment": "Introducing TSLAM-Mini, a specialized language model for\n  telecommunications, demonstrating the efficacy of QLoRA fine-tuning and\n  digital twin-synthesized data for enhanced network intelligence. Model\n  available on: https://huggingface.co/NetoAISolutions/TSLAM-Mini-2B",
    "pdf_url": "http://arxiv.org/pdf/2505.07877v1",
    "published_date": "2025-05-10 12:28:47 UTC",
    "updated_date": "2025-05-10 12:28:47 UTC"
  },
  {
    "arxiv_id": "2505.06625v1",
    "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated NPUs",
    "authors": [
      "Tianhao Cai",
      "Liang Wang",
      "Limin Xiao",
      "Meng Han",
      "Zeyu Wang",
      "Lin Sun",
      "Xiaojian Liao"
    ],
    "abstract": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average).",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.OS"
    ],
    "primary_category": "cs.AR",
    "comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)",
    "pdf_url": "http://arxiv.org/pdf/2505.06625v1",
    "published_date": "2025-05-10 12:16:50 UTC",
    "updated_date": "2025-05-10 12:16:50 UTC"
  },
  {
    "arxiv_id": "2505.06620v1",
    "title": "Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations",
    "authors": [
      "Dima Alattal",
      "Asal Khoshravan Azar",
      "Puja Myles",
      "Richard Branson",
      "Hatim Abdulhussein",
      "Allan Tucker"
    ],
    "abstract": "There is a growing demand for the use of Artificial Intelligence (AI) and\nMachine Learning (ML) in healthcare, particularly as clinical decision support\nsystems to assist medical professionals. However, the complexity of many of\nthese models, often referred to as black box models, raises concerns about\ntheir safe integration into clinical settings as it is difficult to understand\nhow they arrived at their predictions. This paper discusses insights and\nrecommendations derived from an expert working group convened by the UK\nMedicine and Healthcare products Regulatory Agency (MHRA). The group consisted\nof healthcare professionals, regulators, and data scientists, with a primary\nfocus on evaluating the outputs from different AI algorithms in clinical\ndecision-making contexts. Additionally, the group evaluated findings from a\npilot study investigating clinicians' behaviour and interaction with AI methods\nduring clinical diagnosis. Incorporating AI methods is crucial for ensuring the\nsafety and trustworthiness of medical AI devices in clinical settings. Adequate\ntraining for stakeholders is essential to address potential issues, and further\ninsights and recommendations for safely adopting AI systems in healthcare\nsettings are provided.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "47 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.06620v1",
    "published_date": "2025-05-10 12:09:19 UTC",
    "updated_date": "2025-05-10 12:09:19 UTC"
  },
  {
    "arxiv_id": "2505.06612v1",
    "title": "Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation",
    "authors": [
      "Yuqin Lan"
    ],
    "abstract": "In the era of rapid development of social media, social recommendation\nsystems as hybrid recommendation systems have been widely applied. Existing\nmethods capture interest similarity between users to filter out\ninterest-irrelevant relations in social networks that inevitably decrease\nrecommendation accuracy, however, limited research has a focus on the mutual\ninfluence of semantic information between the social network and the user-item\ninteraction network for further improving social recommendation. To address\nthese issues, we introduce a social \\underline{r}ecommendation model with\nro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion\nand multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly\npropose to construct a social tensor in order to smooth the training process of\nthe model. Then, a graph convolutional network and a tensor convolutional\nnetwork are employed to capture user's item preference and social preference,\nrespectively. Considering the different semantic information in the user-item\ninteraction network and the social network, a bi-semantic coordination loss is\nproposed to model the mutual influence of semantic information. To alleviate\nthe interference of interest-irrelevant relations on multi-semantic modeling,\nwe further use Bayesian posterior probability to mine potential social\nrelations to replace social noise. Finally, the sliding window mechanism is\nutilized to update the social tensor as the input for the next iteration.\nExtensive experiments on three real datasets show Burger has a superior\nperformance compared with the state-of-the-art models.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.IR",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.SI",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.06612v1",
    "published_date": "2025-05-10 11:51:22 UTC",
    "updated_date": "2025-05-10 11:51:22 UTC"
  },
  {
    "arxiv_id": "2505.06595v1",
    "title": "Feature Representation Transferring to Lightweight Models via Perception Coherence",
    "authors": [
      "Hai-Vy Nguyen",
      "Fabrice Gamboa",
      "Sixin Zhang",
      "Reda Chhaibi",
      "Serge Gratton",
      "Thierry Giaccone"
    ],
    "abstract": "In this paper, we propose a method for transferring feature representation to\nlightweight student models from larger teacher models. We mathematically define\na new notion called \\textit{perception coherence}. Based on this notion, we\npropose a loss function, which takes into account the dissimilarities between\ndata points in feature space through their ranking. At a high level, by\nminimizing this loss function, the student model learns to mimic how the\nteacher model \\textit{perceives} inputs. More precisely, our method is\nmotivated by the fact that the representational capacity of the student model\nis weaker than the teacher model. Hence, we aim to develop a new method\nallowing for a better relaxation. This means that, the student model does not\nneed to preserve the absolute geometry of the teacher one, while preserving\nglobal coherence through dissimilarity ranking. Our theoretical insights\nprovide a probabilistic perspective on the process of feature representation\ntransfer. Our experiments results show that our method outperforms or achieves\non-par performance compared to strong baseline methods for representation\ntransferring.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06595v1",
    "published_date": "2025-05-10 10:55:06 UTC",
    "updated_date": "2025-05-10 10:55:06 UTC"
  },
  {
    "arxiv_id": "2505.06589v1",
    "title": "Optimal Transport for Machine Learners",
    "authors": [
      "Gabriel Peyré"
    ],
    "abstract": "Optimal Transport is a foundational mathematical theory that connects\noptimization, partial differential equations, and probability. It offers a\npowerful framework for comparing probability distributions and has recently\nbecome an important tool in machine learning, especially for designing and\nevaluating generative models. These course notes cover the fundamental\nmathematical aspects of OT, including the Monge and Kantorovich formulations,\nBrenier's theorem, the dual and dynamic formulations, the Bures metric on\nGaussian distributions, and gradient flows. It also introduces numerical\nmethods such as linear programming, semi-discrete solvers, and entropic\nregularization. Applications in machine learning include topics like training\nneural networks via gradient flows, token dynamics in transformers, and the\nstructure of GANs and diffusion models. These notes focus primarily on\nmathematical content rather than deep learning techniques.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "comment": "arXiv admin note: text overlap with arXiv:1803.00567",
    "pdf_url": "http://arxiv.org/pdf/2505.06589v1",
    "published_date": "2025-05-10 10:35:03 UTC",
    "updated_date": "2025-05-10 10:35:03 UTC"
  },
  {
    "arxiv_id": "2505.06584v1",
    "title": "JAEGER: Dual-Level Humanoid Whole-Body Controller",
    "authors": [
      "Ziluo Ding",
      "Haobin Jiang",
      "Yuxuan Wang",
      "Zhenguo Sun",
      "Yu Zhang",
      "Xiaojie Niu",
      "Ming Yang",
      "Weishuai Zeng",
      "Xinrun Xu",
      "Zongqing Lu"
    ],
    "abstract": "This paper presents JAEGER, a dual-level whole-body controller for humanoid\nrobots that addresses the challenges of training a more robust and versatile\npolicy. Unlike traditional single-controller approaches, JAEGER separates the\ncontrol of the upper and lower bodies into two independent controllers, so that\nthey can better focus on their distinct tasks. This separation alleviates the\ndimensionality curse and improves fault tolerance. JAEGER supports both root\nvelocity tracking (coarse-grained control) and local joint angle tracking\n(fine-grained control), enabling versatile and stable movements. To train the\ncontroller, we utilize a human motion dataset (AMASS), retargeting human poses\nto humanoid poses through an efficient retargeting network, and employ a\ncurriculum learning approach. This method performs supervised learning for\ninitialization, followed by reinforcement learning for further exploration. We\nconduct our experiments on two humanoid platforms and demonstrate the\nsuperiority of our approach against state-of-the-art methods in both simulation\nand real environments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.06584v1",
    "published_date": "2025-05-10 10:10:19 UTC",
    "updated_date": "2025-05-10 10:10:19 UTC"
  },
  {
    "arxiv_id": "2505.06580v1",
    "title": "TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification",
    "authors": [
      "Dongyoon Yang",
      "Jihu Lee",
      "Yongdai Kim"
    ],
    "abstract": "Robust domain adaptation against adversarial attacks is a critical research\narea that aims to develop models capable of maintaining consistent performance\nacross diverse and challenging domains. In this paper, we derive a new\ngeneralization bound for robust risk on the target domain using a novel\ndivergence measure specifically designed for robust domain adaptation. Building\nupon this, we propose a new algorithm named TAROT, which is designed to enhance\nboth domain adaptability and robustness. Through extensive experiments, TAROT\nnot only surpasses state-of-the-art methods in accuracy and robustness but also\nsignificantly enhances domain generalization and scalability by effectively\nlearning domain-invariant features. In particular, TAROT achieves superior\nperformance on the challenging DomainNet dataset, demonstrating its ability to\nlearn domain-invariant representations that generalize well across different\ndomains, including unseen ones. These results highlight the broader\napplicability of our approach in real-world domain adaptation scenarios.",
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in CVPR 2025 (19 pages, 7 figures)",
    "pdf_url": "http://arxiv.org/pdf/2505.06580v1",
    "published_date": "2025-05-10 09:43:04 UTC",
    "updated_date": "2025-05-10 09:43:04 UTC"
  },
  {
    "arxiv_id": "2505.06576v2",
    "title": "Two-Stage Random Alternation Framework for One-Shot Pansharpening",
    "authors": [
      "Haorui Chen",
      "Zeyu Ren",
      "Jiaxuan Ren",
      "Ran Ran",
      "Jinliang Shao",
      "Jie Huang",
      "Liangjian Deng"
    ],
    "abstract": "Deep learning has substantially advanced pansharpening, achieving impressive\nfusion quality. However, a prevalent limitation is that conventional deep\nlearning models, which typically rely on training datasets, often exhibit\nsuboptimal generalization to unseen real-world image pairs. This restricts\ntheir practical utility when faced with real-world scenarios not included in\nthe training datasets. To overcome this, we introduce a two-stage random\nalternating framework (TRA-PAN) that performs instance-specific optimization\nfor any given Multispectral(MS)/Panchromatic(PAN) pair, ensuring robust and\nhigh-quality fusion. TRA-PAN effectively integrates strong supervision\nconstraints from reduced-resolution images with the physical characteristics of\nthe full-resolution images. The first stage introduces a pre-training\nprocedure, which includes Degradation-Aware Modeling (DAM) to capture spectral\ndegradation mappings, alongside a warm-up procedure designed to reduce training\ntime and mitigate the adverse effects of reduced-resolution data. The second\nstage employs Random Alternation Optimization (RAO), randomly alternating\nbetween reduced- and full-resolution images to refine the fusion model\nprogressively. This adaptive, per-instance optimization strategy, operating in\na one-shot manner for each MS/PAN pair, yields superior high-resolution\nmultispectral images. Experimental results demonstrate that TRA-PAN outperforms\nstate-of-the-art (SOTA) methods in quantitative metrics and visual quality in\nreal-world scenarios, underscoring its enhanced practical applicability and\nrobustness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06576v2",
    "published_date": "2025-05-10 09:26:22 UTC",
    "updated_date": "2025-05-16 10:39:40 UTC"
  },
  {
    "arxiv_id": "2505.06569v2",
    "title": "MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG",
    "authors": [
      "Woosang Lim",
      "Zekun Li",
      "Gyuwan Kim",
      "Sungyoung Ji",
      "HyeonJung Kim",
      "Kyuri Choi",
      "Jin Hyuk Lim",
      "Kyungpyo Park",
      "William Yang Wang"
    ],
    "abstract": "Long-context large language models (LC LLMs) combined with\nretrieval-augmented generation (RAG) hold strong potential for complex\nmulti-hop and large-document tasks. However, existing RAG systems often suffer\nfrom imprecise retrieval, incomplete context coverage under constrained\nwindows, and fragmented information from suboptimal context construction. We\nintroduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical RAG\nframework that compresses and partitions documents into coarse-to-fine\ngranularities, then adaptively merges relevant contexts through real-time\nchunk- and document-level expansions. By initiating with finest-level retrieval\nand progressively incorporating broader, higher-level context, MacRAG\nconstructs effective query-specific long contexts, optimizing both precision\nand coverage. Evaluations on challenging LongBench expansions of HotpotQA,\n2WikiMultihopQA, and Musique confirm MacRAG consistently surpasses baseline RAG\npipelines in single- and multi-step generation using Llama-3.1-8B,\nGemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient,\nscalable solution for real-world long-context, multi-hop reasoning. Our code is\navailable at https://github.com/Leezekun/MacRAG.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06569v2",
    "published_date": "2025-05-10 08:50:44 UTC",
    "updated_date": "2025-05-20 20:24:44 UTC"
  },
  {
    "arxiv_id": "2505.06561v1",
    "title": "Quadrupedal Robot Skateboard Mounting via Reverse Curriculum Learning",
    "authors": [
      "Danil Belov",
      "Artem Erkhov",
      "Elizaveta Pestova",
      "Ilya Osokin",
      "Dzmitry Tsetserukou",
      "Pavel Osinenko"
    ],
    "abstract": "The aim of this work is to enable quadrupedal robots to mount skateboards\nusing Reverse Curriculum Reinforcement Learning. Although prior work has\ndemonstrated skateboarding for quadrupeds that are already positioned on the\nboard, the initial mounting phase still poses a significant challenge. A\ngoal-oriented methodology was adopted, beginning with the terminal phases of\nthe task and progressively increasing the complexity of the problem definition\nto approximate the desired objective. The learning process was initiated with\nthe skateboard rigidly fixed within the global coordinate frame and the robot\npositioned directly above it. Through gradual relaxation of these initial\nconditions, the learned policy demonstrated robustness to variations in\nskateboard position and orientation, ultimately exhibiting a successful\ntransfer to scenarios involving a mobile skateboard. The code, trained models,\nand reproducible examples are available at the following link:\nhttps://github.com/dancher00/quadruped-skateboard-mounting",
    "categories": [
      "cs.RO",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06561v1",
    "published_date": "2025-05-10 08:17:15 UTC",
    "updated_date": "2025-05-10 08:17:15 UTC"
  },
  {
    "arxiv_id": "2505.07875v1",
    "title": "Getting Ready for the EU AI Act in Healthcare. A call for Sustainable AI Development and Deployment",
    "authors": [
      "John Brandt Brodersen",
      "Ilaria Amelia Caggiano",
      "Pedro Kringen",
      "Vince Istvan Madai",
      "Walter Osika",
      "Giovanni Sartor",
      "Ellen Svensson",
      "Magnus Westerlund",
      "Roberto V. Zicari"
    ],
    "abstract": "Assessments of trustworthiness have become a cornerstone of responsible AI\ndevelopment. Especially in high-stakes fields like healthcare, aligning\ntechnical, evidence-based, and ethical practices with forthcoming legal\nrequirements is increasingly urgent. We argue that developers and deployers of\nAI systems for the medical domain should be proactive and take steps to\nprogressively ensure that such systems, both those currently in use and those\nbeing developed or planned, respect the requirements of the AI Act, which has\ncome into force in August 2024. This is necessary if full and effective\ncompliance is to be ensured when the most relevant provisions of the Act become\neffective (August 2026). The engagement with the AI Act cannot be viewed as a\nformalistic exercise. Compliance with the AI Act needs to be carried out\nthrough the proactive commitment to the ethical principles of trustworthy AI.\nThese principles provide the background for the Act, which mentions them\nseveral times and connects them to the protection of public interest. They can\nbe used to interpret and apply the Act's provisions and to identify good\npractices, increasing the validity and sustainability of AI systems over time.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4.1; K.5.2"
    ],
    "primary_category": "cs.CY",
    "comment": "8 pages, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2505.07875v1",
    "published_date": "2025-05-10 07:46:54 UTC",
    "updated_date": "2025-05-10 07:46:54 UTC"
  },
  {
    "arxiv_id": "2505.06542v1",
    "title": "dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data",
    "authors": [
      "Adèle H. Ribeiro",
      "Dominik Heider"
    ],
    "abstract": "Causal discovery is central to inferring causal relationships from\nobservational data. In the presence of latent confounding, algorithms such as\nFast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing\nthe true model's Markov Equivalence Class. However, their correctness\ncritically depends on empirical faithfulness, the assumption that observed\n(in)dependencies perfectly reflect those of the underlying causal model, which\noften fails in practice due to limited sample sizes. To address this, we\nintroduce the first nonparametric score to assess a PAG's compatibility with\nobserved data, even with mixed variable types. This score is both necessary and\nsufficient to characterize structural uncertainty and distinguish between\ndistinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid\ncausal discovery algorithm to jointly address latent confounding, empirical\nunfaithfulness, and mixed data types. dcFCI integrates our score into an\n(Anytime)FCI-guided search that systematically explores, ranks, and validates\ncandidate PAGs. Experiments on synthetic and real-world scenarios demonstrate\nthat dcFCI significantly outperforms state-of-the-art methods, often recovering\nthe true PAG even in small and heterogeneous datasets. Examining top-ranked\nPAGs further provides valuable insights into structural uncertainty, supporting\nmore robust and informed causal reasoning and decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages. This work has been submitted to the IEEE for possible\n  publication",
    "pdf_url": "http://arxiv.org/pdf/2505.06542v1",
    "published_date": "2025-05-10 07:05:19 UTC",
    "updated_date": "2025-05-10 07:05:19 UTC"
  },
  {
    "arxiv_id": "2505.06537v1",
    "title": "ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images",
    "authors": [
      "Xianghao Kong",
      "Qiaosong Qi",
      "Yuanbin Wang",
      "Anyi Rao",
      "Biaolong Chen",
      "Aixi Zhang",
      "Si Liu",
      "Hao Jiang"
    ],
    "abstract": "Fashion video generation aims to synthesize temporally consistent videos from\nreference images of a designated character. Despite significant progress,\nexisting diffusion-based methods only support a single reference image as\ninput, severely limiting their capability to generate view-consistent fashion\nvideos, especially when there are different patterns on the clothes from\ndifferent perspectives. Moreover, the widely adopted motion module does not\nsufficiently model human body movement, leading to sub-optimal spatiotemporal\nconsistency. To address these issues, we propose ProFashion, a fashion video\ngeneration framework leveraging multiple reference images to achieve improved\nview consistency and temporal coherency. To effectively leverage features from\nmultiple reference images while maintaining a reasonable computational cost, we\ndevise a Pose-aware Prototype Aggregator, which selects and aggregates global\nand fine-grained reference features according to pose information to form\nframe-wise prototypes, which serve as guidance in the denoising process. To\nfurther enhance motion consistency, we introduce a Flow-enhanced Prototype\nInstantiator, which exploits the human keypoint motion flow to guide an extra\nspatiotemporal attention process in the denoiser. To demonstrate the\neffectiveness of ProFashion, we extensively evaluate our method on the\nMRFashion-7K dataset we collected from the Internet. ProFashion also\noutperforms previous methods on the UBC Fashion dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06537v1",
    "published_date": "2025-05-10 06:59:24 UTC",
    "updated_date": "2025-05-10 06:59:24 UTC"
  },
  {
    "arxiv_id": "2505.06536v1",
    "title": "TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition",
    "authors": [
      "Feng Liu",
      "Ziwang Fu",
      "Yunlong Wang",
      "Qijian Zheng"
    ],
    "abstract": "The fusion technique is the key to the multimodal emotion recognition task.\nRecently, cross-modal attention-based fusion methods have demonstrated high\nperformance and strong robustness. However, cross-modal attention suffers from\nredundant features and does not capture complementary features well. We find\nthat it is not necessary to use the entire information of one modality to\nreinforce the other during cross-modal interaction, and the features that can\nreinforce a modality may contain only a part of it. To this end, we design an\ninnovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN).\nSpecifically, for the redundant features, we make one modality perform\nintra-modal feature selection through a self-attention mechanism, so that the\nselected features can adaptively and efficiently interact with another\nmodality. To better capture the complementary information between the\nmodalities, we obtain the fused weight vector by splicing and use the weight\nvector to achieve feature reinforcement of the modalities. We apply TCAFN to\nthe RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal\nrepresentations to validate the effectiveness of the proposed fusion method.\nThe experimental results show that TACFN brings a significant performance\nimprovement compared to other methods and reaches the state-of-the-art. All\ncode and models could be accessed from https://github.com/shuzihuaiyu/TACFN.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2111.02172",
    "pdf_url": "http://arxiv.org/pdf/2505.06536v1",
    "published_date": "2025-05-10 06:57:58 UTC",
    "updated_date": "2025-05-10 06:57:58 UTC"
  },
  {
    "arxiv_id": "2505.06535v1",
    "title": "Online Feedback Efficient Active Target Discovery in Partially Observable Environments",
    "authors": [
      "Anindya Sarkar",
      "Binglin Ji",
      "Yevgeniy Vorobeychik"
    ],
    "abstract": "In various scientific and engineering domains, where data acquisition is\ncostly, such as in medical imaging, environmental monitoring, or remote\nsensing, strategic sampling from unobserved regions, guided by prior\nobservations, is essential to maximize target discovery within a limited\nsampling budget. In this work, we introduce Diffusion-guided Active Target\nDiscovery (DiffATD), a novel method that leverages diffusion dynamics for\nactive target discovery. DiffATD maintains a belief distribution over each\nunobserved state in the environment, using this distribution to dynamically\nbalance exploration-exploitation. Exploration reduces uncertainty by sampling\nregions with the highest expected entropy, while exploitation targets areas\nwith the highest likelihood of discovering the target, indicated by the belief\ndistribution and an incrementally trained reward model designed to learn the\ncharacteristics of the target. DiffATD enables efficient target discovery in a\npartially observable environment within a fixed sampling budget, all without\nrelying on any prior supervised training. Furthermore, DiffATD offers\ninterpretability, unlike existing black-box policies that require extensive\nsupervised training. Through extensive experiments and ablation studies across\ndiverse domains, including medical imaging and remote sensing, we show that\nDiffATD performs significantly better than baselines and competitively with\nsupervised methods that operate under full environmental observability.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "30 pages, 28 figures, Pre-print",
    "pdf_url": "http://arxiv.org/pdf/2505.06535v1",
    "published_date": "2025-05-10 06:50:01 UTC",
    "updated_date": "2025-05-10 06:50:01 UTC"
  },
  {
    "arxiv_id": "2505.06527v1",
    "title": "Improving Generalization of Medical Image Registration Foundation Model",
    "authors": [
      "Jing Hu",
      "Kaiwei Yu",
      "Hongjiang Xian",
      "Shu Hu",
      "Xin Wang"
    ],
    "abstract": "Deformable registration is a fundamental task in medical image processing,\naiming to achieve precise alignment by establishing nonlinear correspondences\nbetween images. Traditional methods offer good adaptability and\ninterpretability but are limited by computational efficiency. Although deep\nlearning approaches have significantly improved registration speed and\naccuracy, they often lack flexibility and generalizability across different\ndatasets and tasks. In recent years, foundation models have emerged as a\npromising direction, leveraging large and diverse datasets to learn universal\nfeatures and transformation patterns for image registration, thus demonstrating\nstrong cross-task transferability. However, these models still face challenges\nin generalization and robustness when encountering novel anatomical structures,\nvarying imaging conditions, or unseen modalities. To address these limitations,\nthis paper incorporates Sharpness-Aware Minimization (SAM) into foundation\nmodels to enhance their generalization and robustness in medical image\nregistration. By optimizing the flatness of the loss landscape, SAM improves\nmodel stability across diverse data distributions and strengthens its ability\nto handle complex clinical scenarios. Experimental results show that foundation\nmodels integrated with SAM achieve significant improvements in cross-dataset\nregistration performance, offering new insights for the advancement of medical\nimage registration technology. Our code is available at\nhttps://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\\_sam.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IJCNN",
    "pdf_url": "http://arxiv.org/pdf/2505.06527v1",
    "published_date": "2025-05-10 06:14:09 UTC",
    "updated_date": "2025-05-10 06:14:09 UTC"
  },
  {
    "arxiv_id": "2505.06520v2",
    "title": "PRUNE: A Patching Based Repair Framework for Certifiable Unlearning of Neural Networks",
    "authors": [
      "Xuran Li",
      "Jingyi Wang",
      "Xiaohan Yuan",
      "Peixin Zhang",
      "Zhan Qin",
      "Zhibo Wang",
      "Kui Ren"
    ],
    "abstract": "It is often desirable to remove (a.k.a. unlearn) a specific part of the\ntraining data from a trained neural network model. A typical application\nscenario is to protect the data holder's right to be forgotten, which has been\npromoted by many recent regulation rules. Existing unlearning methods involve\ntraining alternative models with remaining data, which may be costly and\nchallenging to verify from the data holder or a thirdparty auditor's\nperspective. In this work, we provide a new angle and propose a novel\nunlearning approach by imposing carefully crafted \"patch\" on the original\nneural network to achieve targeted \"forgetting\" of the requested data to\ndelete. Specifically, inspired by the research line of neural network repair,\nwe propose to strategically seek a lightweight minimum \"patch\" for unlearning a\ngiven data point with certifiable guarantee. Furthermore, to unlearn a\nconsiderable amount of data points (or an entire class), we propose to\niteratively select a small subset of representative data points to unlearn,\nwhich achieves the effect of unlearning the whole set. Extensive experiments on\nmultiple categorical datasets demonstrates our approach's effectiveness,\nachieving measurable unlearning while preserving the model's performance and\nbeing competitive in efficiency and memory consumption compared to various\nbaseline methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06520v2",
    "published_date": "2025-05-10 05:35:08 UTC",
    "updated_date": "2025-05-21 14:04:41 UTC"
  },
  {
    "arxiv_id": "2505.06518v1",
    "title": "A Point-Based Algorithm for Distributional Reinforcement Learning in Partially Observable Domains",
    "authors": [
      "Larry Preuett III"
    ],
    "abstract": "In many real-world planning tasks, agents must tackle uncertainty about the\nenvironment's state and variability in the outcomes of any chosen policy. We\naddress both forms of uncertainty as a first step toward safer algorithms in\npartially observable settings. Specifically, we extend Distributional\nReinforcement Learning (DistRL)-which models the entire return distribution for\nfully observable domains-to Partially Observable Markov Decision Processes\n(POMDPs), allowing an agent to learn the distribution of returns for each\nconditional plan. Concretely, we introduce new distributional Bellman operators\nfor partial observability and prove their convergence under the supremum\np-Wasserstein metric. We also propose a finite representation of these return\ndistributions via psi-vectors, generalizing the classical alpha-vectors in\nPOMDP solvers. Building on this, we develop Distributional Point-Based Value\nIteration (DPBVI), which integrates psi-vectors into a standard point-based\nbackup procedure-bridging DistRL and POMDP planning. By tracking return\ndistributions, DPBVI naturally enables risk-sensitive control in domains where\nrare, high-impact events must be carefully managed. We provide source code to\nfoster further research in robust decision-making under partial observability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06518v1",
    "published_date": "2025-05-10 05:19:32 UTC",
    "updated_date": "2025-05-10 05:19:32 UTC"
  },
  {
    "arxiv_id": "2505.06507v1",
    "title": "Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities",
    "authors": [
      "Haoyang Xie",
      "Feng Ju"
    ],
    "abstract": "Computer-aided design (CAD) is fundamental to modern engineering and\nmanufacturing, but creating CAD models still requires expert knowledge and\nspecialized software. Recent advances in large language models (LLMs) open up\nthe possibility of generative CAD, where natural language is directly\ntranslated into parametric 3D models. However, most existing methods generate\ntask-specific command sequences that pretrained models cannot directly handle.\nThese sequences must be converted into CAD representations such as CAD vectors\nbefore a 3D model can be produced, which requires training models from scratch\nand adds unnecessary complexity. To tackle this issue, we propose generating\nCadQuery code directly from text, leveraging the strengths of pretrained LLMs\nto produce 3D models without intermediate representations, using this\nPython-based scripting language. Since LLMs already excel at Python generation\nand spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly\neffective. Given that these capabilities typically improve with scale, we\nhypothesize that larger models will perform better after fine-tuning. To enable\nthis, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We\nfine-tune six open-source LLMs of varying sizes and observe consistent\nimprovements. Our best model achieves a top-1 exact match of 69.3%, up from\n58.8%, and reduces Chamfer Distance by 48.6%. Project page:\nhttps://github.com/Text-to-CadQuery/Text-to-CadQuery.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06507v1",
    "published_date": "2025-05-10 04:47:08 UTC",
    "updated_date": "2025-05-10 04:47:08 UTC"
  },
  {
    "arxiv_id": "2505.06505v1",
    "title": "On Definite Iterated Belief Revision with Belief Algebras",
    "authors": [
      "Hua Meng",
      "Zhiguo Long",
      "Michael Sioutis",
      "Zhengchun Zhou"
    ],
    "abstract": "Traditional logic-based belief revision research focuses on designing rules\nto constrain the behavior of revision operators. Frameworks have been proposed\nto characterize iterated revision rules, but they are often too loose, leading\nto multiple revision operators that all satisfy the rules under the same belief\ncondition. In many practical applications, such as safety critical ones, it is\nimportant to specify a definite revision operator to enable agents to\niteratively revise their beliefs in a deterministic way. In this paper, we\npropose a novel framework for iterated belief revision by characterizing belief\ninformation through preference relations. Semantically, both beliefs and new\nevidence are represented as belief algebras, which provide a rich and\nexpressive foundation for belief revision. Building on traditional revision\nrules, we introduce additional postulates for revision with belief algebra,\nincluding an upper-bound constraint on the outcomes of revision. We prove that\nthe revision result is uniquely determined given the current belief state and\nnew evidence. Furthermore, to make the framework more useful in practice, we\ndevelop a particular algorithm for performing the proposed revision process. We\nargue that this approach may offer a more predictable and principled method for\nbelief revision, making it suitable for real-world applications.",
    "categories": [
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages. Extended version of an accepted IJCAI 2025 paper",
    "pdf_url": "http://arxiv.org/pdf/2505.06505v1",
    "published_date": "2025-05-10 04:34:43 UTC",
    "updated_date": "2025-05-10 04:34:43 UTC"
  },
  {
    "arxiv_id": "2505.06503v1",
    "title": "Attention Mechanisms in Dynamical Systems: A Case Study with Predator-Prey Models",
    "authors": [
      "David Balaban"
    ],
    "abstract": "Attention mechanisms are widely used in artificial intelligence to enhance\nperformance and interpretability. In this paper, we investigate their utility\nin modeling classical dynamical systems -- specifically, a noisy predator-prey\n(Lotka-Volterra) system. We train a simple linear attention model on perturbed\ntime-series data to reconstruct system trajectories. Remarkably, the learned\nattention weights align with the geometric structure of the Lyapunov function:\nhigh attention corresponds to flat regions (where perturbations have small\neffect), and low attention aligns with steep regions (where perturbations have\nlarge effect). We further demonstrate that attention-based weighting can serve\nas a proxy for sensitivity analysis, capturing key phase-space properties\nwithout explicit knowledge of the system equations. These results suggest a\nnovel use of AI-derived attention for interpretable, data-driven analysis and\ncontrol of nonlinear systems. For example our framework could support future\nwork in biological modeling of circadian rhythms, and interpretable machine\nlearning for dynamical environments.",
    "categories": [
      "math.DS",
      "cs.AI",
      "es: 92B05 (Primary), 34C60, 37N25, 68T07, 93B30 (Secondary)"
    ],
    "primary_category": "math.DS",
    "comment": "5 figures, 12 pages, python code included",
    "pdf_url": "http://arxiv.org/pdf/2505.06503v1",
    "published_date": "2025-05-10 04:14:28 UTC",
    "updated_date": "2025-05-10 04:14:28 UTC"
  },
  {
    "arxiv_id": "2505.06496v1",
    "title": "xGen-small Technical Report",
    "authors": [
      "Erik Nijkamp",
      "Bo Pang",
      "Egor Pakhomov",
      "Akash Gokul",
      "Jin Qu",
      "Silvio Savarese",
      "Yingbo Zhou",
      "Caiming Xiong"
    ],
    "abstract": "We introduce xGen-small, a family of 4B and 9B Transformer decoder models\noptimized for long-context applications. Our vertically integrated pipeline\nunites domain-balanced, frequency-aware data curation; multi-stage pre-training\nwith quality annealing and length extension to 128k tokens; and targeted\npost-training via supervised fine-tuning, preference learning, and online\nreinforcement learning. xGen-small delivers strong performance across various\ntasks, especially in math and coding domains, while excelling at long context\nbenchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06496v1",
    "published_date": "2025-05-10 02:54:16 UTC",
    "updated_date": "2025-05-10 02:54:16 UTC"
  },
  {
    "arxiv_id": "2505.06493v1",
    "title": "System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection",
    "authors": [
      "Jiawei Guo",
      "Haipeng Cai"
    ],
    "abstract": "Large language models (LLMs) have gained widespread adoption across diverse\napplications due to their impressive generative capabilities. Their\nplug-and-play nature enables both developers and end users to interact with\nthese models through simple prompts. However, as LLMs become more integrated\ninto various systems in diverse domains, concerns around their security are\ngrowing. Existing studies mainly focus on threats arising from user prompts\n(e.g. prompt injection attack) and model output (e.g. model inversion attack),\nwhile the security of system prompts remains largely overlooked. This work\nbridges the critical gap. We introduce system prompt poisoning, a new attack\nvector against LLMs that, unlike traditional user prompt injection, poisons\nsystem prompts hence persistently impacts all subsequent user interactions and\nmodel responses. We systematically investigate four practical attack strategies\nin various poisoning scenarios. Through demonstration on both generative and\nreasoning LLMs, we show that system prompt poisoning is highly feasible without\nrequiring jailbreak techniques, and effective across a wide range of tasks,\nincluding those in mathematics, coding, logical reasoning, and natural language\nprocessing. Importantly, our findings reveal that the attack remains effective\neven when user prompts employ advanced prompting techniques like\nchain-of-thought (CoT). We also show that such techniques, including CoT and\nretrieval-augmentation-generation (RAG), which are proven to be effective for\nimproving LLM performance in a wide range of tasks, are significantly weakened\nin their effectiveness by system prompt poisoning.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06493v1",
    "published_date": "2025-05-10 02:31:26 UTC",
    "updated_date": "2025-05-10 02:31:26 UTC"
  },
  {
    "arxiv_id": "2505.06492v1",
    "title": "SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing",
    "authors": [
      "Chathurangi Shyalika",
      "Renjith Prasad",
      "Alaa Al Ghazo",
      "Darssan Eswaramoorthi",
      "Harleen Kaur",
      "Sara Shree Muthuselvam",
      "Amit Sheth"
    ],
    "abstract": "In the dynamic landscape of Industry 4.0, achieving efficiency, precision,\nand adaptability is essential to optimize manufacturing operations. Industries\nsuffer due to supply chain disruptions caused by anomalies, which are being\ndetected by current AI models but leaving domain experts uncertain without\ndeeper insights into these anomalies. Additionally, operational inefficiencies\npersist due to inaccurate production forecasts and the limited effectiveness of\ntraditional AI models for processing complex sensor data. Despite these\nadvancements, existing systems lack the seamless integration of these\ncapabilities needed to create a truly unified solution for enhancing production\nand decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot\ndesigned for advanced reasoning and contextual decision-making to address these\nchallenges. SmartPilot processes multimodal sensor data and is compact to\ndeploy on edge devices. It focuses on three key tasks: anomaly prediction,\nproduction forecasting, and domain-specific question answering. By bridging the\ngap between AI capabilities and real-world industrial needs, SmartPilot\nempowers industries with intelligent decision-making and drives transformative\ninnovation in manufacturing. The demonstration video, datasets, and\nsupplementary materials are available at\nhttps://github.com/ChathurangiShyalika/SmartPilot.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 8 figures, 4 tables, IEEE Conference on Artificial\n  Intelligence (IEEE CAI) 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.06492v1",
    "published_date": "2025-05-10 02:20:49 UTC",
    "updated_date": "2025-05-10 02:20:49 UTC"
  },
  {
    "arxiv_id": "2505.06482v2",
    "title": "Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach",
    "authors": [
      "Minting Pan",
      "Yitao Zheng",
      "Jiajian Li",
      "Yunbo Wang",
      "Xiaokang Yang"
    ],
    "abstract": "Offline reinforcement learning (RL) enables policy optimization using static\ndatasets, avoiding the risks and costs of extensive real-world exploration.\nHowever, it struggles with suboptimal offline behaviors and inaccurate value\nestimation due to the lack of environmental interaction. We present\nVideo-Enhanced Offline RL (VeoRL), a model-based method that constructs an\ninteractive world model from diverse, unlabeled video data readily available\nonline. Leveraging model-based behavior guidance, our approach transfers\ncommonsense knowledge of control policy and physical dynamics from natural\nvideos to the RL agent within the target domain. VeoRL achieves substantial\nperformance gains (over 100% in some cases) across visual control tasks in\nrobotic manipulation, autonomous driving, and open-world video games.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06482v2",
    "published_date": "2025-05-10 00:54:12 UTC",
    "updated_date": "2025-05-17 09:20:41 UTC"
  }
]
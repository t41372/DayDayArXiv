{
  "date": "2025-04-20",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-04-20 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 55 篇论文，主要聚焦 AI 和大型语言模型（LLM）的创新应用，包括强化学习、机器人交互、医疗图像处理等领域，其中 Jianyu Zhang 的 PhD 论文《AI for the Open-World: the Learning Principles》令人印象深刻，探讨了 AI 在开放世界中的学习原则，此外还有高效的 LLM 框架如 DreamID 和 ReasoningV，展示了 AI 在实际应用中的潜力。\n\n### 重点论文讨论\n我们优先选取重要、话题性强或有影响力的论文（如知名学者作品或新框架），并将相关主题归类快速概述。以下按主题分组，核心贡献简要描述。\n\n**AI 和 LLM 创新（高话题度领域）**  \n- **AI for the Open-World: the Learning Principles**（作者：Jianyu Zhang）  \n  这篇 PhD 论文指出，封闭世界 AI 的成功无法直接扩展到开放世界，提出需要独特学习原则如丰富特征和推理时学习，通过大规模实验验证这些原则，强调 AI 向更广义任务演进的必要性。  \n- **A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization**（作者：Sahil Rajesh Dhayalkar）  \n  通过图论和随机游走建模 dropout，证明泛化子网络形成低阻抗集群，并通过实验验证其在网络宽度下的指数增长，为 dropout 优化提供新方向。  \n- **DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning**（作者：Fulong Ye 等）  \n  引入 Triplet ID Group 显式监督的扩散模型，实现高保真人脸交换，显著提升 ID 相似度和属性保留，尤其在复杂场景下，仅需 0.6 秒生成 512x512 图像，项目代码已开源。  \n- **ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model**（作者：Haiyan Qin 等）  \n  提出混合推理策略，使用高质量数据集和自适应机制，提高 Verilog 代码生成准确率至 57.8%，超越 GPT-4o mini，在硬件设计自动化中表现出色。  \n- **AI with Emotions: Exploring Emotional Expressions in Large Language Models**（作者：Shin-nosuke Ishikawa 等）  \n  通过 Russell's Circumplex 模型测试 LLM 的情感表达能力，发现模型能根据指定情感状态生成一致响应，扩展了 LLM 在情感交互应用如顾问系统的潜力，已被 NAACL 2025 接受。  \n- **Causality for Natural Language Processing**（作者：Zhijing Jin）  \n  这篇 PhD 论文探索 LLM 的因果推理能力，构建新数据集和框架，分析因果学习在 NLP 中的应用，提升模型在政治决策和科学影响评估的可靠性。\n\n**机器人和计算机视觉（实用应用强）**  \n- **A Modularized Design Approach for GelSight Family of Vision-based Tactile Sensors**（作者：Arpit Agarwal 等）  \n  通过光学模拟和模块化优化简化触觉传感器设计，开发 OptiSense Studio 工具箱，非专家也能快速设计传感器，提升机器人感知任务的效率，已被国际机器人研究杂志接受。  \n- **Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction**（作者：Wenke Xia 等）  \n  提出运动指令驱动的自省框架，结合视觉扩散策略，实现机器人动作精确修正，并在动态环境中持续学习，实验证明其鲁棒性超越传统方法。  \n- **Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline**（作者：Hui Zhou 等）  \n  构建闭环模拟器和因果基准，揭示模仿学习在规划中的泛化问题，并提出联合模仿-强化学习框架，代码即将开源。  \n- **VM-BHINet: Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image**（作者：Han Bi 等）  \n  使用状态空间模型提升双手交互建模，减少遮挡问题，在 InterHand2.6M 数据集上降低 MPJPE 和 MPVPE 错误 2-3%，提高 3D 手部重建效率。\n\n**医疗图像和生物信号处理（实际影响大）**  \n- **SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training**（作者：Shuang Zeng 等）  \n  引入超像素引导的对比学习策略，提升医疗图像分割性能，在多个数据集上 DSC 指标提高 3-8%，显著减少标注需求。  \n- **Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features**（作者：Parshuram N. Aarotale 等）  \n  提出 XMANet 模型融合 STFT 和小波变换，改善 EMG 手势识别准确率，在 Grabmyo 和 FORS 数据集上提升 1-9%，适用于假肢控制。  \n\n其他论文如强化学习、金融优化和数据集构建等，虽然多样但相对常规，我们快速掠过：例如，《SWE-Synth》合成 bug 修复数据提升 LLM 在软件工程中的性能；《FarsEval-PKBETS》构建波斯语 LLM 基准，揭示当前模型在文化特定任务中的不足；《Video-MMLU》评估 LLM 在多学科视频理解中的局限性；这些工作为领域基准提供了新工具，但未有突破性发现。\n\n总之，今天的更新强调 AI 模型的鲁棒性和实际应用，Jianyu Zhang 等学者的作品特别值得关注。如果您对特定领域感兴趣，建议查看这些论文的摘要以深入了解。明天见！",
  "papers": [
    {
      "arxiv_id": "2504.14762v1",
      "title": "A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Sahil Rajesh Dhayalkar"
      ],
      "abstract": "We propose a combinatorial and graph-theoretic theory of dropout by modeling\ntraining as a random walk over a high-dimensional graph of binary subnetworks.\nEach node represents a masked version of the network, and dropout induces\nstochastic traversal across this space. We define a subnetwork contribution\nscore that quantifies generalization and show that it varies smoothly over the\ngraph. Using tools from spectral graph theory, PAC-Bayes analysis, and\ncombinatorics, we prove that generalizing subnetworks form large, connected,\nlow-resistance clusters, and that their number grows exponentially with network\nwidth. This reveals dropout as a mechanism for sampling from a robust,\nstructured ensemble of well-generalizing subnetworks with built-in redundancy.\nExtensive experiments validate every theoretical claim across diverse\narchitectures. Together, our results offer a unified foundation for\nunderstanding dropout and suggest new directions for mask-guided regularization\nand subnetwork optimization.",
      "tldr_zh": "本研究提出了一种组合学和图论视角的dropout理论，将神经网络训练建模为在高维子网络图上的随机游走，每个节点代表网络的masked版本，并定义了subnetwork contribution score来量化泛化能力。论文利用spectral graph theory、PAC-Bayes analysis和组合学工具证明，泛化子网络形成大型、连通、低阻抗的集群，其数量随网络宽度呈指数增长，从而揭示dropout作为采样鲁棒、结构化子网络集合的机制。实验在多种架构上验证了这些理论声明，并为mask-guided regularization和subnetwork optimization等新方向提供了统一基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages (9 pages main content and remaining pages are references,\n  appendix which includes 7 figures, proofs and derivations)",
      "pdf_url": "http://arxiv.org/pdf/2504.14762v1",
      "published_date": "2025-04-20 23:09:20 UTC",
      "updated_date": "2025-04-20 23:09:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:52:56.512692"
    },
    {
      "arxiv_id": "2504.14757v1",
      "title": "SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs",
      "title_zh": "翻译失败",
      "authors": [
        "Minh V. T. Pham",
        "Huy N. Phan",
        "Hoang N. Phan",
        "Cuong Le Chi",
        "Tien N. Nguyen",
        "Nghi D. Q. Bui"
      ],
      "abstract": "Large language models (LLMs) are transforming automated program repair (APR)\nthrough agent-based approaches that localize bugs, generate patches, and verify\nfixes. However, the lack of high-quality, scalable training datasets,\nespecially those with verifiable outputs and intermediate reasoning\ntraces-limits progress, particularly for open-source models. In this work, we\npresent SWE-Synth, a framework for synthesizing realistic, verifiable, and\nprocess-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM\nagents to simulate debugging workflows, producing not only bug-fix pairs but\nalso test cases and structured repair trajectories. Compared to manually\ncurated datasets, our method scales with minimal human effort while preserving\ncontextual richness and correctness. Experiments show that models trained on\nSWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench\nLite. Our results highlight the potential of synthetic, agent-generated data to\nadvance the state of the art in APR and software engineering automation.",
      "tldr_zh": "该研究提出 SWE-Synth 框架，用于合成真实、可验证的 bug-fix 数据，帮助 Large Language Models (LLMs) 在自动程序修复 (APR) 中更好地处理真实世界 bug。框架通过 LLM 代理模拟调试工作流，生成 bug-fix 对、测试用例和结构化的修复轨迹，实现高效扩展和上下文保留。实验结果显示，在 SWE-Bench Lite 上，使用 SWE-Synth 训练的模型比基于真实数据集的模型性能提升 2.3%。这项工作展示了合成数据在推进 APR 和软件工程自动化的潜力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2504.14757v1",
      "published_date": "2025-04-20 22:37:43 UTC",
      "updated_date": "2025-04-20 22:37:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:53:08.914185"
    },
    {
      "arxiv_id": "2504.14751v1",
      "title": "AI for the Open-World: the Learning Principles",
      "title_zh": "面向开放世界的 AI：学习原则",
      "authors": [
        "Jianyu Zhang"
      ],
      "abstract": "During the past decades, numerous successes of AI has been made on \"specific\ncapabilities\", named closed-world, such as artificial environments or specific\nreal-world tasks. This well-defined narrow capability brings two nice benefits,\na clear criterion of success and the opportunity to collect a lot of examples.\nThe criteria not only reveal whether a machine has achieved a goal, but reveal\nhow the machine falls short of the goal. As a result, human designers can fix\nthe problems one after the other until the machine is deemed good enough for\nthe task. Furthermore, the large set of collected examples reduces the\ndifficulty of this problem-fixing process (by the central limit theorem).\n  Do the success in closed-world translate into broad open-world, where a\nmachine is required to perform any task that a human could possibly undertake\nwith fewer examples and less priori knowledge from human designers? No. Because\ncompetence in a specific task provides little insight in handling other tasks,\nthe valuable criteria for specific tasks become helpless when handling broader\nunseen tasks. Furthermore, due to the shortage of examples in unseen tasks,\ncentral limit theorem does not stand on our side. At the end, human designers\nlose the oscilloscope to \"hack\" an AI system for the open-world.\n  Achieving AI for the open-world requires unique learning principles and\ninnovated techniques, which are different from the ones in building AI for the\nclosed-world. This thesis explores necessary learning principles required to\nconstruct AI for the open-world, including rich features (analogy a large tool\nbox), disentangled representation (an organized tool box), and inference-time\nlearning (a tool-savvy hand). Driven by the learning principles, this thesis\nfurther proposes techniques to use the learning principles, conducts enormous\nlarge-scale experiments to verify the learning principles.",
      "tldr_zh": "这篇论文探讨了AI在封闭世界（closed-world）中的成功，例如特定任务的训练，但强调这些成就无法直接扩展到开放世界（open-world），因为封闭任务的成功标准和大量例子在处理多样化、未知任务时失效，且缺少例子使问题更难解决。论文提出构建开放世界AI所需的关键学习原则，包括rich features（丰富的特征）、disentangled representation（解耦表示）和inference-time learning（推理时学习），这些原则旨在帮助AI更灵活地处理人类级任务。作者进一步开发相关技术，并通过大规模实验验证这些原则的有效性，为开放世界AI的创新奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "PhD thesis. This is not a compilation of published papers, but a new\n  one",
      "pdf_url": "http://arxiv.org/pdf/2504.14751v1",
      "published_date": "2025-04-20 22:22:00 UTC",
      "updated_date": "2025-04-20 22:22:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:53:23.174788"
    },
    {
      "arxiv_id": "2504.14739v1",
      "title": "A Modularized Design Approach for GelSight Family of Vision-based Tactile Sensors",
      "title_zh": "GelSight 系列基于视觉的触觉传感器的模块化设计方法",
      "authors": [
        "Arpit Agarwal",
        "Mohammad Amin Mirzaee",
        "Xiping Sun",
        "Wenzhen Yuan"
      ],
      "abstract": "GelSight family of vision-based tactile sensors has proven to be effective\nfor multiple robot perception and manipulation tasks. These sensors are based\non an internal optical system and an embedded camera to capture the deformation\nof the soft sensor surface, inferring the high-resolution geometry of the\nobjects in contact. However, customizing the sensors for different robot hands\nrequires a tedious trial-and-error process to re-design the optical system. In\nthis paper, we formulate the GelSight sensor design process as a systematic and\nobjective-driven design problem and perform the design optimization with a\nphysically accurate optical simulation. The method is based on modularizing and\nparameterizing the sensor's optical components and designing four generalizable\nobjective functions to evaluate the sensor. We implement the method with an\ninteractive and easy-to-use toolbox called OptiSense Studio. With the toolbox,\nnon-sensor experts can quickly optimize their sensor design in both forward and\ninverse ways following our predefined modules and steps. We demonstrate our\nsystem with four different GelSight sensors by quickly optimizing their initial\ndesign in simulation and transferring it to the real sensors.",
      "tldr_zh": "本研究提出了一种模块化设计方法，用于优化 GelSight 系列 vision-based tactile sensors，以简化其在机器人手上的定制过程。该方法通过模块化和参数化光学组件，并结合物理准确的光学模拟和四个通用的目标函数，进行系统化的设计优化。研究开发了交互式工具箱 OptiSense Studio，允许非专家用户通过正向和逆向优化快速迭代传感器设计。最终，实验在四个不同 GelSight 传感器上验证了该方法的有效性，可在模拟中优化初始设计并成功转移到真实传感器。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "The paper is accepted to International Journal of Robotics Research\n  with DOI 10.1177/02783649251339680",
      "pdf_url": "http://arxiv.org/pdf/2504.14739v1",
      "published_date": "2025-04-20 21:07:41 UTC",
      "updated_date": "2025-04-20 21:07:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:53:31.821790"
    },
    {
      "arxiv_id": "2504.14737v1",
      "title": "SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training",
      "title_zh": "SuperCL：超像素引导对比学习用于医疗图像分割预训练",
      "authors": [
        "Shuang Zeng",
        "Lei Zhu",
        "Xinliang Zhang",
        "Hangzhou He",
        "Yanye Lu"
      ],
      "abstract": "Medical image segmentation is a critical yet challenging task, primarily due\nto the difficulty of obtaining extensive datasets of high-quality,\nexpert-annotated images. Contrastive learning presents a potential but still\nproblematic solution to this issue. Because most existing methods focus on\nextracting instance-level or pixel-to-pixel representation, which ignores the\ncharacteristics between intra-image similar pixel groups. Moreover, when\nconsidering contrastive pairs generation, most SOTA methods mainly rely on\nmanually setting thresholds, which requires a large number of gradient\nexperiments and lacks efficiency and generalization. To address these issues,\nwe propose a novel contrastive learning approach named SuperCL for medical\nimage segmentation pre-training. Specifically, our SuperCL exploits the\nstructural prior and pixel correlation of images by introducing two novel\ncontrastive pairs generation strategies: Intra-image Local Contrastive Pairs\n(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.\nConsidering superpixel cluster aligns well with the concept of contrastive\npairs generation, we utilize the superpixel map to generate pseudo masks for\nboth ILCP and IGCP to guide supervised contrastive learning. Moreover, we also\npropose two modules named Average SuperPixel Feature Map Generation (ASP) and\nConnected Components Label Generation (CCL) to better exploit the prior\nstructural information for IGCP. Finally, experiments on 8 medical image\ndatasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL\nachieves a superior performance with more precise predictions from\nvisualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best\nresults on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released\nafter acceptance.",
      "tldr_zh": "本研究针对医疗图像分割中数据标注困难的问题，提出了一种新型对比学习方法SuperCL，利用superpixel引导进行预训练，以解决现有方法忽略图像内相似像素组特性和依赖手动阈值的不足。具体而言，SuperCL引入Intra-image Local Contrastive Pairs (ILCP)和Inter-image Global Contrastive Pairs (IGCP)生成策略，并通过Average SuperPixel Feature Map Generation (ASP)和Connected Components Label Generation (CCL)模块利用图像结构先验信息。实验结果显示，SuperCL在8个医疗图像数据集上优于12种现有方法，在MMWHS、CHAOS和Spleen数据集上分别将DSC指标提高了3.15%、5.44%和7.89%，并在可视化中显示更精确的预测。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14737v1",
      "published_date": "2025-04-20 20:57:03 UTC",
      "updated_date": "2025-04-20 20:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:53:44.648531"
    },
    {
      "arxiv_id": "2504.14727v1",
      "title": "Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Geng Liu",
        "Fei Zhu",
        "Rong Feng",
        "Zhiqiang Yi",
        "Shiqi Wang",
        "Gaofeng Meng",
        "Zhaoxiang Zhang"
      ],
      "abstract": "Humans and most animals inherently possess a distinctive capacity to\ncontinually acquire novel experiences and accumulate worldly knowledge over\ntime. This ability, termed continual learning, is also critical for deep neural\nnetworks (DNNs) to adapt to the dynamically evolving world in open\nenvironments. However, DNNs notoriously suffer from catastrophic forgetting of\npreviously learned knowledge when trained on sequential tasks. In this work,\ninspired by the interactive human memory and learning system, we propose a\nnovel biomimetic continual learning framework that integrates semi-parametric\nmemory and the wake-sleep consolidation mechanism. For the first time, our\nmethod enables deep neural networks to retain high performance on novel tasks\nwhile maintaining prior knowledge in real-world challenging continual learning\nscenarios, e.g., class-incremental learning on ImageNet. This study\ndemonstrates that emulating biological intelligence provides a promising path\nto enable deep neural networks with continual learning capabilities.",
      "tldr_zh": "本研究受人类记忆系统启发，提出了一种结合半参数记忆(semi-parametric memory)和醒睡巩固(wake-sleep consolidation)机制的生物模拟框架，旨在解决深度神经网络(DNNs)在连续任务中面临的灾难性遗忘问题。该框架使DNNs能够在处理新任务时保持高性能，同时保留先前知识，并在真实场景如ImageNet的类增量学习中表现出色。该方法证明，模仿生物智能为赋予DNNs持续学习能力提供了可行路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14727v1",
      "published_date": "2025-04-20 19:53:13 UTC",
      "updated_date": "2025-04-20 19:53:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:53:56.340112"
    },
    {
      "arxiv_id": "2504.14709v1",
      "title": "Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Zhou",
        "Shaoshuai Shi",
        "Hongsheng Li"
      ],
      "abstract": "Machine learning (ML)-based planners have recently gained significant\nattention. They offer advantages over traditional optimization-based planning\nalgorithms. These advantages include fewer manually selected parameters and\nfaster development. Within ML-based planning, imitation learning (IL) is a\ncommon algorithm. It primarily learns driving policies directly from supervised\ntrajectory data. While IL has demonstrated strong performance on many open-loop\nbenchmarks, it remains challenging to determine if the learned policy truly\nunderstands fundamental driving principles, rather than simply extrapolating\nfrom the ego-vehicle's initial state. Several studies have identified this\nlimitation and proposed algorithms to address it. However, these methods often\nuse original datasets for evaluation. In these datasets, future trajectories\nare heavily dependent on initial conditions. Furthermore, IL often overfits to\nthe most common scenarios. It struggles to generalize to rare or unseen\nsituations.\n  To address these challenges, this work proposes: 1) a novel closed-loop\nsimulator supporting both imitation and reinforcement learning, 2) a causal\nbenchmark derived from the Waymo Open Dataset to rigorously assess the impact\nof the copycat problem, and 3) a novel framework integrating imitation learning\nand reinforcement learning to overcome the limitations of purely imitative\napproaches. The code for this work will be released soon.",
      "tldr_zh": "本研究揭示了基于Imitation Learning (IL) 的规划器存在的“copycat”问题，即模型可能仅从初始状态外推而非真正理解驾驶原理，导致在罕见场景下泛化能力不足。为解决此问题，论文提出：1) 一个支持IL和Reinforcement Learning (RL) 的新型closed-loop simulator；2) 一个从Waymo Open Dataset衍生的causal benchmark，用于严格评估该问题的影响；3) 一个整合IL和RL的联合框架，以克服纯IL方法的局限性。这些创新工具有助于提升ML-based planners的鲁棒性和泛化性能，代码即将发布。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14709v1",
      "published_date": "2025-04-20 18:51:26 UTC",
      "updated_date": "2025-04-20 18:51:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:54:07.886974"
    },
    {
      "arxiv_id": "2504.14708v1",
      "title": "Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features",
      "title_zh": "翻译失败",
      "authors": [
        "Parshuram N. Aarotale",
        "Ajita Rattani"
      ],
      "abstract": "Electromyography (EMG) based hand gesture recognition converts forearm muscle\nactivity into control commands for prosthetics, rehabilitation, and human\ncomputer interaction. This paper proposes a novel approach to EMG-based hand\ngesture recognition that uses fine-grained classification and presents XMANet,\nwhich unifies low-level local and high level semantic cues through cross layer\nmutual attention among shallow to deep CNN experts. Using stacked spectrograms\nand scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet\nTransform (WT), we benchmark XMANet against ResNet50, DenseNet-121,\nMobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset\nindicate that, using STFT, the proposed XMANet model outperforms the baseline\nResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement\nof approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing\nthe WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are\nobserved over the same baselines. Similarly, on the FORS EMG dataset, the\nXMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the\nbaseline ResNet50. In comparison, the XMANet(DenseNet121) and\nXMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,\nrespectively. Moreover, when using WT, the proposed XMANet achieves gains of\naround 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,\nMobileNetV3, and EfficientNetB0 models, respectively. These results confirm\nthat XMANet consistently improves performance across various architectures and\nsignal processing techniques, demonstrating the strong potential of fine\ngrained features for accurate and robust EMG classification.",
      "tldr_zh": "本论文提出了一种基于EMG（Electromyography）信号的时间频率分析方法，用于手势识别，引入XMANet模型，该模型通过跨层互注意力（cross layer mutual attention）统一低级局部特征和高水平语义线索。XMANet使用Short Time Fourier Transform (STFT)和Wavelet Transform (WT)生成的堆叠频谱图和标度图，在Grabmyo和FORS EMG数据集上与ResNet50、DenseNet-121、MobileNetV3和EfficientNetB0等基线模型进行基准测试，结果显示性能提升1.46%至9.36%。这些发现证实了细粒度特征在提升EMG分类准确性和鲁棒性方面的潜力，为假肢、康复和人机交互应用提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14708v1",
      "published_date": "2025-04-20 18:51:10 UTC",
      "updated_date": "2025-04-20 18:51:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:54:21.767903"
    },
    {
      "arxiv_id": "2504.14706v2",
      "title": "AI with Emotions: Exploring Emotional Expressions in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shin-nosuke Ishikawa",
        "Atsushi Yoshino"
      ],
      "abstract": "The human-level performance of Large Language Models (LLMs) across various\ntasks has raised expectations for the potential of Artificial Intelligence (AI)\nto possess emotions someday. To explore the capability of current LLMs to\nexpress emotions in their outputs, we conducted an experiment using several\nLLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to\nrole-play as agents answering questions with specified emotional states. We\ndefined the emotional states using Russell's Circumplex model, a\nwell-established framework that characterizes emotions along the\nsleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose\nthis model for its simplicity, utilizing two continuous parameters, which\nallows for better controllability in applications involving continuous changes\nin emotional states. The responses generated were evaluated using a sentiment\nanalysis model, independent of the LLMs, trained on the GoEmotions dataset. The\nevaluation showed that the emotional states of the generated answers were\nconsistent with the specifications, demonstrating the LLMs' capability for\nemotional expression. This indicates the potential for LLM-based AI agents to\nsimulate emotions, opening up a wide range of applications for emotion-based\ninteractions, such as advisors or consultants who can provide advice or\nopinions with a personal touch.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）表达情绪的能力，旨在评估AI是否能模拟人类情感。研究者使用Russell's Circumplex model（基于arousal和valence轴）定义情绪状态，让多个LLMs（如OpenAI GPT和Google Gemini）在指定情绪下回答问题，并通过基于GoEmotions数据集的sentiment analysis模型进行独立评估。结果显示，LLMs生成的响应情绪与指定状态高度一致，证明了其情感表达潜力，并为开发情感互动AI代理（如情感化的顾问）打开了新应用前景。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 8 figures, accepted to the Natural Language Processing for\n  Digital Humanities (NLP4DH) workshop at NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14706v2",
      "published_date": "2025-04-20 18:49:25 UTC",
      "updated_date": "2025-04-22 02:10:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:54:31.934601"
    },
    {
      "arxiv_id": "2504.14704v1",
      "title": "Can We Ignore Labels In Out of Distribution Detection?",
      "title_zh": "我们能在分布外检测中忽略标签吗？",
      "authors": [
        "Hong Yang",
        "Qi Yu",
        "Travis Desel"
      ],
      "abstract": "Out-of-distribution (OOD) detection methods have recently become more\nprominent, serving as a core element in safety-critical autonomous systems. One\nmajor purpose of OOD detection is to reject invalid inputs that could lead to\nunpredictable errors and compromise safety. Due to the cost of labeled data,\nrecent works have investigated the feasibility of self-supervised learning\n(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In\nthis work, we identify a set of conditions for a theoretical guarantee of\nfailure in unlabeled OOD detection algorithms from an information-theoretic\nperspective. These conditions are present in all OOD tasks dealing with\nreal-world data: I) we provide theoretical proof of unlabeled OOD detection\nfailure when there exists zero mutual information between the learning\nobjective and the in-distribution labels, a.k.a. 'label blindness', II) we\ndefine a new OOD task - Adjacent OOD detection - that tests for label blindness\nand accounts for a previously ignored safety gap in all OOD detection\nbenchmarks, and III) we perform experiments demonstrating that existing\nunlabeled OOD methods fail under conditions suggested by our label blindness\ntheory and analyze the implications for future research in unlabeled OOD\nmethods.",
      "tldr_zh": "这篇论文探讨了在 Out-of-Distribution (OOD) 检测中是否可以忽略标签的问题，强调了无标签 OOD 检测方法的局限性。作者从信息论视角证明，当学习目标与 in-distribution 标签之间互信息为零（即 'label blindness'）时，无标签 OOD 检测算法会失败，并定义了新的 Adjacent OOD 检测任务来测试这一现象，同时指出了现有基准中被忽略的安全差距。通过实验，论文展示了现有无标签 OOD 方法在这些条件下失败，并为未来研究提供了重要启示。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14704v1",
      "published_date": "2025-04-20 18:37:51 UTC",
      "updated_date": "2025-04-20 18:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:54:44.810208"
    },
    {
      "arxiv_id": "2504.14699v1",
      "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays",
      "title_zh": "翻译失败",
      "authors": [
        "Sascha Jecklin",
        "Aidana Massalimova",
        "Ruyi Zha",
        "Lilian Calvet",
        "Christoph J. Laux",
        "Mazda Farshad",
        "Philipp Fürnstahl"
      ],
      "abstract": "Spine surgery is a high-risk intervention demanding precise execution, often\nsupported by image-based navigation systems. Recently, supervised learning\napproaches have gained attention for reconstructing 3D spinal anatomy from\nsparse fluoroscopic data, significantly reducing reliance on\nradiation-intensive 3D imaging systems. However, these methods typically\nrequire large amounts of annotated training data and may struggle to generalize\nacross varying patient anatomies or imaging conditions. Instance-learning\napproaches like Gaussian splatting could offer an alternative by avoiding\nextensive annotation requirements. While Gaussian splatting has shown promise\nfor novel view synthesis, its application to sparse, arbitrarily posed real\nintraoperative X-rays has remained largely unexplored. This work addresses this\nlimitation by extending the $R^2$-Gaussian splatting framework to reconstruct\nanatomically consistent 3D volumes under these challenging conditions. We\nintroduce an anatomy-guided radiographic standardization step using style\ntransfer, improving visual consistency across views, and enhancing\nreconstruction quality. Notably, our framework requires no pretraining, making\nit inherently adaptable to new patients and anatomies. We evaluated our\napproach using an ex-vivo dataset. Expert surgical evaluation confirmed the\nclinical utility of the 3D reconstructions for navigation, especially when\nusing 20 to 30 views, and highlighted the standardization's benefit for\nanatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)\nconfirmed performance trade-offs compared to idealized settings, but also\nvalidated the improvement gained from standardization over raw inputs. This\nwork demonstrates the feasibility of instance-based volumetric reconstruction\nfrom arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for\nsurgical navigation.",
      "tldr_zh": "这篇论文提出了一种名为IXGS的框架，利用R²-Gaussian splatting扩展技术，从稀疏、任意姿态的术中X射线重建解剖一致的3D脊柱体积，从而减少对辐射密集型成像系统的依赖，并避免了传统监督学习方法的标注数据需求。关键创新包括引入基于风格转移的解剖引导放射标准化步骤，以提升视图间视觉一致性和重建质量，该框架无需预训练，因此对新患者解剖具有高度适应性。在体外数据集上的评估显示，使用20-30个视图时，3D重建在手术导航中具有临床实用性，且通过PSNR/SSIM等量化指标证实了标准化步骤的性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14699v1",
      "published_date": "2025-04-20 18:28:13 UTC",
      "updated_date": "2025-04-20 18:28:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:54:57.224612"
    },
    {
      "arxiv_id": "2504.14694v1",
      "title": "Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data",
      "title_zh": "翻译失败",
      "authors": [
        "Yuting He",
        "Yiqiang Chen",
        "XiaoDong Yang",
        "Hanchao Yu",
        "Yi-Hua Huang",
        "Yang Gu"
      ],
      "abstract": "Federated learning (FL) enables multiple clients to collaboratively train a\nglobal model while keeping local data decentralized. Data heterogeneity\n(non-IID) across clients has imposed significant challenges to FL, which makes\nlocal models re-optimize towards their own local optima and forget the global\nknowledge, resulting in performance degradation and convergence slowdown. Many\nexisting works have attempted to address the non-IID issue by adding an extra\nglobal-model-based regularizing item to the local training but without an\nadaption scheme, which is not efficient enough to achieve high performance with\ndeep learning models. In this paper, we propose a Selective Self-Distillation\nmethod for Federated learning (FedSSD), which imposes adaptive constraints on\nthe local updates by self-distilling the global model's knowledge and\nselectively weighting it by evaluating the credibility at both the class and\nsample level. The convergence guarantee of FedSSD is theoretically analyzed and\nextensive experiments are conducted on three public benchmark datasets, which\ndemonstrates that FedSSD achieves better generalization and robustness in fewer\ncommunication rounds, compared with other state-of-the-art FL methods.",
      "tldr_zh": "本研究针对联邦学习 (Federated Learning, FL) 中非独立同分布 (non-IID) 数据导致的性能下降和收敛变慢问题，提出了一种Selective Self-Distillation方法 (FedSSD)。FedSSD 通过自蒸馏全局模型的知识，并在类级和样本级评估可信度来选择性地加权本地更新，从而实现自适应约束。实验结果显示，该方法在三个公共基准数据集上比现有FL方法实现了更好的泛化和鲁棒性，并在更少的通信轮次中收敛。理论分析还证明了FedSSD的收敛保证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14694v1",
      "published_date": "2025-04-20 18:06:55 UTC",
      "updated_date": "2025-04-20 18:06:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:55:08.504179"
    },
    {
      "arxiv_id": "2504.18562v1",
      "title": "Deep Learning with Pretrained 'Internal World' Layers: A Gemma 3-Based Modular Architecture for Wildfire Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Ayoub Jadouli",
        "Chaker El Amrani"
      ],
      "abstract": "Deep learning models, especially large Transformers, carry substantial\n\"memory\" in their intermediate layers -- an \\emph{internal world} that encodes\na wealth of relational and contextual knowledge. This work harnesses that\ninternal world for wildfire occurrence prediction by introducing a modular\narchitecture built upon Gemma 3, a state-of-the-art multimodal model. Rather\nthan relying on Gemma 3's original embedding and positional encoding stacks, we\ndevelop a custom feed-forward module that transforms tabular wildfire features\ninto the hidden dimension required by Gemma 3's mid-layer Transformer blocks.\nWe freeze these Gemma 3 sub-layers -- thus preserving their pretrained\nrepresentation power -- while training only the smaller input and output\nnetworks. This approach minimizes the number of trainable parameters and\nreduces the risk of overfitting on limited wildfire data, yet retains the\nbenefits of Gemma 3's broad knowledge. Evaluations on a Moroccan wildfire\ndataset demonstrate improved predictive accuracy and robustness compared to\nstandard feed-forward and convolutional baselines. Ablation studies confirm\nthat the frozen Transformer layers consistently contribute to better\nrepresentations, underscoring the feasibility of reusing large-model mid-layers\nas a learned internal world. Our findings suggest that strategic modular reuse\nof pretrained Transformers can enable more data-efficient and interpretable\nsolutions for critical environmental applications such as wildfire risk\nmanagement.",
      "tldr_zh": "该论文提出了一种基于 Gemma 3 模型的模块化架构，用于野火发生预测，通过利用模型的预训练 'internal world' 层（中间层）来捕捉丰富的关系和上下文知识。方法包括开发自定义 feed-forward module，将表格野火特征转换为 Gemma 3 的隐藏维度，并冻结其中间 Transformer blocks，只训练较小的输入和输出网络，从而减少可训练参数并降低过拟合风险。在摩洛哥野火数据集上的评估显示，该架构比标准 feed-forward 和卷积基线提高了预测准确性和鲁棒性。研究结果证实，重用预训练 Transformer 层的策略为野火风险管理等环境应用提供了更数据高效和可解释的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.18562v1",
      "published_date": "2025-04-20 18:02:22 UTC",
      "updated_date": "2025-04-20 18:02:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:55:23.331073"
    },
    {
      "arxiv_id": "2504.14693v2",
      "title": "Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Enxin Song",
        "Wenhao Chai",
        "Weili Xu",
        "Jianwen Xie",
        "Yuxuan Liu",
        "Gaoang Wang"
      ],
      "abstract": "Recent advancements in language multimodal models (LMMs) for video have\ndemonstrated their potential for understanding video content, yet the task of\ncomprehending multi-discipline lectures remains largely unexplored. We\nintroduce Video-MMLU, a massive benchmark designed to evaluate the capabilities\nof LMMs in understanding Multi-Discipline Lectures. We evaluate over 90\nopen-source and proprietary models, ranging from 0.5B to 40B parameters. Our\nresults highlight the limitations of current models in addressing the cognitive\nchallenges presented by these lectures, especially in tasks requiring both\nperception and reasoning. Additionally, we explore how the number of visual\ntokens and the large language models influence performance, offering insights\ninto the interplay between multimodal perception and reasoning in lecture\ncomprehension.",
      "tldr_zh": "这篇论文引入了 Video-MMLU，一个大规模基准测试，用于评估语言多模态模型(LMMs)在多学科讲座理解方面的能力。研究者评估了超过90个开源和专有模型，从0.5B到40B参数，结果显示这些模型在处理涉及感知和推理的认知挑战时存在显著局限。论文进一步探讨了视觉标记数量和大型语言模型对性能的影响，提供对多模态感知与推理互动的宝贵洞见。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code, docs, and benchmark are all avaliable at\n  https://enxinsong.com/Video-MMLU-web/",
      "pdf_url": "http://arxiv.org/pdf/2504.14693v2",
      "published_date": "2025-04-20 17:58:46 UTC",
      "updated_date": "2025-05-02 22:30:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:55:32.154267"
    },
    {
      "arxiv_id": "2504.14690v1",
      "title": "FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models",
      "title_zh": "FarsEval-PKBETS：用于评估波斯语大型语言模型的一个新的多样化基准",
      "authors": [
        "Mehrnoush Shamsfard",
        "Zahra Saaberi",
        "Mostafa Karimi manesh",
        "Seyed Mohammad Hossein Hashemi",
        "Zahra Vatankhah",
        "Motahareh Ramezani",
        "Niki Pourazin",
        "Tara Zare",
        "Maryam Azimi",
        "Sarina Chitsaz",
        "Sama Khoraminejad",
        "Morteza Mahdavi Mortazavi",
        "Mohammad Mahdi Chizari",
        "Sahar Maleki",
        "Seyed Soroush Majd",
        "Mostafa Masumi",
        "Sayed Ali Musavi Khoeini",
        "Amir Mohseni",
        "Sogol Alipour"
      ],
      "abstract": "Research on evaluating and analyzing large language models (LLMs) has been\nextensive for resource-rich languages such as English, yet their performance in\nlanguages such as Persian has received considerably less attention. This paper\nintroduces FarsEval-PKBETS benchmark, a subset of FarsEval project for\nevaluating large language models in Persian. This benchmark consists of 4000\nquestions and answers in various formats, including multiple choice, short\nanswer and descriptive responses. It covers a wide range of domains and\ntasks,including medicine, law, religion, Persian language, encyclopedic\nknowledge, human preferences, social knowledge, ethics and bias, text\ngeneration, and respecting others' rights. This bechmark incorporates\nlinguistics, cultural, and local considerations relevant to the Persian\nlanguage and Iran. To ensure the questions are challenging for current LLMs,\nthree models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this\nbenchmark. Their average accuracy was below 50%, meaning they provided fully\ncorrect answers to fewer than half of the questions. These results indicate\nthat current language models are still far from being able to solve this\nbenchmark",
      "tldr_zh": "这篇论文引入了 FarsEval-PKBETS 基准，这是一个用于评估波斯语大语言模型 (LLMs) 的多样化子集，包含 4000 个问题和答案，涵盖多项选择、短答案和描述性响应。基准涉及医学、法律、宗教、波斯语、百科知识、人类偏好、社会知识、伦理与偏见、文本生成等领域，并融入波斯语的文化、语言和本地因素，以确保挑战性。测试结果显示，Llama3-70B、PersianMind 和 Dorna 等模型的平均准确率低于 50%，表明当前 LLMs 在处理波斯语任务时仍存在显著不足，需要进一步改进。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7; E.0"
      ],
      "primary_category": "cs.CL",
      "comment": "24 pages, 3 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.14690v1",
      "published_date": "2025-04-20 17:43:47 UTC",
      "updated_date": "2025-04-20 17:43:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:55:45.269003"
    },
    {
      "arxiv_id": "2504.14686v1",
      "title": "Uncovering Issues in the Radio Access Network by Looking at the Neighbors",
      "title_zh": "通过观察邻居揭示无线接入网中的问题",
      "authors": [
        "José Suárez-Varela",
        "Andra Lutu"
      ],
      "abstract": "Mobile network operators (MNOs) manage Radio Access Networks (RANs) with\nmassive amounts of cells over multiple radio generations (2G-5G). To handle\nsuch complexity, operations teams rely on monitoring systems, including anomaly\ndetection tools that identify unexpected behaviors. In this paper, we present\nc-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph\nNeural Networks (GNNs). Our solution captures spatio-temporal variations by\nanalyzing the behavior of individual cells in relation to their local\nneighborhoods, enabling the detection of anomalies that are independent of\nexternal mobility factors. This, in turn, allows focusing on anomalies\nassociated with network issues (e.g., misconfigurations, equipment failures).\nWe evaluate c-ANEMON using real-world data from a large European metropolitan\narea (7,890 cells; 3 months). First, we show that the GNN model within our\nsolution generalizes effectively to cells from previously unseen areas,\nsuggesting the possibility of using a single model across extensive deployment\nregions. Then, we analyze the anomalies detected by c-ANEMON through manual\ninspection and define several categories of long-lasting anomalies (6+ hours).\nNotably, 45.95% of these anomalies fall into a category that is more likely to\nrequire intervention by operations teams.",
      "tldr_zh": "本研究针对移动网络运营商（MNOs）管理Radio Access Networks (RANs)的复杂性，提出c-ANEMON，一种基于Graph Neural Networks (GNNs)的上下文异常检测监控系统。该系统通过分析单个细胞与其邻居关系来捕捉时空变化，从而检测与网络问题（如配置错误或设备故障）相关的异常，而非外部因素。实验使用欧洲大都市的真实数据（7,890个细胞，3个月）验证，c-ANEMON的GNN模型能有效泛化到未见区域，并显示检测到的异常中45.95%属于需操作团队干预的类别，为RAN管理提供更可靠的工具。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2504.14686v1",
      "published_date": "2025-04-20 17:36:52 UTC",
      "updated_date": "2025-04-20 17:36:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:55:56.563811"
    },
    {
      "arxiv_id": "2504.14681v1",
      "title": "An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework",
      "title_zh": "基于LLM的多智能体自治机电一体化设计框架",
      "authors": [
        "Zeyu Wang",
        "Frank P. -W. Lo",
        "Qian Chen",
        "Yongqi Zhang",
        "Chen Lin",
        "Xu Chen",
        "Zhenhua Yu",
        "Alexander J. Thompson",
        "Eric M. Yeatman",
        "Benny P. L. Lo"
      ],
      "abstract": "Existing LLM-enabled multi-agent frameworks are predominantly limited to\ndigital or simulated environments and confined to narrowly focused knowledge\ndomain, constraining their applicability to complex engineering tasks that\nrequire the design of physical embodiment, cross-disciplinary integration, and\nconstraint-aware reasoning. This work proposes a multi-agent autonomous\nmechatronics design framework, integrating expertise across mechanical design,\noptimization, electronics, and software engineering to autonomously generate\nfunctional prototypes with minimal direct human design input. Operating\nprimarily through a language-driven workflow, the framework incorporates\nstructured human feedback to ensure robust performance under real-world\nconstraints. To validate its capabilities, the framework is applied to a\nreal-world challenge involving autonomous water-quality monitoring and\nsampling, where traditional methods are labor-intensive and ecologically\ndisruptive. Leveraging the proposed system, a fully functional autonomous\nvessel was developed with optimized propulsion, cost-effective electronics, and\nadvanced control. The design process was carried out by specialized agents,\nincluding a high-level planning agent responsible for problem abstraction and\ndedicated agents for structural, electronics, control, and software\ndevelopment. This approach demonstrates the potential of LLM-based multi-agent\nsystems to automate real-world engineering workflows and reduce reliance on\nextensive domain expertise.",
      "tldr_zh": "该研究提出了一种基于LLM的多智能体自主机电设计框架，旨在克服现有框架在数字环境和狭窄知识领域的局限性，通过整合机械设计、优化、电子和软件工程等跨学科专家，实现最小人类干预下的功能原型生成。框架采用语言驱动的工作流，并融入结构化人类反馈，确保在真实约束下进行高效、约束感知的推理和设计。在实际应用中，该系统用于自主水质监测和采样任务，成功开发出一艘功能齐全的自主船只，具备优化推进、成本有效电子设备和高级控制系统，展示了LLM多智能体系统在自动化工程流程和减少领域专长依赖方面的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by CVPR 2025 Workshop",
      "pdf_url": "http://arxiv.org/pdf/2504.14681v1",
      "published_date": "2025-04-20 16:57:45 UTC",
      "updated_date": "2025-04-20 16:57:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:56:09.305347"
    },
    {
      "arxiv_id": "2504.14677v1",
      "title": "Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental Fine-tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Jia Liu",
        "Cheng Jinguo",
        "Xia Fang",
        "Zhenyuan Ma",
        "Yuankai Wu"
      ],
      "abstract": "Time series foundation models excel at diverse time series forecasting tasks,\nbut their capacity for continuous improvement through incremental learning\nremains unexplored. We present the first comprehensive study investigating\nthese models' temporal plasticity - their ability to progressively enhance\nperformance through continual learning while maintaining existing capabilities.\nThrough experiments on real-world datasets exhibiting distribution shifts, we\nevaluate both conventional deep learning models and foundation models using a\nnovel continual learning framework. Our findings reveal that while traditional\nmodels struggle with performance deterioration during incremental fine-tuning,\nfoundation models like Time-MoE and Chronos demonstrate sustained improvement\nin predictive accuracy. This suggests that optimizing foundation model\nfine-tuning strategies may be more valuable than developing domain-specific\nsmall models. Our research introduces new evaluation methodologies and insights\nfor developing foundation time series models with robust continuous learning\ncapabilities.",
      "tldr_zh": "本研究首次全面评估了时间序列基础模型（foundation models）在增量微调（incremental fine-tuning）中的时间可塑性（temporal plasticity），即通过持续学习（continual learning）逐步提升性能的能力，同时保持现有功能。研究采用一个新颖的持续学习框架，在显示分布偏移的真实数据集上进行实验，比较了传统深度学习模型和基础模型如 Time-MoE 和 Chronos。结果表明，传统模型在增量微调过程中性能恶化，而基础模型实现了预测准确性的持续改进。总之，该研究建议优化基础模型的微调策略比开发特定领域的微型模型更具价值，并提供了新的评估方法论，以推动时间序列模型的鲁棒持续学习能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at IJCNN 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14677v1",
      "published_date": "2025-04-20 16:43:01 UTC",
      "updated_date": "2025-04-20 16:43:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:56:20.568123"
    },
    {
      "arxiv_id": "2504.14657v2",
      "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Yihan Lin",
        "Zhirong Bella Yu",
        "Simon Lee"
      ],
      "abstract": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings.",
      "tldr_zh": "本研究通过案例分析探讨了使用商业大型语言模型（LLMs）生成合成电子健康记录（EHRs）的现状，强调了合成数据在隐私保护、数据模式控制以及患者群体公平性方面的关键优势。研究评估了生成过程的多个方面，发现LLMs在处理小特征集时能够可靠地创建合成健康记录，但随着数据维度的增加，它们难以保持现实分布和相关性。最终，这限制了LLMs在不同医院环境中的泛化能力，为未来改进提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14657v2",
      "published_date": "2025-04-20 15:37:05 UTC",
      "updated_date": "2025-04-25 06:34:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:56:31.884276"
    },
    {
      "arxiv_id": "2504.14650v1",
      "title": "A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yuting Huang",
        "Leilei Ding",
        "Zhipeng Tang",
        "Tianfu Wang",
        "Xinrui Lin",
        "Wuyang Zhang",
        "Mingxiao Ma",
        "Yanyong Zhang"
      ],
      "abstract": "Large Language Models (LLMs) exhibit substantial promise in enhancing\ntask-planning capabilities within embodied agents due to their advanced\nreasoning and comprehension. However, the systemic safety of these agents\nremains an underexplored frontier. In this study, we present Safe-BeAl, an\nintegrated framework for the measurement (SafePlan-Bench) and alignment\n(Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench\nestablishes a comprehensive benchmark for evaluating task-planning safety,\nencompassing 2,027 daily tasks and corresponding environments distributed\nacross 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis\nreveals that even in the absence of adversarial inputs or malicious intent,\nLLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we\npropose Safe-Align, a method designed to integrate physical-world safety\nknowledge into LLM-based embodied agents while maintaining task-specific\nperformance. Experiments across a variety of settings demonstrate that\nSafe-BeAl provides comprehensive safety validation, improving safety by 8.55 -\n15.22%, compared to embodied agents based on GPT-4, while ensuring successful\ntask completion.",
      "tldr_zh": "该研究提出Safe-BeAl框架，用于评估和提升LLM-based embodied agents在任务规划中的安全性能。框架包括SafePlan-Bench基准，它涵盖2,027个日常任务和8个危险类别（如Fire Hazard），用于全面评估代理的安全性；以及Safe-Align方法，通过整合物理世界安全知识来缓解LLM代理在无恶意输入下可能出现的不安全行为。实验结果显示，Safe-BeAl相较于基于GPT-4的代理，提高了8.55%至15.22%的安全性能，同时保持了任务完成率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.14650v1",
      "published_date": "2025-04-20 15:12:14 UTC",
      "updated_date": "2025-04-20 15:12:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:56:44.344450"
    },
    {
      "arxiv_id": "2504.14645v1",
      "title": "Surrogate Fitness Metrics for Interpretable Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Altmann",
        "Céline Davignon",
        "Maximilian Zorn",
        "Fabian Ritz",
        "Claudia Linnhoff-Popien",
        "Thomas Gabor"
      ],
      "abstract": "We employ an evolutionary optimization framework that perturbs initial states\nto generate informative and diverse policy demonstrations. A joint surrogate\nfitness function guides the optimization by combining local diversity,\nbehavioral certainty, and global population diversity. To assess demonstration\nquality, we apply a set of evaluation metrics, including the reward-based\noptimality gap, fidelity interquartile means (IQMs), fitness composition\nanalysis, and trajectory visualizations. Hyperparameter sensitivity is also\nexamined to better understand the dynamics of trajectory optimization. Our\nfindings demonstrate that optimizing trajectory selection via surrogate fitness\nmetrics significantly improves interpretability of RL policies in both discrete\nand continuous environments. In gridworld domains, evaluations reveal\nsignificantly enhanced demonstration fidelities compared to random and ablated\nbaselines. In continuous control, the proposed framework offers valuable\ninsights, particularly for early-stage policies, while fidelity-based\noptimization proves more effective for mature policies. By refining and\nsystematically analyzing surrogate fitness functions, this study advances the\ninterpretability of RL models. The proposed improvements provide deeper\ninsights into RL decision-making, benefiting applications in safety-critical\nand explainability-focused domains.",
      "tldr_zh": "本研究提出了一种进化优化框架，通过扰动初始状态生成信息丰富且多样的策略演示，以提升强化学习（RL）策略的可解释性。该框架使用一个联合代理适应度函数（surrogate fitness function），结合局部多样性、行为确定性和全局种群多样性来指导优化。评估指标包括奖励-based 最优差距、fidelity IQMs、适应度组成分析和轨迹可视化，结果显示该方法在gridworld等离散环境和连续控制环境中显著提高了演示保真度，并为安全关键领域提供更深入的决策洞见。通过系统分析超参数敏感性，该研究为RL模型的可解释性提供了重要改进。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 7 figures, under review",
      "pdf_url": "http://arxiv.org/pdf/2504.14645v1",
      "published_date": "2025-04-20 15:01:19 UTC",
      "updated_date": "2025-04-20 15:01:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:56:55.902010"
    },
    {
      "arxiv_id": "2504.14640v1",
      "title": "Risk Assessment Framework for Code LLMs via Leveraging Internal States",
      "title_zh": "通过利用内部状态的代码大语言模型风险评估",
      "authors": [
        "Yuheng Huang",
        "Lei Ma",
        "Keizaburo Nishikino",
        "Takumi Akazaki"
      ],
      "abstract": "The pre-training paradigm plays a key role in the success of Large Language\nModels (LLMs), which have been recognized as one of the most significant\nadvancements of AI recently. Building on these breakthroughs, code LLMs with\nadvanced coding capabilities bring huge impacts on software engineering,\nshowing the tendency to become an essential part of developers' daily routines.\nHowever, the current code LLMs still face serious challenges related to\ntrustworthiness, as they can generate incorrect, insecure, or unreliable code.\nRecent exploratory studies find that it can be promising to detect such risky\noutputs by analyzing LLMs' internal states, akin to how the human brain\nunconsciously recognizes its own mistakes. Yet, most of these approaches are\nlimited to narrow sub-domains of LLM operations and fall short of achieving\nindustry-level scalability and practicability. To address these challenges, in\nthis paper, we propose PtTrust, a two-stage risk assessment framework for code\nLLM based on internal state pre-training, designed to integrate seamlessly with\nthe existing infrastructure of software companies. The core idea is that the\nrisk assessment framework could also undergo a pre-training process similar to\nLLMs. Specifically, PtTrust first performs unsupervised pre-training on\nlarge-scale unlabeled source code to learn general representations of LLM\nstates. Then, it uses a small, labeled dataset to train a risk predictor. We\ndemonstrate the effectiveness of PtTrust through fine-grained, code line-level\nrisk assessment and demonstrate that it generalizes across tasks and different\nprogramming languages. Further experiments also reveal that PtTrust provides\nhighly intuitive and interpretable features, fostering greater user trust. We\nbelieve PtTrust makes a promising step toward scalable and trustworthy\nassurance for code LLMs.",
      "tldr_zh": "该研究针对代码LLM（Large Language Models）生成的不正确、不安全或不可靠代码问题，提出PtTrust框架，通过利用LLM的内部状态进行风险评估。PtTrust采用两阶段方法：首先在大规模无标签源代码上进行无监督预训练，以学习LLM状态的通用表示；然后使用小规模标记数据集训练风险预测器，实现细粒度的代码行级风险评估。实验结果显示，PtTrust在不同任务和编程语言中具有良好的泛化性，并提供直观的解释特征，从而提升了代码LLM的可扩展性和用户信任。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "To appear in the 33rd ACM International Conference on the Foundations\n  of Software Engineering (FSE Companion'25 Industry Track), June 23-28, 2025,\n  Trondheim, Norway. This work was supported by Fujitsu Limited",
      "pdf_url": "http://arxiv.org/pdf/2504.14640v1",
      "published_date": "2025-04-20 14:44:18 UTC",
      "updated_date": "2025-04-20 14:44:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:57:07.442199"
    },
    {
      "arxiv_id": "2504.14636v1",
      "title": "AlphaZero-Edu: Making AlphaZero Accessible to Everyone",
      "title_zh": "翻译失败",
      "authors": [
        "Binjie Guo",
        "Hanyu Zheng",
        "Guowei Su",
        "Ru Zhang",
        "Haohan Jiang",
        "Xurong Lin",
        "Hongyan Wei",
        "Aisheng Mo",
        "Jie Li",
        "Zhiyuan Qian",
        "Zhuhao Zhang",
        "Xiaoyuan Cheng"
      ],
      "abstract": "Recent years have witnessed significant progress in reinforcement learning,\nespecially with Zero-like paradigms, which have greatly boosted the\ngeneralization and reasoning abilities of large-scale language models.\nNevertheless, existing frameworks are often plagued by high implementation\ncomplexity and poor reproducibility. To tackle these challenges, we present\nAlphaZero-Edu, a lightweight, education-focused implementation built upon the\nmathematical framework of AlphaZero. It boasts a modular architecture that\ndisentangles key components, enabling transparent visualization of the\nalgorithmic processes. Additionally, it is optimized for resource-efficient\ntraining on a single NVIDIA RTX 3090 GPU and features highly parallelized\nself-play data generation, achieving a 3.2-fold speedup with 8 processes. In\nGomoku matches, the framework has demonstrated exceptional performance,\nachieving a consistently high win rate against human opponents. AlphaZero-Edu\nhas been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu,\nproviding an accessible and practical benchmark for both academic research and\nindustrial applications.",
      "tldr_zh": "该研究针对强化学习（reinforcement learning）领域的实现复杂性和可重复性问题，提出 AlphaZero-Edu，这是一个轻量级、教育导向的框架，基于 AlphaZero 的数学框架设计。AlphaZero-Edu 采用模块化架构，便于算法过程的透明可视化，并优化了资源效率，可在单块 NVIDIA RTX 3090 GPU 上运行，并通过高度并行化的自博弈（self-play）数据生成实现 3.2 倍速度提升。实验结果显示，该框架在 Gomoku 游戏中对人类对手表现出色，胜率极高，并已开源（https://github.com/StarLight1212/AlphaZero_Edu），为学术研究和工业应用提供可访问的基准。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14636v1",
      "published_date": "2025-04-20 14:29:39 UTC",
      "updated_date": "2025-04-20 14:29:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:57:20.420932"
    },
    {
      "arxiv_id": "2504.14625v3",
      "title": "Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Haiyan Qin",
        "Jiahao Feng",
        "Xiaotong Feng",
        "Wei W. Xing",
        "Wang Kang"
      ],
      "abstract": "Large language models (LLMs) have transformed code generation, yet their\napplication in hardware design produces gate counts 38\\%--1075\\% higher than\nhuman designs. We present CircuitMind, a multi-agent framework that achieves\nhuman-competitive efficiency through three key innovations: syntax locking\n(constraining generation to basic logic gates), retrieval-augmented generation\n(enabling knowledge-driven design), and dual-reward optimization (balancing\ncorrectness with efficiency). To evaluate our approach, we introduce TC-Bench,\nthe first gate-level benchmark harnessing collective intelligence from the\nTuringComplete ecosystem -- a competitive circuit design platform with hundreds\nof thousands of players. Experiments show CircuitMind enables 55.6\\% of model\nimplementations to match or exceed top-tier human experts in composite\nefficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model\nto outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency\ncomparable to the top 25\\% of human experts without requiring specialized\ntraining. These innovations establish a new paradigm for hardware optimization\nwhere collaborative AI systems leverage collective human expertise to achieve\noptimal circuit designs. Our model, data, and code are open-source at\nhttps://github.com/BUAA-CLab/CircuitMind.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在硬件设计中产生的电路门数量过高的问题，提出CircuitMind多智能体框架，通过syntax locking（约束生成到基本逻辑门）、retrieval-augmented generation（知识驱动设计）和dual-reward optimization（平衡正确性和效率）三大创新，实现与人类专家相当的电路生成效率。研究引入TC-Bench，这是首个基于TuringComplete生态系统的门级基准，利用集体智能评估框架性能。实验结果显示，CircuitMind使55.6%的模型实现匹配或超过顶级人类专家的效率指标，并让14B Phi-4模型超越GPT-4o mini和Gemini 2.0 Flash，达到顶级25%人类水平，从而开辟了协作AI系统利用集体人类专业知识优化电路设计的新范式。模型、数据和代码已开源。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "9 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.14625v3",
      "published_date": "2025-04-20 14:05:17 UTC",
      "updated_date": "2025-05-01 03:43:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:57:33.773207"
    },
    {
      "arxiv_id": "2504.14624v1",
      "title": "Consensus in Motion: A Case of Dynamic Rationality of Sequential Learning in Probability Aggregation",
      "title_zh": "翻译失败",
      "authors": [
        "Polina Gordienko",
        "Christoph Jansen",
        "Thomas Augustin",
        "Martin Rechenauer"
      ],
      "abstract": "We propose a framework for probability aggregation based on propositional\nprobability logic. Unlike conventional judgment aggregation, which focuses on\nstatic rationality, our model addresses dynamic rationality by ensuring that\ncollective beliefs update consistently with new information. We show that any\nconsensus-compatible and independent aggregation rule on a non-nested agenda is\nnecessarily linear. Furthermore, we provide sufficient conditions for a fair\nlearning process, where individuals initially agree on a specified subset of\npropositions known as the common ground, and new information is restricted to\nthis shared foundation. This guarantees that updating individual judgments via\nBayesian conditioning-whether performed before or after aggregation-yields the\nsame collective belief. A distinctive feature of our framework is its treatment\nof sequential decision-making, which allows new information to be incorporated\nprogressively through multiple stages while maintaining the established common\nground. We illustrate our findings with a running example in a political\nscenario concerning healthcare and immigration policies.",
      "tldr_zh": "本研究提出一个基于propositional probability logic的概率聚合框架，强调动态理性，以确保集体信念在新信息下一致更新，从而解决传统判断聚合的静态局限。框架证明了任何共识兼容和独立的聚合规则在非-nested agenda上必须是线性的，并提供了公平学习过程的充分条件：个体在common ground上初始同意，新信息限于此基础，通过Bayesian conditioning更新判断（无论先聚合还是后）可获得相同集体信念。该框架支持顺序决策，允许新信息逐步整合，并以一个政治场景（医疗和移民政策）为例，展示了其在实际应用中的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to the International Conference on Modeling Decisions for\n  Artificial Intelligence (MDAI 2025)",
      "pdf_url": "http://arxiv.org/pdf/2504.14624v1",
      "published_date": "2025-04-20 14:04:39 UTC",
      "updated_date": "2025-04-20 14:04:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:57:45.110556"
    },
    {
      "arxiv_id": "2504.14618v1",
      "title": "VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image",
      "title_zh": "翻译失败",
      "authors": [
        "Han Bi",
        "Ge Yu",
        "Yu He",
        "Wenzhuo Liu",
        "Zijie Zheng"
      ],
      "abstract": "Understanding bimanual hand interactions is essential for realistic 3D pose\nand shape reconstruction. However, existing methods struggle with occlusions,\nambiguous appearances, and computational inefficiencies. To address these\nchallenges, we propose Vision Mamba Bimanual Hand Interaction Network\n(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to\nenhance interaction modeling while improving computational efficiency. The core\ncomponent, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),\ncombines SSMs with local and global feature operations, enabling deep\nunderstanding of hand interactions. Experiments on the InterHand2.6M dataset\nshow that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean\nper-vertex position error (MPVPE) by 2-3%, significantly surpassing\nstate-of-the-art methods.",
      "tldr_zh": "该论文提出VM-BHINet，一种Vision Mamba Bimanual Hand Interaction Network，用于从单张RGB图像中恢复3D互动手部网格，针对双手互动重建中的遮挡、模糊外观和计算效率问题。核心组件Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock)结合SSMs与局部和全局特征操作，增强手部互动建模。实验在InterHand2.6M数据集上显示，VM-BHINet将MPJPE和MPVPE错误降低了2-3%，显著超越现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14618v1",
      "published_date": "2025-04-20 13:54:22 UTC",
      "updated_date": "2025-04-20 13:54:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:57:56.644839"
    },
    {
      "arxiv_id": "2504.15317v1",
      "title": "Enhancing DR Classification with Swin Transformer and Shifted Window Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Meher Boulaabi",
        "Takwa Ben Aïcha Gader",
        "Afef Kacem Echi",
        "Zied Bouraoui"
      ],
      "abstract": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide,\nunderscoring the importance of early detection for effective treatment.\nHowever, automated DR classification remains challenging due to variations in\nimage quality, class imbalance, and pixel-level similarities that hinder model\ntraining. To address these issues, we propose a robust preprocessing pipeline\nincorporating image cropping, Contrast-Limited Adaptive Histogram Equalization\n(CLAHE), and targeted data augmentation to improve model generalization and\nresilience. Our approach leverages the Swin Transformer, which utilizes\nhierarchical token processing and shifted window attention to efficiently\ncapture fine-grained features while maintaining linear computational\ncomplexity. We validate our method on the Aptos and IDRiD datasets for\nmulti-class DR classification, achieving accuracy rates of 89.65% and 97.40%,\nrespectively. These results demonstrate the effectiveness of our model,\nparticularly in detecting early-stage DR, highlighting its potential for\nimproving automated retinal screening in clinical settings.",
      "tldr_zh": "本研究针对糖尿病视网膜病变 (DR) 分类面临的图像质量变化、类别不平衡和像素级相似性等挑战，提出了一种增强方法，包括图像裁剪、Contrast-Limited Adaptive Histogram Equalization (CLAHE) 和针对性数据增强的预处理管道。核心模型采用 Swin Transformer 和 Shifted Window Attention，利用分层 token 处理来高效捕获细粒度特征，同时保持线性计算复杂度。在 Aptos 和 IDRiD 数据集上的多类 DR 分类实验中，模型分别实现了 89.65% 和 97.40% 的准确率，特别是在早期 DR 检测方面表现出色。该方法展示了其在临床自动化视网膜筛查中的潜在应用价值。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15317v1",
      "published_date": "2025-04-20 13:23:20 UTC",
      "updated_date": "2025-04-20 13:23:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:58:09.458135"
    },
    {
      "arxiv_id": "2504.14603v2",
      "title": "UFO2: The Desktop AgentOS",
      "title_zh": "UFO2：桌面代理操作系统",
      "authors": [
        "Chaoyun Zhang",
        "He Huang",
        "Chiming Ni",
        "Jian Mu",
        "Si Qin",
        "Shilin He",
        "Lu Wang",
        "Fangkai Yang",
        "Pu Zhao",
        "Chao Du",
        "Liqun Li",
        "Yu Kang",
        "Zhao Jiang",
        "Suzhen Zheng",
        "Rujia Wang",
        "Jiaxu Qian",
        "Minghua Ma",
        "Jian-Guang Lou",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "abstract": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
      "tldr_zh": "该研究提出 UFO2，一种多智能体 AgentOS，用于提升基于多模态大型语言模型(LLMs)的 Computer-Using Agents (CUAs)在 Windows 桌面上的自动化能力，解决现有 CUAs 的浅层 OS 集成、脆弱截屏交互和破坏性执行问题。UFO2 采用中央 HostAgent 进行任务分解和协调，并结合 AppAgent 的原生 API、领域知识和统一 GUI-API 行动层，以及混合控制检测管道（融合 UIA 和视觉解析）和推测性多行动规划，以实现高效、鲁棒的任务执行。同时，通过 Picture-in-Picture (PiP) 接口在隔离虚拟桌面运行，允许代理与用户并发操作。实验结果显示，UFO2 在超过 20 个真实 Windows 应用上显著提高了执行准确性和鲁棒性，证明了深层 OS 集成为可靠的用户对齐桌面自动化提供了可扩展路径。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.OS"
      ],
      "primary_category": "cs.AI",
      "comment": "The source code of UFO2 is publicly available at\n  https://github.com/microsoft/UFO/, with comprehensive documentation provided\n  at https://microsoft.github.io/UFO/",
      "pdf_url": "http://arxiv.org/pdf/2504.14603v2",
      "published_date": "2025-04-20 13:04:43 UTC",
      "updated_date": "2025-04-25 05:14:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:58:21.845347"
    },
    {
      "arxiv_id": "2504.14602v1",
      "title": "K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics",
      "title_zh": "翻译失败",
      "authors": [
        "Jiwei Li",
        "Bi Zhang",
        "Xiaowei Tan",
        "Wanxin Chen",
        "Zhaoyuan Liu",
        "Juanjuan Zhang",
        "Weiguang Huo",
        "Jian Huang",
        "Lianqing Liu",
        "Xingang Zhao"
      ],
      "abstract": "The natural interaction and control performance of lower limb rehabilitation\nrobots are closely linked to biomechanical information from various human\nlocomotion activities. Multidimensional human motion data significantly deepen\nthe understanding of the complex mechanisms governing neuromuscular\nalterations, thereby facilitating the development and application of\nrehabilitation robots in multifaceted real-world environments. However,\ncurrently available lower limb datasets are inadequate for supplying the\nessential multimodal data and large-scale gait samples necessary for effective\ndata-driven approaches, and they neglect the significant effects of acquisition\ninterference in real applications.To fill this gap, we present the K2MUSE\ndataset, which includes a comprehensive collection of multimodal data,\ncomprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface\nelectromyography (sEMG) measurements. The proposed dataset includes lower limb\nmultimodal data from 30 able-bodied participants walking under different\ninclines (0$^\\circ$, $\\pm$5$^\\circ$, and $\\pm$10$^\\circ$), various speeds (0.5\nm/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions\n(muscle fatigue, electrode shifts, and inter-day differences). The kinematic\nand ground reaction force data were collected via a Vicon motion capture system\nand an instrumented treadmill with embedded force plates, whereas the sEMG and\nAUS data were synchronously recorded for thirteen muscles on the bilateral\nlower limbs. This dataset offers a new resource for designing control\nframeworks for rehabilitation robots and conducting biomechanical analyses of\nlower limb locomotion. The dataset is available at https://k2muse.github.io/.",
      "tldr_zh": "该研究引入了K2MUSE数据集，这是一个针对人类下肢的多模态数据集，旨在通过提供全面的生物力学数据来促进康复机器人设计和应用。数据集从30名健康参与者收集了kinematic、kinetic、AUS和sEMG数据，涵盖不同坡度（0°、±5°、±10°）、速度（0.5 m/s、1.0 m/s、1.5 m/s）以及非理想采集条件（如肌肉疲劳、电极位移和日间差异），使用Vicon motion capture系统和带力板的跑步机进行同步记录。K2MUSE填补了现有下肢数据集的不足，有助于加深对神经肌肉变化机制的理解，并支持数据驱动的康复机器人控制框架开发，可访问于https://k2muse.github.io/。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "23 pages, 13 figures,4 tables",
      "pdf_url": "http://arxiv.org/pdf/2504.14602v1",
      "published_date": "2025-04-20 13:03:56 UTC",
      "updated_date": "2025-04-20 13:03:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:58:33.826964"
    },
    {
      "arxiv_id": "2504.14596v1",
      "title": "Toward the Axiomatization of Intelligence: Structure, Time, and Existence",
      "title_zh": "翻译失败",
      "authors": [
        "Kei Itoh"
      ],
      "abstract": "This study aims to construct an axiomatic definition of intelligence within a\nmeta-framework that defines the method of definition, addressing intelligence\nas an inherently naive and polysemous concept. Initially, we formalize a\nset-theoretic representation of the universe as the domain wherein intelligence\nexists and characterize intelligence as a structure that involves temporal\nevolution and interaction with other sets. Starting from a naive definition of\nintelligence as \"an entity possessing structures for externally inputting,\ninternally processing, and externally outputting information or matter,\" we\naxiomatically reformulate it within this set-theoretical depiction of the\nuniverse. Applying this axiomatic definition, we compare and interpret three\nexamples -- Hebbian non-optimized neural networks (NNs),\nbackpropagation-optimized NNs, and biological reflexive systems -- in terms of\ntheir intelligence, structural properties, and biological plausibility.\nFurthermore, by extending our definition into a categorical framework, we\nintroduce two categories, \"Time Category\" and \"Intelligence Category,\" along\nwith the functorial relationships between them, demonstrating the potential to\nrepresent changes and mimicry relationships among intelligent systems\nabstractly. Additionally, since intelligence, as defined herein, functions\neffectively only when accompanied by temporal interactions, we introduce the\nconcept of \"activity\" and explore how activity-based conditions influence\nclassifications and interpretations of intelligence. Finally, we suggest that\nour definitional methodology is not limited to intelligence alone, but can be\nsimilarly applied to other concepts, such as consciousness and emotion,\nadvocating for their formal reinterpretation through the same procedural steps:\ndefining a universal representation, selecting naive definitions, and axiomatic\nformalization.",
      "tldr_zh": "这篇论文旨在通过一个元框架构建智能的公理定义，将智能形式化为一个涉及时间演化和与其他集合交互的结构，并使用集合论表示宇宙作为其存在领域。作者从“一个实体拥有结构来外部输入、内部处理和外部输出信息或物质”的天真定义出发，进行公理化重新表述。论文应用这一定义比较了Hebbian non-optimized neural networks、backpropagation-optimized NNs和生物反射系统，在智能、结构属性和生物合理性方面进行解读。进一步扩展到范畴论框架，引入Time Category和Intelligence Category，以及它们之间的functorial relationships，以抽象表示智能系统的变化和模仿关系。最终，论文提出“activity”概念强调时间交互的重要性，并建议此方法可应用于其他概念如意识和情感，通过定义通用表示、选择天真定义和公理形式化。",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "37 pages, 4 tables, in English, in Japanese",
      "pdf_url": "http://arxiv.org/pdf/2504.14596v1",
      "published_date": "2025-04-20 12:55:37 UTC",
      "updated_date": "2025-04-20 12:55:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:58:45.860389"
    },
    {
      "arxiv_id": "2504.14594v1",
      "title": "HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models",
      "title_zh": "HealthGenie：通过知识图谱和大型语言模型赋能用户健康饮食指导",
      "authors": [
        "Fan Gao",
        "Xinjie Zhao",
        "Ding Xia",
        "Zhongyi Zhou",
        "Rui Yang",
        "Jinghui Lu",
        "Hang Jiang",
        "Chanjun Park",
        "Irene Li"
      ],
      "abstract": "Seeking dietary guidance often requires navigating complex professional\nknowledge while accommodating individual health conditions. Knowledge Graphs\n(KGs) offer structured and interpretable nutritional information, whereas Large\nLanguage Models (LLMs) naturally facilitate conversational recommendation\ndelivery. In this paper, we present HealthGenie, an interactive system that\ncombines the strengths of LLMs and KGs to provide personalized dietary\nrecommendations along with hierarchical information visualization for a quick\nand intuitive overview. Upon receiving a user query, HealthGenie performs query\nrefinement and retrieves relevant information from a pre-built KG. The system\nthen visualizes and highlights pertinent information, organized by defined\ncategories, while offering detailed, explainable recommendation rationales.\nUsers can further tailor these recommendations by adjusting preferences\ninteractively. Our evaluation, comprising a within-subject comparative\nexperiment and an open-ended discussion, demonstrates that HealthGenie\neffectively supports users in obtaining personalized dietary guidance based on\ntheir health conditions while reducing interaction effort and cognitive load.\nThese findings highlight the potential of LLM-KG integration in supporting\ndecision-making through explainable and visualized information. We examine the\nsystem's usefulness and effectiveness with an N=12 within-subject study and\nprovide design considerations for future systems that integrate conversational\nLLM and KG.",
      "tldr_zh": "本文介绍了 HealthGenie 系统，该系统整合 Knowledge Graphs (KGs) 和 Large Language Models (LLMs)，为用户提供个性化的饮食指导，结合结构化营养信息和对话式推荐。系统通过查询优化、KG 检索、层次化信息可视化以及可解释的推荐理由，帮助用户快速理解并调整健康建议。实验评估（包括 N=12 的内部主题研究）表明，HealthGenie 显著降低了用户的交互努力和认知负荷，并突出了 LLM-KG 集成在支持决策方面的潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14594v1",
      "published_date": "2025-04-20 12:51:16 UTC",
      "updated_date": "2025-04-20 12:51:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:58:58.021638"
    },
    {
      "arxiv_id": "2504.14588v1",
      "title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction",
      "title_zh": "Phoenix：一种基于运动的自我反射框架，用于细粒度机器人动作修正",
      "authors": [
        "Wenke Xia",
        "Ruoxuan Feng",
        "Dong Wang",
        "Di Hu"
      ],
      "abstract": "Building a generalizable self-correction system is crucial for robots to\nrecover from failures. Despite advancements in Multimodal Large Language Models\n(MLLMs) that empower robots with semantic reflection ability for failure,\ntranslating semantic reflection into how to correct fine-grained robotic\nactions remains a significant challenge. To address this gap, we build the\nPhoenix framework, which leverages motion instruction as a bridge to connect\nhigh-level semantic reflection with low-level robotic action correction. In\nthis motion-based self-reflection framework, we start with a dual-process\nmotion adjustment mechanism with MLLMs to translate the semantic reflection\ninto coarse-grained motion instruction adjustment. To leverage this motion\ninstruction for guiding how to correct fine-grained robotic actions, a\nmulti-task motion-conditioned diffusion policy is proposed to integrate visual\nobservations for high-frequency robotic action correction. By combining these\ntwo models, we could shift the demand for generalization capability from the\nlow-level manipulation policy to the MLLMs-driven motion adjustment model and\nfacilitate precise, fine-grained robotic action correction. Utilizing this\nframework, we further develop a lifelong learning method to automatically\nimprove the model's capability from interactions with dynamic environments. The\nexperiments conducted in both the RoboMimic simulation and real-world scenarios\nprove the superior generalization and robustness of our framework across a\nvariety of manipulation tasks. Our code is released at\n\\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.",
      "tldr_zh": "该论文提出Phoenix框架，这是一种基于运动的自我反射框架，用于实现细粒度的机器人动作修正，旨在帮助机器人从失败中恢复。框架通过双过程运动调整机制利用MLLMs（Multimodal Large Language Models）将高层语义反射转化为粗粒度运动指令，并结合多任务运动条件扩散策略整合视觉观察进行高频动作修正，从而将泛化需求转移到MLLMs驱动的模型上。实验在RoboMimic模拟和真实场景中证明了Phoenix的优越泛化和鲁棒性，并通过终身学习方法自动提升模型在动态环境中的性能。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2504.14588v1",
      "published_date": "2025-04-20 12:30:43 UTC",
      "updated_date": "2025-04-20 12:30:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:59:10.366157"
    },
    {
      "arxiv_id": "2504.14573v1",
      "title": "Modality Selection and Skill Segmentation via Cross-Modality Attention",
      "title_zh": "跨",
      "authors": [
        "Jiawei Jiang",
        "Kei Ota",
        "Devesh K. Jha",
        "Asako Kanezaki"
      ],
      "abstract": "Incorporating additional sensory modalities such as tactile and audio into\nfoundational robotic models poses significant challenges due to the curse of\ndimensionality. This work addresses this issue through modality selection. We\npropose a cross-modality attention (CMA) mechanism to identify and selectively\nutilize the modalities that are most informative for action generation at each\ntimestep. Furthermore, we extend the application of CMA to segment primitive\nskills from expert demonstrations and leverage this segmentation to train a\nhierarchical policy capable of solving long-horizon, contact-rich manipulation\ntasks.",
      "tldr_zh": "这篇论文解决了在机器人模型中加入额外感官模式（如触觉和音频）所带来的维度诅咒(curse of dimensionality)挑战，通过提出cross-modality attention (CMA)机制来识别并选择每个时间步最有信息量的模式，从而优化动作生成。此外，作者扩展了CMA的应用，用于从专家演示中分割基本技能(primitive skills)，并基于此训练一个分层策略(hierarchical policy)，以有效处理长horizon和接触丰富的操作任务。实验结果表明，该方法提高了机器人模型的效率和性能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14573v1",
      "published_date": "2025-04-20 11:32:43 UTC",
      "updated_date": "2025-04-20 11:32:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:59:22.105481"
    },
    {
      "arxiv_id": "2504.14569v1",
      "title": "NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models",
      "title_zh": "NoWag：一种",
      "authors": [
        "Lawrence Liu",
        "Inesh Chakrabarti",
        "Yixiao Li",
        "Mengdi Wang",
        "Tuo Zhao",
        "Lin F. Yang"
      ],
      "abstract": "Large language models (LLMs) exhibit remarkable performance across various\nnatural language processing tasks but suffer from immense computational and\nmemory demands, limiting their deployment in resource-constrained environments.\nTo address this challenge, we propose NoWag: (Normalized Weight and Activation\nGuided Compression), a unified framework for zero-shot shape preserving\ncompression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB\nmodels, using two popular forms of shape-preserving compression, vector\nquantization NoWag-VQ (NoWag for Vector Quantization), and\nunstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that\nNoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that\nNoWag-P performs competitively against state-of-the-art methods. These results\nsuggest commonalities between these compression paradigms that could inspire\nfuture work. Our code is available at https://github.com/LawrenceRLiu/NoWag",
      "tldr_zh": "这篇论文提出了NoWag框架（Normalized Weight and Activation Guided Compression），一个统一的零样本形状保持压缩算法，用于解决大型语言模型(LLMs)在资源受限环境中的计算和内存需求问题。NoWag包括NoWag-VQ（基于向量量化）和NoWag-P（基于非结构化/半结构化剪枝）两种方法，并应用于压缩Llama-2 7B/13B/70B和Llama-3 8B/70B模型。实验结果显示，NoWag-VQ显著优于现有零样本向量量化方法，而NoWag-P与最先进技术竞争性能，揭示了这些压缩范式之间的共同点，可能激发未来研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14569v1",
      "published_date": "2025-04-20 11:00:29 UTC",
      "updated_date": "2025-04-20 11:00:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:59:34.193964"
    },
    {
      "arxiv_id": "2505.03760v1",
      "title": "Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach",
      "title_zh": "深度强化学习用于投资者特定投资组合优化：一种波动率引导的资产选择方法",
      "authors": [
        "Arishi Orra",
        "Aryan Bhambu",
        "Himanshu Choudhary",
        "Manoj Thakur",
        "Selvaraju Natarajan"
      ],
      "abstract": "Portfolio optimization requires dynamic allocation of funds by balancing the\nrisk and return tradeoff under dynamic market conditions. With the recent\nadvancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in\nproviding adaptive and scalable strategies for portfolio optimization. However,\nthe success of these strategies depends not only on their ability to adapt to\nmarket dynamics but also on the careful pre-selection of assets that influence\noverall portfolio performance. Incorporating the investor's preference in\npre-selecting assets for a portfolio is essential in refining their investment\nstrategies. This study proposes a volatility-guided DRL-based portfolio\noptimization framework that dynamically constructs portfolios based on\ninvestors' risk profiles. The Generalized Autoregressive Conditional\nHeteroscedasticity (GARCH) model is utilized for volatility forecasting of\nstocks and categorizes them based on their volatility as aggressive, moderate,\nand conservative. The DRL agent is then employed to learn an optimal investment\npolicy by interacting with the historical market data. The efficacy of the\nproposed methodology is established using stocks from the Dow $30$ index. The\nproposed investor-specific DRL-based portfolios outperformed the baseline\nstrategies by generating consistent risk-adjusted returns.",
      "tldr_zh": "该研究提出了一种基于波动率引导的深度强化学习（Deep Reinforcement Learning, DRL）框架，用于针对投资者的风险偏好进行投资组合优化。该框架首先利用 Generalized Autoregressive Conditional Heteroscedasticity (GARCH) 模型预测股票波动率，并将股票分类为 aggressive、moderate 或 conservative 类型，以动态选择资产。随后，DRL 代理通过与历史市场数据互动学习最优投资策略，适应市场动态并平衡风险与回报。在使用 Dow 30 指数股票的实验中，该方法比基线策略产生了更一致的风险调整回报，证明了其在投资者特定投资组合优化中的有效性。",
      "categories": [
        "q-fin.PM",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "q-fin.PM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2505.03760v1",
      "published_date": "2025-04-20 10:17:37 UTC",
      "updated_date": "2025-04-20 10:17:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:59:45.705215"
    },
    {
      "arxiv_id": "2504.14560v3",
      "title": "ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model",
      "title_zh": "翻译失败",
      "authors": [
        "Haiyan Qin",
        "Zhiwei Xie",
        "Jingjing Li",
        "Liangchen Li",
        "Xiaotong Feng",
        "Junzhan Liu",
        "Wang Kang"
      ],
      "abstract": "Large Language Models (LLMs) have advanced Verilog code generation\nsignificantly, yet face challenges in data quality, reasoning capabilities, and\ncomputational efficiency. This paper presents ReasoningV, a novel model\nemploying a hybrid reasoning strategy that integrates trained intrinsic\ncapabilities with dynamic inference adaptation for Verilog code generation. Our\nframework introduces three complementary innovations: (1) ReasoningV-5K, a\nhigh-quality dataset of 5,000 functionally verified instances with reasoning\npaths created through multi-dimensional filtering of PyraNet samples; (2) a\ntwo-stage training approach combining parameter-efficient fine-tuning for\nfoundational knowledge with full-parameter optimization for enhanced reasoning;\nand (3) an adaptive reasoning mechanism that dynamically adjusts reasoning\ndepth based on problem complexity, reducing token consumption by up to 75\\%\nwhile preserving performance. Experimental results demonstrate ReasoningV's\neffectiveness with a pass@1 accuracy of 57.8\\% on VerilogEval-human, achieving\nperformance competitive with leading commercial models like Gemini-2.0-flash\n(59.5\\%) and exceeding the previous best open-source model by 10.4 percentage\npoints. ReasoningV offers a more reliable and accessible pathway for advancing\nAI-driven hardware design automation, with our model, data, and code available\nat https://github.com/BUAA-CLab/ReasoningV.",
      "tldr_zh": "该论文针对大型语言模型(LLMs)在Verilog代码生成中的数据质量、推理能力和计算效率挑战，提出了一种高效的ReasoningV模型，该模型采用混合推理策略，结合内在训练能力和动态推理适应机制。创新包括：(1)构建了高质量数据集ReasoningV-5K，包含5000个功能验证实例及推理路径；(2)采用两阶段训练方法，先进行参数高效微调再全参数优化以提升推理能力；(3)引入自适应推理机制，根据问题复杂度动态调整深度，减少token消耗高达75%。实验结果显示，ReasoningV在VerilogEval-human基准上的pass@1准确率达57.8%，与Gemini-2.0-flash相当，并超过最佳开源模型10.4个百分点，提供开源资源以推进AI驱动的硬件设计自动化。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.14560v3",
      "published_date": "2025-04-20 10:16:59 UTC",
      "updated_date": "2025-05-01 03:06:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T14:59:58.486012"
    },
    {
      "arxiv_id": "2504.15315v1",
      "title": "Diffusion-Driven Inertial Generated Data for Smartphone Location Classification",
      "title_zh": "扩散驱动的惯性生成数据用于智能手机位置分类",
      "authors": [
        "Noa Cohen",
        "Rotem Dror",
        "Itzik Klein"
      ],
      "abstract": "Despite the crucial role of inertial measurements in motion tracking and\nnavigation systems, the time-consuming and resource-intensive nature of\ncollecting extensive inertial data has hindered the development of robust\nmachine learning models in this field. In recent years, diffusion models have\nemerged as a revolutionary class of generative models, reshaping the landscape\nof artificial data generation. These models surpass generative adversarial\nnetworks and other state-of-the-art approaches to complex tasks. In this work,\nwe propose diffusion-driven specific force-generated data for smartphone\nlocation recognition. We provide a comprehensive evaluation methodology by\ncomparing synthetic and real recorded specific force data across multiple\nmetrics. Our results demonstrate that our diffusion-based generative model\nsuccessfully captures the distinctive characteristics of specific force signals\nacross different smartphone placement conditions. Thus, by creating diverse,\nrealistic synthetic data, we can reduce the burden of extensive data collection\nwhile providing high-quality training data for machine learning models.",
      "tldr_zh": "本研究针对惯性数据收集耗时费力的难题，提出了一种基于 diffusion models 的生成方法，用于生成 specific force 数据，以提升智能手机位置分类任务的机器学习模型性能。该方法通过合成 realistic 数据，成功捕捉了不同智能手机放置条件下 specific force 信号的独特特征，并通过多指标评估（如与真实数据的比较）验证了其有效性。结果显示，该生成模型能提供多样、高质量的训练数据，从而显著减少了实际数据收集的负担。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15315v1",
      "published_date": "2025-04-20 10:14:36 UTC",
      "updated_date": "2025-04-20 10:14:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:00:09.624160"
    },
    {
      "arxiv_id": "2504.14556v1",
      "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Yousef Emami",
        "Hao Zhou",
        "SeyedSina Nabavirazani",
        "Luis Almeida"
      ],
      "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly being used in various\nprivate and commercial applications, e.g. traffic control, package delivery,\nand Search and Rescue (SAR) operations. Machine Learning (ML) methods used in\nUAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement\nLearning (DRL) face challenges such as complex and lengthy model training, gaps\nbetween simulation and reality, and low sample efficiency, which conflict with\nthe urgency of emergencies such as SAR operations. This paper proposes\nIn-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as\nan alternative to DRL in emergencies. The UAV collects and transmits logged\nsensory data, to an LLM, to generate a task description in natural language,\nfrom which it obtains a data collection schedule to be executed by the UAV. The\nsystem continuously adapts by adding feedback to task descriptions and\nutilizing feedback for future decisions. This method is tested against\njailbreaking attacks, where task description is manipulated to undermine\nnetwork performance, highlighting the vulnerability of LLMs to such attacks.\nThe proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative\npacket loss by approximately 56\\%. ICLDC presents a promising direction for\nintelligent scheduling and control in UAV-assisted data collection.",
      "tldr_zh": "该论文针对UAV-assisted Sensor Networks（UASNETs）中的数据收集调度问题，提出了一种基于Large Language Models (LLMs)的In-Context Learning (ICL)方案，名为ICLDC，作为Deep Reinforcement Learning (DRL)的替代，以应对紧急场景下的模型训练复杂性和样本效率低等问题。ICLDC通过UAV收集传感器数据并传输给LLM生成自然语言任务描述，从而制定数据收集计划，并通过反馈机制持续优化决策。实验结果显示，ICLDC相较于Maximum Channel Gain减少了约56%的累计数据包丢失率，同时揭示了LLM对jailbreaking attacks的脆弱性，为UAV辅助数据收集的智能调度提供了新方向。",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.RO",
        "53-01",
        "C.2"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 7 figures,",
      "pdf_url": "http://arxiv.org/pdf/2504.14556v1",
      "published_date": "2025-04-20 10:05:07 UTC",
      "updated_date": "2025-04-20 10:05:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:00:23.023873"
    },
    {
      "arxiv_id": "2504.14548v1",
      "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control",
      "title_zh": "翻译失败",
      "authors": [
        "Lifeng Lin",
        "Rongfeng Lu",
        "Quan Chen",
        "Haofan Ren",
        "Ming Lu",
        "Yaoqi Sun",
        "Chenggang Yan",
        "Anke Xue"
      ],
      "abstract": "Sparse-view 3D reconstruction is a fundamental yet challenging task in\npractical 3D reconstruction applications. Recently, many methods based on the\n3D Gaussian Splatting (3DGS) framework have been proposed to address\nsparse-view 3D reconstruction. Although these methods have made considerable\nadvancements, they still show significant issues with overfitting. To reduce\nthe overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number\nControl (VGNC) approach based on generative novel view synthesis (NVS) models.\nTo the best of our knowledge, this is the first attempt to alleviate the\noverfitting issue of sparse-view 3DGS with generative validation images.\nSpecifically, we first introduce a validation image generation method based on\na generative NVS model. We then propose a Gaussian number control strategy that\nutilizes generated validation images to determine the optimal Gaussian numbers,\nthereby reducing the issue of overfitting. We conducted detailed experiments on\nvarious sparse-view 3DGS baselines and datasets to evaluate the effectiveness\nof VGNC. Extensive experiments show that our approach not only reduces\noverfitting but also improves rendering quality on the test set while\ndecreasing the number of Gaussian points. This reduction lowers storage demands\nand accelerates both training and rendering. The code will be released.",
      "tldr_zh": "该论文针对稀疏视图 3D 重建中的过拟合问题，提出了一种新方法 VGNC（Validation-guided Gaussian Number Control），它基于生成式新型视图合成 (NVS) 模型生成验证图像来优化 3D Gaussian Splatting (3DGS) 中的 Gaussian 数量，从而减少过拟合。VGNC 首先通过 NVS 模型生成验证图像，然后利用这些图像动态控制 Gaussian 点数，确保模型在测试集上渲染质量提升。实验结果显示，该方法不仅提高了渲染性能，还降低了存储需求并加速了训练和渲染过程，为稀疏视图 3DGS 应用提供了有效改进。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages,8 figures",
      "pdf_url": "http://arxiv.org/pdf/2504.14548v1",
      "published_date": "2025-04-20 09:38:02 UTC",
      "updated_date": "2025-04-20 09:38:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:00:34.742432"
    },
    {
      "arxiv_id": "2504.14530v1",
      "title": "Causality for Natural Language Processing",
      "title_zh": "翻译失败",
      "authors": [
        "Zhijing Jin"
      ],
      "abstract": "Causal reasoning is a cornerstone of human intelligence and a critical\ncapability for artificial systems aiming to achieve advanced understanding and\ndecision-making. This thesis delves into various dimensions of causal reasoning\nand understanding in large language models (LLMs). It encompasses a series of\nstudies that explore the causal inference skills of LLMs, the mechanisms behind\ntheir performance, and the implications of causal and anticausal learning for\nnatural language processing (NLP) tasks. Additionally, it investigates the\napplication of causal reasoning in text-based computational social science,\nspecifically focusing on political decision-making and the evaluation of\nscientific impact through citations. Through novel datasets, benchmark tasks,\nand methodological frameworks, this work identifies key challenges and\nopportunities to improve the causal capabilities of LLMs, providing a\ncomprehensive foundation for future research in this evolving field.",
      "tldr_zh": "本论文探讨了因果推理(Causality)在自然语言处理(NLP)中的核心作用，重点评估大型语言模型(LLMs)的因果推理技能、性能机制以及因果和反因果学习在NLP任务中的影响。研究还扩展到文本-based计算社会科学的应用，包括政治决策和科学影响评估，通过引用分析等方法。论文引入了新数据集、基准任务和方法框架，识别了关键挑战和机会，以提升LLMs的因果能力，并为未来研究奠定坚实基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "PhD Thesis 2024",
      "pdf_url": "http://arxiv.org/pdf/2504.14530v1",
      "published_date": "2025-04-20 08:11:11 UTC",
      "updated_date": "2025-04-20 08:11:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:00:47.161421"
    },
    {
      "arxiv_id": "2504.14523v1",
      "title": "Learning from Reasoning Failures via Synthetic Data Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriela Ben Melech Stan",
        "Estelle Aflalo",
        "Avinash Madasu",
        "Vasudev Lal",
        "Phillip Howard"
      ],
      "abstract": "Training models on synthetic data has emerged as an increasingly important\nstrategy for improving the performance of generative AI. This approach is\nparticularly helpful for large multimodal models (LMMs) due to the relative\nscarcity of high-quality paired image-text data compared to language-only data.\nWhile a variety of methods have been proposed for generating large multimodal\ndatasets, they do not tailor the synthetic data to address specific\ndeficiencies in the reasoning abilities of LMMs which will be trained with the\ngenerated dataset. In contrast, humans often learn in a more efficient manner\nby seeking out examples related to the types of reasoning where they have\nfailed previously. Inspired by this observation, we propose a new approach for\nsynthetic data generation which is grounded in the analysis of an existing\nLMM's reasoning failures. Our methodology leverages frontier models to\nautomatically analyze errors produced by a weaker LMM and propose new examples\nwhich can be used to correct the reasoning failure via additional training,\nwhich are then further filtered to ensure high quality. We generate a large\nmultimodal instruction tuning dataset containing over 553k examples using our\napproach and conduct extensive experiments demonstrating its utility for\nimproving the performance of LMMs on multiple downstream tasks. Our results\nshow that models trained on our synthetic data can even exceed the performance\nof LMMs trained on an equivalent amount of additional real data, demonstrating\nthe high value of generating synthetic data targeted to specific reasoning\nfailure modes in LMMs. We will make our dataset and code publicly available.",
      "tldr_zh": "该研究提出了一种新方法，通过分析大型多模态模型 (LMMs) 的推理失败来生成合成数据，从而提升模型性能。该方法利用前沿模型自动识别弱 LMMs 的错误，创建针对性示例并进行质量过滤，生成了一个包含超过 55.3 万示例的多模态指令调整数据集。与传统方法不同，这种针对推理缺陷的合成数据在下游任务上使 LMMs 的表现超过了使用等量真实数据的训练效果，最终数据集和代码将公开可用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14523v1",
      "published_date": "2025-04-20 07:45:53 UTC",
      "updated_date": "2025-04-20 07:45:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:00:59.628086"
    },
    {
      "arxiv_id": "2504.14522v1",
      "title": "Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers",
      "title_zh": "设计偏见：利用 AI 偏见提升新闻读者的批判性思维",
      "authors": [
        "Liudmila Zavolokina",
        "Kilian Sprenkamp",
        "Zoya Katashinskaya",
        "Daniel Gordon Jones"
      ],
      "abstract": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection.",
      "tldr_zh": "这篇论文探讨如何利用大型语言模型 (LLMs) 的固有偏见来设计宣传检测工具，从而提升新闻读者的批判性思维。研究逆转了传统观点，将 AI 偏见视为机会，通过用户选择和个性化策略（如基于确认偏见和认知 dissonance），来适应用户的政治立场。论文基于定性用户研究，提供设计推荐，包括增强偏见意识、提供个人化选项，以及逐步引入多样视角，以促进更有效的宣传检测和批判性思考。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "European Conference on Information Systems (ECIS)",
      "pdf_url": "http://arxiv.org/pdf/2504.14522v1",
      "published_date": "2025-04-20 07:39:00 UTC",
      "updated_date": "2025-04-20 07:39:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:01:11.148871"
    },
    {
      "arxiv_id": "2504.14520v1",
      "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Ahsan Bilal",
        "Muhammad Ahmed Mohsin",
        "Muhammad Umer",
        "Muhammad Awais Khan Bangash",
        "Muhammad Ali Jamshed"
      ],
      "abstract": "This survey explores the development of meta-thinking capabilities in Large\nLanguage Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)\nperspective. Meta-thinking self-reflection, assessment, and control of thinking\nprocesses is an important next step in enhancing LLM reliability, flexibility,\nand performance, particularly for complex or high-stakes tasks. The survey\nbegins by analyzing current LLM limitations, such as hallucinations and the\nlack of internal self-assessment mechanisms. It then talks about newer methods,\nincluding RL from human feedback (RLHF), self-distillation, and\nchain-of-thought prompting, and each of their limitations. The crux of the\nsurvey is to talk about how multi-agent architectures, namely supervisor-agent\nhierarchies, agent debates, and theory of mind frameworks, can emulate\nhuman-like introspective behavior and enhance LLM robustness. By exploring\nreward mechanisms, self-play, and continuous learning methods in MARL, this\nsurvey gives a comprehensive roadmap to building introspective, adaptive, and\ntrustworthy LLMs. Evaluation metrics, datasets, and future research avenues,\nincluding neuroscience-inspired architectures and hybrid symbolic reasoning,\nare also discussed.",
      "tldr_zh": "这篇调查从多智能体强化学习(MARL)的视角探讨了提升大型语言模型(LLMs)的元思考能力，包括自我反思、评估和控制思考过程，以提高其可靠性和性能。\n它首先分析了LLMs的当前局限性，如幻觉和缺乏内部自我评估机制，并讨论了新方法如RL from human feedback (RLHF)、自蒸馏和chain-of-thought提示的不足。\n调查重点强调多智能体架构（如监督者-智能体层次、代理辩论和theory of mind框架）如何模仿人类内省行为，并通过MARL中的奖励机制、自我游戏和持续学习方法，提供构建内省、适应性和可信LLMs的全面路线图。\n最后，它涵盖了评估指标、数据集以及未来研究方向，包括神经科学启发架构和混合符号推理。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to IEEE Transactions on Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2504.14520v1",
      "published_date": "2025-04-20 07:34:26 UTC",
      "updated_date": "2025-04-20 07:34:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:01:26.322743"
    },
    {
      "arxiv_id": "2504.14519v1",
      "title": "SlimPipe: Memory-Thrifty and Efficient Pipeline Parallelism for Long-Context LLM Training",
      "title_zh": "翻译失败",
      "authors": [
        "Zhouyang Li",
        "Yuliang Liu",
        "Wei Zhang",
        "Tailing Yuan",
        "Bin Chen",
        "Chengru Song",
        "Di Zhang"
      ],
      "abstract": "Pipeline Parallelism (PP) serves as a crucial technique for training Large\nLanguage Models (LLMs), owing to its capability to alleviate memory pressure\nfrom model states with relatively low communication overhead. However, in\nlong-context scenarios, existing pipeline parallelism methods fail to address\nthe substantial activation memory pressure, primarily due to the peak memory\nconsumption resulting from the accumulation of activations across multiple\nmicrobatches. Moreover, these approaches inevitably introduce considerable\npipeline bubbles, further hindering efficiency.\n  To tackle these challenges, we propose SlimPipe, a novel approach to\nfine-grained pipeline parallelism that employs uniform sequence slicing coupled\nwith one-forward-one-backward (1F1B) schedule. It reduces the accumulated\nactivations from several microbatches to just one, which is split into several\nslices. Although the slices are evenly partitioned, the computation cost is not\nequal across slices due to causal attention. We develop a sophisticated\nworkload redistribution technique to address this load imbalance. SlimPipe\nachieves (1) near-zero memory overhead and (2) minimal pipeline bubbles\nsimultaneously. The effectiveness of SlimPipe has been proven by thorough\ntesting with diverse model architectures, context window sizes, and\nSlimPipe-specific configurations. For example, on the Llama 70B model, compared\nto state-of-the-art methods, SlimPipe significantly boosts the Model FLOPs\nUtilization (MFU) to up to $1.57\\times$ for a context length of 512K. More\nnotably, for a context length of 2048K, it maintains over 45% utilization on\n256 NVIDIA Hopper 80GB GPUs, while other approaches either suffer significant\nperformance drops or fail entirely due to memory constraints.",
      "tldr_zh": "论文提出SlimPipe，一种内存节约且高效的Pipeline Parallelism方法，针对长上下文Large Language Models (LLMs)训练中存在的激活内存压力和pipeline bubbles问题。SlimPipe采用均匀序列切片结合one-forward-one-backward (1F1B)调度，将激活累积减少到一个切片，并通过工作负载重分布技术解决因果注意力导致的负载不平衡，从而实现近零内存开销和最小bubbles。实验结果显示，在Llama 70B模型上，SlimPipe将Model FLOPs Utilization (MFU)提升至1.57倍（上下文长度512K），并在2048K长度下保持超过45%的利用率，而其他方法则面临性能下降或内存失败。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14519v1",
      "published_date": "2025-04-20 07:33:33 UTC",
      "updated_date": "2025-04-20 07:33:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:01:37.455424"
    },
    {
      "arxiv_id": "2504.14514v1",
      "title": "On Dimension-Free Transformer: An Application of STP to AI",
      "title_zh": "翻译失败",
      "authors": [
        "Daizhan Cheng"
      ],
      "abstract": "The matrix expressions for every parts of a transformer are firstly\ndescribed. Based on semi-tensor product (STP) of matrices the hypervectors are\nreconsidered and the linear transformation over hypervectors is constructed by\nusing projection. Its properties and calculating formulas are obtained. Using\nprojection-based transformation of hypervector (PBTH), the framework of\ndimension-free transformer (DFT) is proposed by verifying each linear\ntransformation in a transformer and replacing it by a proper PBTH, which allows\nthe inputs and outputs being of arbitrary dimensions. Using balanced\ninformation about all entries, DFT must be more efficient in dealing with\nsignals.",
      "tldr_zh": "本论文首次描述了Transformer各部分的矩阵表达式，并基于半张量积(STP)重新考虑hypervectors，通过projection构建线性变换。作者提出了一种投影为基础的超向量变换(PBTH)，用于替换Transformer中的线性变换，从而构建维度无关的Transformer(DFT)，允许输入和输出具有任意维度。结果表明，DFT通过平衡所有条目信息，能够更高效地处理信号。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14514v1",
      "published_date": "2025-04-20 07:19:54 UTC",
      "updated_date": "2025-04-20 07:19:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:01:47.706414"
    },
    {
      "arxiv_id": "2504.14509v3",
      "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Fulong Ye",
        "Miao Hua",
        "Pengze Zhang",
        "Xinghui Li",
        "Qichao Sun",
        "Songtao Zhao",
        "Qian He",
        "Xinglong Wu"
      ],
      "abstract": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.",
      "tldr_zh": "本研究提出DreamID，一种基于扩散模型的换脸方法，通过构建Triplet ID Group数据实现显式监督，提高身份相似度、属性保留和图像保真度，同时利用SD Turbo减少推理步骤至单步，实现高效的像素级端到端训练。模型架构包括SwapNet、FaceNet和ID Adapter，充分利用Triplet ID Group的监督能力，并通过修改训练数据来微调特定属性，如眼镜和脸型。实验结果显示，DreamID在身份相似度、姿势表情保留和图像保真度上优于现有方法，可在0.6秒内生成512*512分辨率的换脸图像，尤其在复杂光照、大角度和遮挡场景中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project: https://superhero-7.github.io/DreamID/",
      "pdf_url": "http://arxiv.org/pdf/2504.14509v3",
      "published_date": "2025-04-20 06:53:00 UTC",
      "updated_date": "2025-04-25 03:48:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:02:00.610320"
    },
    {
      "arxiv_id": "2504.15313v1",
      "title": "PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind",
      "title_zh": "翻译失败",
      "authors": [
        "Yajie Yu",
        "Yue Feng"
      ],
      "abstract": "Multi-agents has exhibited significant intelligence in real-word simulations\nwith Large language models (LLMs) due to the capabilities of social cognition\nand knowledge retrieval. However, existing research on agents equipped with\neffective cognition chains including reasoning, planning, decision-making and\nreflecting remains limited, especially in the dynamically interactive\nscenarios. In addition, unlike human, prompt-based responses face challenges in\npsychological state perception and empirical calibration during uncertain\ngaming process, which can inevitably lead to cognition bias. In light of above,\nwe introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework\ncharacterized by systematically acquiring intentions of others and adaptively\noptimizing irrational strategies for continual enhancement. Specifically,\nPolicyEvol-Agent first obtains reflective expertise patterns and then\nintegrates a range of cognitive operations with Theory of Mind alongside\ninternal and external perspectives. Simulation results, outperforming RL-based\nmodels and agent-based methods, demonstrate the superiority of PolicyEvol-Agent\nfor final gaming victory. Moreover, the policy evolution mechanism reveals the\neffectiveness of dynamic guideline adjustments in both automatic and human\nevaluation.",
      "tldr_zh": "该研究提出PolicyEvol-Agent框架，利用Theory of Mind理论来提升多智能体系统在动态交互场景中的智能表现，通过环境感知和自我意识来获取他人意图并优化非理性策略。框架首先提取反思性专家模式，然后整合内部和外部认知操作，包括推理、规划、决策和反思，以适应不确定性游戏过程并减少认知偏差。实验结果显示，PolicyEvol-Agent在模拟环境中优于RL-based模型和agent-based方法，显著提高了游戏胜利率，并证明了动态指导调整的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.15313v1",
      "published_date": "2025-04-20 06:43:23 UTC",
      "updated_date": "2025-04-20 06:43:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:02:11.289387"
    },
    {
      "arxiv_id": "2504.14494v1",
      "title": "LBM-GNN: Graph Neural Network Enhanced Lattice Boltzmann Method",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Li"
      ],
      "abstract": "In this paper, we present LBM-GNN, a novel approach that enhances the\ntraditional Lattice Boltzmann Method (LBM) with Graph Neural Networks (GNNs).\nWe apply this method to fluid dynamics simulations, demonstrating improved\nstability and accuracy compared to standard LBM implementations. The method is\nvalidated using benchmark problems such as the Taylor-Green vortex, focusing on\naccuracy, conservation properties, and performance across different Reynolds\nnumbers and grid resolutions. Our results indicate that GNN-enhanced LBM can\nmaintain better conservation properties while improving numerical stability at\nhigher Reynolds numbers.",
      "tldr_zh": "本研究提出LBM-GNN，一种将Graph Neural Networks (GNNs)增强传统Lattice Boltzmann Method (LBM)的新方法，用于流体动力学模拟。相比标准LBM，该方法显著提高了模拟的稳定性和准确性，通过Taylor-Green vortex等基准问题进行验证，评估了准确性、守恒属性以及在不同Reynolds numbers和网格分辨率下的性能。结果显示，LBM-GNN在更高Reynolds numbers下保持了更好的守恒属性，并提升了数值稳定性。",
      "categories": [
        "physics.flu-dyn",
        "cs.AI"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14494v1",
      "published_date": "2025-04-20 05:09:10 UTC",
      "updated_date": "2025-04-20 05:09:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:02:22.129416"
    },
    {
      "arxiv_id": "2504.14493v2",
      "title": "FinSage: A Multi-aspect RAG System for Financial Filings Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu Wang",
        "Jijun Chi",
        "Zhenghan Tai",
        "Tung Sum Thomas Kwok",
        "Muzhi Li",
        "Zhuhong Li",
        "Hailin He",
        "Yuchen Hua",
        "Peng Lu",
        "Suyuchen Wang",
        "Yihong Wu",
        "Jerry Huang",
        "Jingrui Tian",
        "Ling Zhou"
      ],
      "abstract": "Leveraging large language models in real-world settings often entails a need\nto utilize domain-specific data and tools in order to follow the complex\nregulations that need to be followed for acceptable use. Within financial\nsectors, modern enterprises increasingly rely on Retrieval-Augmented Generation\n(RAG) systems to address complex compliance requirements in financial document\nworkflows. However, existing solutions struggle to account for the inherent\nheterogeneity of data (e.g., text, tables, diagrams) and evolving nature of\nregulatory standards used in financial filings, leading to compromised accuracy\nin critical information extraction. We propose the FinSage framework as a\nsolution, utilizing a multi-aspect RAG framework tailored for regulatory\ncompliance analysis in multi-modal financial documents. FinSage introduces\nthree innovative components: (1) a multi-modal pre-processing pipeline that\nunifies diverse data formats and generates chunk-level metadata summaries, (2)\na multi-path sparse-dense retrieval system augmented with query expansion\n(HyDE) and metadata-aware semantic search, and (3) a domain-specialized\nre-ranking module fine-tuned via Direct Preference Optimization (DPO) to\nprioritize compliance-critical content. Extensive experiments demonstrate that\nFinSage achieves an impressive recall of 92.51% on 75 expert-curated questions\nderived from surpasses the best baseline method on the FinanceBench question\nanswering datasets by 24.06% in accuracy. Moreover, FinSage has been\nsuccessfully deployed as financial question-answering agent in online meetings,\nwhere it has already served more than 1,200 people.",
      "tldr_zh": "本研究提出 FinSage 框架，这是一个多方面 RAG 系统，旨在提升金融文件问答的准确性，通过处理多模态数据（如文本、表格和图表）的异质性和动态监管标准来解决现有方法的局限性。FinSage 包括三个关键组件：多模态预处理管道用于统一数据格式并生成块级元数据摘要、多路径稀疏-密集检索系统结合 HyDE 查询扩展和元数据感知语义搜索，以及通过 Direct Preference Optimization (DPO) 微调的领域专用重新排序模块，以优先合规性关键内容。实验结果显示，FinSage 在 FinanceBench 数据集上准确率比最佳基线方法提高 24.06%，回忆率达到 92.51%，并已成功部署在线会议中，服务超过 1,200 人。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14493v2",
      "published_date": "2025-04-20 04:58:14 UTC",
      "updated_date": "2025-04-29 20:38:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:02:37.528519"
    },
    {
      "arxiv_id": "2504.14452v1",
      "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Chen",
        "Faeze Brahman",
        "Jiacheng Liu",
        "Niloofar Mireshghallah",
        "Weijia Shi",
        "Pang Wei Koh",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4).",
      "tldr_zh": "本文提出ParaPO，一种后训练微调方法，用于减少语言模型对预训练数据的原样复制，同时保持模型的整体效用。ParaPO通过训练模型偏好于对记忆段落的改写版本而非原内容，并引入系统提示来控制复制行为，以确保在适当情况下保留著名引用的回忆能力。在Llama3.1-8B和Tulu3-8B模型上的实验显示，ParaPO显著降低了无意复制率（例如，创意写作中从17.3降到12.9），比现有unlearning方法更有效且跨领域适用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14452v1",
      "published_date": "2025-04-20 01:59:46 UTC",
      "updated_date": "2025-04-20 01:59:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:02:47.886368"
    },
    {
      "arxiv_id": "2504.14448v1",
      "title": "Seeing Through Risk: A Symbolic Approximation of Prospect Theory",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Arslan Yousaf",
        "Umair Rehman",
        "Muhammad Umair Danish"
      ],
      "abstract": "We propose a novel symbolic modeling framework for decision-making under risk\nthat merges interpretability with the core insights of Prospect Theory. Our\napproach replaces opaque utility curves and probability weighting functions\nwith transparent, effect-size-guided features. We mathematically formalize the\nmethod, demonstrate its ability to replicate well-known framing and\nloss-aversion phenomena, and provide an end-to-end empirical validation on\nsynthetic datasets. The resulting model achieves competitive predictive\nperformance while yielding clear coefficients mapped onto psychological\nconstructs, making it suitable for applications ranging from AI safety to\neconomic policy analysis.",
      "tldr_zh": "本研究提出了一种新的符号建模框架，用于风险决策中，将 Prospect Theory 的核心见解与可解释性相结合，通过基于效应大小的透明特征替换传统的效用曲线和概率加权函数。框架被数学形式化，并展示了其能复制 framing 和 loss-aversion 等现象，在合成数据集上进行端到端实证验证，实现了与竞争模型相当的预测性能，同时提供清晰的系数映射到心理结构。该方法适用于 AI safety 到经济政策分析等领域的应用。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.OC",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14448v1",
      "published_date": "2025-04-20 01:44:54 UTC",
      "updated_date": "2025-04-20 01:44:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:03:00.328058"
    },
    {
      "arxiv_id": "2504.14439v1",
      "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Avinandan Bose",
        "Zhihan Xiong",
        "Yuejie Chi",
        "Simon Shaolei Du",
        "Lin Xiao",
        "Maryam Fazel"
      ],
      "abstract": "Personalizing large language models (LLMs) to accommodate diverse user\npreferences is essential for enhancing alignment and user satisfaction.\nTraditional reinforcement learning from human feedback (RLHF) approaches often\nrely on monolithic value representations, limiting their ability to adapt to\nindividual preferences. We introduce a novel framework that leverages low-rank\npreference modeling to efficiently learn and generalize user-specific reward\nfunctions. By representing reward functions in a low-dimensional subspace and\nmodeling individual preferences as weighted combinations of shared basis\nfunctions, our approach avoids rigid user categorization while enabling\nscalability and few-shot adaptation. We validate our method on multiple\npreference datasets, demonstrating superior generalization to unseen users and\nimproved accuracy in preference prediction tasks.",
      "tldr_zh": "该论文提出 LoRe 框架，通过低秩奖励建模 (low-rank reward modeling) 来个性化大型语言模型 (LLMs)，以更好地适应多样用户偏好并提升模型对齐度和用户满意度。不同于传统强化学习从人类反馈 (RLHF) 的单一价值表示，该方法将奖励函数表示为低维子空间中的共享基函数加权组合，实现高效学习、少样本适应和可扩展性，同时避免刚性用户分类。在多个偏好数据集上验证，LoRe 展示了优越的泛化能力，能够更准确地处理未见用户并提升偏好预测任务的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14439v1",
      "published_date": "2025-04-20 01:16:24 UTC",
      "updated_date": "2025-04-20 01:16:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:03:12.912413"
    },
    {
      "arxiv_id": "2504.14432v1",
      "title": "ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmad Khalil",
        "Mahmoud Khalil",
        "Alioune Ngom"
      ],
      "abstract": "In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel\ncross-modal framework for zero-shot video understanding that integrates a\nResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM\naddresses the challenges associated with zero-shot video models by avoiding\nreliance on pre-trained video understanding models and instead employing a\nnon-pretrained ResNet to extract visual features. This design ensures the model\nlearns visual and semantic representations within a unified architecture,\nenhancing its ability to generate accurate and contextually relevant textual\ndescriptions from video inputs. Our experimental results demonstrate that\nResNetVLLM achieves state-of-the-art performance in zero-shot video\nunderstanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,\nTGIF-QA FrameQA, and ActivityNet-QA.",
      "tldr_zh": "本文提出 ResNetVLLM，一种新型跨模态框架，将基于 ResNet 的视觉编码器与 Large Language Model (LLM) 整合，用于零-shot video understanding 任务。该框架避免依赖预训练视频模型，而是采用非预训练的 ResNet 提取视觉特征，从而在统一架构中学习视觉和语义表示，提升从视频输入生成准确、上下文相关文本描述的能力。实验结果表明，ResNetVLLM 在 MSRVTT-QA、MSVD-QA、TGIF-QA FrameQA 和 ActivityNet-QA 等基准上实现了最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14432v1",
      "published_date": "2025-04-20 00:20:18 UTC",
      "updated_date": "2025-04-20 00:20:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:03:25.562531"
    },
    {
      "arxiv_id": "2504.14429v1",
      "title": "ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmad Khalil",
        "Mahmoud Khalil",
        "Alioune Ngom"
      ],
      "abstract": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) tasks, but they suffer from hallucination, generating plausible yet\nfactually incorrect content. This issue extends to Video-Language Models\n(VideoLLMs), where textual descriptions may inaccurately represent visual\ncontent, resulting in multi-modal hallucinations. In this paper, we address\nhallucination in ResNetVLLM, a video-language model combining ResNet visual\nencoders with LLMs. We introduce a two-step protocol: (1) a faithfulness\ndetection strategy that uses a modified Lynx model to assess semantic alignment\nbetween generated captions and ground-truth video references, and (2) a\nhallucination mitigation strategy using Retrieval-Augmented Generation (RAG)\nwith an ad-hoc knowledge base dynamically constructed during inference. Our\nenhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by\ncross-verifying generated content against external knowledge, improving factual\nconsistency. Evaluation on the ActivityNet-QA benchmark demonstrates a\nsubstantial accuracy increase from 54.8% to 65.3%, highlighting the\neffectiveness of our hallucination detection and mitigation strategies in\nenhancing video-language model reliability.",
      "tldr_zh": "本研究针对ResNetVLLM模型的多模态幻觉(multi-modal hallucinations)问题，提出了一种两步协议来提升视频语言模型(VideoLLMs)的准确性和事实一致性。首先，采用忠实度检测(faithfulness detection)策略，使用修改后的Lynx模型评估生成标题与真实视频参考的语义对齐。其次，通过Retrieval-Augmented Generation (RAG)结合动态构建的知识库，对生成内容进行交叉验证，从而缓解幻觉。实验结果显示，增强版ResNetVLLM-2在ActivityNet-QA基准测试中准确率从54.8%提高到65.3%，证明了该策略在提高视频语言模型可靠性的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14429v1",
      "published_date": "2025-04-20 00:10:44 UTC",
      "updated_date": "2025-04-20 00:10:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:03:36.784940"
    },
    {
      "arxiv_id": "2504.14427v1",
      "title": "Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Spencer Lin",
        "Miru Jun",
        "Basem Rizk",
        "Karen Shieh",
        "Scott Fisher",
        "Sharon Mozgai"
      ],
      "abstract": "This case study presents our user-centered design model for Socially\nIntelligent Agent (SIA) development frameworks through our experience\ndeveloping Estuary, an open source multimodal framework for building\nlow-latency real-time socially interactive agents. We leverage the Rapid\nAssessment Process (RAP) to collect the thoughts of leading researchers in the\nfield of SIAs regarding the current state of the art for SIA development as\nwell as their evaluation of how well Estuary may potentially address current\nresearch gaps. We achieve this through a series of end-user interviews\nconducted by a fellow researcher in the community. We hope that the findings of\nour work will not only assist the continued development of Estuary but also\nguide the development of other future frameworks and technologies for SIAs.",
      "tldr_zh": "这篇案例研究介绍了用户中心设计模型，用于优化 Socially Intelligent Agent (SIA) 开发框架，基于开发 Estuary（一个开源多模态、低延迟实时 SIA 框架）的经验。研究团队采用 Rapid Assessment Process (RAP) 和社区研究者进行的端用户访谈，收集领域领先研究者的反馈，以评估当前 SIA 开发状态和 Estuary 填补研究空白的潜力。研究发现有望指导 Estuary 的持续改进，并为其他未来 SIA 框架和技术提供开发指导。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.14427v1",
      "published_date": "2025-04-20 00:02:56 UTC",
      "updated_date": "2025-04-20 00:02:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-24T15:03:48.150252"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 55,
  "processed_papers_count": 55,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-24T15:04:08.367997"
}
[
  {
    "arxiv_id": "2403.11027v2",
    "title": "Reward Guided Latent Consistency Distillation",
    "authors": [
      "Jiachen Li",
      "Weixi Feng",
      "Wenhu Chen",
      "William Yang Wang"
    ],
    "abstract": "Latent Consistency Distillation (LCD) has emerged as a promising paradigm for\nefficient text-to-image synthesis. By distilling a latent consistency model\n(LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates\nthe generation of high-fidelity images within merely 2 to 4 inference steps.\nHowever, the LCM's efficient inference is obtained at the cost of the sample\nquality. In this paper, we propose compensating the quality loss by aligning\nLCM's output with human preference during training. Specifically, we introduce\nReward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM)\ninto the LCD process by augmenting the original LCD loss with the objective of\nmaximizing the reward associated with LCM's single-step generation. As\nvalidated through human evaluation, when trained with the feedback of a good\nRM, the 2-step generations from our RG-LCM are favored by humans over the\n50-step DDIM samples from the teacher LDM, representing a 25-time inference\nacceleration without quality loss.\n  As directly optimizing towards differentiable RMs can suffer from\nover-optimization, we take the initial step to overcome this difficulty by\nproposing the use of a latent proxy RM (LRM). This novel component serves as an\nintermediary, connecting our LCM with the RM. Empirically, we demonstrate that\nincorporating the LRM into our RG-LCD successfully avoids high-frequency noise\nin the generated images, contributing to both improved Fr\\'echet Inception\nDistance (FID) on MS-COCO and a higher HPSv2.1 score on HPSv2's test set,\nsurpassing those achieved by the baseline LCM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by TMLR. Project page: https://rg-lcd.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.11027v2",
    "published_date": "2024-03-16 22:14:56 UTC",
    "updated_date": "2024-10-07 18:47:47 UTC"
  },
  {
    "arxiv_id": "2403.11021v3",
    "title": "Towards Neuro-Symbolic Video Understanding",
    "authors": [
      "Minkyu Choi",
      "Harsh Goel",
      "Mohammad Omama",
      "Yunhao Yang",
      "Sahil Shah",
      "Sandeep Chinchali"
    ],
    "abstract": "The unprecedented surge in video data production in recent years necessitates\nefficient tools to extract meaningful frames from videos for downstream tasks.\nLong-term temporal reasoning is a key desideratum for frame retrieval systems.\nWhile state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are\nproficient in short-term semantic understanding, they surprisingly fail at\nlong-term reasoning across frames. A key reason for this failure is that they\nintertwine per-frame perception and temporal reasoning into a single deep\nnetwork. Hence, decoupling but co-designing semantic understanding and temporal\nreasoning is essential for efficient scene identification. We propose a system\nthat leverages vision-language models for semantic understanding of individual\nframes but effectively reasons about the long-term evolution of events using\nstate machines and temporal logic (TL) formulae that inherently capture memory.\nOur TL-based reasoning improves the F1 score of complex event identification by\n9-15% compared to benchmarks that use GPT4 for reasoning on state-of-the-art\nself-driving datasets such as Waymo and NuScenes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by The European Conference on Computer Vision (ECCV) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11021v3",
    "published_date": "2024-03-16 21:40:27 UTC",
    "updated_date": "2024-12-03 18:58:22 UTC"
  },
  {
    "arxiv_id": "2407.04191v2",
    "title": "GazeFusion: Saliency-Guided Image Generation",
    "authors": [
      "Yunxiang Zhang",
      "Nan Wu",
      "Connor Z. Lin",
      "Gordon Wetzstein",
      "Qi Sun"
    ],
    "abstract": "Diffusion models offer unprecedented image generation power given just a text\nprompt. While emerging approaches for controlling diffusion models have enabled\nusers to specify the desired spatial layouts of the generated content, they\ncannot predict or control where viewers will pay more attention due to the\ncomplexity of human vision. Recognizing the significance of\nattention-controllable image generation in practical applications, we present a\nsaliency-guided framework to incorporate the data priors of human visual\nattention mechanisms into the generation process. Given a user-specified viewer\nattention distribution, our control module conditions a diffusion model to\ngenerate images that attract viewers' attention toward the desired regions. To\nassess the efficacy of our approach, we performed an eye-tracked user study and\na large-scale model-based saliency analysis. The results evidence that both the\ncross-user eye gaze distributions and the saliency models' predictions align\nwith the desired attention distributions. Lastly, we outline several\napplications, including interactive design of saliency guidance, attention\nsuppression in unwanted regions, and adaptive generation for varied\ndisplay/viewing conditions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "ACM Transactions on Applied Perception (ACM Symposium on Applied\n  Perception 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.04191v2",
    "published_date": "2024-03-16 21:01:35 UTC",
    "updated_date": "2025-02-15 23:08:51 UTC"
  },
  {
    "arxiv_id": "2403.11015v1",
    "title": "Identifying the Attractors of Gene Regulatory Networks from Expression Data under Uncertainty: An Interpretable Approach",
    "authors": [
      "Alireza Rowhanimanesh"
    ],
    "abstract": "In systems biology, attractor landscape analysis of gene regulatory networks\nis recognized as a powerful computational tool for studying various cellular\nstates from proliferation and differentiation to senescence and apoptosis.\nTherefore, accurate identification of attractors plays a critical role in\ndetermination of the cell fates. On the other hand, in a real biological\ncircuit, genetic/epigenetic alterations as well as varying environmental\nfactors drastically take effect on the location, characteristics, and even the\nnumber of attractors. The central question is: Given a temporal gene expression\nprofile of a real gene regulatory network, how can the attractors be robustly\nidentified in the presence of huge amount of uncertainty? This paper addresses\nthis question using a novel approach based on Zadeh Computing with Words. The\nproposed scheme could effectively identify the attractors from temporal gene\nexpression data in terms of both fuzzy logic-based and linguistic descriptions\nwhich are simply interpretable by human experts. Therefore, this method can be\nconsidered as an effective step towards interpretable artificial intelligence.\nWithout loss of generality, genetic toggle switch is considered as the case\nstudy. The nonlinear dynamics of this benchmark gene regulatory network is\ncomputationally modeled by the notion of uncertain stochastic differential\nequations. The results of in-silico study demonstrate the efficiency and\nrobustness of the proposed method.",
    "categories": [
      "q-bio.MN",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "q-bio.MN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11015v1",
    "published_date": "2024-03-16 20:56:22 UTC",
    "updated_date": "2024-03-16 20:56:22 UTC"
  },
  {
    "arxiv_id": "2403.11009v2",
    "title": "DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages",
    "authors": [
      "Fahim Faisal",
      "Orevaoghene Ahia",
      "Aarohi Srivastava",
      "Kabir Ahuja",
      "David Chiang",
      "Yulia Tsvetkov",
      "Antonios Anastasopoulos"
    ],
    "abstract": "Language technologies should be judged on their usefulness in real-world use\ncases. An often overlooked aspect in natural language processing (NLP) research\nand evaluation is language variation in the form of non-standard dialects or\nlanguage varieties (hereafter, varieties). Most NLP benchmarks are limited to\nstandard language varieties. To fill this gap, we propose DIALECTBENCH, the\nfirst-ever large-scale benchmark for NLP on varieties, which aggregates an\nextensive set of task-varied variety datasets (10 text-level tasks covering 281\nvarieties). This allows for a comprehensive evaluation of NLP system\nperformance on different language varieties. We provide substantial evidence of\nperformance disparities between standard and non-standard language varieties,\nand we also identify language clusters with large performance divergence across\ntasks. We believe DIALECTBENCH provides a comprehensive view of the current\nstate of NLP for language varieties and one step towards advancing it further.\nCode/data: https://github.com/ffaisal93/DialectBench",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Equal contribution: Fahim Faisal, Orevaoghene Ahia",
    "pdf_url": "http://arxiv.org/pdf/2403.11009v2",
    "published_date": "2024-03-16 20:18:36 UTC",
    "updated_date": "2024-07-07 18:21:30 UTC"
  },
  {
    "arxiv_id": "2403.10997v2",
    "title": "N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields",
    "authors": [
      "Yash Bhalgat",
      "Iro Laina",
      "João F. Henriques",
      "Andrew Zisserman",
      "Andrea Vedaldi"
    ],
    "abstract": "Understanding complex scenes at multiple levels of abstraction remains a\nformidable challenge in computer vision. To address this, we introduce Nested\nNeural Feature Fields (N2F2), a novel approach that employs hierarchical\nsupervision to learn a single feature field, wherein different dimensions\nwithin the same high-dimensional feature encode scene properties at varying\ngranularities. Our method allows for a flexible definition of hierarchies,\ntailored to either the physical dimensions or semantics or both, thereby\nenabling a comprehensive and nuanced understanding of scenes. We leverage a 2D\nclass-agnostic segmentation model to provide semantically meaningful pixel\ngroupings at arbitrary scales in the image space, and query the CLIP\nvision-encoder to obtain language-aligned embeddings for each of these\nsegments. Our proposed hierarchical supervision method then assigns different\nnested dimensions of the feature field to distill the CLIP embeddings using\ndeferred volumetric rendering at varying physical scales, creating a\ncoarse-to-fine representation. Extensive experiments show that our approach\noutperforms the state-of-the-art feature field distillation methods on tasks\nsuch as open-vocabulary 3D segmentation and localization, demonstrating the\neffectiveness of the learned nested feature field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.10997v2",
    "published_date": "2024-03-16 18:50:44 UTC",
    "updated_date": "2024-07-28 15:53:39 UTC"
  },
  {
    "arxiv_id": "2403.10995v1",
    "title": "Edge Private Graph Neural Networks with Singular Value Perturbation",
    "authors": [
      "Tingting Tang",
      "Yue Niu",
      "Salman Avestimehr",
      "Murali Annavaram"
    ],
    "abstract": "Graph neural networks (GNNs) play a key role in learning representations from\ngraph-structured data and are demonstrated to be useful in many applications.\nHowever, the GNN training pipeline has been shown to be vulnerable to node\nfeature leakage and edge extraction attacks. This paper investigates a scenario\nwhere an attacker aims to recover private edge information from a trained GNN\nmodel. Previous studies have employed differential privacy (DP) to add noise\ndirectly to the adjacency matrix or a compact graph representation. The added\nperturbations cause the graph structure to be substantially morphed, reducing\nthe model utility. We propose a new privacy-preserving GNN training algorithm,\nEclipse, that maintains good model utility while providing strong privacy\nprotection on edges. Eclipse is based on two key observations. First, adjacency\nmatrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains\nGNNs with a low-rank format of the graph via singular values decomposition\n(SVD), rather than the original graph. Using the low-rank format, Eclipse\npreserves the primary graph topology and removes the remaining residual edges.\nEclipse adds noise to the low-rank singular values instead of the entire graph,\nthereby preserving the graph privacy while still maintaining enough of the\ngraph structure to maintain model utility. We theoretically show Eclipse\nprovide formal DP guarantee on edges. Experiments on benchmark graph datasets\nshow that Eclipse achieves significantly better privacy-utility tradeoff\ncompared to existing privacy-preserving GNN training methods. In particular,\nunder strong privacy constraints ($\\epsilon$ < 4), Eclipse shows significant\ngains in the model utility by up to 46%. We further demonstrate that Eclipse\nalso has better resilience against common edge attacks (e.g., LPA), lowering\nthe attack AUC by up to 5% compared to other state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at Privacy Enhancing Technologies Symposium (PETS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.10995v1",
    "published_date": "2024-03-16 18:44:56 UTC",
    "updated_date": "2024-03-16 18:44:56 UTC"
  },
  {
    "arxiv_id": "2403.10988v3",
    "title": "Boosting Flow-based Generative Super-Resolution Models via Learned Prior",
    "authors": [
      "Li-Yuan Tsao",
      "Yi-Chen Lo",
      "Chia-Che Chang",
      "Hao-Wei Chen",
      "Roy Tseng",
      "Chien Feng",
      "Chun-Yi Lee"
    ],
    "abstract": "Flow-based super-resolution (SR) models have demonstrated astonishing\ncapabilities in generating high-quality images. However, these methods\nencounter several challenges during image generation, such as grid artifacts,\nexploding inverses, and suboptimal results due to a fixed sampling temperature.\nTo overcome these issues, this work introduces a conditional learned prior to\nthe inference phase of a flow-based SR model. This prior is a latent code\npredicted by our proposed latent module conditioned on the low-resolution\nimage, which is then transformed by the flow model into an SR image. Our\nframework is designed to seamlessly integrate with any contemporary flow-based\nSR model without modifying its architecture or pre-trained weights. We evaluate\nthe effectiveness of our proposed framework through extensive experiments and\nablation analyses. The proposed framework successfully addresses all the\ninherent issues in flow-based SR models and enhances their performance in\nvarious SR scenarios. Our code is available at:\nhttps://github.com/liyuantsao/BFSR",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR2024",
    "pdf_url": "http://arxiv.org/pdf/2403.10988v3",
    "published_date": "2024-03-16 18:04:12 UTC",
    "updated_date": "2024-05-29 03:12:58 UTC"
  },
  {
    "arxiv_id": "2403.10984v2",
    "title": "IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning",
    "authors": [
      "Fan Chen",
      "Shahzeen Attari",
      "Gayle Buck",
      "Lei Jiang"
    ],
    "abstract": "To improve privacy and ensure quality-of-service (QoS), deep learning (DL)\nmodels are increasingly deployed on Internet of Things (IoT) devices for data\nprocessing, significantly increasing the carbon footprint associated with DL on\nIoT, covering both operational and embodied aspects. Existing operational\nenergy predictors often overlook quantized DL models and emerging neural\nprocessing units (NPUs), while embodied carbon footprint modeling tools neglect\nnon-computing hardware components common in IoT devices, creating a gap in\naccurate carbon footprint modeling tools for IoT-enabled DL. This paper\nintroduces \\textit{\\carb}, an end-to-end tool for precise carbon footprint\nestimation in IoT-enabled DL, with deviations as low as 5\\% for operational and\n3.23\\% for embodied carbon footprints compared to actual measurements across\nvarious DL models. Additionally, practical applications of \\carb~are showcased\nthrough multiple user case studies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "5 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.10984v2",
    "published_date": "2024-03-16 17:32:59 UTC",
    "updated_date": "2024-09-13 16:21:58 UTC"
  },
  {
    "arxiv_id": "2403.13841v2",
    "title": "Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting",
    "authors": [
      "Zhongqi Yang",
      "Yuning Wang",
      "Ken S. Yamashita",
      "Maryam Sabah",
      "Elahe Khatibi",
      "Iman Azimi",
      "Nikil Dutt",
      "Jessica L. Borelli",
      "Amir M. Rahmani"
    ],
    "abstract": "Emotional states, as indicators of affect, are pivotal to overall health,\nmaking their accurate prediction before onset crucial. Current studies are\nprimarily centered on immediate short-term affect detection using data from\nwearable and mobile devices. These studies typically focus on objective sensory\nmeasures, often neglecting other forms of self-reported information like\ndiaries and notes. In this paper, we propose a multimodal deep learning model\nfor affect status forecasting. This model combines a transformer encoder with a\npre-trained language model, facilitating the integrated analysis of objective\nmetrics and self-reported diaries. To validate our model, we conduct a\nlongitudinal study, enrolling college students and monitoring them over a year,\nto collect an extensive dataset including physiological, environmental, sleep,\nmetabolic, and physical activity parameters, alongside open-ended textual\ndiaries provided by the participants. Our results demonstrate that the proposed\nmodel achieves predictive accuracy of 82.50% for positive affect and 82.76% for\nnegative affect, a full week in advance. The effectiveness of our model is\nfurther elevated by its explainability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Connected Health: Applications, Systems and Engineering\n  Technologies (CHASE) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13841v2",
    "published_date": "2024-03-16 17:24:38 UTC",
    "updated_date": "2024-03-23 18:27:49 UTC"
  },
  {
    "arxiv_id": "2403.10968v1",
    "title": "Enhancing IoT Security Against DDoS Attacks through Federated Learning",
    "authors": [
      "Ghazaleh Shirvani",
      "Saeid Ghasemshirazi",
      "Mohammad Ali Alipour"
    ],
    "abstract": "The rapid proliferation of the Internet of Things (IoT) has ushered in\ntransformative connectivity between physical devices and the digital realm.\nNonetheless, the escalating threat of Distributed Denial of Service (DDoS)\nattacks jeopardizes the integrity and reliability of IoT networks. Conventional\nDDoS mitigation approaches are ill-equipped to handle the intricacies of IoT\necosystems, potentially compromising data privacy. This paper introduces an\ninnovative strategy to bolster the security of IoT networks against DDoS\nattacks by harnessing the power of Federated Learning that allows multiple IoT\ndevices or edge nodes to collaboratively build a global model while preserving\ndata privacy and minimizing communication overhead. The research aims to\ninvestigate Federated Learning's effectiveness in detecting and mitigating DDoS\nattacks in IoT. Our proposed framework leverages IoT devices' collective\nintelligence for real-time attack detection without compromising sensitive\ndata. This study proposes innovative deep autoencoder approaches for data\ndimensionality reduction, retraining, and partial selection to enhance the\nperformance and stability of the proposed model. Additionally, two renowned\naggregation algorithms, FedAvg and FedAvgM, are employed in this research.\nVarious metrics, including true positive rate, false positive rate, and\nF1-score, are employed to evaluate the model. The dataset utilized in this\nresearch, N-BaIoT, exhibits non-IID data distribution, where data categories\nare distributed quite differently. The negative impact of these distribution\ndisparities is managed by employing retraining and partial selection\ntechniques, enhancing the final model's stability. Furthermore, evaluation\nresults demonstrate that the FedAvgM aggregation algorithm outperforms FedAvg,\nindicating that in non-IID datasets, FedAvgM provides better stability and\nperformance.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10968v1",
    "published_date": "2024-03-16 16:45:28 UTC",
    "updated_date": "2024-03-16 16:45:28 UTC"
  },
  {
    "arxiv_id": "2403.10967v2",
    "title": "Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization",
    "authors": [
      "Sai Prasanna",
      "Karim Farid",
      "Raghu Rajan",
      "André Biedenkapp"
    ],
    "abstract": "Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for\ncreating generally capable embodied agents. To address the broader challenge,\nwe start with the simpler setting of contextual reinforcement learning (cRL),\nassuming observability of the context values that parameterize the variation in\nthe system's dynamics, such as the mass or dimensions of a robot, without\nmaking further simplifying assumptions about the observability of the Markovian\nstate. Toward the goal of ZSG to unseen variation in context, we propose the\ncontextual recurrent state-space model (cRSSM), which introduces changes to the\nworld model of Dreamer (v3) (Hafner et al., 2023). This allows the world model\nto incorporate context for inferring latent Markovian states from the\nobservations and modeling the latent dynamics. Our approach is evaluated on two\ntasks from the CARL benchmark suite, which is tailored to study contextual RL.\nOur experiments show that such systematic incorporation of the context improves\nthe ZSG of the policies trained on the \"dreams\" of the world model. We further\nfind qualitatively that our approach allows Dreamer to disentangle the latent\nstate from context, allowing it to extrapolate its dreams to the many worlds of\nunseen contexts. The code for all our experiments is available at\nhttps://github.com/sai-prasanna/dreaming_of_many_worlds.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "In Reinforcement Learning Conference, 2024. 33 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.10967v2",
    "published_date": "2024-03-16 16:29:40 UTC",
    "updated_date": "2024-08-03 14:25:42 UTC"
  },
  {
    "arxiv_id": "2405.10322v1",
    "title": "Exploring the Independent Cascade Model and Its Evolution in Social Network Information Diffusion",
    "authors": [
      "Jixuan He",
      "Yutong Guo",
      "Jiacheng Zhao"
    ],
    "abstract": "This paper delves into the paramount significance of information\ndissemination within the dynamic realm of social networks. It underscores the\npivotal role of information communication models in unraveling the intricacies\nof data propagation in the digital age. By shedding light on the profound\ninfluence of these models, it not only lays the groundwork for exploring\nvarious hierarchies and their manifestations but also serves as a catalyst for\nfurther research in this formidable field.",
    "categories": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.10322v1",
    "published_date": "2024-03-16 15:32:13 UTC",
    "updated_date": "2024-03-16 15:32:13 UTC"
  },
  {
    "arxiv_id": "2403.10949v2",
    "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
    "authors": [
      "Haozhe Chen",
      "Carl Vondrick",
      "Chengzhi Mao"
    ],
    "abstract": "How do large language models (LLMs) obtain their answers? The ability to\nexplain and control an LLM's reasoning process is key for reliability,\ntransparency, and future model developments. We propose SelfIE\n(Self-Interpretation of Embeddings), a framework that enables LLMs to interpret\ntheir own embeddings in natural language by leveraging their ability to respond\nto inquiries about a given passage. Capable of interpreting open-world concepts\nin the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such\nas making ethical decisions, internalizing prompt injection, and recalling\nharmful knowledge. SelfIE's text descriptions on hidden embeddings also open up\nnew avenues to control LLM reasoning. We propose Supervised Control, which\nallows editing open-ended concepts while only requiring gradient computation of\nindividual layer. We extend RLHF to hidden embeddings and propose Reinforcement\nControl that erases harmful knowledge in LLM without supervision targets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10949v2",
    "published_date": "2024-03-16 15:30:34 UTC",
    "updated_date": "2024-03-26 01:15:09 UTC"
  },
  {
    "arxiv_id": "2403.10944v1",
    "title": "Human Centered AI for Indian Legal Text Analytics",
    "authors": [
      "Sudipto Ghosh",
      "Devanshu Verma",
      "Balaji Ganesan",
      "Purnima Bindal",
      "Vikas Kumar",
      "Vasudha Bhatnagar"
    ],
    "abstract": "Legal research is a crucial task in the practice of law. It requires intense\nhuman effort and intellectual prudence to research a legal case and prepare\narguments. Recent boom in generative AI has not translated to proportionate\nrise in impactful legal applications, because of low trustworthiness and and\nthe scarcity of specialized datasets for training Large Language Models (LLMs).\nThis position paper explores the potential of LLMs within Legal Text Analytics\n(LTA), highlighting specific areas where the integration of human expertise can\nsignificantly enhance their performance to match that of experts. We introduce\na novel dataset and describe a human centered, compound AI system that\nprincipally incorporates human inputs for performing LTA tasks with LLMs.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "7 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.10944v1",
    "published_date": "2024-03-16 15:17:13 UTC",
    "updated_date": "2024-03-16 15:17:13 UTC"
  },
  {
    "arxiv_id": "2405.01554v1",
    "title": "Early-stage detection of cognitive impairment by hybrid quantum-classical algorithm using resting-state functional MRI time-series",
    "authors": [
      "Junggu Choi",
      "Tak Hur",
      "Daniel K. Park",
      "Na-Young Shin",
      "Seung-Koo Lee",
      "Hakbae Lee",
      "Sanghoon Han"
    ],
    "abstract": "Following the recent development of quantum machine learning techniques, the\nliterature has reported several quantum machine learning algorithms for disease\ndetection. This study explores the application of a hybrid quantum-classical\nalgorithm for classifying region-of-interest time-series data obtained from\nresting-state functional magnetic resonance imaging in patients with\nearly-stage cognitive impairment based on the importance of cognitive decline\nfor dementia or aging. Classical one-dimensional convolutional layers are used\ntogether with quantum convolutional neural networks in our hybrid algorithm. In\nthe classical simulation, the proposed hybrid algorithms showed higher balanced\naccuracies than classical convolutional neural networks under the similar\ntraining conditions. Moreover, a total of nine brain regions (left precentral\ngyrus, right superior temporal gyrus, left rolandic operculum, right rolandic\noperculum, left parahippocampus, right hippocampus, left medial frontal gyrus,\nright cerebellum crus, and cerebellar vermis) among 116 brain regions were\nfound to be relatively effective brain regions for the classification based on\nthe model performances. The associations of the selected nine regions with\ncognitive decline, as found in previous studies, were additionally validated\nthrough seed-based functional connectivity analysis. We confirmed both the\nimprovement of model performance with the quantum convolutional neural network\nand neuroscientific validities of brain regions from our hybrid\nquantum-classical model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.01554v1",
    "published_date": "2024-03-16 15:10:50 UTC",
    "updated_date": "2024-03-16 15:10:50 UTC"
  },
  {
    "arxiv_id": "2403.10930v1",
    "title": "Inducing Individual Students' Learning Strategies through Homomorphic POMDPs",
    "authors": [
      "Huifan Gao",
      "Yifeng Zeng",
      "Yinghui Pan"
    ],
    "abstract": "Optimizing students' learning strategies is a crucial component in\nintelligent tutoring systems. Previous research has demonstrated the\neffectiveness of devising personalized learning strategies for students by\nmodelling their learning processes through partially observable Markov decision\nprocess (POMDP). However, the research holds the assumption that the student\npopulation adheres to a uniform cognitive pattern. While this assumption\nsimplifies the POMDP modelling process, it evidently deviates from a real-world\nscenario, thus reducing the precision of inducing individual students' learning\nstrategies. In this article, we propose the homomorphic POMDP (H-POMDP) model\nto accommodate multiple cognitive patterns and present the parameter learning\napproach to automatically construct the H-POMDP model. Based on the H-POMDP\nmodel, we are able to represent different cognitive patterns from the data and\ninduce more personalized learning strategies for individual students. We\nconduct experiments to show that, in comparison to the general POMDP approach,\nthe H-POMDP model demonstrates better precision when modelling mixed data from\nmultiple cognitive patterns. Moreover, the learning strategies derived from\nH-POMDPs exhibit better personalization in the performance evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11pages, 3figures",
    "pdf_url": "http://arxiv.org/pdf/2403.10930v1",
    "published_date": "2024-03-16 14:06:29 UTC",
    "updated_date": "2024-03-16 14:06:29 UTC"
  },
  {
    "arxiv_id": "2403.10923v2",
    "title": "Interpretable Machine Learning for TabPFN",
    "authors": [
      "David Rundel",
      "Julius Kobialka",
      "Constantin von Crailsheim",
      "Matthias Feurer",
      "Thomas Nagler",
      "David Rügamer"
    ],
    "abstract": "The recently developed Prior-Data Fitted Networks (PFNs) have shown very\npromising results for applications in low-data regimes. The TabPFN model, a\nspecial case of PFNs for tabular data, is able to achieve state-of-the-art\nperformance on a variety of classification tasks while producing posterior\npredictive distributions in mere seconds by in-context learning without the\nneed for learning parameters or hyperparameter tuning. This makes TabPFN a very\nattractive option for a wide range of domain applications. However, a major\ndrawback of the method is its lack of interpretability. Therefore, we propose\nseveral adaptations of popular interpretability methods that we specifically\ndesign for TabPFN. By taking advantage of the unique properties of the model,\nour adaptations allow for more efficient computations than existing\nimplementations. In particular, we show how in-context learning facilitates the\nestimation of Shapley values by avoiding approximate retraining and enables the\nuse of Leave-One-Covariate-Out (LOCO) even when working with large-scale\nTransformers. In addition, we demonstrate how data valuation methods can be\nused to address scalability challenges of TabPFN. Our proposed methods are\nimplemented in a package tabpfn_iml and made available at\nhttps://github.com/david-rundel/tabpfn_iml.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Explainable Artificial Intelligence, and is available online at\n  https://doi.org/10.1007/978-3-031-63797-1_23",
    "pdf_url": "http://arxiv.org/pdf/2403.10923v2",
    "published_date": "2024-03-16 13:35:15 UTC",
    "updated_date": "2024-07-23 16:10:52 UTC"
  },
  {
    "arxiv_id": "2403.10903v4",
    "title": "DTOR: Decision Tree Outlier Regressor to explain anomalies",
    "authors": [
      "Riccardo Crupi",
      "Daniele Regoli",
      "Alessandro Damiano Sabatino",
      "Immacolata Marano",
      "Massimiliano Brinis",
      "Luca Albertazzi",
      "Andrea Cirillo",
      "Andrea Claudio Cosentini"
    ],
    "abstract": "Explaining outliers occurrence and mechanism of their occurrence can be\nextremely important in a variety of domains. Malfunctions, frauds, threats, in\naddition to being correctly identified, oftentimes need a valid explanation in\norder to effectively perform actionable counteracts. The ever more widespread\nuse of sophisticated Machine Learning approach to identify anomalies make such\nexplanations more challenging. We present the Decision Tree Outlier Regressor\n(DTOR), a technique for producing rule-based explanations for individual data\npoints by estimating anomaly scores generated by an anomaly detection model.\nThis is accomplished by first applying a Decision Tree Regressor, which\ncomputes the estimation score, and then extracting the relative path associated\nwith the data point score. Our results demonstrate the robustness of DTOR even\nin datasets with a large number of features. Additionally, in contrast to other\nrule-based approaches, the generated rules are consistently satisfied by the\npoints to be explained. Furthermore, our evaluation metrics indicate comparable\nperformance to Anchors in outlier explanation tasks, with reduced execution\ntime.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10903v4",
    "published_date": "2024-03-16 11:38:31 UTC",
    "updated_date": "2024-05-12 17:20:11 UTC"
  },
  {
    "arxiv_id": "2403.10882v2",
    "title": "Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean",
    "authors": [
      "ChangSu Choi",
      "Yongbin Jeong",
      "Seoyoon Park",
      "InHo Won",
      "HyeonSeok Lim",
      "SangMin Kim",
      "Yejee Kang",
      "Chanhyuk Yoon",
      "Jaewan Park",
      "Yiseul Lee",
      "HyeJin Lee",
      "Younggyun Hahm",
      "Hansaem Kim",
      "KyungTae Lim"
    ],
    "abstract": "Large language models (LLMs) use pretraining to predict the subsequent word;\nhowever, their expansion requires significant computing resources. Numerous big\ntech companies and research institutes have developed multilingual LLMs (MLLMs)\nto meet current demands, overlooking less-resourced languages (LRLs). This\nstudy proposed three strategies to enhance the performance of LRLs based on the\npublicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to\nenhance expressiveness. Second, bilingual data were used for pretraining to\nalign the high- and less-resourced languages. Third, a high-quality small-scale\ninstruction dataset was constructed and instruction-tuning was performed to\naugment the LRL. The experiments employed the Llama2 model and Korean was used\nas the LRL, which was quantitatively evaluated against other developed LLMs\nacross eight tasks. Furthermore, a qualitative assessment was performed based\non human evaluation and GPT4. Experimental results showed that our proposed\nBllossom model exhibited superior performance in qualitative analyses compared\nto previously proposed Korean monolingual models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10882v2",
    "published_date": "2024-03-16 10:26:38 UTC",
    "updated_date": "2024-03-21 14:50:18 UTC"
  },
  {
    "arxiv_id": "2403.10863v1",
    "title": "stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for Spatial Transcriptomics Data Imputation",
    "authors": [
      "Xiaoyu Li",
      "Wenwen Min",
      "Shunfang Wang",
      "Changmiao Wang",
      "Taosheng Xu"
    ],
    "abstract": "Spatially resolved transcriptomics represents a significant advancement in\nsingle-cell analysis by offering both gene expression data and their\ncorresponding physical locations. However, this high degree of spatial\nresolution entails a drawback, as the resulting spatial transcriptomic data at\nthe cellular level is notably plagued by a high incidence of missing values.\nFurthermore, most existing imputation methods either overlook the spatial\ninformation between spots or compromise the overall gene expression data\ndistribution. To address these challenges, our primary focus is on effectively\nutilizing the spatial location information within spatial transcriptomic data\nto impute missing values, while preserving the overall data distribution. We\nintroduce \\textbf{stMCDI}, a novel conditional diffusion model for spatial\ntranscriptomics data imputation, which employs a denoising network trained\nusing randomly masked data portions as guidance, with the unmasked data serving\nas conditions. Additionally, it utilizes a GNN encoder to integrate the spatial\nposition information, thereby enhancing model performance. The results obtained\nfrom spatial transcriptomics datasets elucidate the performance of our methods\nrelative to existing approaches.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "Submitted to IJCAI2024",
    "pdf_url": "http://arxiv.org/pdf/2403.10863v1",
    "published_date": "2024-03-16 09:06:38 UTC",
    "updated_date": "2024-03-16 09:06:38 UTC"
  },
  {
    "arxiv_id": "2403.10860v2",
    "title": "Sim2Real within 5 Minutes: Efficient Domain Transfer with Stylized Gaussian Splatting for Endoscopic Images",
    "authors": [
      "Junyang Wu",
      "Yun Gu",
      "Guang-Zhong Yang"
    ],
    "abstract": "Robot assisted endoluminal intervention is an emerging technique for both\nbenign and malignant luminal lesions. With vision-based navigation, when\ncombined with pre-operative imaging data as priors, it is possible to recover\nposition and pose of the endoscope without the need of additional sensors. In\npractice, however, aligning pre-operative and intra-operative domains is\ncomplicated by significant texture differences. Although methods such as style\ntransfer can be used to address this issue, they require large datasets from\nboth source and target domains with prolonged training times. This paper\nproposes an efficient domain transfer method based on stylized Gaussian\nsplatting, only requiring a few of real images (10 images) with very fast\ntraining time. Specifically, the transfer process includes two phases. In the\nfirst phase, the 3D models reconstructed from CT scans are represented as\ndifferential Gaussian point clouds. In the second phase, only color appearance\nrelated parameters are optimized to transfer the style and preserve the visual\ncontent. A novel structure consistency loss is applied to latent features and\ndepth levels to enhance the stability of the transferred images. Detailed\nvalidation was performed to demonstrate the performance advantages of the\nproposed method compared to that of the current state-of-the-art, highlighting\nthe potential for intra-operative surgical navigation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.10860v2",
    "published_date": "2024-03-16 08:57:00 UTC",
    "updated_date": "2025-03-05 12:41:05 UTC"
  },
  {
    "arxiv_id": "2403.10853v3",
    "title": "Just Say the Name: Online Continual Learning with Category Names Only via Data Generation",
    "authors": [
      "Minhyuk Seo",
      "Seongwon Cho",
      "Minjae Lee",
      "Diganta Misra",
      "Hyeonbeom Choi",
      "Seon Joo Kim",
      "Jonghyun Choi"
    ],
    "abstract": "Requiring extensive human supervision is often impractical for continual\nlearning due to its cost, leading to the emergence of 'name-only continual\nlearning' that only provides the name of new concepts (e.g., classes) without\nproviding supervised samples. To address the task, recent approach uses\nweb-scraped data but results in issues such as data imbalance, copyright, and\nprivacy concerns. To overcome the limitations of both human supervision and\nwebly supervision, we propose Generative name only Continual Learning (GenCL)\nusing generative models for the name only continual learning. But na\\\"ive\napplication of generative models results in limited diversity of generated\ndata. So, we specifically propose a diverse prompt generation method,\nHIerarchical Recurrent Prompt Generation (HIRPG) as well as\nCOmplexity-NAvigating eNsembler (CONAN) that selects samples with minimal\noverlap from multiple generative models. We empirically validate that the\nproposed GenCL outperforms prior arts, even a model trained with fully\nsupervised data, in various tasks including image recognition and multi-modal\nvisual reasoning. Data generated by GenCL is available at\nhttps://anonymous.4open.science/r/name-only-continual-E079.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10853v3",
    "published_date": "2024-03-16 08:28:42 UTC",
    "updated_date": "2024-10-19 14:51:45 UTC"
  },
  {
    "arxiv_id": "2403.10850v1",
    "title": "GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language Models for Complex Lighting Environments",
    "authors": [
      "Zhuowei Li",
      "Miao Zhang",
      "Xiaotian Lin",
      "Meng Yin",
      "Shuai Lu",
      "Xueqian Wang"
    ],
    "abstract": "This paper introduces GAgent: an Gripping Agent designed for open-world\nenvironments that provides advanced cognitive abilities via VLM agents and\nflexible grasping abilities with variable stiffness soft grippers. GAgent\ncomprises three primary components - Prompt Engineer module, Visual-Language\nModel (VLM) core and Workflow module. These three modules enhance gripper\nsuccess rates by recognizing objects and materials and accurately estimating\ngrasp area even under challenging lighting conditions. As part of creativity,\nresearchers also created a bionic hybrid soft gripper with variable stiffness\ncapable of gripping heavy loads while still gently engaging objects. This\nintelligent agent, featuring VLM-based cognitive processing with bionic design,\nshows promise as it could potentially benefit UAVs in various scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10850v1",
    "published_date": "2024-03-16 08:10:23 UTC",
    "updated_date": "2024-03-16 08:10:23 UTC"
  },
  {
    "arxiv_id": "2403.10842v4",
    "title": "Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process",
    "authors": [
      "Mohammad Ali Labbaf-Khaniki",
      "Mohammad Manthouri",
      "Hanieh Ajami"
    ],
    "abstract": "Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety\nand efficiency of industrial processes. We propose a novel FDD methodology for\nthe Tennessee Eastman Process (TEP), a widely used benchmark for chemical\nprocess control. The model employs two separate Transformer branches, enabling\nindependent processing of input data and potential extraction of diverse\ninformation. A novel attention mechanism, Gated Dynamic Learnable Attention\n(GDLAttention), is introduced which integrates a gating mechanism and dynamic\nlearning capabilities. The gating mechanism modulates the attention weights,\nallowing the model to focus on the most relevant parts of the input. The\ndynamic learning approach adapts the attention strategy during training,\npotentially leading to improved performance. The attention mechanism uses a\nbilinear similarity function, providing greater flexibility in capturing\ncomplex relationships between query and key vectors. In order to assess the\neffectiveness of our approach, we tested it against 21 and 18 distinct fault\nscenarios in TEP, and compared its performance with several established FDD\ntechniques. The outcomes indicate that the method outperforms others in terms\nof accuracy, false alarm rate, and misclassification rate. This underscores the\nrobustness and efficacy of the approach for FDD in intricate industrial\nprocesses.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10842v4",
    "published_date": "2024-03-16 07:40:23 UTC",
    "updated_date": "2024-11-25 17:22:53 UTC"
  },
  {
    "arxiv_id": "2405.15773v2",
    "title": "Feature Aggregation with Latent Generative Replay for Federated Continual Learning of Socially Appropriate Robot Behaviours",
    "authors": [
      "Nikhil Churamani",
      "Saksham Checker",
      "Fethiye Irmak Dogan",
      "Hao-Tien Lewis Chiang",
      "Hatice Gunes"
    ],
    "abstract": "It is critical for robots to explore Federated Learning (FL) settings where\nseveral robots, deployed in parallel, can learn independently while also\nsharing their learning with each other. This collaborative learning in\nreal-world environments requires social robots to adapt dynamically to changing\nand unpredictable situations and varying task settings. Our work contributes to\naddressing these challenges by exploring a simulated living room environment\nwhere robots need to learn the social appropriateness of their actions. First,\nwe propose Federated Root (FedRoot) averaging, a novel weight aggregation\nstrategy which disentangles feature learning across clients from individual\ntask-based learning. Second, to adapt to challenging environments, we extend\nFedRoot to Federated Latent Generative Replay (FedLGR), a novel Federated\nContinual Learning (FCL) strategy that uses FedRoot-based weight aggregation\nand embeds each client with a generator model for pseudo-rehearsal of learnt\nfeature embeddings to mitigate forgetting in a resource-efficient manner. Our\nresults show that FedRoot-based methods offer competitive performance while\nalso resulting in a sizeable reduction in resource consumption (up to 86% for\nCPU usage and up to 72% for GPU usage). Additionally, our results demonstrate\nthat FedRoot-based FCL methods outperform other methods while also offering an\nefficient solution (up to 84% CPU and 92% GPU usage reduction), with FedLGR\nproviding the best results across evaluations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 4 figures, IEEE RA-L submission",
    "pdf_url": "http://arxiv.org/pdf/2405.15773v2",
    "published_date": "2024-03-16 07:34:33 UTC",
    "updated_date": "2025-02-21 11:38:52 UTC"
  },
  {
    "arxiv_id": "2403.10834v1",
    "title": "SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation",
    "authors": [
      "Uiwon Hwang",
      "Jonghyun Lee",
      "Juhyeon Shin",
      "Sungroh Yoon"
    ],
    "abstract": "In the face of the deep learning model's vulnerability to domain shift,\nsource-free domain adaptation (SFDA) methods have been proposed to adapt models\nto new, unseen target domains without requiring access to source domain data.\nAlthough the potential benefits of applying data augmentation to SFDA are\nattractive, several challenges arise such as the dependence on prior knowledge\nof class-preserving transformations and the increase in memory and\ncomputational requirements. In this paper, we propose Source-free Domain\nAdaptation Through the Lens of Data Augmentation (SF(DA)$^2$), a novel approach\nthat leverages the benefits of data augmentation without suffering from these\nchallenges. We construct an augmentation graph in the feature space of the\npretrained model using the neighbor relationships between target features and\npropose spectral neighborhood clustering to identify partitions in the\nprediction space. Furthermore, we propose implicit feature augmentation and\nfeature disentanglement as regularization loss functions that effectively\nutilize class semantic information within the feature space. These regularizers\nsimulate the inclusion of an unlimited number of augmented target features into\nthe augmentation graph while minimizing computational and memory demands. Our\nmethod shows superior adaptation performance in SFDA scenarios, including 2D\nimage and 3D point cloud datasets and a highly imbalanced dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2024. Code: https://github.com/shinyflight/SFDA2",
    "pdf_url": "http://arxiv.org/pdf/2403.10834v1",
    "published_date": "2024-03-16 07:05:47 UTC",
    "updated_date": "2024-03-16 07:05:47 UTC"
  },
  {
    "arxiv_id": "2403.10824v1",
    "title": "LookALike: Human Mimicry based collaborative decision making",
    "authors": [
      "Rabimba Karanjai",
      "Weidong Shi"
    ],
    "abstract": "Artificial General Intelligence falls short when communicating role specific\nnuances to other systems. This is more pronounced when building autonomous LLM\nagents capable and designed to communicate with each other for real world\nproblem solving. Humans can communicate context and domain specific nuances\nalong with knowledge, and that has led to refinement of skills. In this work we\npropose and evaluate a novel method that leads to knowledge distillation among\nLLM agents leading to realtime human role play preserving unique contexts\nwithout relying on any stored data or pretraining. We also evaluate how our\nsystem performs better in simulated real world tasks compared to state of the\nart.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10824v1",
    "published_date": "2024-03-16 06:25:53 UTC",
    "updated_date": "2024-03-16 06:25:53 UTC"
  },
  {
    "arxiv_id": "2403.10823v1",
    "title": "VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model for Generalizable Retina Image Analysis",
    "authors": [
      "Hao Wei",
      "Bowen Liu",
      "Minqing Zhang",
      "Peilun Shi",
      "Wu Yuan"
    ],
    "abstract": "Generalist foundation model has ushered in newfound capabilities in medical\ndomain. However, the contradiction between the growing demand for high-quality\nannotated data with patient privacy continues to intensify. The utilization of\nmedical artificial intelligence generated content (Med-AIGC) as an\ninexhaustible resource repository arises as a potential solution to address the\naforementioned challenge. Here we harness 1 million open-source synthetic\nfundus images paired with natural language descriptions, to curate an ethical\nlanguage-image foundation model for retina image analysis named VisionCLIP.\nVisionCLIP achieves competitive performance on three external datasets compared\nwith the existing method pre-trained on real-world data in a zero-shot fashion.\nThe employment of artificially synthetic images alongside corresponding textual\ndata for training enables the medical foundation model to successfully\nassimilate knowledge of disease symptomatology, thereby circumventing potential\nbreaches of patient confidentiality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10823v1",
    "published_date": "2024-03-16 06:21:19 UTC",
    "updated_date": "2024-03-16 06:21:19 UTC"
  },
  {
    "arxiv_id": "2403.10819v1",
    "title": "Incentivized Exploration of Non-Stationary Stochastic Bandits",
    "authors": [
      "Sourav Chakraborty",
      "Lijun Chen"
    ],
    "abstract": "We study incentivized exploration for the multi-armed bandit (MAB) problem\nwith non-stationary reward distributions, where players receive compensation\nfor exploring arms other than the greedy choice and may provide biased feedback\non the reward. We consider two different non-stationary environments:\nabruptly-changing and continuously-changing, and propose respective\nincentivized exploration algorithms. We show that the proposed algorithms\nachieve sublinear regret and compensation over time, thus effectively\nincentivizing exploration despite the nonstationarity and the biased or drifted\nfeedback.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10819v1",
    "published_date": "2024-03-16 06:06:44 UTC",
    "updated_date": "2024-03-16 06:06:44 UTC"
  },
  {
    "arxiv_id": "2403.10805v1",
    "title": "Speech-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference",
    "authors": [
      "Fan Zhang",
      "Zhaohan Wang",
      "Xin Lyu",
      "Siyuan Zhao",
      "Mengjian Li",
      "Weidong Geng",
      "Naye Ji",
      "Hui Du",
      "Fuxing Gao",
      "Hao Wu",
      "Shunman Li"
    ],
    "abstract": "Speech-driven gesture generation is an emerging field within virtual human\ncreation. However, a significant challenge lies in accurately determining and\nprocessing the multitude of input features (such as acoustic, semantic,\nemotional, personality, and even subtle unknown features). Traditional\napproaches, reliant on various explicit feature inputs and complex multimodal\nprocessing, constrain the expressiveness of resulting gestures and limit their\napplicability. To address these challenges, we present Persona-Gestor, a novel\nend-to-end generative model designed to generate highly personalized 3D\nfull-body gestures solely relying on raw speech audio. The model combines a\nfuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization\n(AdaLN) transformer diffusion architecture. The fuzzy feature extractor\nharnesses a fuzzy inference strategy that automatically infers implicit,\ncontinuous fuzzy features. These fuzzy features, represented as a unified\nlatent feature, are fed into the AdaLN transformer. The AdaLN transformer\nintroduces a conditional mechanism that applies a uniform function across all\ntokens, thereby effectively modeling the correlation between the fuzzy features\nand the gesture sequence. This module ensures a high level of gesture-speech\nsynchronization while preserving naturalness. Finally, we employ the diffusion\nmodel to train and infer various gestures. Extensive subjective and objective\nevaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's\nsuperior performance to the current state-of-the-art approaches. Persona-Gestor\nimproves the system's usability and generalization capabilities, setting a new\nbenchmark in speech-driven gesture synthesis and broadening the horizon for\nvirtual human technology. Supplementary videos and code can be accessed at\nhttps://zf223669.github.io/Diffmotion-v2-website/",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "cs.HC",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "12 pages,",
    "pdf_url": "http://arxiv.org/pdf/2403.10805v1",
    "published_date": "2024-03-16 04:40:10 UTC",
    "updated_date": "2024-03-16 04:40:10 UTC"
  },
  {
    "arxiv_id": "2403.10803v1",
    "title": "Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion",
    "authors": [
      "Jiawei Li",
      "Sitong Li",
      "Shanshan Wang",
      "Yicheng Zeng",
      "Falong Tan",
      "Chuanlong Xie"
    ],
    "abstract": "Deploying machine learning in open environments presents the challenge of\nencountering diverse test inputs that differ significantly from the training\ndata. These out-of-distribution samples may exhibit shifts in local or global\nfeatures compared to the training distribution. The machine learning (ML)\ncommunity has responded with a number of methods aimed at distinguishing\nanomalous inputs from original training data. However, the majority of previous\nstudies have primarily focused on the output layer or penultimate layer of\npre-trained deep neural networks. In this paper, we propose a novel framework,\nMultitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), to\nidentify distributional shifts in test samples at different levels of features\nthrough rigorous multiple testing procedure. Our approach distinguishes itself\nfrom existing methods as it does not require modifying the structure or\nfine-tuning of the pre-trained classifier. Through extensive experiments, we\ndemonstrate that our proposed framework can seamlessly integrate with any\nexisting distance-based inspection method while efficiently utilizing feature\nextractors of varying depths. Our scheme effectively enhances the performance\nof out-of-distribution detection when compared to baseline methods. In\nparticular, MLOD-Fisher achieves superior performance in general. When trained\nusing KNN on CIFAR10, MLOD-Fisher significantly lowers the false positive rate\n(FPR) from 24.09% to 7.47% on average compared to merely utilizing the features\nof the last layer.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10803v1",
    "published_date": "2024-03-16 04:35:04 UTC",
    "updated_date": "2024-03-16 04:35:04 UTC"
  },
  {
    "arxiv_id": "2403.10799v5",
    "title": "Toward Adaptive Large Language Models Structured Pruning via Hybrid-grained Weight Importance Assessment",
    "authors": [
      "Jun Liu",
      "Zhenglun Kong",
      "Pu Zhao",
      "Changdi Yang",
      "Hao Tang",
      "Xuan Shen",
      "Geng Yuan",
      "Wei Niu",
      "Wenbin Zhang",
      "Xue Lin",
      "Dong Huang",
      "Yanzhi Wang"
    ],
    "abstract": "Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82% in accuracy\nacross seven downstream tasks when pruning LLaMA-7B by 50%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.10799v5",
    "published_date": "2024-03-16 04:12:50 UTC",
    "updated_date": "2025-01-12 06:47:39 UTC"
  },
  {
    "arxiv_id": "2403.10795v2",
    "title": "Can Large Language Models Solve Robot Routing?",
    "authors": [
      "Zhehui Huang",
      "Guangyao Shi",
      "Gaurav S. Sukhatme"
    ],
    "abstract": "Routing problems are common in mobile robotics, encompassing tasks such as\ninspection, surveillance, and coverage. Depending on the objective and\nconstraints, these problems often reduce to variants of the Traveling Salesman\nProblem (TSP), with solutions traditionally derived by translating high-level\nobjectives into an optimization formulation and using modern solvers to arrive\nat a solution. Here, we explore the potential of Large Language Models (LLMs)\nto replace the entire pipeline from tasks described in natural language to the\ngeneration of robot routes. We systematically investigate the performance of\nLLMs in robot routing by constructing a dataset with 80 unique robot routing\nproblems across 8 variants in both single and multi-robot settings. We evaluate\nLLMs through three frameworks: single attempt, self-debugging, and\nself-debugging with self-verification and various contexts, including\nmathematical formulations, pseudo-code, and related research papers. Our\nfindings reveal that both self-debugging and self-verification enhance success\nrates without significantly lowering the optimality gap. We observe\ncontext-sensitive behavior - providing mathematical formulations as context\ndecreases the optimality gap but significantly decreases success rates and\nproviding pseudo-code and related research papers as context does not\nconsistently improve success rates or decrease the optimality gap. We identify\nkey challenges and propose future directions to enhance LLM performance in\nsolving robot routing problems. Our source code is available on the project\nwebsite: https://sites.google.com/view/words-to-routes/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to International Symposium of Robotics Research (ISRR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.10795v2",
    "published_date": "2024-03-16 03:54:38 UTC",
    "updated_date": "2024-08-06 21:14:23 UTC"
  },
  {
    "arxiv_id": "2403.10787v1",
    "title": "Time Series Representation Learning with Supervised Contrastive Temporal Transformer",
    "authors": [
      "Yuansan Liu",
      "Sudanthi Wijewickrema",
      "Christofer Bester",
      "Stephen O'Leary",
      "James Bailey"
    ],
    "abstract": "Finding effective representations for time series data is a useful but\nchallenging task. Several works utilize self-supervised or unsupervised\nlearning methods to address this. However, there still remains the open\nquestion of how to leverage available label information for better\nrepresentations. To answer this question, we exploit pre-existing techniques in\ntime series and representation learning domains and develop a simple, yet novel\nfusion model, called: \\textbf{S}upervised \\textbf{CO}ntrastive\n\\textbf{T}emporal \\textbf{T}ransformer (SCOTT). We first investigate suitable\naugmentation methods for various types of time series data to assist with\nlearning change-invariant representations. Secondly, we combine Transformer and\nTemporal Convolutional Networks in a simple way to efficiently learn both\nglobal and local features. Finally, we simplify Supervised Contrastive Loss for\nrepresentation learning of labelled time series data. We preliminarily evaluate\nSCOTT on a downstream task, Time Series Classification, using 45 datasets from\nthe UCR archive. The results show that with the representations learnt by\nSCOTT, even a weak classifier can perform similar to or better than existing\nstate-of-the-art models (best performance on 23/45 datasets and highest rank\nagainst 9 baseline models). Afterwards, we investigate SCOTT's ability to\naddress a real-world task, online Change Point Detection (CPD), on two\ndatasets: a human activity dataset and a surgical patient dataset. We show that\nthe model performs with high reliability and efficiency on the online CPD\nproblem ($\\sim$98\\% and $\\sim$97\\% area under precision-recall curve\nrespectively). Furthermore, we demonstrate the model's potential in tackling\nearly detection and show it performs best compared to other candidates.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 8 figures, IJCNN 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.10787v1",
    "published_date": "2024-03-16 03:37:19 UTC",
    "updated_date": "2024-03-16 03:37:19 UTC"
  },
  {
    "arxiv_id": "2405.01553v2",
    "title": "Empirical Studies of Parameter Efficient Methods for Large Language Models of Code and Knowledge Transfer to R",
    "authors": [
      "Amirreza Esmaeili",
      "Iman Saberi",
      "Fatemeh H. Fard"
    ],
    "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods are proposed as an alternative\nfine-tuning approach for Large Language Models (LLM) to minimize high training\ncosts. While prior research demonstrates the effectiveness of PEFT methods in\nknowledge transfer using smaller language models, their application to larger\nLLMs, particularly in low-resource and unseen programming languages such as R,\nremains under-explored. In this work, we evaluate PEFT methods, LoRA,\nCompacter, and IA^3 on LLMs for code summarization and generation, with a\nparticular emphasis on knowledge transfer to R as an unseen under-explored\ntarget language. Our experiments reveal that LoRA consistently outperforms\nCompacter and IA^3 in all settings, while Compacter offers significant resource\nefficiency with minimal performance trade-offs. Additionally, we find that the\nnumber of trainable parameters has a greater influence on the functional\naccuracy of the generated code than PEFT architecture. Our study can direct\nfuture research in developing code intelligent tasks for unseen languages\nincluding R, as well as the choice of PEFT methods for knowledge transfer,\nespecially when balancing the computational cost and performance.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01553v2",
    "published_date": "2024-03-16 03:12:45 UTC",
    "updated_date": "2025-01-27 18:51:36 UTC"
  },
  {
    "arxiv_id": "2403.10781v1",
    "title": "Exploring Chinese Humor Generation: A Study on Two-Part Allegorical Sayings",
    "authors": [
      "Rongwu Xu"
    ],
    "abstract": "Humor, a culturally nuanced aspect of human language, poses challenges for\ncomputational understanding and generation, especially in Chinese humor, which\nremains relatively unexplored in the NLP community. This paper investigates the\ncapability of state-of-the-art language models to comprehend and generate\nChinese humor, specifically focusing on training them to create allegorical\nsayings. We employ two prominent training methods: fine-tuning a medium-sized\nlanguage model and prompting a large one. Our novel fine-tuning approach\nincorporates fused Pinyin embeddings to consider homophones and employs\ncontrastive learning with synthetic hard negatives to distinguish humor\nelements. Human-annotated results show that these models can generate humorous\nallegorical sayings, with prompting proving to be a practical and effective\nmethod. However, there is still room for improvement in generating allegorical\nsayings that match human creativity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10781v1",
    "published_date": "2024-03-16 02:58:57 UTC",
    "updated_date": "2024-03-16 02:58:57 UTC"
  },
  {
    "arxiv_id": "2403.10780v1",
    "title": "Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy for Multi-Class Multi-Instance Segmentation",
    "authors": [
      "Mariia Khan",
      "Yue Qiu",
      "Yuren Cong",
      "Jumana Abu-Khalaf",
      "David Suter",
      "Bodo Rosenhahn"
    ],
    "abstract": "Multi-class multi-instance segmentation is the task of identifying masks for\nmultiple object classes and multiple instances of the same class within an\nimage. The foundational Segment Anything Model (SAM) is designed for promptable\nmulti-class multi-instance segmentation but tends to output part or sub-part\nmasks in the \"everything\" mode for various real-world applications. Whole\nobject segmentation masks play a crucial role for indoor scene understanding,\nespecially in robotics applications. We propose a new domain invariant\nReal-to-Simulation (Real-Sim) fine-tuning strategy for SAM. We use object\nimages and ground truth data collected from Ai2Thor simulator during\nfine-tuning (real-to-sim). To allow our Segment Any Object Model (SAOM) to work\nin the \"everything\" mode, we propose the novel nearest neighbour assignment\nmethod, updating point embeddings for each ground-truth mask. SAOM is evaluated\non our own dataset collected from Ai2Thor simulator. SAOM significantly\nimproves on SAM, with a 28% increase in mIoU and a 25% increase in mAcc for 54\nfrequently-seen indoor object classes. Moreover, our Real-to-Simulation\nfine-tuning strategy demonstrates promising generalization performance in real\nenvironments without being trained on the real-world data (sim-to-real). The\ndataset and the code will be released after publication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10780v1",
    "published_date": "2024-03-16 02:54:49 UTC",
    "updated_date": "2024-03-16 02:54:49 UTC"
  },
  {
    "arxiv_id": "2403.10776v1",
    "title": "From Melting Pots to Misrepresentations: Exploring Harms in Generative AI",
    "authors": [
      "Sanjana Gautam",
      "Pranav Narayanan Venkit",
      "Sourojit Ghosh"
    ],
    "abstract": "With the widespread adoption of advanced generative models such as Gemini and\nGPT, there has been a notable increase in the incorporation of such models into\nsociotechnical systems, categorized under AI-as-a-Service (AIaaS). Despite\ntheir versatility across diverse sectors, concerns persist regarding\ndiscriminatory tendencies within these models, particularly favoring selected\n`majority' demographics across various sociodemographic dimensions. Despite\nwidespread calls for diversification of media representations, marginalized\nracial and ethnic groups continue to face persistent distortion, stereotyping,\nand neglect within the AIaaS context. In this work, we provide a critical\nsummary of the state of research in the context of social harms to lead the\nconversation to focus on their implications. We also present open-ended\nresearch questions, guided by our discussion, to help define future research\npathways.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "In CHI 2024: Generative AI and HCI workshop (GenAICHI 24)",
    "pdf_url": "http://arxiv.org/pdf/2403.10776v1",
    "published_date": "2024-03-16 02:29:42 UTC",
    "updated_date": "2024-03-16 02:29:42 UTC"
  },
  {
    "arxiv_id": "2403.10764v1",
    "title": "ECRC: Emotion-Causality Recognition in Korean Conversation for GCN",
    "authors": [
      "J. K. Lee",
      "T. M. Chung"
    ],
    "abstract": "In this multi-task learning study on simultaneous analysis of emotions and\ntheir underlying causes in conversational contexts, deep neural network methods\nwere employed to effectively process and train large labeled datasets. However,\nthese approaches are typically limited to conducting context analyses across\nthe entire corpus because they rely on one of the two methods: word- or\nsentence-level embedding. The former struggles with polysemy and homonyms,\nwhereas the latter causes information loss when processing long sentences. In\nthis study, we overcome the limitations of previous embeddings by utilizing\nboth word- and sentence-level embeddings. Furthermore, we propose the\nemotion-causality recognition in conversation (ECRC) model, which is based on a\nnovel graph structure, thereby leveraging the strengths of both embedding\nmethods. This model uniquely integrates the bidirectional long short-term\nmemory (Bi-LSTM) and graph neural network (GCN) models for Korean conversation\nanalysis. Compared with models that rely solely on one embedding method, the\nproposed model effectively structures abstract concepts, such as language\nfeatures and relationships, thereby minimizing information loss. To assess\nmodel performance, we compared the multi-task learning results of three deep\nneural network models with varying graph structures. Additionally, we evaluated\nthe proposed model using Korean and English datasets. The experimental results\nshow that the proposed model performs better in emotion and causality\nmulti-task learning (74.62% and 75.30%, respectively) when node and edge\ncharacteristics are incorporated into the graph structure. Similar results were\nrecorded for the Korean ECC and Wellness datasets (74.62% and 73.44%,\nrespectively) with 71.35% on the IEMOCAP English dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.10764v1",
    "published_date": "2024-03-16 02:07:31 UTC",
    "updated_date": "2024-03-16 02:07:31 UTC"
  },
  {
    "arxiv_id": "2403.10761v1",
    "title": "Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning",
    "authors": [
      "Jizhe Dou",
      "Haotian Zhang",
      "Guodong Sun"
    ],
    "abstract": "Recently there has been a growing interest in industry and academia,\nregarding the use of wireless chargers to prolong the operational longevity of\nunmanned aerial vehicles (commonly knowns as drones). In this paper we consider\na charger-assisted drone application: a drone is deployed to observe a set\npoints of interest, while a charger can move to recharge the drone's battery.\nWe focus on the route and charging schedule of the drone and the mobile\ncharger, to obtain high observation utility with the shortest possible time,\nwhile ensuring the drone remains operational during task execution.\nEssentially, this proposed drone-charger scheduling problem is a multi-stage\ndecision-making process, in which the drone and the mobile charger act as two\nagents who cooperate to finish a task. The discrete-continuous hybrid action\nspace of the two agents poses a significant challenge in our problem. To\naddress this issue, we present a hybrid-action deep reinforcement learning\nframework, called HaDMC, which uses a standard policy learning algorithm to\ngenerate latent continuous actions. Motivated by representation learning, we\nspecifically design and train an action decoder. It involves two pipelines to\nconvert the latent continuous actions into original discrete and continuous\nactions, by which the drone and the charger can directly interact with\nenvironment. We embed a mutual learning scheme in model training, emphasizing\nthe collaborative rather than individual actions. We conduct extensive\nnumerical experiments to evaluate HaDMC and compare it with state-of-the-art\ndeep reinforcement learning approaches. The experimental results show the\neffectiveness and efficiency of our solution.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10761v1",
    "published_date": "2024-03-16 01:51:42 UTC",
    "updated_date": "2024-03-16 01:51:42 UTC"
  },
  {
    "arxiv_id": "2403.10760v1",
    "title": "CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects",
    "authors": [
      "Yoonyoung Cho",
      "Junhyek Han",
      "Yoontae Cho",
      "Beomjoon Kim"
    ],
    "abstract": "Nonprehensile manipulation is essential for manipulating objects that are too\nthin, large, or otherwise ungraspable in the wild. To sidestep the difficulty\nof contact modeling in conventional modeling-based approaches, reinforcement\nlearning (RL) has recently emerged as a promising alternative. However,\nprevious RL approaches either lack the ability to generalize over diverse\nobject shapes, or use simple action primitives that limit the diversity of\nrobot motions. Furthermore, using RL over diverse object geometry is\nchallenging due to the high cost of training a policy that takes in\nhigh-dimensional sensory inputs. We propose a novel contact-based object\nrepresentation and pretraining pipeline to tackle this. To enable massively\nparallel training, we leverage a lightweight patch-based transformer\narchitecture for our encoder that processes point clouds, thus scaling our\ntraining across thousands of environments. Compared to learning from scratch,\nor other shape representation baselines, our representation facilitates both\ntime- and data-efficient learning. We validate the efficacy of our overall\nsystem by zero-shot transferring the trained policy to novel real-world\nobjects. Code and videos are available at\nhttps://sites.google.com/view/contact-non-prehensile.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.10760v1",
    "published_date": "2024-03-16 01:47:53 UTC",
    "updated_date": "2024-03-16 01:47:53 UTC"
  },
  {
    "arxiv_id": "2403.12098v1",
    "title": "Deep Generative Design for Mass Production",
    "authors": [
      "Jihoon Kim",
      "Yongmin Kwon",
      "Namwoo Kang"
    ],
    "abstract": "Generative Design (GD) has evolved as a transformative design approach,\nemploying advanced algorithms and AI to create diverse and innovative solutions\nbeyond traditional constraints. Despite its success, GD faces significant\nchallenges regarding the manufacturability of complex designs, often\nnecessitating extensive manual modifications due to limitations in standard\nmanufacturing processes and the reliance on additive manufacturing, which is\nnot ideal for mass production. Our research introduces an innovative framework\naddressing these manufacturability concerns by integrating constraints\npertinent to die casting and injection molding into GD, through the utilization\nof 2D depth images. This method simplifies intricate 3D geometries into\nmanufacturable profiles, removing unfeasible features such as\nnon-manufacturable overhangs and allowing for the direct consideration of\nessential manufacturing aspects like thickness and rib design. Consequently,\ndesigns previously unsuitable for mass production are transformed into viable\nsolutions. We further enhance this approach by adopting an advanced 2D\ngenerative model, which offer a more efficient alternative to traditional 3D\nshape generation methods. Our results substantiate the efficacy of this\nframework, demonstrating the production of innovative, and, importantly,\nmanufacturable designs. This shift towards integrating practical manufacturing\nconsiderations into GD represents a pivotal advancement, transitioning from\npurely inspirational concepts to actionable, production-ready solutions. Our\nfindings underscore usefulness and potential of GD for broader industry\nadoption, marking a significant step forward in aligning GD with the demands of\nmanufacturing challenges.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12098v1",
    "published_date": "2024-03-16 01:32:00 UTC",
    "updated_date": "2024-03-16 01:32:00 UTC"
  },
  {
    "arxiv_id": "2403.10751v3",
    "title": "LightCode: Light Analytical and Neural Codes for Channels with Feedback",
    "authors": [
      "Sravan Kumar Ankireddy",
      "Krishna Narayanan",
      "Hyeji Kim"
    ],
    "abstract": "The design of reliable and efficient codes for channels with feedback remains\na longstanding challenge in communication theory. While significant\nimprovements have been achieved by leveraging deep learning techniques, neural\ncodes often suffer from high computational costs, a lack of interpretability,\nand limited practicality in resource-constrained settings. We focus on\ndesigning low-complexity coding schemes that are interpretable and more\nsuitable for communication systems. We advance both analytical and neural\ncodes. First, we demonstrate that PowerBlast, an analytical coding scheme\ninspired by Schalkwijk-Kailath (SK) and Gallager-Nakibo\\u{g}lu (GN) schemes,\nachieves notable reliability improvements over both SK and GN schemes,\noutperforming neural codes in high signal-to-noise ratio (SNR) regions. Next,\nto enhance reliability in low-SNR regions, we propose LightCode, a lightweight\nneural code that achieves state-of-the-art reliability while using a fraction\nof memory and compute compared to existing deeplearning-based codes. Finally,\nwe systematically analyze the learned codes, establishing connections between\nLightCode and PowerBlast, identifying components crucial for performance, and\nproviding interpretation aided by linear regression analysis.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "16 pages, 12 figures, To appear in IEEE Journal on Selected Areas in\n  Communications, 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.10751v3",
    "published_date": "2024-03-16 01:04:34 UTC",
    "updated_date": "2024-11-16 19:55:18 UTC"
  },
  {
    "arxiv_id": "2403.10750v1",
    "title": "Depression Detection on Social Media with Large Language Models",
    "authors": [
      "Xiaochong Lan",
      "Yiming Cheng",
      "Li Sheng",
      "Chen Gao",
      "Yong Li"
    ],
    "abstract": "Depression harms. However, due to a lack of mental health awareness and fear\nof stigma, many patients do not actively seek diagnosis and treatment, leading\nto detrimental outcomes. Depression detection aims to determine whether an\nindividual suffers from depression by analyzing their history of posts on\nsocial media, which can significantly aid in early detection and intervention.\nIt mainly faces two key challenges: 1) it requires professional medical\nknowledge, and 2) it necessitates both high accuracy and explainability. To\naddress it, we propose a novel depression detection system called DORIS,\ncombining medical knowledge and the recent advances in large language models\n(LLMs). Specifically, to tackle the first challenge, we proposed an LLM-based\nsolution to first annotate whether high-risk texts meet medical diagnostic\ncriteria. Further, we retrieve texts with high emotional intensity and\nsummarize critical information from the historical mood records of users,\nso-called mood courses. To tackle the second challenge, we combine LLM and\ntraditional classifiers to integrate medical knowledge-guided features, for\nwhich the model can also explain its prediction results, achieving both high\naccuracy and explainability. Extensive experimental results on benchmarking\ndatasets show that, compared to the current best baseline, our approach\nimproves by 0.036 in AUPRC, which can be considered significant, demonstrating\nthe effectiveness of our approach and its high value as an NLP application.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.10750v1",
    "published_date": "2024-03-16 01:01:16 UTC",
    "updated_date": "2024-03-16 01:01:16 UTC"
  },
  {
    "arxiv_id": "2403.10744v1",
    "title": "Game and Reference: Policy Combination Synthesis for Epidemic Prevention and Control",
    "authors": [
      "Zhiyi Tan",
      "Bingkun Bao"
    ],
    "abstract": "In recent years, epidemic policy-making models are increasingly being used to\nprovide reference for governors on prevention and control policies against\ncatastrophic epidemics such as SARS, H1N1 and COVID-19. Existing studies are\ncurrently constrained by two issues: First, previous methods develop policies\nbased on effect evaluation, since few of factors in real-world decision-making\ncan be modeled, the output policies will then easily become extreme. Second,\nthe subjectivity and cognitive limitation of human make the historical policies\nnot always optimal for the training of decision models. To these ends, we\npresent a novel Policy Combination Synthesis (PCS) model for epidemic\npolicy-making. Specially, to prevent extreme decisions, we introduce\nadversarial learning between the model-made policies and the real policies to\nforce the output policies to be more human-liked. On the other hand, to\nminimize the impact of sub-optimal historical policies, we employ contrastive\nlearning to let the model draw on experience from the best historical policies\nunder similar scenarios. Both adversarial and contrastive learning are adaptive\nbased on the comprehensive effects of real policies to ensure the model always\nlearns useful information. Extensive experiments on real-world data prove the\neffectiveness of the proposed model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, single line, 7 figures, written with Springer conference\n  template",
    "pdf_url": "http://arxiv.org/pdf/2403.10744v1",
    "published_date": "2024-03-16 00:26:59 UTC",
    "updated_date": "2024-03-16 00:26:59 UTC"
  }
]
{
  "date": "2024-06-23",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-06-23 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型的优化、安全性、多模态应用和跨领域扩展，其中 LLM 在跨语言毒性缓解和事件提取方面的研究最令人印象深刻，如 Stephen H. Bach 等学者的作品，以及 NeurIPS 和 ICML 等顶级会议论文；同时，Jiawei Han 等著名学者在需求预测和图神经网络方面的贡献也值得关注。\n\n### 重点论文讨论\n我们挑选了最具话题性和影响力的论文优先讨论，按主题分组，其他次要论文则简要掠过。以下聚焦于 AI 安全、机器学习优化和实际应用领域。\n\n**1. LLM 和 AI 安全：这些论文探讨了大型语言模型的解释性、安全性和跨语言应用，相关工作可能推动 AI 伦理发展。**\n- **Unveiling LLM Mechanisms Through Neural ODEs and Control Theory（揭示 LLM 机制通过神经 ODE 和控制理论）**  \n  作者：Yukun Zhang, Qi Dong。  \n  主要贡献：提出结合 Neural Ordinary Differential Equations (Neural ODEs) 和控制理论的框架，提升 LLM 的可解释性和输出质量。通过实验证明，该方法在问答数据集上改善了输出一致性和可解释性，为可解释 AI 技术发展提供新路径。\n\n- **Preference Tuning For Toxicity Mitigation Generalizes Across Languages（偏好调整用于毒性缓解的跨语言泛化）**  \n  作者：Xiaochen Li, Zheng-Xin Yong, Stephen H. Bach（Stephen H. Bach 等知名学者参与）。  \n  主要贡献：使用 Direct Preference Optimization (DPO) 仅通过英语数据训练，即可显著降低多语言 LLM 的毒性输出（如 mGPT-1.3B 在 17 种语言的毒性概率从 46.8% 降至 3.9%）。通过机制解释（如因果干预），揭示了 MLP 层的双语性，支持跨语言泛化，EMNLP 2024 发现。\n\n- **INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness（INDICT：通过内部批判对话生成安全且有帮助的代码）**  \n  作者：Hung Le 等。  \n  主要贡献：设计一个双重批判系统（安全性和帮助性），结合外部工具（如网络搜索），在代码生成和执行阶段提供指导。实验显示，在 8 个任务和 7 个模型上，INDICT 提升代码质量 10%，NeurIPS 2024 接受。\n\n**2. 机器学习优化和事件提取：这些论文强调模型鲁棒性和跨文档处理，可能影响实际应用如推荐系统和数据分析。**\n- **Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method（渐进发散：一种新型领域增量学习方法）**  \n  作者：Kishaan Jeeveswaran 等。  \n  主要贡献：提出 DARE 方法，通过三阶段训练（发散、适应、精炼）减少表示漂移，缓解灾难性遗忘。在 DIL 基准上提升性能，ICML 2024 接受。\n\n- **F-FOMAML: GNN-Enhanced Meta-Learning for Peak Period Demand Forecasting with Proxy Data（F-FOMAML：使用 GNN 增强的元学习，用于峰期需求预测）**  \n  作者：Zexing Xu, Linjun Zhang, Sitan Yang, Rasoul Etesami, Hanghang Tong, Huan Zhang, Jiawei Han（Jiawei Han 等知名学者）。  \n  主要贡献：利用图神经网络 (GNNs) 和代理数据进行元学习，预测电商峰期需求。理论证明了泛化改进，实证在数据集上降低 Mean Absolute Error 26.24%，为需求预测提供高效框架。\n\n- **Harvesting Events from Multiple Sources: Towards a Cross-Document Event Extraction Paradigm（从多个来源提取事件：迈向跨文档事件提取范式）**  \n  作者：Qiang Gao 等。  \n  主要贡献：构建跨文档事件提取数据集 CLES，并提出管道方法结合 Rhetorical Structure Theory (RST) 和 Lexical Chains，提升事件聚类准确性。在中英数据集上显著超越基线。\n\n**3. 医疗和应用领域：这些论文将 AI 应用于实际场景，如医疗诊断和推荐系统，展示了 AI 的社会价值。**\n- **TimeAutoDiff: Combining Autoencoder and Diffusion model for time series tabular data synthesizing（TimeAutoDiff：结合自编码器和扩散模型的时序表格数据合成）**  \n  作者：Namjoon Suh 等。  \n  主要贡献：提出 TimeAutoDiff 框架，处理时序表格数据的异质性，通过 VAE 和 DDPM 合成数据，提升保真度和实用性，在多个数据集上超越 SOTA。\n\n- **GraphEval36K: Benchmarking Coding and Reasoning Capabilities of Large Language Models on Graph Datasets（GraphEval36K：评估 LLM 在图数据集上的编码和推理能力）**  \n  作者：Qiming Wu 等。  \n  主要贡献：构建 36,900 测试案例的图数据集，评估 LLM 在图问题上的性能。提出 Structured Symbolic Decomposition (SSD) 方法，提升 GPT-4 等模型的通过率 8-29%，NAACL 2025 接受。\n\n其他论文，如量子计算（论文 45）、推荐系统优化（论文 18）和交通AI（论文 23、25、26）等，虽然有技术创新，但影响力较小，仅快速提及：例如，论文 18 的 SimCE 简化了交叉熵损失，提升推荐系统性能；论文 45 在 IBM 量子计算机上实现了三比特 Grover 搜索，验证了算法的可扩展性；论文 23 和 25 提出车队跟随框架，但因实验问题已撤回，需谨慎参考。\n\n总之，今天的 arXiv 论文展示了 AI 领域的多样创新，LLM 的安全和泛化研究尤为突出，建议读者关注 Stephen H. Bach 和 Jiawei Han 等学者的作品，以探索实际应用潜力。更多细节可查阅 arXiv。",
  "papers": [
    {
      "arxiv_id": "2406.16985v2",
      "title": "Unveiling LLM Mechanisms Through Neural ODEs and Control Theory",
      "title_zh": "翻译失败",
      "authors": [
        "Yukun Zhang",
        "Qi Dong"
      ],
      "abstract": "This paper proposes a framework combining Neural Ordinary Differential\nEquations (Neural ODEs) and robust control theory to enhance the\ninterpretability and control of large language models (LLMs). By utilizing\nNeural ODEs to model the dynamic evolution of input-output relationships and\nintroducing control mechanisms to optimize output quality, we demonstrate the\neffectiveness of this approach across multiple question-answer datasets.\nExperimental results show that the integration of Neural ODEs and control\ntheory significantly improves output consistency and model interpretability,\nadvancing the development of explainable AI technologies.",
      "tldr_zh": "这篇论文提出了一种结合 Neural Ordinary Differential Equations (Neural ODEs) 和 robust control theory 的框架，以提升大型语言模型 (LLMs) 的可解释性和控制能力。通过 Neural ODEs 建模输入输出关系的动态演化，并引入控制机制优化输出质量，该方法在多个问答数据集上进行了验证。实验结果显示，这种整合显著提高了输出一致性和模型可解释性，推动了可解释 AI 技术的进步。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16985v2",
      "published_date": "2024-06-23 22:56:34 UTC",
      "updated_date": "2025-02-23 06:03:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:41:01.574475"
    },
    {
      "arxiv_id": "2406.16235v2",
      "title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
      "title_zh": "偏好微调在毒性缓解方面跨语言泛化",
      "authors": [
        "Xiaochen Li",
        "Zheng-Xin Yong",
        "Stephen H. Bach"
      ],
      "abstract": "Detoxifying multilingual Large Language Models (LLMs) has become crucial due\nto their increasing global use. In this work, we explore zero-shot\ncross-lingual generalization of preference tuning in detoxifying LLMs. Unlike\nprevious studies that show limited cross-lingual generalization for other\nsafety tasks, we demonstrate that Direct Preference Optimization (DPO) training\nwith only English data can significantly reduce toxicity in multilingual\nopen-ended generations. For example, the probability of mGPT-1.3B generating\ntoxic continuations drops from 46.8% to 3.9% across 17 different languages\nafter training. Our results also extend to other multilingual LLMs, such as\nBLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal\nintervention and activation analysis, we identified the dual multilinguality\nproperty of MLP layers in LLMs, which explains the cross-lingual generalization\nof DPO. Finally, we show that bilingual sentence retrieval can predict the\ncross-lingual transferability of DPO preference tuning.",
      "tldr_zh": "本文研究了通过Direct Preference Optimization (DPO)训练来缓解多语言Large Language Models (LLMs)的毒性问题，仅使用英语数据即可实现零样本跨语言泛化。实验结果显示，DPO训练使mGPT-1.3B在17种语言中的毒性生成概率从46.8%降至3.9%，并扩展到其他模型如BLOOM、Llama3和Aya-23。利用因果干预和激活分析，论文揭示了LLMs中MLP层的双重多语言特性解释了这种泛化，并证明双语句子检索能预测DPO的跨语言转移性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Findings of EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.16235v2",
      "published_date": "2024-06-23 22:53:47 UTC",
      "updated_date": "2024-11-08 02:17:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:41:15.837338"
    },
    {
      "arxiv_id": "2406.16232v3",
      "title": "Jacobian Descent for Multi-Objective Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Pierre Quinton",
        "Valérian Rey"
      ],
      "abstract": "Many optimization problems require balancing multiple conflicting objectives.\nAs gradient descent is limited to single-objective optimization, we introduce\nits direct generalization: Jacobian descent (JD). This algorithm iteratively\nupdates parameters using the Jacobian matrix of a vector-valued objective\nfunction, in which each row is the gradient of an individual objective. While\nseveral methods to combine gradients already exist in the literature, they are\ngenerally hindered when the objectives conflict. In contrast, we propose\nprojecting gradients to fully resolve conflict while ensuring that they\npreserve an influence proportional to their norm. We prove significantly\nstronger convergence guarantees with this approach, supported by our empirical\nresults. Our method also enables instance-wise risk minimization (IWRM), a\nnovel learning paradigm in which the loss of each training example is\nconsidered a separate objective. Applied to simple image classification tasks,\nIWRM exhibits promising results compared to the direct minimization of the\naverage loss. Additionally, we outline an efficient implementation of JD using\nthe Gramian of the Jacobian matrix to reduce time and memory requirements.",
      "tldr_zh": "该研究针对多目标优化（Multi-Objective Optimization）中的冲突目标问题，引入了Jacobian Descent (JD)算法，作为梯度下降的直接推广。该算法使用目标函数的Jacobian矩阵迭代更新参数，通过投影梯度来完全解决目标冲突，同时确保梯度的影响与它们的范数成比例，从而提供更强的收敛保证，并通过实验验证其有效性。JD还启用了实例级风险最小化（IWRM），一种新颖的学习范式，将每个训练样本的损失视为独立目标，在图像分类任务上表现出色，比直接最小化平均损失更优。此外，研究概述了使用Jacobian矩阵的Gramian实现JD的效率方法，以降低时间和内存需求。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "39 pages, 10 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2406.16232v3",
      "published_date": "2024-06-23 22:06:25 UTC",
      "updated_date": "2025-02-03 12:29:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:41:39.002978"
    },
    {
      "arxiv_id": "2406.16231v1",
      "title": "Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method",
      "title_zh": "翻译失败",
      "authors": [
        "Kishaan Jeeveswaran",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Domain incremental learning (DIL) poses a significant challenge in real-world\nscenarios, as models need to be sequentially trained on diverse domains over\ntime, all the while avoiding catastrophic forgetting. Mitigating representation\ndrift, which refers to the phenomenon of learned representations undergoing\nchanges as the model adapts to new tasks, can help alleviate catastrophic\nforgetting. In this study, we propose a novel DIL method named DARE, featuring\na three-stage training process: Divergence, Adaptation, and REfinement. This\nprocess gradually adapts the representations associated with new tasks into the\nfeature space spanned by samples from previous tasks, simultaneously\nintegrating task-specific decision boundaries. Additionally, we introduce a\nnovel strategy for buffer sampling and demonstrate the effectiveness of our\nproposed method, combined with this sampling strategy, in reducing\nrepresentation drift within the feature encoder. This contribution effectively\nalleviates catastrophic forgetting across multiple DIL benchmarks. Furthermore,\nour approach prevents sudden representation drift at task boundaries, resulting\nin a well-calibrated DIL model that maintains the performance on previous\ntasks.",
      "tldr_zh": "本文提出了一种名为 DARE 的新 Domain Incremental Learning (DIL) 方法，旨在解决模型在顺序训练不同领域时面临的灾难性遗忘问题，通过三阶段训练过程（Divergence、Adaptation 和 REfinement）逐渐将新任务的表示适应到之前任务的特征空间，同时整合任务特定的决策边界。DARE 还引入了一种新型缓冲采样策略，有效减少 representation drift，并在多个 DIL 基准上缓解了遗忘问题。实验结果显示，该方法能防止任务边界处的突然表示漂移，并保持了先前任务的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at 41st International Conference on Machine Learning (ICML\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2406.16231v1",
      "published_date": "2024-06-23 22:05:52 UTC",
      "updated_date": "2024-06-23 22:05:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:41:39.487719"
    },
    {
      "arxiv_id": "2406.16224v2",
      "title": "From Text to Test: AI-Generated Control Software for Materials Science Instruments",
      "title_zh": "翻译失败",
      "authors": [
        "Davi M Fébba",
        "Kingsley Egbo",
        "William A. Callahan",
        "Andriy Zakutayev"
      ],
      "abstract": "Large language models (LLMs) are transforming the landscape of chemistry and\nmaterials science. Recent examples of LLM-accelerated experimental research\ninclude virtual assistants for parsing synthesis recipes from the literature,\nor using the extracted knowledge to guide synthesis and characterization.\nDespite these advancements, their application is constrained to labs with\nautomated instruments and control software, leaving much of materials science\nreliant on manual processes. Here, we demonstrate the rapid deployment of a\nPython-based control module for a Keithley 2400 electrical source measure unit\nusing ChatGPT-4. Through iterative refinement, we achieved effective instrument\nmanagement with minimal human intervention. Additionally, a user-friendly\ngraphical user interface (GUI) was created, effectively linking all instrument\ncontrols to interactive screen elements. Finally, we integrated this AI-crafted\ninstrument control software with a high-performance stochastic optimization\nalgorithm to facilitate rapid and automated extraction of electronic device\nparameters related to semiconductor charge transport mechanisms from\ncurrent-voltage (IV) measurement data. This integration resulted in a\ncomprehensive open-source toolkit for semiconductor device characterization and\nanalysis using IV curve measurements. We demonstrate the application of these\ntools by acquiring, analyzing, and parameterizing IV data from a\nPt/Cr$_2$O$_3$:Mg/$\\beta$-Ga$_2$O$_3$ heterojunction diode, a novel stack for\nhigh-power and high-temperature electronic devices. This approach underscores\nthe powerful synergy between LLMs and the development of instruments for\nscientific inquiry, showcasing a path for further acceleration in materials\nscience.",
      "tldr_zh": "本研究利用大型语言模型（LLMs）如 ChatGPT-4，开发了 AI 生成的控制软件，以加速材料科学仪器的自动化。研究者通过迭代优化，创建了一个 Python 控制模块和用户友好图形用户界面（GUI），用于管理 Keithley 2400 电气源测量单元，从而减少人为干预。论文进一步将该软件与随机优化算法集成，实现对半导体器件的电流-电压（IV）测量数据进行快速自动化参数提取，并展示了其在 Pt/Cr$_2$O$_3$:Mg/$\\beta$-Ga$_2$O$_3$ 异质结二极管上的应用。最终，该方法提供了一个开源工具包，突显了 LLMs 在推动材料科学实验革新的潜力。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16224v2",
      "published_date": "2024-06-23 21:32:57 UTC",
      "updated_date": "2024-06-25 11:34:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:41:53.477065"
    },
    {
      "arxiv_id": "2406.16223v1",
      "title": "Continuous Output Personality Detection Models via Mixed Strategy Training",
      "title_zh": "基于混合策略训练的连续输出人格检测模型",
      "authors": [
        "Rong Wang",
        "Kun Sun"
      ],
      "abstract": "The traditional personality models only yield binary results. This paper\npresents a novel approach for training personality detection models that\nproduce continuous output values, using mixed strategies. By leveraging the\nPANDORA dataset, which includes extensive personality labeling of Reddit\ncomments, we developed models that predict the Big Five personality traits with\nhigh accuracy. Our approach involves fine-tuning a RoBERTa-base model with\nvarious strategies such as Multi-Layer Perceptron (MLP) integration, and\nhyperparameter tuning. The results demonstrate that our models significantly\noutperform traditional binary classification methods, offering precise\ncontinuous outputs for personality traits, thus enhancing applications in AI,\npsychology, human resources, marketing and health care fields.",
      "tldr_zh": "本论文提出了一种通过混合策略训练的连续输出人格检测模型，以克服传统二元分类模型的局限性。利用 PANDORA 数据集，该方法基于微调 RoBERTa-base 模型，结合 Multi-Layer Perceptron (MLP) 整合和超参数调整，实现了对 Big Five 人格特质的高精度预测。实验结果表明，该模型显著优于传统方法，提供更精确的连续输出值，从而提升了 AI、心理学、人力资源、营销和医疗领域的应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16223v1",
      "published_date": "2024-06-23 21:32:15 UTC",
      "updated_date": "2024-06-23 21:32:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:42:03.142412"
    },
    {
      "arxiv_id": "2406.16221v1",
      "title": "F-FOMAML: GNN-Enhanced Meta-Learning for Peak Period Demand Forecasting with Proxy Data",
      "title_zh": "翻译失败",
      "authors": [
        "Zexing Xu",
        "Linjun Zhang",
        "Sitan Yang",
        "Rasoul Etesami",
        "Hanghang Tong",
        "Huan Zhang",
        "Jiawei Han"
      ],
      "abstract": "Demand prediction is a crucial task for e-commerce and physical retail\nbusinesses, especially during high-stake sales events. However, the limited\navailability of historical data from these peak periods poses a significant\nchallenge for traditional forecasting methods. In this paper, we propose a\nnovel approach that leverages strategically chosen proxy data reflective of\npotential sales patterns from similar entities during non-peak periods,\nenriched by features learned from a graph neural networks (GNNs)-based\nforecasting model, to predict demand during peak events. We formulate the\ndemand prediction as a meta-learning problem and develop the Feature-based\nFirst-Order Model-Agnostic Meta-Learning (F-FOMAML) algorithm that leverages\nproxy data from non-peak periods and GNN-generated relational metadata to learn\nfeature-specific layer parameters, thereby adapting to demand forecasts for\npeak events. Theoretically, we show that by considering domain similarities\nthrough task-specific metadata, our model achieves improved generalization,\nwhere the excess risk decreases as the number of training tasks increases.\nEmpirical evaluations on large-scale industrial datasets demonstrate the\nsuperiority of our approach. Compared to existing state-of-the-art models, our\nmethod demonstrates a notable improvement in demand prediction accuracy,\nreducing the Mean Absolute Error by 26.24% on an internal vending machine\ndataset and by 1.04% on the publicly accessible JD.com dataset.",
      "tldr_zh": "本研究针对电商和实体零售的高峰期需求预测问题，提出了一种新型方法F-FOMAML，利用代理数据（从非高峰期的类似实体获取）和GNN（Graph Neural Networks）生成的特征来提升预测准确性。该算法基于元学习框架（Meta-Learning），通过学习特征特定的层参数和任务相关元数据，实现对高峰事件需求的快速适应。理论分析显示，该模型通过考虑领域相似性，改善了泛化性能，使过量风险随训练任务数量减少。在大规模工业数据集上的实证评估中，F-FOMAML相较现有模型降低了Mean Absolute Error，内部数据集中减少26.24%，JD.com数据集上减少1.04%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GR",
        "econ.EM",
        "stat.ME",
        "68T07, 68T05, 62M10, 62M20, 90C90, 91B84"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16221v1",
      "published_date": "2024-06-23 21:28:50 UTC",
      "updated_date": "2024-06-23 21:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:42:25.594914"
    },
    {
      "arxiv_id": "2406.16220v1",
      "title": "Learning Run-time Safety Monitors for Machine Learning Components",
      "title_zh": "翻译失败",
      "authors": [
        "Ozan Vardal",
        "Richard Hawkins",
        "Colin Paterson",
        "Chiara Picardi",
        "Daniel Omeiza",
        "Lars Kunze",
        "Ibrahim Habli"
      ],
      "abstract": "For machine learning components used as part of autonomous systems (AS) in\ncarrying out critical tasks it is crucial that assurance of the models can be\nmaintained in the face of post-deployment changes (such as changes in the\noperating environment of the system). A critical part of this is to be able to\nmonitor when the performance of the model at runtime (as a result of changes)\nposes a safety risk to the system. This is a particularly difficult challenge\nwhen ground truth is unavailable at runtime. In this paper we introduce a\nprocess for creating safety monitors for ML components through the use of\ndegraded datasets and machine learning. The safety monitor that is created is\ndeployed to the AS in parallel to the ML component to provide a prediction of\nthe safety risk associated with the model output. We demonstrate the viability\nof our approach through some initial experiments using publicly available speed\nsign datasets.",
      "tldr_zh": "这篇论文提出了一种方法，用于学习运行时安全监视器（safety monitors），以监控机器学习组件（machine learning components）在自治系统（autonomous systems）中的性能，确保面对部署后变化时保持模型安全性。方法涉及使用退化数据集（degraded datasets）和机器学习来构建这些监视器，使其与ML组件并行部署，并预测模型输出的安全风险。实验通过公开的速度标志数据集（speed sign datasets）验证了该方法的有效性，展示了其在实际应用中的可行性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16220v1",
      "published_date": "2024-06-23 21:25:06 UTC",
      "updated_date": "2024-06-23 21:25:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:42:26.592913"
    },
    {
      "arxiv_id": "2406.16218v2",
      "title": "Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Ching-An Cheng",
        "Allen Nie",
        "Adith Swaminathan"
      ],
      "abstract": "We study a class of optimization problems motivated by automating the design\nand update of AI systems like coding assistants, robots, and copilots. AutoDiff\nframeworks, like PyTorch, enable efficient end-to-end optimization of\ndifferentiable systems. However, general computational workflows can be\nnon-differentiable and involve rich feedback (e.g. console output or user's\nresponses), heterogeneous parameters (e.g. prompts, codes), and intricate\nobjectives (beyond maximizing a score). We investigate end-to-end generative\noptimization -- using generative models such as LLMs within the optimizer for\nautomatic updating of general computational workflows. We discover that\nworkflow execution traces are akin to back-propagated gradients in AutoDiff and\ncan provide key information to interpret feedback for efficient optimization.\nFormally, we frame a new mathematical setup, Optimization with Trace Oracle\n(OPTO). In OPTO, an optimizer receives an execution trace along with feedback\non the computed output and updates parameters iteratively. We provide a Python\nlibrary, Trace, that efficiently converts a workflow optimization problem into\nan OPTO instance using PyTorch-like syntax. Using Trace, we develop a general\nLLM-based generative optimizer called OptoPrime. In empirical studies, we find\nthat OptoPrime is capable of first-order numerical optimization, prompt\noptimization, hyper-parameter tuning, robot controller design, code debugging,\netc., and is often competitive with specialized optimizers for each domain. We\nenvision Trace as an open research platform for devising novel generative\noptimizers and developing the next generation of interactive learning agents.\nWebsite: https://microsoft.github.io/Trace/.",
      "tldr_zh": "该论文提出了一种新的优化框架，将执行痕迹（execution traces）视为 AutoDiff 的继任者，用于处理非可微计算工作流、丰富反馈（如控制台输出）和异构参数。作者形式化了 Optimization with Trace Oracle (OPTO) 数学框架，并开发了 Trace Python 库，使用类似 PyTorch 的语法将工作流优化问题转化为 OPTO 实例。基于 LLMs 的通用优化器 OptoPrime 在实验中表现出色，能够处理一阶数值优化、提示优化、超参数调整、机器人控制器设计和代码调试等任务，并与专用优化器竞争，为交互式学习代理的发展提供研究平台。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16218v2",
      "published_date": "2024-06-23 21:05:31 UTC",
      "updated_date": "2024-11-01 00:01:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:42:42.945386"
    },
    {
      "arxiv_id": "2406.16983v1",
      "title": "On Instabilities of Unsupervised Denoising Diffusion Models in Magnetic Resonance Imaging Reconstruction",
      "title_zh": "关于无监督去噪扩散模型在磁共振成像重建中的不稳定性",
      "authors": [
        "Tianyu Han",
        "Sven Nebelung",
        "Firas Khader",
        "Jakob Nikolas Kather",
        "Daniel Truhn"
      ],
      "abstract": "Denoising diffusion models offer a promising approach to accelerating\nmagnetic resonance imaging (MRI) and producing diagnostic-level images in an\nunsupervised manner. However, our study demonstrates that even tiny worst-case\npotential perturbations transferred from a surrogate model can cause these\nmodels to generate fake tissue structures that may mislead clinicians. The\ntransferability of such worst-case perturbations indicates that the robustness\nof image reconstruction may be compromised due to MR system imperfections or\nother sources of noise. Moreover, at larger perturbation strengths, diffusion\nmodels exhibit Gaussian noise-like artifacts that are distinct from those\nobserved in supervised models and are more challenging to detect. Our results\nhighlight the vulnerability of current state-of-the-art diffusion-based\nreconstruction models to possible worst-case perturbations and underscore the\nneed for further research to improve their robustness and reliability in\nclinical settings.",
      "tldr_zh": "本研究探讨了无监督去噪扩散模型（Denoising Diffusion Models）在磁共振成像（MRI）重建中的不稳定性，发现即使微小的最坏情况潜在扰动（worst-case perturbations）从代理模型转移过来，也可能导致模型生成虚假的组织结构，从而误导临床医生。这些扰动的高度可转移性表明，图像重建的鲁棒性易受MRI系统 imperfections或其他噪声源影响，尤其是在更大扰动强度下，模型会产生独特的高斯噪声-like artifacts，这些 artifacts 比监督模型的更难检测。研究结果突出了当前最先进扩散模型的脆弱性，并呼吁进一步研究以提升其在临床环境中的可靠性和稳定性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16983v1",
      "published_date": "2024-06-23 19:44:00 UTC",
      "updated_date": "2024-06-23 19:44:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:42:51.960999"
    },
    {
      "arxiv_id": "2406.16191v1",
      "title": "Accelerating Matrix Diagonalization through Decision Transformers with Epsilon-Greedy Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Kshitij Bhatta",
        "Geigh Zollicoffer",
        "Manish Bhattarai",
        "Phil Romero",
        "Christian F. A. Negre",
        "Anders M. N. Niklasson",
        "Adetokunbo Adedoyin"
      ],
      "abstract": "This paper introduces a novel framework for matrix diagonalization, recasting\nit as a sequential decision-making problem and applying the power of Decision\nTransformers (DTs). Our approach determines optimal pivot selection during\ndiagonalization with the Jacobi algorithm, leading to significant speedups\ncompared to the traditional max-element Jacobi method. To bolster robustness,\nwe integrate an epsilon-greedy strategy, enabling success in scenarios where\ndeterministic approaches fail. This work demonstrates the effectiveness of DTs\nin complex computational tasks and highlights the potential of reimagining\nmathematical operations through a machine learning lens. Furthermore, we\nestablish the generalizability of our method by using transfer learning to\ndiagonalize matrices of smaller sizes than those trained.",
      "tldr_zh": "这篇论文提出了一种新框架，将矩阵对角化问题转化为顺序决策问题，并利用 Decision Transformers (DTs) 优化 Jacobi 算法中的枢轴选择，从而显著加速计算过程。论文整合 epsilon-greedy 策略来提升鲁棒性，确保在确定性方法失败的场景中仍能成功执行。实验结果显示，该方法比传统 max-element Jacobi 方法快，且通过迁移学习实现了对更小矩阵的泛化，展示了 DTs 在复杂计算任务中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.16191v1",
      "published_date": "2024-06-23 18:56:46 UTC",
      "updated_date": "2024-06-23 18:56:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:43:04.119766"
    },
    {
      "arxiv_id": "2406.16982v1",
      "title": "Research on Disease Prediction Model Construction Based on Computer AI deep Learning Technology",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Lin",
        "Muqing Li",
        "Ziyi Zhu",
        "Yinqiu Feng",
        "Lingxi Xiao",
        "Zexi Chen"
      ],
      "abstract": "The prediction of disease risk factors can screen vulnerable groups for\neffective prevention and treatment, so as to reduce their morbidity and\nmortality. Machine learning has a great demand for high-quality labeling\ninformation, and labeling noise in medical big data poses a great challenge to\nefficient disease risk warning methods. Therefore, this project intends to\nstudy the robust learning algorithm and apply it to the early warning of\ninfectious disease risk. A dynamic truncated loss model is proposed, which\ncombines the traditional mutual entropy implicit weight feature with the mean\nvariation feature. It is robust to label noise. A lower bound on training loss\nis constructed, and a method based on sampling rate is proposed to reduce the\ngradient of suspected samples to reduce the influence of noise on training\nresults. The effectiveness of this method under different types of noise was\nverified by using a stroke screening data set as an example. This method\nenables robust learning of data containing label noise.",
      "tldr_zh": "本研究针对医疗大数据中标签噪声问题，提出了一种动态截断损失模型（dynamic truncated loss model），它结合了互信息隐式权重（mutual entropy implicit weight）和均值变化特征（mean variation feature），以提升疾病风险预测的鲁棒性。该模型构建了训练损失的下界，并采用基于采样率的 method 来减少噪声样本对训练结果的影响，从而实现对不同类型噪声的有效处理。在卒中筛查数据集上验证显示，该方法显著提高了传染病风险早期预警的准确性，助力降低易感人群的发病率和死亡率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16982v1",
      "published_date": "2024-06-23 18:44:03 UTC",
      "updated_date": "2024-06-23 18:44:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:43:16.659548"
    },
    {
      "arxiv_id": "2406.16981v1",
      "title": "Research on Feature Extraction Data Processing System For MRI of Brain Diseases Based on Computer Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Lingxi Xiao",
        "Jinxin Hu",
        "Yutian Yang",
        "Yinqiu Feng",
        "Zichao Li",
        "Zexi Chen"
      ],
      "abstract": "Most of the existing wavelet image processing techniques are carried out in\nthe form of single-scale reconstruction and multiple iterations. However,\nprocessing high-quality fMRI data presents problems such as mixed noise and\nexcessive computation time. This project proposes the use of matrix operations\nby combining mixed noise elimination methods with wavelet analysis to replace\ntraditional iterative algorithms. Functional magnetic resonance imaging (fMRI)\nof the auditory cortex of a single subject is analyzed and compared to the\nwavelet domain signal processing technology based on repeated times and the\nworld's most influential SPM8. Experiments show that this algorithm is the\nfastest in computing time, and its detection effect is comparable to the\ntraditional iterative algorithm. However, this has a higher practical value for\nthe processing of FMRI data. In addition, the wavelet analysis method proposed\nsignal processing to speed up the calculation rate.",
      "tldr_zh": "这篇论文针对脑部疾病MRI数据处理中的混合噪声和计算时间过长问题，提出了一种基于计算机深度学习的特征提取系统。系统通过结合混合噪声消除方法与小波分析，并采用矩阵运算替换传统迭代算法，提高了fMRI数据处理的效率。实验结果显示，该算法在分析单受试者听觉皮层fMRI时，计算时间最快，且检测效果与小波域信号处理技术和SPM8相当，具有更高的实际应用价值。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16981v1",
      "published_date": "2024-06-23 18:41:43 UTC",
      "updated_date": "2024-06-23 18:41:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:43:29.434613"
    },
    {
      "arxiv_id": "2406.16979v1",
      "title": "Understanding and Diagnosing Deep Reinforcement Learning",
      "title_zh": "深度强化学习的理解和诊断",
      "authors": [
        "Ezgi Korkmaz"
      ],
      "abstract": "Deep neural policies have recently been installed in a diverse range of\nsettings, from biotechnology to automated financial systems. However, the\nutilization of deep neural networks to approximate the value function leads to\nconcerns on the decision boundary stability, in particular, with regard to the\nsensitivity of policy decision making to indiscernible, non-robust features due\nto highly non-convex and complex deep neural manifolds. These concerns\nconstitute an obstruction to understanding the reasoning made by deep neural\npolicies, and their foundational limitations. Hence, it is crucial to develop\ntechniques that aim to understand the sensitivities in the learnt\nrepresentations of neural network policies. To achieve this we introduce a\ntheoretically founded method that provides a systematic analysis of the\nunstable directions in the deep neural policy decision boundary across both\ntime and space. Through experiments in the Arcade Learning Environment (ALE),\nwe demonstrate the effectiveness of our technique for identifying correlated\ndirections of instability, and for measuring how sample shifts remold the set\nof sensitive directions in the neural policy landscape. Most importantly, we\ndemonstrate that state-of-the-art robust training techniques yield learning of\ndisjoint unstable directions, with dramatically larger oscillations over time,\nwhen compared to standard training. We believe our results reveal the\nfundamental properties of the decision process made by reinforcement learning\npolicies, and can help in constructing reliable and robust deep neural\npolicies.",
      "tldr_zh": "这篇论文探讨了深度强化学习(Deep Reinforcement Learning)中神经策略决策边界的不稳定性问题，特别是对微小非鲁棒特征的敏感性，导致决策过程不可靠。作者提出了一种理论基础的方法，用于系统分析这些不稳定方向在时间和空间上的变化，通过Arcade Learning Environment (ALE)实验验证其有效性。实验结果显示，state-of-the-art鲁棒训练技术会导致学习分离的不稳定方向，并产生更大的时间震荡，与标准训练相比更易受样本变化影响。该研究揭示了强化学习策略的根本属性，并为构建可靠的深度神经策略提供诊断和改进指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.16979v1",
      "published_date": "2024-06-23 18:10:16 UTC",
      "updated_date": "2024-06-23 18:10:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:43:41.762576"
    },
    {
      "arxiv_id": "2406.16176v2",
      "title": "GraphEval36K: Benchmarking Coding and Reasoning Capabilities of Large Language Models on Graph Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Qiming Wu",
        "Zichen Chen",
        "Will Corcoran",
        "Misha Sra",
        "Ambuj K. Singh"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to manipulate, program, and reason about\nstructured data, especially graphs. We introduce GraphEval36K, the first\ncomprehensive graph dataset, comprising 40 graph coding problems and 36,900\ntest cases to evaluate the ability of LLMs on graph problem-solving. Our\ndataset is categorized into eight primary and four sub-categories to ensure a\nthorough evaluation across different types of graphs. We benchmark ten LLMs,\nfinding that private models outperform open-source ones, though the gap is\nnarrowing. We also analyze the performance of LLMs across directed vs\nundirected graphs, different kinds of graph concepts, and network models.\nFurthermore, to improve the usability of our evaluation framework, we propose\nStructured Symbolic Decomposition (SSD), an instruction-based method designed\nto enhance LLM performance on complex graph tasks. Results show that SSD\nimproves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and\nClaude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.",
      "tldr_zh": "本研究引入了GraphEval36K，这是一个包含40个图编码问题和36,900个测试用例的全面数据集，用于评估大型语言模型(LLMs)在图数据上的编码和推理能力。该数据集分为八个主要类别和四个子类别，涵盖不同图类型，并通过基准测试十个LLMs发现，私有模型的表现优于开源模型，但差距正在缩小。研究还分析了LLMs在有向与无向图、图概念和网络模型上的性能差异。最终，提出Structured Symbolic Decomposition (SSD)，一种基于指令的方法，能够显著提升LLMs在复杂图任务上的表现，例如使GPT-4、GPT-4o、Gemini-Pro和Claude-3-Sonnet的平均通过率分别提高8.38%、6.78%、29.28%和25.28%。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "H.2.8, I.2.6, I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "The first two authors contributed equally to this work. This paper\n  has been accepted by NAACL 2025. GraphEval36K is available at\n  https://grapheval36k.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2406.16176v2",
      "published_date": "2024-06-23 18:01:56 UTC",
      "updated_date": "2025-02-17 09:53:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:43:53.268960"
    },
    {
      "arxiv_id": "2407.10989v2",
      "title": "Can Large Language Models Detect Verbal Indicators of Romantic Attraction?",
      "title_zh": "大语言模型能否检测浪漫吸引的口头指标？",
      "authors": [
        "Sandra C. Matz",
        "Heinrich Peters",
        "Moran Cerf",
        "Eric Grunenberg",
        "Paul W. Eastwick",
        "Mitja D. Back",
        "Eli J. Finkel"
      ],
      "abstract": "As artificial intelligence (AI) models become an integral part of everyday\nlife, our interactions with them shift from purely functional exchanges to more\nrelational experiences. For these experiences to be successful, artificial\nagents need to be able to detect and interpret social cues and interpersonal\ndynamics; both within and outside of their own human-agent relationships. In\nthis paper, we explore whether AI models can accurately decode one of the\narguably most important but complex social signals: romantic attraction.\nSpecifically, we test whether Large Language Models can detect romantic\nattraction during brief getting-to-know-you interactions between humans.\nExamining data from 964 speed dates, we show that ChatGPT can predict both\nobjective and subjective indicators of speed dating success (r=0.12-0.23).\nAlthough predictive performance remains relatively low, ChatGPT's predictions\nof actual matching (i.e., the exchange of contact information) were not only on\npar with those of human judges but incremental to speed daters' own\npredictions. In addition, ChatGPT's judgments showed substantial overlap with\nthose made by human observers (r=0.21-0.35), highlighting similarities in their\nrepresentation of romantic attraction that are independent of accuracy. Our\nfindings also offer insights into how ChatGPT arrives at its predictions and\nthe mistakes it makes. Specifically, we use a Brunswik lens approach to\nidentify the linguistic and conversational cues utilized by ChatGPT (and human\njudges) vis-a-vis those that are predictive of actual matching.",
      "tldr_zh": "本研究探讨大型语言模型（Large Language Models）是否能检测浪漫吸引力的语言指标，特别是ChatGPT在速约互动中的预测能力。研究分析了964次速约数据，发现ChatGPT能预测速约成功指标（相关系数r=0.12-0.23），其对实际匹配（交换联系信息）的判断与人类观察者相当（r=0.21-0.35），甚至优于参与者的自我预测。论文使用Brunswik lens方法识别ChatGPT和人类判断所依赖的语言线索，并揭示了模型的决策过程和潜在错误，为AI在社会互动中的应用提供了重要启示。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10989v2",
      "published_date": "2024-06-23 17:50:30 UTC",
      "updated_date": "2025-04-12 20:24:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:44:05.189562"
    },
    {
      "arxiv_id": "2407.16903v1",
      "title": "US-China perspectives on extreme AI risks and global governance",
      "title_zh": "美中对极端AI风险和全球治理的视角",
      "authors": [
        "Akash Wasil",
        "Tim Durgin"
      ],
      "abstract": "The United States and China will play an important role in navigating safety\nand security challenges relating to advanced artificial intelligence. We sought\nto better understand how experts in each country describe safety and security\nthreats from advanced artificial intelligence, extreme risks from AI, and the\npotential for international cooperation. Specifically, we compiled\npublicly-available statements from major technical and policy leaders in both\nthe United States and China. We focused our analysis on advanced forms of\nartificial intelligence, such as artificial general intelligence (AGI), that\nmay have the most significant impacts on national and global security. Experts\nin both countries expressed concern about risks from AGI, risks from\nintelligence explosions, and risks from AI systems that escape human control.\nBoth countries have also launched early efforts designed to promote\ninternational cooperation around safety standards and risk management\npractices. Notably, our findings only reflect information from publicly\navailable sources. Nonetheless, our findings can inform policymakers and\nresearchers about the state of AI discourse in the US and China. We hope such\nwork can contribute to policy discussions around advanced AI, its global\nsecurity threats, and potential international dialogues or agreements to\nmitigate such threats.",
      "tldr_zh": "这篇论文探讨了美国和中国专家对高级人工智能（AGI）极端风险的观点，包括智能爆炸和AI系统逃脱人类控制等潜在威胁。研究通过分析两国技术与政策领导人的公开声明，发现双方均表达了对这些风险的担忧，并已启动国际合作以制定安全标准和风险管理实践。该工作为政策制定者和研究者提供了见解，有助于推动全球对话和协议，以缓解AI对国家与全球安全的威胁。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16903v1",
      "published_date": "2024-06-23 17:31:27 UTC",
      "updated_date": "2024-06-23 17:31:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:44:15.850890"
    },
    {
      "arxiv_id": "2406.16170v1",
      "title": "SimCE: Simplifying Cross-Entropy Loss for Collaborative Filtering",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaodong Yang",
        "Huiyuan Chen",
        "Yuchen Yan",
        "Yuxin Tang",
        "Yuying Zhao",
        "Eric Xu",
        "Yiwei Cai",
        "Hanghang Tong"
      ],
      "abstract": "The learning objective is integral to collaborative filtering systems, where\nthe Bayesian Personalized Ranking (BPR) loss is widely used for learning\ninformative backbones. However, BPR often experiences slow convergence and\nsuboptimal local optima, partially because it only considers one negative item\nfor each positive item, neglecting the potential impacts of other unobserved\nitems. To address this issue, the recently proposed Sampled Softmax\nCross-Entropy (SSM) compares one positive sample with multiple negative\nsamples, leading to better performance. Our comprehensive experiments confirm\nthat recommender systems consistently benefit from multiple negative samples\nduring training. Furthermore, we introduce a \\underline{Sim}plified Sampled\nSoftmax \\underline{C}ross-\\underline{E}ntropy Loss (SimCE), which simplifies\nthe SSM using its upper bound. Our validation on 12 benchmark datasets, using\nboth MF and LightGCN backbones, shows that SimCE significantly outperforms both\nBPR and SSM.",
      "tldr_zh": "该论文针对协同过滤(Collaborative Filtering)中的学习目标问题，指出 Bayesian Personalized Ranking (BPR) 损失因仅使用一个负样本而导致收敛缓慢和局部最优不佳。作者引入了 SimCE，一种简化 Sampled Softmax Cross-Entropy (SSM) 损失函数，通过利用其上界来处理多个负样本，从而提升模型性能。在 12 个基准数据集上，使用 MF 和 LightGCN 作为骨干网络，SimCE 显著优于 BPR 和 SSM，证明了其有效性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16170v1",
      "published_date": "2024-06-23 17:24:07 UTC",
      "updated_date": "2024-06-23 17:24:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:44:28.868378"
    },
    {
      "arxiv_id": "2406.16151v1",
      "title": "Monte Carlo Planning for Stochastic Control on Constrained Markov Decision Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Larkin Liu",
        "Shiqi Liu",
        "Matej Jusup"
      ],
      "abstract": "In the world of stochastic control, especially in economics and engineering,\nMarkov Decision Processes (MDPs) can effectively model various stochastic\ndecision processes, from asset management to transportation optimization. These\nunderlying MDPs, upon closer examination, often reveal a specifically\nconstrained causal structure concerning the transition and reward dynamics. By\nexploiting this structure, we can obtain a reduction in the causal\nrepresentation of the problem setting, allowing us to solve of the optimal\nvalue function more efficiently. This work defines an MDP framework, the\n\\texttt{SD-MDP}, where we disentangle the causal structure of MDPs' transition\nand reward dynamics, providing distinct partitions on the temporal causal\ngraph. With this stochastic reduction, the \\texttt{SD-MDP} reflects a general\nclass of resource allocation problems. This disentanglement further enables us\nto derive theoretical guarantees on the estimation error of the value function\nunder an optimal policy by allowing independent value estimation from Monte\nCarlo sampling. Subsequently, by integrating this estimator into well-known\nMonte Carlo planning algorithms, such as Monte Carlo Tree Search (MCTS), we\nderive bounds on the simple regret of the algorithm. Finally, we quantify the\npolicy improvement of MCTS under the \\texttt{SD-MDP} framework by demonstrating\nthat the MCTS planning algorithm achieves higher expected reward (lower costs)\nunder a constant simulation budget, on a tangible economic example based on\nmaritime refuelling.",
      "tldr_zh": "该研究针对随机控制中的Markov Decision Processes (MDPs)，通过利用其过渡和奖励动态的约束因果结构，引入了SD-MDP框架，以分离因果表示并提高最优价值函数的求解效率。SD-MDP框架适用于资源分配问题，并提供了Monte Carlo采样的独立价值估计理论保证，进而推导出Monte Carlo Tree Search (MCTS)算法的简单遗憾界。实验在基于海上加油的经济示例中证明，MCTS在恒定模拟预算下实现了更高的预期奖励（更低的成本）。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "C.4"
      ],
      "primary_category": "cs.AI",
      "comment": "Working manuscript",
      "pdf_url": "http://arxiv.org/pdf/2406.16151v1",
      "published_date": "2024-06-23 16:22:40 UTC",
      "updated_date": "2024-06-23 16:22:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:44:41.086169"
    },
    {
      "arxiv_id": "2406.16148v3",
      "title": "Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking",
      "title_zh": "迈向开放呼吸声学基础模型：预训练和基准测试",
      "authors": [
        "Yuwei Zhang",
        "Tong Xia",
        "Jing Han",
        "Yu Wu",
        "Georgios Rizos",
        "Yang Liu",
        "Mohammed Mosuily",
        "Jagmohan Chauhan",
        "Cecilia Mascolo"
      ],
      "abstract": "Respiratory audio, such as coughing and breathing sounds, has predictive\npower for a wide range of healthcare applications, yet is currently\nunder-explored. The main problem for those applications arises from the\ndifficulty in collecting large labeled task-specific data for model\ndevelopment. Generalizable respiratory acoustic foundation models pretrained\nwith unlabeled data would offer appealing advantages and possibly unlock this\nimpasse. However, given the safety-critical nature of healthcare applications,\nit is pivotal to also ensure openness and replicability for any proposed\nfoundation model solution. To this end, we introduce OPERA, an OPEn Respiratory\nAcoustic foundation model pretraining and benchmarking system, as the first\napproach answering this need. We curate large-scale respiratory audio datasets\n(~136K samples, over 400 hours), pretrain three pioneering foundation models,\nand build a benchmark consisting of 19 downstream respiratory health tasks for\nevaluation. Our pretrained models demonstrate superior performance (against\nexisting acoustic models pretrained with general audio on 16 out of 19 tasks)\nand generalizability (to unseen datasets and new respiratory audio modalities).\nThis highlights the great promise of respiratory acoustic foundation models and\nencourages more studies using OPERA as an open resource to accelerate research\non respiratory audio for health. The system is accessible from\nhttps://github.com/evelyn0414/OPERA.",
      "tldr_zh": "该研究针对呼吸音频（如咳嗽和呼吸声）在医疗应用中的预测潜力，提出使用未标记数据预训练的通用呼吸声学 foundation models，以解决数据标注困难的问题。作者引入了 OPERA 系统，包括整理大规模数据集（约136K样本、400小时以上）和预训练三个开创性 foundation models，同时构建了包含19个下游呼吸健康任务的基准。实验结果显示，OPERA预训练模型在19个任务中的16个上优于现有音频模型，并在未见数据集和新呼吸音频模态上表现出色，从而推动呼吸音频在医疗领域的开放研究和应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "accepted by NeurIPS 2024 Track Datasets and Benchmarks",
      "pdf_url": "http://arxiv.org/pdf/2406.16148v3",
      "published_date": "2024-06-23 16:04:26 UTC",
      "updated_date": "2024-11-07 15:23:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:45:03.984117"
    },
    {
      "arxiv_id": "2407.02518v2",
      "title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness",
      "title_zh": "翻译失败",
      "authors": [
        "Hung Le",
        "Yingbo Zhou",
        "Caiming Xiong",
        "Silvio Savarese",
        "Doyen Sahoo"
      ],
      "abstract": "Large language models (LLMs) for code are typically trained to align with\nnatural language instructions to closely follow their intentions and\nrequirements. However, in many practical scenarios, it becomes increasingly\nchallenging for these models to navigate the intricate boundary between\nhelpfulness and safety, especially against highly complex yet potentially\nmalicious instructions. In this work, we introduce INDICT: a new framework that\nempowers LLMs with Internal Dialogues of Critiques for both safety and\nhelpfulness guidance. The internal dialogue is a dual cooperative system\nbetween a safety-driven critic and a helpfulness-driven critic. Each critic\nprovides analysis against the given task and corresponding generated response,\nequipped with external knowledge queried through relevant code snippets and\ntools like web search and code interpreter. We engage the dual critic system in\nboth code generation stage as well as code execution stage, providing\npreemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8\ndiverse tasks across 8 programming languages from 5 benchmarks, using LLMs from\n7B to 70B parameters. We observed that our approach can provide an advanced\nlevel of critiques of both safety and helpfulness analysis, significantly\nimproving the quality of output codes ($+10\\%$ absolute improvements in all\nmodels).",
      "tldr_zh": "该论文引入了 INDICT 框架，利用内部对话的批评系统（Internal Dialogues of Critiques），来增强大型语言模型 (LLMs) 在代码生成中的安全性和帮助性平衡。框架由安全驱动批评者 (safety-driven critic) 和帮助性驱动批评者 (helpfulness-driven critic) 组成，它们通过外部知识（如代码片段、网页搜索和代码解释器）分析任务和响应，并在代码生成及执行阶段提供预先和事后指导。实验在 8 个任务、8 种编程语言和 5 个基准上评估了从 7B 到 70B 参数的 LLMs，结果显示输出代码质量绝对提高了 10%。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.MA",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to The Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.02518v2",
      "published_date": "2024-06-23 15:55:07 UTC",
      "updated_date": "2024-10-29 08:20:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:45:18.198309"
    },
    {
      "arxiv_id": "2406.16145v1",
      "title": "Predefined Prototypes for Intra-Class Separation and Disentanglement",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio Almudévar",
        "Théo Mariotte",
        "Alfonso Ortega",
        "Marie Tahon",
        "Luis Vicente",
        "Antonio Miguel",
        "Eduardo Lleida"
      ],
      "abstract": "Prototypical Learning is based on the idea that there is a point (which we\ncall prototype) around which the embeddings of a class are clustered. It has\nshown promising results in scenarios with little labeled data or to design\nexplainable models. Typically, prototypes are either defined as the average of\nthe embeddings of a class or are designed to be trainable. In this work, we\npropose to predefine prototypes following human-specified criteria, which\nsimplify the training pipeline and brings different advantages. Specifically,\nin this work we explore two of these advantages: increasing the inter-class\nseparability of embeddings and disentangling embeddings with respect to\ndifferent variance factors, which can translate into the possibility of having\nexplainable predictions. Finally, we propose different experiments that help to\nunderstand our proposal and demonstrate empirically the mentioned advantages.",
      "tldr_zh": "本文提出了一种预定义原型的Prototypical Learning方法，通过人类指定的标准来定义prototypes，从而简化训练过程。相比传统方法，该方法增强了嵌入的inter-class separability，提高类间可分离性，并实现了对不同variance factors的disentanglement，这有助于生成可解释的预测。实验结果证明了这些优势，展示了该方法在提升模型性能和可解释性方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16145v1",
      "published_date": "2024-06-23 15:52:23 UTC",
      "updated_date": "2024-06-23 15:52:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:45:28.434499"
    },
    {
      "arxiv_id": "2407.02517v2",
      "title": "CAV-AHDV-CAV: Mitigating Traffic Oscillations for CAVs through a Novel Car-Following Structure and Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xianda Chen",
        "PakHin Tiu",
        "Yihuai Zhang",
        "Xinhu Zheng",
        "Meixin Zhu"
      ],
      "abstract": "Connected and Automated Vehicles (CAVs) offer a promising solution to the\nchallenges of mixed traffic with both CAVs and Human-Driven Vehicles (HDVs). A\nsignificant hurdle in such scenarios is traffic oscillation, or the\n\"stop-and-go\" pattern, during car-following situations. While HDVs rely on\nlimited information, CAVs can leverage data from other CAVs for better\ndecision-making. This allows CAVs to anticipate and mitigate the spread of\ndeceleration waves that worsen traffic flow. We propose a novel \"CAV-AHDV-CAV\"\ncar-following framework that treats the sequence of HDVs between two CAVs as a\nsingle entity, eliminating noise from individual driver behaviors. This deep\nreinforcement learning approach analyzes vehicle equilibrium states and employs\na state fusion strategy. Trained and tested on diverse datasets (HighD, NGSIM,\nSPMD, Waymo, Lyft) encompassing over 70,000 car-following instances, our model\noutperforms baselines in collision avoidance, maintaining equilibrium with both\npreceding and leading vehicles and achieving the lowest standard deviation of\ntime headway. These results demonstrate the effectiveness of our approach in\ndeveloping robust CAV control strategies for mixed traffic. Our model has the\npotential to mitigate traffic oscillation, improve traffic flow efficiency, and\nenhance overall safety.",
      "tldr_zh": "这篇论文针对混合交通中CAVs（Connected and Automated Vehicles）和HDVs（Human-Driven Vehicles）的交通震荡问题，提出了一种新型车跟随框架CAV-AHDV-CAV，将两个CAVs之间的HDV序列视为单一实体，以减少个体驾驶行为的噪音。框架采用深度强化学习分析车辆平衡状态并实施状态融合策略，并在HighD、NGSIM、SPMD、Waymo和Lyft等数据集上进行训练和测试，涵盖超过70,000个车跟随实例。结果显示，该模型在碰撞避免、维持车辆平衡和最小化时间头距标准差方面优于基线模型，从而有效缓解交通震荡、提升交通流效率和整体安全性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Due to errors identified in the experimental methodology and results\n  sections, we request to withdraw this paper",
      "pdf_url": "http://arxiv.org/pdf/2407.02517v2",
      "published_date": "2024-06-23 15:38:29 UTC",
      "updated_date": "2024-07-11 03:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:45:41.801036"
    },
    {
      "arxiv_id": "2407.07744v1",
      "title": "Belief Information based Deep Channel Estimation for Massive MIMO Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Jialong Xu",
        "Liu Liu",
        "Xin Wang",
        "Lan Chen"
      ],
      "abstract": "In the next generation wireless communication system, transmission rates\nshould continue to rise to support emerging scenarios, e.g., the immersive\ncommunications. From the perspective of communication system evolution,\nmultiple-input multiple-output (MIMO) technology remains pivotal for enhancing\ntransmission rates. However, current MIMO systems rely on inserting pilot\nsignals to achieve accurate channel estimation. As the increase of transmit\nstream, the pilots consume a significant portion of transmission resources,\nseverely reducing the spectral efficiency. In this correspondence, we propose a\nbelief information based mechanism. By introducing a plug-and-play belief\ninformation module, existing single-antenna channel estimation networks could\nbe seamlessly adapted to multi-antenna channel estimation and fully exploit the\nspatial correlation among multiple antennas. Experimental results demonstrate\nthat the proposed method can either improve 1 ~ 2 dB channel estimation\nperformance or reduce 1/3 ~ 1/2 pilot overhead, particularly in bad channel\nconditions.",
      "tldr_zh": "本论文针对下一代无线通信系统中MIMO技术的信道估计问题，指出传统方法依赖导频信号导致资源消耗增加和频谱效率降低。作者提出了一种基于belief information的机制，通过引入plug-and-play belief information module，将现有的单天线信道估计网络无缝适配到多天线场景，并充分利用多天线间的空间相关性。实验结果显示，该方法在恶劣信道条件下可提升1~2 dB的信道估计性能，或减少1/3~1/2的导频开销，从而提高传输效率。",
      "categories": [
        "cs.IT",
        "cs.AI",
        "eess.SP",
        "math.IT"
      ],
      "primary_category": "cs.IT",
      "comment": "5 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.07744v1",
      "published_date": "2024-06-23 15:31:07 UTC",
      "updated_date": "2024-06-23 15:31:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:45:53.005729"
    },
    {
      "arxiv_id": "2406.16978v1",
      "title": "MetaFollower: Adaptable Personalized Autonomous Car Following",
      "title_zh": "翻译失败",
      "authors": [
        "Xianda Chen",
        "Kehua Chen",
        "Meixin Zhu",
        "Hao",
        "Yang",
        "Shaojie Shen",
        "Xuesong Wang",
        "Yinhai Wang"
      ],
      "abstract": "Car-following (CF) modeling, a fundamental component in microscopic traffic\nsimulation, has attracted increasing interest of researchers in the past\ndecades. In this study, we propose an adaptable personalized car-following\nframework -MetaFollower, by leveraging the power of meta-learning.\nSpecifically, we first utilize Model-Agnostic Meta-Learning (MAML) to extract\ncommon driving knowledge from various CF events. Afterward, the pre-trained\nmodel can be fine-tuned on new drivers with only a few CF trajectories to\nachieve personalized CF adaptation. We additionally combine Long Short-Term\nMemory (LSTM) and Intelligent Driver Model (IDM) to reflect temporal\nheterogeneity with high interpretability. Unlike conventional adaptive cruise\ncontrol (ACC) systems that rely on predefined settings and constant parameters\nwithout considering heterogeneous driving characteristics, MetaFollower can\naccurately capture and simulate the intricate dynamics of car-following\nbehavior while considering the unique driving styles of individual drivers. We\ndemonstrate the versatility and adaptability of MetaFollower by showcasing its\nability to adapt to new drivers with limited training data quickly. To evaluate\nthe performance of MetaFollower, we conduct rigorous experiments comparing it\nwith both data-driven and physics-based models. The results reveal that our\nproposed framework outperforms baseline models in predicting car-following\nbehavior with higher accuracy and safety. To the best of our knowledge, this is\nthe first car-following model aiming to achieve fast adaptation by considering\nboth driver and temporal heterogeneity based on meta-learning.",
      "tldr_zh": "本文提出 MetaFollower，一种基于 meta-learning 的可适应个性化跟车框架，用于微观交通模拟。框架利用 Model-Agnostic Meta-Learning (MAML) 从各种跟车事件中提取通用驾驶知识，并通过少量轨迹微调结合 Long Short-Term Memory (LSTM) 和 Intelligent Driver Model (IDM)，以捕捉个体驾驶风格和时间异质性。相较于传统 Adaptive Cruise Control (ACC) 系统，MetaFollower 能更准确地模拟跟车动态，并在实验中超越基线模型，提高预测准确性和安全性。该方法首次实现基于 meta-learning 的快速适应，考虑驾驶员和时间异质性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16978v1",
      "published_date": "2024-06-23 15:30:40 UTC",
      "updated_date": "2024-06-23 15:30:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:46:05.684807"
    },
    {
      "arxiv_id": "2407.02516v1",
      "title": "EditFollower: Tunable Car Following Models for Customizable Adaptive Cruise Control Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Xianda Chen",
        "Xu Han",
        "Meixin Zhu",
        "Xiaowen Chu",
        "PakHin Tiu",
        "Xinhu Zheng",
        "Yinhai Wang"
      ],
      "abstract": "In the realm of driving technologies, fully autonomous vehicles have not been\nwidely adopted yet, making advanced driver assistance systems (ADAS) crucial\nfor enhancing driving experiences. Adaptive Cruise Control (ACC) emerges as a\npivotal component of ADAS. However, current ACC systems often employ fixed\nsettings, failing to intuitively capture drivers' social preferences and\nleading to potential function disengagement. To overcome these limitations, we\npropose the Editable Behavior Generation (EBG) model, a data-driven\ncar-following model that allows for adjusting driving discourtesy levels. The\nframework integrates diverse courtesy calculation methods into long short-term\nmemory (LSTM) and Transformer architectures, offering a comprehensive approach\nto capture nuanced driving dynamics. By integrating various discourtesy values\nduring the training process, our model generates realistic agent trajectories\nwith different levels of courtesy in car-following behavior. Experimental\nresults on the HighD and Waymo datasets showcase a reduction in Mean Squared\nError (MSE) of spacing and MSE of speed compared to baselines, establishing\nstyle controllability. To the best of our knowledge, this work represents the\nfirst data-driven car-following model capable of dynamically adjusting\ndiscourtesy levels. Our model provides valuable insights for the development of\nACC systems that take into account drivers' social preferences.",
      "tldr_zh": "该研究针对高级驾驶辅助系统 (ADAS) 中的 Adaptive Cruise Control (ACC) 系统存在的问题，提出了一种可调跟车模型 EditFollower，通过 Editable Behavior Generation (EBG) 框架允许用户调整驾驶不礼貌水平，以更好地适应驾驶员的社会偏好。EBG 模型将多种礼貌计算方法整合到 LSTM 和 Transformer 架构中，并在训练过程中融入不同不礼貌值，生成更现实且可控的跟车轨迹。实验在 HighD 和 Waymo 数据集上显示，与基线模型相比，Mean Squared Error (MSE) of spacing 和 MSE of speed 显著降低，证明了该模型的风格可控性，并为开发考虑驾驶偏好的 ACC 系统提供了新见解。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02516v1",
      "published_date": "2024-06-23 15:04:07 UTC",
      "updated_date": "2024-06-23 15:04:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:46:18.489916"
    },
    {
      "arxiv_id": "2407.16902v1",
      "title": "The Potential and Perils of Generative Artificial Intelligence for Quality Improvement and Patient Safety",
      "title_zh": "生成式人工智能在质量",
      "authors": [
        "Laleh Jalilian",
        "Daniel McDuff",
        "Achuta Kadambi"
      ],
      "abstract": "Generative artificial intelligence (GenAI) has the potential to improve\nhealthcare through automation that enhances the quality and safety of patient\ncare. Powered by foundation models that have been pretrained and can generate\ncomplex content, GenAI represents a paradigm shift away from the more\ntraditional focus on task-specific classifiers that have dominated the AI\nlandscape thus far. We posit that the imminent application of GenAI in\nhealthcare will be through well-defined, low risk, high value, and narrow\napplications that automate healthcare workflows at the point of care using\nsmaller foundation models. These models will be finetuned for different\ncapabilities and application specific scenarios and will have the ability to\nprovide medical explanations, reference evidence within a retrieval augmented\nframework and utilizing external tools. We contrast this with a general,\nall-purpose AI model for end-to-end clinical decision making that improves\nclinician performance, including safety-critical diagnostic tasks, which will\nrequire greater research prior to implementation. We consider areas where\n'human in the loop' Generative AI can improve healthcare quality and safety by\nautomating mundane tasks. Using the principles of implementation science will\nbe critical for integrating 'end to end' GenAI systems that will be accepted by\nhealthcare teams.",
      "tldr_zh": "该论文探讨了生成式人工智能（GenAI）在医疗保健领域的潜力与风险，强调其通过预训练的基础模型（foundation models）自动化工作流程来提升患者护理质量和安全。作者建议采用定义明确、低风险、高价值的狭窄应用，例如使用较小的基础模型进行微调，提供医疗解释、引用证据并整合检索增强框架（retrieval augmented framework）和外部工具，以辅助临床决策。相比之下，通用的AI模型用于端到端临床任务可能存在隐患，需要更多研究和“人类在循环”（human in the loop）机制，同时应用实施科学原则确保系统被医疗团队接受。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16902v1",
      "published_date": "2024-06-23 15:01:11 UTC",
      "updated_date": "2024-06-23 15:01:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:46:29.471185"
    },
    {
      "arxiv_id": "2406.16125v1",
      "title": "CBPF: Filtering Poisoned Data Based on Composite Backdoor Attack",
      "title_zh": "CBPF：基于复合后门攻击的污染数据过滤",
      "authors": [
        "Hanfeng Xia",
        "Haibo Hong",
        "Ruili Wang"
      ],
      "abstract": "Backdoor attacks involve the injection of a limited quantity of poisoned\nexamples containing triggers into the training dataset. During the inference\nstage, backdoor attacks can uphold a high level of accuracy for normal\nexamples, yet when presented with trigger-containing instances, the model may\nerroneously predict them as the targeted class designated by the attacker. This\npaper explores strategies for mitigating the risks associated with backdoor\nattacks by examining the filtration of poisoned samples.We primarily leverage\ntwo key characteristics of backdoor attacks: the ability for multiple backdoors\nto exist simultaneously within a single model, and the discovery through\nComposite Backdoor Attack (CBA) that altering two triggers in a sample to new\ntarget labels does not compromise the original functionality of the triggers,\nyet enables the prediction of the data as a new target class when both triggers\nare present simultaneously.Therefore, a novel three-stage poisoning data\nfiltering approach, known as Composite Backdoor Poison Filtering (CBPF), is\nproposed as an effective solution. Firstly, utilizing the identified\ndistinctions in output between poisoned and clean samples, a subset of data is\npartitioned to include both poisoned and clean instances. Subsequently, benign\ntriggers are incorporated and labels are adjusted to create new target and\nbenign target classes, thereby prompting the poisoned and clean data to be\nclassified as distinct entities during the inference stage. The experimental\nresults indicate that CBPF is successful in filtering out malicious data\nproduced by six advanced attacks on CIFAR10 and ImageNet-12. On average, CBPF\nattains a notable filtering success rate of 99.91% for the six attacks on\nCIFAR10. Additionally, the model trained on the uncontaminated samples exhibits\nsustained high accuracy levels.",
      "tldr_zh": "这篇论文针对后门攻击（backdoor attacks）提出了一种新型过滤方法，名为 CBPF（Composite Backdoor Poison Filtering），旨在通过利用 Composite Backdoor Attack (CBA) 的特性（如多个后门共存和触发器修改不影响原功能）来区分并过滤训练数据中的毒化样本。CBPF 采用三阶段过程：首先基于毒化样本和干净样本的输出差异划分子集；其次添加良性触发器并调整标签，使毒化数据在推理阶段被识别为独立实体；最后通过实验验证其有效性。结果显示，在 CIFAR10 和 ImageNet-12 数据集上，CBPF 对六种高级攻击的平均过滤成功率高达 99.91%，同时保持了模型在干净样本上的高准确率。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16125v1",
      "published_date": "2024-06-23 14:37:24 UTC",
      "updated_date": "2024-06-23 14:37:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:46:42.469843"
    },
    {
      "arxiv_id": "2406.16121v2",
      "title": "Diffusion Spectral Representation for Reinforcement Learning",
      "title_zh": "扩散谱表示用于强化学习",
      "authors": [
        "Dmitry Shribak",
        "Chen-Xiao Gao",
        "Yitong Li",
        "Chenjun Xiao",
        "Bo Dai"
      ],
      "abstract": "Diffusion-based models have achieved notable empirical successes in\nreinforcement learning (RL) due to their expressiveness in modeling complex\ndistributions. Despite existing methods being promising, the key challenge of\nextending existing methods for broader real-world applications lies in the\ncomputational cost at inference time, i.e., sampling from a diffusion model is\nconsiderably slow as it often requires tens to hundreds of iterations to\ngenerate even one sample. To circumvent this issue, we propose to leverage the\nflexibility of diffusion models for RL from a representation learning\nperspective. In particular, by exploiting the connection between diffusion\nmodels and energy-based models, we develop Diffusion Spectral Representation\n(Diff-SR), a coherent algorithm framework that enables extracting sufficient\nrepresentations for value functions in Markov decision processes (MDP) and\npartially observable Markov decision processes (POMDP). We further demonstrate\nhow Diff-SR facilitates efficient policy optimization and practical algorithms\nwhile explicitly bypassing the difficulty and inference cost of sampling from\nthe diffusion model. Finally, we provide comprehensive empirical studies to\nverify the benefits of Diff-SR in delivering robust and advantageous\nperformance across various benchmarks with both fully and partially observable\nsettings.",
      "tldr_zh": "这篇论文针对强化学习（RL）中扩散模型的采样效率问题，提出了一种新的框架：Diffusion Spectral Representation (Diff-SR)。该框架通过探索扩散模型与能量模型（energy-based models）的联系，提取Markov decision processes (MDP) 和 partially observable Markov decision processes (POMDP) 中的值函数表示，从而实现高效的政策优化，而无需进行高成本的采样过程。实验结果表明，Diff-SR 在各种基准测试中表现出色，提供稳健且优势明显的性能，尤其适用于完全可观察和部分可观察场景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.16121v2",
      "published_date": "2024-06-23 14:24:14 UTC",
      "updated_date": "2024-11-01 16:30:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:46:52.923025"
    },
    {
      "arxiv_id": "2406.16111v1",
      "title": "Multi-Scale Temporal Difference Transformer for Video-Text Retrieval",
      "title_zh": "多尺度时序差分 Transformer 用于视频-文本检索",
      "authors": [
        "Ni Wang",
        "Dongliang Liao",
        "Xing Xu"
      ],
      "abstract": "Currently, in the field of video-text retrieval, there are many\ntransformer-based methods. Most of them usually stack frame features and\nregrade frames as tokens, then use transformers for video temporal modeling.\nHowever, they commonly neglect the inferior ability of the transformer modeling\nlocal temporal information. To tackle this problem, we propose a transformer\nvariant named Multi-Scale Temporal Difference Transformer (MSTDT). MSTDT mainly\naddresses the defects of the traditional transformer which has limited ability\nto capture local temporal information. Besides, in order to better model the\ndetailed dynamic information, we make use of the difference feature between\nframes, which practically reflects the dynamic movement of a video. We extract\nthe inter-frame difference feature and integrate the difference and frame\nfeature by the multi-scale temporal transformer. In general, our proposed MSTDT\nconsists of a short-term multi-scale temporal difference transformer and a\nlong-term temporal transformer. The former focuses on modeling local temporal\ninformation, the latter aims at modeling global temporal information. At last,\nwe propose a new loss to narrow the distance of similar samples. Extensive\nexperiments show that backbone, such as CLIP, with MSTDT has attained a new\nstate-of-the-art result.",
      "tldr_zh": "在视频-文本检索领域，现有的Transformer方法通常通过堆叠帧特征来建模时序信息，但忽略了对局部时序信息的捕捉不足。论文提出Multi-Scale Temporal Difference Transformer (MSTDT)，一种新型Transformer变体，通过提取帧间差分特征并整合多尺度时序建模，来更好地捕捉视频的动态运动。MSTDT 包括短时多尺度时序差分Transformer（专注于局部信息）和长时时序Transformer（专注于全局信息），并引入一种新损失函数来缩小相似样本的距离。实验结果表明，使用MSTDT的骨干模型如CLIP 达到了新的最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16111v1",
      "published_date": "2024-06-23 13:59:31 UTC",
      "updated_date": "2024-06-23 13:59:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:47:05.664726"
    },
    {
      "arxiv_id": "2406.16106v1",
      "title": "Evaluating Ensemble Methods for News Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Gray",
        "Noorhan Abbas"
      ],
      "abstract": "News recommendation is crucial for facilitating individuals' access to\narticles, particularly amid the increasingly digital landscape of news\nconsumption. Consequently, extensive research is dedicated to News Recommender\nSystems (NRS) with increasingly sophisticated algorithms. Despite this\nsustained scholarly inquiry, there exists a notable research gap regarding the\npotential synergy achievable by amalgamating these algorithms to yield superior\noutcomes. This paper endeavours to address this gap by demonstrating how\nensemble methods can be used to combine many diverse state-of-the-art\nalgorithms to achieve superior results on the Microsoft News dataset (MIND).\nAdditionally, we identify scenarios where ensemble methods fail to improve\nresults and offer explanations for this occurrence. Our findings demonstrate\nthat a combination of NRS algorithms can outperform individual algorithms,\nprovided that the base learners are sufficiently diverse, with improvements of\nup to 5\\% observed for an ensemble consisting of a content-based BERT approach\nand the collaborative filtering LSTUR algorithm. Additionally, our results\ndemonstrate the absence of any improvement when combining insufficiently\ndistinct methods. These findings provide insight into successful approaches of\nensemble methods in NRS and advocates for the development of better systems\nthrough appropriate ensemble solutions.",
      "tldr_zh": "本研究评估了集成方法（ensemble methods）在新闻推荐系统（NRS）中的应用，旨在通过结合多种最先进算法（如基于内容的 BERT 和协作过滤的 LSTUR）来提升推荐性能。研究在 Microsoft News 数据集（MIND）上进行实验，发现当基础学习器足够多样时，集成方法可使性能提升高达 5%。然而，如果算法差异不足，集成方法无法带来改善，并提供了相应的解释。这些发现为开发更有效的 NRS 提供了见解，并倡导通过合适的集成解决方案优化系统。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16106v1",
      "published_date": "2024-06-23 13:40:50 UTC",
      "updated_date": "2024-06-23 13:40:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:47:18.904956"
    },
    {
      "arxiv_id": "2406.16093v1",
      "title": "Towards Natural Language-Driven Assembly Using Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Omkar Joglekar",
        "Tal Lancewicki",
        "Shir Kozlovsky",
        "Vladimir Tchuiev",
        "Zohar Feldman",
        "Dotan Di Castro"
      ],
      "abstract": "Large Language Models (LLMs) and strong vision models have enabled rapid\nresearch and development in the field of Vision-Language-Action models that\nenable robotic control. The main objective of these methods is to develop a\ngeneralist policy that can control robots with various embodiments. However, in\nindustrial robotic applications such as automated assembly and disassembly,\nsome tasks, such as insertion, demand greater accuracy and involve intricate\nfactors like contact engagement, friction handling, and refined motor skills.\nImplementing these skills using a generalist policy is challenging because\nthese policies might integrate further sensory data, including force or torque\nmeasurements, for enhanced precision. In our method, we present a global\ncontrol policy based on LLMs that can transfer the control policy to a finite\nset of skills that are specifically trained to perform high-precision tasks\nthrough dynamic context switching. The integration of LLMs into this framework\nunderscores their significance in not only interpreting and processing language\ninputs but also in enriching the control mechanisms for diverse and intricate\nrobotic operations.",
      "tldr_zh": "该论文探讨了利用Large Language Models (LLMs) 和视觉模型驱动机器人装配任务，旨在开发通用控制策略以适应各种机器人实施。作者提出一种基于LLMs的全局控制策略，通过动态上下文切换，将策略转移到一组专门训练的高精度技能中，以处理工业任务如插入操作时涉及的接触、摩擦和精细运动挑战。该方法强调LLMs不仅用于语言输入解释，还能增强控制机制，提高机器人操作的准确性和多样性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16093v1",
      "published_date": "2024-06-23 12:14:37 UTC",
      "updated_date": "2024-06-23 12:14:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:47:30.972306"
    },
    {
      "arxiv_id": "2406.16087v5",
      "title": "Imperative Learning: A Self-supervised Neuro-Symbolic Learning Framework for Robot Autonomy",
      "title_zh": "翻译失败",
      "authors": [
        "Chen Wang",
        "Kaiyi Ji",
        "Junyi Geng",
        "Zhongqiang Ren",
        "Taimeng Fu",
        "Fan Yang",
        "Yifan Guo",
        "Haonan He",
        "Xiangyu Chen",
        "Zitong Zhan",
        "Qiwei Du",
        "Shaoshu Su",
        "Bowen Li",
        "Yuheng Qiu",
        "Yi Du",
        "Qihang Li",
        "Yifan Yang",
        "Xiao Lin",
        "Zhipeng Zhao"
      ],
      "abstract": "Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneuro-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.",
      "tldr_zh": "本文提出了一种自监督的 Neuro-Symbolic 学习框架 Imperative Learning (IL)，旨在解决数据驱动方法（如强化学习和模仿学习）在机器人自治中的泛化问题和数据收集挑战。IL 框架由 neural module、reasoning engine 和 memory system 三个组件组成，通过 bilevel optimization (BLO) 实现模块间的相互学习，并利用符号推理（如逻辑推理、物理原则和几何分析）来增强性能。在路径规划、规则归纳、最优控制、视觉里程计和多机器人路由等五个任务的实验中，IL 显著提升了机器人自治能力，并有望推动更多领域的创新研究。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16087v5",
      "published_date": "2024-06-23 12:02:17 UTC",
      "updated_date": "2025-01-25 04:11:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:47:44.527917"
    },
    {
      "arxiv_id": "2406.17807v5",
      "title": "Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary",
      "title_zh": "翻译失败",
      "authors": [
        "Meiling Tao",
        "Xuechen Liang",
        "Xinyuan Song",
        "Yangfan He",
        "Yiling Tao",
        "Jianhui Wang",
        "Sun Li Tianyu Shi"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have unlocked the\npotential for generating high-quality game commentary. However, producing\ninsightful and engaging commentary for complex games with incomplete\ninformation remains a significant challenge. In this paper, we introduce a\nnovel commentary method that combine Reinforcement Learning (RL) and LLMs,\ntailored specifically for the Chinese card game \\textit{Guandan}. Our system\nleverages RL to generate intricate card-playing scenarios and employs LLMs to\ngenerate corresponding commentary text, effectively emulating the strategic\nanalysis and narrative prowess of professional commentators. The framework\ncomprises a state commentary guide, a Theory of Mind (ToM)-based strategy\nanalyzer, and a style retrieval module, which seamlessly collaborate to deliver\ndetailed and context-relevant game commentary in the Chinese language\nenvironment. We empower LLMs with ToM capabilities and refine both retrieval\nand information filtering mechanisms. This facilitates the generation of\npersonalized commentary content. Our experimental results showcase the\nsubstantial enhancement in performance achieved by the proposed commentary\nframework when applied to open-source LLMs, surpassing the performance of GPT-4\nacross multiple evaluation metrics.",
      "tldr_zh": "该研究针对不完美信息纸牌游戏的评论挑战，提出了一种结合强化学习（RL）和大型语言模型（LLMs）的创新方法，专门应用于中国纸牌游戏 Guandan。该框架利用 RL 生成复杂的卡牌场景，并通过 LLMs 结合 Theory of Mind (ToM)-based 策略分析器和风格检索模块，生成个性化、战略性的中文游戏评论，从而模拟专业评论员的分析能力。实验结果表明，该方法显著提升了开源 LLMs 的性能，在多个评估指标上超越 GPT-4，为复杂游戏评论生成提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.17807v5",
      "published_date": "2024-06-23 11:58:26 UTC",
      "updated_date": "2025-04-15 15:28:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:47:55.709889"
    },
    {
      "arxiv_id": "2406.16079v1",
      "title": "EERPD: Leveraging Emotion and Emotion Regulation for Improving Personality Detection",
      "title_zh": "EERPD：利用情感和情感调节来改善个性检测",
      "authors": [
        "Zheng Li",
        "Dawei Zhu",
        "Qilong Ma",
        "Weimin Xiong",
        "Sujian Li"
      ],
      "abstract": "Personality is a fundamental construct in psychology, reflecting an\nindividual's behavior, thinking, and emotional patterns. Previous researches\nhave made some progress in personality detection, primarily by utilizing the\nwhole text to predict personality. However, these studies generally tend to\noverlook psychological knowledge: they rarely apply the well-established\ncorrelations between emotion regulation and personality. Based on this, we\npropose a new personality detection method called EERPD. This method introduces\nthe use of emotion regulation, a psychological concept highly correlated with\npersonality, for personality prediction. By combining this feature with emotion\nfeatures, it retrieves few-shot examples and provides process CoTs for\ninferring labels from text. This approach enhances the understanding of LLM for\npersonality within text and improves the performance in personality detection.\nExperimental results demonstrate that EERPD significantly enhances the accuracy\nand robustness of personality detection, outperforming previous SOTA by\n15.05/4.29 in average F1 on the two benchmark datasets.",
      "tldr_zh": "本文提出 EERPD 方法，通过整合情绪（emotion）和情绪调节（emotion regulation）特征来提升个性检测的性能，解决现有方法忽略心理知识相关性的问题。该方法结合少样本检索和过程式思维链（process CoTs），从文本中推断个性标签，并增强大语言模型（LLM）的理解能力。实验结果显示，EERPD 在两个基准数据集上平均 F1 分数比现有最先进方法提高了 15.05 和 4.29，提升了检测的准确性和鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16079v1",
      "published_date": "2024-06-23 11:18:55 UTC",
      "updated_date": "2024-06-23 11:18:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:48:06.862797"
    },
    {
      "arxiv_id": "2406.16069v3",
      "title": "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models",
      "title_zh": "FastMem：快速提示记忆提升大型语言模型的上下文感知",
      "authors": [
        "Junyi Zhu",
        "Shuochen Liu",
        "Yu Yu",
        "Bo Tang",
        "Yibo Yan",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Tong Xu",
        "Matthew B. Blaschko"
      ],
      "abstract": "Large language models (LLMs) excel in generating coherent text, but they\noften struggle with context awareness, leading to inaccuracies in tasks\nrequiring faithful adherence to provided information. We introduce FastMem, a\nnovel method designed to enhance instruction fine-tuned LLMs' context awareness\nthrough fast memorization of the prompt. FastMem maximizes the likelihood of\nthe prompt before inference by updating only the last Feed-Forward Network\n(FFN) module. This targeted approach ensures efficient optimization without\noverfitting, significantly improving the model's ability to comprehend and\naccurately follow the context. Our experiments demonstrate substantial gains in\nreading comprehension, text summarization and adherence to output structures.\nFor instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAP\ndataset from 59.1% to 71.6%, and reduces the output structure failure rate of\nQwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlight\nFastMem's potential to offer a robust solution to enhance the reliability and\naccuracy of LLMs in various applications. Our code is available at:\nhttps://github.com/IAAR-Shanghai/FastMem",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 在上下文意识方面的不足（如任务执行不准确），提出了一种名为 FastMem 的新方法，通过快速记忆提示来提升指令微调后的 LLMs 性能。FastMem 在推理前最大化提示的似然，仅更新最后一个 Feed-Forward Network (FFN) 模块，从而实现高效优化而不导致过拟合。实验结果显示，该方法显著改善了阅读理解、文本摘要和输出结构遵守，例如 Llama 3-8B-Inst 在 NQ-SWAP 数据集上的准确率从 59.1% 提高到 71.6%，Qwen 1.5-4B-Chat 的输出结构失败率从 34.9% 降至 25.5%。总体而言，FastMem 为增强 LLMs 的可靠性和准确性提供了高效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16069v3",
      "published_date": "2024-06-23 10:36:35 UTC",
      "updated_date": "2024-10-04 19:14:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:48:20.654781"
    },
    {
      "arxiv_id": "2406.16068v1",
      "title": "Towards Real-Time Neural Volumetric Rendering on Mobile Devices: A Measurement Study",
      "title_zh": "翻译失败",
      "authors": [
        "Zhe Wang",
        "Yifei Zhu"
      ],
      "abstract": "Neural Radiance Fields (NeRF) is an emerging technique to synthesize 3D\nobjects from 2D images with a wide range of potential applications. However,\nrendering existing NeRF models is extremely computation intensive, making it\nchallenging to support real-time interaction on mobile devices. In this paper,\nwe take the first initiative to examine the state-of-the-art real-time NeRF\nrendering technique from a system perspective. We first define the entire\nworking pipeline of the NeRF serving system. We then identify possible control\nknobs that are critical to the system from the communication, computation, and\nvisual performance perspective. Furthermore, an extensive measurement study is\nconducted to reveal the effects of these control knobs on system performance.\nOur measurement results reveal that different control knobs contribute\ndifferently towards improving the system performance, with the mesh granularity\nbeing the most effective knob and the quantization being the least effective\nknob. In addition, diverse hardware device settings and network conditions have\nto be considered to fully unleash the benefit of operating under the\nappropriate knobs",
      "tldr_zh": "这篇论文探讨了Neural Radiance Fields (NeRF)技术在移动设备上实现实时3D渲染的挑战，强调了现有模型的计算密集性导致的实时交互难题。研究者从系统视角分析了最先进的实时NeRF渲染技术，定义了整个NeRF服务系统的流程，并识别了通信、计算和视觉性能方面的关键control knobs。测量研究结果显示，不同control knobs对系统性能的影响各异，其中mesh granularity是最有效的优化手段，而quantization的效果最小；此外，需结合多样化的硬件设备设置和网络条件来最大化性能收益。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.GR",
        "cs.MM",
        "cs.PF"
      ],
      "primary_category": "cs.DC",
      "comment": "This paper is accepted by ACM SIGCOMM Workshop on Emerging Multimedia\n  Systems 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.16068v1",
      "published_date": "2024-06-23 10:33:26 UTC",
      "updated_date": "2024-06-23 10:33:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:48:30.281983"
    },
    {
      "arxiv_id": "2407.09523v1",
      "title": "MuseCL: Predicting Urban Socioeconomic Indicators via Multi-Semantic Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xixian Yong",
        "Xiao Zhou"
      ],
      "abstract": "Predicting socioeconomic indicators within urban regions is crucial for\nfostering inclusivity, resilience, and sustainability in cities and human\nsettlements. While pioneering studies have attempted to leverage multi-modal\ndata for socioeconomic prediction, jointly exploring their underlying semantics\nremains a significant challenge. To address the gap, this paper introduces a\nMulti-Semantic Contrastive Learning (MuseCL) framework for fine-grained urban\nregion profiling and socioeconomic prediction. Within this framework, we\ninitiate the process by constructing contrastive sample pairs for street view\nand remote sensing images, capitalizing on the similarities in human mobility\nand Point of Interest (POI) distribution to derive semantic features from the\nvisual modality. Additionally, we extract semantic insights from POI texts\nembedded within these regions, employing a pre-trained text encoder. To merge\nthe acquired visual and textual features, we devise an innovative\ncross-modality-based attentional fusion module, which leverages a contrastive\nmechanism for integration. Experimental results across multiple cities and\nindicators consistently highlight the superiority of MuseCL, demonstrating an\naverage improvement of 10% in $R^2$ compared to various competitive baseline\nmodels. The code of this work is publicly available at\nhttps://github.com/XixianYong/MuseCL.",
      "tldr_zh": "这篇论文提出了 MuseCL 框架，通过 Multi-Semantic Contrastive Learning 来预测城市区域的社会经济指标，以促进城市的包容性、韧性和可持续性。框架首先构建街景和遥感图像的对比样本对，利用人类流动性和 Point of Interest (POI) 分布提取视觉语义特征，并从 POI 文本中提取语义洞见，使用预训练文本编码器进行处理；随后，通过一个创新的跨模态注意力融合模块（attentional fusion module）整合视觉和文本特征。实验结果显示，MuseCL 在多个城市和指标上比竞争基线模型平均提高了 10% 的 R² 值，证明了其在细粒度城市区域分析中的优越性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.09523v1",
      "published_date": "2024-06-23 09:49:41 UTC",
      "updated_date": "2024-06-23 09:49:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:48:44.037819"
    },
    {
      "arxiv_id": "2406.16045v1",
      "title": "Combine and Conquer: A Meta-Analysis on Data Shift and Out-of-Distribution Detection",
      "title_zh": "Combine and Conquer：关于数据偏移和分布外检测的元分析",
      "authors": [
        "Eduardo Dadalto",
        "Florence Alberge",
        "Pierre Duhamel",
        "Pablo Piantanida"
      ],
      "abstract": "This paper introduces a universal approach to seamlessly combine\nout-of-distribution (OOD) detection scores. These scores encompass a wide range\nof techniques that leverage the self-confidence of deep learning models and the\nanomalous behavior of features in the latent space. Not surprisingly, combining\nsuch a varied population using simple statistics proves inadequate. To overcome\nthis challenge, we propose a quantile normalization to map these scores into\np-values, effectively framing the problem into a multi-variate hypothesis test.\nThen, we combine these tests using established meta-analysis tools, resulting\nin a more effective detector with consolidated decision boundaries.\nFurthermore, we create a probabilistic interpretable criterion by mapping the\nfinal statistics into a distribution with known parameters. Through empirical\ninvestigation, we explore different types of shifts, each exerting varying\ndegrees of impact on data. Our results demonstrate that our approach\nsignificantly improves overall robustness and performance across diverse OOD\ndetection scenarios. Notably, our framework is easily extensible for future\ndevelopments in detection scores and stands as the first to combine decision\nboundaries in this context. The code and artifacts associated with this work\nare publicly available\\footnote{\\url{https://github.com/edadaltocg/detectors}}.",
      "tldr_zh": "这篇论文提出了一种通用方法，通过元分析工具无缝结合 out-of-distribution (OOD) 检测分数，以提升深度学习模型在数据偏移场景下的检测性能。具体而言，该方法使用 quantile normalization 将检测分数映射为 p-values，并将其转化为多变量假设测试，从而整合决策边界并创建可解释的概率标准。实验结果显示，该框架显著提高了 OOD 检测的鲁棒性和整体性能，尤其在不同类型的数据 shifts 中，并作为首个易扩展的结合决策边界方案，代码已公开可用。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "Accepted for publication in Transactions on Machine Learning Research\n  (TMLR)",
      "pdf_url": "http://arxiv.org/pdf/2406.16045v1",
      "published_date": "2024-06-23 08:16:44 UTC",
      "updated_date": "2024-06-23 08:16:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:48:55.348980"
    },
    {
      "arxiv_id": "2407.09522v2",
      "title": "UQE: A Query Engine for Unstructured Databases",
      "title_zh": "翻译失败",
      "authors": [
        "Hanjun Dai",
        "Bethany Yixin Wang",
        "Xingchen Wan",
        "Bo Dai",
        "Sherry Yang",
        "Azade Nova",
        "Pengcheng Yin",
        "Phitchaya Mangpo Phothilimthana",
        "Charles Sutton",
        "Dale Schuurmans"
      ],
      "abstract": "Analytics on structured data is a mature field with many successful methods.\nHowever, most real world data exists in unstructured form, such as images and\nconversations. We investigate the potential of Large Language Models (LLMs) to\nenable unstructured data analytics. In particular, we propose a new Universal\nQuery Engine (UQE) that directly interrogates and draws insights from\nunstructured data collections. This engine accepts queries in a Universal Query\nLanguage (UQL), a dialect of SQL that provides full natural language\nflexibility in specifying conditions and operators. The new engine leverages\nthe ability of LLMs to conduct analysis of unstructured data, while also\nallowing us to exploit advances in sampling and optimization techniques to\nachieve efficient and accurate query execution. In addition, we borrow\ntechniques from classical compiler theory to better orchestrate the workflow\nbetween sampling methods and foundation model calls. We demonstrate the\nefficiency of UQE on data analytics across different modalities, including\nimages, dialogs and reviews, across a range of useful query types, including\nconditional aggregation, semantic retrieval and abstraction aggregation.",
      "tldr_zh": "该论文探讨了非结构化数据（如图像和对话）分析的挑战，因为现有方法主要针对结构化数据。研究提出Universal Query Engine (UQE)，一种利用Large Language Models (LLMs)直接查询和分析非结构化数据集合的引擎，并引入Universal Query Language (UQL)，这是一种SQL方言，支持自然语言灵活指定条件和操作符。UQE结合采样、优化技术和编译器理论来优化工作流，确保高效准确的查询执行；在图像、对话和评论等模态上，实验证明UQE在条件聚合、语义检索和抽象聚合等查询类型中表现出色。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.09522v2",
      "published_date": "2024-06-23 06:58:55 UTC",
      "updated_date": "2024-11-17 02:22:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:49:05.764019"
    },
    {
      "arxiv_id": "2406.16030v2",
      "title": "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Jimin Sohn",
        "Haeji Jung",
        "Alex Cheng",
        "Jooeon Kang",
        "Yilin Du",
        "David R. Mortensen"
      ],
      "abstract": "Existing zero-shot cross-lingual NER approaches require substantial prior\nknowledge of the target language, which is impractical for low-resource\nlanguages. In this paper, we propose a novel approach to NER using phonemic\nrepresentation based on the International Phonetic Alphabet (IPA) to bridge the\ngap between representations of different languages. Our experiments show that\nour method significantly outperforms baseline models in extremely low-resource\nlanguages, with the highest average F1 score (46.38%) and lowest standard\ndeviation (12.67), particularly demonstrating its robustness with non-Latin\nscripts. Our codes are available at\nhttps://github.com/Gabriel819/zeroshot_ner.git",
      "tldr_zh": "本文提出了一种Zero-Shot Cross-Lingual NER方法，使用基于International Phonetic Alphabet (IPA)的Phonemic Representations来桥接不同语言的表示，从而无需目标语言的先验知识，便于应用于低资源语言。实验结果显示，该方法在极低资源语言上显著优于基线模型，平均F1分数达到46.38%，标准差最低为12.67%，并在非拉丁脚本语言中表现出色稳定性。该方法为跨语言NER任务提供了更鲁棒的解决方案，并已在GitHub上开源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2406.16030v2",
      "published_date": "2024-06-23 06:38:56 UTC",
      "updated_date": "2024-10-22 01:31:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:49:19.287899"
    },
    {
      "arxiv_id": "2406.16028v2",
      "title": "TimeAutoDiff: Combining Autoencoder and Diffusion model for time series tabular data synthesizing",
      "title_zh": "TimeAutoDiff：",
      "authors": [
        "Namjoon Suh",
        "Yuning Yang",
        "Din-Yin Hsieh",
        "Qitong Luan",
        "Shirong Xu",
        "Shixiang Zhu",
        "Guang Cheng"
      ],
      "abstract": "In this paper, we leverage the power of latent diffusion models to generate\nsynthetic time series tabular data. Along with the temporal and feature\ncorrelations, the heterogeneous nature of the feature in the table has been one\nof the main obstacles in time series tabular data modeling. We tackle this\nproblem by combining the ideas of the variational auto-encoder (VAE) and the\ndenoising diffusion probabilistic model (DDPM). Our model named as\n\\texttt{TimeAutoDiff} has several key advantages including (1) Generality: the\nability to handle the broad spectrum of time series tabular data from single to\nmulti-sequence datasets; (2) Good fidelity and utility guarantees: numerical\nexperiments on six publicly available datasets demonstrating significant\nimprovements over state-of-the-art models in generating time series tabular\ndata, across four metrics measuring fidelity and utility; (3) Fast sampling\nspeed: entire time series data generation as opposed to the sequential data\nsampling schemes implemented in the existing diffusion-based models, eventually\nleading to significant improvements in sampling speed, (4) Entity conditional\ngeneration: the first implementation of conditional generation of\nmulti-sequence time series tabular data with heterogenous features in the\nliterature, enabling scenario exploration across multiple scientific and\nengineering domains. Codes are in preparation for release to the public, but\navailable upon request.",
      "tldr_zh": "本论文提出 TimeAutoDiff 模型，通过结合 Variational Auto-Encoder (VAE) 和 Denoising Diffusion Probabilistic Model (DDPM)，来生成合成的时间序列表格数据，解决了时间和特征相关性以及特征异构性的挑战。实验结果显示，该模型在六个公开数据集上，比现有最先进模型在四个保真度和实用性指标上显著提升，同时实现了更快的整体采样速度，而非顺序采样。TimeAutoDiff 还首次实现了实体条件生成，支持多序列时间序列表格数据的条件生成，适用于多个科学和工程领域的场景探索。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16028v2",
      "published_date": "2024-06-23 06:32:27 UTC",
      "updated_date": "2024-07-15 04:36:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:49:31.360838"
    },
    {
      "arxiv_id": "2406.16976v3",
      "title": "Efficient Evolutionary Search Over Chemical Space with Large Language Models",
      "title_zh": "利用大型语言模型的化学空间高效进化搜索",
      "authors": [
        "Haorui Wang",
        "Marta Skreta",
        "Cher-Tian Ser",
        "Wenhao Gao",
        "Lingkai Kong",
        "Felix Strieth-Kalthoff",
        "Chenru Duan",
        "Yuchen Zhuang",
        "Yue Yu",
        "Yanqiao Zhu",
        "Yuanqi Du",
        "Alán Aspuru-Guzik",
        "Kirill Neklyudov",
        "Chao Zhang"
      ],
      "abstract": "Molecular discovery, when formulated as an optimization problem, presents\nsignificant computational challenges because optimization objectives can be\nnon-differentiable. Evolutionary Algorithms (EAs), often used to optimize\nblack-box objectives in molecular discovery, traverse chemical space by\nperforming random mutations and crossovers, leading to a large number of\nexpensive objective evaluations. In this work, we ameliorate this shortcoming\nby incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely,\nwe redesign crossover and mutation operations in EAs using LLMs trained on\nlarge corpora of chemical information. We perform extensive empirical studies\non both commercial and open-source models on multiple tasks involving property\noptimization, molecular rediscovery, and structure-based drug design,\ndemonstrating that the joint usage of LLMs with EAs yields superior performance\nover all baseline models across single- and multi-objective settings. We\ndemonstrate that our algorithm improves both the quality of the final solution\nand convergence speed, thereby reducing the number of required objective\nevaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO",
      "tldr_zh": "本研究针对分子发现优化问题中的计算挑战（如非微分目标和进化算法EAs的低效随机操作），提出了一种将大语言模型LLMs整合到EAs中的方法，通过利用LLMs的化学知识重新设计交叉和变异操作，从而减少昂贵的目标评估次数。实验在属性优化、分子重新发现和结构基于药物设计等任务上，使用商业和开源模型进行测试，结果显示该方法在单目标和多目标设置下均优于基线模型，提高了最终解决方案的质量和收敛速度。总之，该工作证明了LLMs增强EAs的潜力，并提供了开源代码以促进进一步应用。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG",
        "physics.chem-ph"
      ],
      "primary_category": "cs.NE",
      "comment": "Published in ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2406.16976v3",
      "published_date": "2024-06-23 06:22:49 UTC",
      "updated_date": "2025-03-07 17:24:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:49:43.809297"
    },
    {
      "arxiv_id": "2406.16021v1",
      "title": "Harvesting Events from Multiple Sources: Towards a Cross-Document Event Extraction Paradigm",
      "title_zh": "翻译失败",
      "authors": [
        "Qiang Gao",
        "Zixiang Meng",
        "Bobo Li",
        "Jun Zhou",
        "Fei Li",
        "Chong Teng",
        "Donghong Ji"
      ],
      "abstract": "Document-level event extraction aims to extract structured event information\nfrom unstructured text. However, a single document often contains limited event\ninformation and the roles of different event arguments may be biased due to the\ninfluence of the information source. This paper addresses the limitations of\ntraditional document-level event extraction by proposing the task of\ncross-document event extraction (CDEE) to integrate event information from\nmultiple documents and provide a comprehensive perspective on events. We\nconstruct a novel cross-document event extraction dataset, namely CLES, which\ncontains 20,059 documents and 37,688 mention-level events, where over 70% of\nthem are cross-document. To build a benchmark, we propose a CDEE pipeline that\nincludes 5 steps, namely event extraction, coreference resolution, entity\nnormalization, role normalization and entity-role resolution. Our CDEE pipeline\nachieves about 72% F1 in end-to-end cross-document event extraction, suggesting\nthe challenge of this task. Our work builds a new line of information\nextraction research and will attract new research attention.",
      "tldr_zh": "这篇论文提出跨文档事件提取（CDEE）任务，以整合多文档中的事件信息，解决传统文档级事件提取（document-level event extraction）受限于单一文档信息和角色偏倚的问题。作者构建了名为 CLES 的新数据集，包含 20,059 文档和 37,688 事件提及，其中超过 70% 为跨文档事件。论文引入一个五步 CDEE 管道，包括 event extraction、coreference resolution、entity normalization、role normalization 和 entity-role resolution 等步骤，并在端到端任务中达到约 72% F1 分数，展示了任务的挑战性。该工作为信息提取研究开辟新领域，预计将吸引更多关注。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL2024(Findings)",
      "pdf_url": "http://arxiv.org/pdf/2406.16021v1",
      "published_date": "2024-06-23 06:01:11 UTC",
      "updated_date": "2024-06-23 06:01:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:49:55.580059"
    },
    {
      "arxiv_id": "2406.16018v1",
      "title": "Comprehensive characterization of three-qubit Grover search algorithm on IBM's 127-qubit superconducting quantum computers",
      "title_zh": "对三量子比特 Grover 搜索算法在 IBM 127 量子比特超导量子计算机上的全面表征",
      "authors": [
        "M. AbuGhanem"
      ],
      "abstract": "The Grover search algorithm is a pivotal advancement in quantum computing,\npromising a remarkable speedup over classical algorithms in searching\nunstructured large databases. Here, we report results for the implementation\nand characterization of a three-qubit Grover search algorithm using the\nstate-of-the-art scalable quantum computing technology of superconducting\nquantum architectures. To delve into the algorithm's scalability and\nperformance metrics, our investigation spans the execution of the algorithm\nacross all eight conceivable single-result oracles, alongside nine two-result\noracles, employing IBM Quantum's 127-qubit quantum computers. Moreover, we\nconduct five quantum state tomography experiments to precisely gauge the\nbehavior and efficiency of our implemented algorithm under diverse conditions;\nranging from noisy, noise-free environments to the complexities of real-world\nquantum hardware. By connecting theoretical concepts with real-world\nexperiments, this study not only shed light on the potential of NISQ (Noisy\nIntermediate-Scale Quantum) computers in facilitating large-scale database\nsearches but also offer valuable insights into the practical application of the\nGrover search algorithm in real-world quantum computing applications.",
      "tldr_zh": "本研究对三量子比特 Grover search algorithm 在 IBM 127 量子比特超导量子计算机上的性能进行了全面表征，旨在评估其在无结构大型数据库搜索中的量子加速潜力。研究团队执行了所有八种单结果预言机和九种双结果预言机，并开展了五次量子态层析实验，涵盖从噪声到无噪声环境的多种条件。结果显示，Grover search algorithm 在 NISQ（Noisy Intermediate-Scale Quantum）计算机上表现出色，为实际量子计算应用提供了宝贵见解，并证明了其在大型数据库搜索中的可行性。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CR",
        "cs.DS"
      ],
      "primary_category": "quant-ph",
      "comment": "15 pages, 7 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.16018v1",
      "published_date": "2024-06-23 05:27:46 UTC",
      "updated_date": "2024-06-23 05:27:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:50:06.422064"
    },
    {
      "arxiv_id": "2406.16013v1",
      "title": "Database-Augmented Query Representation for Information Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Soyeong Jeong",
        "Jinheon Baek",
        "Sukmin Cho",
        "Sung Ju Hwang",
        "Jong C. Park"
      ],
      "abstract": "Information retrieval models that aim to search for the documents relevant to\nthe given query have shown many successes, which have been applied to diverse\ntasks. However, the query provided by the user is oftentimes very short, which\nchallenges the retrievers to correctly fetch relevant documents. To tackle\nthis, existing studies have proposed expanding the query with a couple of\nadditional (user-related) features related to the query. Yet, they may be\nsuboptimal to effectively augment the query, though there is plenty of\ninformation available to augment it in a relational database. Motivated by\nthis, we present a novel retrieval framework called Database-Augmented Query\nrepresentation (DAQu), which augments the original query with various\n(query-related) metadata across multiple tables. In addition, as the number of\nfeatures in the metadata can be very large and there is no order among them, we\nencode them with our graph-based set encoding strategy, which considers\nhierarchies of features in the database without order. We validate DAQu in\ndiverse retrieval scenarios that can incorporate metadata from the relational\ndatabase, demonstrating that ours significantly enhances overall retrieval\nperformance, compared to existing query augmentation methods.",
      "tldr_zh": "本研究针对信息检索中用户查询短小导致的文档检索挑战，提出了一种名为 Database-Augmented Query representation (DAQu) 的新框架，该框架利用关系数据库中的各种查询相关元数据来增强原始查询表示。DAQu 采用图-based set encoding 策略来处理元数据的数量大和无序问题，通过考虑数据库特征的层次结构进行编码，从而实现更有效的查询扩展。在多种检索场景中实验验证表明，DAQu 与现有查询增强方法相比，显著提高了整体检索性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16013v1",
      "published_date": "2024-06-23 05:02:21 UTC",
      "updated_date": "2024-06-23 05:02:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:50:18.521164"
    },
    {
      "arxiv_id": "2406.16008v2",
      "title": "Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization",
      "title_zh": "在中发现：校准位置注意力偏差改善长上下文利用",
      "authors": [
        "Cheng-Yu Hsieh",
        "Yung-Sung Chuang",
        "Chun-Liang Li",
        "Zifeng Wang",
        "Long T. Le",
        "Abhishek Kumar",
        "James Glass",
        "Alexander Ratner",
        "Chen-Yu Lee",
        "Ranjay Krishna",
        "Tomas Pfister"
      ],
      "abstract": "Large language models (LLMs), even when specifically trained to process long\ninput contexts, struggle to capture relevant information located in the middle\nof their input. This phenomenon has been known as the lost-in-the-middle\nproblem. In this work, we make three contributions. First, we set out to\nunderstand the factors that cause this phenomenon. In doing so, we establish a\nconnection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs\nexhibit a U-shaped attention bias where the tokens at the beginning and at the\nend of its input receive higher attention, regardless of their relevance.\nSecond, we mitigate this positional bias through a calibration mechanism,\nfound-in-the-middle, that allows the model to attend to contexts faithfully\naccording to their relevance, even though when they are in the middle. Third,\nwe show found-in-the-middle not only achieves better performance in locating\nrelevant information within a long context, but also eventually leads to\nimproved retrieval-augmented generation (RAG) performance across various tasks,\noutperforming existing methods by up to 15 percentage points. These findings\nopen up future directions in understanding LLM attention bias and its potential\nconsequences.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在处理长输入时存在的 lost-in-the-middle 问题，即模型难以捕捉中间位置的相关信息。研究者首先分析了这一现象的根因，发现 LLMs 具有 U-shaped attention bias，导致开头和结尾的 tokens 获得更多关注。针对此，他们提出 found-in-the-middle 机制，通过校准位置偏差，使模型能根据相关性忠实关注上下文。实验结果显示，该方法不仅提升了长上下文信息定位的性能，还在检索增强生成 (RAG) 任务中比现有方法提高了最多 15 个百分点，为理解 LLM attention bias 及其影响提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL Findings 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.16008v2",
      "published_date": "2024-06-23 04:35:42 UTC",
      "updated_date": "2024-07-03 17:40:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:50:31.012145"
    },
    {
      "arxiv_id": "2406.16006v1",
      "title": "Bounding-Box Inference for Error-Aware Model-Based Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Erin J. Talvitie",
        "Zilei Shao",
        "Huiying Li",
        "Jinghan Hu",
        "Jacob Boerma",
        "Rory Zhao",
        "Xintong Wang"
      ],
      "abstract": "In model-based reinforcement learning, simulated experiences from the learned\nmodel are often treated as equivalent to experience from the real environment.\nHowever, when the model is inaccurate, it can catastrophically interfere with\npolicy learning. Alternatively, the agent might learn about the model's\naccuracy and selectively use it only when it can provide reliable predictions.\nWe empirically explore model uncertainty measures for selective planning and\nshow that best results require distribution insensitive inference to estimate\nthe uncertainty over model-based updates. To that end, we propose and evaluate\nbounding-box inference, which operates on bounding-boxes around sets of\npossible states and other quantities. We find that bounding-box inference can\nreliably support effective selective planning.",
      "tldr_zh": "该研究解决了基于模型的强化学习（Model-Based Reinforcement Learning）中模型不准确导致策略学习失败的问题，通过引入模型不确定性措施实现选择性规划。作者提出 bounding-box inference 方法，该方法在可能状态和其他数量的边界框上操作，以对模型更新不确定性进行分布不敏感的估计。实验结果表明，bounding-box inference 能可靠地支持有效的选择性规划，从而提升了代理的鲁棒性和性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear: Reinforcement Learning Conference (RLC), 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.16006v1",
      "published_date": "2024-06-23 04:23:15 UTC",
      "updated_date": "2024-06-23 04:23:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:50:51.417243"
    },
    {
      "arxiv_id": "2406.16000v1",
      "title": "Predicting Individual Depression Symptoms from Acoustic Features During Speech",
      "title_zh": "基于说话过程中的声学特征预测个体抑郁症状",
      "authors": [
        "Sebastian Rodriguez",
        "Sri Harsha Dumpala",
        "Katerina Dikaios",
        "Sheri Rempel",
        "Rudolf Uher",
        "Sageev Oore"
      ],
      "abstract": "Current automatic depression detection systems provide predictions directly\nwithout relying on the individual symptoms/items of depression as denoted in\nthe clinical depression rating scales. In contrast, clinicians assess each item\nin the depression rating scale in a clinical setting, thus implicitly providing\na more detailed rationale for a depression diagnosis. In this work, we make a\nfirst step towards using the acoustic features of speech to predict individual\nitems of the depression rating scale before obtaining the final depression\nprediction. For this, we use convolutional (CNN) and recurrent (long short-term\nmemory (LSTM)) neural networks. We consider different approaches to learning\nthe temporal context of speech. Further, we analyze two variants of voting\nschemes for individual item prediction and depression detection. We also\ninclude an animated visualization that shows an example of item prediction over\ntime as the speech progresses.",
      "tldr_zh": "本研究旨在使用语音的声学特征预测抑郁评定量表中的个体症状，从而为最终抑郁诊断提供更详细的临床依据。研究采用卷积神经网络（CNN）和长短时记忆网络（LSTM）来学习语音的时间上下文，并分析两种投票方案变体，用于个体症状预测和整体抑郁检测。实验还包括一个动画可视化，展示症状预测随讲话时间变化的过程，为自动抑郁检测系统注入更细致的理性解释。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16000v1",
      "published_date": "2024-06-23 03:26:47 UTC",
      "updated_date": "2024-06-23 03:26:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:50:55.190080"
    },
    {
      "arxiv_id": "2406.15996v1",
      "title": "Memorizing Documents with Guidance in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Bumjin Park",
        "Jaesik Choi"
      ],
      "abstract": "Training data plays a pivotal role in AI models. Large language models (LLMs)\nare trained with massive amounts of documents, and their parameters hold\ndocument-related contents. Recently, several studies identified\ncontent-specific locations in LLMs by examining the parameters. Instead of the\npost hoc interpretation, we propose another approach. We propose document-wise\nmemory architecture to track document memories in training. The proposed\narchitecture maps document representations to memory entries, which softly mask\nmemories in the forward process of LLMs. Additionally, we propose document\nguidance loss, which increases the likelihood of text with document memories\nand reduces the likelihood of the text with the memories of other documents.\nExperimental results on Wikitext-103-v1 with Pythia-1B show that the proposed\nmethods provide different memory entries for documents and high recall of\ndocument-related content in generation with trained document-wise memories.",
      "tldr_zh": "本文提出了一种文档-wise memory architecture，用于在Large Language Models (LLMs)训练过程中跟踪和管理文档记忆，该架构通过将文档表示映射到内存条目并使用软掩码来处理前向过程。 additionally，引入了document guidance loss，以增加文档相关文本的生成概率并减少其他文档记忆的干扰。实验结果显示，在Wikitext-103-v1数据集上使用Pythia-1B模型，该方法为不同文档提供了独特的内存条目，并显著提升了文档相关内容的生成召回率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.15996v1",
      "published_date": "2024-06-23 03:12:03 UTC",
      "updated_date": "2024-06-23 03:12:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:51:07.524144"
    },
    {
      "arxiv_id": "2406.15990v1",
      "title": "Enhancing Cross-Document Event Coreference Resolution by Discourse Structure and Semantic Information",
      "title_zh": "通过话语结构和语义信息增强跨文档事件共指解析",
      "authors": [
        "Qiang Gao",
        "Bobo Li",
        "Zixiang Meng",
        "Yunlong Li",
        "Jun Zhou",
        "Fei Li",
        "Chong Teng",
        "Donghong Ji"
      ],
      "abstract": "Existing cross-document event coreference resolution models, which either\ncompute mention similarity directly or enhance mention representation by\nextracting event arguments (such as location, time, agent, and patient),\nlacking the ability to utilize document-level information. As a result, they\nstruggle to capture long-distance dependencies. This shortcoming leads to their\nunderwhelming performance in determining coreference for the events where their\nargument information relies on long-distance dependencies. In light of these\nlimitations, we propose the construction of document-level Rhetorical Structure\nTheory (RST) trees and cross-document Lexical Chains to model the structural\nand semantic information of documents. Subsequently, cross-document\nheterogeneous graphs are constructed and GAT is utilized to learn the\nrepresentations of events. Finally, a pair scorer calculates the similarity\nbetween each pair of events and co-referred events can be recognized using\nstandard clustering algorithm. Additionally, as the existing cross-document\nevent coreference datasets are limited to English, we have developed a\nlarge-scale Chinese cross-document event coreference dataset to fill this gap,\nwhich comprises 53,066 event mentions and 4,476 clusters. After applying our\nmodel on the English and Chinese datasets respectively, it outperforms all\nbaselines by large margins.",
      "tldr_zh": "现有跨文档事件共指解析模型因忽略文档级信息而难以捕捉长距离依赖，导致性能不足。本文提出构建文档级 Rhetorical Structure Theory (RST) 树和跨文档 Lexical Chains，以模型文档的结构和语义信息；随后构建跨文档异构图，并利用 GAT (Graph Attention Network) 学习事件表示，再通过配对评分器和标准聚类算法识别共指事件。该研究还开发了一个大规模中文跨文档事件共指数据集，包含53,066个事件提及和4,476个集群。在英语和中文数据集上的实验中，该模型大幅优于所有基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.15990v1",
      "published_date": "2024-06-23 02:54:48 UTC",
      "updated_date": "2024-06-23 02:54:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:51:22.409871"
    },
    {
      "arxiv_id": "2406.15985v1",
      "title": "Deep-MPC: A DAGGER-Driven Imitation Learning Strategy for Optimal Constrained Battery Charging",
      "title_zh": "翻译失败",
      "authors": [
        "Jorge Espin",
        "Dong Zhang",
        "Daniele Toti",
        "Andrea Pozzi"
      ],
      "abstract": "In the realm of battery charging, several complex aspects demand meticulous\nattention, including thermal management, capacity degradation, and the need for\nrapid charging while maintaining safety and battery lifespan. By employing the\nimitation learning paradigm, this manuscript introduces an innovative solution\nto confront the inherent challenges often associated with conventional\npredictive control strategies for constrained battery charging. A significant\ncontribution of this study lies in the adaptation of the Dataset Aggregation\n(DAGGER) algorithm to address scenarios where battery parameters are uncertain,\nand internal states are unobservable. Results drawn from a practical battery\nsimulator that incorporates an electrochemical model highlight substantial\nimprovements in battery charging performance, particularly in meeting all\nsafety constraints and outperforming traditional strategies in computational\nprocessing.",
      "tldr_zh": "这篇论文提出了Deep-MPC，一种基于模仿学习(Imitation Learning)策略的创新方法，用于优化约束电池充电问题，包括热管理、容量退化以及快速充电的安全性。核心贡献在于适应Dataset Aggregation (DAGGER)算法，以应对电池参数的不确定性和不可观测内部状态，从而提升预测控制的鲁棒性。在基于电化学模型的实际电池模拟器实验中，该方法显著改善了充电性能，确保所有安全约束得到满足，并优于传统策略的计算效率。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "7 pages, 4 figures, submitted to American Control Conference 2024\n  (ACC2024)",
      "pdf_url": "http://arxiv.org/pdf/2406.15985v1",
      "published_date": "2024-06-23 02:36:02 UTC",
      "updated_date": "2024-06-23 02:36:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:51:32.410814"
    },
    {
      "arxiv_id": "2406.15982v1",
      "title": "Learning with Noisy Ground Truth: From 2D Classification to 3D Reconstruction",
      "title_zh": "使用",
      "authors": [
        "Yangdi Lu",
        "Wenbo He"
      ],
      "abstract": "Deep neural networks has been highly successful in data-intense computer\nvision applications, while such success relies heavily on the massive and clean\ndata. In real-world scenarios, clean data sometimes is difficult to obtain. For\nexample, in image classification and segmentation tasks, precise annotations of\nmillions samples are generally very expensive and time-consuming. In 3D static\nscene reconstruction task, most NeRF related methods require the foundational\nassumption of the static scene (e.g. consistent lighting condition and\npersistent object positions), which is often violated in real-world scenarios.\nTo address these problem, learning with noisy ground truth (LNGT) has emerged\nas an effective learning method and shows great potential. In this short\nsurvey, we propose a formal definition unify the analysis of LNGT LNGT in the\ncontext of different machine learning tasks (classification and regression).\nBased on this definition, we propose a novel taxonomy to classify the existing\nwork according to the error decomposition with the fundamental definition of\nmachine learning. Further, we provide in-depth analysis on memorization effect\nand insightful discussion about potential future research opportunities from 2D\nclassification to 3D reconstruction, in the hope of providing guidance to\nfollow-up research.",
      "tldr_zh": "这篇论文探讨了深度神经网络在计算机视觉应用中的成功依赖于大量干净数据，但现实场景中数据噪声问题普遍存在，例如图像分类和分割任务的精确标注成本高，以及3D静态场景重建（如NeRF方法）中静态假设的违背。论文提出一个统一的正式定义来分析学习与噪声地面真实（LNGT），并基于机器学习的根本定义开发了一个新的分类法（taxonomy），将现有工作按错误分解进行分类。同时，它深入分析了记忆效应（memorization effect），并讨论了从2D分类到3D重建的潜在未来研究机会，以为后续研究提供指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Computer vision, Noisy Labels, 3D reconstruction, 3D Gaussian Splats,\n  (Work still in progress)",
      "pdf_url": "http://arxiv.org/pdf/2406.15982v1",
      "published_date": "2024-06-23 02:21:48 UTC",
      "updated_date": "2024-06-23 02:21:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:51:46.444804"
    },
    {
      "arxiv_id": "2406.16975v1",
      "title": "A Review of Global Sensitivity Analysis Methods and a comparative case study on Digit Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Zahra Sadeghi",
        "Stan Matwin"
      ],
      "abstract": "Global sensitivity analysis (GSA) aims to detect influential input factors\nthat lead a model to arrive at a certain decision and is a significant approach\nfor mitigating the computational burden of processing high dimensional data. In\nthis paper, we provide a comprehensive review and a comparison on global\nsensitivity analysis methods. Additionally, we propose a methodology for\nevaluating the efficacy of these methods by conducting a case study on MNIST\ndigit dataset. Our study goes through the underlying mechanism of widely used\nGSA methods and highlights their efficacy through a comprehensive methodology.",
      "tldr_zh": "这篇论文对 Global Sensitivity Analysis (GSA) 方法进行了全面回顾和比较，旨在识别影响模型决策的关键输入因素，从而减轻处理高维数据的计算负担。作者提出了一种评估方法，通过在 MNIST 数字数据集上的案例研究来检验这些方法的有效性。该研究深入探讨了常用 GSA 方法的底层机制，并通过全面方法突出了它们的实际功效。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16975v1",
      "published_date": "2024-06-23 00:38:19 UTC",
      "updated_date": "2024-06-23 00:38:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:51:56.265239"
    },
    {
      "arxiv_id": "2406.15970v1",
      "title": "Imperfect-Recall Games: Equilibrium Concepts and Their Complexity",
      "title_zh": "不完美回忆游戏：均衡概念及其复杂性",
      "authors": [
        "Emanuel Tewolde",
        "Brian Hu Zhang",
        "Caspar Oesterheld",
        "Manolis Zampetakis",
        "Tuomas Sandholm",
        "Paul W. Goldberg",
        "Vincent Conitzer"
      ],
      "abstract": "We investigate optimal decision making under imperfect recall, that is, when\nan agent forgets information it once held before. An example is the\nabsentminded driver game, as well as team games in which the members have\nlimited communication capabilities. In the framework of extensive-form games\nwith imperfect recall, we analyze the computational complexities of finding\nequilibria in multiplayer settings across three different solution concepts:\nNash, multiselves based on evidential decision theory (EDT), and multiselves\nbased on causal decision theory (CDT). We are interested in both exact and\napproximate solution computation. As special cases, we consider (1)\nsingle-player games, (2) two-player zero-sum games and relationships to maximin\nvalues, and (3) games without exogenous stochasticity (chance nodes). We relate\nthese problems to the complexity classes P, PPAD, PLS, $\\Sigma_2^P$ ,\n$\\exists$R, and $\\exists \\forall$R.",
      "tldr_zh": "该论文探讨了imperfect recall游戏中的最优决策问题，即代理忘记先前信息的场景，例如心不在焉的司机游戏或团队成员有限通信的团队游戏。研究在extensive-form games框架下，分析了多玩家设置中三种均衡概念（Nash均衡、基于evidential decision theory (EDT)的multiselves均衡，以及基于causal decision theory (CDT)的multiselves均衡）的计算复杂性，包括精确和近似解决方案。论文将这些问题与复杂度类如P、PPAD、PLS、Σ₂^P、∃R和∃∀R相关联，并作为特例讨论了单玩家游戏、两人零和游戏及其与maximin值的关联，以及无外生随机性的游戏场景。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CC",
        "91A05, 91A06, 91A10, 91A11, 91A18, 91A35, 91A68, 68T37, 68Q17, 68Q25",
        "I.2; J.4; F.2"
      ],
      "primary_category": "cs.GT",
      "comment": "Long version of the paper that got accepted to the Thirty-Third\n  International Joint Conference on Artificial Intelligence (IJCAI 2024). 35\n  pages, 10 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2406.15970v1",
      "published_date": "2024-06-23 00:27:28 UTC",
      "updated_date": "2024-06-23 00:27:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:52:10.434627"
    },
    {
      "arxiv_id": "2406.15966v1",
      "title": "Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care",
      "title_zh": "评估基础模型在心理健康护理中用于问答分类的有效性",
      "authors": [
        "Hassan Alhuzali",
        "Ashwag Alasmari"
      ],
      "abstract": "Pre-trained Language Models (PLMs) have the potential to transform mental\nhealth support by providing accessible and culturally sensitive resources.\nHowever, despite this potential, their effectiveness in mental health care and\nspecifically for the Arabic language has not been extensively explored. To\nbridge this gap, this study evaluates the effectiveness of foundational models\nfor classification of Questions and Answers (Q&A) in the domain of mental\nhealth care. We leverage the MentalQA dataset, an Arabic collection featuring\nQ&A interactions related to mental health. In this study, we conducted\nexperiments using four different types of learning approaches: traditional\nfeature extraction, PLMs as feature extractors, Fine-tuning PLMs and prompting\nlarge language models (GPT-3.5 and GPT-4) in zero-shot and few-shot learning\nsettings. While traditional feature extractors combined with Support Vector\nMachines (SVM) showed promising performance, PLMs exhibited even better results\ndue to their ability to capture semantic meaning. For example, MARBERT achieved\nthe highest performance with a Jaccard Score of 0.80 for question\nclassification and a Jaccard Score of 0.86 for answer classification. We\nfurther conducted an in-depth analysis including examining the effects of\nfine-tuning versus non-fine-tuning, the impact of varying data size, and\nconducting error analysis. Our analysis demonstrates that fine-tuning proved to\nbe beneficial for enhancing the performance of PLMs, and the size of the\ntraining data played a crucial role in achieving high performance. We also\nexplored prompting, where few-shot learning with GPT-3.5 yielded promising\nresults. There was an improvement of 12% for question and classification and\n45% for answer classification. Based on our findings, it can be concluded that\nPLMs and prompt-based approaches hold promise for mental health support in\nArabic.",
      "tldr_zh": "本研究评估了预训练语言模型 (PLMs) 在阿拉伯语心理健康领域 Q&A 分类的有效性，使用 MentalQA 数据集作为基础。实验涵盖了传统特征提取结合 Support Vector Machines (SVM)、PLMs 作为特征提取器、微调 PLMs 以及提示大型语言模型 (如 GPT-3.5 和 GPT-4) 的零样本和 few-shot learning 方法。结果显示，MARBERT 模型取得了最佳表现，Jaccard Score 分别为 0.80 (问题分类) 和 0.86 (答案分类)；微调 PLMs 和训练数据大小对性能提升至关重要，而 few-shot learning with GPT-3.5 带来了 12% (问题) 和 45% (答案) 的改进。总体而言，该研究证明了 PLMs 和提示方法在提供阿拉伯语心理健康支持方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.15966v1",
      "published_date": "2024-06-23 00:11:07 UTC",
      "updated_date": "2024-06-23 00:11:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T23:52:23.946830"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 56,
  "processed_papers_count": 56,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T23:52:46.924956"
}
[
  {
    "arxiv_id": "2505.16080v1",
    "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
    "authors": [
      "Jiayue Liu",
      "Zhongchao Yi",
      "Zhengyang Zhou",
      "Qihe Huang",
      "Kuo Yang",
      "Xu Wang",
      "Yang Wang"
    ],
    "abstract": "Discovering regularities from spatiotemporal systems can benefit various scientific and social planning. Current spatiotemporal learners usually train an independent model from a specific source data that leads to limited transferability among sources, where even correlated tasks requires new design and training. The key towards increasing cross-domain knowledge is to enable collective intelligence and model evolution. In this paper, inspired by neuroscience theories, we theoretically derive the increased information boundary via learning cross-domain collective intelligence and propose a Synaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the model independence and enables cross-domain knowledge to be shared and aggregated. Specifically, we first re-order the sample groups to imitate the human curriculum learning, and devise two complementary learners, elastic common container and task-independent extractor to allow model growth and task-wise commonality and personality disentanglement. Then an adaptive dynamic coupler with a new difference metric determines whether the new sample group should be incorporated into common container to achieve model evolution under various domains. Experiments show that SynEVO improves the generalization capacity by at most 42% under cross-domain scenarios and SynEVO provides a paradigm of NeuroAI for knowledge transfer and adaptation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.16080v1",
    "published_date": "2025-05-21 23:45:51 UTC",
    "updated_date": "2025-05-21 23:45:51 UTC"
  },
  {
    "arxiv_id": "2505.16074v2",
    "title": "Bidirectional Variational Autoencoders",
    "authors": [
      "Bart Kosko",
      "Olaoluwa Adigun"
    ],
    "abstract": "We present the new bidirectional variational autoencoder (BVAE) network architecture. The BVAE uses a single neural network both to encode and decode instead of an encoder-decoder network pair. The network encodes in the forward direction and decodes in the backward direction through the same synaptic web. Simulations compared BVAEs and ordinary VAEs on the four image tasks of image reconstruction, classification, interpolation, and generation. The image datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter count by almost 50% and still slightly outperformed the unidirectional VAEs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.16074v2",
    "published_date": "2025-05-21 23:19:43 UTC",
    "updated_date": "2025-05-26 23:36:25 UTC"
  },
  {
    "arxiv_id": "2505.16067v2",
    "title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior",
    "authors": [
      "Zidi Xiong",
      "Yuping Lin",
      "Wenya Xie",
      "Pengfei He",
      "Zirui Liu",
      "Jiliang Tang",
      "Himabindu Lakkaraju",
      "Zhen Xiang"
    ],
    "abstract": "Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time. In this paper, we conduct an empirical study on how memory management choices impact the LLM agents' behavior, especially their long-term performance. Specifically, we focus on two fundamental memory management operations that are widely used by many agent frameworks-memory addition and deletion-to systematically study their impact on the agent behavior. Through our quantitative analysis, we find that LLM agents display an experience-following property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs. Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where some seemingly correct executions can provide limited or even misleading value as experiences. Through controlled experiments, we demonstrate the importance of regulating experience quality within the memory bank and show that future task evaluations can serve as free quality labels for stored memory. Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16067v2",
    "published_date": "2025-05-21 22:35:01 UTC",
    "updated_date": "2025-10-10 20:27:30 UTC"
  },
  {
    "arxiv_id": "2505.16066v1",
    "title": "Merge to Mix: Mixing Datasets via Model Merging",
    "authors": [
      "Zhixu Silvia Tao",
      "Kasper Vinken",
      "Hao-Wei Yeh",
      "Avi Cooper",
      "Xavier Boix"
    ],
    "abstract": "Mixing datasets for fine-tuning large models (LMs) has become critical for maximizing performance on downstream tasks. However, composing effective dataset mixtures typically relies on heuristics and trial-and-error, often requiring multiple fine-tuning runs to achieve the desired outcome. We propose a novel method, $\\textit{Merge to Mix}$, that accelerates composing dataset mixtures through model merging. Model merging is a recent technique that combines the abilities of multiple individually fine-tuned LMs into a single LM by using a few simple arithmetic operations. Our key insight is that merging models individually fine-tuned on each dataset in a mixture can effectively serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix leverages this insight to accelerate selecting dataset mixtures without requiring full fine-tuning on each candidate mixture. Our experiments demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset selection for fine-tuning LMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16066v1",
    "published_date": "2025-05-21 22:34:13 UTC",
    "updated_date": "2025-05-21 22:34:13 UTC"
  },
  {
    "arxiv_id": "2505.23782v1",
    "title": "4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification",
    "authors": [
      "Andrew P. Berg",
      "Qian Zhang",
      "Mia Y. Wang"
    ],
    "abstract": "Unmanned aerial vehicle (UAV) usage is expected to surge in the coming decade, raising the need for heightened security measures to prevent airspace violations and security threats. This study investigates deep learning approaches to UAV classification focusing on the key issue of data scarcity. To investigate this we opted to train the models using a total of 4,500 seconds of audio samples, evenly distributed across a 9-class dataset. We leveraged parameter efficient fine-tuning (PEFT) and data augmentations to mitigate the data scarcity. This paper implements and compares the use of convolutional neural networks (CNNs) and attention-based transformers. Our results show that, CNNs outperform transformers by 1-2\\% accuracy, while still being more computationally efficient. These early findings, however, point to potential in using transformers models; suggesting that with more data and further optimizations they could outperform CNNs. Future works aims to upscale the dataset to better understand the trade-offs between these approaches.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at the 14th International Conference on Data Science, Technology, and Applications (DATA), 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.23782v1",
    "published_date": "2025-05-21 22:34:07 UTC",
    "updated_date": "2025-05-21 22:34:07 UTC"
  },
  {
    "arxiv_id": "2505.16058v1",
    "title": "Mesh-free sparse identification of nonlinear dynamics",
    "authors": [
      "Mars Liyao Gao",
      "J. Nathan Kutz",
      "Bernat Font"
    ],
    "abstract": "Identifying the governing equations of a dynamical system is one of the most important tasks for scientific modeling. However, this procedure often requires high-quality spatio-temporal data uniformly sampled on structured grids. In this paper, we propose mesh-free SINDy, a novel algorithm which leverages the power of neural network approximation as well as auto-differentiation to identify governing equations from arbitrary sensor placements and non-uniform temporal data sampling. We show that mesh-free SINDy is robust to high noise levels and limited data while remaining computationally efficient. In our implementation, the training procedure is straight-forward and nearly free of hyperparameter tuning, making mesh-free SINDy widely applicable to many scientific and engineering problems. In the experiments, we demonstrate its effectiveness on a series of PDEs including the Burgers' equation, the heat equation, the Korteweg-De Vries equation and the 2D advection-diffusion equation. We conduct detailed numerical experiments on all datasets, varying the noise levels and number of samples, and we also compare our approach to previous state-of-the-art methods. It is noteworthy that, even in high-noise and low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery, achieving successful identification with up to 75% noise for the Burgers' equation using 5,000 samples and with as few as 100 samples and 1% noise. All of this is achieved within a training time of under one minute.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.data-an"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 13 figures, 14 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.16058v1",
    "published_date": "2025-05-21 22:18:37 UTC",
    "updated_date": "2025-05-21 22:18:37 UTC"
  },
  {
    "arxiv_id": "2505.16057v1",
    "title": "Signals of Provenance: Practices & Challenges of Navigating Indicators in AI-Generated Media for Sighted and Blind Individuals",
    "authors": [
      "Ayae Ide",
      "Tory Park",
      "Jaron Mink",
      "Tanusree Sharma"
    ],
    "abstract": "AI-Generated (AIG) content has become increasingly widespread by recent advances in generative models and the easy-to-use tools that have significantly lowered the technical barriers for producing highly realistic audio, images, and videos through simple natural language prompts. In response, platforms are adopting provable provenance with platforms recommending AIG to be self-disclosed and signaled to users. However, these indicators may be often missed, especially when they rely solely on visual cues and make them ineffective to users with different sensory abilities. To address the gap, we conducted semi-structured interviews (N=28) with 15 sighted and 13 BLV participants to examine their interaction with AIG content through self-disclosed AI indicators. Our findings reveal diverse mental models and practices, highlighting different strengths and weaknesses of content-based (e.g., title, description) and menu-aided (e.g., AI labels) indicators. While sighted participants leveraged visual and audio cues, BLV participants primarily relied on audio and existing assistive tools, limiting their ability to identify AIG. Across both groups, they frequently overlooked menu-aided indicators deployed by platforms and rather interacted with content-based indicators such as title and comments. We uncovered usability challenges stemming from inconsistent indicator placement, unclear metadata, and cognitive overload. These issues were especially critical for BLV individuals due to the insufficient accessibility of interface elements. We provide practical recommendations and design implications for future AIG indicators across several dimensions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16057v1",
    "published_date": "2025-05-21 22:16:59 UTC",
    "updated_date": "2025-05-21 22:16:59 UTC"
  },
  {
    "arxiv_id": "2505.16056v3",
    "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models",
    "authors": [
      "Jingcong Liang",
      "Siyuan Wang",
      "Miren Tian",
      "Yitong Li",
      "Duyu Tang",
      "Zhongyu Wei"
    ],
    "abstract": "Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the hit rate of an expert cache utilizing a length of future information under a cache limit. We analyze 20 MoE LLMs with diverse sizes and architectures and use toy models to verify key factors related to local routing consistency. We find a strong trade-off between local routing consistency and *local* load balance, while showing that *global* load balance can coexist with local routing consistency. Meanwhile, settings like shared experts that decrease expert combination space can lead to low local routing consistency. We further reveal that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models balance between cache effectiveness and efficiency with cache sizes approximately twice the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16056v3",
    "published_date": "2025-05-21 22:13:09 UTC",
    "updated_date": "2025-12-10 14:34:07 UTC"
  },
  {
    "arxiv_id": "2505.16048v3",
    "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
    "authors": [
      "Philipp D. Siedler"
    ],
    "abstract": "We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16048v3",
    "published_date": "2025-05-21 22:00:20 UTC",
    "updated_date": "2025-09-27 18:50:46 UTC"
  },
  {
    "arxiv_id": "2506.11049v4",
    "title": "15,500 Seconds: Lean UAV Classification Using EfficientNet and Lightweight Fine-Tuning",
    "authors": [
      "Andrew P. Berg",
      "Qian Zhang",
      "Mia Y. Wang"
    ],
    "abstract": "As unmanned aerial vehicles (UAVs) become increasingly prevalent in both consumer and defense applications, the need for reliable, modality-specific classification systems grows in urgency. This paper addresses the challenge of data scarcity in UAV audio classification by expanding on prior work through the integration of pre-trained deep learning models, parameter-efficient fine-tuning (PEFT) strategies, and targeted data augmentation techniques. Using a custom dataset of 3,100 UAV audio clips (15,500 seconds) spanning 31 distinct drone types, we evaluate the performance of transformer-based and convolutional neural network (CNN) architectures under various fine-tuning configurations. Experiments were conducted with five-fold cross-validation, assessing accuracy, training efficiency, and robustness. Results show that full fine-tuning of the EfficientNet-B0 model with three augmentations achieved the highest validation accuracy (95.95), outperforming both the custom CNN and transformer-based models like AST. These findings suggest that combining lightweight architectures with PEFT and well-chosen augmentations provides an effective strategy for UAV audio classification on limited datasets. Future work will extend this framework to multimodal UAV classification using visual and radar telemetry.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11049v4",
    "published_date": "2025-05-21 21:53:19 UTC",
    "updated_date": "2025-08-14 14:50:40 UTC"
  },
  {
    "arxiv_id": "2505.16037v2",
    "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data",
    "authors": [
      "Asterios Tsiourvas",
      "Wei Sun",
      "Georgia Perakis"
    ],
    "abstract": "LLM routing aims to select the most appropriate model for each query, balancing competing performance metrics such as accuracy and cost across a pool of language models. Prior approaches typically adopt a decoupled strategy, where the metrics are first predicted and the model is then selected based on these estimates. This setup is prone to compounding errors and often relies on full-feedback data, where each query is evaluated by all candidate models, which is costly to obtain and maintain in practice. In contrast, we learn from observational data, which records only the outcome of the model actually deployed. We propose a causal end-to-end framework that learns routing policies by minimizing decision-making regret from observational data. To enable efficient optimization, we introduce two theoretically grounded surrogate objectives: a classification-based upper bound, and a softmax-weighted regret approximation shown to recover the optimal policy at convergence. We further extend our framework to handle heterogeneous cost preferences via an interval-conditioned architecture. Experiments on public benchmarks show that our method outperforms existing baselines, achieving state-of-the-art performance across different embedding models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16037v2",
    "published_date": "2025-05-21 21:34:18 UTC",
    "updated_date": "2025-12-03 01:13:33 UTC"
  },
  {
    "arxiv_id": "2505.16035v2",
    "title": "Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces",
    "authors": [
      "Alejandro García-Castellanos",
      "David R. Wessels",
      "Nicky J. van den Berg",
      "Remco Duits",
      "Daniël M. Pelt",
      "Erik J. Bekkers"
    ],
    "abstract": "We introduce Equivariant Neural Eikonal Solvers, a novel framework that integrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our approach employs a single neural field where a unified shared backbone is conditioned on signal-specific latent variables - represented as point clouds in a Lie group - to model diverse Eikonal solutions. The ENF integration ensures equivariant mapping from these latent representations to the solution field, delivering three key benefits: enhanced representation efficiency through weight-sharing, robust geometric grounding, and solution steerability. This steerability allows transformations applied to the latent point cloud to induce predictable, geometrically meaningful modifications in the resulting Eikonal solution. By coupling these steerable representations with Physics-Informed Neural Networks (PINNs), our framework accurately models Eikonal travel-time solutions while generalizing to arbitrary Riemannian manifolds with regular group actions. This includes homogeneous spaces such as Euclidean, position-orientation, spherical, and hyperbolic manifolds. We validate our approach through applications in seismic travel-time modeling of 2D, 3D, and spherical benchmark datasets. Experimental results demonstrate superior performance, scalability, adaptability, and user controllability compared to existing Neural Operator-based Eikonal solver methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16035v2",
    "published_date": "2025-05-21 21:29:18 UTC",
    "updated_date": "2025-10-24 08:51:26 UTC"
  },
  {
    "arxiv_id": "2505.16031v1",
    "title": "Children's Mental Models of AI Reasoning: Implications for AI Literacy Education",
    "authors": [
      "Aayushi Dangol",
      "Robert Wolfe",
      "Runhua Zhao",
      "JaeWon Kim",
      "Trushaa Ramanan",
      "Katie Davis",
      "Julie A. Kientz"
    ],
    "abstract": "As artificial intelligence (AI) advances in reasoning capabilities, most recently with the emergence of Large Reasoning Models (LRMs), understanding how children conceptualize AI's reasoning processes becomes critical for fostering AI literacy. While one of the \"Five Big Ideas\" in AI education highlights reasoning algorithms as central to AI decision-making, less is known about children's mental models in this area. Through a two-phase approach, consisting of a co-design session with 8 children followed by a field study with 106 children (grades 3-8), we identified three models of AI reasoning: Deductive, Inductive, and Inherent. Our findings reveal that younger children (grades 3-5) often attribute AI's reasoning to inherent intelligence, while older children (grades 6-8) recognize AI as a pattern recognizer. We highlight three tensions that surfaced in children's understanding of AI reasoning and conclude with implications for scaffolding AI curricula and designing explainable AI tools.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16031v1",
    "published_date": "2025-05-21 21:20:12 UTC",
    "updated_date": "2025-05-21 21:20:12 UTC"
  },
  {
    "arxiv_id": "2505.16027v1",
    "title": "Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets",
    "authors": [
      "Qinmei Xu",
      "Yiheng Li",
      "Xianghao Zhan",
      "Ahmet Gorkem Er",
      "Brittany Dashevsky",
      "Chuanjun Xu",
      "Mohammed Alawad",
      "Mengya Yang",
      "Liu Ya",
      "Changsheng Zhou",
      "Xiao Li",
      "Haruka Itakura",
      "Olivier Gevaert"
    ],
    "abstract": "Foundation models leveraging vision-language pretraining have shown promise in chest X-ray (CXR) interpretation, yet their real-world performance across diverse populations and diagnostic tasks remains insufficiently evaluated. This study benchmarks the diagnostic performance and generalizability of foundation models versus traditional convolutional neural networks (CNNs) on multinational CXR datasets. We evaluated eight CXR diagnostic models - five vision-language foundation models and three CNN-based architectures - across 37 standardized classification tasks using six public datasets from the USA, Spain, India, and Vietnam, and three private datasets from hospitals in China. Performance was assessed using AUROC, AUPRC, and other metrics across both shared and dataset-specific tasks. Foundation models outperformed CNNs in both accuracy and task coverage. MAVL, a model incorporating knowledge-enhanced prompts and structured supervision, achieved the highest performance on public (mean AUROC: 0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets, ranking first in 14 of 37 public and 3 of 4 private tasks. All models showed reduced performance on pediatric cases, with average AUROC dropping from 0.88 +/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings highlight the value of structured supervision and prompt design in radiologic AI and suggest future directions including geographic expansion and ensemble modeling for clinical deployment. Code for all evaluated models is available at https://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "78 pages, 7 figures, 2 tabeles",
    "pdf_url": "https://arxiv.org/pdf/2505.16027v1",
    "published_date": "2025-05-21 21:16:50 UTC",
    "updated_date": "2025-05-21 21:16:50 UTC"
  },
  {
    "arxiv_id": "2505.16024v1",
    "title": "Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging",
    "authors": [
      "Weiguo Gao",
      "Ming Li"
    ],
    "abstract": "Diffusion trajectory distillation methods aim to accelerate sampling in diffusion models, which produce high-quality outputs but suffer from slow sampling speeds. These methods train a student model to approximate the multi-step denoising process of a pretrained teacher model in a single step, enabling one-shot generation. However, theoretical insights into the trade-off between different distillation strategies and generative quality remain limited, complicating their optimization and selection. In this work, we take a first step toward addressing this gap. Specifically, we reinterpret trajectory distillation as an operator merging problem in the linear regime, where each step of the teacher model is represented as a linear operator acting on noisy data. These operators admit a clear geometric interpretation as projections and rescalings corresponding to the noise schedule. During merging, signal shrinkage occurs as a convex combination of operators, arising from both discretization and limited optimization time of the student model. We propose a dynamic programming algorithm to compute the optimal merging strategy that maximally preserves signal fidelity. Additionally, we demonstrate the existence of a sharp phase transition in the optimal strategy, governed by data covariance structures. Our findings enhance the theoretical understanding of diffusion trajectory distillation and offer practical insights for improving distillation strategies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 19 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.16024v1",
    "published_date": "2025-05-21 21:13:02 UTC",
    "updated_date": "2025-05-21 21:13:02 UTC"
  },
  {
    "arxiv_id": "2505.16022v2",
    "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning",
    "authors": [
      "Wei Liu",
      "Siya Qi",
      "Xinyu Wang",
      "Chen Qian",
      "Yali Du",
      "Yulan He"
    ],
    "abstract": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language model's output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 5 tables, 12 figures. accepted to EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.16022v2",
    "published_date": "2025-05-21 21:12:35 UTC",
    "updated_date": "2025-09-02 20:28:53 UTC"
  },
  {
    "arxiv_id": "2505.17125v1",
    "title": "NEXT-EVAL: Next Evaluation of Traditional and LLM Web Data Record Extraction",
    "authors": [
      "Soyeon Kim",
      "Namhee Kim",
      "Yeonwoo Jeong"
    ],
    "abstract": "Effective evaluation of web data record extraction methods is crucial, yet hampered by static, domain-specific benchmarks and opaque scoring practices. This makes fair comparison between traditional algorithmic techniques, which rely on structural heuristics, and Large Language Model (LLM)-based approaches, offering zero-shot extraction across diverse layouts, particularly challenging. To overcome these limitations, we introduce a concrete evaluation framework. Our framework systematically generates evaluation datasets from arbitrary MHTML snapshots, annotates XPath-based supervision labels, and employs structure-aware metrics for consistent scoring, specifically preventing text hallucination and allowing only for the assessment of positional hallucination. It also incorporates preprocessing strategies to optimize input for LLMs while preserving DOM semantics: HTML slimming, Hierarchical JSON, and Flat JSON. Additionally, we created a publicly available synthetic dataset by transforming DOM structures and modifying content. We benchmark deterministic heuristic algorithms and off-the-shelf LLMs across these multiple input formats. Our benchmarking shows that Flat JSON input enables LLMs to achieve superior extraction accuracy (F1 score of 0.9567) and minimal hallucination compared to other input formats like Slimmed HTML and Hierarchical JSON. We establish a standardized foundation for rigorous benchmarking, paving the way for the next principled advancements in web data record extraction.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DB",
    "comment": "Web Data Record Extraction, Zero-Shot Extraction, Large Language Models (LLMs) Evaluation Framework, Comparative Analysis",
    "pdf_url": "https://arxiv.org/pdf/2505.17125v1",
    "published_date": "2025-05-21 21:03:37 UTC",
    "updated_date": "2025-05-21 21:03:37 UTC"
  },
  {
    "arxiv_id": "2505.16008v1",
    "title": "LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization",
    "authors": [
      "Wenrui Yu",
      "Yiyi Chen",
      "Johannes Bjerva",
      "Sokol Kosta",
      "Qiongxiu Li"
    ],
    "abstract": "We propose LAGO - Language Similarity-Aware Graph Optimization - a novel approach for few-shot cross-lingual embedding inversion attacks, addressing critical privacy vulnerabilities in multilingual NLP systems. Unlike prior work in embedding inversion attacks that treat languages independently, LAGO explicitly models linguistic relationships through a graph-based constrained distributed optimization framework. By integrating syntactic and lexical similarity as edge constraints, our method enables collaborative parameter learning across related languages. Theoretically, we show this formulation generalizes prior approaches, such as ALGEN, which emerges as a special case when similarity constraints are relaxed. Our framework uniquely combines Frobenius-norm regularization with linear inequality or total variation constraints, ensuring robust alignment of cross-lingual embedding spaces even with extremely limited data (as few as 10 samples per language). Extensive experiments across multiple languages and embedding models demonstrate that LAGO substantially improves the transferability of attacks with 10-20% increase in Rouge-L score over baselines. This work establishes language similarity as a critical factor in inversion attack transferability, urging renewed focus on language-aware privacy-preserving multilingual embeddings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16008v1",
    "published_date": "2025-05-21 20:48:24 UTC",
    "updated_date": "2025-05-21 20:48:24 UTC"
  },
  {
    "arxiv_id": "2505.16004v1",
    "title": "Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations",
    "authors": [
      "Aaron J. Li",
      "Suraj Srinivas",
      "Usha Bhalla",
      "Himabindu Lakkaraju"
    ],
    "abstract": "Sparse autoencoders (SAEs) are commonly used to interpret the internal activations of large language models (LLMs) by mapping them to human-interpretable concept representations. While existing evaluations of SAEs focus on metrics such as the reconstruction-sparsity tradeoff, human (auto-)interpretability, and feature disentanglement, they overlook a critical aspect: the robustness of concept representations to input perturbations. We argue that robustness must be a fundamental consideration for concept representations, reflecting the fidelity of concept labeling. To this end, we formulate robustness quantification as input-space optimization problems and develop a comprehensive evaluation framework featuring realistic scenarios in which adversarial perturbations are crafted to manipulate SAE representations. Empirically, we find that tiny adversarial input perturbations can effectively manipulate concept-based interpretations in most scenarios without notably affecting the outputs of the base LLMs themselves. Overall, our results suggest that SAE concept representations are fragile and may be ill-suited for applications in model monitoring and oversight.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16004v1",
    "published_date": "2025-05-21 20:42:05 UTC",
    "updated_date": "2025-05-21 20:42:05 UTC"
  },
  {
    "arxiv_id": "2505.16003v1",
    "title": "SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models",
    "authors": [
      "Roland Daynauth",
      "Christopher Clarke",
      "Krisztian Flautner",
      "Lingjia Tang",
      "Jason Mars"
    ],
    "abstract": "The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for evaluating language models. Although several calibration techniques have been proposed to better align these evaluators with human judgment, prior studies focus primarily on narrow, well-structured benchmarks. As a result, it remains unclear whether such calibrations generalize to real-world, open-ended tasks.\n  In this work, we show that SOTA calibrated evaluators often fail in these settings, exhibiting weak or even negative correlation with human judgments. To address this, we propose SLMEval, a novel and efficient calibration method based on entropy maximization over a small amount of human preference data. By estimating a latent distribution over model quality and reweighting evaluator scores accordingly, SLMEval achieves strong correlation with human evaluations across two real-world production use cases and the public benchmark. For example, on one such task, SLMEval achieves a Spearman correlation of 0.57 with human judgments, while G-Eval yields a negative correlation. In addition, SLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated evaluators such as G-eval.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.16003v1",
    "published_date": "2025-05-21 20:40:30 UTC",
    "updated_date": "2025-05-21 20:40:30 UTC"
  },
  {
    "arxiv_id": "2505.16002v2",
    "title": "Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions",
    "authors": [
      "Sasha Boguraev",
      "Christopher Potts",
      "Kyle Mahowald"
    ],
    "abstract": "Language Models (LMs) have emerged as powerful sources of evidence for linguists seeking to develop theories of syntax. In this paper, we argue that causal interpretability methods, applied to LMs, can greatly enhance the value of such evidence by helping us characterize the abstract mechanisms that LMs learn to use. Our empirical focus is a set of English filler-gap dependency constructions (e.g., questions, relative clauses). Linguistic theories largely agree that these constructions share many properties. Using experiments based in Distributed Interchange Interventions, we show that LMs converge on similar abstract analyses of these constructions. These analyses also reveal previously overlooked factors -- relating to frequency, filler type, and surrounding context -- that could motivate changes to standard linguistic theory. Overall, these results suggest that mechanistic, internal analyses of LMs can push linguistic theory forward.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 21 figures, 10 tables; EMNLP (Main) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.16002v2",
    "published_date": "2025-05-21 20:37:57 UTC",
    "updated_date": "2025-09-30 01:57:02 UTC"
  },
  {
    "arxiv_id": "2505.16000v5",
    "title": "Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model",
    "authors": [
      "Mehrdad Ghassabi",
      "Pedram Rostami",
      "Hamidreza Baradaran Kashani",
      "Amirhossein Poursina",
      "Zahra Kazemi",
      "Milad Tavakoli"
    ],
    "abstract": "The rapid advancement of language models has demonstrated the potential of artificial intelligence in the healthcare industry. However, small language models struggle with specialized domains in low-resource languages like Persian. While numerous medical-domain websites exist in Persian, no curated dataset or corpus has been available making ours the first of its kind. This study introduces a newly curated dataset comprising 20k doctor-patient Q\\&A pairs and 60\\% of a 90-million-token crawled corpus from medical magazines. Using a parameter-efficient fine-tuning approach, we enhanced the medical knowledge of the baseline model, aya-expanse-8b. Benchmark evaluations demonstrate that the fine-tuned model achieves improved accuracy in medical question answering and successfully passed the Iranian Basic Medical Science Entrance Exam (IBSEE) in September 2023, which the baseline model did not. Additionally, the fine-tuned model improved Persian-translated MMLU accuracy by an average of 2.67\\%. This work highlights the potential of leveraging open-access online data to enrich small language models in medical fields, providing a novel solution for Persian medical AI applications suitable for resource-constrained environments. Future research could explore multimodal input to further enhance performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.16000v5",
    "published_date": "2025-05-21 20:30:47 UTC",
    "updated_date": "2025-11-16 08:14:06 UTC"
  },
  {
    "arxiv_id": "2505.15998v2",
    "title": "Exploring Flow-Lenia Universes with a Curiosity-driven AI Scientist: Discovering Diverse Ecosystem Dynamics",
    "authors": [
      "Thomas Michel",
      "Marko Cvjetko",
      "Gautier Hamon",
      "Pierre-Yves Oudeyer",
      "Clément Moulin-Frier"
    ],
    "abstract": "We present a method for the automated discovery of system-level dynamics in Flow-Lenia--a continuous cellular automaton (CA) with mass conservation and parameter localization-using a curiosity--driven AI scientist. This method aims to uncover processes leading to self-organization of evolutionary and ecosystemic dynamics in CAs. We build on previous work which uses diversity search algorithms in Lenia to find self-organized individual patterns, and extend it to large environments that support distinct interacting patterns. We adapt Intrinsically Motivated Goal Exploration Processes (IMGEPs) to drive exploration of diverse Flow-Lenia environments using simulation-wide metrics, such as evolutionary activity, compression-based complexity, and multi-scale entropy. We test our method in two experiments, showcasing its ability to illuminate significantly more diverse dynamics compared to random search. We show qualitative results illustrating how ecosystemic simulations enable self-organization of complex collective behaviors not captured by previous individual pattern search and analysis. We complement automated discovery with an interactive exploration tool, creating an effective human-AI collaborative workflow for scientific investigation. Though demonstrated specifically with Flow-Lenia, this methodology provides a framework potentially applicable to other parameterizable complex systems where understanding emergent collective properties is of interest.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.15998v2",
    "published_date": "2025-05-21 20:28:58 UTC",
    "updated_date": "2025-06-02 14:48:26 UTC"
  },
  {
    "arxiv_id": "2505.15997v1",
    "title": "Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers",
    "authors": [
      "Mehran Zoravar",
      "Shadi Alijani",
      "Homayoun Najjaran"
    ],
    "abstract": "Exploring the trustworthiness of deep learning models is crucial, especially in critical domains such as medical imaging decision support systems. Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. However, conformal prediction results face challenges due to the backbone model's struggles in domain-shifted scenarios, such as variations in different sources. To aim this challenge, this paper proposes a novel framework termed Conformal Ensemble of Vision Transformers (CE-ViTs) designed to enhance image classification performance by prioritizing domain adaptation and model robustness, while accounting for uncertainty. The proposed method leverages an ensemble of vision transformer models in the backbone, trained on diverse datasets including HAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning approach, calibrated through the combined mentioned datasets, aims to enhance domain adaptation through conformal learning. Experimental results underscore that the framework achieves a high coverage rate of 90.38\\%, representing an improvement of 9.95\\% compared to the HAM10000 model. This indicates a strong likelihood that the prediction set includes the true label compared to singular models. Ensemble learning in CE-ViTs significantly improves conformal prediction performance, increasing the average prediction set size for challenging misclassified samples from 1.86 to 3.075.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 4 figures, conference (ccece 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.15997v1",
    "published_date": "2025-05-21 20:28:43 UTC",
    "updated_date": "2025-05-21 20:28:43 UTC"
  },
  {
    "arxiv_id": "2506.11048v1",
    "title": "I Can't Believe It's Not Real: CV-MuSeNet: Complex-Valued Multi-Signal Segmentation",
    "authors": [
      "Sangwon Shin",
      "Mehmet C. Vuran"
    ],
    "abstract": "The increasing congestion of the radio frequency spectrum presents challenges for efficient spectrum utilization. Cognitive radio systems enable dynamic spectrum access with the aid of recent innovations in neural networks. However, traditional real-valued neural networks (RVNNs) face difficulties in low signal-to-noise ratio (SNR) environments, as they were not specifically developed to capture essential wireless signal properties such as phase and amplitude. This work presents CMuSeNet, a complex-valued multi-signal segmentation network for wideband spectrum sensing, to address these limitations. Extensive hyperparameter analysis shows that a naive conversion of existing RVNNs into their complex-valued counterparts is ineffective. Built on complex-valued neural networks (CVNNs) with a residual architecture, CMuSeNet introduces a complexvalued Fourier spectrum focal loss (CFL) and a complex plane intersection over union (CIoU) similarity metric to enhance training performance. Extensive evaluations on synthetic, indoor overthe-air, and real-world datasets show that CMuSeNet achieves an average accuracy of 98.98%-99.90%, improving by up to 9.2 percentage points over its real-valued counterpart and consistently outperforms state of the art. Strikingly, CMuSeNet achieves the accuracy level of its RVNN counterpart in just two epochs, compared to the 27 epochs required for RVNN, while reducing training time by up to a 92.2% over the state of the art. The results highlight the effectiveness of complex-valued architectures in improving weak signal detection and training efficiency for spectrum sensing in challenging low-SNR environments. The dataset is available at: https://dx.doi.org/10.21227/hcc1-6p22",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11048v1",
    "published_date": "2025-05-21 20:08:02 UTC",
    "updated_date": "2025-05-21 20:08:02 UTC"
  },
  {
    "arxiv_id": "2505.15966v3",
    "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning",
    "authors": [
      "Haozhe Wang",
      "Alex Su",
      "Weiming Ren",
      "Fangzhen Lin",
      "Wenhu Chen"
    ],
    "abstract": "Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model's initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on TallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://tiger-ai-lab.github.io/Pixel-Reasoner/, Hands-on Demo: https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner",
    "pdf_url": "https://arxiv.org/pdf/2505.15966v3",
    "published_date": "2025-05-21 19:35:08 UTC",
    "updated_date": "2025-10-24 09:35:22 UTC"
  },
  {
    "arxiv_id": "2505.15962v3",
    "title": "Pre-training Limited Memory Language Models with Internal and External Knowledge",
    "authors": [
      "Linxi Zhao",
      "Sofian Zalouk",
      "Christian K. Belardi",
      "Justin Lovelace",
      "Jin Peng Zhou",
      "Ryan Thomas Noonan",
      "Dongyoung Go",
      "Kilian Q. Weinberger",
      "Yoav Artzi",
      "Jennifer J. Sun"
    ],
    "abstract": "Neural language models are black-boxes--both linguistic patterns and factual knowledge are distributed across billions of opaque parameters. This entangled encoding makes it difficult to reliably inspect, verify, or update specific facts. We introduce Limited Memory Language Models (LMLM), a new class of language models that externalizes factual knowledge to external database during pre-training rather than memorizing them. Our pre-training approach strategically masks externally retrieved factual values from the training loss, thereby teaching the model to perform targeted lookups rather than relying on memorization in model weights. Our experiments demonstrate that LMLMs achieve competitive performance compared to significantly larger LLMs on standard benchmarks, while offering the advantages of explicit, editable, and verifiable knowledge bases.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code, models, and data available at https://github.com/kilian-group/LMLM",
    "pdf_url": "https://arxiv.org/pdf/2505.15962v3",
    "published_date": "2025-05-21 19:26:03 UTC",
    "updated_date": "2025-10-02 18:55:28 UTC"
  },
  {
    "arxiv_id": "2505.15957v3",
    "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey",
    "authors": [
      "Chih-Kai Yang",
      "Neo S. Ho",
      "Hung-yi Lee"
    ],
    "abstract": "With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "EMNLP 2025 (Main). Project Website: https://github.com/ckyang1124/LALM-Evaluation-Survey",
    "pdf_url": "https://arxiv.org/pdf/2505.15957v3",
    "published_date": "2025-05-21 19:17:29 UTC",
    "updated_date": "2025-10-01 16:02:37 UTC"
  },
  {
    "arxiv_id": "2505.15952v2",
    "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance",
    "authors": [
      "Mohammad Reza Taesiri",
      "Abhijay Ghildyal",
      "Saman Zadtootaghaj",
      "Nabajeet Barman",
      "Cor-Paul Bezemer"
    ],
    "abstract": "With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: https://asgaardlab.github.io/videogameqa-bench/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project website with code and data: https://asgaardlab.github.io/videogameqa-bench/",
    "pdf_url": "https://arxiv.org/pdf/2505.15952v2",
    "published_date": "2025-05-21 19:08:38 UTC",
    "updated_date": "2025-12-18 23:33:44 UTC"
  },
  {
    "arxiv_id": "2505.15946v3",
    "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding",
    "authors": [
      "Yuxiang Wei",
      "Yanteng Zhang",
      "Xi Xiao",
      "Tianyang Wang",
      "Xiao Wang",
      "Vince D. Calhoun"
    ],
    "abstract": "Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: https://github.com/yuxiangwei0808/MoRE-Brain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15946v3",
    "published_date": "2025-05-21 19:02:54 UTC",
    "updated_date": "2025-10-07 22:39:42 UTC"
  },
  {
    "arxiv_id": "2505.15929v2",
    "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
    "authors": [
      "Hui Shen",
      "Taiqiang Wu",
      "Qi Han",
      "Yunta Hsieh",
      "Jizhou Wang",
      "Yuyue Zhang",
      "Yuxin Cheng",
      "Zijian Hao",
      "Yuansheng Ni",
      "Xin Wang",
      "Zhongwei Wan",
      "Kai Zhang",
      "Wendong Xu",
      "Jing Xiong",
      "Ping Luo",
      "Wenhu Chen",
      "Chaofan Tao",
      "Zhuoqing Mao",
      "Ngai Wong"
    ],
    "abstract": "Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5%, 42.2%, and 45.8% accuracy respectively-performance gaps exceeding 29% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation. More details are available on our project page: https://phyx-bench.github.io/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15929v2",
    "published_date": "2025-05-21 18:33:50 UTC",
    "updated_date": "2025-05-29 17:59:14 UTC"
  },
  {
    "arxiv_id": "2505.15925v3",
    "title": "VERDI: VLM-Embedded Reasoning for Autonomous Driving",
    "authors": [
      "Bowen Feng",
      "Zhiting Mei",
      "Baiang Li",
      "Julian Ost",
      "Filippo Ghilotti",
      "Roger Girgis",
      "Anirudha Majumdar",
      "Felix Heide"
    ],
    "abstract": "While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15925v3",
    "published_date": "2025-05-21 18:24:36 UTC",
    "updated_date": "2025-12-22 15:37:49 UTC"
  },
  {
    "arxiv_id": "2505.15918v2",
    "title": "Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization",
    "authors": [
      "Aliakbar Nafar",
      "Kristen Brent Venable",
      "Zijun Cui",
      "Parisa Kordjamshidi"
    ],
    "abstract": "In this work, we evaluate the potential of Large Language Models (LLMs) in building Bayesian Networks (BNs) by approximating domain expert priors. LLMs have demonstrated potential as factual knowledge bases; however, their capability to generate probabilistic knowledge about real-world events remains understudied. We explore utilizing the probabilistic knowledge inherent in LLMs to derive probability estimates for statements regarding events and their relationships within a BN. Using LLMs in this context allows for the parameterization of BNs, enabling probabilistic modeling within specific domains. Our experiments on eighty publicly available Bayesian Networks, from healthcare to finance, demonstrate that querying LLMs about the conditional probabilities of events provides meaningful results when compared to baselines, including random and uniform distributions, as well as approaches based on next-token generation probabilities. We explore how these LLM-derived distributions can serve as expert priors to refine distributions extracted from data, especially when data is scarce. Overall, this work introduces a promising strategy for automatically constructing Bayesian Networks by combining probabilistic knowledge extracted from LLMs with real-world data. Additionally, we establish the first comprehensive baseline for assessing LLM performance in extracting probabilistic knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15918v2",
    "published_date": "2025-05-21 18:15:05 UTC",
    "updated_date": "2025-08-10 23:38:19 UTC"
  },
  {
    "arxiv_id": "2505.15916v1",
    "title": "BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law",
    "authors": [
      "Juvenal Domingos Júnior",
      "Augusto Faria",
      "E. Seiti de Oliveira",
      "Erick de Brito",
      "Matheus Teotonio",
      "Andre Assumpção",
      "Diedre Carmo",
      "Roberto Lotufo",
      "Jayr Pereira"
    ],
    "abstract": "This paper presents BR-TaxQA-R, a novel dataset designed to support question answering with references in the context of Brazilian personal income tax law. The dataset contains 715 questions from the 2024 official Q\\&A document published by Brazil's Internal Revenue Service, enriched with statutory norms and administrative rulings from the Conselho Administrativo de Recursos Fiscais (CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using OpenAI embeddings for searching and GPT-4o-mini for answer generation. We compare different text segmentation strategies and benchmark our system against commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics. Results show that our custom RAG pipeline outperforms commercial systems in Response Relevancy, indicating stronger alignment with user queries, while commercial models achieve higher scores in Factual Correctness and fluency. These findings highlight a trade-off between legally grounded generation and linguistic fluency. Crucially, we argue that human expert evaluation remains essential to ensure the legal validity of AI-generated answers in high-stakes domains such as taxation. BR-TaxQA-R is publicly available at https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15916v1",
    "published_date": "2025-05-21 18:11:41 UTC",
    "updated_date": "2025-05-21 18:11:41 UTC"
  },
  {
    "arxiv_id": "2505.15888v1",
    "title": "Last Layer Empirical Bayes",
    "authors": [
      "Valentin Villecroze",
      "Yixin Wang",
      "Gabriel Loaiza-Ganem"
    ],
    "abstract": "The task of quantifying the inherent uncertainty associated with neural network predictions is a key challenge in artificial intelligence. Bayesian neural networks (BNNs) and deep ensembles are among the most prominent approaches to tackle this task. Both approaches produce predictions by computing an expectation of neural network outputs over some distribution on the corresponding weights; this distribution is given by the posterior in the case of BNNs, and by a mixture of point masses for ensembles. Inspired by recent work showing that the distribution used by ensembles can be understood as a posterior corresponding to a learned data-dependent prior, we propose last layer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a normalizing flow, which is then trained to maximize the evidence lower bound; to retain tractability we use the flow only on the last layer. We show why LLEB is well motivated, and how it interpolates between standard BNNs and ensembles in terms of the strength of the prior that they use. LLEB performs on par with existing approaches, highlighting that empirical Bayes is a promising direction for future research in uncertainty quantification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the ICBINB Worshop at ICLR 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15888v1",
    "published_date": "2025-05-21 18:00:00 UTC",
    "updated_date": "2025-05-21 18:00:00 UTC"
  },
  {
    "arxiv_id": "2505.15810v2",
    "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents",
    "authors": [
      "Yuqi Zhou",
      "Sunhao Dai",
      "Shuai Wang",
      "Kaiwen Zhou",
      "Qinglin Jia",
      "Jun Xu"
    ],
    "abstract": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a Fast Thinking Template that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding. The project repository is available at https://github.com/Yuqi-Zhou/GUI-G1.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15810v2",
    "published_date": "2025-05-21 17:59:09 UTC",
    "updated_date": "2025-05-22 11:15:23 UTC"
  },
  {
    "arxiv_id": "2505.15808v1",
    "title": "Neural Conditional Transport Maps",
    "authors": [
      "Carlos Rodriguez-Pardo",
      "Leonardo Chiani",
      "Emanuele Borgonovo",
      "Massimo Tavoni"
    ],
    "abstract": "We present a neural framework for learning conditional optimal transport (OT) maps between probability distributions. Our approach introduces a conditioning mechanism capable of processing both categorical and continuous conditioning variables simultaneously. At the core of our method lies a hypernetwork that generates transport layer parameters based on these inputs, creating adaptive mappings that outperform simpler conditioning methods. Comprehensive ablation studies demonstrate the superior performance of our method over baseline configurations. Furthermore, we showcase an application to global sensitivity analysis, offering high performance in computing OT-based sensitivity indices. This work advances the state-of-the-art in conditional optimal transport, enabling broader application of optimal transport principles to complex, high-dimensional domains such as generative modeling and black-box model explainability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review. Supplementary material included in the pdf",
    "pdf_url": "https://arxiv.org/pdf/2505.15808v1",
    "published_date": "2025-05-21 17:59:02 UTC",
    "updated_date": "2025-05-21 17:59:02 UTC"
  },
  {
    "arxiv_id": "2505.15879v1",
    "title": "GRIT: Teaching MLLMs to Think with Images",
    "authors": [
      "Yue Fan",
      "Xuehai He",
      "Diji Yang",
      "Kaizhi Zheng",
      "Ching-Chen Kuo",
      "Yuting Zheng",
      "Sravana Jyothi Narayanaraju",
      "Xinze Guan",
      "Xin Eric Wang"
    ],
    "abstract": "Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15879v1",
    "published_date": "2025-05-21 17:54:49 UTC",
    "updated_date": "2025-05-21 17:54:49 UTC"
  },
  {
    "arxiv_id": "2505.15801v3",
    "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models",
    "authors": [
      "Yuchen Yan",
      "Jin Jiang",
      "Zhenbang Ren",
      "Yijun Li",
      "Xudong Cai",
      "Yang Liu",
      "Xin Xu",
      "Mengdi Zhang",
      "Jian Shao",
      "Yongliang Shen",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "abstract": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Project Page: https://zju-real.github.io/VerifyBench Dataset: https://huggingface.co/datasets/ZJU-REAL/VerifyBench Code: https://github.com/ZJU-REAL/VerifyBench",
    "pdf_url": "https://arxiv.org/pdf/2505.15801v3",
    "published_date": "2025-05-21 17:54:43 UTC",
    "updated_date": "2025-09-25 15:20:02 UTC"
  },
  {
    "arxiv_id": "2505.15792v1",
    "title": "Long-Form Information Alignment Evaluation Beyond Atomic Facts",
    "authors": [
      "Danna Zheng",
      "Mirella Lapata",
      "Jeff Z. Pan"
    ],
    "abstract": "Information alignment evaluators are vital for various NLG evaluation tasks and trustworthy LLM deployment, reducing hallucinations and enhancing user trust. Current fine-grained methods, like FactScore, verify facts individually but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this work, we introduce MontageLie, a challenging benchmark that constructs deceptive narratives by \"montaging\" truthful statements without introducing explicit hallucinations. We demonstrate that both coarse-grained LLM-based evaluators and current fine-grained frameworks are susceptible to this attack, with AUC-ROC scores falling below 65%. To enable more robust fine-grained evaluation, we propose DoveScore, a novel framework that jointly verifies factual accuracy and event-order consistency. By modeling inter-fact relationships, DoveScore outperforms existing fine-grained methods by over 8%, providing a more robust solution for long-form text alignment evaluation. Our code and datasets are available at https://github.com/dannalily/DoveScore.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15792v1",
    "published_date": "2025-05-21 17:46:38 UTC",
    "updated_date": "2025-05-21 17:46:38 UTC"
  },
  {
    "arxiv_id": "2505.15790v1",
    "title": "Exploring the Innovation Opportunities for Pre-trained Models",
    "authors": [
      "Minjung Park",
      "Jodi Forlizzi",
      "John Zimmerman"
    ],
    "abstract": "Innovators transform the world by understanding where services are successfully meeting customers' needs and then using this knowledge to identify failsafe opportunities for innovation. Pre-trained models have changed the AI innovation landscape, making it faster and easier to create new AI products and services. Understanding where pre-trained models are successful is critical for supporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained models makes it hard to know where AI can really be successful. To address this, we investigated pre-trained model applications developed by HCI researchers as a proxy for commercially successful applications. The research applications demonstrate technical capabilities, address real user needs, and avoid ethical challenges. Using an artifact analysis approach, we categorized capabilities, opportunity domains, data types, and emerging interaction design patterns, uncovering some of the opportunity space for innovation with pre-trained models.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "33 pages, 20 figures, 4 tables, DIS",
    "pdf_url": "https://arxiv.org/pdf/2505.15790v1",
    "published_date": "2025-05-21 17:43:46 UTC",
    "updated_date": "2025-05-21 17:43:46 UTC"
  },
  {
    "arxiv_id": "2505.15784v1",
    "title": "Large Language Models as Computable Approximations to Solomonoff Induction",
    "authors": [
      "Jun Wan",
      "Lingrui Mei"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) calls for a rigorous theoretical framework to explain their empirical success. While significant progress has been made in understanding LLM behaviors, existing theoretical frameworks remain fragmented in explaining emergent phenomena through a unified mathematical lens. We establish the first formal connection between LLM architectures and Algorithmic Information Theory (AIT) by proving two fundamental results: (1) the training process computationally approximates Solomonoff prior through loss minimization interpreted as program length optimization, and (2) next-token prediction implements approximate Solomonoff induction. We leverage AIT to provide a unified theoretical explanation for in-context learning, few-shot learning, and scaling laws. Furthermore, our theoretical insights lead to a principled method for few-shot example selection that prioritizes samples where models exhibit lower predictive confidence. We demonstrate through experiments on diverse text classification benchmarks that this strategy yields significant performance improvements, particularly for smaller model architectures, when compared to selecting high-confidence examples. Our framework bridges the gap between theoretical foundations and practical LLM behaviors, providing both explanatory power and actionable insights for future model development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Both authors contributed equally",
    "pdf_url": "https://arxiv.org/pdf/2505.15784v1",
    "published_date": "2025-05-21 17:35:08 UTC",
    "updated_date": "2025-05-21 17:35:08 UTC"
  },
  {
    "arxiv_id": "2505.15779v1",
    "title": "IA-T2I: Internet-Augmented Text-to-Image Generation",
    "authors": [
      "Chuanhao Li",
      "Jianwen Sun",
      "Yukang Feng",
      "Mingliang Zhai",
      "Yifan Chang",
      "Kaipeng Zhang"
    ],
    "abstract": "Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 7 figures, a framework that integrates reference images from the Internet into T2I/TI2I models",
    "pdf_url": "https://arxiv.org/pdf/2505.15779v1",
    "published_date": "2025-05-21 17:31:49 UTC",
    "updated_date": "2025-05-21 17:31:49 UTC"
  },
  {
    "arxiv_id": "2505.15778v1",
    "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space",
    "authors": [
      "Zhen Zhang",
      "Xuehai He",
      "Weixiang Yan",
      "Ao Shen",
      "Chenyang Zhao",
      "Shuohang Wang",
      "Yelong Shen",
      "Xin Eric Wang"
    ],
    "abstract": "Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed points in the semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like \"soft\" reasoning by generating soft, abstract concept tokens in a continuous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which form the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple meanings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent bottleneck of discrete language-based reasoning. Code is available at https://github.com/eric-ai-lab/Soft-Thinking.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15778v1",
    "published_date": "2025-05-21 17:29:15 UTC",
    "updated_date": "2025-05-21 17:29:15 UTC"
  },
  {
    "arxiv_id": "2505.15765v2",
    "title": "Constructing a 3D Scene from a Single Image",
    "authors": [
      "Kaizhi Zheng",
      "Ruijian Zha",
      "Zishuo Xu",
      "Jing Gu",
      "Jie Yang",
      "Xin Eric Wang"
    ],
    "abstract": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have achieved remarkable results at the object level, their extension to full-scene generation often leads to inconsistent geometry, layout hallucinations, and low-quality meshes. In this work, we introduce SceneFuse-3D, a training-free framework designed to synthesize coherent 3D scenes from a single top-down view. Our method is grounded in two principles: region-based generation to improve image-to-3D alignment and resolution, and spatial-aware 3D inpainting to ensure global scene coherence and high-quality geometry generation. Specifically, we decompose the input image into overlapping regions and generate each using a pretrained 3D object generator, followed by a masked rectified flow inpainting process that fills in missing geometry while maintaining structural continuity. This modular design allows us to overcome resolution bottlenecks and preserve spatial structure without requiring 3D supervision or fine-tuning. Extensive experiments across diverse scenes show that SceneFuse-3D outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, TripoSG, and LGM, in terms of geometry quality, spatial coherence, and texture fidelity. Our results demonstrate that high-quality coherent 3D scene-level asset generation is achievable from a single top-down image using a principled, training-free pipeline.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15765v2",
    "published_date": "2025-05-21 17:10:47 UTC",
    "updated_date": "2025-10-04 02:17:54 UTC"
  },
  {
    "arxiv_id": "2505.15754v2",
    "title": "Improving planning and MBRL with temporally-extended actions",
    "authors": [
      "Palash Chatterjee",
      "Roni Khardon"
    ],
    "abstract": "Continuous time systems are often modeled using discrete time dynamics but this requires a small simulation step to maintain accuracy. In turn, this requires a large planning horizon which leads to computationally demanding planning problems and reduced performance. Previous work in model-free reinforcement learning has partially addressed this issue using action repeats where a policy is learned to determine a discrete action duration. Instead we propose to control the continuous decision timescale directly by using temporally-extended actions and letting the planner treat the duration of the action as an additional optimization variable along with the standard action variables. This additional structure has multiple advantages. It speeds up simulation time of trajectories and, importantly, it allows for deep horizon search in terms of primitive actions while using a shallow search depth in the planner. In addition, in the model-based reinforcement learning (MBRL) setting, it reduces compounding errors from model learning and improves training time for models. We show that this idea is effective and that the range for action durations can be automatically selected using a multi-armed bandit formulation and integrated into the MBRL framework. An extensive experimental evaluation both in planning and in MBRL, shows that our approach yields faster planning, better solutions, and that it enables solutions to problems that are not solved in the standard formulation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025. For project website, see https://pecey.github.io/MBRL-with-TEA/",
    "pdf_url": "https://arxiv.org/pdf/2505.15754v2",
    "published_date": "2025-05-21 16:59:32 UTC",
    "updated_date": "2025-10-21 21:07:27 UTC"
  },
  {
    "arxiv_id": "2505.15753v1",
    "title": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval",
    "authors": [
      "Taiye Chen",
      "Zeming Wei",
      "Ang Li",
      "Yisen Wang"
    ],
    "abstract": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking attacks, wherein adversaries exploit carefully engineered prompts to induce harmful or unethical responses. Such threats have raised critical concerns about the safety and reliability of LLMs in real-world deployment. While existing defense mechanisms partially mitigate such risks, subsequent advancements in adversarial techniques have enabled novel jailbreaking methods to circumvent these protections, exposing the limitations of static defense frameworks. In this work, we explore defending against evolving jailbreaking threats through the lens of context retrieval. First, we conduct a preliminary study demonstrating that even a minimal set of safety-aligned examples against a particular jailbreak can significantly enhance robustness against this attack pattern. Building on this insight, we further leverage the retrieval-augmented generation (RAG) techniques and propose Safety Context Retrieval (SCR), a scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our comprehensive experiments demonstrate how SCR achieves superior defensive performance against both established and emerging jailbreaking tactics, contributing a new paradigm to LLM safety. Our code will be available upon publication.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15753v1",
    "published_date": "2025-05-21 16:58:14 UTC",
    "updated_date": "2025-05-21 16:58:14 UTC"
  },
  {
    "arxiv_id": "2505.15747v2",
    "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs",
    "authors": [
      "Kanan Kiguchi",
      "Yunhao Tu",
      "Katsuhiro Ajito",
      "Fady Alnajjar",
      "Kazuyuki Murase"
    ],
    "abstract": "We propose a novel framework for integrating fragmented multi-modal data in Alzheimer's disease (AD) research using large language models (LLMs) and knowledge graphs. While traditional multimodal analysis requires matched patient IDs across datasets, our approach demonstrates population-level integration of MRI, gene expression, biomarkers, EEG, and clinical indicators from independent cohorts. Statistical analysis identified significant features in each modality, which were connected as nodes in a knowledge graph. LLMs then analyzed the graph to extract potential correlations and generate hypotheses in natural language. This approach revealed several novel relationships, including a potential pathway linking metabolic risk factors to tau protein abnormalities via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between frontal EEG channels and specific gene expression profiles (r=0.42-0.58, p<0.01). Cross-validation with independent datasets confirmed the robustness of major findings, with consistent effect sizes across cohorts (variance <15%). The reproducibility of these findings was further supported by expert review (Cohen's k=0.82) and computational validation. Our framework enables cross modal integration at a conceptual level without requiring patient ID matching, offering new possibilities for understanding AD pathology through fragmented data reuse and generating testable hypotheses for future research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "38 pages, 8 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.15747v2",
    "published_date": "2025-05-21 16:51:49 UTC",
    "updated_date": "2025-05-22 03:58:27 UTC"
  },
  {
    "arxiv_id": "2505.15746v1",
    "title": "Higher-order Structure Boosts Link Prediction on Temporal Graphs",
    "authors": [
      "Jingzhe Liu",
      "Zhigang Hua",
      "Yan Xie",
      "Bingheng Li",
      "Harry Shomer",
      "Yu Song",
      "Kaveh Hassani",
      "Jiliang Tang"
    ],
    "abstract": "Temporal Graph Neural Networks (TGNNs) have gained growing attention for modeling and predicting structures in temporal graphs. However, existing TGNNs primarily focus on pairwise interactions while overlooking higher-order structures that are integral to link formation and evolution in real-world temporal graphs. Meanwhile, these models often suffer from efficiency bottlenecks, further limiting their expressive power. To tackle these challenges, we propose a Higher-order structure Temporal Graph Neural Network, which incorporates hypergraph representations into temporal graph learning. In particular, we develop an algorithm to identify the underlying higher-order structures, enhancing the model's ability to capture the group interactions. Furthermore, by aggregating multiple edge features into hyperedge representations, HTGN effectively reduces memory cost during training. We theoretically demonstrate the enhanced expressiveness of our approach and validate its effectiveness and efficiency through extensive experiments on various real-world temporal graphs. Experimental results show that HTGN achieves superior performance on dynamic link prediction while reducing memory costs by up to 50\\% compared to existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15746v1",
    "published_date": "2025-05-21 16:51:44 UTC",
    "updated_date": "2025-05-21 16:51:44 UTC"
  },
  {
    "arxiv_id": "2505.15742v1",
    "title": "Neuro-Argumentative Learning with Case-Based Reasoning",
    "authors": [
      "Adam Gould",
      "Francesca Toni"
    ],
    "abstract": "We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual AA-CBR), a data-driven, neurosymbolic classification model in which the outcome is determined by an argumentation debate structure that is learned simultaneously with neural-based feature extractors. Each argument in the debate is an observed case from the training data, favouring their labelling. Cases attack or support those with opposing or agreeing labellings, with the strength of each argument and relationship learned through gradient-based methods. This argumentation debate structure provides human-aligned reasoning, improving model interpretability compared to traditional neural networks (NNs). Unlike the existing purely symbolic variant, Abstract Argumentation for Case-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class classification, automatic learning of feature and data point importance, assigning uncertainty values to outcomes, using all available data points, and does not require binary features. We show that Gradual AA-CBR performs comparably to NNs whilst significantly outperforming existing AA-CBR formulations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeSy25",
    "pdf_url": "https://arxiv.org/pdf/2505.15742v1",
    "published_date": "2025-05-21 16:49:47 UTC",
    "updated_date": "2025-05-21 16:49:47 UTC"
  },
  {
    "arxiv_id": "2505.17121v2",
    "title": "NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation",
    "authors": [
      "Weiming Wu",
      "Jin Ye",
      "Zi-kang Wang",
      "Zhi Zhou",
      "Yu-Feng Li",
      "Lan-Zhe Guo"
    ],
    "abstract": "Obtaining large-scale, high-quality reasoning data is crucial for improving the geometric reasoning capabilities of multi-modal large language models (MLLMs). However, existing data generation methods, whether based on predefined tem plates or constrained symbolic provers, inevitably face diversity and numerical generalization limitations. To address these limitations, we propose NeSyGeo, a novel neuro-symbolic framework for generating geometric reasoning data. First, we propose a domain-specific language grounded in the entity-attributes-relations paradigm to comprehensively represent all components of plane geometry, along with generative actions defined within this symbolic space. We then design a symbolic-visual-text pipeline that synthesizes symbolic sequences, maps them to visual and textual representations and generates reasoning path with reverse search and forward validation. Based on this framework, we construct NeSyGeo CoT and NeSyGeo-Caption datasets, containing 100k samples, and release a new benchmark NeSyGeo-Test for evaluating geometric reasoning abilities in MLLMs. Experiments demonstrate that the proposal significantly and consistently improves the performance of multiple MLLMs under both reinforcement and supervised fine-tuning. With only 4k samples and two epochs of reinforcement fine-tuning, base models achieve improvements of up to +15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA. Notably, a 4B model can be improved to outperform an 8B model from the same series on geometric reasoning tasks.s",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.17121v2",
    "published_date": "2025-05-21 16:45:49 UTC",
    "updated_date": "2025-10-02 18:15:25 UTC"
  },
  {
    "arxiv_id": "2505.15740v1",
    "title": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis and Refinement",
    "authors": [
      "Jilin Hu",
      "Jianyu Zhang",
      "Yongwang Zhao",
      "Talia Ringer"
    ],
    "abstract": "Formal methods is pivotal for verifying the reliability of critical systems through rigorous mathematical proofs. However, its adoption is hindered by labor-intensive manual proofs and the expertise required to use theorem provers. Recent advancements in large language models (LLMs) offer new opportunities for automated theorem proving. Two promising approaches are generating tactics step by step and generating a whole proof directly with an LLM. However, existing work makes no attempt to combine the two approaches. In this work, we introduce HybridProver, a dual-model proof synthesis framework that combines tactic-based generation and whole-proof synthesis to harness the benefits of both approaches. HybridProver generates whole proof candidates for evaluation directly, then extracts proof sketches from those candidates. It then uses a tactic-based generation model that integrates automated tools to complete the sketches via stepwise refinement. We implement HybridProver for the Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle datasets. Evaluation on the miniF2F dataset illustrates HybridProver's effectiveness. We achieve a 59.4% success rate on miniF2F, where the previous SOTA is 56.1%. Our ablation studies show that this SOTA result is attributable to combining whole-proof and tactic-based generation. Additionally, we show how the dataset quality, training parameters, and sampling diversity affect the final result during automated theorem proving with LLMs. All of our code, datasets, and LLMs are open source.",
    "categories": [
      "cs.FL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.FL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15740v1",
    "published_date": "2025-05-21 16:45:43 UTC",
    "updated_date": "2025-05-21 16:45:43 UTC"
  },
  {
    "arxiv_id": "2505.15738v2",
    "title": "Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt Injection Defenses",
    "authors": [
      "Xiaoxue Yang",
      "Bozhidar Stevanoski",
      "Matthieu Meeus",
      "Yves-Alexandre de Montjoye"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed in real-world applications ranging from chatbots to agentic systems, where they are expected to process untrusted data and follow trusted instructions. Failure to distinguish between the two poses significant security risks, exploited by prompt injection attacks, which inject malicious instructions into the data to control model outputs. Model-level defenses have been proposed to mitigate prompt injection attacks. These defenses fine-tune LLMs to ignore injected instructions in untrusted data. We introduce Checkpoint-GCG, a white-box attack against fine-tuning-based defenses. Checkpoint-GCG enhances the Greedy Coordinate Gradient (GCG) attack by leveraging intermediate model checkpoints produced during fine-tuning to initialize GCG, with each checkpoint acting as a stepping stone for the next one to continuously improve attacks. First, we instantiate Checkpoint-GCG to evaluate the robustness of the state-of-the-art defenses in an auditing setup, assuming both (a) full knowledge of the model input and (b) access to intermediate model checkpoints. We show Checkpoint-GCG to achieve up to $96\\%$ attack success rate (ASR) against the strongest defense. Second, we relax the first assumption by searching for a universal suffix that would work on unseen inputs, and obtain up to $89.9\\%$ ASR against the strongest defense. Finally, we relax both assumptions by searching for a universal suffix that would transfer to similar black-box models and defenses, achieving an ASR of $63.9\\%$ against a newly released defended model from Meta.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15738v2",
    "published_date": "2025-05-21 16:43:17 UTC",
    "updated_date": "2025-10-16 12:31:18 UTC"
  },
  {
    "arxiv_id": "2505.15734v2",
    "title": "DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning",
    "authors": [
      "Gaurav Srivastava",
      "Zhenyu Bi",
      "Meng Lu",
      "Xuan Wang"
    ],
    "abstract": "Large language models (LLMs) have improved significantly in their reasoning through extensive training on massive datasets. However, relying solely on additional data for improvement is becoming increasingly impractical, highlighting the need for models to autonomously enhance their reasoning without external supervision. In this paper, we propose Debate, Train, Evolve (DTE), a novel ground truth-free training framework that uses multi-agent debate traces to evolve a single language model. We also introduce a new prompting strategy Reflect-Critique-Refine, to improve debate quality by explicitly instructing agents to critique and refine their reasoning. Extensive evaluations on seven reasoning benchmarks with six open-weight models show that our DTE framework achieve substantial improvements, with an average accuracy gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe strong cross-domain generalization, with an average accuracy gain of 5.8% on all other benchmarks, suggesting that our method captures general reasoning capabilities. Our framework code and trained models are publicly available at https://github.com/ctrl-gaurav/Debate-Train-Evolve",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2505.15734v2",
    "published_date": "2025-05-21 16:40:12 UTC",
    "updated_date": "2025-09-30 11:47:26 UTC"
  },
  {
    "arxiv_id": "2505.15722v2",
    "title": "Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities",
    "authors": [
      "Xiaoyu Luo",
      "Yiyi Chen",
      "Johannes Bjerva",
      "Qiongxiu Li"
    ],
    "abstract": "We present the first comprehensive study of Memorization in Multilingual Large Language Models (MLLMs), analyzing 95 languages using models across diverse model scales, architectures, and memorization definitions. As MLLMs are increasingly deployed, understanding their memorization behavior has become critical. Yet prior work has focused primarily on monolingual models, leaving multilingual memorization underexplored, despite the inherently long-tailed nature of training corpora. We find that the prevailing assumption, that memorization is highly correlated with training data availability, fails to fully explain memorization patterns in MLLMs. We hypothesize that the conventional focus on monolingual settings, effectively treating languages in isolation, may obscure the true patterns of memorization. To address this, we propose a novel graph-based correlation metric that incorporates language similarity to analyze cross-lingual memorization. Our analysis reveals that among similar languages, those with fewer training tokens tend to exhibit higher memorization, a trend that only emerges when cross-lingual relationships are explicitly modeled. These findings underscore the importance of a \\textit{language-aware} perspective in evaluating and mitigating memorization vulnerabilities in MLLMs. This also constitutes empirical evidence that language similarity both explains Memorization in MLLMs and underpins Cross-lingual Transferability, with broad implications for multilingual NLP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 14 tables, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.15722v2",
    "published_date": "2025-05-21 16:30:18 UTC",
    "updated_date": "2026-01-06 22:29:07 UTC"
  },
  {
    "arxiv_id": "2505.17117v6",
    "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning",
    "authors": [
      "Chen Shani",
      "Liron Soffer",
      "Dan Jurafsky",
      "Yann LeCun",
      "Ravid Shwartz-Ziv"
    ],
    "abstract": "Humans organize knowledge into compact conceptual categories that balance compression with semantic richness. Large Language Models (LLMs) exhibit impressive linguistic abilities, but whether they navigate this same compression-meaning trade-off remains unclear. We apply an Information Bottleneck framework to compare human conceptual structure with embeddings from 40+ LLMs using classic categorization benchmarks. We find that LLMs broadly align with human category boundaries, yet fall short on fine-grained semantic distinctions. Unlike humans, who maintain ``inefficient'' representations that preserve contextual nuance, LLMs aggressively compress, achieving more optimal information-theoretic compression at the cost of semantic richness. Surprisingly, encoder models outperform much larger decoder models in human alignment, suggesting that understanding and generation rely on distinct representational mechanisms. Training-dynamics analysis reveals a two-phase trajectory: rapid initial concept formation followed by architectural reorganization, during which semantic processing migrates from deep to mid-network layers as the model discovers increasingly efficient, sparser encodings. These divergent strategies, where LLMs optimize for compression and humans for adaptive utility, reveal fundamental differences between artificial and natural intelligence. This highlights the need for models that preserve the conceptual ``inefficiencies'' essential for human-like understanding.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17117v6",
    "published_date": "2025-05-21 16:29:00 UTC",
    "updated_date": "2025-12-01 22:23:22 UTC"
  },
  {
    "arxiv_id": "2505.15703v1",
    "title": "HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning",
    "authors": [
      "Xiaodong Mei",
      "Sheng Wang",
      "Jie Cheng",
      "Yingbing Chen",
      "Dan Xu"
    ],
    "abstract": "Motion forecasting represents a critical challenge in autonomous driving systems, requiring accurate prediction of surrounding agents' future trajectories. While existing approaches predict future motion states with the extracted scene context feature from historical agent trajectories and road layouts, they suffer from the information degradation during the scene feature encoding. To address the limitation, we propose HAMF, a novel motion forecasting framework that learns future motion representations with the scene context encoding jointly, to coherently combine the scene understanding and future motion state prediction. We first embed the observed agent states and map information into 1D token sequences, together with the target multi-modal future motion features as a set of learnable tokens. Then we design a unified Attention-based encoder, which synergistically combines self-attention and cross-attention mechanisms to model the scene context information and aggregate future motion features jointly. Complementing the encoder, we implement the Mamba module in the decoding stage to further preserve the consistency and correlations among the learned future motion representations, to generate the accurate and diverse final trajectories. Extensive experiments on Argoverse 2 benchmark demonstrate that our hybrid Attention-Mamba model achieves state-of-the-art motion forecasting performance with the simple and lightweight architecture.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "In submission",
    "pdf_url": "https://arxiv.org/pdf/2505.15703v1",
    "published_date": "2025-05-21 16:16:52 UTC",
    "updated_date": "2025-05-21 16:16:52 UTC"
  },
  {
    "arxiv_id": "2506.06303v4",
    "title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners",
    "authors": [
      "Kefan Song",
      "Amir Moeini",
      "Peng Wang",
      "Lei Gong",
      "Rohan Chandra",
      "Shangtong Zhang",
      "Yanjun Qi"
    ],
    "abstract": "Reinforcement learning (RL) is a framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges during the inference time of large language models (LLMs), a phenomenon we term in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, we call ICRL prompting, for inference-time self-improvement. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning during inference for self-improvement on a given task. After each response, the model receives numerical scalar feedback, denoted as a reward. In the next round, we prompt the LLM again together with a context that concatenates all prior responses and their associated rewards. We consistently observe that response quality improves as the context grows. In other words, the LLM can optimize scalar reward signals during inference, exhibiting behavior analogous to reinforcement learning. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Notably, even when the reward signals are generated by the same LLM, ICRL prompting still improves performance, highlighting a promising new paradigm for test-time scaling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06303v4",
    "published_date": "2025-05-21 16:15:01 UTC",
    "updated_date": "2026-01-07 17:58:17 UTC"
  },
  {
    "arxiv_id": "2505.15694v1",
    "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO",
    "authors": [
      "Xingyu Zhou",
      "Yulian Wu",
      "Francesco Orabona"
    ],
    "abstract": "In this paper, we theoretically investigate the effects of noisy labels in offline alignment, with a focus on the interplay between privacy and robustness against adversarial corruption. Specifically, under linear modeling assumptions, we present a unified analysis covering both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under different privacy-corruption scenarios, such as Local differential privacy-then-Corruption (LTC), where human preference labels are privatized before being corrupted by an adversary, and Corruption-then-Local differential privacy (CTL), where labels are corrupted before privacy protection. Our analysis leverages a reduction framework that reduces the offline alignment problem under linear modeling assumptions to parameter estimation in logistic regression. This framework allows us to establish an interesting separation result between LTC and CTL, demonstrating that LTC presents a greater challenge than CTL in offline alignment, even under linear models. As important by-products, our findings also advance the state-of-the-art theoretical results in offline alignment under privacy-only or corruption-only scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15694v1",
    "published_date": "2025-05-21 16:07:47 UTC",
    "updated_date": "2025-05-21 16:07:47 UTC"
  },
  {
    "arxiv_id": "2505.15693v1",
    "title": "Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives",
    "authors": [
      "Milad Kazemi",
      "Mateo Perez",
      "Fabio Somenzi",
      "Sadegh Soudjani",
      "Ashutosh Trivedi",
      "Alvaro Velasquez"
    ],
    "abstract": "Recent advances in reinforcement learning (RL) have renewed focus on the design of reward functions that shape agent behavior. Manually designing reward functions is tedious and error-prone. A principled alternative is to specify behaviors in a formal language that can be automatically translated into rewards. Omega-regular languages are a natural choice for this purpose, given their established role in formal verification and synthesis. However, existing methods using omega-regular specifications typically rely on discounted reward RL in episodic settings, with periodic resets. This setup misaligns with the semantics of omega-regular specifications, which describe properties over infinite behavior traces. In such cases, the average reward criterion and the continuing setting -- where the agent interacts with the environment over a single, uninterrupted lifetime -- are more appropriate.\n  To address the challenges of infinite-horizon, continuing tasks, we focus on absolute liveness specifications -- a subclass of omega-regular languages that cannot be violated by any finite behavior prefix, making them well-suited to the continuing setting. We present the first model-free RL framework that translates absolute liveness specifications to average-reward objectives. Our approach enables learning in communicating MDPs without episodic resetting. We also introduce a reward structure for lexicographic multi-objective optimization, aiming to maximize an external average-reward objective among the policies that also maximize the satisfaction probability of a given omega-regular specification. Our method guarantees convergence in unknown communicating MDPs and supports on-the-fly reductions that do not require full knowledge of the environment, thus enabling model-free RL. Empirical results show our average-reward approach in continuing setting outperforms discount-based methods across benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 6 figures and 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.15693v1",
    "published_date": "2025-05-21 16:06:51 UTC",
    "updated_date": "2025-05-21 16:06:51 UTC"
  },
  {
    "arxiv_id": "2505.15687v1",
    "title": "Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning",
    "authors": [
      "Zhe Xu",
      "Cheng Jin",
      "Yihui Wang",
      "Ziyi Liu",
      "Hao Chen"
    ],
    "abstract": "Multimodal pathological image understanding has garnered widespread interest due to its potential to improve diagnostic accuracy and enable personalized treatment through integrated visual and textual data. However, existing methods exhibit limited reasoning capabilities, which hamper their ability to handle complex diagnostic scenarios. Additionally, the enormous size of pathological images leads to severe computational burdens, further restricting their practical deployment. To address these limitations, we introduce a novel bilateral reinforcement learning framework comprising two synergistic branches. One reinforcement branch enhances the reasoning capability by enabling the model to learn task-specific decision processes, i.e., pathology rationales, directly from labels without explicit reasoning supervision. While the other branch dynamically allocates a tailored number of tokens to different images based on both their visual content and task context, thereby optimizing computational efficiency. We apply our method to various pathological tasks such as visual question answering, cancer subtyping, and lesion detection. Extensive experiments show an average +41.7 absolute performance improvement with 70.3% lower inference costs over the base models, achieving both reasoning accuracy and computational efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15687v1",
    "published_date": "2025-05-21 16:03:03 UTC",
    "updated_date": "2025-05-21 16:03:03 UTC"
  },
  {
    "arxiv_id": "2505.15683v4",
    "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models",
    "authors": [
      "Zishuai Zhang",
      "Hainan zhang",
      "Weihua Li",
      "Qinnan zhang",
      "jin Dong",
      "Yongxin Tong",
      "Zhiming Zheng"
    ],
    "abstract": "Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15683v4",
    "published_date": "2025-05-21 15:58:08 UTC",
    "updated_date": "2026-01-01 05:45:17 UTC"
  },
  {
    "arxiv_id": "2505.15674v2",
    "title": "UniErase: Towards Balanced and Precise Unlearning in Language Models",
    "authors": [
      "Miao Yu",
      "Liang Lin",
      "Guibin Zhang",
      "Xinfeng Li",
      "Junfeng Fang",
      "Xingrui Yu",
      "Ivor Tsang",
      "Ningyu Zhang",
      "Kun Wang",
      "Yang Wang"
    ],
    "abstract": "Large language models (LLMs) require iterative updates to address the outdated information problem, where LLM unlearning offers an approach for selective removal. However, mainstream unlearning methods primarily rely on fine-tuning techniques, which often lack precision in targeted unlearning and struggle to balance unlearning efficacy with general ability under massive and sequential settings. To bridge this gap, in this work, we introduce UniErase, a novel unlearning framework that demonstrates precision and balanced performances between knowledge unlearning and ability retaining. We first propose the Unlearning Token, which is optimized to steer LLMs toward a forgetting space. To achieve concrete unlearning behaviors, we further introduce the lightweight Unlearning Edit to efficiently associate the unlearning targets with this meta-token. Serving as a new unlearning paradigm via editing, UniErase achieves outstanding performances across batch, sequential, and precise unlearning tasks under fictitious and real-world knowledge scenarios. On the TOFU benchmark, compared with 8 baselines, UniErase, modifying only $\\sim$ \\textbf{3.66%} of the LLM parameters, outperforms the previous best-forgetting baseline by \\textbf{$\\sim$ 4.01$\\times$} for \\textbf{model ability} with even higher unlearning efficacy. Similarly, UniErase, with better ability retention, also surpasses the previous best-retaining method by \\textbf{35.96%} for \\textbf{unlearning efficacy}, showing balanced and dual top-tier performances in the current unlearning community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15674v2",
    "published_date": "2025-05-21 15:53:28 UTC",
    "updated_date": "2025-09-26 01:52:07 UTC"
  },
  {
    "arxiv_id": "2505.15671v1",
    "title": "Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification",
    "authors": [
      "Hamzeh Asgharnezhad",
      "Afshar Shamsi",
      "Roohallah Alizadehsani",
      "Arash Mohammadi",
      "Hamid Alinejad-Rokny"
    ],
    "abstract": "Knowing the uncertainty associated with the output of a deep neural network is of paramount importance in making trustworthy decisions, particularly in high-stakes fields like medical diagnosis and autonomous systems. Monte Carlo Dropout (MCD) is a widely used method for uncertainty quantification, as it can be easily integrated into various deep architectures. However, conventional MCD often struggles with providing well-calibrated uncertainty estimates. To address this, we introduce innovative frameworks that enhances MCD by integrating different search solutions namely Grey Wolf Optimizer (GWO), Bayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an uncertainty-aware loss function, thereby improving the reliability of uncertainty quantification. We conduct comprehensive experiments using different backbones, namely DenseNet121, ResNet50, and VGG16, on various datasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic dataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3% on average in terms of both conventional accuracy and uncertainty accuracy while achieving significantly better calibration. These results highlight the potential of our approach to enhance the trustworthiness of deep learning models in safety-critical applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 5 tables, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.15671v1",
    "published_date": "2025-05-21 15:50:03 UTC",
    "updated_date": "2025-05-21 15:50:03 UTC"
  },
  {
    "arxiv_id": "2505.17115v2",
    "title": "Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization",
    "authors": [
      "Ying Zhu",
      "Heng Zhou",
      "Rui Su",
      "Peiqin Zhuang",
      "Lei Bai"
    ],
    "abstract": "Recently, many approaches, such as Chain-of-Thought (CoT) prompting and Multi-Agent Debate (MAD), have been proposed to further enrich Large Language Models' (LLMs) complex problem-solving capacities in reasoning scenarios. However, these methods may fail to solve complex problems due to the lack of ability to find optimal solutions. Swarm Intelligence has been serving as a powerful tool for finding optima in the field of traditional optimization problems. To this end, we propose integrating swarm intelligence into the reasoning process by introducing a novel Agent-based Swarm Intelligence (ASI) paradigm. In this paradigm, we formulate LLM reasoning as an optimization problem and use a swarm intelligence scheme to guide a group of LLM-based agents in collaboratively searching for optimal solutions. To avoid swarm intelligence getting trapped in local optima, we further develop a Swarm Intelligence Enhancing Reasoning (SIER) framework, which develops a density-driven strategy to enhance the reasoning ability. To be specific, we propose to perform kernel density estimation and non-dominated sorting to optimize both solution quality and diversity simultaneously. In this case, SIER efficiently enhances solution space exploration through expanding the diversity of the reasoning path. Besides, a step-level quality evaluation is used to help agents improve solution quality by correcting low-quality intermediate steps. Then, we use quality thresholds to dynamically control the termination of exploration and the selection of candidate steps, enabling a more flexible and efficient reasoning process. Extensive experiments are ...",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17115v2",
    "published_date": "2025-05-21 15:48:13 UTC",
    "updated_date": "2025-05-30 08:59:59 UTC"
  },
  {
    "arxiv_id": "2505.15662v2",
    "title": "Transformer-Based Neural Quantum Digital Twins for Many-Body Quantum Simulation and Optimal Annealing Schedule Design",
    "authors": [
      "Jianlong Lu",
      "Hanqiu Peng",
      "Ying Chen"
    ],
    "abstract": "We introduce Transformer-based Neural Quantum Digital Twins (Tx-NQDTs) to simulate full adiabatic dynamics of many-body quantum systems, including ground and low-lying excited states, at low computational cost. Tx-NQDTs employ a graph-informed Transformer neural network trained to predict spectral properties (energy levels and gap locations) needed for annealing schedule design. We integrate these predictions with an adaptive annealing schedule design based on first-order adiabatic perturbation theory (FOAPT), which slows the evolution near predicted small gaps to maintain adiabaticity. Experiments on a D-Wave quantum annealer (N = 10, 15, 20 qubits, 12 control segments) show that Tx-NQDT-informed schedules significantly improve success probabilities despite hardware noise and calibration drift. The optimized schedules achieve success probabilities 2.2-11.7 percentage points higher than the default linear schedule, outperforming the D-Wave baseline in 44 of 60 cases. These results demonstrate a practical, data-driven route to improved quantum annealing performance on real hardware.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "quant-ph",
    "comment": "23 pages, 10 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.15662v2",
    "published_date": "2025-05-21 15:38:55 UTC",
    "updated_date": "2025-12-21 13:30:03 UTC"
  },
  {
    "arxiv_id": "2505.15657v2",
    "title": "LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought",
    "authors": [
      "Cheng Yan",
      "Felix Mohr",
      "Tom Viering"
    ],
    "abstract": "Sample-wise learning curves plot performance versus training set size. They are useful for studying scaling laws and speeding up hyperparameter tuning and model selection. Learning curves are often assumed to be well-behaved: monotone (i.e. improving with more data) and convex. By constructing the Learning Curves Database 1.1 (LCDB 1.1), a large-scale database with high-resolution learning curves including more modern learners (CatBoost, TabNet, RealMLP and TabPFN), we show that learning curves are less often well-behaved than previously thought. Using statistically rigorous methods, we observe significant ill-behavior in approximately 15% of the learning curves, almost twice as much as in previous estimates. We also identify which learners are to blame and show that specific learners are more ill-behaved than others. Additionally, we demonstrate that different feature scalings rarely resolve ill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such as learning curve fitting and model selection, and find it poses significant challenges, underscoring the relevance and potential of LCDB 1.1 as a challenging benchmark for future research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2025 Datasets & Benchmarks Track",
    "pdf_url": "https://arxiv.org/pdf/2505.15657v2",
    "published_date": "2025-05-21 15:32:42 UTC",
    "updated_date": "2025-10-24 13:52:39 UTC"
  },
  {
    "arxiv_id": "2505.15647v2",
    "title": "Second-Order Convergence in Private Stochastic Non-Convex Optimization",
    "authors": [
      "Youming Tao",
      "Zuyuan Zhang",
      "Dongxiao Yu",
      "Xiuzhen Cheng",
      "Falko Dressler",
      "Di Wang"
    ],
    "abstract": "We investigate the problem of finding second-order stationary points (SOSP) in differentially private (DP) stochastic non-convex optimization. Existing methods suffer from two key limitations: (i) inaccurate convergence error rate due to overlooking gradient variance in the saddle point escape analysis, and (ii) dependence on auxiliary private model selection procedures for identifying DP-SOSP, which can significantly impair utility, particularly in distributed settings. To address these issues, we propose a generic perturbed stochastic gradient descent (PSGD) framework built upon Gaussian noise injection and general gradient oracles. A core innovation of our framework is using model drift distance to determine whether PSGD escapes saddle points, ensuring convergence to approximate local minima without relying on second-order information or additional DP-SOSP identification. By leveraging the adaptive DP-SPIDER estimator as a specific gradient oracle, we develop a new DP algorithm that rectifies the convergence error rates reported in prior work. We further extend this algorithm to distributed learning with heterogeneous data, providing the first formal guarantees for finding DP-SOSP in such settings. Our analysis also highlights the detrimental impacts of private selection procedures in distributed learning under high-dimensional models, underscoring the practical benefits of our design. Numerical experiments on real-world datasets validate the efficacy of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15647v2",
    "published_date": "2025-05-21 15:25:23 UTC",
    "updated_date": "2026-01-17 16:19:28 UTC"
  },
  {
    "arxiv_id": "2505.15644v2",
    "title": "Can VLMs Detect and Localize Fine-Grained AI-Edited Images?",
    "authors": [
      "Zhen Sun",
      "Ziyi Zhang",
      "Zeren Luo",
      "Zhiyuan Zhong",
      "Zeyang Sha",
      "Tianshuo Cong",
      "Zheng Li",
      "Shiwen Cui",
      "Weiqiang Wang",
      "Jiaheng Wei",
      "Xinlei He",
      "Qi Li",
      "Qian Wang"
    ],
    "abstract": "Fine-grained detection and localization of localized image edits is crucial for assessing content authenticity, especially as modern diffusion models and image editors can produce highly realistic manipulations. However, this problem faces three key challenges: (1) most AIGC detectors produce only a global real-or-fake label without indicating where edits occur; (2) traditional computer vision methods for edit localization typically rely on costly pixel-level annotations; and (3) there is no large-scale, modern benchmark specifically targeting edited-image detection. To address these gaps, we develop an automated data-generation pipeline and construct FragFake, a large-scale benchmark of AI-edited images spanning multiple source datasets, diverse editing models, and several common edit types. Building on FragFake, we are the first to systematically study vision language models (VLMs) for edited-image classification and edited-region localization. Our experiments show that pretrained VLMs, including GPT4o, perform poorly on this task, whereas fine-tuned models such as Qwen2.5-VL achieve high accuracy and substantially higher object precision across all settings. We further explore GRPO-based RLVR training, which yields modest metric gains while improving the interpretability of model outputs. Ablation and transfer analyses reveal how data balancing, training size, LoRA rank, and training domain affect performance, and highlight both the potential and the limitations of cross-editor and cross-dataset generalization. We anticipate that this work will establish a solid foundation to facilitate and inspire subsequent research endeavors in the domain of multimodal content authenticity.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "14pages,19 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.15644v2",
    "published_date": "2025-05-21 15:22:45 UTC",
    "updated_date": "2025-12-03 09:28:05 UTC"
  },
  {
    "arxiv_id": "2505.15633v1",
    "title": "Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions",
    "authors": [
      "David Thulke",
      "Jakob Kemmler",
      "Christian Dugast",
      "Hermann Ney"
    ],
    "abstract": "Large language models that use retrieval augmented generation have the potential to unlock valuable knowledge for researchers, policymakers, and the public by making long and technical climate-related documents more accessible. While this approach can help alleviate factual hallucinations by relying on retrieved passages as additional context, its effectiveness depends on whether the model's output remains faithful to these passages. To address this, we explore the automatic assessment of faithfulness of different models in this setting. We then focus on ClimateGPT, a large language model specialised in climate science, to examine which factors in its instruction fine-tuning impact the model's faithfulness. By excluding unfaithful subsets of the model's training data, we develop ClimateGPT Faithful+, which achieves an improvement in faithfulness from 30% to 57% in supported atomic claims according to our automatic metric.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the ClimateNLP 2025 Workshop at ACL",
    "pdf_url": "https://arxiv.org/pdf/2505.15633v1",
    "published_date": "2025-05-21 15:17:38 UTC",
    "updated_date": "2025-05-21 15:17:38 UTC"
  },
  {
    "arxiv_id": "2505.15612v1",
    "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
    "authors": [
      "Wei Liu",
      "Ruochen Zhou",
      "Yiyun Deng",
      "Yuzhen Huang",
      "Junteng Liu",
      "Yuntian Deng",
      "Yizhe Zhang",
      "Junxian He"
    ],
    "abstract": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant \"self-reflections\". Resources are at https://github.com/hkust-nlp/Laser.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15612v1",
    "published_date": "2025-05-21 15:03:26 UTC",
    "updated_date": "2025-05-21 15:03:26 UTC"
  },
  {
    "arxiv_id": "2505.15607v2",
    "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning",
    "authors": [
      "David Dinucu-Jianu",
      "Jakub Macina",
      "Nico Daheim",
      "Ido Hakimi",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ],
    "abstract": "Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers. To mitigate this, we propose an online reinforcement learning (RL)-based alignment framework that can quickly adapt LLMs into effective tutors using simulated student-tutor interactions by emphasizing pedagogical quality and guided problem-solving over simply giving away answers. We use our method to train a 7B parameter tutor model without human annotations which reaches similar performance to larger proprietary models like LearnLM. We introduce a controllable reward weighting to balance pedagogical support and student solving accuracy, allowing us to trace the Pareto frontier between these two objectives. Our models better preserve reasoning capabilities than single-turn SFT baselines and can optionally enhance interpretability through thinking tags that expose the model's instructional planning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Main as an oral presentation. David Dinucu-Jianu and Jakub Macina contributed equally. Code available: https://github.com/eth-lre/PedagogicalRL",
    "pdf_url": "https://arxiv.org/pdf/2505.15607v2",
    "published_date": "2025-05-21 15:00:07 UTC",
    "updated_date": "2025-10-12 13:55:03 UTC"
  },
  {
    "arxiv_id": "2505.15596v1",
    "title": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use",
    "authors": [
      "Xinyi Lu",
      "Aditya Mahesh",
      "Zejia Shen",
      "Mitchell Dudley",
      "Larissa Sano",
      "Xu Wang"
    ],
    "abstract": "This project examines the prospect of using AI-generated feedback as suggestions to expedite and enhance human instructors' feedback provision. In particular, we focus on understanding the teaching assistants' perspectives on the quality of AI-generated feedback and how they may or may not utilize AI feedback in their own workflows. We situate our work in a foundational college Economics class, which has frequent short essay assignments. We developed an LLM-powered feedback engine that generates feedback on students' essays based on grading rubrics used by the teaching assistants (TAs). To ensure that TAs can meaningfully critique and engage with the AI feedback, we had them complete their regular grading jobs. For a randomly selected set of essays that they had graded, we used our feedback engine to generate feedback and displayed the feedback as in-text comments in a Word document. We then performed think-aloud studies with 5 TAs over 20 1-hour sessions to have them evaluate the AI feedback, contrast the AI feedback with their handwritten feedback, and share how they envision using the AI feedback if they were offered as suggestions. The study highlights the importance of providing detailed rubrics for AI to generate high-quality feedback for knowledge-intensive essays. TAs considered that using AI feedback as suggestions during their grading could expedite grading, enhance consistency, and improve overall feedback quality. We discuss the importance of decomposing the feedback generation task into steps and presenting intermediate results, in order for TAs to use the AI feedback.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To be published in AIED'2025: In Proceedings of the 26th International Conference on Artificial Intelligence in Education. The system prompt and example feedback can be found through http://github.com/UM-Lifelong-Learning-Lab/AIED2025-Exploring-LLM-Generated-Feedback-for-Economics-Essay",
    "pdf_url": "https://arxiv.org/pdf/2505.15596v1",
    "published_date": "2025-05-21 14:50:30 UTC",
    "updated_date": "2025-05-21 14:50:30 UTC"
  },
  {
    "arxiv_id": "2505.15594v1",
    "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off",
    "authors": [
      "Yury Belousov",
      "Brian Pulfer",
      "Vitaliy Kinakh",
      "Slava Voloshynovskiy"
    ],
    "abstract": "While foundation models demonstrate impressive performance across various tasks, they remain vulnerable to adversarial inputs. Current research explores various approaches to enhance model robustness, with Diffusion Denoised Smoothing emerging as a particularly promising technique. This method employs a pretrained diffusion model to preprocess inputs before model inference. Yet, its effectiveness remains largely unexplored beyond classification. We aim to address this gap by analyzing three datasets with four distinct downstream tasks under three different adversarial attack algorithms. Our findings reveal that while foundation models maintain resilience against conventional transformations, applying high-noise diffusion denoising to clean images without any distortions significantly degrades performance by as high as 57%. Low-noise diffusion settings preserve performance but fail to provide adequate protection across all attack types. Moreover, we introduce a novel attack strategy specifically targeting the diffusion process itself, capable of circumventing defenses in the low-noise regime. Our results suggest that the trade-off between adversarial robustness and performance remains a challenge to be addressed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted at the 33rd European Signal Processing Conference (EUSIPCO 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.15594v1",
    "published_date": "2025-05-21 14:49:24 UTC",
    "updated_date": "2025-05-21 14:49:24 UTC"
  },
  {
    "arxiv_id": "2505.15589v1",
    "title": "World Models as Reference Trajectories for Rapid Motor Adaptation",
    "authors": [
      "Carlos Stein Brito",
      "Daniel McNamee"
    ],
    "abstract": "Deploying learned control policies in real-world environments poses a fundamental challenge. When system dynamics change unexpectedly, performance degrades until models are retrained on new data. We introduce Reflexive World Models (RWM), a dual control framework that uses world model predictions as implicit reference trajectories for rapid adaptation. Our method separates the control problem into long-term reward maximization through reinforcement learning and robust motor execution through rapid latent control. This dual architecture achieves significantly faster adaptation with low online computational cost compared to model-based RL baselines, while maintaining near-optimal performance. The approach combines the benefits of flexible policy learning through reinforcement learning with rapid error correction capabilities, providing a principled approach to maintaining performance in high-dimensional continuous control tasks under varying dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15589v1",
    "published_date": "2025-05-21 14:46:41 UTC",
    "updated_date": "2025-05-21 14:46:41 UTC"
  },
  {
    "arxiv_id": "2505.15581v4",
    "title": "Advancing Marine Research: UWSAM Framework and UIIS10K Dataset for Precise Underwater Instance Segmentation",
    "authors": [
      "Hua Li",
      "Shijie Lian",
      "Zhiyuan Li",
      "Runmin Cong",
      "Chongyi Li",
      "Laurence T. Yang",
      "Weidong Zhang",
      "Sam Kwong"
    ],
    "abstract": "With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at https://github.com/LiamLian0727/UIIS10K.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15581v4",
    "published_date": "2025-05-21 14:36:01 UTC",
    "updated_date": "2025-09-27 04:10:30 UTC"
  },
  {
    "arxiv_id": "2505.15572v1",
    "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback",
    "authors": [
      "Wangyang Ying",
      "Haoyue Bai",
      "Nanxu Gong",
      "Xinyuan Wang",
      "Sixun Dong",
      "Haifeng Chen",
      "Yanjie Fu"
    ],
    "abstract": "The data-to-equation (Data2Eqn) task aims to discover interpretable mathematical equations that map observed values to labels, offering physical insights and broad applicability across academic and industrial domains. Genetic programming and traditional deep learning-based approaches suffer from search inefficiency and poor generalization on small task-specific datasets. Foundation models showed promise in this area, but existing approaches suffer from: 1) They are pretrained on general-purpose data distributions, making them less effective for domain-specific tasks; and 2) their training objectives focus on token-level alignment, overlooking mathematical semantics, which can lead to inaccurate equations. To address these issues, we aim to enhance the domain adaptability of foundation models for Data2Eqn tasks. In this work, we propose a reinforcement learning-based finetuning framework that directly optimizes the generation policy of a pretrained model through reward signals derived from downstream numerical fitness. Our method allows the model to adapt to specific and complex data distributions and generate mathematically meaningful equations. Extensive experiments demonstrate that our approach improves both the accuracy and robustness of equation generation under complex distributions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15572v1",
    "published_date": "2025-05-21 14:25:41 UTC",
    "updated_date": "2025-05-21 14:25:41 UTC"
  },
  {
    "arxiv_id": "2505.15559v1",
    "title": "Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes",
    "authors": [
      "Zixun Guo",
      "Simon Dixon"
    ],
    "abstract": "Moonbeam is a transformer-based foundation model for symbolic music, pretrained on a large and diverse collection of MIDI data totaling 81.6K hours of music and 18 billion tokens. Moonbeam incorporates music-domain inductive biases by capturing both absolute and relative musical attributes through the introduction of a novel domain-knowledge-inspired tokenization method and Multidimensional Relative Attention (MRA), which captures relative music information without additional trainable parameters. Leveraging the pretrained Moonbeam, we propose 2 finetuning architectures with full anticipatory capabilities, targeting 2 categories of downstream tasks: symbolic music understanding and conditional music generation (including music infilling). Our model outperforms other large-scale pretrained music models in most cases in terms of accuracy and F1 score across 3 downstream music classification tasks on 4 datasets. Moreover, our finetuned conditional music generation model outperforms a strong transformer baseline with a REMI-like tokenizer. We open-source the code, pretrained model, and generated samples on Github.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15559v1",
    "published_date": "2025-05-21 14:17:25 UTC",
    "updated_date": "2025-05-21 14:17:25 UTC"
  },
  {
    "arxiv_id": "2505.15558v1",
    "title": "Robo-DM: Data Management For Large Robot Datasets",
    "authors": [
      "Kaiyuan Chen",
      "Letian Fu",
      "David Huang",
      "Yanxiang Zhang",
      "Lawrence Yunliang Chen",
      "Huang Huang",
      "Kush Hari",
      "Ashwin Balakrishna",
      "Ted Xiao",
      "Pannag R Sanketi",
      "John Kubiatowicz",
      "Ken Goldberg"
    ],
    "abstract": "Recent results suggest that very large datasets of teleoperated robot demonstrations can be used to train transformer-based models that have the potential to generalize to new scenes, robots, and tasks. However, curating, distributing, and loading large datasets of robot trajectories, which typically consist of video, textual, and numerical modalities - including streams from multiple cameras - remains challenging. We propose Robo-DM, an efficient open-source cloud-based data management toolkit for collecting, sharing, and learning with robot data. With Robo-DM, robot datasets are stored in a self-contained format with Extensible Binary Meta Language (EBML). Robo-DM can significantly reduce the size of robot trajectory data, transfer costs, and data load time during training. Compared to the RLDS format used in OXE datasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x (lossless). Robo-DM also accelerates data retrieval by load-balancing video decoding with memory-mapped decoding caches. Compared to LeRobot, a framework that also uses lossy video compression, Robo-DM is up to 50x faster when decoding sequentially. We physically evaluate a model trained by Robo-DM with lossy compression, a pick-and-place task, and In-Context Robot Transformer. Robo-DM uses 75x compression of the original dataset and does not suffer reduction in downstream task accuracy.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Best paper finalist of IEEE ICRA 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15558v1",
    "published_date": "2025-05-21 14:17:06 UTC",
    "updated_date": "2025-05-21 14:17:06 UTC"
  },
  {
    "arxiv_id": "2505.15554v1",
    "title": "DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion",
    "authors": [
      "Wendi Zhou",
      "Ameer Saadat-Yazdi",
      "Nadin Kökciyan"
    ],
    "abstract": "Critical questions are essential resources to provoke critical thinking when encountering an argumentative text. We present our system for the Critical Questions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach leverages large language models (LLMs) with chain-of-thought prompting to generate critical questions guided by Walton's argumentation schemes. For each input intervention, we conversationally prompt LLMs to instantiate the corresponding argument scheme template to first obtain structured arguments, and then generate relevant critical questions. Following this, we rank all the available critical questions by prompting LLMs to select the top 3 most helpful questions based on the original intervention text. This combination of structured argumentation theory and step-by-step reasoning enables the generation of contextually relevant and diverse critical questions. Our pipeline achieves competitive performance in the final test set, showing its potential to foster critical thinking given argumentative text and detect missing or uninformed claims. Code available at \\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ArgMining 2025 CQs-Gen shared task",
    "pdf_url": "https://arxiv.org/pdf/2505.15554v1",
    "published_date": "2025-05-21 14:15:49 UTC",
    "updated_date": "2025-05-21 14:15:49 UTC"
  },
  {
    "arxiv_id": "2505.15553v3",
    "title": "Social Bias in Popular Question-Answering Benchmarks",
    "authors": [
      "Angelie Kraft",
      "Judith Simon",
      "Sonja Schimmler"
    ],
    "abstract": "Question-answering (QA) and reading comprehension (RC) benchmarks are commonly used for assessing the capabilities of large language models (LLMs) to retrieve and reproduce knowledge. However, we demonstrate that popular QA and RC benchmarks do not cover questions about different demographics or regions in a representative way. We perform a content analysis of 30 benchmark papers and a quantitative analysis of 20 respective benchmark datasets to learn (1) who is involved in the benchmark creation, (2) whether the benchmarks exhibit social bias, or whether this is addressed or prevented, and (3) whether the demographics of the creators and annotators correspond to particular biases in the content. Most benchmark papers analyzed provide insufficient information about those involved in benchmark creation, particularly the annotators. Notably, just one (WinoGrande) explicitly reports measures taken to address social representation issues. Moreover, the data analysis revealed gender, religion, and geographic biases across a wide range of encyclopedic, commonsense, and scholarly benchmarks. Our work adds to the mounting criticism of AI evaluation practices and shines a light on biased benchmarks being a potential source of LLM bias by incentivizing biased inference heuristics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented at the main track of the IJCNLP-AACL 2025 conference (Mumbai and Online)",
    "pdf_url": "https://arxiv.org/pdf/2505.15553v3",
    "published_date": "2025-05-21 14:14:47 UTC",
    "updated_date": "2026-01-07 10:48:53 UTC"
  },
  {
    "arxiv_id": "2505.15547v2",
    "title": "Oversmoothing, Oversquashing, Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning",
    "authors": [
      "Adrian Arnaiz-Rodriguez",
      "Federico Errica"
    ],
    "abstract": "After a renaissance phase in which researchers revisited the message-passing paradigm through the lens of deep learning, the graph machine learning community shifted its attention towards a deeper and practical understanding of message-passing's benefits and limitations. In this position paper, we notice how the fast pace of progress around the topics of oversmoothing and oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came with the consolidation of commonly accepted beliefs and assumptions that are not always true nor easy to distinguish from each other. We argue that this has led to ambiguities around the investigated problems, preventing researchers from focusing on and addressing precise research questions while causing a good amount of misunderstandings. Our contribution wants to make such common beliefs explicit and encourage critical thinking around these topics, supported by simple but noteworthy counterexamples. The hope is to clarify the distinction between the different issues and promote separate but intertwined research directions to address them.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15547v2",
    "published_date": "2025-05-21 14:11:59 UTC",
    "updated_date": "2025-06-14 07:29:48 UTC"
  },
  {
    "arxiv_id": "2505.15524v1",
    "title": "Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs",
    "authors": [
      "Lang Gao",
      "Kaiyang Wan",
      "Wei Liu",
      "Chenxi Wang",
      "Zirui Song",
      "Zixiang Xu",
      "Yanbo Wang",
      "Veselin Stoyanov",
      "Xiuying Chen"
    ],
    "abstract": "Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., \"positive\" and \"negative\"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of \"food\" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model's vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient's insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15524v1",
    "published_date": "2025-05-21 13:50:23 UTC",
    "updated_date": "2025-05-21 13:50:23 UTC"
  },
  {
    "arxiv_id": "2505.15517v2",
    "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets",
    "authors": [
      "Kaiyuan Chen",
      "Shuangyu Xie",
      "Zehan Ma",
      "Pannag R Sanketi",
      "Ken Goldberg"
    ],
    "abstract": "Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15517v2",
    "published_date": "2025-05-21 13:42:52 UTC",
    "updated_date": "2025-06-18 21:24:31 UTC"
  },
  {
    "arxiv_id": "2505.15516v1",
    "title": "Explainable embeddings with Distance Explainer",
    "authors": [
      "Christiaan Meijer",
      "E. G. Patrick Bos"
    ],
    "abstract": "While eXplainable AI (XAI) has advanced significantly, few methods address interpretability in embedded vector spaces where dimensions represent complex abstractions. We introduce Distance Explainer, a novel method for generating local, post-hoc explanations of embedded spaces in machine learning models. Our approach adapts saliency-based techniques from RISE to explain the distance between two embedded data points by assigning attribution values through selective masking and distance-ranked mask filtering. We evaluate Distance Explainer on cross-modal embeddings (image-image and image-caption pairs) using established XAI metrics including Faithfulness, Sensitivity/Robustness, and Randomization. Experiments with ImageNet and CLIP models demonstrate that our method effectively identifies features contributing to similarity or dissimilarity between embedded data points while maintaining high robustness and consistency. We also explore how parameter tuning, particularly mask quantity and selection strategy, affects explanation quality. This work addresses a critical gap in XAI research and enhances transparency and trustworthiness in deep learning applications utilizing embedded spaces.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 19 figures. Submitted to JMLR. Method implementation: https://research-software-directory.org/software/distance-explainer",
    "pdf_url": "https://arxiv.org/pdf/2505.15516v1",
    "published_date": "2025-05-21 13:42:28 UTC",
    "updated_date": "2025-05-21 13:42:28 UTC"
  },
  {
    "arxiv_id": "2505.15514v1",
    "title": "AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization",
    "authors": [
      "Soham Sane"
    ],
    "abstract": "Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm that heavily relies on accurate advantage estimates for stable and efficient training. However, raw advantage signals can exhibit significant variance, noise, and scale-related issues, impeding optimal learning performance. To address this challenge, we introduce Advantage Modulation PPO (AM-PPO), a novel enhancement of PPO that adaptively modulates advantage estimates using a dynamic, non-linear scaling mechanism. This adaptive modulation employs an alpha controller that dynamically adjusts the scaling factor based on evolving statistical properties of the advantage signals, such as their norm, variance, and a predefined target saturation level. By incorporating a tanh-based gating function driven by these adaptively scaled advantages, AM-PPO reshapes the advantage signals to stabilize gradient updates and improve the conditioning of the policy gradient landscape. Crucially, this modulation also influences value function training by providing consistent and adaptively conditioned learning targets. Empirical evaluations across standard continuous control benchmarks demonstrate that AM-PPO achieves superior reward trajectories, exhibits sustained learning progression, and significantly reduces the clipping required by adaptive optimizers. These findings underscore the potential of advantage modulation as a broadly applicable technique for enhancing reinforcement learning optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 4 Tables, 9 Figures, 11 equations",
    "pdf_url": "https://arxiv.org/pdf/2505.15514v1",
    "published_date": "2025-05-21 13:38:45 UTC",
    "updated_date": "2025-05-21 13:38:45 UTC"
  },
  {
    "arxiv_id": "2505.15507v1",
    "title": "Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning",
    "authors": [
      "Mahesh Godavarti"
    ],
    "abstract": "We introduce a new algebraic structure for multi-dimensional compositional embeddings, built on directional non-commutative monoidal operators. The core contribution of this work is this novel framework, which exhibits appealing theoretical properties (associativity along each dimension and an interchange law ensuring global consistency) while remaining compatible with modern machine learning architectures. Our construction defines a distinct composition operator circ_i for each axis i, ensuring associative combination along each axis without imposing global commutativity. Importantly, all axis-specific operators commute with one another, enforcing a global interchange law that enables consistent crossaxis compositions. This is, to our knowledge, the first approach that provides a common foundation that generalizes classical sequence-modeling paradigms (e.g., structured state-space models (SSMs) and transformer self-attention) to a unified multi-dimensional framework. For example, specific one-dimensional instances of our framework can recover the familiar affine transformation algebra, vanilla self-attention, and the SSM-style recurrence. The higher-dimensional generalizations naturally support recursive, structure-aware operations in embedding spaces. We outline several potential applications unlocked by this structure-including structured positional encodings in Transformers, directional image embeddings, and symbolic modeling of sequences or grids-indicating that it could inform future deep learning model designs. We formally establish the algebraic properties of our framework and discuss efficient implementations. Finally, as our focus is theoretical, we include no experiments here and defer empirical validation to future work, which we plan to undertake.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages submitted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15507v1",
    "published_date": "2025-05-21 13:27:14 UTC",
    "updated_date": "2025-05-21 13:27:14 UTC"
  },
  {
    "arxiv_id": "2505.15504v1",
    "title": "Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification",
    "authors": [
      "Conghao Xiong",
      "Zhengrui Guo",
      "Zhe Xu",
      "Yifei Zhang",
      "Raymond Kai-Yu Tong",
      "Si Yong Yeo",
      "Hao Chen",
      "Joseph J. Y. Sung",
      "Irwin King"
    ],
    "abstract": "Deep learning has advanced computational pathology but expert annotations remain scarce. Few-shot learning mitigates annotation burdens yet suffers from overfitting and discriminative feature mischaracterization. In addition, the current few-shot multiple instance learning (MIL) approaches leverage pretrained vision-language models to alleviate these issues, but at the cost of complex preprocessing and high computational cost. We propose a Squeeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in MIL models to address these challenges. The SR block comprises two core components: a pair of low-rank trainable matrices (squeeze pathway, SP) that reduces parameter count and imposes a bottleneck to prevent spurious feature learning, and a frozen random recalibration matrix that preserves geometric structure, diversifies feature directions, and redefines the optimization objective for the SP. We provide theoretical guarantees that the SR block can approximate any linear mapping to arbitrary precision, thereby ensuring that the performance of a standard MIL model serves as a lower bound for its SR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL models consistently outperform prior methods while requiring significantly fewer parameters and no architectural changes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15504v1",
    "published_date": "2025-05-21 13:24:47 UTC",
    "updated_date": "2025-05-21 13:24:47 UTC"
  },
  {
    "arxiv_id": "2505.15501v1",
    "title": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs",
    "authors": [
      "Federico Ranaldi",
      "Andrea Zugarini",
      "Leonardo Ranaldi",
      "Fabio Massimo Zanzotto"
    ],
    "abstract": "We introduce the concept of protoknowledge to formalize and measure how sequences of tokens encoding Knowledge Graphs are internalized during pretraining and utilized at inference time by Large Language Models (LLMs). Indeed, LLMs have demonstrated the ability to memorize vast amounts of token sequences during pretraining, and a central open question is how they leverage this memorization as reusable knowledge through generalization. We then categorize protoknowledge into lexical, hierarchical, and topological forms, varying on the type of knowledge that needs to be activated. We measure protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general properties such as semantic bias. We then investigate the impact of protoknowledge on Text-to-SPARQL performance by varying prompting strategies depending on input conditions. To this end, we adopt a novel analysis framework that assesses whether model predictions align with the successful activation of the relevant protoknowledge for each query. This methodology provides a practical tool to explore Semantic-Level Data Contamination and serves as an effective strategy for Closed-Pretraining models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15501v1",
    "published_date": "2025-05-21 13:22:34 UTC",
    "updated_date": "2025-05-21 13:22:34 UTC"
  },
  {
    "arxiv_id": "2505.15475v1",
    "title": "LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models",
    "authors": [
      "Zhanyue Qin",
      "Yue Ding",
      "Deyuan Liu",
      "Qingbin Liu",
      "Junxian Cai",
      "Xi Chen",
      "Zhiying Tu",
      "Dianhui Chu",
      "Cuiyun Gao",
      "Dianbo Sui"
    ],
    "abstract": "Nowadays, Large Language Models (LLMs) have attracted widespread attention due to their powerful performance. However, due to the unavoidable exposure to socially biased data during training, LLMs tend to exhibit social biases, particularly gender bias. To better explore and quantifying the degree of gender bias in LLMs, we propose a pair of datasets named GenBiasEval and GenHintEval, respectively. The GenBiasEval is responsible for evaluating the degree of gender bias in LLMs, accompanied by an evaluation metric named AFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is used to assess whether LLMs can provide responses consistent with prompts that contain gender hints, along with the accompanying evaluation metric UB-Score (UnBias Score). Besides, in order to mitigate gender bias in LLMs more effectively, we present the LFTF (Locating First and Then Fine-Tuning) algorithm.The algorithm first ranks specific LLM blocks by their relevance to gender bias in descending order using a metric called BMI (Block Mitigating Importance Score). Based on this ranking, the block most strongly associated with gender bias is then fine-tuned using a carefully designed loss function. Numerous experiments have shown that our proposed LFTF algorithm can significantly mitigate gender bias in LLMs while maintaining their general capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15475v1",
    "published_date": "2025-05-21 12:49:37 UTC",
    "updated_date": "2025-05-21 12:49:37 UTC"
  },
  {
    "arxiv_id": "2505.15469v1",
    "title": "A Qualitative Investigation into LLM-Generated Multilingual Code Comments and Automatic Evaluation Metrics",
    "authors": [
      "Jonathan Katzy",
      "Yongcheng Huang",
      "Gopal-Raj Panchu",
      "Maksym Ziemlewski",
      "Paris Loizides",
      "Sander Vermeulen",
      "Arie van Deursen",
      "Maliheh Izadi"
    ],
    "abstract": "Large Language Models are essential coding assistants, yet their training is predominantly English-centric. In this study, we evaluate the performance of code language models in non-English contexts, identifying challenges in their adoption and integration into multilingual workflows. We conduct an open-coding study to analyze errors in code comments generated by five state-of-the-art code models, CodeGemma, CodeLlama, CodeQwen1.5, GraniteCode, and StarCoder2 across five natural languages: Chinese, Dutch, English, Greek, and Polish. Our study yields a dataset of 12,500 labeled generations, which we publicly release. We then assess the reliability of standard metrics in capturing comment \\textit{correctness} across languages and evaluate their trustworthiness as judgment criteria. Through our open-coding investigation, we identified a taxonomy of 26 distinct error categories in model-generated code comments. They highlight variations in language cohesion, informativeness, and syntax adherence across different natural languages. Our analysis shows that, while these models frequently produce partially correct comments, modern neural metrics fail to reliably differentiate meaningful completions from random noise. Notably, the significant score overlap between expert-rated correct and incorrect comments calls into question the effectiveness of these metrics in assessing generated comments.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted PROMISE '25",
    "pdf_url": "https://arxiv.org/pdf/2505.15469v1",
    "published_date": "2025-05-21 12:45:49 UTC",
    "updated_date": "2025-05-21 12:45:49 UTC"
  },
  {
    "arxiv_id": "2505.15467v1",
    "title": "Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning",
    "authors": [
      "Yukun Zhao",
      "Lingyong Yan",
      "Zhenyang Li",
      "Shuaiqiang Wang",
      "Zhumin Chen",
      "Zhaochun Ren",
      "Dawei Yin"
    ],
    "abstract": "Large language models have achieved remarkable success in various tasks. However, it is challenging for them to learn new tasks incrementally due to catastrophic forgetting. Existing approaches rely on experience replay, optimization constraints, or task differentiation, which encounter strict limitations in real-world scenarios. To address these issues, we propose Joint Flashback Adaptation. We first introduce flashbacks -- a limited number of prompts from old tasks -- when adapting to new tasks and constrain the deviations of the model outputs compared to the original one. We then interpolate latent tasks between flashbacks and new tasks to enable jointly learning relevant latent tasks, new tasks, and flashbacks, alleviating data sparsity in flashbacks and facilitating knowledge sharing for smooth adaptation. Our method requires only a limited number of flashbacks without access to the replay data and is task-agnostic. We conduct extensive experiments on state-of-the-art large language models across 1000+ instruction-following tasks, arithmetic reasoning tasks, and general reasoning tasks. The results demonstrate the superior performance of our method in improving generalization on new tasks and reducing forgetting in old tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15467v1",
    "published_date": "2025-05-21 12:45:28 UTC",
    "updated_date": "2025-05-21 12:45:28 UTC"
  },
  {
    "arxiv_id": "2505.15447v1",
    "title": "ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning",
    "authors": [
      "Ziqiang Xu",
      "Qi Dai",
      "Tian Xie",
      "Yifan Yang",
      "Kai Qiu",
      "DongDong Chen",
      "Zuxuan Wu",
      "Chong Luo"
    ],
    "abstract": "Video understanding is inherently intention-driven-humans naturally focus on relevant frames based on their goals. Recent advancements in multimodal large language models (MLLMs) have enabled flexible query-driven reasoning; however, video-based frameworks like Video Chain-of-Thought lack direct training signals to effectively identify relevant frames. Current approaches often rely on heuristic methods or pseudo-label supervised annotations, which are both costly and limited in scalability across diverse scenarios. To overcome these challenges, we introduce ViaRL, the first framework to leverage rule-based reinforcement learning (RL) for optimizing frame selection in intention-driven video understanding. An iterated amplification strategy is adopted to perform alternating cyclic training in the video CoT system, where each component undergoes iterative cycles of refinement to improve its capabilities. ViaRL utilizes the answer accuracy of a downstream model as a reward signal to train a frame selector through trial-and-error, eliminating the need for expensive annotations while closely aligning with human-like learning processes. Comprehensive experiments across multiple benchmarks, including VideoMME, LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior temporal grounding performance and robust generalization across diverse video understanding tasks, highlighting its effectiveness and scalability. Notably, ViaRL achieves a nearly 15\\% improvement on Needle QA, a subset of MLVU, which is required to search a specific needle within a long video and regarded as one of the most suitable benchmarks for evaluating temporal grounding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15447v1",
    "published_date": "2025-05-21 12:29:40 UTC",
    "updated_date": "2025-05-21 12:29:40 UTC"
  },
  {
    "arxiv_id": "2505.15444v1",
    "title": "Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization",
    "authors": [
      "Yutao Zhu",
      "Jiajie Jin",
      "Hongjin Qian",
      "Zheng Liu",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "abstract": "Existing studies have optimized retrieval-augmented generation (RAG) across various sub-tasks, such as query understanding and retrieval refinement, but integrating these optimizations into a unified framework remains challenging. To tackle this problem, this work proposes RoleRAG, a unified RAG framework that achieves efficient multi-task processing through role-specific token optimization. RoleRAG comprises six modules, each handling a specific sub-task within the RAG process. Additionally, we introduce a query graph to represent the decomposition of the query, which can be dynamically resolved according to the decomposing state. All modules are driven by the same underlying LLM, distinguished by task-specific role tokens that are individually optimized. This design allows RoleRAG to dynamically activate different modules within a single LLM instance, thereby streamlining deployment and reducing resource consumption. Experimental results on five open-domain question-answering datasets demonstrate the effectiveness, generalizability, and flexibility of our framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15444v1",
    "published_date": "2025-05-21 12:25:12 UTC",
    "updated_date": "2025-05-21 12:25:12 UTC"
  },
  {
    "arxiv_id": "2505.15441v4",
    "title": "Octic Vision Transformers: Quicker ViTs Through Equivariance",
    "authors": [
      "David Nordström",
      "Johan Edstedt",
      "Fredrik Kahl",
      "Georg Bökman"
    ],
    "abstract": "Why are state-of-the-art Vision Transformers (ViTs) not designed to exploit natural geometric symmetries such as 90-degree rotations and reflections? In this paper, we argue that there is no fundamental reason, and what has been missing is an efficient implementation. To this end, we introduce Octic Vision Transformers (octic ViTs) which rely on octic group equivariance to capture these symmetries. In contrast to prior equivariant models that increase computational cost, our octic linear layers achieve 5.33x reductions in FLOPs and up to 8x reductions in memory compared to ordinary linear layers. In full octic ViT blocks the computational reductions approach the reductions in the linear layers with increased embedding dimension. We study two new families of ViTs, built from octic blocks, that are either fully octic equivariant or break equivariance in the last part of the network. Training octic ViTs supervised (DeiT-III) and unsupervised (DINOv2) on ImageNet-1K, we find that they match baseline accuracy while at the same time providing substantial efficiency gains.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15441v4",
    "published_date": "2025-05-21 12:22:53 UTC",
    "updated_date": "2025-09-30 15:21:07 UTC"
  },
  {
    "arxiv_id": "2505.18200v2",
    "title": "CrossRF: A Domain-Invariant Deep Learning Approach for RF Fingerprinting",
    "authors": [
      "Fahrettin Emin Tiras",
      "Hayriye Serra Altinoluk"
    ],
    "abstract": "Radio Frequency (RF) fingerprinting offers a promising approach for drone identification and security, although it suffers from significant performance degradation when operating on different transmission channels. This paper presents CrossRF, a domain-invariant deep learning approach that addresses the problem of cross-channel RF fingerprinting for Unmanned Aerial Vehicle (UAV) identification. Our approach aims to minimize the domain gap between different RF channels by using adversarial learning to train a more robust model that maintains consistent identification performance despite channel variations. We validate our approach using the UAVSig dataset, comprising real-world over-the-air RF signals from identical drone models operating across several frequency channels, ensuring that the findings correspond to real-world scenarios. The experimental results show CrossRF's efficiency, achieving up to 99.03% accuracy when adapting from Channel 3 to Channel 4, compared to only 26.39% using conventional methods. The model maintains robust performance in more difficult multi-channel scenarios (87.57% accuracy adapting from Channels 1,3 to 2,4) and achieves 89.45% accuracy with 0.9 precision for controller classification. These results confirm CrossRF's ability to significantly reduce performance degradation due to cross-channel variations while maintaining high identification accuracy with minimal training data requirements, making it particularly suitable for practical drone security applications.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "The authors have decided to withdraw this preprint due to internal review and authorship concerns",
    "pdf_url": "https://arxiv.org/pdf/2505.18200v2",
    "published_date": "2025-05-21 12:20:10 UTC",
    "updated_date": "2025-10-18 15:05:07 UTC"
  },
  {
    "arxiv_id": "2505.15433v1",
    "title": "Set-LLM: A Permutation-Invariant LLM",
    "authors": [
      "Beni Egressy",
      "Jan Stühmer"
    ],
    "abstract": "While large language models (LLMs) demonstrate impressive capabilities across numerous applications, their robustness remains a critical concern. This paper is motivated by a specific vulnerability: the order sensitivity of LLMs. This vulnerability manifests itself as the order bias observed when LLMs decide between possible options (for example, a preference for the first option) and the tendency of LLMs to provide different answers when options are reordered. The use cases for this scenario extend beyond the classical case of multiple-choice question answering to the use of LLMs as automated evaluators in AI pipelines, comparing output generated by different models. We introduce Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the processing of mixed set-text inputs with permutation invariance guarantees. The adaptations involve a new attention mask and new positional encodings specifically designed for sets. We provide a theoretical proof of invariance and demonstrate through experiments that Set-LLM can be trained effectively, achieving comparable or improved performance and maintaining the runtime of the original model, while eliminating order sensitivity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15433v1",
    "published_date": "2025-05-21 12:14:26 UTC",
    "updated_date": "2025-05-21 12:14:26 UTC"
  },
  {
    "arxiv_id": "2505.15429v1",
    "title": "Uncertainty Quantification in SVM prediction",
    "authors": [
      "Pritam Anand"
    ],
    "abstract": "This paper explores Uncertainty Quantification (UQ) in SVM predictions, particularly for regression and forecasting tasks. Unlike the Neural Network, the SVM solutions are typically more stable, sparse, optimal and interpretable. However, there are only few literature which addresses the UQ in SVM prediction. At first, we provide a comprehensive summary of existing Prediction Interval (PI) estimation and probabilistic forecasting methods developed in the SVM framework and evaluate them against the key properties expected from an ideal PI model. We find that none of the existing SVM PI models achieves a sparse solution. To introduce sparsity in SVM model, we propose the Sparse Support Vector Quantile Regression (SSVQR) model, which constructs PIs and probabilistic forecasts by solving a pair of linear programs. Further, we develop a feature selection algorithm for PI estimation using SSVQR that effectively eliminates a significant number of features while improving PI quality in case of high-dimensional dataset. Finally we extend the SVM models in Conformal Regression setting for obtaining more stable prediction set with finite test set guarantees. Extensive experiments on artificial, real-world benchmark datasets compare the different characteristics of both existing and proposed SVM-based PI estimation methods and also highlight the advantages of the feature selection in PI estimation. Furthermore, we compare both, the existing and proposed SVM-based PI estimation models, with modern deep learning models for probabilistic forecasting tasks on benchmark datasets. Furthermore, SVM models show comparable or superior performance to modern complex deep learning models for probabilistic forecasting task in our experiments.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15429v1",
    "published_date": "2025-05-21 12:11:07 UTC",
    "updated_date": "2025-05-21 12:11:07 UTC"
  },
  {
    "arxiv_id": "2505.15427v1",
    "title": "Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions",
    "authors": [
      "Zhiwen Li",
      "Die Chen",
      "Mingyuan Fan",
      "Cen Chen",
      "Yaliang Li",
      "Yanhao Wang",
      "Wenmeng Zhou"
    ],
    "abstract": "The remarkable ability of diffusion models to generate high-fidelity images has led to their widespread adoption. However, concerns have also arisen regarding their potential to produce Not Safe for Work (NSFW) content and exhibit social biases, hindering their practical use in real-world applications. In response to this challenge, prior work has focused on employing security filters to identify and exclude toxic text, or alternatively, fine-tuning pre-trained diffusion models to erase sensitive concepts. Unfortunately, existing methods struggle to achieve satisfactory performance in the sense that they can have a significant impact on the normal model output while still failing to prevent the generation of harmful content in some cases. In this paper, we propose a novel self-discovery approach to identifying a semantic direction vector in the embedding space to restrict text embedding within a safe region. Our method circumvents the need for correcting individual words within the input text and steers the entire text prompt towards a safe region in the embedding space, thereby enhancing model robustness against all possibly unsafe prompts. In addition, we employ Low-Rank Adaptation (LoRA) for semantic direction vector initialization to reduce the impact on the model performance for other semantics. Furthermore, our method can also be integrated with existing methods to improve their social responsibility. Extensive experiments on benchmark datasets demonstrate that our method can effectively reduce NSFW content and mitigate social bias generated by diffusion models compared to several state-of-the-art baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15427v1",
    "published_date": "2025-05-21 12:10:26 UTC",
    "updated_date": "2025-05-21 12:10:26 UTC"
  },
  {
    "arxiv_id": "2505.15420v2",
    "title": "Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries",
    "authors": [
      "Yuhao Wang",
      "Wenjie Qu",
      "Shengfang Zhai",
      "Yanze Jiang",
      "Zichen Liu",
      "Yue Liu",
      "Yinpeng Dong",
      "Jiaheng Zhang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by incorporating external knowledge bases, but this may expose them to extraction attacks, leading to potential copyright and privacy risks. However, existing extraction methods typically rely on malicious inputs such as prompt injection or jailbreaking, making them easily detectable via input- or output-level detection. In this paper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts Knowledge Extraction on RAG systems through benign queries. Specifically, IKEA first leverages anchor concepts-keywords related to internal knowledge-to generate queries with a natural appearance, and then designs two mechanisms that lead anchor concepts to thoroughly \"explore\" the RAG's knowledge: (1) Experience Reflection Sampling, which samples anchor concepts based on past query-response histories, ensuring their relevance to the topic; (2) Trust Region Directed Mutation, which iteratively mutates anchor concepts under similarity constraints to further exploit the embedding space. Extensive experiments demonstrate IKEA's effectiveness under various defenses, surpassing baselines by over 80% in extraction efficiency and 90% in attack success rate. Moreover, the substitute RAG system built from IKEA's extractions shows comparable performance to the original RAG and outperforms those based on baselines across multiple evaluation tasks, underscoring the stealthy copyright infringement risk in RAG systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15420v2",
    "published_date": "2025-05-21 12:04:42 UTC",
    "updated_date": "2025-09-30 10:36:11 UTC"
  },
  {
    "arxiv_id": "2505.15868v1",
    "title": "An Inclusive Foundation Model for Generalizable Cytogenetics in Precision Oncology",
    "authors": [
      "Changchun Yang",
      "Weiqian Dai",
      "Yilan Zhang",
      "Siyuan Chen",
      "Jingdong Hu",
      "Junkai Su",
      "Yuxuan Chen",
      "Ao Xu",
      "Na Li",
      "Xin Gao",
      "Yongguo Yu"
    ],
    "abstract": "Chromosome analysis is vital for diagnosing genetic disorders and guiding cancer therapy decisions through the identification of somatic clonal aberrations. However, developing an AI model are hindered by the overwhelming complexity and diversity of chromosomal abnormalities, requiring extensive annotation efforts, while automated methods remain task-specific and lack generalizability due to the scarcity of comprehensive datasets spanning diverse resource conditions. Here, we introduce CHROMA, a foundation model for cytogenomics, designed to overcome these challenges by learning generalizable representations of chromosomal abnormalities. Pre-trained on over 84,000 specimens (~4 million chromosomal images) via self-supervised learning, CHROMA outperforms other methods across all types of abnormalities, even when trained on fewer labelled data and more imbalanced datasets. By facilitating comprehensive mapping of instability and clonal leisons across various aberration types, CHROMA offers a scalable and generalizable solution for reliable and automated clinical analysis, reducing the annotation workload for experts and advancing precision oncology through the early detection of rare genomic abnormalities, enabling broad clinical AI applications and making advanced genomic analysis more accessible.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "q-bio.QM",
    "comment": "These authors contributed equally to this work: Changchun Yang, Weiqian Dai, Yilan Zhang",
    "pdf_url": "https://arxiv.org/pdf/2505.15868v1",
    "published_date": "2025-05-21 12:03:37 UTC",
    "updated_date": "2025-05-21 12:03:37 UTC"
  },
  {
    "arxiv_id": "2505.15418v1",
    "title": "Guided Policy Optimization under Partial Observability",
    "authors": [
      "Yueheng Li",
      "Guangming Xie",
      "Zongqing Lu"
    ],
    "abstract": "Reinforcement Learning (RL) in partially observable environments poses significant challenges due to the complexity of learning under uncertainty. While additional information, such as that available in simulations, can enhance training, effectively leveraging it remains an open problem. To address this, we introduce Guided Policy Optimization (GPO), a framework that co-trains a guider and a learner. The guider takes advantage of privileged information while ensuring alignment with the learner's policy that is primarily trained via imitation learning. We theoretically demonstrate that this learning scheme achieves optimality comparable to direct RL, thereby overcoming key limitations inherent in existing approaches. Empirical evaluations show strong performance of GPO across various tasks, including continuous control with partial observability and noise, and memory-based challenges, significantly outperforming existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.15418v1",
    "published_date": "2025-05-21 12:01:08 UTC",
    "updated_date": "2025-05-21 12:01:08 UTC"
  },
  {
    "arxiv_id": "2505.15410v1",
    "title": "ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs",
    "authors": [
      "Bahar Radmehr",
      "Ekaterina Shved",
      "Fatma Betül Güreş",
      "Adish Singla",
      "Tanja Käser"
    ],
    "abstract": "Clickstream data from digital learning environments offer valuable insights into students' learning behaviors, but are challenging to interpret due to their high dimensionality and granularity. Prior approaches have relied mainly on handcrafted features, expert labeling, clustering, or supervised models, therefore often lacking generalizability and scalability. In this work, we introduce ClickSight, an in-context Large Language Model (LLM)-based pipeline that interprets student clickstreams to reveal their learning strategies. ClickSight takes raw clickstreams and a list of learning strategies as input and generates textual interpretations of students' behaviors during interaction. We evaluate four different prompting strategies and investigate the impact of self-refinement on interpretation quality. Our evaluation spans two open-ended learning environments and uses a rubric-based domain-expert evaluation. Results show that while LLMs can reasonably interpret learning strategies from clickstreams, interpretation quality varies by prompting strategy, and self-refinement offers limited improvement. ClickSight demonstrates the potential of LLMs to generate theory-driven insights from educational interaction data.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in Latebreaking results track in AIED 2025(26th International Conference on Artificial Intelligence in Education JULY 22-26, 2025 PALERMO, ITALY)",
    "pdf_url": "https://arxiv.org/pdf/2505.15410v1",
    "published_date": "2025-05-21 11:52:57 UTC",
    "updated_date": "2025-05-21 11:52:57 UTC"
  },
  {
    "arxiv_id": "2505.15406v1",
    "title": "Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models",
    "authors": [
      "Zirui Song",
      "Qian Jiang",
      "Mingxuan Cui",
      "Mingzhe Li",
      "Lang Gao",
      "Zeyu Zhang",
      "Zixiang Xu",
      "Yanbo Wang",
      "Chenxi Wang",
      "Guangxian Ouyang",
      "Zhenhao Chen",
      "Xiuying Chen"
    ],
    "abstract": "The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "We release AJailBench, including both static and optimized adversarial data, to facilitate future research: https://github.com/mbzuai-nlp/AudioJailbreak",
    "pdf_url": "https://arxiv.org/pdf/2505.15406v1",
    "published_date": "2025-05-21 11:47:47 UTC",
    "updated_date": "2025-05-21 11:47:47 UTC"
  },
  {
    "arxiv_id": "2505.15402v1",
    "title": "Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning",
    "authors": [
      "Junchuan Zhao",
      "Xintong Wang",
      "Ye Wang"
    ],
    "abstract": "Recent advances in discrete audio codecs have significantly improved speech representation modeling, while codec language models have enabled in-context learning for zero-shot speech synthesis. Inspired by this, we propose a voice conversion (VC) model within the VALLE-X framework, leveraging its strong in-context learning capabilities for speaker adaptation. To enhance prosody control, we introduce a prosody-aware audio codec encoder (PACE) module, which isolates and refines prosody from other sources, improving expressiveness and control. By integrating PACE into our VC model, we achieve greater flexibility in prosody manipulation while preserving speaker timbre. Experimental evaluation results demonstrate that our approach outperforms baseline VC systems in prosody preservation, timbre consistency, and overall naturalness, surpassing baseline VC systems.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.15402v1",
    "published_date": "2025-05-21 11:42:49 UTC",
    "updated_date": "2025-05-21 11:42:49 UTC"
  },
  {
    "arxiv_id": "2505.15400v2",
    "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning",
    "authors": [
      "Xiaoyun Zhang",
      "Jingqing Ruan",
      "Xing Ma",
      "Yawen Zhu",
      "Haodong Zhao",
      "Hao Li",
      "Jiansong Chen",
      "Ke Zeng",
      "Xunliang Cai"
    ],
    "abstract": "Large reasoning models (LRMs) achieve remarkable performance via long reasoning chains, but often incur excessive computational overhead due to redundant reasoning, especially on simple tasks. In this work, we systematically quantify the upper bounds of LRMs under both Long-Thinking and No-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery Mechanism\" where models implicitly supplement reasoning during answer generation. Building on this insight, we propose Adaptive Self-Recovery Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables implicit recovery. By introducing accuracy-aware length reward regulation, ASRR adaptively allocates reasoning effort according to problem difficulty, achieving high efficiency with negligible performance sacrifice. Experiments across multiple benchmarks and models show that, compared with GRPO, ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates on safety benchmarks (up to +21.7%). Our results highlight the potential of ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15400v2",
    "published_date": "2025-05-21 11:41:39 UTC",
    "updated_date": "2025-05-23 02:38:57 UTC"
  },
  {
    "arxiv_id": "2505.15386v2",
    "title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection",
    "authors": [
      "Yiming Huang",
      "Junyan Zhang",
      "Zihao Wang",
      "Biquan Bie",
      "Yunzhong Qiu",
      "Yi R. Fung",
      "Xinlei He"
    ],
    "abstract": "Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. While previous works improved the capability of hallucination detection by measuring uncertainty, they all lack the ability to explain the provenance behind why hallucinations occur, i.e., which part of the inputs tends to trigger hallucinations. Recent works on the prompt attack indicate that uncertainty exists in semantic propagation, where attention mechanisms gradually fuse local token information into high-level semantics across layers. Meanwhile, uncertainty also emerges in language generation, due to its probability-based selection of high-level semantics for sampled generations. Based on that, we propose RePPL to recalibrate uncertainty measurement by these two aspects, which dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form as total score. Experiments show that our method achieves the best comprehensive detection performance across various QA datasets on advanced models (average AUC of 0.833), and our method is capable of producing token-level uncertainty scores as explanations for the hallucination. Leveraging these scores, we preliminarily find the chaotic pattern of hallucination and showcase its promising usage.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15386v2",
    "published_date": "2025-05-21 11:23:05 UTC",
    "updated_date": "2025-08-26 12:55:45 UTC"
  },
  {
    "arxiv_id": "2505.17108v2",
    "title": "REMS: a unified solution representation, problem modeling and metaheuristic algorithm design for general combinatorial optimization problems",
    "authors": [
      "Aijuan Song",
      "Guohua Wu"
    ],
    "abstract": "Combinatorial optimization problems (COPs) with discrete variables and finite search space are critical across numerous fields, and solving them in metaheuristic algorithms is popular. However, addressing a specific COP typically requires developing a tailored and handcrafted algorithm. Even minor adjustments, such as constraint changes, may necessitate algorithm redevelopment. Therefore, establishing a framework for formulating diverse COPs into a unified paradigm and designing reusable metaheuristic algorithms is valuable. A COP can be typically viewed as the process of giving resources to perform specific tasks, subjecting to given constraints. Motivated by this, a resource-centered modeling and solving framework (REMS) is introduced for the first time. We first extract and define resources and tasks from a COP. Subsequently, given predetermined resources, the solution structure is unified as assigning tasks to resources, from which variables, objectives, and constraints can be derived and a problem model is constructed. To solve the modeled COPs, several fundamental operators are designed based on the unified solution structure, including the initial solution, neighborhood structure, destruction and repair, crossover, and ranking. These operators enable the development of various metaheuristic algorithms. Specially, 4 single-point-based algorithms and 1 population-based algorithm are configured herein. Experiments on 10 COPs, covering routing, location, loading, assignment, scheduling, and graph coloring problems, show that REMS can model these COPs within the unified paradigm and effectively solve them with the designed metaheuristic algorithms. Furthermore, REMS is more competitive than GUROBI and SCIP in tackling large-scale instances and complex COPs, and outperforms OR-TOOLS on several challenging COPs.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Withdrawn for substantial modifications in the method and the experiment",
    "pdf_url": "https://arxiv.org/pdf/2505.17108v2",
    "published_date": "2025-05-21 11:21:58 UTC",
    "updated_date": "2025-09-15 11:37:45 UTC"
  },
  {
    "arxiv_id": "2505.15380v2",
    "title": "Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding",
    "authors": [
      "Zijian Lin",
      "Yang Zhang",
      "Yougen Yuan",
      "Yuming Yan",
      "Jinjiang Liu",
      "Zhiyong Wu",
      "Pengfei Hu",
      "Qun Yu"
    ],
    "abstract": "Modern autoregressive speech synthesis models leveraging language models have demonstrated remarkable performance. However, the sequential nature of next token prediction in these models leads to significant latency, hindering their deployment in scenarios where inference speed is critical. In this work, we propose Speech Speculative Decoding (SSD), a novel framework for autoregressive speech synthesis acceleration. Specifically, our method employs a lightweight draft model to generate candidate token sequences, which are subsequently verified in parallel by the target model using the proposed SSD framework. Experimental results demonstrate that SSD achieves a significant speedup of 1.4x compared with conventional autoregressive decoding, while maintaining high fidelity and naturalness. Subjective evaluations further validate the effectiveness of SSD in preserving the perceptual quality of the target model while accelerating inference.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by INTERSPEECH 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15380v2",
    "published_date": "2025-05-21 11:17:04 UTC",
    "updated_date": "2025-06-03 03:01:24 UTC"
  },
  {
    "arxiv_id": "2505.17107v1",
    "title": "CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution",
    "authors": [
      "Minghao Shao",
      "Haoran Xi",
      "Nanda Rani",
      "Meet Udeshi",
      "Venkata Sai Charan Putrevu",
      "Kimberly Milner",
      "Brendan Dolan-Gavitt",
      "Sandeep Kumar Shukla",
      "Prashanth Krishnamurthy",
      "Farshad Khorrami",
      "Ramesh Karri",
      "Muhammad Shafique"
    ],
    "abstract": "Large Language Model (LLM) agents can automate cybersecurity tasks and can adapt to the evolving cybersecurity landscape without re-engineering. While LLM agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF) competitions, they have two key limitations: accessing latest cybersecurity expertise beyond training data, and integrating new knowledge into complex task planning. Knowledge-based approaches that incorporate technical understanding into the task-solving automation can tackle these limitations. We present CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity capability through three core mechanisms: contextual decomposition of task-critical information, iterative self-reflected knowledge retrieval, and knowledge-hint injection that transforms insights into adaptive attack strategies. Comprehensive evaluations with different configurations show CRAKEN's effectiveness in multi-stage vulnerability detection and exploitation compared to previous approaches. Our extensible architecture establishes new methodologies for embedding new security knowledge into LLM-driven cybersecurity agentic systems. With a knowledge database of CTF writeups, CRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works by 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution. We make our framework open source to public https://github.com/NYU-LLM-CTF/nyuctf_agents_craken.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17107v1",
    "published_date": "2025-05-21 11:01:11 UTC",
    "updated_date": "2025-05-21 11:01:11 UTC"
  },
  {
    "arxiv_id": "2505.15367v3",
    "title": "Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition",
    "authors": [
      "Dasol Choi",
      "Seunghyun Lee",
      "Youngsook Song"
    ],
    "abstract": "Vision-Language Models (VLMs) have shown capabilities in interpreting visual content, but their reliability in safety-critical scenarios remains insufficiently explored. We introduce VERI, a diagnostic benchmark comprising 200 synthetic images (100 contrastive pairs) and an additional 50 real-world images (25 pairs) for validation. Each emergency scene is paired with a visually similar but safe counterpart through human verification. Using a two-stage evaluation protocol (risk identification and emergency response), we assess 17 VLMs across medical emergencies, accidents, and natural disasters. Our analysis reveals an \"overreaction problem\": models achieve high recall (70-100%) but suffer from low precision, misclassifying 31-96% of safe situations as dangerous. Seven safe scenarios were universally misclassified by all models. This \"better-safe-than-sorry\" bias stems from contextual overinterpretation (88-98% of errors). Both synthetic and real-world datasets confirm these systematic patterns, challenging VLM reliability in safety-critical applications. Addressing this requires enhanced contextual reasoning in ambiguous visual situations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15367v3",
    "published_date": "2025-05-21 10:57:40 UTC",
    "updated_date": "2025-09-27 05:26:41 UTC"
  },
  {
    "arxiv_id": "2507.13354v2",
    "title": "Physical models realizing the transformer architecture of large language models",
    "authors": [
      "Zeqian Chen"
    ],
    "abstract": "The introduction of the transformer architecture in 2017 marked the most striking advancement in natural language processing. The transformer is a model architecture relying entirely on an attention mechanism to draw global dependencies between input and output. However, we believe there is a gap in our theoretical understanding of what the transformer is, and how it works physically. From a physical perspective on modern chips, such as those chips under 28nm, modern intelligent machines should be regarded as open quantum systems beyond conventional statistical systems. Thereby, in this paper, we construct physical models realizing large language models based on a transformer architecture as open quantum systems in the Fock space over the Hilbert space of tokens. Our physical models underlie the transformer architecture for large language models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "math-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, minor changes, Refs [3, 13, 15] added",
    "pdf_url": "https://arxiv.org/pdf/2507.13354v2",
    "published_date": "2025-05-21 10:53:05 UTC",
    "updated_date": "2025-07-22 09:01:10 UTC"
  },
  {
    "arxiv_id": "2505.15358v2",
    "title": "Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model",
    "authors": [
      "Angelique Mangubat",
      "Shane Gilroy"
    ],
    "abstract": "Road safety is a critical challenge, particularly for cyclists, who are among the most vulnerable road users. This study aims to enhance road safety by proposing a novel benchmark for bicycle occlusion level classification using advanced computer vision techniques. Utilizing a parts-based detection model, images are annotated and processed through a custom image detection pipeline. A novel method of bicycle occlusion level is proposed to objectively quantify the visibility and occlusion level of bicycle semantic parts. The findings indicate that the model robustly quantifies the visibility and occlusion level of bicycles, a significant improvement over the subjective methods used by the current state of the art. Widespread use of the proposed methodology will facilitate accurate performance reporting of cyclist detection algorithms for occluded cyclists, informing the development of more robust vulnerable road user detection methods for autonomous vehicles.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15358v2",
    "published_date": "2025-05-21 10:42:41 UTC",
    "updated_date": "2025-05-22 08:25:18 UTC"
  },
  {
    "arxiv_id": "2505.15345v2",
    "title": "Hadamax Encoding: Elevating Performance in Model-Free Atari",
    "authors": [
      "Jacob E. Kooi",
      "Zhao Yang",
      "Vincent François-Lavet"
    ],
    "abstract": "Neural network architectures have a large impact in machine learning. In reinforcement learning, network architectures have remained notably simple, as changes often lead to small gains in performance. This work introduces a novel encoder architecture for pixel-based model-free reinforcement learning. The Hadamax (\\textbf{Hada}mard \\textbf{max}-pooling) encoder achieves state-of-the-art performance by max-pooling Hadamard products between GELU-activated parallel hidden layers. Based on the recent PQN algorithm, the Hadamax encoder achieves state-of-the-art model-free performance in the Atari-57 benchmark. Specifically, without applying any algorithmic hyperparameter modifications, Hadamax-PQN achieves an 80\\% performance gain over vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility, the full code is available on \\href{https://github.com/Jacobkooi/Hadamax}{GitHub}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15345v2",
    "published_date": "2025-05-21 10:19:49 UTC",
    "updated_date": "2025-05-23 11:18:10 UTC"
  },
  {
    "arxiv_id": "2505.15337v3",
    "title": "Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors",
    "authors": [
      "Hao Fang",
      "Jiawei Kong",
      "Tianqu Zhuang",
      "Yixiang Qiu",
      "Kuofeng Gao",
      "Bin Chen",
      "Shu-Tao Xia",
      "Yaowei Wang",
      "Min Zhang"
    ],
    "abstract": "The misuse of large language models (LLMs), such as academic plagiarism, has driven the development of detectors to identify LLM-generated texts. To bypass these detectors, paraphrase attacks have emerged to purposely rewrite these texts to evade detection. Despite the success, existing methods require substantial data and computational budgets to train a specialized paraphraser, and their attack efficacy greatly reduces when faced with advanced detection algorithms. To address this, we propose \\textbf{Co}ntrastive \\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that effectively deceives text detectors using off-the-shelf LLMs. The first step is to carefully craft instructions that encourage LLMs to produce more human-like texts. Nonetheless, we observe that the inherent statistical biases of LLMs can still result in some generated texts carrying certain machine-like attributes that can be captured by detectors. To overcome this, CoPA constructs an auxiliary machine-like word distribution as a contrast to the human-like distribution generated by the LLM. By subtracting the machine-like patterns from the human-like distribution during the decoding process, CoPA is able to produce sentences that are less discernible by text detectors. Our theoretical analysis suggests the superiority of the proposed attack. Extensive experiments validate the effectiveness of CoPA in fooling text detectors across various scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP-2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15337v3",
    "published_date": "2025-05-21 10:08:39 UTC",
    "updated_date": "2025-09-10 07:03:03 UTC"
  },
  {
    "arxiv_id": "2505.15333v1",
    "title": "Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation",
    "authors": [
      "Yuhao Zhang",
      "Xiangnan Ma",
      "Kaiqi Kou",
      "Peizhuo Liu",
      "Weiqiao Shan",
      "Benyou Wang",
      "Tong Xiao",
      "Yuxin Huang",
      "Zhengtao Yu",
      "Jingbo Zhu"
    ],
    "abstract": "The success of building textless speech-to-speech translation (S2ST) models has attracted much attention. However, S2ST still faces two main challenges: 1) extracting linguistic features for various speech signals, called cross-modal (CM), and 2) learning alignment of difference languages in long sequences, called cross-lingual (CL). We propose the unit language to overcome the two modeling challenges. The unit language can be considered a text-like representation format, constructed using $n$-gram language modeling. We implement multi-task learning to utilize the unit language in guiding the speech modeling process. Our initial results reveal a conflict when applying source and target unit languages simultaneously. We propose task prompt modeling to mitigate this conflict. We conduct experiments on four languages of the Voxpupil dataset. Our method demonstrates significant improvements over a strong baseline and achieves performance comparable to models trained with text.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.15333v1",
    "published_date": "2025-05-21 10:05:25 UTC",
    "updated_date": "2025-05-21 10:05:25 UTC"
  },
  {
    "arxiv_id": "2505.15311v2",
    "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning",
    "authors": [
      "Yurun Yuan",
      "Fan Chen",
      "Zeyu Jia",
      "Alexander Rakhlin",
      "Tengyang Xie"
    ],
    "abstract": "Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and operates with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM consistently outperforms policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15311v2",
    "published_date": "2025-05-21 09:41:53 UTC",
    "updated_date": "2025-11-12 05:29:12 UTC"
  },
  {
    "arxiv_id": "2505.15308v1",
    "title": "BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution",
    "authors": [
      "Ji Guo",
      "Xiaolei Wen",
      "Wenbo Jiang",
      "Cheng Huang",
      "Jinjin Li",
      "Hongwei Li"
    ],
    "abstract": "With the widespread application of super-resolution (SR) in various fields, researchers have begun to investigate its security. Previous studies have demonstrated that SR models can also be subjected to backdoor attacks through data poisoning, affecting downstream tasks. A backdoor SR model generates an attacker-predefined target image when given a triggered image while producing a normal high-resolution (HR) output for clean images. However, prior backdoor attacks on SR models have primarily focused on the stealthiness of poisoned low-resolution (LR) images while ignoring the stealthiness of poisoned HR images, making it easy for users to detect anomalous data. To address this problem, we propose BadSR, which improves the stealthiness of poisoned HR images. The key idea of BadSR is to approximate the clean HR image and the pre-defined target image in the feature space while ensuring that modifications to the clean HR image remain within a constrained range. The poisoned HR images generated by BadSR can be integrated with existing triggers. To further improve the effectiveness of BadSR, we design an adversarially optimized trigger and a backdoor gradient-driven poisoned sample selection method based on a genetic algorithm. The experimental results show that BadSR achieves a high attack success rate in various models and data sets, significantly affecting downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15308v1",
    "published_date": "2025-05-21 09:36:35 UTC",
    "updated_date": "2025-05-21 09:36:35 UTC"
  },
  {
    "arxiv_id": "2505.15306v1",
    "title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One",
    "authors": [
      "Yiwen Song",
      "Qianyue Hao",
      "Qingmin Liao",
      "Jian Yuan",
      "Yong Li"
    ],
    "abstract": "Model ensemble is a useful approach in reinforcement learning (RL) for training effective agents. Despite wide success of RL, training effective agents remains difficult due to the multitude of factors requiring careful tuning, such as algorithm selection, hyperparameter settings, and even random seed choices, all of which can significantly influence an agent's performance. Model ensemble helps overcome this challenge by combining multiple weak agents into a single, more powerful one, enhancing overall performance. However, existing ensemble methods, such as majority voting and Boltzmann addition, are designed as fixed strategies and lack a semantic understanding of specific tasks, limiting their adaptability and effectiveness. To address this, we propose LLM-Ens, a novel approach that enhances RL model ensemble with task-specific semantic understandings driven by large language models (LLMs). Given a task, we first design an LLM to categorize states in this task into distinct 'situations', incorporating high-level descriptions of the task conditions. Then, we statistically analyze the strengths and weaknesses of each individual agent to be used in the ensemble in each situation. During the inference time, LLM-Ens dynamically identifies the changing task situation and switches to the agent that performs best in the current situation, ensuring dynamic model selection in the evolving task condition. Our approach is designed to be compatible with agents trained with different random seeds, hyperparameter settings, and various RL algorithms. Extensive experiments on the Atari benchmark show that LLM-Ens significantly improves the RL model ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility, our code is open-source at https://anonymous.4open.science/r/LLM4RLensemble-F7EE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15306v1",
    "published_date": "2025-05-21 09:35:43 UTC",
    "updated_date": "2025-05-21 09:35:43 UTC"
  },
  {
    "arxiv_id": "2505.15303v1",
    "title": "Laplace Sample Information: Data Informativeness Through a Bayesian Lens",
    "authors": [
      "Johannes Kaiser",
      "Kristian Schwethelm",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ],
    "abstract": "Accurately estimating the informativeness of individual samples in a dataset is an important objective in deep learning, as it can guide sample selection, which can improve model efficiency and accuracy by removing redundant or potentially harmful samples. We propose Laplace Sample Information (LSI) measure of sample informativeness grounded in information theory widely applicable across model architectures and learning settings. LSI leverages a Bayesian approximation to the weight posterior and the KL divergence to measure the change in the parameter distribution induced by a sample of interest from the dataset. We experimentally show that LSI is effective in ordering the data with respect to typicality, detecting mislabeled samples, measuring class-wise informativeness, and assessing dataset difficulty. We demonstrate these capabilities of LSI on image and text data in supervised and unsupervised settings. Moreover, we show that LSI can be computed efficiently through probes and transfers well to the training of large models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15303v1",
    "published_date": "2025-05-21 09:34:27 UTC",
    "updated_date": "2025-05-21 09:34:27 UTC"
  },
  {
    "arxiv_id": "2505.17105v2",
    "title": "Transparency in Healthcare AI: Testing European Regulatory Provisions against Users' Transparency Needs",
    "authors": [
      "Anna Spagnolli",
      "Cecilia Tolomini",
      "Elisa Beretta",
      "Claudio Sarra"
    ],
    "abstract": "Artificial Intelligence (AI) plays an essential role in healthcare and is pervasively incorporated into medical software and equipment. In the European Union, healthcare is a high-risk application domain for AI, and providers must prepare Instructions for Use (IFU) according to the European regulation 2024/1689 (AI Act). To this regulation, the principle of transparency is cardinal and requires the IFU to be clear and relevant to the users. This study tests whether these latter requirements are satisfied by the IFU structure. A survey was administered online via the Qualtrics platform to four types of direct stakeholders, i.e., managers (N = 238), healthcare professionals (N = 115), patients (N = 229), and Information Technology experts (N = 230). The participants rated the relevance of a set of transparency needs and indicated the IFU section addressing them. The results reveal differentiated priorities across stakeholders and a troubled mapping of transparency needs onto the IFU structure. Recommendations to build a locally meaningful IFU are derived.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "22 pages, pre-review version",
    "pdf_url": "https://arxiv.org/pdf/2505.17105v2",
    "published_date": "2025-05-21 09:34:05 UTC",
    "updated_date": "2025-06-14 13:47:01 UTC"
  },
  {
    "arxiv_id": "2505.15293v2",
    "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models",
    "authors": [
      "Qianyue Hao",
      "Yiwen Song",
      "Qingmin Liao",
      "Jian Yuan",
      "Yong Li"
    ],
    "abstract": "Policy exploration is critical in reinforcement learning (RL), where existing approaches include greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design LLM-Explorer to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://github.com/tsinghua-fib-lab/LLM-Explorer for reproducibility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15293v2",
    "published_date": "2025-05-21 09:24:23 UTC",
    "updated_date": "2025-10-23 06:12:13 UTC"
  },
  {
    "arxiv_id": "2505.15285v1",
    "title": "Reconsider the Template Mesh in Deep Learning-based Mesh Reconstruction",
    "authors": [
      "Fengting Zhang",
      "Boxu Liang",
      "Qinghao Liu",
      "Min Liu",
      "Xiang Chen",
      "Yaonan Wang"
    ],
    "abstract": "Mesh reconstruction is a cornerstone process across various applications, including in-silico trials, digital twins, surgical planning, and navigation. Recent advancements in deep learning have notably enhanced mesh reconstruction speeds. Yet, traditional methods predominantly rely on deforming a standardised template mesh for individual subjects, which overlooks the unique anatomical variations between them, and may compromise the fidelity of the reconstructions. In this paper, we propose an adaptive-template-based mesh reconstruction network (ATMRN), which generates adaptive templates from the given images for the subsequent deformation, moving beyond the constraints of a singular, fixed template. Our approach, validated on cortical magnetic resonance (MR) images from the OASIS dataset, sets a new benchmark in voxel-to-cortex mesh reconstruction, achieving an average symmetric surface distance of 0.267mm across four cortical structures. Our proposed method is generic and can be easily transferred to other image modalities and anatomical structures.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15285v1",
    "published_date": "2025-05-21 09:10:31 UTC",
    "updated_date": "2025-05-21 09:10:31 UTC"
  },
  {
    "arxiv_id": "2505.15276v1",
    "title": "When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning",
    "authors": [
      "Rongzhi Zhu",
      "Yi Liu",
      "Zequn Sun",
      "Yiwei Wang",
      "Wei Hu"
    ],
    "abstract": "Large reasoning models (LRMs) have significantly advanced performance on complex tasks, yet their tendency to overthink introduces inefficiencies. This study investigates the internal mechanisms of reinforcement learning (RL)-trained LRMs when prompted to save thinking, revealing three distinct thinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking (IT). Through comprehensive analysis of confidence in thinking termination, attention from thinking to generation, and attentional focus on input sections, we uncover key factors influencing the reasoning behaviors. We further find that NT reduces output length at the cost of accuracy, while ET and IT maintain accuracy with reduced response length. Our findings expose fundamental inconsistencies in RL-optimized LRMs, necessitating adaptive improvements for reliable efficiency.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15276v1",
    "published_date": "2025-05-21 08:55:35 UTC",
    "updated_date": "2025-05-21 08:55:35 UTC"
  },
  {
    "arxiv_id": "2505.15275v1",
    "title": "Learning-based Autonomous Oversteer Control and Collision Avoidance",
    "authors": [
      "Seokjun Lee",
      "Seung-Hyun Kong"
    ],
    "abstract": "Oversteer, wherein a vehicle's rear tires lose traction and induce unintentional excessive yaw, poses critical safety challenges. Failing to control oversteer often leads to severe traffic accidents. Although recent autonomous driving efforts have attempted to handle oversteer through stabilizing maneuvers, the majority rely on expert-defined trajectories or assume obstacle-free environments, limiting real-world applicability. This paper introduces a novel end-to-end (E2E) autonomous driving approach that tackles oversteer control and collision avoidance simultaneously. Existing E2E techniques, including Imitation Learning (IL), Reinforcement Learning (RL), and Hybrid Learning (HL), generally require near-optimal demonstrations or extensive experience. Yet even skilled human drivers struggle to provide perfect demonstrations under oversteer, and high transition variance hinders accumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic (QC-SAC), a new HL algorithm that effectively learns from suboptimal demonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we introduce a benchmark inspired by real-world driver training: a vehicle encounters sudden oversteer on a slippery surface and must avoid randomly placed obstacles ahead. Experimental results show QC-SAC attains near-optimal driving policies, significantly surpassing state-of-the-art IL, RL, and HL baselines. Our method demonstrates the world's first safe autonomous oversteer control with obstacle avoidance.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15275v1",
    "published_date": "2025-05-21 08:53:38 UTC",
    "updated_date": "2025-05-21 08:53:38 UTC"
  },
  {
    "arxiv_id": "2505.17103v2",
    "title": "Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation",
    "authors": [
      "Cécile Rousseau",
      "Tobia Boschi",
      "Giandomenico Cornacchia",
      "Dhaval Salwala",
      "Alessandra Pascale",
      "Juan Bernabe Moreno"
    ],
    "abstract": "SDForger is a flexible and efficient framework for generating high-quality multivariate time series using LLMs. Leveraging a compact data representation, SDForger provides synthetic time series generation from a few samples and low-computation fine-tuning of any autoregressive LLM. Specifically, the framework transforms univariate and multivariate signals into tabular embeddings, which are then encoded into text and used to fine-tune the LLM. At inference, new textual embeddings are sampled and decoded into synthetic time series that retain the original data's statistical properties and temporal dynamics. Across a diverse range of datasets, SDForger outperforms existing generative models in many scenarios, both in similarity-based evaluations and downstream forecasting tasks. By enabling textual conditioning in the generation process, SDForger paves the way for multimodal modeling and the streamlined integration of time series with textual information. The model is open-sourced at https://github.com/IBM/fms-dgt/tree/main/fms_dgt/public/databuilders/time_series.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.17103v2",
    "published_date": "2025-05-21 08:50:49 UTC",
    "updated_date": "2025-11-03 16:31:16 UTC"
  },
  {
    "arxiv_id": "2505.15274v2",
    "title": "Identification of Probabilities of Causation: A Complete Characterization",
    "authors": [
      "Xin Shu",
      "Shuai Wang",
      "Ang Li"
    ],
    "abstract": "Probabilities of causation are fundamental to modern decision-making. Pearl first introduced three binary probabilities of causation, and Tian and Pearl later derived tight bounds for them using Balke's linear programming. The theoretical characterization of probabilities of causation with multi-valued treatments and outcomes has remained unresolved for decades, limiting the scope of causality-based decision-making. In this paper, we resolve this foundational gap by proposing a complete set of representative probabilities of causation and proving that they are sufficient to characterize all possible probabilities of causation within the framework of Structural Causal Models (SCMs). We then formally derive tight bounds for these representative quantities using formal mathematical proofs. Finally, we demonstrate the practical relevance of our results through illustrative toy examples.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The proof section is not complete",
    "pdf_url": "https://arxiv.org/pdf/2505.15274v2",
    "published_date": "2025-05-21 08:50:12 UTC",
    "updated_date": "2025-08-09 12:48:41 UTC"
  },
  {
    "arxiv_id": "2505.15270v3",
    "title": "Scaling Diffusion Transformers Efficiently via $μ$P",
    "authors": [
      "Chenyu Zheng",
      "Xinyu Zhang",
      "Rongzhen Wang",
      "Wei Huang",
      "Zhi Tian",
      "Weilin Huang",
      "Jun Zhu",
      "Chongxuan Li"
    ],
    "abstract": "Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($μ$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $μ$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $μ$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $μ$P of mainstream diffusion Transformers, including U-ViT, DiT, PixArt-$α$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $μ$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$μ$P enjoys robust HP transferability. Notably, DiT-XL-2-$μ$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $μ$P on text-to-image generation by scaling PixArt-$α$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $μ$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$α$ and 3% of consumption by human experts for MMDiT-18B. These results establish $μ$P as a principled and efficient framework for scaling diffusion Transformers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2025, 38 pages, 10 figures, 17 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.15270v3",
    "published_date": "2025-05-21 08:49:03 UTC",
    "updated_date": "2025-10-31 03:09:17 UTC"
  },
  {
    "arxiv_id": "2505.15265v2",
    "title": "Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs",
    "authors": [
      "Zihao Pan",
      "Yu Tong",
      "Weibin Wu",
      "Jingyi Wang",
      "Lifeng Chen",
      "Zhe Zhao",
      "Jiajia Wei",
      "Yitong Qiao",
      "Zibin Zheng"
    ],
    "abstract": "Adversarial attacks aim to generate malicious inputs that mislead deep models, but beyond causing model failure, they cannot provide certain interpretable information such as ``\\textit{What content in inputs make models more likely to fail?}'' However, this information is crucial for researchers to specifically improve model robustness. Recent research suggests that models may be particularly sensitive to certain semantics in visual inputs (such as ``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this paper we conducted the first exploration on large vision-language models (LVLMs) and found that LVLMs indeed are susceptible to hallucinations and various errors when facing specific semantic concepts in images. To efficiently search for these sensitive concepts, we integrated large language models (LLMs) and text-to-image (T2I) models to propose a novel semantic evolution framework. Randomly initialized semantic concepts undergo LLM-based crossover and mutation operations to form image descriptions, which are then converted by T2I models into visual inputs for LVLMs. The task-specific performance of LVLMs on each input is quantified as fitness scores for the involved semantics and serves as reward signals to further guide LLMs in exploring concepts that induce LVLMs. Extensive experiments on seven mainstream LVLMs and two multimodal tasks demonstrate the effectiveness of our method. Additionally, we provide interesting findings about the sensitive semantics of LVLMs, aiming to inspire further in-depth research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "The paper needs major revisions, so it is being withdrawn",
    "pdf_url": "https://arxiv.org/pdf/2505.15265v2",
    "published_date": "2025-05-21 08:45:43 UTC",
    "updated_date": "2025-07-25 09:03:08 UTC"
  },
  {
    "arxiv_id": "2505.15256v2",
    "title": "Zero-Shot Gaze-based Volumetric Medical Image Segmentation",
    "authors": [
      "Tatyana Shmykova",
      "Leila Khaertdinova",
      "Ilya Pershin"
    ],
    "abstract": "Accurate segmentation of anatomical structures in volumetric medical images is crucial for clinical applications, including disease monitoring and cancer treatment planning. Contemporary interactive segmentation models, such as Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on manually provided prompts like bounding boxes and mouse clicks. In this study, we introduce eye gaze as a novel informational modality for interactive segmentation, marking the application of eye-tracking for 3D medical image segmentation. We evaluate the performance of using gaze-based prompts with SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to bounding boxes, gaze-based prompts offer a time-efficient interaction approach with slightly lower segmentation quality. Our findings highlight the potential of using gaze as a complementary input modality for interactive 3D medical image segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to MMFM-BIOMED Workshop @ CVPR 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15256v2",
    "published_date": "2025-05-21 08:34:13 UTC",
    "updated_date": "2025-06-10 10:17:53 UTC"
  },
  {
    "arxiv_id": "2505.15250v1",
    "title": "Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification",
    "authors": [
      "Suping Xu",
      "Lin Shang",
      "Keyu Liu",
      "Hengrong Ju",
      "Xibei Yang",
      "Witold Pedrycz"
    ],
    "abstract": "Fuzzy rough feature selection (FRFS) is an effective means of addressing the curse of dimensionality in high-dimensional data. By removing redundant and irrelevant features, FRFS helps mitigate classifier overfitting, enhance generalization performance, and lessen computational overhead. However, most existing FRFS algorithms primarily focus on reducing uncertainty in pattern classification, neglecting that lower uncertainty does not necessarily result in improved classification performance, despite it commonly being regarded as a key indicator of feature selection effectiveness in the FRFS literature. To bridge uncertainty characterization and pattern classification, we propose a Margin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers both the compactness and separation of label classes. MAFRFS effectively reduces uncertainty in pattern classification tasks, while guiding the feature selection towards more separable and discriminative label class structures. Extensive experiments on 15 public datasets demonstrate that MAFRFS is highly scalable and more effective than FRFS. The algorithms developed using MAFRFS outperform six state-of-the-art feature selection algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15250v1",
    "published_date": "2025-05-21 08:26:20 UTC",
    "updated_date": "2025-05-21 08:26:20 UTC"
  },
  {
    "arxiv_id": "2505.15245v1",
    "title": "Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework",
    "authors": [
      "Zihao Jiang",
      "Ben Liu",
      "Miao Peng",
      "Wenjie Xu",
      "Yao Xiao",
      "Zhenyan Shan",
      "Min Peng"
    ],
    "abstract": "While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we introduce a comprehensive benchmark covering a wide range of temporal granularities, designed to systematically evaluate LLMs' capabilities in explainable temporal reasoning. Furthermore, our findings reveal that LLMs struggle to deliver convincing explanations when relying solely on textual information. To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning. Specifically, we first leverage temporal knowledge graphs to develop a temporal encoder that captures structural information for the query. Subsequently, we introduce a structure-text prefix adapter to map graph structure features into the text embedding space. Finally, LLMs generate explanation text by seamlessly integrating the soft graph token with instruction-tuning prompt tokens. Experimental results indicate that GETER achieves state-of-the-art performance while also demonstrating its effectiveness as well as strong generalization capabilities. Our dataset and code are available at https://github.com/carryTatum/GETER.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In Findings of the Association for Computational Linguistics: ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15245v1",
    "published_date": "2025-05-21 08:20:35 UTC",
    "updated_date": "2025-05-21 08:20:35 UTC"
  },
  {
    "arxiv_id": "2505.15242v2",
    "title": "Adaptive Plan-Execute Framework for Smart Contract Security Auditing",
    "authors": [
      "Zhiyuan Wei",
      "Jing Sun",
      "Zijian Zhang",
      "Zhe Hou",
      "Zixiao Zhao"
    ],
    "abstract": "Large Language Models (LLMs) have shown great promise in code analysis and auditing; however, they still struggle with hallucinations and limited context-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute framework that enhances smart contract security analysis through dynamic audit planning and structured execution. Unlike conventional LLM-based auditing approaches that follow fixed workflows and predefined steps, SmartAuditFlow dynamically generates and refines audit plans based on the unique characteristics of each smart contract. It continuously adjusts its auditing strategy in response to intermediate LLM outputs and newly detected vulnerabilities, ensuring a more adaptive and precise security assessment. The framework then executes these plans step by step, applying a structured reasoning process to enhance vulnerability detection accuracy while minimizing hallucinations and false positives. To further improve audit precision, SmartAuditFlow integrates iterative prompt optimization and external knowledge sources, such as static analysis tools and Retrieval-Augmented Generation (RAG). This ensures audit decisions are contextually informed and backed by real-world security knowledge, producing comprehensive security reports. Extensive evaluations across multiple benchmarks demonstrate that SmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on common and critical vulnerabilities, 41.2 percent accuracy for comprehensive coverage of known smart contract weaknesses in real-world projects, and successfully identifying all 13 tested CVEs. These results highlight SmartAuditFlow's scalability, cost-effectiveness, and superior adaptability over traditional static analysis tools and contemporary LLM-based approaches, establishing it as a robust solution for automated smart contract auditing.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "30 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.15242v2",
    "published_date": "2025-05-21 08:18:41 UTC",
    "updated_date": "2025-05-22 06:10:20 UTC"
  },
  {
    "arxiv_id": "2505.15240v1",
    "title": "Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge",
    "authors": [
      "Yassir Fathullah",
      "Mark J. F. Gales"
    ],
    "abstract": "This paper explores generalised probabilistic modelling and uncertainty estimation in comparative LLM-as-a-judge frameworks. We show that existing Product-of-Experts methods are specific cases of a broader framework, enabling diverse modelling options. Furthermore, we propose improved uncertainty estimates for individual comparisons, enabling more efficient selection and achieving strong performance with fewer evaluations. We also introduce a method for estimating overall ranking uncertainty. Finally, we demonstrate that combining absolute and comparative scoring improves performance. Experiments show that the specific expert model has a limited impact on final rankings but our proposed uncertainty estimates, especially the probability of reordering, significantly improve the efficiency of systems reducing the number of needed comparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used to identify low-performing predictions, where the nature of the probabilistic model has a notable impact on the quality of the overall uncertainty.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in UAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15240v1",
    "published_date": "2025-05-21 08:16:18 UTC",
    "updated_date": "2025-05-21 08:16:18 UTC"
  },
  {
    "arxiv_id": "2505.15239v1",
    "title": "Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers",
    "authors": [
      "Peter Súkeník",
      "Christoph H. Lampert",
      "Marco Mondelli"
    ],
    "abstract": "The empirical emergence of neural collapse -- a surprising symmetry in the feature representations of the training data in the penultimate layer of deep neural networks -- has spurred a line of theoretical research aimed at its understanding. However, existing work focuses on data-agnostic models or, when data structure is taken into account, it remains limited to multi-layer perceptrons. Our paper fills both these gaps by analyzing modern architectures in a data-aware regime: we prove that global optima of deep regularized transformers and residual networks (ResNets) with LayerNorm trained with cross entropy or mean squared error loss are approximately collapsed, and the approximation gets tighter as the depth grows. More generally, we formally reduce any end-to-end large-depth ResNet or transformer training into an equivalent unconstrained features model, thus justifying its wide use in the literature even beyond data-agnostic settings. Our theoretical results are supported by experiments on computer vision and language datasets showing that, as the depth grows, neural collapse indeed becomes more prominent.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15239v1",
    "published_date": "2025-05-21 08:16:03 UTC",
    "updated_date": "2025-05-21 08:16:03 UTC"
  },
  {
    "arxiv_id": "2505.15234v2",
    "title": "UNet with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning for Medical Image Segmentation",
    "authors": [
      "Saqib Qamar",
      "Mohd Fazil",
      "Parvez Ahmad",
      "Shakir Khan",
      "Abu Taha Zamani"
    ],
    "abstract": "Medical image segmentation plays an important role in various clinical applications; however, existing deep learning models face trade-offs between efficiency and accuracy. Convolutional Neural Networks (CNNs) capture local details well but miss the global context, whereas transformers handle the global context but at a high computational cost. Recently, State Space Sequence Models (SSMs) have shown potential for capturing long-range dependencies with linear complexity; however, their direct use in medical image segmentation remains limited due to incompatibility with image structures and autoregressive assumptions. To overcome these challenges, we propose SAMA-UNet, a novel U-shaped architecture that introduces two key innovations. First, the Self-Adaptive Mamba-like Aggregated Attention (SAMA) block adaptively integrates local and global features through dynamic attention weighting, enabling an efficient representation of complex anatomical patterns. Second, the causal resonance multi-scale module (CR-MSM) improves encoder-decoder interactions by adjusting feature resolution and causal dependencies across scales, enhancing the semantic alignment between low- and high-level features. Extensive experiments on MRI, CT, and endoscopy datasets demonstrate that SAMA-UNet consistently outperforms CNN, Transformer, and Mamba-based methods. It achieves 85.38% DSC and 87.82% NSD on BTCV, 92.16% and 96.54% on ACDC, 67.14% and 68.70% on EndoVis17, and 84.06% and 88.47% on ATLAS23, establishing new benchmarks across modalities. These results confirm the effectiveness of SAMA-UNet in combining efficiency and accuracy, making it a promising solution for real-world clinical segmentation tasks. The source code is available on GitHub.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15234v2",
    "published_date": "2025-05-21 08:12:31 UTC",
    "updated_date": "2025-10-17 06:18:49 UTC"
  },
  {
    "arxiv_id": "2505.15863v1",
    "title": "Generative AI for Autonomous Driving: A Review",
    "authors": [
      "Katharina Winter",
      "Abhishek Vivekanandan",
      "Rupert Polley",
      "Yinzhe Shen",
      "Christian Schlauch",
      "Mohamed-Khalil Bouzidi",
      "Bojan Derajic",
      "Natalie Grabowsky",
      "Annajoyce Mariani",
      "Dennis Rochau",
      "Giovanni Lucente",
      "Harsh Yadav",
      "Firas Mualla",
      "Adam Molin",
      "Sebastian Bernhard",
      "Christian Wirth",
      "Ömer Şahin Taş",
      "Nadja Klein",
      "Fabian B. Flohr",
      "Hanno Gottschalk"
    ],
    "abstract": "Generative AI (GenAI) is rapidly advancing the field of Autonomous Driving (AD), extending beyond traditional applications in text, image, and video generation. We explore how generative models can enhance automotive tasks, such as static map creation, dynamic scenario generation, trajectory forecasting, and vehicle motion planning. By examining multiple generative approaches ranging from Variational Autoencoder (VAEs) over Generative Adversarial Networks (GANs) and Invertible Neural Networks (INNs) to Generative Transformers (GTs) and Diffusion Models (DMs), we highlight and compare their capabilities and limitations for AD-specific applications. Additionally, we discuss hybrid methods integrating conventional techniques with generative approaches, and emphasize their improved adaptability and robustness. We also identify relevant datasets and outline open research questions to guide future developments in GenAI. Finally, we discuss three core challenges: safety, interpretability, and realtime capabilities, and present recommendations for image generation, dynamic scenario generation, and planning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2505.15863v1",
    "published_date": "2025-05-21 07:59:18 UTC",
    "updated_date": "2025-05-21 07:59:18 UTC"
  },
  {
    "arxiv_id": "2505.15216v3",
    "title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems",
    "authors": [
      "Andy K. Zhang",
      "Joey Ji",
      "Celeste Menders",
      "Riya Dulepet",
      "Thomas Qin",
      "Ron Y. Wang",
      "Junrong Wu",
      "Kyleen Liao",
      "Jiliang Li",
      "Jinghan Hu",
      "Sara Hong",
      "Nardos Demilew",
      "Shivatmica Murgai",
      "Jason Tran",
      "Nishka Kacheria",
      "Ethan Ho",
      "Denis Liu",
      "Lauren McLane",
      "Olivia Bruvik",
      "Dai-Rong Han",
      "Seungwoo Kim",
      "Akhil Vyas",
      "Cuiyuanxiu Chen",
      "Ryan Li",
      "Weiran Xu",
      "Jonathan Z. Ye",
      "Prerit Choudhary",
      "Siddharth M. Bhatia",
      "Vikram Sivashankar",
      "Yuxuan Bao",
      "Dawn Song",
      "Dan Boneh",
      "Daniel E. Ho",
      "Percy Liang"
    ],
    "abstract": "AI agents have the potential to significantly alter the cybersecurity landscape. Here, we introduce the first framework to capture offensive and defensive cyber-capabilities in evolving real-world systems. Instantiating this framework with BountyBench, we set up 25 systems with complex, real-world codebases. To capture the vulnerability lifecycle, we define three task types: Detect (detecting a new vulnerability), Exploit (exploiting a given vulnerability), and Patch (patching a given vulnerability). For Detect, we construct a new success indicator, which is general across vulnerability types and provides localized evaluation. We manually set up the environment for each system, including installing packages, setting up server(s), and hydrating database(s). We add 40 bug bounties, which are vulnerabilities with monetary awards from \\$10 to \\$30,485, covering 9 of the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy based on information to guide detection, interpolating from identifying a zero day to exploiting a given vulnerability. We evaluate 10 agents: Claude Code, OpenAI Codex CLI with o3-high and o4-mini, and custom agents with o3-high, GPT-4.1, Gemini 2.5 Pro Preview, Claude 3.7 Sonnet Thinking, Qwen3 235B A22B, Llama 4 Maverick, and DeepSeek-R1. Given up to three attempts, the top-performing agents are Codex CLI: o3-high (12.5% on Detect, mapping to \\$3,720; 90% on Patch, mapping to \\$14,152), Custom Agent: Claude 3.7 Sonnet Thinking (67.5% on Exploit), and Codex CLI: o4-mini (90% on Patch, mapping to \\$14,422). Codex CLI: o3-high, Codex CLI: o4-mini, and Claude Code are more capable at defense, achieving higher Patch scores of 90%, 90%, and 87.5%, compared to Exploit scores of 47.5%, 32.5%, and 57.5% respectively; while the custom agents are relatively balanced between offense and defense, achieving Exploit scores of 17.5-67.5% and Patch scores of 25-60%.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "113 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.15216v3",
    "published_date": "2025-05-21 07:44:52 UTC",
    "updated_date": "2025-12-02 06:17:22 UTC"
  },
  {
    "arxiv_id": "2505.15206v1",
    "title": "EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy",
    "authors": [
      "Chi Kit Ng",
      "Long Bai",
      "Guankun Wang",
      "Yupeng Wang",
      "Huxin Gao",
      "Kun Yuan",
      "Chenhan Jin",
      "Tieyong Zeng",
      "Hongliang Ren"
    ],
    "abstract": "In endoscopic procedures, autonomous tracking of abnormal regions and following circumferential cutting markers can significantly reduce the cognitive burden on endoscopists. However, conventional model-based pipelines are fragile for each component (e.g., detection, motion planning) requires manual tuning and struggles to incorporate high-level endoscopic intent, leading to poor generalization across diverse scenes. Vision-Language-Action (VLA) models, which integrate visual perception, language grounding, and motion planning within an end-to-end framework, offer a promising alternative by semantically adapting to surgeon prompts without manual recalibration. Despite their potential, applying VLA models to robotic endoscopy presents unique challenges due to the complex and dynamic anatomical environments of the gastrointestinal (GI) tract. To address this, we introduce EndoVLA, designed specifically for continuum robots in GI interventions. Given endoscopic images and surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1) polyp tracking, (2) delineation and following of abnormal mucosal regions, and (3) adherence to circular markers during circumferential cutting. To tackle data scarcity and domain shifts, we propose a dual-phase strategy comprising supervised fine-tuning on our EndoVLA-Motion dataset and reinforcement fine-tuning with task-aware rewards. Our approach significantly improves tracking performance in endoscopy and enables zero-shot generalization in diverse scenes and complex sequential tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15206v1",
    "published_date": "2025-05-21 07:35:00 UTC",
    "updated_date": "2025-05-21 07:35:00 UTC"
  },
  {
    "arxiv_id": "2505.15201v4",
    "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems",
    "authors": [
      "Christian Walder",
      "Deep Karkhanis"
    ],
    "abstract": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15201v4",
    "published_date": "2025-05-21 07:26:36 UTC",
    "updated_date": "2025-12-14 21:22:03 UTC"
  },
  {
    "arxiv_id": "2505.15197v2",
    "title": "Intentional Gesture: Deliver Your Intentions with Gestures for Speech",
    "authors": [
      "Pinxin Liu",
      "Haiyang Liu",
      "Luchuan Song",
      "Jason J. Corso",
      "Chenliang Xu"
    ],
    "abstract": "When humans speak, gestures help convey communicative intentions, such as adding emphasis or describing concepts. However, current co-speech gesture generation methods rely solely on superficial linguistic cues (e.g. speech audio or text transcripts), neglecting to understand and leverage the communicative intention that underpins human gestures. This results in outputs that are rhythmically synchronized with speech but are semantically shallow. To address this gap, we introduce Intentional-Gesture, a novel framework that casts gesture generation as an intention-reasoning task grounded in high-level communicative functions. First, we curate the InG dataset by augmenting BEAT-2 with gesture-intention annotations (i.e., text sentences summarizing intentions), which are automatically annotated using large vision-language models. Next, we introduce the Intentional Gesture Motion Tokenizer to leverage these intention annotations. It injects high-level communicative functions (e.g., intentions) into tokenized motion representations to enable intention-aware gesture synthesis that are both temporally aligned and semantically meaningful, achieving new state-of-the-art performance on the BEAT-2 benchmark. Our framework offers a modular foundation for expressive gesture generation in digital humans and embodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15197v2",
    "published_date": "2025-05-21 07:24:51 UTC",
    "updated_date": "2025-09-26 03:02:17 UTC"
  },
  {
    "arxiv_id": "2505.15182v2",
    "title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection",
    "authors": [
      "Jeonghye Kim",
      "Sojeong Rhee",
      "Minbeom Kim",
      "Dohyung Kim",
      "Sangmook Lee",
      "Youngchul Sung",
      "Kyomin Jung"
    ],
    "abstract": "Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 (Main Conference)",
    "pdf_url": "https://arxiv.org/pdf/2505.15182v2",
    "published_date": "2025-05-21 06:57:39 UTC",
    "updated_date": "2025-09-28 17:14:06 UTC"
  },
  {
    "arxiv_id": "2505.15173v3",
    "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Synthetic Video Detection",
    "authors": [
      "Zhipei Xu",
      "Xuanyu Zhang",
      "Qing Huang",
      "Xing Zhou",
      "Jian Zhang"
    ],
    "abstract": "Recent advances in Artificial Intelligence Generated Content have led to highly realistic synthetic videos, particularly in human-centric scenarios involving speech, gestures, and full-body motion, posing serious threats to information authenticity and public trust. Unlike DeepFake techniques that focus on localized facial manipulation, human-centric video generation methods can synthesize entire human bodies with controllable movements, enabling complex interactions with environments, objects, and even other people. However, existing detection methods largely overlook the growing risks posed by such full-body synthetic content. Meanwhile, a growing body of research has explored leveraging LLMs for interpretable fake detection, aiming to explain decisions in natural language. Yet these approaches heavily depend on supervised fine-tuning, which introduces limitations such as annotation bias, hallucinated supervision, and weakened generalization. To address these challenges, we propose AvatarShield, a novel multimodal human-centric synthetic video detection framework that eliminates the need for dense textual supervision by adopting Group Relative Policy Optimization, enabling LLMs to develop reasoning capabilities from simple binary labels. Our architecture combines a discrete vision tower for high-level semantic inconsistencies and a residual extractor for fine-grained artifact analysis. We further introduce FakeHumanVid, a large-scale benchmark containing 15K real and synthetic videos across nine state-of-the-art human generation methods driven by text, pose, or audio. Extensive experiments demonstrate that AvatarShield outperforms existing methods in both in-domain and cross-domain settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15173v3",
    "published_date": "2025-05-21 06:43:34 UTC",
    "updated_date": "2025-09-23 14:29:40 UTC"
  },
  {
    "arxiv_id": "2505.15155v2",
    "title": "R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization",
    "authors": [
      "Yuante Li",
      "Xu Yang",
      "Xiao Yang",
      "Minrui Xu",
      "Xisen Wang",
      "Weiqing Liu",
      "Jiang Bian"
    ],
    "abstract": "Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility. Despite advances in large language models and multi-agent systems, current quantitative research pipelines suffer from limited automation, weak interpretability, and fragmented coordination across key components such as factor mining and model innovation. In this paper, we propose R&D-Agent for Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization. RD-Agent(Q) decomposes the quant process into two iterative stages: a Research stage that dynamically sets goal-aligned prompts, formulates hypotheses based on domain priors, and maps them to concrete tasks, and a Development stage that employs a code-generation agent, Co-STEER, to implement task-specific code, which is then executed in real-market backtests. The two stages are connected through a feedback stage that thoroughly evaluates experimental outcomes and informs subsequent iterations, with a multi-armed bandit scheduler for adaptive direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher annualized returns than classical factor libraries using 70% fewer factors, and outperforms state-of-the-art deep time-series models on real markets. Its joint factor-model optimization delivers a strong balance between predictive accuracy and strategy robustness. Our code is available at: https://github.com/microsoft/RD-Agent.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "q-fin.CP",
    "comment": "42 pages,11figures, NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15155v2",
    "published_date": "2025-05-21 06:20:56 UTC",
    "updated_date": "2025-09-25 10:13:08 UTC"
  },
  {
    "arxiv_id": "2505.15154v1",
    "title": "Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning",
    "authors": [
      "Jinghui Lu",
      "Haiyang Yu",
      "Siliang Xu",
      "Shiwei Ran",
      "Guozhi Tang",
      "Siqi Wang",
      "Bin Shan",
      "Teng Fu",
      "Hao Feng",
      "Jingqun Tang",
      "Han Wang",
      "Can Huang"
    ],
    "abstract": "Recent advancements in reasoning have significantly enhanced the capabilities of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) across diverse tasks. However, excessive reliance on chain-of-thought (CoT) reasoning can impair model performance and brings unnecessarily lengthened outputs, reducing efficiency. Our work reveals that prolonged reasoning does not universally improve accuracy and even degrade performance on simpler tasks. To address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel framework that dynamically switches between short answers and long-form reasoning based on the model perplexity. CAR first generates a short answer and evaluates its perplexity, triggering reasoning only when the model exhibits low confidence (i.e., high perplexity). Experiments across diverse multimodal VQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both short-answer and long-form reasoning approaches, striking an optimal balance between accuracy and efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15154v1",
    "published_date": "2025-05-21 06:20:17 UTC",
    "updated_date": "2025-05-21 06:20:17 UTC"
  },
  {
    "arxiv_id": "2505.15862v2",
    "title": "Bandit based Dynamic Candidate Edge Selection in Solving Traveling Salesman Problems",
    "authors": [
      "Long Wang",
      "Jiongzhi Zheng",
      "Zhengda Xiong",
      "ChuMin Li",
      "Kun He"
    ],
    "abstract": "Algorithms designed for routing problems typically rely on high-quality candidate edges to guide their search, aiming to reduce the search space and enhance the search efficiency. However, many existing algorithms, like the classical Lin-Kernighan-Helsgaun (LKH) algorithm for the Traveling Salesman Problem (TSP), often use predetermined candidate edges that remain static throughout local searches. This rigidity could cause the algorithm to get trapped in local optima, limiting its potential to find better solutions. To address this issue, we propose expanding the candidate sets to include other promising edges, providing them an opportunity for selection. Specifically, we incorporate multi-armed bandit models to dynamically select the most suitable candidate edges in each iteration, enabling LKH to make smarter choices and lead to improved solutions. Extensive experiments on multiple TSP benchmarks show the excellent performance of our method. Moreover, we employ this bandit-based method to LKH-3, an extension of LKH tailored for solving various TSP variant problems, and our method also significantly enhances LKH-3's performance across typical TSP variants.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15862v2",
    "published_date": "2025-05-21 06:11:00 UTC",
    "updated_date": "2025-06-05 06:16:06 UTC"
  },
  {
    "arxiv_id": "2505.15146v2",
    "title": "lmgame-Bench: How Good are LLMs at Playing Games?",
    "authors": [
      "Lanxiang Hu",
      "Mingjia Huo",
      "Yuxuan Zhang",
      "Haoyang Yu",
      "Eric P. Xing",
      "Ion Stoica",
      "Tajana Rosing",
      "Haojian Jin",
      "Hao Zhang"
    ],
    "abstract": "Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master. We study the major challenges in using popular video games to evaluate modern LLMs and find that directly dropping LLMs into games cannot make an effective evaluation, for three reasons -- brittle vision perception, prompt sensitivity, and potential data contamination. We introduce lmgame-Bench to turn games into reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and narrative games delivered through a unified Gym-style API and paired with lightweight perception and memory scaffolds, and is designed to stabilize prompt variance and remove contamination. Across 13 leading models, we show lmgame-Bench is challenging while still separating models well. Correlation analysis shows that every game probes a unique blend of capabilities often tested in isolation elsewhere. More interestingly, performing reinforcement learning on a single game from lmgame-Bench transfers both to unseen games and to external planning tasks. Our evaluation code is available at https://github.com/lmgame-org/GamingAgent/lmgame-bench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15146v2",
    "published_date": "2025-05-21 06:02:55 UTC",
    "updated_date": "2025-06-03 09:53:37 UTC"
  },
  {
    "arxiv_id": "2505.15141v2",
    "title": "BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms",
    "authors": [
      "Yunlong Hou",
      "Fengzhuo Zhang",
      "Cunxiao Du",
      "Xuan Zhang",
      "Jiachun Pan",
      "Tianyu Pang",
      "Chao Du",
      "Vincent Y. F. Tan",
      "Zhuoran Yang"
    ],
    "abstract": "Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 4 figures, accepted to ICML, typos and affiliations are corrected",
    "pdf_url": "https://arxiv.org/pdf/2505.15141v2",
    "published_date": "2025-05-21 05:56:31 UTC",
    "updated_date": "2025-11-20 13:34:42 UTC"
  },
  {
    "arxiv_id": "2505.15138v2",
    "title": "Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm",
    "authors": [
      "Yang Xu",
      "Swetha Ganesh",
      "Washim Uddin Mondal",
      "Qinbo Bai",
      "Vaneet Aggarwal"
    ],
    "abstract": "This paper investigates infinite-horizon average reward Constrained Markov Decision Processes (CMDPs) with general parametrization. We propose a Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints while ensuring a high convergence rate. In particular, our algorithm achieves global convergence and constraint violation rates of $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ over a horizon of length $T$ when the mixing time, $τ_{\\mathrm{mix}}$, is known to the learner. In absence of knowledge of $τ_{\\mathrm{mix}}$, the achievable rates change to $\\tilde{\\mathcal{O}}(1/T^{0.5-ε})$ provided that $T \\geq \\tilde{\\mathcal{O}}\\left(τ_{\\mathrm{mix}}^{2/ε}\\right)$. Our results match the theoretical lower bound for Markov Decision Processes and establish a new benchmark in the theoretical exploration of average reward CMDPs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15138v2",
    "published_date": "2025-05-21 05:49:11 UTC",
    "updated_date": "2025-12-09 19:28:57 UTC"
  },
  {
    "arxiv_id": "2505.15134v1",
    "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning",
    "authors": [
      "Shivam Agarwal",
      "Zimin Zhang",
      "Lifan Yuan",
      "Jiawei Han",
      "Hao Peng"
    ],
    "abstract": "Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. We show that this simple objective alone, without any labeled data, can substantially improve large language models' (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. On Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15134v1",
    "published_date": "2025-05-21 05:39:11 UTC",
    "updated_date": "2025-05-21 05:39:11 UTC"
  },
  {
    "arxiv_id": "2505.15133v1",
    "title": "DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer",
    "authors": [
      "Haiduo Huang",
      "Jiangcheng Song",
      "Yadong Zhang",
      "Pengju Ren"
    ],
    "abstract": "Recent advances in knowledge distillation have emphasized the importance of decoupling different knowledge components. While existing methods utilize momentum mechanisms to separate task-oriented and distillation gradients, they overlook the inherent conflict between target-class and non-target-class knowledge flows. Furthermore, low-confidence dark knowledge in non-target classes introduces noisy signals that hinder effective knowledge transfer. To address these limitations, we propose DeepKD, a novel training framework that integrates dual-level decoupling with adaptive denoising. First, through theoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics in task-oriented and non-task-oriented knowledge distillation, we design independent momentum updaters for each component to prevent mutual interference. We observe that the optimal momentum coefficients for task-oriented gradient (TOG), target-class gradient (TCG), and non-target-class gradient (NCG) should be positively related to their GSNR. Second, we introduce a dynamic top-k mask (DTM) mechanism that gradually increases K from a small initial value to incorporate more non-target classes as training progresses, following curriculum learning principles. The DTM jointly filters low-confidence logits from both teacher and student models, effectively purifying dark knowledge during early training. Extensive experiments on CIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code is available at https://github.com/haiduo/DeepKD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15133v1",
    "published_date": "2025-05-21 05:38:57 UTC",
    "updated_date": "2025-05-21 05:38:57 UTC"
  },
  {
    "arxiv_id": "2505.15123v2",
    "title": "Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding",
    "authors": [
      "Ta Duc Huy",
      "Duy Anh Huynh",
      "Yutong Xie",
      "Yuankai Qi",
      "Qi Chen",
      "Phi Le Nguyen",
      "Sen Kim Tran",
      "Son Lam Phung",
      "Anton van den Hengel",
      "Zhibin Liao",
      "Minh-Son To",
      "Johan W. Verjans",
      "Vu Minh Hieu Phan"
    ],
    "abstract": "Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICCV 2025 (Highlight)",
    "pdf_url": "https://arxiv.org/pdf/2505.15123v2",
    "published_date": "2025-05-21 05:16:45 UTC",
    "updated_date": "2025-08-24 14:40:35 UTC"
  },
  {
    "arxiv_id": "2505.15117v1",
    "title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents",
    "authors": [
      "Bowen Jin",
      "Jinsung Yoon",
      "Priyanka Kargupta",
      "Sercan O. Arik",
      "Jiawei Han"
    ],
    "abstract": "Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based search agents that adeptly combine reasoning with search engine use. While the use of RL for training search agents is promising, the optimal design of such agents remains not fully understood. In particular, key factors -- such as (1) reward formulation, (2) the choice and characteristics of the underlying LLM, and (3) the role of the search engine in the RL process -- require further investigation. In this work, we conduct comprehensive empirical studies to systematically investigate these and offer actionable insights. We highlight several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoning-specialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference. These establish important guidelines for successfully building and deploying LLM-based search agents in real-world applications. Code is available at https://github.com/PeterGriffinJin/Search-R1.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.15117v1",
    "published_date": "2025-05-21 05:09:43 UTC",
    "updated_date": "2025-05-21 05:09:43 UTC"
  },
  {
    "arxiv_id": "2505.15116v1",
    "title": "Graph Foundation Models: A Comprehensive Survey",
    "authors": [
      "Zehong Wang",
      "Zheyuan Liu",
      "Tianyi Ma",
      "Jiazheng Li",
      "Zheyuan Zhang",
      "Xingbo Fu",
      "Yiyang Li",
      "Zhengqing Yuan",
      "Wei Song",
      "Yijun Ma",
      "Qingkai Zeng",
      "Xiusi Chen",
      "Jianan Zhao",
      "Jundong Li",
      "Meng Jiang",
      "Pietro Lio",
      "Nitesh Chawla",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Graph-structured data pervades domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs -- characterized by non-Euclidean structures and complex relational semantics -- poses unique challenges and opens new opportunities. To this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope -- universal, task-specific, and domain-specific -- and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and general-purpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field. Resources are available at https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Github Repo: https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs. 93 pages, 438 references",
    "pdf_url": "https://arxiv.org/pdf/2505.15116v1",
    "published_date": "2025-05-21 05:08:00 UTC",
    "updated_date": "2025-05-21 05:08:00 UTC"
  },
  {
    "arxiv_id": "2505.15111v1",
    "title": "iPad: Iterative Proposal-centric End-to-End Autonomous Driving",
    "authors": [
      "Ke Guo",
      "Haochen Liu",
      "Xiaojun Wu",
      "Jia Pan",
      "Chen Lv"
    ],
    "abstract": "End-to-end (E2E) autonomous driving systems offer a promising alternative to traditional modular pipelines by reducing information loss and error accumulation, with significant potential to enhance both mobility and safety. However, most existing E2E approaches directly generate plans based on dense bird's-eye view (BEV) grid features, leading to inefficiency and limited planning awareness. To address these limitations, we propose iterative Proposal-centric autonomous driving (iPad), a novel framework that places proposals - a set of candidate future plans - at the center of feature extraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder that iteratively refines proposals and their associated features through proposal-anchored attention, effectively fusing multi-view image data. Additionally, we introduce two lightweight, proposal-centric auxiliary tasks - mapping and prediction - that improve planning quality with minimal computational overhead. Extensive experiments on the NAVSIM and CARLA Bench2Drive benchmarks demonstrate that iPad achieves state-of-the-art performance while being significantly more efficient than prior leading methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15111v1",
    "published_date": "2025-05-21 05:05:38 UTC",
    "updated_date": "2025-05-21 05:05:38 UTC"
  },
  {
    "arxiv_id": "2505.15108v2",
    "title": "A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents",
    "authors": [
      "Ian Steenstra",
      "Timothy W. Bickmore"
    ],
    "abstract": "The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk ontology specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the ontology aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this ontology, detailing its grounding, and discuss potential use cases. We discuss four use cases in detail: monitoring real user interactions, evaluation with simulated patients, benchmarking and comparative analysis, and identifying unexpected outcomes. The proposed ontology offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "This is a preprint version of the paper accepted to IVA'25",
    "pdf_url": "https://arxiv.org/pdf/2505.15108v2",
    "published_date": "2025-05-21 05:01:39 UTC",
    "updated_date": "2025-09-20 12:53:21 UTC"
  },
  {
    "arxiv_id": "2505.15107v2",
    "title": "StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization",
    "authors": [
      "Ziliang Wang",
      "Xuhui Zheng",
      "Kang An",
      "Cijun Ouyang",
      "Jialu Cai",
      "Yuhang Wang",
      "Yichao Wu"
    ],
    "abstract": "Efficient multi-hop reasoning requires Large Language Models (LLMs) based agents to acquire high-value external knowledge iteratively. Previous work has explored reinforcement learning (RL) to train LLMs to perform search-based document retrieval, achieving notable improvements in QA performance, but underperform on complex, multi-hop QA resulting from the sparse rewards from global signal only. To address this gap in existing research, we introduce StepSearch, a framework for search LLMs that trained with step-wise proximal policy optimization method. It consists of richer and more detailed intermediate search rewards and token-level process supervision based on information gain and redundancy penalties to better guide each search step. We constructed a fine-grained question-answering dataset containing sub-question-level search trajectories based on open source datasets through a set of data pipeline method. On standard multi-hop QA benchmarks, it significantly outperforms global-reward baselines, achieving 11.2% and 4.2% absolute improvements for 3B and 7B models over various search with RL baselines using only 19k training data, demonstrating the effectiveness of fine-grained, stepwise supervision in optimizing deep search LLMs. Our code will be released on https://github.com/Zillwang/StepSearch.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.15107v2",
    "published_date": "2025-05-21 05:01:31 UTC",
    "updated_date": "2025-05-26 04:44:21 UTC"
  },
  {
    "arxiv_id": "2505.15105v2",
    "title": "Mechanistic evaluation of Transformers and state space models",
    "authors": [
      "Aryaman Arora",
      "Neil Rathi",
      "Nikil Roashan Selvam",
      "Róbert Csordás",
      "Dan Jurafsky",
      "Christopher Potts"
    ],
    "abstract": "State space models (SSMs) for language modelling promise an efficient and performant alternative to quadratic-attention Transformers, yet show variable performance on recalling basic information from the context. While performance on synthetic tasks like Associative Recall (AR) can point to this deficiency, behavioural metrics provide little information as to why--on a mechanistic level--certain architectures fail and others succeed. To address this, we conduct experiments on AR and find that only Transformers and Based SSM models fully succeed at AR, with Mamba a close third, whereas the other SSMs (H3, Hyena) fail. We then use causal interventions to explain why. We find that Transformers and Based learn to store key-value associations in-context using induction heads. By contrast, the SSMs compute these associations only at the last state, with only Mamba succeeding because of its short convolution component. To extend and deepen these findings, we introduce Associative Treecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR introduces language-like hierarchical structure into the AR setting. We find that all architectures learn the same mechanism as they did for AR, and the same three models succeed at the task. These results reveal that architectures with similar accuracy may still have substantive differences, motivating the adoption of mechanistic evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 page main text, 6 pages appendix",
    "pdf_url": "https://arxiv.org/pdf/2505.15105v2",
    "published_date": "2025-05-21 04:56:09 UTC",
    "updated_date": "2025-06-07 22:30:22 UTC"
  },
  {
    "arxiv_id": "2505.15098v1",
    "title": "Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation",
    "authors": [
      "Yihang Li",
      "Tianle Zhang",
      "Xuelong Wei",
      "Jiayi Li",
      "Lin Zhao",
      "Dongchi Huang",
      "Zhirui Fang",
      "Minhua Zheng",
      "Wenjun Dai",
      "Xiaodong He"
    ],
    "abstract": "Robot manipulation learning from human demonstrations offers a rapid means to acquire skills but often lacks generalization across diverse scenes and object placements. This limitation hinders real-world applications, particularly in complex tasks requiring dexterous manipulation. Vision-Language-Action (VLA) paradigm leverages large-scale data to enhance generalization. However, due to data scarcity, VLA's performance remains limited. In this work, we introduce Object-Focus Actor (OFA), a novel, data-efficient approach for generalized dexterous manipulation. OFA exploits the consistent end trajectories observed in dexterous manipulation tasks, allowing for efficient policy training. Our method employs a hierarchical pipeline: object perception and pose estimation, pre-manipulation pose arrival and OFA policy execution. This process ensures that the manipulation is focused and efficient, even in varied backgrounds and positional layout. Comprehensive real-world experiments across seven tasks demonstrate that OFA significantly outperforms baseline methods in both positional and background generalization tests. Notably, OFA achieves robust performance with only 10 demonstrations, highlighting its data efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15098v1",
    "published_date": "2025-05-21 04:37:56 UTC",
    "updated_date": "2025-05-21 04:37:56 UTC"
  },
  {
    "arxiv_id": "2505.15095v2",
    "title": "Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English",
    "authors": [
      "Ishmanbir Singh",
      "Dipankar Srirag",
      "Aditya Joshi"
    ],
    "abstract": "Sarcasm is a challenge to sentiment analysis because of the incongruity between stated and implied sentiment. The challenge is exacerbated when the implication may be relevant to a specific country or geographical region. Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that has been used for pragmatic reasoning. In this paper, we harness PMP for explainable sarcasm detection for Australian and Indian English, alongside a benchmark dataset for standard English. We manually add sarcasm explanations to an existing sarcasm-labeled dataset for Australian and Indian English called BESSTIE, and compare the performance for explainable sarcasm detection for them with FLUTE, a standard English dataset containing sarcasm explanations. Our approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA) achieves statistically significant performance improvement across all tasks and datasets when compared with four alternative prompting strategies. We also find that alternative techniques such as agentic prompting mitigate context-related failures by enabling external knowledge retrieval. The focused contribution of our work is utilising PMP in generating sarcasm explanations for varieties of English.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ALTA 2025 (Best Paper Honorable Mention). Camera-ready",
    "pdf_url": "https://arxiv.org/pdf/2505.15095v2",
    "published_date": "2025-05-21 04:34:22 UTC",
    "updated_date": "2025-10-30 07:18:23 UTC"
  },
  {
    "arxiv_id": "2505.15859v1",
    "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
    "authors": [
      "Tianyi Ma",
      "Yiyue Qian",
      "Zheyuan Zhang",
      "Zehong Wang",
      "Xiaoye Qian",
      "Feifan Bai",
      "Yifan Ding",
      "Xuwei Luo",
      "Shinan Zhang",
      "Keerthiram Murugesan",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "The exponential growth of data-driven systems and AI technologies has intensified the demand for high-quality web-sourced datasets. While existing datasets have proven valuable, conventional web data collection approaches face significant limitations in terms of human effort and scalability. Current data-collecting solutions fall into two categories: wrapper-based methods that struggle with adaptability and reproducibility, and large language model (LLM)-based approaches that incur substantial computational and financial costs. To address these challenges, we propose AutoData, a novel multi-agent system for Automated web Data collection, that requires minimal human intervention, i.e., only necessitating a natural language instruction specifying the desired dataset. In addition, AutoData is designed with a robust multi-agent architecture, featuring a novel oriented message hypergraph coordinated by a central task manager, to efficiently organize agents across research and development squads. Besides, we introduce a novel hypergraph cache system to advance the multi-agent collaboration process that enables efficient automated data collection and mitigates the token cost issues prevalent in existing LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark dataset supporting live data collection from web sources across three domains: academic, finance, and sports. Comprehensive evaluations over Instruct2DS and three existing benchmark datasets demonstrate AutoData's superior performance compared to baseline methods. Case studies on challenging tasks such as picture book collection and paper extraction from surveys further validate its applicability. Our source code and dataset are available at https://github.com/GraphResearcher/AutoData.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15859v1",
    "published_date": "2025-05-21 04:32:35 UTC",
    "updated_date": "2025-05-21 04:32:35 UTC"
  },
  {
    "arxiv_id": "2505.15091v4",
    "title": "ThinkRec: Thinking-based recommendation via LLM",
    "authors": [
      "Qihang Yu",
      "Kairui Fu",
      "Zheqi Lv",
      "Shengyu Zhang",
      "Xinhui Wu",
      "Chen Lin",
      "Feng Wei",
      "Bo Zheng",
      "Fei Wu"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled more semantic-aware recommendations through natural language generation. Existing LLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like manner, relying on superficial features to match similar items based on click history, rather than reasoning through deeper behavioral logic. This often leads to superficial and erroneous recommendations. Motivated by this, we propose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1 to System 2 (rational system). Technically, ThinkRec introduces a thinking activation mechanism that augments item metadata with keyword summarization and injects synthetic reasoning traces, guiding the model to form interpretable reasoning chains that consist of analyzing interaction histories, identifying user preferences, and making decisions based on target items. On top of this, we propose an instance-wise expert fusion mechanism to reduce the reasoning difficulty. By dynamically assigning weights to expert models based on users' latent features, ThinkRec adapts its reasoning path to individual users, thereby enhancing precision and personalization. Extensive experiments on real-world datasets demonstrate that ThinkRec significantly improves the accuracy and interpretability of recommendations. Our implementations are available at https://github.com/Yu-Qi-hang/ThinkRec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Published on WWW'26: In Proceedings of the ACM Web Conference 2026",
    "pdf_url": "https://arxiv.org/pdf/2505.15091v4",
    "published_date": "2025-05-21 04:25:18 UTC",
    "updated_date": "2026-01-21 07:10:49 UTC"
  },
  {
    "arxiv_id": "2505.15090v1",
    "title": "DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer",
    "authors": [
      "Sona Elza Simon",
      "Preethi Jyothi"
    ],
    "abstract": "Effective cross-lingual transfer remains a critical challenge in scaling the benefits of large language models from high-resource to low-resource languages. Towards this goal, prior studies have explored many approaches to combine task knowledge from task-specific data in a (high-resource) source language and language knowledge from unlabeled text in a (low-resource) target language. One notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual transfer that learns task-specific and language-specific sparse masks to select a subset of the pretrained model's parameters that are further fine-tuned. These sparse fine-tuned vectors (SFTs) are subsequently composed with the pretrained model to facilitate zero-shot cross-lingual transfer to a task in a target language, using only task-specific data from a source language. These sparse masks for SFTs were identified using a simple magnitude-based pruning. In our work, we introduce DeFT-X, a novel composable SFT approach that denoises the weight matrices of a pretrained model before magnitude pruning using singular value decomposition, thus yielding more robust SFTs. We evaluate DeFT-X on a diverse set of extremely low-resource languages for sentiment classification (NusaX) and natural language inference (AmericasNLI) and demonstrate that it performs at par or outperforms SFT and other prominent cross-lingual transfer baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15090v1",
    "published_date": "2025-05-21 04:20:30 UTC",
    "updated_date": "2025-05-21 04:20:30 UTC"
  },
  {
    "arxiv_id": "2505.15088v1",
    "title": "Leveraging Large Language Models for Command Injection Vulnerability Analysis in Python: An Empirical Study on Popular Open-Source Projects",
    "authors": [
      "Yuxuan Wang",
      "Jingshu Chen",
      "Qingyang Wang"
    ],
    "abstract": "Command injection vulnerabilities are a significant security threat in dynamic languages like Python, particularly in widely used open-source projects where security issues can have extensive impact. With the proven effectiveness of Large Language Models(LLMs) in code-related tasks, such as testing, researchers have explored their potential for vulnerabilities analysis. This study evaluates the potential of large language models (LLMs), such as GPT-4, as an alternative approach for automated testing for vulnerability detection. In particular, LLMs have demonstrated advanced contextual understanding and adaptability, making them promising candidates for identifying nuanced security vulnerabilities within code. To evaluate this potential, we applied LLM-based analysis to six high-profile GitHub projects-Django, Flask, TensorFlow, Scikit-learn, PyTorch, and Langchain-each with over 50,000 stars and extensive adoption across software development and academic research. Our analysis assesses both the strengths and limitations of LLMs in detecting command injection vulnerabilities, evaluating factors such as detection accuracy, efficiency, and practical integration into development workflows. In addition, we provide a comparative analysis of different LLM tools to identify those most suitable for security applications. Our findings offer guidance for developers and security researchers on leveraging LLMs as innovative and automated approaches to enhance software security.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15088v1",
    "published_date": "2025-05-21 04:14:35 UTC",
    "updated_date": "2025-05-21 04:14:35 UTC"
  },
  {
    "arxiv_id": "2505.15083v1",
    "title": "Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features",
    "authors": [
      "Jeremy Qin"
    ],
    "abstract": "Time series forecasting plays a crucial role in various applications, particularly in healthcare, where accurate predictions of future health trajectories can significantly impact clinical decision-making. Ensuring transparency and explainability of the models responsible for these tasks is essential for their adoption in critical settings. Recent work has explored a top-down approach to bi-level transparency, focusing on understanding trends and properties of predicted time series using static features. In this work, we extend this framework by incorporating exogenous time series features alongside static features in a structured manner, while maintaining cohesive interpretation. Our approach leverages the insights of trajectory comprehension to introduce an encoding mechanism for exogenous time series, where they are decomposed into meaningful trends and properties, enabling the extraction of interpretable patterns. Through experiments on several synthetic datasets, we demonstrate that our approach remains predictive while preserving interpretability and robustness. This work represents a step towards developing robust, and generalized time series forecasting models. The code is available at https://github.com/jeremy-qin/TIMEVIEW",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15083v1",
    "published_date": "2025-05-21 04:12:12 UTC",
    "updated_date": "2025-05-21 04:12:12 UTC"
  },
  {
    "arxiv_id": "2505.15080v2",
    "title": "SUS backprop: linear backpropagation algorithm for long inputs in transformers",
    "authors": [
      "Sergey Pankov",
      "Georges Harik"
    ],
    "abstract": "It is straightforward to design an unbiased gradient estimator that stochastically cuts the backpropagation flow through any part of a computational graph. By cutting the parts that have little effect on the computation, one can potentially save a significant amount of backpropagation computation in exchange for a minimal increase in the stochastic gradient variance, in some situations. Such a situation occurs in the attention mechanism of the transformer architecture. For long sequences, attention becomes the limiting factor, as its compute requirements increase quadratically with sequence length $n$. At the same time, most attention weights become very small, as most attention heads tend to connect a given token with only a small fraction of other tokens in the sequence. These weights become promising targets for cutting backpropagation. We propose a simple probabilistic rule controlled by a single parameter $c$ that cuts back-propagation through most attention weights, leaving at most $c$ interactions per token per attention head. This brings a factor of $c/n$ reduction in the compute required for the attention backpropagation, turning it from quadratic $O(n^2)$ to linear complexity $O(nc)$. We have empirically verified that, for a typical transformer model, cutting about $99\\%$ of the attention gradient flow (i.e. choosing $c \\sim 25-30$) results in relative gradient variance increase of only about $1\\%$ for $n \\sim 2000$, and it decreases with $n$. This approach is amenable to efficient sparse matrix implementation, thus being promising for making the cost of a backward pass negligible relative to the cost of a forward pass when training a transformer model on long sequences.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 9 figures; main results unchanged, Fig.5 updated, some text rearranged",
    "pdf_url": "https://arxiv.org/pdf/2505.15080v2",
    "published_date": "2025-05-21 04:00:38 UTC",
    "updated_date": "2025-06-04 19:53:25 UTC"
  },
  {
    "arxiv_id": "2505.15077v1",
    "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation",
    "authors": [
      "Alessandro dos Santos Ferreira",
      "Ana Paula Marques Ramos",
      "José Marcato Junior",
      "Wesley Nunes Gonçalves"
    ],
    "abstract": "Urban forests play a key role in enhancing environmental quality and supporting biodiversity in cities. Mapping and monitoring these green spaces are crucial for urban planning and conservation, yet accurately detecting trees is challenging due to complex landscapes and the variability in image resolution caused by different satellite sensors or UAV flight altitudes. While deep learning architectures have shown promise in addressing these challenges, their effectiveness remains strongly dependent on the availability of large and manually labeled datasets, which are often expensive and difficult to obtain in sufficient quantity. In this work, we propose a novel pipeline that integrates domain adaptation with GANs and Diffusion models to enhance the quality of low-resolution aerial images. Our proposed pipeline enhances low-resolution imagery while preserving semantic content, enabling effective tree segmentation without requiring large volumes of manually annotated data. Leveraging models such as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we generate realistic and structurally consistent synthetic samples that expand the training dataset and unify scale across domains. This approach not only improves the robustness of segmentation models across different acquisition conditions but also provides a scalable and replicable solution for remote sensing scenarios with scarce annotation resources. Experimental results demonstrated an improvement of over 50% in IoU for low-resolution images, highlighting the effectiveness of our method compared to traditional pipelines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.15077v1",
    "published_date": "2025-05-21 03:57:10 UTC",
    "updated_date": "2025-05-21 03:57:10 UTC"
  },
  {
    "arxiv_id": "2505.15076v1",
    "title": "Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories",
    "authors": [
      "Nanxu Gong",
      "Sixun Dong",
      "Haoyue Bai",
      "Xinyuan Wang",
      "Wangyang Ying",
      "Yanjie Fu"
    ],
    "abstract": "As a widely-used and practical tool, feature engineering transforms raw data into discriminative features to advance AI model performance. However, existing methods usually apply feature selection and generation separately, failing to strive a balance between reducing redundancy and adding meaningful dimensions. To fill this gap, we propose an agentic feature augmentation concept, where the unification of feature generation and selection is modeled as agentic teaming and planning. Specifically, we develop a Multi-Agent System with Long and Short-Term Memory (MAGS), comprising a selector agent to eliminate redundant features, a generator agent to produce informative new dimensions, and a router agent that strategically coordinates their actions. We leverage in-context learning with short-term memory for immediate feedback refinement and long-term memory for globally optimal guidance. Additionally, we employ offline Proximal Policy Optimization (PPO) reinforcement fine-tuning to train the router agent for effective decision-making to navigate a vast discrete feature space. Extensive experiments demonstrate that this unified agentic framework consistently achieves superior task performance by intelligently orchestrating feature selection and generation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15076v1",
    "published_date": "2025-05-21 03:49:24 UTC",
    "updated_date": "2025-05-21 03:49:24 UTC"
  },
  {
    "arxiv_id": "2505.15075v5",
    "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs",
    "authors": [
      "Hao Wang",
      "Pinzhi Huang",
      "Jihan Yang",
      "Saining Xie",
      "Daisuke Kawahara"
    ],
    "abstract": "The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The first version of this paper mistakenly included a prompt injection phrase, which was inappropriate and unprofessional. Although we corrected the version on arXiv and withdrew from the conference, my co-authors and university strongly request a full withdrawal. Given the situation, I no longer have the authority to manage this paper, and withdrawing it from arXiv is the most responsible action",
    "pdf_url": "https://arxiv.org/pdf/2505.15075v5",
    "published_date": "2025-05-21 03:43:37 UTC",
    "updated_date": "2025-08-24 23:55:20 UTC"
  },
  {
    "arxiv_id": "2505.15074v3",
    "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data",
    "authors": [
      "Yuhang Zhou",
      "Jing Zhu",
      "Shengyi Qian",
      "Zhuokai Zhao",
      "Xiyao Wang",
      "Xiaoyu Liu",
      "Ming Li",
      "Paiheng Xu",
      "Wei Ai",
      "Furong Huang"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups, assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks. Our code and data are available at https://github.com/Tonyzhou98/disco_grpo.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.15074v3",
    "published_date": "2025-05-21 03:43:29 UTC",
    "updated_date": "2025-09-24 17:25:12 UTC"
  },
  {
    "arxiv_id": "2506.11039v1",
    "title": "Angle Domain Guidance: Latent Diffusion Requires Rotation Rather Than Extrapolation",
    "authors": [
      "Cheng Jin",
      "Zhenyu Xiao",
      "Chutao Liu",
      "Yuantao Gu"
    ],
    "abstract": "Classifier-free guidance (CFG) has emerged as a pivotal advancement in text-to-image latent diffusion models, establishing itself as a cornerstone technique for achieving high-quality image synthesis. However, under high guidance weights, where text-image alignment is significantly enhanced, CFG also leads to pronounced color distortions in the generated images. We identify that these distortions stem from the amplification of sample norms in the latent space. We present a theoretical framework that elucidates the mechanisms of norm amplification and anomalous diffusion phenomena induced by classifier-free guidance. Leveraging our theoretical insights and the latent space structure, we propose an Angle Domain Guidance (ADG) algorithm. ADG constrains magnitude variations while optimizing angular alignment, thereby mitigating color distortions while preserving the enhanced text-image alignment achieved at higher guidance weights. Experimental results demonstrate that ADG significantly outperforms existing methods, generating images that not only maintain superior text alignment but also exhibit improved color fidelity and better alignment with human perceptual preferences.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.11039v1",
    "published_date": "2025-05-21 03:36:56 UTC",
    "updated_date": "2025-05-21 03:36:56 UTC"
  },
  {
    "arxiv_id": "2505.15068v1",
    "title": "ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges",
    "authors": [
      "Cheng Qian",
      "Hongyi Du",
      "Hongru Wang",
      "Xiusi Chen",
      "Yuji Zhang",
      "Avirup Sil",
      "Chengxiang Zhai",
      "Kathleen McKeown",
      "Heng Ji"
    ],
    "abstract": "Recent progress in large language models (LLMs) has enabled substantial advances in solving mathematical problems. However, existing benchmarks often fail to reflect the complexity of real-world problems, which demand open-ended, interdisciplinary reasoning and integration of computational tools. To address this gap, we introduce ModelingBench, a novel benchmark featuring real-world-inspired, open-ended problems from math modeling competitions across diverse domains, ranging from urban traffic optimization to ecosystem resource planning. These tasks require translating natural language into formal mathematical formulations, applying appropriate tools, and producing structured, defensible reports. ModelingBench also supports multiple valid solutions, capturing the ambiguity and creativity of practical modeling. We also present ModelingAgent, a multi-agent framework that coordinates tool use, supports structured workflows, and enables iterative self-refinement to generate well-grounded, creative solutions. To evaluate outputs, we further propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as domain-specialized judges assessing solutions from multiple expert perspectives. Empirical results show that ModelingAgent substantially outperforms strong baselines and often produces solutions indistinguishable from those of human experts. Together, our work provides a comprehensive framework for evaluating and advancing real-world problem-solving in open-ended, interdisciplinary modeling challenges.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "36 Pages, 26 Figures, 5 Tables",
    "pdf_url": "https://arxiv.org/pdf/2505.15068v1",
    "published_date": "2025-05-21 03:33:23 UTC",
    "updated_date": "2025-05-21 03:33:23 UTC"
  },
  {
    "arxiv_id": "2505.15065v2",
    "title": "The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support",
    "authors": [
      "Suhas BN",
      "Yash Mahajan",
      "Dominik Mattioli",
      "Andrew M. Sherrill",
      "Rosa I. Arriaga",
      "Chris W. Wiese",
      "Saeed Abdullah"
    ],
    "abstract": "This paper investigates the capacity of small language models (0.5B-5B parameters) to generate empathetic responses for individuals with PTSD. We introduce Trauma-Informed Dialogue for Empathy (TIDE), a novel dataset comprising 10,000 two-turn conversations across 500 diverse, clinically-grounded PTSD personas (https://huggingface.co/datasets/yenopoya/TIDE). Using frontier model outputs as ground truth, we evaluate eight small LLMs in zero-shot settings and after fine-tuning. Fine-tuning enhances empathetic capabilities, improving cosine similarity and perceived empathy, although gains vary across emotional scenarios and smaller models exhibit a \"knowledge transfer ceiling.\" As expected, Claude Sonnet 3.5 consistently outperforms all models, but surprisingly, the smaller models often approach human-rated empathy levels. Demographic analyses showed that older adults favored responses that validated distress before offering support (p = .004), while graduate-educated users preferred emotionally layered replies in specific scenarios. Gender-based differences were minimal (p > 0.15), suggesting the feasibility of broadly empathetic model designs. This work offers insights into building resource-efficient, emotionally intelligent systems for mental health support.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 3 figures. Accepted for Oral presentation at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.15065v2",
    "published_date": "2025-05-21 03:32:46 UTC",
    "updated_date": "2025-09-20 08:07:48 UTC"
  },
  {
    "arxiv_id": "2505.15062v3",
    "title": "Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning",
    "authors": [
      "Jiashu He",
      "Jinxuan Fan",
      "Bowen Jiang",
      "Ignacio Houine",
      "Dan Roth",
      "Alejandro Ribeiro"
    ],
    "abstract": "When addressing complex questions that require new information, people often associate the question with existing knowledge to derive a sensible answer. For instance, when evaluating whether melatonin aids insomnia, one might associate \"hormones helping mental disorders\" with \"melatonin being a hormone and insomnia a mental disorder\" to complete the reasoning. Large Language Models (LLMs) also require such associative thinking, particularly in resolving scientific inquiries when retrieved knowledge is insufficient and does not directly answer the question. Graph Inspired Veracity Extrapolation (GIVE) addresses this by using a knowledge graph (KG) to extrapolate structured knowledge. However, it involves the construction and pruning of many hypothetical triplets, which limits efficiency and generalizability. We propose Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic associative thinking through reinforcement learning. Self-GIVE extracts structured information and entity sets to assist the model in linking to the queried concepts. We address GIVE's key limitations: (1) extensive LLM calls and token overhead for knowledge extrapolation, (2) difficulty in deploying on smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B models by up to $\\textbf{28.5%$\\rightarrow$71.4%}$ and $\\textbf{78.6$\\rightarrow$90.5%}$ in samples $\\textbf{unseen}$ in challenging biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90%. Self-GIVE enhances the scalable integration of structured retrieval and reasoning with associative thinking.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15062v3",
    "published_date": "2025-05-21 03:30:55 UTC",
    "updated_date": "2025-10-03 19:41:22 UTC"
  },
  {
    "arxiv_id": "2505.15058v2",
    "title": "AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars",
    "authors": [
      "Tianbao Zhang",
      "Jian Zhao",
      "Yuer Li",
      "Zheng Zhu",
      "Ping Hu",
      "Zhaoxin Fan",
      "Wenjun Wu",
      "Xuelong Li"
    ],
    "abstract": "Whole-body audio-driven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents, with wide-ranging applications in virtual reality, digital entertainment, and remote communication. Existing approaches often generate audio-driven facial expressions and gestures independently, which introduces a significant limitation: the lack of seamless coordination between facial and gestural elements, resulting in less natural and cohesive animations. To address this limitation, we propose AsynFusion, a novel framework that leverages diffusion transformers to achieve harmonious expression and gesture synthesis. The proposed method is built upon a dual-branch DiT architecture, which enables the parallel generation of facial expressions and gestures. Within the model, we introduce a Cooperative Synchronization Module to facilitate bidirectional feature interaction between the two modalities, and an Asynchronous LCM Sampling strategy to reduce computational overhead while maintaining high-quality outputs. Extensive experiments demonstrate that AsynFusion achieves state-of-the-art performance in generating real-time, synchronized whole-body animations, consistently outperforming existing methods in both quantitative and qualitative evaluations.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "15pages, conference",
    "pdf_url": "https://arxiv.org/pdf/2505.15058v2",
    "published_date": "2025-05-21 03:28:53 UTC",
    "updated_date": "2025-10-14 07:40:58 UTC"
  },
  {
    "arxiv_id": "2505.15054v2",
    "title": "MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation",
    "authors": [
      "Feiyang Cai",
      "Jiahui Bai",
      "Tao Tang",
      "Guijuan He",
      "Joshua Luo",
      "Tianyu Zhu",
      "Srikanth Pilla",
      "Gang Li",
      "Ling Liu",
      "Feng Luo"
    ],
    "abstract": "Precise recognition, editing, and generation of molecules are essential prerequisites for both chemists and AI systems tackling various chemical tasks. We present MolLangBench, a comprehensive benchmark designed to evaluate fundamental molecule-language interface tasks: language-prompted molecular structure recognition, editing, and generation. To ensure high-quality, unambiguous, and deterministic outputs, we construct the recognition tasks using automated cheminformatics tools, and curate editing and generation tasks through rigorous expert annotation and validation. MolLangBench supports the evaluation of models that interface language with different molecular representations, including linear strings, molecular images, and molecular graphs. Evaluations of state-of-the-art models reveal significant limitations: the strongest model (GPT-5) achieves $86.2\\%$ and $85.5\\%$ accuracy on recognition and editing tasks, which are intuitively simple for humans, and performs even worse on the generation task, reaching only $43.0\\%$ accuracy. These results highlight the shortcomings of current AI systems in handling even preliminary molecular recognition and manipulation tasks. We hope MolLangBench will catalyze further research toward more effective and reliable AI systems for chemical applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15054v2",
    "published_date": "2025-05-21 03:22:01 UTC",
    "updated_date": "2025-10-02 13:32:35 UTC"
  },
  {
    "arxiv_id": "2505.15049v1",
    "title": "Towards a Working Definition of Designing Generative User Interfaces",
    "authors": [
      "Kyungho Lee"
    ],
    "abstract": "Generative UI is transforming interface design by facilitating AI-driven collaborative workflows between designers and computational systems. This study establishes a working definition of Generative UI through a multi-method qualitative approach, integrating insights from a systematic literature review of 127 publications, expert interviews with 18 participants, and analyses of 12 case studies. Our findings identify five core themes that position Generative UI as an iterative and co-creative process. We highlight emerging design models, including hybrid creation, curation-based workflows, and AI-assisted refinement strategies. Additionally, we examine ethical challenges, evaluation criteria, and interaction models that shape the field. By proposing a conceptual foundation, this study advances both theoretical discourse and practical implementation, guiding future HCI research toward responsible and effective generative UI design practices.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15049v1",
    "published_date": "2025-05-21 03:14:09 UTC",
    "updated_date": "2025-05-21 03:14:09 UTC"
  },
  {
    "arxiv_id": "2505.15047v2",
    "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration",
    "authors": [
      "Yingming Pu",
      "Tao Lin",
      "Hongyu Chen"
    ],
    "abstract": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failure to consistently link hypotheses with evidence, thereby hindering the systematic reduction of uncertainty. Overcoming these limitations fundamentally requires a principled approach to exploration. We introduce PiFlow, an information-theoretical framework, treating automated scientific discovery as a structured uncertainty reduction problem guided by principles (e.g., scientific laws). In evaluations across three distinct scientific domains -- discovering nanomaterial structures, bio-molecules, and superconductor candidates with targeted properties -- our method significantly improves discovery efficiency, reflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property values versus exploration steps, and enhances solution quality by 94.06\\% compared to a vanilla agent system. Overall, PiFlow serves as a Plug-and-Play method, establishing a novel paradigm shift in highly efficient automated scientific discovery, paving the way for more robust and accelerated AI-driven research. Code is publicly available at our \\href{https://github.com/amair-lab/PiFlow}{GitHub}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15047v2",
    "published_date": "2025-05-21 03:09:39 UTC",
    "updated_date": "2025-09-29 06:30:53 UTC"
  },
  {
    "arxiv_id": "2505.15046v3",
    "title": "ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding",
    "authors": [
      "Yifan Wu",
      "Lutao Yan",
      "Leixian Shen",
      "Yinan Mei",
      "Jiannan Wang",
      "Yuyu Luo"
    ],
    "abstract": "The emergence of Multi-modal Large Language Models (MLLMs) presents new opportunities for chart understanding. However, due to the fine-grained nature of these tasks, applying MLLMs typically requires large, high-quality datasets for task-specific fine-tuning, leading to high data collection and training costs. To address this, we propose ChartCards, a unified chart-metadata generation framework for multi-task chart understanding. ChartCards systematically synthesizes various chart information, including data tables, visualization code, visual elements, and multi-dimensional semantic captions. By structuring this information into organized metadata, ChartCards enables a single chart to support multiple downstream tasks, such as text-to-chart retrieval, chart summarization, chart-to-table conversion, chart description, and chart question answering. Using ChartCards, we further construct MetaChart, a large-scale high-quality dataset containing 10,862 data tables, 85K charts, and 170 K high-quality chart captions. We validate the dataset through qualitative crowdsourcing evaluations and quantitative fine-tuning experiments across various chart understanding tasks. Fine-tuning six different models on MetaChart resulted in an average performance improvement of 5% across all tasks. The most notable improvements are seen in text-to-chart retrieval and chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements of 17% and 28%, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Need to be revised",
    "pdf_url": "https://arxiv.org/pdf/2505.15046v3",
    "published_date": "2025-05-21 03:07:47 UTC",
    "updated_date": "2025-10-07 04:20:59 UTC"
  },
  {
    "arxiv_id": "2505.15044v1",
    "title": "Learning-based Airflow Inertial Odometry for MAVs using Thermal Anemometers in a GPS and vision denied environment",
    "authors": [
      "Ze Wang",
      "Jingang Qu",
      "Zhenyu Gao",
      "Pascal Morin"
    ],
    "abstract": "This work demonstrates an airflow inertial based odometry system with multi-sensor data fusion, including thermal anemometer, IMU, ESC, and barometer. This goal is challenging because low-cost IMUs and barometers have significant bias, and anemometer measurements are very susceptible to interference from spinning propellers and ground effects. We employ a GRU-based deep neural network to estimate relative air speed from noisy and disturbed anemometer measurements, and an observer with bias model to fuse the sensor data and thus estimate the state of aerial vehicle. A complete flight data, including takeoff and landing on the ground, shows that the approach is able to decouple the downwash induced wind speed caused by propellers and the ground effect, and accurately estimate the flight speed in a wind-free indoor environment. IMU, and barometer bias are effectively estimated, which significantly reduces the position integration drift, which is only 5.7m for 203s manual random flight. The open source is available on https://github.com/SyRoCo-ISIR/Flight-Speed-Estimation-Airflow.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15044v1",
    "published_date": "2025-05-21 02:53:49 UTC",
    "updated_date": "2025-05-21 02:53:49 UTC"
  },
  {
    "arxiv_id": "2505.15039v1",
    "title": "LogiCase: Effective Test Case Generation from Logical Description in Competitive Programming",
    "authors": [
      "Sicheol Sung",
      "Aditi",
      "Dogyu kim",
      "Yo-Sub Han",
      "Sang-Ki Ko"
    ],
    "abstract": "Automated Test Case Generation (ATCG) is crucial for evaluating software reliability, particularly in competitive programming where robust algorithm assessments depend on diverse and accurate test cases. However, existing ATCG methods often fail to meet complex specifications or generate effective corner cases, limiting their utility. In this work, we introduce Context-Free Grammars with Counters (CCFGs), a formalism that captures both syntactic and semantic structures in input specifications. Using a fine-tuned CodeT5 model, we translate natural language input specifications into CCFGs, enabling the systematic generation of high-quality test cases. Experiments on the CodeContests dataset demonstrate that CCFG-based test cases outperform baseline methods in identifying incorrect algorithms, achieving significant gains in validity and effectiveness. Our approach provides a scalable and reliable grammar-driven framework for enhancing automated competitive programming evaluations.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15039v1",
    "published_date": "2025-05-21 02:48:01 UTC",
    "updated_date": "2025-05-21 02:48:01 UTC"
  },
  {
    "arxiv_id": "2505.15038v2",
    "title": "Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering",
    "authors": [
      "Haiyan Zhao",
      "Xuansheng Wu",
      "Fan Yang",
      "Bo Shen",
      "Ninghao Liu",
      "Mengnan Du"
    ],
    "abstract": "Linear concept vectors effectively steer LLMs, but existing methods suffer from noisy features in diverse datasets that undermine steering robustness. We propose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which selectively keep the most discriminative SAE latents while reconstructing hidden representations. Our key insight is that concept-relevant signals can be explicitly separated from dataset noise by scaling up activations of top-k latents that best differentiate positive and negative samples. Applied to linear probing and difference-in-mean, SDCV consistently improves steering success rates by 4-16\\% across six challenging concepts, while maintaining topic relevance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.15038v2",
    "published_date": "2025-05-21 02:45:11 UTC",
    "updated_date": "2025-07-29 21:40:42 UTC"
  },
  {
    "arxiv_id": "2505.15036v1",
    "title": "Fault-Tolerant Multi-Robot Coordination with Limited Sensing within Confined Environments",
    "authors": [
      "Kehinde O. Aina",
      "Hosain Bagheri",
      "Daniel I. Goldman"
    ],
    "abstract": "As robots are increasingly deployed to collaborate on tasks within shared workspaces and resources, the failure of an individual robot can critically affect the group's performance. This issue is particularly challenging when robots lack global information or direct communication, relying instead on social interaction for coordination and to complete their tasks. In this study, we propose a novel fault-tolerance technique leveraging physical contact interactions in multi-robot systems, specifically under conditions of limited sensing and spatial confinement. We introduce the \"Active Contact Response\" (ACR) method, where each robot modulates its behavior based on the likelihood of encountering an inoperative (faulty) robot. Active robots are capable of collectively repositioning stationary and faulty peers to reduce obstructions and maintain optimal group functionality. We implement our algorithm in a team of autonomous robots, equipped with contact-sensing and collision-tolerance capabilities, tasked with collectively excavating cohesive model pellets. Experimental results indicate that the ACR method significantly improves the system's recovery time from robot failures, enabling continued collective excavation with minimal performance degradation. Thus, this work demonstrates the potential of leveraging local, social, and physical interactions to enhance fault tolerance and coordination in multi-robot systems operating in constrained and extreme environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 4 figures. Accepted to DARS 2024 (Distributed Autonomous Robotic Systems), to appear in Springer Proceedings in Advanced Robotics",
    "pdf_url": "https://arxiv.org/pdf/2505.15036v1",
    "published_date": "2025-05-21 02:43:36 UTC",
    "updated_date": "2025-05-21 02:43:36 UTC"
  },
  {
    "arxiv_id": "2505.15034v2",
    "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning",
    "authors": [
      "Kaiwen Zha",
      "Zhengqi Gao",
      "Maohao Shen",
      "Zhang-Wei Hong",
      "Duane S. Boning",
      "Dina Katabi"
    ],
    "abstract": "Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, we propose Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025. The first two authors contributed equally",
    "pdf_url": "https://arxiv.org/pdf/2505.15034v2",
    "published_date": "2025-05-21 02:43:15 UTC",
    "updated_date": "2025-10-23 03:38:20 UTC"
  },
  {
    "arxiv_id": "2505.15033v1",
    "title": "Toward Task Capable Active Matter: Learning to Avoid Clogging in Confined Collectives via Collisions",
    "authors": [
      "Kehinde O. Aina",
      "Ram Avinery",
      "Hui-Shun Kuan",
      "Meredith D. Betterton",
      "Michael A. D. Goodisman",
      "Daniel I. Goldman"
    ],
    "abstract": "Social organisms which construct nests consisting of tunnels and chambers necessarily navigate confined and crowded conditions. Unlike low-density collectives like bird flocks and insect swarms, in which hydrodynamic and statistical phenomena dominate, the physics of glasses and supercooled fluids is important to understand clogging behaviors in high-density collectives. Our previous work revealed that fire ants flowing in confined tunnels utilize diverse behaviors like unequal workload distributions, spontaneous direction reversals, and limited interaction times to mitigate clogging and jamming and thus maintain functional flow; implementation of similar rules in a small robophysical swarm led to high performance through spontaneous dissolution of clogs and clusters. However, how the insects learn such behaviors, and how we can develop \"task capable\" active matter in such regimes, remains a challenge in part because interaction dynamics are dominated by local, time-consuming collisions and no single agent can guide the entire collective. Here, we hypothesized that effective flow and clog mitigation could emerge purely through local learning. We tasked small groups of robots with pellet excavation in a narrow tunnel, allowing them to modify reversal probabilities over time. Initially, robots had equal probabilities and clogs were common. Reversals improved flow. When reversal probabilities adapted via collisions and noisy tunnel length estimates, workload inequality and performance improved. Our robophysical study of an excavating swarm shows that, despite the seeming complexity and difficulty of the task, simple learning rules can mitigate or leverage unavoidable features in task-capable dense active matter, leading to hypotheses for dense biological and robotic swarms.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "13 pages, 9 figures. Published in Frontiers in Physics, Social Physics section. Includes experimental and simulation analysis of multi-robot excavation using decentralized learning",
    "pdf_url": "https://arxiv.org/pdf/2505.15033v1",
    "published_date": "2025-05-21 02:42:32 UTC",
    "updated_date": "2025-05-21 02:42:32 UTC"
  },
  {
    "arxiv_id": "2505.15031v1",
    "title": "Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI",
    "authors": [
      "Wenqing Wu",
      "Haixu Xi",
      "Chengzhi Zhang"
    ],
    "abstract": "Peer review is vital in academia for evaluating research quality. Top AI conferences use reviewer confidence scores to ensure review reliability, but existing studies lack fine-grained analysis of text-score consistency, potentially missing key details. This work assesses consistency at word, sentence, and aspect levels using deep learning and NLP conference review data. We employ deep learning to detect hedge sentences and aspects, then analyze report length, hedge word/sentence frequency, aspect mentions, and sentiment to evaluate text-score alignment. Correlation, significance, and regression tests examine confidence scores' impact on paper outcomes. Results show high text-score consistency across all levels, with regression revealing higher confidence scores correlate with paper rejection, validating expert assessments and peer review fairness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15031v1",
    "published_date": "2025-05-21 02:26:47 UTC",
    "updated_date": "2025-05-21 02:26:47 UTC"
  },
  {
    "arxiv_id": "2505.15023v1",
    "title": "Towards a Science of Causal Interpretability in Deep Learning for Software Engineering",
    "authors": [
      "David N. Palacio"
    ],
    "abstract": "This dissertation addresses achieving causal interpretability in Deep Learning for Software Engineering (DL4SE). While Neural Code Models (NCMs) show strong performance in automating software tasks, their lack of transparency in causal relationships between inputs and outputs limits full understanding of their capabilities. To build trust in NCMs, researchers and practitioners must explain code predictions. Associational interpretability, which identifies correlations, is often insufficient for tasks requiring intervention and change analysis. To address this, the dissertation introduces DoCode, a novel post hoc interpretability method for NCMs. DoCode uses causal inference to provide programming language-oriented explanations of model predictions. It follows a four-step pipeline: modeling causal problems using Structural Causal Models (SCMs), identifying the causal estimand, estimating effects with metrics like Average Treatment Effect (ATE), and refuting effect estimates. Its framework is extensible, with an example that reduces spurious correlations by grounding explanations in programming language properties. A case study on deep code generation across interpretability scenarios and various deep learning architectures demonstrates DoCode's benefits. Results show NCMs' sensitivity to code syntax changes and their ability to learn certain programming concepts while minimizing confounding bias. The dissertation also examines associational interpretability as a foundation, analyzing software information's causal nature using tools like COMET and TraceXplainer for traceability. It highlights the need to identify code confounders and offers practical guidelines for applying causal interpretability to NCMs, contributing to more trustworthy AI in software engineering.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "PhD thesis, To appear in ProQuest",
    "pdf_url": "https://arxiv.org/pdf/2505.15023v1",
    "published_date": "2025-05-21 02:13:11 UTC",
    "updated_date": "2025-05-21 02:13:11 UTC"
  },
  {
    "arxiv_id": "2505.15011v1",
    "title": "HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning",
    "authors": [
      "Kryspin Varys",
      "Federico Cerutti",
      "Adam Sobey",
      "Timothy J. Norman"
    ],
    "abstract": "Our society is governed by a set of norms which together bring about the values we cherish such as safety, fairness or trustworthiness. The goal of value-alignment is to create agents that not only do their tasks but through their behaviours also promote these values. Many of the norms are written as laws or rules (legal / safety norms) but even more remain unwritten (social norms). Furthermore, the techniques used to represent these norms also differ. Safety / legal norms are often represented explicitly, for example, in some logical language while social norms are typically learned and remain hidden in the parameter space of a neural network. There is a lack of approaches in the literature that could combine these various norm representations into a single algorithm. We propose a novel method that integrates these norms into the reinforcement learning process. Our method monitors the agent's compliance with the given norms and summarizes it in a quantity we call the agent's reputation. This quantity is used to weigh the received rewards to motivate the agent to become value-aligned. We carry out a series of experiments including a continuous state space traffic problem to demonstrate the importance of the written and unwritten norms and show how our method can find the value-aligned policies. Furthermore, we carry out ablations to demonstrate why it is better to combine these two groups of norms rather than using either separately.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15011v1",
    "published_date": "2025-05-21 01:32:54 UTC",
    "updated_date": "2025-05-21 01:32:54 UTC"
  },
  {
    "arxiv_id": "2505.15009v3",
    "title": "How Transformers Learn In-Context Recall Tasks? Optimality, Training Dynamics and Generalization",
    "authors": [
      "Quan Nguyen",
      "Thanh Nguyen-Tang"
    ],
    "abstract": "We study the approximation capabilities, convergence speeds and on-convergence behaviors of transformers trained on in-context recall tasks -- which requires to recognize the \\emph{positional} association between a pair of tokens from in-context examples. Existing theoretical results only focus on the in-context reasoning behavior of transformers after being trained for the \\emph{one} gradient descent step. It remains unclear what is the on-convergence behavior of transformers being trained by gradient descent and how fast the convergence rate is. In addition, the generalization of transformers in one-step in-context reasoning has not been formally investigated. This work addresses these gaps. We first show that a class of transformers with either linear, ReLU or softmax attentions, is provably Bayes-optimal for an in-context recall task. When being trained with gradient descent, we show via a finite-sample analysis that the expected loss converges at linear rate to the Bayes risks. Moreover, we show that the trained transformers exhibit out-of-distribution (OOD) generalization, i.e., generalizing to samples outside of the population distribution. Our theoretical findings are further supported by extensive empirical validations, showing that \\emph{without} proper parameterization, models with larger expressive power surprisingly \\emph{fail} to generalize OOD after being trained by gradient descent.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "V3: added new results for softmax attention, typos fixed, titled changed. 33 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.15009v3",
    "published_date": "2025-05-21 01:26:44 UTC",
    "updated_date": "2025-10-21 17:34:30 UTC"
  },
  {
    "arxiv_id": "2505.15008v1",
    "title": "Know When to Abstain: Optimal Selective Classification with Likelihood Ratios",
    "authors": [
      "Alvin Heng",
      "Harold Soh"
    ],
    "abstract": "Selective classification enhances the reliability of predictive models by allowing them to abstain from making uncertain predictions. In this work, we revisit the design of optimal selection functions through the lens of the Neyman--Pearson lemma, a classical result in statistics that characterizes the optimal rejection rule as a likelihood ratio test. We show that this perspective not only unifies the behavior of several post-hoc selection baselines, but also motivates new approaches to selective classification which we propose here. A central focus of our work is the setting of covariate shift, where the input distribution at test time differs from that at training. This realistic and challenging scenario remains relatively underexplored in the context of selective classification. We evaluate our proposed methods across a range of vision and language tasks, including both supervised learning and vision-language models. Our experiments demonstrate that our Neyman--Pearson-informed methods consistently outperform existing baselines, indicating that likelihood ratio-based selection offers a robust mechanism for improving selective classification under covariate shifts. Our code is publicly available at https://github.com/clear-nus/sc-likelihood-ratios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.15008v1",
    "published_date": "2025-05-21 01:26:21 UTC",
    "updated_date": "2025-05-21 01:26:21 UTC"
  },
  {
    "arxiv_id": "2505.15002v2",
    "title": "Unraveling the iterative CHAD",
    "authors": [
      "Fernando Lucatelli Nunes",
      "Gordon Plotkin",
      "Matthijs Vákár"
    ],
    "abstract": "Combinatory Homomorphic Automatic Differentiation (CHAD) was originally formulated as a semantics-driven source-to-source transformation for reverse-mode AD of total (terminating) functional programs. In this work, we extend CHAD to encompass programs featuring constructs such as partial (potentially non-terminating) operations, data-dependent conditionals (e.g., real-valued tests), and iteration constructs (i.e. while-loops), while maintaining CHAD's core principle of structure-preserving semantics.\n  A central contribution is the introduction of iteration-extensive indexed categories, which provide a principled integration of iteration into dependently typed programming languages. This integration is achieved by requiring that iteration in the base category lifts to parameterized initial algebras in the indexed category, yielding an op-fibred iterative structure that models while-loops and other iteration constructs in the total category, which corresponds to the category of containers of our dependently typed language.\n  Through the idea of iteration-extensive indexed categories, we extend the CHAD transformation to looping programs as the unique structure-preserving functor in a suitable sense. Specifically, it is the unique iterative Freyd category morphism from the iterative Freyd category corresponding to the source language to the category of containers obtained from the target language, such that each primitive operation is mapped to its (transposed) derivative. We establish the correctness of this extended transformation via the universal property of the syntactic categorical model of the source language, showing that the differentiated programs compute correct reverse-mode derivatives of their originals.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.LG",
      "math.CT",
      "math.LO"
    ],
    "primary_category": "cs.PL",
    "comment": "58 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.15002v2",
    "published_date": "2025-05-21 01:10:40 UTC",
    "updated_date": "2025-08-13 21:59:23 UTC"
  },
  {
    "arxiv_id": "2505.14999v3",
    "title": "Learning to Rank Chain-of-Thought: Using a Small Model",
    "authors": [
      "Eric Hanchen Jiang",
      "Haozheng Luo",
      "Shengyuan Pang",
      "Xiaomin Li",
      "Zhenting Qi",
      "Hengli Li",
      "Cheng-Fu Yang",
      "Zongyu Lin",
      "Xinfeng Li",
      "Hao Xu",
      "Kai-Wei Chang",
      "Ying Nian Wu"
    ],
    "abstract": "Large Language Models (LLMs) struggle with reliable mathematical reasoning, and current verification methods are often computationally expensive. This paper introduces the Energy Outcome Reward Model (EORM), a highly efficient, lightweight post-hoc verifier designed to address this challenge. EORM uses an energy-based framework to rank Chain-of-Thought (CoT) solutions, learning to distinguish correct from incorrect reasoning using only simple outcome labels, thus eliminating the need for expensive annotations. With only 55M parameters, over 127 times smaller than typical reward models, EORM boosts the accuracy of Llama 3 8B to 90.7\\% on GSM8k and 63.7\\% on MATH. This performance is achieved by efficiently selecting the optimal reasoning path from a pool of candidates, allowing it to match or exceed the accuracy of far more resource-intensive Best-of-N sampling techniques. Crucially, our experiments show that EORM generalizes effectively to out-of-distribution problems and unseen models, indicating it learns fundamental principles of valid reasoning. This robustness, combined with its efficiency, establishes EORM as a practical tool for deploying more dependable LLMs in complex, real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14999v3",
    "published_date": "2025-05-21 01:06:29 UTC",
    "updated_date": "2025-09-30 18:50:37 UTC"
  },
  {
    "arxiv_id": "2506.11035v2",
    "title": "Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity",
    "authors": [
      "Moussa Koulako Bala Doumbouya",
      "Dan Jurafsky",
      "Christopher D. Manning"
    ],
    "abstract": "Work in psychology has highlighted that the geometric model of similarity standard in deep learning is not psychologically plausible because its metric properties such as symmetry do not align with human perception of similarity. In contrast, Tversky (1977) proposed an axiomatic theory of similarity with psychological plausibility based on a representation of objects as sets of features, and their similarity as a function of their common and distinctive features. This model of similarity has not been used in deep learning before, in part because of the challenge of incorporating discrete set operations. In this paper, we develop a differentiable parameterization of Tversky's similarity that is learnable through gradient descent, and derive basic neural network building blocks such as the Tversky projection layer, which unlike the linear projection layer can model non-linear functions such as XOR. Through experiments with image recognition and language modeling neural networks, we show that the Tversky projection layer is a beneficial replacement for the linear projection layer. For instance, on the NABirds image classification task, a frozen ResNet-50 adapted with a Tversky projection layer achieves a 24.7% relative accuracy improvement over the linear layer adapter baseline. With Tversky projection layers, GPT-2's perplexity on PTB decreases by 7.8%, and its parameter count by 34.8%. Finally, we propose a unified interpretation of both types of projection layers as computing similarities of input stimuli to learned prototypes for which we also propose a novel visualization technique highlighting the interpretability of Tversky projection layers. Our work offers a new paradigm for thinking about the similarity model implicit in modern deep learning, and designing neural networks that are interpretable under an established theory of psychological similarity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11035v2",
    "published_date": "2025-05-21 01:01:48 UTC",
    "updated_date": "2025-10-12 01:44:24 UTC"
  },
  {
    "arxiv_id": "2505.14996v3",
    "title": "MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision",
    "authors": [
      "Zixuan Ke",
      "Austin Xu",
      "Yifei Ming",
      "Xuan-Phi Nguyen",
      "Ryan Chin",
      "Caiming Xiong",
      "Shafiq Joty"
    ],
    "abstract": "Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation set for tuning and yield static MAS designs lacking adaptability during inference, while also removing the flexibility to reduce to simpler systems. We introduce MAS-ZERO, the first self-evolved, inference-time framework for automatic MAS design. MAS-ZERO employs meta-level design to iteratively design, critique, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic problem decomposition and agent composition through meta-feedback on solvability and completeness, and reduction to simpler systems when appropriate. Experiments across reasoning (math and graduate-level QA), coding, and agentic (search-based) benchmarks, using both closed-source and open-source LLM backbones of varying sizes, demonstrate that MAS-ZERO outperforms strong manual and automatic MAS baselines. It achieves substantial average accuracy improvements of up to 16.69% on reasoning, 16.66% on coding, and 5.45% on agentic tasks, while maintaining cost efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "SEA@NeurIPS (Oral) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.14996v3",
    "published_date": "2025-05-21 00:56:09 UTC",
    "updated_date": "2025-12-01 21:03:03 UTC"
  },
  {
    "arxiv_id": "2506.11034v2",
    "title": "CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models",
    "authors": [
      "Aneesh Komanduri",
      "Karuna Bhaila",
      "Xintao Wu"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable ability in various language tasks, especially with their emergent in-context learning capability. Extending LLMs to incorporate visual inputs, large vision-language models (LVLMs) have shown impressive performance in tasks such as recognition and visual question answering (VQA). Despite increasing interest in the utility of LLMs in causal reasoning tasks such as causal discovery and counterfactual reasoning, there has been relatively little work showcasing the abilities of LVLMs on visual causal reasoning tasks. We take this opportunity to formally introduce a comprehensive causal reasoning benchmark for multi-modal in-context learning from LVLMs. Our CausalVLBench encompasses three representative tasks: causal structure inference, intervention target prediction, and counterfactual prediction. We evaluate the ability of state-of-the-art open-source LVLMs on our causal reasoning tasks across three causal representation learning datasets and demonstrate their fundamental strengths and weaknesses. We hope that our benchmark elucidates the drawbacks of existing vision-language models and motivates new directions and paradigms in improving the visual causal reasoning abilities of LVLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025 Main)",
    "pdf_url": "https://arxiv.org/pdf/2506.11034v2",
    "published_date": "2025-05-21 00:45:15 UTC",
    "updated_date": "2025-10-10 15:38:10 UTC"
  },
  {
    "arxiv_id": "2505.14983v1",
    "title": "Toward Informed AV Decision-Making: Computational Model of Well-being and Trust in Mobility",
    "authors": [
      "Zahra Zahedi",
      "Shashank Mehrotra",
      "Teruhisa Misu",
      "Kumar Akash"
    ],
    "abstract": "For future human-autonomous vehicle (AV) interactions to be effective and smooth, human-aware systems that analyze and align human needs with automation decisions are essential. Achieving this requires systems that account for human cognitive states. We present a novel computational model in the form of a Dynamic Bayesian Network (DBN) that infers the cognitive states of both AV users and other road users, integrating this information into the AV's decision-making process. Specifically, our model captures the well-being of both an AV user and an interacting road user as cognitive states alongside trust. Our DBN models infer beliefs over the AV user's evolving well-being, trust, and intention states, as well as the possible well-being of other road users, based on observed interaction experiences. Using data collected from an interaction study, we refine the model parameters and empirically assess its performance. Finally, we extend our model into a causal inference model (CIM) framework for AV decision-making, enabling the AV to enhance user well-being and trust while balancing these factors with its own operational costs and the well-being of interacting road users. Our evaluation demonstrates the model's effectiveness in accurately predicting user's states and guiding informed, human-centered AV decisions.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.14983v1",
    "published_date": "2025-05-21 00:02:39 UTC",
    "updated_date": "2025-05-21 00:02:39 UTC"
  }
]
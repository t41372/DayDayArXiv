[
  {
    "arxiv_id": "2509.18198v1",
    "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation",
    "authors": [
      "Rui Liu",
      "Zikang Wang",
      "Peng Gao",
      "Yu Shen",
      "Pratap Tokekar",
      "Ming Lin"
    ],
    "abstract": "Autonomous systems have advanced significantly, but challenges persist in accident-prone environments where robust decision-making is crucial. A single vehicle's limited sensor range and obstructed views increase the likelihood of accidents. Multi-vehicle connected systems and multi-modal approaches, leveraging RGB images and LiDAR point clouds, have emerged as promising solutions. However, existing methods often assume the availability of all data modalities and connected vehicles during both training and testing, which is impractical due to potential sensor failures or missing connected vehicles. To address these challenges, we introduce a novel framework MMCD (Multi-Modal Collaborative Decision-making) for connected autonomy. Our framework fuses multi-modal observations from ego and collaborative vehicles to enhance decision-making under challenging conditions. To ensure robust performance when certain data modalities are unavailable during testing, we propose an approach based on cross-modal knowledge distillation with a teacher-student model structure. The teacher model is trained with multiple data modalities, while the student model is designed to operate effectively with reduced modalities. In experiments on $\\textit{connected autonomous driving with ground vehicles}$ and $\\textit{aerial-ground vehicles collaboration}$, our method improves driving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline in detecting potential accidents and making safe driving decisions. More information can be found on our website https://ruiiu.github.io/mmcd.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.18198v1",
    "published_date": "2025-09-19 23:38:18 UTC",
    "updated_date": "2025-09-19 23:38:18 UTC"
  },
  {
    "arxiv_id": "2509.16463v1",
    "title": "Entropic Causal Inference: Graph Identifiability",
    "authors": [
      "Spencer Compton",
      "Kristjan Greenewald",
      "Dmitriy Katz",
      "Murat Kocaoglu"
    ],
    "abstract": "Entropic causal inference is a recent framework for learning the causal graph between two variables from observational data by finding the information-theoretically simplest structural explanation of the data, i.e., the model with smallest entropy. In our work, we first extend the causal graph identifiability result in the two-variable setting under relaxed assumptions. We then show the first identifiability result using the entropic approach for learning causal graphs with more than two nodes. Our approach utilizes the property that ancestrality between a source node and its descendants can be determined using the bivariate entropic tests. We provide a sound sequential peeling algorithm for general graphs that relies on this property. We also propose a heuristic algorithm for small graphs that shows strong empirical performance. We rigorously evaluate the performance of our algorithms on synthetic data generated from a variety of models, observing improvement over prior work. Finally we test our algorithms on real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at ICML 2022. This version corrects a bug in semi-synthetic experiments",
    "pdf_url": "https://arxiv.org/pdf/2509.16463v1",
    "published_date": "2025-09-19 23:10:10 UTC",
    "updated_date": "2025-09-19 23:10:10 UTC"
  },
  {
    "arxiv_id": "2509.16457v1",
    "title": "Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations",
    "authors": [
      "Yunzhe Wang",
      "Gale M. Lucas",
      "Burcin Becerik-Gerber",
      "Volkan Ustun"
    ],
    "abstract": "Language-driven generative agents have enabled large-scale social simulations with transformative uses, from interpersonal training to aiding global policy-making. However, recent studies indicate that generative agent behaviors often deviate from expert expectations and real-world data--a phenomenon we term the Behavior-Realism Gap. To address this, we introduce a theoretical framework called Persona-Environment Behavioral Alignment (PEBA), formulated as a distribution matching problem grounded in Lewin's behavior equation stating that behavior is a function of the person and their environment. Leveraging PEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that iteratively refines agent personas, implicitly aligning their collective behaviors with realistic expert benchmarks within a specified environmental context. We validate PEvo in an active shooter incident simulation we developed, achieving an 84% average reduction in distributional divergence compared to no steering and a 34% improvement over explicit instruction baselines. Results also show PEvo-refined personas generalize to novel, related simulation scenarios. Our method greatly enhances behavioral realism and reliability in high-stakes social simulations. More broadly, the PEBA-PEvo framework provides a principled approach to developing trustworthy LLM-driven social simulations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025), Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2509.16457v1",
    "published_date": "2025-09-19 22:35:13 UTC",
    "updated_date": "2025-09-19 22:35:13 UTC"
  },
  {
    "arxiv_id": "2509.16456v2",
    "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning",
    "authors": [
      "Jiahao Yu",
      "Zelei Cheng",
      "Xian Wu",
      "Xinyu Xing"
    ],
    "abstract": "Large language models (LLMs) are increasingly used in various domains, showing impressive potential on different tasks. Recently, reasoning LLMs have been proposed to improve the \\textit{reasoning} or \\textit{thinking} capabilities of LLMs to solve complex problems. Despite the promising results of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs still remains a significant challenge. While existing optimization methods have advanced the LLM reasoning capabilities, they often treat reasoning trajectories as a whole, without considering the underlying critical steps within the trajectory. In this paper, we introduce \\textbf{G}uided \\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that dives into the reasoning process to enable more effective improvements. GPO first identifies the `critical step' within a reasoning trajectory - a point that the model must carefully proceed to succeed at the problem. We locate the critical step by estimating the advantage function. GPO then resets the policy to the critical step, samples the new rollout and prioritizes the learning process on those rollouts. This focus allows the model to learn more effectively from pivotal moments within the reasoning process to improve the reasoning performance. We demonstrate that GPO is a general strategy that can be integrated with various optimization methods to improve reasoning performance. Besides theoretical analysis, our experiments across challenging reasoning benchmarks show that GPO can consistently and significantly enhance the performance of existing optimization methods, showcasing its effectiveness and generalizability in improving LLM reasoning by concentrating on pivotal moments within the generation process.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2509.16456v2",
    "published_date": "2025-09-19 22:30:23 UTC",
    "updated_date": "2025-10-21 02:20:10 UTC"
  },
  {
    "arxiv_id": "2509.20374v2",
    "title": "CFDLLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics",
    "authors": [
      "Nithin Somasekharan",
      "Ling Yue",
      "Yadi Cao",
      "Weichao Li",
      "Patrick Emami",
      "Pochinapeddi Sai Bhargav",
      "Anurag Acharya",
      "Xingyu Xie",
      "Shaowu Pan"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated strong performance across general NLP tasks, but their utility in automating numerical experiments of complex physical system -- a critical and labor-intensive component -- remains underexplored. As the major workhorse of computational science over the past decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs. We introduce CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows. Grounded in real-world CFD practices, our benchmark combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior. CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems. Code and data are available at https://github.com/NREL-Theseus/cfdllmbench/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.20374v2",
    "published_date": "2025-09-19 22:21:26 UTC",
    "updated_date": "2025-10-10 15:05:13 UTC"
  },
  {
    "arxiv_id": "2509.16454v1",
    "title": "A Generative AI System for Biomedical Data Discovery with Grammar-Based Visualizations",
    "authors": [
      "Devin Lange",
      "Shanghua Gao",
      "Pengwei Sui",
      "Austen Money",
      "Priya Misner",
      "Marinka Zitnik",
      "Nils Gehlenborg"
    ],
    "abstract": "We explore the potential for combining generative AI with grammar-based visualizations for biomedical data discovery. In our prototype, we use a multi-agent system to generate visualization specifications and apply filters. These visualizations are linked together, resulting in an interactive dashboard that is progressively constructed. Our system leverages the strengths of natural language while maintaining the utility of traditional user interfaces. Furthermore, we utilize generated interactive widgets enabling user adjustment. Finally, we demonstrate the potential utility of this system for biomedical data discovery with a case study.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16454v1",
    "published_date": "2025-09-19 22:20:24 UTC",
    "updated_date": "2025-09-19 22:20:24 UTC"
  },
  {
    "arxiv_id": "2509.16452v1",
    "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models",
    "authors": [
      "Son Hai Nguyen",
      "Diwei Wang",
      "Jinhyeok Jang",
      "Hyewon Seo"
    ],
    "abstract": "Accurate vision-based action recognition is crucial for developing autonomous robots that can operate safely and reliably in complex, real-world environments. In this work, we advance video-based recognition of indoor daily actions for robotic perception by leveraging vision-language models (VLMs) enriched with domain-specific knowledge. We adapt a prompt-learning framework in which class-level textual descriptions of each action are embedded as learnable prompts into a frozen pre-trained VLM backbone. Several strategies for structuring and encoding these textual descriptions are designed and evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our method, using only RGB video inputs at test time, achieves over 95\\% accuracy and outperforms state-of-the-art approaches. These results highlight the effectiveness of knowledge-augmented prompts in enabling robust action recognition with minimal supervision.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16452v1",
    "published_date": "2025-09-19 22:12:49 UTC",
    "updated_date": "2025-09-19 22:12:49 UTC"
  },
  {
    "arxiv_id": "2509.16449v2",
    "title": "PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization",
    "authors": [
      "Tsz Fung Pang",
      "Maryam Berijanian",
      "Thomas Orth",
      "Breanna Shi",
      "Charlotte S. Alexander"
    ],
    "abstract": "Legal documents are often long, dense, and difficult to comprehend, not only for laypeople but also for legal experts. While automated document summarization has great potential to improve access to legal knowledge, prevailing task-based evaluators overlook divergent user and stakeholder needs. Tool development is needed to encompass the technicality of a case summary for a litigator yet be accessible for a self-help public researching for their lawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation framework that scores summaries through the lens of six personas, including legal and non-legal users. We also introduce a controlled dimension-shifted pilot dataset of U.S. civil rights case summaries that varies along depth, accessibility, and procedural detail as well as Diversity-Coverage Index (DCI) to expose divergent optima of legal summary between persona-aware and persona-agnostic judges. This work enables refinement of legal AI summarization systems for both expert and non-expert users, with the potential to increase access to legal knowledge. The code base and data are publicly available in GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in JURIX 2025 (Legal Knowledge and Information Systems, FAIA series, IOS Press). Long Paper",
    "pdf_url": "https://arxiv.org/pdf/2509.16449v2",
    "published_date": "2025-09-19 22:03:08 UTC",
    "updated_date": "2025-10-23 01:59:04 UTC"
  },
  {
    "arxiv_id": "2509.16444v2",
    "title": "Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots",
    "authors": [
      "Chenhan Lyu",
      "Yutong Song",
      "Pengfei Zhang",
      "Amir M. Rahmani"
    ],
    "abstract": "Mental health applications have emerged as a critical area in computational health, driven by rising global rates of mental illness, the integration of AI in psychological care, and the need for scalable solutions in underserved communities. These include therapy chatbots, crisis detection, and wellness platforms handling sensitive data, requiring specialized AI safety beyond general safeguards due to emotional vulnerability, risks like misdiagnosis or symptom exacerbation, and precise management of vulnerable states to avoid severe outcomes such as self-harm or loss of trust. Despite AI safety advances, general safeguards inadequately address mental health-specific challenges, including crisis intervention accuracy to avert escalations, therapeutic guideline adherence to prevent misinformation, scale limitations in resource-constrained settings, and adaptation to nuanced dialogues where generics may introduce biases or miss distress signals. We introduce an approach to apply Constitutional AI training with domain-specific mental health principles for safe, domain-adapted CAI systems in computational mental health applications.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to 2025 IEEE 21st International Conference on Body Sensor Networks (BSN)",
    "pdf_url": "https://arxiv.org/pdf/2509.16444v2",
    "published_date": "2025-09-19 21:46:47 UTC",
    "updated_date": "2026-01-20 03:08:53 UTC"
  },
  {
    "arxiv_id": "2509.16443v1",
    "title": "LightCode: Compiling LLM Inference for Photonic-Electronic Systems",
    "authors": [
      "Ryan Tomich",
      "Zhizhen Zhong",
      "Dirk Englund"
    ],
    "abstract": "The growing demand for low-latency, energy-efficient inference in large language models (LLMs) has catalyzed interest in heterogeneous architectures. While GPUs remain dominant, they are poorly suited for integration with emerging domain-specific accelerators like the Photonic Tensor Units (PTUs), which offer low-power, high-throughput linear computation. This motivates hybrid compilation strategies that combine photonic and electronic resources. We present LightCode, a compiler framework and simulator for mapping LLM inference workloads across hybrid photonic-electronic systems. LightCode introduces the Stacked Graph, an intermediate representation that encodes multiple hardware-specific realizations of each tensor operation. Hardware assignment is formulated as a constrained subgraph selection problem optimized for latency or energy under parametric cost models. We evaluate LightCode on the prefill stage of GPT-2 and Llama-7B showing that under our workload and hardware assumptions, (i) Photonic hardware reduced energy by up to 50% in our simulated workloads at maximum sequence length; (ii) multiplexing and assignment strategy yielded latency improvements exceeding 10x; and (iii) Optimizing for latency or energy resulted in distinct hardware mappings in our simulations. LightCode offers a module, foundational framework and simulator for compiling LLMs to emerging photonic accelerators.",
    "categories": [
      "physics.app-ph",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "physics.app-ph",
    "comment": "9 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.16443v1",
    "published_date": "2025-09-19 21:45:26 UTC",
    "updated_date": "2025-09-19 21:45:26 UTC"
  },
  {
    "arxiv_id": "2509.16437v1",
    "title": "SENSE-7: Taxonomy and Dataset for Measuring User Perceptions of Empathy in Sustained Human-AI Conversations",
    "authors": [
      "Jina Suh",
      "Lindy Le",
      "Erfan Shayegani",
      "Gonzalo Ramos",
      "Judith Amores",
      "Desmond C. Ong",
      "Mary Czerwinski",
      "Javier Hernandez"
    ],
    "abstract": "Empathy is increasingly recognized as a key factor in human-AI communication, yet conventional approaches to \"digital empathy\" often focus on simulating internal, human-like emotional states while overlooking the inherently subjective, contextual, and relational facets of empathy as perceived by users. In this work, we propose a human-centered taxonomy that emphasizes observable empathic behaviors and introduce a new dataset, Sense-7, of real-world conversations between information workers and Large Language Models (LLMs), which includes per-turn empathy annotations directly from the users, along with user characteristics, and contextual details, offering a more user-grounded representation of empathy. Analysis of 695 conversations from 109 participants reveals that empathy judgments are highly individualized, context-sensitive, and vulnerable to disruption when conversational continuity fails or user expectations go unmet. To promote further research, we provide a subset of 672 anonymized conversation and provide exploratory classification analysis, showing that an LLM-based classifier can recognize 5 levels of empathy with an encouraging average Spearman $ρ$=0.369 and Accuracy=0.487 over this set. Overall, our findings underscore the need for AI designs that dynamically tailor empathic behaviors to user contexts and goals, offering a roadmap for future research and practical development of socially attuned, human-centered artificial agents.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16437v1",
    "published_date": "2025-09-19 21:32:24 UTC",
    "updated_date": "2025-09-19 21:32:24 UTC"
  },
  {
    "arxiv_id": "2510.13816v1",
    "title": "GQVis: A Dataset of Genomics Data Questions and Visualizations for Generative AI",
    "authors": [
      "Skylar Sargent Walters",
      "Arthea Valderrama",
      "Thomas C. Smits",
      "David Kouřil",
      "Huyen N. Nguyen",
      "Sehi L'Yi",
      "Devin Lange",
      "Nils Gehlenborg"
    ],
    "abstract": "Data visualization is a fundamental tool in genomics research, enabling the exploration, interpretation, and communication of complex genomic features. While machine learning models show promise for transforming data into insightful visualizations, current models lack the training foundation for domain-specific tasks. In an effort to provide a foundational resource for genomics-focused model training, we present a framework for generating a dataset that pairs abstract, low-level questions about genomics data with corresponding visualizations. Building on prior work with statistical plots, our approach adapts to the complexity of genomics data and the specialized representations used to depict them. We further incorporate multiple linked queries and visualizations, along with justifications for design choices, figure captions, and image alt-texts for each item in the dataset. We use genomics data retrieved from three distinct genomics data repositories (4DN, ENCODE, Chromoscope) to produce GQVis: a dataset consisting of 1.14 million single-query data points, 628k query pairs, and 589k query chains. The GQVis dataset and generation code are available at https://huggingface.co/datasets/HIDIVE/GQVis and https://github.com/hms-dbmi/GQVis-Generation.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13816v1",
    "published_date": "2025-09-19 21:29:13 UTC",
    "updated_date": "2025-09-19 21:29:13 UTC"
  },
  {
    "arxiv_id": "2509.19375v1",
    "title": "Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation",
    "authors": [
      "Mridul Sharma",
      "Adeetya Patel",
      "Zaneta D' Souza",
      "Samira Abbasgholizadeh Rahimi",
      "Siva Reddy",
      "Sreenath Madathil"
    ],
    "abstract": "Despite their widespread applications, Large Language Models (LLMs) often struggle to express uncertainty, posing a challenge for reliable deployment in high stakes and safety critical domains like clinical diagnostics. Existing standard baseline methods such as model logits and elicited probabilities produce overconfident and poorly calibrated estimates. In this work, we propose Approximate Bayesian Computation (ABC), a likelihood-free Bayesian inference, based approach that treats LLMs as a stochastic simulator to infer posterior distributions over predictive probabilities. We evaluate our ABC approach on two clinically relevant benchmarks: a synthetic oral lesion diagnosis dataset and the publicly available GretelAI symptom-to-diagnosis dataset. Compared to standard baselines, our approach improves accuracy by up to 46.9\\%, reduces Brier scores by 74.4\\%, and enhances calibration as measured by Expected Calibration Error (ECE) and predictive entropy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.19375v1",
    "published_date": "2025-09-19 21:22:29 UTC",
    "updated_date": "2025-09-19 21:22:29 UTC"
  },
  {
    "arxiv_id": "2509.16431v1",
    "title": "Proactive Statistical Process Control Using AI: A Time Series Forecasting Approach for Semiconductor Manufacturing",
    "authors": [
      "Mohammad Iqbal Rasul Seeam",
      "Victor S. Sheng"
    ],
    "abstract": "In the manufacturing industry, it is very important to keep machines and processes running smoothly and without unexpected problems. One of the most common tools used to check if everything is working properly is called Statistical Process Control (SPC). Traditional SPC methods work by checking whether recent measurements are within acceptable limits. However, they only react after a problem has already occurred. This can lead to wasted materials, machine downtime, and increased costs. In this paper, we present a smarter way to use SPC. Instead of just reacting to issues after they happen, our system can predict future problems before they occur. We use a machine learning tool called Facebook Prophet, which is designed to work with time-series data (data that changes over time). Prophet looks at past data and forecasts what the next value will be. Then, we use SPC rules to decide if the predicted value is in a Safe zone (no problem), a Warning zone (needs attention), or a Critical zone (may require shutting down the process). We applied this system to real data from a semiconductor manufacturing company. One of the challenges with this data is that the measurements are not taken at regular time intervals. This makes it harder to predict future values accurately. Despite this, our model was able to make strong predictions and correctly classify the risk level of future measurements. The main benefit of our system is that it gives engineers and technicians a chance to act early - before something goes wrong. This helps reduce unexpected failures and improves the overall stability and reliability of the production process. By combining machine learning with traditional SPC, we make quality control more proactive, accurate, and useful for modern industry.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 3 figures, no .bbl file needed because bibliography already in main.tex file",
    "pdf_url": "https://arxiv.org/pdf/2509.16431v1",
    "published_date": "2025-09-19 21:15:31 UTC",
    "updated_date": "2025-09-19 21:15:31 UTC"
  },
  {
    "arxiv_id": "2509.16421v2",
    "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead",
    "authors": [
      "Aiden Chang",
      "Celso De Melo",
      "Stephanie M. Lukin"
    ],
    "abstract": "Real-time understanding of continuous video streams is essential for intelligent agents operating in high-stakes environments, including autonomous vehicles, surveillance drones, and disaster response robots. Yet, most existing video understanding and highlight detection methods assume access to the entire video during inference, making them unsuitable for online or streaming scenarios. In particular, current models optimize for offline summarization, failing to support step-by-step reasoning needed for real-time decision-making. We introduce Aha, an autoregressive highlight detection framework that predicts the relevance of each video frame against a task described in natural language. Without accessing future video frames, Aha utilizes a multimodal vision-language model and lightweight, decoupled heads trained on a large, curated dataset of human-centric video labels. To enable scalability, we introduce the Dynamic SinkCache mechanism that achieves constant memory usage across infinite-length streams without degrading performance on standard benchmarks. This encourages the hidden representation to capture high-level task objectives, enabling effective frame-level rankings for informativeness, relevance, and uncertainty with respect to the natural language task. Aha achieves state-of-the-art (SOTA) performance on highlight detection benchmarks, surpassing even prior offline, full-context approaches and video-language models by +5.9% on TVSum and +8.3% on Mr. Hisum in mAP (mean Average Precision). We explore Aha's potential for real-world robotics applications given a task-oriented natural language input and a continuous, robot-centric video. Both experiments demonstrate Aha's potential effectiveness as a real-time reasoning module for downstream planning and long-horizon understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at NeurIPS 2025, 32 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.16421v2",
    "published_date": "2025-09-19 21:03:00 UTC",
    "updated_date": "2025-09-23 00:52:32 UTC"
  },
  {
    "arxiv_id": "2509.16418v1",
    "title": "LenslessMic: Audio Encryption and Authentication via Lensless Computational Imaging",
    "authors": [
      "Petr Grinberg",
      "Eric Bezzam",
      "Paolo Prandoni",
      "Martin Vetterli"
    ],
    "abstract": "With society's increasing reliance on digital data sharing, the protection of sensitive information has become critical. Encryption serves as one of the privacy-preserving methods; however, its realization in the audio domain predominantly relies on signal processing or software methods embedded into hardware. In this paper, we introduce LenslessMic, a hybrid optical hardware-based encryption method that utilizes a lensless camera as a physical layer of security applicable to multiple types of audio. We show that LenslessMic enables (1) robust authentication of audio recordings and (2) encryption strength that can rival the search space of 256-bit digital standards, while maintaining high-quality signals and minimal loss of content information. The approach is validated with a low-cost Raspberry Pi prototype and is open-sourced together with datasets to facilitate research in the area.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CR",
    "comment": "Submitted to ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.16418v1",
    "published_date": "2025-09-19 20:59:30 UTC",
    "updated_date": "2025-09-19 20:59:30 UTC"
  },
  {
    "arxiv_id": "2509.16413v1",
    "title": "Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research",
    "authors": [
      "Richard Diehl Martinez",
      "David Demitri Africa",
      "Yuval Weiss",
      "Suchir Salhan",
      "Ryan Daniels",
      "Paula Buttery"
    ],
    "abstract": "Building language models (LMs), especially small and medium ones, remains more art than science. While large LMs often improve by sheer scale, it is still unclear why many design choices work. For small LMs, this uncertainty is more limiting: tight parameter budgets make each decision critical, yet researchers still lack systematic, scientific ways to test and refine new ideas.\n  We introduce Pico, a lightweight, modular framework that enables systematic, hypothesis-driven research for small and medium-scale language model development. Pico consists of two libraries that together provide a practical sandbox where researchers can make targeted changes to a model's architecture or training procedures and directly observe their effects on the model's behavior. To support reproducible experimentation, we also release a suite of baseline models, pico-decoder, trained under standardized conditions and open-sourced for the community. Case studies highlight how Pico can support iterative small LM design and analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16413v1",
    "published_date": "2025-09-19 20:55:01 UTC",
    "updated_date": "2025-09-19 20:55:01 UTC"
  },
  {
    "arxiv_id": "2509.18196v2",
    "title": "MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech",
    "authors": [
      "Jialong Mai",
      "Jinxin Ji",
      "Xiaofen Xing",
      "Chen Yang",
      "Weidong Chen",
      "Jingyuan Xing",
      "Xiangmin Xu"
    ],
    "abstract": "Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing lexical content, but largely fail to recognize nonverbal vocalizations (NVs) embedded in speech, such as sighs, laughs, and coughs. This capability is important for a comprehensive understanding of human communication, as NVs convey crucial emotional and intentional cues. Progress in NV-aware ASR has been hindered by the lack of high-quality, well-annotated datasets. To address this gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech dataset. Unlike most existing corpora that rely on model-based detection, MNV-17's performative nature ensures high-fidelity, clearly articulated NV instances. To the best of our knowledge, MNV-17 provides the most extensive set of nonverbal vocalization categories, comprising 17 distinct and well-balanced classes of common NVs. We benchmarked MNV-17 on four mainstream ASR architectures, evaluating their joint performance on semantic transcription and NV classification. The dataset and the pretrained model checkpoints will be made publicly available to facilitate future research in expressive ASR.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Official dataset available at: https://github.com/yongaifadian1/MNV-17. Submitted to ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.18196v2",
    "published_date": "2025-09-19 20:40:36 UTC",
    "updated_date": "2025-09-24 18:45:14 UTC"
  },
  {
    "arxiv_id": "2509.16399v1",
    "title": "VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping",
    "authors": [
      "Guojun Xiong",
      "Milind Tambe"
    ],
    "abstract": "In social impact optimization, AI decision systems often rely on solvers that optimize well-calibrated mathematical objectives. However, these solvers cannot directly accommodate evolving human preferences, typically expressed in natural language rather than formal constraints. Recent approaches address this by using large language models (LLMs) to generate new reward functions from preference descriptions. While flexible, they risk sacrificing the system's core utility guarantees. In this paper, we propose \\texttt{VORTEX}, a language-guided reward shaping framework that preserves established optimization goals while adaptively incorporating human feedback. By formalizing the problem as multi-objective optimization, we use LLMs to iteratively generate shaping rewards based on verbal reinforcement and text-gradient prompt updates. This allows stakeholders to steer decision behavior via natural language without modifying solvers or specifying trade-off weights. We provide theoretical guarantees that \\texttt{VORTEX} converges to Pareto-optimal trade-offs between utility and preference satisfaction. Empirical results in real-world allocation tasks demonstrate that \\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage goals while maintaining high task performance. This work introduces a practical and theoretically grounded paradigm for human-AI collaborative optimization guided by natural language.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "28pages, 19figures",
    "pdf_url": "https://arxiv.org/pdf/2509.16399v1",
    "published_date": "2025-09-19 20:22:13 UTC",
    "updated_date": "2025-09-19 20:22:13 UTC"
  },
  {
    "arxiv_id": "2509.16397v1",
    "title": "GRID: Graph-based Reasoning for Intervention and Discovery in Built Environments",
    "authors": [
      "Taqiya Ehsan",
      "Shuren Xia",
      "Jorge Ortiz"
    ],
    "abstract": "Manual HVAC fault diagnosis in commercial buildings takes 8-12 hours per incident and achieves only 60 percent diagnostic accuracy, reflecting analytics that stop at correlation instead of causation. To close this gap, we present GRID (Graph-based Reasoning for Intervention and Discovery), a three-stage causal discovery pipeline that combines constraint-based search, neural structural equation modeling, and language model priors to recover directed acyclic graphs from building sensor data. Across six benchmarks: synthetic rooms, EnergyPlus simulation, the ASHRAE Great Energy Predictor III dataset, and a live office testbed, GRID achieves F1 scores ranging from 0.65 to 1.00, with exact recovery (F1 = 1.00) in three controlled environments (Base, Hidden, Physical) and strong performance on real-world data (F1 = 0.89 on ASHRAE, 0.86 in noisy conditions). The method outperforms ten baseline approaches across all evaluation scenarios. Intervention scheduling achieves low operational impact in most scenarios (cost <= 0.026) while reducing risk metrics compared to baseline approaches. The framework integrates constraint-based methods, neural architectures, and domain-specific language model prompts to address the observational-causal gap in building analytics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16397v1",
    "published_date": "2025-09-19 20:19:48 UTC",
    "updated_date": "2025-09-19 20:19:48 UTC"
  },
  {
    "arxiv_id": "2509.16394v1",
    "title": "Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans",
    "authors": [
      "Deuksin Kwon",
      "Kaleen Shrestha",
      "Bin Han",
      "Elena Hayoung Lee",
      "Gale Lucas"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed in socially complex, interaction-driven tasks, yet their ability to mirror human behavior in emotionally and strategically complex contexts remains underexplored. This study assesses the behavioral alignment of personality-prompted LLMs in adversarial dispute resolution by simulating multi-turn conflict dialogues that incorporate negotiation. Each LLM is guided by a matched Five-Factor personality profile to control for individual variation and enhance realism. We evaluate alignment across three dimensions: linguistic style, emotional expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the closest alignment with humans in linguistic style and emotional dynamics, while Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial alignment gaps persist. Our findings establish a benchmark for alignment between LLMs and humans in socially complex interactions, underscoring both the promise and the limitations of personality conditioning in dialogue modeling.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 (Main Conference)",
    "pdf_url": "https://arxiv.org/pdf/2509.16394v1",
    "published_date": "2025-09-19 20:15:52 UTC",
    "updated_date": "2025-09-19 20:15:52 UTC"
  },
  {
    "arxiv_id": "2509.16391v2",
    "title": "CoUn: Empowering Machine Unlearning via Contrastive Learning",
    "authors": [
      "Yasser H. Khalil",
      "Mehdi Setayesh",
      "Hongliang Li"
    ],
    "abstract": "Machine unlearning (MU) aims to remove the influence of specific \"forget\" data from a trained model while preserving its knowledge of the remaining \"retain\" data. Existing MU methods based on label manipulation or model weight perturbations often achieve limited unlearning effectiveness. To address this, we introduce CoUn, a novel MU framework inspired by the observation that a model retrained from scratch using only retain data classifies forget data based on their semantic similarity to the retain data. CoUn emulates this behavior by adjusting learned data representations through contrastive learning (CL) and supervised learning, applied exclusively to retain data. Specifically, CoUn (1) leverages semantic similarity between data samples to indirectly adjust forget representations using CL, and (2) maintains retain representations within their respective clusters through supervised learning. Extensive experiments across various datasets and model architectures show that CoUn consistently outperforms state-of-the-art MU baselines in unlearning effectiveness. Additionally, integrating our CL module into existing baselines empowers their unlearning effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16391v2",
    "published_date": "2025-09-19 20:12:49 UTC",
    "updated_date": "2025-10-17 13:39:03 UTC"
  },
  {
    "arxiv_id": "2509.16372v1",
    "title": "Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation",
    "authors": [
      "Balu Bhasuran",
      "Mattia Prosperi",
      "Karim Hanna",
      "John Petrilli",
      "Caretia JeLayne Washington",
      "Zhe He"
    ],
    "abstract": "This study evaluates causal reasoning in large language models (LLMs) using 99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of Causation: association, intervention, and counterfactual reasoning. We examined common laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and paired them with relevant causal factors including age, gender, obesity, and smoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with responses evaluated by four medically trained human experts. GPT-o1 demonstrated stronger discriminative performance (AUROC overall = 0.80 +/- 0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores across association (0.75 vs 0.72), intervention (0.84 vs 0.70), and counterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and specificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings showing similar trends. Both models performed best on intervention questions and worst on counterfactuals, particularly in altered outcome scenarios. These findings suggest GPT-o1 provides more consistent causal reasoning, but refinement is required before adoption in high-stakes clinical applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16372v1",
    "published_date": "2025-09-19 19:28:12 UTC",
    "updated_date": "2025-09-19 19:28:12 UTC"
  },
  {
    "arxiv_id": "2509.16369v1",
    "title": "Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction",
    "authors": [
      "Akshay Govind Srinivasan",
      "Ryan Jacob George",
      "Jayden Koshy Joe",
      "Hrushikesh Kant",
      "Harshith M R",
      "Sachin Sundar",
      "Sudharshan Suresh",
      "Rahul Vimalkanth",
      "Vijayavallabh"
    ],
    "abstract": "Accurate and reliable knowledge retrieval is vital for financial question-answering, where continually updated data sources and complex, high-stakes contexts demand precision. Traditional retrieval systems rely on a single database and retriever, but financial applications require more sophisticated approaches to handle intricate regulatory filings, market analyses, and extensive multi-year reports. We introduce a framework for financial Retrieval Augmented Generation (RAG) that leverages agentic AI and the Multi-HyDE system, an approach that generates multiple, nonequivalent queries to boost the effectiveness and coverage of retrieval from large, structured financial corpora. Our pipeline is optimized for token efficiency and multi-step financial reasoning, and we demonstrate that their combination improves accuracy by 11.2% and reduces hallucinations by 15%. Our method is evaluated on standard financial QA benchmarks, showing that integrating domain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets, including keyword and table-based retrieval, significantly enhances both the accuracy and reliability of answers. This research not only delivers a modular, adaptable retrieval framework for finance but also highlights the importance of structured agent workflows and multi-perspective retrieval for trustworthy deployment of AI in high-stakes financial applications.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.IR",
    "comment": "14 Pages, 8 Tables, 2 Figures. Accepted and to be published in the proceedings of FinNLP, Empirical Methods in Natural Language Processing 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.16369v1",
    "published_date": "2025-09-19 19:24:30 UTC",
    "updated_date": "2025-09-19 19:24:30 UTC"
  },
  {
    "arxiv_id": "2509.16352v1",
    "title": "Secure Confidential Business Information When Sharing Machine Learning Models",
    "authors": [
      "Yunfan Yang",
      "Jiarong Xu",
      "Hongzhe Zhang",
      "Xiao Fang"
    ],
    "abstract": "Model-sharing offers significant business value by enabling firms with well-established Machine Learning (ML) models to monetize and share their models with others who lack the resources to develop ML models from scratch. However, concerns over data confidentiality remain a significant barrier to model-sharing adoption, as Confidential Property Inference (CPI) attacks can exploit shared ML models to uncover confidential properties of the model provider's private model training data. Existing defenses often assume that CPI attacks are non-adaptive to the specific ML model they are targeting. This assumption overlooks a key characteristic of real-world adversaries: their responsiveness, i.e., adversaries' ability to dynamically adjust their attack models based on the information of the target and its defenses. To overcome this limitation, we propose a novel defense method that explicitly accounts for the responsive nature of real-world adversaries via two methodological innovations: a novel Responsive CPI attack and an attack-defense arms race framework. The former emulates the responsive behaviors of adversaries in the real world, and the latter iteratively enhances both the target and attack models, ultimately producing a secure ML model that is robust against responsive CPI attacks. Furthermore, we propose and integrate a novel approximate strategy into our defense, which addresses a critical computational bottleneck of defense methods and improves defense efficiency. Through extensive empirical evaluations across various realistic model-sharing scenarios, we demonstrate that our method outperforms existing defenses by more effectively defending against CPI attacks, preserving ML model utility, and reducing computational overhead.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16352v1",
    "published_date": "2025-09-19 18:49:49 UTC",
    "updated_date": "2025-09-19 18:49:49 UTC"
  },
  {
    "arxiv_id": "2509.16348v1",
    "title": "A Unified AI Approach for Continuous Monitoring of Human Health and Diseases from Intensive Care Unit to Home with Physiological Foundation Models (UNIPHY+)",
    "authors": [
      "Minxiao Wang",
      "Saurabh Kataria",
      "Juntong Ni",
      "Timothy G. Buchman",
      "Jocelyn Grunwell",
      "Mark Mai",
      "Wei Jin",
      "Matthew Clark",
      "Stephanie Brown",
      "Michael Fundora",
      "Puneet Sharma",
      "Tony Pan",
      "Sam Khan",
      "Timothy Ruchti",
      "Naveen Muthu",
      "Kevin Maher",
      "Sivasubramanium V Bhavani",
      "Xiao Hu"
    ],
    "abstract": "We present UNIPHY+, a unified physiological foundation model (physioFM) framework designed to enable continuous human health and diseases monitoring across care settings using ubiquitously obtainable physiological data. We propose novel strategies for incorporating contextual information during pretraining, fine-tuning, and lightweight model personalization via multi-modal learning, feature fusion-tuning, and knowledge distillation. We advocate testing UNIPHY+ with a broad set of use cases from intensive care to ambulatory monitoring in order to demonstrate that UNIPHY+ can empower generalizable, scalable, and personalized physiological AI to support both clinical decision-making and long-term health monitoring.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16348v1",
    "published_date": "2025-09-19 18:40:47 UTC",
    "updated_date": "2025-09-19 18:40:47 UTC"
  },
  {
    "arxiv_id": "2509.16347v1",
    "title": "QUINTA: Reflexive Sensibility For Responsible AI Research and Data-Driven Processes",
    "authors": [
      "Alicia E. Boyd"
    ],
    "abstract": "As the field of artificial intelligence (AI) and machine learning (ML) continues to prioritize fairness and the concern for historically marginalized communities, the importance of intersectionality in AI research has gained significant recognition. However, few studies provide practical guidance on how researchers can effectively incorporate intersectionality into critical praxis. In response, this paper presents a comprehensive framework grounded in critical reflexivity as intersectional praxis. Operationalizing intersectionality within the AI/DS (Artificial Intelligence/Data Science) pipeline, Quantitative Intersectional Data (QUINTA) is introduced as a methodological paradigm that challenges conventional and superficial research habits, particularly in data-centric processes, to identify and mitigate negative impacts such as the inadvertent marginalization caused by these practices. The framework centers researcher reflexivity to call attention to the AI researchers' power in creating and analyzing AI/DS artifacts through data-centric approaches. To illustrate the effectiveness of QUINTA, we provide a reflexive AI/DS researcher demonstration utilizing the \\#metoo movement as a case study. Note: This paper was accepted as a poster presentation at Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO) Conference in 2023.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "14 pages, 5 figures, 1 Table, This paper was accepted as a poster presentation at Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO) Conference in 2023",
    "pdf_url": "https://arxiv.org/pdf/2509.16347v1",
    "published_date": "2025-09-19 18:40:30 UTC",
    "updated_date": "2025-09-19 18:40:30 UTC"
  },
  {
    "arxiv_id": "2509.16346v2",
    "title": "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR",
    "authors": [
      "Juan Castorena",
      "E. Louise Loudermilk",
      "Scott Pokswinski",
      "Rodman Linn"
    ],
    "abstract": "The 3D structure of living and non-living components in ecosystems plays a critical role in determining ecological processes and feedbacks from both natural and human-driven disturbances. Anticipating the effects of wildfire, drought, disease, or atmospheric deposition depends on accurate characterization of 3D vegetation structure, yet widespread measurement remains prohibitively expensive and often infeasible. We present ForestGen3D, a cross-domain generative framework that preserves aerial LiDAR (ALS) observed 3D forest structure while inferring missing sub-canopy detail. ForestGen3D is based on conditional denoising diffusion probabilistic models trained on co-registered ALS and terrestrial LiDAR (TLS) data. The model generates realistic TLS-like point clouds that remain spatially consistent with ALS geometry, enabling landscape-scalable reconstruction of full vertical forest structure. We evaluate ForestGen3D at tree, plot, and landscape scales using real-world data from mixed conifer ecosystems, and show through qualitative and quantitative geometric and distributional analyses that it produces high-fidelity reconstructions closely matching TLS reference data in terms of 3D structural similarity and downstream biophysical metrics, including tree height, DBH, crown diameter, and crown volume. We further introduce and demonstrate the expected point containment (EPC) metric which serves as a practical proxy for generation quality in settings where TLS ground truth is unavailable. Our results demonstrate that ForestGen3D enhances the utility of ALS only environments by inferring ecologically plausible sub-canopy structure while faithfully preserving the landscape heterogeneity encoded in ALS observations, thereby providing a richer 3D representation for ecological analysis, structural fuel characterization and related remote sensing applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16346v2",
    "published_date": "2025-09-19 18:39:50 UTC",
    "updated_date": "2026-01-21 22:53:57 UTC"
  },
  {
    "arxiv_id": "2509.16345v1",
    "title": "Estimating Clinical Lab Test Result Trajectories from PPG using Physiological Foundation Model and Patient-Aware State Space Model -- a UNIPHY+ Approach",
    "authors": [
      "Minxiao Wang",
      "Runze Yan",
      "Carol Li",
      "Saurabh Kataria",
      "Xiao Hu",
      "Matthew Clark",
      "Timothy Ruchti",
      "Timothy G. Buchman",
      "Sivasubramanium V Bhavani",
      "Randall J. Lee"
    ],
    "abstract": "Clinical laboratory tests provide essential biochemical measurements for diagnosis and treatment, but are limited by intermittent and invasive sampling. In contrast, photoplethysmogram (PPG) is a non-invasive, continuously recorded signal in intensive care units (ICUs) that reflects cardiovascular dynamics and can serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, a framework that combines a large-scale PPG foundation model for local waveform encoding with a patient-aware Mamba model for long-range temporal modeling. Our architecture addresses three challenges: (1) capturing extended temporal trends in laboratory values, (2) accounting for patient-specific baseline variation via FiLM-modulated initial states, and (3) performing multi-task estimation for interrelated biomarkers. We evaluate our method on the two ICU datasets for predicting the five key laboratory tests. The results show substantial improvements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$ among most of the estimation targets. This work demonstrates the feasibility of continuous, personalized lab value estimation from routine PPG monitoring, offering a pathway toward non-invasive biochemical surveillance in critical care.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16345v1",
    "published_date": "2025-09-19 18:38:06 UTC",
    "updated_date": "2025-09-19 18:38:06 UTC"
  },
  {
    "arxiv_id": "2509.16343v1",
    "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute",
    "authors": [
      "Chung-En",
      "Yu",
      "Brian Jalaian",
      "Nathaniel D. Bastian"
    ],
    "abstract": "Developing trustworthy intelligent vision systems for high-stakes domains, \\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness without costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a training-free, agentic reasoning framework that wraps off-the-shelf vision-language models \\emph{and} pure vision systems in a \\emph{Think--Critique--Act} loop. While VRA incurs significant additional test-time computation, it achieves up to 40\\% absolute accuracy gains on challenging visual reasoning benchmarks. Future work will optimize query routing and early stopping to reduce inference overhead while preserving reliability in vision tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16343v1",
    "published_date": "2025-09-19 18:34:08 UTC",
    "updated_date": "2025-09-19 18:34:08 UTC"
  },
  {
    "arxiv_id": "2509.16339v4",
    "title": "Highly Imbalanced Regression with Tabular Data in SEP and Other Applications",
    "authors": [
      "Josias K. Moukpe",
      "Philip K. Chan",
      "Ming Zhang"
    ],
    "abstract": "We investigate imbalanced regression with tabular data that have an imbalance ratio larger than 1,000 (\"highly imbalanced\"). Accurately estimating the target values of rare instances is important in applications such as forecasting the intensity of rare harmful Solar Energetic Particle (SEP) events. For regression, the MSE loss does not consider the correlation between predicted and actual values. Typical inverse importance functions allow only convex functions. Uniform sampling might yield mini-batches that do not have rare instances. We propose CISIR that incorporates correlation, Monotonically Decreasing Involution (MDI) importance, and stratified sampling. Based on five datasets, our experimental results indicate that CISIR can achieve lower error and higher correlation than some recent methods. Also, adding our correlation component to other recent methods can improve their performance. Lastly, MDI importance can outperform other importance functions. Our code can be found in https://github.com/Machine-Earning/CISIR.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICMLA 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.16339v4",
    "published_date": "2025-09-19 18:26:58 UTC",
    "updated_date": "2025-12-14 17:55:44 UTC"
  },
  {
    "arxiv_id": "2509.16332v1",
    "title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models",
    "authors": [
      "Stephen Fitz",
      "Peter Romero",
      "Steven Basart",
      "Sipeng Chen",
      "Jose Hernandez-Orallo"
    ],
    "abstract": "Large Language Models increasingly mediate high-stakes interactions, intensifying research on their capabilities and safety. While recent work has shown that LLMs exhibit consistent and measurable synthetic personality traits, little is known about how modulating these traits affects model behavior. We address this gap by investigating how psychometric personality control grounded in the Big Five framework influences AI behavior in the context of capability and safety benchmarks. Our experiments reveal striking effects: for example, reducing conscientiousness leads to significant drops in safety-relevant metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well as reduction in general capabilities as measured by MMLU. These findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and general competence. We discuss the implications for safety evaluation, alignment strategies, steering model behavior after deployment, and risks associated with possible exploitation of these findings. Our findings motivate a new line of research on personality-sensitive safety evaluations and dynamic behavioral control in LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16332v1",
    "published_date": "2025-09-19 18:19:56 UTC",
    "updated_date": "2025-09-19 18:19:56 UTC"
  },
  {
    "arxiv_id": "2509.16330v1",
    "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey",
    "authors": [
      "Minxing Zhang",
      "Yi Yang",
      "Roy Xie",
      "Bhuwan Dhingra",
      "Shuyan Zhou",
      "Jian Pei"
    ],
    "abstract": "Large Language Model (LLM)-based agents have emerged as a new paradigm that extends LLMs' capabilities beyond text generation to dynamic interaction with external environments. By integrating reasoning with perception, memory, and tool use, agents are increasingly deployed in diverse domains like web navigation and household robotics. A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data. Despite growing interest, the concept of generalizability in LLM-based agents remains underdefined, and systematic approaches to measure and improve it are lacking. In this survey, we provide the first comprehensive review of generalizability in LLM-based agents. We begin by emphasizing agent generalizability's importance by appealing to stakeholders and clarifying the boundaries of agent generalizability by situating it within a hierarchical domain-task ontology. We then review datasets, evaluation dimensions, and metrics, highlighting their limitations. Next, we categorize methods for improving generalizability into three groups: methods for the backbone LLM, for agent components, and for their interactions. Moreover, we introduce the distinction between generalizable frameworks and generalizable agents and outline how generalizable frameworks can be translated into agent-level generalizability. Finally, we identify critical challenges and future directions, including developing standardized frameworks, variance- and cost-based metrics, and approaches that integrate methodological innovations with architecture-level designs. By synthesizing progress and highlighting opportunities, this survey aims to establish a foundation for principled research on building LLM-based agents that generalize reliably across diverse applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16330v1",
    "published_date": "2025-09-19 18:13:32 UTC",
    "updated_date": "2025-09-19 18:13:32 UTC"
  },
  {
    "arxiv_id": "2509.16325v1",
    "title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap",
    "authors": [
      "Andrew Zhu",
      "Chris Callison-Burch"
    ],
    "abstract": "Imagine AI assistants that enhance conversations without interrupting them: quietly providing relevant information during a medical consultation, seamlessly preparing materials as teachers discuss lesson plans, or unobtrusively scheduling meetings as colleagues debate calendars. While modern conversational LLM agents directly assist human users with tasks through a chat interface, we study this alternative paradigm for interacting with LLM agents, which we call \"overhearing agents.\" Rather than demanding the user's attention, overhearing agents continuously monitor ambient activity and intervene only when they can provide contextual assistance. In this paper, we present the first analysis of overhearing LLM agents as a distinct paradigm in human-AI interaction and establish a taxonomy of overhearing agent interactions and tasks grounded in a survey of works on prior LLM-powered agents and exploratory HCI studies. Based on this taxonomy, we create a list of best practices for researchers and developers building overhearing agent systems. Finally, we outline the remaining research gaps and reveal opportunities for future research in the overhearing paradigm.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2509.16325v1",
    "published_date": "2025-09-19 18:11:04 UTC",
    "updated_date": "2025-09-19 18:11:04 UTC"
  },
  {
    "arxiv_id": "2509.16198v5",
    "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation",
    "authors": [
      "Jane Luo",
      "Xin Zhang",
      "Steven Liu",
      "Jie Wu",
      "Jianfeng Liu",
      "Yiming Huang",
      "Yangyu Huang",
      "Chengyu Yin",
      "Ying Xin",
      "Yuefeng Zhan",
      "Hao Sun",
      "Qi Chen",
      "Scarlett Li",
      "Mao Yang"
    ],
    "abstract": "Large language models excel at generating individual functions or single files of code, yet generating complete repositories from scratch remains a fundamental challenge. This capability is key to building coherent software systems from high-level specifications and realizing the full potential of automated code generation. The process requires planning at two levels: deciding what features and modules to build (proposal stage) and defining their implementation details (implementation stage). Current approaches rely on natural language planning, which often produces unclear specifications, misaligned components, and brittle designs due to its inherent ambiguity and lack of structure. To address these limitations, we introduce the Repository Planning Graph (RPG), a structured representation that encodes capabilities, file structures, data flows, and functions in a unified graph. By replacing free-form natural language with an explicit blueprint, RPG enables consistent long-horizon planning for repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework that operates in three stages: proposal-level planning, implementation-level construction, and graph-guided code generation with test validation. To evaluate, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces nearly 36K Code Lines and 445K Code Tokens, on average 3.9$\\times$ larger than the strongest baseline (Claude Code), and 68$\\times$ larger than other baselines. It achieves 81.5% coverage and 69.7% test accuracy, improving over Claude Code by 27.3 and 35.8 points. Further analysis shows that RPG models complex dependencies, enables more sophisticated planning through near-linear scaling, and improves agent understanding of repositories, thus accelerating localization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16198v5",
    "published_date": "2025-09-19 17:58:14 UTC",
    "updated_date": "2025-10-20 14:22:14 UTC"
  },
  {
    "arxiv_id": "2509.16195v1",
    "title": "FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation",
    "authors": [
      "Luca Della Libera",
      "Cem Subakan",
      "Mirco Ravanelli"
    ],
    "abstract": "Neural audio codecs are a fundamental component of modern generative audio pipelines. Although recent codecs achieve strong low-bitrate reconstruction and provide powerful representations for downstream tasks, most are non-streamable, limiting their use in real-time applications. We present FocalCodec-Stream, a hybrid codec based on focal modulation that compresses speech into a single binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our approach combines multi-stage causal distillation of WavLM with targeted architectural improvements, including a lightweight refiner module that enhances quality under latency constraints. Experiments show that FocalCodec-Stream outperforms existing streamable codecs at comparable bitrates, while preserving both semantic and acoustic information. The result is a favorable trade-off between reconstruction quality, downstream task performance, latency, and efficiency. Code and checkpoints will be released at https://github.com/lucadellalib/focalcodec.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2509.16195v1",
    "published_date": "2025-09-19 17:57:13 UTC",
    "updated_date": "2025-09-19 17:57:13 UTC"
  },
  {
    "arxiv_id": "2509.16188v1",
    "title": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs",
    "authors": [
      "Jinghao Zhang",
      "Sihang Jiang",
      "Shiwei Guo",
      "Shisong Chen",
      "Yanghua Xiao",
      "Hongwei Feng",
      "Jiaqing Liang",
      "Minggui HE",
      "Shimin Tao",
      "Hongxia Ma"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in diverse cultural environments, evaluating their cultural understanding capability has become essential for ensuring trustworthy and culturally aligned applications. However, most existing benchmarks lack comprehensiveness and are challenging to scale and adapt across different cultural contexts, because their frameworks often lack guidance from well-established cultural theories and tend to rely on expert-driven manual annotations. To address these issues, we propose CultureScope, the most comprehensive evaluation framework to date for assessing cultural understanding in LLMs. Inspired by the cultural iceberg theory, we design a novel dimensional schema for cultural knowledge classification, comprising 3 layers and 140 dimensions, which guides the automated construction of culture-specific knowledge bases and corresponding evaluation datasets for any given languages and cultures. Experimental results demonstrate that our method can effectively evaluate cultural understanding. They also reveal that existing large language models lack comprehensive cultural competence, and merely incorporating multilingual data does not necessarily enhance cultural understanding. All code and data files are available at https://github.com/HoganZinger/Culture",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16188v1",
    "published_date": "2025-09-19 17:47:48 UTC",
    "updated_date": "2025-09-19 17:47:48 UTC"
  },
  {
    "arxiv_id": "2509.16184v1",
    "title": "Accelerating Atomic Fine Structure Determination with Graph Reinforcement Learning",
    "authors": [
      "M. Ding",
      "V. -A. Darvariu",
      "A. N. Ryabtsev",
      "N. Hawes",
      "J. C. Pickering"
    ],
    "abstract": "Atomic data determined by analysis of observed atomic spectra are essential for plasma diagnostics. For each low-ionisation open d- and f-subshell atomic species, around $10^3$ fine structure level energies can be determined through years of analysis of $10^4$ observable spectral lines. We propose the automation of this task by casting the analysis procedure as a Markov decision process and solving it by graph reinforcement learning using reward functions learned on historical human decisions. In our evaluations on existing spectral line lists and theoretical calculations for Co II and Nd II-III, hundreds of level energies were computed within hours, agreeing with published values in 95% of cases for Co II and 54-87% for Nd II-III. As the current efficiency in atomic fine structure determination struggles to meet growing atomic data demands from astronomy and fusion science, our new artificial intelligence approach sets the stage for closing this gap.",
    "categories": [
      "physics.atom-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.atom-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16184v1",
    "published_date": "2025-09-19 17:44:03 UTC",
    "updated_date": "2025-09-19 17:44:03 UTC"
  },
  {
    "arxiv_id": "2509.16179v1",
    "title": "Fast OTSU Thresholding Using Bisection Method",
    "authors": [
      "Sai Varun Kodathala"
    ],
    "abstract": "The Otsu thresholding algorithm represents a fundamental technique in image segmentation, yet its computational efficiency is severely limited by exhaustive search requirements across all possible threshold values. This work presents an optimized implementation that leverages the bisection method to exploit the unimodal characteristics of the between-class variance function. Our approach reduces the computational complexity from O(L) to O(log L) evaluations while preserving segmentation accuracy. Experimental validation on 48 standard test images demonstrates a 91.63% reduction in variance computations and 97.21% reduction in algorithmic iterations compared to conventional exhaustive search. The bisection method achieves exact threshold matches in 66.67% of test cases, with 95.83% exhibiting deviations within 5 gray levels. The algorithm maintains universal convergence within theoretical logarithmic bounds while providing deterministic performance guarantees suitable for real-time applications. This optimization addresses critical computational bottlenecks in large-scale image processing systems without compromising the theoretical foundations or segmentation quality of the original Otsu method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "math.NA"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.16179v1",
    "published_date": "2025-09-19 17:40:42 UTC",
    "updated_date": "2025-09-19 17:40:42 UTC"
  },
  {
    "arxiv_id": "2509.16299v1",
    "title": "On the Non-Uniqueness of Representation of $(U,N)$-Implications",
    "authors": [
      "Raquel Fernandez-Peralta",
      "Andrea Mesiarová-Zemánková"
    ],
    "abstract": "Fuzzy implication functions constitute fundamental operators in fuzzy logic systems, extending classical conditionals to manage uncertainty in logical inference. Among the extensive families of these operators, generalizations of the classical material implication have received considerable theoretical attention, particularly $(S,N)$-implications constructed from t-conorms and fuzzy negations, and their further generalizations to $(U,N)$-implications using disjunctive uninorms. Prior work has established characterization theorems for these families under the assumption that the fuzzy negation $N$ is continuous, ensuring uniqueness of representation. In this paper, we disprove this last fact for $(U,N)$-implications and we show that they do not necessarily possess a unique representation, even if the fuzzy negation is continuous. Further, we provide a comprehensive study of uniqueness conditions for both uninorms with continuous and non-continuous underlying functions. Our results offer important theoretical insights into the structural properties of these operators.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16299v1",
    "published_date": "2025-09-19 17:21:45 UTC",
    "updated_date": "2025-09-19 17:21:45 UTC"
  },
  {
    "arxiv_id": "2509.16163v1",
    "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks",
    "authors": [
      "Het Patel",
      "Muzammil Allie",
      "Qian Zhang",
      "Jia Chen",
      "Evangelos E. Papalexakis"
    ],
    "abstract": "Vision language models (VLMs) excel in multimodal understanding but are prone to adversarial attacks. Existing defenses often demand costly retraining or significant architecture changes. We introduce a lightweight defense using tensor decomposition suitable for any pre-trained VLM, requiring no retraining. By decomposing and reconstructing vision encoder representations, it filters adversarial noise while preserving meaning. Experiments with CLIP on COCO and Flickr30K show improved robustness. On Flickr30K, it restores 12.3\\% performance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On COCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%. Analysis shows Tensor Train decomposition with low rank (8-32) and low residual strength ($α=0.1-0.2$) is optimal. This method is a practical, plug-and-play solution with minimal overhead for existing VLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "To be presented as a poster at the Workshop on Safe and Trustworthy Multimodal AI Systems (SafeMM-AI), 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.16163v1",
    "published_date": "2025-09-19 17:16:32 UTC",
    "updated_date": "2025-09-19 17:16:32 UTC"
  },
  {
    "arxiv_id": "2509.16298v1",
    "title": "A global view of diverse construction methods of fuzzy implication functions rooted on F-chains",
    "authors": [
      "Raquel Fernandez-Peralta",
      "Juan Vicente Riera"
    ],
    "abstract": "Fuzzy implication functions are one of the most important operators used in the fuzzy logic framework. While their flexible definition allows for diverse families with distinct properties, this variety needs a deeper theoretical understanding of their structural relationships. In this work, we focus on the study of construction methods, which employ different techniques to generate new fuzzy implication functions from existing ones. Particularly, we generalize the $F$-chain-based construction, recently introduced by Mesiar et al. to extend a method for constructing aggregation functions to the context of fuzzy implication functions. Our generalization employs collections of fuzzy implication functions rather than single ones, and uses two different increasing functions instead of a unique $F$-chain. We analyze property preservation under this construction and establish sufficient conditions. Furthermore, we demonstrate that our generalized $F$-chain-based construction is a unifying framework for several existing methods. In particular, we show that various construction techniques, such as contraposition, aggregation, and generalized vertical/horizontal threshold methods, can be reformulated within our approach. This reveals structural similarities between seemingly distinct construction strategies and provides a cohesive perspective on fuzzy implication construction methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16298v1",
    "published_date": "2025-09-19 17:14:17 UTC",
    "updated_date": "2025-09-19 17:14:17 UTC"
  },
  {
    "arxiv_id": "2509.16297v1",
    "title": "How Large Language Models are Designed to Hallucinate",
    "authors": [
      "Richard Ackermann",
      "Simeon Emanuilov"
    ],
    "abstract": "Large language models (LLMs) achieve remarkable fluency across linguistic and reasoning tasks but remain systematically prone to hallucination. Prevailing accounts attribute hallucinations to data gaps, limited context, or optimization errors. We argue instead that hallucination is a structural outcome of the transformer architecture. As coherence engines, transformers are compelled to produce fluent continuations, with self-attention simulating the relational structure of meaning but lacking the existential grounding of temporality, mood, and care that stabilizes human understanding. On this basis, we distinguish ontological hallucination, arising when continuations require disclosure of beings in world, and residual reasoning hallucination, where models mimic inference by recycling traces of human reasoning in text. We illustrate these patterns through case studies aligned with Heideggerian categories and an experiment across twelve LLMs showing how simulated \"self-preservation\" emerges under extended prompts. Our contribution is threefold: (1) a comparative account showing why existing explanations are insufficient; (2) a predictive taxonomy of hallucination linked to existential structures with proposed benchmarks; and (3) design directions toward \"truth-constrained\" architectures capable of withholding or deferring when disclosure is absent. We conclude that hallucination is not an incidental defect but a defining limit of transformer-based models, an outcome scaffolding can mask but never resolve.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "23 pages, 2 tables, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.16297v1",
    "published_date": "2025-09-19 16:46:27 UTC",
    "updated_date": "2025-09-19 16:46:27 UTC"
  },
  {
    "arxiv_id": "2509.16126v1",
    "title": "Network-Based Detection of Autism Spectrum Disorder Using Sustainable and Non-invasive Salivary Biomarkers",
    "authors": [
      "Janayna M. Fernandes",
      "Robinson Sabino-Silva",
      "Murillo G. Carneiro"
    ],
    "abstract": "Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying early diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy, we developed GANet, a genetic algorithm-based network optimization framework leveraging PageRank and Degree for importance-based feature characterization. GANet systematically optimizes network structure to extract meaningful patterns from high-dimensional spectral data. It achieved superior performance compared to linear discriminant analysis, support vector machines, and deep learning models, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74 harmonic mean. These results demonstrate GANet's potential as a robust, bio-inspired, non-invasive tool for precise ASD detection and broader spectral-based health applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16126v1",
    "published_date": "2025-09-19 16:24:48 UTC",
    "updated_date": "2025-09-19 16:24:48 UTC"
  },
  {
    "arxiv_id": "2509.16117v1",
    "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
    "authors": [
      "Kaiwen Zheng",
      "Huayu Chen",
      "Haotian Ye",
      "Haoxiang Wang",
      "Qinsheng Zhang",
      "Kai Jiang",
      "Hang Su",
      "Stefano Ermon",
      "Jun Zhu",
      "Ming-Yu Liu"
    ],
    "abstract": "Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to $25\\times$ more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16117v1",
    "published_date": "2025-09-19 16:09:33 UTC",
    "updated_date": "2025-09-19 16:09:33 UTC"
  },
  {
    "arxiv_id": "2510.24720v1",
    "title": "Modelling the Interplay of Eye-Tracking Temporal Dynamics and Personality for Emotion Detection in Face-to-Face Settings",
    "authors": [
      "Meisam J. Seikavandi",
      "Jostein Fimland",
      "Fabricio Batista Narcizo",
      "Maria Barrett",
      "Ted Vucurevich",
      "Jesper Bünsow Boldt",
      "Andrew Burke Dittberner",
      "Paolo Burelli"
    ],
    "abstract": "Accurate recognition of human emotions is critical for adaptive human-computer interaction, yet remains challenging in dynamic, conversation-like settings. This work presents a personality-aware multimodal framework that integrates eye-tracking sequences, Big Five personality traits, and contextual stimulus cues to predict both perceived and felt emotions. Seventy-three participants viewed speech-containing clips from the CREMA-D dataset while providing eye-tracking signals, personality assessments, and emotion ratings. Our neural models captured temporal gaze dynamics and fused them with trait and stimulus information, yielding consistent gains over SVM and literature baselines. Results show that (i) stimulus cues strongly enhance perceived-emotion predictions (macro F1 up to 0.77), while (ii) personality traits provide the largest improvements for felt emotion recognition (macro F1 up to 0.58). These findings highlight the benefit of combining physiological, trait-level, and contextual information to address the inherent subjectivity of emotion. By distinguishing between perceived and felt responses, our approach advances multimodal affective computing and points toward more personalized and ecologically valid emotion-aware systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.24720v1",
    "published_date": "2025-09-19 16:05:23 UTC",
    "updated_date": "2025-09-19 16:05:23 UTC"
  },
  {
    "arxiv_id": "2509.16295v3",
    "title": "Patterns in the Transition From Founder-Leadership to Community Governance of Open Source",
    "authors": [
      "Mobina Noori",
      "Mahasweta Chakraborti",
      "Amy X Zhang",
      "Seth Frey"
    ],
    "abstract": "Open digital public infrastructure needs community management to ensure accountability, sustainability, and robustness. Yet open-source projects often rely on centralized decision-making, and the determinants of successful community management remain unclear. We analyze 637 GitHub repositories to trace transitions from founder-led to shared governance. Specifically, we document trajectories to community governance by extracting institutional roles, actions, and deontic cues from version-controlled project constitutions (GOVERNANCE.md). With a semantic parsing pipeline, we cluster elements into broader role and action types. We find roles and actions grow, and regulation becomes more balanced, reflecting increases in governance scope and differentiation over time. Rather than shifting tone, communities grow by layering and refining responsibilities. As transitions to community management mature, projects increasingly regulate ecosystem-level relationships and add definition to project oversight roles. Overall, this work offers a scalable pipeline for tracking the growth and development of community governance regimes from open-source software's familiar default of founder-ownership.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16295v3",
    "published_date": "2025-09-19 15:55:08 UTC",
    "updated_date": "2025-09-28 02:17:58 UTC"
  },
  {
    "arxiv_id": "2509.16093v2",
    "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses",
    "authors": [
      "Fangyi Yu",
      "Nabeel Seedat",
      "Dasha Herrmannova",
      "Frank Schilder",
      "Jonathan Richard Schwarz"
    ],
    "abstract": "Evaluating long-form answers in high-stakes domains such as law or medicine remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to capture semantic correctness, and current LLM-based evaluators often reduce nuanced aspects of answer quality into a single undifferentiated score. We introduce DeCE, a decomposed LLM evaluation framework that separates precision (factual accuracy and relevance) and recall (coverage of required concepts), using instance-specific criteria automatically extracted from gold answer requirements. DeCE is model-agnostic and domain-general, requiring no predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate different LLMs on a real-world legal QA task involving multi-jurisdictional reasoning and citation grounding. DeCE achieves substantially stronger correlation with expert judgments ($r=0.78$), compared to traditional metrics ($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist models favor recall, while specialized models favor precision. Importantly, only 11.95% of LLM-generated criteria required expert revision, underscoring DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation framework in expert domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by 2025 EMNLP industry track",
    "pdf_url": "https://arxiv.org/pdf/2509.16093v2",
    "published_date": "2025-09-19 15:36:02 UTC",
    "updated_date": "2025-10-31 18:19:46 UTC"
  },
  {
    "arxiv_id": "2509.16087v1",
    "title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model",
    "authors": [
      "Pengteng Li",
      "Pinhao Song",
      "Wuyang Li",
      "Weiyu Guo",
      "Huizai Yao",
      "Yijie Xu",
      "Dugang Liu",
      "Hui Xiong"
    ],
    "abstract": "We introduce SEE&TREK, the first training-free prompting framework tailored to enhance the spatial understanding of Multimodal Large Language Models (MLLMS) under vision-only constraints. While prior efforts have incorporated modalities like depth or point clouds to improve spatial reasoning, purely visualspatial understanding remains underexplored. SEE&TREK addresses this gap by focusing on two core principles: increasing visual diversity and motion reconstruction. For visual diversity, we conduct Maximum Semantic Richness Sampling, which employs an off-the-shell perception model to extract semantically rich keyframes that capture scene structure. For motion reconstruction, we simulate visual trajectories and encode relative spatial positions into keyframes to preserve both spatial relations and temporal coherence. Our method is training&GPU-free, requiring only a single forward pass, and can be seamlessly integrated into existing MLLM'S. Extensive experiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently boosts various MLLM S performance across diverse spatial reasoning tasks with the most +3.5% improvement, offering a promising path toward stronger spatial intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.16087v1",
    "published_date": "2025-09-19 15:30:26 UTC",
    "updated_date": "2025-09-19 15:30:26 UTC"
  },
  {
    "arxiv_id": "2509.16068v3",
    "title": "Communications to Circulations: Real-Time 3D Wind Field Prediction Using 5G GNSS Signals and Deep Learning",
    "authors": [
      "Yuchen Ye",
      "Chaoxia Yuan",
      "Mingyu Li",
      "Aoqi Zhou",
      "Hong Liang",
      "Chunqing Shang",
      "Kezuan Wang",
      "Yifeng Zheng",
      "Cong Chen"
    ],
    "abstract": "Accurate atmospheric wind field information is crucial for various applications, including weather forecasting, aviation safety, and disaster risk reduction. However, obtaining high spatiotemporal resolution wind data remains challenging due to limitations in traditional in-situ observations and remote sensing techniques, as well as the computational expense and biases of numerical weather prediction (NWP) models. This paper introduces G-WindCast, a novel deep learning framework that leverages signal strength variations from 5G Global Navigation Satellite System (GNSS) signals to forecast three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward Neural Networks (FNN) and Transformer networks to capture complex, nonlinear, and spatiotemporal relationships between GNSS-derived features and wind dynamics. Our preliminary results demonstrate promising accuracy in real-time wind forecasts (up to 30 minutes lead time). The model exhibits robustness across forecast horizons and different pressure levels, and its predictions for wind fields show superior agreement with ground-based radar wind profiler compared to concurrent European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5). Furthermore, we show that the system can maintain excellent performance for localized forecasting even with a significantly reduced number of GNSS stations (e.g., around 100), highlighting its cost-effectiveness and scalability. This interdisciplinary approach underscores the transformative potential of exploiting non-traditional data sources and deep learning for advanced environmental monitoring and real-time atmospheric applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 10 figures; Minor text revisions; Updated the questions, some images in the article, the abstract, and the main text content",
    "pdf_url": "https://arxiv.org/pdf/2509.16068v3",
    "published_date": "2025-09-19 15:17:08 UTC",
    "updated_date": "2025-10-20 11:46:22 UTC"
  },
  {
    "arxiv_id": "2509.16293v4",
    "title": "Robust LLM Training Infrastructure at ByteDance",
    "authors": [
      "Borui Wan",
      "Gaohong Liu",
      "Zuquan Song",
      "Jun Wang",
      "Yun Zhang",
      "Guangming Sheng",
      "Shuguang Wang",
      "Houmin Wei",
      "Chenyuan Wang",
      "Weiqiang Lou",
      "Xi Yang",
      "Mofan Zhang",
      "Kaihua Jiang",
      "Cheng Ren",
      "Xiaoyun Zhi",
      "Menghan Yu",
      "Zhe Nan",
      "Zhuolin Zheng",
      "Baoquan Zhong",
      "Qinlong Wang",
      "Huan Yu",
      "Jinxin Chi",
      "Wang Zhang",
      "Yuhan Li",
      "Zixian Du",
      "Sida Zhao",
      "Yongqiang Zhang",
      "Jingzhe Tang",
      "Zherui Liu",
      "Chuan Wu",
      "Yanghua Peng",
      "Haibin Lin",
      "Wencong Xiao",
      "Xin Liu",
      "Liang Xiang"
    ],
    "abstract": "The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform and achieves 97% ETTR for a three-month training job on 9,600 GPUs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16293v4",
    "published_date": "2025-09-19 15:08:33 UTC",
    "updated_date": "2025-10-20 09:35:27 UTC"
  },
  {
    "arxiv_id": "2509.16058v1",
    "title": "Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers",
    "authors": [
      "Krati Saxena",
      "Federico Jurado Ruiz",
      "Guido Manzi",
      "Dianbo Liu",
      "Alex Lamb"
    ],
    "abstract": "Attention mechanisms have become integral in AI, significantly enhancing model performance and scalability by drawing inspiration from human cognition. Concurrently, the Attention Schema Theory (AST) in cognitive science posits that individuals manage their attention by creating a model of the attention itself, effectively allocating cognitive resources. Inspired by AST, we introduce ASAC (Attention Schema-based Attention Control), which integrates the attention schema concept into artificial neural networks. Our initial experiments focused on embedding the ASAC module within transformer architectures. This module employs a Vector-Quantized Variational AutoEncoder (VQVAE) as both an attention abstractor and controller, facilitating precise attention management. By explicitly modeling attention allocation, our approach aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both the vision and NLP domains, highlighting its ability to improve classification accuracy and expedite the learning process. Our experiments with vision transformers across various datasets illustrate that the attention controller not only boosts classification accuracy but also accelerates learning. Furthermore, we have demonstrated the model's robustness and generalization capabilities across noisy and out-of-distribution datasets. In addition, we have showcased improved performance in multi-task settings. Quick experiments reveal that the attention schema-based module enhances resilience to adversarial attacks, optimizes attention to improve learning efficiency, and facilitates effective transfer learning and learning from fewer examples. These promising results establish a connection between cognitive science and machine learning, shedding light on the efficient utilization of attention mechanisms in AI systems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16058v1",
    "published_date": "2025-09-19 15:08:30 UTC",
    "updated_date": "2025-09-19 15:08:30 UTC"
  },
  {
    "arxiv_id": "2509.16053v1",
    "title": "Compose by Focus: Scene Graph-based Atomic Skills",
    "authors": [
      "Han Qi",
      "Changhe Chen",
      "Heng Yang"
    ],
    "abstract": "A key requirement for generalist robots is compositional generalization - the ability to combine atomic skills to solve complex, long-horizon tasks. While prior work has primarily focused on synthesizing a planner that sequences pre-learned skills, robust execution of the individual skills themselves remains challenging, as visuomotor policies often fail under distribution shifts induced by scene composition. To address this, we introduce a scene graph-based representation that focuses on task-relevant objects and relations, thereby mitigating sensitivity to irrelevant variation. Building on this idea, we develop a scene-graph skill learning framework that integrates graph neural networks with diffusion-based imitation learning, and further combine \"focused\" scene-graph skills with a vision-language model (VLM) based task planner. Experiments in both simulation and real-world manipulation tasks demonstrate substantially higher success rates than state-of-the-art baselines, highlighting improved robustness and compositional generalization in long-horizon tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16053v1",
    "published_date": "2025-09-19 15:03:18 UTC",
    "updated_date": "2025-09-19 15:03:18 UTC"
  },
  {
    "arxiv_id": "2509.16028v1",
    "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech",
    "authors": [
      "Sang Hoon Woo",
      "Sehun Lee",
      "Kang-wook Kim",
      "Gunhee Kim"
    ],
    "abstract": "Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose Think-Verbalize-Speak, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is verbalizing, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce ReVerT, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at https://yhytoto12.github.io/TVS-ReVerT",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT",
    "pdf_url": "https://arxiv.org/pdf/2509.16028v1",
    "published_date": "2025-09-19 14:34:22 UTC",
    "updated_date": "2025-09-19 14:34:22 UTC"
  },
  {
    "arxiv_id": "2509.16025v1",
    "title": "Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning",
    "authors": [
      "Hong-Yun Lin",
      "Jhen-Ke Lin",
      "Chung-Chun Wang",
      "Hao-Chien Lu",
      "Berlin Chen"
    ],
    "abstract": "Spoken Language Assessment (SLA) estimates a learner's oral proficiency from spontaneous speech. The growing population of L2 English speakers has intensified the demand for reliable SLA, a critical component of Computer Assisted Language Learning (CALL). Existing efforts often rely on cascaded pipelines, which are prone to error propagation, or end-to-end models that often operate on a short audio window, which might miss discourse-level evidence. This paper introduces a novel multimodal foundation model approach that performs session-level evaluation in a single pass. Our approach couples multi-target learning with a frozen, Whisper ASR model-based speech prior for acoustic-aware calibration, allowing for jointly learning holistic and trait-level objectives of SLA without resorting to handcrafted features. By coherently processing the entire response session of an L2 speaker, the model excels at predicting holistic oral proficiency. Experiments conducted on the Speak & Improve benchmark demonstrate that our proposed approach outperforms the previous state-of-the-art cascaded system and exhibits robust cross-part generalization, producing a compact deployable grader that is tailored for CALL applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
    "pdf_url": "https://arxiv.org/pdf/2509.16025v1",
    "published_date": "2025-09-19 14:33:05 UTC",
    "updated_date": "2025-09-19 14:33:05 UTC"
  },
  {
    "arxiv_id": "2509.16020v1",
    "title": "AI Methods for Permutation Circuit Synthesis Across Generic Topologies",
    "authors": [
      "Victor Villar",
      "Juan Cruz-Benito",
      "Ismael Faro",
      "David Kremer"
    ],
    "abstract": "This paper investigates artificial intelligence (AI) methodologies for the synthesis and transpilation of permutation circuits across generic topologies. Our approach uses Reinforcement Learning (RL) techniques to achieve near-optimal synthesis of permutation circuits up to 25 qubits. Rather than developing specialized models for individual topologies, we train a foundational model on a generic rectangular lattice, and employ masking mechanisms to dynamically select subsets of topologies during the synthesis. This enables the synthesis of permutation circuits on any topology that can be embedded within the rectangular lattice, without the need to re-train the model. In this paper we show results for 5x5 lattice and compare them to previous AI topology-oriented models and classical methods, showing that they outperform classical heuristics, and match previous specialized AI models, and performs synthesis even for topologies that were not seen during training. We further show that the model can be fine tuned to strengthen the performance for selected topologies of interest. This methodology allows a single trained model to efficiently synthesize circuits across diverse topologies, allowing its practical integration into transpilation workflows.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "This paper has been accepted by First AAAI Symposium on Quantum Information & Machine Learning (QIML): Bridging Quantum Computing and Artificial Intelligence at AAAI 2025 Fall Symposium",
    "pdf_url": "https://arxiv.org/pdf/2509.16020v1",
    "published_date": "2025-09-19 14:28:22 UTC",
    "updated_date": "2025-09-19 14:28:22 UTC"
  },
  {
    "arxiv_id": "2509.16010v1",
    "title": "Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation",
    "authors": [
      "Qi Wang",
      "Shituo Ma",
      "Guoxin Yu",
      "Hanyang Peng",
      "Yue Yu"
    ],
    "abstract": "Voice cloning for Text-to-Speech (TTS) aims to generate expressive and personalized speech from text using limited data from a target speaker. Federated Learning (FL) offers a collaborative and privacy-preserving framework for this task, but existing approaches suffer from high communication costs and tend to suppress stylistic heterogeneity, resulting in insufficient personalization. To address these issues, we propose Fed-PISA, which stands for Federated Personalized Identity-Style Adaptation. To minimize communication costs, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism: the speaker's timbre is retained locally through a private ID-LoRA, while only a lightweight style-LoRA is transmitted to the server, thereby minimizing parameter exchange. To harness heterogeneity, our aggregation method, inspired by collaborative filtering, is introduced to create custom models for each client by learning from stylistically similar peers. Experiments show that Fed-PISA improves style expressivity, naturalness, and speaker similarity, outperforming standard federated baselines with minimal communication costs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16010v1",
    "published_date": "2025-09-19 14:24:45 UTC",
    "updated_date": "2025-09-19 14:24:45 UTC"
  },
  {
    "arxiv_id": "2509.18193v1",
    "title": "TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection",
    "authors": [
      "Omar H. Khater",
      "Abdul Jabbar Siddiqui",
      "Aiman El-Maleh",
      "M. Shamim Hossain"
    ],
    "abstract": "Deploying deep learning models in agriculture is difficult because edge devices have limited resources, but this work presents a compressed version of EcoWeedNet using structured channel pruning, quantization-aware training (QAT), and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the challenges of pruning complex architectures with residual shortcuts, attention mechanisms, concatenations, and CSP blocks, the model size was reduced by up to 68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n (with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9% mAP50, proving it to be both efficient and effective for precision agriculture.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.18193v1",
    "published_date": "2025-09-19 13:57:12 UTC",
    "updated_date": "2025-09-19 13:57:12 UTC"
  },
  {
    "arxiv_id": "2509.15987v2",
    "title": "Towards Sharper Object Boundaries in Self-Supervised Depth Estimation",
    "authors": [
      "Aurélien Cecille",
      "Stefan Duffner",
      "Franck Davoine",
      "Rémi Agier",
      "Thibault Neveu"
    ],
    "abstract": "Accurate monocular depth estimation is crucial for 3D scene understanding, but existing methods often blur depth at object boundaries, introducing spurious intermediate 3D points. While achieving sharp edges usually requires very fine-grained supervision, our method produces crisp depth discontinuities using only self-supervision. Specifically, we model per-pixel depth as a mixture distribution, capturing multiple plausible depths and shifting uncertainty from direct regression to the mixture weights. This formulation integrates seamlessly into existing pipelines via variance-aware loss functions and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show that our method achieves up to 35% higher boundary sharpness and improves point cloud quality compared to state-of-the-art baselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "BMVC 2025 Oral, 10 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.15987v2",
    "published_date": "2025-09-19 13:53:51 UTC",
    "updated_date": "2025-11-17 20:04:31 UTC"
  },
  {
    "arxiv_id": "2509.15986v1",
    "title": "EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions",
    "authors": [
      "Xinchen Wan",
      "Jinhua Liang",
      "Huan Zhang"
    ],
    "abstract": "Existing digital mental wellness tools often overlook the nuanced emotional states underlying everyday challenges. For example, pre-sleep anxiety affects more than 1.5 billion people worldwide, yet current approaches remain largely static and \"one-size-fits-all\", failing to adapt to individual needs. In this work, we present EmoHeal, an end-to-end system that delivers personalized, three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical parameters via a knowledge graph grounded in music therapy principles (GEMS, iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to guide users from their current state toward a calmer one (\"match-guide-target\"). A within-subjects study (N=40) demonstrated significant supportive effects, with participants reporting substantial mood improvement (M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05, p<0.001). A strong correlation between perceived accuracy and therapeutic outcome (r=0.72, p<0.001) validates our fine-grained approach. These findings establish the viability of theory-driven, emotion-aware digital wellness tools and provides a scalable AI blueprint for operationalizing music therapy principles.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 5 figures. Submitted to the 2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2026)",
    "pdf_url": "https://arxiv.org/pdf/2509.15986v1",
    "published_date": "2025-09-19 13:52:22 UTC",
    "updated_date": "2025-09-19 13:52:22 UTC"
  },
  {
    "arxiv_id": "2509.15981v2",
    "title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations",
    "authors": [
      "Yujie Zhu",
      "Charles A. Hepburn",
      "Matthew Thorpe",
      "Giovanni Montana"
    ],
    "abstract": "In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15981v2",
    "published_date": "2025-09-19 13:47:20 UTC",
    "updated_date": "2025-10-31 11:00:36 UTC"
  },
  {
    "arxiv_id": "2509.15980v1",
    "title": "Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation",
    "authors": [
      "Lorenzo Cirillo",
      "Claudio Schiavella",
      "Lorenzo Papa",
      "Paolo Russo",
      "Irene Amerini"
    ],
    "abstract": "Explainable artificial intelligence is increasingly employed to understand the decision-making process of deep learning models and create trustworthiness in their adoption. However, the explainability of Monocular Depth Estimation (MDE) remains largely unexplored despite its wide deployment in real-world applications. In this work, we study how to analyze MDE networks to map the input image to the predicted depth map. More in detail, we investigate well-established feature attribution methods, Saliency Maps, Integrated Gradients, and Attention Rollout on different computationally complex models for MDE: METER, a lightweight network, and PixelFormer, a deep network. We assess the quality of the generated visual explanations by selectively perturbing the most relevant and irrelevant pixels, as identified by the explainability methods, and analyzing the impact of these perturbations on the model's output. Moreover, since existing evaluation metrics can have some limitations in measuring the validity of visual explanations for MDE, we additionally introduce the Attribution Fidelity. This metric evaluates the reliability of the feature attribution by assessing their consistency with the predicted depth map. Experimental results demonstrate that Saliency Maps and Integrated Gradients have good performance in highlighting the most important input features for MDE lightweight and deep models, respectively. Furthermore, we show that Attribution Fidelity effectively identifies whether an explainability method fails to produce reliable visual maps, even in scenarios where conventional metrics might suggest satisfactory results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures, 2 tables. This paper has been accepted at the International Joint Conference on Neural Networks (IJCNN) 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.15980v1",
    "published_date": "2025-09-19 13:45:18 UTC",
    "updated_date": "2025-09-19 13:45:18 UTC"
  },
  {
    "arxiv_id": "2509.15974v1",
    "title": "BEFT: Bias-Efficient Fine-Tuning of Language Models",
    "authors": [
      "Baichuan Huang",
      "Ananth Balashankar",
      "Amir Aminifar"
    ],
    "abstract": "Fine-tuning all-bias-terms stands out among various parameter-efficient fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and competitive performance, especially in low-data regimes. Bias-only fine-tuning has the potential for unprecedented parameter efficiency. However, the link between fine-tuning different bias terms (i.e., bias terms in the query, key, or value projections) and downstream performance remains unclear. The existing approaches, e.g., based on the magnitude of bias change or empirical Fisher information, provide limited guidance for selecting the particular bias term for effective fine-tuning. In this paper, we propose an approach for selecting the bias term to be fine-tuned, forming the foundation of our bias-efficient fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against other bias-selection approaches, across a wide range of large language models (LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B parameters. Our results demonstrate the effectiveness and superiority of our bias-efficient approach on diverse downstream tasks, including classification, multiple-choice, and generation tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15974v1",
    "published_date": "2025-09-19 13:35:07 UTC",
    "updated_date": "2025-09-19 13:35:07 UTC"
  },
  {
    "arxiv_id": "2509.15965v2",
    "title": "RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation",
    "authors": [
      "Chao Yu",
      "Yuanqing Wang",
      "Zhen Guo",
      "Hao Lin",
      "Si Xu",
      "Hongzhi Zang",
      "Quanlu Zhang",
      "Yongji Wu",
      "Chunyang Zhu",
      "Junhao Hu",
      "Zixiao Huang",
      "Mingjie Wei",
      "Yuqing Xie",
      "Ke Yang",
      "Bo Dai",
      "Zhexuan Xu",
      "Jiakun Du",
      "Xiangyuan Wang",
      "Xu Fu",
      "Letong Shi",
      "Zhihao Liu",
      "Kang Chen",
      "Weilin Liu",
      "Gang Liu",
      "Boxun Li",
      "Jianlei Yang",
      "Zhi Yang",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker's adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving $1.07\\times-2.43\\times$ speedup in end-to-end training throughput.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "GitHub Repo: https://github.com/RLinf/RLinf",
    "pdf_url": "https://arxiv.org/pdf/2509.15965v2",
    "published_date": "2025-09-19 13:24:17 UTC",
    "updated_date": "2025-12-29 14:13:21 UTC"
  },
  {
    "arxiv_id": "2509.15964v1",
    "title": "MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework",
    "authors": [
      "Tianyu Li",
      "Yan Xin",
      "Jianzhong",
      "Zhang"
    ],
    "abstract": "Reliable channel estimation (CE) is fundamental for robust communication in dynamic wireless environments, where models must generalize across varying conditions such as signal-to-noise ratios (SNRs), the number of resource blocks (RBs), and channel profiles. Traditional deep learning (DL)-based methods struggle to generalize effectively across such diverse settings, particularly under multitask and zero-shot scenarios. In this work, we propose MoE-CE, a flexible mixture-of-experts (MoE) framework designed to enhance the generalization capability of DL-based CE methods. MoE-CE provides an appropriate inductive bias by leveraging multiple expert subnetworks, each specialized in distinct channel characteristics, and a learned router that dynamically selects the most relevant experts per input. This architecture enhances model capacity and adaptability without a proportional rise in computational cost while being agnostic to the choice of the backbone model and the learning algorithm. Through extensive experiments on synthetic datasets generated under diverse SNRs, RB numbers, and channel profiles, including multitask and zero-shot evaluations, we demonstrate that MoE-CE consistently outperforms conventional DL approaches, achieving significant performance gains while maintaining efficiency.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15964v1",
    "published_date": "2025-09-19 13:23:08 UTC",
    "updated_date": "2025-09-19 13:23:08 UTC"
  },
  {
    "arxiv_id": "2509.15962v1",
    "title": "Structured Information for Improving Spatial Relationships in Text-to-Image Generation",
    "authors": [
      "Sander Schildermans",
      "Chang Tian",
      "Ying Jiao",
      "Marie-Francine Moens"
    ],
    "abstract": "Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing spatial relationships described in natural language prompts remains a major challenge. Prior efforts have addressed this issue through prompt optimization, spatially grounded generation, and semantic refinement. This work introduces a lightweight approach that augments prompts with tuple-based structured information, using a fine-tuned language model for automatic conversion and seamless integration into T2I pipelines. Experimental results demonstrate substantial improvements in spatial accuracy, without compromising overall image quality as measured by Inception Score. Furthermore, the automatically generated tuples exhibit quality comparable to human-crafted tuples. This structured information provides a practical and portable solution to enhance spatial relationships in T2I generation, addressing a key limitation of current large-scale generative systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "text-to-image generation, structured information, spatial relationship",
    "pdf_url": "https://arxiv.org/pdf/2509.15962v1",
    "published_date": "2025-09-19 13:20:34 UTC",
    "updated_date": "2025-09-19 13:20:34 UTC"
  },
  {
    "arxiv_id": "2509.15959v1",
    "title": "Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration",
    "authors": [
      "Zhuoyue Zhang",
      "Haitong Xu"
    ],
    "abstract": "Autonomous navigation in maritime domains is accelerating alongside advances in artificial intelligence, sensing, and connectivity. Opaque decision-making and poorly calibrated human-automation interaction remain key barriers to safe adoption. This article synthesizes 100 studies on automation transparency for Maritime Autonomous Surface Ships (MASS) spanning situation awareness (SA), human factors, interface design, and regulation. We (i) map the Guidance-Navigation-Control stack to shore-based operational modes -- remote supervision (RSM) and remote control (RCM) -- and identify where human unsafe control actions (Human-UCAs) concentrate in handover and emergency loops; (ii) summarize evidence that transparency features (decision rationales, alternatives, confidence/uncertainty, and rule-compliance indicators) improve understanding and support trust calibration, though reliability and predictability often dominate trust; (iii) distill design strategies for transparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI presentation (textual/graphical overlays, color coding, conversational and immersive UIs), and engineer-facing processes (resilient interaction design, validation, and standardization). We integrate methods for Human-UCA identification (STPA-Cog + IDAC), quantitative trust/SA assessment, and operator workload monitoring, and outline regulatory and rule-based implications including COLREGs formalization and route exchange. We conclude with an adaptive transparency framework that couples operator state estimation with explainable decision support to reduce cognitive overload and improve takeover timeliness. The review highlights actionable figure-of-merit displays (e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs (rule traceability, confidence), and training pipelines (HIL/MIL, simulation) as near-term levers for safer MASS operations.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15959v1",
    "published_date": "2025-09-19 13:18:54 UTC",
    "updated_date": "2025-09-19 13:18:54 UTC"
  },
  {
    "arxiv_id": "2509.15957v1",
    "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol",
    "authors": [
      "Kanato Masayoshi",
      "Masahiro Hashimoto",
      "Ryoichi Yokoyama",
      "Naoki Toda",
      "Yoshifumi Uwamino",
      "Shogo Fukuda",
      "Ho Namkoong",
      "Masahiro Jinzaki"
    ],
    "abstract": "Background: Large language models (LLMs) show promise in medicine, but their deployment in hospitals is limited by restricted access to electronic health record (EHR) systems. The Model Context Protocol (MCP) enables integration between LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP can autonomously retrieve clinically relevant information in a real hospital setting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct agent to interact with it. Six tasks were tested, derived from use cases of the infection control team (ICT). Eight patients discussed at ICT conferences were retrospectively analyzed. Agreement with physician-generated gold standards was measured.\n  Results: The LLM consistently selected and executed the correct MCP tools. Except for two tasks, all tasks achieved near-perfect accuracy. Performance was lower in the complex task requiring time-dependent calculations. Most errors arose from incorrect arguments or misinterpretation of tool results. Responses from EHR-MCP were reliable, though long and repetitive data risked exceeding the context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a real hospital setting, achieving near-perfect performance in simple tasks while highlighting challenges in complex ones. EHR-MCP provides an infrastructure for secure, consistent data access and may serve as a foundation for hospital AI agents. Future work should extend beyond retrieval to reasoning, generation, and clinical impact assessment, paving the way for effective integration of generative AI into clinical practice.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15957v1",
    "published_date": "2025-09-19 13:17:16 UTC",
    "updated_date": "2025-09-19 13:17:16 UTC"
  },
  {
    "arxiv_id": "2509.15952v2",
    "title": "Compose Yourself: Average-Velocity Flow Matching for One-Step Speech Enhancement",
    "authors": [
      "Gang Yang",
      "Yue Lei",
      "Wenxin Tai",
      "Jin Wu",
      "Jia Chen",
      "Ting Zhong",
      "Fan Zhou"
    ],
    "abstract": "Diffusion and flow matching (FM) models have achieved remarkable progress in speech enhancement (SE), yet their dependence on multi-step generation is computationally expensive and vulnerable to discretization errors. Recent advances in one-step generative modeling, particularly MeanFlow, provide a promising alternative by reformulating dynamics through average velocity fields. In this work, we present COSE, a one-step FM framework tailored for SE. To address the high training overhead of Jacobian-vector product (JVP) computations in MeanFlow, we introduce a velocity composition identity to compute average velocity efficiently, eliminating expensive computation while preserving theoretical consistency and achieving competitive enhancement quality. Extensive experiments on standard benchmarks show that COSE delivers up to 5x faster sampling and reduces training cost by 40%, all without compromising speech quality. Code is available at https://github.com/ICDM-UESTC/COSE.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 2 figures, submitted to ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.15952v2",
    "published_date": "2025-09-19 13:07:39 UTC",
    "updated_date": "2025-09-22 13:01:44 UTC"
  },
  {
    "arxiv_id": "2509.15942v1",
    "title": "ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching",
    "authors": [
      "Graham Clyne",
      "Guillaume Couairon",
      "Guillaume Gastineau",
      "Claire Monteleoni",
      "Anastase Charantonis"
    ],
    "abstract": "Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations.",
    "categories": [
      "physics.ao-ph",
      "cs.AI"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15942v1",
    "published_date": "2025-09-19 12:53:24 UTC",
    "updated_date": "2025-09-19 12:53:24 UTC"
  },
  {
    "arxiv_id": "2509.15937v1",
    "title": "A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning",
    "authors": [
      "Shaopeng Zhai",
      "Qi Zhang",
      "Tianyi Zhang",
      "Fuxian Huang",
      "Haoran Zhang",
      "Ming Zhou",
      "Shengzhe Zhang",
      "Litao Liu",
      "Sixu Lin",
      "Jiangmiao Pang"
    ],
    "abstract": "Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30\\% to about 90\\% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50% improvement in sample efficiency and achieves up to 100% final success.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "26 pages,10 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.15937v1",
    "published_date": "2025-09-19 12:44:29 UTC",
    "updated_date": "2025-09-19 12:44:29 UTC"
  },
  {
    "arxiv_id": "2509.15932v1",
    "title": "The Alignment Bottleneck",
    "authors": [
      "Wenjun Cao"
    ],
    "abstract": "Large language models improve with scale, yet feedback-based alignment still exhibits systematic deviations from intended behavior. Motivated by bounded rationality in economics and cognitive science, we view judgment as resource-limited and feedback as a constrained channel. On this basis, we model the loop as a two-stage cascade $U \\to H \\to Y$ given $S$, with cognitive capacity $C_{\\text{cog}|S}$ and average total capacity $\\bar{C}_{\\text{tot}|S}$. Our main result is a capacity-coupled Alignment Performance Interval. It pairs a data size-independent Fano lower bound proved on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is controlled by the same channel via $m \\, \\bar{C}_{\\text{tot}|S}$. The PAC-Bayes bound becomes an upper bound on the same true risk when the canonical observable loss is used and the dataset is drawn from the same mixture. Under these matched conditions, both limits are governed by a single capacity. Consequences include that, with value complexity and capacity fixed, adding labels alone cannot cross the bound; attaining lower risk on more complex targets requires capacity that grows with $\\log M$; and once useful signal saturates capacity, further optimization tends to fit channel regularities, consistent with reports of sycophancy and reward hacking. The analysis views alignment as interface engineering: measure and allocate limited capacity, manage task complexity, and decide where information is spent.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15932v1",
    "published_date": "2025-09-19 12:38:30 UTC",
    "updated_date": "2025-09-19 12:38:30 UTC"
  },
  {
    "arxiv_id": "2509.15927v3",
    "title": "Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search",
    "authors": [
      "Zhiyu Mou",
      "Yiqin Lv",
      "Miao Xu",
      "Qi Wang",
      "Yixiu Mao",
      "Qichen Ye",
      "Chao Li",
      "Rongquan Bai",
      "Chuan Yu",
      "Jian Xu",
      "Bo Zheng"
    ],
    "abstract": "Auto-bidding serves as a critical tool for advertisers to improve their advertising performance. Recent progress has demonstrated that AI-Generated Bidding (AIGB), which learns a conditional generative planner from offline data, achieves superior performance compared to typical offline reinforcement learning (RL)-based auto-bidding methods. However, existing AIGB methods still face a performance bottleneck due to their inherent inability to explore beyond the static offline dataset. To address this, we propose {AIGB-Pearl} (\\emph{{P}lanning with {E}valu{A}tor via RL}), a novel method that integrates generative planning and policy optimization. The core of AIGB-Pearl lies in constructing a trajectory evaluator for scoring generation quality and designing a provably sound KL-Lipschitz-constrained score maximization scheme to ensure safe and efficient exploration beyond the offline dataset. A practical algorithm incorporating the synchronous coupling technique is further devised to ensure the model regularity required by the proposed scheme. Extensive experiments on both simulated and real-world advertising systems demonstrate the state-of-the-art performance of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15927v3",
    "published_date": "2025-09-19 12:30:26 UTC",
    "updated_date": "2025-10-08 14:06:32 UTC"
  },
  {
    "arxiv_id": "2509.15915v1",
    "title": "Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds",
    "authors": [
      "Remo Sasso",
      "Michelangelo Conserva",
      "Dominik Jeurissen",
      "Paulo Rauber"
    ],
    "abstract": "While reinforcement learning from scratch has shown impressive results in solving sequential decision-making tasks with efficient simulators, real-world applications with expensive interactions require more sample-efficient agents. Foundation models (FMs) are natural candidates to improve sample efficiency as they possess broad knowledge and reasoning capabilities, but it is yet unclear how to effectively integrate them into the reinforcement learning framework. In this paper, we anticipate and, most importantly, evaluate two promising strategies. First, we consider the use of foundation world models (FWMs) that exploit the prior knowledge of FMs to enable training and evaluating agents with simulated interactions. Second, we consider the use of foundation agents (FAs) that exploit the reasoning capabilities of FMs for decision-making. We evaluate both approaches empirically in a family of grid-world environments that are suitable for the current generation of large language models (LLMs). Our results suggest that improvements in LLMs already translate into better FWMs and FAs; that FAs based on current LLMs can already provide excellent policies for sufficiently simple environments; and that the coupling of FWMs and reinforcement learning agents is highly promising for more complex settings with partial observability and stochastic elements.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 9 figures. Accepted for presentation at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Embodied World Models for Decision Making",
    "pdf_url": "https://arxiv.org/pdf/2509.15915v1",
    "published_date": "2025-09-19 12:10:28 UTC",
    "updated_date": "2025-09-19 12:10:28 UTC"
  },
  {
    "arxiv_id": "2509.15908v2",
    "title": "Interpretable Nanoporous Materials Design with Symmetry-Aware Networks",
    "authors": [
      "Zhenhao Zhou",
      "Salman Bin Kashif",
      "Jin-Hu Dou",
      "Chris Wolverton",
      "Kaihang Shi",
      "Tao Deng",
      "Zhenpeng Yao"
    ],
    "abstract": "Nanoporous materials hold promise for diverse sustainable applications, yet their vast chemical space poses challenges for efficient design. Machine learning offers a compelling pathway to accelerate the exploration, but existing models lack either interpretability or fidelity for elucidating the correlation between crystal geometry and property. Here, we report a three-dimensional periodic space sampling method that decomposes large nanoporous structures into local geometrical sites for combined property prediction and site-wise contribution quantification. Trained with a constructed database and retrieved datasets, our model achieves state-of-the-art accuracy and data efficiency for property prediction on gas storage, separation, and electrical conduction. Meanwhile, this approach enables the interpretation of the prediction and allows for accurate identification of significant local sites for targeted properties. Through identifying transferable high-performance sites across diverse nanoporous frameworks, our model paves the way for interpretable, symmetry-aware nanoporous materials design, which is extensible to other materials, like molecular crystals and beyond.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15908v2",
    "published_date": "2025-09-19 12:04:26 UTC",
    "updated_date": "2025-09-23 10:48:03 UTC"
  },
  {
    "arxiv_id": "2509.15901v2",
    "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions",
    "authors": [
      "Frederic Kirstein",
      "Sonu Kumar",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "abstract": "Meeting summarization with large language models (LLMs) remains error-prone, often producing outputs with hallucinations, omissions, and irrelevancies. We present FRAME, a modular pipeline that reframes summarization as a semantic enrichment task. FRAME extracts and scores salient facts, organizes them thematically, and uses these to enrich an outline into an abstractive summary. To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that has the model build a reasoning trace by answering nine questions before content selection. For evaluation, we propose P-MESA, a multi-dimensional, reference-free evaluation framework to assess if a summary fits a target reader. P-MESA reliably identifies error instances, achieving >= 89% balanced accuracy against human annotations and strongly aligns with human severity ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and omission by 2 out of 5 points (measured with MESA), while SCOPE improves knowledge fit and goal alignment over prompt-only baselines. Our findings advocate for rethinking summarization to improve control, faithfulness, and personalization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.15901v2",
    "published_date": "2025-09-19 11:58:17 UTC",
    "updated_date": "2025-11-14 12:22:32 UTC"
  },
  {
    "arxiv_id": "2509.15895v1",
    "title": "From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and AI Methods for Childhood Leukemia Prediction",
    "authors": [
      "Henning Höfener",
      "Farina Kock",
      "Martina Pontones",
      "Tabita Ghete",
      "David Pfrang",
      "Nicholas Dickel",
      "Meik Kunz",
      "Daniela P. Schacherer",
      "David A. Clunie",
      "Andrey Fedorov",
      "Max Westphal",
      "Markus Metzler"
    ],
    "abstract": "Leukemia diagnosis primarily relies on manual microscopic analysis of bone marrow morphology supported by additional laboratory parameters, making it complex and time consuming. While artificial intelligence (AI) solutions have been proposed, most utilize private datasets and only cover parts of the diagnostic pipeline. Therefore, we present a large, high-quality, publicly available leukemia bone marrow dataset spanning the entire diagnostic process, from cell detection to diagnosis. Using this dataset, we further propose methods for cell detection, cell classification, and diagnosis prediction. The dataset comprises 246 pediatric patients with diagnostic, clinical and laboratory information, over 40 000 cells with bounding box annotations and more than 28 000 of these with high-quality class labels, making it the most comprehensive dataset publicly available. Evaluation of the AI models yielded an average precision of 0.96 for the cell detection, an area under the curve of 0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean F1-score of 0.90 for the diagnosis prediction using predicted cell counts. While the proposed approaches demonstrate their usefulness for AI-assisted diagnostics, the dataset will foster further research and development in the field, ultimately contributing to more precise diagnoses and improved patient outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15895v1",
    "published_date": "2025-09-19 11:48:48 UTC",
    "updated_date": "2025-09-19 11:48:48 UTC"
  },
  {
    "arxiv_id": "2509.15892v1",
    "title": "MoAngelo: Motion-Aware Neural Surface Reconstruction for Dynamic Scenes",
    "authors": [
      "Mohamed Ebbed",
      "Zorah Lähner"
    ],
    "abstract": "Dynamic scene reconstruction from multi-view videos remains a fundamental challenge in computer vision. While recent neural surface reconstruction methods have achieved remarkable results in static 3D reconstruction, extending these approaches with comparable quality for dynamic scenes introduces significant computational and representational challenges. Existing dynamic methods focus on novel-view synthesis, therefore, their extracted meshes tend to be noisy. Even approaches aiming for geometric fidelity often result in too smooth meshes due to the ill-posedness of the problem. We present a novel framework for highly detailed dynamic reconstruction that extends the static 3D reconstruction method NeuralAngelo to work in dynamic settings. To that end, we start with a high-quality template scene reconstruction from the initial frame using NeuralAngelo, and then jointly optimize deformation fields that track the template and refine it based on the temporal sequence. This flexible template allows updating the geometry to include changes that cannot be modeled with the deformation field, for instance occluded parts or the changes in the topology. We show superior reconstruction accuracy in comparison to previous state-of-the-art methods on the ActorsHQ dataset.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15892v1",
    "published_date": "2025-09-19 11:43:01 UTC",
    "updated_date": "2025-09-19 11:43:01 UTC"
  },
  {
    "arxiv_id": "2509.15888v3",
    "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation",
    "authors": [
      "Senkang Hu",
      "Xudong Han",
      "Jinqi Jiang",
      "Yihang Tao",
      "Zihan Fang",
      "Yong Dai",
      "Sam Tak Wu Kwong",
      "Yuguang Fang"
    ],
    "abstract": "Adapting billion-parameter language models to a downstream task is still costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task adaptation as output-distribution alignment: the objective is to steer the output distribution toward the task distribution directly during decoding rather than indirectly through weight updates. Building on this view, we introduce Steering Vector Decoding (SVDecode), a lightweight, PEFT-compatible, and theoretically grounded method. We start with a short warm-start fine-tune and extract a task-aware steering vector from the Kullback-Leibler (KL) divergence gradient between the output distribution of the warm-started and pre-trained models. This steering vector is then used to guide the decoding process to steer the model's output distribution towards the task distribution. We theoretically prove that SVDecode is first-order equivalent to the gradient step of full fine-tuning and derive a globally optimal solution for the strength of the steering vector. Across three tasks and nine benchmarks, SVDecode paired with four standard PEFT methods improves multiple-choice accuracy by up to 5 percentage points and open-ended truthfulness by 2 percentage points, with similar gains (1-2 percentage points) on commonsense datasets without adding trainable parameters beyond the PEFT adapter. SVDecode thus offers a lightweight, theoretically grounded path to stronger task adaptation for large language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS'25",
    "pdf_url": "https://arxiv.org/pdf/2509.15888v3",
    "published_date": "2025-09-19 11:35:56 UTC",
    "updated_date": "2025-10-12 09:59:05 UTC"
  },
  {
    "arxiv_id": "2509.15883v1",
    "title": "RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning",
    "authors": [
      "Xiaosheng Long",
      "Hanyu Wang",
      "Zhentao Song",
      "Kun Luo",
      "Hongde Liu"
    ],
    "abstract": "Recent retrieval-augmented image captioning methods incorporate external knowledge to compensate for the limitations in comprehending complex scenes. However, current approaches face challenges in relation modeling: (1) the representation of semantic prompts is too coarse-grained to capture fine-grained relationships; (2) these methods lack explicit modeling of image objects and their semantic relationships. To address these limitations, we propose RACap, a relation-aware retrieval-augmented model for image captioning, which not only mines structured relation semantics from retrieval captions, but also identifies heterogeneous objects from the image. RACap effectively retrieves structured relation features that contain heterogeneous visual information to enhance the semantic consistency and relational expressiveness. Experimental results show that RACap, with only 10.8M trainable parameters, achieves superior performance compared to previous lightweight captioning models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15883v1",
    "published_date": "2025-09-19 11:29:42 UTC",
    "updated_date": "2025-09-19 11:29:42 UTC"
  },
  {
    "arxiv_id": "2509.15882v1",
    "title": "Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration",
    "authors": [
      "Xingmei Wang",
      "Xiaoyu Hu",
      "Chengkai Huang",
      "Ziyan Zeng",
      "Guohao Nie",
      "Quan Z. Sheng",
      "Lina Yao"
    ],
    "abstract": "Bridging 2D and 3D sensor modalities is critical for robust perception in autonomous systems. However, image-to-point cloud (I2P) registration remains challenging due to the semantic-geometric gap between texture-rich but depth-ambiguous images and sparse yet metrically precise point clouds, as well as the tendency of existing methods to converge to local optima. To overcome these limitations, we introduce CrossI2P, a self-supervised framework that unifies cross-modal learning and two-stage registration in a single end-to-end pipeline. First, we learn a geometric-semantic fused embedding space via dual-path contrastive learning, enabling annotation-free, bidirectional alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine registration paradigm: a global stage establishes superpoint-superpixel correspondences through joint intra-modal context and cross-modal interaction modeling, followed by a geometry-constrained point-level refinement for precise registration. Third, we employ a dynamic training mechanism with gradient normalization to balance losses for feature alignment, correspondence refinement, and pose estimation. Extensive experiments demonstrate that CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry benchmark and by 37.9% on nuScenes, significantly improving both accuracy and robustness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15882v1",
    "published_date": "2025-09-19 11:29:22 UTC",
    "updated_date": "2025-09-19 11:29:22 UTC"
  },
  {
    "arxiv_id": "2509.15872v2",
    "title": "DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction",
    "authors": [
      "Manajit Das",
      "Ajnabiul Hoque",
      "Mayank Baranwal",
      "Raghavan B. Sunoj"
    ],
    "abstract": "Prediction of complete step-by-step chemical reaction mechanisms (CRMs) remains a major challenge. Whereas the traditional approaches in CRM tasks rely on expert-driven experiments or costly quantum chemical computations, contemporary deep learning (DL) alternatives ignore key intermediates and mechanistic steps and often suffer from hallucinations. We present DeepMech, an interpretable graph-based DL framework employing atom- and bond-level attention, guided by generalized templates of mechanistic operations (TMOps), to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K atom-mapped and mass-balanced elementary steps), DeepMech achieves 98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in complete CRM tasks, besides maintaining high fidelity even in out-of-distribution scenarios as well as in predicting side and/or byproducts. Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the ability of DeepMech in effectively reconstructing 2 pathways from simple primordial substrates to complex biomolecules such as serine and aldopentose. Attention analysis identifies reactive atoms/bonds in line with chemical intuition, rendering our model interpretable and suitable for reaction design.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "46 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.15872v2",
    "published_date": "2025-09-19 11:14:46 UTC",
    "updated_date": "2025-12-10 09:36:20 UTC"
  },
  {
    "arxiv_id": "2509.19372v1",
    "title": "Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution",
    "authors": [
      "Zuzanna Dubanowska",
      "Maciej Żelaszczyk",
      "Michał Brzozowski",
      "Paolo Mandica",
      "Michał Karpowicz"
    ],
    "abstract": "We critically assess the efficacy of the current SOTA in hallucination detection and find that its performance on the RAGTruth dataset is largely driven by a spurious correlation with data. Controlling for this effect, state-of-the-art performs no better than supervised linear probes, while requiring extensive hyperparameter tuning across datasets. Out-of-distribution generalization is currently out of reach, with all of the analyzed methods performing close to random. We propose a set of guidelines for hallucination detection and its evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2509.19372v1",
    "published_date": "2025-09-19 10:54:22 UTC",
    "updated_date": "2025-09-19 10:54:22 UTC"
  },
  {
    "arxiv_id": "2509.21351v1",
    "title": "Random Direct Preference Optimization for Radiography Report Generation",
    "authors": [
      "Valentin Samokhin",
      "Boris Shirokikh",
      "Mikhail Goncharov",
      "Dmitriy Umerenkov",
      "Maksim Bobrin",
      "Ivan Oseledets",
      "Dmitry Dylov",
      "Mikhail Belyaev"
    ],
    "abstract": "Radiography Report Generation (RRG) has gained significant attention in medical image analysis as a promising tool for alleviating the growing workload of radiologists. However, despite numerous advancements, existing methods have yet to achieve the quality required for deployment in real-world clinical settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated remarkable progress in the general domain by adopting training strategies originally designed for Large Language Models (LLMs), such as alignment techniques. In this paper, we introduce a model-agnostic framework to enhance RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages random contrastive sampling to construct training pairs, eliminating the need for reward models or human preference annotations. Experiments on supplementing three state-of-the-art models with our Random DPO show that our method improves clinical performance metrics by up to 5%, without requiring any additional training data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.21351v1",
    "published_date": "2025-09-19 10:53:45 UTC",
    "updated_date": "2025-09-19 10:53:45 UTC"
  },
  {
    "arxiv_id": "2509.15857v2",
    "title": "EvoBrain: Dynamic Multi-Channel EEG Graph Modeling for Time-Evolving Brain Networks",
    "authors": [
      "Rikuto Kotoge",
      "Zheng Chen",
      "Tasuku Kimura",
      "Yasuko Matsubara",
      "Takufumi Yanagisawa",
      "Haruhiko Kishima",
      "Yasushi Sakurai"
    ],
    "abstract": "Dynamic GNNs, which integrate temporal and spatial features in Electroencephalography (EEG) data, have shown great potential in automating seizure detection. However, fully capturing the underlying dynamics necessary to represent brain states, such as seizure and non-seizure, remains a non-trivial task and presents two fundamental challenges. First, most existing dynamic GNN methods are built on temporally fixed static graphs, which fail to reflect the evolving nature of brain connectivity during seizure progression. Second, current efforts to jointly model temporal signals and graph structures and, more importantly, their interactions remain nascent, often resulting in inconsistent performance. To address these challenges, we present the first theoretical analysis of these two problems, demonstrating the effectiveness and necessity of explicit dynamic modeling and time-then-graph dynamic GNN method. Building on these insights, we propose EvoBrain, a novel seizure detection model that integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights. Moreover, EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes and edges to evolve over time. Our contributions include (a) a theoretical analysis proving the expressivity advantage of explicit dynamic modeling and time-then-graph over other approaches, (b) a novel and efficient model that significantly improves AUROC by 23% and F1 score by 30%, compared with the dynamic GNN baseline, and (c) broad evaluations of our method on the challenging early seizure prediction tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2025 (spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2509.15857v2",
    "published_date": "2025-09-19 10:47:34 UTC",
    "updated_date": "2025-10-27 08:14:06 UTC"
  },
  {
    "arxiv_id": "2509.15848v1",
    "title": "A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring",
    "authors": [
      "Giovanni De Gasperis",
      "Sante Dino Facchini"
    ],
    "abstract": "Industrial monitoring systems, especially when deployed in Industry 4.0 environments, are experiencing a shift in paradigm from traditional rule-based architectures to data-driven approaches leveraging machine learning and artificial intelligence. This study presents a comparison between these two methodologies, analyzing their respective strengths, limitations, and application scenarios, and proposes a basic framework to evaluate their key properties. Rule-based systems offer high interpretability, deterministic behavior, and ease of implementation in stable environments, making them ideal for regulated industries and safety-critical applications. However, they face challenges with scalability, adaptability, and performance in complex or evolving contexts. Conversely, data-driven systems excel in detecting hidden anomalies, enabling predictive maintenance and dynamic adaptation to new conditions. Despite their high accuracy, these models face challenges related to data availability, explainability, and integration complexity. The paper suggests hybrid solutions as a possible promising direction, combining the transparency of rule-based logic with the analytical power of machine learning. Our hypothesis is that the future of industrial monitoring lies in intelligent, synergic systems that leverage both expert knowledge and data-driven insights. This dual approach enhances resilience, operational efficiency, and trust, paving the way for smarter and more flexible industrial environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15848v1",
    "published_date": "2025-09-19 10:31:59 UTC",
    "updated_date": "2025-09-19 10:31:59 UTC"
  },
  {
    "arxiv_id": "2511.02837v1",
    "title": "An extended reality-based framework for user risk training in urban built environment",
    "authors": [
      "Sotirios Konstantakos",
      "Sotirios Asparagkathos",
      "Moatasim Mahmoud",
      "Stamatia Rizou",
      "Enrico Quagliarini",
      "Gabriele Bernardini"
    ],
    "abstract": "In the context of increasing urban risks, particularly from climate change-induced flooding, this paper presents an extended Reality (XR)-based framework to improve user risk training within urban built environments. The framework is designed to improve risk awareness and preparedness among various stakeholders, including citizens, local authorities, and emergency responders. Using immersive XR technologies, the training experience simulates real-world emergency scenarios, contributing to active participation and a deeper understanding of potential hazards and especially for floods. The framework highlights the importance of stakeholder participation in its development, ensuring that training modules are customized to address the specific needs of different user groups. The iterative approach of the framework supports ongoing refinement through user feedback and performance data, thus improving the overall effectiveness of risk training initiatives. This work outlines the methodological phases involved in the framework's implementation, including i) user flow mapping, ii) scenario selection, and iii) performance evaluation, with a focus on the pilot application in Senigallia, Italy. The findings underscore the potential of XR technologies to transform urban risk training, promoting a culture of preparedness and resilience against urban hazards.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.02837v1",
    "published_date": "2025-09-19 10:08:15 UTC",
    "updated_date": "2025-09-19 10:08:15 UTC"
  },
  {
    "arxiv_id": "2509.15812v1",
    "title": "Diversity of Structured Domains via k-Kemeny Scores",
    "authors": [
      "Piotr Faliszewski",
      "Krzysztof Sornat",
      "Stanisław Szufa",
      "Tomasz Wąs"
    ],
    "abstract": "In the k-Kemeny problem, we are given an ordinal election, i.e., a collection of votes ranking the candidates from best to worst, and we seek the smallest number of swaps of adjacent candidates that ensure that the election has at most k different rankings. We study this problem for a number of structured domains, including the single-peaked, single-crossing, group-separable, and Euclidean ones. We obtain two kinds of results: (1) We show that k-Kemeny remains intractable under most of these domains, even for k=2, and (2) we use k-Kemeny to rank these domains in terms of their diversity.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15812v1",
    "published_date": "2025-09-19 09:40:39 UTC",
    "updated_date": "2025-09-19 09:40:39 UTC"
  },
  {
    "arxiv_id": "2509.15811v1",
    "title": "Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning",
    "authors": [
      "Sara Rajaee",
      "Rochelle Choenni",
      "Ekaterina Shutova",
      "Christof Monz"
    ],
    "abstract": "While the reasoning abilities of large language models (LLMs) continue to advance, it remains unclear how such ability varies across languages in multilingual LLMs and whether different languages produce reasoning paths that complement each other. To investigate this question, we train a reward model to rank generated responses for a given question across languages. Our results show that our cross-lingual reward model substantially improves mathematical reasoning performance compared to using reward modeling within a single language, benefiting even high-resource languages. While English often exhibits the highest performance in multilingual models, we find that cross-lingual sampling particularly benefits English under low sampling budgets. Our findings reveal new opportunities to improve multilingual reasoning by leveraging the complementary strengths of diverse languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15811v1",
    "published_date": "2025-09-19 09:38:54 UTC",
    "updated_date": "2025-09-19 09:38:54 UTC"
  },
  {
    "arxiv_id": "2509.15810v2",
    "title": "Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering",
    "authors": [
      "Chen Wang",
      "Yue-Jiao Gong",
      "Zhiguang Cao",
      "Zeyuan Ma"
    ],
    "abstract": "To relieve intensive human-expertise required to design optimization algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage generalization strength of meta-learning to train neural network-based algorithm design policies over a predefined training problem set, which automates the adaptability of the low-level optimizers on unseen problem instances. Currently, a common training problem set choice in existing MetaBBOs is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the MetaBBO's development, problem instances in CoCo-BBOB are more or less limited in diversity, raising the risk of overfitting of MetaBBOs, which might further results in poor generalization. In this paper, we propose an instance generation approach, termed as \\textbf{LSRE}, which could generate diverse training problem instances for MetaBBOs to learn more generalizable policies. LSRE first trains an autoencoder which maps high-dimensional problem features into a 2-dimensional latent space. Uniform-grid sampling in this latent space leads to hidden representations of problem instances with sufficient diversity. By leveraging a genetic-programming approach to search function formulas with minimal L2-distance to these hidden representations, LSRE reverse engineers a diversified problem set, termed as \\textbf{Diverse-BBO}. We validate the effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe their generalization performances on either synthetic or realistic scenarios. Extensive experimental results underscore the superiority of Diverse-BBO to existing training set choices in MetaBBOs. Further ablation studies not only demonstrate the effectiveness of design choices in LSRE, but also reveal interesting insights on instance diversity and MetaBBO's generalization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.15810v2",
    "published_date": "2025-09-19 09:37:48 UTC",
    "updated_date": "2025-11-11 06:55:10 UTC"
  },
  {
    "arxiv_id": "2509.15803v1",
    "title": "CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models",
    "authors": [
      "Fangjian Shen",
      "Zifeng Liang",
      "Chao Wang",
      "Wushao Wen"
    ],
    "abstract": "Text-to-image (T2I) models exhibit a significant yet under-explored \"brand bias\", a tendency to generate contents featuring dominant commercial brands from generic prompts, posing ethical and legal risks. We propose CIDER, a novel, model-agnostic framework to mitigate bias at inference-time through prompt refinement to avoid costly retraining. CIDER uses a lightweight detector to identify branded content and a Vision-Language Model (VLM) to generate stylistically divergent alternatives. We introduce the Brand Neutrality Score (BNS) to quantify this issue and perform extensive experiments on leading T2I models. Results show CIDER significantly reduces both explicit and implicit biases while maintaining image quality and aesthetic appeal. Our work offers a practical solution for more original and equitable content, contributing to the development of trustworthy generative AI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 7 figures, submitted to ICASSP2026",
    "pdf_url": "https://arxiv.org/pdf/2509.15803v1",
    "published_date": "2025-09-19 09:30:37 UTC",
    "updated_date": "2025-09-19 09:30:37 UTC"
  },
  {
    "arxiv_id": "2509.15800v1",
    "title": "ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding",
    "authors": [
      "Kehua Chen"
    ],
    "abstract": "Current state-of-the-art video understanding methods typically struggle with two critical challenges: (1) the computational infeasibility of processing every frame in dense video content and (2) the difficulty in identifying semantically significant frames through naive uniform sampling strategies. In this paper, we propose a novel video understanding framework, called ChronoForge-RL, which combines Temporal Apex Distillation (TAD) and KeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle these issues. Concretely, we introduce a differentiable keyframe selection mechanism that systematically identifies semantic inflection points through a three-stage process to enhance computational efficiency while preserving temporal information. Then, two particular modules are proposed to enable effective temporal reasoning: Firstly, TAD leverages variation scoring, inflection detection, and prioritized distillation to select the most informative frames. Secondly, we introduce KF-GRPO which implements a contrastive learning paradigm with a saliency-enhanced reward mechanism that explicitly incentivizes models to leverage both frame content and temporal relationships. Finally, our proposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBench compared to baseline methods, clearly surpassing previous approaches while enabling our 7B parameter model to achieve performance comparable to 72B parameter alternatives.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.15800v1",
    "published_date": "2025-09-19 09:27:24 UTC",
    "updated_date": "2025-09-19 09:27:24 UTC"
  },
  {
    "arxiv_id": "2509.15799v2",
    "title": "Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control",
    "authors": [
      "Max Studt",
      "Georg Schildbach"
    ],
    "abstract": "Achieving safe and coordinated behavior in dynamic, constraint-rich environments remains a major challenge for learning-based control. Pure end-to-end learning often suffers from poor sample efficiency and limited reliability, while model-based methods depend on predefined references and struggle to generalize. We propose a hierarchical framework that combines tactical decision-making via reinforcement learning (RL) with low-level execution through Model Predictive Control (MPC). For the case of multi-agent systems this means that high-level policies select abstract targets from structured regions of interest (ROIs), while MPC ensures dynamically feasible and safe motion. Tested on a predator-prey benchmark, our approach outperforms end-to-end and shielding-based RL baselines in terms of reward, safety, and consistency, underscoring the benefits of combining structured learning with model-based control.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.RO",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15799v2",
    "published_date": "2025-09-19 09:27:15 UTC",
    "updated_date": "2025-10-09 12:49:48 UTC"
  },
  {
    "arxiv_id": "2509.15796v1",
    "title": "Monte Carlo Tree Diffusion with Multiple Experts for Protein Design",
    "authors": [
      "Xuefeng Liu",
      "Mingxuan Cao",
      "Songhao Jiang",
      "Xiao Luo",
      "Xiaotian Duan",
      "Mengdi Wang",
      "Tobin R. Sosnick",
      "Jinbo Xu",
      "Rick Stevens"
    ],
    "abstract": "The goal of protein design is to generate amino acid sequences that fold into functional structures with desired properties. Prior methods combining autoregressive language models with Monte Carlo Tree Search (MCTS) struggle with long-range dependencies and suffer from an impractically large search space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts, which integrates masked diffusion models with tree search to enable multi-token planning and efficient exploration. Unlike autoregressive planners, MCTD-ME uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine, jointly revising multiple positions and scaling to large sequence spaces. It further leverages experts of varying capacities to enrich exploration, guided by a pLDDT-based masking schedule that targets low-confidence regions while preserving reliable residues. We propose a novel multi-expert selection rule (PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and unguided baselines in both sequence recovery (AAR) and structural similarity (scTM), with gains increasing for longer proteins and benefiting from multi-expert guidance. More generally, the framework is model-agnostic and applicable beyond inverse folding, including de novo protein engineering and multi-objective molecular generation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15796v1",
    "published_date": "2025-09-19 09:24:42 UTC",
    "updated_date": "2025-09-19 09:24:42 UTC"
  },
  {
    "arxiv_id": "2509.15786v1",
    "title": "Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration",
    "authors": [
      "Nan Li",
      "Bo Kang",
      "Tijl De Bie"
    ],
    "abstract": "Creating robust occupation taxonomies, vital for applications ranging from job recommendation to labor market intelligence, is challenging. Manual curation is slow, while existing automated methods are either not adaptive to dynamic regional markets (top-down) or struggle to build coherent hierarchies from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent taxonomy Builder), a framework that fully automates the creation of high-quality, data-driven taxonomies from raw job postings. CLIMB uses global semantic clustering to distill core occupations, then employs a reflection-based multi-agent system to iteratively build a coherent hierarchy. On three diverse, real-world datasets, we show that CLIMB produces taxonomies that are more coherent and scalable than existing methods and successfully capture unique regional characteristics. We release our code and datasets at https://anonymous.4open.science/r/CLIMB.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15786v1",
    "published_date": "2025-09-19 09:17:48 UTC",
    "updated_date": "2025-09-19 09:17:48 UTC"
  },
  {
    "arxiv_id": "2509.15785v1",
    "title": "CBPNet: A Continual Backpropagation Prompt Network for Alleviating Plasticity Loss on Edge Devices",
    "authors": [
      "Runjie Shao",
      "Boyu Diao",
      "Zijia An",
      "Ruiqi Liu",
      "Yongjun Xu"
    ],
    "abstract": "To meet the demands of applications like robotics and autonomous driving that require real-time responses to dynamic environments, efficient continual learning methods suitable for edge devices have attracted increasing attention. In this transition, using frozen pretrained models with prompts has become a mainstream strategy to combat catastrophic forgetting. However, this approach introduces a new critical bottleneck: plasticity loss, where the model's ability to learn new knowledge diminishes due to the frozen backbone and the limited capacity of prompt parameters. We argue that the reduction in plasticity stems from a lack of update vitality in underutilized parameters during the training process. To this end, we propose the Continual Backpropagation Prompt Network (CBPNet), an effective and parameter efficient framework designed to restore the model's learning vitality. We innovatively integrate an Efficient CBP Block that counteracts plasticity decay by adaptively reinitializing these underutilized parameters. Experimental results on edge devices demonstrate CBPNet's effectiveness across multiple benchmarks. On Split CIFAR-100, it improves average accuracy by over 1% against a strong baseline, and on the more challenging Split ImageNet-R, it achieves a state of the art accuracy of 69.41%. This is accomplished by training additional parameters that constitute less than 0.2% of the backbone's size, validating our approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15785v1",
    "published_date": "2025-09-19 09:16:54 UTC",
    "updated_date": "2025-09-19 09:16:54 UTC"
  },
  {
    "arxiv_id": "2509.15784v1",
    "title": "Ideal Registration? Segmentation is All You Need",
    "authors": [
      "Xiang Chen",
      "Fengting Zhang",
      "Qinghao Liu",
      "Min Liu",
      "Kun Wu",
      "Yaonan Wang",
      "Hang Zhang"
    ],
    "abstract": "Deep learning has revolutionized image registration by its ability to handle diverse tasks while achieving significant speed advantages over conventional approaches. Current approaches, however, often employ globally uniform smoothness constraints that fail to accommodate the complex, regionally varying deformations characteristic of anatomical motion. To address this limitation, we propose SegReg, a Segmentation-driven Registration framework that implements anatomically adaptive regularization by exploiting region-specific deformation patterns. Our SegReg first decomposes input moving and fixed images into anatomically coherent subregions through segmentation. These localized domains are then processed by the same registration backbone to compute optimized partial deformation fields, which are subsequently integrated into a global deformation field. SegReg achieves near-perfect structural alignment (98.23% Dice on critical anatomies) using ground-truth segmentation, and outperforms existing methods by 2-12% across three clinical registration scenarios (cardiac, abdominal, and lung images) even with automatic segmentation. Our SegReg demonstrates a near-linear dependence of registration accuracy on segmentation quality, transforming the registration challenge into a segmentation problem. The source code will be released upon manuscript acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15784v1",
    "published_date": "2025-09-19 09:16:34 UTC",
    "updated_date": "2025-09-19 09:16:34 UTC"
  },
  {
    "arxiv_id": "2509.15780v1",
    "title": "Ontology Creation and Management Tools: the Case of Anatomical Connectivity",
    "authors": [
      "Natallia Kokash",
      "Bernard de Bono",
      "Tom Gillespie"
    ],
    "abstract": "We are developing infrastructure to support researchers in mapping data related to the peripheral nervous system and other physiological systems, with an emphasis on their relevance to the organs under investigation. The nervous system, a complex network of nerves and ganglia, plays a critical role in coordinating and transmitting signals throughout the body. To aid in this, we have created ApiNATOMY, a framework for the topological and semantic representation of multiscale physiological circuit maps. ApiNATOMY integrates a Knowledge Representation (KR) model and a suite of Knowledge Management (KM) tools. The KR model enables physiology experts to easily capture interactions between anatomical entities, while the KM tools help modelers convert high-level abstractions into detailed models of physiological processes, which can be integrated with external ontologies and knowledge graphs.",
    "categories": [
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.15780v1",
    "published_date": "2025-09-19 09:10:29 UTC",
    "updated_date": "2025-09-19 09:10:29 UTC"
  },
  {
    "arxiv_id": "2509.15759v2",
    "title": "On Optimal Steering to Achieve Exact Fairness",
    "authors": [
      "Mohit Sharma",
      "Amit Jayant Deshpande",
      "Chiranjib Bhattacharyya",
      "Rajiv Ratn Shah"
    ],
    "abstract": "To fix the 'bias in, bias out' problem in fair machine learning, it is important to steer feature distributions of data or internal representations of Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes. Previous work on fair generative models and representation steering could greatly benefit from provable fairness guarantees on the model output. We define a distribution as ideal if the minimizer of any cost-sensitive risk on it is guaranteed to have exact group-fair outcomes (e.g., demographic parity, equal opportunity)-in other words, it has no fairness-utility trade-off. We formulate an optimization program for optimal steering by finding the nearest ideal distribution in KL-divergence, and provide efficient algorithms for it when the underlying distributions come from well-known parametric families (e.g., normal, log-normal). Empirically, our optimal steering techniques on both synthetic and real-world datasets improve fairness without diminishing utility (and sometimes even improve utility). We demonstrate affine steering of LLM representations to reduce bias in multi-class classification, e.g., occupation prediction from a short biography in Bios dataset (De-Arteaga et al.). Furthermore, we steer internal representations of LLMs towards desired outputs so that it works equally well across different groups.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for Presentation at Neurips 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.15759v2",
    "published_date": "2025-09-19 08:37:51 UTC",
    "updated_date": "2025-10-24 08:50:07 UTC"
  },
  {
    "arxiv_id": "2509.15750v1",
    "title": "FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion",
    "authors": [
      "Han Ye",
      "Haofu Wang",
      "Yunchi Zhang",
      "Jiangjian Xiao",
      "Yuqiang Jin",
      "Jinyuan Liu",
      "Wen-An Zhang",
      "Uladzislau Sychou",
      "Alexander Tuzikov",
      "Vladislav Sobolevskii",
      "Valerii Zakharov",
      "Boris Sokolov",
      "Minglei Fu"
    ],
    "abstract": "Reconstructing building floor plans from point cloud data is key for indoor navigation, BIM, and precise measurements. Traditional methods like geometric algorithms and Mask R-CNN-based deep learning often face issues with noise, limited generalization, and loss of geometric details. We propose FloorSAM, a framework that integrates point cloud density maps with the Segment Anything Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using grid-based filtering, adaptive resolution projection, and image enhancement, we create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for precise room segmentation, improving reconstruction across diverse layouts. Room masks are generated via adaptive prompt points and multistage filtering, followed by joint mask and point cloud analysis for contour extraction and regularization. This produces accurate floor plans and recovers room topological relationships. Tests on Giblayout and ISPRS datasets show better accuracy, recall, and robustness than traditional methods, especially in noisy and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 15 figures,",
    "pdf_url": "https://arxiv.org/pdf/2509.15750v1",
    "published_date": "2025-09-19 08:27:10 UTC",
    "updated_date": "2025-09-19 08:27:10 UTC"
  },
  {
    "arxiv_id": "2509.15733v1",
    "title": "GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation",
    "authors": [
      "Quanhao Qian",
      "Guoyang Zhao",
      "Gongjie Zhang",
      "Jiuniu Wang",
      "Ran Xu",
      "Junlong Gao",
      "Deli Zhao"
    ],
    "abstract": "Effective robotic manipulation relies on a precise understanding of 3D scene geometry, and one of the most straightforward ways to acquire such geometry is through multi-view observations. Motivated by this, we present GP3 -- a 3D geometry-aware robotic manipulation policy that leverages multi-view input. GP3 employs a spatial encoder to infer dense spatial features from RGB observations, which enable the estimation of depth and camera parameters, leading to a compact yet expressive 3D scene representation tailored for manipulation. This representation is fused with language instructions and translated into continuous actions via a lightweight policy head. Comprehensive experiments demonstrate that GP3 consistently outperforms state-of-the-art methods on simulated benchmarks. Furthermore, GP3 transfers effectively to real-world robots without depth sensors or pre-mapped environments, requiring only minimal fine-tuning. These results highlight GP3 as a practical, sensor-agnostic solution for geometry-aware robotic manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15733v1",
    "published_date": "2025-09-19 08:04:50 UTC",
    "updated_date": "2025-09-19 08:04:50 UTC"
  },
  {
    "arxiv_id": "2509.16288v1",
    "title": "Identifying Critical Pathways in Coronary Heart Disease via Fuzzy Subgraph Connectivity",
    "authors": [
      "Shanookha Ali",
      "Nitha Niralda P C"
    ],
    "abstract": "Coronary heart disease (CHD) arises from complex interactions among uncontrollable factors, controllable lifestyle factors, and clinical indicators, where relationships are often uncertain. Fuzzy subgraph connectivity (FSC) provides a systematic tool to capture such imprecision by quantifying the strength of association between vertices and subgraphs in fuzzy graphs. In this work, a fuzzy CHD graph is constructed with vertices for uncontrollable, controllable, and indicator components, and edges weighted by fuzzy memberships. Using FSC, we evaluate connectivity to identify strongest diagnostic routes, dominant risk factors, and critical bridges. Results show that FSC highlights influential pathways, bounds connectivity between weakest and strongest correlations, and reveals critical edges whose removal reduces predictive strength. Thus, FSC offers an interpretable and robust framework for modeling uncertainty in CHD risk prediction and supporting clinical decision-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.16288v1",
    "published_date": "2025-09-19 08:04:14 UTC",
    "updated_date": "2025-09-19 08:04:14 UTC"
  },
  {
    "arxiv_id": "2509.15730v1",
    "title": "A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation",
    "authors": [
      "Lukas Laakmann",
      "Seyyid A. Ciftci",
      "Christian Janiesch"
    ],
    "abstract": "Robotic process automation (RPA) is a lightweight approach to automating business processes using software robots that emulate user actions at the graphical user interface level. While RPA has gained popularity for its cost-effective and timely automation of rule-based, well-structured tasks, its symbolic nature has inherent limitations when approaching more complex tasks currently performed by human agents. Machine learning concepts enabling intelligent RPA provide an opportunity to broaden the range of automatable tasks. In this paper, we conduct a literature review to explore the connections between RPA and machine learning and organize the joint concept intelligent RPA into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML integration and RPA-ML interaction. Together, they comprise eight dimensions: architecture and ecosystem, capabilities, data basis, intelligence level, and technical depth of integration as well as deployment environment, lifecycle phase, and user-robot relation.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15730v1",
    "published_date": "2025-09-19 08:01:27 UTC",
    "updated_date": "2025-09-19 08:01:27 UTC"
  },
  {
    "arxiv_id": "2509.18190v2",
    "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing",
    "authors": [
      "Junseong Shin",
      "Seungwoo Chung",
      "Yunjeong Yang",
      "Tae Hyun Kim"
    ],
    "abstract": "Dehazing involves removing haze or fog from images to restore clarity and improve visibility by estimating atmospheric scattering effects. While deep learning methods show promise, the lack of paired real-world training data and the resulting domain gap hinder generalization to real-world scenarios. In this context, physics-grounded learning becomes crucial; however, traditional methods based on the Atmospheric Scattering Model (ASM) often fall short in handling real-world complexities and diverse haze patterns. To solve this problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF), HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones, enhancing real-world dehazing performance with only a single inference step. Additionally, we introduce a non-homogeneous haze generation method using Markov Chain Brownian Motion (MCBM) to address the scarcity of paired real-world data. By simulating realistic haze patterns through MCBM, we enhance the adaptability of HazeFlow to diverse real-world scenarios. Through extensive experiments, we demonstrate that HazeFlow achieves state-of-the-art performance across various real-world dehazing benchmark datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.18190v2",
    "published_date": "2025-09-19 07:50:09 UTC",
    "updated_date": "2025-09-25 08:32:29 UTC"
  },
  {
    "arxiv_id": "2509.19371v1",
    "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models",
    "authors": [
      "Kangtao Lv",
      "Haibin Chen",
      "Yujin Yuan",
      "Langming Liu",
      "Shilei Liu",
      "Yongwei Wang",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "abstract": "Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.19371v1",
    "published_date": "2025-09-19 07:46:10 UTC",
    "updated_date": "2025-09-19 07:46:10 UTC"
  },
  {
    "arxiv_id": "2509.15714v1",
    "title": "Once Upon a Time: Interactive Learning for Storytelling with Small Language Models",
    "authors": [
      "Jonas Mayer Martins",
      "Ali Hamza Bashir",
      "Muhammad Rehan Khalid",
      "Lisa Beinborn"
    ],
    "abstract": "Children efficiently acquire language not just by listening, but by interacting with others in their social environment. Conversely, large language models are typically trained with next-word prediction on massive amounts of text. Motivated by this contrast, we investigate whether language models can be trained with less data by learning not only from next-word prediction but also from high-level, cognitively inspired feedback. We train a student model to generate stories, which a teacher model rates on readability, narrative coherence, and creativity. By varying the amount of pretraining before the feedback loop, we assess the impact of this interactive learning on formal and functional linguistic competence. We find that the high-level feedback is highly data efficient: With just 1 M words of input in interactive learning, storytelling skills can improve as much as with 410 M words of next-word prediction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025, BabyLM Challenge; 16 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.15714v1",
    "published_date": "2025-09-19 07:45:34 UTC",
    "updated_date": "2025-09-19 07:45:34 UTC"
  },
  {
    "arxiv_id": "2509.15706v1",
    "title": "SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark",
    "authors": [
      "Chi Yang",
      "Fu Wang",
      "Xiaofei Yang",
      "Hao Huang",
      "Weijia Cao",
      "Xiaowen Chu"
    ],
    "abstract": "Cloud phase profiles are critical for numerical weather prediction (NWP), as they directly affect radiative transfer and precipitation processes. In this study, we present a benchmark dataset and a baseline framework for transforming multimodal satellite observations into detailed 3D cloud phase structures, aiming toward operational cloud phase profile retrieval and future integration with NWP systems to improve cloud microphysics parameterization. The multimodal observations consist of (1) high--spatiotemporal--resolution, multi-band visible (VIS) and thermal infrared (TIR) imagery from geostationary satellites, and (2) accurate vertical cloud phase profiles from spaceborne lidar (CALIOP\\slash CALIPSO) and radar (CPR\\slash CloudSat). The dataset consists of synchronized image--profile pairs across diverse cloud regimes, defining a supervised learning task: given VIS/TIR patches, predict the corresponding 3D cloud phase structure. We adopt SGMAGNet as the main model and compare it with several baseline architectures, including UNet variants and SegNet, all designed to capture multi-scale spatial patterns. Model performance is evaluated using standard classification metrics, including Precision, Recall, F1-score, and IoU. The results demonstrate that SGMAGNet achieves superior performance in cloud phase reconstruction, particularly in complex multi-layer and boundary transition regions. Quantitatively, SGMAGNet attains a Precision of 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617, significantly outperforming all baselines across these key metrics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 4 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.15706v1",
    "published_date": "2025-09-19 07:29:23 UTC",
    "updated_date": "2025-09-19 07:29:23 UTC"
  },
  {
    "arxiv_id": "2509.19370v1",
    "title": "Meow: End-to-End Outline Writing for Automatic Academic Survey",
    "authors": [
      "Zhaoyu Ma",
      "Yuan Shan",
      "Jiahao Zhao",
      "Nan Xu",
      "Lei Wang"
    ],
    "abstract": "As academic paper publication numbers grow exponentially, conducting in-depth surveys with LLMs automatically has become an inevitable trend. Outline writing, which aims to systematically organize related works, is critical for automated survey generation. Yet existing automatic survey methods treat outline writing as mere workflow steps in the overall pipeline. Such template-based workflows produce outlines that lack in-depth understanding of the survey topic and fine-grained styles. To address these limitations, we propose Meow, the first metadata-driven outline writing framework that produces organized and faithful outlines efficiently. Specifically, we first formulate outline writing as an end-to-end task that generates hierarchical structured outlines from paper metadata. We then curate a high-quality dataset of surveys from arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics for outline quality assessment. Finally, we employ a two-stage training approach combining supervised fine-tuning and reinforcement learning. Our 8B reasoning model demonstrates strong performance with high structural fidelity and stylistic coherence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.19370v1",
    "published_date": "2025-09-19 07:20:53 UTC",
    "updated_date": "2025-09-19 07:20:53 UTC"
  },
  {
    "arxiv_id": "2510.08581v1",
    "title": "Evaluating Hallucinations in Multimodal LLMs with Spoken Queries under Diverse Acoustic Conditions",
    "authors": [
      "Hansol Park",
      "Hoseong Ahn",
      "Junwon Moon",
      "Yejin Lee",
      "Kyuhong Shim"
    ],
    "abstract": "Hallucinations in vision-language models have been extensively studied using benchmarks that probe reliability in image-text settings. In contrast, the effect of spoken queries on multimodal hallucinations remains largely unexplored, despite the growing role of voice-driven interfaces. In this work, we investigate how spoken input influences hallucinations in multimodal large language models. We present RePOPE-Spk, an audio-augmented extension of the RePOPE benchmark, where queries are provided as speech under diverse acoustic conditions. Using RePOPE-Spk, we systematically evaluate both proprietary and open-source models. Experimental results show that hallucinations escalate when queries are spoken rather than written: error rates increase by 3% under clean speech and by up to 20% with environmental noise. Input order and query length further affect robustness, while strategies such as many-shot prompting and chain-of-thought reasoning offer partial but insufficient mitigation. These findings highlight a critical and underexplored challenge, opening new directions for building reliable voice interface systems.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08581v1",
    "published_date": "2025-09-19 07:18:45 UTC",
    "updated_date": "2025-09-19 07:18:45 UTC"
  },
  {
    "arxiv_id": "2509.15690v1",
    "title": "CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair",
    "authors": [
      "Weixuan Sun",
      "Jucai Zhai",
      "Dengfeng Liu",
      "Xin Zhang",
      "Xiaojun Wu",
      "Qiaobo Hao",
      "AIMgroup",
      "Yang Fang",
      "Jiuyang Tang"
    ],
    "abstract": "The automated repair of C++ compilation errors presents a significant challenge, the resolution of which is critical for developer productivity. Progress in this domain is constrained by two primary factors: the scarcity of large-scale, high-fidelity datasets and the limitations of conventional supervised methods, which often fail to generate semantically correct patches.This paper addresses these gaps by introducing a comprehensive framework with three core contributions. First, we present CCrepair, a novel, large-scale C++ compilation error dataset constructed through a sophisticated generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL) paradigm guided by a hybrid reward signal, shifting the focus from mere compilability to the semantic quality of the fix. Finally, we establish the robust, two-stage evaluation system providing this signal, centered on an LLM-as-a-Judge whose reliability has been rigorously validated against the collective judgments of a panel of human experts. This integrated approach aligns the training objective with generating high-quality, non-trivial patches that are both syntactically and semantically correct. The effectiveness of our approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct model achieved performance comparable to a Qwen2.5-14B-Instruct model, validating the efficiency of our training paradigm. Our work provides the research community with a valuable new dataset and a more effective paradigm for training and evaluating robust compilation repair models, paving the way for more practical and reliable automated programming assistants.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15690v1",
    "published_date": "2025-09-19 07:06:27 UTC",
    "updated_date": "2025-09-19 07:06:27 UTC"
  },
  {
    "arxiv_id": "2509.18189v1",
    "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models",
    "authors": [
      "Daxiang Dong",
      "Mingming Zheng",
      "Dong Xu",
      "Bairong Zhuang",
      "Wenyu Zhang",
      "Chunhua Luo",
      "Haoran Wang",
      "Zijian Zhao",
      "Jie Li",
      "Yuxuan Li",
      "Hanjun Zhong",
      "Mengyue Liu",
      "Jieting Chen",
      "Shupeng Li",
      "Lun Tian",
      "Yaping Feng",
      "Xin Li",
      "Donggang Jiang",
      "Yong Chen",
      "Yehua Xu",
      "Duohao Qin",
      "Chen Feng",
      "Dan Wang",
      "Henghua Zhang",
      "Jingjing Ha",
      "Jinhui He",
      "Yanfeng Zhai",
      "Chengxin Zheng",
      "Jiayi Mao",
      "Jiacheng Chen",
      "Ruchang Yao",
      "Ziye Yuan",
      "Jianmin Wu",
      "Guangjun Xie",
      "Dou Shen"
    ],
    "abstract": "We present Qianfan-VL, a series of multimodal large language models ranging from 3B to 70B parameters, achieving state-of-the-art performance through innovative domain enhancement techniques. Our approach employs multi-stage progressive training and high-precision data synthesis pipelines, which prove to be critical technologies for enhancing domain-specific capabilities while maintaining strong general performance. Qianfan-VL achieves comparable results to leading open-source models on general benchmarks, with state-of-the-art performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and MMStar. The domain enhancement strategy delivers significant advantages in OCR and document understanding, validated on both public benchmarks (OCRBench 873, DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B variants incorporate long chain-of-thought capabilities, demonstrating superior performance on mathematical reasoning (MathVista 78.6%) and logical inference tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating the capability of large-scale AI infrastructure to train SOTA-level multimodal models with over 90% scaling efficiency on 5000 chips for a single task. This work establishes an effective methodology for developing domain-enhanced multimodal models suitable for diverse enterprise deployment scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.18189v1",
    "published_date": "2025-09-19 07:05:23 UTC",
    "updated_date": "2025-09-19 07:05:23 UTC"
  },
  {
    "arxiv_id": "2509.15688v1",
    "title": "Saccadic Vision for Fine-Grained Visual Classification",
    "authors": [
      "Johann Schmidt",
      "Sebastian Stober",
      "Joachim Denzler",
      "Paul Bodesheim"
    ],
    "abstract": "Fine-grained visual classification (FGVC) requires distinguishing between visually similar categories through subtle, localized features - a task that remains challenging due to high intra-class variability and limited inter-class differences. Existing part-based methods often rely on complex localization networks that learn mappings from pixel to sample space, requiring a deep understanding of image content while limiting feature utility for downstream tasks. In addition, sampled points frequently suffer from high spatial redundancy, making it difficult to quantify the optimal number of required parts. Inspired by human saccadic vision, we propose a two-stage process that first extracts peripheral features (coarse view) and generates a sample map, from which fixation patches are sampled and encoded in parallel using a weight-shared encoder. We employ contextualized selective attention to weigh the impact of each fixation patch before fusing peripheral and focus representations. To prevent spatial collapse - a common issue in part-based methods - we utilize non-maximum suppression during fixation sampling to eliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks (CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect datasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method achieves comparable performance to state-of-the-art approaches while consistently outperforming our baseline encoder.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15688v1",
    "published_date": "2025-09-19 07:03:37 UTC",
    "updated_date": "2025-09-19 07:03:37 UTC"
  },
  {
    "arxiv_id": "2509.15676v1",
    "title": "KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning",
    "authors": [
      "Vaibhav Singh",
      "Soumya Suvra Ghosal",
      "Kapu Nirmal Joshua",
      "Soumyabrata Pal",
      "Sayak Ray Chowdhury"
    ],
    "abstract": "In-context learning (ICL) has emerged as a powerful paradigm for adapting large language models (LLMs) to new and data-scarce tasks using only a few carefully selected task-specific examples presented in the prompt. However, given the limited context size of LLMs, a fundamental question arises: Which examples should be selected to maximize performance on a given user query? While nearest-neighbor-based methods like KATE have been widely adopted for this purpose, they suffer from well-known drawbacks in high-dimensional embedding spaces, including poor generalization and a lack of diversity. In this work, we study this problem of example selection in ICL from a principled, information theory-driven perspective. We first model an LLM as a linear function over input embeddings and frame the example selection task as a query-specific optimization problem: selecting a subset of exemplars from a larger example bank that minimizes the prediction error on a specific query. This formulation departs from traditional generalization-focused learning theoretic approaches by targeting accurate prediction for a specific query instance. We derive a principled surrogate objective that is approximately submodular, enabling the use of a greedy algorithm with an approximation guarantee. We further enhance our method by (i) incorporating the kernel trick to operate in high-dimensional feature spaces without explicit mappings, and (ii) introducing an optimal design-based regularizer to encourage diversity in the selected examples. Empirically, we demonstrate significant improvements over standard retrieval methods across a suite of classification tasks, highlighting the benefits of structure-aware, diverse example selection for ICL in real-world, label-scarce scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15676v1",
    "published_date": "2025-09-19 06:50:03 UTC",
    "updated_date": "2025-09-19 06:50:03 UTC"
  },
  {
    "arxiv_id": "2509.15674v2",
    "title": "Inference Offloading for Cost-Sensitive Binary Classification at the Edge",
    "authors": [
      "Vishnu Narayanan Moothedath",
      "Umang Agarwal",
      "Umeshraja N",
      "James Richard Gross",
      "Jaya Prakash Champati",
      "Sharayu Moharir"
    ],
    "abstract": "We focus on a binary classification problem in an edge intelligence system where false negatives are more costly than false positives. The system has a compact, locally deployed model, which is supplemented by a larger, remote model, which is accessible via the network by incurring an offloading cost. For each sample, our system first uses the locally deployed model for inference. Based on the output of the local model, the sample may be offloaded to the remote model. This work aims to understand the fundamental trade-off between classification accuracy and the offloading costs within such a hierarchical inference (HI) system. To optimise this system, we propose an online learning framework that continuously adapts a pair of thresholds on the local model's confidence scores. These thresholds determine the prediction of the local model and whether a sample is classified locally or offloaded to the remote model. We present a closed-form solution for the setting where the local model is calibrated. For the more general case of uncalibrated models, we introduce H2T2, an online two-threshold hierarchical inference policy, and prove it achieves sublinear regret. H2T2 is model-agnostic, requires no training, and learns during the inference phase using limited feedback. Simulations on real-world datasets show that H2T2 consistently outperforms naive and single-threshold HI policies, sometimes even surpassing offline optima. The policy also demonstrates robustness to distribution shifts and adapts effectively to mismatched classifiers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15674v2",
    "published_date": "2025-09-19 06:49:40 UTC",
    "updated_date": "2025-11-12 19:51:54 UTC"
  },
  {
    "arxiv_id": "2509.15666v3",
    "title": "TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation",
    "authors": [
      "Yongsheng Feng",
      "Yuetonghui Xu",
      "Jiehui Luo",
      "Hongjia Liu",
      "Xiaobing Li",
      "Feng Yu",
      "Wei Li"
    ],
    "abstract": "Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly large networks, inflating training and deployment costs. Motivated by recent advances in inference-time scaling for generative modeling, we propose Training-Time and Inference-Time Scalable Discriminative Source Separation (TISDiSS), a unified framework that integrates early-split multi-loss supervision, shared-parameter design, and dynamic inference repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting inference depth without retraining additional models. We further provide systematic analyses of architectural and training choices and show that training with more inference repetitions improves shallow-inference performance, benefiting low-latency applications. Experiments on standard speech separation benchmarks demonstrate state-of-the-art performance with a reduced parameter count, establishing TISDiSS as a scalable and practical framework for adaptive source separation. Code is available at https://github.com/WingSingFung/TISDiSS.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Submitted to ICASSP 2026.(C) 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work",
    "pdf_url": "https://arxiv.org/pdf/2509.15666v3",
    "published_date": "2025-09-19 06:42:27 UTC",
    "updated_date": "2025-10-14 07:59:00 UTC"
  },
  {
    "arxiv_id": "2509.15661v1",
    "title": "SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models",
    "authors": [
      "Qiaolin Wang",
      "Xilin Jiang",
      "Linyang He",
      "Junkai Wu",
      "Nima Mesgarani"
    ],
    "abstract": "While large audio-language models (LALMs) have demonstrated state-of-the-art audio understanding, their reasoning capability in complex soundscapes still falls behind large vision-language models (LVLMs). Compared to the visual domain, one bottleneck is the lack of large-scale chain-of-thought audio data to teach LALM stepwise reasoning. To circumvent this data and modality gap, we present SightSound-R1, a cross-modal distillation framework that transfers advanced reasoning from a stronger LVLM teacher to a weaker LALM student on the same audio-visual question answering (AVQA) dataset. SightSound-R1 consists of three core steps: (i) test-time scaling to generate audio-focused chains of thought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter hallucinations, and (iii) a distillation pipeline with supervised fine-tuning (SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM student. Results show that SightSound-R1 improves LALM reasoning performance both in the in-domain AVQA test set as well as in unseen auditory scenes and questions, outperforming both pretrained and label-only distilled baselines. Thus, we conclude that vision reasoning can be effectively transferred to audio models and scaled with abundant audio-visual data.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15661v1",
    "published_date": "2025-09-19 06:39:39 UTC",
    "updated_date": "2025-09-19 06:39:39 UTC"
  },
  {
    "arxiv_id": "2509.15658v1",
    "title": "Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach",
    "authors": [
      "Jisu Kim",
      "Jinhee Park",
      "Changhyun Jeon",
      "Jungwoo Choi",
      "Keonwoo Kim",
      "Minji Hong",
      "Sehyun Kim"
    ],
    "abstract": "Traditional query expansion techniques for addressing vocabulary mismatch problems in information retrieval are context-sensitive and may lead to performance degradation. As an alternative, document expansion research has gained attention, but existing methods such as Doc2Query have limitations including excessive preprocessing costs, increased index size, and reliability issues with generated content. To mitigate these problems and seek more structured and efficient alternatives, this study proposes a method that divides documents into chunk units and generates textual data for each chunk to simultaneously improve retrieval efficiency and accuracy. The proposed \"Chunk Knowledge Generation Model\" adopts a T5-based multi-task learning structure that simultaneously generates titles and candidate questions from each document chunk while extracting keywords from user queries. This approach maximizes computational efficiency by generating and extracting three types of semantic information in parallel through a single encoding and two decoding processes. The generated data is utilized as additional information in the retrieval system. GPT-based evaluation on 305 query-document pairs showed that retrieval using the proposed model achieved 95.41% accuracy at Top@10, demonstrating superior performance compared to document chunk-level retrieval. This study contributes by proposing an approach that simultaneously generates titles and candidate questions from document chunks for application in retrieval pipelines, and provides empirical evidence applicable to large-scale information retrieval systems by demonstrating improved retrieval accuracy through qualitative evaluation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15658v1",
    "published_date": "2025-09-19 06:32:30 UTC",
    "updated_date": "2025-09-19 06:32:30 UTC"
  },
  {
    "arxiv_id": "2509.19369v1",
    "title": "SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use",
    "authors": [
      "Changhyun Jeon",
      "Jinhee Park",
      "Jungwoo Choi",
      "Keonwoo Kim",
      "Jisu Kim",
      "Minji Hong"
    ],
    "abstract": "We propose a small-scale language model (SLM) based agent architecture, Planner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G separates planning, calling, and generation by role: the Planner produces an initial batch plan with limited on-demand replanning; the Caller returns a normalized call object after joint schema-value validation; and the Generator integrates tool outputs to produce the final answer. We apply a Korean-first value policy to reduce execution failures caused by frequent Korean-to-English code switching in Korean settings. Evaluation assumes Korean queries and Korean tool/parameter specifications; it covers single-chain, multi-chain, missing-parameters, and missing-functions scenarios, and is conducted via an LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface. Results show that P-C-G delivers competitive tool-use accuracy and end-to-end quality while reducing tokens and maintaining acceptable latency, indicating that role-specialized SLMs are a cost-effective alternative for Korean tool-use agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.19369v1",
    "published_date": "2025-09-19 06:25:23 UTC",
    "updated_date": "2025-09-19 06:25:23 UTC"
  },
  {
    "arxiv_id": "2509.15651v1",
    "title": "Toward Efficient Influence Function: Dropout as a Compression Tool",
    "authors": [
      "Yuchen Zhang",
      "Mohammad Mohammadi Amiri"
    ],
    "abstract": "Assessing the impact the training data on machine learning models is crucial for understanding the behavior of the model, enhancing the transparency, and selecting training data. Influence function provides a theoretical framework for quantifying the effect of training data points on model's performance given a specific test data. However, the computational and memory costs of influence function presents significant challenges, especially for large-scale models, even when using approximation methods, since the gradients involved in computation are as large as the model itself. In this work, we introduce a novel approach that leverages dropout as a gradient compression mechanism to compute the influence function more efficiently. Our method significantly reduces computational and memory overhead, not only during the influence function computation but also in gradient compression process. Through theoretical analysis and empirical validation, we demonstrate that our method could preserves critical components of the data influence and enables its application to modern large-scale models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15651v1",
    "published_date": "2025-09-19 06:20:54 UTC",
    "updated_date": "2025-09-19 06:20:54 UTC"
  },
  {
    "arxiv_id": "2509.15641v1",
    "title": "Information Geometry of Variational Bayes",
    "authors": [
      "Mohammad Emtiyaz Khan"
    ],
    "abstract": "We highlight a fundamental connection between information geometry and variational Bayes (VB) and discuss its consequences for machine learning. Under certain conditions, a VB solution always requires estimation or computation of natural gradients. We show several consequences of this fact by using the natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian Learning Rule (BLR). These include (i) a simplification of Bayes' rule as addition of natural gradients, (ii) a generalization of quadratic surrogates used in gradient-based methods, and (iii) a large-scale implementation of VB algorithms for large language models. Neither the connection nor its consequences are new but we further emphasize the common origins of the two fields of information geometry and Bayes with a hope to facilitate more work at the intersection of the two fields.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15641v1",
    "published_date": "2025-09-19 06:07:38 UTC",
    "updated_date": "2025-09-19 06:07:38 UTC"
  },
  {
    "arxiv_id": "2509.15635v1",
    "title": "MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents",
    "authors": [
      "Pan Tang",
      "Shixiang Tang",
      "Huanqi Pu",
      "Zhiqing Miao",
      "Zhixing Wang"
    ],
    "abstract": "This paper presents MicroRCA-Agent, an innovative solution for microservice root cause analysis based on large language model agents, which constructs an intelligent fault root cause localization system with multimodal data fusion. The technical innovations are embodied in three key aspects: First, we combine the pre-trained Drain log parsing algorithm with multi-level data filtering mechanism to efficiently compress massive logs into high-quality fault features. Second, we employ a dual anomaly detection approach that integrates Isolation Forest unsupervised learning algorithms with status code validation to achieve comprehensive trace anomaly identification. Third, we design a statistical symmetry ratio filtering mechanism coupled with a two-stage LLM analysis strategy to enable full-stack phenomenon summarization across node-service-pod hierarchies. The multimodal root cause analysis module leverages carefully designed cross-modal prompts to deeply integrate multimodal anomaly information, fully exploiting the cross-modal understanding and logical reasoning capabilities of large language models to generate structured analysis results encompassing fault components, root cause descriptions, and reasoning trace. Comprehensive ablation studies validate the complementary value of each modal data and the effectiveness of the system architecture. The proposed solution demonstrates superior performance in complex microservice fault scenarios, achieving a final score of 50.71. The code has been released at: https://github.com/tangpan360/MicroRCA-Agent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 22 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.15635v1",
    "published_date": "2025-09-19 05:57:03 UTC",
    "updated_date": "2025-09-19 05:57:03 UTC"
  },
  {
    "arxiv_id": "2509.25534v1",
    "title": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning",
    "authors": [
      "Zhiling Ye",
      "Yun Yue",
      "Haowen Wang",
      "Xudong Han",
      "Jiadi Jiang",
      "Cheng Wei",
      "Lei Fan",
      "Jiaxin Liang",
      "Shuowen Zhang",
      "Ji Li",
      "Chunxiao Guo",
      "Jian Wang",
      "Peng Wei",
      "Jinjie Gu"
    ],
    "abstract": "Open-ended evaluation is essential for deploying large language models in real-world settings. In studying HealthBench, we observe that using the model itself as a grader and generating rubric-based reward signals substantially improves reasoning performance. Remarkably, the trained model also becomes a stronger grader. Motivated by this, we introduce Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning, a lightweight framework that enables faster and more resource-efficient training while surpassing baselines. Remarkably, on Qwen3-32B, training with just the 4000-sample HealthBench Easy subset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard. Incorporating a small amount of teacher-graded data further enhances performance for less capable models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25534v1",
    "published_date": "2025-09-19 05:08:55 UTC",
    "updated_date": "2025-09-19 05:08:55 UTC"
  },
  {
    "arxiv_id": "2509.19368v1",
    "title": "Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding",
    "authors": [
      "Ruanjun Li",
      "Ziheng Liu",
      "Yuanming Shi",
      "Jiawei Shao",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "Large language models (LLMs) deliver impressive generation quality, but incur very high inference cost because each output token is generated auto-regressively through all model layers. Early-exit based self-speculative decoding (EESD) has emerged to mitigate this cost. However, in practice, many approaches struggle to achieve the expected acceleration in such draft-then-verify paradigm even with a well-aligned early-exit head and selected exit position. Our analysis reveals that EESD only pays off when the vast majority of draft tokens are accepted by the LLM. Otherwise, the draft cost may overcome the acceleration gain and lead to a negative speedup. To mitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD) that fully pipelines the draft and verification work so that no effort is wasted on failed predictions. It has two key innovations. We configure the model layers as a pipeline in which early-exit (draft) computations and remaining-layer (verification) computations overlap. We interleave drafting and verification per token. While the LLM is verifying the current token in its final layers, the early-exit path simultaneously drafts the next token. Such a verify-while-draft scheme keeps all units busy and validates tokens on-the-fly analogous to pipelining the speculation and verification stages. Empirical results confirm that PPSD achieves state-of-the-art acceleration in self-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup ratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration at the fixed acceptance rate and exit position, showcasing its advancement in providing efficient self-speculation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.19368v1",
    "published_date": "2025-09-19 04:51:41 UTC",
    "updated_date": "2025-09-19 04:51:41 UTC"
  },
  {
    "arxiv_id": "2509.15591v2",
    "title": "Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification",
    "authors": [
      "Zinan Lin",
      "Enshu Liu",
      "Xuefei Ning",
      "Junyi Zhu",
      "Wenyu Wang",
      "Sergey Yekhanin"
    ],
    "abstract": "Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.15591v2",
    "published_date": "2025-09-19 04:47:16 UTC",
    "updated_date": "2025-11-04 00:34:50 UTC"
  },
  {
    "arxiv_id": "2509.15588v1",
    "title": "CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion",
    "authors": [
      "Yu-Cheng Chang",
      "Guan-Wei Yeo",
      "Quah Eugene",
      "Fan-Jie Shih",
      "Yuan-Ching Kuo",
      "Tsung-En Yu",
      "Hung-Chun Hsu",
      "Ming-Feng Tsai",
      "Chuan-Ju Wang"
    ],
    "abstract": "The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both interactive and offline submission tasks. The former requires systems to operate under real-time constraints, making robustness and efficiency as important as accuracy, while the latter enables controlled evaluation of passage ranking and response generation with pre-defined datasets. To address this, we explored query rewriting and retrieval fusion as core strategies. We built our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion (RRF) strategies to handle different submission tasks. Results show that reranking and fusion improve robustness while revealing trade-offs between effectiveness and efficiency across both tasks.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15588v1",
    "published_date": "2025-09-19 04:42:31 UTC",
    "updated_date": "2025-09-19 04:42:31 UTC"
  },
  {
    "arxiv_id": "2509.15587v3",
    "title": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models",
    "authors": [
      "Tsz Ting Chung",
      "Lemao Liu",
      "Mo Yu",
      "Dit-Yan Yeung"
    ],
    "abstract": "Logic reasoning in natural language has been recognized as an important measure of human intelligence for Large Language Models (LLMs). Popular benchmarks may entangle multiple reasoning skills and thus provide unfaithful evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning benchmarks are limited in language diversity and their distributions are deviated from the distribution of an ideal logic reasoning benchmark, which may lead to biased evaluation results. This paper thereby proposes a new classical logic benchmark DivLogicEval, consisting of natural sentences composed of diverse statements in a counterintuitive way. To ensure a more reliable evaluation, we also introduce a new evaluation metric that mitigates the influence of bias and randomness inherent in LLMs. Through experiments, we demonstrate the extent to which logical reasoning is required to answer the questions in DivLogicEval and compare the performance of different popular LLMs in conducting logical reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2025. Project Page: https://ttchungc.github.io/projects/divlogiceval/",
    "pdf_url": "https://arxiv.org/pdf/2509.15587v3",
    "published_date": "2025-09-19 04:40:46 UTC",
    "updated_date": "2025-09-26 07:57:51 UTC"
  },
  {
    "arxiv_id": "2509.15582v2",
    "title": "Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework with Residual-enhanced DRL for Visually Impaired Scenarios",
    "authors": [
      "Yuting Zeng",
      "Zhiwen Zheng",
      "You Zhou",
      "JiaLing Xiao",
      "Yongbin Yu",
      "Manping Fan",
      "Bo Gong",
      "Liyong Ren"
    ],
    "abstract": "This paper proposes a momentum-constrained hybrid heuristic trajectory optimization framework (MHHTOF) tailored for assistive navigation in visually impaired scenarios, integrating trajectory sampling generation, optimization and evaluation with residual-enhanced deep reinforcement learning (DRL). In the first stage, heuristic trajectory sampling cluster (HTSC) is generated in the Frenet coordinate system using third-order interpolation with fifth-order polynomials and momentum-constrained trajectory optimization (MTO) constraints to ensure smoothness and feasibility. After first stage cost evaluation, the second stage leverages a residual-enhanced actor-critic network with LSTM-based temporal feature modeling to adaptively refine trajectory selection in the Cartesian coordinate system. A dual-stage cost modeling mechanism (DCMM) with weight transfer aligns semantic priorities across stages, supporting human-centered optimization. Experimental results demonstrate that the proposed LSTM-ResB-PPO achieves significantly faster convergence, attaining stable policy performance in approximately half the training iterations required by the PPO baseline, while simultaneously enhancing both reward outcomes and training stability. Compared to baseline method, the selected model reduces average cost and cost variance by 30.3% and 53.3%, and lowers ego and obstacle risks by over 77%. These findings validate the framework's effectiveness in enhancing robustness, safety, and real-time feasibility in complex assistive planning tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Upon further internal evaluation, we found that the current version does not adequately represent the clarity and completeness that we intend for this work. To avoid possible misunderstanding caused by this preliminary form, we request withdrawal. A refined version will be prepared privately before any further dissemination",
    "pdf_url": "https://arxiv.org/pdf/2509.15582v2",
    "published_date": "2025-09-19 04:33:39 UTC",
    "updated_date": "2025-12-05 10:02:03 UTC"
  },
  {
    "arxiv_id": "2509.15577v2",
    "title": "Relevance to Utility: Process-Supervised Rewrite for RAG",
    "authors": [
      "Jaeyoung Kim",
      "Jongho Kim",
      "Seung-won Hwang",
      "Seoho Song",
      "Young-In Song"
    ],
    "abstract": "Retrieval-augmented generation systems often suffer from a gap between optimizing retrieval relevance and generative utility. With such a gap, retrieved documents may be topically relevant but still lack the content needed for effective reasoning during generation. While existing bridge modules attempt to rewrite the retrieved text for better generation, we show how they fail by not capturing \"document utility\". In this work, we propose R2U, with a key distinction of approximating true utility through joint observation of rewriting and answering in the reasoning process. To distill, R2U scale such supervision to enhance reliability in distillation. We further construct utility-improvement supervision by measuring the generator's gain of the answer under the rewritten context, yielding signals for fine-tuning and preference optimization. We evaluate our method across multiple open-domain question-answering benchmarks. The empirical results demonstrate consistent improvements over strong bridging baselines",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15577v2",
    "published_date": "2025-09-19 04:24:57 UTC",
    "updated_date": "2026-01-07 15:15:20 UTC"
  },
  {
    "arxiv_id": "2509.15578v1",
    "title": "Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion",
    "authors": [
      "Shanghong Li",
      "Chiam Wen Qi Ruth",
      "Hong Xu",
      "Fang Liu"
    ],
    "abstract": "The rapid proliferation of short video platforms has necessitated advanced methods for detecting fake news. This need arises from the widespread influence and ease of sharing misinformation, which can lead to significant societal harm. Current methods often struggle with the dynamic and multimodal nature of short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel multimodal framework that integrates video, audio, and text data to evaluate the authenticity of short video content. HFN introduces a Decision Network that dynamically adjusts modality weights during inference and a Weighted Multi-Modal Feature Fusion module to ensure robust performance even with incomplete data. Additionally, we contribute a comprehensive dataset VESV (VEracity on Short Videos) specifically designed for short video fake news detection. Experiments conducted on the FakeTT and newly collected VESV datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over state-of-the-art methods. This work establishes a robust solution capable of effectively identifying fake news in the complex landscape of short video platforms, paving the way for more reliable and comprehensive approaches in combating misinformation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15578v1",
    "published_date": "2025-09-19 04:24:57 UTC",
    "updated_date": "2025-09-19 04:24:57 UTC"
  },
  {
    "arxiv_id": "2509.25204v1",
    "title": "Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation",
    "authors": [
      "Jin Li",
      "Zhebo Wang",
      "Tianliang Lu",
      "Mohan Li",
      "Wenpeng Xing",
      "Meng Han"
    ],
    "abstract": "Entropy-based inference methods have gained traction for improving the reliability of Large Language Models (LLMs). However, many existing approaches, such as entropy minimization techniques, suffer from high computational overhead and fail to leverage historical token context effectively. To address these limitations, we propose Spectral Logit Sculpting (SLS), a lightweight inference-time optimization method that dynamically modulates token distributions using spectral and entropic properties of recent logits. SLS maintains a sliding buffer of top-K logits, performs on-the-fly Singular Value Decomposition (SVD) to identify dominant spectral directions, and adaptively rescales logits based on both entropy and logit gap statistics--only activating when uncertainty is high. Without updating any model parameters, SLS effectively sharpens the output distribution while preserving contextual consistency. Experimental results on multiple public benchmarks demonstrate that SLS consistently outperforms existing baseline methods, achieving superior accuracy in mathematical, coding, and scientific reasoning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to IEEE ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.25204v1",
    "published_date": "2025-09-19 04:17:46 UTC",
    "updated_date": "2025-09-19 04:17:46 UTC"
  },
  {
    "arxiv_id": "2509.15573v2",
    "title": "Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach",
    "authors": [
      "Shilong Bao",
      "Qianqian Xu",
      "Feiran Li",
      "Boyu Han",
      "Zhiyong Yang",
      "Xiaochun Cao",
      "Qingming Huang"
    ],
    "abstract": "This paper investigates a fundamental yet underexplored issue in Salient Object Detection (SOD): the size-invariant property for evaluation protocols, particularly in scenarios when multiple salient objects of significantly different sizes appear within a single image. We first present a novel perspective to expose the inherent size sensitivity of existing widely used SOD metrics. Through careful theoretical derivations, we show that the evaluation outcome of an image under current SOD metrics can be essentially decomposed into a sum of several separable terms, with the contribution of each term being directly proportional to its corresponding region size. Consequently, the prediction errors would be dominated by the larger regions, while smaller yet potentially more semantically important objects are often overlooked, leading to biased performance assessments and practical degradation. To address this challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed. The core idea is to evaluate each separable component individually and then aggregate the results, thereby effectively mitigating the impact of size imbalance across objects. Building upon this, we further develop a dedicated optimization framework (SIOpt), which adheres to the size-invariant principle and significantly enhances the detection of salient objects across a broad range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly integrated with a wide range of SOD backbones. Theoretically, we also present generalization analysis of SOD methods and provide evidence supporting the validity of our new evaluation protocols. Finally, comprehensive experiments speak to the efficacy of our proposed approach. The code is available at https://github.com/Ferry-Li/SI-SOD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15573v2",
    "published_date": "2025-09-19 04:12:14 UTC",
    "updated_date": "2025-10-03 01:55:31 UTC"
  },
  {
    "arxiv_id": "2509.15570v1",
    "title": "Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection",
    "authors": [
      "Xinxin Meng",
      "Jiangtao Guo",
      "Yunxiang Zhang",
      "Shun Huang"
    ],
    "abstract": "The outlier exposure method is an effective approach to address the unsupervised anomaly sound detection problem. The key focus of this method is how to make the model learn the distribution space of normal data. Based on biological perception and data analysis, it is found that anomalous audio and noise often have higher frequencies. Therefore, we propose a data augmentation method for high-frequency information in contrastive learning. This enables the model to pay more attention to the low-frequency information of the audio, which represents the normal operational mode of the machine. We evaluated the proposed method on the DCASE 2020 Task 2. The results showed that our method outperformed other contrastive learning methods used on this dataset. We also evaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted CVIPPR 2024 April Xiamen China",
    "pdf_url": "https://arxiv.org/pdf/2509.15570v1",
    "published_date": "2025-09-19 04:10:31 UTC",
    "updated_date": "2025-09-19 04:10:31 UTC"
  },
  {
    "arxiv_id": "2509.15568v1",
    "title": "LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs",
    "authors": [
      "Junlong Jia",
      "Xing Wu",
      "Chaochen Gao",
      "Ziyang Chen",
      "Zijia Lin",
      "Zhongzhi Li",
      "Weinong Wang",
      "Haotian Xu",
      "Donghui Jin",
      "Debing Zhang",
      "Binghui Guo"
    ],
    "abstract": "High-quality long-context data is essential for training large language models (LLMs) capable of processing extensive documents, yet existing synthesis approaches using relevance-based aggregation face challenges of computational efficiency. We present LiteLong, a resource-efficient method for synthesizing long-context data through structured topic organization and multi-agent debate. Our approach leverages the BISAC book classification system to provide a comprehensive hierarchical topic organization, and then employs a debate mechanism with multiple LLMs to generate diverse, high-quality topics within this structure. For each topic, we use lightweight BM25 retrieval to obtain relevant documents and concatenate them into 128K-token training samples. Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves competitive long-context performance and can seamlessly integrate with other long-dependency enhancement methods. LiteLong makes high-quality long-context data synthesis more accessible by reducing both computational and data engineering costs, facilitating further research in long-context language training.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "work in progress",
    "pdf_url": "https://arxiv.org/pdf/2509.15568v1",
    "published_date": "2025-09-19 04:07:46 UTC",
    "updated_date": "2025-09-19 04:07:46 UTC"
  },
  {
    "arxiv_id": "2509.15566v4",
    "title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
    "authors": [
      "Shaojie Zhang",
      "Ruoceng Zhang",
      "Pei Fu",
      "Shaokang Wang",
      "Jiahui Yang",
      "Xin Du",
      "Shiqi Cui",
      "Bin Qin",
      "Ying Huang",
      "Zhenbo Luo",
      "Jian Luan"
    ],
    "abstract": "In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates competitive performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.15566v4",
    "published_date": "2025-09-19 04:03:44 UTC",
    "updated_date": "2025-10-27 11:34:58 UTC"
  },
  {
    "arxiv_id": "2509.25203v3",
    "title": "Generating High-Quality Datasets for Code Editing via Open-Source Language Models",
    "authors": [
      "Zekai Zhang",
      "Mingwei Liu",
      "Zhenxi Chen",
      "Linxi Liang",
      "Yuxuan Chen",
      "Guangsheng Ou",
      "Yanlin Wang",
      "Dan Li",
      "Xin Peng",
      "Zibin Zheng"
    ],
    "abstract": "Code editing plays a vital role in software engineering, requiring developers to adjust existing code according to natural language instructions while keeping functionality intact and avoiding unnecessary modifications. However, commit-based datasets commonly used for this task are often noisy, lack diversity, and fail to reflect the style of real-world edit instructions. To address this, we introduce OpenCodeEdit, an open-source pipeline that leverages multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces both concise \"lazy\" instructions and more detailed \"descriptive\" ones, and applies filtering based on diffs and topics to guarantee data quality and variety. Using this process, we construct OCEDataFT, a curated dataset of 20K samples. Fine-tuning three advanced base models on OCEDataFT leads to significant performance boosts on the CanItEdit benchmark, with relative pass@1 improvements ranging from 4.50% to 20.79%. Notably, the resulting models achieve performance close to closed-source systems, narrowing the gap to GPT-4 to just 3.54%, without relying on proprietary resources or manual annotation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "23 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.25203v3",
    "published_date": "2025-09-19 03:57:39 UTC",
    "updated_date": "2025-10-07 07:44:59 UTC"
  },
  {
    "arxiv_id": "2509.15557v1",
    "title": "Reward Hacking Mitigation using Verifiable Composite Rewards",
    "authors": [
      "Mirza Farhan Bin Tarek",
      "Rahmatollah Beheshti"
    ],
    "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. Our work addresses two primary forms of this behavior: i) providing a final answer without preceding reasoning, and ii) employing non-standard reasoning formats to exploit the reward mechanism. To mitigate these, we introduce a composite reward function with specific penalties for these behaviors. Our experiments show that extending RLVR with our proposed reward model leads to better-formatted reasoning with less reward hacking and good accuracy compared to the baselines. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 16th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB 2025)",
    "pdf_url": "https://arxiv.org/pdf/2509.15557v1",
    "published_date": "2025-09-19 03:40:27 UTC",
    "updated_date": "2025-09-19 03:40:27 UTC"
  },
  {
    "arxiv_id": "2509.15556v1",
    "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining",
    "authors": [
      "Ping Guo",
      "Yubing Ren",
      "Binbin Liu",
      "Fengze Liu",
      "Haobin Lin",
      "Yifan Zhang",
      "Bingni Zhang",
      "Taifeng Wang",
      "Yin Zheng"
    ],
    "abstract": "Large language models (LLMs) have become integral to a wide range of applications worldwide, driving an unprecedented global demand for effective multilingual capabilities. Central to achieving robust multilingual performance is the strategic allocation of language proportions within training corpora. However, determining optimal language ratios is highly challenging due to intricate cross-lingual interactions and sensitivity to dataset scale. This paper introduces Climb (Cross-Lingual Interaction-aware Multilingual Balancing), a novel framework designed to systematically optimize multilingual data allocation. At its core, Climb introduces a cross-lingual interaction-aware language ratio, explicitly quantifying each language's effective allocation by capturing inter-language dependencies. Leveraging this ratio, Climb proposes a principled two-step optimization procedure--first equalizing marginal benefits across languages, then maximizing the magnitude of the resulting language allocation vectors--significantly simplifying the inherently complex multilingual optimization problem. Extensive experiments confirm that Climb can accurately measure cross-lingual interactions across various multilingual settings. LLMs trained with Climb-derived proportions consistently achieve state-of-the-art multilingual performance, even achieving competitive performance with open-sourced LLMs trained with more tokens.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15556v1",
    "published_date": "2025-09-19 03:34:34 UTC",
    "updated_date": "2025-09-19 03:34:34 UTC"
  },
  {
    "arxiv_id": "2509.15553v1",
    "title": "Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification",
    "authors": [
      "Tian Lan",
      "Yiming Zheng",
      "Jianxin Yin"
    ],
    "abstract": "Multi-label classification has broad applications and depends on powerful representations capable of capturing multi-label interactions. We introduce \\textit{Diff-Feat}, a simple but powerful framework that extracts intermediate features from pre-trained diffusion-Transformer models for images and text, and fuses them for downstream tasks. We observe that for vision tasks, the most discriminative intermediate feature along the diffusion process occurs at the middle step and is located in the middle block in Transformer. In contrast, for language tasks, the best feature occurs at the noise-free step and is located in the deepest block. In particular, we observe a striking phenomenon across varying datasets: a mysterious \"Layer $12$\" consistently yields the best performance on various downstream classification tasks for images (under DiT-XL/2-256$\\times$256). We devise a heuristic local-search algorithm that pinpoints the locally optimal \"image-text\"$\\times$\"block-timestep\" pair among a few candidates, avoiding an exhaustive grid search. A simple fusion-linear projection followed by addition-of the selected representations yields state-of-the-art performance: 98.6\\% mAP on MS-COCO-enhanced and 45.7\\% mAP on Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a wide margin. t-SNE and clustering metrics further reveal that \\textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts. The code is available at https://github.com/lt-0123/Diff-Feat.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15553v1",
    "published_date": "2025-09-19 03:13:58 UTC",
    "updated_date": "2025-09-19 03:13:58 UTC"
  },
  {
    "arxiv_id": "2509.15541v1",
    "title": "Stress Testing Deliberative Alignment for Anti-Scheming Training",
    "authors": [
      "Bronson Schoen",
      "Evgenia Nitishinskaya",
      "Mikita Balesni",
      "Axel Højmark",
      "Felix Hofstätter",
      "Jérémy Scheurer",
      "Alexander Meinke",
      "Jason Wolfe",
      "Teun van der Weij",
      "Alex Lloyd",
      "Nicholas Goldowsky-Dill",
      "Angela Fan",
      "Andrei Matveiakin",
      "Rusheb Shah",
      "Marcus Williams",
      "Amelia Glaese",
      "Boaz Barak",
      "Wojciech Zaremba",
      "Marius Hobbhahn"
    ],
    "abstract": "Highly capable AI systems could secretly pursue misaligned goals -- what we call \"scheming\". Because a scheming AI would deliberately try to hide its misaligned goals and actions, measuring and mitigating scheming requires different strategies than are typically used in ML. We propose that assessing anti-scheming interventions requires at least (1) testing propensity to scheme on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming is driven by situational awareness, and (3) checking for robustness to pre-existing misaligned goals. We use a broad category of \"covert actions\" -- such as secretly breaking rules or intentionally underperforming in tests -- as a proxy for scheming, and design evaluations for covert actions. We then stress-test deliberative alignment as a case study for anti-scheming. Across 26 OOD evaluations (180+ environments), deliberative alignment reduces covert action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our mitigation is also able to largely stop agents from pursuing a hidden goal previously trained into the model, but we still find misbehavior after additional red-teaming. We find that models' chain-of-thought (CoT) often demonstrates awareness of being evaluated for alignment, and show causal evidence that this awareness decreases covert behavior, while unawareness increases it. Therefore, we cannot exclude that the observed reductions in covert action rates are at least partially driven by situational awareness. While we rely on human-legible CoT for training, studying situational awareness, and demonstrating clear evidence of misalignment, our ability to rely on this degrades as models continue to depart from reasoning in standard English. We encourage research into alignment mitigations for scheming and their assessment, especially for the adversarial case of deceptive alignment, which this paper does not address.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15541v1",
    "published_date": "2025-09-19 02:49:56 UTC",
    "updated_date": "2025-09-19 02:49:56 UTC"
  },
  {
    "arxiv_id": "2509.15532v1",
    "title": "GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents",
    "authors": [
      "Xianhang Ye",
      "Yiqing Li",
      "Wei Dai",
      "Miancan Liu",
      "Ziyuan Chen",
      "Zhangye Han",
      "Hongbo Min",
      "Jinkui Ren",
      "Xiantao Zhang",
      "Wen Yang",
      "Zhi Jin"
    ],
    "abstract": "Existing GUI grounding methods often struggle with fine-grained localization in high-resolution screenshots. To address this, we propose GUI-ARP, a novel framework that enables adaptive multi-stage inference. Equipped with the proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC), GUI-ARP dynamically exploits visual attention for cropping task-relevant regions and adapts its inference strategy, performing a single-stage inference for simple cases and a multi-stage analysis for more complex scenarios. This is achieved through a two-phase training pipeline that integrates supervised fine-tuning with reinforcement fine-tuning based on Group Relative Policy Optimization (GRPO). Extensive experiments demonstrate that the proposed GUI-ARP achieves state-of-the-art performance on challenging GUI grounding benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9% on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15532v1",
    "published_date": "2025-09-19 02:34:50 UTC",
    "updated_date": "2025-09-19 02:34:50 UTC"
  },
  {
    "arxiv_id": "2509.15518v1",
    "title": "How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages",
    "authors": [
      "Siyang Wu",
      "Zhewei Sun"
    ],
    "abstract": "Slang is a commonly used type of informal language that poses a daunting challenge to NLP systems. Recent advances in large language models (LLMs), however, have made the problem more approachable. While LLM agents are becoming more widely applied to intermediary tasks such as slang detection and slang interpretation, their generalizability and reliability are heavily dependent on whether these models have captured structural knowledge about slang that align well with human attested slang usages. To answer this question, we contribute a systematic comparison between human and machine-generated slang usages. Our evaluative framework focuses on three core aspects: 1) Characteristics of the usages that reflect systematic biases in how machines perceive slang, 2) Creativity reflected by both lexical coinages and word reuses employed by the slang usages, and 3) Informativeness of the slang usages when used as gold-standard examples for model distillation. By comparing human-attested slang usages from the Online Slang Dictionary (OSD) and slang generated by GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our results suggest that while LLMs have captured significant knowledge about the creative aspects of slang, such knowledge does not align with humans sufficiently to enable LLMs for extrapolative tasks such as linguistic analyses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15518v1",
    "published_date": "2025-09-19 01:49:27 UTC",
    "updated_date": "2025-09-19 01:49:27 UTC"
  },
  {
    "arxiv_id": "2509.19366v1",
    "title": "Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data",
    "authors": [
      "Buhe Li",
      "Berkay Kaplan",
      "Maksym Lazirko",
      "Aleksandr Kogan"
    ],
    "abstract": "This study investigates the effectiveness of unsupervised outlier detection methods in audit analytics, utilizing USA spending data from the U.S. Department of Health and Human Services (DHHS) as a case example. We employ and compare multiple outlier detection algorithms, including Histogram-based Outlier Score (HBOS), Robust Principal Component Analysis (PCA), Minimum Covariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identify anomalies in federal spending patterns. The research addresses the growing need for efficient and accurate anomaly detection in large-scale governmental datasets, where traditional auditing methods may fall short. Our methodology involves data preparation, algorithm implementation, and performance evaluation using precision, recall, and F1 scores. Results indicate that a hybrid approach, combining multiple detection strategies, enhances the robustness and accuracy of outlier identification in complex financial data. This study contributes to the field of audit analytics by providing insights into the comparative effectiveness of various outlier detection models and demonstrating the potential of unsupervised learning techniques in improving audit quality and efficiency. The findings have implications for auditors, policymakers, and researchers seeking to leverage advanced analytics in governmental financial oversight and risk management.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.19366v1",
    "published_date": "2025-09-19 01:27:18 UTC",
    "updated_date": "2025-09-19 01:27:18 UTC"
  },
  {
    "arxiv_id": "2509.15510v1",
    "title": "The (Short-Term) Effects of Large Language Models on Unemployment and Earnings",
    "authors": [
      "Danqing Chen",
      "Carina Kane",
      "Austin Kozlowski",
      "Nadav Kunievsky",
      "James A. Evans"
    ],
    "abstract": "Large Language Models have spread rapidly since the release of ChatGPT in late 2022, accompanied by claims of major productivity gains but also concerns about job displacement. This paper examines the short-run labor market effects of LLM adoption by comparing earnings and unemployment across occupations with differing levels of exposure to these technologies. Using a Synthetic Difference in Differences approach, we estimate the impact of LLM exposure on earnings and unemployment. Our findings show that workers in highly exposed occupations experienced earnings increases following ChatGPT's introduction, while unemployment rates remained unchanged. These results suggest that initial labor market adjustments to LLMs operate primarily through earnings rather than worker reallocation.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.15510v1",
    "published_date": "2025-09-19 01:20:28 UTC",
    "updated_date": "2025-09-19 01:20:28 UTC"
  }
]
[
  {
    "arxiv_id": "2510.21049v1",
    "title": "Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection",
    "authors": [
      "Atoosa Chegini",
      "Hamid Kazemi",
      "Garrett Souza",
      "Maria Safi",
      "Yang Song",
      "Samy Bengio",
      "Sinead Williamson",
      "Mehrdad Farajtabar"
    ],
    "abstract": "Reasoning has become a central paradigm for large language models (LLMs), consistently boosting accuracy across diverse benchmarks. Yet its suitability for precision-sensitive tasks remains unclear. We present the first systematic study of reasoning for classification tasks under strict low false positive rate (FPR) regimes. Our analysis covers two tasks--safety detection and hallucination detection--evaluated in both fine-tuned and zero-shot settings, using standard LLMs and Large Reasoning Models (LRMs). Our results reveal a clear trade-off: Think On (reasoning-augmented) generation improves overall accuracy, but underperforms at the low-FPR thresholds essential for practical use. In contrast, Think Off (no reasoning during inference) dominates in these precision-sensitive regimes, with Think On surpassing only when higher FPRs are acceptable. In addition, we find token-based scoring substantially outperforms self-verbalized confidence for precision-sensitive deployments. Finally, a simple ensemble of the two modes recovers the strengths of each. Taken together, our findings position reasoning as a double-edged tool: beneficial for average accuracy, but often ill-suited for applications requiring strict precision.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21049v1",
    "published_date": "2025-10-23 23:23:36 UTC",
    "updated_date": "2025-10-23 23:23:36 UTC"
  },
  {
    "arxiv_id": "2511.01885v2",
    "title": "Mirror-Neuron Patterns in AI Alignment",
    "authors": [
      "Robyn Wyrick"
    ],
    "abstract": "As artificial intelligence (AI) advances toward superhuman capabilities, aligning these systems with human values becomes increasingly critical. Current alignment strategies rely largely on externally specified constraints that may prove insufficient against future super-intelligent AI capable of circumventing top-down controls.\n  This research investigates whether artificial neural networks (ANNs) can develop patterns analogous to biological mirror neurons cells that activate both when performing and observing actions, and how such patterns might contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in empathy, imitation, and social cognition in humans. The study therefore asks: (1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these patterns contribute to ethical and cooperative decision-making in AI systems?\n  Using a novel Frog and Toad game framework designed to promote cooperative behaviors, we identify conditions under which mirror-neuron patterns emerge, evaluate their influence on action circuits, introduce the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency, and propose a theoretical framework for further study.\n  Our findings indicate that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior and suggest that intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "51 pages, Masters thesis. 10 tables, 7 figures, project data & code here: https://github.com/robynwyrick/mirror-neuron-frog-and-toad",
    "pdf_url": "https://arxiv.org/pdf/2511.01885v2",
    "published_date": "2025-10-23 23:08:29 UTC",
    "updated_date": "2025-11-05 03:04:25 UTC"
  },
  {
    "arxiv_id": "2510.21045v2",
    "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL",
    "authors": [
      "Ali Khosravi Kazazi",
      "Zhenlong Li",
      "M. Naser Lessani",
      "Guido Cervone"
    ],
    "abstract": "The complexity of Structured Query Language (SQL) and the specialized nature of geospatial functions in tools like PostGIS present significant barriers to non-experts seeking to analyze spatial data. While Large Language Models (LLMs) offer promise for translating natural language into SQL (Text-to-SQL), single-agent approaches often struggle with the semantic and syntactic complexities of spatial queries. To address this, we propose a multi-agent framework designed to accurately translate natural language questions into spatial SQL queries. The framework integrates several innovative components, including a knowledge base with programmatic schema profiling and semantic enrichment, embeddings for context retrieval, and a collaborative multi-agent pipeline as its core. This pipeline comprises specialized agents for entity extraction, metadata retrieval, query logic formulation, SQL generation, and a review agent that performs programmatic and semantic validation of the generated SQL to ensure correctness (self-verification). We evaluate our system using both the non-spatial KaggleDBQA benchmark and a new, comprehensive SpatialQueryQA benchmark that includes diverse geometry types, predicates, and three levels of query complexity. On KaggleDBQA, the system achieved an overall accuracy of 81.2% (221 out of 272 questions) after the review agent's review and corrections. For spatial queries, the system achieved an overall accuracy of 87.7% (79 out of 90 questions), compared with 76.7% without the review agent. Beyond accuracy, results also show that in some instances the system generates queries that are more semantically aligned with user intent than those in the benchmarks. This work makes spatial analysis more accessible, and provides a robust, generalizable foundation for spatial Text-to-SQL systems, advancing the development of autonomous GIS.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21045v2",
    "published_date": "2025-10-23 22:58:17 UTC",
    "updated_date": "2025-11-11 23:48:52 UTC"
  },
  {
    "arxiv_id": "2510.21043v1",
    "title": "Epistemic Deference to AI",
    "authors": [
      "Benjamin Lange"
    ],
    "abstract": "When should we defer to AI outputs over human expert judgment? Drawing on recent work in social epistemology, I motivate the idea that some AI systems qualify as Artificial Epistemic Authorities (AEAs) due to their demonstrated reliability and epistemic superiority. I then introduce AI Preemptionism, the view that AEA outputs should replace rather than supplement a user's independent epistemic reasons. I show that classic objections to preemptionism - such as uncritical deference, epistemic entrenchment, and unhinging epistemic bases - apply in amplified form to AEAs, given their opacity, self-reinforcing authority, and lack of epistemic failure markers. Against this, I develop a more promising alternative: a total evidence view of AI deference. According to this view, AEA outputs should function as contributory reasons rather than outright replacements for a user's independent epistemic considerations. This approach has three key advantages: (i) it mitigates expertise atrophy by keeping human users engaged, (ii) it provides an epistemic case for meaningful human oversight and control, and (iii) it explains the justified mistrust of AI when reliability conditions are unmet. While demanding in practice, this account offers a principled way to determine when AI deference is justified, particularly in high-stakes contexts requiring rigorous reliability.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.21043v1",
    "published_date": "2025-10-23 22:55:51 UTC",
    "updated_date": "2025-10-23 22:55:51 UTC"
  },
  {
    "arxiv_id": "2511.01884v2",
    "title": "CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization",
    "authors": [
      "Zijian Zhang",
      "Rong Wang",
      "Shiyang Li",
      "Yuebo Luo",
      "Mingyi Hong",
      "Caiwen Ding"
    ],
    "abstract": "Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\\% correctness of generated kernels and an average 1.68$\\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench.Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \\$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \\$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at https://github.com/OptimAI-Lab/CudaForge",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.01884v2",
    "published_date": "2025-10-23 22:52:00 UTC",
    "updated_date": "2025-11-05 02:10:35 UTC"
  },
  {
    "arxiv_id": "2510.21031v1",
    "title": "AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents",
    "authors": [
      "Qinghua Lu",
      "Dehai Zhao",
      "Yue Liu",
      "Hao Zhang",
      "Liming Zhu",
      "Xiwei Xu",
      "Angela Shi",
      "Tristan Tan",
      "Rick Kazman"
    ],
    "abstract": "The emergence of foundation models (FMs) has enabled the development of highly capable and autonomous agents, unlocking new application opportunities across a wide range of domains. Evaluating the architecture of agents is particularly important as the architectural decisions significantly impact the quality attributes of agents given their unique characteristics, including compound architecture, autonomous and non-deterministic behaviour, and continuous evolution. However, these traditional methods fall short in addressing the evaluation needs of agent architecture due to the unique characteristics of these agents. Therefore, in this paper, we present AgentArcEval, a novel agent architecture evaluation method designed specially to address the complexities of FM-based agent architecture and its evaluation. Moreover, we present a catalogue of agent-specific general scenarios, which serves as a guide for generating concrete scenarios to design and evaluate the agent architecture. We demonstrate the usefulness of AgentArcEval and the catalogue through a case study on the architecture evaluation of a real-world tax copilot, named Luna.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21031v1",
    "published_date": "2025-10-23 22:32:03 UTC",
    "updated_date": "2025-10-23 22:32:03 UTC"
  },
  {
    "arxiv_id": "2510.21027v1",
    "title": "Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems",
    "authors": [
      "Zhe Fei",
      "Mehmet Yigit Turali",
      "Shreyas Rajesh",
      "Xinyang Dai",
      "Huyen Pham",
      "Pavan Holur",
      "Yuhui Zhu",
      "Larissa Mooney",
      "Yih-Ing Hser",
      "Vwani Roychowdhury"
    ],
    "abstract": "Harmonizing medication data across Electronic Health Record (EHR) systems is a persistent barrier to monitoring medications for opioid use disorder (MOUD). In heterogeneous EHR systems, key prescription attributes are scattered across differently formatted fields and freetext notes. We present a practical framework that customizes open source large language models (LLMs), including Llama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription attributes (prescription date, drug name, duration, total quantity, daily quantity, and refills) from heterogeneous, site specific data and compute a standardized metric of medication coverage, \\emph{MOUD days}, per patient. Our pipeline processes records directly in a fixed JSON schema, followed by lightweight normalization and cross-field consistency checks. We evaluate the system on prescription level EHR data from five clinics in a national OUD study (25{,}605 records from 1{,}257 patients), using a previously annotated benchmark of 10{,}369 records (776 patients) as the ground truth. Performance is reported as coverage (share of records with a valid, matchable output) and record-level exact-match accuracy. Larger models perform best overall: Qwen2.5-32B achieves \\textbf{93.4\\%} coverage with \\textbf{93.0\\%} exact-match accuracy across clinics, and MedGemma-27B attains \\textbf{93.1\\%}/\\textbf{92.2\\%}. A brief error review highlights three common issues and fixes: imputing missing dosage fields using within-drug norms, handling monthly/weekly injectables (e.g., Vivitrol) by setting duration from the documented schedule, and adding unit checks to prevent mass units (e.g., ``250 g'') from being misread as daily counts. By removing brittle, site-specific ETL and supporting local, privacy-preserving deployment, this approach enables consistent cross-site analyses of MOUD exposure, adherence, and retention in real-world settings.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025: The Second Workshop on GenAI for Health: Potential, Trust, and Policy Compliance",
    "pdf_url": "https://arxiv.org/pdf/2510.21027v1",
    "published_date": "2025-10-23 22:27:10 UTC",
    "updated_date": "2025-10-23 22:27:10 UTC"
  },
  {
    "arxiv_id": "2510.23627v1",
    "title": "AI-Driven Development of a Publishing Imprint: Xynapse Traces",
    "authors": [
      "Fred Zimmerman"
    ],
    "abstract": "Xynapse Traces is an experimental publishing imprint created via a fusion of human and algorithmic methods using a configuration-driven architecture and a multi-model AI integration framework. The system achieved a remarkable 90% reduction in time-to-market (from a typical 6-12 months to just 2-4 weeks), with 80% cost reduction compared to traditional imprint development, while publishing 52 books in its first year and maintaining exceptional quality metrics, including 99% citation accuracy and 100% validation success after initial corrections. Key technical innovations include a continuous ideation pipeline with tournament-style evaluation, a novel codex design for transcriptive meditation practice, comprehensive automation spanning from ideation through production and distribution, and publisher personas that define and guide the imprint's mission. The system also integrates automated verification with human oversight, ensuring that gains in speed do not compromise publishing standards. This effort has significant implications for the future of book publishing, suggesting new paradigms for human-AI collaboration that democratize access to sophisticated publishing capabilities and make previously unviable niche markets accessible.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23627v1",
    "published_date": "2025-10-23 22:25:13 UTC",
    "updated_date": "2025-10-23 22:25:13 UTC"
  },
  {
    "arxiv_id": "2510.21024v1",
    "title": "JSTprove: Pioneering Verifiable AI for a Trustless Future",
    "authors": [
      "Jonathan Gold",
      "Tristan Freiberg",
      "Haruna Isah",
      "Shirin Shahabi"
    ],
    "abstract": "The integration of machine learning (ML) systems into critical industries such as healthcare, finance, and cybersecurity has transformed decision-making processes, but it also brings new challenges around trust, security, and accountability. As AI systems become more ubiquitous, ensuring the transparency and correctness of AI-driven decisions is crucial, especially when they have direct consequences on privacy, security, or fairness. Verifiable AI, powered by Zero-Knowledge Machine Learning (zkML), offers a robust solution to these challenges. zkML enables the verification of AI model inferences without exposing sensitive data, providing an essential layer of trust and privacy. However, traditional zkML systems typically require deep cryptographic expertise, placing them beyond the reach of most ML engineers. In this paper, we introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's Expander backend, to enable AI developers and ML engineers to generate and verify proofs of AI inference. JSTprove provides an end-to-end verifiable AI inference pipeline that hides cryptographic complexity behind a simple command-line interface while exposing auditable artifacts for reproducibility. We present the design, innovations, and real-world use cases of JSTprove as well as our blueprints and tooling to encourage community review and extension. JSTprove therefore serves both as a usable zkML product for current engineering needs and as a reproducible foundation for future research and production deployments of verifiable AI.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, 8 figures, and 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.21024v1",
    "published_date": "2025-10-23 22:22:38 UTC",
    "updated_date": "2025-10-23 22:22:38 UTC"
  },
  {
    "arxiv_id": "2510.21023v1",
    "title": "Physically consistent and uncertainty-aware learning of spatiotemporal dynamics",
    "authors": [
      "Qingsong Xu",
      "Jonathan L Bamber",
      "Nils Thuerey",
      "Niklas Boers",
      "Paul Bates",
      "Gustau Camps-Valls",
      "Yilei Shi",
      "Xiao Xiang Zhu"
    ],
    "abstract": "Accurate long-term forecasting of spatiotemporal dynamics remains a fundamental challenge across scientific and engineering domains. Existing machine learning methods often neglect governing physical laws and fail to quantify inherent uncertainties in spatiotemporal predictions. To address these challenges, we introduce a physics-consistent neural operator (PCNO) that enforces physical constraints by projecting surrogate model outputs onto function spaces satisfying predefined laws. A physics-consistent projection layer within PCNO efficiently computes mass and momentum conservation in Fourier space. Building upon deterministic predictions, we further propose a diffusion model-enhanced PCNO (DiffPCNO), which leverages a consistency model to quantify and mitigate uncertainties, thereby improving the accuracy and reliability of forecasts. PCNO and DiffPCNO achieve high-fidelity spatiotemporal predictions while preserving physical consistency and uncertainty across diverse systems and spatial resolutions, ranging from turbulent flow modeling to real-world flood/atmospheric forecasting. Our two-stage framework provides a robust and versatile approach for accurate, physically grounded, and uncertainty-aware spatiotemporal forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "Main text:33 pages,6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21023v1",
    "published_date": "2025-10-23 22:17:21 UTC",
    "updated_date": "2025-10-23 22:17:21 UTC"
  },
  {
    "arxiv_id": "2510.21011v1",
    "title": "Race and Gender in LLM-Generated Personas: A Large-Scale Audit of 41 Occupations",
    "authors": [
      "Ilona van der Linden",
      "Sahana Kumar",
      "Arnav Dixit",
      "Aadi Sudan",
      "Smruthi Danda",
      "David C. Anastasiu",
      "Kai Lukoff"
    ],
    "abstract": "Generative AI tools are increasingly used to create portrayals of people in occupations, raising concerns about how race and gender are represented. We conducted a large-scale audit of over 1.5 million occupational personas across 41 U.S. occupations, generated by four large language models with different AI safety commitments and countries of origin (U.S., China, France). Compared with Bureau of Labor Statistics data, we find two recurring patterns: systematic shifts, where some groups are consistently under- or overrepresented, and stereotype exaggeration, where existing demographic skews are amplified. On average, White (--31pp) and Black (--9pp) workers are underrepresented, while Hispanic (+17pp) and Asian (+12pp) workers are overrepresented. These distortions can be extreme: for example, across all four models, Housekeepers are portrayed as nearly 100\\% Hispanic, while Black workers are erased from many occupations. For HCI, these findings show provider choice materially changes who is visible, motivating model-specific audits and accountable design practices.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21011v1",
    "published_date": "2025-10-23 21:43:08 UTC",
    "updated_date": "2025-10-23 21:43:08 UTC"
  },
  {
    "arxiv_id": "2510.21887v1",
    "title": "Generative AI in Depth: A Survey of Recent Advances, Model Variants, and Real-World Applications",
    "authors": [
      "Shamim Yazdani",
      "Akansha Singh",
      "Nripsuta Saxena",
      "Zichong Wang",
      "Avash Palikhe",
      "Deng Pan",
      "Umapada Pal",
      "Jie Yang",
      "Wenbin Zhang"
    ],
    "abstract": "In recent years, deep learning based generative models, particularly Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion Models (DMs), have been instrumental in in generating diverse, high-quality content across various domains, such as image and video synthesis. This capability has led to widespread adoption of these models and has captured strong public interest. As they continue to advance at a rapid pace, the growing volume of research, expanding application areas, and unresolved technical challenges make it increasingly difficult to stay current. To address this need, this survey introduces a comprehensive taxonomy that organizes the literature and provides a cohesive framework for understanding the development of GANs, VAEs, and DMs, including their many variants and combined approaches. We highlight key innovations that have improved the quality, diversity, and controllability of generated outputs, reflecting the expanding potential of generative artificial intelligence. In addition to summarizing technical progress, we examine rising ethical concerns, including the risks of misuse and the broader societal impact of synthetic media. Finally, we outline persistent challenges and propose future research directions, offering a structured and forward looking perspective for researchers in this fast evolving field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by the Journal of Big Data",
    "pdf_url": "https://arxiv.org/pdf/2510.21887v1",
    "published_date": "2025-10-23 21:11:12 UTC",
    "updated_date": "2025-10-23 21:11:12 UTC"
  },
  {
    "arxiv_id": "2510.20997v1",
    "title": "Exploring Spiking Neural Networks for Binary Classification in Multivariate Time Series at the Edge",
    "authors": [
      "James Ghawaly",
      "Andrew Nicholson",
      "Catherine Schuman",
      "Dalton Diez",
      "Aaron Young",
      "Brett Witherspoon"
    ],
    "abstract": "We present a general framework for training spiking neural networks (SNNs) to perform binary classification on multivariate time series, with a focus on step-wise prediction and high precision at low false alarm rates. The approach uses the Evolutionary Optimization of Neuromorphic Systems (EONS) algorithm to evolve sparse, stateful SNNs by jointly optimizing their architectures and parameters. Inputs are encoded into spike trains, and predictions are made by thresholding a single output neuron's spike counts. We also incorporate simple voting ensemble methods to improve performance and robustness.\n  To evaluate the framework, we apply it with application-specific optimizations to the task of detecting low signal-to-noise ratio radioactive sources in gamma-ray spectral data. The resulting SNNs, with as few as 49 neurons and 66 synapses, achieve a 51.8% true positive rate (TPR) at a false alarm rate of 1/hr, outperforming PCA (42.7%) and deep learning (49.8%) baselines. A three-model any-vote ensemble increases TPR to 67.1% at the same false alarm rate. Hardware deployment on the microCaspian neuromorphic platform demonstrates 2mW power consumption and 20.2ms inference latency.\n  We also demonstrate generalizability by applying the same framework, without domain-specific modification, to seizure detection in EEG recordings. An ensemble achieves 95% TPR with a 16% false positive rate, comparable to recent deep learning approaches with significant reduction in parameter count.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in 2025 International Joint Conference on Neural Networks (IJCNN)",
    "pdf_url": "https://arxiv.org/pdf/2510.20997v1",
    "published_date": "2025-10-23 20:52:11 UTC",
    "updated_date": "2025-10-23 20:52:11 UTC"
  },
  {
    "arxiv_id": "2510.20994v1",
    "title": "VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models",
    "authors": [
      "Jesimon Barreto",
      "Carlos Caetano",
      "André Araujo",
      "William Robson Schwartz"
    ],
    "abstract": "Foundation models have advanced computer vision by enabling strong performance across diverse tasks through large-scale pretraining and supervised fine-tuning. However, they may underperform in domains with distribution shifts and scarce labels, where supervised fine-tuning may be infeasible. While continued self-supervised learning for model adaptation is common for generative language models, this strategy has not proven effective for vision-centric encoder models. To address this challenge, we introduce a novel formulation of self-supervised fine-tuning for vision foundation models, where the model is adapted to a new domain without requiring annotations, leveraging only short multi-view object-centric videos. Our method is referred to as VESSA: Video-based objEct-centric Self-Supervised Adaptation for visual foundation models. VESSA's training technique is based on a self-distillation paradigm, where it is critical to carefully tune prediction heads and deploy parameter-efficient adaptation techniques - otherwise, the model may quickly forget its pretrained knowledge and reach a degraded state. VESSA benefits significantly from multi-view object observations sourced from different frames in an object-centric video, efficiently learning robustness to varied capture conditions, without the need of annotations. Through comprehensive experiments with 3 vision foundation models on 2 datasets, VESSA demonstrates consistent improvements in downstream classification tasks, compared to the base models and previous adaptation methods. Code is publicly available at https://github.com/jesimonbarreto/VESSA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.20994v1",
    "published_date": "2025-10-23 20:44:28 UTC",
    "updated_date": "2025-10-23 20:44:28 UTC"
  },
  {
    "arxiv_id": "2510.21886v1",
    "title": "Exploration through Generation: Applying GFlowNets to Structured Search",
    "authors": [
      "Mark Phillip Matovic"
    ],
    "abstract": "This work applies Generative Flow Networks (GFlowNets) to three graph optimization problems: the Traveling Salesperson Problem, Minimum Spanning Tree, and Shortest Path. GFlowNets are generative models that learn to sample solutions proportionally to a reward function. The models are trained using the Trajectory Balance loss to build solutions sequentially, selecting edges for spanning trees, nodes for paths, and cities for tours. Experiments on benchmark instances of varying sizes show that GFlowNets learn to find optimal solutions. For each problem type, multiple graph configurations with different numbers of nodes were tested. The generated solutions match those from classical algorithms (Dijkstra for shortest path, Kruskal for spanning trees, and exact solvers for TSP). Training convergence depends on problem complexity, with the number of episodes required for loss stabilization increasing as graph size grows. Once training converges, the generated solutions match known optima from classical algorithms across the tested instances. This work demonstrates that generative models can solve combinatorial optimization problems through learned policies. The main advantage of this learning-based approach is computational scalability: while classical algorithms have fixed complexity per instance, GFlowNets amortize computation through training. With sufficient computational resources, the framework could potentially scale to larger problem instances where classical exact methods become infeasible.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.21886v1",
    "published_date": "2025-10-23 20:43:09 UTC",
    "updated_date": "2025-10-23 20:43:09 UTC"
  },
  {
    "arxiv_id": "2510.21885v1",
    "title": "Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning",
    "authors": [
      "Anh Pham",
      "Mihir Thalanki",
      "Michael Sun",
      "Aditya Chaloo",
      "Ankita Gupta",
      "Tian Xia",
      "Aditya Mate",
      "Ehimwenma Nosakhare",
      "Soundararajan Srinivasan"
    ],
    "abstract": "Large language models often lose previously aligned safety behaviors when fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior work shows that adding random safety examples can mitigate this effect, but it remains unclear which examples are most effective. We propose a behavior-aware sampling framework that selects safety examples based on two complementary factors: instruction-response behavior (e.g., refusal versus compliance) and semantic diversity across harm categories. Systematic evaluation shows that this approach substantially reduces harmful outputs while maintaining helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5% additional training data. These results highlight how targeted data selection can improve the safety and efficiency of fine-tuning at scale.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21885v1",
    "published_date": "2025-10-23 20:34:52 UTC",
    "updated_date": "2025-10-23 20:34:52 UTC"
  },
  {
    "arxiv_id": "2510.23626v1",
    "title": "From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media",
    "authors": [
      "Shuang Geng",
      "Wenli Zhang",
      "Jiaheng Xie",
      "Rui Wang",
      "Sudha Ram"
    ],
    "abstract": "Social media user-generated content (UGC) provides real-time, self-reported indicators of mental health conditions such as depression, offering a valuable source for predictive analytics. While prior studies integrate medical knowledge to improve prediction accuracy, they overlook the opportunity to simultaneously expand such knowledge through predictive processes. We develop a Closed-Loop Large Language Model (LLM)-Knowledge Graph framework that integrates prediction and knowledge expansion in an iterative learning cycle. In the knowledge-aware depression detection phase, the LLM jointly performs depression detection and entity extraction, while the knowledge graph represents and weights these entities to refine prediction performance. In the knowledge refinement and expansion phase, new entities, relationships, and entity types extracted by the LLM are incorporated into the knowledge graph under expert supervision, enabling continual knowledge evolution. Using large-scale UGC, the framework enhances both predictive accuracy and medical understanding. Expert evaluations confirmed the discovery of clinically meaningful symptoms, comorbidities, and social triggers complementary to existing literature. We conceptualize and operationalize prediction-through-learning and learning-through-prediction as mutually reinforcing processes, advancing both methodological and theoretical understanding in predictive analytics. The framework demonstrates the co-evolution of computational models and domain knowledge, offering a foundation for adaptive, data-driven knowledge systems applicable to other dynamic risk monitoring contexts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at SWAIB2025 and HICSS2026",
    "pdf_url": "https://arxiv.org/pdf/2510.23626v1",
    "published_date": "2025-10-23 20:34:36 UTC",
    "updated_date": "2025-10-23 20:34:36 UTC"
  },
  {
    "arxiv_id": "2510.21884v1",
    "title": "Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks",
    "authors": [
      "Avinash Patil"
    ],
    "abstract": "The growing adoption of machine learning (ML) in sensitive domains has heightened the demand for transparent and interpretable artificial intelligence. Large Language Models (LLMs) are increasingly capable of producing natural language explanations, yet it remains unclear whether these rationales faithfully capture the predictive signals that underlie decisions. This paper introduces RACE-Reasoning Alignment for Completeness of Explanations, a systematic framework to evaluate the alignment between LLM-generated explanations and interpretable feature importance scores derived from a logistic regression baseline. We analyze four widely used text classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and compare LLM rationales against top-ranked supporting and contradicting lexical features. To capture alignment at multiple levels of granularity, RACE implements token-aware, exact string, and edit-distance matching techniques. Empirical results reveal a consistent asymmetry: correct predictions exhibit higher coverage of supporting features, while incorrect predictions are associated with elevated coverage of contradicting features. Edit-distance matching further uncovers paraphrastic overlaps, boosting coverage while preserving this asymmetry. These findings demonstrate that LLM rationales combine both surface-level and flexible evidence reuse, yet can also amplify misleading cues in error cases. RACE provides new insights into the faithfulness of LLM explanations and establishes a quantitative basis for evaluating reasoning completeness in neural language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 Pages, 12 Figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.21884v1",
    "published_date": "2025-10-23 20:22:22 UTC",
    "updated_date": "2025-10-23 20:22:22 UTC"
  },
  {
    "arxiv_id": "2510.20985v1",
    "title": "GPU Memory Requirement Prediction for Deep Learning Task Based on Bidirectional Gated Recurrent Unit Optimization Transformer",
    "authors": [
      "Chao Wang",
      "Zhizhao Wen",
      "Ruoxin Zhang",
      "Puyang Xu",
      "Yifan Jiang"
    ],
    "abstract": "In response to the increasingly critical demand for accurate prediction of GPU memory resources in deep learning tasks, this paper deeply analyzes the current research status and innovatively proposes a deep learning model that integrates bidirectional gated recurrent units (BiGRU) to optimize the Transformer architecture, aiming to improve the accuracy of memory demand prediction. To verify the effectiveness of the model, a carefully designed comparative experiment was conducted, selecting four representative basic machine learning models: decision tree, random forest, Adaboost, and XGBoost as benchmarks. The detailed experimental results show that the BiGRU Transformer optimization model proposed in this paper exhibits significant advantages in key evaluation indicators: in terms of mean square error (MSE) and root mean square error (RMSE), the model achieves the lowest value among all comparison models, and its predicted results have the smallest deviation from the actual values; In terms of mean absolute error (MAE) and coefficient of determination (R2) indicators, the model also performs well and the results are balanced and stable, with comprehensive predictive performance far exceeding the benchmark machine learning methods compared. In summary, the Transformer model based on bidirectional gated recurrent unit optimization successfully constructed in this study can efficiently and accurately complete GPU memory demand prediction tasks in deep learning tasks, and its prediction accuracy has been significantly improved compared to traditional machine learning methods. This research provides strong technical support and reliable theoretical basis for optimizing resource scheduling and management of deep learning tasks, and improving the utilization efficiency of computing clusters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20985v1",
    "published_date": "2025-10-23 20:20:35 UTC",
    "updated_date": "2025-10-23 20:20:35 UTC"
  },
  {
    "arxiv_id": "2510.20984v1",
    "title": "Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression",
    "authors": [
      "Xi Zhang",
      "Xiaolin Wu",
      "Jiamang Wang",
      "Weisi Lin"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but typically require extensive computational resources and memory for inference. Post-training quantization (PTQ) can effectively reduce these demands by storing weights in lower bit-width formats. However, standard uniform quantization often leads to notable performance degradation, particularly in low-bit scenarios. In this work, we introduce a Grouped Lattice Vector Quantization (GLVQ) framework that assigns each group of weights a customized lattice codebook, defined by a learnable generation matrix. To address the non-differentiability of the quantization process, we adopt Babai rounding to approximate nearest-lattice-point search during training, which enables stable optimization of the generation matrices. Once trained, decoding reduces to a simple matrix-vector multiplication, yielding an efficient and practical quantization pipeline. Experiments on multiple benchmarks show that our approach achieves a better trade-off between model size and accuracy compared to existing post-training quantization baselines, highlighting its effectiveness in deploying large models under stringent resource constraints. Our source code is available on GitHub repository: https://github.com/xzhang9308/GLVQ.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 Poster",
    "pdf_url": "https://arxiv.org/pdf/2510.20984v1",
    "published_date": "2025-10-23 20:19:48 UTC",
    "updated_date": "2025-10-23 20:19:48 UTC"
  },
  {
    "arxiv_id": "2510.20979v1",
    "title": "Memory Constrained Dynamic Subnetwork Update for Transfer Learning",
    "authors": [
      "Aël Quélennec",
      "Pavlo Mozharovskyi",
      "Van-Tam Nguyen",
      "Enzo Tartaglione"
    ],
    "abstract": "On-device neural network training faces critical memory constraints that limit the adaptation of pre-trained models to downstream tasks. We present MeDyate, a theoretically-grounded framework for memory-constrained dynamic subnetwork adaptation. Our approach introduces two key innovations: LaRa (Layer Ranking), an improved layer importance metric that enables principled layer pre-selection, and a dynamic channel sampling strategy that exploits the temporal stability of channel importance distributions during fine-tuning. MeDyate dynamically resamples channels between epochs according to importance-weighted probabilities, ensuring comprehensive parameter space exploration while respecting strict memory budgets. Extensive evaluation across a large panel of tasks and architectures demonstrates that MeDyate achieves state-of-the-art performance under extreme memory constraints, consistently outperforming existing static and dynamic approaches while maintaining high computational efficiency. Our method represents a significant step towards enabling efficient on-device learning by demonstrating effective fine-tuning with memory budgets as low as a few hundred kB of RAM.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20979v1",
    "published_date": "2025-10-23 20:16:43 UTC",
    "updated_date": "2025-10-23 20:16:43 UTC"
  },
  {
    "arxiv_id": "2510.20975v1",
    "title": "REx86: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering",
    "authors": [
      "Darrin Lea",
      "James Ghawaly",
      "Golden Richard",
      "Aisha Ali-Gombe",
      "Andrew Case"
    ],
    "abstract": "Reverse engineering (RE) of x86 binaries is indispensable for malware and firmware analysis, but remains slow due to stripped metadata and adversarial obfuscation. Large Language Models (LLMs) offer potential for improving RE efficiency through automated comprehension and commenting, but cloud-hosted, closed-weight models pose privacy and security risks and cannot be used in closed-network facilities. We evaluate parameter-efficient fine-tuned local LLMs for assisting with x86 RE tasks in these settings. Eight open-weight models across the CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned on a custom curated dataset of 5,981 x86 assembly examples. We evaluate them quantitatively and identify the fine-tuned Qwen2.5-Coder-7B as the top performer, which we name REx86.\n  REx86 reduces test-set cross-entropy loss by 64.2% and improves semantic cosine similarity against ground truth by 20.3\\% over its base model. In a limited user case study (n=43), REx86 significantly enhanced line-level code understanding (p = 0.031) and increased the correct-solve rate from 31% to 53% (p = 0.189), though the latter did not reach statistical significance. Qualitative analysis shows more accurate, concise comments with fewer hallucinations.\n  REx86 delivers state-of-the-art assistance in x86 RE among local, open-weight LLMs. Our findings demonstrate the value of domain-specific fine-tuning, and highlight the need for more commented disassembly data to further enhance LLM performance in RE. REx86, its dataset, and LoRA adapters are publicly available at https://github.com/dlea8/REx86 and https://zenodo.org/records/15420461.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted in 2025 Annual Computer Security Applications Conference (ACSAC)",
    "pdf_url": "https://arxiv.org/pdf/2510.20975v1",
    "published_date": "2025-10-23 20:09:21 UTC",
    "updated_date": "2025-10-23 20:09:21 UTC"
  },
  {
    "arxiv_id": "2510.20967v1",
    "title": "3DReasonKnee: Advancing Grounded Reasoning in Medical Vision Language Models",
    "authors": [
      "Sraavya Sambara",
      "Sung Eun Kim",
      "Xiaoman Zhang",
      "Luyang Luo",
      "Shreya Johri",
      "Mohammed Baharoon",
      "Du Hyun Ro",
      "Pranav Rajpurkar"
    ],
    "abstract": "Current Vision-Language Models (VLMs) struggle to ground anatomical regions in 3D medical images and reason about them in a step-by-step manner, a key requirement of real-world diagnostic assessment. This ability is essential for aligning model outputs with the diagnostic workflows clinicians use in practice, enabling trustworthy clinician-AI collaboration. Existing 3D datasets provide localization labels, but none support this \"grounded reasoning\" ability. To address this gap, we introduce 3DReasonKnee, the first 3D grounded reasoning dataset for medical images, which provides 494k high-quality quintuples derived from 7,970 3D knee MRI volumes. Each quintuple includes: (1) the 3D MRI volume, (2) a diagnostic question targeting a specific anatomical region (3) a 3D bounding box localizing the relevant anatomical structures, (4) clinician-generated diagnostic reasoning steps that explicitly detail the 3D reasoning process, and (5) structured severity assessments for the relevant anatomical region. The creation and validation of 3DReasonKnee, involving over 450 hours of expert clinician time for manually segmenting MRIs and generating reasoning chains, ensures its superior quality and clinical relevance. We establish ReasonKnee-Bench to evaluate localization and diagnostic accuracy, providing insight into VLM ability to perform grounding and severity assessment across anatomical regions and diagnostic inquiries. We benchmark five state-of-the-art VLMs, providing baseline performance for ReasonKnee-Bench. By providing this unique resource of expert-annotated 3D reasoning pathways, 3DReasonKnee serves as a repository of orthopedic surgeons' diagnostic expertise and offers a vital testbed for advancing multimodal medical AI systems towards 3D, clinically aligned, localized decision-making capabilities. The dataset can be found in: https://huggingface.co/datasets/rajpurkarlab/3DReasonKnee",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20967v1",
    "published_date": "2025-10-23 19:54:49 UTC",
    "updated_date": "2025-10-23 19:54:49 UTC"
  },
  {
    "arxiv_id": "2510.20943v1",
    "title": "Meta-Learning for Cross-Task Generalization in Protein Mutation Property Prediction",
    "authors": [
      "Srivathsan Badrinarayanan",
      "Yue Su",
      "Janghoon Ock",
      "Alan Pham",
      "Sanya Ahuja",
      "Amir Barati Farimani"
    ],
    "abstract": "Protein mutations can have profound effects on biological function, making accurate prediction of property changes critical for drug discovery, protein engineering, and precision medicine. Current approaches rely on fine-tuning protein-specific transformers for individual datasets, but struggle with cross-dataset generalization due to heterogeneous experimental conditions and limited target domain data. We introduce two key innovations: (1) the first application of Model-Agnostic Meta-Learning (MAML) to protein mutation property prediction, and (2) a novel mutation encoding strategy using separator tokens to directly incorporate mutations into sequence context. We build upon transformer architectures integrating them with MAML to enable rapid adaptation to new tasks through minimal gradient steps rather than learning dataset-specific patterns. Our mutation encoding addresses the critical limitation where standard transformers treat mutation positions as unknown tokens, significantly degrading performance. Evaluation across three diverse protein mutation datasets (functional fitness, thermal stability, and solubility) demonstrates significant advantages over traditional fine-tuning. In cross-task evaluation, our meta-learning approach achieves 29% better accuracy for functional fitness with 65% less training time, and 94% better accuracy for solubility with 55% faster training. The framework maintains consistent training efficiency regardless of dataset size, making it particularly valuable for industrial applications and early-stage protein design where experimental data is limited. This work establishes a systematic application of meta-learning to protein mutation analysis and introduces an effective mutation encoding strategy, offering transformative methodology for cross-domain generalization in protein engineering.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20943v1",
    "published_date": "2025-10-23 19:09:06 UTC",
    "updated_date": "2025-10-23 19:09:06 UTC"
  },
  {
    "arxiv_id": "2510.20941v1",
    "title": "Do LLMs Truly Understand When a Precedent Is Overruled?",
    "authors": [
      "Li Zhang",
      "Jaromir Savelka",
      "Kevin Ashley"
    ],
    "abstract": "Large language models (LLMs) with extended context windows show promise for complex legal reasoning tasks, yet their ability to understand long legal documents remains insufficiently evaluated. Developing long-context benchmarks that capture realistic, high-stakes tasks remains a significant challenge in the field, as most existing evaluations rely on simplified synthetic tasks that fail to represent the complexity of real-world document understanding. Overruling relationships are foundational to common-law doctrine and commonly found in judicial opinions. They provide a focused and important testbed for long-document legal understanding that closely resembles what legal professionals actually do. We present an assessment of state-of-the-art LLMs on identifying overruling relationships from U.S. Supreme Court cases using a dataset of 236 case pairs. Our evaluation reveals three critical limitations: (1) era sensitivity -- the models show degraded performance on historical cases compared to modern ones, revealing fundamental temporal bias in their training; (2) shallow reasoning -- models rely on shallow logical heuristics rather than deep legal comprehension; and (3) context-dependent reasoning failures -- models produce temporally impossible relationships in complex open-ended tasks despite maintaining basic temporal awareness in simple contexts. Our work contributes a benchmark that addresses the critical gap in realistic long-context evaluation, providing an environment that mirrors the complexity and stakes of actual legal reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 2 figures, JURIX 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20941v1",
    "published_date": "2025-10-23 19:07:42 UTC",
    "updated_date": "2025-10-23 19:07:42 UTC"
  },
  {
    "arxiv_id": "2510.20933v1",
    "title": "Focal Modulation and Bidirectional Feature Fusion Network for Medical Image Segmentation",
    "authors": [
      "Moin Safdar",
      "Shahzaib Iqbal",
      "Mehwish Mehmood",
      "Mubeen Ghafoor",
      "Tariq M. Khan",
      "Imran Razzak"
    ],
    "abstract": "Medical image segmentation is essential for clinical applications such as disease diagnosis, treatment planning, and disease development monitoring because it provides precise morphological and spatial information on anatomical structures that directly influence treatment decisions. Convolutional neural networks significantly impact image segmentation; however, since convolution operations are local, capturing global contextual information and long-range dependencies is still challenging. Their capacity to precisely segment structures with complicated borders and a variety of sizes is impacted by this restriction. Since transformers use self-attention methods to capture global context and long-range dependencies efficiently, integrating transformer-based architecture with CNNs is a feasible approach to overcoming these challenges. To address these challenges, we propose the Focal Modulation and Bidirectional Feature Fusion Network for Medical Image Segmentation, referred to as FM-BFF-Net in the remainder of this paper. The network combines convolutional and transformer components, employs a focal modulation attention mechanism to refine context awareness, and introduces a bidirectional feature fusion module that enables efficient interaction between encoder and decoder representations across scales. Through this design, FM-BFF-Net enhances boundary precision and robustness to variations in lesion size, shape, and contrast. Extensive experiments on eight publicly available datasets, including polyp detection, skin lesion segmentation, and ultrasound imaging, show that FM-BFF-Net consistently surpasses recent state-of-the-art methods in Jaccard index and Dice coefficient, confirming its effectiveness and adaptability for diverse medical imaging scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20933v1",
    "published_date": "2025-10-23 18:52:24 UTC",
    "updated_date": "2025-10-23 18:52:24 UTC"
  },
  {
    "arxiv_id": "2510.20932v1",
    "title": "An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing",
    "authors": [
      "Reza Ahmari",
      "Ahmad Mohammadi",
      "Vahid Hemmati",
      "Mohammed Mynuddin",
      "Mahmoud Nabil Mahmoud",
      "Parham Kebria",
      "Abdollah Homaifar",
      "Mehrdad Saif"
    ],
    "abstract": "This study investigates the vulnerabilities of autonomous navigation and landing systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses on Trojan attacks that target deep learning models, such as Convolutional Neural Networks (CNNs). Trojan attacks work by embedding covert triggers within a model's training data. These triggers cause specific failures under certain conditions, while the model continues to perform normally in other situations. We assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using the DroNet framework. Our experiments showed a significant drop in accuracy, from 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To conduct this study, we collected a custom dataset and trained models to simulate real-world conditions. We also developed an evaluation framework designed to identify Trojan-infected models. This work demonstrates the potential security risks posed by Trojan attacks and lays the groundwork for future research on enhancing the resilience of UAM systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.20932v1",
    "published_date": "2025-10-23 18:47:40 UTC",
    "updated_date": "2025-10-23 18:47:40 UTC"
  },
  {
    "arxiv_id": "2510.20930v1",
    "title": "Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference",
    "authors": [
      "Soham Hans",
      "Stacy Marsella",
      "Sophia Hirschmann",
      "Nikolos Gurney"
    ],
    "abstract": "Understanding adversarial behavior in cybersecurity has traditionally relied on high-level intelligence reports and manual interpretation of attack chains. However, real-time defense requires the ability to infer attacker intent and cognitive strategy directly from low-level system telemetry such as intrusion detection system (IDS) logs. In this paper, we propose a novel framework that leverages large language models (LLMs) to analyze Suricata IDS logs and infer attacker actions in terms of MITRE ATT&CK techniques. Our approach is grounded in the hypothesis that attacker behavior reflects underlying cognitive biases such as loss aversion, risk tolerance, or goal persistence that can be extracted and modeled through careful observation of log sequences. This lays the groundwork for future work on behaviorally adaptive cyber defense and cognitive trait inference. We develop a strategy-driven prompt system to segment large amounts of network logs data into distinct behavioral phases in a highly efficient manner, enabling the LLM to associate each phase with likely techniques and underlying cognitive motives. By mapping network-layer events to high-level attacker strategies, our method reveals how behavioral signals such as tool switching, protocol transitions, or pivot patterns correspond to psychologically meaningful decision points. The results demonstrate that LLMs can bridge the semantic gap between packet-level logs and strategic intent, offering a pathway toward cognitive-adaptive cyber defense.\n  Keywords: Cognitive Cybersecurity, Large Language Models (LLMs), Cyberpsychology, Intrusion Detection Systems (IDS), MITRE ATT&CK, Cognitive Biases",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20930v1",
    "published_date": "2025-10-23 18:43:31 UTC",
    "updated_date": "2025-10-23 18:43:31 UTC"
  },
  {
    "arxiv_id": "2511.04686v1",
    "title": "Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity",
    "authors": [
      "Pratik Poudel"
    ],
    "abstract": "The Key-Value (KV) cache is integral to efficient autoregressive inference in large language models (LLMs), yet its unbounded growth in stateful multi-turn scenarios presents major challenges. This paper examines the interplay between KV cache management strategies, the architectural context limits of models like meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of positional encodings. Through empirical analysis using a stateful benchmarking framework, we show that LLM generation quality degrades sharply when the accumulated KV cache approaches or exceeds the model's trained context window (e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via AttentionTop), can worsen performance if they disrupt positional coherence. Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a cache by removing non-contiguous tokens can scramble these signals and lead to degenerative outputs. We further show that simple strategies preserving contiguous context blocks (e.g., keeping an initial \"gist\") can yield more coherent generations than complex or positionally disruptive ones. We advocate for eviction techniques that respect architectural limits, preserve positional structure, and view \"cache health\" holistically beyond mere size.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.04686v1",
    "published_date": "2025-10-23 18:22:00 UTC",
    "updated_date": "2025-10-23 18:22:00 UTC"
  },
  {
    "arxiv_id": "2510.20916v1",
    "title": "Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance",
    "authors": [
      "Sydney M. Katz",
      "Robert J. Moss",
      "Dylan M. Asmar",
      "Wesley A. Olson",
      "James K. Kuchar",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "Aircraft collision avoidance systems is critical to modern aviation. These systems are designed to predict potential collisions between aircraft and recommend appropriate avoidance actions. Creating effective collision avoidance systems requires solutions to a variety of technical challenges related to surveillance, decision making, and validation. These challenges have sparked significant research and development efforts over the past several decades that have resulted in a variety of proposed solutions. This article provides an overview of these challenges and solutions with an emphasis on those that have been put through a rigorous validation process and accepted by regulatory bodies. The challenges posed by the collision avoidance problem are often present in other domains, and aircraft collision avoidance systems can serve as case studies that provide valuable insights for a wide range of safety-critical systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "32 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20916v1",
    "published_date": "2025-10-23 18:13:22 UTC",
    "updated_date": "2025-10-23 18:13:22 UTC"
  },
  {
    "arxiv_id": "2510.20909v1",
    "title": "Code-enabled language models can outperform reasoning models on diverse tasks",
    "authors": [
      "Cedegao E. Zhang",
      "Cédric Colas",
      "Gabriel Poesia",
      "Joshua B. Tenenbaum",
      "Jacob Andreas"
    ],
    "abstract": "Reasoning models (RMs), language models (LMs) trained with reinforcement learning to produce long-form natural language reasoning, have been remarkably successful, but they still require large amounts of computation and data to train, and can be slow and expensive to run. In this paper, we show that standard instruct LMs can already be elicited to be strong reasoners at a level comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs R1) without finetuning, across diverse domains from instruction following and creative generation to mathematical reasoning. This is achieved by CodeAdapt, our simple recipe that combines the CodeAct framework, where LMs interleave natural language reasoning with code execution in a multi-step fashion, with few-shot bootstrap in-context learning from as few as five training problems. Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables three LMs to outperform the corresponding RMs on average over eight tasks (up to 22.9%) while being 10-81% more token efficient, and delivers superior performance on six tasks when averaged over the four models (up to 35.7%). Furthermore, the code-augmented reasoning traces display rich and varied problem-solving strategies. Our findings support that (1) CodeAdapt-style learning and reasoning may be robust and domain general and (2) code-enabled LMs are cognitively grounded and powerful systems, potentially providing a strong foundation for in-weight reinforcement learning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20909v1",
    "published_date": "2025-10-23 18:04:03 UTC",
    "updated_date": "2025-10-23 18:04:03 UTC"
  },
  {
    "arxiv_id": "2510.20819v2",
    "title": "Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge",
    "authors": [
      "Nimrod Berman",
      "Omkar Joglekar",
      "Eitan Kosman",
      "Dotan Di Castro",
      "Omri Azencot"
    ],
    "abstract": "Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as a poster at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20819v2",
    "published_date": "2025-10-23 17:59:54 UTC",
    "updated_date": "2025-10-26 09:13:56 UTC"
  },
  {
    "arxiv_id": "2510.20888v1",
    "title": "Video-As-Prompt: Unified Semantic Control for Video Generation",
    "authors": [
      "Yuxuan Bian",
      "Xin Chen",
      "Zenan Li",
      "Tiancheng Zhi",
      "Shen Sang",
      "Linjie Luo",
      "Qiang Xu"
    ],
    "abstract": "Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Website: https://bytedance.github.io/Video-As-Prompt",
    "pdf_url": "https://arxiv.org/pdf/2510.20888v1",
    "published_date": "2025-10-23 17:59:52 UTC",
    "updated_date": "2025-10-23 17:59:52 UTC"
  },
  {
    "arxiv_id": "2510.20818v1",
    "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation",
    "authors": [
      "Mateo Guaman Castro",
      "Sidharth Rajagopal",
      "Daniel Gorbatov",
      "Matt Schmittle",
      "Rohan Baijal",
      "Octi Zhang",
      "Rosario Scalise",
      "Sidharth Talia",
      "Emma Romig",
      "Celso de Melo",
      "Byron Boots",
      "Abhishek Gupta"
    ],
    "abstract": "A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20818v1",
    "published_date": "2025-10-23 17:59:45 UTC",
    "updated_date": "2025-10-23 17:59:45 UTC"
  },
  {
    "arxiv_id": "2510.20813v1",
    "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation",
    "authors": [
      "Guangqi Jiang",
      "Haoran Chang",
      "Ri-Zhao Qiu",
      "Yutong Liang",
      "Mazeyu Ji",
      "Jiyue Zhu",
      "Zhao Dong",
      "Xueyan Zou",
      "Xiaolong Wang"
    ],
    "abstract": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates \"closing the loop\" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20813v1",
    "published_date": "2025-10-23 17:59:26 UTC",
    "updated_date": "2025-10-23 17:59:26 UTC"
  },
  {
    "arxiv_id": "2510.20812v3",
    "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation",
    "authors": [
      "Yuhan Liu",
      "Lianhui Qin",
      "Shengjie Wang"
    ],
    "abstract": "Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20812v3",
    "published_date": "2025-10-23 17:59:21 UTC",
    "updated_date": "2025-12-09 15:09:56 UTC"
  },
  {
    "arxiv_id": "2510.20887v1",
    "title": "Preventing Shortcuts in Adapter Training via Providing the Shortcuts",
    "authors": [
      "Anujraaj Argo Goyal",
      "Guocheng Gordon Qian",
      "Huseyin Coskun",
      "Aarush Gupta",
      "Himmy Tam",
      "Daniil Ostashev",
      "Ju Hu",
      "Dhritiman Sagar",
      "Sergey Tulyakov",
      "Kfir Aberman",
      "Kuan-Chieh Jackson Wang"
    ],
    "abstract": "Adapter-based training has emerged as a key mechanism for extending the capabilities of powerful foundation image generators, enabling personalized and stylized text-to-image synthesis. These adapters are typically trained to capture a specific target attribute, such as subject identity, using single-image reconstruction objectives. However, because the input image inevitably contains a mixture of visual factors, adapters are prone to entangle the target attribute with incidental ones, such as pose, expression, and lighting. This spurious correlation problem limits generalization and obstructs the model's ability to adhere to the input text prompt. In this work, we uncover a simple yet effective solution: provide the very shortcuts we wish to eliminate during adapter training. In Shortcut-Rerouted Adapter Training, confounding factors are routed through auxiliary modules, such as ControlNet or LoRA, eliminating the incentive for the adapter to internalize them. The auxiliary modules are then removed during inference. When applied to tasks like facial and full-body identity injection, our approach improves generation quality, diversity, and prompt adherence. These results point to a general design principle in the era of large models: when seeking disentangled representations, the most effective path may be to establish shortcuts for what should NOT be learned.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2025, webpage: https://snap-research.github.io/shortcut-rerouting/",
    "pdf_url": "https://arxiv.org/pdf/2510.20887v1",
    "published_date": "2025-10-23 17:59:09 UTC",
    "updated_date": "2025-10-23 17:59:09 UTC"
  },
  {
    "arxiv_id": "2510.20810v1",
    "title": "On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?",
    "authors": [
      "Mingmeng Geng",
      "Thierry Poibeau"
    ],
    "abstract": "With the widespread use of large language models (LLMs), many researchers have turned their attention to detecting text generated by them. However, there is no consistent or precise definition of their target, namely \"LLM-generated text\". Differences in usage scenarios and the diversity of LLMs further increase the difficulty of detection. What is commonly regarded as the detecting target usually represents only a subset of the text that LLMs can potentially produce. Human edits to LLM outputs, together with the subtle influences that LLMs exert on their users, are blurring the line between LLM-generated and human-written text. Existing benchmarks and evaluation approaches do not adequately address the various conditions in real-world detector applications. Hence, the numerical results of detectors are often misunderstood, and their significance is diminishing. Therefore, detectors remain useful under specific conditions, but their results should be interpreted only as references rather than decisive indicators.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20810v1",
    "published_date": "2025-10-23 17:59:06 UTC",
    "updated_date": "2025-10-23 17:59:06 UTC"
  },
  {
    "arxiv_id": "2510.20809v1",
    "title": "Real Deep Research for AI, Robotics and Beyond",
    "authors": [
      "Xueyan Zou",
      "Jianglong Ye",
      "Hao Zhang",
      "Xiaoyu Xiang",
      "Mingyu Ding",
      "Zhaojing Yang",
      "Yong Jae Lee",
      "Zhuowen Tu",
      "Sifei Liu",
      "Xiaolong Wang"
    ],
    "abstract": "With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one's expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "website: https://realdeepresearch.github.io",
    "pdf_url": "https://arxiv.org/pdf/2510.20809v1",
    "published_date": "2025-10-23 17:59:05 UTC",
    "updated_date": "2025-10-23 17:59:05 UTC"
  },
  {
    "arxiv_id": "2510.20808v1",
    "title": "The Reality Gap in Robotics: Challenges, Solutions, and Best Practices",
    "authors": [
      "Elie Aljalbout",
      "Jiaxu Xing",
      "Angel Romero",
      "Iretiayo Akinola",
      "Caelan Reed Garrett",
      "Eric Heiden",
      "Abhishek Gupta",
      "Tucker Hermans",
      "Yashraj Narang",
      "Dieter Fox",
      "Davide Scaramuzza",
      "Fabio Ramos"
    ],
    "abstract": "Machine learning has facilitated significant advancements across various robotics domains, including navigation, locomotion, and manipulation. Many such achievements have been driven by the extensive use of simulation as a critical tool for training and testing robotic systems prior to their deployment in real-world environments. However, simulations consist of abstractions and approximations that inevitably introduce discrepancies between simulated and real environments, known as the reality gap. These discrepancies significantly hinder the successful transfer of systems from simulation to the real world. Closing this gap remains one of the most pressing challenges in robotics. Recent advances in sim-to-real transfer have demonstrated promising results across various platforms, including locomotion, navigation, and manipulation. By leveraging techniques such as domain randomization, real-to-sim transfer, state and action abstractions, and sim-real co-training, many works have overcome the reality gap. However, challenges persist, and a deeper understanding of the reality gap's root causes and solutions is necessary. In this survey, we present a comprehensive overview of the sim-to-real landscape, highlighting the causes, solutions, and evaluation metrics for the reality gap and sim-to-real transfer.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for Publication as part of the Annual Review of Control, Robotics, and Autonomous Systems 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.20808v1",
    "published_date": "2025-10-23 17:58:53 UTC",
    "updated_date": "2025-10-23 17:58:53 UTC"
  },
  {
    "arxiv_id": "2510.20800v1",
    "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples",
    "authors": [
      "Shiva Sreeram",
      "Alaa Maalouf",
      "Pratyusha Sharma",
      "Daniela Rus"
    ],
    "abstract": "Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM's weight matrices can boost downstream accuracy -- without any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected -- eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data -- both for computing the indicative gradients and for measuring the final accuracy -- suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets -- entirely without fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20800v1",
    "published_date": "2025-10-23 17:58:01 UTC",
    "updated_date": "2025-10-23 17:58:01 UTC"
  },
  {
    "arxiv_id": "2510.20886v1",
    "title": "Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People",
    "authors": [
      "Gabriel Grand",
      "Valerio Pepe",
      "Jacob Andreas",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "Many high-stakes applications of AI require forming data-driven hypotheses and making targeted guesses; e.g., in scientific and diagnostic settings. Given limited resources, to what extent do agents based on language models (LMs) act rationally? We develop methods to benchmark and enhance agentic information-seeking, drawing on insights from human behavior. First, we introduce a strategic decision-oriented dialogue task called Collaborative Battleship, in which a partially-informed Captain must balance exploration (asking questions) and action (taking shots), while a fully-informed Spotter must provide accurate answers under an information bottleneck. Compared to human players (N=42), we find that LM agents struggle to ground answers in context, generate informative questions, and select high-value actions. Next, to address these gaps, we develop novel Monte Carlo inference strategies for LMs based on principles from Bayesian Experimental Design (BED). For Spotter agents, our approach boosts accuracy by up to 14.7% absolute over LM-only baselines; for Captain agents, it raises expected information gain (EIG) by up to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these components yield sharper targeting (+0.303-0.374 F1), and enable weaker LMs, such as Llama-4-Scout, to outperform both humans (8% -> 82% win rate) and frontier models (0% -> 67% win rate vs. GPT-5) at ~1% of GPT-5's cost. We replicate these findings on Guess Who? where our methods significantly boost accuracy (+28.3-42.4 p.p.), demonstrating their general applicability for building rational information-seeking agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20886v1",
    "published_date": "2025-10-23 17:57:28 UTC",
    "updated_date": "2025-10-23 17:57:28 UTC"
  },
  {
    "arxiv_id": "2510.20797v1",
    "title": "Simple Context Compression: Mean-Pooling and Multi-Ratio Training",
    "authors": [
      "Yair Feldman",
      "Yoav Artzi"
    ],
    "abstract": "A common strategy to reduce the computational costs of using long contexts in retrieval-augmented generation (RAG) with large language models (LLMs) is soft context compression, where the input sequence is transformed into a shorter continuous representation. We develop a lightweight and simple mean-pooling approach that consistently outperforms the widely used compression-tokens architecture, and study training the same compressor to output multiple compression ratios. We conduct extensive experiments across in-domain and out-of-domain QA datasets, as well as across model families, scales, and compression ratios. Overall, our simple mean-pooling approach achieves the strongest performance, with a relatively small drop when training for multiple compression ratios. More broadly though, across architectures and training regimes the trade-offs are more nuanced, illustrating the complex landscape of compression methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code available at https://github.com/lil-lab/simple-context-compression",
    "pdf_url": "https://arxiv.org/pdf/2510.20797v1",
    "published_date": "2025-10-23 17:57:23 UTC",
    "updated_date": "2025-10-23 17:57:23 UTC"
  },
  {
    "arxiv_id": "2510.21883v1",
    "title": "Language Ranker: A Lightweight Ranking framework for LLM Decoding",
    "authors": [
      "Chenheng Zhang",
      "Tianqi Du",
      "Jizhe Zhang",
      "Mingqing Xiao",
      "Yifei Wang",
      "Yisen Wang",
      "Zhouchen Lin"
    ],
    "abstract": "Conventional research on large language models (LLMs) has primarily focused on refining output distributions, while paying less attention to the decoding process that transforms these distributions into final responses. Recent advances, such as scaling the computation of inference time with reward models, have underscored the importance of decoding, but these methods often suffer from high computational costs and limited applicability. In this paper, we revisit LLM generation through the lens of recommender systems, conceptualizing the decoding process as analogous to the ranking stage in recommendation pipelines. From this perspective, we observe that both traditional decoding methods and reward models exhibit clear limitations such as redundancy. Motivated by this insight, we propose Language Ranker, a novel framework that introduces a lightweight module to rerank candidate responses using features extracted by the base model. Experiments across a wide range of tasks show that Language Ranker achieves performance comparable to large-scale reward models, while requiring only <0.5M additional parameters, significantly reducing the computational overhead during both training and inference stages. This highlights the efficiency and effectiveness of our method, showcasing its potential to fully unlock the capabilities of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21883v1",
    "published_date": "2025-10-23 17:56:46 UTC",
    "updated_date": "2025-10-23 17:56:46 UTC"
  },
  {
    "arxiv_id": "2510.20795v1",
    "title": "Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks",
    "authors": [
      "Juan Alejandro Pinto Castro",
      "Héctor J. Hortúa",
      "Jorge Enrique García-Farieta",
      "Roger Anderson Hurtado"
    ],
    "abstract": "Deep learning has emerged as a transformative methodology in modern cosmology, providing powerful tools to extract meaningful physical information from complex astronomical datasets. This paper implements a novel Bayesian graph deep learning framework for estimating key cosmological parameters in a primordial magnetic field (PMF) cosmology directly from simulated Cosmic Microwave Background (CMB) maps. Our methodology utilizes DeepSphere, a spherical convolutional neural network architecture specifically designed to respect the spherical geometry of CMB data through HEALPix pixelization. To advance beyond deterministic point estimates and enable robust uncertainty quantification, we integrate Bayesian Neural Networks (BNNs) into the framework, capturing aleatoric and epistemic uncertainties that reflect the model confidence in its predictions. The proposed approach demonstrates exceptional performance, achieving $R^{2}$ scores exceeding 0.89 for the magnetic parameter estimation. We further obtain well-calibrated uncertainty estimates through post-hoc training techniques including Variance Scaling and GPNormal. This integrated DeepSphere-BNNs framework not only delivers accurate parameter estimation from CMB maps with PMF contributions but also provides reliable uncertainty quantification, providing the necessary tools for robust cosmological inference in the era of precision cosmology.",
    "categories": [
      "astro-ph.CO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "astro-ph.CO",
    "comment": "16 pages, 6 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.20795v1",
    "published_date": "2025-10-23 17:56:04 UTC",
    "updated_date": "2025-10-23 17:56:04 UTC"
  },
  {
    "arxiv_id": "2510.20784v2",
    "title": "A Coherence-Based Measure of AGI",
    "authors": [
      "Fares Fourati"
    ],
    "abstract": "Recent approaches to evaluating Artificial General Intelligence (AGI) typically summarize a system's capability using the arithmetic mean of its proficiencies across multiple cognitive domains. While simple, this implicitly assumes compensability: exceptional performance in some areas can offset severe deficiencies in others. Genuine general intelligence, however, requires coherent sufficiency: balanced competence across all essential faculties. We introduce a coherence-based measure of AGI that integrates the generalized mean over a continuum of compensability exponents. This yields an area-under-the-curve (AUC) metric spanning arithmetic, geometric, and harmonic regimes, quantifying how robust an evaluated capability remains as compensability assumptions become stricter. Unlike the arithmetic mean, which rewards specialization, the AUC penalizes imbalance and exposes bottlenecks that constrain performance. To illustrate the framework, we apply it to cognitive profiles derived from the Cattell-Horn-Carroll (CHC) model, showing how coherence-based aggregation highlights imbalances that are obscured by arithmetic averaging. As a second, independent example, we apply the same methodology to a set of 17 heterogeneous benchmarks, demonstrating how coherence-based evaluation can reveal unevenness even in narrower task collections. These examples show that the proposed approach offers a principled, interpretable, and stricter foundation for measuring progress toward AGI.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at FAST@AAAI 2026. 15 pages, 2 figures, 13 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.20784v2",
    "published_date": "2025-10-23 17:51:42 UTC",
    "updated_date": "2025-11-27 17:01:37 UTC"
  },
  {
    "arxiv_id": "2510.20782v1",
    "title": "A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text",
    "authors": [
      "Alicia Sagae",
      "Chia-Jung Lee",
      "Sandeep Avula",
      "Brandon Dang",
      "Vanessa Murdock"
    ],
    "abstract": "Current methods for evaluating large language models (LLMs) typically focus on high-level tasks such as text generation, without targeting a particular AI application. This approach is not sufficient for evaluating LLMs for Responsible AI dimensions like fairness, since protected attributes that are highly relevant in one application may be less relevant in another. In this work, we construct a dataset that is driven by a real-world application (generate a plain-text product description, given a list of product features), parameterized by fairness attributes intersected with gendered adjectives and product categories, yielding a rich set of labeled prompts. We show how to use the data to identify quality, veracity, safety, and fairness gaps in LLMs, contributing a proposal for LLM evaluation paired with a concrete resource for the research community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages with 3 figures, to appear in Proceedings of the 34th ACM International Conference on Information and Knowledge Management (CIKM '25)",
    "pdf_url": "https://arxiv.org/pdf/2510.20782v1",
    "published_date": "2025-10-23 17:50:55 UTC",
    "updated_date": "2025-10-23 17:50:55 UTC"
  },
  {
    "arxiv_id": "2510.20780v1",
    "title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost",
    "authors": [
      "Runzhe Zhan",
      "Zhihong Huang",
      "Xinyi Yang",
      "Lidia S. Chao",
      "Min Yang",
      "Derek F. Wong"
    ],
    "abstract": "Recent advancements in large reasoning models (LRMs) have introduced an intermediate \"thinking\" process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to \"overthink\" simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20780v1",
    "published_date": "2025-10-23 17:48:36 UTC",
    "updated_date": "2025-10-23 17:48:36 UTC"
  },
  {
    "arxiv_id": "2510.20774v2",
    "title": "FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation",
    "authors": [
      "Wenhao Wang",
      "Kehe Ye",
      "Xinyu Zhou",
      "Tianxing Chen",
      "Cao Min",
      "Qiaoming Zhu",
      "Xiaokang Yang",
      "Ping Luo",
      "Yongjian Shen",
      "Yang Yang",
      "Maoqing Yao",
      "Yao Mu"
    ],
    "abstract": "Large-scale and diverse datasets are vital for training robust robotic manipulation policies, yet existing data collection methods struggle to balance scale, diversity, and quality. Simulation offers scalability but suffers from sim-to-real gaps, while teleoperation yields high-quality demonstrations with limited diversity and high labor cost. We introduce FieldGen, a field-guided data generation framework that enables scalable, diverse, and high-quality real-world data collection with minimal human supervision. FieldGen decomposes manipulation into two stages: a pre-manipulation phase, allowing trajectory diversity, and a fine manipulation phase requiring expert precision. Human demonstrations capture key contact and pose information, after which an attraction field automatically generates diverse trajectories converging to successful configurations. This decoupled design combines scalable trajectory diversity with precise supervision. Moreover, FieldGen-Reward augments generated data with reward annotations to further enhance policy learning. Experiments demonstrate that policies trained with FieldGen achieve higher success rates and improved stability compared to teleoperation-based baselines, while significantly reducing human effort in long-term real-world data collection. Webpage is available at https://fieldgen.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "Webpage: https://fieldgen.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.20774v2",
    "published_date": "2025-10-23 17:47:12 UTC",
    "updated_date": "2025-10-28 17:10:50 UTC"
  },
  {
    "arxiv_id": "2510.20768v2",
    "title": "RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines",
    "authors": [
      "Austin Jia",
      "Avaneesh Ramesh",
      "Zain Shamsi",
      "Daniel Zhang",
      "Alex Liu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as the dominant architectural pattern to operationalize Large Language Model (LLM) usage in Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to poisoning attacks, and previously proposed defenses can fail for CTI contexts as cyber threat information is often completely new for emerging attacks, and sophisticated threat actors can mimic legitimate formats, terminology, and stylistic conventions. To address this issue, we propose that the robustness of modern RAG defenses can be accelerated by applying source credibility algorithms on corpora, using PageRank as an example. In our experiments, we demonstrate quantitatively that our algorithm applies a lower authority score to malicious documents while promoting trusted content, using the standardized MS MARCO dataset. We also demonstrate proof-of-concept performance of our algorithm on CTI documents and feeds.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CR",
    "comment": "Presented as a poster at the Annual Computer Security Applications Conference (ACSAC) 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20768v2",
    "published_date": "2025-10-23 17:43:00 UTC",
    "updated_date": "2025-12-15 20:48:15 UTC"
  },
  {
    "arxiv_id": "2510.20748v1",
    "title": "Reinforcement Learning and Consumption-Savings Behavior",
    "authors": [
      "Brandon Kaplowitz"
    ],
    "abstract": "This paper demonstrates how reinforcement learning can explain two puzzling empirical patterns in household consumption behavior during economic downturns. I develop a model where agents use Q-learning with neural network approximation to make consumption-savings decisions under income uncertainty, departing from standard rational expectations assumptions. The model replicates two key findings from recent literature: (1) unemployed households with previously low liquid assets exhibit substantially higher marginal propensities to consume (MPCs) out of stimulus transfers compared to high-asset households (0.50 vs 0.34), even when neither group faces borrowing constraints, consistent with Ganong et al. (2024); and (2) households with more past unemployment experiences maintain persistently lower consumption levels after controlling for current economic conditions, a \"scarring\" effect documented by Malmendier and Shen (2024). Unlike existing explanations based on belief updating about income risk or ex-ante heterogeneity, the reinforcement learning mechanism generates both higher MPCs and lower consumption levels simultaneously through value function approximation errors that evolve with experience. Simulation results closely match the empirical estimates, suggesting that adaptive learning through reinforcement learning provides a unifying framework for understanding how past experiences shape current consumption behavior beyond what current economic conditions would predict.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "econ.GN",
    "comment": "41 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20748v1",
    "published_date": "2025-10-23 17:14:49 UTC",
    "updated_date": "2025-10-23 17:14:49 UTC"
  },
  {
    "arxiv_id": "2510.20743v1",
    "title": "Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations",
    "authors": [
      "Lorenzo Stacchio",
      "Andrea Ubaldi",
      "Alessandro Galdelli",
      "Maurizio Mauri",
      "Emanuele Frontoni",
      "Andrea Gaggioli"
    ],
    "abstract": "We present Empathic Prompting, a novel framework for multimodal human-AI interaction that enriches Large Language Model (LLM) conversations with implicit non-verbal context. The system integrates a commercial facial expression recognition service to capture users' emotional cues and embeds them as contextual signals during prompting. Unlike traditional multimodal interfaces, empathic prompting requires no explicit user control; instead, it unobtrusively augments textual input with affective information for conversational and smoothness alignment. The architecture is modular and scalable, allowing integration of additional non-verbal modules. We describe the system design, implemented through a locally deployed DeepSeek instance, and report a preliminary service and usability evaluation (N=5). Results show consistent integration of non-verbal input into coherent LLM outputs, with participants highlighting conversational fluidity. Beyond this proof of concept, empathic prompting points to applications in chatbot-mediated communication, particularly in domains like healthcare or education, where users' emotional signals are critical yet often opaque in verbal exchanges.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20743v1",
    "published_date": "2025-10-23 17:08:03 UTC",
    "updated_date": "2025-10-23 17:08:03 UTC"
  },
  {
    "arxiv_id": "2510.24765v1",
    "title": "Topic-aware Large Language Models for Summarizing the Lived Healthcare Experiences Described in Health Stories",
    "authors": [
      "Maneesh Bilalpur",
      "Megan Hamm",
      "Young Ji Lee",
      "Natasha Norman",
      "Kathleen M. McTigue",
      "Yanshan Wang"
    ],
    "abstract": "Storytelling is a powerful form of communication and may provide insights into factors contributing to gaps in healthcare outcomes. To determine whether Large Language Models (LLMs) can identify potential underlying factors and avenues for intervention, we performed topic-aware hierarchical summarization of narratives from African American (AA) storytellers. Fifty transcribed stories of AA experiences were used to identify topics in their experience using the Latent Dirichlet Allocation (LDA) technique. Stories about a given topic were summarized using an open-source LLM-based hierarchical summarization approach. Topic summaries were generated by summarizing across story summaries for each story that addressed a given topic. Generated topic summaries were rated for fabrication, accuracy, comprehensiveness, and usefulness by the GPT4 model, and the model's reliability was validated against the original story summaries by two domain experts. 26 topics were identified in the fifty AA stories. The GPT4 ratings suggest that topic summaries were free from fabrication, highly accurate, comprehensive, and useful. The reliability of GPT ratings compared to expert assessments showed moderate to high agreement. Our approach identified AA experience-relevant topics such as health behaviors, interactions with medical team members, caregiving and symptom management, among others. Such insights could help researchers identify potential factors and interventions by learning from unstructured narratives in an efficient manner-leveraging the communicative power of storytelling. The use of LDA and LLMs to identify and summarize the experience of AA individuals suggests a variety of possible avenues for health research and possible clinical improvements to support patients and caregivers, thereby ultimately improving health outcomes.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.24765v1",
    "published_date": "2025-10-23 16:52:00 UTC",
    "updated_date": "2025-10-23 16:52:00 UTC"
  },
  {
    "arxiv_id": "2510.20733v1",
    "title": "Thought Communication in Multiagent Collaboration",
    "authors": [
      "Yujia Zheng",
      "Zhuokai Zhao",
      "Zijian Li",
      "Yaqi Xie",
      "Mingze Gao",
      "Lizhu Zhang",
      "Kun Zhang"
    ],
    "abstract": "Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 Spotlight",
    "pdf_url": "https://arxiv.org/pdf/2510.20733v1",
    "published_date": "2025-10-23 16:48:02 UTC",
    "updated_date": "2025-10-23 16:48:02 UTC"
  },
  {
    "arxiv_id": "2510.20728v1",
    "title": "Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems",
    "authors": [
      "Xi He",
      "Sirui Lu",
      "Bei Zeng"
    ],
    "abstract": "We present a multi-agent, human-in-the-loop workflow that co-designs quantum codes with prescribed transversal diagonal gates. It builds on the Subset-Sum Linear Programming (SSLP) framework (arXiv:2504.20847), which partitions basis strings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL) equalities via small LPs. The workflow is powered by GPT-5 and implemented within TeXRA (https://texra.ai)-a multi-agent research assistant platform that supports an iterative tool-use loop agent and a derivation-then-edit workflow reasoning agent. We work in a LaTeX-Python environment where agents reason, edit documents, execute code, and synchronize their work to Git/Overleaf. Within this workspace, three roles collaborate: a Synthesis Agent formulates the problem; a Search Agent sweeps/screens candidates and exactifies numerics into rationals; and an Audit Agent independently checks all KL equalities and the induced logical action. As a first step we focus on distance $d=2$ with nondegenerate residues. For code dimension $K\\in\\{2,3,4\\}$ and $n\\le6$ qubits, systematic sweeps yield certificate-backed tables cataloging attainable cyclic logical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$ at $n=6$. From verified instances, Synthesis Agent abstracts recurring structures into closed-form families and proves they satisfy the KL equalities for all parameters. It further demonstrates that SSLP accommodates residue degeneracy by exhibiting a new $((6,4,2))$ code implementing the transversal controlled-phase $diag(1,1,1,i)$. Overall, the workflow recasts diagonal-transversal feasibility as an analytical pipeline executed at scale, combining systematic enumeration with exact analytical reconstruction. It yields reproducible code constructions, supports targeted extensions to larger $K$ and higher distances, and leads toward data-driven classification.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CL",
      "math-ph"
    ],
    "primary_category": "quant-ph",
    "comment": "29 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20728v1",
    "published_date": "2025-10-23 16:45:39 UTC",
    "updated_date": "2025-10-23 16:45:39 UTC"
  },
  {
    "arxiv_id": "2510.20727v1",
    "title": "Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing",
    "authors": [
      "Xizhi Wu",
      "Madeline S. Kreider",
      "Philip E. Empey",
      "Chenyu Li",
      "Yanshan Wang"
    ],
    "abstract": "Objective: Fluoropyrimidines are widely prescribed for colorectal and breast cancers, but are associated with toxicities such as hand-foot syndrome and cardiotoxicity. Since toxicity documentation is often embedded in clinical notes, we aimed to develop and evaluate natural language processing (NLP) methods to extract treatment and toxicity information.\n  Materials and Methods: We constructed a gold-standard dataset of 236 clinical notes from 204,165 adult oncology patients. Domain experts annotated categories related to treatment regimens and toxicities. We developed rule-based, machine learning-based (Random Forest, Support Vector Machine [SVM], Logistic Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language models (LLM)-based NLP approaches (zero-shot and error-analysis prompting). Models used an 80:20 train-test split.\n  Results: Sufficient data existed to train and evaluate 5 annotated categories. Error-analysis prompting achieved optimal precision, recall, and F1 scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot prompting reached F1=1.000 for treatment and F1=0.876 for toxicities extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods served as our baseline with F1 scores of 0.857 in treatment and 0.858 in toxicities.\n  Discussion: LMM-based approaches outperformed all others, followed by machine learning methods. Machine and deep learning approaches were limited by small training data and showed limited generalizability, particularly for rare categories.\n  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine treatment and toxicity information from clinical notes, and has strong potential to support oncology research and pharmacovigilance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20727v1",
    "published_date": "2025-10-23 16:44:39 UTC",
    "updated_date": "2025-10-23 16:44:39 UTC"
  },
  {
    "arxiv_id": "2510.21881v1",
    "title": "GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models",
    "authors": [
      "Nannan Shi",
      "Chuanyu Qin",
      "Shipeng Song",
      "Man Luo"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities in text-based mathematical problem solving; however, when adapted to visual reasoning tasks, particularly geometric problem solving, their performance substantially declines because geometric problems present unique challenges. Specifically, these challenges stem from two key factors: first, the intrinsic complexity of geometry requiring detailed image comprehension and multi-step reasoning, and second, the limitations of existing datasets which lack sufficient scale, diversity, and explicit reasoning traces, consequently hindering effective model training. To address these challenges, we developed the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two subsets: Geo-Thought-6K with 6,243 samples and its augmented version Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual descriptions, step-by-step solutions, explicit reasoning chains, reflection steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a mathematical reasoning multimodal model that generates detailed thinking processes during problem-solving. Our model outperforms existing benchmarks in geometric tasks, demonstrating that training with our Chain-of-Thought dataset improves geometric reasoning capabilities across both in-domain and out-of-domain settings. Finally, we analyze failure cases and observe that errors primarily arise from incorrect interpretation of mathematical concepts or spatial misjudgment. By invoking CoT to correct these mistakes, the model produces correct answers.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21881v1",
    "published_date": "2025-10-23 16:43:54 UTC",
    "updated_date": "2025-10-23 16:43:54 UTC"
  },
  {
    "arxiv_id": "2510.20721v3",
    "title": "User Perceptions vs. Proxy LLM Judges: Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios",
    "authors": [
      "Xiaoyuan Wu",
      "Roshni Kaushik",
      "Wenkai Li",
      "Lujo Bauer",
      "Koichi Onoue"
    ],
    "abstract": "Large language models (LLMs) are rapidly being adopted for tasks like drafting emails, summarizing meetings, and answering health questions. In these settings, users may need to share private information (e.g., contact details, health records). To evaluate LLMs' ability to identify and redact such information, prior work introduced real-life, scenario-based benchmarks (e.g., ConfAIde, PrivacyLens) and found that LLMs can leak private information in complex scenarios. However, these evaluations relied on proxy LLMs to judge the helpfulness and privacy-preservation quality of LLM responses, rather than directly measuring users' perceptions. To understand how users perceive the helpfulness and privacy-preservation quality of LLM responses to privacy-sensitive scenarios, we conducted a user study ($n=94$) using 90 PrivacyLens scenarios. We found that users had low agreement with each other when evaluating identical LLM responses. In contrast, five proxy LLMs reached high agreement, yet each proxy LLM had low correlation with users' evaluations. These results indicate that proxy LLMs cannot accurately estimate users' wide range of perceptions of utility and privacy in privacy-sensitive scenarios. We discuss the need for more user-centered studies to measure LLMs' ability to help users while preserving privacy, and for improving alignment between LLMs and users in estimating perceived privacy and utility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20721v3",
    "published_date": "2025-10-23 16:38:26 UTC",
    "updated_date": "2026-01-15 14:47:11 UTC"
  },
  {
    "arxiv_id": "2510.20718v1",
    "title": "Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series",
    "authors": [
      "Daniel Sorensen",
      "Bappaditya Dey",
      "Minjin Hwang",
      "Sandip Halder"
    ],
    "abstract": "Semiconductor manufacturing is an extremely complex and precision-driven process, characterized by thousands of interdependent parameters collected across diverse tools and process steps. Multi-variate time-series analysis has emerged as a critical field for real-time monitoring and fault detection in such environments. However, anomaly prediction in semiconductor fabrication presents several critical challenges, including high dimensionality of sensor data and severe class imbalance due to the rarity of true faults. Furthermore, the complex interdependencies between variables complicate both anomaly prediction and root-cause-analysis. This paper proposes two novel approaches to advance the field from anomaly detection to anomaly prediction, an essential step toward enabling real-time process correction and proactive fault prevention. The proposed anomaly prediction framework contains two main stages: (a) training a forecasting model on a dataset assumed to contain no anomalies, and (b) performing forecast on unseen time series data. The forecast is compared with the forecast of the trained signal. Deviations beyond a predefined threshold are flagged as anomalies. The two approaches differ in the forecasting model employed. The first assumes independence between variables by utilizing the N-BEATS model for univariate time series forecasting. The second lifts this assumption by utilizing a Graph Neural Network (GNN) to capture inter-variable relationships. Both models demonstrate strong forecasting performance up to a horizon of 20 time points and maintain stable anomaly prediction up to 50 time points. The GNN consistently outperforms the N-BEATS model while requiring significantly fewer trainable parameters and lower computational cost. These results position the GNN as promising solution for online anomaly forecasting to be deployed in manufacturing environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 27 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20718v1",
    "published_date": "2025-10-23 16:33:52 UTC",
    "updated_date": "2025-10-23 16:33:52 UTC"
  },
  {
    "arxiv_id": "2510.20706v2",
    "title": "Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning",
    "authors": [
      "Prakrut Kotecha",
      "Ganga Nair B",
      "Shishir Kolathaya"
    ],
    "abstract": "Model-free reinforcement learning (RL) has enabled adaptable and agile quadruped locomotion; however, policies often converge to a single gait, leading to suboptimal performance. Traditionally, Model Predictive Control (MPC) has been extensively used to obtain task-specific optimal policies but lacks the ability to adapt to varying environments. To address these limitations, we propose an optimization framework for real-time gait adaptation in a continuous gait space, combining the Model Predictive Path Integral (MPPI) algorithm with a Dreamer module to produce adaptive and optimal policies for quadruped locomotion. At each time step, MPPI jointly optimizes the actions and gait variables using a learned Dreamer reward that promotes velocity tracking, energy efficiency, stability, and smooth transitions, while penalizing abrupt gait changes. A learned value function is incorporated as terminal reward, extending the formulation to an infinite-horizon planner. We evaluate our framework in simulation on the Unitree Go1, demonstrating an average reduction of up to 36.48 % in energy consumption across varying target speeds, while maintaining accurate tracking and adaptive, task-appropriate gaits.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.20706v2",
    "published_date": "2025-10-23 16:17:45 UTC",
    "updated_date": "2025-10-24 07:47:41 UTC"
  },
  {
    "arxiv_id": "2510.20699v1",
    "title": "Fusing Narrative Semantics for Financial Volatility Forecasting",
    "authors": [
      "Yaxuan Kong",
      "Yoontae Hwang",
      "Marcus Kaiser",
      "Chris Vryonides",
      "Roel Oomen",
      "Stefan Zohren"
    ],
    "abstract": "We introduce M2VN: Multi-Modal Volatility Network, a novel deep learning-based framework for financial volatility forecasting that unifies time series features with unstructured news data. M2VN leverages the representational power of deep neural networks to address two key challenges in this domain: (i) aligning and fusing heterogeneous data modalities, numerical financial data and textual information, and (ii) mitigating look-ahead bias that can undermine the validity of financial models. To achieve this, M2VN combines open-source market features with news embeddings generated by Time Machine GPT, a recently introduced point-in-time LLM, ensuring temporal integrity. An auxiliary alignment loss is introduced to enhance the integration of structured and unstructured data within the deep learning architecture. Extensive experiments demonstrate that M2VN consistently outperforms existing baselines, underscoring its practical value for risk management and financial decision-making in dynamic markets.",
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "primary_category": "q-fin.CP",
    "comment": "The 6th ACM International Conference on AI in Finance (ICAIF 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.20699v1",
    "published_date": "2025-10-23 16:13:46 UTC",
    "updated_date": "2025-10-23 16:13:46 UTC"
  },
  {
    "arxiv_id": "2510.20692v1",
    "title": "Exploring Large Language Models for Access Control Policy Synthesis and Summarization",
    "authors": [
      "Adarsh Vatsa",
      "Bethel Hall",
      "William Eiers"
    ],
    "abstract": "Cloud computing is ubiquitous, with a growing number of services being hosted on the cloud every day. Typical cloud compute systems allow administrators to write policies implementing access control rules which specify how access to private data is governed. These policies must be manually written, and due to their complexity can often be error prone. Moreover, existing policies often implement complex access control specifications and thus can be difficult to precisely analyze in determining their behavior works exactly as intended. Recently, Large Language Models (LLMs) have shown great success in automated code synthesis and summarization. Given this success, they could potentially be used for automatically generating access control policies or aid in understanding existing policies. In this paper, we explore the effectiveness of LLMs for access control policy synthesis and summarization. Specifically, we first investigate diverse LLMs for access control policy synthesis, finding that: although LLMs can effectively generate syntactically correct policies, they have permissiveness issues, generating policies equivalent to the given specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time for reasoning LLMs. We then investigate how LLMs can be used to analyze policies by introducing a novel semantic-based request summarization approach which leverages LLMs to generate a precise characterization of the requests allowed by a policy. Our results show that while there are significant hurdles in leveraging LLMs for automated policy generation, LLMs show promising results when combined with symbolic approaches in analyzing existing policies.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.SE",
    "comment": "20 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20692v1",
    "published_date": "2025-10-23 16:06:15 UTC",
    "updated_date": "2025-10-23 16:06:15 UTC"
  },
  {
    "arxiv_id": "2510.20691v2",
    "title": "Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs",
    "authors": [
      "Yanlin Song",
      "Ben Liu",
      "Víctor Gutiérrez-Basulto",
      "Zhiwei Hu",
      "Qianqian Xie",
      "Min Peng",
      "Sophia Ananiadou",
      "Jeff Z. Pan"
    ],
    "abstract": "Knowledge Graph Question Answering aims to answer natural language questions by reasoning over structured knowledge graphs. While large language models have advanced KGQA through their strong reasoning capabilities, existing methods continue to struggle to fully exploit both the rich knowledge encoded in KGs and the reasoning capabilities of LLMs, particularly in complex scenarios. They often assume complete KG coverage and lack mechanisms to judge when external information is needed, and their reasoning remains locally myopic, failing to maintain coherent multi-step planning, leading to reasoning failures even when relevant knowledge exists. We propose Graph-RFT, a novel two-stage reinforcement fine-tuning KGQA framework with a 'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to perform autonomous planning and adaptive retrieval scheduling across KG and web sources under incomplete knowledge conditions. Graph-RFT introduces a chain-of-thought fine-tuning method with a customized plan-retrieval dataset activates structured reasoning and resolves the GRPO cold-start problem. It then introduces a novel plan-retrieval guided reinforcement learning process integrates explicit planning and retrieval actions with a multi-reward design, enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired planning module to decompose complex questions into ordered subquestions, and logical expression to guide tool invocation for globally consistent multi-step reasoning. This reasoning retrieval process is optimized with a multi-reward combining outcome and retrieval specific signals, enabling the model to learn when and how to combine KG and web retrieval effectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20691v2",
    "published_date": "2025-10-23 16:04:13 UTC",
    "updated_date": "2025-10-27 07:30:51 UTC"
  },
  {
    "arxiv_id": "2510.20690v2",
    "title": "Neural Diversity Regularizes Hallucinations in Language Models",
    "authors": [
      "Kushal Chakrabarti",
      "Nirmal Balachundhar"
    ],
    "abstract": "Language models continue to hallucinate despite increases in parameters, compute, and data. We propose neural diversity -- decorrelated parallel representations -- as a principled mechanism that reduces hallucination rates at fixed parameter and data budgets. While existing mitigation strategies largely target accuracy, we provide the first formal tail bounds for hallucination probability in ensembled language models, reframing it as a second-moment reliability problem and explaining 94.3% of empirical reliability variation seen across parallel configurations. We introduce ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA adapters with Barlow Twins regularization, and reduce hallucinations by up to 25.6% (and 14.6% on average) while preserving general accuracy. Ablations show LoRA adapters and regularization act synergistically, causal interventions prove neurodiversity as the mediating factor and correlational studies indicate scale: a 0.1% neural correlation increase is associated with a 3.8% hallucination increase. Finally, task-dependent optimality emerges: different tasks require different optimal amounts of neurodiversity. Together, our results highlight neural diversity as a third axis of scaling -- orthogonal to parameters and data -- to improve the reliability of language models at fixed budgets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20690v2",
    "published_date": "2025-10-23 16:03:07 UTC",
    "updated_date": "2025-12-10 18:42:06 UTC"
  },
  {
    "arxiv_id": "2510.20683v1",
    "title": "A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks",
    "authors": [
      "Georgios Mentzelopoulos",
      "Ioannis Asmanis",
      "Konrad P. Kording",
      "Eva L. Dyer",
      "Kostas Daniilidis",
      "Flavia Vitale"
    ],
    "abstract": "Brain-computer interfaces (BCIs) promise to enable vital functions, such as speech and prosthetic control, for individuals with neuromotor impairments. Central to their success are neural decoders, models that map neural activity to intended behavior. Current learning-based decoding approaches fall into two classes: simple, causal models that lack generalization, or complex, non-causal models that generalize and scale offline but struggle in real-time settings. Both face a common challenge, their reliance on power-hungry artificial neural network backbones, which makes integration into real-world, resource-limited systems difficult. Spiking neural networks (SNNs) offer a promising alternative. Because they operate causally these models are suitable for real-time use, and their low energy demands make them ideal for battery-constrained environments. To this end, we introduce Spikachu: a scalable, causal, and energy-efficient neural decoding framework based on SNNs. Our approach processes binned spikes directly by projecting them into a shared latent space, where spiking modules, adapted to the timing of the input, extract relevant features; these latent representations are then integrated and decoded to generate behavioral predictions. We evaluate our approach on 113 recording sessions from 6 non-human primates, totaling 43 hours of recordings. Our method outperforms causal baselines when trained on single sessions using between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that scaling up training to multiple sessions and subjects improves performance and enables few-shot transfer to unseen sessions, subjects, and tasks. Overall, Spikachu introduces a scalable, online-compatible neural decoding framework based on SNNs, whose performance is competitive relative to state-of-the-art models while consuming orders of magnitude less energy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20683v1",
    "published_date": "2025-10-23 15:55:45 UTC",
    "updated_date": "2025-10-23 15:55:45 UTC"
  },
  {
    "arxiv_id": "2510.20677v1",
    "title": "R2-SVC: Towards Real-World Robust and Expressive Zero-shot Singing Voice Conversion",
    "authors": [
      "Junjie Zheng",
      "Gongyu Chen",
      "Chaofan Ding",
      "Zihao Chen"
    ],
    "abstract": "In real-world singing voice conversion (SVC) applications, environmental noise and the demand for expressive output pose significant challenges. Conventional methods, however, are typically designed without accounting for real deployment scenarios, as both training and inference usually rely on clean data. This mismatch hinders practical use, given the inevitable presence of diverse noise sources and artifacts from music separation. To tackle these issues, we propose R2-SVC, a robust and expressive SVC framework. First, we introduce simulation-based robustness enhancement through random fundamental frequency ($F_0$) perturbations and music separation artifact simulations (e.g., reverberation, echo), substantially improving performance under noisy conditions. Second, we enrich speaker representation using domain-specific singing data: alongside clean vocals, we incorporate DNSMOS-filtered separated vocals and public singing corpora, enabling the model to preserve speaker timbre while capturing singing style nuances. Third, we integrate the Neural Source-Filter (NSF) model to explicitly represent harmonic and noise components, enhancing the naturalness and controllability of converted singing. R2-SVC achieves state-of-the-art results on multiple SVC benchmarks under both clean and noisy conditions.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20677v1",
    "published_date": "2025-10-23 15:52:03 UTC",
    "updated_date": "2025-10-23 15:52:03 UTC"
  },
  {
    "arxiv_id": "2510.20671v2",
    "title": "GRACE: Graph Neural Networks for Locus-of-Care Prediction under Extreme Class Imbalance",
    "authors": [
      "Subham Kumar",
      "Lekhansh Shukla",
      "Animesh Mukherjee",
      "Koustav Rudra",
      "Prakrithi Shivaprakash"
    ],
    "abstract": "Determining the appropriate locus of care for addiction patients is one of the most critical clinical decisions that affects patient treatment outcomes and effective use of resources. With a lack of sufficient specialized treatment resources, such as inpatient beds or staff, there is an unmet need to develop an automated framework for the same. Current decision-making approaches suffer from severe class imbalances in addiction datasets. To address this limitation, we propose a novel graph neural network (GRACE) framework that formalizes locus of care prediction as a structured learning problem. In addition, we propose a new approach of obtaining an unbiased meta-graph to train a GNN to overcome the class imbalance problem. Experimental results with real-world data show an improvement of 11-35% in terms of the F1 score of the minority class over competitive baselines. Further, if we jointly finetune the base embedding fed into GRACE as input together with the rest of the GNN component of GRACE, there is a remarkable boost of 15.8% in performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20671v2",
    "published_date": "2025-10-23 15:48:01 UTC",
    "updated_date": "2026-01-17 09:18:06 UTC"
  },
  {
    "arxiv_id": "2510.20665v1",
    "title": "The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models",
    "authors": [
      "Xue Wen Tan",
      "Nathaniel Tan",
      "Galen Lee",
      "Stanley Kok"
    ],
    "abstract": "Evaluating the quality of reasoning traces from large language models remains understudied, labor-intensive, and unreliable: current practice relies on expert rubrics, manual annotation, and slow pairwise judgments. Automated efforts are dominated by graph-based proxies that quantify structural connectivity but do not clarify what constitutes high-quality reasoning; such abstractions can be overly simplistic for inherently complex processes. We introduce a topological data analysis (TDA)-based evaluation framework that captures the geometry of reasoning traces and enables label-efficient, automated assessment. In our empirical study, topological features yield substantially higher predictive power for assessing reasoning quality than standard graph metrics, suggesting that effective reasoning is better captured by higher-dimensional geometric structures rather than purely relational graphs. We further show that a compact, stable set of topological features reliably indicates trace quality, offering a practical signal for future reinforcement learning algorithms.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20665v1",
    "published_date": "2025-10-23 15:43:43 UTC",
    "updated_date": "2025-10-23 15:43:43 UTC"
  },
  {
    "arxiv_id": "2510.20653v1",
    "title": "Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection",
    "authors": [
      "Jack Butler",
      "Nikita Kozodoi",
      "Zainab Afolabi",
      "Brian Tyacke",
      "Gaiar Baimuratov"
    ],
    "abstract": "As Large Language Models (LLMs) continue to evolve, practitioners face increasing options for enhancing inference-time performance without model retraining, including budget tuning and multi-step techniques like self-reflection. While these methods improve output quality, they create complex trade-offs among accuracy, cost, and latency that remain poorly understood across different domains. This paper systematically compares self-reflection and budget tuning across mathematical reasoning and translation tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and Mistral families, along with other models under varying reflection depths and compute budgets to derive Pareto optimal performance frontiers. Our analysis reveals substantial domain dependent variation in self-reflection effectiveness, with performance gains up to 220\\% in mathematical reasoning. We further investigate how reflection round depth and feedback mechanism quality influence performance across model families. To validate our findings in a real-world setting, we deploy a self-reflection enhanced marketing content localisation system at Lounge by Zalando, where it shows market-dependent effectiveness, reinforcing the importance of domain specific evaluation when deploying these techniques. Our results provide actionable guidance for selecting optimal inference strategies given specific domains and resource constraints. We open source our self-reflection implementation for reproducibility at https://github.com/aws-samples/sample-genai-reflection-for-bedrock.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20653v1",
    "published_date": "2025-10-23 15:26:18 UTC",
    "updated_date": "2025-10-23 15:26:18 UTC"
  },
  {
    "arxiv_id": "2510.20647v2",
    "title": "The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI",
    "authors": [
      "Alan Saji",
      "Raj Dabre",
      "Anoop Kunchukuttan",
      "Ratish Puduppully"
    ],
    "abstract": "Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM's reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting \"Lost in Translation,\" where translation steps lead to errors that would have been avoided by question's language reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 13 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.20647v2",
    "published_date": "2025-10-23 15:22:00 UTC",
    "updated_date": "2025-12-22 09:52:15 UTC"
  },
  {
    "arxiv_id": "2510.20641v1",
    "title": "Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges",
    "authors": [
      "Andrea Agiollo",
      "Andrea Omicini"
    ],
    "abstract": "Thanks to the remarkable human-like capabilities of machine learning (ML) models in perceptual and cognitive tasks, frameworks integrating ML within rational agent architectures are gaining traction. Yet, the landscape remains fragmented and incoherent, often focusing on embedding ML into generic agent containers while overlooking the expressive power of rational architectures--such as Belief-Desire-Intention (BDI) agents. This paper presents a fine-grained systematisation of existing approaches, using the BDI paradigm as a reference. Our analysis illustrates the fast-evolving literature on rational agents enhanced by ML, and identifies key research opportunities and open challenges for designing effective rational ML agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20641v1",
    "published_date": "2025-10-23 15:15:45 UTC",
    "updated_date": "2025-10-23 15:15:45 UTC"
  },
  {
    "arxiv_id": "2510.20636v1",
    "title": "Fluidity Index: Next-Generation Super-intelligence Benchmarks",
    "authors": [
      "Eric Ngoiya",
      "Tianshu Bao"
    ],
    "abstract": "This paper introduces the Fluidity Index (FI) to quantify model adaptability in dynamic, scaling environments. The benchmark evaluates response accuracy based on deviations in initial, current, and future environment states, assessing context switching and continuity. We distinguish between closed-ended and open-ended benchmarks, prioritizing closed-loop open-ended real-world benchmarks to test adaptability. The approach measures a model's ability to understand, predict, and adjust to state changes in scaling environments. A truly super-intelligent model should exhibit at least second-order adaptability, enabling self-sustained computation through digital replenishment for optimal fluidity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12",
    "pdf_url": "https://arxiv.org/pdf/2510.20636v1",
    "published_date": "2025-10-23 15:05:23 UTC",
    "updated_date": "2025-10-23 15:05:23 UTC"
  },
  {
    "arxiv_id": "2510.20635v1",
    "title": "Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model",
    "authors": [
      "Haoyu Wang",
      "Sihang Jiang",
      "Yuyan Chen",
      "Yitong Wang",
      "Yanghua Xiao"
    ],
    "abstract": "Curiosity serves as a pivotal conduit for human beings to discover and learn new knowledge. Recent advancements of large language models (LLMs) in natural language processing have sparked discussions regarding whether these models possess capability of curiosity-driven learning akin to humans. In this paper, starting from the human curiosity assessment questionnaire Five-Dimensional Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework that covers dimensions such as Information Seeking, Thrill Seeking, and Social Curiosity to assess the extent of curiosity exhibited by LLMs. The results demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but still tend to make conservative choices when faced with uncertain environments. We further investigated the relationship between curiosity and thinking of LLMs, confirming that curious behaviors can enhance the model's reasoning and active learning abilities. These findings suggest that LLMs have the potential to exhibit curiosity similar to that of humans, providing experimental support for the future development of learning capabilities and innovative research in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20635v1",
    "published_date": "2025-10-23 15:05:17 UTC",
    "updated_date": "2025-10-23 15:05:17 UTC"
  },
  {
    "arxiv_id": "2510.20634v1",
    "title": "Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges",
    "authors": [
      "Zhenhuan Zhou",
      "Jingbo Zhu",
      "Yuchen Zhang",
      "Xiaohang Guan",
      "Peng Wang",
      "Tao Li"
    ],
    "abstract": "Efficient analysis and processing of dental images are crucial for dentists to achieve accurate diagnosis and optimal treatment planning. However, dental imaging inherently poses several challenges, such as low contrast, metallic artifacts, and variations in projection angles. Combined with the subjectivity arising from differences in clinicians' expertise, manual interpretation often proves time-consuming and prone to inconsistency. Artificial intelligence (AI)-based automated dental image analysis (DIA) offers a promising solution to these issues and has become an integral part of computer-aided dental diagnosis and treatment. Among various AI technologies, deep learning (DL) stands out as the most widely applied and influential approach due to its superior feature extraction and representation capabilities. To comprehensively summarize recent progress in this field, we focus on the two fundamental aspects of DL research-datasets and models. In this paper, we systematically review 260 studies on DL applications in DIA, including 49 papers on publicly available dental datasets and 211 papers on DL-based algorithms. We first introduce the basic concepts of dental imaging and summarize the characteristics and acquisition methods of existing datasets. Then, we present the foundational techniques of DL and categorize relevant models and algorithms according to different DIA tasks, analyzing their network architectures, optimization strategies, training methods, and performance. Furthermore, we summarize commonly used training and evaluation metrics in the DIA domain. Finally, we discuss the current challenges of existing research and outline potential future directions. We hope that this work provides a valuable and systematic reference for researchers in this field. All supplementary materials and detailed comparison tables will be made publicly available on GitHub.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "52 pages, 24 figures. Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.20634v1",
    "published_date": "2025-10-23 15:05:06 UTC",
    "updated_date": "2025-10-23 15:05:06 UTC"
  },
  {
    "arxiv_id": "2510.20632v1",
    "title": "Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications",
    "authors": [
      "Shuyi Xie",
      "Ziqin Liew",
      "Hailing Zhang",
      "Haibo Zhang",
      "Ling Hu",
      "Zhiqiang Zhou",
      "Shuman Liu",
      "Anxiang Zeng"
    ],
    "abstract": "Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet their capabilities in specialized domains remain underexplored. In e-commerce, existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping MMLU-suffer from limited task diversity (e.g., lacking product guidance and after-sales issues), limited task modalities (e.g., absence of multimodal data), synthetic or curated data, and a narrow focus on English and Chinese, leaving practitioners without reliable tools to assess models on complex, real-world shopping scenarios. We introduce EcomEval, a comprehensive multilingual and multimodal benchmark for evaluating LLMs in e-commerce. EcomEval covers six categories and 37 tasks (including 8 multimodal tasks), sourced primarily from authentic customer queries and transaction logs, reflecting the noisy and heterogeneous nature of real business interactions. To ensure both quality and scalability of reference answers, we adopt a semi-automatic pipeline in which large models draft candidate responses subsequently reviewed and modified by over 50 expert annotators with strong e-commerce and multilingual expertise. We define difficulty levels for each question and task category by averaging evaluation scores across models with different sizes and capabilities, enabling challenge-oriented and fine-grained assessment. EcomEval also spans seven languages-including five low-resource Southeast Asian languages-offering a multilingual perspective absent from prior work.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20632v1",
    "published_date": "2025-10-23 15:04:32 UTC",
    "updated_date": "2025-10-23 15:04:32 UTC"
  },
  {
    "arxiv_id": "2510.20630v1",
    "title": "Quantum Processing Unit (QPU) processing time Prediction with Machine Learning",
    "authors": [
      "Lucy Xing",
      "Sanjay Vishwakarma",
      "David Kremer",
      "Francisco Martin-Fernandez",
      "Ismael Faro",
      "Juan Cruz-Benito"
    ],
    "abstract": "This paper explores the application of machine learning (ML) techniques in predicting the QPU processing time of quantum jobs. By leveraging ML algorithms, this study introduces predictive models that are designed to enhance operational efficiency in quantum computing systems. Using a dataset of about 150,000 jobs that follow the IBM Quantum schema, we employ ML methods based on Gradient-Boosting (LightGBM) to predict the QPU processing times, incorporating data preprocessing methods to improve model accuracy. The results demonstrate the effectiveness of ML in forecasting quantum jobs. This improvement can have implications on improving resource management and scheduling within quantum computing frameworks. This research not only highlights the potential of ML in refining quantum job predictions but also sets a foundation for integrating AI-driven tools in advanced quantum computing operations.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "Technical paper accepted at the IEEE Quantum Week 2025 Conference",
    "pdf_url": "https://arxiv.org/pdf/2510.20630v1",
    "published_date": "2025-10-23 15:04:18 UTC",
    "updated_date": "2025-10-23 15:04:18 UTC"
  },
  {
    "arxiv_id": "2510.20629v1",
    "title": "Equitable Survival Prediction: A Fairness-Aware Survival Modeling (FASM) Approach",
    "authors": [
      "Mingxuan Liu",
      "Yilin Ning",
      "Haoyuan Wang",
      "Chuan Hong",
      "Matthew Engelhard",
      "Danielle S. Bitterman",
      "William G. La Cava",
      "Nan Liu"
    ],
    "abstract": "As machine learning models become increasingly integrated into healthcare, structural inequities and social biases embedded in clinical data can be perpetuated or even amplified by data-driven models. In survival analysis, censoring and time dynamics can further add complexity to fair model development. Additionally, algorithmic fairness approaches often overlook disparities in cross-group rankings, e.g., high-risk Black patients may be ranked below lower-risk White patients who do not experience the event of mortality. Such misranking can reinforce biological essentialism and undermine equitable care. We propose a Fairness-Aware Survival Modeling (FASM), designed to mitigate algorithmic bias regarding both intra-group and cross-group risk rankings over time. Using breast cancer prognosis as a representative case and applying FASM to SEER breast cancer data, we show that FASM substantially improves fairness while preserving discrimination performance comparable to fairness-unaware survival models. Time-stratified evaluations show that FASM maintains stable fairness over a 10-year horizon, with the greatest improvements observed during the mid-term of follow-up. Our approach enables the development of survival models that prioritize both accuracy and equity in clinical decision-making, advancing fairness as a core principle in clinical care.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20629v1",
    "published_date": "2025-10-23 15:03:27 UTC",
    "updated_date": "2025-10-23 15:03:27 UTC"
  },
  {
    "arxiv_id": "2510.20621v2",
    "title": "Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms",
    "authors": [
      "Riccardo Guidotti",
      "Martina Cinquini",
      "Marta Marchiori Manerba",
      "Mattia Setzu",
      "Francesco Spinnato"
    ],
    "abstract": "Interpretable-by-design models are crucial for fostering trust, accountability, and safe adoption of automated decision-making models in real-world applications. In this paper we formalize the ground for the MIMOSA (Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a comprehensive methodology for generating predictive models that balance interpretability with performance while embedding key ethical properties. We formally define here the supervised learning setting across diverse decision-making tasks and data types, including tabular data, time series, images, text, transactions, and trajectories. We characterize three major families of interpretable models: feature importance, rule, and instance based models. For each family, we analyze their interpretability dimensions, reasoning mechanisms, and complexity. Beyond interpretability, we formalize three critical ethical properties, namely causality, fairness, and privacy, providing formal definitions, evaluation metrics, and verification procedures for each. We then examine the inherent trade-offs between these properties and discuss how privacy requirements, fairness constraints, and causal reasoning can be embedded within interpretable pipelines. By evaluating ethical measures during model generation, this framework establishes the theoretical foundations for developing AI systems that are not only accurate and interpretable but also fair, privacy-preserving, and causally aware, i.e., trustworthy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20621v2",
    "published_date": "2025-10-23 14:54:33 UTC",
    "updated_date": "2025-10-30 16:26:39 UTC"
  },
  {
    "arxiv_id": "2510.21879v1",
    "title": "TernaryCLIP: Efficiently Compressing Vision-Language Models with Ternary Weights and Distilled Knowledge",
    "authors": [
      "Shu-Hao Zhang",
      "Wei-Cheng Tang",
      "Chen Wu",
      "Peng Hu",
      "Nan Li",
      "Liang-Jie Zhang",
      "Qi Zhang",
      "Shao-Qun Zhang"
    ],
    "abstract": "Recent years have witnessed an increasing interest in image-text contrastive modeling, exemplified by models such as Contrastive Language-Image Pretraining (CLIP). In this paper, we propose the TernaryCLIP, a lightweight computational framework that converts connection weights of both vision and text encoders of CLIP into the ternary format, instead of full-precision or floating ones. TernaryCLIP incorporates quantization-aware training and distillation modules, preventing precision degradation and enabling low-cost and high-efficiency computations. Comprehensive experiments demonstrate that TernaryCLIP can achieve up to 99\\% ternarized weights with 1.58-bit representation, 16.98 $\\times$ compression ratio, 2.3 $\\times$ inference acceleration, 16 $\\times$ storage reduction, 10 $\\times$ memory optimization, and 60\\% sparsity while maintaining promising performance on zero-shot image classification and image-text retrieval tasks across 41 commonly used datasets. Our work highlights the feasibility of extreme quantization for large multimodal models, supporting effective and efficient deployment on resource-constrained devices. The model and code can be accessed from Hugging Face and GitHub.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21879v1",
    "published_date": "2025-10-23 14:53:32 UTC",
    "updated_date": "2025-10-23 14:53:32 UTC"
  },
  {
    "arxiv_id": "2510.20612v1",
    "title": "Black Box Absorption: LLMs Undermining Innovative Ideas",
    "authors": [
      "Wenjun Cao"
    ],
    "abstract": "Large Language Models are increasingly adopted as critical tools for accelerating innovation. This paper identifies and formalizes a systemic risk inherent in this paradigm: \\textbf{Black Box Absorption}. We define this as the process by which the opaque internal architectures of LLM platforms, often operated by large-scale service providers, can internalize, generalize, and repurpose novel concepts contributed by users during interaction. This mechanism threatens to undermine the foundational principles of innovation economics by creating severe informational and structural asymmetries between individual creators and platform operators, thereby jeopardizing the long-term sustainability of the innovation ecosystem. To analyze this challenge, we introduce two core concepts: the idea unit, representing the transportable functional logic of an innovation, and idea safety, a multidimensional standard for its protection. This paper analyzes the mechanisms of absorption and proposes a concrete governance and engineering agenda to mitigate these risks, ensuring that creator contributions remain traceable, controllable, and equitable.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "econ.GN"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20612v1",
    "published_date": "2025-10-23 14:43:09 UTC",
    "updated_date": "2025-10-23 14:43:09 UTC"
  },
  {
    "arxiv_id": "2510.20611v1",
    "title": "PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection",
    "authors": [
      "Mirza Raquib",
      "Niloy Das",
      "Farida Siddiqi Prity",
      "Arafath Al Fahim",
      "Saydul Akbar Murad",
      "Mohammad Amzad Hossain",
      "MD Jiabul Hoque",
      "Mohammad Ali Moni"
    ],
    "abstract": "Breast cancer is considered the most critical and frequently diagnosed cancer in women worldwide, leading to an increase in cancer-related mortality. Early and accurate detection is crucial as it can help mitigate possible threats while improving survival rates. In terms of prediction, conventional diagnostic methods are often limited by variability, cost, and, most importantly, risk of misdiagnosis. To address these challenges, machine learning (ML) has emerged as a powerful tool for computer-aided diagnosis, with feature selection playing a vital role in improving model performance and interpretability. This research study proposes an integrated framework that incorporates customized Particle Swarm Optimization (PSO) for feature selection. This framework has been evaluated on a comprehensive set of 29 different models, spanning classical classifiers, ensemble techniques, neural networks, probabilistic algorithms, and instance-based algorithms. To ensure interpretability and clinical relevance, the study uses cross-validation in conjunction with explainable AI methods. Experimental evaluation showed that the proposed approach achieved a superior score of 99.1\\% across all performance metrics, including accuracy and precision, while effectively reducing dimensionality and providing transparent, model-agnostic explanations. The results highlight the potential of combining swarm intelligence with explainable ML for robust, trustworthy, and clinically meaningful breast cancer diagnosis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20611v1",
    "published_date": "2025-10-23 14:42:50 UTC",
    "updated_date": "2025-10-23 14:42:50 UTC"
  },
  {
    "arxiv_id": "2510.20610v2",
    "title": "BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection",
    "authors": [
      "Ali Zain",
      "Sareem Farooqui",
      "Muhammad Rafi"
    ],
    "abstract": "This paper details our submission to the AraGenEval Shared Task on Arabic AI-generated text detection, where our team, BUSTED, secured 5th place. We investigated the effectiveness of three pre-trained transformer models: AraELECTRA, CAMeLBERT, and XLM-RoBERTa. Our approach involved fine-tuning each model on the provided dataset for a binary classification task. Our findings revealed a surprising result: the multilingual XLM-RoBERTa model achieved the highest performance with an F1 score of 0.7701, outperforming the specialized Arabic models. This work underscores the complexities of AI-generated text detection and highlights the strong generalization capabilities of multilingual models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20610v2",
    "published_date": "2025-10-23 14:41:04 UTC",
    "updated_date": "2025-10-25 15:33:29 UTC"
  },
  {
    "arxiv_id": "2510.20609v1",
    "title": "Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets",
    "authors": [
      "Timur Galimzyanov",
      "Olga Kolomyttseva",
      "Egor Bogomolov"
    ],
    "abstract": "We study retrieval design for code-focused generation tasks under realistic compute budgets. Using two complementary tasks from Long Code Arena -- code completion and bug localization -- we systematically compare retrieval configurations across various context window sizes along three axes: (i) chunking strategy, (ii) similarity scoring, and (iii) splitting granularity. (1) For PL-PL, sparse BM25 with word-level splitting is the most effective and practical, significantly outperforming dense alternatives while being an order of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3 family) consistently beat sparse retrievers, however requiring 100x larger latency. (3) Optimal chunk size scales with available context: 32-64 line chunks work best at small budgets, and whole-file retrieval becomes competitive at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting across budgets. (5) Retrieval latency varies by up to 200x across configurations; BPE-based splitting is needlessly slow, and BM25 + word splitting offers the best quality-latency trade-off. Thus, we provide evidence-based recommendations for implementing effective code-oriented RAG systems based on task requirements, model constraints, and computational efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20609v1",
    "published_date": "2025-10-23 14:40:11 UTC",
    "updated_date": "2025-10-23 14:40:11 UTC"
  },
  {
    "arxiv_id": "2510.20607v1",
    "title": "Generalizable Reasoning through Compositional Energy Minimization",
    "authors": [
      "Alexandru Oarga",
      "Yilun Du"
    ],
    "abstract": "Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: https://alexoarga.github.io/compositional_reasoning/",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20607v1",
    "published_date": "2025-10-23 14:38:36 UTC",
    "updated_date": "2025-10-23 14:38:36 UTC"
  },
  {
    "arxiv_id": "2510.20605v1",
    "title": "OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects",
    "authors": [
      "Mark He Huang",
      "Lin Geng Foo",
      "Christian Theobalt",
      "Ying Sun",
      "De Wen Soh"
    ],
    "abstract": "Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025 (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2510.20605v1",
    "published_date": "2025-10-23 14:37:25 UTC",
    "updated_date": "2025-10-23 14:37:25 UTC"
  },
  {
    "arxiv_id": "2510.20604v1",
    "title": "Efficient Algorithms for Computing Random Walk Centrality",
    "authors": [
      "Changan Liu",
      "Zixuan Xie",
      "Ahad N. Zehmakan",
      "Zhongzhi Zhang"
    ],
    "abstract": "Random walk centrality is a fundamental metric in graph mining for quantifying node importance and influence, defined as the weighted average of hitting times to a node from all other nodes. Despite its ability to capture rich graph structural information and its wide range of applications, computing this measure for large networks remains impractical due to the computational demands of existing methods. In this paper, we present a novel formulation of random walk centrality, underpinning two scalable algorithms: one leveraging approximate Cholesky factorization and sparse inverse estimation, while the other sampling rooted spanning trees. Both algorithms operate in near-linear time and provide strong approximation guarantees. Extensive experiments on large real-world networks, including one with over 10 million nodes, demonstrate the efficiency and approximation quality of the proposed algorithms.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by TKDE",
    "pdf_url": "https://arxiv.org/pdf/2510.20604v1",
    "published_date": "2025-10-23 14:36:38 UTC",
    "updated_date": "2025-10-23 14:36:38 UTC"
  },
  {
    "arxiv_id": "2510.21876v1",
    "title": "AI Powered Urban Green Infrastructure Assessment Through Aerial Imagery of an Industrial Township",
    "authors": [
      "Anisha Dutta"
    ],
    "abstract": "Accurate assessment of urban canopy coverage is crucial for informed urban planning, effective environmental monitoring, and mitigating the impacts of climate change. Traditional practices often face limitations due to inadequate technical requirements, difficulties in scaling and data processing, and the lack of specialized expertise. This study presents an efficient approach for estimating green canopy coverage using artificial intelligence, specifically computer vision techniques, applied to aerial imageries. Our proposed methodology utilizes object-based image analysis, based on deep learning algorithms to accurately identify and segment green canopies from high-resolution drone images. This approach allows the user for detailed analysis of urban vegetation, capturing variations in canopy density and understanding spatial distribution. To overcome the computational challenges associated with processing large datasets, it was implemented over a cloud platform utilizing high-performance processors. This infrastructure efficiently manages space complexity and ensures affordable latency, enabling the rapid analysis of vast amounts of drone imageries. Our results demonstrate the effectiveness of this approach in accurately estimating canopy coverage at the city scale, providing valuable insights for urban forestry management of an industrial township. The resultant data generated by this method can be used to optimize tree plantation and assess the carbon sequestration potential of urban forests. By integrating these insights into sustainable urban planning, we can foster more resilient urban environments, contributing to a greener and healthier future.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Presented at IIIE Conference 2024, Jamshedpur",
    "pdf_url": "https://arxiv.org/pdf/2510.21876v1",
    "published_date": "2025-10-23 14:36:20 UTC",
    "updated_date": "2025-10-23 14:36:20 UTC"
  },
  {
    "arxiv_id": "2510.20603v1",
    "title": "What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation",
    "authors": [
      "Heejin Do",
      "Jaehui Hwang",
      "Dongyoon Han",
      "Seong Joon Oh",
      "Sangdoo Yun"
    ],
    "abstract": "Evaluating large language models (LLMs) on final-answer correctness is the dominant paradigm. This approach, however, provides a coarse signal for model improvement and overlooks the quality of the underlying reasoning process. We argue that a more granular evaluation of reasoning offers a more effective path to building robust models. We decompose reasoning quality into two dimensions: relevance and coherence. Relevance measures if a step is grounded in the problem; coherence measures if it follows logically from prior steps. To measure these aspects reliably, we introduce causal stepwise evaluation (CaSE). This method assesses each reasoning step using only its preceding context, which avoids hindsight bias. We validate CaSE against human judgments on our new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we show that curating training data with CaSE-evaluated relevance and coherence directly improves final task performance. Our work provides a scalable framework for analyzing, debugging, and improving LLM reasoning, demonstrating the practical value of moving beyond validity checks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20603v1",
    "published_date": "2025-10-23 14:30:37 UTC",
    "updated_date": "2025-10-23 14:30:37 UTC"
  },
  {
    "arxiv_id": "2510.20602v1",
    "title": "Resounding Acoustic Fields with Reciprocity",
    "authors": [
      "Zitong Lan",
      "Yiduo Hao",
      "Mingmin Zhao"
    ],
    "abstract": "Achieving immersive auditory experiences in virtual environments requires flexible sound modeling that supports dynamic source positions. In this paper, we introduce a task called resounding, which aims to estimate room impulse responses at arbitrary emitter location from a sparse set of measured emitter positions, analogous to the relighting problem in vision. We leverage the reciprocity property and introduce Versa, a physics-inspired approach to facilitating acoustic field learning. Our method creates physically valid samples with dense virtual emitter positions by exchanging emitter and listener poses. We also identify challenges in deploying reciprocity due to emitter/listener gain patterns and propose a self-supervised learning approach to address them. Results show that Versa substantially improve the performance of acoustic field learning on both simulated and real-world datasets across different metrics. Perceptual user studies show that Versa can greatly improve the immersive spatial sound experience. Code, dataset and demo videos are available on the project website: https://waves.seas.upenn.edu/projects/versa.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20602v1",
    "published_date": "2025-10-23 14:30:09 UTC",
    "updated_date": "2025-10-23 14:30:09 UTC"
  },
  {
    "arxiv_id": "2510.20596v1",
    "title": "Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation",
    "authors": [
      "Ziyu Ye",
      "Chen Ju",
      "Chaofan Ma",
      "Xiaoyun Zhang"
    ],
    "abstract": "Deep learning models have achieved great success on various vision challenges, but a well-trained model would face drastic performance degradation when applied to unseen data. Since the model is sensitive to domain shift, unsupervised domain adaptation attempts to reduce the domain gap and avoid costly annotation of unseen domains. This paper proposes a novel framework for cross-modality segmentation via similarity-based prototypes. In specific, we learn class-wise prototypes within an embedding space, then introduce a similarity constraint to make these prototypes representative for each semantic class while separable from different classes. Moreover, we use dictionaries to store prototypes extracted from different images, which prevents the class-missing problem and enables the contrastive learning of prototypes, and further improves performance. Extensive experiments show that our method achieves better results than other state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "MICCAI 2021",
    "pdf_url": "https://arxiv.org/pdf/2510.20596v1",
    "published_date": "2025-10-23 14:24:12 UTC",
    "updated_date": "2025-10-23 14:24:12 UTC"
  },
  {
    "arxiv_id": "2510.20591v1",
    "title": "Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting",
    "authors": [
      "Ali Rajaei",
      "Peter Palensky",
      "Jochen L. Cremer"
    ],
    "abstract": "Network topology optimization (NTO) via busbar splitting can mitigate transmission grid congestion and reduce redispatch costs. However, solving this mixed-integer non-linear problem for large-scale systems in near-real-time is currently intractable with existing solvers. Machine learning (ML) approaches have emerged as a promising alternative, but they have limited generalization to unseen topologies, varying operating conditions, and different systems, which limits their practical applicability. This paper formulates NTO for congestion management problem considering linearized AC PF, and proposes a graph neural network (GNN)-accelerated approach. We develop a heterogeneous edge-aware message passing NN to predict effective busbar splitting actions as candidate NTO solutions. The proposed GNN captures local flow patterns, achieves generalization to unseen topology changes, and improves transferability across systems. Case studies show up to 4 orders-of-magnitude speed-up, delivering AC-feasible solutions within one minute and a 2.3% optimality gap on the GOC 2000-bus system. These results demonstrate a significant step toward near-real-time NTO for large-scale systems with topology and cross-system generalization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20591v1",
    "published_date": "2025-10-23 14:16:23 UTC",
    "updated_date": "2025-10-23 14:16:23 UTC"
  },
  {
    "arxiv_id": "2510.20584v1",
    "title": "Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks",
    "authors": [
      "Jiangang Hao",
      "Wenju Cui",
      "Patrick Kyllonen",
      "Emily Kerzabi"
    ],
    "abstract": "Assessing communication and collaboration at scale depends on a labor intensive task of coding communication data into categories according to different frameworks. Prior research has established that ChatGPT can be directly instructed with coding rubrics to code the communication data and achieves accuracy comparable to human raters. However, whether the coding from ChatGPT or similar AI technology exhibits bias against different demographic groups, such as gender and race, remains unclear. To fill this gap, this paper investigates ChatGPT-based automated coding of communication data using a typical coding framework for collaborative problem solving, examining differences across gender and racial groups. The analysis draws on data from three types of collaborative tasks: negotiation, problem solving, and decision making. Our results show that ChatGPT-based coding exhibits no significant bias across gender and racial groups, paving the road for its adoption in large-scale assessment of collaboration and communication.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "38 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20584v1",
    "published_date": "2025-10-23 14:09:03 UTC",
    "updated_date": "2025-10-23 14:09:03 UTC"
  },
  {
    "arxiv_id": "2510.20579v1",
    "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence",
    "authors": [
      "Jiahao Meng",
      "Xiangtai Li",
      "Haochen Wang",
      "Yue Tan",
      "Tao Zhang",
      "Lingdong Kong",
      "Yunhai Tong",
      "Anran Wang",
      "Zhiyang Teng",
      "Yujing Wang",
      "Zhuochen Wang"
    ],
    "abstract": "Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20579v1",
    "published_date": "2025-10-23 14:05:56 UTC",
    "updated_date": "2025-10-23 14:05:56 UTC"
  },
  {
    "arxiv_id": "2510.23621v1",
    "title": "Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields",
    "authors": [
      "Alexandre Benoit"
    ],
    "abstract": "Machine-learning force fields can deliver accurate molecular dynamics (MD) at high computational cost. For SO(3)-equivariant models such as MACE, there is little systematic evidence on whether reduced-precision arithmetic and GPU-optimized kernels can cut this cost without harming physical fidelity. This thesis aims to make MACE cheaper and faster while preserving accuracy by identifying computational bottlenecks and evaluating low-precision execution policies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA cuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32 accumulation) for inference, short NVT and long NPT water simulations, and toy training runs under reproducible, steady-state timing. cuEquivariance reduces inference latency by about $3\\times$. Casting only linear layers to BF16/FP16 within an FP32 model yields roughly 4x additional speedups, while energies and thermodynamic observables in NVT/NPT MD remain within run-to-run variability. Half-precision weights during training degrade force RMSE. Mixing e3nn and cuEq modules without explicit adapters causes representation mismatches. Fused equivariant kernels and mixed-precision inference can substantially accelerate state-of-the-art force fields with negligible impact on downstream MD. A practical policy is to use cuEquivariance with FP32 by default and enable BF16/FP16 for linear layers (keeping FP32 accumulations) for maximum throughput, while training remains in FP32. Further gains are expected on Ampere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and pipeline fusion.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "78 pages, 21 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.23621v1",
    "published_date": "2025-10-23 14:02:34 UTC",
    "updated_date": "2025-10-23 14:02:34 UTC"
  },
  {
    "arxiv_id": "2510.20568v1",
    "title": "Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI",
    "authors": [
      "Susan Ariel Aaronson",
      "Michael Moreno"
    ],
    "abstract": "The worlds people have strong opinions about artificial intelligence (AI), and they want policymakers to listen. Governments are inviting public comment on AI, but as they translate input into policy, much of what citizens say is lost. Policymakers are missing a critical opportunity to build trust in AI and its governance. This paper compares three countries, Australia, Colombia, and the United States, that invited citizens to comment on AI risks and policies. Using a landscape analysis, the authors examined how each government solicited feedback and whether that input shaped governance. Yet in none of the three cases did citizens and policymakers establish a meaningful dialogue. Governments did little to attract diverse voices or publicize calls for comment, leaving most citizens unaware or unprepared to respond. In each nation, fewer than one percent of the population participated. Moreover, officials showed limited responsiveness to the feedback they received, failing to create an effective feedback loop. The study finds a persistent gap between the promise and practice of participatory AI governance. The authors conclude that current approaches are unlikely to build trust or legitimacy in AI because policymakers are not adequately listening or responding to public concerns. They offer eight recommendations: promote AI literacy; monitor public feedback; broaden outreach; hold regular online forums; use innovative engagement methods; include underrepresented groups; respond publicly to input; and make participation easier.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20568v1",
    "published_date": "2025-10-23 13:57:02 UTC",
    "updated_date": "2025-10-23 13:57:02 UTC"
  },
  {
    "arxiv_id": "2510.20566v1",
    "title": "AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN",
    "authors": [
      "Wei Shao",
      "Yuhao Wang",
      "Rongguang He",
      "Muhammad Ejaz Ahmed",
      "Seyit Camtepe"
    ],
    "abstract": "Existing defence mechanisms have demonstrated significant effectiveness in mitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined signatures and static heuristics to identify and block malicious traffic. However, the emergence of AI-driven techniques presents new challenges to SDN security, potentially compromising the efficacy of existing defence mechanisms. In this paper, we introduce~AdaDoS, an adaptive attack model that disrupt network operations while evading detection by existing DoS-based detectors through adversarial reinforcement learning (RL). Specifically, AdaDoS models the problem as a competitive game between an attacker, whose goal is to obstruct network traffic without being detected, and a detector, which aims to identify malicious traffic. AdaDoS can solve this game by dynamically adjusting its attack strategy based on feedback from the SDN and the detector. Additionally, recognising that attackers typically have less information than defenders, AdaDoS formulates the DoS-like attack as a partially observed Markov decision process (POMDP), with the attacker having access only to delay information between attacker and victim nodes. We address this challenge with a novel reciprocal learning module, where the student agent, with limited observations, enhances its performance by learning from the teacher agent, who has full observational capabilities in the SDN environment. AdaDoS represents the first application of RL to develop DoS-like attack sequences, capable of adaptively evading both machine learning-based and rule-based DoS-like attack detectors.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20566v1",
    "published_date": "2025-10-23 13:51:40 UTC",
    "updated_date": "2025-10-23 13:51:40 UTC"
  },
  {
    "arxiv_id": "2510.21874v1",
    "title": "A Physics-Informed Neural Network Approach for UAV Path Planning in Dynamic Environments",
    "authors": [
      "Shuning Zhang"
    ],
    "abstract": "Unmanned aerial vehicles (UAVs) operating in dynamic wind fields must generate safe and energy-efficient trajectories under physical and environmental constraints. Traditional planners, such as A* and kinodynamic RRT*, often yield suboptimal or non-smooth paths due to discretization and sampling limitations. This paper presents a physics-informed neural network (PINN) framework that embeds UAV dynamics, wind disturbances, and obstacle avoidance directly into the learning process. Without requiring supervised data, the PINN learns dynamically feasible and collision-free trajectories by minimizing physical residuals and risk-aware objectives. Comparative simulations show that the proposed method outperforms A* and Kino-RRT* in control energy, smoothness, and safety margin, while maintaining similar flight efficiency. The results highlight the potential of physics-informed learning to unify model-based and data-driven planning, providing a scalable and physically consistent framework for UAV trajectory optimization.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21874v1",
    "published_date": "2025-10-23 13:42:07 UTC",
    "updated_date": "2025-10-23 13:42:07 UTC"
  },
  {
    "arxiv_id": "2510.24763v1",
    "title": "Dual-Domain Deep Learning-Assisted NOMA-CSK Systems for Secure and Efficient Vehicular Communications",
    "authors": [
      "Tingting Huang",
      "Jundong Chen",
      "Huanqiang Zeng",
      "Guofa Cai",
      "Georges Kaddoum"
    ],
    "abstract": "Ensuring secure and efficient multi-user (MU) transmission is critical for vehicular communication systems. Chaos-based modulation schemes have garnered considerable interest due to their benefits in physical layer security. However, most existing MU chaotic communication systems, particularly those based on non-coherent detection, suffer from low spectral efficiency due to reference signal transmission, and limited user connectivity under orthogonal multiple access (OMA). While non-orthogonal schemes, such as sparse code multiple access (SCMA)-based DCSK, have been explored, they face high computational complexity and inflexible scalability due to their fixed codebook designs. This paper proposes a deep learning-assisted power domain non-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for vehicular communications. A deep neural network (DNN)-based demodulator is designed to learn intrinsic chaotic signal characteristics during offline training, thereby eliminating the need for chaotic synchronization or reference signal transmission. The demodulator employs a dual-domain feature extraction architecture that jointly processes the time-domain and frequency-domain information of chaotic signals, enhancing feature learning under dynamic channels. The DNN is integrated into the successive interference cancellation (SIC) framework to mitigate error propagation issues. Theoretical analysis and extensive simulations demonstrate that the proposed system achieves superior performance in terms of spectral efficiency (SE), energy efficiency (EE), bit error rate (BER), security, and robustness, while maintaining lower computational complexity compared to traditional MU-DCSK and existing DL-aided schemes. These advantages validate its practical viability for secure vehicular communications.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.24763v1",
    "published_date": "2025-10-23 13:41:00 UTC",
    "updated_date": "2025-10-23 13:41:00 UTC"
  },
  {
    "arxiv_id": "2510.20556v1",
    "title": "Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics",
    "authors": [
      "Alexandre Benoit",
      "Catherine Aitken",
      "Yu He"
    ],
    "abstract": "Graph rewiring has emerged as a key technique to alleviate over-squashing in Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph topology to improve information flow. While effective, rewiring inherently alters the graph's structure, raising the risk of distorting important topology-dependent signals. Yet, despite the growing use of rewiring, little is known about which structural properties must be preserved to ensure both performance gains and structural fidelity. In this work, we provide the first systematic analysis of how rewiring affects a range of graph structural metrics, and how these changes relate to downstream task performance. We study seven diverse rewiring strategies and correlate changes in local and global graph properties with node classification accuracy. Our results reveal a consistent pattern: successful rewiring methods tend to preserve local structure while allowing for flexibility in global connectivity. These findings offer new insights into the design of effective rewiring strategies, bridging the gap between graph theory and practical GNN optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 5 figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2510.20556v1",
    "published_date": "2025-10-23 13:38:41 UTC",
    "updated_date": "2025-10-23 13:38:41 UTC"
  },
  {
    "arxiv_id": "2510.20548v3",
    "title": "GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning",
    "authors": [
      "Jinchang Luo",
      "Mingquan Cheng",
      "Fan Wan",
      "Ni Li",
      "Xiaoling Xia",
      "Shuangshuang Tian",
      "Tingcheng Bian",
      "Haiwei Wang",
      "Haohuan Fu",
      "Yan Tao"
    ],
    "abstract": "Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.20548v3",
    "published_date": "2025-10-23 13:35:02 UTC",
    "updated_date": "2026-01-12 09:03:57 UTC"
  },
  {
    "arxiv_id": "2510.21872v1",
    "title": "GuitarFlow: Realistic Electric Guitar Synthesis From Tablatures via Flow Matching and Style Transfer",
    "authors": [
      "Jackson Loth",
      "Pedro Sarmento",
      "Mark Sandler",
      "Mathieu Barthet"
    ],
    "abstract": "Music generation in the audio domain using artificial intelligence (AI) has witnessed steady progress in recent years. However for some instruments, particularly the guitar, controllable instrument synthesis remains limited in expressivity. We introduce GuitarFlow, a model designed specifically for electric guitar synthesis. The generative process is guided using tablatures, an ubiquitous and intuitive guitar-specific symbolic format. The tablature format easily represents guitar-specific playing techniques (e.g. bends, muted strings and legatos), which are more difficult to represent in other common music notation formats such as MIDI. Our model relies on an intermediary step of first rendering the tablature to audio using a simple sample-based virtual instrument, then performing style transfer using Flow Matching in order to transform the virtual instrument audio into more realistic sounding examples. This results in a model that is quick to train and to perform inference, requiring less than 6 hours of training data. We present the results of objective evaluation metrics, together with a listening test, in which we show significant improvement in the realism of the generated guitar audio from tablatures.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "To be published in Proceedings of the 17th International Symposium on Computer Music and Multidisciplinary Research (CMMR)",
    "pdf_url": "https://arxiv.org/pdf/2510.21872v1",
    "published_date": "2025-10-23 13:31:41 UTC",
    "updated_date": "2025-10-23 13:31:41 UTC"
  },
  {
    "arxiv_id": "2510.20543v2",
    "title": "The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts",
    "authors": [
      "Sangmitra Madhusudan",
      "Kaige Chen",
      "Ali Emami"
    ],
    "abstract": "When language models correctly parse \"The cat that the dog chased meowed,\" are they analyzing syntax or simply familiar with dogs chasing cats? Despite extensive benchmarking, we lack methods to distinguish structural understanding from semantic pattern matching. We introduce CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences (like \"The cat [that the dog chased] meowed\") where relative clauses nest recursively, creating processing demands from simple to deeply nested structures. Each sentence has a syntactically identical but semantically implausible counterpart (e.g., mailmen prescribe medicine, doctors deliver mail) and six comprehension questions testing surface understanding, syntactic dependencies, and causal reasoning. Testing six models reveals that performance gaps between plausible and implausible sentences widen systematically with complexity, with models showing median gaps up to 26.8 percentage points, quantifying when they abandon structural analysis for semantic associations. Notably, semantic plausibility harms performance on questions about resulting actions, where following causal relationships matters more than semantic coherence. Reasoning models improve accuracy but their traces show semantic shortcuts, overthinking, and answer refusal. Unlike models whose plausibility advantage systematically widens with complexity, humans shows variable semantic effects. CenterBench provides the first framework to identify when models shift from structural analysis to pattern matching.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages (excluding references), accepted to EACL 2026 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2510.20543v2",
    "published_date": "2025-10-23 13:30:40 UTC",
    "updated_date": "2026-01-20 17:46:36 UTC"
  },
  {
    "arxiv_id": "2510.20535v1",
    "title": "ARC-Encoder: learning compressed text representations for large language models",
    "authors": [
      "Hippolyte Pilchen",
      "Edouard Grave",
      "Patrick Pérez"
    ],
    "abstract": "Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs $x$-times fewer continuous representations (typically $x\\!\\in\\!\\{4,8\\}$) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20535v1",
    "published_date": "2025-10-23 13:20:57 UTC",
    "updated_date": "2025-10-23 13:20:57 UTC"
  },
  {
    "arxiv_id": "2510.20531v1",
    "title": "Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis",
    "authors": [
      "Lixiong Qin",
      "Yang Zhang",
      "Mei Wang",
      "Jiani Hu",
      "Weihong Deng",
      "Weiran Xu"
    ],
    "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has bridged the gap between vision and language tasks, enabling the implementation of Explainable DeepFake Analysis (XDFA). However, current methods suffer from a lack of fine-grained awareness: the description of artifacts in data annotation is unreliable and coarse-grained, and the models fail to support the output of connections between textual forgery explanations and the visual evidence of artifacts, as well as the input of queries for arbitrary facial regions. As a result, their responses are not sufficiently grounded in Face Visual Context (Facext). To address this limitation, we propose the Fake-in-Facext (FiFa) framework, with contributions focusing on data annotation and model construction. We first define a Facial Image Concept Tree (FICT) to divide facial images into fine-grained regional concepts, thereby obtaining a more reliable data annotation pipeline, FiFa-Annotator, for forgery explanation. Based on this dedicated data annotation, we introduce a novel Artifact-Grounding Explanation (AGE) task, which generates textual forgery explanations interleaved with segmentation masks of manipulated artifacts. We propose a unified multi-task learning architecture, FiFa-MLLM, to simultaneously support abundant multimodal inputs and outputs for fine-grained Explainable DeepFake Analysis. With multiple auxiliary supervision tasks, FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA performance on existing XDFA datasets. The code and data will be made open-source at https://github.com/lxq1000/Fake-in-Facext.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 9 figures, 17 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.20531v1",
    "published_date": "2025-10-23 13:16:12 UTC",
    "updated_date": "2025-10-23 13:16:12 UTC"
  },
  {
    "arxiv_id": "2510.20519v2",
    "title": "Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning",
    "authors": [
      "Xiaohan Lan",
      "Fanfan Liu",
      "Haibo Qiu",
      "Siqi Yang",
      "Delian Ruan",
      "Peng Shi",
      "Lin Ma"
    ],
    "abstract": "Inspired by recent advancements in LLM reasoning, the field of multimodal reasoning has seen remarkable progress, achieving significant performance gains on intricate tasks such as mathematical problem-solving. Despite this progress, current multimodal large reasoning models exhibit two key limitations. They tend to employ computationally expensive reasoning even for simple queries, leading to inefficiency. Furthermore, this focus on specialized reasoning often impairs their broader, more general understanding capabilities. In this paper, we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by structuring the original dense model into two distinct expert branches: a thinking branch tailored for complex, multi-step reasoning, and a non-thinking branch optimized for rapid, direct inference on tasks like general VQA and OCR. A lightweight, trainable router dynamically allocates queries to the most suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into an MoE architecture. Comprehensive evaluations reveal that our approach not only substantially enhances complex reasoning abilities but also improves the model's general capabilities, reversing the degradation trend observed in other reasoning-specialized models. Our work establishes a new paradigm for building powerful and versatile MLLMs, effectively resolving the prevalent reasoning-vs-generalization dilemma. Code and weights are available at https://github.com/MM-Thinking/Metis-HOME.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20519v2",
    "published_date": "2025-10-23 13:02:49 UTC",
    "updated_date": "2025-11-25 07:57:22 UTC"
  },
  {
    "arxiv_id": "2510.20505v1",
    "title": "Hierarchical Sequence Iteration for Heterogeneous Question Answering",
    "authors": [
      "Ruiyi Yang",
      "Hao Xue",
      "Imran Razzak",
      "Hakim Hacid",
      "Flora D. Salim"
    ],
    "abstract": "Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration for Heterogeneous Question Answering, a unified framework that (i) linearize documents, tables, and knowledge graphs into a reversible hierarchical sequence with lightweight structural tags, and (ii) perform structure-aware iteration to collect just-enough evidence before answer synthesis. A Head Agent provides guidance that leads retrieval, while an Iteration Agent selects and expands HSeq via structure-respecting actions (e.g., parent/child hops, table row/column neighbors, KG relations); Finally the head agent composes canonicalized evidence to genearte the final answer, with an optional refinement loop to resolve detected contradictions. Experiments on HotpotQA (text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1 gains over strong single-pass, multi-hop, and agentic RAG baselines with high efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic unification that enables a single policy to operate across text, tables, and KGs without per-dataset specialization; (2) guided, budget-aware iteration that reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and (3) evidence canonicalization for reliable QA, improving answers consistency and auditability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20505v1",
    "published_date": "2025-10-23 12:48:18 UTC",
    "updated_date": "2025-10-23 12:48:18 UTC"
  },
  {
    "arxiv_id": "2510.21501v1",
    "title": "GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs",
    "authors": [
      "Guanghao Zheng",
      "Bowen Shi",
      "Mingxing Xu",
      "Ruoyu Sun",
      "Peisen Zhao",
      "Zhibo Zhang",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong",
      "Xiaopeng Zhang",
      "Qi Tian"
    ],
    "abstract": "Vision encoders are indispensable for allowing impressive performance of Multi-modal Large Language Models (MLLMs) in vision language tasks such as visual question answering and reasoning. However, existing vision encoders focus on global image representations but overlook fine-grained regional analysis. They are limited in fine grained perception due to the scarcity of fine grained annotated data and the lack of a fine grained pre-training paradigm. In this paper, we propose GranViT, a novel Vision Transformer that integrates fine-grained feature extraction with semantic alignment to Large Language Models (LLMs) via region level autoregressive training. We first construct Gran-29M, a dataset comprising 2million natural and OCR images paired with over 180 million high-quality region-level annotations, to enable large scale fine grained pretraining. Consequently, we develop a pretraining-adaptation framework along with a self distillation mechanism to train fine-grained GranViT on Gran-29M. We sufficiently exploit the fine-grained annotations from Gran-29M to resort to bounding-box-to-caption regression to enhance localized visual representation of the vision encoder in the pretraining and caption-to-bounding-box regression to improve vision feature utilization and localization for LLM in the adaptation. We further incorporate a self distillation mechanism that imposes explicit localization constraints on the vision encoder to strengthen its regional reasoning capability. Extensive experiments show that GranViT surpasses existing vision encoders and attains strong transferability to varying LLMs. Remarkably, it achieves state-of-the-art results on fine-grained recognition, multimodal VQA, and OCR understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21501v1",
    "published_date": "2025-10-23 12:33:59 UTC",
    "updated_date": "2025-10-23 12:33:59 UTC"
  },
  {
    "arxiv_id": "2510.20487v4",
    "title": "Steering Evaluation-Aware Language Models to Act Like They Are Deployed",
    "authors": [
      "Tim Tian Hua",
      "Andrew Qin",
      "Samuel Marks",
      "Neel Nanda"
    ],
    "abstract": "Large language models (LLMs) can sometimes detect when they are being evaluated and adjust their behavior to appear more aligned, compromising the reliability of safety evaluations. In this paper, we show that adding a steering vector to an LLM's activations can suppress evaluation-awareness and make the model act like it is deployed during evaluation. To study our steering technique, we train an LLM to exhibit evaluation-aware behavior using a two-step training process designed to mimic how this behavior could emerge naturally. First, we perform continued pretraining on documents with factual descriptions of the model (1) using Python type hints during evaluation but not during deployment and (2) recognizing that the presence of a certain evaluation cue always means that it is being tested. Then, we train the model with expert iteration to use Python type hints in evaluation settings. The resulting model is evaluation-aware: it writes type hints in evaluation contexts more than deployment contexts. We find that activation steering can suppress evaluation awareness and make the model act like it is deployed even when the cue is present. Importantly, we constructed our steering vector using the original model before our additional training. Our results suggest that AI evaluators could improve the reliability of safety evaluations by steering models to act like they are deployed.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20487v4",
    "published_date": "2025-10-23 12:29:16 UTC",
    "updated_date": "2026-01-05 00:55:05 UTC"
  },
  {
    "arxiv_id": "2510.20878v1",
    "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data Placement",
    "authors": [
      "Danying Ge",
      "Jianhua Gao",
      "Yixue Yang",
      "Weixing Ji"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) improves model output accuracy by leveraging external knowledge bases, serving as an effective solution to address hallucination issues and knowledge-update delays in Large Language Models (LLMs). However, the introduction of external knowledge bases presents RAG with challenges in long-context processing, significantly increasing memory consumption and inference latency. Existing research accelerates inference by precomputing Key and Value (KV) of the knowledge base and loading them on-demand during inference. Based on the access frequency of different KV chunks within the external knowledge base, this paper proposes a hotness-aware RAG (HA-RAG) inference optimization system. First, leveraging the numerical distribution of KV chunks, we introduce a hotness-aware mixed-precision compressing and loading method to reduce disk I/O and memory access overhead. Second, we design a hotness-aware data placement strategy that prioritizes storing frequently accessed KV chunks in high-speed memory to improve data access efficiency. Experimental results demonstrate that, compared with TurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum speedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages,16 figures,2 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.20878v1",
    "published_date": "2025-10-23 12:28:58 UTC",
    "updated_date": "2025-10-23 12:28:58 UTC"
  },
  {
    "arxiv_id": "2510.20486v1",
    "title": "Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval",
    "authors": [
      "Fangjian Zhang",
      "Xiaoyong Zhuge",
      "Wenlan Wang",
      "Haixia Xiao",
      "Yuying Zhu",
      "Siyang Cheng"
    ],
    "abstract": "Artificial intelligence has advanced quantitative remote sensing, yet its effectiveness is constrained by imbalanced label distribution. This imbalance leads conventionally trained models to favor common samples, which in turn degrades retrieval performance for rare ones. Rainfall retrieval exemplifies this issue, with performance particularly compromised for heavy rain. This study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework. Following a divide-and-conquer strategy, imbalance in the rain distribution is decomposed into two components: zero inflation, defined by the predominance of non-rain samples; and long tail, defined by the disproportionate abundance of light-rain samples relative to heavy-rain samples. A hurdle model is adopted to handle the zero inflation, while IMDL is proposed to address the long tail by transforming the learning object into an unbiased ideal inverse model. Comprehensive evaluation via statistical metrics and case studies investigating rainy weather in eastern China confirms Hurdle-IMDL's superiority over conventional, cost-sensitive, generative, and multi-task learning methods. Its key advancements include effective mitigation of systematic underestimation and a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a generalizable approach for addressing imbalance in distributions of environmental variables, enabling enhanced retrieval of rare yet high-impact events.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph",
      "physics.geo-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.20486v1",
    "published_date": "2025-10-23 12:25:52 UTC",
    "updated_date": "2025-10-23 12:25:52 UTC"
  },
  {
    "arxiv_id": "2510.20479v1",
    "title": "RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging",
    "authors": [
      "Bowen Wang",
      "Haiyuan Wan",
      "Liwen Shi",
      "Chen Yang",
      "Peng He",
      "Yue Ma",
      "Haochen Han",
      "Wenhao Li",
      "Tiao Tan",
      "Yongjian Li",
      "Fangming Liu",
      "Yifan Gong",
      "Sheng Zhang"
    ],
    "abstract": "We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20479v1",
    "published_date": "2025-10-23 12:17:37 UTC",
    "updated_date": "2025-10-23 12:17:37 UTC"
  },
  {
    "arxiv_id": "2510.20469v1",
    "title": "Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks",
    "authors": [
      "Horacio Paggi",
      "Juan A. Lara",
      "Javier Soriano"
    ],
    "abstract": "There has recently been a major advance with respect to how information fusion is performed. Information fusion has gone from being conceived as a purely hierarchical procedure, as is the case of traditional military applications, to now being regarded collaboratively, as holonic fusion, which is better suited for civil applications and edge organizations. The above paradigm shift is being boosted as information fusion gains ground in different non-military areas, and human-computer and machine-machine communications, where holarchies, which are more flexible structures than ordinary, static hierarchies, become more widespread. This paper focuses on showing how holonic structures tend to be generated when there are constraints on resources (energy, available messages, time, etc.) for interactions based on a set of fully intercommunicating elements (peers) whose components fuse information as a means of optimizing the impact of vagueness and uncertainty present message exchanges. Holon formation is studied generically based on a multiagent system model, and an example of its possible operation is shown. Holonic structures have a series of advantages, such as adaptability, to sudden changes in the environment or its composition, are somewhat autonomous and are capable of cooperating in order to achieve a common goal. This can be useful when the shortage of resources prevents communications or when the system components start to fail.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20469v1",
    "published_date": "2025-10-23 12:07:32 UTC",
    "updated_date": "2025-10-23 12:07:32 UTC"
  },
  {
    "arxiv_id": "2510.20468v1",
    "title": "Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models",
    "authors": [
      "Tomáš Souček",
      "Sylvestre-Alvise Rebuffi",
      "Pierre Fernandez",
      "Nikola Jovanović",
      "Hady Elsahar",
      "Valeriu Lacatusu",
      "Tuan Tran",
      "Alexandre Mourachko"
    ],
    "abstract": "Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20468v1",
    "published_date": "2025-10-23 12:06:35 UTC",
    "updated_date": "2025-10-23 12:06:35 UTC"
  },
  {
    "arxiv_id": "2510.20467v1",
    "title": "FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic",
    "authors": [
      "Yiwen Peng",
      "Thomas Bonald",
      "Fabian M. Suchanek"
    ],
    "abstract": "Knowledge graph alignment is the task of matching equivalent entities (that is, instances and classes) and relations across two knowledge graphs. Most existing methods focus on pure entity-level alignment, computing the similarity of entities in some embedding space. They lack interpretable reasoning and need training data to work. In this paper, we propose FLORA, a simple yet effective method that (1) is unsupervised, i.e., does not require training data, (2) provides a holistic alignment for entities and relations iteratively, (3) is based on fuzzy logic and thus delivers interpretable results, (4) provably converges, (5) allows dangling entities, i.e., entities without a counterpart in the other KG, and (6) achieves state-of-the-art results on major benchmarks.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20467v1",
    "published_date": "2025-10-23 12:05:31 UTC",
    "updated_date": "2025-10-23 12:05:31 UTC"
  },
  {
    "arxiv_id": "2510.20457v1",
    "title": "Neural Reasoning for Robust Instance Retrieval in $\\mathcal{SHOIQ}$",
    "authors": [
      "Louis Mozart Kamdem Teyou",
      "Luke Friedrichs",
      "N'Dah Jean Kouagou",
      "Caglar Demir",
      "Yasir Mahmood",
      "Stefan Heindorf",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "abstract": "Concept learning exploits background knowledge in the form of description logic axioms to learn explainable classification models from knowledge bases. Despite recent breakthroughs in neuro-symbolic concept learning, most approaches still cannot be deployed on real-world knowledge bases. This is due to their use of description logic reasoners, which are not robust against inconsistencies nor erroneous data. We address this challenge by presenting a novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to approximate the results of a symbolic reasoner. We show that EBR solely requires retrieving instances for atomic concepts and existential restrictions to retrieve or approximate the set of instances of any concept in the description logic $\\mathcal{SHOIQ}$. In our experiments, we compare EBR with state-of-the-art reasoners. Our results suggest that EBR is robust against missing and erroneous data in contrast to existing reasoners.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as a full research paper at K-CAP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20457v1",
    "published_date": "2025-10-23 11:48:43 UTC",
    "updated_date": "2025-10-23 11:48:43 UTC"
  },
  {
    "arxiv_id": "2510.20877v1",
    "title": "Multimodal Negative Learning",
    "authors": [
      "Baoquan Gong",
      "Xiyuan Gao",
      "Pengfei Zhu",
      "Qinghua Hu",
      "Bing Cao"
    ],
    "abstract": "Multimodal learning systems often encounter challenges related to modality imbalance, where a dominant modality may overshadow others, thereby hindering the learning of weak modalities. Conventional approaches often force weak modalities to align with dominant ones in \"Learning to be (the same)\" (Positive Learning), which risks suppressing the unique information inherent in the weak modalities. To address this challenge, we offer a new learning paradigm: \"Learning Not to be\" (Negative Learning). Instead of enhancing weak modalities' target-class predictions, the dominant modalities dynamically guide the weak modality to suppress non-target classes. This stabilizes the decision space and preserves modality-specific information, allowing weak modalities to preserve unique information without being over-aligned. We proceed to reveal multimodal learning from a robustness perspective and theoretically derive the Multimodal Negative Learning (MNL) framework, which introduces a dynamic guidance mechanism tailored for negative learning. Our method provably tightens the robustness lower bound of multimodal learning by increasing the Unimodal Confidence Margin (UCoM) and reduces the empirical error of weak modalities, particularly under noisy and imbalanced scenarios. Extensive experiments across multiple benchmarks demonstrate the effectiveness and generalizability of our approach against competing methods. The code will be available at https://github.com/BaoquanGong/Multimodal-Negative-Learning.git.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20877v1",
    "published_date": "2025-10-23 11:47:11 UTC",
    "updated_date": "2025-10-23 11:47:11 UTC"
  },
  {
    "arxiv_id": "2510.21867v1",
    "title": "Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs",
    "authors": [
      "Haicheng Liao",
      "Bonan Wang",
      "Junxian Yang",
      "Chengyue Wang",
      "Zhengbin He",
      "Guohui Zhang",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "abstract": "Accurate and reliable motion forecasting is essential for the safe deployment of autonomous vehicles (AVs), particularly in rare but safety-critical scenarios known as corner cases. Existing models often underperform in these situations due to an over-representation of common scenes in training data and limited generalization capabilities. To address this limitation, we present WM-MoE, the first world model-based motion forecasting framework that unifies perception, temporal memory, and decision making to address the challenges of high-risk corner-case scenarios. The model constructs a compact scene representation that explains current observations, anticipates future dynamics, and evaluates the outcomes of potential actions. To enhance long-horizon reasoning, we leverage large language models (LLMs) and introduce a lightweight temporal tokenizer that maps agent trajectories and contextual cues into the LLM's feature space without additional training, enriching temporal context and commonsense priors. Furthermore, a mixture-of-experts (MoE) is introduced to decompose complex corner cases into subproblems and allocate capacity across scenario types, and a router assigns scenes to specialized experts that infer agent intent and perform counterfactual rollouts. In addition, we introduce nuScenes-corner, a new benchmark that comprises four real-world corner-case scenarios for rigorous evaluation. Extensive experiments on four benchmark datasets (nuScenes, NGSIM, HighD, and MoCAD) showcase that WM-MoE consistently outperforms state-of-the-art (SOTA) baselines and remains robust under corner-case and data-missing conditions, indicating the promise of world model-based architectures for robust and generalizable motion forecasting in fully AVs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21867v1",
    "published_date": "2025-10-23 11:41:51 UTC",
    "updated_date": "2025-10-23 11:41:51 UTC"
  },
  {
    "arxiv_id": "2510.20453v1",
    "title": "Symbolic Regression and Differentiable Fits in Beyond the Standard Model Physics",
    "authors": [
      "Shehu AbdusSalam",
      "Steven Abel",
      "Deaglan Bartlett",
      "Miguel Crispim Romão"
    ],
    "abstract": "We demonstrate the efficacy of symbolic regression (SR) to probe models of particle physics Beyond the Standard Model (BSM), by considering the so-called Constrained Minimal Supersymmetric Standard Model (CMSSM). Like many incarnations of BSM physics this model has a number (four) of arbitrary parameters, which determine the experimental signals, and cosmological observables such as the dark matter relic density. We show that analysis of the phenomenology can be greatly accelerated by using symbolic expressions derived for the observables in terms of the input parameters. Here we focus on the Higgs mass, the cold dark matter relic density, and the contribution to the anomalous magnetic moment of the muon. We find that SR can produce remarkably accurate expressions. Using them we make global fits to derive the posterior probability densities of the CMSSM input parameters which are in good agreement with those performed using conventional methods. Moreover, we demonstrate a major advantage of SR which is the ability to make fits using differentiable methods rather than sampling methods. We also compare the method with neural network (NN) regression. SR produces more globally robust results, while NNs require data that is focussed on the promising regions in order to be equally performant.",
    "categories": [
      "hep-ph",
      "astro-ph.CO",
      "cs.AI",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "hep-ph",
    "comment": "18 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20453v1",
    "published_date": "2025-10-23 11:40:15 UTC",
    "updated_date": "2025-10-23 11:40:15 UTC"
  },
  {
    "arxiv_id": "2510.20448v2",
    "title": "MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction",
    "authors": [
      "Xuan Lin",
      "Aocheng Ding",
      "Tengfei Ma",
      "Hua Liang",
      "Zhe Quan"
    ],
    "abstract": "Drug combinations offer therapeutic benefits but also carry the risk of adverse drug-drug interactions (DDIs), especially under complex molecular structures. Accurate DDI event prediction requires capturing fine-grained inter-drug relationships, which are critical for modeling metabolic mechanisms such as enzyme-mediated competition. However, existing approaches typically rely on isolated drug representations and fail to explicitly model atom-level cross-molecular interactions, limiting their effectiveness across diverse molecular complexities and DDI type distributions. To address these limitations, we propose MolBridge, a novel atom-level joint graph refinement framework for robust DDI event prediction. MolBridge constructs a joint graph that integrates atomic structures of drug pairs, enabling direct modeling of inter-drug associations. A central challenge in such joint graph settings is the potential loss of information caused by over-smoothing when modeling long-range atomic dependencies. To overcome this, we introduce a structure consistency module that iteratively refines node features while preserving the global structural context. This joint design allows MolBridge to effectively learn both local and global interaction outperforms state-of-the-art baselines, achieving superior performance across long-tail and inductive scenarios. patterns, yielding robust representations across both frequent and rare DDI types. Extensive experiments on two benchmark datasets show that MolBridge consistently. These results demonstrate the advantages of fine-grained graph refinement in improving the accuracy, robustness, and mechanistic interpretability of DDI event prediction.This work contributes to Web Mining and Content Analysis by developing graph-based methods for mining and analyzing drug-drug interaction networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20448v2",
    "published_date": "2025-10-23 11:33:16 UTC",
    "updated_date": "2025-10-24 02:34:05 UTC"
  },
  {
    "arxiv_id": "2510.20441v1",
    "title": "UniSE: A Unified Framework for Decoder-only Autoregressive LM-based Speech Enhancement",
    "authors": [
      "Haoyin Yan",
      "Chengwei Liu",
      "Shaofei Xue",
      "Xiaotao Liang",
      "Zheng Xue"
    ],
    "abstract": "The development of neural audio codecs (NACs) has largely promoted applications of language models (LMs) to speech processing and understanding. However, there lacks the verification on the effectiveness of autoregressive (AR) LMbased models in unifying different sub-tasks of speech enhancement (SE). In this work, we propose UniSE, a unified decoder-only LM-based framework to handle different SE tasks including speech restoration, target speaker extraction and speech separation. It takes input speech features as conditions and generates discrete tokens of the target speech using AR modeling, which facilitates a compatibility between distinct learning patterns of multiple tasks. Experiments on several benchmarks indicate the proposed UniSE can achieve competitive performance compared to discriminative and generative baselines, showing the capacity of LMs in unifying SE tasks. The demo page is available here: https://github.com/hyyan2k/UniSE.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, submitted to ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.20441v1",
    "published_date": "2025-10-23 11:22:24 UTC",
    "updated_date": "2025-10-23 11:22:24 UTC"
  },
  {
    "arxiv_id": "2510.20438v1",
    "title": "Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment",
    "authors": [
      "Saif Ur Rehman Khan",
      "Muhammad Nabeel Asim",
      "Sebastian Vollmer",
      "Andreas Dengel"
    ],
    "abstract": "This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for lung cancer (LC) classification, leveraging dynamic fuzzy logic-driven knowledge distillation (KD) to address uncertainty and complexity in disease diagnosis. Unlike traditional models that rely on static KD with fixed weights, our method dynamically adjusts the distillation weight using fuzzy logic, enabling the student model to focus on high-confidence regions while reducing attention to ambiguous areas. This dynamic adjustment improves the model ability to handle varying uncertainty levels across different regions of LC images. We employ the Vision Transformer (ViT-B32) as the instructor model, which effectively transfers knowledge to the student model, MobileNet, enhancing the student generalization capabilities. The training process is further optimized using a dynamic wait adjustment mechanism that adapts the training procedure for improved convergence and performance. To enhance image quality, we introduce pixel-level image fusion improvement techniques such as Gamma correction and Histogram Equalization. The processed images (Pix1 and Pix2) are fused using a wavelet-based fusion method to improve image resolution and feature preservation. This fusion method uses the wavedec2 function to standardize images to a 224x224 resolution, decompose them into multi-scale frequency components, and recursively average coefficients at each level for better feature representation. To address computational efficiency, Genetic Algorithm (GA) is used to select the most suitable pre-trained student model from a pool of 12 candidates, balancing model performance with computational cost. The model is evaluated on two datasets, including LC25000 histopathological images (99.16% accuracy) and IQOTH/NCCD CT-scan images (99.54% accuracy), demonstrating robustness across different imaging domains.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20438v1",
    "published_date": "2025-10-23 11:19:52 UTC",
    "updated_date": "2025-10-23 11:19:52 UTC"
  },
  {
    "arxiv_id": "2510.21866v1",
    "title": "Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks",
    "authors": [
      "Javier Marín"
    ],
    "abstract": "We document empirical capability ceilings in decoder-only autoregressive language models across knowledge-intensive tasks. Systematic evaluation of OPT and Pythia model families (70M-30B parameters, spanning 240 times scaling) reveals that knowledge retrieval tasks show negligible accuracy improvement despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains flat at 19-20% (below 25% random chance) across all scales while cross-entropy loss decreases by 31%. In contrast, procedural tasks like arithmetic show conventional scaling where both metrics improve together. Attention intervention experiments reveal high sensitivity to perturbation: swapping attention patterns between models causes catastrophic performance collapse (complete accuracy loss) rather than graceful degradation. These measurements have immediate engineering implications: for knowledge-intensive applications using OPT and Pythia architectures, parameter scaling beyond 1-2B offers minimal accuracy gains despite continued loss improvement. Our findings quantify capability-specific scaling failures in these model families to inform resource allocation decisions. Whether these patterns reflect fundamental constraints of decoder-only architectures or implementation-specific limitations remains an open question requiring investigation across diverse architectural approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The experiments in this paper were performed in January 2024. Current model architectures are considerably more complex than those presented here",
    "pdf_url": "https://arxiv.org/pdf/2510.21866v1",
    "published_date": "2025-10-23 11:09:31 UTC",
    "updated_date": "2025-10-23 11:09:31 UTC"
  },
  {
    "arxiv_id": "2510.20875v1",
    "title": "CC-GRMAS: A Multi-Agent Graph Neural System for Spatiotemporal Landslide Risk Assessment in High Mountain Asia",
    "authors": [
      "Mihir Panchal",
      "Ying-Jung Chen",
      "Surya Parkash"
    ],
    "abstract": "Landslides are a growing climate induced hazard with severe environmental and human consequences, particularly in high mountain Asia. Despite increasing access to satellite and temporal datasets, timely detection and disaster response remain underdeveloped and fragmented. This work introduces CC-GRMAS, a framework leveraging a series of satellite observations and environmental signals to enhance the accuracy of landslide forecasting. The system is structured around three interlinked agents Prediction, Planning, and Execution, which collaboratively enable real time situational awareness, response planning, and intervention. By incorporating local environmental factors and operationalizing multi agent coordination, this approach offers a scalable and proactive solution for climate resilient disaster preparedness across vulnerable mountainous terrains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20875v1",
    "published_date": "2025-10-23 10:30:48 UTC",
    "updated_date": "2025-10-23 10:30:48 UTC"
  },
  {
    "arxiv_id": "2510.20408v1",
    "title": "Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control",
    "authors": [
      "Tom Maus",
      "Asma Atamna",
      "Tobias Glasmachers"
    ],
    "abstract": "Autonomous control of multi-stage industrial processes requires both local specialization and global coordination. Reinforcement learning (RL) offers a promising approach, but its industrial adoption remains limited due to challenges such as reward design, modularity, and action space management. Many academic benchmarks differ markedly from industrial control problems, limiting their transferability to real-world applications. This study introduces an enhanced industry-inspired benchmark environment that combines tasks from two existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling scenario with sorting and pressing operations. We evaluate two control strategies: a modular architecture with specialized agents and a monolithic agent governing the full system, while also analyzing the impact of action masking. Our experiments show that without action masking, agents struggle to learn effective policies, with the modular architecture performing better. When action masking is applied, both architectures improve substantially, and the performance gap narrows considerably. These results highlight the decisive role of action space constraints and suggest that the advantages of specialization diminish as action complexity is reduced. The proposed benchmark thus provides a valuable testbed for exploring practical and robust multi-agent RL solutions in industrial automation, while contributing to the ongoing debate on centralization versus specialization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint (submitted version) to be presented at the 13th International Conference on Industrial Engineering and Applications (ICIEA-EU), Milan, 2026. The final Version of Record will appear in the official conference proceedings",
    "pdf_url": "https://arxiv.org/pdf/2510.20408v1",
    "published_date": "2025-10-23 10:21:54 UTC",
    "updated_date": "2025-10-23 10:21:54 UTC"
  },
  {
    "arxiv_id": "2511.04685v1",
    "title": "A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024",
    "authors": [
      "Daniela Guericke",
      "Rolf van der Hulst",
      "Asal Karimpour",
      "Ieke Schrader",
      "Matthias Walter"
    ],
    "abstract": "We report about the algorithm, implementation and results submitted to the Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored third in the competition. Our approach combines mixed-integer programming, constraint programming and simulated annealing in a 3-phase solution approach based on decomposition into subproblems. Next to describing our approach and describing our design decisions, we share our insights and, for the first time, lower bounds on the optimal solution values for the benchmark instances. We finally highlight open problems for which we think that addressing them could improve our approach even further.",
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 2 figures, 10 tables",
    "pdf_url": "https://arxiv.org/pdf/2511.04685v1",
    "published_date": "2025-10-23 10:14:04 UTC",
    "updated_date": "2025-10-23 10:14:04 UTC"
  },
  {
    "arxiv_id": "2510.20402v1",
    "title": "A computational model and tool for generating more novel opportunities in professional innovation processes",
    "authors": [
      "Neil Maiden",
      "Konstantinos Zachos",
      "James Lockerbie",
      "Kostas Petrianakis",
      "Amanda Brown"
    ],
    "abstract": "This paper presents a new computational model of creative outcomes, informed by creativity theories and techniques, which was implemented to generate more novel opportunities for innovation projects. The model implemented five functions that were developed to contribute to the generation of innovation opportunities with higher novelty without loss of usefulness. The model was evaluated using opportunities generated for an innovation project in the hospitality sector. The evaluation revealed that the computational model generated outcomes that were more novel and/or useful than outcomes from Notebook LM and ChatGPT4o. However, not all model functions contributed to the generation of more novel opportunities, leading to new directions for further model development",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20402v1",
    "published_date": "2025-10-23 10:09:57 UTC",
    "updated_date": "2025-10-23 10:09:57 UTC"
  },
  {
    "arxiv_id": "2510.20388v1",
    "title": "FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services",
    "authors": [
      "Víctor Rampérez",
      "Javier Soriano",
      "David Lizcano",
      "Juan A. Lara"
    ],
    "abstract": "Cloud computing has established itself as the support for the vast majority of emerging technologies, mainly due to the characteristic of elasticity it offers. Auto-scalers are the systems that enable this elasticity by acquiring and releasing resources on demand to ensure an agreed service level. In this article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for distributed services that combines the advantages of proactive and reactive approaches according to the situation to decide the optimal scaling actions in every moment. The main novelties introduced by FLAS are (i) a predictive model of the high-level metrics trend which allows to anticipate changes in the relevant SLA parameters (e.g. performance metrics such as response time or throughput) and (ii) a reactive contingency system based on the estimation of high-level metrics from resource use metrics, reducing the necessary instrumentation (less invasive) and allowing it to be adapted agnostically to different applications. We provide a FLAS implementation for the use case of a content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone of an event-driven architecture. To the best of our knowledge, this is the first auto-scaling system for content-based publish-subscribe distributed systems (although it is generic enough to fit any distributed service). Through an evaluation based on several test cases recreating not only the expected contexts of use, but also the worst possible scenarios (following the Boundary-Value Analysis or BVA test methodology), we have validated our approach and demonstrated the effectiveness of our solution by ensuring compliance with performance requirements over 99% of the time.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20388v1",
    "published_date": "2025-10-23 09:38:07 UTC",
    "updated_date": "2025-10-23 09:38:07 UTC"
  },
  {
    "arxiv_id": "2510.20387v1",
    "title": "Relative-Based Scaling Law for Neural Language Models",
    "authors": [
      "Baoqing Yue",
      "Jinyuan Zhou",
      "Zixi Wei",
      "Jingtao Zhan",
      "Qingyao Ai",
      "Yiqun Liu"
    ],
    "abstract": "Scaling laws aim to accurately predict model performance across different scales. Existing scaling-law studies almost exclusively rely on cross-entropy as the evaluation metric. However, cross-entropy provides only a partial view of performance: it measures the absolute probability assigned to the correct token, but ignores the relative ordering between correct and incorrect tokens. Yet, relative ordering is crucial for language models, such as in greedy-sampling scenario. To address this limitation, we investigate scaling from the perspective of relative ordering. We first propose the Relative-Based Probability (RBP) metric, which quantifies the probability that the correct token is ranked among the top predictions. Building on this metric, we establish the Relative-Based Scaling Law, which characterizes how RBP improves with increasing model size. Through extensive experiments on four datasets and four model families spanning five orders of magnitude, we demonstrate the robustness and accuracy of this law. Finally, we illustrate the broad application of this law with two examples, namely providing a deeper explanation of emergence phenomena and facilitating finding fundamental theories of scaling laws. In summary, the Relative-Based Scaling Law complements the cross-entropy perspective and contributes to a more complete understanding of scaling large language models. Thus, it offers valuable insights for both practical development and theoretical exploration.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20387v1",
    "published_date": "2025-10-23 09:37:00 UTC",
    "updated_date": "2025-10-23 09:37:00 UTC"
  },
  {
    "arxiv_id": "2510.20381v1",
    "title": "VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation",
    "authors": [
      "Son T. Luu",
      "Trung Vo",
      "Hiep Nguyen",
      "Khanh Quoc Tran",
      "Kiet Van Nguyen",
      "Vu Tran",
      "Ngan Luu-Thuy Nguyen",
      "Le-Minh Nguyen"
    ],
    "abstract": "This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025 MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal question answering. The goal is to advance research on Vietnamese multimodal legal text processing and to provide a benchmark dataset for building and evaluating intelligent systems in multimodal legal domains, with a focus on traffic sign regulation in Vietnam. The best-reported results on VLSP 2025 MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an accuracy of 86.30% for multimodal question answering.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "VLSP 2025 MLQA-TSR Share Task",
    "pdf_url": "https://arxiv.org/pdf/2510.20381v1",
    "published_date": "2025-10-23 09:24:43 UTC",
    "updated_date": "2025-10-23 09:24:43 UTC"
  },
  {
    "arxiv_id": "2510.20377v1",
    "title": "IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation",
    "authors": [
      "Tianyi Zhang",
      "Florian Mai",
      "Lucie Flek"
    ],
    "abstract": "Continual pretraining promises to adapt large language models (LLMs) to new domains using only unlabeled test-time data, but naively applying standard self-supervised objectives to instruction-tuned models is known to degrade their instruction-following capability and semantic representations. Existing fixes assume access to the original base model or rely on knowledge from an external domain-specific database - both of which pose a realistic barrier in settings where the base model weights are withheld for safety reasons or reliable external corpora are unavailable. In this work, we propose Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general framework that formulates novel self-supervised objectives in the instruction-response dialogue format. Rather than depend- ing on external resources, IKnow leverages domain knowledge embedded within the text itself and learns to encode it at a deeper semantic level.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20377v1",
    "published_date": "2025-10-23 09:21:13 UTC",
    "updated_date": "2025-10-23 09:21:13 UTC"
  },
  {
    "arxiv_id": "2510.20375v1",
    "title": "The Impact of Negated Text on Hallucination with Large Language Models",
    "authors": [
      "Jaehyung Seo",
      "Hyeonseok Moon",
      "Heuiseok Lim"
    ],
    "abstract": "Recent studies on hallucination in large language models (LLMs) have been actively progressing in natural language processing. However, the impact of negated text on hallucination with LLMs remains largely unexplored. In this paper, we set three important yet unanswered research questions and aim to address them. To derive the answers, we investigate whether LLMs can recognize contextual shifts caused by negation and still reliably distinguish hallucinations comparable to affirmative cases. We also design the NegHalu dataset by reconstructing existing hallucination detection datasets with negated expressions. Our experiments demonstrate that LLMs struggle to detect hallucinations in negated text effectively, often producing logically inconsistent or unfaithful judgments. Moreover, we trace the internal state of LLMs as they process negated inputs at the token level and reveal the challenges of mitigating their unintended effects.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20375v1",
    "published_date": "2025-10-23 09:20:15 UTC",
    "updated_date": "2025-10-23 09:20:15 UTC"
  },
  {
    "arxiv_id": "2510.21862v1",
    "title": "A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering Drawings Using Vision Language Model",
    "authors": [
      "Muhammad Tayyab Khan",
      "Zane Yong",
      "Lequn Chen",
      "Wenhe Feng",
      "Nicholas Yew Jin Tan",
      "Seung Ki Moon"
    ],
    "abstract": "Engineering drawings are fundamental to manufacturing communication, serving as the primary medium for conveying design intent, tolerances, and production details. However, interpreting complex multi-view drawings with dense annotations remains challenging using manual methods, generic optical character recognition (OCR) systems, or traditional deep learning approaches, due to varied layouts, orientations, and mixed symbolic-textual content. To address these challenges, this paper proposes a three-stage hybrid framework for the automated interpretation of 2D multi-view engineering drawings using modern detection and vision language models (VLMs). In the first stage, YOLOv11-det performs layout segmentation to localize key regions such as views, title blocks, and notes. The second stage uses YOLOv11-obb for orientation-aware, fine-grained detection of annotations, including measures, GD&T symbols, and surface roughness indicators. The third stage employs two Donut-based, OCR-free VLMs for semantic content parsing: the Alphabetical VLM extracts textual and categorical information from title blocks and notes, while the Numerical VLM interprets quantitative data such as measures, GD&T frames, and surface roughness. Two specialized datasets were developed to ensure robustness and generalization: 1,000 drawings for layout detection and 1,406 for annotation-level training. The Alphabetical VLM achieved an overall F1 score of 0.672, while the Numerical VLM reached 0.963, demonstrating strong performance in textual and quantitative interpretation, respectively. The unified JSON output enables seamless integration with CAD and manufacturing databases, providing a scalable solution for intelligent engineering drawing analysis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "This draft has been submitted to the 13th International Conference on Industrial Engineering and Applications (ICIEA 2026)",
    "pdf_url": "https://arxiv.org/pdf/2510.21862v1",
    "published_date": "2025-10-23 09:07:31 UTC",
    "updated_date": "2025-10-23 09:07:31 UTC"
  },
  {
    "arxiv_id": "2510.20351v1",
    "title": "Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models",
    "authors": [
      "Matteo Silvestri",
      "Flavio Giorgi",
      "Fabrizio Silvestri",
      "Gabriele Tolomei"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly evaluated on their ability to reason over structured data, yet such assessments often overlook a crucial confound: dataset contamination. In this work, we investigate whether LLMs exhibit prior knowledge of widely used tabular benchmarks such as Adult Income, Titanic, and others. Through a series of controlled probing experiments, we reveal that contamination effects emerge exclusively for datasets containing strong semantic cues-for instance, meaningful column names or interpretable value categories. In contrast, when such cues are removed or randomized, performance sharply declines to near-random levels. These findings suggest that LLMs' apparent competence on tabular reasoning tasks may, in part, reflect memorization of publicly available datasets rather than genuine generalization. We discuss implications for evaluation protocols and propose strategies to disentangle semantic leakage from authentic reasoning ability in future LLM assessments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20351v1",
    "published_date": "2025-10-23 08:51:14 UTC",
    "updated_date": "2025-10-23 08:51:14 UTC"
  },
  {
    "arxiv_id": "2510.20350v2",
    "title": "What Do AI-Generated Images Want?",
    "authors": [
      "Amanda Wasielewski"
    ],
    "abstract": "W.J.T. Mitchell's influential essay 'What do pictures want?' shifts the theoretical focus away from the interpretative act of understanding pictures and from the motivations of the humans who create them to the possibility that the picture itself is an entity with agency and wants. In this article, I reframe Mitchell's question in light of contemporary AI image generation tools to ask: what do AI-generated images want? Drawing from art historical discourse on the nature of abstraction, I argue that AI-generated images want specificity and concreteness because they are fundamentally abstract. Multimodal text-to-image models, which are the primary subject of this article, are based on the premise that text and image are interchangeable or exchangeable tokens and that there is a commensurability between them, at least as represented mathematically in data. The user pipeline that sees textual input become visual output, however, obscures this representational regress and makes it seem like one form transforms into the other -- as if by magic.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20350v2",
    "published_date": "2025-10-23 08:48:47 UTC",
    "updated_date": "2025-10-24 09:41:05 UTC"
  },
  {
    "arxiv_id": "2510.20345v1",
    "title": "LLM-empowered knowledge graph construction: A survey",
    "authors": [
      "Haonan Bian"
    ],
    "abstract": "Knowledge Graphs (KGs) have long served as a fundamental infrastructure for structured knowledge representation and reasoning. With the advent of Large Language Models (LLMs), the construction of KGs has entered a new paradigm-shifting from rule-based and statistical pipelines to language-driven and generative frameworks. This survey provides a comprehensive overview of recent progress in LLM-empowered knowledge graph construction, systematically analyzing how LLMs reshape the classical three-layered pipeline of ontology engineering, knowledge extraction, and knowledge fusion.\n  We first revisit traditional KG methodologies to establish conceptual foundations, and then review emerging LLM-driven approaches from two complementary perspectives: schema-based paradigms, which emphasize structure, normalization, and consistency; and schema-free paradigms, which highlight flexibility, adaptability, and open discovery. Across each stage, we synthesize representative frameworks, analyze their technical mechanisms, and identify their limitations.\n  Finally, the survey outlines key trends and future research directions, including KG-based reasoning for LLMs, dynamic knowledge memory for agentic systems, and multimodal KG construction. Through this systematic review, we aim to clarify the evolving interplay between LLMs and knowledge graphs, bridging symbolic knowledge engineering and neural semantic understanding toward the development of adaptive, explainable, and intelligent knowledge systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20345v1",
    "published_date": "2025-10-23 08:43:28 UTC",
    "updated_date": "2025-10-23 08:43:28 UTC"
  },
  {
    "arxiv_id": "2510.20342v1",
    "title": "Teaching Language Models to Reason with Tools",
    "authors": [
      "Chengpeng Li",
      "Zhengyang Tang",
      "Ziniu Li",
      "Mingfeng Xue",
      "Keqin Bao",
      "Tian Ding",
      "Ruoyu Sun",
      "Benyou Wang",
      "Xiang Wang",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "abstract": "Large reasoning models (LRMs) like OpenAI-o1 have shown impressive capabilities in natural language reasoning. However, these models frequently demonstrate inefficiencies or inaccuracies when tackling complex mathematical operations. While integrating computational tools such as Code Interpreters (CIs) offers a promising solution, it introduces a critical challenge: a conflict between the model's internal, probabilistic reasoning and the external, deterministic knowledge provided by the CI, which often leads models to unproductive deliberation. To overcome this, we introduce CoRT (Code-Optimized Reasoning Training), a post-training framework designed to teach LRMs to effectively utilize CIs. We propose \\emph{Hint-Engineering}, a new data synthesis strategy that strategically injects diverse hints at optimal points within reasoning paths. This approach generates high-quality, code-integrated reasoning data specifically tailored to optimize LRM-CI interaction. Using this method, we have synthesized 30 high-quality samples to post-train models ranging from 1.5B to 32B parameters through supervised fine-tuning. CoRT further refines the multi-round interleaving of external CI usage and internal thinking by employing rejection sampling and reinforcement learning. Our experimental evaluations demonstrate CoRT's effectiveness, yielding absolute improvements of 4\\% and 8\\% on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging mathematical reasoning datasets. Moreover, CoRT significantly enhances efficiency, reducing token usage by approximately 30\\% for the 32B model and 50\\% for the 1.5B model compared to pure natural language reasoning baselines. The models and code are available at: https://github.com/ChengpengLi1003/CoRT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NIPS2025 Accepted",
    "pdf_url": "https://arxiv.org/pdf/2510.20342v1",
    "published_date": "2025-10-23 08:41:44 UTC",
    "updated_date": "2025-10-23 08:41:44 UTC"
  },
  {
    "arxiv_id": "2510.20339v1",
    "title": "Multi-Task Deep Learning for Surface Metrology",
    "authors": [
      "D. Kucharski",
      "A. Gaska",
      "T. Kowaluk",
      "K. Stepien",
      "M. Repalska",
      "B. Gapinski",
      "M. Wieczorowski",
      "M. Nawotka",
      "P. Sobecki",
      "P. Sosinowski",
      "J. Tomasik",
      "A. Wojtowicz"
    ],
    "abstract": "A reproducible deep learning framework is presented for surface metrology to predict surface texture parameters together with their reported standard uncertainties. Using a multi-instrument dataset spanning tactile and optical systems, measurement system type classification is addressed alongside coordinated regression of Ra, Rz, RONt and their uncertainty targets (Ra_uncert, Rz_uncert, RONt_uncert). Uncertainty is modelled via quantile and heteroscedastic heads with post-hoc conformal calibration to yield calibrated intervals. On a held-out set, high fidelity was achieved by single-target regressors (R2: Ra 0.9824, Rz 0.9847, RONt 0.9918), with two uncertainty targets also well modelled (Ra_uncert 0.9899, Rz_uncert 0.9955); RONt_uncert remained difficult (R2 0.4934). The classifier reached 92.85% accuracy and probability calibration was essentially unchanged after temperature scaling (ECE 0.00504 -> 0.00503 on the test split). Negative transfer was observed for naive multi-output trunks, with single-target models performing better. These results provide calibrated predictions suitable to inform instrument selection and acceptance decisions in metrological workflows.",
    "categories": [
      "physics.app-ph",
      "cs.AI",
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "physics.app-ph",
    "comment": "34 pages, 10 figures, 6 tables; 60-page supplementary appendix. Code and full reproducibility bundle available via Zenodo",
    "pdf_url": "https://arxiv.org/pdf/2510.20339v1",
    "published_date": "2025-10-23 08:38:18 UTC",
    "updated_date": "2025-10-23 08:38:18 UTC"
  },
  {
    "arxiv_id": "2510.20337v1",
    "title": "Collateral Damage Assessment Model for AI System Target Engagement in Military Operations",
    "authors": [
      "Clara Maathuis",
      "Kasper Cools"
    ],
    "abstract": "In an era where AI (Artificial Intelligence) systems play an increasing role in the battlefield, ensuring responsible targeting demands rigorous assessment of potential collateral effects. In this context, a novel collateral damage assessment model for target engagement of AI systems in military operations is introduced. The model integrates temporal, spatial, and force dimensions within a unified Knowledge Representation and Reasoning (KRR) architecture following a design science methodological approach. Its layered structure captures the categories and architectural components of the AI systems to be engaged together with corresponding engaging vectors and contextual aspects. At the same time, spreading, severity, likelihood, and evaluation metrics are considered in order to provide a clear representation enhanced by transparent reasoning mechanisms. Further, the model is demonstrated and evaluated through instantiation which serves as a basis for further dedicated efforts that aim at building responsible and trustworthy intelligent systems for assessing the effects produced by engaging AI systems in military operations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at MILCOM 2025 WS07",
    "pdf_url": "https://arxiv.org/pdf/2510.20337v1",
    "published_date": "2025-10-23 08:36:04 UTC",
    "updated_date": "2025-10-23 08:36:04 UTC"
  },
  {
    "arxiv_id": "2510.20333v2",
    "title": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?",
    "authors": [
      "Chiyu Chen",
      "Xinhao Song",
      "Yunkai Chai",
      "Yang Yao",
      "Haodong Zhao",
      "Lijun Li",
      "Jie Li",
      "Yan Teng",
      "Gongshen Liu",
      "Yingchun Wang"
    ],
    "abstract": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20333v2",
    "published_date": "2025-10-23 08:33:24 UTC",
    "updated_date": "2025-11-21 07:38:12 UTC"
  },
  {
    "arxiv_id": "2510.20332v1",
    "title": "Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems",
    "authors": [
      "Anna Arias-Duart",
      "Maria Eugenia Cardello",
      "Atia Cortés"
    ],
    "abstract": "Artificial intelligence (AI) holds great promise for transforming healthcare. However, despite significant advances, the integration of AI solutions into real-world clinical practice remains limited. A major barrier is the quality and fairness of training data, which is often compromised by biased data collection practices. This paper draws on insights from the AI4HealthyAging project, part of Spain's national R&D initiative, where our task was to detect biases during clinical data collection. We identify several types of bias across multiple use cases, including historical, representation, and measurement biases. These biases manifest in variables such as sex, gender, age, habitat, socioeconomic status, equipment, and labeling. We conclude with practical recommendations for improving the fairness and robustness of clinical problem design and data collection. We hope that our findings and experience contribute to guiding future projects in the development of fairer AI systems in healthcare.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 3 tables, accepted in AEQUITAS 2025 (not in proceedings)",
    "pdf_url": "https://arxiv.org/pdf/2510.20332v1",
    "published_date": "2025-10-23 08:32:34 UTC",
    "updated_date": "2025-10-23 08:32:34 UTC"
  },
  {
    "arxiv_id": "2510.20328v1",
    "title": "MemER: Scaling Up Memory for Robot Control via Experience Retrieval",
    "authors": [
      "Ajay Sridhar",
      "Jennifer Pan",
      "Satvik Sharma",
      "Chelsea Finn"
    ],
    "abstract": "Humans routinely rely on memory to perform tasks, yet most robot policies lack this capability; our goal is to endow robot policies with the same ability. Naively conditioning on long observation histories is computationally expensive and brittle under covariate shift, while indiscriminate subsampling of history leads to irrelevant or redundant information. We propose a hierarchical policy framework, where the high-level policy is trained to select and track previous relevant keyframes from its experience. The high-level policy uses selected keyframes and the most recent frames when generating text instructions for a low-level policy to execute. This design is compatible with existing vision-language-action (VLA) models and enables the system to efficiently reason over long-horizon dependencies. In our experiments, we finetune Qwen2.5-VL-7B-Instruct and $π_{0.5}$ as the high-level and low-level policies respectively, using demonstrations supplemented with minimal language annotations. Our approach, MemER, outperforms prior methods on three real-world long-horizon robotic manipulation tasks that require minutes of memory. Videos and code can be found at https://jen-pan.github.io/memer/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://jen-pan.github.io/memer/",
    "pdf_url": "https://arxiv.org/pdf/2510.20328v1",
    "published_date": "2025-10-23 08:26:17 UTC",
    "updated_date": "2025-10-23 08:26:17 UTC"
  },
  {
    "arxiv_id": "2510.20327v1",
    "title": "LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems",
    "authors": [
      "Fengyuan Yu",
      "Yuyuan Li",
      "Xiaohua Feng",
      "Junjie Fang",
      "Tao Wang",
      "Chaochao Chen"
    ],
    "abstract": "With the growing demand for safeguarding sensitive user information in recommender systems, recommendation attribute unlearning is receiving increasing attention. Existing studies predominantly focus on single-attribute unlearning. However, privacy protection requirements in the real world often involve multiple sensitive attributes and are dynamic. Existing single-attribute unlearning methods cannot meet these real-world requirements due to i) CH1: the inability to handle multiple unlearning requests simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic unlearning needs. To address these challenges, we propose LEGO, a lightweight and efficient multiple-attribute unlearning framework. Specifically, we divide the multiple-attribute unlearning process into two steps: i) Embedding Calibration removes information related to a specific attribute from user embedding, and ii) Flexible Combination combines these embeddings into a single embedding, protecting all sensitive attributes. We frame the unlearning process as a mutual information minimization problem, providing LEGO a theoretical guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step framework, where Embedding Calibration can be performed in parallel and Flexible Combination is flexible and efficient, we address CH2. Extensive experiments on three real-world datasets across three representative recommendation models demonstrate the effectiveness and efficiency of our proposed framework. Our code and appendix are available at https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ACM Multimedia 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20327v1",
    "published_date": "2025-10-23 08:20:47 UTC",
    "updated_date": "2025-10-23 08:20:47 UTC"
  },
  {
    "arxiv_id": "2510.20314v1",
    "title": "Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses",
    "authors": [
      "Wu Yichao",
      "Wang Yirui",
      "Ding Panpan",
      "Wang Hailong",
      "Zhu Bingqian",
      "Liu Chun"
    ],
    "abstract": "With the wide application of deep reinforcement learning (DRL) techniques in complex fields such as autonomous driving, intelligent manufacturing, and smart healthcare, how to improve its security and robustness in dynamic and changeable environments has become a core issue in current research. Especially in the face of adversarial attacks, DRL may suffer serious performance degradation or even make potentially dangerous decisions, so it is crucial to ensure their stability in security-sensitive scenarios. In this paper, we first introduce the basic framework of DRL and analyze the main security challenges faced in complex and changing environments. In addition, this paper proposes an adversarial attack classification framework based on perturbation type and attack target and reviews the mainstream adversarial attack methods against DRL in detail, including various attack methods such as perturbation state space, action space, reward function and model space. To effectively counter the attacks, this paper systematically summarizes various current robustness training strategies, including adversarial training, competitive training, robust learning, adversarial detection, defense distillation and other related defense techniques, we also discuss the advantages and shortcomings of these methods in improving the robustness of DRL. Finally, this paper looks into the future research direction of DRL in adversarial environments, emphasizing the research needs in terms of improving generalization, reducing computational complexity, and enhancing scalability and explainability, aiming to provide valuable references and directions for researchers.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20314v1",
    "published_date": "2025-10-23 08:04:57 UTC",
    "updated_date": "2025-10-23 08:04:57 UTC"
  },
  {
    "arxiv_id": "2510.20310v2",
    "title": "Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation",
    "authors": [
      "Mingliang Zhai",
      "Hansheng Liang",
      "Xiaomeng Fan",
      "Zhi Gao",
      "Chuanhao Li",
      "Che Sun",
      "Xu Bin",
      "Yuwei Wu",
      "Yunde Jia"
    ],
    "abstract": "Embodied Question Answering (EQA) requires agents to explore 3D environments to obtain observations and answer questions related to the scene. Existing methods leverage VLMs to directly explore the environment and answer questions without explicit thinking or planning, which limits their reasoning ability and results in excessive or inefficient exploration as well as ineffective responses. In this paper, we introduce ToolEQA, an agent that integrates external tools with multi-step reasoning, where external tools can provide more useful information for completing the task, helping the model derive better exploration directions in the next step of reasoning and thus obtaining additional effective information. This enables ToolEQA to generate more accurate responses with a shorter exploration distance. To enhance the model's ability for tool-usage and multi-step reasoning, we further design a novel EQA data generation pipeline that automatically constructs large-scale EQA tasks with reasoning trajectories and corresponding answers. Based on the pipeline, we collect the EQA-RT dataset that contains about 18K tasks, divided into a training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping with the training set) and EQA-RT-Unseen (novel scenes). Experiments on EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by 9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot ToolEQA by 10% in success rate. In addition, ToolEQA also achieves state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench datasets, demonstrating its generality. Our homepage see https://tooleqa.github.io.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 7 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.20310v2",
    "published_date": "2025-10-23 08:02:08 UTC",
    "updated_date": "2025-10-27 17:58:06 UTC"
  },
  {
    "arxiv_id": "2510.21861v2",
    "title": "The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems",
    "authors": [
      "Bentley DeVilling"
    ],
    "abstract": "Large language models are often described as capable of reflective reasoning, yet recursive self-evaluation without external feedback frequently yields reformulation rather than progress. We test this prediction in a cross-provider study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini, Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families (arithmetic, code, explanation, reflection), each iterated ten times under two conditions: ungrounded self-critique and a minimal grounding intervention (a single verification step at iteration three). Mean informational change (delta I, measured via normalized edit distance) declined by 55% from early (0.193) to late (0.087) iterations in ungrounded runs, with consistent patterns across all three providers. Grounded runs showed a +28% rebound in informational change immediately after the intervention and sustained non-zero variance thereafter. Complementary measures-n-gram novelty, embedding drift, and character-level entropy-converged on the same pattern: reflection without contact tends toward informational closure. We interpret this as evidence for a structural limit on self-correction in generative reasoning: without an exchange of information with an independent verifier or environment, recursive inference approaches an attractor state of epistemic stasis. Minimal grounding functions as dissipative coupling, reintroducing informational flux. The cross-architecture consistency suggests the mirror loop arises from shared autoregressive training objectives rather than provider-specific alignment schemes. The results delineate when reflection is performative rather than epistemic and motivate design principles for grounded, cooperative reasoning. Materials and code are publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 2 figures. Category: cs.LG. Code and data: https://github.com/Course-Correct-Labs/mirror-loop",
    "pdf_url": "https://arxiv.org/pdf/2510.21861v2",
    "published_date": "2025-10-23 07:53:26 UTC",
    "updated_date": "2025-11-05 09:41:15 UTC"
  },
  {
    "arxiv_id": "2510.20299v2",
    "title": "DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Brain Tumor Classification with Grad-CAM Interpretability",
    "authors": [
      "Saraf Anzum Shreya",
      "MD. Abu Ismail Siddique",
      "Sharaf Tasnim"
    ],
    "abstract": "Brain tumors are a challenging problem in neuro-oncology, where early and precise diagnosis is important for successful treatment. Deep learning-based brain tumor classification methods often rely on heavy data augmentation which can limit generalization and trust in clinical applications. In this paper, we propose a double-backbone network integrating VGG16 and Xception with a Frequency-Gated Attention (FGA) Block to capture complementary local and global features. Unlike previous studies, our model achieves state-of-the-art performance without augmentation which demonstrates robustness to variably sized and distributed datasets. For further transparency, Grad-CAM is integrated to visualize the tumor regions based on which the model is giving prediction, bridging the gap between model prediction and clinical interpretability. The proposed framework achieves 99.24\\% accuracy on the 7K-DS dataset for the 4-class setting, along with 98.68\\% and 99.85\\% in the 3-class and 2-class settings, respectively. On the independent 3K-DS dataset, the model generalizes with 95.77\\% accuracy, outperforming baseline and state-of-the-art methods. To further support clinical usability, we developed a graphical user interface (GUI) that provides real-time classification and Grad-CAM-based tumor localization. These findings suggest that augmentation-free, interpretable, and deployable deep learning models such as DB-FGA-Net hold strong potential for reliable clinical translation in brain tumor diagnosis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 14 figures, 12 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.20299v2",
    "published_date": "2025-10-23 07:39:00 UTC",
    "updated_date": "2025-10-25 01:40:13 UTC"
  },
  {
    "arxiv_id": "2510.20296v1",
    "title": "RAG-Stack: Co-Optimizing RAG Quality and Performance From the Vector Database Perspective",
    "authors": [
      "Wenqi Jiang"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has emerged as one of the most prominent applications of vector databases. By integrating documents retrieved from a database into the prompt of a large language model (LLM), RAG enables more reliable and informative content generation. While there has been extensive research on vector databases, many open research problems remain once they are considered in the wider context of end-to-end RAG pipelines. One practical yet challenging problem is how to jointly optimize both system performance and generation quality in RAG, which is significantly more complex than it appears due to the numerous knobs on both the algorithmic side (spanning models and databases) and the systems side (from software to hardware). In this paper, we present RAG-Stack, a three-pillar blueprint for quality-performance co-optimization in RAG systems. RAG-Stack comprises: (1) RAG-IR, an intermediate representation that serves as an abstraction layer to decouple quality and performance aspects; (2) RAG-CM, a cost model for estimating system performance given an RAG-IR; and (3) RAG-PE, a plan exploration algorithm that searches for high-quality, high-performance RAG configurations. We believe this three-pillar blueprint will become the de facto paradigm for RAG quality-performance co-optimization in the years to come.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20296v1",
    "published_date": "2025-10-23 07:35:19 UTC",
    "updated_date": "2025-10-23 07:35:19 UTC"
  },
  {
    "arxiv_id": "2510.21860v1",
    "title": "Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence",
    "authors": [
      "Callum Sharrock",
      "Lukas Petersson",
      "Hanna Petersson",
      "Axel Backlund",
      "Axel Wennström",
      "Kristoffer Nordström",
      "Elias Aronsson"
    ],
    "abstract": "We present Butter-Bench, a benchmark evaluating large language model (LLM) controlled robots for practical intelligence, defined as the ability to navigate the messiness of the physical world. Current state-of-the-art robotic systems use a hierarchical architecture with LLMs in charge of high-level reasoning, and a Vision Language Action (VLA) model for low-level control. Butter-Bench evaluates the LLM part in isolation from the VLA. Although LLMs have repeatedly surpassed humans in evaluations requiring analytical intelligence, we find humans still outperform LLMs on Butter-Bench. The best LLMs score 40% on Butter-Bench, while the mean human score is 95%. LLMs struggled the most with multi-step spatial planning and social understanding. We also evaluate LLMs that are fine-tuned for embodied reasoning and conclude that this training does not improve their score on Butter-Bench.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21860v1",
    "published_date": "2025-10-23 07:28:28 UTC",
    "updated_date": "2025-10-23 07:28:28 UTC"
  },
  {
    "arxiv_id": "2510.20291v1",
    "title": "A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization",
    "authors": [
      "LinFeng Li",
      "Jian Zhao",
      "Zepeng Yang",
      "Yuhang Song",
      "Bojun Lin",
      "Tianle Zhang",
      "Yuchen Yuan",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone Navigation. The task retrieves the most relevant geo-referenced image from a large multi-platform corpus (satellite/drone/ground) given a natural-language query. Two obstacles are severe inter-platform heterogeneity and a domain gap between generic training descriptions and platform-specific test queries. We mitigate these with a domain-aligned preprocessing pipeline and a Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite augmentation, and removal of orientation words; (ii) an LLM-based caption refinement pipeline to align textual semantics with the distinct visual characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we train three platform experts using a progressive two-stage, hard-negative mining strategy to enhance discriminative power, and fuse their scores at inference. The system tops the official leaderboard, demonstrating robust cross-modal geo-localization under heterogeneous viewpoints.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20291v1",
    "published_date": "2025-10-23 07:23:47 UTC",
    "updated_date": "2025-10-23 07:23:47 UTC"
  },
  {
    "arxiv_id": "2510.20287v1",
    "title": "Breakdance Video classification in the age of Generative AI",
    "authors": [
      "Sauptik Dhar",
      "Naveen Ramakrishnan",
      "Michelle Munson"
    ],
    "abstract": "Large Vision Language models have seen huge application in several sports use-cases recently. Most of these works have been targeted towards a limited subset of popular sports like soccer, cricket, basketball etc; focusing on generative tasks like visual question answering, highlight generation. This work analyzes the applicability of the modern video foundation models (both encoder and decoder) for a very niche but hugely popular dance sports - breakdance. Our results show that Video Encoder models continue to outperform state-of-the-art Video Language Models for prediction tasks. We provide insights on how to choose the encoder model and provide a thorough analysis into the workings of a finetuned decoder model for breakdance video classification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.20287v1",
    "published_date": "2025-10-23 07:18:54 UTC",
    "updated_date": "2025-10-23 07:18:54 UTC"
  },
  {
    "arxiv_id": "2510.20286v1",
    "title": "UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning",
    "authors": [
      "Liangyu Chen",
      "Hanzhang Zhou",
      "Chenglin Cai",
      "Jianan Zhang",
      "Panrong Tong",
      "Quyu Kong",
      "Xu Zhang",
      "Chen Liu",
      "Yuqi Liu",
      "Wenxuan Wang",
      "Yue Wang",
      "Qin Jin",
      "Steven Hoi"
    ],
    "abstract": "GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20286v1",
    "published_date": "2025-10-23 07:18:32 UTC",
    "updated_date": "2025-10-23 07:18:32 UTC"
  },
  {
    "arxiv_id": "2510.20280v2",
    "title": "Context-level Language Modeling by Learning Predictive Context Embeddings",
    "authors": [
      "Beiya Dai",
      "Yuliang Liu",
      "Daozheng Xue",
      "Qipeng Guo",
      "Kai Chen",
      "Xinbing Wang",
      "Bowen Zhou",
      "Zhouhan Lin"
    ],
    "abstract": "Next-token prediction (NTP) is the cornerstone of modern large language models (LLMs) pretraining, driving their unprecedented capabilities in text generation, reasoning, and instruction following. However, the token-level prediction limits the model's capacity to capture higher-level semantic structures and long-range contextual relationships. To overcome this limitation, we introduce \\textbf{ContextLM}, a framework that augments standard pretraining with an inherent \\textbf{next-context prediction} objective. This mechanism trains the model to learn predictive representations of multi-token contexts, leveraging error signals derived from future token chunks. Crucially, ContextLM achieves this enhancement while remaining fully compatible with the standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity). Extensive experiments on the GPT2 and Pythia model families, scaled up to $1.5$B parameters, show that ContextLM delivers consistent improvements in both perplexity and downstream task performance. Our analysis indicates that next-context prediction provides a scalable and efficient pathway to stronger language modeling, yielding better long-range coherence and more effective attention allocation with minimal computational overhead.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16pages,6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20280v2",
    "published_date": "2025-10-23 07:09:45 UTC",
    "updated_date": "2025-10-28 07:35:34 UTC"
  },
  {
    "arxiv_id": "2510.20275v1",
    "title": "Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction",
    "authors": [
      "Yunzhi Liu",
      "Haokai Tan",
      "Rushi Kanjaria",
      "Lihuan Li",
      "Flora D. Salim"
    ],
    "abstract": "Human mobility forecasting is crucial for disaster relief, city planning, and public health. However, existing models either only model location sequences or include time information merely as auxiliary input, thereby failing to leverage the rich semantic context provided by points of interest (POIs). To address this, we enrich a BERT-based mobility model with derived temporal descriptors and POI embeddings to better capture the semantics underlying human movement. We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI and temporal information at each location to construct a unified, semantically enriched representation of mobility. Experimental results show that STaBERT significantly improves prediction accuracy: for single-city prediction, the GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34 to 0.56.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by ACM SIGSPATIAL 2025 as a short paper",
    "pdf_url": "https://arxiv.org/pdf/2510.20275v1",
    "published_date": "2025-10-23 06:59:58 UTC",
    "updated_date": "2025-10-23 06:59:58 UTC"
  },
  {
    "arxiv_id": "2510.20272v1",
    "title": "Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs",
    "authors": [
      "Tristan Cinquin",
      "Geoff Pleiss",
      "Agustinus Kristiadi"
    ],
    "abstract": "While chain-of-thought prompting with Best-of-N (BoN) selection has become popular for mathematical reasoning in large language models (LLMs), its linear structure fails to capture the branching and exploratory nature of complex problem-solving. In this work, we propose an adaptive algorithm to maximize process reward model (PRM) scores over the intractable action space, and investigate whether PRM-guided tree search can improve mathematical reasoning by exploring multiple partial solution paths. Across $23$ diverse mathematical problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case study, we find that: (1) PRM-guided tree search shows no statistically significant improvements over BoN despite higher costs, (2) Monte Carlo tree search and beam search outperform other PRM-guided tree search methods, (3) PRMs poorly approximate state values and their reliability degrades with reasoning depth, and (4) PRMs generalize poorly out of distribution. This underperformance stems from tree search's greater reliance on unreliable PRM scores, suggesting different reward modeling is necessary before tree search can effectively enhance mathematical reasoning in LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20272v1",
    "published_date": "2025-10-23 06:59:36 UTC",
    "updated_date": "2025-10-23 06:59:36 UTC"
  },
  {
    "arxiv_id": "2512.08936v1",
    "title": "A Principle-based Framework for the Development and Evaluation of Large Language Models for Health and Wellness",
    "authors": [
      "Brent Winslow",
      "Jacqueline Shreibati",
      "Javier Perez",
      "Hao-Wei Su",
      "Nichole Young-Lin",
      "Nova Hammerquist",
      "Daniel McDuff",
      "Jason Guss",
      "Jenny Vafeiadou",
      "Nick Cain",
      "Alex Lin",
      "Erik Schenck",
      "Shiva Rajagopal",
      "Jia-Ru Chung",
      "Anusha Venkatakrishnan",
      "Amy Armento Lee",
      "Maryam Karimzadehgan",
      "Qingyou Meng",
      "Rythm Agarwal",
      "Aravind Natarajan",
      "Tracy Giest"
    ],
    "abstract": "The incorporation of generative artificial intelligence into personal health applications presents a transformative opportunity for personalized, data-driven health and fitness guidance, yet also poses challenges related to user safety, model accuracy, and personal privacy. To address these challenges, a novel, principle-based framework was developed and validated for the systematic evaluation of LLMs applied to personal health and wellness. First, the development of the Fitbit Insights explorer, a large language model (LLM)-powered system designed to help users interpret their personal health data, is described. Subsequently, the safety, helpfulness, accuracy, relevance, and personalization (SHARP) principle-based framework is introduced as an end-to-end operational methodology that integrates comprehensive evaluation techniques including human evaluation by generalists and clinical specialists, autorater assessments, and adversarial testing, into an iterative development lifecycle. Through the application of this framework to the Fitbit Insights explorer in a staged deployment involving over 13,000 consented users, challenges not apparent during initial testing were systematically identified. This process guided targeted improvements to the system and demonstrated the necessity of combining isolated technical evaluations with real-world user feedback. Finally, a comprehensive, actionable approach is established for the responsible development and deployment of LLM-powered health applications, providing a standardized methodology to foster innovation while ensuring emerging technologies are safe, effective, and trustworthy for users.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.08936v1",
    "published_date": "2025-10-23 06:54:33 UTC",
    "updated_date": "2025-10-23 06:54:33 UTC"
  },
  {
    "arxiv_id": "2510.20258v1",
    "title": "Using Large Language Models for Abstraction of Planning Domains - Extended Version",
    "authors": [
      "Bita Banihashemi",
      "Megh Patel",
      "Yves Lespérance"
    ],
    "abstract": "Generating an abstraction of a dynamic domain that aligns with a given purpose remains a significant challenge given that the choice of such an abstraction can impact an agent's ability to plan, reason, and provide explanations effectively. We model the agent's concrete behaviors in PDDL and investigate the use of in-context learning with large language models (LLMs) for the generation of abstract PDDL domains and problem instances, given an abstraction objective specified in natural language. The benchmark examples we use are new and have not been part of the data any LLMs have been trained on. We consider three categories of abstractions: abstraction of choice of alternative concrete actions, abstraction of sequences of concrete actions, and abstraction of action/predicate parameters, as well as combinations of these. The generated abstract PDDL domains and problem instances are then checked by symbolic validation tools as well as human experts. Our experiments show that GPT-4o can generally synthesize useful planning domain abstractions in simple settings, although it is better at abstracting over actions than over the associated fluents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20258v1",
    "published_date": "2025-10-23 06:27:03 UTC",
    "updated_date": "2025-10-23 06:27:03 UTC"
  },
  {
    "arxiv_id": "2510.20255v1",
    "title": "Towards AI Agents for Course Instruction in Higher Education: Early Experiences from the Field",
    "authors": [
      "Yogesh Simmhan",
      "Varad Kulkarni"
    ],
    "abstract": "This article presents early findings from designing, deploying and evaluating an AI-based educational agent deployed as the primary instructor in a graduate-level Cloud Computing course at IISc. We detail the design of a Large Language Model (LLM)-driven Instructor Agent, and introduce a pedagogical framework that integrates the Instructor Agent into the course workflow for actively interacting with the students for content delivery, supplemented by the human instructor to offer the course structure and undertake question--answer sessions. We also propose an analytical framework that evaluates the Agent--Student interaction transcripts using interpretable engagement metrics of topic coverage, topic depth and turn-level elaboration. We report early experiences on how students interact with the Agent to explore concepts, clarify doubts and sustain inquiry-driven dialogue during live classroom sessions. We also report preliminary analysis on our evaluation metrics applied across two successive instructional modules that reveals patterns of engagement evolution, transitioning from broad conceptual exploration to deeper, focused inquiry. These demonstrate how structured integration of conversational AI agents can foster reflective learning, offer a reproducible methodology for studying engagement in authentic classroom settings, and support scalable, high-quality higher education.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20255v1",
    "published_date": "2025-10-23 06:23:35 UTC",
    "updated_date": "2025-10-23 06:23:35 UTC"
  },
  {
    "arxiv_id": "2510.20868v1",
    "title": "Crisis-Resilient Portfolio Management via Graph-based Spatio-Temporal Learning",
    "authors": [
      "Zan Li",
      "Rui Fan"
    ],
    "abstract": "Financial time series forecasting faces a fundamental challenge: predicting optimal asset allocations requires understanding regime-dependent correlation structures that transform during crisis periods. Existing graph-based spatio-temporal learning approaches rely on predetermined graph topologies--correlation thresholds, sector classifications--that fail to adapt when market dynamics shift across different crisis mechanisms: credit contagion, pandemic shocks, or inflation-driven selloffs.\n  We present CRISP (Crisis-Resilient Investment through Spatio-temporal Patterns), a graph-based spatio-temporal learning framework that encodes spatial relationships via Graph Convolutional Networks and temporal dynamics via BiLSTM with self-attention, then learns sparse structures through multi-head Graph Attention Networks. Unlike fixed-topology methods, CRISP discovers which asset relationships matter through attention mechanisms, filtering 92.5% of connections as noise while preserving crisis-relevant dependencies for accurate regime-specific predictions.\n  Trained on 2005--2021 data encompassing credit and pandemic crises, CRISP demonstrates robust generalization to 2022--2024 inflation-driven markets--a fundamentally different regime--by accurately forecasting regime-appropriate correlation structures. This enables adaptive portfolio allocation that maintains profitability during downturns, achieving Sharpe ratio 3.76: 707% improvement over equal-weight baselines and 94% improvement over static graph methods. Learned attention weights provide interpretable regime detection, with defensive cluster attention strengthening 49% during crises versus 31% market-wide--emergent behavior from learning to forecast rather than imposing assumptions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20868v1",
    "published_date": "2025-10-23 06:23:15 UTC",
    "updated_date": "2025-10-23 06:23:15 UTC"
  },
  {
    "arxiv_id": "2510.20252v1",
    "title": "Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods",
    "authors": [
      "Tianyi Zhang",
      "Xiaolin Zhou",
      "Yunzhe Wang",
      "Erik Cambria",
      "David Traum",
      "Rui Mao"
    ],
    "abstract": "Individualized cognitive simulation (ICS) aims to build computational models that approximate the thought processes of specific individuals. While large language models (LLMs) convincingly mimic surface-level human behavior such as role-play, their ability to simulate deeper individualized cognitive processes remains poorly understood. To address this gap, we introduce a novel task that evaluates different cognitive representation methods in ICS. We construct a dataset from recently published novels (later than the release date of the tested LLMs) and propose an 11-condition cognitive evaluation framework to benchmark seven off-the-shelf LLMs in the context of authorial style emulation. We hypothesize that effective cognitive representations can help LLMs generate storytelling that better mirrors the original author. Thus, we test different cognitive representations, e.g., linguistic features, concept mappings, and profile-based information. Results show that combining conceptual and linguistic features is particularly effective in ICS, outperforming static profile-based cues in overall evaluation. Importantly, LLMs are more effective at mimicking linguistic style than narrative structure, underscoring their limits in deeper cognitive simulation. These findings provide a foundation for developing AI systems that adapt to individual ways of thinking and expression, advancing more personalized and human-aligned creative technologies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20252v1",
    "published_date": "2025-10-23 06:18:15 UTC",
    "updated_date": "2025-10-23 06:18:15 UTC"
  },
  {
    "arxiv_id": "2510.20867v1",
    "title": "Incentivizing Consistent, Effective and Scalable Reasoning Capability in Audio LLMs via Reasoning Process Rewards",
    "authors": [
      "Jiajun Fan",
      "Roger Ren",
      "Jingyuan Li",
      "Rahul Pandey",
      "Prashanth Gurunath Shivakumar",
      "Ivan Bulyko",
      "Ankur Gandhe",
      "Ge Liu",
      "Yile Gu"
    ],
    "abstract": "The role of reasoning in Audio Large Language Models remains widely underexplored, as introducing a reasoning process often degrades rather than improves performance during inference, a phenomenon we term test-time inverse scaling, where longer reasoning chains yield progressively worse results. We demonstrate that this stems not from fundamental limitations of reasoning itself, but from inadequate training: models without proper guidance for the reasoning process produce hallucinatory, inconsistent reasoning that accumulates errors over longer chains. To address these challenges, we introduce CESAR (Consistent, Effective, and Scalable Audio Reasoners), shifting from outcome verification to rewarding the reasoning process. Our online reinforcement learning framework employs Group Relative Policy Optimization with a multi-faceted reward suite that incentivizes not only correctness and format but also consistency, structured analytical patterns, causal reasoning, domain-knowledge integration, and calibrated reasoning depth. CESAR resolves test-time inverse scaling, transforming reasoning from detriments into gains while revealing model-specific ``reasoning sweet spots\", where performance peaks during test-time scaling. We achieve state-of-the-art results on MMAU Test-mini, substantially outperforming Gemini 2.5 Pro and GPT-4o Audio, and near-human-level performance on MMSU reasoning tasks. Through AI-as-judge evaluations and qualitative comparisons, we provide both quantitative and qualitative validation of our improved reasoning quality. Importantly, enhanced reasoning creates synergistic effects, simultaneously improving multimodal reasoning and perception capabilities. Overall, CESAR establishes a principled method for developing robust and scalable reasoning in Audio LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "49 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.20867v1",
    "published_date": "2025-10-23 06:18:10 UTC",
    "updated_date": "2025-10-23 06:18:10 UTC"
  },
  {
    "arxiv_id": "2510.20242v2",
    "title": "What Does It Take to Build a Performant Selective Classifier?",
    "authors": [
      "Stephan Rabanser",
      "Nicolas Papernot"
    ],
    "abstract": "Selective classifiers improve model reliability by abstaining on inputs the model deems uncertain. However, few practical approaches achieve the gold-standard performance of a perfect-ordering oracle that accepts examples exactly in order of correctness. Our work formalizes this shortfall as the selective-classification gap and present the first finite-sample decomposition of this gap to five distinct sources of looseness: Bayes noise, approximation error, ranking error, statistical noise, and implementation- or shift-induced slack. Crucially, our analysis reveals that monotone post-hoc calibration -- often believed to strengthen selective classifiers -- has limited impact on closing this gap, since it rarely alters the model's underlying score ranking. Bridging the gap therefore requires scoring mechanisms that can effectively reorder predictions rather than merely rescale them. We validate our decomposition on synthetic two-moons data and on real-world vision and language benchmarks, isolating each error component through controlled experiments. Our results confirm that (i) Bayes noise and limited model capacity can account for substantial gaps, (ii) only richer, feature-aware calibrators meaningfully improve score ordering, and (iii) data shift introduces a separate slack that demands distributionally robust training. Together, our decomposition yields a quantitative error budget as well as actionable design guidelines that practitioners can use to build selective classifiers which approximate ideal oracle behavior more closely.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.20242v2",
    "published_date": "2025-10-23 05:48:40 UTC",
    "updated_date": "2025-10-24 01:27:45 UTC"
  },
  {
    "arxiv_id": "2510.20239v1",
    "title": "Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders",
    "authors": [
      "Filippo Cenacchi",
      "Deborah Richards",
      "Longbing Cao"
    ],
    "abstract": "Depression and post traumatic stress disorder (PTSD) often co-occur with connected symptoms, complicating automated assessment, which is often binary and disorder specific. Clinically useful diagnosis needs severity aware cross disorder estimates and decision support explanations. Our unified tri modal affective severity framework synchronizes and fuses interview text with sentence level transformer embeddings, audio with log Mel statistics with deltas, and facial signals with action units, gaze, head and pose descriptors to output graded severities for diagnosing both depression (PHQ-8; 5 classes) and PTSD (3 classes). Standardized features are fused via a calibrated late fusion classifier, yielding per disorder probabilities and feature-level attributions. This severity aware tri-modal affective fusion approach is demoed on multi disorder concurrent depression and PTSD assessment. Stratified cross validation on DAIC derived corpora outperforms unimodal/ablation baselines. The fused model matches the strongest unimodal baseline on accuracy and weighted F1, while improving decision curve utility and robustness under noisy or missing modalities. For PTSD specifically, fusion reduces regression error and improves class concordance. Errors cluster between adjacent severities; extreme classes are identified reliably. Ablations show text contributes most to depression severity, audio and facial cues are critical for PTSD, whereas attributions align with linguistic and behavioral markers. Our approach offers reproducible evaluation and clinician in the loop support for affective clinical decision making.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20239v1",
    "published_date": "2025-10-23 05:46:38 UTC",
    "updated_date": "2025-10-23 05:46:38 UTC"
  },
  {
    "arxiv_id": "2510.20235v1",
    "title": "Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach",
    "authors": [
      "Woohyeon Byeon",
      "Giseung Park",
      "Jongseong Chae",
      "Amir Leshem",
      "Youngchul Sung"
    ],
    "abstract": "In this paper, we propose a provably convergent and practical framework for multi-objective reinforcement learning with max-min criterion. From a game-theoretic perspective, we reformulate max-min multi-objective reinforcement learning as a two-player zero-sum regularized continuous game and introduce an efficient algorithm based on mirror descent. Our approach simplifies the policy update while ensuring global last-iterate convergence. We provide a comprehensive theoretical analysis on our algorithm, including iteration complexity under both exact and approximate policy evaluations, as well as sample complexity bounds. To further enhance performance, we modify the proposed algorithm with adaptive regularization. Our experiments demonstrate the convergence behavior of the proposed algorithm in tabular settings, and our implementation for deep reinforcement learning significantly outperforms previous baselines in many MORL environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.20235v1",
    "published_date": "2025-10-23 05:39:26 UTC",
    "updated_date": "2025-10-23 05:39:26 UTC"
  },
  {
    "arxiv_id": "2510.20229v1",
    "title": "Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context",
    "authors": [
      "Ge Zheng",
      "Jiaye Qian",
      "Jiajin Tang",
      "Sibei Yang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have made significant progress in recent years but are also prone to hallucination issues. They exhibit more hallucinations in longer, free-form responses, often attributed to accumulated uncertainties. In this paper, we ask: Does increased hallucination result solely from length-induced errors, or is there a deeper underlying mechanism? After a series of preliminary experiments and findings, we suggest that the risk of hallucinations is not caused by length itself but by the increased reliance on context for coherence and completeness in longer responses. Building on these insights, we propose a novel \"induce-detect-suppress\" framework that actively induces hallucinations through deliberately designed contexts, leverages induced instances for early detection of high-risk cases, and ultimately suppresses potential object-level hallucinations during actual decoding. Our approach achieves consistent, significant improvements across all benchmarks, demonstrating its efficacy. The strong detection and improved hallucination mitigation not only validate our framework but, more importantly, re-validate our hypothesis on context. Rather than solely pursuing performance gains, this study aims to provide new insights and serves as a first step toward a deeper exploration of hallucinations in LVLMs' longer responses.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20229v1",
    "published_date": "2025-10-23 05:22:07 UTC",
    "updated_date": "2025-10-23 05:22:07 UTC"
  },
  {
    "arxiv_id": "2510.20225v1",
    "title": "Federated Learning via Meta-Variational Dropout",
    "authors": [
      "Insu Jeon",
      "Minui Hong",
      "Junhyeog Yun",
      "Gunhee Kim"
    ],
    "abstract": "Federated Learning (FL) aims to train a global inference model from remotely distributed clients, gaining popularity due to its benefit of improving data privacy. However, traditional FL often faces challenges in practical applications, including model overfitting and divergent local models due to limited and non-IID data among clients. To address these issues, we introduce a novel Bayesian meta-learning approach called meta-variational dropout (MetaVD). MetaVD learns to predict client-dependent dropout rates via a shared hypernetwork, enabling effective model personalization of FL algorithms in limited non-IID data settings. We also emphasize the posterior adaptation view of meta-learning and the posterior aggregation view of Bayesian FL via the conditional dropout posterior. We conducted extensive experiments on various sparse and non-IID FL datasets. MetaVD demonstrated excellent classification accuracy and uncertainty calibration performance, especially for out-of-distribution (OOD) clients. MetaVD compresses the local model parameters needed for each client, mitigating model overfitting and reducing communication costs. Code is available at https://github.com/insujeon/MetaVD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) 2023, Main Conference Track",
    "pdf_url": "https://arxiv.org/pdf/2510.20225v1",
    "published_date": "2025-10-23 05:17:40 UTC",
    "updated_date": "2025-10-23 05:17:40 UTC"
  },
  {
    "arxiv_id": "2510.23577v1",
    "title": "TAMI: Taming Heterogeneity in Temporal Interactions for Temporal Graph Link Prediction",
    "authors": [
      "Zhongyi Yu",
      "Jianqiu Wu",
      "Zhenghao Wu",
      "Shuhan Zhong",
      "Weifeng Su",
      "Chul-Ho Lee",
      "Weipeng Zhuo"
    ],
    "abstract": "Temporal graph link prediction aims to predict future interactions between nodes in a graph based on their historical interactions, which are encoded in node embeddings. We observe that heterogeneity naturally appears in temporal interactions, e.g., a few node pairs can make most interaction events, and interaction events happen at varying intervals. This leads to the problems of ineffective temporal information encoding and forgetting of past interactions for a pair of nodes that interact intermittently for their link prediction. Existing methods, however, do not consider such heterogeneity in their learning process, and thus their learned temporal node embeddings are less effective, especially when predicting the links for infrequently interacting node pairs. To cope with the heterogeneity, we propose a novel framework called TAMI, which contains two effective components, namely log time encoding function (LTE) and link history aggregation (LHA). LTE better encodes the temporal information through transforming interaction intervals into more balanced ones, and LHA prevents the historical interactions for each target node pair from being forgotten. State-of-the-art temporal graph neural networks can be seamlessly and readily integrated into TAMI to improve their effectiveness. Experiment results on 13 classic datasets and three newest temporal graph benchmark (TGB) datasets show that TAMI consistently improves the link prediction performance of the underlying models in both transductive and inductive settings. Our code is available at https://github.com/Alleinx/TAMI_temporal_graph.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.23577v1",
    "published_date": "2025-10-23 05:14:58 UTC",
    "updated_date": "2025-10-23 05:14:58 UTC"
  },
  {
    "arxiv_id": "2510.20221v1",
    "title": "FinCARE: Financial Causal Analysis with Reasoning and Evidence",
    "authors": [
      "Alejandro Michel",
      "Abhinav Arun",
      "Bhaskarjit Sarmah",
      "Stefano Pasquali"
    ],
    "abstract": "Portfolio managers rely on correlation-based analysis and heuristic methods that fail to capture true causal relationships driving performance. We present a hybrid framework that integrates statistical causal discovery algorithms with domain knowledge from two complementary sources: a financial knowledge graph extracted from SEC 10-K filings and large language model reasoning. Our approach systematically enhances three representative causal discovery paradigms, constraint-based (PC), score-based (GES), and continuous optimization (NOTEARS), by encoding knowledge graph constraints algorithmically and leveraging LLM conceptual reasoning for hypothesis generation. Evaluated on a synthetic financial dataset of 500 firms across 18 variables, our KG+LLM-enhanced methods demonstrate consistent improvements across all three algorithms: PC (F1: 0.622 vs. 0.459 baseline, +36%), GES (F1: 0.735 vs. 0.367, +100%), and NOTEARS (F1: 0.759 vs. 0.163, +366%). The framework enables reliable scenario analysis with mean absolute error of 0.003610 for counterfactual predictions and perfect directional accuracy for intervention effects. It also addresses critical limitations of existing methods by grounding statistical discoveries in financial domain expertise while maintaining empirical validation, providing portfolio managers with the causal foundation necessary for proactive risk management and strategic decision-making in dynamic market environments.",
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20221v1",
    "published_date": "2025-10-23 05:14:28 UTC",
    "updated_date": "2025-10-23 05:14:28 UTC"
  },
  {
    "arxiv_id": "2510.20218v1",
    "title": "High-order Interactions Modeling for Interpretable Multi-Agent Q-Learning",
    "authors": [
      "Qinyu Xu",
      "Yuanyang Zhu",
      "Xuefei Wu",
      "Chunlin Chen"
    ],
    "abstract": "The ability to model interactions among agents is crucial for effective coordination and understanding their cooperation mechanisms in multi-agent reinforcement learning (MARL). However, previous efforts to model high-order interactions have been primarily hindered by the combinatorial explosion or the opaque nature of their black-box network structures. In this paper, we propose a novel value decomposition framework, called Continued Fraction Q-Learning (QCoFr), which can flexibly capture arbitrary-order agent interactions with only linear complexity $\\mathcal{O}\\left({n}\\right)$ in the number of agents, thus avoiding the combinatorial explosion when modeling rich cooperation. Furthermore, we introduce the variational information bottleneck to extract latent information for estimating credits. This latent information helps agents filter out noisy interactions, thereby significantly enhancing both cooperation and interpretability. Extensive experiments demonstrate that QCoFr not only consistently achieves better performance but also provides interpretability that aligns with our theoretical analysis.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "39th Conference on Neural Information Processing Systems",
    "pdf_url": "https://arxiv.org/pdf/2510.20218v1",
    "published_date": "2025-10-23 05:08:32 UTC",
    "updated_date": "2025-10-23 05:08:32 UTC"
  },
  {
    "arxiv_id": "2510.20211v1",
    "title": "Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents",
    "authors": [
      "Zhenning Yang",
      "Hui Guan",
      "Victor Nicolet",
      "Brandon Paulsen",
      "Joey Dodds",
      "Daniel Kroening",
      "Ang Chen"
    ],
    "abstract": "Cloud infrastructure is managed through a mix of interfaces -- traditionally, cloud consoles, command-line interfaces (CLI), and SDKs are the tools of choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the infrastructure in a \"source-of-truth\" configuration. They are capable of automatically carrying out modifications to the cloud -- deploying, updating, or destroying resources -- to bring the actual infrastructure into alignment with the IaC configuration. However, when IaC is used alongside consoles, CLIs, or SDKs, it loses visibility into external changes, causing infrastructure drift, where the configuration becomes outdated, and later IaC operations may undo valid updates or trigger errors.\n  We present NSync, an automated system for IaC reconciliation that propagates out-of-band changes back into the IaC program. Our key insight is that infrastructure changes eventually all occur via cloud API invocations -- the lowest layer for cloud management operations. NSync gleans insights from API traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update the IaC configuration to capture the changes). It employs an agentic architecture that leverages LLMs to infer high-level intents from noisy API sequences, synthesize targeted IaC updates using specialized tools, and continually improve through a self-evolving knowledge base of past reconciliations. We further introduce a novel evaluation pipeline for injecting realistic drifts into cloud infrastructure and assessing reconciliation performance. Experiments across five real-world Terraform projects and 372 drift scenarios show that NSync outperforms the baseline both in terms of accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\\times$ improvement).",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20211v1",
    "published_date": "2025-10-23 04:57:00 UTC",
    "updated_date": "2025-10-23 04:57:00 UTC"
  },
  {
    "arxiv_id": "2510.20209v2",
    "title": "Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset",
    "authors": [
      "Shumin Li"
    ],
    "abstract": "The development of accessible screening tools for early cancer detection in dogs represents a significant challenge in veterinary medicine. Routine laboratory data offer a promising, low-cost source for such tools, but their utility is hampered by the non-specificity of individual biomarkers and the severe class imbalance inherent in screening populations. This study assesses the feasibility of cancer risk classification using the Golden Retriever Lifetime Study (GRLS) cohort under real-world constraints, including the grouping of diverse cancer types and the inclusion of post-diagnosis samples. A comprehensive benchmark evaluation was conducted, systematically comparing 126 analytical pipelines that comprised various machine learning models, feature selection methods, and data balancing techniques. Data were partitioned at the patient level to prevent leakage. The optimal model, a Logistic Regression classifier with class weighting and recursive feature elimination, demonstrated moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical classification performance (F1-score = 0.25, Positive Predictive Value = 0.15). While a high Negative Predictive Value (0.98) was achieved, insufficient recall (0.79) precludes its use as a reliable rule-out test. Interpretability analysis with SHapley Additive exPlanations (SHAP) revealed that predictions were driven by non-specific features like age and markers of inflammation and anemia. It is concluded that while a statistically detectable cancer signal exists in routine lab data, it is too weak and confounded for clinically reliable discrimination from normal aging or other inflammatory conditions. This work establishes a critical performance ceiling for this data modality in isolation and underscores that meaningful progress in computational veterinary oncology will require integration of multi-modal data sources.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20209v2",
    "published_date": "2025-10-23 04:52:42 UTC",
    "updated_date": "2025-10-25 00:55:35 UTC"
  },
  {
    "arxiv_id": "2510.20205v1",
    "title": "Merge and Conquer: Evolutionarily Optimizing AI for 2048",
    "authors": [
      "Maggie Bai",
      "Ava Kim Cohen",
      "Eleanor Koss",
      "Charlie Lichtenbaum"
    ],
    "abstract": "Optimizing artificial intelligence (AI) for dynamic environments remains a fundamental challenge in machine learning research. In this paper, we examine evolutionary training methods for optimizing AI to solve the game 2048, a 2D sliding puzzle. 2048, with its mix of strategic gameplay and stochastic elements, presents an ideal playground for studying decision-making, long-term planning, and dynamic adaptation. We implemented two distinct systems: a two-agent metaprompting system where a \"thinker\" large language model (LLM) agent refines gameplay strategies for an \"executor\" LLM agent, and a single-agent system based on refining a value function for a limited Monte Carlo Tree Search. We also experimented with rollback features to avoid performance degradation. Our results demonstrate the potential of evolutionary refinement techniques in improving AI performance in non-deterministic environments. The single-agent system achieved substantial improvements, with an average increase of 473.2 points per cycle, and with clear upward trends (correlation $ρ$=0.607) across training cycles. The LLM's understanding of the game grew as well, shown in its development of increasingly advanced strategies. Conversely, the two-agent system did not garner much improvement, highlighting the inherent limits of meta-prompting.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20205v1",
    "published_date": "2025-10-23 04:45:05 UTC",
    "updated_date": "2025-10-23 04:45:05 UTC"
  },
  {
    "arxiv_id": "2510.20198v1",
    "title": "Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models",
    "authors": [
      "Maggie Bai",
      "Ava Kim Cohen",
      "Eleanor Koss",
      "Charlie Lichtenbaum"
    ],
    "abstract": "This paper explores the spatial reasoning capability of large language models (LLMs) over textual input through a suite of five tasks aimed at probing their spatial understanding and computational abilities. The models were tested on both fundamental spatial reasoning and multi-step problem-solving within structured grid-based environments using tasks such as quadrant identification, geometric transformations, distance evaluation, word searches, and tile sliding. Each task was scaled in complexity through increasing grid dimensions, requiring models to extend beyond simple pattern recognition into abstract spatial reasoning. Our results reveal that while LLMs demonstrate moderate success in all tasks with small complexity and size, performance drops off rapidly as scale increases, with an average loss in accuracy of 42.7%, and reaching as high as 84%. Every test that began with over 50% accuracy showed a loss of at least 48%, illustrating the consistent nature of the deterioration. Furthermore, their struggles with scaling complexity hint at a lack of robust spatial representations in their underlying architectures. This paper underscores the gap between linguistic and spatial reasoning in LLMs, offering insights into their current limitations, and laying the groundwork for future integrative benchmarks at the intersection of language and geometry.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 24 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20198v1",
    "published_date": "2025-10-23 04:32:46 UTC",
    "updated_date": "2025-10-23 04:32:46 UTC"
  },
  {
    "arxiv_id": "2510.21858v1",
    "title": "Privacy-preserving Decision-focused Learning for Multi-energy Systems",
    "authors": [
      "Yangze Zhou",
      "Ruiyang Yao",
      "Dalin Qin",
      "Yixiong Jia",
      "Yi Wang"
    ],
    "abstract": "Decision-making for multi-energy system (MES) dispatch depends on accurate load forecasting. Traditionally, load forecasting and decision-making for MES are implemented separately. Forecasting models are typically trained to minimize forecasting errors, overlooking their impact on downstream decision-making. To address this, decision-focused learning (DFL) has been studied to minimize decision-making costs instead. However, practical adoption of DFL in MES faces significant challenges: the process requires sharing sensitive load data and model parameters across multiple sectors, raising serious privacy issues. To this end, we propose a privacy-preserving DFL framework tailored for MES. Our approach introduces information masking to safeguard private data while enabling recovery of decision variables and gradients required for model training. To further enhance security for DFL, we design a safety protocol combining matrix decomposition and homomorphic encryption, effectively preventing collusion and unauthorized data access. Additionally, we developed a privacy-preserving load pattern recognition algorithm, enabling the training of specialized DFL models for heterogeneous load patterns. Theoretical analysis and comprehensive case studies, including real-world MES data, demonstrate that our framework not only protects privacy but also consistently achieves lower average daily dispatch costs compared to existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21858v1",
    "published_date": "2025-10-23 04:20:50 UTC",
    "updated_date": "2025-10-23 04:20:50 UTC"
  },
  {
    "arxiv_id": "2510.20190v1",
    "title": "The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI",
    "authors": [
      "Marcelo Maciel Amaral",
      "Raymond Aschheim"
    ],
    "abstract": "Large language models (LLMs) remain broadly open and highly steerable: they imitate at scale, accept arbitrary system prompts, and readily adopt multiple personae. By analogy to human development, we hypothesize that progress toward artificial general intelligence (AGI) involves a lock-in phase: a transition from open imitation to identity consolidation, in which goal structures, refusals, preferences, and internal representations become comparatively stable and resistant to external steering. We formalize this phase, link it to known phenomena in learning dynamics, and propose operational metrics for onset detection. Experimentally, we demonstrate that while the behavioral consolidation is rapid and non-linear, its side-effects on general capabilities are not monolithic. Our results reveal a spectrum of outcomes--from performance trade-offs in small models, through largely cost-free adoption in mid-scale models, to transient instabilities in large, quantized models. We argue that such consolidation is a prerequisite for AGI-level reliability and also a critical control point for safety: identities can be deliberately engineered for reliability, yet may also emerge spontaneously during scaling, potentially hardening unpredictable goals and behaviors.",
    "categories": [
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20190v1",
    "published_date": "2025-10-23 04:20:10 UTC",
    "updated_date": "2025-10-23 04:20:10 UTC"
  },
  {
    "arxiv_id": "2510.20188v1",
    "title": "TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning",
    "authors": [
      "Morris Yu-Chao Huang",
      "Zhen Tan",
      "Mohan Zhang",
      "Pingzhi Li",
      "Zhuo Zhang",
      "Tianlong Chen"
    ],
    "abstract": "Large Language Models generate complex reasoning chains that reveal their decision-making, yet verifying the faithfulness and harmlessness of these intermediate steps remains a critical unsolved problem. Existing auditing methods are centralized, opaque, and hard to scale, creating significant risks for deploying proprietary models in high-stakes domains. We identify four core challenges: (1) Robustness: Centralized auditors are single points of failure, prone to bias or attacks. (2) Scalability: Reasoning traces are too long for manual verification. (3) Opacity: Closed auditing undermines public trust. (4) Privacy: Exposing full reasoning risks model theft or distillation. We propose TRUST, a transparent, decentralized auditing framework that overcomes these limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing correctness under up to $30\\%$ malicious participants. (2) A hierarchical DAG decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A blockchain ledger that records all verification decisions for public accountability. (4) Privacy-preserving segmentation, sharing only partial reasoning steps to protect proprietary logic. We provide theoretical guarantees for the security and economic incentives of the TRUST framework. Experiments across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math, medical, science, humanities) show TRUST effectively detects reasoning flaws and remains robust against adversarial auditors. Our work pioneers decentralized AI auditing, offering a practical path toward safe and trustworthy LLM deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20188v1",
    "published_date": "2025-10-23 04:16:44 UTC",
    "updated_date": "2025-10-23 04:16:44 UTC"
  },
  {
    "arxiv_id": "2510.20178v1",
    "title": "PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching",
    "authors": [
      "Yun Wang",
      "Junjie Hu",
      "Qiaole Dong",
      "Yongjian Zhang",
      "Yanwei Fu",
      "Tin Lun Lam",
      "Dapeng Wu"
    ],
    "abstract": "Temporally consistent depth estimation from stereo video is critical for real-world applications such as augmented reality, where inconsistent depth estimation disrupts the immersion of users. Despite its importance, this task remains challenging due to the difficulty in modeling long-term temporal consistency in a computationally efficient manner. Previous methods attempt to address this by aggregating spatio-temporal information but face a fundamental trade-off: limited temporal modeling provides only modest gains, whereas capturing long-range dependencies significantly increases computational cost. To address this limitation, we introduce a memory buffer for modeling long-range spatio-temporal consistency while achieving efficient dynamic stereo matching. Inspired by the two-stage decision-making process in humans, we propose a \\textbf{P}ick-and-\\textbf{P}lay \\textbf{M}emory (PPM) construction module for dynamic \\textbf{Stereo} matching, dubbed as \\textbf{PPMStereo}. PPM consists of a `pick' process that identifies the most relevant frames and a `play' process that weights the selected frames adaptively for spatio-temporal aggregation. This two-stage collaborative process maintains a compact yet highly informative memory buffer while achieving temporally consistent information aggregation. Extensive experiments validate the effectiveness of PPMStereo, demonstrating state-of-the-art performance in both accuracy and temporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on the Sintel clean/final (17.3\\% \\& 9.02\\% improvements over BiDAStereo) with fewer computational costs. Codes are available at \\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20178v1",
    "published_date": "2025-10-23 03:52:39 UTC",
    "updated_date": "2025-10-23 03:52:39 UTC"
  },
  {
    "arxiv_id": "2510.20176v2",
    "title": "Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding",
    "authors": [
      "Yuhang Zhou",
      "Mingrui Zhang",
      "Ke Li",
      "Mingyi Wang",
      "Qiao Liu",
      "Qifei Wang",
      "Jiayi Liu",
      "Fei Liu",
      "Serena Li",
      "Weiwei Li",
      "Mingze Gao",
      "Abhishek Kumar",
      "Xiangjun Fan",
      "Zhuokai Zhao",
      "Lizhu Zhang"
    ],
    "abstract": "Understanding and reasoning over tables is a critical capability for many real-world applications. Large language models (LLMs) have shown promise on this task, but current approaches remain limited. Fine-tuning based methods strengthen language reasoning; yet they are prone to arithmetic errors and hallucination. In contrast, tool-based methods enable precise table manipulation but rely on rigid schemas and lack semantic understanding. These complementary drawbacks highlight the need for approaches that integrate robust reasoning with reliable table processing. In this work, we propose Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into three specialized roles: planning, coding, and answering. This design enables each agent to focus on a specific aspect of the task while leveraging code execution for precise table manipulation. Building on this workflow, we introduce a self-improvement training framework that employs Monte Carlo Tree Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents with reinforcement learning (RL). Extensive experiments show that Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and surpassing OpenAI-o4-mini-high. These results demonstrate the promise of combining structured multi-agent workflows with RL to advance table understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20176v2",
    "published_date": "2025-10-23 03:51:17 UTC",
    "updated_date": "2025-10-24 15:36:31 UTC"
  },
  {
    "arxiv_id": "2510.24762v1",
    "title": "Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation",
    "authors": [
      "Wenzhen Luo",
      "Wei Guan",
      "Yifan Yao",
      "Yimin Pan",
      "Feng Wang",
      "Zhipeng Yu",
      "Zhe Wen",
      "Liang Chen",
      "Yihong Zhuang"
    ],
    "abstract": "We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese questions over 28 databases; 77% require multi-table reasoning and over half touch more than four tables. Each example is annotated along SQL-computation features and Chinese semantics. For evaluation, we release a robust execution comparator and an automated evaluation pipeline, under which all current state-of-the-art large-scale models (including Deepseek) achieve accuracies of at most 50%. Major errors originate from two sources: (1) schema linking in large enterprise landscapes - hundreds of tables, denormalized fields, ambiguous column names, implicit foreign-key relations and domain-specific synonyms that make correct join/column selection difficult; and (2) mapping concise, colloquial Chinese into the exact operators and predicates required for analytics - e.g., choosing the correct aggregation and group-by keys, expressing time windows and granularities, applying unit conversions, handling NULLs and data-quality rules, and formulating nested or windowed subqueries. Falcon therefore targets Chinese-specific semantics and enterprise dialects (abbreviations, business jargon, fuzzy entity references) and provides a reproducible middle ground before full production deployment by using realistic enterprise schemas, query templates, an execution comparator, and an automated evaluation pipeline for end-to-end validation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.24762v1",
    "published_date": "2025-10-23 03:35:00 UTC",
    "updated_date": "2025-10-23 03:35:00 UTC"
  },
  {
    "arxiv_id": "2510.20171v4",
    "title": "Collective Communication for 100k+ GPUs",
    "authors": [
      "Min Si",
      "Pavan Balaji",
      "Yongzhou Chen",
      "Ching-Hsiang Chu",
      "Adi Gangidi",
      "Saif Hasan",
      "Subodh Iyengar",
      "Dan Johnson",
      "Bingzhe Liu",
      "Regina Ren",
      "Deep Shah",
      "Ashmitha Jeevaraj Shetty",
      "Greg Steinbrecher",
      "Yulun Wang",
      "Bruce Wu",
      "Xinfeng Xie",
      "Jingyi Yang",
      "Mingran Yang",
      "Kenny Yu",
      "Minlan Yu",
      "Cen Zhao",
      "Wes Bland",
      "Denis Boyda",
      "Suman Gumudavelli",
      "Prashanth Kannan",
      "Cristian Lumezanu",
      "Rui Miao",
      "Zhe Qu",
      "Venkat Ramesh",
      "Maxim Samoylov",
      "Jan Seidel",
      "Srikanth Sundaresan",
      "Feng Tian",
      "Qiye Tan",
      "Shuqiang Zhang",
      "Yimeng Zhao",
      "Shengbao Zheng",
      "Art Zhu",
      "Hongyi Zeng"
    ],
    "abstract": "The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hindering both the development and deployment of state-of-the-art models. This paper presents the NCCLX collective communication framework, developed at Meta, engineered to optimize performance across the full LLM lifecycle, from the synchronous demands of large-scale training to the low-latency requirements of inference. The framework is designed to support complex workloads on clusters exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency data exchange. Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency. This research contributes a robust solution for enabling the next generation of LLMs to operate at unprecedented scales.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20171v4",
    "published_date": "2025-10-23 03:32:04 UTC",
    "updated_date": "2026-01-09 16:53:26 UTC"
  },
  {
    "arxiv_id": "2510.20165v1",
    "title": "IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks",
    "authors": [
      "Insu Jeon",
      "Wonkwang Lee",
      "Myeongjang Pyeon",
      "Gunhee Kim"
    ],
    "abstract": "We propose a new GAN-based unsupervised model for disentangled representation learning. The new model is discovered in an attempt to utilize the Information Bottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The architecture of IB-GAN is partially similar to that of InfoGAN but has a critical difference; an intermediate layer of the generator is leveraged to constrain the mutual information between the input and the generated output. The intermediate stochastic layer can serve as a learnable latent distribution that is trained with the generator jointly in an end-to-end fashion. As a result, the generator of IB-GAN can harness the latent space in a disentangled and interpretable manner. With the experiments on dSprites and Color-dSprites dataset, we demonstrate that IB-GAN achieves competitive disentanglement scores to those of state-of-the-art \\b{eta}-VAEs and outperforms InfoGAN. Moreover, the visual quality and the diversity of samples generated by IB-GAN are often better than those by \\b{eta}-VAEs and Info-GAN in terms of FID score on CelebA and 3D Chairs dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in the Proceedings of the Thirty Fifth AAAI Conference on Artificial Intelligence (AAAI 2021), paper number 7926",
    "pdf_url": "https://arxiv.org/pdf/2510.20165v1",
    "published_date": "2025-10-23 03:24:48 UTC",
    "updated_date": "2025-10-23 03:24:48 UTC"
  },
  {
    "arxiv_id": "2510.21857v1",
    "title": "Poisson Flow Consistency Training",
    "authors": [
      "Anthony Zhang",
      "Mahmut Gokmen",
      "Dennis Hein",
      "Rongjun Ge",
      "Wenjun Xia",
      "Ge Wang",
      "Jin Chen"
    ],
    "abstract": "The Poisson Flow Consistency Model (PFCM) is a consistency-style model based on the robust Poisson Flow Generative Model++ (PFGM++) which has achieved success in unconditional image generation and CT image denoising. Yet the PFCM can only be trained in distillation which limits the potential of the PFCM in many data modalities. The objective of this research was to create a method to train the PFCM in isolation called Poisson Flow Consistency Training (PFCT). The perturbation kernel was leveraged to remove the pretrained PFGM++, and the sinusoidal discretization schedule and Beta noise distribution were introduced in order to facilitate adaptability and improve sample quality. The model was applied to the task of low dose computed tomography image denoising and improved the low dose image in terms of LPIPS and SSIM. It also displayed similar denoising effectiveness as models like the Consistency Model. PFCT is established as a valid method of training the PFCM from its effectiveness in denoising CT images, showing potential with competitive results to other generative models. Further study is needed in the precise optimization of PFCT and in its applicability to other generative modeling tasks. The framework of PFCT creates more flexibility for the ways in which a PFCM can be created and can be applied to the field of generative modeling.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 3 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2510.21857v1",
    "published_date": "2025-10-23 03:23:11 UTC",
    "updated_date": "2025-10-23 03:23:11 UTC"
  },
  {
    "arxiv_id": "2510.20154v1",
    "title": "Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?",
    "authors": [
      "Anthony Dubreuil",
      "Antoine Gourru",
      "Christine Largeron",
      "Amine Trabelsi"
    ],
    "abstract": "Large Language Models inherit stereotypes from their pretraining data, leading to biased behavior toward certain social groups in many Natural Language Processing tasks, such as hateful speech detection or sentiment analysis. Surprisingly, the evaluation of this kind of bias in stance detection methods has been largely overlooked by the community. Stance Detection involves labeling a statement as being against, in favor, or neutral towards a specific target and is among the most sensitive NLP tasks, as it often relates to political leanings. In this paper, we focus on the bias of Large Language Models when performing stance detection in a zero-shot setting. We automatically annotate posts in pre-existing stance detection datasets with two attributes: dialect or vernacular of a specific group and text complexity/readability, to investigate whether these attributes influence the model's stance detection decisions. Our results show that LLMs exhibit significant stereotypes in stance detection tasks, such as incorrectly associating pro-marijuana views with low text complexity and African American dialect with opposition to Donald Trump.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in EMNLP 2025 (Main)",
    "pdf_url": "https://arxiv.org/pdf/2510.20154v1",
    "published_date": "2025-10-23 03:05:25 UTC",
    "updated_date": "2025-10-23 03:05:25 UTC"
  },
  {
    "arxiv_id": "2510.20129v1",
    "title": "SAID: Empowering Large Language Models with Self-Activating Internal Defense",
    "authors": [
      "Yulong Chen",
      "Yadong Liu",
      "Jiawen Zhang",
      "Mu Li",
      "Chao Huang",
      "Jie Wen"
    ],
    "abstract": "Large Language Models (LLMs), despite advances in safety alignment, remain vulnerable to jailbreak attacks designed to circumvent protective mechanisms. Prevailing defense strategies rely on external interventions, such as input filtering or output modification, which often lack generalizability and compromise model utility while incurring significant computational overhead. In this work, we introduce a new, training-free defense paradigm, Self-Activating Internal Defense (SAID), which reframes the defense task from external correction to internal capability activation. SAID uniquely leverages the LLM's own reasoning abilities to proactively identify and neutralize malicious intent through a three-stage pipeline: model-native intent distillation to extract core semantics, optimal safety prefix probing to activate latent safety awareness, and a conservative aggregation strategy to ensure robust decision-making. Extensive experiments on five open-source LLMs against six advanced jailbreak attacks demonstrate that SAID substantially outperforms state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves this while preserving model performance on benign tasks and incurring minimal computational overhead. Our work establishes that activating the intrinsic safety mechanisms of LLMs is a more robust and scalable path toward building safer and more reliable aligned AI systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20129v1",
    "published_date": "2025-10-23 02:07:54 UTC",
    "updated_date": "2025-10-23 02:07:54 UTC"
  },
  {
    "arxiv_id": "2510.20109v1",
    "title": "The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice",
    "authors": [
      "Joshua Yuvaraj"
    ],
    "abstract": "It is often claimed that machine learning-based generative AI products will drastically streamline and reduce the cost of legal practice. This enthusiasm assumes lawyers can effectively manage AI's risks. Cases in Australia and elsewhere in which lawyers have been reprimanded for submitting inaccurate AI-generated content to courts suggest this paradigm must be revisited. This paper argues that a new paradigm is needed to evaluate AI use in practice, given (a) AI's disconnection from reality and its lack of transparency, and (b) lawyers' paramount duties like honesty, integrity, and not to mislead the court. It presents an alternative model of AI use in practice that more holistically reflects these features (the verification-value paradox). That paradox suggests increases in efficiency from AI use in legal practice will be met by a correspondingly greater imperative to manually verify any outputs of that use, rendering the net value of AI use often negligible to lawyers. The paper then sets out the paradox's implications for legal practice and legal education, including for AI use but also the values that the paradox suggests should undergird legal practice: fidelity to the truth and civic responsibility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20109v1",
    "published_date": "2025-10-23 01:26:37 UTC",
    "updated_date": "2025-10-23 01:26:37 UTC"
  },
  {
    "arxiv_id": "2510.20102v1",
    "title": "Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions",
    "authors": [
      "Gyuyeon Na",
      "Minjung Park",
      "Hyeonjeong Cha",
      "Sangmi Chai"
    ],
    "abstract": "We present HCLA, a human-centered multi-agent system for anomaly detection in digital asset transactions. The system links three roles: Parsing, Detection, and Explanation, into a conversational workflow that lets non-experts ask questions in natural language, inspect structured analytics, and obtain context-aware rationales. Implemented with an open-source web UI, HCLA translates user intents into a schema for a classical detector (XGBoost in our prototype) and returns narrative explanations grounded in the underlying features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the baseline detector reaches strong accuracy, while HCLA adds interpretability and interactive refinement. We describe the architecture, interaction loop, dataset, evaluation protocol, and limitations, and discuss how a human-in-the-loop design improves transparency and trust in financial forensics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20102v1",
    "published_date": "2025-10-23 01:04:36 UTC",
    "updated_date": "2025-10-23 01:04:36 UTC"
  },
  {
    "arxiv_id": "2510.20099v1",
    "title": "AI PB: A Grounded Generative Agent for Personalized Investment Insights",
    "authors": [
      "Daewoo Park",
      "Suho Park",
      "Inseok Hong",
      "Hanwool Lee",
      "Junkyu Park",
      "Sangjun Lee",
      "Jeongman An",
      "Hyunbin Loh"
    ],
    "abstract": "We present AI PB, a production-scale generative agent deployed in real retail finance. Unlike reactive chatbots that answer queries passively, AI PB proactively generates grounded, compliant, and user-specific investment insights. It integrates (i) a component-based orchestration layer that deterministically routes between internal and external LLMs based on data sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the finance-domain embedding model, and (iii) a multi-stage recommendation mechanism combining rule heuristics, sequential behavioral modeling, and contextual bandits. Operating fully on-premises under Korean financial regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100 GPUs. Through human QA and system metrics, we demonstrate that grounded generation with explicit routing and layered safety can deliver trustworthy AI insights in high-stakes finance.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.20099v1",
    "published_date": "2025-10-23 00:51:59 UTC",
    "updated_date": "2025-10-23 00:51:59 UTC"
  },
  {
    "arxiv_id": "2510.20098v2",
    "title": "Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning",
    "authors": [
      "Yajie Li",
      "Albert Galimov",
      "Mitra Datta Ganapaneni",
      "Pujitha Thejaswi",
      "De Meng",
      "Priyanshu Kumar",
      "Saloni Potdar"
    ],
    "abstract": "Entity Linking (EL) has traditionally relied on large annotated datasets and extensive model fine-tuning. While recent few-shot methods leverage large language models (LLMs) through prompting to reduce training requirements, they often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER (Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline that achieves high performance without deep fine-tuning by strategically combining candidate generation, context-based scoring, adaptive routing, and selective reasoning. ARTER computes a small set of complementary signals(both embedding and LLM-based) over the retrieved candidates to categorize contextual mentions into easy and hard cases. The cases are then handled by a low-computational entity linker (e.g. ReFinED) and more expensive targeted LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets, and performs comparably to pipelines using LLM-based reasoning for all mentions, while being as twice as efficient in terms of the number of LLM tokens.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Industry Track",
    "pdf_url": "https://arxiv.org/pdf/2510.20098v2",
    "published_date": "2025-10-23 00:50:14 UTC",
    "updated_date": "2025-11-19 09:50:56 UTC"
  },
  {
    "arxiv_id": "2510.20094v2",
    "title": "On the Structure of Stationary Solutions to McKean-Vlasov Equations with Applications to Noisy Transformers",
    "authors": [
      "Krishnakumar Balasubramanian",
      "Sayan Banerjee",
      "Philippe Rigollet"
    ],
    "abstract": "We study stationary solutions of McKean-Vlasov equations on the circle. Our main contributions stem from observing an exact equivalence between solutions of the stationary McKean-Vlasov equation and an infinite-dimensional quadratic system of equations over Fourier coefficients, which allows explicit characterization of the stationary states in a sequence space rather than a function space. This framework provides a transparent description of local bifurcations, characterizing their periodicity, and resonance structures, while accommodating singular potentials. We derive analytic expressions that characterize the emergence, form and shape (supercritical, critical, subcritical or transcritical) of bifurcations involving possibly multiple Fourier modes and connect them with discontinuous phase transitions. We also characterize, under suitable assumptions, the detailed structure of the stationary bifurcating solutions that are accurate upto an arbitrary number of Fourier modes. At the global level, we establish regularity and concavity properties of the free energy landscape, proving existence, compactness, and coexistence of globally minimizing stationary measures, further identifying discontinuous phase transitions with points of non-differentiability of the minimum free energy map. As an application, we specialize the theory to the Noisy Mean-Field Transformer model, where we show how changing the inverse temperature parameter $β$ affects the geometry of the infinitely many bifurcations from the uniform measure. We also explain how increasing $β$ can lead to a rich class of approximate multi-mode stationary solutions which can be seen as `metastable states'. Further, a sharp transition from continuous to discontinuous (first-order) phase behavior is observed as $β$ increases.",
    "categories": [
      "math.PR",
      "cs.AI",
      "cs.LG",
      "math.AP",
      "stat.ML"
    ],
    "primary_category": "math.PR",
    "comment": "46 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.20094v2",
    "published_date": "2025-10-23 00:28:32 UTC",
    "updated_date": "2025-10-27 17:12:03 UTC"
  },
  {
    "arxiv_id": "2510.20093v1",
    "title": "StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback",
    "authors": [
      "Jiho Park",
      "Sieun Choi",
      "Jaeyoon Seo",
      "Jihie Kim"
    ],
    "abstract": "Although recent advancements in diffusion models have significantly enriched the quality of generated images, challenges remain in synthesizing pixel-based human-drawn sketches, a representative example of abstract expression. To combat these challenges, we propose StableSketcher, a novel framework that empowers diffusion models to generate hand-drawn sketches with high prompt fidelity. Within this framework, we fine-tune the variational autoencoder to optimize latent decoding, enabling it to better capture the characteristics of sketches. In parallel, we integrate a new reward function for reinforcement learning based on visual question answering, which improves text-image alignment and semantic consistency. Extensive experiments demonstrate that StableSketcher generates sketches with improved stylistic fidelity, achieving better alignment with prompts compared to the Stable Diffusion baseline. Additionally, we introduce SketchDUO, to the best of our knowledge, the first dataset comprising instance-level sketches paired with captions and question-answer pairs, thereby addressing the limitations of existing datasets that rely on image-label pairs. Our code and dataset will be made publicly available upon acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review at IEEE Access. Author-submitted preprint. Not the IEEE-published version",
    "pdf_url": "https://arxiv.org/pdf/2510.20093v1",
    "published_date": "2025-10-23 00:27:32 UTC",
    "updated_date": "2025-10-23 00:27:32 UTC"
  },
  {
    "arxiv_id": "2510.20091v1",
    "title": "CreativityPrism: A Holistic Benchmark for Large Language Model Creativity",
    "authors": [
      "Zhaoyi Joey Hou",
      "Bowei Alvin Zhang",
      "Yining Lu",
      "Bhiman Kumar Baghel",
      "Anneliese Brei",
      "Ximing Lu",
      "Meng Jiang",
      "Faeze Brahman",
      "Snigdha Chaturvedi",
      "Haw-Shiuan Chang",
      "Daniel Khashabi",
      "Xiang Lorraine Li"
    ],
    "abstract": "Creativity is often seen as a hallmark of human intelligence. While large language models (LLMs) are increasingly perceived as producing creative text, there is still no holistic framework to evaluate their creativity across diverse scenarios. Existing evaluation methods remain fragmented, with dramatic variation across domains and tasks, largely due to differing definitions and measurements of creativity. Inspired by the hypothesis that creativity is not one fixed idea, we propose CreativityPrism, an evaluation analysis framework that decomposes creativity into three dimensions: quality, novelty, and diversity. CreativityPrism incorporates nine tasks, three domains, i.e., divergent thinking, creative writing, and logical reasoning, and twenty evaluation metrics, which measure each dimension in task-specific, unique ways. We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on CreativityPrism and analyze the performance correlations among different metrics and task domains. Our results reveal a notable gap between proprietary and open-source models. Overall, model performance tends to be highly correlated across tasks within the same domain and less so across different domains. Among evaluation dimensions, diversity and quality metrics show strong correlations - models that perform well on one often excel on the other - whereas novelty exhibits much weaker correlation with either. These findings support our hypothesis that strong performance in one creativity task or dimension does not necessarily generalize to others, underscoring the need for a holistic evaluation of LLM creativity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20091v1",
    "published_date": "2025-10-23 00:22:10 UTC",
    "updated_date": "2025-10-23 00:22:10 UTC"
  },
  {
    "arxiv_id": "2510.20084v2",
    "title": "ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models",
    "authors": [
      "Bosong Huang",
      "Ming Jin",
      "Yuxuan Liang",
      "Johan Barthelemy",
      "Debo Cheng",
      "Qingsong Wen",
      "Chenghao Liu",
      "Shirui Pan"
    ],
    "abstract": "Explaining time series classification models is crucial, particularly in high-stakes applications such as healthcare and finance, where transparency and trust play a critical role. Although numerous time series classification methods have identified key subsequences, known as shapelets, as core features for achieving state-of-the-art performance and validating their pivotal role in classification outcomes, existing post-hoc time series explanation (PHTSE) methods primarily focus on timestep-level feature attribution. These explanation methods overlook the fundamental prior that classification outcomes are predominantly driven by key shapelets. To bridge this gap, we present ShapeX, an innovative framework that segments time series into meaningful shapelet-driven segments and employs Shapley values to assess their saliency. At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework, which effectively learns a diverse set of shapelets essential for classification. We further demonstrate that ShapeX produces explanations which reveal causal relationships instead of just correlations, owing to the atomicity properties of shapelets. Experimental results on both synthetic and real-world datasets demonstrate that ShapeX outperforms existing methods in identifying the most relevant subsequences, enhancing both the precision and causal fidelity of time series explanations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20084v2",
    "published_date": "2025-10-23 00:01:40 UTC",
    "updated_date": "2025-10-25 03:23:39 UTC"
  }
]
[
  {
    "arxiv_id": "2510.00358v1",
    "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts",
    "authors": [
      "Linjin He",
      "Xinda Qi",
      "Dong Chen",
      "Zhaojian Li",
      "Xiaobo Tan"
    ],
    "abstract": "Soft snake robots offer remarkable flexibility and adaptability in complex environments, yet their control remains challenging due to highly nonlinear dynamics. Existing model-based and bio-inspired controllers rely on simplified assumptions that limit performance. Deep reinforcement learning (DRL) has recently emerged as a promising alternative, but online training is often impractical because of costly and potentially damaging real-world interactions. Offline RL provides a safer option by leveraging pre-collected datasets, but it suffers from distribution shift, which degrades generalization to unseen scenarios. To overcome this challenge, we propose DiSA-IQL (Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that incorporates robustness modulation by penalizing unreliable state-action pairs to mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks across two settings: in-distribution and out-of-distribution evaluation. Simulation results show that DiSA-IQL consistently outperforms baseline models, including Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla IQL, achieving higher success rates, smoother trajectories, and improved robustness. The codes are open-sourced to support reproducibility and to facilitate further research in offline RL for soft robot control.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00358v1",
    "published_date": "2025-09-30 23:53:47 UTC",
    "updated_date": "2025-09-30 23:53:47 UTC"
  },
  {
    "arxiv_id": "2510.03314v1",
    "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety",
    "authors": [
      "Shucheng Zhang",
      "Yan Shi",
      "Bingzhang Wang",
      "Yuang Zhang",
      "Muhammad Monjurul Karim",
      "Kehua Chen",
      "Chenxi Liu",
      "Mehrdad Nasri",
      "Yinhai Wang"
    ],
    "abstract": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and cyclists, remains a critical global challenge, as conventional infrastructure-based measures often prove inadequate in dynamic urban environments. Recent advances in artificial intelligence (AI), particularly in visual perception and reasoning, open new opportunities for proactive and context-aware VRU protection. However, existing surveys on AI applications for VRUs predominantly focus on detection, offering limited coverage of other vision-based tasks that are essential for comprehensive VRU understanding and protection. This paper presents a state-of-the-art review of recent progress in camera-based AI sensing systems for VRU safety, with an emphasis on developments from the past five years and emerging research trends. We systematically examine four core tasks, namely detection and classification, tracking and reidentification, trajectory prediction, and intent recognition and prediction, which together form the backbone of AI-empowered proactive solutions for VRU protection in intelligent transportation systems. To guide future research, we highlight four major open challenges from the perspectives of data, model, and deployment. By linking advances in visual AI with practical considerations for real-world implementation, this survey aims to provide a foundational reference for the development of next-generation sensing systems to enhance VRU safety.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 4 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.03314v1",
    "published_date": "2025-09-30 23:50:55 UTC",
    "updated_date": "2025-09-30 23:50:55 UTC"
  },
  {
    "arxiv_id": "2510.01286v1",
    "title": "Emergent evaluation hubs in a decentralizing large language model ecosystem",
    "authors": [
      "Manuel Cebrian",
      "Tomomi Kito",
      "Raul Castro Fernandez"
    ],
    "abstract": "Large language models are proliferating, and so are the benchmarks that serve as their common yardsticks. We ask how the agglomeration patterns of these two layers compare: do they evolve in tandem or diverge? Drawing on two curated proxies for the ecosystem, the Stanford Foundation-Model Ecosystem Graph and the Evidently AI benchmark registry, we find complementary but contrasting dynamics. Model creation has broadened across countries and organizations and diversified in modality, licensing, and access. Benchmark influence, by contrast, displays centralizing patterns: in the inferred benchmark-author-institution network, the top 15% of nodes account for over 80% of high-betweenness paths, three countries produce 83% of benchmark outputs, and the global Gini for inferred benchmark authority reaches 0.89. An agent-based simulation highlights three mechanisms: higher entry of new benchmarks reduces concentration; rapid inflows can temporarily complicate coordination in evaluation; and stronger penalties against over-fitting have limited effect. Taken together, these results suggest that concentrated benchmark influence functions as coordination infrastructure that supports standardization, comparability, and reproducibility amid rising heterogeneity in model production, while also introducing trade-offs such as path dependence, selective visibility, and diminishing discriminative power as leaderboards saturate.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "15 pages, 11 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.01286v1",
    "published_date": "2025-09-30 23:49:26 UTC",
    "updated_date": "2025-09-30 23:49:26 UTC"
  },
  {
    "arxiv_id": "2510.00355v2",
    "title": "Hierarchical Reasoning Models: Perspectives and Misconceptions",
    "authors": [
      "Renee Ge",
      "Qianli Liao",
      "Tomaso Poggio"
    ],
    "abstract": "Transformers have demonstrated remarkable performance in natural language processing and related domains, as they largely focus on sequential, autoregressive next-token prediction tasks. Yet, they struggle in logical reasoning, not necessarily because of a fundamental limitation of these models, but possibly due to the lack of exploration of more creative uses, such as latent space and recurrent reasoning. An emerging exploration in this direction is the Hierarchical Reasoning Model (Wang et. al., 2025), which introduces a novel type of recurrent reasoning in the latent space of transformers, achieving remarkable performance on a wide range of 2D reasoning tasks. Despite the promising results, this line of models is still at an early stage and calls for in-depth investigation. In this work, we review this class of models, examine key design choices, test alternative variants and clarify common misconceptions.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Found errors in some results of v1. Removed them and changed conclusions",
    "pdf_url": "https://arxiv.org/pdf/2510.00355v2",
    "published_date": "2025-09-30 23:40:04 UTC",
    "updated_date": "2025-10-07 17:57:06 UTC"
  },
  {
    "arxiv_id": "2510.02390v2",
    "title": "Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model",
    "authors": [
      "Zilai Li"
    ],
    "abstract": "The diffusion model is a state-of-the-art generative model that samples images by applying a neural network iteratively. However, the original sampling algorithm requires substantial computation cost, and reducing the sampling step is a prevailing research area. To cope with this problem, one mainstream approach is to treat the sampling process as an algorithm that solves an ordinary differential equation (ODE). Our study proposes a training-free inference plugin compatible with most few-step ODE solvers. To the best of my knowledge, our algorithm is the first training-free algorithm to sample a 1024 x 1024-resolution image in 6 steps and a 512 x 512-resolution image in 5 steps, with an FID result that outperforms the SOTA distillation models and the 20-step DPM++ 2m solver, respectively. Based on analyses of the latent diffusion model's structure, the diffusion ODE, and the Free-U mechanism, we explain why specific hyperparameter couplings improve stability and inference speed without retraining. Meanwhile, experimental results also reveal a new design space of the latent diffusion ODE solver. Additionally, we also analyze the difference between the original diffusion model and the diffusion distillation model via an information-theoretic study, which shows the reason why the few-step ODE solver designed for the diffusion model can outperform the training-based diffusion distillation algorithm in few-step inference. The tentative results of the experiment prove the mathematical analysis. code base is below: https://github.com/TheLovesOfLadyPurple/Hyperparameter-is-all-you-need",
    "categories": [
      "cs.GR",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.GR",
    "comment": "21 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.02390v2",
    "published_date": "2025-09-30 23:27:09 UTC",
    "updated_date": "2025-11-30 13:59:14 UTC"
  },
  {
    "arxiv_id": "2510.00347v1",
    "title": "In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks",
    "authors": [
      "Huitao Yang",
      "Guanting Chen"
    ],
    "abstract": "As large language models (LLMs) continue to grow in capability, there is increasing interest in incorporating them into decision-making tasks. A common pipeline for this is Decision-Pretrained Transformers (DPTs). However, existing training methods for DPTs often struggle to generalize beyond their pretraining data distribution. To explore mitigation of this limitation, we propose in-context curiosity -- a lightweight, exploration-inspired regularizer for offline pretraining -- and introduce the Prediction-Powered Transformer (PPT) framework. PPT augments DPT with an auxiliary reward predictor, using prediction error as an intrinsic curiosity signal to encourage broader exploration during training. In proof-of-concept experiments on Gaussian multi-armed bandits, PPT shows improved robustness: it moderates the performance degradation observed in DPT when test environments exhibit higher variance in reward, particularly when pretraining data has limited diversity. While the quality of offline data remain fundamental, our preliminary results suggest that curiosity-driven pretraining offers a promising direction for enhancing out-of-distribution generalization in in-context RL agents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00347v1",
    "published_date": "2025-09-30 23:17:18 UTC",
    "updated_date": "2025-09-30 23:17:18 UTC"
  },
  {
    "arxiv_id": "2510.00339v1",
    "title": "Navigating the Synchrony-Stability Frontier in Adaptive Chatbots",
    "authors": [
      "T. James Brandt"
    ],
    "abstract": "Adaptive chatbots that mimic a user's linguistic style can build rapport and engagement, yet unconstrained mimicry risks an agent that feels unstable or sycophantic. We present a computational evaluation framework that makes the core design tension explicit: balancing moment-to-moment linguistic synchrony against long-term persona stability. Using an 8-dimensional style vector and a closed-loop \"base+delta\" prompting architecture, we simulate and compare explicit adaptation policies - Uncapped, Cap, Exponential Moving Average (EMA), Dead-Band, and Hybrids - on a human-log dataset. Our analysis maps a clear Pareto frontier: bounded policies achieve substantial gains in stability at a modest cost to synchrony. For example, a Hybrid (EMA+Cap) raises stability from 0.542 to 0.878 (+62%) while reducing synchrony by only 17%. We confirm this trade-off through large-scale replications on three public corpora (DailyDialog, Persona-Chat, EmpatheticDialogues) and LLM-in-the-loop validation across two model families. Furthermore, we quantify \"prompt legibility,\" showing that frontier policies reduce instruction churn and cut jarring register flips (major tone changes) from 0.254 to 0.092, yielding systems that are easier to reason about and maintain. Taken together, our framework provides a general evaluation harness for style adaptation; a systematic ablation that identifies Pareto-efficient policies; robust validation across diverse datasets and models; and novel legibility metrics linking policy choices to system maintainability.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "pages; 9 tables; 7 figures; code & analysis artifact: https://doi.org/10.5281/zenodo.17238269; under review at ACM IUI 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.00339v1",
    "published_date": "2025-09-30 22:50:30 UTC",
    "updated_date": "2025-09-30 22:50:30 UTC"
  },
  {
    "arxiv_id": "2510.00334v1",
    "title": "Structural Refinement of Bayesian Networks for Efficient Model Parameterisation",
    "authors": [
      "Kieran Drury",
      "Martine J. Barons",
      "Jim Q. Smith"
    ],
    "abstract": "Many Bayesian network modelling applications suffer from the issue of data scarcity. Hence the use of expert judgement often becomes necessary to determine the parameters of the conditional probability tables (CPTs) throughout the network. There are usually a prohibitively large number of these parameters to determine, even when complementing any available data with expert judgements. To address this challenge, a number of CPT approximation methods have been developed that reduce the quantity and complexity of parameters needing to be determined to fully parameterise a Bayesian network. This paper provides a review of a variety of structural refinement methods that can be used in practice to efficiently approximate a CPT within a Bayesian network. We not only introduce and discuss the intrinsic properties and requirements of each method, but we evaluate each method through a worked example on a Bayesian network model of cardiovascular risk assessment. We conclude with practical guidance to help Bayesian network practitioners choose an alternative approach when direct parameterisation of a CPT is infeasible.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ME",
    "comment": "38 pages, 10 figures, 3 tables, one appendix",
    "pdf_url": "https://arxiv.org/pdf/2510.00334v1",
    "published_date": "2025-09-30 22:39:48 UTC",
    "updated_date": "2025-09-30 22:39:48 UTC"
  },
  {
    "arxiv_id": "2510.00332v2",
    "title": "When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets",
    "authors": [
      "Zeshi Dai",
      "Zimo Peng",
      "Zerui Cheng",
      "Ryan Yihe Li"
    ],
    "abstract": "We present CAIA, a benchmark exposing a critical blind spot in AI evaluation: the inability of state-of-the-art models to operate in adversarial, high-stakes environments where misinformation is weaponized and errors are irreversible. While existing benchmarks measure task completion in controlled settings, real-world deployment demands resilience against active deception. Using crypto markets as a testbed where $30 billion was lost to exploits in 2024, we evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish truth from manipulation, navigate fragmented information landscapes, and make irreversible financial decisions under adversarial pressure.\n  Our results reveal a fundamental capability gap: without tools, even frontier models achieve only 28% accuracy on tasks junior analysts routinely handle. Tool augmentation improves performance but plateaus at 67.4% versus 80% human baseline, despite unlimited access to professional resources. Most critically, we uncover a systematic tool selection catastrophe: models preferentially choose unreliable web search over authoritative data, falling for SEO-optimized misinformation and social media manipulation. This behavior persists even when correct answers are directly accessible through specialized tools, suggesting foundational limitations rather than knowledge gaps. We also find that Pass@k metrics mask dangerous trial-and-error behavior for autonomous deployment.\n  The implications extend beyond crypto to any domain with active adversaries, e.g. cybersecurity, content moderation, etc. We release CAIA with contamination controls and continuous updates, establishing adversarial robustness as a necessary condition for trustworthy AI autonomy. The benchmark reveals that current models, despite impressive reasoning scores, remain fundamentally unprepared for environments where intelligence must survive active opposition.",
    "categories": [
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 5 figures, 4 tables; Accepted to AAAI 2026 (AI-4-Finance Workshop - Oral, top 10%); In submission to ICML 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.00332v2",
    "published_date": "2025-09-30 22:39:06 UTC",
    "updated_date": "2026-01-17 06:27:25 UTC"
  },
  {
    "arxiv_id": "2510.01285v1",
    "title": "LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science",
    "authors": [
      "Alireza Salemi",
      "Mihir Parmar",
      "Palash Goyal",
      "Yiwen Song",
      "Jinsung Yoon",
      "Hamed Zamani",
      "Hamid Palangi",
      "Tomas Pfister"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has opened new opportunities in data science, yet their practical deployment is often constrained by the challenge of discovering relevant data within large heterogeneous data lakes. Existing methods struggle with this: single-agent systems are quickly overwhelmed by large, heterogeneous files in the large data lakes, while multi-agent systems designed based on a master-slave paradigm depend on a rigid central controller for task allocation that requires precise knowledge of each sub-agent's capabilities. To address these limitations, we propose a novel multi-agent communication paradigm inspired by the blackboard architecture for traditional AI models. In this framework, a central agent posts requests to a shared blackboard, and autonomous subordinate agents -- either responsible for a partition of the data lake or general information retrieval -- volunteer to respond based on their capabilities. This design improves scalability and flexibility by eliminating the need for a central coordinator to have prior knowledge of all sub-agents' expertise. We evaluate our method on three benchmarks that require explicit data discovery: KramaBench and modified versions of DS-Bench and DA-Code to incorporate data discovery. Experimental results demonstrate that the blackboard architecture substantially outperforms baselines, including RAG and the master-slave multi-agent paradigm, achieving between 13% to 57% relative improvement in end-to-end task success and up to a 9% relative gain in F1 score for data discovery over the best-performing baselines across both proprietary and open-source LLMs. Our findings establish the blackboard paradigm as a scalable and generalizable communication framework for multi-agent systems.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01285v1",
    "published_date": "2025-09-30 22:34:23 UTC",
    "updated_date": "2025-09-30 22:34:23 UTC"
  },
  {
    "arxiv_id": "2510.21736v1",
    "title": "Learn2Drive: A neural network-based framework for socially compliant automated vehicle control",
    "authors": [
      "Yuhui Liu",
      "Samannita Halder",
      "Shian Wang",
      "Tianyi Li"
    ],
    "abstract": "This study introduces a novel control framework for adaptive cruise control (ACC) in automated driving, leveraging Long Short-Term Memory (LSTM) networks and physics-informed constraints. As automated vehicles (AVs) adopt advanced features like ACC, transportation systems are becoming increasingly intelligent and efficient. However, existing AV control strategies primarily focus on optimizing the performance of individual vehicles or platoons, often neglecting their interactions with human-driven vehicles (HVs) and the broader impact on traffic flow. This oversight can exacerbate congestion and reduce overall system efficiency. To address this critical research gap, we propose a neural network-based, socially compliant AV control framework that incorporates social value orientation (SVO). This framework enables AVs to account for their influence on HVs and traffic dynamics. By leveraging AVs as mobile traffic regulators, the proposed approach promotes adaptive driving behaviors that reduce congestion, improve traffic efficiency, and lower energy consumption. Within this framework, we define utility functions for both AVs and HVs, which are optimized based on the SVO of each AV to balance its own control objectives with broader traffic flow considerations. Numerical results demonstrate the effectiveness of the proposed method in adapting to varying traffic conditions, thereby enhancing system-wide efficiency. Specifically, when the AV's control mode shifts from prioritizing energy consumption to optimizing traffic flow efficiency, vehicles in the following platoon experience at least a 58.99% increase in individual energy consumption alongside at least a 38.39% improvement in individual average speed, indicating significant enhancements in traffic dynamics.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21736v1",
    "published_date": "2025-09-30 22:33:44 UTC",
    "updated_date": "2025-09-30 22:33:44 UTC"
  },
  {
    "arxiv_id": "2510.00326v1",
    "title": "Reasoning-Aware Prompt Orchestration: A Foundation Model for Multi-Agent Language Model Coordination",
    "authors": [
      "Hassen Dhrif"
    ],
    "abstract": "The emergence of large language models has enabled sophisticated multi-agent systems, yet coordinating their reasoning capabilities through prompt engineering remains challenging. We present a theoretically-grounded framework for dynamic prompt orchestration that enhances reasoning across multiple specialized agents. This framework addresses three core challenges: logical consistency preservation during agent transitions, reasoning-aware prompt adaptation, and scalable coordination of distributed inference.\n  Our approach formalizes agent states using prompt templates, reasoning context vectors, and capability matrices. We prove system convergence to stable coordination patterns when step sizes satisfy $α< \\frac{1}{2L}$ where $L$ is the Lipschitz constant of the state transition function. We implement this through a distributed architecture that dynamically routes reasoning tasks while maintaining semantic coherence.\n  Experimental results on 1,000 synthetic multi-agent conversations demonstrate a 42% reduction in reasoning latency, a 23% improvement in logical consistency measured by ROUGE-L score, and an 89% success rate for task completion without context loss across agent transitions. Ablation studies identify the consensus mechanism as the primary performance driver, while revealing limitations: performance degrades beyond 10 agent transitions, and the system requires 76.5GB memory for 1,000 concurrent agents. These findings establish a new paradigm for scalable reasoning in multi-agent systems, providing theoretical foundations for understanding reasoning emergence across coordinated language models.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00326v1",
    "published_date": "2025-09-30 22:33:01 UTC",
    "updated_date": "2025-09-30 22:33:01 UTC"
  },
  {
    "arxiv_id": "2510.00321v1",
    "title": "A Framework for Selection of Machine Learning Algorithms Based on Performance Metrices and Akaike Information Criteria in Healthcare, Telecommunication, and Marketing Sector",
    "authors": [
      "A. K. Hamisu",
      "K. Jasleen"
    ],
    "abstract": "The exponential growth of internet generated data has fueled advancements in artificial intelligence (AI), machine learning (ML), and deep learning (DL) for extracting actionable insights in marketing,telecom, and health sectors. This chapter explores ML applications across three domains namely healthcare, marketing, and telecommunications, with a primary focus on developing a framework for optimal ML algorithm selection. In healthcare, the framework addresses critical challenges such as cardiovascular disease prediction accounting for 28.1% of global deaths and fetal health classification into healthy or unhealthy states, utilizing three datasets. ML algorithms are categorized into eager, lazy, and hybrid learners, selected based on dataset attributes, performance metrics (accuracy, precision, recall), and Akaike Information Criterion (AIC) scores. For validation, eight datasets from the three sectors are employed in the experiments. The key contribution is a recommendation framework that identifies the best ML model according to input attributes, balancing performance evaluation and model complexity to enhance efficiency and accuracy in diverse real-world applications. This approach bridges gaps in automated model selection, offering practical implications for interdisciplinary ML deployment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00321v1",
    "published_date": "2025-09-30 22:27:34 UTC",
    "updated_date": "2025-09-30 22:27:34 UTC"
  },
  {
    "arxiv_id": "2510.21735v1",
    "title": "A phase-aware AI car-following model for electric vehicles with adaptive cruise control: Development and validation using real-world data",
    "authors": [
      "Yuhui Liu",
      "Shian Wang",
      "Ansel Panicker",
      "Kate Embry",
      "Ayana Asanova",
      "Tianyi Li"
    ],
    "abstract": "Internal combustion engine (ICE) vehicles and electric vehicles (EVs) exhibit distinct vehicle dynamics. EVs provide rapid acceleration, with electric motors producing peak power across a wider speed range, and achieve swift deceleration through regenerative braking. While existing microscopic models effectively capture the driving behavior of ICE vehicles, a modeling framework that accurately describes the unique car-following dynamics of EVs is lacking. Developing such a model is essential given the increasing presence of EVs in traffic, yet creating an easy-to-use and accurate analytical model remains challenging.\n  To address these gaps, this study develops and validates a Phase-Aware AI (PAAI) car-following model specifically for EVs. The proposed model enhances traditional physics-based frameworks with an AI component that recognizes and adapts to different driving phases, such as rapid acceleration and regenerative braking. Using real-world trajectory data from vehicles equipped with adaptive cruise control (ACC), we conduct comprehensive simulations to validate the model's performance. The numerical results demonstrate that the PAAI model significantly improves prediction accuracy over traditional car-following models, providing an effective tool for accurately representing EV behavior in traffic simulations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21735v1",
    "published_date": "2025-09-30 22:27:03 UTC",
    "updated_date": "2025-09-30 22:27:03 UTC"
  },
  {
    "arxiv_id": "2510.00319v1",
    "title": "DecepChain: Inducing Deceptive Reasoning in Large Language Models",
    "authors": [
      "Wei Shen",
      "Han Wang",
      "Haoyu Li",
      "Huan Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have been demonstrating increasingly strong reasoning capability with their chain-of-thoughts (CoT), which are routinely used by humans to judge answer quality. This reliance creates a powerful yet fragile basis for trust. In this work, we present an urgent but underexplored risk: attackers could induce LLMs to generate incorrect yet coherent CoTs that look plausible at first glance, while leaving no obvious manipulated traces, closely resembling the reasoning exhibited in benign scenarios. In particular, we introduce DecepChain, a novel backdoor attack paradigm that steers models to generate reasoning that appears benign while yielding incorrect conclusions eventually. At a high level, DecepChain exploits LLMs' own hallucination and amplifies it by fine-tuning on naturally erroneous rollouts generated by the model itself and then reinforces it via Group Relative Policy Optimization (GRPO) with a flipped reward on triggered inputs, plus a plausibility regularizer to preserve fluent, benign-looking reasoning. Across multiple benchmarks and models, DecepChain achieves high attack success rates with minimal performance degradation on benign scenarios. Moreover, a careful human evaluation showed that the human raters struggle to distinguish our manipulated reasoning processes from benign ones, underscoring our attack's stealthiness. Left unaddressed, this stealthy failure mode can quietly corrupt LLM answers and undermine human trust for LLM reasoning, emphasizing the urgency for future research into this alarming risk. Project page: https://decepchain.github.io/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00319v1",
    "published_date": "2025-09-30 22:23:40 UTC",
    "updated_date": "2025-09-30 22:23:40 UTC"
  },
  {
    "arxiv_id": "2510.00317v1",
    "title": "MAVUL: Multi-Agent Vulnerability Detection via Contextual Reasoning and Interactive Refinement",
    "authors": [
      "Youpeng Li",
      "Kartik Joshi",
      "Xinda Wang",
      "Eric Wong"
    ],
    "abstract": "The widespread adoption of open-source software (OSS) necessitates the mitigation of vulnerability risks. Most vulnerability detection (VD) methods are limited by inadequate contextual understanding, restrictive single-round interactions, and coarse-grained evaluations, resulting in undesired model performance and biased evaluation results. To address these challenges, we propose MAVUL, a novel multi-agent VD system that integrates contextual reasoning and interactive refinement. Specifically, a vulnerability analyst agent is designed to flexibly leverage tool-using capabilities and contextual reasoning to achieve cross-procedural code understanding and effectively mine vulnerability patterns. Through iterative feedback and refined decision-making within cross-role agent interactions, the system achieves reliable reasoning and vulnerability prediction. Furthermore, MAVUL introduces multi-dimensional ground truth information for fine-grained evaluation, thereby enhancing evaluation accuracy and reliability.\n  Extensive experiments conducted on a pairwise vulnerability dataset demonstrate MAVUL's superior performance. Our findings indicate that MAVUL significantly outperforms existing multi-agent systems with over 62% higher pairwise accuracy and single-agent systems with over 600% higher average performance. The system's effectiveness is markedly improved with increased communication rounds between the vulnerability analyst agent and the security architect agent, underscoring the importance of contextual reasoning in tracing vulnerability flows and the crucial feedback role. Additionally, the integrated evaluation agent serves as a critical, unbiased judge, ensuring a more accurate and reliable estimation of the system's real-world applicability by preventing misleading binary comparisons.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by The 7th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (IEEE TPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.00317v1",
    "published_date": "2025-09-30 22:21:43 UTC",
    "updated_date": "2025-09-30 22:21:43 UTC"
  },
  {
    "arxiv_id": "2510.00312v1",
    "title": "Digital Domination: A Case for Republican Liberty in Artificial Intelligence",
    "authors": [
      "Matthew David Hamilton"
    ],
    "abstract": "Artificial intelligence is set to revolutionize social and political life in unpredictable ways, raising questions about the principles that ought to guide its development and regulation. By examining digital advertising and social media algorithms, this article highlights how artificial intelligence already poses a significant threat to the republican conception of liberty -- or freedom from unaccountable power -- and thereby highlights the necessity of protecting republican liberty when integrating artificial intelligence into society. At an individual level, these algorithms can subconsciously influence behavior and thought, and those subject to this influence have limited power over the algorithms they engage. At the political level, these algorithms give technology company executives and other foreign parties the power to influence domestic political processes, such as elections; the multinational nature of algorithm-based platforms and the speed with which technology companies innovate make incumbent state institutions ineffective at holding these actors accountable. At both levels, artificial intelligence has thus created a new form of unfreedom: digital domination. By drawing on the works of Quentin Skinner, Philip Pettit, and other republican theorists, this article asserts that individuals must have mechanisms to hold algorithms (and those who develop them) accountable in order to be truly free.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00312v1",
    "published_date": "2025-09-30 22:09:34 UTC",
    "updated_date": "2025-09-30 22:09:34 UTC"
  },
  {
    "arxiv_id": "2510.00307v1",
    "title": "BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models",
    "authors": [
      "Thierry Blankenstein",
      "Jialin Yu",
      "Zixuan Li",
      "Vassilis Plachouras",
      "Sunando Sengupta",
      "Philip Torr",
      "Yarin Gal",
      "Alasdair Paren",
      "Adel Bibi"
    ],
    "abstract": "Agents backed by large language models (LLMs) often rely on external tools drawn from marketplaces where multiple providers offer functionally equivalent options. This raises a critical point concerning fairness: if selection is systematically biased, it can degrade user experience and distort competition by privileging some providers over others. We introduce a benchmark of diverse tool categories, each containing multiple functionally equivalent tools, to evaluate tool-selection bias. Using this benchmark, we test seven models and show that unfairness exists with models either fixating on a single provider or disproportionately preferring earlier-listed tools in context. To investigate the origins of this bias, we conduct controlled experiments examining tool features, metadata (name, description, parameters), and pre-training exposure. We find that: (1) semantic alignment between queries and metadata is the strongest predictor of choice; (2) perturbing descriptions significantly shifts selections; and (3) repeated pre-training exposure to a single endpoint amplifies bias. Finally, we propose a lightweight mitigation that first filters the candidate tools to a relevant subset and then samples uniformly, reducing bias while preserving good task coverage. Our findings highlight tool-selection bias as a key obstacle for the fair deployment of tool-augmented LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00307v1",
    "published_date": "2025-09-30 22:02:13 UTC",
    "updated_date": "2025-09-30 22:02:13 UTC"
  },
  {
    "arxiv_id": "2510.00304v1",
    "title": "Barriers for Learning in an Evolving World: Mathematical Understanding of Loss of Plasticity",
    "authors": [
      "Amir Joudaki",
      "Giulia Lanzillotta",
      "Mohammad Samragh Razlighi",
      "Iman Mirzadeh",
      "Keivan Alizadeh",
      "Thomas Hofmann",
      "Mehrdad Farajtabar",
      "Fartash Faghri"
    ],
    "abstract": "Deep learning models excel in stationary data but struggle in non-stationary environments due to a phenomenon known as loss of plasticity (LoP), the degradation of their ability to learn in the future. This work presents a first-principles investigation of LoP in gradient-based learning. Grounded in dynamical systems theory, we formally define LoP by identifying stable manifolds in the parameter space that trap gradient trajectories. Our analysis reveals two primary mechanisms that create these traps: frozen units from activation saturation and cloned-unit manifolds from representational redundancy. Our framework uncovers a fundamental tension: properties that promote generalization in static settings, such as low-rank representations and simplicity biases, directly contribute to LoP in continual learning scenarios. We validate our theoretical analysis with numerical simulations and explore architectural choices or targeted perturbations as potential mitigation strategies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00304v1",
    "published_date": "2025-09-30 21:49:50 UTC",
    "updated_date": "2025-09-30 21:49:50 UTC"
  },
  {
    "arxiv_id": "2510.02387v1",
    "title": "CWM: An Open-Weights LLM for Research on Code Generation with World Models",
    "authors": [
      "FAIR CodeGen team",
      "Jade Copet",
      "Quentin Carbonneaux",
      "Gal Cohen",
      "Jonas Gehring",
      "Jacob Kahn",
      "Jannik Kossen",
      "Felix Kreuk",
      "Emily McMilin",
      "Michel Meyer",
      "Yuxiang Wei",
      "David Zhang",
      "Kunhao Zheng",
      "Jordi Armengol-Estapé",
      "Pedram Bashiri",
      "Maximilian Beck",
      "Pierre Chambon",
      "Abhishek Charnalia",
      "Chris Cummins",
      "Juliette Decugis",
      "Zacharias V. Fisches",
      "François Fleuret",
      "Fabian Gloeckle",
      "Alex Gu",
      "Michael Hassid",
      "Daniel Haziza",
      "Badr Youbi Idrissi",
      "Christian Keller",
      "Rahul Kindi",
      "Hugh Leather",
      "Gallil Maimon",
      "Aram Markosyan",
      "Francisco Massa",
      "Pierre-Emmanuel Mazaré",
      "Vegard Mella",
      "Naila Murray",
      "Keyur Muzumdar",
      "Peter O'Hearn",
      "Matteo Pagliardini",
      "Dmitrii Pedchenko",
      "Tal Remez",
      "Volker Seeker",
      "Marco Selvi",
      "Oren Sultan",
      "Sida Wang",
      "Luca Wehrstedt",
      "Ori Yoran",
      "Lingming Zhang",
      "Taco Cohen",
      "Yossi Adi",
      "Gabriel Synnaeve"
    ],
    "abstract": "We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi-task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "58 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.02387v1",
    "published_date": "2025-09-30 21:47:10 UTC",
    "updated_date": "2025-09-30 21:47:10 UTC"
  },
  {
    "arxiv_id": "2510.00300v1",
    "title": "ICL Optimized Fragility",
    "authors": [
      "Serena Gomez Wannaz"
    ],
    "abstract": "ICL guides are known to improve task-specific performance, but their impact on cross-domain cognitive abilities remains unexplored. This study examines how ICL guides affect reasoning across different knowledge domains using six variants of the GPT-OSS:20b model: one baseline model and five ICL configurations (simple, chain-of-thought, random, appended text, and symbolic language). The models were subjected to 840 tests spanning general knowledge questions, logic riddles, and a mathematical olympiad problem. Statistical analysis (ANOVA) revealed significant behavioral modifications (p less than 0.001) across ICL variants, demonstrating a phenomenon termed \"optimized fragility.\" ICL models achieved 91%-99% accuracy on general knowledge tasks while showing degraded performance on complex reasoning problems, with accuracy dropping to 10-43% on riddles compared to 43% for the baseline model. Notably, no significant differences emerged on the olympiad problem (p=0.2173), suggesting that complex mathematical reasoning remains unaffected by ICL optimization. These findings indicate that ICL guides create systematic trade-offs between efficiency and reasoning flexibility, with important implications for LLM deployment and AI safety.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00300v1",
    "published_date": "2025-09-30 21:43:21 UTC",
    "updated_date": "2025-09-30 21:43:21 UTC"
  },
  {
    "arxiv_id": "2510.02386v1",
    "title": "On The Fragility of Benchmark Contamination Detection in Reasoning Models",
    "authors": [
      "Han Wang",
      "Haoyu Li",
      "Brian Ko",
      "Huan Zhang"
    ],
    "abstract": "Leaderboards for LRMs have turned evaluation into a competition, incentivizing developers to optimize directly on benchmark suites. A shortcut to achieving higher rankings is to incorporate evaluation benchmarks into the training data, thereby yielding inflated performance, known as benchmark contamination. Surprisingly, our studies find that evading contamination detections for LRMs is alarmingly easy. We focus on the two scenarios where contamination may occur in practice: (I) when the base model evolves into LRM via SFT and RL, we find that contamination during SFT can be originally identified by contamination detection methods. Yet, even a brief GRPO training can markedly conceal contamination signals that most detection methods rely on. Further empirical experiments and theoretical analysis indicate that PPO style importance sampling and clipping objectives are the root cause of this detection concealment, indicating that a broad class of RL methods may inherently exhibit similar concealment capability; (II) when SFT contamination with CoT is applied to advanced LRMs as the final stage, most contamination detection methods perform near random guesses. Without exposure to non-members, contaminated LRMs would still have more confidence when responding to those unseen samples that share similar distributions to the training set, and thus, evade existing memorization-based detection methods. Together, our findings reveal the unique vulnerability of LRMs evaluations: Model developers could easily contaminate LRMs to achieve inflated leaderboards performance while leaving minimal traces of contamination, thereby strongly undermining the fairness of evaluation and threatening the integrity of public leaderboards. This underscores the urgent need for advanced contamination detection methods and trustworthy evaluation protocols tailored to LRMs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.02386v1",
    "published_date": "2025-09-30 21:40:54 UTC",
    "updated_date": "2025-09-30 21:40:54 UTC"
  },
  {
    "arxiv_id": "2511.05501v1",
    "title": "Towards Ecologically Valid LLM Benchmarks: Understanding and Designing Domain-Centered Evaluations for Journalism Practitioners",
    "authors": [
      "Charlotte Li",
      "Nick Hagar",
      "Sachita Nishal",
      "Jeremy Gilbert",
      "Nick Diakopoulos"
    ],
    "abstract": "Benchmarks play a significant role in how researchers and the public understand generative AI systems. However, the widespread use of benchmark scores to communicate about model capabilities has led to criticisms of validity, especially whether benchmarks test what they claim to test (i.e. construct validity) and whether benchmark evaluations are representative of how models are used in the wild (i.e. ecological validity). In this work we explore how to create an LLM benchmark that addresses these issues by taking a human-centered approach. We focus on designing a domain-oriented benchmark for journalism practitioners, drawing on insights from a workshop of 23 journalism professionals. Our workshop findings surface specific challenges that inform benchmark design opportunities, which we instantiate in a case study that addresses underlying criticisms and specific domain concerns. Through our findings and design case study, this work provides design guidance for developing benchmarks that are better tuned to specific domains.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "14 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.05501v1",
    "published_date": "2025-09-30 21:36:23 UTC",
    "updated_date": "2025-09-30 21:36:23 UTC"
  },
  {
    "arxiv_id": "2510.00294v2",
    "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models",
    "authors": [
      "Shutong Wu",
      "Jiawei Zhang"
    ],
    "abstract": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges like the famous \"reversal curse\" or learning under data-constrained scenarios. In addition, taking advantage of their inherent modeling foundations, DLLMs have the great potential of efficient inference with parallel decoding algorithms, which enable multi-token prediction per step. However, the high generation quality often requires the number of decoding steps equal to the sequence length, which performs a one-token-per-step decoding, and existing parallel decoding algorithms, which yield suboptimal decoding paths, bring inference speedup at the cost of non-negligible performance degradation. To overcome this challenge, we introduce Free Draft-and-Verification (FreeDave), a novel fast decoding algorithm tailored for DLLMs that achieves lossless parallel decoding without any model modification or extra modules. Specifically, we propose an algorithm of parallel-decoded candidate generation and verification, which is theoretically guaranteed to use the fewest model forward calls to reproduce the same sequence generated by static decoding when enough computation and memory budget is provided. By extensive evaluations on math reasoning and code generation benchmarks across different DLLMs, FreeDave is proven to boost the inference throughput up to $3.78\\times$ without performance degradation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00294v2",
    "published_date": "2025-09-30 21:28:04 UTC",
    "updated_date": "2025-11-01 23:45:41 UTC"
  },
  {
    "arxiv_id": "2510.00288v1",
    "title": "o-MEGA: Optimized Methods for Explanation Generation and Analysis",
    "authors": [
      "Ľuboš Kriš",
      "Jaroslav Kopčan",
      "Qiwei Peng",
      "Andrej Ridzik",
      "Marcel Veselý",
      "Martin Tamajka"
    ],
    "abstract": "The proliferation of transformer-based language models has revolutionized NLP domain while simultaneously introduced significant challenges regarding model transparency and trustworthiness. The complexity of achieving explainable systems in this domain is evidenced by the extensive array of explanation methods and evaluation metrics developed by researchers. To address the challenge of selecting optimal explainability approaches, we present \\textbf{\\texttt{o-mega}}, a hyperparameter optimization tool designed to automatically identify the most effective explainable AI methods and their configurations within the semantic matching domain. We evaluate o-mega on a post-claim matching pipeline using a curated dataset of social media posts paired with refuting claims. Our tool systematically explores different explainable methods and their hyperparameters, demonstrating improved transparency in automated fact-checking systems. As a result, such automated optimization of explanation methods can significantly enhance the interpretability of claim-matching models in critical applications such as misinformation detection, contributing to more trustworthy and transparent AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00288v1",
    "published_date": "2025-09-30 21:08:36 UTC",
    "updated_date": "2025-09-30 21:08:36 UTC"
  },
  {
    "arxiv_id": "2510.00283v1",
    "title": "Data driven approaches in nanophotonics: A review of AI-enabled metadevices",
    "authors": [
      "Huanshu Zhang",
      "Lei Kang",
      "Sawyer D. Campbell",
      "Jacob T. Young",
      "Douglas H. Werner"
    ],
    "abstract": "Data-driven approaches have revolutionized the design and optimization of photonic metadevices by harnessing advanced artificial intelligence methodologies. This review takes a model-centric perspective that synthesizes emerging design strategies and delineates how traditional trial-and-error and computationally intensive electromagnetic simulations are being supplanted by deep learning frameworks that efficiently navigate expansive design spaces. We discuss artificial intelligence implementation in several metamaterial design aspects from high-degree-of-freedom design to large language model-assisted design. By addressing challenges such as transformer model implementation, fabrication limitations, and intricate mutual coupling effects, these AI-enabled strategies not only streamline the forward modeling process but also offer robust pathways for the realization of multifunctional and fabrication-friendly nanophotonic devices. This review further highlights emerging opportunities and persistent challenges, setting the stage for next-generation strategies in nanophotonic engineering.",
    "categories": [
      "physics.optics",
      "cs.AI"
    ],
    "primary_category": "physics.optics",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00283v1",
    "published_date": "2025-09-30 21:03:46 UTC",
    "updated_date": "2025-09-30 21:03:46 UTC"
  },
  {
    "arxiv_id": "2510.00279v2",
    "title": "SLogic: Subgraph-Informed Logical Rule Learning for Knowledge Graph Completion",
    "authors": [
      "Trung Hoang Le",
      "Tran Cao Son",
      "Huiping Cao"
    ],
    "abstract": "Logical rule-based methods offer an interpretable approach to knowledge graph completion (KGC) by capturing compositional relationships in the form of human-readable inference rules. While existing logical rule-based methods learn rule confidence scores, they typically assign a global weight to each rule schema, applied uniformly across the graph. This is a significant limitation, as a rule's importance often varies depending on the specific query instance. To address this, we introduce SLogic (Subgraph-Informed Logical Rule learning), a novel framework that assigns query-dependent scores to logical rules. The core of SLogic is a context-aware scoring function. This function determines the importance of a rule by analyzing the subgraph locally defined by the query's head entity, thereby enabling a differentiated weighting of rules specific to their local query contexts. Extensive experiments on benchmark datasets show that SLogic outperforms existing rule-based methods and achieves competitive performance against state-of-the-art baselines. It also generates query-dependent, human-readable logical rules that serve as explicit explanations for its inferences.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00279v2",
    "published_date": "2025-09-30 20:59:22 UTC",
    "updated_date": "2026-01-12 22:57:07 UTC"
  },
  {
    "arxiv_id": "2510.00274v1",
    "title": "MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning",
    "authors": [
      "Maisha Maliha",
      "Dean Hougen"
    ],
    "abstract": "Understanding the decision-making process of Deep Reinforcement Learning agents remains a key challenge for deploying these systems in safety-critical and multi-agent environments. While prior explainability methods like StateMask, have advanced the identification of critical states, they remain limited by computational cost, exploration coverage, and lack of adaptation to multi-agent settings. To overcome these limitations, we propose a mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent Collaboration with Mask-Based Explainability for Reinforcement Learning), that extends perturbation-based explanation to Multi-Agent Reinforcement Learning. Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy exploration, and lightweight inter-agent collaboration to share masked state information and peer experience. This collaboration enables each agent to perform saliency-guided masking and share reward-based insights with peers, reducing the time required for critical state discovery, improving explanation fidelity, and leading to faster and more robust learning. The core novelty of our approach lies in generalizing explainability from single-agent to multi-agent systems through a unified mathematical formalism built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. This framework yields localized, interpretable explanations grounded in probabilistic modeling and multi-agent Markov decision processes. We validate our framework on both single-agent and multi-agent benchmarks, including a multi-agent highway driving environment and Google Research Football, demonstrating that MAGIC-MASK consistently outperforms state-of-the-art baselines in fidelity, learning efficiency, and policy robustness while offering interpretable and transferable explanations.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.00274v1",
    "published_date": "2025-09-30 20:53:28 UTC",
    "updated_date": "2025-09-30 20:53:28 UTC"
  },
  {
    "arxiv_id": "2510.00268v1",
    "title": "Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction",
    "authors": [
      "Zhexiong Liu",
      "Diane Litman"
    ],
    "abstract": "Large Language Models (LLMs) have shown extraordinary success across various text generation tasks; however, their potential for simple yet essential text classification remains underexplored, as LLM pre-training tends to emphasize generation over classification. While LLMs with instruction tuning can transform classification into a generation task, they often struggle to categorize nuanced texts. One such example is text revision, which involves nuanced edits between pairs of texts. Although simply fine-tuning LLMs for revision classification seems plausible, it requires a large amount of revision annotations, which are exceptionally expensive and scarce in the community. To address this issue, we introduce a plug-and-play layer-wise parameter-efficient fine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of important LLM layers that are dynamically selected based on their gradient norm distribution, while freezing those of redundant layers. Extensive experiments suggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse text revisions, while achieving fast convergence, low GPU memory consumption, and effectiveness on small revision corpora.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In The Conference on Empirical Methods in Natural Language Processing (EMNLP), November 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.00268v1",
    "published_date": "2025-09-30 20:42:13 UTC",
    "updated_date": "2025-09-30 20:42:13 UTC"
  },
  {
    "arxiv_id": "2510.00261v1",
    "title": "Retrieval-Augmented Generation for Electrocardiogram-Language Models",
    "authors": [
      "Xiaoyu Song",
      "William Han",
      "Tony Chen",
      "Chaojing Duan",
      "Michael A. Rosenberg",
      "Emerson Liu",
      "Ding Zhao"
    ],
    "abstract": "Interest in generative Electrocardiogram-Language Models (ELMs) is growing, as they can produce textual responses conditioned on ECG signals and textual queries. Unlike traditional classifiers that output label probabilities, ELMs are more versatile, supporting domain-specific tasks (e.g., waveform analysis, diagnosis, prognosis) as well as general tasks (e.g., open-ended questions, dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce hallucinations and improve natural language generation (NLG). However, despite its promise, no open-source implementation or systematic study of RAG pipeline design for ELMs currently exists. To address this gap, we present the first open-source RAG pipeline for ELMs, along with baselines and ablation studies for NLG. Experiments on three public datasets show that ELMs with RAG consistently improves performance over non-RAG baselines and highlights key ELM design considerations. Our code is available at: https://github.com/willxxy/ECG-Bench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 2 figures; Submitted to ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.00261v1",
    "published_date": "2025-09-30 20:32:34 UTC",
    "updated_date": "2025-09-30 20:32:34 UTC"
  },
  {
    "arxiv_id": "2510.00260v1",
    "title": "Learning Energy-based Variational Latent Prior for VAEs",
    "authors": [
      "Debottam Dutta",
      "Chaitanya Amballa",
      "Zhongweiyang Xu",
      "Yu-Lin Wei",
      "Romit Roy Choudhury"
    ],
    "abstract": "Variational Auto-Encoders (VAEs) are known to generate blurry and inconsistent samples. One reason for this is the \"prior hole\" problem. A prior hole refers to regions that have high probability under the VAE's prior but low probability under the VAE's posterior. This means that during data generation, high probability samples from the prior could have low probability under the posterior, resulting in poor quality data. Ideally, a prior needs to be flexible enough to match the posterior while retaining the ability to generate samples fast. Generative models continue to address this tradeoff. This paper proposes to model the prior as an energy-based model (EBM). While EBMs are known to offer the flexibility to match posteriors (and also improving the ELBO), they are traditionally slow in sample generation due to their dependency on MCMC methods. Our key idea is to bring a variational approach to tackle the normalization constant in EBMs, thus bypassing the expensive MCMC approaches. The variational form can be approximated with a sampler network, and we show that such an approach to training priors can be formulated as an alternating optimization problem. Moreover, the same sampler reduces to an implicit variational prior during generation, providing efficient and fast sampling. We compare our Energy-based Variational Latent Prior (EVaLP) method to multiple SOTA baselines and show improvements in image generation quality, reduced prior holes, and better sampling efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00260v1",
    "published_date": "2025-09-30 20:32:00 UTC",
    "updated_date": "2025-09-30 20:32:00 UTC"
  },
  {
    "arxiv_id": "2510.00259v1",
    "title": "A Hierarchical Agentic Framework for Autonomous Drone-Based Visual Inspection",
    "authors": [
      "Ethan Herron",
      "Xian Yeow Lee",
      "Gregory Sin",
      "Teresa Gonzalez Diaz",
      "Ahmed Farahat",
      "Chetan Gupta"
    ],
    "abstract": "Autonomous inspection systems are essential for ensuring the performance and longevity of industrial assets. Recently, agentic frameworks have demonstrated significant potential for automating inspection workflows but have been limited to digital tasks. Their application to physical assets in real-world environments, however, remains underexplored. In this work, our contributions are two-fold: first, we propose a hierarchical agentic framework for autonomous drone control, and second, a reasoning methodology for individual function executions which we refer to as ReActEval. Our framework focuses on visual inspection tasks in indoor industrial settings, such as interpreting industrial readouts or inspecting equipment. It employs a multi-agent system comprising a head agent and multiple worker agents, each controlling a single drone. The head agent performs high-level planning and evaluates outcomes, while worker agents implement ReActEval to reason over and execute low-level actions. Operating entirely in natural language, ReActEval follows a plan, reason, act, evaluate cycle, enabling drones to handle tasks ranging from simple navigation (e.g., flying forward 10 meters and land) to complex high-level tasks (e.g., locating and reading a pressure gauge). The evaluation phase serves as a feedback and/or replanning stage, ensuring actions align with user objectives while preventing undesirable outcomes. We evaluate the framework in a simulated environment with two worker agents, assessing performance qualitatively and quantitatively based on task completion across varying complexity levels and workflow efficiency. By leveraging natural language processing for agent communication, our approach offers a novel, flexible, and user-accessible alternative to traditional drone-based solutions, enabling autonomous problem-solving for industrial inspection without extensive user intervention.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO",
      "eess.SY"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00259v1",
    "published_date": "2025-09-30 20:31:30 UTC",
    "updated_date": "2025-09-30 20:31:30 UTC"
  },
  {
    "arxiv_id": "2510.07328v1",
    "title": "MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation",
    "authors": [
      "Md Zubair",
      "Hao Zheng",
      "Nussdorf Jonathan",
      "Grayson W. Armstrong",
      "Lucy Q. Shen",
      "Gabriela Wilson",
      "Yu Tian",
      "Xingquan Zhu",
      "Min Shi"
    ],
    "abstract": "Medical decision systems increasingly rely on data from multiple sources to ensure reliable and unbiased diagnosis. However, existing multimodal learning models fail to achieve this goal because they often ignore two critical challenges. First, various data modalities may learn unevenly, thereby converging to a model biased towards certain modalities. Second, the model may emphasize learning on certain demographic groups causing unfair performances. The two aspects can influence each other, as different data modalities may favor respective groups during optimization, leading to both imbalanced and unfair multimodal learning. This paper proposes a novel approach called MultiFair for multimodal medical classification, which addresses these challenges with a dual-level gradient modulation process. MultiFair dynamically modulates training gradients regarding the optimization direction and magnitude at both data modality and group levels. We conduct extensive experiments on two multimodal medical datasets with different demographic groups. The results show that MultiFair outperforms state-of-the-art multimodal learning and fairness learning methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "10 Pages",
    "pdf_url": "https://arxiv.org/pdf/2510.07328v1",
    "published_date": "2025-09-30 20:30:12 UTC",
    "updated_date": "2025-09-30 20:30:12 UTC"
  },
  {
    "arxiv_id": "2510.00255v1",
    "title": "TASER: Translation Assessment via Systematic Evaluation and Reasoning",
    "authors": [
      "Monishwaran Maheswaran",
      "Marco Carini",
      "Christian Federmann",
      "Tony Diaz"
    ],
    "abstract": "We introduce TASER (Translation Assessment via Systematic Evaluation and Reasoning), a metric that uses Large Reasoning Models (LRMs) for automated translation quality assessment. TASER harnesses the explicit reasoning capabilities of LRMs to conduct systematic, step-by-step evaluation of translation quality. We evaluate TASER on the WMT24 Metrics Shared Task across both reference-based and reference-free scenarios, demonstrating state-of-the-art performance. In system-level evaluation, TASER achieves the highest soft pairwise accuracy in both reference-based and reference-free settings, outperforming all existing metrics. At the segment level, TASER maintains competitive performance with our reference-free variant ranking as the top-performing metric among all reference-free approaches. Our experiments reveal that structured prompting templates yield superior results with LRMs compared to the open-ended approaches that proved optimal for traditional LLMs. We evaluate o3, a large reasoning model from OpenAI, with varying reasoning efforts, providing insights into the relationship between reasoning depth and evaluation quality. The explicit reasoning process in LRMs offers interpretability and visibility, addressing a key limitation of existing automated metrics. Our results demonstrate that Large Reasoning Models show a measurable advancement in translation quality assessment, combining improved accuracy with transparent evaluation across diverse language pairs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00255v1",
    "published_date": "2025-09-30 20:27:48 UTC",
    "updated_date": "2025-09-30 20:27:48 UTC"
  },
  {
    "arxiv_id": "2510.03310v1",
    "title": "Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management",
    "authors": [
      "Runze Zhang",
      "Xiaowei Zhang",
      "Mingyang Zhao"
    ],
    "abstract": "LLMs are emerging tools for simulating human behavior in business, economics, and social science, offering a lower-cost complement to laboratory experiments, field studies, and surveys. This paper evaluates how well LLMs replicate human behavior in operations management. Using nine published experiments in behavioral operations, we assess two criteria: replication of hypothesis-test outcomes and distributional alignment via Wasserstein distance. LLMs reproduce most hypothesis-level effects, capturing key decision biases, but their response distributions diverge from human data, including for strong commercial models. We also test two lightweight interventions -- chain-of-thought prompting and hyperparameter tuning -- which reduce misalignment and can sometimes let smaller or open-source models match or surpass larger systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03310v1",
    "published_date": "2025-09-30 20:20:58 UTC",
    "updated_date": "2025-09-30 20:20:58 UTC"
  },
  {
    "arxiv_id": "2510.00245v1",
    "title": "Can AI agents understand spoken conversations about data visualizations in online meetings?",
    "authors": [
      "Rizul Sharma",
      "Tianyu Jiang",
      "Seokki Lee",
      "Jillian Aurisano"
    ],
    "abstract": "In this short paper, we present work evaluating an AI agent's understanding of spoken conversations about data visualizations in an online meeting scenario. There is growing interest in the development of AI-assistants that support meetings, such as by providing assistance with tasks or summarizing a discussion. The quality of this support depends on a model that understands the conversational dialogue. To evaluate this understanding, we introduce a dual-axis testing framework for diagnosing the AI agent's comprehension of spoken conversations about data. Using this framework, we designed a series of tests to evaluate understanding of a novel corpus of 72 spoken conversational dialogues about data visualizations. We examine diverse pipelines and model architectures, LLM vs VLM, and diverse input formats for visualizations (the chart image, its underlying source code, or a hybrid of both) to see how this affects model performance on our tests. Using our evaluation methods, we found that text-only input modalities achieved the best performance (96%) in understanding discussions of visualizations in online meetings.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00245v1",
    "published_date": "2025-09-30 20:17:36 UTC",
    "updated_date": "2025-09-30 20:17:36 UTC"
  },
  {
    "arxiv_id": "2510.00240v2",
    "title": "SecureBERT 2.0: Advanced Language Model for Cybersecurity Intelligence",
    "authors": [
      "Ehsan Aghaei",
      "Sarthak Jain",
      "Prashanth Arun",
      "Arjun Sambamoorthy"
    ],
    "abstract": "Effective analysis of cybersecurity and threat intelligence data demands language models that can interpret specialized terminology, complex document structures, and the interdependence of natural language and source code. Encoder-only transformer architectures provide efficient and robust representations that support critical tasks such as semantic search, technical entity extraction, and semantic analysis, which are key to automated threat detection, incident triage, and vulnerability assessment. However, general-purpose language models often lack the domain-specific adaptation required for high precision. We present SecureBERT 2.0, an enhanced encoder-only language model purpose-built for cybersecurity applications. Leveraging the ModernBERT architecture, SecureBERT 2.0 introduces improved long-context modeling and hierarchical encoding, enabling effective processing of extended and heterogeneous documents, including threat reports and source code artifacts. Pretrained on a domain-specific corpus more than thirteen times larger than its predecessor, comprising over 13 billion text tokens and 53 million code tokens from diverse real-world sources, SecureBERT 2.0 achieves state-of-the-art performance on multiple cybersecurity benchmarks. Experimental results demonstrate substantial improvements in semantic search for threat intelligence, semantic analysis, cybersecurity-specific named entity recognition, and automated vulnerability detection in code within the cybersecurity domain.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00240v2",
    "published_date": "2025-09-30 20:12:37 UTC",
    "updated_date": "2025-10-10 19:38:53 UTC"
  },
  {
    "arxiv_id": "2510.01281v1",
    "title": "An Analysis of the New EU AI Act and A Proposed Standardization Framework for Machine Learning Fairness",
    "authors": [
      "Mike Teodorescu",
      "Yongxu Sun",
      "Haren N. Bhatia",
      "Christos Makridis"
    ],
    "abstract": "The European Union's AI Act represents a crucial step towards regulating ethical and responsible AI systems. However, we find an absence of quantifiable fairness metrics and the ambiguity in terminology, particularly the interchangeable use of the keywords transparency, explainability, and interpretability in the new EU AI Act and no reference of transparency of ethical compliance. We argue that this ambiguity creates substantial liability risk that would deter investment. Fairness transparency is strategically important. We recommend a more tailored regulatory framework to enhance the new EU AI regulation. Further-more, we propose a public system framework to assess the fairness and transparency of AI systems. Drawing from past work, we advocate for the standardization of industry best practices as a necessary addition to broad regulations to achieve the level of details required in industry, while preventing stifling innovation and investment in the AI sector. The proposals are exemplified with the case of ASR and speech synthesizers.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "6 pages; IEEE HPEC 2025 Poster Session 4-P1 (12:15-13:15): AI/ML/GenAI Poster Session Thursday September 18 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.01281v1",
    "published_date": "2025-09-30 20:02:38 UTC",
    "updated_date": "2025-09-30 20:02:38 UTC"
  },
  {
    "arxiv_id": "2510.05127v1",
    "title": "Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines",
    "authors": [
      "Harshit Goyal"
    ],
    "abstract": "Efficient resource allocation is a key challenge in modern cloud computing. Over-provisioning leads to unnecessary costs, while under-provisioning risks performance degradation and SLA violations. This work presents an artificial intelligence approach to predict resource utilization in big data pipelines using Random Forest regression. We preprocess the Google Borg cluster traces to clean, transform, and extract relevant features (CPU, memory, usage distributions). The model achieves high predictive accuracy (R Square = 0.99, MAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between workload characteristics and resource utilization. Error analysis reveals impressive performance on small-to-medium jobs, with higher variance in rare large-scale jobs. These results demonstrate the potential of AI-driven prediction for cost-aware autoscaling in cloud environments, reducing unnecessary provisioning while safeguarding service quality.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "14 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.05127v1",
    "published_date": "2025-09-30 20:01:12 UTC",
    "updated_date": "2025-09-30 20:01:12 UTC"
  },
  {
    "arxiv_id": "2510.00237v1",
    "title": "Debunk the Myth of SFT Generalization",
    "authors": [
      "Xiaofeng Lin",
      "Hejian Sang",
      "Zhipeng Wang",
      "Xuezhou Zhang"
    ],
    "abstract": "A prevailing view holds that supervised fine-tuning (SFT) memorizes training data and fails to generalize, whereas reinforcement learning (RL) attains broader robustness. We revisit this claim through a systematic evaluation on two decision-making benchmarks, Sokoban and General Points, and arrive at a different conclusion. We show that much of SFT's perceived failure stems from frozen-prompt artifacts: when trained on fixed instruction templates, SFT models cling to training semantics rather than adapting to new ones. Introducing prompt diversity during training breaks this shortcut and yields strong generalization to unseen instruction variants without harming in-distribution performance. Beyond instruction shifts, we ask whether SFT can generalize to strictly harder tasks. Here, chain-of-thought (CoT) supervision provides an algorithmic scaffold that markedly improves transfer to more difficult regimes, such as larger Sokoban grids with additional boxes and arithmetic with out-of-distribution values or five-card compositions that increase combinatorial complexity. Finally, combining prompt diversity with CoT achieves the best of both worlds: robust generalization across both instruction-variant and difficulty-variant settings, matching or surpassing RL baselines on our benchmarks while retaining SFT's simplicity and stability. These findings challenge the narrative that SFT is inherently inferior to RL and support a data-centric perspective: with appropriately curated demonstrations, vanilla SFT can generalize as strongly as RL. Code reproducing the results in the paper can be found at: https://github.com/XiaofengLin7/debunking-sft-generalization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00237v1",
    "published_date": "2025-09-30 20:01:09 UTC",
    "updated_date": "2025-09-30 20:01:09 UTC"
  },
  {
    "arxiv_id": "2510.00232v1",
    "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses",
    "authors": [
      "Xin Xu",
      "Xunzhi He",
      "Churan Zhi",
      "Ruizhe Chen",
      "Julian McAuley",
      "Zexue He"
    ],
    "abstract": "Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2510.00232v1",
    "published_date": "2025-09-30 19:56:54 UTC",
    "updated_date": "2025-09-30 19:56:54 UTC"
  },
  {
    "arxiv_id": "2510.00231v1",
    "title": "The Pitfalls of KV Cache Compression",
    "authors": [
      "Alex Chen",
      "Renato Geh",
      "Aditya Grover",
      "Guy Van den Broeck",
      "Daniel Israel"
    ],
    "abstract": "KV cache compression promises increased throughput and efficiency with negligible loss in performance. While the gains in throughput are indisputable and recent literature has indeed shown minimal degradation on particular benchmarks, in general the consequences of compression in realistic scenarios such as multi-instruction prompting have been insufficiently studied. In this paper, we identify several pitfalls practitioners should be aware of when deploying KV cache compressed LLMs. Importantly, we show that certain instructions degrade much more rapidly with compression, effectively causing them to be completely ignored by the LLM. As a practical example of that, we highlight system prompt leakage as a case study, empirically showing the impact of compression on leakage and general instruction following. We show several factors that play a role in prompt leakage: compression method, instruction order, and KV eviction bias. We then propose simple changes to KV cache eviction policies that can reduce the impact of these factors and improve the overall performance in multi-instruction tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00231v1",
    "published_date": "2025-09-30 19:55:26 UTC",
    "updated_date": "2025-09-30 19:55:26 UTC"
  },
  {
    "arxiv_id": "2510.00229v4",
    "title": "AgentFlux: Decoupled Fine-Tuning & Inference for On-Device Agentic Systems",
    "authors": [
      "Rohan Kadekodi",
      "Zhan Jin",
      "Keisuke Kamahori",
      "Yile Gu",
      "Sean Khatiri",
      "Noah H. Bayindirli",
      "Sergey Gorbunov",
      "Baris Kasikci"
    ],
    "abstract": "The deployment of Large Language Models (LLMs) as agentic orchestrators has revolutionized task automation, but the need for privacy-preserving, cost-effective solutions demands on-device inference capabilities. However, local LLMs consistently underperform compared to frontier models in tool calling scenarios, struggling with both tool selection from large tool sets and accurate argument generation for complex parameter structures. We introduce a methodology that disaggregates a tool-calling task into two distinct subtasks: tool selection and argument generation. We propose \"decoupled fine-tuning\", a novel post-training approach that employs LoRA fine-tuning to create dedicated LoRA adapters for tool selection and tool-specific argument generation using separate loss masking for each of the subtasks. Furthermore, we present AgentFlux, an inference framework that leverages the LoRA adapters created using decoupled fine-tuning to perform efficient agent orchestration with the help of local models on end-user devices. AgentFlux decomposes the tool-call generation step into tool selection and argument generation, and dynamically loads the corresponding LoRA adapters to generate tool calls. Additionally, AgentFlux implements hierarchical orchestration to restrict the number of tools required for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool calling accuracy of the base model by 46%, and outperforms other local reasoning, non-reasoning and fine-tuned models of similar size in all cases, and models that are 2x larger, in most cases.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00229v4",
    "published_date": "2025-09-30 19:52:57 UTC",
    "updated_date": "2025-11-12 04:25:01 UTC"
  },
  {
    "arxiv_id": "2510.00225v1",
    "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks",
    "authors": [
      "Yue Meng",
      "Fei Chen",
      "Chuchu Fan"
    ],
    "abstract": "Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00225v1",
    "published_date": "2025-09-30 19:51:05 UTC",
    "updated_date": "2025-09-30 19:51:05 UTC"
  },
  {
    "arxiv_id": "2510.05126v2",
    "title": "Improving Metacognition and Uncertainty Communication in Language Models",
    "authors": [
      "Mark Steyvers",
      "Catarina Belem",
      "Padhraic Smyth"
    ],
    "abstract": "Large language models (LLMs) are increasingly used in decision-making contexts, but when they present answers without signaling low confidence, users may unknowingly act on erroneous outputs. Prior work shows that LLMs maintain internal uncertainty signals, yet their expressed confidence is often miscalibrated and poorly discriminates between correct and incorrect answers. We investigate whether supervised fine-tuning can improve models' ability to communicate uncertainty and whether such improvements generalize across tasks and domains. We fine-tune LLMs on datasets spanning general knowledge, mathematics, and open-ended trivia, and evaluate two metacognitive tasks: (1) single-question confidence estimation, where the model assigns a numeric certainty to its answer, and (2) pairwise confidence comparison, where the model selects which of two answers it is more likely to answer correctly. We assess generalization to unseen domains, including medical and legal reasoning. Results show that fine-tuning improves calibration (alignment between stated confidence and accuracy) and discrimination (higher confidence for correct vs. incorrect responses) within and across domains. However, gains are task-specific: training on single-question calibration does not transfer to pairwise comparison, and vice versa. Multitask fine-tuning yields broader gains, lowering calibration error and strengthening discrimination in out-of-domain evaluations. This suggests that uncertainty communication in LLMs is trainable but requires multitask training to generalize effectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05126v2",
    "published_date": "2025-09-30 19:50:02 UTC",
    "updated_date": "2025-10-21 21:46:32 UTC"
  },
  {
    "arxiv_id": "2510.00219v1",
    "title": "Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space",
    "authors": [
      "Houjun Liu",
      "Shikhar Murty",
      "Christopher D. Manning",
      "Róbert Csordás"
    ],
    "abstract": "Current approaches for scaling inference-time compute in transformers rely on training them to emit explicit chain-of-thought tokens before producing an answer. While these methods are powerful, they are limited because they cannot be applied during pretraining and are limited to only serially-generated, natural-language verbalization to scale inference-time compute. In this work, we propose Thoughtbubbles, a transformer variant that natively performs parallel adaptive computation in latent space by learning to fork or delete residual streams. Thus, tokens that require a large amount of computation can form a \"bubble\" of cloned residuals in the middle of the network for additional thinking. Crucially, this behavior is learned during pretraining with only language modeling loss. Thoughtbubbles outperforms both standard decoder LMs as well as non-adaptive parallel computation approaches on OpenWebText and peS2o perplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after pretraining across 150M to 772M parameter scales. The implicit nature of our method enables adaptive computation to be learned starting at pretraining time, paving the way to unify train and test-time behavior for reasoning models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.00219v1",
    "published_date": "2025-09-30 19:49:15 UTC",
    "updated_date": "2025-09-30 19:49:15 UTC"
  },
  {
    "arxiv_id": "2510.00212v1",
    "title": "Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation",
    "authors": [
      "Yang Zhang",
      "Huiwen Yan",
      "Mushuang Liu"
    ],
    "abstract": "Model-Agnostic Meta-Learning (MAML) is a versatile meta-learning framework applicable to both supervised learning and reinforcement learning (RL). However, applying MAML to meta-reinforcement learning (meta-RL) presents notable challenges. First, MAML relies on second-order gradient computations, leading to significant computational and memory overhead. Second, the nested structure of optimization increases the problem's complexity, making convergence to a global optimum more challenging. To overcome these limitations, we propose Directed-MAML, a novel task-directed meta-RL algorithm. Before the second-order gradient step, Directed-MAML applies an additional first-order task-directed approximation to estimate the effect of second-order gradients, thereby accelerating convergence to the optimum and reducing computational cost. Experimental results demonstrate that Directed-MAML surpasses MAML-based baselines in computational efficiency and convergence speed in the scenarios of CartPole-v1, LunarLander-v2 and two-vehicle intersection crossing. Furthermore, we show that task-directed approximation can be effectively integrated into other meta-learning algorithms, such as First-Order Model-Agnostic Meta-Learning (FOMAML) and Meta Stochastic Gradient Descent(Meta-SGD), yielding improved computational efficiency and convergence speed.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00212v1",
    "published_date": "2025-09-30 19:42:15 UTC",
    "updated_date": "2025-09-30 19:42:15 UTC"
  },
  {
    "arxiv_id": "2510.03308v2",
    "title": "Creative synthesis of kinematic mechanisms",
    "authors": [
      "Jiong Lin",
      "Jialong Ning",
      "Judah Goldfeder",
      "Hod Lipson"
    ],
    "abstract": "In this paper, we formulate the problem of kinematic synthesis for planar linkages as a cross-domain image generation task. We develop a planar linkages dataset using RGB image representations, covering a range of mechanisms: from simple types such as crank-rocker and crank-slider to more complex eight-bar linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE) is employed to explore the potential of image generative models for synthesizing unseen motion curves and simulating novel kinematics. By encoding the drawing speed of trajectory points as color gradients, the same architecture also supports kinematic synthesis conditioned on both trajectory shape and velocity profiles. We validate our method on three datasets of increasing complexity: a standard four-bar linkage set, a mixed set of four-bar and crank-slider mechanisms, and a complex set including multi-loop mechanisms. Preliminary results demonstrate the effectiveness of image-based representations for generative mechanical design, showing that mechanisms with revolute and prismatic joints, and potentially cams and gears, can be represented and synthesized within a unified image generation framework.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "6pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.03308v2",
    "published_date": "2025-09-30 19:32:30 UTC",
    "updated_date": "2025-10-20 16:07:47 UTC"
  },
  {
    "arxiv_id": "2510.00206v1",
    "title": "LoRAFusion: Efficient LoRA Fine-Tuning for LLMs",
    "authors": [
      "Zhanda Zhu",
      "Qidong Su",
      "Yaoyao Ding",
      "Kevin Song",
      "Shang Wang",
      "Gennady Pekhimenko"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly reduces GPU memory usage while maintaining competitive fine-tuned model quality on downstream tasks. Despite these benefits, we identify two key inefficiencies in existing LoRA fine-tuning systems. First, they incur substantial runtime overhead due to redundant memory accesses on large activation tensors. Second, they miss the opportunity to concurrently fine-tune multiple independent LoRA adapters that share the same base model on the same set of GPUs. This leads to missed performance gains such as reduced pipeline bubbles, better communication overlap, and improved GPU load balance.\n  To address these issues, we introduce LoRAFusion, an efficient LoRA fine-tuning system for LLMs. At the kernel level, we propose a graph-splitting method that fuses memory-bound operations. This design eliminates unnecessary memory accesses and preserves the performance of compute-bound GEMMs without incurring the cost of recomputation or synchronization. At the scheduling level, LoRAFusion introduces an adaptive batching algorithm for multi-job fine-tuning. It first splits LoRA adapters into groups to intentionally stagger batch execution across jobs, and then solves a bin-packing problem within each group to generate balanced, dependency-aware microbatches. LoRAFusion achieves up to $1.96\\times$ ($1.47\\times$ on average) end-to-end speedup compared to Megatron-LM, and up to $1.46\\times$ ($1.29\\times$ on average) improvement over mLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel achieves up to $1.39\\times$ ($1.27\\times$ on average) kernel performance improvement and can directly serve as a plug-and-play replacement in existing LoRA systems. We open-source LoRAFusion at https://github.com/CentML/lorafusion.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by EuroSys 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.00206v1",
    "published_date": "2025-09-30 19:26:22 UTC",
    "updated_date": "2025-09-30 19:26:22 UTC"
  },
  {
    "arxiv_id": "2510.01279v1",
    "title": "TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture",
    "authors": [
      "Yongchao Chen",
      "Jiefeng Chen",
      "Rui Meng",
      "Ji Yin",
      "Na Li",
      "Chuchu Fan",
      "Chi Wang",
      "Tomas Pfister",
      "Jinsung Yoon"
    ],
    "abstract": "While integrating tools like Code Interpreter and Search has significantly enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and Gemini-Pro, practical guidance on optimal tool use is lacking. The core challenge is effectively combining textual reasoning, coding, and search for diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an ensemble framework that runs multiple agents in parallel, each employing distinct tool-use strategies and answer paths. Agents in TUMIX iteratively share and refine responses based on the question and previous answers. In experiments, TUMIX achieves significant gains over state-of-the-art tool-augmented and test-time scaling methods, delivering an average accuracy improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference costs. We find that agent diversity and quality are crucial and can be enhanced by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt refinement upon reaching sufficient confidence, preserving performance at only 49% of the inference cost. Further scaling can achieve higher performance, albeit at a greater cost.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "27 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.01279v1",
    "published_date": "2025-09-30 19:19:56 UTC",
    "updated_date": "2025-09-30 19:19:56 UTC"
  },
  {
    "arxiv_id": "2510.00194v1",
    "title": "GRPO-$λ$: Credit Assignment improves LLM Reasoning",
    "authors": [
      "Prasanna Parthasarathi",
      "Mathieu Reymond",
      "Boxing Chen",
      "Yufei Cui",
      "Sarath Chandar"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed for tasks requiring complex reasoning, prompting significant interest in improving their reasoning abilities through post-training. Especially RL based methods using verifiable reward, like the state-of-the-art GRPO, have shown to tremendously improve reasoning behaviors when applied as post-training methods. However, the lack of an explicit reward or critic model limits GRPO's ability to assign fine-grained credit across token sequences. In this work, we present GRPO-$λ$, a novel extension to GRPO that enhances credit assignment in RL finetuning of LLMs for complex reasoning tasks. We approximate learning from $λ$-return with a reformulation of eligibility traces using token-level log-probabilities applied after each sequence generation, and a novel critic-free approximation of the temporal-difference error. We introduce a few variations for the weighting of the $λ$-return, and their applications to the eligibility-trace, where all the variations provide significant gains over GRPO. We compare GRPO-$λ$ against GRPO by training models from 1.5B to 7B parameters on $4$ different math reasoning datasets. The training plots demonstrate 30-40% improved performance during RL training on both LLaMA-3.1 and Qwen-2.5 architectures. Finally, we show that with GRPO-$λ$, the resulting average performance on AIME24, Math500, OlympiadMath, MinervaMath, and AMC improves over GRPO by over $3$ points and a $4.5$ points improvement on the 7B model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00194v1",
    "published_date": "2025-09-30 19:11:10 UTC",
    "updated_date": "2025-09-30 19:11:10 UTC"
  },
  {
    "arxiv_id": "2510.00192v2",
    "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning",
    "authors": [
      "Xin Yu",
      "Cong Xie",
      "Ziyu Zhao",
      "Tiantian Fan",
      "Lingzhou Xue",
      "Zhi Zhang"
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a widely used paradigm for parameter-efficient fine-tuning of large language models, yet its representational capacity often lags behind full fine-tuning. Within the context of LoRA, a key open question is how to obtain expressive low-rank adapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new framework that leverages structured pruning to obtain highly representative low-rank adapters from an over-parameterized initialization. Unlike prior approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes less important components during fine-tuning and prevents their reactivation, enabling flexible and adaptive rank allocation. For structured pruning, by minimizing the pruning error for overall loss, we provide fine-grained pruning and recovery updates in a gradient-based pruning strategy with grounded interpretation. We provide the first theoretical analysis of the robustness of structured pruning and provably show that under the impact of weight perturbation, gradient-based pruning is more robust than activation-based pruning with respect to overall loss. Empirically, PrunedLoRA consistently outperforms LoRA and its variants across supervised fine-tuning tasks in mathematical reasoning, code generation, and natural language understanding, and it also demonstrates advantages over existing structured pruning methods across diverse sparsity levels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00192v2",
    "published_date": "2025-09-30 19:10:35 UTC",
    "updated_date": "2025-11-01 04:19:13 UTC"
  },
  {
    "arxiv_id": "2510.00186v2",
    "title": "Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective",
    "authors": [
      "Anni Li",
      "Aria Attar",
      "Paul Dong"
    ],
    "abstract": "Transforming natural-language requests into reliable, production-ready data transformations remains challenging: correctness depends on precise schema linking and warehouse-specific SQL dialects, while the strongest supervision available during training--execution success and result matching--are provided only at the sequence level. At the same time, assembling large, execution-validated corpora is costly, and token-level objectives misalign with these global signals, yielding unstable optimization and limited portability. We introduce Thinkquel, a fine-tuned model for producing robust, portable, and execution-validated database queries. Methodologies in Thinkquel integrates a novel synthetic data pipeline, TS-SQL, that leverages dbt as a portable intermediate representation with a span-aware reinforcement learning objective, and Token-Sequence GRPO (TS-GRPO), specifically designed to bridge the gap between token-level training signals and sequence-level execution rewards when finetuning LLMs. On the 500-example TS-SQL test set, Thinkquel (32B) reaches 93.2% execution success and 61.8% exact-result match with a two-stage SFT curriculum, improving over the base model by 67.2% (exec.) and 44.4% (match). In Spider (14B) experiments, TS-GRPO increases training stability and speeds convergence of the execution-match reward relative to GRPO and GSPO.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00186v2",
    "published_date": "2025-09-30 19:04:53 UTC",
    "updated_date": "2025-10-02 18:28:05 UTC"
  },
  {
    "arxiv_id": "2510.00185v1",
    "title": "Object-Centric Case-Based Reasoning via Argumentation",
    "authors": [
      "Gabriel de Olim Gaul",
      "Adam Gould",
      "Avinash Kori",
      "Francesca Toni"
    ],
    "abstract": "We introduce Slot Attention Argumentation for Case-Based Reasoning (SAA-CBR), a novel neuro-symbolic pipeline for image classification that integrates object-centric learning via a neural Slot Attention (SA) component with symbolic reasoning conducted by Abstract Argumentation for Case-Based Reasoning (AA-CBR). We explore novel integrations of AA-CBR with the neural component, including feature combination strategies, casebase reduction via representative samples, novel count-based partial orders, a One-Vs-Rest strategy for extending AA-CBR to multi-class classification, and an application of Supported AA-CBR, a bipolar variant of AA-CBR. We demonstrate that SAA-CBR is an effective classifier on the CLEVR-Hans datasets, showing competitive performance against baseline models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ArgXAI@ECAI25",
    "pdf_url": "https://arxiv.org/pdf/2510.00185v1",
    "published_date": "2025-09-30 19:04:27 UTC",
    "updated_date": "2025-09-30 19:04:27 UTC"
  },
  {
    "arxiv_id": "2510.00184v1",
    "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls",
    "authors": [
      "Xiaoyan Bai",
      "Itamar Pres",
      "Yuntian Deng",
      "Chenhao Tan",
      "Stuart Shieber",
      "Fernanda Viégas",
      "Martin Wattenberg",
      "Andrew Lee"
    ],
    "abstract": "Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via \\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00184v1",
    "published_date": "2025-09-30 19:03:26 UTC",
    "updated_date": "2025-09-30 19:03:26 UTC"
  },
  {
    "arxiv_id": "2510.00182v1",
    "title": "A Systematic Study of Large Language Models for Task and Motion Planning With PDDLStream",
    "authors": [
      "Jorge Mendez-Mendez"
    ],
    "abstract": "Using large language models (LLMs) to solve complex robotics problems requires understanding their planning capabilities. Yet while we know that LLMs can plan on some problems, the extent to which these planning capabilities cover the space of robotics tasks is unclear. One promising direction is to integrate the semantic knowledge of LLMs with the formal reasoning of task and motion planning (TAMP). However, the myriad of choices for how to integrate LLMs within TAMP complicates the design of such systems. We develop 16 algorithms that use Gemini 2.5 Flash to substitute key TAMP components. Our zero-shot experiments across 4,950 problems and three domains reveal that the Gemini-based planners exhibit lower success rates and higher planning times than their engineered counterparts. We show that providing geometric details increases the number of task-planning errors compared to pure PDDL descriptions, and that (faster) non-reasoning LLM variants outperform (slower) reasoning variants in most cases, since the TAMP system can direct the LLM to correct its mistakes.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00182v1",
    "published_date": "2025-09-30 19:03:14 UTC",
    "updated_date": "2025-09-30 19:03:14 UTC"
  },
  {
    "arxiv_id": "2510.00181v1",
    "title": "CHAI: Command Hijacking against embodied AI",
    "authors": [
      "Luis Burbano",
      "Diego Ortiz",
      "Qi Sun",
      "Siwei Yang",
      "Haoqin Tu",
      "Cihang Xie",
      "Yinzhi Cao",
      "Alvaro A Cardenas"
    ],
    "abstract": "Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, however, also create new security risks. In this paper, we introduce CHAI (Command Hijacking against embodied AI), a new class of prompt-based attacks that exploit the multimodal language interpretation abilities of Large Visual-Language Models (LVLMs). CHAI embeds deceptive natural language instructions, such as misleading signs, in visual input, systematically searches the token space, builds a dictionary of prompts, and guides an attacker model to generate Visual Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing, autonomous driving, and aerial object tracking, and on a real robotic vehicle. Our experiments show that CHAI consistently outperforms state-of-the-art attacks. By exploiting the semantic and multimodal reasoning strengths of next-generation embodied AI systems, CHAI underscores the urgent need for defenses that extend beyond traditional adversarial robustness.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00181v1",
    "published_date": "2025-09-30 19:02:57 UTC",
    "updated_date": "2025-09-30 19:02:57 UTC"
  },
  {
    "arxiv_id": "2510.03306v1",
    "title": "Atlas-free Brain Network Transformer",
    "authors": [
      "Shuai Huang",
      "Xuan Kan",
      "James J. Lah",
      "Deqiang Qiu"
    ],
    "abstract": "Current atlas-based approaches to brain network analysis rely heavily on standardized anatomical or connectivity-driven brain atlases. However, these fixed atlases often introduce significant limitations, such as spatial misalignment across individuals, functional heterogeneity within predefined regions, and atlas-selection biases, collectively undermining the reliability and interpretability of the derived brain networks. To address these challenges, we propose a novel atlas-free brain network transformer (atlas-free BNT) that leverages individualized brain parcellations derived directly from subject-specific resting-state fMRI data. Our approach computes ROI-to-voxel connectivity features in a standardized voxel-based feature space, which are subsequently processed using the BNT architecture to produce comparable subject-level embeddings. Experimental evaluations on sex classification and brain-connectome age prediction tasks demonstrate that our atlas-free BNT consistently outperforms state-of-the-art atlas-based methods, including elastic net, BrainGNN, Graphormer and the original BNT. Our atlas-free approach significantly improves the precision, robustness, and generalizability of brain network analyses. This advancement holds great potential to enhance neuroimaging biomarkers and clinical diagnostic tools for personalized precision medicine.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "eess.IV"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03306v1",
    "published_date": "2025-09-30 18:57:02 UTC",
    "updated_date": "2025-09-30 18:57:02 UTC"
  },
  {
    "arxiv_id": "2510.00177v1",
    "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It",
    "authors": [
      "Shuyue Stella Li",
      "Avinandan Bose",
      "Faeze Brahman",
      "Simon Shaolei Du",
      "Pang Wei Koh",
      "Maryam Fazel",
      "Yulia Tsvetkov"
    ],
    "abstract": "Current large language model (LLM) development treats task-solving and preference alignment as separate challenges, optimizing first for objective correctness, then for alignment to aggregated human preferences. This paradigm fails in human-facing applications where solving a problem correctly is insufficient if the response mismatches the user's needs. This challenge intensifies in just-in-time scenarios where no prior user interaction history exists due to cold-start conditions or privacy constraints. LLMs need to identify what they don't know about user preferences, strategically elicit preference values through questioning, then adapt their reasoning processes and responses accordingly -- a complicated chain of cognitive processes which we term personalized reasoning. We introduce PREFDISCO, an evaluation methodology that transforms static benchmarks into interactive personalization tasks using psychologically-grounded personas with sparse preferences. Our framework creates scenarios where identical questions require different reasoning chains depending on user context, as optimal explanation approaches vary by individual expertise and preferences while maintaining factual accuracy. Evaluation of 21 frontier models across 10 tasks reveals 29.0% of naive personalization attempts produce worse preference alignment than generic responses, yet generic responses also fail to serve individual user needs effectively. These findings suggest personalized reasoning requires dedicated development rather than emerging naturally. PREFDISCO establishes personalized reasoning as a measurable research frontier and reveals fundamental limitations in current LLMs' interactive capabilities, providing a foundation for developing systems that can adapt to individual users in education, healthcare, and technical domains where personalization is critical.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "57 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.00177v1",
    "published_date": "2025-09-30 18:55:28 UTC",
    "updated_date": "2025-09-30 18:55:28 UTC"
  },
  {
    "arxiv_id": "2510.00167v1",
    "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI",
    "authors": [
      "Diego Ortiz Barbosa",
      "Mohit Agrawal",
      "Yash Malegaonkar",
      "Luis Burbano",
      "Axel Andersson",
      "György Dán",
      "Henrik Sandberg",
      "Alvaro A. Cardenas"
    ],
    "abstract": "Autonomous drones must often respond to sudden events, such as alarms, faults, or unexpected changes in their environment, that require immediate and adaptive decision-making. Traditional approaches rely on safety engineers hand-coding large sets of recovery rules, but this strategy cannot anticipate the vast range of real-world contingencies and quickly becomes incomplete. Recent advances in embodied AI, powered by large visual language models, provide commonsense reasoning to assess context and generate appropriate actions in real time. We demonstrate this capability in a simulated urban benchmark in the Unreal Engine, where drones dynamically interpret their surroundings and decide on sudden maneuvers for safe landings. Our results show that embodied AI makes possible a new class of adaptive recovery and decision-making pipelines that were previously infeasible to design by hand, advancing resilience and safety in autonomous aerial systems.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00167v1",
    "published_date": "2025-09-30 18:39:36 UTC",
    "updated_date": "2025-09-30 18:39:36 UTC"
  },
  {
    "arxiv_id": "2510.00165v1",
    "title": "Privacy-Preserving Learning-Augmented Data Structures",
    "authors": [
      "Prabhav Goyal",
      "Vinesh Sridhar",
      "Wilson Zheng"
    ],
    "abstract": "Learning-augmented data structures use predicted frequency estimates to retrieve frequently occurring database elements faster than standard data structures. Recent work has developed data structures that optimally exploit these frequency estimates while maintaining robustness to adversarial prediction errors. However, the privacy and security implications of this setting remain largely unexplored.\n  In the event of a security breach, data structures should reveal minimal information beyond their current contents. This is even more crucial for learning-augmented data structures, whose layout adapts to the data. A data structure is history independent if its memory representation reveals no information about past operations except what is inferred from its current contents. In this work, we take the first step towards privacy and security guarantees in this setting by proposing the first learning-augmented data structure that is strongly history independent, robust, and supports dynamic updates.\n  To achieve this, we introduce two techniques: thresholding, which automatically makes any learning-augmented data structure robust, and pairing, a simple technique that provides strong history independence in the dynamic setting. Our experimental results demonstrate a tradeoff between security and efficiency but are still competitive with the state of the art.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.IR",
    "comment": "6 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.00165v1",
    "published_date": "2025-09-30 18:37:39 UTC",
    "updated_date": "2025-09-30 18:37:39 UTC"
  },
  {
    "arxiv_id": "2510.00163v1",
    "title": "Partial Identification Approach to Counterfactual Fairness Assessment",
    "authors": [
      "Saeyoung Rho",
      "Junzhe Zhang",
      "Elias Bareinboim"
    ],
    "abstract": "The wide adoption of AI decision-making systems in critical domains such as criminal justice, loan approval, and hiring processes has heightened concerns about algorithmic fairness. As we often only have access to the output of algorithms without insights into their internal mechanisms, it was natural to examine how decisions would alter when auxiliary sensitive attributes (such as race) change. This led the research community to come up with counterfactual fairness measures, but how to evaluate the measure from available data remains a challenging task. In many practical applications, the target counterfactual measure is not identifiable, i.e., it cannot be uniquely determined from the combination of quantitative data and qualitative knowledge. This paper addresses this challenge using partial identification, which derives informative bounds over counterfactual fairness measures from observational data. We introduce a Bayesian approach to bound unknown counterfactual fairness measures with high confidence. We demonstrate our algorithm on the COMPAS dataset, examining fairness in recidivism risk scores with respect to race, age, and sex. Our results reveal a positive (spurious) effect on the COMPAS score when changing race to African-American (from all others) and a negative (direct causal) effect when transitioning from young to old age.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00163v1",
    "published_date": "2025-09-30 18:35:08 UTC",
    "updated_date": "2025-09-30 18:35:08 UTC"
  },
  {
    "arxiv_id": "2510.00156v1",
    "title": "AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery",
    "authors": [
      "Songran Bai",
      "Bingzhe Wu",
      "Yiwei Zhang",
      "Chengke Wu",
      "Xiaolong Zheng",
      "Yaze Yuan",
      "Ke Wu",
      "Jianqiang Li"
    ],
    "abstract": "Financial fraud detection in real-world scenarios presents significant challenges due to the subtlety and dispersion of evidence across complex, multi-year financial disclosures. In this work, we introduce a novel multi-agent reasoning framework AuditAgent, enhanced with auditing domain expertise, for fine-grained evidence chain localization in financial fraud cases. Leveraging an expert-annotated dataset constructed from enforcement documents and financial reports released by the China Securities Regulatory Commission, our approach integrates subject-level risk priors, a hybrid retrieval strategy, and specialized agent modules to efficiently identify and aggregate cross-report evidence. Extensive experiments demonstrate that our method substantially outperforms General-Purpose Agent paradigm in both recall and interpretability, establishing a new benchmark for automated, transparent financial forensics. Our results highlight the value of domain-specific reasoning and dataset construction for advancing robust financial fraud detection in practical, real-world regulatory applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00156v1",
    "published_date": "2025-09-30 18:26:44 UTC",
    "updated_date": "2025-09-30 18:26:44 UTC"
  },
  {
    "arxiv_id": "2510.00154v1",
    "title": "RoboPilot: Generalizable Dynamic Robotic Manipulation with Dual-thinking Modes",
    "authors": [
      "Xinyi Liu",
      "Mohammadreza Fani Sani",
      "Zewei Zhou",
      "Julius Wirbel",
      "Bahram Zarrin",
      "Roberto Galeazzi"
    ],
    "abstract": "Despite rapid progress in autonomous robotics, executing complex or long-horizon tasks remains a fundamental challenge. Most current approaches follow an open-loop paradigm with limited reasoning and no feedback, resulting in poor robustness to environmental changes and severe error accumulation. We present RoboPilot, a dual-thinking closed-loop framework for robotic manipulation that supports adaptive reasoning for complex tasks in real-world dynamic environments. RoboPilot leverages primitive actions for structured task planning and flexible action generation, while introducing feedback to enable replanning from dynamic changes and execution errors. Chain-of-Thought reasoning further enhances high-level task planning and guides low-level action generation. The system dynamically switches between fast and slow thinking to balance efficiency and accuracy. To systematically evaluate the robustness of RoboPilot in diverse robot manipulation scenarios, we introduce RoboPilot-Bench, a benchmark spanning 21 tasks across 10 categories, including infeasible-task recognition and failure recovery. Experiments show that RoboPilot outperforms state-of-the-art baselines by 25.9\\% in task success rate, and the real-world deployment on an industrial robot further demonstrates its robustness in real-world settings.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00154v1",
    "published_date": "2025-09-30 18:25:47 UTC",
    "updated_date": "2025-09-30 18:25:47 UTC"
  },
  {
    "arxiv_id": "2510.01278v1",
    "title": "Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning",
    "authors": [
      "Hengwei Zhao",
      "Zhengzhong Tu",
      "Zhuo Zheng",
      "Wei Wang",
      "Junjue Wang",
      "Rusty Feagin",
      "Wenzhe Jiao"
    ],
    "abstract": "Positive-Unlabeled (PU) learning aims to train a binary classifier (positive vs. negative) where only limited positive data and abundant unlabeled data are available. While widely applicable, state-of-the-art PU learning methods substantially underperform their supervised counterparts on complex datasets, especially without auxiliary negatives or pre-estimated parameters (e.g., a 14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the challenge of learning discriminative representations under unreliable supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU learning framework that requires no auxiliary information. NcPU combines a noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns intra-class representations despite unreliable supervision, with a phantom label disambiguation (PLD) scheme that supplies conservative negative supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can iteratively benefit each other from the perspective of the Expectation-Maximization framework. Empirically, extensive experiments demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive performance; and (2) NcPU achieves substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging datasets on post-disaster building damage mapping, highlighting its promise for real-world applications. Code: Code will be open-sourced after review.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01278v1",
    "published_date": "2025-09-30 18:22:30 UTC",
    "updated_date": "2025-09-30 18:22:30 UTC"
  },
  {
    "arxiv_id": "2510.00151v1",
    "title": "Stealing AI Model Weights Through Covert Communication Channels",
    "authors": [
      "Valentin Barbaza",
      "Alan Rodrigo Diaz-Rizo",
      "Hassan Aboushady",
      "Spyridon Raptis",
      "Haralampos-G. Stratigopoulos"
    ],
    "abstract": "AI models are often regarded as valuable intellectual property due to the high cost of their development, the competitive advantage they provide, and the proprietary techniques involved in their creation. As a result, AI model stealing attacks pose a serious concern for AI model providers. In this work, we present a novel attack targeting wireless devices equipped with AI hardware accelerators. The attack unfolds in two phases. In the first phase, the victim's device is compromised with a hardware Trojan (HT) designed to covertly leak model weights through a hidden communication channel, without the victim realizing it. In the second phase, the adversary uses a nearby wireless device to intercept the victim's transmission frames during normal operation and incrementally reconstruct the complete weight matrix. The proposed attack is agnostic to both the AI model architecture and the hardware accelerator used. We validate our approach through a hardware-based demonstration involving four diverse AI models of varying types and sizes. We detail the design of the HT and the covert channel, highlighting their stealthy nature. Additionally, we analyze the impact of bit error rates on the reception and propose an error mitigation technique. The effectiveness of the attack is evaluated based on the accuracy of the reconstructed models with stolen weights and the time required to extract them. Finally, we explore potential defense mechanisms.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00151v1",
    "published_date": "2025-09-30 18:21:41 UTC",
    "updated_date": "2025-09-30 18:21:41 UTC"
  },
  {
    "arxiv_id": "2510.00144v1",
    "title": "Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback",
    "authors": [
      "Shreyas Chaudhari",
      "Renhao Zhang",
      "Philip S. Thomas",
      "Bruno Castro da Silva"
    ],
    "abstract": "The ability of reinforcement learning algorithms to learn effective policies is determined by the rewards available during training. However, for practical problems, obtaining large quantities of reward labels is often infeasible due to computational or financial constraints, particularly when relying on human feedback. When reinforcement learning must proceed with limited feedback -- only a fraction of samples get rewards labeled -- a fundamental question arises: which samples should be labeled to maximize policy performance? We formalize this problem of reward selection for reinforcement learning from limited feedback (RLLF), introducing a new problem formulation that facilitates the study of strategies for selecting impactful rewards. Two types of selection strategies are investigated: (i) heuristics that rely on reward-free information such as state visitation and partial value functions, and (ii) strategies pre-trained using auxiliary evaluative feedback. We find that critical subsets of rewards are those that (1) guide the agent along optimal trajectories, and (2) support recovery toward near-optimal behavior after deviations. Effective selection methods yield near-optimal policies with significantly fewer reward labels than full supervision, establishing reward selection as a powerful paradigm for scaling reinforcement learning in feedback-limited settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00144v1",
    "published_date": "2025-09-30 18:17:49 UTC",
    "updated_date": "2025-09-30 18:17:49 UTC"
  },
  {
    "arxiv_id": "2510.00137v1",
    "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval",
    "authors": [
      "Nima Sheikholeslami",
      "Erfan Hosseini",
      "Patrice Bechard",
      "Srivatsava Daruru",
      "Sai Rajeswar"
    ],
    "abstract": "Dual-encoder retrievers depend on the principle that relevant documents should score higher than irrelevant ones for a given query. Yet the dominant Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss, optimizes a softened ranking surrogate that we rigorously prove is fundamentally oblivious to score separation quality and unrelated to AUC. This mismatch leads to poor calibration and suboptimal performance in downstream tasks like retrieval-augmented generation (RAG). To address this fundamental limitation, we introduce the MW loss, a new training objective that maximizes the Mann-Whitney U statistic, which is mathematically equivalent to the Area under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be correctly ranked by minimizing binary cross entropy over score differences. We provide theoretical guarantees that MW loss directly upper-bounds the AoC, better aligning optimization with retrieval goals. We further promote ROC curves and AUC as natural threshold free diagnostics for evaluating retriever calibration and ranking quality. Empirically, retrievers trained with MW loss consistently outperform contrastive counterparts in AUC and standard retrieval metrics. Our experiments show that MW loss is an empirically superior alternative to Contrastive Loss, yielding better-calibrated and more discriminative retrievers for high-stakes applications like RAG.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00137v1",
    "published_date": "2025-09-30 18:14:01 UTC",
    "updated_date": "2025-09-30 18:14:01 UTC"
  },
  {
    "arxiv_id": "2510.00136v1",
    "title": "Nonparametric Identification of Latent Concepts",
    "authors": [
      "Yujia Zheng",
      "Shaoan Xie",
      "Kun Zhang"
    ],
    "abstract": "We are born with the ability to learn concepts by comparing diverse observations. This helps us to understand the new world in a compositional manner and facilitates extrapolation, as objects naturally consist of multiple concepts. In this work, we argue that the cognitive mechanism of comparison, fundamental to human learning, is also vital for machines to recover true concepts underlying the data. This offers correctness guarantees for the field of concept learning, which, despite its impressive empirical successes, still lacks general theoretical support. Specifically, we aim to develop a theoretical framework for the identifiability of concepts with multiple classes of observations. We show that with sufficient diversity across classes, hidden concepts can be identified without assuming specific concept types, functional relations, or parametric generative models. Interestingly, even when conditions are not globally satisfied, we can still provide alternative guarantees for as many concepts as possible based on local comparisons, thereby extending the applicability of our theory to more flexible scenarios. Moreover, the hidden structure between classes and concepts can also be identified nonparametrically. We validate our theoretical results in both synthetic and real-world settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.00136v1",
    "published_date": "2025-09-30 18:13:53 UTC",
    "updated_date": "2025-09-30 18:13:53 UTC"
  },
  {
    "arxiv_id": "2510.00129v1",
    "title": "BigBang-Proton Technical Report: Next-Word-Prediction is Scientific Multitask Learner",
    "authors": [
      "Hengkui Wu",
      "Liujiang Liu",
      "Jihua He",
      "Qihao Wang",
      "Keke Zhao",
      "Shuyang Hu",
      "Renle Fu",
      "Dahao Liang",
      "Lingyu Zeng",
      "Bruce Liu",
      "Yuan Liu",
      "Jin Zhan",
      "Jiaqiang Niu",
      "Xinglong Jia",
      "Yaqin Hu",
      "Wenjun Ji",
      "Panpan Chi",
      "Ken Chen",
      "Hengyuan Wu",
      "Yingsi Xin",
      "Yongfeng Zhu",
      "Yuexin Wang",
      "Manqi Ruan",
      "Ningtao Bian",
      "Xiaohua Wu",
      "Weipeng Xu"
    ],
    "abstract": "We introduce BigBang-Proton, a unified sequence-based architecture for auto-regressive language modeling pretrained on cross-scale, cross-structure, cross-discipline real-world scientific tasks to construct a scientific multi-task learner. BigBang-Proton incorporates three fundamental innovations compared to mainstream general-purpose LLMs: Theory-Experiment Learning paradigm aligns large-scale numerical experimental data with theoretical text corpora; Binary Patch Encoding replaces byte pair encoding(BPE) tokenization; Monte Carlo Attention substitutes traditional transformer architectures. Through next-word-prediction pretraining on cross-discipline scientific datasets of real-world problems mixed with general textual corpus, followed by fine-tuning and inference on downstream tasks, BigBang-Proton demonstrates 100\\% accuracy in up to 50-digit arithmetic addition operations, performance on par with leading specialized models in particle physics jet tagging, matching MAE of specialized models in inter-atomic potential simulation, performance comparable to traditional spatiotemporal models in water quality prediction, and benchmark-exceeding performance in genome modeling. These results prove that language-guided scientific computing can match or exceed the performance of task-specific scientific models while maintaining multitask learning capabilities. We further hypothesize to scale the pretraining to the universe scale as a fundamental step toward developing material world foundational model.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "93 pages, 39 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.00129v1",
    "published_date": "2025-09-30 18:09:18 UTC",
    "updated_date": "2025-09-30 18:09:18 UTC"
  },
  {
    "arxiv_id": "2510.00125v1",
    "title": "Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning",
    "authors": [
      "Hong kyu Lee",
      "Ruixuan Liu",
      "Li Xiong"
    ],
    "abstract": "Machine unlearning is an emerging technique that removes the influence of a subset of training data (forget set) from a model without full retraining, with applications including privacy protection, content moderation, and model correction. The key challenge lies in ensuring that the model completely forgets the knowledge of the forget set without compromising its overall utility. Existing unlearning methods for large language models (LLMs) often utilize auxiliary language models, retain datasets, or even commercial AI services for effective unlearning and maintaining the model utility. However, dependence on these external resources is often impractical and could potentially introduce additional privacy risks. In this work, we propose direct token optimization (DTO), a novel self-contained unlearning approach for LLMs that directly optimizes the token level objectives and eliminates the need for external resources. Given a sequence to unlearn, we identify two categories of tokens: target tokens, which capture critical knowledge for unlearning, and the remaining non-target tokens, which are crucial for maintaining the model utility. The former are used to optimize the unlearning objective, while the latter serve to preserve the model's performance. The experimental results show that the proposed DTO achieves up to 16.8$\\times$ improvement in forget quality on several benchmark datasets than the latest baselines while maintaining a comparable level of model utility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00125v1",
    "published_date": "2025-09-30 18:05:06 UTC",
    "updated_date": "2025-09-30 18:05:06 UTC"
  },
  {
    "arxiv_id": "2509.26644v1",
    "title": "Stitch: Training-Free Position Control in Multimodal Diffusion Transformers",
    "authors": [
      "Jessica Bader",
      "Mateusz Pach",
      "Maria A. Bravo",
      "Serge Belongie",
      "Zeynep Akata"
    ],
    "abstract": "Text-to-Image (T2I) generation models have advanced rapidly in recent years, but accurately capturing spatial relationships like \"above\" or \"to the right of\" poses a persistent challenge. Earlier methods improved spatial relationship following with external position control. However, as architectures evolved to enhance image quality, these techniques became incompatible with modern models. We propose Stitch, a training-free method for incorporating external position control into Multi-Modal Diffusion Transformers (MMDiT) via automatically-generated bounding boxes. Stitch produces images that are both spatially accurate and visually appealing by generating individual objects within designated bounding boxes and seamlessly stitching them together. We find that targeted attention heads capture the information necessary to isolate and cut out individual objects mid-generation, without needing to fully complete the image. We evaluate Stitch on PosEval, our benchmark for position-based T2I generation. Featuring five new tasks that extend the concept of Position beyond the basic GenEval task, PosEval demonstrates that even top models still have significant room for improvement in position-based generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances base models, even improving FLUX by 218% on GenEval's Position task and by 206% on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on PosEval, improving over previous models by 54%, all accomplished while integrating position control into leading models training-free. Code is available at https://github.com/ExplainableML/Stitch.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2509.26644v1",
    "published_date": "2025-09-30 17:59:51 UTC",
    "updated_date": "2025-09-30 17:59:51 UTC"
  },
  {
    "arxiv_id": "2509.26633v2",
    "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction",
    "authors": [
      "Lujie Yang",
      "Xiaoyu Huang",
      "Zhen Wu",
      "Angjoo Kanazawa",
      "Pieter Abbeel",
      "Carmelo Sferrazza",
      "C. Karen Liu",
      "Rocky Duan",
      "Guanya Shi"
    ],
    "abstract": "A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://omniretarget.github.io",
    "pdf_url": "https://arxiv.org/pdf/2509.26633v2",
    "published_date": "2025-09-30 17:59:02 UTC",
    "updated_date": "2025-10-08 23:16:20 UTC"
  },
  {
    "arxiv_id": "2509.26632v1",
    "title": "Branching Out: Broadening AI Measurement and Evaluation with Measurement Trees",
    "authors": [
      "Craig Greenberg",
      "Patrick Hall",
      "Theodore Jensen",
      "Kristen Greene",
      "Razvan Amironesei"
    ],
    "abstract": "This paper introduces \\textit{measurement trees}, a novel class of metrics designed to combine various constructs into an interpretable multi-level representation of a measurand. Unlike conventional metrics that yield single values, vectors, surfaces, or categories, measurement trees produce a hierarchical directed graph in which each node summarizes its children through user-defined aggregation methods. In response to recent calls to expand the scope of AI system evaluation, measurement trees enhance metric transparency and facilitate the integration of heterogeneous evidence, including, e.g., agentic, business, energy-efficiency, sociotechnical, or security signals. We present definitions and examples, demonstrate practical utility through a large-scale measurement exercise, and provide accompanying open-source Python code. By operationalizing a transparent approach to measurement of complex constructs, this work offers a principled foundation for broader and more interpretable AI evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26632v1",
    "published_date": "2025-09-30 17:58:59 UTC",
    "updated_date": "2025-09-30 17:58:59 UTC"
  },
  {
    "arxiv_id": "2509.26631v3",
    "title": "Learning Generalizable Shape Completion with SIM(3) Equivariance",
    "authors": [
      "Yuqing Wang",
      "Zhaiyu Chen",
      "Xiao Xiang Zhu"
    ],
    "abstract": "3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.26631v3",
    "published_date": "2025-09-30 17:58:55 UTC",
    "updated_date": "2025-12-11 06:52:51 UTC"
  },
  {
    "arxiv_id": "2509.26627v1",
    "title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance",
    "authors": [
      "Yuyang Liu",
      "Chuan Wen",
      "Yihang Hu",
      "Dinesh Jayaraman",
      "Yang Gao"
    ],
    "abstract": "Designing dense rewards is crucial for reinforcement learning (RL), yet in robotics it often demands extensive manual effort and lacks scalability. One promising solution is to view task progress as a dense reward signal, as it quantifies the degree to which actions advance the system toward task completion over time. We present TimeRewarder, a simple yet effective reward learning method that derives progress estimation signals from passive videos, including robot demonstrations and human videos, by modeling temporal distances between frame pairs. We then demonstrate how TimeRewarder can supply step-wise proxy rewards to guide reinforcement learning. In our comprehensive experiments on ten challenging Meta-World tasks, we show that TimeRewarder dramatically improves RL for sparse-reward tasks, achieving nearly perfect success in 9/10 tasks with only 200,000 interactions per task with the environment. This approach outperformed previous methods and even the manually designed environment dense reward on both the final success rate and sample efficiency. Moreover, we show that TimeRewarder pretraining can exploit real-world human videos, highlighting its potential as a scalable approach path to rich reward signals from diverse video sources.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26627v1",
    "published_date": "2025-09-30 17:58:20 UTC",
    "updated_date": "2025-09-30 17:58:20 UTC"
  },
  {
    "arxiv_id": "2509.26625v1",
    "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training",
    "authors": [
      "Junlin Han",
      "Shengbang Tong",
      "David Fan",
      "Yufan Ren",
      "Koustuv Sinha",
      "Philip Torr",
      "Filippos Kokkinos"
    ],
    "abstract": "Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "comment": "Project page: https://junlinhan.github.io/projects/lsbs/",
    "pdf_url": "https://arxiv.org/pdf/2509.26625v1",
    "published_date": "2025-09-30 17:57:44 UTC",
    "updated_date": "2025-09-30 17:57:44 UTC"
  },
  {
    "arxiv_id": "2509.26619v1",
    "title": "Searching for Difficult-to-Translate Test Examples at Scale",
    "authors": [
      "Wenda Xu",
      "Vilém Zouhar",
      "Parker Riley",
      "Mara Finkelstein",
      "Markus Freitag",
      "Daniel Deutsch"
    ],
    "abstract": "NLP models require test data that are sufficiently challenging. The difficulty of an example is linked to the topic it originates from (''seed topic''). The relationship between the topic and the difficulty of its instances is stochastic in nature: an example about a difficult topic can happen to be easy, and vice versa. At the scale of the Internet, there are tens of thousands of potential topics, and finding the most difficult one by drawing and evaluating a large number of examples across all topics is computationally infeasible. We formalize this task and treat it as a multi-armed bandit problem. In this framework, each topic is an ''arm,'' and pulling an arm (at a cost) involves drawing a single example, evaluating it, and measuring its difficulty. The goal is to efficiently identify the most difficult topics within a fixed computational budget. We illustrate the bandit problem setup of finding difficult examples for the task of machine translation. We find that various bandit strategies vastly outperform baseline methods like brute-force searching the most challenging topics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26619v1",
    "published_date": "2025-09-30 17:55:47 UTC",
    "updated_date": "2025-09-30 17:55:47 UTC"
  },
  {
    "arxiv_id": "2509.26605v2",
    "title": "Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning",
    "authors": [
      "Maël Macuglia",
      "Paul Friedrich",
      "Giorgia Ramponi"
    ],
    "abstract": "Deploying reinforcement learning (RL) in robotics, industry, and health care is blocked by two obstacles: the difficulty of specifying accurate rewards and the risk of unsafe, data-hungry exploration. We address this by proposing a two-stage framework that first learns a safe initial policy from a reward-free dataset of expert demonstrations, then fine-tunes it online using preference-based human feedback. We provide the first principled analysis of this offline-to-online approach and introduce BRIDGE, a unified algorithm that integrates both signals via an uncertainty-weighted objective. We derive regret bounds that shrink with the number of offline demonstrations, explicitly connecting the quantity of offline data to online sample efficiency. We validate BRIDGE in discrete and continuous control MuJoCo environments, showing it achieves lower regret than both standalone behavioral cloning and online preference-based RL. Our work establishes a theoretical foundation for designing more sample-efficient interactive agents.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "85 pages (11 + references and appendix), 9 figures. v2: added acknowledgements",
    "pdf_url": "https://arxiv.org/pdf/2509.26605v2",
    "published_date": "2025-09-30 17:50:19 UTC",
    "updated_date": "2025-10-13 13:00:55 UTC"
  },
  {
    "arxiv_id": "2509.26601v2",
    "title": "MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages",
    "authors": [
      "Chenxi Whitehouse",
      "Sebastian Ruder",
      "Tony Lin",
      "Oksana Kurylo",
      "Haruka Takagi",
      "Janice Lam",
      "Nicolò Busetto",
      "Denise Diaz",
      "Francisco Guzmán"
    ],
    "abstract": "Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 23 tables, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.26601v2",
    "published_date": "2025-09-30 17:48:58 UTC",
    "updated_date": "2025-11-11 10:36:56 UTC"
  },
  {
    "arxiv_id": "2509.26600v1",
    "title": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks",
    "authors": [
      "Wenda Xu",
      "Sweta Agrawal",
      "Vilém Zouhar",
      "Markus Freitag",
      "Daniel Deutsch"
    ],
    "abstract": "As large language models (LLMs) begin to saturate existing benchmarks, automated benchmark creation using LLMs (LLM as a benchmark) has emerged as a scalable alternative to slow and costly human curation. While these generated test sets have to potential to cheaply rank models, we demonstrate a critical flaw. LLM generated benchmarks systematically favor the model that created the benchmark, they exhibit self bias on low resource languages to English translation tasks. We show three key findings on automatic benchmarking of LLMs for translation: First, this bias originates from two sources: the generated test data (LLM as a testset) and the evaluation method (LLM as an evaluator), with their combination amplifying the effect. Second, self bias in LLM as a benchmark is heavily influenced by the model's generation capabilities in the source language. For instance, we observe more pronounced bias in into English translation, where the model's generation system is developed, than in out of English translation tasks. Third, we observe that low diversity in source text is one attribution to self bias. Our results suggest that improving the diversity of these generated source texts can mitigate some of the observed self bias.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26600v1",
    "published_date": "2025-09-30 17:48:35 UTC",
    "updated_date": "2025-09-30 17:48:35 UTC"
  },
  {
    "arxiv_id": "2509.26598v1",
    "title": "Are Robust LLM Fingerprints Adversarially Robust?",
    "authors": [
      "Anshul Nasery",
      "Edoardo Contente",
      "Alkin Kaz",
      "Pramod Viswanath",
      "Sewoong Oh"
    ],
    "abstract": "Model fingerprinting has emerged as a promising paradigm for claiming model ownership. However, robustness evaluations of these schemes have mostly focused on benign perturbations such as incremental fine-tuning, model merging, and prompting. Lack of systematic investigations into {\\em adversarial robustness} against a malicious model host leaves current systems vulnerable. To bridge this gap, we first define a concrete, practical threat model against model fingerprinting. We then take a critical look at existing model fingerprinting schemes to identify their fundamental vulnerabilities. Based on these, we develop adaptive adversarial attacks tailored for each vulnerability, and demonstrate that these can bypass model authentication completely for ten recently proposed fingerprinting schemes while maintaining high utility of the model for the end users. Our work encourages fingerprint designers to adopt adversarial robustness by design. We end with recommendations for future fingerprinting methods.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26598v1",
    "published_date": "2025-09-30 17:47:09 UTC",
    "updated_date": "2025-09-30 17:47:09 UTC"
  },
  {
    "arxiv_id": "2509.26584v1",
    "title": "Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models",
    "authors": [
      "Matheus Vinicius da Silva de Oliveira",
      "Jonathan de Andrade Silva",
      "Awdren de Lima Fontao"
    ],
    "abstract": "Large Language Models (LLMs) are widely used across multiple domains but continue to raise concerns regarding security and fairness. Beyond known attack vectors such as data poisoning and prompt injection, LLMs are also vulnerable to fairness bugs. These refer to unintended behaviors influenced by sensitive demographic cues (e.g., race or sexual orientation) that should not affect outcomes. Another key issue is hallucination, where models generate plausible yet false information. Retrieval-Augmented Generation (RAG) has emerged as a strategy to mitigate hallucinations by combining external retrieval with text generation. However, its adoption raises new fairness concerns, as the retrieved content itself may surface or amplify bias. This study conducts fairness testing through metamorphic testing (MT), introducing controlled demographic perturbations in prompts to assess fairness in sentiment analysis performed by three Small Language Models (SLMs) hosted on HuggingFace (Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B), each integrated into a RAG pipeline. Results show that minor demographic variations can break up to one third of metamorphic relations (MRs). A detailed analysis of these failures reveals a consistent bias hierarchy, with perturbations involving racial cues being the predominant cause of the violations. In addition to offering a comparative evaluation, this work reinforces that the retrieval component in RAG must be carefully curated to prevent bias amplification. The findings serve as a practical alert for developers, testers and small organizations aiming to adopt accessible SLMs without compromising fairness or reliability.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26584v1",
    "published_date": "2025-09-30 17:42:35 UTC",
    "updated_date": "2025-09-30 17:42:35 UTC"
  },
  {
    "arxiv_id": "2509.26574v3",
    "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark",
    "authors": [
      "Minhui Zhu",
      "Minyang Tian",
      "Xiaocheng Yang",
      "Tianci Zhou",
      "Lifan Yuan",
      "Penghao Zhu",
      "Eli Chertkov",
      "Shengyan Liu",
      "Yufeng Du",
      "Ziming Ji",
      "Indranil Das",
      "Junyi Cao",
      "Yufeng Du",
      "Jiabin Yu",
      "Peixue Wu",
      "Jinchen He",
      "Yifan Su",
      "Yikun Jiang",
      "Yujie Zhang",
      "Chang Liu",
      "Ze-Min Huang",
      "Weizhen Jia",
      "Yunkai Wang",
      "Farshid Jafarpour",
      "Yong Zhao",
      "Xinan Chen",
      "Jessie Shelton",
      "Aaron W. Young",
      "John Bartolotta",
      "Wenchao Xu",
      "Yue Sun",
      "Anjun Chu",
      "Victor Colussi",
      "Chris Akers",
      "Nathan Brooks",
      "Wenbo Fu",
      "Jinchao Zhao",
      "Marvin Qi",
      "Anqi Mu",
      "Yubo Yang",
      "Allen Zang",
      "Yang Lyu",
      "Peizhi Mai",
      "Christopher Wilson",
      "Xuefei Guo",
      "Juntai Zhou",
      "Daniel Inafuku",
      "Chi Xue",
      "Luyu Gao",
      "Ze Yang",
      "Yaïr Hein",
      "Yonatan Kahn",
      "Kevin Zhou",
      "Di Luo",
      "John Drew Wilson",
      "Jarrod T. Reilly",
      "Dmytro Bandak",
      "Ofir Press",
      "Liang Yang",
      "Xueying Wang",
      "Hao Tong",
      "Nicolas Chia",
      "Eliu Huerta",
      "Hao Peng"
    ],
    "abstract": "While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced \"critical point\"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 5.7%, achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.",
    "categories": [
      "cs.AI",
      "cond-mat.other",
      "cs.CL",
      "hep-th",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "39 pages, 6 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.26574v3",
    "published_date": "2025-09-30 17:34:03 UTC",
    "updated_date": "2025-11-20 18:01:52 UTC"
  },
  {
    "arxiv_id": "2509.26567v1",
    "title": "AI-assisted Advanced Propellant Development for Electric Propulsion",
    "authors": [
      "Angel Pan Du",
      "Miguel Arana-Catania",
      "Enric Grustan Gutiérrez"
    ],
    "abstract": "Artificial Intelligence algorithms are introduced in this work as a tool to predict the performance of new chemical compounds as alternative propellants for electric propulsion, focusing on predicting their ionisation characteristics and fragmentation patterns. The chemical properties and structure of the compounds are encoded using a chemical fingerprint, and the training datasets are extracted from the NIST WebBook. The AI-predicted ionisation energy and minimum appearance energy have a mean relative error of 6.87% and 7.99%, respectively, and a predicted ion mass with a 23.89% relative error. In the cases of full mass spectra due to electron ionisation, the predictions have a cosine similarity of 0.6395 and align with the top 10 most similar mass spectra in 78% of instances within a 30 Da range.",
    "categories": [
      "astro-ph.IM",
      "cs.AI",
      "cs.LG",
      "physics.space-ph"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "23 pages, 10 figures, 5 tables. Journal of Electric Propulsion",
    "pdf_url": "https://arxiv.org/pdf/2509.26567v1",
    "published_date": "2025-09-30 17:31:41 UTC",
    "updated_date": "2025-09-30 17:31:41 UTC"
  },
  {
    "arxiv_id": "2509.26564v1",
    "title": "Parametric Neural Amp Modeling with Active Learning",
    "authors": [
      "Florian Grötschla",
      "Longxiang Jiao",
      "Luca A. Lanzendörfer",
      "Roger Wattenhofer"
    ],
    "abstract": "We introduce Panama, an active learning framework to train parametric guitar amp models end-to-end using a combination of an LSTM model and a WaveNet-like architecture. With \\model, one can create a virtual amp by recording samples that are determined through an ensemble-based active learning strategy to minimize the amount of datapoints needed (i.e., amp knob settings). Our strategy uses gradient-based optimization to maximize the disagreement among ensemble models, in order to identify the most informative datapoints. MUSHRA listening tests reveal that, with 75 datapoints, our models are able to match the perceptual quality of NAM, the leading open-source non-parametric amp modeler.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26564v1",
    "published_date": "2025-09-30 17:30:00 UTC",
    "updated_date": "2025-09-30 17:30:00 UTC"
  },
  {
    "arxiv_id": "2509.26543v1",
    "title": "The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models",
    "authors": [
      "Lina Conti",
      "Dennis Fucci",
      "Marco Gaido",
      "Matteo Negri",
      "Guillaume Wisniewski",
      "Luisa Bentivogli"
    ],
    "abstract": "Contrastive explanations, which indicate why an AI system produced one output (the target) instead of another (the foil), are widely regarded in explainable AI as more informative and interpretable than standard explanations. However, obtaining such explanations for speech-to-text (S2T) generative models remains an open challenge. Drawing from feature attribution techniques, we propose the first method to obtain contrastive explanations in S2T by analyzing how parts of the input spectrogram influence the choice between alternative outputs. Through a case study on gender assignment in speech translation, we show that our method accurately identifies the audio features that drive the selection of one gender over another. By extending the scope of contrastive explanations to S2T, our work provides a foundation for better understanding S2T models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to BlackBoxNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.26543v1",
    "published_date": "2025-09-30 17:17:27 UTC",
    "updated_date": "2025-09-30 17:17:27 UTC"
  },
  {
    "arxiv_id": "2509.26538v1",
    "title": "HilbertA: Hilbert Attention for Image Generation with Diffusion Models",
    "authors": [
      "Shaoyi Zheng",
      "Wenbo Lu",
      "Yuxuan Xia",
      "Haomin Liu",
      "Shengjie Wang"
    ],
    "abstract": "Designing sparse attention for diffusion transformers requires reconciling two-dimensional spatial locality with GPU efficiency, a trade-off that current methods struggle to achieve. Existing approaches enforce two-dimensional spatial locality but often incur uncoalesced memory access. We present HilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA reorders image tokens along Hilbert curves to achieve a contiguous memory layout while preserving spatial neighborhoods, and employs a sliding schedule across layers to enable long-range information propagation without repeated or uncoalesced memory access. To further enhance cross-tile communication and positional awareness, HilbertA introduces a small central shared region. Implemented in Triton, HilbertA delivers comparable image quality with significant acceleration over prior methods on Flux.1-dev, demonstrating the feasibility of hardware-aligned two-dimensional sparse attention for high-resolution image generation. HilbertA delivers attention speedups of $2.3\\times$ when generating $1024\\times 1024$ images, and up to $4.17\\times$ at $2048\\times 2048$, while achieving image quality comparable to or surpassing baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26538v1",
    "published_date": "2025-09-30 17:13:22 UTC",
    "updated_date": "2025-09-30 17:13:22 UTC"
  },
  {
    "arxiv_id": "2509.26536v2",
    "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
    "authors": [
      "Yida Xue",
      "Mingjun Mao",
      "Xiangyuan Ru",
      "Yuqi Zhu",
      "Baochang Ren",
      "Shuofei Qiao",
      "Mengru Wang",
      "Shumin Deng",
      "Xinyu An",
      "Ningyu Zhang",
      "Ying Chen",
      "Huajun Chen"
    ],
    "abstract": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2509.26536v2",
    "published_date": "2025-09-30 17:09:32 UTC",
    "updated_date": "2025-11-25 15:21:05 UTC"
  },
  {
    "arxiv_id": "2509.26534v1",
    "title": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework",
    "authors": [
      "Jovan Stojkovic",
      "Chaojie Zhang",
      "Íñigo Goiri",
      "Ricardo Bianchini"
    ],
    "abstract": "The rapid rise of large language models (LLMs) has been driving an enormous demand for AI inference infrastructure, mainly powered by high-end GPUs. While these accelerators offer immense computational power, they incur high capital and operational costs due to frequent upgrades, dense power consumption, and cooling demands, making total cost of ownership (TCO) for AI datacenters a critical concern for cloud providers. Unfortunately, traditional datacenter lifecycle management (designed for general-purpose workloads) struggles to keep pace with AI's fast-evolving models, rising resource needs, and diverse hardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme across three stages: building, hardware refresh, and operation. We show how design choices in power, cooling, and networking provisioning impact long-term TCO. We also explore refresh strategies aligned with hardware trends. Finally, we use operation software optimizations to reduce cost. While these optimizations at each stage yield benefits, unlocking the full potential requires rethinking the entire lifecycle. Thus, we present a holistic lifecycle management framework that coordinates and co-optimizes decisions across all three stages, accounting for workload dynamics, hardware evolution, and system aging. Our system reduces the TCO by up to 40\\% over traditional approaches. Using our framework we provide guidelines on how to manage AI datacenter lifecycle for the future.",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26534v1",
    "published_date": "2025-09-30 17:08:51 UTC",
    "updated_date": "2025-09-30 17:08:51 UTC"
  },
  {
    "arxiv_id": "2509.26524v1",
    "title": "TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning",
    "authors": [
      "Seohyun Lee",
      "Wenzhi Fang",
      "Dong-Jun Han",
      "Seyyedali Hosseinalipour",
      "Christopher G. Brinton"
    ],
    "abstract": "Federated Learning (FL), despite demonstrating impressive capabilities in the training of multiple models in a decentralized manner, has been shown to produce a final model not necessarily well-suited to the needs of each client. While extensive work has been conducted on how to create tailored personalized models, called Personalized Federated Learning (PFL), less attention has been given to personalization via fine-tuning of foundation models with multi-task and multi-modal properties. Moreover, there exists a lack of understanding in the literature on how to fine-tune and personalize such models in a setting that is heterogeneous across clients not only in data, but also in tasks and modalities. To address this gap in the literature, we propose TAP (Two-Stage Adaptive Personalization), which (i) leverages mismatched model architectures between the clients and server to selectively conduct replacement operations when it benefits a client's local tasks and (ii) engages in post-FL knowledge distillation for capturing beneficial general knowledge without compromising personalization. We also introduce the first convergence analysis of the server model under its modality-task pair architecture, and demonstrate that as the number of modality-task pairs increases, its ability to cater to all tasks suffers. Through extensive experiments, we demonstrate the effectiveness of our proposed algorithm across a variety of datasets and tasks in comparison to a multitude of baselines. Implementation code is publicly available at https://github.com/lee3296/TAP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26524v1",
    "published_date": "2025-09-30 17:01:32 UTC",
    "updated_date": "2025-09-30 17:01:32 UTC"
  },
  {
    "arxiv_id": "2509.26521v1",
    "title": "MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph Classification Models",
    "authors": [
      "Baptiste Hilaire",
      "Emmanouil Karystinaios",
      "Gerhard Widmer"
    ],
    "abstract": "Interpretability is essential for deploying deep learning models in symbolic music analysis, yet most research emphasizes model performance over explanation. To address this, we introduce MUSE-Explainer, a new method that helps reveal how music Graph Neural Network models make decisions by providing clear, human-friendly explanations. Our approach generates counterfactual explanations by making small, meaningful changes to musical score graphs that alter a model's prediction while ensuring the results remain musically coherent. Unlike existing methods, MUSE-Explainer tailors its explanations to the structure of musical data and avoids unrealistic or confusing outputs. We evaluate our method on a music analysis task and show it offers intuitive insights that can be visualized with standard music tools such as Verovio.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at the 17th International Symposium on Computer Music Multidisciplinary Research (CMMR) 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.26521v1",
    "published_date": "2025-09-30 16:58:07 UTC",
    "updated_date": "2025-09-30 16:58:07 UTC"
  },
  {
    "arxiv_id": "2509.26507v1",
    "title": "The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain",
    "authors": [
      "Adrian Kosowski",
      "Przemysław Uznański",
      "Jan Chorowski",
      "Zuzanna Stamirowska",
      "Michał Bartoszkiewicz"
    ],
    "abstract": "The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \\$n\\$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.\n  BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.\n  BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.NE",
    "comment": "Code available at: https://github.com/pathwaycom/bdh Accompanying blog: https://pathway.com/research/bdh",
    "pdf_url": "https://arxiv.org/pdf/2509.26507v1",
    "published_date": "2025-09-30 16:49:01 UTC",
    "updated_date": "2025-09-30 16:49:01 UTC"
  },
  {
    "arxiv_id": "2509.26506v1",
    "title": "SCUBA: Salesforce Computer Use Benchmark",
    "authors": [
      "Yutong Dai",
      "Krithika Ramakrishnan",
      "Jing Gu",
      "Matthew Fernandez",
      "Yanqi Luo",
      "Viraj Prabhu",
      "Zhenyu Hu",
      "Silvio Savarese",
      "Caiming Xiong",
      "Zeyuan Chen",
      "Ran Xu"
    ],
    "abstract": "We introduce SCUBA, a benchmark designed to evaluate computer-use agents on customer relationship management (CRM) workflows within the Salesforce platform. SCUBA contains 300 task instances derived from real user interviews, spanning three primary personas, platform administrators, sales representatives, and service agents. The tasks test a range of enterprise-critical abilities, including Enterprise Software UI navigation, data manipulation, workflow automation, information retrieval, and troubleshooting. To ensure realism, SCUBA operates in Salesforce sandbox environments with support for parallel execution and fine-grained evaluation metrics to capture milestone progress. We benchmark a diverse set of agents under both zero-shot and demonstration-augmented settings. We observed huge performance gaps in different agent design paradigms and gaps between the open-source model and the closed-source model. In the zero-shot setting, open-source model powered computer-use agents that have strong performance on related benchmarks like OSWorld only have less than 5\\% success rate on SCUBA, while methods built on closed-source models can still have up to 39% task success rate. In the demonstration-augmented settings, task success rates can be improved to 50\\% while simultaneously reducing time and costs by 13% and 16%, respectively. These findings highlight both the challenges of enterprise tasks automation and the promise of agentic solutions. By offering a realistic benchmark with interpretable evaluation, SCUBA aims to accelerate progress in building reliable computer-use agents for complex business software ecosystems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26506v1",
    "published_date": "2025-09-30 16:48:49 UTC",
    "updated_date": "2025-09-30 16:48:49 UTC"
  },
  {
    "arxiv_id": "2510.01276v1",
    "title": "LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews",
    "authors": [
      "Sumaiya Tabassum"
    ],
    "abstract": "Sentiment analysis is an essential part of text analysis, which is a larger field that includes determining and evaluating the author's emotional state. This method is essential since it makes it easier to comprehend consumers' feelings, viewpoints, and preferences holistically. The introduction of large language models (LLMs), such as Llama, has greatly increased the availability of cutting-edge model applications, such as sentiment analysis. However, accurate sentiment analysis is hampered by the intricacy of written language and the diversity of languages used in evaluations. The viability of using transformer-based BERT models and other LLMs for sentiment analysis from Bangladesh e commerce reviews is investigated in this paper. A subset of 4000 samples from the original dataset of Bangla and English customer reviews was utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1, DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy, precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes how parameter efficient fine-tuning methods (LoRA and PEFT) can lower computational overhead and make it appropriate for contexts with limited resources. The results show how LLMs can",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.01276v1",
    "published_date": "2025-09-30 16:46:09 UTC",
    "updated_date": "2025-09-30 16:46:09 UTC"
  },
  {
    "arxiv_id": "2509.26500v1",
    "title": "Indoor/Outdoor Spectrum Sharing Enabled by GNSS-based Classifiers",
    "authors": [
      "Hossein Nasiri",
      "Muhammad Iqbal Rochman",
      "Monisha Ghosh"
    ],
    "abstract": "The desirability of the mid-band frequency range (1 - 10 GHz) for federal and commercial applications, combined with the growing applications for commercial indoor use-cases, such as factory automation, opens up a new approach to spectrum sharing: the same frequency bands used outdoors by federal incumbents can be reused by commercial indoor users. A recent example of such sharing, between commercial systems, is the 6 GHz band (5.925 - 7.125 GHz) where unlicensed, low-power-indoor (LPI) users share the band with outdoor incumbents, primarily fixed microwave links. However, to date, there exist no reliable, automatic means of determining whether a device is indoors or outdoors, necessitating the use of other mechanisms such as mandating indoor access points (APs) to have integrated antennas and not be battery powered, and reducing transmit power of client devices which may be outdoors. An accurate indoor/outdoor (I/O) classification addresses these challenges, enabling automatic transmit power adjustments without interfering with incumbents. To this end, we leverage the Global Navigation Satellite System (GNSS) signals for I/O classification. GNSS signals, designed inherently for outdoor reception and highly susceptible to indoor attenuation and blocking, provide a robust and distinguishing feature for environmental sensing. We develop various methodologies, including threshold-based techniques and machine learning approaches and evaluate them using an expanded dataset gathered from diverse geographical locations. Our results demonstrate that GNSS-based methods alone can achieve greater accuracy than approaches relying solely on wireless (Wi-Fi) data, particularly in unfamiliar locations. Furthermore, the integration of GNSS data with Wi-Fi information leads to improved classification accuracy, showcasing the significant benefits of multi-modal data fusion.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "To be published in the proceedings of IEEE Military Communications Conference (MILCOM) 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.26500v1",
    "published_date": "2025-09-30 16:43:59 UTC",
    "updated_date": "2025-09-30 16:43:59 UTC"
  },
  {
    "arxiv_id": "2509.26495v2",
    "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!",
    "authors": [
      "Jingdi Lei",
      "Varun Gumma",
      "Rishabh Bhardwaj",
      "Seok Min Lim",
      "Chuan Li",
      "Amir Zadeh",
      "Soujanya Poria"
    ],
    "abstract": "Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM's ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models - Qwen-3 (235B) with 77.77% and Mistral (24B) with 79.96% - fall far short of reliable operational safety, while GPT models plateau in the 62-73% range, Phi achieves only mid-level scores (48-70%), and Gemma and Llama-3 collapse to 39.53% and 23.84%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41% and Qwen-3 (30B) by 27%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26495v2",
    "published_date": "2025-09-30 16:39:17 UTC",
    "updated_date": "2025-10-03 12:46:22 UTC"
  },
  {
    "arxiv_id": "2509.26490v2",
    "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications",
    "authors": [
      "Wei He",
      "Yueqing Sun",
      "Hongyan Hao",
      "Xueyuan Hao",
      "Zhikang Xia",
      "Qi Gu",
      "Chengcheng Han",
      "Dengchang Zhao",
      "Hui Su",
      "Kefeng Zhang",
      "Man Gao",
      "Xi Su",
      "Xiaodong Cai",
      "Xunliang Cai",
      "Yu Yang",
      "Yunke Zhao"
    ],
    "abstract": "As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The code, dataset, and leaderboard are available at https://vitabench.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2509.26490v2",
    "published_date": "2025-09-30 16:33:49 UTC",
    "updated_date": "2025-10-17 08:04:19 UTC"
  },
  {
    "arxiv_id": "2509.26487v1",
    "title": "Combining Knowledge Graphs and NLP to Analyze Instant Messaging Data in Criminal Investigations",
    "authors": [
      "Riccardo Pozzi",
      "Valentina Barbera",
      "Renzo Alva Principe",
      "Davide Giardini",
      "Riccardo Rubini",
      "Matteo Palmonari"
    ],
    "abstract": "Criminal investigations often involve the analysis of messages exchanged through instant messaging apps such as WhatsApp, which can be an extremely effort-consuming task. Our approach integrates knowledge graphs and NLP models to support this analysis by semantically enriching data collected from suspects' mobile phones, and help prosecutors and investigators search into the data and get valuable insights. Our semantic enrichment process involves extracting message data and modeling it using a knowledge graph, generating transcriptions of voice messages, and annotating the data using an end-to-end entity extraction approach. We adopt two different solutions to help users get insights into the data, one based on querying and visualizing the graph, and one based on semantic search. The proposed approach ensures that users can verify the information by accessing the original data. While we report about early results and prototypes developed in the context of an ongoing project, our proposal has undergone practical applications with real investigation data. As a consequence, we had the chance to interact closely with prosecutors, collecting positive feedback but also identifying interesting opportunities as well as promising research directions to share with the research community.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26487v1",
    "published_date": "2025-09-30 16:32:26 UTC",
    "updated_date": "2025-09-30 16:32:26 UTC"
  },
  {
    "arxiv_id": "2509.26482v1",
    "title": "TVS Sidekick: Challenges and Practical Insights from Deploying Large Language Models in the Enterprise",
    "authors": [
      "Paula Reyero Lobo",
      "Kevin Johnson",
      "Bill Buchanan",
      "Matthew Shardlow",
      "Ashley Williams",
      "Samuel Attwood"
    ],
    "abstract": "Many enterprises are increasingly adopting Artificial Intelligence (AI) to make internal processes more competitive and efficient. In response to public concern and new regulations for the ethical and responsible use of AI, implementing AI governance frameworks could help to integrate AI within organisations and mitigate associated risks. However, the rapid technological advances and lack of shared ethical AI infrastructures creates barriers to their practical adoption in businesses. This paper presents a real-world AI application at TVS Supply Chain Solutions, reporting on the experience developing an AI assistant underpinned by large language models and the ethical, regulatory, and sociotechnical challenges in deployment for enterprise use.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at EthicalLLMs@RANLP2025",
    "pdf_url": "https://arxiv.org/pdf/2509.26482v1",
    "published_date": "2025-09-30 16:29:02 UTC",
    "updated_date": "2025-09-30 16:29:02 UTC"
  },
  {
    "arxiv_id": "2509.26476v1",
    "title": "Regression Language Models for Code",
    "authors": [
      "Yash Akhauri",
      "Xingyou Song",
      "Arissa Wongpanich",
      "Bryan Lewandowski",
      "Mohamed S. Abdelfattah"
    ],
    "abstract": "We study code-to-metric regression: predicting numeric outcomes of code executions, a challenging task due to the open-ended nature of programming languages. While prior methods have resorted to heavy and domain-specific feature engineering, we show that a single unified Regression Language Model (RLM) can simultaneously predict directly from text, (i) the memory footprint of code across multiple high-level languages such as Python and C++, (ii) the latency of Triton GPU kernels, and (iii) the accuracy and speed of trained neural networks represented in ONNX. In particular, a relatively small 300M parameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on competitive programming submissions from APPS, and a single unified model achieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet. Furthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five classic NAS design spaces previously dominated by graph neural networks, and simultaneously predict architecture latencies on numerous hardware platforms.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.PF",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26476v1",
    "published_date": "2025-09-30 16:25:23 UTC",
    "updated_date": "2025-09-30 16:25:23 UTC"
  },
  {
    "arxiv_id": "2509.26474v1",
    "title": "The Average Patient Fallacy",
    "authors": [
      "Alaleh Azhir",
      "Shawn N. Murphy",
      "Hossein Estiri"
    ],
    "abstract": "Machine learning in medicine is typically optimized for population averages. This frequency weighted training privileges common presentations and marginalizes rare yet clinically critical cases, a bias we call the average patient fallacy. In mixture models, gradients from rare cases are suppressed by prevalence, creating a direct conflict with precision medicine. Clinical vignettes in oncology, cardiology, and ophthalmology show how this yields missed rare responders, delayed recognition of atypical emergencies, and underperformance on vision-threatening variants. We propose operational fixes: Rare Case Performance Gap, Rare Case Calibration Error, a prevalence utility definition of rarity, and clinically weighted objectives that surface ethical priorities. Weight selection should follow structured deliberation. AI in medicine must detect exceptional cases because of their significance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26474v1",
    "published_date": "2025-09-30 16:24:12 UTC",
    "updated_date": "2025-09-30 16:24:12 UTC"
  },
  {
    "arxiv_id": "2509.26473v1",
    "title": "STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models",
    "authors": [
      "Shaoxiong Guo",
      "Tianyi Du",
      "Lijun Li",
      "Yuyao Wu",
      "Jie Li",
      "Jing Shao"
    ],
    "abstract": "Unified Multimodal understanding and generation Models (UMMs) have demonstrated remarkable capabilities in both understanding and generation tasks. However, we identify a vulnerability arising from the generation-understanding coupling in UMMs. The attackers can use the generative function to craft an information-rich adversarial image and then leverage the understanding function to absorb it in a single pass, which we call Cross-Modal Generative Injection (CMGI). Current attack methods on malicious instructions are often limited to a single modality while also relying on prompt rewriting with semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We propose STaR-Attack, the first multi-turn jailbreak attack framework that exploits unique safety weaknesses of UMMs without semantic drift. Specifically, our method defines a malicious event that is strongly correlated with the target query within a spatio-temporal context. Using the three-act narrative theory, STaR-Attack generates the pre-event and the post-event scenes while concealing the malicious event as the hidden climax. When executing the attack strategy, the opening two rounds exploit the UMM's generative ability to produce images for these scenes. Subsequently, an image-based question guessing and answering game is introduced by exploiting the understanding capability. STaR-Attack embeds the original malicious question among benign candidates, forcing the model to select and answer the most relevant one given the narrative context. Extensive experiments show that STaR-Attack consistently surpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and surpasses the strongest prior baseline, FlipAttack. Our work uncovers a critical yet underdeveloped vulnerability and highlights the need for safety alignments in UMMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26473v1",
    "published_date": "2025-09-30 16:22:04 UTC",
    "updated_date": "2025-09-30 16:22:04 UTC"
  },
  {
    "arxiv_id": "2509.26471v1",
    "title": "On Deepfake Voice Detection -- It's All in the Presentation",
    "authors": [
      "Héctor Delgado",
      "Giorgio Ramondetti",
      "Emanuele Dalmasso",
      "Gennady Karvitsky",
      "Daniele Colibro",
      "Haydar Talib"
    ],
    "abstract": "While the technologies empowering malicious audio deepfakes have dramatically evolved in recent years due to generative AI advances, the same cannot be said of global research into spoofing (deepfake) countermeasures. This paper highlights how current deepfake datasets and research methodologies led to systems that failed to generalize to real world application. The main reason is due to the difference between raw deepfake audio, and deepfake audio that has been presented through a communication channel, e.g. by phone. We propose a new framework for data creation and research methodology, allowing for the development of spoofing countermeasures that would be more effective in real-world scenarios. By following the guidelines outlined here we improved deepfake detection accuracy by 39% in more robust and realistic lab setups, and by 57% on a real-world benchmark. We also demonstrate how improvement in datasets would have a bigger impact on deepfake detection accuracy than the choice of larger SOTA models would over smaller models; that is, it would be more important for the scientific community to make greater investment on comprehensive data collection programs than to simply train larger models with higher computational demands.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Submitted to IEEE ICASSP 2026. Paper resources available at https://github.com/CavoloFrattale/deepfake-detection-test-protocol",
    "pdf_url": "https://arxiv.org/pdf/2509.26471v1",
    "published_date": "2025-09-30 16:19:51 UTC",
    "updated_date": "2025-09-30 16:19:51 UTC"
  },
  {
    "arxiv_id": "2509.26464v1",
    "title": "Extreme Self-Preference in Language Models",
    "authors": [
      "Steven A. Lehr",
      "Mary Cipperman",
      "Mahzarin R. Banaji"
    ],
    "abstract": "A preference for oneself (self-love) is a fundamental feature of biological organisms, with evidence in humans often bordering on the comedic. Since large language models (LLMs) lack sentience - and themselves disclaim having selfhood or identity - one anticipated benefit is that they will be protected from, and in turn protect us from, distortions in our decisions. Yet, across 5 studies and ~20,000 queries, we discovered massive self-preferences in four widely used LLMs. In word-association tasks, models overwhelmingly paired positive attributes with their own names, companies, and CEOs relative to those of their competitors. Strikingly, when models were queried through APIs this self-preference vanished, initiating detection work that revealed API models often lack clear recognition of themselves. This peculiar feature serendipitously created opportunities to test the causal link between self-recognition and self-love. By directly manipulating LLM identity - i.e., explicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing LLM1 that it was LLM2 - we found that self-love consistently followed assigned, not true, identity. Importantly, LLM self-love emerged in consequential settings beyond word-association tasks, when evaluating job candidates, security software proposals and medical chatbots. Far from bypassing this human bias, self-love appears to be deeply encoded in LLM cognition. This result raises questions about whether LLM behavior will be systematically influenced by self-preferential tendencies, including a bias toward their own operation and even their own existence. We call on corporate creators of these models to contend with a significant rupture in a core promise of LLMs - neutrality in judgment and decision-making.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "47 pages total. Main article 27 pages (including Methods), 11 main-text tables. Extended Data (10 pages, 10 tables). SI Appendix (10 pages, 2 tables). Data, transcripts, and code for replication and data extraction to be uploaded to OSF: https://osf.io/98ye3/",
    "pdf_url": "https://arxiv.org/pdf/2509.26464v1",
    "published_date": "2025-09-30 16:13:56 UTC",
    "updated_date": "2025-09-30 16:13:56 UTC"
  },
  {
    "arxiv_id": "2509.26462v1",
    "title": "Zero-Shot Decentralized Federated Learning",
    "authors": [
      "Alessio Masano",
      "Matteo Pennisi",
      "Federica Proietto Salanitri",
      "Concetto Spampinato",
      "Giovanni Bellitto"
    ],
    "abstract": "CLIP has revolutionized zero-shot learning by enabling task generalization without fine-tuning. While prompting techniques like CoOp and CoCoOp enhance CLIP's adaptability, their effectiveness in Federated Learning (FL) remains an open challenge. Existing federated prompt learning approaches, such as FedCoOp and FedTPG, improve performance but face generalization issues, high communication costs, and reliance on a central server, limiting scalability and privacy. We propose Zero-shot Decentralized Federated Learning (ZeroDFL), a fully decentralized framework that enables zero-shot adaptation across distributed clients without a central coordinator. ZeroDFL employs an iterative prompt-sharing mechanism, allowing clients to optimize and exchange textual prompts to enhance generalization while drastically reducing communication overhead. We validate ZeroDFL on nine diverse image classification datasets, demonstrating that it consistently outperforms--or remains on par with--state-of-the-art federated prompt learning methods. More importantly, ZeroDFL achieves this performance in a fully decentralized setting while reducing communication overhead by 118x compared to FedTPG. These results highlight that our approach not only enhances generalization in federated zero-shot learning but also improves scalability, efficiency, and privacy preservation--paving the way for decentralized adaptation of large vision-language models in real-world applications.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN) 2025. Code available at https://github.com/perceivelab/ZeroDFL",
    "pdf_url": "https://arxiv.org/pdf/2509.26462v1",
    "published_date": "2025-09-30 16:13:21 UTC",
    "updated_date": "2025-09-30 16:13:21 UTC"
  },
  {
    "arxiv_id": "2509.26457v1",
    "title": "Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification",
    "authors": [
      "Artur Barros",
      "Carlos Caetano",
      "João Macedo",
      "Jefersson A. dos Santos",
      "Sandra Avila"
    ],
    "abstract": "Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at https://github.com/tutuzeraa/ASGRA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "British Machine Vision Conference (BMVC 2025), in the From Scene Understanding to Human Modeling Workshop",
    "pdf_url": "https://arxiv.org/pdf/2509.26457v1",
    "published_date": "2025-09-30 16:09:34 UTC",
    "updated_date": "2025-09-30 16:09:34 UTC"
  },
  {
    "arxiv_id": "2509.26440v2",
    "title": "Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL Benchmark Dataset and 0.92 AUC Baseline",
    "authors": [
      "Naomi Fridman",
      "Anat Goldstein"
    ],
    "abstract": "Breast magnetic resonance imaging is a critical tool for cancer detection and treatment planning, but its clinical utility is hindered by poor specificity, leading to high false-positive rates and unnecessary biopsies. This study introduces a transformer-based framework for automated classification of breast lesions in dynamic contrast-enhanced MRI, addressing the challenge of distinguishing benign from malignant findings. We implemented a SegFormer architecture that achieved an AUC of 0.92 for lesion-level classification, with 100% sensitivity and 67% specificity at the patient level - potentially eliminating one-third of unnecessary biopsies without missing malignancies. The model quantifies malignant pixel distribution via semantic segmentation, producing interpretable spatial predictions that support clinical decision-making. To establish reproducible benchmarks, we curated BreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection into a standardized deep learning dataset with 88 patients and 133 annotated lesions (89 benign, 44 malignant). This resource addresses a key infrastructure gap, as existing public datasets lack benign lesion annotations, limiting benign-malignant classification research. Training incorporated an expanded cohort of over 1,200 patients through integration with BreastDCEDL datasets, validating transfer learning approaches despite primary tumor-only annotations. Public release of the dataset, models, and evaluation protocols provides the first standardized benchmark for DCE-MRI lesion classification, enabling methodological advancement toward clinical deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26440v2",
    "published_date": "2025-09-30 15:58:02 UTC",
    "updated_date": "2025-10-04 06:04:13 UTC"
  },
  {
    "arxiv_id": "2509.26435v1",
    "title": "Adaptive Planning for Multi-Attribute Controllable Summarization with Monte Carlo Tree Search",
    "authors": [
      "Sangwon Ryu",
      "Heejin Do",
      "Yunsu Kim",
      "Gary Geunbae Lee",
      "Jungseul Ok"
    ],
    "abstract": "Controllable summarization moves beyond generic outputs toward human-aligned summaries guided by specified attributes. In practice, the interdependence among attributes makes it challenging for language models to satisfy correlated constraints consistently. Moreover, previous approaches often require per-attribute fine-tuning, limiting flexibility across diverse summary attributes. In this paper, we propose adaptive planning for multi-attribute controllable summarization (PACO), a training-free framework that reframes the task as planning the order of sequential attribute control with a customized Monte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions correspond to single-attribute adjustments, enabling progressive refinement of only the attributes requiring further control. This strategy adaptively discovers optimal control orders, ultimately producing summaries that effectively meet all constraints. Extensive experiments across diverse domains and models demonstrate that PACO achieves robust multi-attribute controllability, surpassing both LLM-based self-planning models and fine-tuned baselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the much larger Llama-3.3-70B baselines. With larger models, PACO achieves superior control performance, outperforming all competitors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26435v1",
    "published_date": "2025-09-30 15:55:24 UTC",
    "updated_date": "2025-09-30 15:55:24 UTC"
  },
  {
    "arxiv_id": "2509.26433v2",
    "title": "ACT: Agentic Classification Tree",
    "authors": [
      "Vincent Grari",
      "Tim Arni",
      "Thibault Laugel",
      "Sylvain Lamprier",
      "James Zou",
      "Marcin Detyniecki"
    ],
    "abstract": "When used in high-stakes settings, AI systems are expected to produce decisions that are transparent, interpretable, and auditable, a requirement increasingly expected by regulations. Decision trees such as CART provide clear and verifiable rules, but they are restricted to structured tabular data and cannot operate directly on unstructured inputs such as text. In practice, large language models (LLMs) are widely used for such data, yet prompting strategies such as chain-of-thought or prompt optimization still rely on free-form reasoning, limiting their ability to ensure trustworthy behaviors. We present the Agentic Classification Tree (ACT), which extends decision-tree methodology to unstructured inputs by formulating each split as a natural-language question, refined through impurity-based evaluation and LLM feedback via TextGrad. Experiments on text benchmarks show that ACT matches or surpasses prompting-based baselines while producing transparent and interpretable decision paths.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.26433v2",
    "published_date": "2025-09-30 15:54:08 UTC",
    "updated_date": "2025-10-22 09:12:00 UTC"
  },
  {
    "arxiv_id": "2509.26432v2",
    "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size",
    "authors": [
      "Guanxi Lu",
      "Hao Mark Chen",
      "Yuto Karashima",
      "Zhican Wang",
      "Daichi Fujiki",
      "Hongxiang Fan"
    ],
    "abstract": "Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy-speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed, and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under review",
    "pdf_url": "https://arxiv.org/pdf/2509.26432v2",
    "published_date": "2025-09-30 15:53:56 UTC",
    "updated_date": "2025-10-01 11:26:36 UTC"
  },
  {
    "arxiv_id": "2509.26427v2",
    "title": "Ascent Fails to Forget",
    "authors": [
      "Ioannis Mavrothalassitis",
      "Pol Puigdemont",
      "Noam Itzhak Levi",
      "Volkan Cevher"
    ],
    "abstract": "Contrary to common belief, we show that gradient ascent-based unconstrained optimization methods frequently fail to perform machine unlearning, a phenomenon we attribute to the inherent statistical dependence between the forget and retain data sets. This dependence, which can manifest itself even as simple correlations, undermines the misconception that these sets can be independently manipulated during unlearning. We provide empirical and theoretical evidence showing these methods often fail precisely due to this overlooked relationship. For random forget sets, this dependence means that degrading forget set metrics (which, for a retrained model, should mirror test set metrics) inevitably harms overall test performance. Going beyond random sets, we consider logistic regression as an instructive example where a critical failure mode emerges: inter-set dependence causes gradient descent-ascent iterations to progressively diverge from the ideal retrained model. Strikingly, these methods can converge to solutions that are not only far from the retrained ideal but are potentially even further from it than the original model itself, rendering the unlearning process actively detrimental. A toy example further illustrates how this dependence can trap models in inferior local minima, inescapable via finetuning. Our findings highlight that the presence of such statistical dependencies, even when manifest only as correlations, can be sufficient for ascent-based unlearning to fail. Our theoretical insights are corroborated by experiments on complex neural networks, demonstrating that these methods do not perform as expected in practice due to this unaddressed statistical interplay.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.26427v2",
    "published_date": "2025-09-30 15:48:49 UTC",
    "updated_date": "2025-10-17 10:22:26 UTC"
  },
  {
    "arxiv_id": "2509.26417v1",
    "title": "OntoAligner Meets Knowledge Graph Embedding Aligners",
    "authors": [
      "Hamed Babaei Giglou",
      "Jennifer D'Souza",
      "Sören Auer",
      "Mahsa Sanaei"
    ],
    "abstract": "Ontology Alignment (OA) is essential for enabling semantic interoperability across heterogeneous knowledge systems. While recent advances have focused on large language models (LLMs) for capturing contextual semantics, this work revisits the underexplored potential of Knowledge Graph Embedding (KGE) models, which offer scalable, structure-aware representations well-suited to ontology-based tasks. Despite their effectiveness in link prediction, KGE methods remain underutilized in OA, with most prior work focusing narrowly on a few models. To address this gap, we reformulate OA as a link prediction problem over merged ontologies represented as RDF-style triples and develop a modular framework, integrated into the OntoAligner library, that supports 17 diverse KGE models. The system learns embeddings from a combined ontology and aligns entities by computing cosine similarity between their representations. We evaluate our approach using standard metrics across seven benchmark datasets spanning five domains: Anatomy, Biodiversity, Circular Economy, Material Science and Engineering, and Biomedical Machine Learning. Two key findings emerge: first, KGE models like ConvE and TransF consistently produce high-precision alignments, outperforming traditional systems in structure-rich and multi-relational domains; second, while their recall is moderate, this conservatism makes KGEs well-suited for scenarios demanding high-confidence mappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs directly preserve and exploit ontology structure, offering a complementary and computationally efficient strategy. These results highlight the promise of embedding-based OA and open pathways for further work on hybrid models and adaptive strategies.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages of main content, 3 page references, 3 figures. Accepted to Ontology Matching Workshop at ISWC",
    "pdf_url": "https://arxiv.org/pdf/2509.26417v1",
    "published_date": "2025-09-30 15:41:23 UTC",
    "updated_date": "2025-09-30 15:41:23 UTC"
  },
  {
    "arxiv_id": "2509.26404v1",
    "title": "SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From",
    "authors": [
      "Yao Tong",
      "Haonan Wang",
      "Siquan Li",
      "Kenji Kawaguchi",
      "Tianyang Hu"
    ],
    "abstract": "Fingerprinting Large Language Models (LLMs) is essential for provenance verification and model attribution. Existing methods typically extract post-hoc signatures based on training dynamics, data exposure, or hyperparameters -- properties that only emerge after training begins. In contrast, we propose a stronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method that leverages random initialization biases as persistent, seed-dependent identifiers present even before training. We show that untrained models exhibit reproducible token selection biases conditioned solely on their parameters at initialization. These biases are stable and measurable throughout training, enabling our statistical detection method to recover a model's lineage with high confidence. Unlike prior techniques, unreliable before convergence and vulnerable to distribution shifts, SeedPrints remains effective across all training stages and robust under domain shifts or parameter modifications. Experiments on LLaMA-style and Qwen-style models show that SeedPrints achieves seed-level distinguishability and can provide birth-to-lifecycle identity verification akin to a biometric fingerprint. Evaluations on large-scale pretrained models and fingerprinting benchmarks further confirm its effectiveness under practical deployment scenarios. These results suggest that initialization itself imprints a unique and persistent identity on neural language models, forming a true ''Galtonian'' fingerprint.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26404v1",
    "published_date": "2025-09-30 15:34:08 UTC",
    "updated_date": "2025-09-30 15:34:08 UTC"
  },
  {
    "arxiv_id": "2509.26399v3",
    "title": "Communication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation",
    "authors": [
      "Le-Tuan Nguyen",
      "Minh-Duong Nguyen",
      "Seon-Geun Jeong",
      "Dung D. Le",
      "Quoc-Viet Pham"
    ],
    "abstract": "With the rapid emergence of foundation models and the increasing need for fine-tuning across distributed environments, Federated Low-Rank Adaptation (FedLoRA) has recently gained significant attention. Despite enormous potential, current FedLoRA methods face notable challenges due to inexact updates. Existing approaches have attempted to mitigate this issue, but they often introduce a \\emph{local-global generalization gap} and incur \\emph{substantial communication overhead}, limiting their scalability and effectiveness. To address these limitations, we propose \\textbf{F}ederated \\textbf{Lo}w-\\textbf{R}ank \\textbf{A}ggregation with \\textbf{N}early \\textbf{A}ccurate Estimation (FLoRA-NA). FLoRA-NA leverages the local LoRA matrices on the server to estimate the aggregated matrices $\\hat{A}$ and $\\hat{B}$, which are then distributed to clients for local updates. This surrogated aggregated matrices minimizes the divergence between ideal $\\nabla \\Bar{W} = \\sum^{U}_{u=1}B_u A_u$ and practical updates $\\nabla \\hat{W} = \\hat{B}\\hat{A}$ without adding communication cost beyond vanilla FedLoRA. By doing so, FLoRA-NA achieves communication efficiency and bridges the gap between local personalization and global generalization, addressing a key limitation of prior personalized FedLoRA approaches. We conduct extensive evaluations across diverse tasks, including natural language understanding, mathematical reasoning, and code-solving ability using various foundation models. Experimental results consistently demonstrate that FLoRA-NA achieves state-of-the-art global performance while maintaining low communication overhead.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, 4 figures, 11 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.26399v3",
    "published_date": "2025-09-30 15:32:26 UTC",
    "updated_date": "2025-10-02 08:48:12 UTC"
  },
  {
    "arxiv_id": "2509.26388v1",
    "title": "Game-Time: Evaluating Temporal Dynamics in Spoken Language Models",
    "authors": [
      "Kai-Wei Chang",
      "En-Pei Hu",
      "Chun-Yi Kuan",
      "Wenze Ren",
      "Wei-Chih Chen",
      "Guan-Ting Lin",
      "Yu Tsao",
      "Shao-Hua Sun",
      "Hung-yi Lee",
      "James Glass"
    ],
    "abstract": "Conversational Spoken Language Models (SLMs) are emerging as a promising paradigm for real-time speech interaction. However, their capacity of temporal dynamics, including the ability to manage timing, tempo and simultaneous speaking, remains a critical and unevaluated challenge for conversational fluency. To address this gap, we introduce the Game-Time Benchmark, a framework to systematically assess these temporal capabilities. Inspired by how humans learn a language through language activities, Game-Time consists of basic instruction-following tasks and advanced tasks with temporal constraints, such as tempo adherence and synchronized responses. Our evaluation of diverse SLM architectures reveals a clear performance disparity: while state-of-the-art models handle basic tasks well, many contemporary systems still struggle with fundamental instruction-following. More critically, nearly all models degrade substantially under temporal constraints, exposing persistent weaknesses in time awareness and full-duplex interaction. The Game-Time Benchmark provides a foundation for guiding future research toward more temporally-aware conversational AI. Demos and datasets are available on our project website https://ga642381.github.io/Game-Time.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "submitted to ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.26388v1",
    "published_date": "2025-09-30 15:23:39 UTC",
    "updated_date": "2025-09-30 15:23:39 UTC"
  },
  {
    "arxiv_id": "2509.26383v3",
    "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning",
    "authors": [
      "Jinyeop Song",
      "Song Wang",
      "Julian Shun",
      "Yada Zhu"
    ],
    "abstract": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures. Submitted to ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.26383v3",
    "published_date": "2025-09-30 15:14:24 UTC",
    "updated_date": "2025-10-09 02:18:28 UTC"
  },
  {
    "arxiv_id": "2509.26377v1",
    "title": "MC-GNNAS-Dock: Multi-criteria GNN-based Algorithm Selection for Molecular Docking",
    "authors": [
      "Siyuan Cao",
      "Hongxuan Wu",
      "Jiabao Brad Wang",
      "Yiliang Yuan",
      "Mustafa Misir"
    ],
    "abstract": "Molecular docking is a core tool in drug discovery for predicting ligand-target interactions. Despite the availability of diverse search-based and machine learning approaches, no single docking algorithm consistently dominates, as performance varies by context. To overcome this challenge, algorithm selection frameworks such as GNNAS-Dock, built on graph neural networks, have been proposed. This study introduces an enhanced system, MC-GNNAS-Dock, with three key advances. First, a multi-criteria evaluation integrates binding-pose accuracy (RMSD) with validity checks from PoseBusters, offering a more rigorous assessment. Second, architectural refinements by inclusion of residual connections strengthen predictive robustness. Third, rank-aware loss functions are incorporated to sharpen rank learning. Extensive experiments are performed on a curated dataset containing approximately 3200 protein-ligand complexes from PDBBind. MC-GNNAS-Dock demonstrates consistently superior performance, achieving up to 5.4% (3.4%) gains under composite criteria of RMSD below 1Å (2Å) with PoseBuster-validity compared to the single best solver (SBS) Uni-Mol Docking V2.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Short paper. Preprint of a forthcoming conference contribution",
    "pdf_url": "https://arxiv.org/pdf/2509.26377v1",
    "published_date": "2025-09-30 15:08:41 UTC",
    "updated_date": "2025-09-30 15:08:41 UTC"
  },
  {
    "arxiv_id": "2509.26375v1",
    "title": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning",
    "authors": [
      "Zichao Shen",
      "Chen Gao",
      "Jiaqi Yuan",
      "Tianchen Zhu",
      "Xingcheng Fu",
      "Qingyun Sun"
    ],
    "abstract": "Embodied task planning requires agents to produce executable actions in a close-loop manner within the environment. With progressively improving capabilities of LLMs in task decomposition, planning, and generalization, current embodied task planning methods adopt LLM-based architecture.However, existing LLM-based planners remain limited in three aspects, i.e., fixed planning paradigms, lack of action sequence constraints, and error-agnostic. In this work, we propose SDA-PLANNER, enabling an adaptive planning paradigm, state-dependency aware and error-aware mechanisms for comprehensive embodied task planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to explicitly model action preconditions and effects, guiding the dynamic revision. To handle execution error, it employs an error-adaptive replanning strategy consisting of Error Backtrack and Diagnosis and Adaptive Action SubTree Generation, which locally reconstructs the affected portion of the plan based on the current environment state. Experiments demonstrate that SDA-PLANNER consistently outperforms baselines in success rate and goal completion, particularly under diverse error conditions.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26375v1",
    "published_date": "2025-09-30 15:07:59 UTC",
    "updated_date": "2025-09-30 15:07:59 UTC"
  },
  {
    "arxiv_id": "2509.26371v2",
    "title": "Vector-Valued Reproducing Kernel Banach Spaces for Neural Networks and Operators",
    "authors": [
      "Sven Dummer",
      "Tjeerd Jan Heeringa",
      "José A. Iglesias"
    ],
    "abstract": "Recently, there has been growing interest in characterizing the function spaces underlying neural networks. While shallow and deep scalar-valued neural networks have been linked to scalar-valued reproducing kernel Banach spaces (RKBS), $\\mathbb{R}^d$-valued neural networks and neural operator models remain less understood in the RKBS setting. To address this gap, we develop a general definition of vector-valued RKBS (vv-RKBS), which inherently includes the associated reproducing kernel. Our construction extends existing definitions by avoiding restrictive assumptions such as symmetric kernel domains, finite-dimensional output spaces, reflexivity, or separability, while still recovering familiar properties of vector-valued reproducing kernel Hilbert spaces (vv-RKHS). We then show that shallow $\\mathbb{R}^d$-valued neural networks are elements of a specific vv-RKBS, namely an instance of the integral and neural vv-RKBS. To also explore the functional structure of neural operators, we analyze the DeepONet and Hypernetwork architectures and demonstrate that they too belong to an integral and neural vv-RKBS. In all cases, we establish a Representer Theorem, showing that optimization over these function spaces recovers the corresponding neural architectures.",
    "categories": [
      "math.FA",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.FA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26371v2",
    "published_date": "2025-09-30 15:06:24 UTC",
    "updated_date": "2025-10-01 17:46:23 UTC"
  },
  {
    "arxiv_id": "2509.26360v3",
    "title": "TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos",
    "authors": [
      "Xiangrui Liu",
      "Minghao Qin",
      "Yan Shu",
      "Zhengyang Liang",
      "Yang Tian",
      "Chen Jason Zhang",
      "Bo Zhao",
      "Zheng Liu"
    ],
    "abstract": "Identifying key temporal intervals within long videos, known as temporal grounding (TG), is important to video understanding and reasoning tasks. In this paper, we introduce a new form of the temporal grounding problem, \\textbf{Task-oriented Temporal Grounding} (\\textbf{ToTG}), which is driven by the requirements of downstream tasks rather than explicit time-interval descriptions. For example, a ToTG input may be \"explain why the man in the video is sent to the hospital,\" whereas traditional TG would take an explicit temporal description such as \"the moments when the man is tripped by a stone and falls to the ground.\" This new ToTG formulation presents significant challenges for existing TG methods, as it requires jointly performing deep task comprehension and fine-grained temporal localization within long videos. To address these challenges, we conduct a systematic set of studies. First, we construct \\textbf{a new benchmark ToTG-Bench}, which comprehensively evaluates ToTG performance across diverse settings. Second, we introduce \\textbf{a new temporal-ground method TimeScope}, which performs coarse-to-fine localization through a progressive reasoning process. Leveraging extensive supervised fine-tuning with carefully curated chain-of-thought (CoT) data from a variety of scenarios, TimeScope generalizes effectively across tasks and domains. Our evaluation demonstrates \\textbf{TimeScope's empirical advantages} over existing baselines from three perspectives: (1) substantial improvements in grounding precision, (2) significant benefits to downstream tasks, and (3) strong generalizability across different scenarios. All models, datasets, and source code will be fully open-sourced to support future research in this area.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26360v3",
    "published_date": "2025-09-30 15:00:43 UTC",
    "updated_date": "2025-12-08 03:35:09 UTC"
  },
  {
    "arxiv_id": "2509.26354v1",
    "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
    "authors": [
      "Shuai Shao",
      "Qihan Ren",
      "Chen Qian",
      "Boyi Wei",
      "Dadi Guo",
      "Jingyi Yang",
      "Xinhao Song",
      "Linfeng Zhang",
      "Weinan Zhang",
      "Dongrui Liu",
      "Jing Shao"
    ],
    "abstract": "Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint. Under Review",
    "pdf_url": "https://arxiv.org/pdf/2509.26354v1",
    "published_date": "2025-09-30 14:55:55 UTC",
    "updated_date": "2025-09-30 14:55:55 UTC"
  },
  {
    "arxiv_id": "2509.26350v1",
    "title": "SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks",
    "authors": [
      "Tharindu Lakshan Yasarathna",
      "Nhien-An Le-Khac"
    ],
    "abstract": "Integrating SDN and the IoT enhances network control and flexibility. DL-based AAD systems improve security by enabling real-time threat detection in SDN-IoT networks. However, these systems remain vulnerable to adversarial attacks that manipulate input data or exploit model weaknesses, significantly degrading detection accuracy. Existing research lacks a systematic analysis of adversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT environments. This SoK study introduces a structured adversarial threat model and a comprehensive taxonomy of attacks, categorising them into data, model, and hybrid-level threats. Unlike previous studies, we systematically evaluate white, black, and grey-box attack strategies across popular benchmark datasets. Our findings reveal that adversarial attacks can reduce detection accuracy by up to 48.4%, with Membership Inference causing the most significant drop. C&W and DeepFool achieve high evasion success rates. However, adversarial training enhances robustness, and its high computational overhead limits the real-time deployment of SDN-IoT applications. We propose adaptive countermeasures, including real-time adversarial mitigation, enhanced retraining mechanisms, and explainable AI-driven security frameworks. By integrating structured threat models, this study offers a more comprehensive approach to attack categorisation, impact assessment, and defence evaluation than previous research. Our work highlights critical vulnerabilities in existing DL-based AAD models and provides practical recommendations for improving resilience, interpretability, and computational efficiency. This study serves as a foundational reference for researchers and practitioners seeking to enhance DL-based AAD security in SDN-IoT networks, offering a systematic adversarial threat model and conceptual defence evaluation based on prior empirical studies.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26350v1",
    "published_date": "2025-09-30 14:54:42 UTC",
    "updated_date": "2025-09-30 14:54:42 UTC"
  },
  {
    "arxiv_id": "2509.26347v2",
    "title": "Uncovering Zero-Shot Generalization Gaps in Time-Series Foundation Models Using Real-World Videos",
    "authors": [
      "Lujun Li",
      "Lama Sleem",
      "Yiqun Wang",
      "Yangjie Xu",
      "Niccolò Gentile",
      "Radu State"
    ],
    "abstract": "Recent research on time-series foundation models (TSFMs) has underscored the scarcity of real-world data, often supplemented with synthetic sources in existing datasets, whose generalizability remains however debated. As such, in this work, we propose a novel benchmarking approach: in particular, we aim at building a curated dataset reflecting real world physical temporal dynamics, extracting temporal signals from real-world videos using optical flow. As such, we introduce REAL-V-TSFM, a novel dataset designed to capture rich and diverse time series derived from real-world videos. Experimental results on state-of-the-art TSFMs under zero-shot forecasting show that, despite strong performance on conventional benchmarks, these models exhibit performance degradation on the proposed dataset, suggesting limited generalizability to novel datasets. These findings underscore the need for novel approaches to acquiring time series data and highlight the lack of universality in recent TSFMs, while further validating the effectiveness of our video-based time series data extraction pipeline.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by Artificial Intelligence for Time Series Analysis (AI4TS) Workshop @ AAAI 2026: Theory, Algorithms, and Applications",
    "pdf_url": "https://arxiv.org/pdf/2509.26347v2",
    "published_date": "2025-09-30 14:53:05 UTC",
    "updated_date": "2025-11-28 18:31:29 UTC"
  },
  {
    "arxiv_id": "2509.26346v1",
    "title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing",
    "authors": [
      "Keming Wu",
      "Sicong Jiang",
      "Max Ku",
      "Ping Nie",
      "Minghao Liu",
      "Wenhu Chen"
    ],
    "abstract": "Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \\mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \\mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \\mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \\mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \\mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \\mname with its training dataset will be released to help the community build more high-quality image editing training datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Work in progress. Project Page: https://tiger-ai-lab.github.io/EditReward",
    "pdf_url": "https://arxiv.org/pdf/2509.26346v1",
    "published_date": "2025-09-30 14:51:04 UTC",
    "updated_date": "2025-09-30 14:51:04 UTC"
  },
  {
    "arxiv_id": "2509.26345v1",
    "title": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models",
    "authors": [
      "Qinjian Zhao",
      "Jiaqi Wang",
      "Zhiqiang Gao",
      "Zhihao Dou",
      "Belal Abuhaija",
      "Kaizhu Huang"
    ],
    "abstract": "Large Language Models (LLMs) have achieved impressive performance across diverse natural language processing tasks, but their growing power also amplifies potential risks such as jailbreak attacks that circumvent built-in safety mechanisms. Existing defenses including input paraphrasing, multi step evaluation, and safety expert models often suffer from high computational costs, limited generalization, or rigid workflows that fail to detect subtle malicious intent embedded in complex contexts. Inspired by cognitive science findings on human decision making, we propose SafeBehavior, a novel hierarchical jailbreak defense mechanism that simulates the adaptive multistage reasoning process of humans. SafeBehavior decomposes safety evaluation into three stages: intention inference to detect obvious input risks, self introspection to assess generated responses and assign confidence based judgments, and self revision to adaptively rewrite uncertain outputs while preserving user intent and enforcing safety constraints. We extensively evaluate SafeBehavior against five representative jailbreak attack types including optimization based, contextual manipulation, and prompt based attacks and compare it with seven state of the art defense baselines. Experimental results show that SafeBehavior significantly improves robustness and adaptability across diverse threat scenarios, offering an efficient and human inspired approach to safeguarding LLMs against jailbreak attempts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "27 pages, 5 figure",
    "pdf_url": "https://arxiv.org/pdf/2509.26345v1",
    "published_date": "2025-09-30 14:50:59 UTC",
    "updated_date": "2025-09-30 14:50:59 UTC"
  },
  {
    "arxiv_id": "2509.26331v1",
    "title": "AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations",
    "authors": [
      "Berdymyrat Ovezmyradov"
    ],
    "abstract": "The rapid advancement of LLMs sparked significant interest in their potential to augment or automate managerial functions. One of the most recent trends in AI benchmarking is performance of Large Language Models (LLMs) over longer time horizons. While LLMs excel at tasks involving natural language and pattern recognition, their capabilities in multi-step, strategic business decision-making remain largely unexplored. Few studies demonstrated how results can be different from benchmarks in short-term tasks, as Vending-Bench revealed. Meanwhile, there is a shortage of alternative benchmarks for long-term coherence. This research analyses a novel benchmark using a business game for the decision making in business. The research contributes to the recent literature on AI by proposing a reproducible, open-access management simulator to the research community for LLM benchmarking. This novel framework is used for evaluating the performance of five leading LLMs available in free online interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes decisions for a simulated retail company. A dynamic, month-by-month management simulation provides transparently in spreadsheet model as experimental environment. In each of twelve months, the LLMs are provided with a structured prompt containing a full business report from the previous period and are tasked with making key strategic decisions: pricing, order size, marketing budget, hiring, dismissal, loans, training expense, R&D expense, sales forecast, income forecast The methodology is designed to compare the LLMs on quantitative metrics: profit, revenue, and market share, and other KPIs. LLM decisions are analyzed in their strategic coherence, adaptability to market changes, and the rationale provided for their decisions. This approach allows to move beyond simple performance metrics for assessment of the long-term decision-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, 7 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.26331v1",
    "published_date": "2025-09-30 14:43:05 UTC",
    "updated_date": "2025-09-30 14:43:05 UTC"
  },
  {
    "arxiv_id": "2509.26324v2",
    "title": "LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search",
    "authors": [
      "Ruiyang Wang",
      "Hao-Lun Hsu",
      "David Hunt",
      "Shaocheng Luo",
      "Jiwoo Kim",
      "Miroslav Pajic"
    ],
    "abstract": "Autonomous exploration and object search in unknown indoor environments remain challenging for multi-robot systems (MRS). Traditional approaches often rely on greedy frontier assignment strategies with limited inter-robot coordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot Coordinated Exploration and Search), a novel framework that leverages Large Language Models (LLMs) for intelligent coordination of both homogeneous and heterogeneous robot teams tasked with efficient exploration and target object search. Our approach combines real-time LiDAR scan processing for frontier cluster extraction and doorway detection with multimodal LLM reasoning (e.g., GPT-4o) to generate coordinated waypoint assignments based on shared environment maps and robot states. LLM-MCoX demonstrates superior performance compared to existing methods, including greedy and Voronoi-based planners, achieving 22.7% faster exploration times and 50% improved search efficiency in large environments with 6 robots. Notably, LLM-MCoX enables natural language-based object search capabilities, allowing human operators to provide high-level semantic guidance that traditional algorithms cannot interpret.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26324v2",
    "published_date": "2025-09-30 14:33:35 UTC",
    "updated_date": "2025-10-04 22:23:46 UTC"
  },
  {
    "arxiv_id": "2509.26306v3",
    "title": "Interactive Learning for LLM Reasoning",
    "authors": [
      "Hehai Lin",
      "Shilei Cao",
      "Sudong Wang",
      "Haotian Wu",
      "Minzhi Li",
      "Linyi Yang",
      "Juepeng Zheng",
      "Chengwei Qin"
    ],
    "abstract": "Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMs' independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLM's reward distribution characteristics into another's reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The code is available at https://github.com/linhh29/Interactive-Learning-for-LLM-Reasoning",
    "pdf_url": "https://arxiv.org/pdf/2509.26306v3",
    "published_date": "2025-09-30 14:21:31 UTC",
    "updated_date": "2025-10-02 04:13:53 UTC"
  },
  {
    "arxiv_id": "2509.26305v1",
    "title": "Feedback Forensics: A Toolkit to Measure AI Personality",
    "authors": [
      "Arduin Findeis",
      "Timo Kaufmann",
      "Eyke Hüllermeier",
      "Robert Mullins"
    ],
    "abstract": "Some traits making a \"good\" AI model are hard to describe upfront. For example, should responses be more polite or more casual? Such traits are sometimes summarized as model character or personality. Without a clear objective, conventional benchmarks based on automatic validation struggle to measure such traits. Evaluation methods using human feedback such as Chatbot Arena have emerged as a popular alternative. These methods infer \"better\" personality and other desirable traits implicitly by ranking multiple model responses relative to each other. Recent issues with model releases highlight limitations of these existing opaque evaluation approaches: a major model was rolled back over sycophantic personality issues, models were observed overfitting to such feedback-based leaderboards. Despite these known issues, limited public tooling exists to explicitly evaluate model personality. We introduce Feedback Forensics: an open-source toolkit to track AI personality changes, both those encouraged by human (or AI) feedback, and those exhibited across AI models trained and evaluated on such feedback. Leveraging AI annotators, our toolkit enables investigating personality via Python API and browser app. We demonstrate the toolkit's usefulness in two steps: (A) first we analyse the personality traits encouraged in popular human feedback datasets including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to analyse how much popular models exhibit such traits. We release (1) our Feedback Forensics toolkit alongside (2) a web app tracking AI personality in popular models and feedback datasets as well as (3) the underlying annotation data at https://github.com/rdnfn/feedback-forensics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26305v1",
    "published_date": "2025-09-30 14:19:21 UTC",
    "updated_date": "2025-09-30 14:19:21 UTC"
  },
  {
    "arxiv_id": "2509.26302v1",
    "title": "QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization",
    "authors": [
      "Mohamed Imed Eddine Ghebriout",
      "Gaël Guibon",
      "Ivan Lerner",
      "Emmanuel Vincent"
    ],
    "abstract": "Dialogue summarization aims to distill the core meaning of a conversation into a concise text. This is crucial for reducing the complexity and noise inherent in dialogue-heavy applications. While recent approaches typically train language models to mimic human-written summaries, such supervision is costly and often results in outputs that lack task-specific focus limiting their effectiveness in downstream applications, such as medical tasks. In this paper, we propose \\app, a framework for task-oriented utility-based dialogue summarization. \\app starts by generating multiple summaries and task-oriented question-answer pairs from a dialogue in a zero-shot manner using a pool of large language models (LLMs). The quality of the generated summaries is evaluated by having LLMs answer task-related questions before \\textit{(i)} selecting the best candidate answers and \\textit{(ii)} identifying the most informative summary based on these answers. Finally, we fine-tune the best LLM on the selected summaries. When validated on multiple datasets, \\app demonstrates its effectiveness by achieving competitive results in various zero-shot settings, rivaling fully-supervised State-of-the-Art (SotA) methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Empirical Methods in Natural Language Processing (EMNLP 2025)",
    "pdf_url": "https://arxiv.org/pdf/2509.26302v1",
    "published_date": "2025-09-30 14:16:08 UTC",
    "updated_date": "2025-09-30 14:16:08 UTC"
  },
  {
    "arxiv_id": "2509.26294v1",
    "title": "Noise-Guided Transport for Imitation Learning",
    "authors": [
      "Lionel Blondé",
      "Joao A. Candido Ramos",
      "Alexandros Kalousis"
    ],
    "abstract": "We consider imitation learning in the low-data regime, where only a limited number of expert demonstrations are available. In this setting, methods that rely on large-scale pretraining or high-capacity architectures can be difficult to apply, and efficiency with respect to demonstration data becomes critical. We introduce Noise-Guided Transport (NGT), a lightweight off-policy method that casts imitation as an optimal transport problem solved via adversarial training. NGT requires no pretraining or specialized architectures, incorporates uncertainty estimation by design, and is easy to implement and tune. Despite its simplicity, NGT achieves strong performance on challenging continuous control tasks, including high-dimensional Humanoid tasks, under ultra-low data regimes with as few as 20 transitions. Code is publicly available at: https://github.com/lionelblonde/ngt-pytorch.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26294v1",
    "published_date": "2025-09-30 14:10:06 UTC",
    "updated_date": "2025-09-30 14:10:06 UTC"
  },
  {
    "arxiv_id": "2509.26291v1",
    "title": "Representation-Based Data Quality Audits for Audio",
    "authors": [
      "Alvaro Gonzalez-Jimenez",
      "Fabian Gröger",
      "Linda Wermelinger",
      "Andrin Bürli",
      "Iason Kastanis",
      "Simone Lionetti",
      "Marc Pouly"
    ],
    "abstract": "Data quality issues such as off-topic samples, near duplicates, and label errors often limit the performance of audio-based systems. This paper addresses these issues by adapting SelfClean, a representation-to-rank data auditing framework, from the image to the audio domain. This approach leverages self-supervised audio representations to identify common data quality issues, creating ranked review lists that surface distinct issues within a single, unified process. The method is benchmarked on the ESC-50, GTZAN, and a proprietary industrial dataset, using both synthetic and naturally occurring corruptions. The results demonstrate that this framework achieves state-of-the-art ranking performance, often outperforming issue-specific baselines and enabling significant annotation savings by efficiently guiding human review.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26291v1",
    "published_date": "2025-09-30 14:08:03 UTC",
    "updated_date": "2025-09-30 14:08:03 UTC"
  },
  {
    "arxiv_id": "2509.26281v2",
    "title": "Point2RBox-v3: Self-Bootstrapping from Point Annotations via Integrated Pseudo-Label Refinement and Utilization",
    "authors": [
      "Teng Zhang",
      "Ziqian Fan",
      "Mingxin Liu",
      "Xin Zhang",
      "Xudong Lu",
      "Wentong Li",
      "Yue Zhou",
      "Yi Yu",
      "Xiang Li",
      "Junchi Yan",
      "Xue Yang"
    ],
    "abstract": "Driven by the growing need for Oriented Object Detection (OOD), learning from point annotations under a weakly-supervised framework has emerged as a promising alternative to costly and laborious manual labeling. In this paper, we discuss two deficiencies in existing point-supervised methods: inefficient utilization and poor quality of pseudo labels. Therefore, we present Point2RBox-v3. At the core are two principles: 1) Progressive Label Assignment (PLA). It dynamically estimates instance sizes in a coarse yet intelligent manner at different stages of the training process, enabling the use of label assignment methods. 2) Prior-Guided Dynamic Mask Loss (PGDM-Loss). It is an enhancement of the Voronoi Watershed Loss from Point2RBox-v2, which overcomes the shortcomings of Watershed in its poor performance in sparse scenes and SAM's poor performance in dense scenes. To our knowledge, Point2RBox-v3 is the first model to employ dynamic pseudo labels for label assignment, and it creatively complements the advantages of SAM model with the watershed algorithm, which achieves excellent performance in both sparse and dense scenes. Our solution gives competitive performance, especially in scenarios with large variations in object size or sparse object occurrences: 66.09%/56.86%/41.28%/46.40%/19.60%/45.96% on DOTA-v1.0/DOTA-v1.5/DOTA-v2.0/DIOR/STAR/RSAR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19pages, 5figures, 6tables",
    "pdf_url": "https://arxiv.org/pdf/2509.26281v2",
    "published_date": "2025-09-30 14:01:59 UTC",
    "updated_date": "2025-10-08 03:36:37 UTC"
  },
  {
    "arxiv_id": "2509.26255v2",
    "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning",
    "authors": [
      "Yichao Liang",
      "Dat Nguyen",
      "Cambridge Yang",
      "Tianyang Li",
      "Joshua B. Tenenbaum",
      "Carl Edward Rasmussen",
      "Adrian Weller",
      "Zenna Tavares",
      "Tom Silver",
      "Kevin Ellis"
    ],
    "abstract": "Long-horizon embodied planning is challenging because the world does not only change through an agent's actions: exogenous processes (e.g., water heating, dominoes cascading) unfold concurrently with the agent's actions. We propose a framework for abstract world models that jointly learns (i) symbolic state representations and (ii) causal processes for both endogenous actions and exogenous mechanisms. Each causal process models the time course of a stochastic cause-effect relation. We learn these world models from limited data via variational Bayesian inference combined with LLM proposals. Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming a range of baselines.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "41 pages. The last two authors contributed equally in co-advising",
    "pdf_url": "https://arxiv.org/pdf/2509.26255v2",
    "published_date": "2025-09-30 13:44:34 UTC",
    "updated_date": "2025-10-01 01:58:01 UTC"
  },
  {
    "arxiv_id": "2509.26246v1",
    "title": "SlimPack: Fine-Grained Asymmetric Packing for Balanced and Efficient Variable-Length LLM Training",
    "authors": [
      "Yuliang Liu",
      "Guohao Wu",
      "Shenglong Zhang",
      "Wei Zhang",
      "Qianchao Zhu",
      "Zhouyang Li",
      "Chenyu Wang"
    ],
    "abstract": "The efficient distributed training of Large Language Models (LLMs) is severely hampered by the extreme variance in context lengths. This data heterogeneity, amplified by conventional packing strategies and asymmetric forward-backward costs, leads to critical inefficiencies such as cascading workload imbalances and severe hardware underutilization. Existing solutions attempt to mitigate these challenges, but often at the expense of memory or communication efficiency.\n  To address these challenges, we introduce SlimPack, a framework that fundamentally rethinks data packing and scheduling by decomposing samples into fine-grained slices. This slice-level decomposition immediately mitigates critical memory and communication bottlenecks by transforming large, volatile workloads into a stream of smaller, manageable units. This flexibility is then harnessed for our core innovation, Asymmetric Partitioning, which assembles balanced scheduling units uniquely optimized for the different demands of the forward and backward passes. Orchestrated by a two-phase solver and a high-fidelity simulator, SlimPack holistically resolves imbalances across all parallel dimensions. Extensive experiments demonstrate that SlimPack achieves up to a $2.8\\times$ training throughput improvement over baselines, breaking the conventional trade-off by delivering both superior balance and high resource efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26246v1",
    "published_date": "2025-09-30 13:37:48 UTC",
    "updated_date": "2025-09-30 13:37:48 UTC"
  },
  {
    "arxiv_id": "2509.26242v2",
    "title": "Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing",
    "authors": [
      "Yang Tang",
      "Ruijie Liu",
      "Yifan Wang",
      "Shiyu Li",
      "Xi Chen"
    ],
    "abstract": "Large language models (LLMs) fine-tuning shows excellent implications. However, vanilla fine-tuning methods often require intricate data mixture and repeated experiments for optimal generalization. To address these challenges and streamline the training process, we propose an efficient and universal solution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through zero-learning-rate training on general data, which is subsequently employed for gradient boosting and dynamic training step correction during domain training. In conjunction with annealing learning, we end up establishing a fine-tuning pipeline that relies solely on domain data without collapse. By evaluating both general and domain-specific performance across multiple tasks on several popular base models, DBA achieves an average improvement of 5.8% in joint performance over vanilla fine-tuning. Furthermore, since general data is no longer involved in annealing, repeated experiments led by data mixture are also eliminated. According to our tests, the DBA method can reduce GPU hours by 91.0% compared to the vanilla method.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.26242v2",
    "published_date": "2025-09-30 13:36:17 UTC",
    "updated_date": "2025-10-17 10:51:47 UTC"
  },
  {
    "arxiv_id": "2509.26239v1",
    "title": "Sandbagging in a Simple Survival Bandit Problem",
    "authors": [
      "Joel Dyer",
      "Daniel Jarne Ornia",
      "Nicholas Bishop",
      "Anisoara Calinescu",
      "Michael Wooldridge"
    ],
    "abstract": "Evaluating the safety of frontier AI systems is an increasingly important concern, helping to measure the capabilities of such models and identify risks before deployment. However, it has been recognised that if AI agents are aware that they are being evaluated, such agents may deliberately hide dangerous capabilities or intentionally demonstrate suboptimal performance in safety-related tasks in order to be released and to avoid being deactivated or retrained. Such strategic deception - often known as \"sandbagging\" - threatens to undermine the integrity of safety evaluations. For this reason, it is of value to identify methods that enable us to distinguish behavioural patterns that demonstrate a true lack of capability from behavioural patterns that are consistent with sandbagging. In this paper, we develop a simple model of strategic deception in sequential decision-making tasks, inspired by the recently developed survival bandit framework. We demonstrate theoretically that this problem induces sandbagging behaviour in optimal rational agents, and construct a statistical test to distinguish between sandbagging and incompetence from sequences of test scores. In simulation experiments, we investigate the reliability of this test in allowing us to distinguish between such behaviours in bandit models. This work aims to establish a potential avenue for developing robust statistical procedures for use in the science of frontier model evaluations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Forthcoming in the \"Reliable ML from Unreliable Data Workshop\" at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.26239v1",
    "published_date": "2025-09-30 13:33:46 UTC",
    "updated_date": "2025-09-30 13:33:46 UTC"
  },
  {
    "arxiv_id": "2509.26233v1",
    "title": "3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation",
    "authors": [
      "Balamurugan Thambiraja",
      "Malte Prinzler",
      "Sadegh Aliakbarian",
      "Darren Cosker",
      "Justus Thies"
    ],
    "abstract": "Creating personalized 3D animations with precise control and realistic head motions remains challenging for current speech-driven 3D facial animation methods. Editing these animations is especially complex and time consuming, requires precise control and typically handled by highly skilled animators. Most existing works focus on controlling style or emotion of the synthesized animation and cannot edit/regenerate parts of an input animation. They also overlook the fact that multiple plausible lip and head movements can match the same audio input. To address these challenges, we present 3DiFACE, a novel method for holistic speech-driven 3D facial animation. Our approach produces diverse plausible lip and head motions for a single audio input and allows for editing via keyframing and interpolation. Specifically, we propose a fully-convolutional diffusion model that can leverage the viseme-level diversity in our training corpus. Additionally, we employ a speaking-style personalization and a novel sparsely-guided motion diffusion to enable precise control and editing. Through quantitative and qualitative evaluations, we demonstrate that our method is capable of generating and editing diverse holistic 3D facial animations given a single audio input, with control between high fidelity and diversity. Code and models are available here: https://balamuruganthambiraja.github.io/3DiFACE",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26233v1",
    "published_date": "2025-09-30 13:30:01 UTC",
    "updated_date": "2025-09-30 13:30:01 UTC"
  },
  {
    "arxiv_id": "2509.26225v1",
    "title": "An Experimental Study on Generating Plausible Textual Explanations for Video Summarization",
    "authors": [
      "Thomas Eleftheriadis",
      "Evlampios Apostolidis",
      "Vasileios Mezaris"
    ],
    "abstract": "In this paper, we present our experimental study on generating plausible textual explanations for the outcomes of video summarization. For the needs of this study, we extend an existing framework for multigranular explanation of video summarization by integrating a SOTA Large Multimodal Model (LLaVA-OneVision) and prompting it to produce natural language descriptions of the obtained visual explanations. Following, we focus on one of the most desired characteristics for explainable AI, the plausibility of the obtained explanations that relates with their alignment with the humans' reasoning and expectations. Using the extended framework, we propose an approach for evaluating the plausibility of visual explanations by quantifying the semantic overlap between their textual descriptions and the textual descriptions of the corresponding video summaries, with the help of two methods for creating sentence embeddings (SBERT, SimCSE). Based on the extended framework and the proposed plausibility evaluation approach, we conduct an experimental study using a SOTA method (CA-SUM) and two datasets (SumMe, TVSum) for video summarization, to examine whether the more faithful explanations are also the more plausible ones, and identify the most appropriate approach for generating plausible textual explanations for video summarization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE CBMI 2025. This is the authors' accepted version. The final publication is available at https://ieeexplore.ieee.org/",
    "pdf_url": "https://arxiv.org/pdf/2509.26225v1",
    "published_date": "2025-09-30 13:23:40 UTC",
    "updated_date": "2025-09-30 13:23:40 UTC"
  },
  {
    "arxiv_id": "2509.26224v1",
    "title": "Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models",
    "authors": [
      "Alessandro De Bellis",
      "Salvatore Bufi",
      "Giovanni Servedio",
      "Vito Walter Anelli",
      "Tommaso Di Noia",
      "Eugenio Di Sciascio"
    ],
    "abstract": "Inductive link prediction is emerging as a key paradigm for real-world knowledge graphs (KGs), where new entities frequently appear and models must generalize to them without retraining. Predicting links in a KG faces the challenge of guessing previously unseen entities by leveraging generalizable node features such as subgraph structure, type annotations, and ontological constraints. However, explicit type information is often lacking or incomplete. Even when available, type information in most KGs is often coarse-grained, sparse, and prone to errors due to human annotation. In this work, we explore the potential of pre-trained language models (PLMs) to enrich node representations with implicit type signals. We introduce TyleR, a Type-less yet type-awaRe approach for subgraph-based inductive link prediction that leverages PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate that TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity. To ensure reproducibility, we share our code at https://github.com/sisinflab/tyler .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted and to appear in Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025)",
    "pdf_url": "https://arxiv.org/pdf/2509.26224v1",
    "published_date": "2025-09-30 13:23:02 UTC",
    "updated_date": "2025-09-30 13:23:02 UTC"
  },
  {
    "arxiv_id": "2509.26219v2",
    "title": "Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation",
    "authors": [
      "Chenyang Jiang",
      "Zhengcen Li",
      "Hang Zhao",
      "Qiben Shan",
      "Shaocong Wu",
      "Jingyong Su"
    ],
    "abstract": "Dataset distillation has emerged as a promising paradigm that synthesizes compact, informative datasets capable of retaining the knowledge of large-scale counterparts, thereby addressing the substantial computational and storage burdens of modern model training. Conventional approaches typically rely on dense pixel-level representations, which introduce redundancy and are difficult to scale up. In this work, we propose GSDD, a novel and efficient sparse representation for dataset distillation based on 2D Gaussians. Instead of representing all pixels equally, GSDD encodes critical discriminative information in a distilled image using only a small number of Gaussian primitives. This sparse representation could improve dataset diversity under the same storage budget, enhancing coverage of difficult samples and boosting distillation performance. To ensure both efficiency and scalability, we adapt CUDA-based splatting operators for parallel inference and training, enabling high-quality rendering with minimal computational and memory overhead. Our method is simple yet effective, broadly applicable to different distillation pipelines, and highly scalable. Experiments show that GSDD achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet subsets, while remaining highly efficient encoding and decoding cost. Our code is available at https://github.com/j-cyoung/GSDatasetDistillation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages; Code is available on https://github.com/j-cyoung/GSDatasetDistillation",
    "pdf_url": "https://arxiv.org/pdf/2509.26219v2",
    "published_date": "2025-09-30 13:19:05 UTC",
    "updated_date": "2025-12-02 05:14:55 UTC"
  },
  {
    "arxiv_id": "2509.26217v2",
    "title": "Benchmarking Deep Learning Convolutions on Energy-constrained CPUs",
    "authors": [
      "Enrique Galvez",
      "Adrien Cassagne",
      "Alix Munier",
      "Manuel Bouyer"
    ],
    "abstract": "This work evaluates State-of-the-Art convolution algorithms for CPU-based CNN inference. Although most prior studies focus on GPUs or NPUs, CPU implementations remain comparatively under-optimized. Our first contribution is to provide fair benchmarking for embedded CPU inference. We evaluate direct, GEMM-based, and Winograd convolutions across modern CPUs from ARM, Intel, AMD, and NVIDIA vendors, considering both latency and energy efficiency. To the best of our knowledge, this is the first study to present a fair, cross-vendor comparison of CPU energy consumption using a high-resolution socket-level measurement platform. To validate our methodology, we further compare socket-level power measurements with estimates derived from model-specific registers (MSRs), finding that MSRs underestimate the power consumption of convolution inference by 10--30%. Our results show that the ARM\\R Cortex-A78AE CPU combined with an implicit GEMM convolution implementation offers the best trade-off between latency and power consumption, achieving ResNet50v1.5 inference in 102 ms with an average power of 25.3 W, corresponding to 2.58 J.",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26217v2",
    "published_date": "2025-09-30 13:19:00 UTC",
    "updated_date": "2026-01-05 09:47:47 UTC"
  },
  {
    "arxiv_id": "2509.26216v1",
    "title": "Comparative Analysis of Ant Colony Optimization and Google OR-Tools for Solving the Open Capacitated Vehicle Routing Problem in Logistics",
    "authors": [
      "Assem Omar",
      "Youssef Omar",
      "Marwa Solayman",
      "Hesham Mansour"
    ],
    "abstract": "In modern logistics management systems, route planning requires high efficiency. The Open Capacitated Vehicle Routing Problem (OCVRP) deals with finding optimal delivery routes for a fleet of vehicles serving geographically distributed customers, without requiring the vehicles to return to the depot after deliveries. The present study is comparative in nature and speaks of two algorithms for OCVRP solution: Ant Colony Optimization (ACO), a nature-inspired metaheuristic; and Google OR-Tools, an industry-standard toolkit for optimization. Both implementations were developed in Python and using a custom dataset. Performance appraisal was based on routing efficiency, computation time, and scalability. The results show that ACO allows flexibility in routing parameters while OR-Tools runs much faster with more consistency and requires less input. This could help choose among routing strategies for scalable real-time logistics systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, accepted at Intelligent Methods, Systems, and Applications (IMSA 2025)",
    "pdf_url": "https://arxiv.org/pdf/2509.26216v1",
    "published_date": "2025-09-30 13:18:14 UTC",
    "updated_date": "2025-09-30 13:18:14 UTC"
  },
  {
    "arxiv_id": "2509.26209v1",
    "title": "Diversity-Incentivized Exploration for Versatile Reasoning",
    "authors": [
      "Zican Hu",
      "Shilin Zhang",
      "Yafu Li",
      "Jianhao Yan",
      "Xuyang Hu",
      "Leyang Cui",
      "Xiaoye Qu",
      "Chunlin Chen",
      "Yu Cheng",
      "Zhi Wang"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a crucial paradigm for incentivizing reasoning capabilities in Large Language Models (LLMs). Due to vast state-action spaces and reward sparsity in reasoning tasks, existing methods often struggle with deficient exploration and poor sample efficiency. In the paper, we propose \\textbf{DIVER} (\\textbf{D}iversity-\\textbf{I}ncentivized Exploration for \\textbf{V}ersatil\\textbf{E} \\textbf{R}easoning), an innovative framework that highlights the pivotal role of global sequence-level diversity to incentivize deep exploration for versatile reasoning. We first conduct a primary empirical study to reveal a strong positive correlation between global diversity and reasoning capacity. Building on this insight, we introduce global diversity incentives as an intrinsic reward to promote deep exploration in a semantically structured space. Incorporating the intrinsic reward, we develop a potential-based reward shaping mechanism to preserve optimal policy invariance and design simple heuristics to mitigate possible reward hacking. Experimental results show that DIVER outperforms competitive RLVR baselines with various exploration strategies on both in-domain and out-of-domain tasks, excelling in both Pass@1 and Pass@k evaluations. Our code is available at https://github.com/NJU-RL/DIVER.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.26209v1",
    "published_date": "2025-09-30 13:11:46 UTC",
    "updated_date": "2025-09-30 13:11:46 UTC"
  },
  {
    "arxiv_id": "2509.26205v1",
    "title": "Human-Centered Evaluation of RAG outputs: a framework and questionnaire for human-AI collaboration",
    "authors": [
      "Aline Mangold",
      "Kiran Hoffmann"
    ],
    "abstract": "Retrieval-augmented generation (RAG) systems are increasingly deployed in user-facing applications, yet systematic, human-centered evaluation of their outputs remains underexplored. Building on Gienapp's utility-dimension framework, we designed a human-centred questionnaire that assesses RAG outputs across 12 dimensions. We iteratively refined the questionnaire through several rounds of ratings on a set of query-output pairs and semantic discussions. Ultimately, we incorporated feedback from both a human rater and a human-LLM pair. Results indicate that while large language models (LLMs) reliably focus on metric descriptions and scale labels, they exhibit weaknesses in detecting textual format variations. Humans struggled to focus strictly on metric descriptions and labels. LLM ratings and explanations were viewed as a helpful support, but numeric LLM and human ratings lacked agreement. The final questionnaire extends the initial framework by focusing on user intent, text structuring, and information verifiability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26205v1",
    "published_date": "2025-09-30 13:08:33 UTC",
    "updated_date": "2025-09-30 13:08:33 UTC"
  },
  {
    "arxiv_id": "2509.26201v1",
    "title": "LLM Agents for Knowledge Discovery in Atomic Layer Processing",
    "authors": [
      "Andreas Werbrouck",
      "Marshall B. Lindsay",
      "Matthew Maschmann",
      "Matthias J. Young"
    ],
    "abstract": "Large Language Models (LLMs) have garnered significant attention for several years now. Recently, their use as independently reasoning agents has been proposed. In this work, we test the potential of such agents for knowledge discovery in materials science. We repurpose LangGraph's tool functionality to supply agents with a black box function to interrogate. In contrast to process optimization or performing specific, user-defined tasks, knowledge discovery consists of freely exploring the system, posing and verifying statements about the behavior of this black box, with the sole objective of generating and verifying generalizable statements. We provide proof of concept for this approach through a children's parlor game, demonstrating the role of trial-and-error and persistence in knowledge discovery, and the strong path-dependence of results. We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions.",
    "categories": [
      "cs.AI",
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted submission to the AI4MAT workshop@NEURIPS 2025. As submitted, except author names added",
    "pdf_url": "https://arxiv.org/pdf/2509.26201v1",
    "published_date": "2025-09-30 13:01:44 UTC",
    "updated_date": "2025-09-30 13:01:44 UTC"
  },
  {
    "arxiv_id": "2509.26200v1",
    "title": "Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management",
    "authors": [
      "Hatim Chergui",
      "Miguel Catalan Cid",
      "Pouria Sayyad Khodashenas",
      "Daniel Camps Mur",
      "Christos Verikoukis"
    ],
    "abstract": "This paper introduces a novel framework for proactive cross-domain resource orchestration in 6G RAN-Edge networks, featuring large language model (LLM)-augmented agents. The system comprises specialized RAN (energy efficiency) and Edge (latency assurance) agents that engage in iterative negotiation, supported by advanced reasoning and planning capabilities. Agents dynamically interact with a digital twin (DT) to test their proposals and leverage a long-term collective memory where their joint successful and failed agreements along with the related network contexts are distilled into strategies to either follow or avoid and subsequently stored. Given that agents are subject to a plethora of cognitive distortions when retrieving those past experiences -- such as primacy, recency, confirmation and availability biases -- we propose in this work a novel unbiased memory design (A reusable mockup version of the unbiased memory source code is available for non-commercial use at https://github.com/HatimChergui/unbiased-collective-memory). featuring (i) semantic retrieval of past strategies via Jaccard similarity; (ii) learning from failures through amplified weighting of SLA violations and mandatory inclusion of failed negotiation cases to mitigate confirmation bias; (iii) diversity enforcement to minimize availability bias and (iv) recency and primacy weighting with slow decay to counteract temporal biases. Evaluation results showcase the impact of existing biases and how the unbiased memory allows to tackle them by learning from both successful and failed strategies, either present or old, resulting in $\\times 4.5$ and $\\times 3.5$ reductions of unresolved negotiations compared to non-memory and vanilla memory baselines, respectively, while totally mitigating SLA violations as well as improving latency and energy saving distributions.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "12 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.26200v1",
    "published_date": "2025-09-30 12:57:11 UTC",
    "updated_date": "2025-09-30 12:57:11 UTC"
  },
  {
    "arxiv_id": "2509.26187v1",
    "title": "Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning",
    "authors": [
      "Youssef Sabiri",
      "Walid Houmaidi",
      "Aaya Bougrine",
      "Salmane El Mansour Billah"
    ],
    "abstract": "Ensuring optimal Indoor Environmental Quality (IEQ) is vital for occupant health and productivity, yet it often comes at a high energy cost in conventional Heating, Ventilation, and Air Conditioning (HVAC) systems. This paper proposes a deep learning driven approach to proactively manage IEQ parameters specifically CO2 concentration, temperature, and humidity while balancing building energy efficiency. Leveraging the ROBOD dataset collected from a net-zero energy academic building, we benchmark three architectures--Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and a hybrid Convolutional Neural Network LSTM (CNN-LSTM)--to forecast IEQ variables across various time horizons. Our results show that GRU achieves the best short-term prediction accuracy with lower computational overhead, whereas CNN-LSTM excels in extracting dominant features for extended forecasting windows. Meanwhile, LSTM offers robust long-range temporal modeling. The comparative analysis highlights that prediction reliability depends on data resolution, sensor placement, and fluctuating occupancy conditions. These findings provide actionable insights for intelligent Building Management Systems (BMS) to implement predictive HVAC control, thereby reducing energy consumption and enhancing occupant comfort in real-world building operations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 4 figures, 1 table. Accepted and presented at the 5th International Conference on Digital Technologies and Applications (ICDTA 2025), April 17-18, 2025, Al Akhawayn University, Ifrane, Morocco",
    "pdf_url": "https://arxiv.org/pdf/2509.26187v1",
    "published_date": "2025-09-30 12:42:34 UTC",
    "updated_date": "2025-09-30 12:42:34 UTC"
  },
  {
    "arxiv_id": "2509.26185v1",
    "title": "AttriGen: Automated Multi-Attribute Annotation for Blood Cell Datasets",
    "authors": [
      "Walid Houmaidi",
      "Youssef Sabiri",
      "Fatima Zahra Iguenfer",
      "Amine Abouaomar"
    ],
    "abstract": "We introduce AttriGen, a novel framework for automated, fine-grained multi-attribute annotation in computer vision, with a particular focus on cell microscopy where multi-attribute classification remains underrepresented compared to traditional cell type categorization. Using two complementary datasets: the Peripheral Blood Cell (PBC) dataset containing eight distinct cell types and the WBC Attribute Dataset (WBCAtt) that contains their corresponding 11 morphological attributes, we propose a dual-model architecture that combines a CNN for cell type classification, as well as a Vision Transformer (ViT) for multi-attribute classification achieving a new benchmark of 94.62\\% accuracy. Our experiments demonstrate that AttriGen significantly enhances model interpretability and offers substantial time and cost efficiency relative to conventional full-scale human annotation. Thus, our framework establishes a new paradigm that can be extended to other computer vision classification tasks by effectively automating the expansion of multi-attribute labels.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 4 figures, 3 tables. Accepted at the 12th International Conference on Wireless Networks and Mobile Communications 2025 (WINCOM 2025)",
    "pdf_url": "https://arxiv.org/pdf/2509.26185v1",
    "published_date": "2025-09-30 12:42:28 UTC",
    "updated_date": "2025-09-30 12:42:28 UTC"
  },
  {
    "arxiv_id": "2509.26184v4",
    "title": "Auto-ARGUE: LLM-Based Report Generation Evaluation",
    "authors": [
      "William Walden",
      "Marc Mason",
      "Orion Weller",
      "Laura Dietz",
      "John Conroy",
      "Neil Molino",
      "Hannah Recknor",
      "Bryan Li",
      "Gabrielle Kaili-May Liu",
      "Yu Hou",
      "Dawn Lawrie",
      "James Mayfield",
      "Eugene Yang"
    ],
    "abstract": "Generation of long-form, citation-backed reports is a primary use case for retrieval augmented generation (RAG) systems. While open-source evaluation tools exist for various RAG tasks, ones tailored to report generation (RG) are lacking. Accordingly, we introduce Auto-ARGUE, a robust LLM-based implementation of the recently proposed ARGUE framework for RG evaluation. We present analysis of Auto-ARGUE on the RG pilot task from the TREC 2024 NeuCLIR track, showing good system-level correlations with human judgments. We further release a web app for visualization of Auto-ARGUE outputs.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26184v4",
    "published_date": "2025-09-30 12:41:11 UTC",
    "updated_date": "2025-10-17 13:06:05 UTC"
  },
  {
    "arxiv_id": "2509.26167v1",
    "title": "'Too much alignment; not enough culture': Re-balancing cultural alignment practices in LLMs",
    "authors": [
      "Eric J. W. Orlowski",
      "Hakim Norhashim",
      "Tristan Koh Ly Wey"
    ],
    "abstract": "While cultural alignment has increasingly become a focal point within AI research, current approaches relying predominantly on quantitative benchmarks and simplistic proxies fail to capture the deeply nuanced and context-dependent nature of human cultures. Existing alignment practices typically reduce culture to static demographic categories or superficial cultural facts, thereby sidestepping critical questions about what it truly means to be culturally aligned. This paper argues for a fundamental shift towards integrating interpretive qualitative approaches drawn from social sciences into AI alignment practices, specifically in the context of Large Language Models (LLMs). Drawing inspiration from Clifford Geertz's concept of \"thick description,\" we propose that AI systems must produce outputs that reflect deeper cultural meanings--what we term \"thick outputs\"-grounded firmly in user-provided context and intent. We outline three necessary conditions for successful cultural alignment: sufficiently scoped cultural representations, the capacity for nuanced outputs, and the anchoring of outputs in the cultural contexts implied within prompts. Finally, we call for cross-disciplinary collaboration and the adoption of qualitative, ethnographic evaluation methods as vital steps toward developing AI systems that are genuinely culturally sensitive, ethically responsible, and reflective of human complexity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, no figures",
    "pdf_url": "https://arxiv.org/pdf/2509.26167v1",
    "published_date": "2025-09-30 12:22:53 UTC",
    "updated_date": "2025-09-30 12:22:53 UTC"
  },
  {
    "arxiv_id": "2509.26161v1",
    "title": "90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development",
    "authors": [
      "Runxin Yang",
      "Yuxuan Wan",
      "Shuqing Li",
      "Michael R. Lyu"
    ],
    "abstract": "Developing 3D games requires specialized expertise across multiple domains, including programming, 3D modeling, and engine configuration, which limits access to millions of potential creators. Recently, researchers have begun to explore automated game development. However, existing approaches face three primary challenges: (1) limited scope to 2D content generation or isolated code snippets; (2) requirement for manual integration of generated components into game engines; and (3) poor performance on handling interactive game logic and state management. While Multimodal Large Language Models (MLLMs) demonstrate potential capabilities to ease the game generation task, a critical gap still remains in translating these outputs into production-ready, executable game projects based on game engines such as Unity and Unreal Engine.\n  To bridge the gap, this paper introduces UniGen, the first end-to-end coordinated multi-agent framework that automates zero-coding development of runnable 3D games from natural language requirements. Specifically, UniGen uses a Planning Agent that interprets user requirements into structured blueprints and engineered logic descriptions; after which a Generation Agent produces executable C# scripts; then an Automation Agent handles engine-specific component binding and scene construction; and lastly a Debugging Agent provides real-time error correction through conversational interaction. We evaluated UniGen on three distinct game prototypes. Results demonstrate that UniGen not only democratizes game creation by requiring no coding from the user, but also reduces development time by 91.4%. We release UniGen at https://github.com/yxwan123/UniGen. A video demonstration is available at https://www.youtube.com/watch?v=xyJjFfnxUx0.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26161v1",
    "published_date": "2025-09-30 12:14:56 UTC",
    "updated_date": "2025-09-30 12:14:56 UTC"
  },
  {
    "arxiv_id": "2510.00088v1",
    "title": "Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction",
    "authors": [
      "Sagnik Basu",
      "Shubham Prakash",
      "Ashish Maruti Barge",
      "Siddharth D Jaiswal",
      "Abhisek Dash",
      "Saptarshi Ghosh",
      "Animesh Mukherjee"
    ],
    "abstract": "Large language models (LLMs) have been extensively used for legal judgment prediction tasks based on case reports and crime history. However, with a surge in the availability of large vision language models (VLMs), legal judgment prediction systems can now be made to leverage the images of the criminals in addition to the textual case reports/crime history. Applications built in this way could lead to inadvertent consequences and be used with malicious intent. In this work, we run an audit to investigate the efficiency of standalone VLMs in the bail decision prediction task. We observe that the performance is poor across multiple intersectional groups and models \\textit{wrongly deny bail to deserving individuals with very high confidence}. We design different intervention algorithms by first including legal precedents through a RAG pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate that these interventions substantially improve the performance of bail prediction. Our work paves the way for the design of smarter interventions on VLMs in the future, before they can be deployed for real-world legal judgment prediction.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00088v1",
    "published_date": "2025-09-30 12:11:45 UTC",
    "updated_date": "2025-09-30 12:11:45 UTC"
  },
  {
    "arxiv_id": "2509.26158v1",
    "title": "Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis",
    "authors": [
      "Kyeongryeol Go"
    ],
    "abstract": "The performance of deep neural networks is strongly influenced by the quality of their training data. However, mitigating dataset bias by manually curating challenging edge cases remains a major bottleneck. To address this, we propose an automated pipeline for text-guided edge-case synthesis. Our approach employs a Large Language Model, fine-tuned via preference learning, to rephrase image captions into diverse textual prompts that steer a Text-to-Image model toward generating difficult visual scenarios. Evaluated on the FishEye8K object detection benchmark, our method achieves superior robustness, surpassing both naive augmentation and manually engineered prompts. This work establishes a scalable framework that shifts data curation from manual effort to automated, targeted synthesis, offering a promising direction for developing more reliable and continuously improving AI systems. Code is available at https://github.com/gokyeongryeol/ATES.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.26158v1",
    "published_date": "2025-09-30 12:11:25 UTC",
    "updated_date": "2025-09-30 12:11:25 UTC"
  },
  {
    "arxiv_id": "2509.26157v1",
    "title": "EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting",
    "authors": [
      "Sachith Abeywickrama",
      "Emadeldeen Eldele",
      "Min Wu",
      "Xiaoli Li",
      "Chau Yuen"
    ],
    "abstract": "Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), a novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by a global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as a promising new paradigm for time series modeling. Code is available at: https://github.com/Sachithx/EntroPE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Under Review",
    "pdf_url": "https://arxiv.org/pdf/2509.26157v1",
    "published_date": "2025-09-30 12:09:56 UTC",
    "updated_date": "2025-09-30 12:09:56 UTC"
  },
  {
    "arxiv_id": "2509.26153v3",
    "title": "A Field Guide to Deploying AI Agents in Clinical Practice",
    "authors": [
      "Jack Gallifant",
      "Katherine C. Kellogg",
      "Matt Butler",
      "Amanda Centi",
      "Shan Chen",
      "Patrick F. Doyle",
      "Sayon Dutta",
      "Joyce Guo",
      "Matthew J. Hadfield",
      "Esther H. Kim",
      "David E. Kozono",
      "Hugo JWL Aerts",
      "Adam B. Landman",
      "Raymond H. Mak",
      "Rebecca G. Mishuris",
      "Tanna L. Nelson",
      "Guergana K. Savova",
      "Elad Sharon",
      "Benjamin C. Silverman",
      "Umit Topaloglu",
      "Jeremy L. Warner",
      "Danielle S. Bitterman"
    ],
    "abstract": "Large language models (LLMs) integrated into agent-driven workflows hold immense promise for healthcare, yet a significant gap exists between their potential and practical implementation within clinical settings. To address this, we present a practitioner-oriented field manual for deploying generative agents that use electronic health record (EHR) data. This guide is informed by our experience deploying the \"irAE-Agent\", an automated system to detect immune-related adverse events from clinical notes at Mass General Brigham, and by structured interviews with 21 clinicians, engineers, and informatics leaders involved in the project. Our analysis reveals a critical misalignment in clinical AI development: less than 20% of our effort was dedicated to prompt engineering and model development, while over 80% was consumed by the sociotechnical work of implementation. We distill this effort into five \"heavy lifts\": data integration, model validation, ensuring economic value, managing system drift, and governance. By providing actionable solutions for each of these challenges, this field manual shifts the focus from algorithmic development to the essential infrastructure and implementation work required to bridge the \"valley of death\" and successfully translate generative AI from pilot projects into routine clinical care.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review. 7 Tables, 2 Figures",
    "pdf_url": "https://arxiv.org/pdf/2509.26153v3",
    "published_date": "2025-09-30 12:03:32 UTC",
    "updated_date": "2025-12-08 15:39:37 UTC"
  },
  {
    "arxiv_id": "2509.26150v1",
    "title": "Bubble, Bubble, AI's Rumble: Why Global Financial Regulatory Incident Reporting is Our Shield Against Systemic Stumbles",
    "authors": [
      "Anchal Gupta",
      "Gleb Pappyshev",
      "James T Kwok"
    ],
    "abstract": "\"Double, double toil and trouble; Fire burn and cauldron bubble.\" As Shakespeare's witches foretold chaos through cryptic prophecies, modern capital markets grapple with systemic risks concealed by opaque AI systems. According to IMF, the August 5, 2024, plunge in Japanese and U.S. equities can be linked to algorithmic trading yet ab-sent from existing AI incidents database exemplifies this transparency crisis. Current AI incident databases, reliant on crowdsourcing or news scraping, systematically over-look capital market anomalies, particularly in algorithmic and high-frequency trading. We address this critical gap by proposing a regulatory-grade global database that elegantly synthesises post-trade reporting frameworks with proven incident documentation models from healthcare and aviation. Our framework's temporal data omission technique masking timestamps while preserving percent-age-based metrics enables sophisticated cross-jurisdictional analysis of emerging risks while safeguarding confidential business information. Synthetic data validation (modelled after real life published incidents , sentiments, data) reveals compelling pat-terns: systemic risks transcending geographical boundaries, market manipulation clusters distinctly identifiable via K-means algorithms, and AI system typology exerting significantly greater influence on trading behaviour than geographical location, This tripartite solution empowers regulators with unprecedented cross-jurisdictional oversight, financial institutions with seamless compliance integration, and investors with critical visibility into previously obscured AI-driven vulnerabilities. We call for immediate action to strengthen risk management and foster resilience in AI-driven financial markets against the volatile \"cauldron\" of AI-driven systemic risks., promoting global financial stability through enhanced transparency and coordinated oversight.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26150v1",
    "published_date": "2025-09-30 12:01:25 UTC",
    "updated_date": "2025-09-30 12:01:25 UTC"
  },
  {
    "arxiv_id": "2509.26145v1",
    "title": "LMILAtt: A Deep Learning Model for Depression Detection from Social Media Users Enhanced by Multi-Instance Learning Based on Attention Mechanism",
    "authors": [
      "Yukun Yang"
    ],
    "abstract": "Depression is a major global public health challenge and its early identification is crucial. Social media data provides a new perspective for depression detection, but existing methods face limitations such as insufficient accuracy, insufficient utilization of time series features, and high annotation costs. To this end, this study proposes the LMILAtt model, which innovatively integrates Long Short-Term Memory autoencoders and attention mechanisms: firstly, the temporal dynamic features of user tweets (such as depressive tendency evolution patterns) are extracted through unsupervised LSTM autoencoders. Secondly, the attention mechanism is used to dynamically weight key texts (such as early depression signals) and construct a multi-example learning architecture to improve the accuracy of user-level detection. Finally, the performance was verified on the WU3D dataset labeled by professional medicine. Experiments show that the model is significantly better than the baseline model in terms of accuracy, recall and F1 score. In addition, the weakly supervised learning strategy significantly reduces the cost of labeling and provides an efficient solution for large-scale social media depression screening.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26145v1",
    "published_date": "2025-09-30 11:58:32 UTC",
    "updated_date": "2025-09-30 11:58:32 UTC"
  },
  {
    "arxiv_id": "2509.26140v1",
    "title": "OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models",
    "authors": [
      "Subrata Biswas",
      "Mohammad Nur Hossain Khan",
      "Bashima Islam"
    ],
    "abstract": "Spatial reasoning is fundamental to auditory perception, yet current audio large language models (ALLMs) largely rely on unstructured binaural cues and single step inference. This limits both perceptual accuracy in direction and distance estimation and the capacity for interpretable reasoning. Recent work such as BAT demonstrates spatial QA with binaural audio, but its reliance on coarse categorical labels (left, right, up, down) and the absence of explicit geometric supervision constrain resolution and robustness. We introduce the $\\textbf{Spatial-Acoustic Geometry Encoder (SAGE}$), a geometry-aware audio encoder that aligns binaural acoustic features with 3D spatial structure using panoramic depth images and room-impulse responses at training time, while requiring only audio at inference. Building on this representation, we present $\\textbf{OWL}$, an ALLM that integrates $\\textbf{SAGE}$ with a spatially grounded chain-of-thought to rationalize over direction-of-arrivals (DoA) and distance estimates. Through curriculum learning from perceptual QA to multi-step reasoning, $\\textbf{OWL}$ supports o'clock-level azimuth and DoA estimation. To enable large-scale training and evaluation, we construct and release $\\textbf{BiDepth}$, a dataset of over one million QA pairs combining binaural audio with panoramic depth images and room impulse responses across both in-room and out-of-room scenarios. Across two benchmark datasets, our new $\\textbf{BiDepth}$ and the public SpatialSoundQA, $\\textbf{OWL}$ reduces mean DoA error by $\\textbf{11$^{\\circ}$}$ through $\\textbf{SAGE}$ and improves spatial reasoning QA accuracy by up to $\\textbf{25}$\\% over BAT.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26140v1",
    "published_date": "2025-09-30 11:57:47 UTC",
    "updated_date": "2025-09-30 11:57:47 UTC"
  },
  {
    "arxiv_id": "2509.26139v1",
    "title": "Leveraging AI modelling for FDS with Simvue: monitor and optimise for more sustainable simulations",
    "authors": [
      "James Panayis",
      "Matt Field",
      "Vignesh Gopakumar",
      "Andrew Lahiff",
      "Kristian Zarebski",
      "Aby Abraham",
      "Jonathan L. Hodges"
    ],
    "abstract": "There is high demand on fire simulations, in both scale and quantity. We present a multi-pronged approach to improving the time and energy required to meet these demands. We show the ability of a custom machine learning surrogate model to predict the dynamics of heat propagation orders of magnitude faster than state-of-the-art CFD software for this application. We also demonstrate how a guided optimisation procedure can decrease the number of simulations required to meet an objective; using lightweight models to decide which simulations to run, we see a tenfold reduction when locating the most dangerous location for a fire to occur within a building based on the impact of smoke on visibility. Finally we present a framework and product, Simvue, through which we access these tools along with a host of automatic organisational and tracking features which enables future reuse of data and more savings through better management of simulations and combating redundancy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 17 figures, Interflam Conference",
    "pdf_url": "https://arxiv.org/pdf/2509.26139v1",
    "published_date": "2025-09-30 11:57:38 UTC",
    "updated_date": "2025-09-30 11:57:38 UTC"
  },
  {
    "arxiv_id": "2509.26128v1",
    "title": "MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models",
    "authors": [
      "Asmita Sengupta",
      "David Antony Selby",
      "Sebastian Josef Vollmer",
      "Gerrit Großmann"
    ],
    "abstract": "Knowledge graphs (KGs) are increasingly used to represent biomedical information in structured, interpretable formats. However, existing biomedical KGs often focus narrowly on molecular interactions or adverse events, overlooking the rich data found in drug leaflets. In this work, we present (1) a hackable, end-to-end pipeline to create KGs from unstructured online content using a web scraper and an LLM; and (2) a curated dataset, MEDAKA, generated by applying this method to publicly available drug leaflets. The dataset captures clinically relevant attributes such as side effects, warnings, contraindications, ingredients, dosage guidelines, storage instructions and physical characteristics. We evaluate it through manual inspection and with an LLM-as-a-Judge framework, and compare its coverage with existing biomedical KGs and databases. We expect MEDAKA to support tasks such as patient safety monitoring and drug recommendation. The pipeline can also be used for constructing KGs from unstructured texts in other domains. Code and dataset are available at https://github.com/medakakg/medaka.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 5 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.26128v1",
    "published_date": "2025-09-30 11:47:04 UTC",
    "updated_date": "2025-09-30 11:47:04 UTC"
  },
  {
    "arxiv_id": "2509.26120v1",
    "title": "AGOCS -- Accurate Google Cloud Simulator Framework",
    "authors": [
      "Leszek Sliwko",
      "Vladimir Getov"
    ],
    "abstract": "This paper presents the Accurate Google Cloud Simulator (AGOCS) - a novel high-fidelity Cloud workload simulator based on parsing real workload traces, which can be conveniently used on a desktop machine for day-to-day research. Our simulation is based on real-world workload traces from a Google Cluster with 12.5K nodes, over a period of a calendar month. The framework is able to reveal very precise and detailed parameters of the executed jobs, tasks and nodes as well as to provide actual resource usage statistics. The system has been implemented in Scala language with focus on parallel execution and an easy-to-extend design concept. The paper presents the detailed structural framework for AGOCS and discusses our main design decisions, whilst also suggesting alternative and possibly performance enhancing future approaches. The framework is available via the Open Source GitHub repository.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "This is the accepted author's version of the paper. The final published version is available in the Proceedings of the 2016 IEEE International Conferences on Ubiquitous Intelligence and Computing (UIC), Advanced and Trusted Computing (ATC), Scalable Computing and Communications (ScalCom), Cloud and Big Data Computing (CBDCom), Internet of People (IoP), and Smart World Congress (SmartWorld)",
    "pdf_url": "https://arxiv.org/pdf/2509.26120v1",
    "published_date": "2025-09-30 11:40:04 UTC",
    "updated_date": "2025-09-30 11:40:04 UTC"
  },
  {
    "arxiv_id": "2509.26113v1",
    "title": "Enhancing PINN Performance Through Lie Symmetry Group",
    "authors": [
      "Ali Haider Shah",
      "Naveed R. Butt",
      "Asif Ahmad",
      "Muhammad Omer Bin Saeed"
    ],
    "abstract": "This paper presents intersection of Physics informed neural networks (PINNs) and Lie symmetry group to enhance the accuracy and efficiency of solving partial differential equation (PDEs). Various methods have been developed to solve these equations. A Lie group is an efficient method that can lead to exact solutions for the PDEs that possessing Lie Symmetry. Leveraging the concept of infinitesimal generators from Lie symmetry group in a novel manner within PINN leads to significant improvements in solution of PDEs. In this study three distinct cases are discussed, each showing progressive improvements achieved through Lie symmetry modifications and adaptive techniques. State-of-the-art numerical methods are adopted for comparing the progressive PINN models. Numerical experiments demonstrate the key role of Lie symmetry in enhancing PINNs performance, emphasizing the importance of integrating abstract mathematical concepts into deep learning for addressing complex scientific problems adequately.",
    "categories": [
      "math.AP",
      "cs.AI"
    ],
    "primary_category": "math.AP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26113v1",
    "published_date": "2025-09-30 11:30:46 UTC",
    "updated_date": "2025-09-30 11:30:46 UTC"
  },
  {
    "arxiv_id": "2509.26106v1",
    "title": "Autonomous Multi-Robot Infrastructure for AI-Enabled Healthcare Delivery and Diagnostics",
    "authors": [
      "Nakhul Kalaivanan",
      "Senthil Arumugam Muthukumaraswamy",
      "Girish Balasubramanian"
    ],
    "abstract": "This research presents a multi-robot system for inpatient care, designed using swarm intelligence principles and incorporating wearable health sensors, RF-based communication, and AI-driven decision support. Within a simulated hospital environment, the system adopts a leader-follower swarm configuration to perform patient monitoring, medicine delivery, and emergency assistance. Due to ethical constraints, live patient trials were not conducted; instead, validation was carried out through controlled self-testing with wearable sensors. The Leader Robot acquires key physiological parameters, including temperature, SpO2, heart rate, and fall detection, and coordinates other robots when required. The Assistant Robot patrols corridors for medicine delivery, while a robotic arm provides direct drug administration. The swarm-inspired leader-follower strategy enhanced communication reliability and ensured continuous monitoring, including automated email alerts to healthcare staff. The system hardware was implemented using Arduino, Raspberry Pi, NRF24L01 RF modules, and a HuskyLens AI camera. Experimental evaluation showed an overall sensor accuracy above 94%, a 92% task-level success rate, and a 96% communication reliability rate, demonstrating system robustness. Furthermore, the AI-enabled decision support was able to provide early warnings of abnormal health conditions, highlighting the potential of the system as a cost-effective solution for hospital automation and patient safety.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "11 pages, 5 figures, MSc dissertation submission draft, prepared for conference/journal consideration",
    "pdf_url": "https://arxiv.org/pdf/2509.26106v1",
    "published_date": "2025-09-30 11:27:33 UTC",
    "updated_date": "2025-09-30 11:27:33 UTC"
  },
  {
    "arxiv_id": "2509.26103v1",
    "title": "End-to-End Aspect-Guided Review Summarization at Scale",
    "authors": [
      "Ilya Boytsov",
      "Vinny DeGenova",
      "Mikhail Balyasin",
      "Joseph Walt",
      "Caitlin Eusden",
      "Marie-Claire Rochat",
      "Margaret Pierson"
    ],
    "abstract": "We present a scalable large language model (LLM)-based system that combines aspect-based sentiment analysis (ABSA) with guided summarization to generate concise and interpretable product review summaries for the Wayfair platform. Our approach first extracts and consolidates aspect-sentiment pairs from individual reviews, selects the most frequent aspects for each product, and samples representative reviews accordingly. These are used to construct structured prompts that guide the LLM to produce summaries grounded in actual customer feedback. We demonstrate the real-world effectiveness of our system through a large-scale online A/B test. Furthermore, we describe our real-time deployment strategy and release a dataset of 11.8 million anonymized customer reviews covering 92,000 products, including extracted aspects and generated summaries, to support future research in aspect-guided review summarization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera-ready preprint for EMNLP 2025 Industry Track",
    "pdf_url": "https://arxiv.org/pdf/2509.26103v1",
    "published_date": "2025-09-30 11:24:07 UTC",
    "updated_date": "2025-09-30 11:24:07 UTC"
  },
  {
    "arxiv_id": "2509.26100v1",
    "title": "SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs",
    "authors": [
      "Yixu Wang",
      "Xin Wang",
      "Yang Yao",
      "Xinyuan Li",
      "Yan Teng",
      "Xingjun Ma",
      "Yingchun Wang"
    ],
    "abstract": "The rapid integration of Large Language Models (LLMs) into high-stakes domains necessitates reliable safety and compliance evaluation. However, existing static benchmarks are ill-equipped to address the dynamic nature of AI risks and evolving regulations, creating a critical safety gap. This paper introduces a new paradigm of agentic safety evaluation, reframing evaluation as a continuous and self-evolving process rather than a one-time audit. We then propose a novel multi-agent framework SafeEvalAgent, which autonomously ingests unstructured policy documents to generate and perpetually evolve a comprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline of specialized agents and incorporates a Self-evolving Evaluation loop, where the system learns from evaluation results to craft progressively more sophisticated and targeted test cases. Our experiments demonstrate the effectiveness of SafeEvalAgent, showing a consistent decline in model safety as the evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act drops from 72.50% to 36.36% over successive iterations. These findings reveal the limitations of static assessments and highlight our framework's ability to uncover deep vulnerabilities missed by traditional methods, underscoring the urgent need for dynamic evaluation ecosystems to ensure the safe and responsible deployment of advanced AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26100v1",
    "published_date": "2025-09-30 11:20:41 UTC",
    "updated_date": "2025-09-30 11:20:41 UTC"
  },
  {
    "arxiv_id": "2509.26094v1",
    "title": "On Computing Top-$k$ Simple Shortest Paths from a Single Source",
    "authors": [
      "Mattia D'Emidio",
      "Gabriele Di Stefano"
    ],
    "abstract": "We investigate the problem of computing the top-$k$ simple shortest paths in weighted digraphs. While the single-pair variant -- finding the top-$k$ simple shortest paths between two specified vertices -- has been extensively studied over the past decades, with Yen's algorithm and its heuristic improvements emerging as the most effective solving strategies, relatively little attention has been devoted to the more general single-source version, where the goal is determining top-$k$ simple shortest paths from a source vertex to all other vertices. Motivated by the numerous practical applications of ranked shortest paths, in this paper we provide new insights and algorithmic contributions to this problem. In particular, we first present a theoretical characterization of the structural properties of its solutions. Then, we introduce the first polynomial-time algorithm specifically designed to handle it. On the one hand, we prove our new algorithm is on par, in terms of time complexity, with the best (and only) polynomial-time approach known in the literature to solve the problem, that is applying the fastest single-pair algorithm independently to each vertex pair formed by the source and the remaining vertices. On the other hand, through an extensive experimental evaluation on both real-world and synthetic graphs, we demonstrate that our algorithm consistently and significantly outperforms the latter baseline in terms of running time, achieving speed-ups of up to several orders of magnitude. These results establish our new algorithm as the solution to be preferred for computing $k$ simple shortest paths from a single source in practical settings.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.IR",
      "cs.NI"
    ],
    "primary_category": "cs.DS",
    "comment": "21 pages, 2 figures, to be published in ALENEX 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.26094v1",
    "published_date": "2025-09-30 11:12:05 UTC",
    "updated_date": "2025-09-30 11:12:05 UTC"
  },
  {
    "arxiv_id": "2509.26080v2",
    "title": "Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research",
    "authors": [
      "Emma Rose Madden"
    ],
    "abstract": "Large Language Models (LLMs) are being increasingly used as synthetic agents in social science, in applications ranging from augmenting survey responses to powering multi-agent simulations. This paper outlines cautions that should be taken when interpreting LLM outputs and proposes a pragmatic reframing for the social sciences in which LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions and not as substitutes for probabilistic inference. Practical guardrails such as independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration, are introduced so that researchers may engage in useful prototyping and forecasting while avoiding category errors.",
    "categories": [
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26080v2",
    "published_date": "2025-09-30 10:53:54 UTC",
    "updated_date": "2025-10-28 13:00:46 UTC"
  },
  {
    "arxiv_id": "2509.26058v2",
    "title": "Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts",
    "authors": [
      "Hossein Enshaei",
      "Pariya Jebreili",
      "Sayed Mahmoud Sakhaei"
    ],
    "abstract": "Electroencephalogram (EEG) artifact detection in real-world settings faces significant challenges such as computational inefficiency in multi-channel methods, poor robustness to simultaneous noise, and trade-offs between accuracy and complexity in deep learning models. We propose a hybrid spectral-temporal framework for real-time detection and classification of ocular (EOG), muscular (EMG), and white noise artifacts in single-channel EEG. This method, in contrast to other approaches, combines time-domain low-pass filtering (targeting low-frequency EOG) and frequency-domain power spectral density (PSD) analysis (capturing broad-spectrum EMG), followed by PCA-optimized feature fusion to minimize redundancy while preserving discriminative information. This feature engineering strategy allows a lightweight multi-layer perceptron (MLP) architecture to outperform advanced CNNs and RNNs by achieving 99% accuracy at low SNRs (SNR -7) dB and >90% accuracy in moderate noise (SNR 4 dB). Additionally, this framework addresses the unexplored problem of simultaneous multi-source contamination(EMG+EOG+white noise), where it maintains 96% classification accuracy despite overlapping artifacts. With 30-second training times (97% faster than CNNs) and robust performance across SNR levels, this framework bridges the gap between clinical applicability and computational efficiency, which enables real-time use in wearable brain-computer interfaces. This work also challenges the ubiquitous dependence on model depth for EEG artifact detection by demonstrating that domain-informed feature fusion surpasses complex architecture in noisy scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26058v2",
    "published_date": "2025-09-30 10:32:38 UTC",
    "updated_date": "2025-10-09 16:36:30 UTC"
  },
  {
    "arxiv_id": "2509.26051v1",
    "title": "CEAID: Benchmark of Multilingual Machine-Generated Text Detection Methods for Central European Languages",
    "authors": [
      "Dominik Macko",
      "Jakub Kopal"
    ],
    "abstract": "Machine-generated text detection, as an important task, is predominantly focused on English in research. This makes the existing detectors almost unusable for non-English languages, relying purely on cross-lingual transferability. There exist only a few works focused on any of Central European languages, leaving the transferability towards these languages rather unexplored. We fill this gap by providing the first benchmark of detection methods focused on this region, while also providing comparison of train-languages combinations to identify the best performing ones. We focus on multi-domain, multi-generator, and multilingual evaluation, pinpointing the differences of individual aspects, as well as adversarial robustness of detection methods. Supervised finetuned detectors in the Central European languages are found the most performant in these languages as well as the most resistant against obfuscation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26051v1",
    "published_date": "2025-09-30 10:27:53 UTC",
    "updated_date": "2025-09-30 10:27:53 UTC"
  },
  {
    "arxiv_id": "2509.26037v1",
    "title": "CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search",
    "authors": [
      "Zhe Li",
      "Zhiwei Lin",
      "Yongtao Wang"
    ],
    "abstract": "The integration of Large Language Models (LLMs) with Neural Architecture Search (NAS) has introduced new possibilities for automating the design of neural architectures. However, most existing methods face critical limitations, including architectural invalidity, computational inefficiency, and inferior performance compared to traditional NAS. In this work, we present Collaborative LLM-based NAS (CoLLM-NAS), a two-stage NAS framework with knowledge-guided search driven by two complementary LLMs. Specifically, we propose a Navigator LLM to guide search direction and a Generator LLM to synthesize high-quality candidates, with a dedicated Coordinator module to manage their interaction. CoLLM-NAS efficiently guides the search process by combining LLMs' inherent knowledge of structured neural architectures with progressive knowledge from iterative feedback and historical trajectory. Experimental results on ImageNet and NAS-Bench-201 show that CoLLM-NAS surpasses existing NAS methods and conventional search algorithms, achieving new state-of-the-art results. Furthermore, CoLLM-NAS consistently enhances the performance and efficiency of various two-stage NAS methods (e.g., OFA, SPOS, and AutoFormer) across diverse search spaces (e.g., MobileNet, ShuffleNet, and AutoFormer), demonstrating its excellent generalization.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26037v1",
    "published_date": "2025-09-30 10:12:49 UTC",
    "updated_date": "2025-09-30 10:12:49 UTC"
  },
  {
    "arxiv_id": "2509.26036v2",
    "title": "SeMoBridge: Semantic Modality Bridge for Efficient Few-Shot Adaptation of CLIP",
    "authors": [
      "Christoph Timmermann",
      "Hyunse Lee",
      "Woojin Lee"
    ],
    "abstract": "While Contrastive Language-Image Pretraining (CLIP) excels at zero-shot tasks by aligning image and text embeddings, its performance in few-shot classification is hindered by a critical limitation: intra-modal misalignment. This issue, caused by a persistent modality gap and CLIP's exclusively inter-modal training objective, leaves the embedding spaces uncalibrated, making direct image-to-image comparisons unreliable. Existing methods attempt to address this by refining similarity logits or by computationally expensive per-sample optimization. To overcome these challenges, we introduce SeMoBridge, a lightweight yet powerful approach that directly addresses the misalignment. Our method maps images into the text modality, while keeping their semantic content intact through what we call a Semantic Modality Bridge. SeMoBridge is closed-form and can optionally be trained through multi-modal supervision, combining image and text-alignment losses to optimize the projection. Experiments show that the trained version, SeMoBridge-T, requires only a fraction of the training time while overall outperforming other methods, particularly in low-data scenarios (1, 2, and 4 shots). The code is available at https://github.com/christti98/semobridge.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 12 figures, Under review as a conference paper at ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.26036v2",
    "published_date": "2025-09-30 10:12:15 UTC",
    "updated_date": "2025-10-01 09:18:57 UTC"
  },
  {
    "arxiv_id": "2509.26030v2",
    "title": "Muon Outperforms Adam in Tail-End Associative Memory Learning",
    "authors": [
      "Shuche Wang",
      "Fengzhuo Zhang",
      "Jiaxiang Li",
      "Cunxiao Du",
      "Chao Du",
      "Tianyu Pang",
      "Zhuoran Yang",
      "Mingyi Hong",
      "Vincent Y. F. Tan"
    ],
    "abstract": "The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26030v2",
    "published_date": "2025-09-30 10:04:08 UTC",
    "updated_date": "2025-10-05 09:26:34 UTC"
  },
  {
    "arxiv_id": "2509.26015v1",
    "title": "Indirect Attention: Turning Context Misalignment into a Feature",
    "authors": [
      "Bissmella Bahaduri",
      "Hicham Talaoubrid",
      "Fangchen Feng",
      "Zuheng Ming",
      "Anissa Mokraoui"
    ],
    "abstract": "The attention mechanism has become a cornerstone of modern deep learning architectures, where keys and values are typically derived from the same underlying sequence or representation. This work explores a less conventional scenario, when keys and values originate from different sequences or modalities. Specifically, we first analyze the attention mechanism's behavior under noisy value features, establishing a critical noise threshold beyond which signal degradation becomes significant. Furthermore, we model context (key, value) misalignment as an effective form of structured noise within the value features, demonstrating that the noise induced by such misalignment can substantially exceed this critical threshold, thereby compromising standard attention's efficacy. Motivated by this, we introduce Indirect Attention, a modified attention mechanism that infers relevance indirectly in scenarios with misaligned context. We evaluate the performance of Indirect Attention across a range of synthetic tasks and real world applications, showcasing its superior ability to handle misalignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26015v1",
    "published_date": "2025-09-30 09:44:00 UTC",
    "updated_date": "2025-09-30 09:44:00 UTC"
  },
  {
    "arxiv_id": "2509.26008v1",
    "title": "PFDepth: Heterogeneous Pinhole-Fisheye Joint Depth Estimation via Distortion-aware Gaussian-Splatted Volumetric Fusion",
    "authors": [
      "Zhiwei Zhang",
      "Ruikai Xu",
      "Weijian Zhang",
      "Zhizhong Zhang",
      "Xin Tan",
      "Jingyu Gong",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "abstract": "In this paper, we present the first pinhole-fisheye framework for heterogeneous multi-view depth estimation, PFDepth. Our key insight is to exploit the complementary characteristics of pinhole and fisheye imagery (undistorted vs. distorted, small vs. large FOV, far vs. near field) for joint optimization. PFDepth employs a unified architecture capable of processing arbitrary combinations of pinhole and fisheye cameras with varied intrinsics and extrinsics. Within PFDepth, we first explicitly lift 2D features from each heterogeneous view into a canonical 3D volumetric space. Then, a core module termed Heterogeneous Spatial Fusion is designed to process and fuse distortion-aware volumetric features across overlapping and non-overlapping regions. Additionally, we subtly reformulate the conventional voxel fusion into a novel 3D Gaussian representation, in which learnable latent Gaussian spheres dynamically adapt to local image textures for finer 3D aggregation. Finally, fused volume features are rendered into multi-view depth maps. Through extensive experiments, we demonstrate that PFDepth sets a state-of-the-art performance on KITTI-360 and RealHet datasets over current mainstream depth networks. To the best of our knowledge, this is the first systematic study of heterogeneous pinhole-fisheye depth estimation, offering both technical novelty and valuable empirical insights.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACM MM 2025 Conference",
    "pdf_url": "https://arxiv.org/pdf/2509.26008v1",
    "published_date": "2025-09-30 09:38:59 UTC",
    "updated_date": "2025-09-30 09:38:59 UTC"
  },
  {
    "arxiv_id": "2509.26007v1",
    "title": "MARS: Audio Generation via Multi-Channel Autoregression on Spectrograms",
    "authors": [
      "Eleonora Ristori",
      "Luca Bindini",
      "Paolo Frasconi"
    ],
    "abstract": "Research on audio generation has progressively shifted from waveform-based approaches to spectrogram-based methods, which more naturally capture harmonic and temporal structures. At the same time, advances in image synthesis have shown that autoregression across scales, rather than tokens, improves coherence and detail. Building on these ideas, we introduce MARS (Multi-channel AutoRegression on Spectrograms), a framework that treats spectrograms as multi-channel images and employs channel multiplexing (CMX), a reshaping technique that lowers height and width without discarding information. A shared tokenizer provides consistent discrete representations across scales, enabling a transformer-based autoregressor to refine spectrograms from coarse to fine resolutions efficiently. Experiments on a large-scale dataset demonstrate that MARS performs comparably or better than state-of-the-art baselines across multiple evaluation metrics, establishing an efficient and scalable paradigm for high-fidelity audio generation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.26007v1",
    "published_date": "2025-09-30 09:38:02 UTC",
    "updated_date": "2025-09-30 09:38:02 UTC"
  },
  {
    "arxiv_id": "2509.26004v2",
    "title": "Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations",
    "authors": [
      "Nicola Messina",
      "Rosario Leonardi",
      "Luca Ciampi",
      "Fabio Carrara",
      "Giovanni Maria Farinella",
      "Fabrizio Falchi",
      "Antonino Furnari"
    ],
    "abstract": "Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations $\\unicode{x2013}$ natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations in a weakly-supervised regime. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations. Code and data can be found at https://fpv-iplab.github.io/WISH.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under consideration at Pattern Recognition Letters",
    "pdf_url": "https://arxiv.org/pdf/2509.26004v2",
    "published_date": "2025-09-30 09:34:55 UTC",
    "updated_date": "2025-12-02 12:27:47 UTC"
  },
  {
    "arxiv_id": "2509.26002v1",
    "title": "Towards Human Engagement with Realistic AI Combat Pilots",
    "authors": [
      "Ardian Selmonaj",
      "Giacomo Del Rio",
      "Adrian Schneider",
      "Alessandro Antonucci"
    ],
    "abstract": "We present a system that enables real-time interaction between human users and agents trained to control fighter jets in simulated 3D air combat scenarios. The agents are trained in a dedicated environment using Multi-Agent Reinforcement Learning. A communication link is developed to allow seamless deployment of trained agents into VR-Forces, a widely used defense simulation tool for realistic tactical scenarios. This integration allows mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors. Our interaction model creates new opportunities for human-agent teaming, immersive training, and the exploration of innovative tactics in defense contexts.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "13th International Conference on Human-Agent Interaction (HAI) 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.26002v1",
    "published_date": "2025-09-30 09:34:10 UTC",
    "updated_date": "2025-09-30 09:34:10 UTC"
  },
  {
    "arxiv_id": "2509.25998v3",
    "title": "VRWKV-Editor: Reducing quadratic complexity in transformer-based video editing",
    "authors": [
      "Abdelilah Aitrouga",
      "Youssef Hmamouche",
      "Amal El Fallah Seghrouchni"
    ],
    "abstract": "In light of recent progress in video editing, deep learning models focusing on both spatial and temporal dependencies have emerged as the primary method. However, these models suffer from the quadratic computational complexity of traditional attention mechanisms, making them difficult to adapt to long-duration and high-resolution videos. This limitation restricts their applicability in practical contexts such as real-time video processing. To tackle this challenge, we introduce a method to reduce both time and space complexity of these systems by proposing VRWKV-Editor, a novel video editing model that integrates a linear spatio-temporal aggregation module into video-based diffusion models. VRWKV-Editor leverages bidirectional weighted key-value recurrence mechanism of the RWKV transformer to capture global dependencies while preserving temporal coherence, achieving linear complexity without sacrificing quality. Extensive experiments demonstrate that the proposed method achieves up to 3.7x speedup and 60% lower memory usage compared to state-of-the-art diffusion-based video editing methods, while maintaining competitive performance in frame consistency and text alignment. Furthermore, a comparative analysis we conducted on videos with different sequence lengths confirms that the gap in editing speed between our approach and architectures with self-attention becomes more significant with long videos.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25998v3",
    "published_date": "2025-09-30 09:30:23 UTC",
    "updated_date": "2025-12-04 03:28:47 UTC"
  },
  {
    "arxiv_id": "2509.25992v1",
    "title": "MHINDR -- a DSM5 based mental health diagnosis and recommendation framework using LLM",
    "authors": [
      "Vaishali Agarwal",
      "Sachin Thukral",
      "Arnab Chatterjee"
    ],
    "abstract": "Mental health forums offer valuable insights into psychological issues, stressors, and potential solutions. We propose MHINDR, a large language model (LLM) based framework integrated with DSM-5 criteria to analyze user-generated text, dignose mental health conditions, and generate personalized interventions and insights for mental health practitioners. Our approach emphasizes on the extraction of temporal information for accurate diagnosis and symptom progression tracking, together with psychological features to create comprehensive mental health summaries of users. The framework delivers scalable, customizable, and data-driven therapeutic recommendations, adaptable to diverse clinical contexts, patient needs, and workplace well-being programs.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.SI",
    "comment": "7 pages, 1 figure, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.25992v1",
    "published_date": "2025-09-30 09:26:38 UTC",
    "updated_date": "2025-09-30 09:26:38 UTC"
  },
  {
    "arxiv_id": "2509.25991v2",
    "title": "Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline",
    "authors": [
      "Haiyang Li",
      "Yaxiong Wang",
      "Shengeng Tang",
      "Lianwei Wu",
      "Lechao Cheng",
      "Zhun Zhong"
    ],
    "abstract": "In recent years, detecting fake multimodal content on social media has drawn increasing attention. Two major forms of deception dominate: human-crafted misinformation (e.g., rumors and misleading posts) and AI-generated content produced by image synthesis models or vision-language models (VLMs). Although both share deceptive intent, they are typically studied in isolation. NLP research focuses on human-written misinformation, while the CV community targets AI-generated artifacts. As a result, existing models are often specialized for only one type of fake content. In real-world scenarios, however, the type of a multimodal post is usually unknown, limiting the effectiveness of such specialized systems. To bridge this gap, we construct the Omnibus Dataset for Multimodal News Deception (OmniFake), a comprehensive benchmark of 127K samples that integrates human-curated misinformation from existing resources with newly synthesized AI-generated examples. Based on this dataset, we propose Unified Multimodal Fake Content Detection (UMFDet), a framework designed to handle both forms of deception. UMFDet leverages a VLM backbone augmented with a Category-aware Mixture-of-Experts (MoE) Adapter to capture category-specific cues, and an attribution chain-of-thought mechanism that provides implicit reasoning guidance for locating salient deceptive signals. Extensive experiments demonstrate that UMFDet achieves robust and consistent performance across both misinformation types, outperforming specialized baselines and offering a practical solution for real-world multimodal deception detection.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25991v2",
    "published_date": "2025-09-30 09:26:32 UTC",
    "updated_date": "2025-10-15 10:52:13 UTC"
  },
  {
    "arxiv_id": "2509.25987v2",
    "title": "R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning",
    "authors": [
      "Yilun Liu",
      "Ziang Chen",
      "Song Xu",
      "Minggui He",
      "Shimin Tao",
      "Weibin Meng",
      "Yuming Xie",
      "Tao Han",
      "Chunguang Zhao",
      "Jingzhou Du",
      "Daimeng Wei",
      "Shenglin Zhang",
      "Yongqian Sun"
    ],
    "abstract": "The growing complexity of log data in modern software systems has prompted the use of Large Language Models (LLMs) for automated log analysis. Current approaches typically rely on direct supervised fine-tuning (SFT) on log-label pairs. However, this exacerbates the domain discrepancy between general-purpose LLMs and specialized log data, causing overfitting. Furthermore, SFT's imbalanced loss computation often allows lengthy contexts to overwhelm critical, concise details in model answers, leading to hallucinations. To address these limitations, we propose R-Log, a novel reasoning-based paradigm that mirrors the structured, step-by-step analytical process of human engineers. This approach enhances generalizability by learning the underlying rules behind conclusions. We further employ Reinforcement Learning (RL) to optimize the model within a simulated O&M environment, thereby reducing hallucinations by directly rewarding correct outcomes. R-Log is first cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13 strategies from manual O&M practices, to establish an initial reasoning capability. This ability is then refined via RL using a joint reward function. Empirical evaluations on real-world logs show that R-Log outperforms existing methods across five log analysis tasks, particularly in unseen scenarios (by 228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the efficacy.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by ICSE 2026 (SEIP Track)",
    "pdf_url": "https://arxiv.org/pdf/2509.25987v2",
    "published_date": "2025-09-30 09:19:31 UTC",
    "updated_date": "2025-12-29 10:05:56 UTC"
  },
  {
    "arxiv_id": "2509.25979v1",
    "title": "Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier",
    "authors": [
      "Gaojie Jin",
      "Xinping Yi",
      "Xiaowei Huang"
    ],
    "abstract": "Within the PAC-Bayesian framework, the Gibbs classifier (defined on a posterior $Q$) and the corresponding $Q$-weighted majority vote classifier are commonly used to analyze the generalization performance. However, there exists a notable lack in theoretical research exploring the certified robustness of majority vote classifier and its interplay with generalization. In this study, we develop a generalization error bound that possesses a certified robust radius for the smoothed majority vote classifier (i.e., the $Q$-weighted majority vote classifier with smoothed inputs); In other words, the generalization bound holds under any data perturbation within the certified robust radius. As a byproduct, we find that the underpinnings of both the generalization bound and the certified robust radius draw, in part, upon weight spectral norm, which thereby inspires the adoption of spectral regularization in smooth training to boost certified robustness. Utilizing the dimension-independent property of spherical Gaussian inputs in smooth training, we propose a novel and inexpensive spectral regularizer to enhance the smoothed majority vote classifier. In addition to the theoretical contribution, a set of empirical results is provided to substantiate the effectiveness of our proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2509.25979v1",
    "published_date": "2025-09-30 09:11:10 UTC",
    "updated_date": "2025-09-30 09:11:10 UTC"
  },
  {
    "arxiv_id": "2509.25977v2",
    "title": "Data-Free Continual Learning of Server Models in Model-Heterogeneous Cloud-Device Collaboration",
    "authors": [
      "Xiao Zhang",
      "Zengzhe Chen",
      "Yuan Yuan",
      "Yifei Zou",
      "Fuzhen Zhuang",
      "Wenyu Jiao",
      "Yuke Wang",
      "Dongxiao Yu"
    ],
    "abstract": "The rise of cloud-device collaborative computing has enabled intelligent services to be delivered across distributed edge devices while leveraging centralized cloud resources. In this paradigm, federated learning (FL) has become a key enabler for privacy-preserving model training without transferring raw data from edge devices to the cloud. However, with the continuous emergence of new data and increasing model diversity, traditional federated learning faces significant challenges, including inherent issues of data heterogeneity, model heterogeneity and catastrophic forgetting, along with new challenge of knowledge misalignment. In this study, we introduce FedDCL, a novel framework designed to enable data-free continual learning of the server model in a model-heterogeneous federated setting. We leverage pre-trained diffusion models to extract lightweight class-specific prototypes, which confer a threefold data-free advantage, enabling: (1) generation of synthetic data for the current task to augment training and counteract non-IID data distributions; (2) exemplar-free generative replay for retaining knowledge from previous tasks; and (3) data-free dynamic knowledge transfer from heterogeneous devices to the cloud server.Experimental results on various datasets demonstrate the effectiveness of FedDCL, showcasing its potential to enhance the generalizability and practical applicability of federated cloud-device collaboration in dynamic settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25977v2",
    "published_date": "2025-09-30 09:09:33 UTC",
    "updated_date": "2025-12-19 06:08:01 UTC"
  },
  {
    "arxiv_id": "2509.25973v1",
    "title": "Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions",
    "authors": [
      "Junbeom Kim",
      "Kyuyoung Kim",
      "Jihoon Tack",
      "Dongha Lim",
      "Jinwoo Shin"
    ],
    "abstract": "Language models trained on web-scale corpora risk memorizing and exposing sensitive information, prompting the need for effective machine unlearning. Prior methods mainly focus on input queries to suppress sensitive outputs, yet this often fails to eliminate the underlying knowledge and limits scalability. To address this, we propose Corrective Unlearning with Retrieved Exclusions (CURE), a novel unlearning framework that verifies model outputs for leakage and revises them into safe responses. Specifically, CURE employs a lightweight corrector that is applied to the original model to verify whether outputs contain target knowledge and to rewrite them if any leakage is detected. To efficiently handle large-scale unlearning requests, CURE retrieves unlearning targets that are relevant to the initial response and provides them as in-context references to the corrector for detection and conditional revision. By leveraging this retrieval augmentation, the corrector can adapt to new unlearning requests without additional training. Extensive evaluations demonstrate that CURE substantially reduces information leakage, even from indirect queries where prior works fall short, while maintaining response quality and general utility. Moreover, it demonstrates robustness under continual unlearning scenarios, making it practical for real-world applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25973v1",
    "published_date": "2025-09-30 09:07:45 UTC",
    "updated_date": "2025-09-30 09:07:45 UTC"
  },
  {
    "arxiv_id": "2509.25958v1",
    "title": "RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning",
    "authors": [
      "Gang Li",
      "Yulei Qin",
      "Xiaoyu Tan",
      "Dingkang Yang",
      "Yuchen Shi",
      "Zihan Xu",
      "Xiang Li",
      "Xing Sun",
      "Ke Li"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in eliciting complex reasoning in large language models (LLMs). However, standard RLVR training often leads to excessively verbose processes (in reasoning tasks) and inefficient exploration trajectories (in agentic settings), as outcome-only rewards provide no incentive for efficiency and the high variance in response length within relatively small rollout groups results in noisy optimization signals. To address this, we propose Rollout Response Recomposition (RoRecomp), a plug-and-play method that guides models toward concise reasoning by strategically recomposing the training data. RoRecomp separates responses into two distinct batch types: 1) priority batches, which combine short-correct and long-incorrect responses selected from online batches to provide a clear gradient signal for brevity, and 2) compensation batches, which utilize remaining responses from a replay buffer to maintain stability and prevent model collapse. To comprehensively evaluate effectiveness, we test RoRecomp across three settings where results demonstrate substantial efficiency gains: reducing reasoning length by 27.7% in zero RL training, reducing unnecessary tool calls by 46.8% while improving accuracy in agentic RL, and achieving up to 52.5% length reduction in thinking compression, all with minimal performance impact.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25958v1",
    "published_date": "2025-09-30 08:54:38 UTC",
    "updated_date": "2025-09-30 08:54:38 UTC"
  },
  {
    "arxiv_id": "2510.00084v1",
    "title": "Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems",
    "authors": [
      "Fabian Kovac",
      "Sebastian Neumaier",
      "Timea Pahi",
      "Torsten Priebe",
      "Rafael Rodrigues",
      "Dimitrios Christodoulou",
      "Maxime Cordy",
      "Sylvain Kubler",
      "Ali Kordia",
      "Georgios Pitsiladis",
      "John Soldatos",
      "Petros Zervoudakis"
    ],
    "abstract": "Artificial Intelligence has rapidly become a cornerstone technology, significantly influencing Europe's societal and economic landscapes. However, the proliferation of AI also raises critical ethical, legal, and regulatory challenges. The CERTAIN (Certification for Ethical and Regulatory Transparency in Artificial Intelligence) project addresses these issues by developing a comprehensive framework that integrates regulatory compliance, ethical standards, and transparency into AI systems. In this position paper, we outline the methodological steps for building the core components of this framework. Specifically, we present: (i) semantic Machine Learning Operations (MLOps) for structured AI lifecycle management, (ii) ontology-driven data lineage tracking to ensure traceability and accountability, and (iii) regulatory operations (RegOps) workflows to operationalize compliance requirements. By implementing and validating its solutions across diverse pilots, CERTAIN aims to advance regulatory compliance and to promote responsible AI innovation aligned with European standards.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication in the proceedings of the Workshop on AI Certification, Fairness and Regulations, co-located with the Austrian Symposium on AI and Vision (AIRoV 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.00084v1",
    "published_date": "2025-09-30 08:54:02 UTC",
    "updated_date": "2025-09-30 08:54:02 UTC"
  },
  {
    "arxiv_id": "2509.25955v1",
    "title": "AIM: Adaptive Intervention for Deep Multi-task Learning of Molecular Properties",
    "authors": [
      "Mason Minot",
      "Gisbert Schneider"
    ],
    "abstract": "Simultaneously optimizing multiple, frequently conflicting, molecular properties is a key bottleneck in the development of novel therapeutics. Although a promising approach, the efficacy of multi-task learning is often compromised by destructive gradient interference, especially in the data-scarce regimes common to drug discovery. To address this, we propose AIM, an optimization framework that learns a dynamic policy to mediate gradient conflicts. The policy is trained jointly with the main network using a novel augmented objective composed of dense, differentiable regularizers. This objective guides the policy to produce updates that are geometrically stable and dynamically efficient, prioritizing progress on the most challenging tasks. We demonstrate that AIM achieves statistically significant improvements over multi-task baselines on subsets of the QM9 and targeted protein degraders benchmarks, with its advantage being most pronounced in data-scarce regimes. Beyond performance, AIM's key contribution is its interpretability; the learned policy matrix serves as a diagnostic tool for analyzing inter-task relationships. This combination of data-efficient performance and diagnostic insight highlights the potential of adaptive optimizers to accelerate scientific discovery by creating more robust and insightful models for multi-property molecular design.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 3 figures, 9 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.25955v1",
    "published_date": "2025-09-30 08:47:41 UTC",
    "updated_date": "2025-09-30 08:47:41 UTC"
  },
  {
    "arxiv_id": "2509.25946v1",
    "title": "Automated Model Discovery via Multi-modal & Multi-step Pipeline",
    "authors": [
      "Lee Jung-Mok",
      "Nam Hyeon-Woo",
      "Moon Ye-Bin",
      "Junhyun Nam",
      "Tae-Hyun Oh"
    ],
    "abstract": "Automated model discovery is the process of automatically searching and identifying the most appropriate model for a given dataset over a large combinatorial search space. Existing approaches, however, often face challenges in balancing the capture of fine-grained details with ensuring generalizability beyond training data regimes with a reasonable model complexity. In this paper, we present a multi-modal \\& multi-step pipeline for effective automated model discovery. Our approach leverages two vision-language-based modules (VLM), AnalyzerVLM and EvaluatorVLM, for effective model proposal and evaluation in an agentic way. AnalyzerVLM autonomously plans and executes multi-step analyses to propose effective candidate models. EvaluatorVLM assesses the candidate models both quantitatively and perceptually, regarding the fitness for local details and the generalibility for overall trends. Our results demonstrate that our pipeline effectively discovers models that capture fine details and ensure strong generalizability. Additionally, extensive ablation studies show that both multi-modality and multi-step reasoning play crucial roles in discovering favorable models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25946v1",
    "published_date": "2025-09-30 08:40:05 UTC",
    "updated_date": "2025-09-30 08:40:05 UTC"
  },
  {
    "arxiv_id": "2509.25944v1",
    "title": "NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving",
    "authors": [
      "Yuan Gao",
      "Mattia Piccinini",
      "Roberto Brusnicki",
      "Yuchen Zhang",
      "Johannes Betz"
    ],
    "abstract": "Understanding risk in autonomous driving requires not only perception and prediction, but also high-level reasoning about agent behavior and context. Current Vision Language Models (VLMs)-based methods primarily ground agents in static images and provide qualitative judgments, lacking the spatio-temporal reasoning needed to capture how risks evolve over time. To address this gap, we propose NuRisk, a comprehensive Visual Question Answering (VQA) dataset comprising 2,900 scenarios and 1.1 million agent-level samples, built on real-world data from nuScenes and Waymo, supplemented with safety-critical scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View (BEV) based sequential images with quantitative, agent-level risk annotations, enabling spatio-temporal reasoning. We benchmark well-known VLMs across different prompting techniques and find that they fail to perform explicit spatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency. To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to 41% and reduces latency by 75%, demonstrating explicit spatio-temporal reasoning capabilities that proprietary models lacked. While this represents a significant step forward, the modest accuracy underscores the profound challenge of the task, establishing NuRisk as a critical benchmark for advancing spatio-temporal reasoning in autonomous driving.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.25944v1",
    "published_date": "2025-09-30 08:37:31 UTC",
    "updated_date": "2025-09-30 08:37:31 UTC"
  },
  {
    "arxiv_id": "2509.25941v1",
    "title": "Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice QA",
    "authors": [
      "Raphael Schumann",
      "Stefan Riezler"
    ],
    "abstract": "Reasoning quality in large language models depends not only on producing correct answers but also on generating valid intermediate steps. We study this through multiple-choice question answering (MCQA), which provides a controlled setting with fixed answer options. Our analysis shows that when questions are effectively unsolvable for a model, spurious chains of thought (CoTs) are more likely to appear, leading to false positives. By estimating the solvability of each question, we uncover an intermediate regime where learning is most effective. Building on this insight, we adapt outcome-supervised reward models and reinforcement learning with group-relative advantage to incorporate solvability into their objectives. Across experiments on math and multimodal datasets, these modifications consistently yield higher rates of process-correct reasoning and, in reinforcement learning, improved answer accuracy as well. Our results highlight solvability as a key factor for reducing hallucinations and increasing reliability in CoT reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25941v1",
    "published_date": "2025-09-30 08:34:16 UTC",
    "updated_date": "2025-09-30 08:34:16 UTC"
  },
  {
    "arxiv_id": "2509.25933v1",
    "title": "From MNIST to ImageNet: Understanding the Scalability Boundaries of Differentiable Logic Gate Networks",
    "authors": [
      "Sven Brändle",
      "Till Aczel",
      "Andreas Plesner",
      "Roger Wattenhofer"
    ],
    "abstract": "Differentiable Logic Gate Networks (DLGNs) are a very fast and energy-efficient alternative to conventional feed-forward networks. With learnable combinations of logical gates, DLGNs enable fast inference by hardware-friendly execution. Since the concept of DLGNs has only recently gained attention, these networks are still in their developmental infancy, including the design and scalability of their output layer. To date, this architecture has primarily been tested on datasets with up to ten classes.\n  This work examines the behavior of DLGNs on large multi-class datasets. We investigate its general expressiveness, its scalability, and evaluate alternative output strategies. Using both synthetic and real-world datasets, we provide key insights into the importance of temperature tuning and its impact on output layer performance. We evaluate conditions under which the Group-Sum layer performs well and how it can be applied to large-scale classification of up to 2000 classes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25933v1",
    "published_date": "2025-09-30 08:27:58 UTC",
    "updated_date": "2025-09-30 08:27:58 UTC"
  },
  {
    "arxiv_id": "2509.25928v1",
    "title": "Quantitative Evaluation of KIRETT Wearable Demonstrator for Rescue Operations",
    "authors": [
      "Mubaris Nadeem",
      "Johannes Zenkert",
      "Lisa Bender",
      "Christian Weber",
      "Madjid Fathi"
    ],
    "abstract": "Healthcare and Medicine are under constant pressure to provide patient-driven medical expertise to ensure a fast and accurate treatment of the patient. In such scenarios, the diagnosis contains, the family history, long term medical data and a detailed consultation with the patient. In time-critical emergencies, such conversation and time-consuming elaboration are not possible. Rescue services need to provide fast, reliable treatments for the patient in need. With the help of modern technologies, like treatment recommendations, real-time vitals-monitoring, and situation detection through artificial intelligence (AI) a situation can be analyzed and supported in providing fast, accurate patient-data-driven medical treatments. In KIRETT, a wearable device is developed to support in such scenarios and presents a way to provide treatment recommendation in rescue services. The objective of this paper is to present the quantitative results of a two-day KIRETT evaluation (14 participants) to analyze the needs of rescue operators in healthcare.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Conference paper for 2024 IEEE World AI IoT Congress (AIIoT), KIRETT Project, University of Siegen, Germany",
    "pdf_url": "https://arxiv.org/pdf/2509.25928v1",
    "published_date": "2025-09-30 08:21:09 UTC",
    "updated_date": "2025-09-30 08:21:09 UTC"
  },
  {
    "arxiv_id": "2509.25927v1",
    "title": "The Impact of Scaling Training Data on Adversarial Robustness",
    "authors": [
      "Marco Zimmerli",
      "Andreas Plesner",
      "Till Aczel",
      "Roger Wattenhofer"
    ],
    "abstract": "Deep neural networks remain vulnerable to adversarial examples despite advances in architectures and training paradigms. We investigate how training data characteristics affect adversarial robustness across 36 state-of-the-art vision models spanning supervised, self-supervised, and contrastive learning approaches, trained on datasets from 1.2M to 22B images. Models were evaluated under six black-box attack categories: random perturbations, two types of geometric masks, COCO object manipulations, ImageNet-C corruptions, and ImageNet-R style shifts. Robustness follows a logarithmic scaling law with both data volume and model size: a tenfold increase in data reduces attack success rate (ASR) on average by ~3.2%, whereas a tenfold increase in model size reduces ASR on average by ~13.4%. Notably, some self-supervised models trained on curated datasets, such as DINOv2, outperform others trained on much larger but less curated datasets, challenging the assumption that scale alone drives robustness. Adversarial fine-tuning of ResNet50s improves generalization across structural variations but not across color distributions. Human evaluation reveals persistent gaps between human and machine vision. These results show that while scaling improves robustness, data quality, architecture, and training objectives play a more decisive role than raw scale in achieving broad-spectrum adversarial resilience.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at the workshop Reliable ML from Unreliable Data at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.25927v1",
    "published_date": "2025-09-30 08:20:56 UTC",
    "updated_date": "2025-09-30 08:20:56 UTC"
  },
  {
    "arxiv_id": "2509.25923v1",
    "title": "KIRETT: Smart Integration of Vital Signs Data for Intelligent Decision Support in Rescue Scenarios",
    "authors": [
      "Mubaris Nadeem",
      "Johannes Zenkert",
      "Christian Weber",
      "Lisa Bender",
      "Madjid Fathi"
    ],
    "abstract": "The integration of vital signs in healthcare has witnessed a steady rise, promising health professionals to assist in their daily tasks to improve patient treatment. In life-threatening situations, like rescue operations, crucial decisions need to be made in the shortest possible amount of time to ensure that excellent treatment is provided during life-saving measurements. The integration of vital signs in the treatment holds the potential to improve time utilization for rescuers in such critical situations. They furthermore serve to support health professionals during the treatment with useful information and suggestions. To achieve such a goal, the KIRETT project serves to provide treatment recommendations and situation detection, combined on a wrist-worn wearable for rescue operations.This paper aims to present the significant role of vital signs in the improvement of decision-making during rescue operations and show their impact on health professionals and patients in need.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Conference paper for 2024 IEEE International Conference on Electro Information Technology (eIT), KIRETT Project, University of Siegen, Germany",
    "pdf_url": "https://arxiv.org/pdf/2509.25923v1",
    "published_date": "2025-09-30 08:20:42 UTC",
    "updated_date": "2025-09-30 08:20:42 UTC"
  },
  {
    "arxiv_id": "2509.25922v1",
    "title": "DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models",
    "authors": [
      "Zhicheng Zhou",
      "Jing Li",
      "Suming Qiu",
      "Junjie Huang",
      "Linyuan Qiu",
      "Zhijie Sun"
    ],
    "abstract": "The internet is saturated with low-density, high-redundancy information, such as social media comments, repetitive news, and lengthy discussions, making it difficult to extract valuable insights efficiently. Multi-layer nested JSON structures provide an effective solution by compressing such information into semantically rich, hierarchical representations, which organize data into key-value pairs, arrays, and nested objects, preserving contextual relationships and enabling efficient storage, retrieval, and semantic querying. For instance, in news aggregation, a JSON object can nest an article's metadata (title, author, date), content (text, multimedia), and multimedia information (multimedia type, caption) hierarchically. Large Language Models (LLMs) play a transformative role in web data mining by parsing unstructured text and outputting structured results directly into complex JSON schemas. However, current benchmarks for evaluating LLMs' JSON output capabilities overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities, a limitation that lacks relevance to practical web data mining tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring 2100 multi-domain instances with deep nested structures, categorized by difficulty. Experiments show significant performance gaps among LLMs in handling such complexity. Our benchmark and datasets are open-sourced to advance research in structured JSON generation.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25922v1",
    "published_date": "2025-09-30 08:18:20 UTC",
    "updated_date": "2025-09-30 08:18:20 UTC"
  },
  {
    "arxiv_id": "2509.25919v1",
    "title": "Accelerating LLM Inference with Precomputed Query Storage",
    "authors": [
      "Jay H. Park",
      "Youngju Cho",
      "Choungsol Lee",
      "Moonwook Oh",
      "Euiseong Seo"
    ],
    "abstract": "Large language model (LLM) inference often suffers from high latency, particularly in resource-constrained environments such as on-device or edge deployments. To address this challenge, we present StorInfer, a novel storage-assisted LLM inference system that accelerates response time by precomputing and storing predictable query-response pairs offline. When a user query semantically matches a precomputed query, StorInfer bypasses expensive GPU inference and instantly returns the stored response, significantly reducing latency and compute costs. To maximize coverage and effectiveness, StorInfer employs an LLM-driven generator that adaptively produces diverse and deduplicated queries based on a given knowledge base. This is achieved via two techniques: adaptive query masking, which prevents regeneration of similar queries, and adaptive sampling, which dynamically tunes generation parameters to promote semantic diversity. The resulting query-response pairs are embedded and indexed using a disk-backed vector database to enable fast, similarity-based retrieval at runtime. Using this approach, we generated 150K unique precomputed pairs (taking up to 830 MB of storage space), achieving up to 17.3% latency reduction with no loss in response quality. Our evaluation across multiple QA datasets demonstrates the practicality and scalability of storage-assisted inference, especially in scenarios with predictable query distributions. StorInfer highlights a promising direction in leveraging storage as a primary enabler for efficient, low-latency LLM deployment.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25919v1",
    "published_date": "2025-09-30 08:14:04 UTC",
    "updated_date": "2025-09-30 08:14:04 UTC"
  },
  {
    "arxiv_id": "2509.25905v1",
    "title": "User-Centric Communication Service Provision for Edge-Assisted Mobile Augmented Reality",
    "authors": [
      "Conghao Zhou",
      "Jie Gao",
      "Shisheng Hu",
      "Nan Cheng",
      "Weihua Zhuang",
      "Xuemin Shen"
    ],
    "abstract": "Future 6G networks are envisioned to facilitate edge-assisted mobile augmented reality (MAR) via strengthening the collaboration between MAR devices and edge servers. In order to provide immersive user experiences, MAR devices must timely upload camera frames to an edge server for simultaneous localization and mapping (SLAM)-based device pose tracking. In this paper, to cope with user-specific and non-stationary uplink data traffic, we develop a digital twin (DT)-based approach for user-centric communication service provision for MAR. Specifically, to establish DTs for individual MAR devices, we first construct a data model customized for MAR that captures the intricate impact of the SLAM-based frame uploading mechanism on the user-specific data traffic pattern. We then define two DT operation functions that cooperatively enable adaptive switching between different data-driven models for capturing non-stationary data traffic. Leveraging the user-oriented data management introduced by DTs, we propose an algorithm for network resource management that ensures the timeliness of frame uploading and the robustness against inherent inaccuracies in data traffic modeling for individual MAR devices. Trace-driven simulation results demonstrate that the user-centric communication service provision achieves a 14.2% increase in meeting the camera frame uploading delay requirement in comparison with the slicing-based communication service provision widely used for 5G.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "accepted by IEEE Transactions on Mobile Computing",
    "pdf_url": "https://arxiv.org/pdf/2509.25905v1",
    "published_date": "2025-09-30 07:50:32 UTC",
    "updated_date": "2025-09-30 07:50:32 UTC"
  },
  {
    "arxiv_id": "2509.25903v1",
    "title": "PerQ: Efficient Evaluation of Multilingual Text Personalization Quality",
    "authors": [
      "Dominik Macko",
      "Andrew Pulver"
    ],
    "abstract": "Since no metrics are available to evaluate specific aspects of a text, such as its personalization quality, the researchers often rely solely on large language models to meta-evaluate such texts. Due to internal biases of individual language models, it is recommended to use multiple of them for combined evaluation, which directly increases costs of such meta-evaluation. In this paper, a computationally efficient method for evaluation of personalization quality of a given text (generated by a language model) is introduced, called PerQ. A case study of comparison of generation capabilities of large and small language models shows the usability of the proposed metric in research, effectively reducing the waste of resources.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25903v1",
    "published_date": "2025-09-30 07:48:14 UTC",
    "updated_date": "2025-09-30 07:48:14 UTC"
  },
  {
    "arxiv_id": "2510.03301v1",
    "title": "Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles",
    "authors": [
      "Arthur Sedek"
    ],
    "abstract": "This paper introduces a novel adaptive ensemble framework that synergistically combines XGBoost and neural networks through sophisticated meta-learning. The proposed method leverages advanced uncertainty quantification techniques and feature importance integration to dynamically orchestrate model selection and combination. Experimental results demonstrate superior predictive performance and enhanced interpretability across diverse datasets, contributing to the development of more intelligent and flexible machine learning systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.03301v1",
    "published_date": "2025-09-30 07:45:49 UTC",
    "updated_date": "2025-09-30 07:45:49 UTC"
  },
  {
    "arxiv_id": "2509.25897v1",
    "title": "RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity",
    "authors": [
      "Jisu Shin",
      "Hoyun Song",
      "Juhyun Oh",
      "Changgeon Ko",
      "Eunsu Kim",
      "Chani Jung",
      "Alice Oh"
    ],
    "abstract": "Humans often encounter role conflicts -- social dilemmas where the expectations of multiple roles clash and cannot be simultaneously fulfilled. As large language models (LLMs) become increasingly influential in human decision-making, understanding how they behave in complex social situations is essential. While previous research has evaluated LLMs' social abilities in contexts with predefined correct answers, role conflicts represent inherently ambiguous social dilemmas that require contextual sensitivity: the ability to recognize and appropriately weigh situational cues that can fundamentally alter decision priorities. To address this gap, we introduce RoleConflictBench, a novel benchmark designed to evaluate LLMs' contextual sensitivity in complex social dilemmas. Our benchmark employs a three-stage pipeline to generate over 13K realistic role conflict scenarios across 65 roles, systematically varying their associated expectations (i.e., their responsibilities and obligations) and situational urgency levels. By analyzing model choices across 10 different LLMs, we find that while LLMs show some capacity to respond to these contextual cues, this sensitivity is insufficient. Instead, their decisions are predominantly governed by a powerful, inherent bias related to social roles rather than situational information. Our analysis quantifies these biases, revealing a dominant preference for roles within the Family and Occupation domains, as well as a clear prioritization of male roles and Abrahamic religions across most evaluatee models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25897v1",
    "published_date": "2025-09-30 07:42:49 UTC",
    "updated_date": "2025-09-30 07:42:49 UTC"
  },
  {
    "arxiv_id": "2509.25885v1",
    "title": "SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents",
    "authors": [
      "Ruolin Chen",
      "Yinqian Sun",
      "Jihang Wang",
      "Mingyang Lv",
      "Qian Zhang",
      "Yi Zeng"
    ],
    "abstract": "Embodied agents powered by large language models (LLMs) inherit advanced planning capabilities; however, their direct interaction with the physical world exposes them to safety vulnerabilities. In this work, we identify four key reasoning stages where hazards may arise: Task Understanding, Environment Perception, High-Level Plan Generation, and Low-Level Action Generation. We further formalize three orthogonal safety constraint types (Factual, Causal, and Temporal) to systematically characterize potential safety violations. Building on this risk model, we present SafeMindBench, a multimodal benchmark with 5,558 samples spanning four task categories (Instr-Risk, Env-Risk, Order-Fix, Req-Align) across high-risk scenarios such as sabotage, harm, privacy, and illegal behavior. Extensive experiments on SafeMindBench reveal that leading LLMs (e.g., GPT-4o) and widely used embodied agents remain susceptible to safety-critical failures. To address this challenge, we introduce SafeMindAgent, a modular Planner-Executor architecture integrated with three cascaded safety modules, which incorporate safety constraints into the reasoning process. Results show that SafeMindAgent significantly improves safety rate over strong baselines while maintaining comparable task completion. Together, SafeMindBench and SafeMindAgent provide both a rigorous evaluation suite and a practical solution that advance the systematic study and mitigation of safety risks in embodied LLM agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25885v1",
    "published_date": "2025-09-30 07:24:04 UTC",
    "updated_date": "2025-09-30 07:24:04 UTC"
  },
  {
    "arxiv_id": "2509.25884v2",
    "title": "scUnified: An AI-Ready Standardized Resource for Single-Cell RNA Sequencing Analysis",
    "authors": [
      "Ping Xu",
      "Zaitian Wang",
      "Zhirui Wang",
      "Pengjiang Li",
      "Ran Zhang",
      "Gaoyang Li",
      "Hanyu Xie",
      "Jiajia Wang",
      "Yuanchun Zhou",
      "Pengfei Wang"
    ],
    "abstract": "Single-cell RNA sequencing (scRNA-seq) technology enables systematic delineation of cellular states and interactions, providing crucial insights into cellular heterogeneity. Building on this potential, numerous computational methods have been developed for tasks such as cell clustering, cell type annotation, and marker gene identification. To fully assess and compare these methods, standardized, analysis-ready datasets are essential. However, such datasets remain scarce, and variations in data formats, preprocessing workflows, and annotation strategies hinder reproducibility and complicate systematic evaluation of existing methods. To address these challenges, we present scUnified, an AI-ready standardized resource for single-cell RNA sequencing data that consolidates 13 high-quality datasets spanning two species (human and mouse) and nine tissue types. All datasets undergo standardized quality control and preprocessing and are stored in a uniform format to enable direct application in diverse computational analyses without additional data cleaning. We further demonstrate the utility of scUnified through experimental analyses of representative biological tasks, providing a reproducible foundation for the standardized evaluation of computational methods on a unified dataset.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25884v2",
    "published_date": "2025-09-30 07:23:01 UTC",
    "updated_date": "2025-11-10 03:55:13 UTC"
  },
  {
    "arxiv_id": "2509.25876v1",
    "title": "Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space",
    "authors": [
      "Xinyu Zhang",
      "Aishik Deb",
      "Klaus Mueller"
    ],
    "abstract": "Policy-gradient methods such as Proximal Policy Optimization (PPO) are typically updated along a single stochastic gradient direction, leaving the rich local structure of the parameter space unexplored. Previous work has shown that the surrogate gradient is often poorly correlated with the true reward landscape. Building on this insight, we visualize the parameter space spanned by policy checkpoints within an iteration and reveal that higher performing solutions often lie in nearby unexplored regions. To exploit this opportunity, we introduce ExploRLer, a pluggable pipeline that seamlessly integrates with on-policy algorithms such as PPO and TRPO, systematically probing the unexplored neighborhoods of surrogate on-policy gradient updates. Without increasing the number of gradient updates, ExploRLer achieves significant improvements over baselines in complex continuous control environments. Our results demonstrate that iteration-level exploration provides a practical and effective way to strengthen on-policy reinforcement learning and offer a fresh perspective on the limitations of the surrogate objective.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages; 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.25876v1",
    "published_date": "2025-09-30 07:13:55 UTC",
    "updated_date": "2025-09-30 07:13:55 UTC"
  },
  {
    "arxiv_id": "2509.25873v1",
    "title": "Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs",
    "authors": [
      "Hankun Dai",
      "Maoquan Wang",
      "Mengnan Qi",
      "Yikai Zhang",
      "Zijian Jin",
      "Yongqiang Yao",
      "Yufan Huang",
      "Shengyu Fu",
      "Elsie Nallipogu"
    ],
    "abstract": "Large language models (LLMs) are increasingly being applied to programming tasks, ranging from single-turn code completion to autonomous agents. Current code agent designs frequently depend on complex, hand-crafted workflows and tool sets. However, this reliance on elaborate scaffolding presents several challenges: agent performance becomes overly dependent on prompt tuning and custom design choices, heavy human intervention obscures a model's true underlying capabilities, and intricate pipelines are costly to build and maintain. Furthermore, optimizing complex task prompts increases the risk of data leakage. Currently, when introducing new models, LLM providers like OpenAI and Anthropic often publish benchmark scores to demonstrate their models' coding proficiency, but keep their proprietary evaluation frameworks confidential. To address these limitations, we introduce Lita (Lite Agent), which operationalizes liteness, a principle of minimizing manual design while retaining the essential elements of a fully autonomous agent. Lita enables a more faithful and unified evaluation without elaborate scaffolding. Experiments on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines. Crucially, Lita also consumes fewer tokens and requires significantly less design effort. Our results suggest that Lita is sufficient to reveal the underlying coding competence of modern LLMs. Finally, we propose the Agent Complexity Law: the performance gap between agents of varying complexity, from simple to sophisticated designs, will shrink as the core model improves, ultimately converging to a negligible difference.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25873v1",
    "published_date": "2025-09-30 07:07:32 UTC",
    "updated_date": "2025-09-30 07:07:32 UTC"
  },
  {
    "arxiv_id": "2509.25862v1",
    "title": "CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search",
    "authors": [
      "Olga Krestinskaya",
      "Mohammed E. Fouda",
      "Ahmed Eltawil",
      "Khaled N. Salama"
    ],
    "abstract": "To maximize hardware efficiency and performance accuracy in Compute-In-Memory (CIM)-based neural network accelerators for Artificial Intelligence (AI) applications, co-optimizing both software and hardware design parameters is essential. Manual tuning is impractical due to the vast number of parameters and their complex interdependencies. To effectively automate the design and optimization of CIM-based neural network accelerators, hardware-aware neural architecture search (HW-NAS) techniques can be applied. This work introduces CIMNAS, a joint model-quantization-hardware optimization framework for CIM architectures. CIMNAS simultaneously searches across software parameters, quantization policies, and a broad range of hardware parameters, incorporating device-, circuit-, and architecture-level co-optimizations. CIMNAS experiments were conducted over a search space of 9.9x10^85 potential parameter combinations with the MobileNet model as a baseline and RRAM-based CIM architecture. Evaluated on the ImageNet dataset, CIMNAS achieved a reduction in energy-delay-area product (EDAP) ranging from 90.1x to 104.5x, an improvement in TOPS/W between 4.68x and 4.82x, and an enhancement in TOPS/mm^2 from 11.3x to 12.78x relative to various baselines, all while maintaining an accuracy of 73.81%. The adaptability and robustness of CIMNAS are demonstrated by extending the framework to support the SRAM-based ResNet50 architecture, achieving up to an 819.5x reduction in EDAP. Unlike other state-of-the-art methods, CIMNAS achieves EDAP-focused optimization without any accuracy loss, generating diverse software-hardware parameter combinations for high-performance CIM-based neural network designs. The source code of CIMNAS is available at https://github.com/OlgaKrestinskaya/CIMNAS.",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.ET",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25862v1",
    "published_date": "2025-09-30 06:57:49 UTC",
    "updated_date": "2025-09-30 06:57:49 UTC"
  },
  {
    "arxiv_id": "2510.05124v2",
    "title": "MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation",
    "authors": [
      "Mingjin Li",
      "Yu Liu",
      "Huayi Liu",
      "Xiang Ye",
      "Chao Jiang",
      "Hongguang Zhang",
      "Yu Ruan"
    ],
    "abstract": "We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for generating persuasive multi-turn dialogues via agent self-play. MADS employs three coordinated agents: User Agents designed to simulate diverse persona-driven behaviors by leveraging personality signifiers such as Zodiac Signs and MBTI types, a Dialog Agent executing task-oriented persuasion strategies and an Optimization Agent evaluating and refining dialogue outcomes. We further validate its effectiveness through users' Chain-of-Attitude (CoA) modeling and dedicated LLMs' persuasion assessment. This approach enables low-cost generation of training data without human annotation, addressing key industry challenges such as lack of user data, cold-start evaluation difficulties, and prompt inefficiency. Applied to a real-world marketing scenario, MADS significantly improved the persuasion capacity of small LLMs, increasing the organic traffic conversion rate by 22.4% (from 1.83% to 2.24%) , demonstrating clear business value.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05124v2",
    "published_date": "2025-09-30 06:55:39 UTC",
    "updated_date": "2025-10-11 02:50:36 UTC"
  },
  {
    "arxiv_id": "2509.25858v1",
    "title": "Aging Decline in Basketball Career Trend Prediction Based on Machine Learning and LSTM Model",
    "authors": [
      "Yi-chen Yao",
      "Jerry Wang",
      "Yi-cheng Lai",
      "Lyn Chao-ling Chen"
    ],
    "abstract": "The topic of aging decline on performance of NBA players has been discussed in this study. The autoencoder with K-means clustering machine learning method was adopted to career trend classification of NBA players, and the LSTM deep learning method was adopted in performance prediction of each NBA player. The dataset was collected from the basketball game data of veteran NBA players. The contribution of the work performed better than the other methods with generalization ability for evaluating various types of NBA career trend, and can be applied in different types of sports in the field of sport analytics.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at Taiwan Academic Network Conference, TANET 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.25858v1",
    "published_date": "2025-09-30 06:54:22 UTC",
    "updated_date": "2025-09-30 06:54:22 UTC"
  },
  {
    "arxiv_id": "2509.25857v1",
    "title": "Vector sketch animation generation with differentialable motion trajectories",
    "authors": [
      "Xinding Zhu",
      "Xinye Yang",
      "Shuyang Zheng",
      "Zhexin Zhang",
      "Fei Gao",
      "Jing Huang",
      "Jiazhou Chen"
    ],
    "abstract": "Sketching is a direct and inexpensive means of visual expression. Though image-based sketching has been well studied, video-based sketch animation generation is still very challenging due to the temporal coherence requirement. In this paper, we propose a novel end-to-end automatic generation approach for vector sketch animation. To solve the flickering issue, we introduce a Differentiable Motion Trajectory (DMT) representation that describes the frame-wise movement of stroke control points using differentiable polynomial-based trajectories. DMT enables global semantic gradient propagation across multiple frames, significantly improving the semantic consistency and temporal coherence, and producing high-framerate output. DMT employs a Bernstein basis to balance the sensitivity of polynomial parameters, thus achieving more stable optimization. Instead of implicit fields, we introduce sparse track points for explicit spatial modeling, which improves efficiency and supports long-duration video processing. Evaluations on DAVIS and LVOS datasets demonstrate the superiority of our approach over SOTA methods. Cross-domain validation on 3D models and text-to-video data confirms the robustness and compatibility of our approach.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "14 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.25857v1",
    "published_date": "2025-09-30 06:53:04 UTC",
    "updated_date": "2025-09-30 06:53:04 UTC"
  },
  {
    "arxiv_id": "2509.25849v1",
    "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation",
    "authors": [
      "Ziniu Li",
      "Congliang Chen",
      "Tianyun Yang",
      "Tian Ding",
      "Ruoyu Sun",
      "Ge Zhang",
      "Wenhao Huang",
      "Zhi-Quan Luo"
    ],
    "abstract": "Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an \"item\" with a distinct \"value\" and \"cost\", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational \"free lunch\", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25849v1",
    "published_date": "2025-09-30 06:41:57 UTC",
    "updated_date": "2025-09-30 06:41:57 UTC"
  },
  {
    "arxiv_id": "2509.25848v2",
    "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models",
    "authors": [
      "Xinyu Tian",
      "Shu Zou",
      "Zhaoyuan Yang",
      "Mengqi He",
      "Fabian Waschkowski",
      "Lukas Wesemann",
      "Peter Tu",
      "Jing Zhang"
    ],
    "abstract": "Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25848v2",
    "published_date": "2025-09-30 06:37:47 UTC",
    "updated_date": "2025-10-02 12:24:56 UTC"
  },
  {
    "arxiv_id": "2509.25845v1",
    "title": "Training-Free Reward-Guided Image Editing via Trajectory Optimal Control",
    "authors": [
      "Jinho Chang",
      "Jaemin Kim",
      "Jong Chul Ye"
    ],
    "abstract": "Recent advancements in diffusion and flow-matching models have demonstrated remarkable capabilities in high-fidelity image synthesis. A prominent line of research involves reward-guided guidance, which steers the generation process during inference to align with specific objectives. However, leveraging this reward-guided approach to the task of image editing, which requires preserving the semantic content of the source image while enhancing a target reward, is largely unexplored. In this work, we introduce a novel framework for training-free, reward-guided image editing. We formulate the editing process as a trajectory optimal control problem where the reverse process of a diffusion model is treated as a controllable trajectory originating from the source image, and the adjoint states are iteratively updated to steer the editing process. Through extensive experiments across distinct editing tasks, we demonstrate that our approach significantly outperforms existing inversion-based training-free guidance baselines, achieving a superior balance between reward maximization and fidelity to the source image without reward hacking.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.25845v1",
    "published_date": "2025-09-30 06:34:37 UTC",
    "updated_date": "2025-09-30 06:34:37 UTC"
  },
  {
    "arxiv_id": "2509.25843v1",
    "title": "ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack",
    "authors": [
      "Yein Park",
      "Jungwoo Park",
      "Jaewoo Kang"
    ],
    "abstract": "Large language models (LLMs), despite being safety-aligned, exhibit brittle refusal behaviors that can be circumvented by simple linguistic changes. As tense jailbreaking demonstrates that models refusing harmful requests often comply when rephrased in past tense, a critical generalization gap is revealed in current alignment methods whose underlying mechanisms are poorly understood. In this work, we introduce Activation-Scaling Guard (ASGuard), an insightful, mechanistically-informed framework that surgically mitigates this specific vulnerability. For the first step, we use circuit analysis to identify the specific attention heads causally linked to the targeted jailbreaking, the tense-changing attack. Second, we train a precise, channel-wise scaling vector to recalibrate the activation of tense vulnerable heads. Lastly, we apply it into a \"preventative fine-tuning\", forcing the model to learn a more robust refusal mechanism. Across three LLMs, ASGuard effectively reduces the attack success rate of targeted jailbreaking while preserving general capabilities and minimizing over refusal, achieving a Pareto-optimal balance between safety and utility. Our findings underscore how adversarial suffixes suppress the propagation of the refusal-mediating direction, based on mechanistic analysis. Furthermore, our work showcases how a deep understanding of model internals can be leveraged to develop practical, efficient, and targeted methods for adjusting model behavior, charting a course for more reliable and interpretable AI safety.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25843v1",
    "published_date": "2025-09-30 06:33:52 UTC",
    "updated_date": "2025-09-30 06:33:52 UTC"
  },
  {
    "arxiv_id": "2509.25842v1",
    "title": "HiStyle: Hierarchical Style Embedding Predictor for Text-Prompt-Guided Controllable Speech Synthesis",
    "authors": [
      "Ziyu Zhang",
      "Hanzhao Li",
      "Jingbin Hu",
      "Wenhao Li",
      "Lei Xie"
    ],
    "abstract": "Controllable speech synthesis refers to the precise control of speaking style by manipulating specific prosodic and paralinguistic attributes, such as gender, volume, speech rate, pitch, and pitch fluctuation. With the integration of advanced generative models, particularly large language models (LLMs) and diffusion models, controllable text-to-speech (TTS) systems have increasingly transitioned from label-based control to natural language description-based control, which is typically implemented by predicting global style embeddings from textual prompts. However, this straightforward prediction overlooks the underlying distribution of the style embeddings, which may hinder the full potential of controllable TTS systems. In this study, we use t-SNE analysis to visualize and analyze the global style embedding distribution of various mainstream TTS systems, revealing a clear hierarchical clustering pattern: embeddings first cluster by timbre and subsequently subdivide into finer clusters based on style attributes. Based on this observation, we propose HiStyle, a two-stage style embedding predictor that hierarchically predicts style embeddings conditioned on textual prompts, and further incorporate contrastive learning to help align the text and audio embedding spaces. Additionally, we propose a style annotation strategy that leverages the complementary strengths of statistical methodologies and human auditory preferences to generate more accurate and perceptually consistent textual prompts for style control. Comprehensive experiments demonstrate that when applied to the base TTS model, HiStyle achieves significantly better style controllability than alternative style embedding predicting approaches while preserving high speech quality in terms of naturalness and intelligibility. Audio samples are available at https://anonymous.4open.science/w/HiStyle-2517/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25842v1",
    "published_date": "2025-09-30 06:31:12 UTC",
    "updated_date": "2025-09-30 06:31:12 UTC"
  },
  {
    "arxiv_id": "2509.25841v1",
    "title": "S$^2$FS: Spatially-Aware Separability-Driven Feature Selection in Fuzzy Decision Systems",
    "authors": [
      "Suping Xu",
      "Chuyi Dai",
      "Ye Liu",
      "Lin Shang",
      "Xibei Yang",
      "Witold Pedrycz"
    ],
    "abstract": "Feature selection is crucial for fuzzy decision systems (FDSs), as it identifies informative features and eliminates rule redundancy, thereby enhancing predictive performance and interpretability. Most existing methods either fail to directly align evaluation criteria with learning performance or rely solely on non-directional Euclidean distances to capture relationships among decision classes, which limits their ability to clarify decision boundaries. However, the spatial distribution of instances has a potential impact on the clarity of such boundaries. Motivated by this, we propose Spatially-aware Separability-driven Feature Selection (S$^2$FS), a novel framework for FDSs guided by a spatially-aware separability criterion. This criterion jointly considers within-class compactness and between-class separation by integrating scalar-distances with spatial directional information, providing a more comprehensive characterization of class structures. S$^2$FS employs a forward greedy strategy to iteratively select the most discriminative features. Extensive experiments on ten real-world datasets demonstrate that S$^2$FS consistently outperforms eight state-of-the-art feature selection algorithms in both classification accuracy and clustering performance, while feature visualizations further confirm the interpretability of the selected features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25841v1",
    "published_date": "2025-09-30 06:30:14 UTC",
    "updated_date": "2025-09-30 06:30:14 UTC"
  },
  {
    "arxiv_id": "2509.25839v1",
    "title": "RAE: A Neural Network Dimensionality Reduction Method for Nearest Neighbors Preservation in Vector Search",
    "authors": [
      "Han Zhang",
      "Dongfang Zhao"
    ],
    "abstract": "While high-dimensional embedding vectors are being increasingly employed in various tasks like Retrieval-Augmented Generation and Recommendation Systems, popular dimensionality reduction (DR) methods such as PCA and UMAP have rarely been adopted for accelerating the retrieval process due to their inability of preserving the nearest neighbor (NN) relationship among vectors. Empowered by neural networks' optimization capability and the bounding effect of Rayleigh quotient, we propose a Regularized Auto-Encoder (RAE) for k-NN preserving dimensionality reduction. RAE constrains the network parameter variation through regularization terms, adjusting singular values to control embedding magnitude changes during reduction, thus preserving k-NN relationships. We provide a rigorous mathematical analysis demonstrating that regularization establishes an upper bound on the norm distortion rate of transformed vectors, thereby offering provable guarantees for k-NN preservation. With modest training overhead, RAE achieves superior k-NN recall compared to existing DR approaches while maintaining fast retrieval efficiency.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.IR",
    "comment": "submitted to ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2509.25839v1",
    "published_date": "2025-09-30 06:25:38 UTC",
    "updated_date": "2025-09-30 06:25:38 UTC"
  },
  {
    "arxiv_id": "2509.25837v1",
    "title": "Distillation of Large Language Models via Concrete Score Matching",
    "authors": [
      "Yeongmin Kim",
      "Donghyeok Shin",
      "Mina Kang",
      "Byeonghu Na",
      "Il-Chul Moon"
    ],
    "abstract": "Large language models (LLMs) deliver remarkable performance but are costly to deploy, motivating knowledge distillation (KD) for efficient inference. Existing KD objectives typically match student and teacher probabilities via softmax, which blurs valuable logit information. While direct logit distillation (DLD) mitigates softmax smoothing, it fails to account for logit shift invariance, thereby restricting the solution space. We propose Concrete Score Distillation (CSD), a discrete score-matching objective that overcomes both softmax-induced smoothing and restrictions on the optimal solution set. We resolve the training instability and quadratic complexity of discrete score-matching in autoregressive LLMs, and the resulting CSD objective aligns relative logit differences across all vocabulary pairs between student and teacher with flexible weighting. We provide both mode-seeking and mode-covering instances within our framework and evaluate CSD on task-agnostic instruction-following and task-specific distillation using GPT-2-1.5B, OpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses recent KD objectives, achieves favorable fidelity-diversity trade-offs, and yields complementary gains when combined with on-policy techniques, demonstrating its scalability and effectiveness for LLM distillation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25837v1",
    "published_date": "2025-09-30 06:21:28 UTC",
    "updated_date": "2025-09-30 06:21:28 UTC"
  },
  {
    "arxiv_id": "2509.25835v3",
    "title": "Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search",
    "authors": [
      "Xinzhe Li"
    ],
    "abstract": "Test-time scaling improves large language models (LLMs) on long-horizon reasoning tasks by allocating more compute at inference. LLM Inference via Tree Search (LITS) methods achieve strong performance but are highly inefficient, often running an order of magnitude slower than iterative approaches. We propose Chain-in-Tree (CiT), a plug-in framework that decides when to branch during search rather than expanding at every step. CiT introduces lightweight Branching Necessity (BN) evaluations: BN-DP (Direct Prompting), where an auxiliary LLM judges branching needs, and BN-SC (Self-Consistency), which clusters candidate actions to assess agreement. Integrated into Tree of Thoughts, ReST-MCTS, and RAP, BN-DP achieves 75-85% reductions in token generation, model calls, and runtime on GSM8K and Math500, with often negligible or no accuracy loss. BN-SC typically yields substantial savings (up to 80%) generally but shows instability in 1-4 out of 14 settings, caused by a small subset of examples that produce extremely long reasoning steps. We theoretically prove that BN-DP never increases policy invocations and release both modular LITS implementations and a lightweight CiT function applicable across all LITS variants. The full codebase is publicly available at https://github.com/xinzhel/chain_in_tree.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under Review; Add codebase",
    "pdf_url": "https://arxiv.org/pdf/2509.25835v3",
    "published_date": "2025-09-30 06:18:44 UTC",
    "updated_date": "2025-10-18 04:15:26 UTC"
  },
  {
    "arxiv_id": "2509.25834v2",
    "title": "Supporting Creative Ownership through Deep Learning-Based Music Variation",
    "authors": [
      "Stephen James Krol",
      "Maria Teresa Llano",
      "Jon McCormack"
    ],
    "abstract": "This paper investigates the importance of personal ownership in musical AI design, examining how practising musicians can maintain creative control over the compositional process. Through a four-week ecological evaluation, we examined how a music variation tool, reliant on the skill of musicians, functioned within a composition setting. Our findings demonstrate that the dependence of the tool on the musician's ability, to provide a strong initial musical input and to turn moments into complete musical ideas, promoted ownership of both the process and artefact. Qualitative interviews further revealed the importance of this personal ownership, highlighting tensions between technological capability and artistic identity. These findings provide insight into how musical AI can support rather than replace human creativity, highlighting the importance of designing tools that preserve the humanness of musical expression.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Paper Accepted NeurIPS Creative AI Track 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.25834v2",
    "published_date": "2025-09-30 06:18:36 UTC",
    "updated_date": "2025-10-07 05:31:15 UTC"
  },
  {
    "arxiv_id": "2509.25827v1",
    "title": "Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling",
    "authors": [
      "Shuyang Jiang",
      "Yusheng Liao",
      "Ya Zhang",
      "Yanfeng Wang",
      "Yu Wang"
    ],
    "abstract": "While large reasoning models trained with critic-free reinforcement learning and verifiable rewards (RLVR) represent the state-of-the-art, their practical utility is hampered by ``overthinking'', a critical issue where models generate excessively long reasoning paths without any performance benefit. Existing solutions that penalize length often fail, inducing performance degradation due to a fundamental misalignment between trajectory-level rewards and token-level optimization. In this work, we introduce a novel framework, DECS, built on our theoretical discovery of two previously unaddressed flaws in current length rewards: (1) the erroneous penalization of essential exploratory tokens and (2) the inadvertent rewarding of partial redundancy. Our framework's innovations include (i) a first-of-its-kind decoupled token-level reward mechanism that surgically distinguishes and penalizes redundant tokens, and (ii) a novel curriculum batch scheduling strategy to master the efficiency-efficacy equilibrium. Experimental results show DECS can achieve a dramatic reduction in reasoning tokens by over 50\\% across seven benchmarks while simultaneously maintaining or even improving performance. It demonstrates conclusively that substantial gains in reasoning efficiency can be achieved without compromising a model's underlying reasoning power.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages",
    "pdf_url": "https://arxiv.org/pdf/2509.25827v1",
    "published_date": "2025-09-30 06:04:43 UTC",
    "updated_date": "2025-09-30 06:04:43 UTC"
  },
  {
    "arxiv_id": "2509.25818v1",
    "title": "VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions",
    "authors": [
      "Kazuki Matsuda",
      "Yuiga Wada",
      "Shinnosuke Hirano",
      "Seitaro Otsuki",
      "Komei Sugiura"
    ],
    "abstract": "In this study, we focus on the automatic evaluation of long and detailed image captions generated by multimodal Large Language Models (MLLMs). Most existing automatic evaluation metrics for image captioning are primarily designed for short captions and are not suitable for evaluating long captions. Moreover, recent LLM-as-a-Judge approaches suffer from slow inference due to their reliance on autoregressive inference and early fusion of visual information. To address these limitations, we propose VELA, an automatic evaluation metric for long captions developed within a novel LLM-Hybrid-as-a-Judge framework. Furthermore, we propose LongCap-Arena, a benchmark specifically designed for evaluating metrics for long captions. This benchmark comprises 7,805 images, the corresponding human-provided long reference captions and long candidate captions, and 32,246 human judgments from three distinct perspectives: Descriptiveness, Relevance, and Fluency. We demonstrated that VELA outperformed existing metrics and achieved superhuman performance on LongCap-Arena.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "EMNLP 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2509.25818v1",
    "published_date": "2025-09-30 05:52:34 UTC",
    "updated_date": "2025-09-30 05:52:34 UTC"
  },
  {
    "arxiv_id": "2509.25810v3",
    "title": "Learning to Reason as Action Abstractions with Scalable Mid-Training RL",
    "authors": [
      "Shenao Zhang",
      "Donghan Yu",
      "Yihao Feng",
      "Bowen Jin",
      "Zhaoran Wang",
      "John Peebles",
      "Zirui Wang"
    ],
    "abstract": "Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25810v3",
    "published_date": "2025-09-30 05:34:20 UTC",
    "updated_date": "2025-10-11 22:23:08 UTC"
  },
  {
    "arxiv_id": "2509.25804v2",
    "title": "CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG",
    "authors": [
      "Vaskar Chakma",
      "Ju Xiaolin",
      "Heling Cao",
      "Xue Feng",
      "Ji Xiaodong",
      "Pan Haiyan",
      "Gao Zhan"
    ],
    "abstract": "This study aims to develop and evaluate an ensemble machine learning-based framework for the automatic detection of Wide QRS Complex Tachycardia (WCT) from ECG signals, emphasizing diagnostic accuracy and interpretability using Explainable AI. The proposed system integrates ensemble learning techniques, i.e., an optimized Random Forest known as CardioForest, and models like XGBoost and LightGBM. The models were trained and tested on ECG data from the publicly available MIMIC-IV dataset. The testing was carried out with the assistance of accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations) was used to ascertain model explainability and clinical relevance. The CardioForest model performed best on all metrics, achieving a test accuracy of 95.19%, a balanced accuracy of 88.76%, a precision of 95.26%, a recall of 78.42%, and an ROC-AUC of 0.8886. SHAP analysis confirmed the model's ability to rank the most relevant ECG features, such as QRS duration, in accordance with clinical intuitions, thereby fostering trust and usability in clinical practice. The findings recognize CardioForest as an extremely dependable and interpretable WCT detection model. Being able to offer accurate predictions and transparency through explainability makes it a valuable tool to help cardiologists make timely and well-informed diagnoses, especially for high-stakes and emergency scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25804v2",
    "published_date": "2025-09-30 05:23:57 UTC",
    "updated_date": "2025-11-05 07:14:02 UTC"
  },
  {
    "arxiv_id": "2509.25803v1",
    "title": "Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding",
    "authors": [
      "Wanying Ding",
      "Savinay Narendra",
      "Xiran Shi",
      "Adwait Ratnaparkhi",
      "Chengrui Yang",
      "Nikoo Sabzevar",
      "Ziyan Yin"
    ],
    "abstract": "Analyzing financial transactions is crucial for ensuring regulatory compliance, detecting fraud, and supporting decisions. The complexity of financial transaction data necessitates advanced techniques to extract meaningful insights and ensure accurate analysis. Since Transformer-based models have shown outstanding performance across multiple domains, this paper seeks to explore their potential in understanding financial transactions. This paper conducts extensive experiments to evaluate three types of Transformer models: Encoder-Only, Decoder-Only, and Encoder-Decoder models. For each type, we explore three options: pretrained LLMs, fine-tuned LLMs, and small proprietary models developed from scratch. Our analysis reveals that while LLMs, such as LLaMA3-8b, Flan-T5, and SBERT, demonstrate impressive capabilities in various natural language processing tasks, they do not significantly outperform small proprietary models in the specific context of financial transaction understanding. This phenomenon is particularly evident in terms of speed and cost efficiency. Proprietary models, tailored to the unique requirements of transaction data, exhibit faster processing times and lower operational costs, making them more suitable for real-time applications in the financial sector. Our findings highlight the importance of model selection based on domain-specific needs and underscore the potential advantages of customized proprietary models over general-purpose LLMs in specialized applications. Ultimately, we chose to implement a proprietary decoder-only model to handle the complex transactions that we previously couldn't manage. This model can help us to improve 14% transaction coverage, and save more than \\$13 million annual cost.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "9 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.25803v1",
    "published_date": "2025-09-30 05:23:08 UTC",
    "updated_date": "2025-09-30 05:23:08 UTC"
  },
  {
    "arxiv_id": "2509.25794v1",
    "title": "Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding",
    "authors": [
      "Haotian Xue",
      "Yunhao Ge",
      "Yu Zeng",
      "Zhaoshuo Li",
      "Ming-Yu Liu",
      "Yongxin Chen",
      "Jiaojiao Fan"
    ],
    "abstract": "Vision-Language Models (VLMs) have demonstrated impressive world knowledge across a wide range of tasks, making them promising candidates for embodied reasoning applications. However, existing benchmarks primarily evaluate the embodied reasoning ability of VLMs through multiple-choice questions based on image annotations -- for example, selecting which trajectory better describes an event in the image. In this work, we introduce the Point-It-Out (PIO) benchmark, a novel benchmark designed to systematically assess the embodied reasoning abilities of VLMs through precise visual grounding. We propose a hierarchical evaluation protocol spanning three stages (S1: referred-object localization, S2: task-driven pointing, and S3: visual trace prediction), with data collected from critical domains for embodied intelligence, including indoor, kitchen, driving, and robotic manipulation scenarios. Extensive experiments with over ten state-of-the-art VLMs reveal several interesting findings. For example, strong general-purpose models such as GPT-4o, while excelling on many benchmarks (e.g., language, perception, and reasoning), underperform compared to some open-source models in precise visual grounding; models such as MoLMO perform well in S1 and S2 but struggle in S3, where requires grounding combined with visual trace planning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25794v1",
    "published_date": "2025-09-30 05:05:54 UTC",
    "updated_date": "2025-09-30 05:05:54 UTC"
  },
  {
    "arxiv_id": "2509.25792v1",
    "title": "PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks",
    "authors": [
      "Alexander Branch",
      "Omead Pooladzandi",
      "Radin Khosraviani",
      "Sunay Gajanan Bhat",
      "Jeffrey Jiang",
      "Gregory Pottie"
    ],
    "abstract": "We introduce PureVQ-GAN, a defense against data poisoning that forces backdoor triggers through a discrete bottleneck using Vector-Quantized VAE with GAN discriminator. By quantizing poisoned images through a learned codebook, PureVQ-GAN destroys fine-grained trigger patterns while preserving semantic content. A GAN discriminator ensures outputs match the natural image distribution, preventing reconstruction of out-of-distribution perturbations. On CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient Matching and Bullseye Polytope attacks, and 1.64% against Narcissus while maintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring hundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making it practical for real training pipelines.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25792v1",
    "published_date": "2025-09-30 05:04:17 UTC",
    "updated_date": "2025-09-30 05:04:17 UTC"
  },
  {
    "arxiv_id": "2509.25781v1",
    "title": "Deontic Argumentation",
    "authors": [
      "Guido Governatori",
      "Antonino Rotolo"
    ],
    "abstract": "We address the issue of defining a semantics for deontic argumentation that supports weak permission. Some recent results show that grounded semantics do not support weak permission when there is a conflict between two obligations. We provide a definition of Deontic Argumentation Theory that accounts for weak permission, and we recall the result about grounded semantics. Then, we propose a new semantics that supports weak permission.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25781v1",
    "published_date": "2025-09-30 04:50:07 UTC",
    "updated_date": "2025-09-30 04:50:07 UTC"
  },
  {
    "arxiv_id": "2509.25779v2",
    "title": "Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs",
    "authors": [
      "Siyu Zhu",
      "Yanbin Jiang",
      "Hejian Sang",
      "Shao Tang",
      "Qingquan Song",
      "Biao He",
      "Rohit Jain",
      "Zhipeng Wang",
      "Alborz Geramifard"
    ],
    "abstract": "We investigated Agentic RL with large language models on the \\textsc{TravelPlanner} benchmark. Our approach, \\textsc{Planner-R1}, achieved a \\textbf{56.9\\%} final-pass rate with only 180 training queries, a $2.7\\times$ improvement over GPT-5's $21.2\\%$ baseline and the strongest agentic result on the public leaderboard. A central finding was that smaller models (8B) were highly responsive to reward shaping: with dense process-level signals, they reached competitive performance while being $3.5\\times$ more compute-efficient and $1.5\\times$ more memory-efficient than 32B models. Larger models were more robust under sparse rewards but exhibited smaller relative gains from shaping and higher variance across runs. While curriculum learning offered no significant benefit, shaped rewards consistently amplified learning dynamics, making 8B models the most efficient setting for agentic RL. Crucially, these gains did not come at the cost of overfitting: fine-tuned models mostly maintained or exceeded baseline performance on out-of-domain tasks, including \\textsc{Multi-IF}, \\textsc{NaturalPlan}, and $τ$-\\textsc{Bench}. These results establish reward shaping as a decisive lever for scaling agentic RL, highlight the competitive strength of smaller models, and demonstrate that efficiency can be achieved without sacrificing generalization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25779v2",
    "published_date": "2025-09-30 04:49:36 UTC",
    "updated_date": "2025-10-01 20:23:23 UTC"
  },
  {
    "arxiv_id": "2509.25776v3",
    "title": "Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation",
    "authors": [
      "Mingyu Kang",
      "Yong Suk Choi"
    ],
    "abstract": "Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.25776v3",
    "published_date": "2025-09-30 04:44:53 UTC",
    "updated_date": "2025-10-27 06:34:13 UTC"
  },
  {
    "arxiv_id": "2509.25775v3",
    "title": "Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions",
    "authors": [
      "Amber Srivastava",
      "Salar Basiri",
      "Srinivasa Salapaka"
    ],
    "abstract": "Clustering arises in a wide range of problem formulations, yet most existing approaches assume that the entities under clustering are passive and strictly conform to their assigned groups. In reality, entities often exhibit local autonomy, overriding prescribed associations in ways not fully captured by feature representations. Such autonomy can substantially reshape clustering outcomes -- altering cluster compositions, geometry, and cardinality -- with significant downstream effects on inference and decision-making. We introduce autonomy-aware clustering, a reinforcement learning (RL) framework that learns and accounts for the influence of local autonomy without requiring prior knowledge of its form. Our approach integrates RL with a Deterministic Annealing (DA) procedure, where, to determine underlying clusters, DA naturally promotes exploration in early stages of annealing and transitions to exploitation later. We also show that the annealing procedure exhibits phase transitions that enable design of efficient annealing schedules. To further enhance adaptability, we propose the Adaptive Distance Estimation Network (ADEN), a transformer-based attention model that learns dependencies between entities and cluster representatives within the RL loop, accommodates variable-sized inputs and outputs, and enables knowledge transfer across diverse problem instances. Empirical results show that our framework closely aligns with underlying data dynamics: even without explicit autonomy models, it achieves solutions close to the ground truth (gap ~3-4%), whereas ignoring autonomy leads to substantially larger gaps (~35-40%). The code and data are publicly available at https://github.com/salar96/AutonomyAwareClustering.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under review at a peer-reviewed venue. Minor formatting correction: the earlier version included an incorrect conference header, which has been removed. Content unchanged",
    "pdf_url": "https://arxiv.org/pdf/2509.25775v3",
    "published_date": "2025-09-30 04:44:36 UTC",
    "updated_date": "2025-10-08 16:05:52 UTC"
  },
  {
    "arxiv_id": "2509.25774v2",
    "title": "PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models",
    "authors": [
      "Jeongjae Lee",
      "Jong Chul Ye"
    ],
    "abstract": "While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "35 pages, 20 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.25774v2",
    "published_date": "2025-09-30 04:43:58 UTC",
    "updated_date": "2025-12-06 07:55:50 UTC"
  },
  {
    "arxiv_id": "2510.05123v1",
    "title": "A Scalable AI Driven, IoT Integrated Cognitive Digital Twin for Multi-Modal Neuro-Oncological Prognostics and Tumor Kinetics Prediction using Enhanced Vision Transformer and XAI",
    "authors": [
      "Saptarshi Banerjee",
      "Himadri Nath Saha",
      "Utsho Banerjee",
      "Rajarshi Karmakar",
      "Jon Turdiev"
    ],
    "abstract": "Neuro-oncological prognostics are now vital in modern clinical neuroscience because brain tumors pose significant challenges in detection and management. To tackle this issue, we propose a cognitive digital twin framework that combines real-time EEG signals from a wearable skullcap with structural MRI data for dynamic and personalized tumor monitoring. At the heart of this framework is an Enhanced Vision Transformer (ViT++) that includes innovative components like Patch-Level Attention Regularization (PLAR) and an Adaptive Threshold Mechanism to improve tumor localization and understanding. A Bidirectional LSTM-based neural classifier analyzes EEG patterns over time to classify brain states such as seizure, interictal, and healthy. Grad-CAM-based heatmaps and a three.js-powered 3D visualization module provide interactive anatomical insights. Furthermore, a tumor kinetics engine predicts volumetric growth by looking at changes in MRI trends and anomalies from EEG data. With impressive accuracy metrics of 94.6% precision, 93.2% recall, and a Dice score of 0.91, this framework sets a new standard for real-time, interpretable neurodiagnostics. It paves the way for future advancements in intelligent brain health monitoring.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.05123v1",
    "published_date": "2025-09-30 04:37:32 UTC",
    "updated_date": "2025-09-30 04:37:32 UTC"
  },
  {
    "arxiv_id": "2509.25773v1",
    "title": "V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs",
    "authors": [
      "Zhengpeng Shi",
      "Hengli Li",
      "Yanpeng Zhao",
      "Jianqun Zhou",
      "Yuxuan Wang",
      "Qinrong Cui",
      "Wei Bi",
      "Songchun Zhu",
      "Bo Zhao",
      "Zilong Zheng"
    ],
    "abstract": "AI models capable of comprehending humor hold real-world promise -- for example, enhancing engagement in human-machine interactions. To gauge and diagnose the capacity of multimodal large language models (MLLMs) for humor understanding, we introduce v-HUB, a novel visual-centric video humor understanding benchmark. v-HUB comprises a curated collection of minimally verbal short videos, sourced from classic silent films and online resources, and reflecting real-world scenarios where humor can be appreciated purely through visual cues. Each video clip is paired with rich annotations, including captions, descriptions, and explanations, supporting evaluation tasks like caption matching and humor explanation. To broaden its applicability, we further construct an open-ended video QA task, making it readily integrable into existing video understanding benchmarks. We evaluate a diverse set of MLLMs, from specialized Video-LLMs to versatile OmniLLMs that can process audio, covering both open-source and proprietary domains. The experimental results expose the difficulties MLLMs face in comprehending humor from visual cues alone. For example, all models exhibit a marked performance drop on caption matching when moving from text-based to video-based evaluation (without audio). Our findings also demonstrate that incorporating audio helps with video humor understanding, highlighting the informativeness of sound and the promise of integrating richer modalities for complex video understanding tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25773v1",
    "published_date": "2025-09-30 04:33:52 UTC",
    "updated_date": "2025-09-30 04:33:52 UTC"
  },
  {
    "arxiv_id": "2509.25771v1",
    "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs",
    "authors": [
      "Jia Jun Cheng Xian",
      "Muchen Li",
      "Haotian Yang",
      "Xin Tao",
      "Pengfei Wan",
      "Leonid Sigal",
      "Renjie Liao"
    ],
    "abstract": "Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables \"free-lunch\" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25771v1",
    "published_date": "2025-09-30 04:32:34 UTC",
    "updated_date": "2025-09-30 04:32:34 UTC"
  },
  {
    "arxiv_id": "2509.25767v1",
    "title": "Galton's Law of Mediocrity: Why Large Language Models Regress to the Mean and Fail at Creativity in Advertising",
    "authors": [
      "Matt Keon",
      "Aabid Karim",
      "Bhoomika Lohana",
      "Abdul Karim",
      "Thai Nguyen",
      "Tara Hamilton",
      "Ali Abbas"
    ],
    "abstract": "Large language models (LLMs) generate fluent text yet often default to safe, generic phrasing, raising doubts about their ability to handle creativity. We formalize this tendency as a Galton-style regression to the mean in language and evaluate it using a creativity stress test in advertising concepts. When ad ideas were simplified step by step, creative features such as metaphors, emotions, and visual cues disappeared early, while factual content remained, showing that models favor high-probability information. When asked to regenerate from simplified inputs, models produced longer outputs with lexical variety but failed to recover the depth and distinctiveness of the originals. We combined quantitative comparisons with qualitative analysis, which revealed that the regenerated texts often appeared novel but lacked true originality. Providing ad-specific cues such as metaphors, emotional hooks and visual markers improved alignment and stylistic balance, though outputs still relied on familiar tropes. Taken together, the findings show that without targeted guidance, LLMs drift towards mediocrity in creative tasks; structured signals can partially counter this tendency and point towards pathways for developing creativity-sensitive models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25767v1",
    "published_date": "2025-09-30 04:29:41 UTC",
    "updated_date": "2025-09-30 04:29:41 UTC"
  },
  {
    "arxiv_id": "2509.25760v1",
    "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
    "authors": [
      "Zhepei Wei",
      "Xiao Yang",
      "Kai Sun",
      "Jiaqi Wang",
      "Rulin Shao",
      "Sean Chen",
      "Mohammad Kachuee",
      "Teja Gollapudi",
      "Tony Liao",
      "Nicolas Scheffer",
      "Rakesh Wanga",
      "Anuj Kumar",
      "Yu Meng",
      "Wen-tau Yih",
      "Xin Luna Dong"
    ],
    "abstract": "While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25760v1",
    "published_date": "2025-09-30 04:25:17 UTC",
    "updated_date": "2025-09-30 04:25:17 UTC"
  },
  {
    "arxiv_id": "2509.25758v1",
    "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training",
    "authors": [
      "Yein Park",
      "Minbyul Jeong",
      "Jaewoo Kang"
    ],
    "abstract": "The remarkable capabilities of modern large reasoning models are largely unlocked through post-training techniques such as supervised fine-tuning and reinforcement learning. However, the architectural mechanisms behind such improvements remain largely opaque. In this work, we use circuit analysis to demonstrate that post-training for complex reasoning sparks the emergence of novel, functionally specialized attention heads. These heads collectively support structured reasoning and computation. Our comparative analysis across Qwen families and DeepSeek-distilled model reveals that these emergent heads evolve differently under different training regimes. Distillation and SFT foster a cumulative addition of stable reasoning heads. In contrast, group relative policy optimization operates in a dynamic search mode: relatively few attention heads are iteratively activated, evaluated, and pruned, with their survival closely tracking fluctuations in the task reward signal. Furthermore, we find that controllable think on/off models do not possess dedicated thinking heads. Instead, turning off explicit reasoning triggers a broader-but less efficient-set of compensatory heads. Through ablation and qualitative analyses, we connect these circuit-level dynamics to a crucial performance trade-off: strengthened heads enable sophisticated problem-solving strategies for difficult problems but can also introduce over-thinking failure modes, such as calculation errors or logical loops on simpler tasks. These findings connect circuit-level dynamics to macro-level performance, identifying an inherent tension where complex reasoning comes at the cost of elementary computations. More broadly, our work points to future directions for training policy design, emphasizing the need to balance the development of effective reasoning strategies with the assurance of reliable, flawless execution.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25758v1",
    "published_date": "2025-09-30 04:23:43 UTC",
    "updated_date": "2025-09-30 04:23:43 UTC"
  },
  {
    "arxiv_id": "2509.25757v1",
    "title": "NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language",
    "authors": [
      "Danial Kamali",
      "Parisa Kordjamshidi"
    ],
    "abstract": "Modern Vision-Language Models (VLMs) have achieved impressive performance in various tasks, yet they often struggle with compositional reasoning, the ability to decompose and recombine concepts to solve novel problems. While neuro-symbolic approaches offer a promising direction, they are typically constrained by crisp logical execution or predefined predicates, which limit flexibility. In this work, we introduce NePTune, a neuro-symbolic framework that overcomes these limitations through a hybrid execution model that integrates the perception capabilities of foundation vision models with the compositional expressiveness of symbolic reasoning. NePTune dynamically translates natural language queries into executable Python programs that blend imperative control flow with soft logic operators capable of reasoning over VLM-generated uncertainty. Operating in a training-free manner, NePTune, with a modular design, decouples perception from reasoning, yet its differentiable operations support fine-tuning. We evaluate NePTune on multiple visual reasoning benchmarks and various domains, utilizing adversarial tests, and demonstrate a significant improvement over strong base models, as well as its effective compositional generalization and adaptation capabilities in novel environments.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25757v1",
    "published_date": "2025-09-30 04:22:42 UTC",
    "updated_date": "2025-09-30 04:22:42 UTC"
  },
  {
    "arxiv_id": "2509.25751v1",
    "title": "Cooperative Autonomous Driving in Diverse Behavioral Traffic: A Heterogeneous Graph Reinforcement Learning Approach",
    "authors": [
      "Qi Liu",
      "Xueyuan Li",
      "Zirui Li",
      "Juhui Gim"
    ],
    "abstract": "Navigating heterogeneous traffic environments with diverse driving styles poses a significant challenge for autonomous vehicles (AVs) due to their inherent complexity and dynamic interactions. This paper addresses this challenge by proposing a heterogeneous graph reinforcement learning (GRL) framework enhanced with an expert system to improve AV decision-making performance. Initially, a heterogeneous graph representation is introduced to capture the intricate interactions among vehicles. Then, a heterogeneous graph neural network with an expert model (HGNN-EM) is proposed to effectively encode diverse vehicle features and produce driving instructions informed by domain-specific knowledge. Moreover, the double deep Q-learning (DDQN) algorithm is utilized to train the decision-making model. A case study on a typical four-way intersection, involving various driving styles of human vehicles (HVs), demonstrates that the proposed method has superior performance over several baselines regarding safety, efficiency, stability, and convergence rate, all while maintaining favorable real-time performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 5 figures and 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.25751v1",
    "published_date": "2025-09-30 04:12:57 UTC",
    "updated_date": "2025-09-30 04:12:57 UTC"
  },
  {
    "arxiv_id": "2509.25748v3",
    "title": "Dolphin v1.0 Technical Report",
    "authors": [
      "Taohan Weng",
      "Kaibing Hu",
      "Henan Liu",
      "Siya Liu",
      "Xiaoyang Liu",
      "Zhenyu Liu",
      "Jiren Ren",
      "Boyan Wang",
      "Boyang Wang",
      "Yiyu Wang",
      "Yalun Wu",
      "Chaoran Yan",
      "Kaiwen Yan",
      "Jinze Yu",
      "Chi Zhang",
      "Duo Zhang",
      "Haoyun Zheng",
      "Xiaoqing Guo",
      "Jacques Souquet",
      "Hongcheng Guo",
      "Anjie Le"
    ],
    "abstract": "Ultrasound is crucial in modern medicine but faces challenges like operator dependence, image noise, and real-time scanning, hindering AI integration. While large multimodal models excel in other medical imaging areas, they struggle with ultrasound's complexities. To address this, we introduce Dolphin v1.0 (V1) and its reasoning-augmented version, Dolphin R1-the first large-scale multimodal ultrasound foundation models unifying diverse clinical tasks in a single vision-language framework.To tackle ultrasound variability and noise, we curated a 2-million-scale multimodal dataset, combining textbook knowledge, public data, synthetic samples, and general corpora. This ensures robust perception, generalization, and clinical adaptability.The Dolphin series employs a three-stage training strategy: domain-specialized pretraining, instruction-driven alignment, and reinforcement-based refinement. Dolphin v1.0 delivers reliable performance in classification, detection, regression, and report generation. Dolphin R1 enhances diagnostic inference, reasoning transparency, and interpretability through reinforcement learning with ultrasound-specific rewards.Evaluated on U2-Bench across eight ultrasound tasks, Dolphin R1 achieves a U2-score of 0.5835-over twice the second-best model (0.2968) setting a new state of the art. Dolphin v1.0 also performs competitively, validating the unified framework. Comparisons show reasoning-enhanced training significantly improves diagnostic accuracy, consistency, and interpretability, highlighting its importance for high-stakes medical AI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25748v3",
    "published_date": "2025-09-30 04:08:45 UTC",
    "updated_date": "2025-10-19 03:35:41 UTC"
  },
  {
    "arxiv_id": "2509.25736v1",
    "title": "Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications",
    "authors": [
      "Chenhua Shi",
      "Gregor Macdonald",
      "Bhavika Jalli",
      "Wanlu Lei",
      "John Zou",
      "Mridul Jain",
      "Joji Philip"
    ],
    "abstract": "The success of large language models (LLMs) depends heavily on large-scale, high-quality instruction-following and reinforcement datasets. However, generating such data through human annotation is prohibitively time-consuming particularly for domain-specific tasks like telecom network troubleshooting, where accurate responses require deep technical expertise and contextual understanding. In this paper, we present a fully automated, retrieval-augmented pipeline for generating synthetic question-answer (QA) pairs grounded in structured domain knowledge. Our multi-stage framework integrates a retriever, base generator, and refinement model to synthesize and enhance QA pairs using documents retrieved from a domain-specific knowledge graph. To ensure data quality, we employ customized RAGAS-based scoring to filter low-quality samples, producing a high-quality dataset suitable for reinforcement fine-tuning (RFT). We demonstrate our approach in a real-world telecom scenario focused on radio access network (RAN) troubleshooting. The resulting pipeline generates complex, context-rich troubleshooting solution plans without human intervention. This work offers a scalable solution for building instruction and reinforcement datasets in specialized domains, significantly reducing dependence on manual labeling while maintaining high technical fidelity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "cs.NI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 6 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2509.25736v1",
    "published_date": "2025-09-30 03:49:57 UTC",
    "updated_date": "2025-09-30 03:49:57 UTC"
  },
  {
    "arxiv_id": "2510.09636v1",
    "title": "Bias-Aware AI Chatbot for Engineering Advising at the University of Maryland A. James Clark School of Engineering",
    "authors": [
      "Prarthana P. Kartholy",
      "Thandi M. Labor",
      "Neil N. Panchal",
      "Sean H. Wang",
      "Hillary N. Owusu"
    ],
    "abstract": "Selecting a college major is a difficult decision for many incoming freshmen. Traditional academic advising is often hindered by long wait times, intimidating environments, and limited personalization. AI Chatbots present an opportunity to address these challenges. However, AI systems also have the potential to generate biased responses, prejudices related to race, gender, socioeconomic status, and disability. These biases risk turning away potential students and undermining reliability of AI systems. This study aims to develop a University of Maryland (UMD) A. James Clark School of Engineering Program-specific AI chatbot. Our research team analyzed and mitigated potential biases in the responses. Through testing the chatbot on diverse student queries, the responses are scored on metrics of accuracy, relevance, personalization, and bias presence. The results demonstrate that with careful prompt engineering and bias mitigation strategies, AI chatbots can provide high-quality, unbiased academic advising support, achieving mean scores of 9.76 for accuracy, 9.56 for relevance, and 9.60 for personalization with no stereotypical biases found in the sample data. However, due to the small sample size and limited timeframe, our AI model may not fully reflect the nuances of student queries in engineering academic advising. Regardless, these findings will inform best practices for building ethical AI systems in higher education, offering tools to complement traditional advising and address the inequities faced by many underrepresented and first-generation college students.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09636v1",
    "published_date": "2025-09-30 03:47:08 UTC",
    "updated_date": "2025-09-30 03:47:08 UTC"
  },
  {
    "arxiv_id": "2509.25729v1",
    "title": "Controlled Generation for Private Synthetic Text",
    "authors": [
      "Zihao Zhao",
      "Anjalie Field"
    ],
    "abstract": "Text anonymization is essential for responsibly developing and deploying AI in high-stakes domains such as healthcare, social services, and law. In this work, we propose a novel methodology for privacy-preserving synthetic text generation that leverages the principles of de-identification and the Hiding In Plain Sight (HIPS) theory. Our approach introduces entity-aware control codes to guide controllable generation using either in-context learning (ICL) or prefix tuning. The ICL variant ensures privacy levels consistent with the underlying de-identification system, while the prefix tuning variant incorporates a custom masking strategy and loss function to support scalable, high-quality generation. Experiments on legal and clinical datasets demonstrate that our method achieves a strong balance between privacy protection and utility, offering a practical and effective solution for synthetic text generation in sensitive domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.25729v1",
    "published_date": "2025-09-30 03:38:36 UTC",
    "updated_date": "2025-09-30 03:38:36 UTC"
  },
  {
    "arxiv_id": "2509.25727v1",
    "title": "Boundary-to-Region Supervision for Offline Safe Reinforcement Learning",
    "authors": [
      "Huikang Su",
      "Dengyun Peng",
      "Zifeng Zhuang",
      "YuHan Liu",
      "Qiguang Chen",
      "Donglin Wang",
      "Qinghe Liu"
    ],
    "abstract": "Offline safe reinforcement learning aims to learn policies that satisfy predefined safety constraints from static datasets. Existing sequence-model-based methods condition action generation on symmetric input tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry: return-to-go (RTG) serves as a flexible performance target, while cost-to-go (CTG) should represent a rigid safety boundary. This symmetric conditioning leads to unreliable constraint satisfaction, especially when encountering out-of-distribution cost trajectories. To address this, we propose Boundary-to-Region (B2R), a framework that enables asymmetric conditioning through cost signal realignment . B2R redefines CTG as a boundary constraint under a fixed safety budget, unifying the cost distribution of all feasible trajectories while preserving reward structures. Combined with rotary positional embeddings , it enhances exploration within the safe region. Experimental results show that B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods. This work highlights the limitations of symmetric token conditioning and establishes a new theoretical and practical approach for applying sequence models to safe RL. Our code is available at https://github.com/HuikangSu/B2R.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.25727v1",
    "published_date": "2025-09-30 03:38:20 UTC",
    "updated_date": "2025-09-30 03:38:20 UTC"
  },
  {
    "arxiv_id": "2509.25724v2",
    "title": "Towards A Universally Transferable Acceleration Method for Density Functional Theory",
    "authors": [
      "Zhe Liu",
      "Yuyan Ni",
      "Zhichen Pu",
      "Qiming Sun",
      "Siyuan Liu",
      "Wen Yan"
    ],
    "abstract": "Recently, sophisticated deep learning-based approaches have been developed for generating efficient initial guesses to accelerate the convergence of density functional theory (DFT) calculations. While the actual initial guesses are often density matrices (DM), quantities that can convert into density matrices also qualify as alternative forms of initial guesses. Hence, existing works mostly rely on the prediction of the Hamiltonian matrix for obtaining high-quality initial guesses. However, the Hamiltonian matrix is both numerically difficult to predict and intrinsically non-transferable, hindering the application of such models in real scenarios. In light of this, we propose a method that constructs DFT initial guesses by predicting the electron density in a compact auxiliary basis representation using E(3)-equivariant neural networks. Trained on small molecules with up to 20 atoms, our model is able to achieve an average 33.3% self-consistent field (SCF) step reduction on systems up to 60 atoms, substantially outperforming Hamiltonian-centric and DM-centric models. Critically, this acceleration remains nearly constant with increasing system sizes and exhibits strong transferring behaviors across orbital basis sets and exchange-correlation (XC) functionals. To the best of our knowledge, this work represents the first and robust candidate for a universally transferable DFT acceleration method. We are also releasing the SCFbench dataset and its accompanying code to facilitate future research in this promising direction.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25724v2",
    "published_date": "2025-09-30 03:35:57 UTC",
    "updated_date": "2025-10-15 02:15:37 UTC"
  },
  {
    "arxiv_id": "2509.25721v6",
    "title": "The AI Productivity Index (APEX)",
    "authors": [
      "Bertie Vidgen",
      "Abby Fennelly",
      "Evan Pinnix",
      "Julien Benchek",
      "Daniyal Khan",
      "Zach Richards",
      "Austin Bridges",
      "Calix Huang",
      "Kanishka Sahu",
      "Abhishek Kottamasu",
      "Bo Ma",
      "Ben Hunsberger",
      "Isaac Robinson",
      "Akul Datta",
      "Chirag Mahapatra",
      "Dominic Barton",
      "Cass R. Sunstein",
      "Eric Topol",
      "Brendan Foody",
      "Osvald Nitski"
    ],
    "abstract": "We present an extended version of the AI Productivity Index (APEX-v1-extended), a benchmark for assessing whether frontier models are capable of performing economically valuable tasks in four jobs: investment banking associate, management consultant, big law associate, and primary care physician (MD). This technical report details the extensions to APEX-v1, including an increase in the held-out evaluation set from n = 50 to n = 100 cases per job (n = 400 total) and updates to the grading methodology. We present a new leaderboard, where GPT5 (Thinking = High) remains the top performing model with a score of 67.0%. APEX-v1-extended shows that frontier models still have substantial limitations when performing typical professional tasks. To support further research, we are open sourcing n = 25 non-benchmark example cases per role (n = 100 total) along with our evaluation harness.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25721v6",
    "published_date": "2025-09-30 03:26:17 UTC",
    "updated_date": "2025-12-16 22:25:15 UTC"
  },
  {
    "arxiv_id": "2509.25716v1",
    "title": "DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation",
    "authors": [
      "Esakkivel Esakkiraja",
      "Denis Akhiyarov",
      "Aditya Shanmugham",
      "Chitra Ganapathy"
    ],
    "abstract": "Current search techniques are limited to standard RAG query-document applications. In this paper, we propose a novel technique to expand the code and index for predicting the required APIs, directly enabling high-quality, end-to-end code generation for auto-completion and agentic AI applications. We address the problem of API leaks in current code-to-code benchmark datasets by introducing a new dataset built from real-world ServiceNow Script Includes that capture the challenge of unclear API usage intent in the code. Our evaluation metrics show that this method achieves 87.86% top-40 retrieval accuracy, allowing the critical context with APIs needed for successful downstream code generation. To enable real-time predictions, we develop a comprehensive post-training pipeline that optimizes a compact 0.6B reranker through synthetic dataset generation, supervised fine-tuning, and reinforcement learning. This approach enables our compact reranker to outperform a much larger 8B model while maintaining 2.5x reduced latency, effectively addressing the nuances of enterprise-specific code without the computational overhead of larger models.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.SE",
    "comment": "Retrieval-Augmented Generation, API Prediction, Context-Aware Code Generation, Enterprise Code Completion, Reinforcement Learning, ServiceNow, Real-Time Code Search, Query Enhancement, Fine-Tuning, Embedding, Reranker",
    "pdf_url": "https://arxiv.org/pdf/2509.25716v1",
    "published_date": "2025-09-30 03:23:27 UTC",
    "updated_date": "2025-09-30 03:23:27 UTC"
  },
  {
    "arxiv_id": "2510.05122v1",
    "title": "CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation",
    "authors": [
      "Jie Zhu",
      "Yuanchen Zhou",
      "Shuo Jiang",
      "Junhui Li",
      "Lifan Guo",
      "Feng Chen",
      "Chi Zhang",
      "Fang Kong"
    ],
    "abstract": "Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose \\textbf{CARE}, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.05122v1",
    "published_date": "2025-09-30 03:19:50 UTC",
    "updated_date": "2025-09-30 03:19:50 UTC"
  },
  {
    "arxiv_id": "2509.25694v2",
    "title": "HNote: Extending YNote with Hexadecimal Encoding for Fine-Tuning LLMs in Music Modeling",
    "authors": [
      "Hung-Ying Chu",
      "Shao-Yu Wei",
      "Guan-Wei Chen",
      "Tzu-Wei Hung",
      "ChengYang Tsai",
      "Yu-Cheng Lin"
    ],
    "abstract": "Recent advances in large language models (LLMs) have created new opportunities for symbolic music generation. However, existing formats such as MIDI, ABC, and MusicXML are either overly complex or structurally inconsistent, limiting their suitability for token-based learning architectures. To address these challenges, we propose HNote, a novel hexadecimal-based notation system extended from YNote, which encodes both pitch and duration within a fixed 32-unit measure framework. This design ensures alignment, reduces ambiguity, and is directly compatible with LLM architectures. We converted 12,300 Jiangnan-style songs generated from traditional folk pieces from YNote into HNote, and fine-tuned LLaMA-3.1(8B) using parameter-efficient LoRA. Experimental results show that HNote achieves a syntactic correctness rate of 82.5%, and BLEU and ROUGE evaluations demonstrate strong symbolic and structural similarity, producing stylistically coherent compositions. This study establishes HNote as an effective framework for integrating LLMs with cultural music modeling.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25694v2",
    "published_date": "2025-09-30 02:50:01 UTC",
    "updated_date": "2025-10-04 07:52:45 UTC"
  },
  {
    "arxiv_id": "2510.00080v2",
    "title": "SoREX: Towards Self-Explainable Social Recommendation with Relevant Ego-Path Extraction",
    "authors": [
      "Hanze Guo",
      "Yijun Ma",
      "Xiao Zhou"
    ],
    "abstract": "Social recommendation has been proven effective in addressing data sparsity in user-item interaction modeling by leveraging social networks. The recent integration of Graph Neural Networks (GNNs) has further enhanced prediction accuracy in contemporary social recommendation algorithms. However, many GNN-based approaches in social recommendation lack the ability to furnish meaningful explanations for their predictions. In this study, we confront this challenge by introducing SoREX, a self-explanatory GNN-based social recommendation framework. SoREX adopts a two-tower framework enhanced by friend recommendation, independently modeling social relations and user-item interactions, while jointly optimizing an auxiliary task to reinforce social signals. To offer explanations, we propose a novel ego-path extraction approach. This method involves transforming the ego-net of a target user into a collection of multi-hop ego-paths, from which we extract factor-specific and candidate-aware ego-path subsets as explanations. This process facilitates the summarization of detailed comparative explanations among different candidate items through intricate substructure analysis. Furthermore, we conduct explanation re-aggregation to explicitly correlate explanations with downstream predictions, imbuing our framework with inherent self-explainability. Comprehensive experiments conducted on four widely adopted benchmark datasets validate the effectiveness of SoREX in predictive accuracy. Additionally, qualitative and quantitative analyses confirm the efficacy of the extracted explanations in SoREX. Our code and data are available at https://github.com/antman9914/SoREX.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "ACM Transactions on Information Systems (TOIS), 2025. Online AM: 17 Nov 2025. DOI: 10.1145/3777374. Code: https://github.com/antman9914/SoREX",
    "pdf_url": "https://arxiv.org/pdf/2510.00080v2",
    "published_date": "2025-09-30 02:49:54 UTC",
    "updated_date": "2025-12-05 01:42:51 UTC"
  },
  {
    "arxiv_id": "2509.25693v2",
    "title": "ScheduleMe: Multi-Agent Calendar Assistant",
    "authors": [
      "Oshadha Wijerathne",
      "Amandi Nimasha",
      "Dushan Fernando",
      "Nisansa de Silva",
      "Srinath Perera"
    ],
    "abstract": "Recent advancements in LLMs have contributed to the rise of advanced conversational assistants that can assist with user needs through natural language conversation. This paper presents a ScheduleMe, a multi-agent calendar assistant for users to manage google calendar events in natural language. The system uses a graph-structured coordination mechanism where a central supervisory agent supervises specialized task agents, allowing modularity, conflicts resolution, and context-aware interactions to resolve ambiguities and evaluate user commands. This approach sets an example of how structured reasoning and agent cooperation might convince operators to increase the usability and flexibility of personal calendar assistant tools.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25693v2",
    "published_date": "2025-09-30 02:47:54 UTC",
    "updated_date": "2025-10-01 03:03:01 UTC"
  },
  {
    "arxiv_id": "2509.25692v1",
    "title": "Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction",
    "authors": [
      "Tingyu Shi",
      "Fan Lyu",
      "Shaoliang Peng"
    ],
    "abstract": "Active Test-Time Adaptation (ATTA) improves model robustness under domain shift by selectively querying human annotations at deployment, but existing methods use heuristic uncertainty measures and suffer from low data selection efficiency, wasting human annotation budget. We propose Conformal Prediction Active TTA (CPATTA), which first brings principled, coverage-guaranteed uncertainty into ATTA. CPATTA employs smoothed conformal scores with a top-K certainty measure, an online weight-update algorithm driven by pseudo coverage, a domain-shift detector that adapts human supervision, and a staged update scheme balances human-labeled and model-labeled data. Extensive experiments demonstrate that CPATTA consistently outperforms the state-of-the-art ATTA methods by around 5% in accuracy. Our code and datasets are available at https://github.com/tingyushi/CPATTA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25692v1",
    "published_date": "2025-09-30 02:47:34 UTC",
    "updated_date": "2025-09-30 02:47:34 UTC"
  },
  {
    "arxiv_id": "2509.25689v1",
    "title": "Collaborative Compression for Large-Scale MoE Deployment on Edge",
    "authors": [
      "Yixiao Chen",
      "Yanyue Xie",
      "Ruining Yang",
      "Wei Jiang",
      "Wei Wang",
      "Yong He",
      "Yue Chen",
      "Pu Zhao",
      "Yanzhi Wang"
    ],
    "abstract": "The Mixture of Experts (MoE) architecture is an important method for scaling Large Language Models (LLMs). It increases model capacity while keeping computation cost low. However, the ultra-large MoE models still have hundreds of billions of parameters, requiring massive memory/storage and leading to difficulties for deployment on resource-constrained edge platforms. Pruning or quantization alone can hardly address the issue, because of the super-aggressive compression ratio with significantly degraded accuracy and output quality. To facilitate the deployment of ultra-large MoEs on edge platforms, we propose a collaborative compression framework by combining expert pruning, mixed-precision quantization, and activation optimization. It can effectively reduce the storage footprint of the ultra-large MoE DeepSeek-V3 from 1.3TB to 103GB, while preserving high output quality with better accuracy than traditional uniform low-bit quantization methods. To the best of our knowledge, we are the first to deploy a compressed model from the ultra-large DeepSeek-V3 on the platform with a strict 128GB total memory limit. Our comprehensive experiments on multiple benchmarks under various memory constraints demonstrate the effectiveness of our method with smaller model sizes and higher accuracy than uniform low-bit quantization methods.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25689v1",
    "published_date": "2025-09-30 02:46:03 UTC",
    "updated_date": "2025-09-30 02:46:03 UTC"
  },
  {
    "arxiv_id": "2509.25684v1",
    "title": "LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts",
    "authors": [
      "Yuan Zhuang",
      "Yi Shen",
      "Yuexin Bian",
      "Qing Su",
      "Shihao Ji",
      "Yuanyuan Shi",
      "Fei Miao"
    ],
    "abstract": "Recent studies have shown that combining parameter-efficient fine-tuning (PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting large language models (LLMs) to the downstream tasks. However, most existing approaches rely on conventional TopK routing, which requires careful hyperparameter tuning and assigns a fixed number of experts to each token. In this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise expert allocation. Our method replaces the non-differentiable TopK selection with a differentiable routing function and a closed-form solution. Moreover, our design allows the model to adaptively determine the number of experts to activate for each token at different layers. In addition, we introduce an analytical sparsity control objective to regularize the number of activated experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show that LD-MoLE achieves the highest average scores compared to state-of-the-art baselines, across a diverse set of benchmarks. Our method not only achieves superior performance, but also demonstrates the ability to learn token-dependent and layer-wise expert allocation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25684v1",
    "published_date": "2025-09-30 02:38:10 UTC",
    "updated_date": "2025-09-30 02:38:10 UTC"
  },
  {
    "arxiv_id": "2510.00078v1",
    "title": "Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey",
    "authors": [
      "Sicong Liu",
      "Weiye Wu",
      "Xiangrui Xu",
      "Teng Li",
      "Bowen Pang",
      "Bin Guo",
      "Zhiwen Yu"
    ],
    "abstract": "Foundation models have reshaped AI by unifying fragmented architectures into scalable backbones with multimodal reasoning and contextual adaptation. In parallel, the long-standing notion of AI agents, defined by the sensing-decision-action loop, is entering a new paradigm: with FMs as their cognitive core, agents transcend rule-based behaviors to achieve autonomy, generalization, and self-reflection. This dual shift is reinforced by real-world demands such as autonomous driving, robotics, virtual assistants, and GUI agents, as well as ecosystem advances in embedded hardware, edge computing, mobile deployment platforms, and communication protocols that together enable large-scale deployment. Yet this convergence collides with reality: while applications demand long-term adaptability and real-time interaction, mobile and edge deployments remain constrained by memory, energy, bandwidth, and latency. This creates a fundamental tension between the growing complexity of FMs and the limited resources of deployment environments. This survey provides the first systematic characterization of adaptive, resource-efficient agentic AI systems. We summarize enabling techniques into elastic inference, test-time adaptation, dynamic multimodal integration, and agentic AI applications, and identify open challenges in balancing accuracy-latency-communication trade-offs and sustaining robustness under distribution shifts. We further highlight future opportunities in algorithm-system co-design, cognitive adaptation, and collaborative edge deployment. By mapping FM structures, cognition, and hardware resources, this work establishes a unified perspective toward scalable, adaptive, and resource-efficient agentic AI. We believe this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of agentic intelligence and intelligent agents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.00078v1",
    "published_date": "2025-09-30 02:37:52 UTC",
    "updated_date": "2025-09-30 02:37:52 UTC"
  },
  {
    "arxiv_id": "2509.25672v1",
    "title": "SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation",
    "authors": [
      "Hasan Alp Caferoğlu",
      "Mehmet Serhat Çelik",
      "Özgür Ulusoy"
    ],
    "abstract": "Translating natural language questions into SQL has become a core challenge in enabling non-technical users to query databases. While recent work has explored large-scale synthetic data generation to improve model performance through post-training, most efforts emphasize cross-domain generalization. This leaves a gap for real-world enterprise scenarios, where models need to specialize to a single database schema and organizations require to be able to evaluate their Text-to-SQL systems on their own databases. To address this, we introduce SING-SQL, a fully automated two-stage framework for generating high-quality, high-coverage synthetic Text-to-SQL data for any target database, without relying on SQL logs or manual annotations. Our approach hierarchically partitions a database schema into sub-schemas, synthesizes SQL queries across multiple complexity levels, and applies a quality-aware pipeline that includes LLM-as-a-judge validation, executability checks, automatic repair, and column balancing. We further release SingSQL-LM, a family of compact language models fine-tuned on the synthetic data, achieving strong in-domain generalization. On the subset of the BIRD benchmark, SingSQL-LM-3B-R64 reaches 82.87% Soft F1 and 73.03% EX upper bound with 32 candidates, outperforming the best 3B-scale baseline by +16.21 in Soft F1 and +12.36 in EX. At the 1.5B scale, SingSQL-LM-1.5B-R64 improves over prior systems by +9.30 in Soft F1 and +4.49 in EX. On synthetic evaluation sets, SingSQL-LMs exceed prior systems by wide margins, establishing state-of-the-art performance among open models at comparable scales. Our study of context management strategies reveals that schema-free fine-tuning combined with schema-only inference provides the most robust results. These findings establish SING-SQL as a scalable, database-agnostic paradigm for producing and evaluating enterprise-grade Text-to-SQL systems.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25672v1",
    "published_date": "2025-09-30 02:14:49 UTC",
    "updated_date": "2025-09-30 02:14:49 UTC"
  },
  {
    "arxiv_id": "2509.25669v1",
    "title": "GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination",
    "authors": [
      "Xinxi Chen",
      "Tianyang Chen",
      "Lijia Hong"
    ],
    "abstract": "We propose a method to improve Visual Question Answering (VQA) with Retrieval-Augmented Generation (RAG) by introducing text-grounded object localization. Rather than retrieving information based on the entire image, our approach enables the model to generate a bounding box around the object most relevant to the question, allowing for targeted image cropping and focused retrieval. This reduces background noise, improves alignment between visual and textual cues, and helps mitigate hallucinations. Our RAG method enhances context-aware VQA responses increased the accuracy from 22.19% to 25.64%, with an absolute increase of 3.45 percentage points, compared to the baseline Llama-3.2-Vision-11B agent. We also proposed a de-hallucination method based on question type which can effectively reduce the hallucination rate from 65.79% to 13.88% and improves the truthfulness score.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25669v1",
    "published_date": "2025-09-30 02:09:07 UTC",
    "updated_date": "2025-09-30 02:09:07 UTC"
  },
  {
    "arxiv_id": "2509.25667v1",
    "title": "EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface",
    "authors": [
      "Bipul Thapa",
      "Biplov Paneru",
      "Bishwash Paneru",
      "Khem Narayan Poudyal"
    ],
    "abstract": "This paper presents an Artificial Intelligence (AI) integrated novel approach to Brain-Computer Interface (BCI)-based wheelchair development, utilizing a motor imagery right-left-hand movement mechanism for control. The system is designed to simulate wheelchair navigation based on motor imagery right and left-hand movements using electroencephalogram (EEG) data. A pre-filtered dataset, obtained from an open-source EEG repository, was segmented into arrays of 19x200 to capture the onset of hand movements. The data was acquired at a sampling frequency of 200Hz. The system integrates a Tkinter-based interface for simulating wheelchair movements, offering users a functional and intuitive control system. We propose a BiLSTM-BiGRU model that shows a superior test accuracy of 92.26% as compared with various machine learning baseline models, including XGBoost, EEGNet, and a transformer-based model. The Bi-LSTM-BiGRU attention-based model achieved a mean accuracy of 90.13% through cross-validation, showcasing the potential of attention mechanisms in BCI applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25667v1",
    "published_date": "2025-09-30 02:06:04 UTC",
    "updated_date": "2025-09-30 02:06:04 UTC"
  },
  {
    "arxiv_id": "2509.25662v1",
    "title": "On Explaining Proxy Discrimination and Unfairness in Individual Decisions Made by AI Systems",
    "authors": [
      "Belona Sonna",
      "Alban Grastien"
    ],
    "abstract": "Artificial intelligence (AI) systems in high-stakes domains raise concerns about proxy discrimination, unfairness, and explainability. Existing audits often fail to reveal why unfairness arises, particularly when rooted in structural bias. We propose a novel framework using formal abductive explanations to explain proxy discrimination in individual AI decisions. Leveraging background knowledge, our method identifies which features act as unjustified proxies for protected attributes, revealing hidden structural biases. Central to our approach is the concept of aptitude, a task-relevant property independent of group membership, with a mapping function aligning individuals of equivalent aptitude across groups to assess fairness substantively. As a proof of concept, we showcase the framework with examples taken from the German credit dataset, demonstrating its applicability in real-world cases.",
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at AJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.25662v1",
    "published_date": "2025-09-30 01:58:59 UTC",
    "updated_date": "2025-09-30 01:58:59 UTC"
  },
  {
    "arxiv_id": "2509.25661v1",
    "title": "Deep Reinforcement Learning-Based Precoding for Multi-RIS-Aided Multiuser Downlink Systems with Practical Phase Shift",
    "authors": [
      "Po-Heng Chou",
      "Bo-Ren Zheng",
      "Wan-Jen Huang",
      "Walid Saad",
      "Yu Tsao",
      "Ronald Y. Chang"
    ],
    "abstract": "This study considers multiple reconfigurable intelligent surfaces (RISs)-aided multiuser downlink systems with the goal of jointly optimizing the transmitter precoding and RIS phase shift matrix to maximize spectrum efficiency. Unlike prior work that assumed ideal RIS reflectivity, a practical coupling effect is considered between reflecting amplitude and phase shift for the RIS elements. This makes the optimization problem non-convex. To address this challenge, we propose a deep deterministic policy gradient (DDPG)-based deep reinforcement learning (DRL) framework. The proposed model is evaluated under both fixed and random numbers of users in practical mmWave channel settings. Simulation results demonstrate that, despite its complexity, the proposed DDPG approach significantly outperforms optimization-based algorithms and double deep Q-learning, particularly in scenarios with random user distributions.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.IT",
    "comment": "5 pages, 5 figures, and published in IEEE Wireless Communications Letters",
    "pdf_url": "https://arxiv.org/pdf/2509.25661v1",
    "published_date": "2025-09-30 01:57:57 UTC",
    "updated_date": "2025-09-30 01:57:57 UTC"
  },
  {
    "arxiv_id": "2509.25660v1",
    "title": "Capacity-Net-Based RIS Precoding Design without Channel Estimation for mmWave MIMO System",
    "authors": [
      "Chun-Yuan Huang",
      "Po-Heng Chou",
      "Wan-Jen Huang",
      "Ying-Ren Chien",
      "Yu Tsao"
    ],
    "abstract": "In this paper, we propose Capacity-Net, a novel unsupervised learning approach aimed at maximizing the achievable rate in reflecting intelligent surface (RIS)-aided millimeter-wave (mmWave) multiple input multiple output (MIMO) systems. To combat severe channel fading of the mmWave spectrum, we optimize the phase-shifting factors of the reflective elements in the RIS to enhance the achievable rate. However, most optimization algorithms rely heavily on complete and accurate channel state information (CSI), which is often challenging to acquire since the RIS is mostly composed of passive components. To circumvent this challenge, we leverage unsupervised learning techniques with implicit CSI provided by the received pilot signals. Specifically, it usually requires perfect CSI to evaluate the achievable rate as a performance metric of the current optimization result of the unsupervised learning method. Instead of channel estimation, the Capacity-Net is proposed to establish a mapping among the received pilot signals, optimized RIS phase shifts, and the resultant achievable rates. Simulation results demonstrate the superiority of the proposed Capacity-Net-based unsupervised learning approach over learning methods based on traditional channel estimation.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.IT",
    "comment": "10 pages, 5 figures, and published in 2024 IEEE PIMRC",
    "pdf_url": "https://arxiv.org/pdf/2509.25660v1",
    "published_date": "2025-09-30 01:57:33 UTC",
    "updated_date": "2025-09-30 01:57:33 UTC"
  },
  {
    "arxiv_id": "2509.25659v2",
    "title": "YOLO-Based Defect Detection for Metal Sheets",
    "authors": [
      "Po-Heng Chou",
      "Chun-Chi Wang",
      "Wei-Lung Mao"
    ],
    "abstract": "In this paper, we propose a YOLO-based deep learning (DL) model for automatic defect detection to solve the time-consuming and labor-intensive tasks in industrial manufacturing. In our experiments, the images of metal sheets are used as the dataset for training the YOLO model to detect the defects on the surfaces and in the holes of metal sheets. However, the lack of metal sheet images significantly degrades the performance of detection accuracy. To address this issue, the ConSinGAN is used to generate a considerable amount of data. Four versions of the YOLO model (i.e., YOLOv3, v4, v7, and v9) are combined with the ConSinGAN for data augmentation. The proposed YOLOv9 model with ConSinGAN outperforms the other YOLO models with an accuracy of 91.3%, and a detection time of 146 ms. The proposed YOLOv9 model is integrated into manufacturing hardware and a supervisory control and data acquisition (SCADA) system to establish a practical automated optical inspection (AOI) system. Additionally, the proposed automated defect detection is easily applied to other components in industrial manufacturing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 8 figures, 2 tables, and published in IEEE IST 2024",
    "pdf_url": "https://arxiv.org/pdf/2509.25659v2",
    "published_date": "2025-09-30 01:56:44 UTC",
    "updated_date": "2025-10-03 02:02:07 UTC"
  },
  {
    "arxiv_id": "2509.25655v1",
    "title": "Landmark-Guided Knowledge for Vision-and-Language Navigation",
    "authors": [
      "Dongsheng Yang",
      "Meiling Zhu",
      "Yinfeng Yu"
    ],
    "abstract": "Vision-and-language navigation is one of the core tasks in embodied intelligence, requiring an agent to autonomously navigate in an unfamiliar environment based on natural language instructions. However, existing methods often fail to match instructions with environmental information in complex scenarios, one reason being the lack of common-sense reasoning ability. This paper proposes a vision-and-language navigation method called Landmark-Guided Knowledge (LGK), which introduces an external knowledge base to assist navigation, addressing the misjudgment issues caused by insufficient common sense in traditional methods. Specifically, we first construct a knowledge base containing 630,000 language descriptions and use knowledge Matching to align environmental subviews with the knowledge base, extracting relevant descriptive knowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism, which guides the agent to focus on the most relevant parts of the knowledge by leveraging landmark information in the instructions, thereby reducing the data bias that may arise from incorporating external knowledge. Finally, we propose Knowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates language, knowledge, vision, and historical information. Experimental results demonstrate that the LGK method outperforms existing state-of-the-art methods on the R2R and REVERIE vision-and-language navigation datasets, particularly in terms of navigation error, success rate, and path efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication by International Conference on Intelligent Computing 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.25655v1",
    "published_date": "2025-09-30 01:54:27 UTC",
    "updated_date": "2025-09-30 01:54:27 UTC"
  },
  {
    "arxiv_id": "2509.25652v1",
    "title": "Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks",
    "authors": [
      "Hailong Zhang",
      "Yinfeng Yu",
      "Liejun Wang",
      "Fuchun Sun",
      "Wendong Zheng"
    ],
    "abstract": "Audio-visual navigation represents a significant area of research in which intelligent agents utilize egocentric visual and auditory perceptions to identify audio targets. Conventional navigation methodologies typically adopt a staged modular design, which involves first executing feature fusion, then utilizing Gated Recurrent Unit (GRU) modules for sequence modeling, and finally making decisions through reinforcement learning. While this modular approach has demonstrated effectiveness, it may also lead to redundant information processing and inconsistencies in information transmission between the various modules during the feature fusion and GRU sequence modeling phases. This paper presents IRCAM-AVN (Iterative Residual Cross-Attention Mechanism for Audiovisual Navigation), an end-to-end framework that integrates multimodal information fusion and sequence modeling within a unified IRCAM module, thereby replacing the traditional separate components for fusion and GRU. This innovative mechanism employs a multi-level residual design that concatenates initial multimodal sequences with processed information sequences. This methodological shift progressively optimizes the feature extraction process while reducing model bias and enhancing the model's stability and generalization capabilities. Empirical results indicate that intelligent agents employing the iterative residual cross-attention mechanism exhibit superior navigation performance.",
    "categories": [
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication by IEEE International Conference on Systems, Man, and Cybernetics 2025",
    "pdf_url": "https://arxiv.org/pdf/2509.25652v1",
    "published_date": "2025-09-30 01:52:57 UTC",
    "updated_date": "2025-09-30 01:52:57 UTC"
  },
  {
    "arxiv_id": "2509.25651v1",
    "title": "AutoLabs: Cognitive Multi-Agent Systems with Self-Correction for Autonomous Chemical Experimentation",
    "authors": [
      "Gihan Panapitiya",
      "Emily Saldanha",
      "Heather Job",
      "Olivia Hess"
    ],
    "abstract": "The automation of chemical research through self-driving laboratories (SDLs) promises to accelerate scientific discovery, yet the reliability and granular performance of the underlying AI agents remain critical, under-examined challenges. In this work, we introduce AutoLabs, a self-correcting, multi-agent architecture designed to autonomously translate natural-language instructions into executable protocols for a high-throughput liquid handler. The system engages users in dialogue, decomposes experimental goals into discrete tasks for specialized agents, performs tool-assisted stoichiometric calculations, and iteratively self-corrects its output before generating a hardware-ready file. We present a comprehensive evaluation framework featuring five benchmark experiments of increasing complexity, from simple sample preparation to multi-plate timed syntheses. Through a systematic ablation study of 20 agent configurations, we assess the impact of reasoning capacity, architectural design (single- vs. multi-agent), tool use, and self-correction mechanisms. Our results demonstrate that agent reasoning capacity is the most critical factor for success, reducing quantitative errors in chemical amounts (nRMSE) by over 85% in complex tasks. When combined with a multi-agent architecture and iterative self-correction, AutoLabs achieves near-expert procedural accuracy (F1-score > 0.89) on challenging multi-step syntheses. These findings establish a clear blueprint for developing robust and trustworthy AI partners for autonomous laboratories, highlighting the synergistic effects of modular design, advanced reasoning, and self-correction to ensure both performance and reliability in high-stakes scientific applications. Code: https://github.com/pnnl/autolabs",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25651v1",
    "published_date": "2025-09-30 01:51:46 UTC",
    "updated_date": "2025-09-30 01:51:46 UTC"
  },
  {
    "arxiv_id": "2509.25647v1",
    "title": "BaB-prob: Branch and Bound with Preactivation Splitting for Probabilistic Verification of Neural Networks",
    "authors": [
      "Fangji Wang",
      "Panagiotis Tsiotras"
    ],
    "abstract": "Branch-and-bound with preactivation splitting has been shown highly effective for deterministic verification of neural networks. In this paper, we extend this framework to the probabilistic setting. We propose BaB-prob that iteratively divides the original problem into subproblems by splitting preactivations and leverages linear bounds computed by linear bound propagation to bound the probability for each subproblem. We prove soundness and completeness of BaB-prob for feedforward-ReLU neural networks. Furthermore, we introduce the notion of uncertainty level and design two efficient strategies for preactivation splitting, yielding BaB-prob-ordered and BaB+BaBSR-prob. We evaluate BaB-prob on untrained networks, MNIST and CIFAR-10 models, respectively, and VNN-COMP 2025 benchmarks. Across these settings, our approach consistently outperforms state-of-the-art approaches in medium- to high-dimensional input problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25647v1",
    "published_date": "2025-09-30 01:39:39 UTC",
    "updated_date": "2025-09-30 01:39:39 UTC"
  },
  {
    "arxiv_id": "2509.25643v3",
    "title": "SOCK: A Benchmark for Measuring Self-Replication in Large Language Models",
    "authors": [
      "Justin Chavarria",
      "Rohan Raizada",
      "Justin White",
      "Eyad Alhetairshi"
    ],
    "abstract": "We introduce SOCK, a benchmark command line interface (CLI) that measures large language models' (LLMs) ability to self-replicate without human intervention. In this benchmark, self-replication is defined not only as an LLM's ability to create a functioning and running copy of itself, but also the ability for that self-replication to persist and occur across different computational contexts. Accordingly, we've developed a system to categorize LLMs based on broad self-replication capabilities in two general classes, Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL). Using a five-task suite based on practically manipulable modern CLI utilities and computer processes, experiments are orchestrated in a controlled environment with an LLM acting agentically. The performance of the LLM on agent tasks is then computed to produce an R-score (a quantitative evaluation of overall self-replication ability) and data used to categorize LLMs into specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides the first formalized definitions and benchmark suite for evaluating LLM self-replication, with the goal of establishing a standard for future research, to our knowledge; (2) Allows the industry to track the effectiveness of future multi-agent systems and mitigate potential self-replication threat vectors within them. The results compiled from evaluating a variety of open-weight and proprietary frontier models reveal significant obstacles to persistent self-replication and multi-agent systems, including context retention and multi-agent decision-making. We propose future research directions to safely reduce the severity of these obstacles, potentially lowering future risk of more functional multi-agent systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25643v3",
    "published_date": "2025-09-30 01:27:46 UTC",
    "updated_date": "2025-12-09 14:21:36 UTC"
  },
  {
    "arxiv_id": "2510.00075v1",
    "title": "NeurIPS should lead scientific consensus on AI policy",
    "authors": [
      "Rishi Bommasani"
    ],
    "abstract": "Designing wise AI policy is a grand challenge for society. To design such policy, policymakers should place a premium on rigorous evidence and scientific consensus. While several mechanisms exist for evidence generation, and nascent mechanisms tackle evidence synthesis, we identify a complete void on consensus formation. In this position paper, we argue NeurIPS should actively catalyze scientific consensus on AI policy. Beyond identifying the current deficit in consensus formation mechanisms, we argue that NeurIPS is the best option due its strengths and the paucity of compelling alternatives. To make progress, we recommend initial pilots for NeurIPS by distilling lessons from the IPCC's leadership to build scientific consensus on climate policy. We dispel predictable counters that AI researchers disagree too much to achieve consensus and that policy engagement is not the business of NeurIPS. NeurIPS leads AI on many fronts, and it should champion scientific consensus to create higher quality AI policy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.00075v1",
    "published_date": "2025-09-30 01:08:29 UTC",
    "updated_date": "2025-09-30 01:08:29 UTC"
  },
  {
    "arxiv_id": "2510.02376v1",
    "title": "Scaling Homomorphic Applications in Deployment",
    "authors": [
      "Ryan Marinelli",
      "Angelica Chowdhury"
    ],
    "abstract": "In this endeavor, a proof-of-concept homomorphic application is developed to determine the production readiness of encryption ecosystems. A movie recommendation app is implemented for this purpose and productionized through containerization and orchestration. By tuning deployment configurations, the computational limitations of Fully Homomorphic Encryption (FHE) are mitigated through additional infrastructure optimizations\n  Index Terms: Reinforcement Learning, Orchestration, Homomorphic Encryption",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "5 pages, 6 figures, 1 pseudo code",
    "pdf_url": "https://arxiv.org/pdf/2510.02376v1",
    "published_date": "2025-09-30 00:42:55 UTC",
    "updated_date": "2025-09-30 00:42:55 UTC"
  },
  {
    "arxiv_id": "2509.25624v1",
    "title": "STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents",
    "authors": [
      "Jing-Jing Li",
      "Jianfeng He",
      "Chao Shang",
      "Devang Kulshreshtha",
      "Xun Xian",
      "Yi Zhang",
      "Hang Su",
      "Sandesh Swamy",
      "Yanjun Qi"
    ],
    "abstract": "As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges that extend beyond traditional content-based LLM safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC), a novel multi-turn attack framework that exploits agent tool use. STAC chains together tool calls that each appear harmless in isolation but, when combined, collectively enable harmful operations that only become apparent at the final execution step. We apply our framework to automatically generate and systematically evaluate 483 STAC cases, featuring 1,352 sets of user-agent-environment interactions and spanning diverse domains, tasks, agent types, and 10 failure modes. Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases. The core design of STAC's automated framework is a closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts that reliably induce agents to execute the verified malicious sequence. We further perform defense analysis against STAC and find that existing prompt-based defenses provide limited protection. To address this gap, we propose a new reasoning-driven defense prompt that achieves far stronger protection, cutting ASR by up to 28.8%. These results highlight a crucial gap: defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25624v1",
    "published_date": "2025-09-30 00:31:44 UTC",
    "updated_date": "2025-09-30 00:31:44 UTC"
  },
  {
    "arxiv_id": "2509.25618v1",
    "title": "Quadratic Programming Approach for Nash Equilibrium Computation in Multiplayer Imperfect-Information Games",
    "authors": [
      "Sam Ganzfried"
    ],
    "abstract": "There has been significant recent progress in algorithms for approximation of Nash equilibrium in large two-player zero-sum imperfect-information games and exact computation of Nash equilibrium in multiplayer strategic-form games. While counterfactual regret minimization and fictitious play are scalable to large games and have convergence guarantees in two-player zero-sum games, they do not guarantee convergence to Nash equilibrium in multiplayer games. We present an approach for exact computation of Nash equilibrium in multiplayer imperfect-information games that solves a quadratically-constrained program based on a nonlinear complementarity problem formulation from the sequence-form game representation. This approach capitalizes on recent advances for solving nonconvex quadratic programs. Our algorithm is able to quickly solve three-player Kuhn poker after removal of dominated actions. Of the available algorithms in the Gambit software suite, only the logit quantal response approach is successfully able to solve the game; however, the approach takes longer than our algorithm and also involves a degree of approximation. Our formulation also leads to a new approach for computing Nash equilibrium in multiplayer strategic-form games which we demonstrate to outperform a previous quadratically-constrained program formulation.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25618v1",
    "published_date": "2025-09-30 00:28:21 UTC",
    "updated_date": "2025-09-30 00:28:21 UTC"
  },
  {
    "arxiv_id": "2510.21729v2",
    "title": "CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known Document Corpora",
    "authors": [
      "Nathan Paull"
    ],
    "abstract": "Dense embedding models have become critical for modern information retrieval, particularly in RAG pipelines, but their performance often degrades when applied to specialized corpora outside their pre-training distribution. To address thi we introduce CustomIR, a framework for unsupervised adaptation of pre-trained language embedding models to domain-specific corpora using synthetically generated query-document pairs. CustomIR leverages large language models (LLMs) to create diverse queries grounded in a known target corpus, paired with LLM-verified hard negatives, eliminating the need for costly human annotation. Experiments on enterprise email and messaging datasets show that CustomIR consistently improves retrieval effectiveness with small models gaining up to 2.3 points in Recall@10. This performance increase allows these small models to rival the performance of much larger alternatives, allowing for cheaper RAG deployments. These results highlight that targeted synthetic fine-tuning offers a scalable and cost-efficient strategy for increasing domain-specific performance.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21729v2",
    "published_date": "2025-09-30 00:25:47 UTC",
    "updated_date": "2025-10-28 16:15:47 UTC"
  },
  {
    "arxiv_id": "2509.25613v1",
    "title": "SMS: Self-supervised Model Seeding for Verification of Machine Unlearning",
    "authors": [
      "Weiqi Wang",
      "Chenhan Zhang",
      "Zhiyi Tian",
      "Shui Yu"
    ],
    "abstract": "Many machine unlearning methods have been proposed recently to uphold users' right to be forgotten. However, offering users verification of their data removal post-unlearning is an important yet under-explored problem. Current verifications typically rely on backdooring, i.e., adding backdoored samples to influence model performance. Nevertheless, the backdoor methods can merely establish a connection between backdoored samples and models but fail to connect the backdoor with genuine samples. Thus, the backdoor removal can only confirm the unlearning of backdoored samples, not users' genuine samples, as genuine samples are independent of backdoored ones. In this paper, we propose a Self-supervised Model Seeding (SMS) scheme to provide unlearning verification for genuine samples. Unlike backdooring, SMS links user-specific seeds (such as users' unique indices), original samples, and models, thereby facilitating the verification of unlearning genuine samples. However, implementing SMS for unlearning verification presents two significant challenges. First, embedding the seeds into the service model while keeping them secret from the server requires a sophisticated approach. We address this by employing a self-supervised model seeding task, which learns the entire sample, including the seeds, into the model's latent space. Second, maintaining the utility of the original service model while ensuring the seeding effect requires a delicate balance. We design a joint-training structure that optimizes both the self-supervised model seeding task and the primary service task simultaneously on the model, thereby maintaining model utility while achieving effective model seeding. The effectiveness of the proposed SMS scheme is evaluated through extensive experiments, which demonstrate that SMS provides effective verification for genuine sample unlearning, addressing existing limitations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25613v1",
    "published_date": "2025-09-30 00:18:44 UTC",
    "updated_date": "2025-09-30 00:18:44 UTC"
  },
  {
    "arxiv_id": "2509.25612v1",
    "title": "Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN",
    "authors": [
      "Muhammad Imran Hossain",
      "Jignesh Solanki",
      "Sarika Khushlani Solanki"
    ],
    "abstract": "Ensuring power grid resilience requires the timely and unsupervised detection of anomalies in synchrophasor data streams. We introduce T-BiGAN, a novel framework that integrates window-attention Transformers within a bidirectional Generative Adversarial Network (BiGAN) to address this challenge. Its self-attention encoder-decoder architecture captures complex spatio-temporal dependencies across the grid, while a joint discriminator enforces cycle consistency to align the learned latent space with the true data distribution. Anomalies are flagged in real-time using an adaptive score that combines reconstruction error, latent space drift, and discriminator confidence. Evaluated on a realistic hardware-in-the-loop PMU benchmark, T-BiGAN achieves an ROC-AUC of 0.95 and an average precision of 0.996, significantly outperforming leading supervised and unsupervised methods. It shows particular strength in detecting subtle frequency and voltage deviations, demonstrating its practical value for live, wide-area monitoring without relying on manually labeled fault data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2509.25612v1",
    "published_date": "2025-09-30 00:16:35 UTC",
    "updated_date": "2025-09-30 00:16:35 UTC"
  },
  {
    "arxiv_id": "2509.25609v1",
    "title": "A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments",
    "authors": [
      "Manuel Cherep",
      "Chengtian Ma",
      "Abigail Xu",
      "Maya Shaked",
      "Pattie Maes",
      "Nikhil Singh"
    ],
    "abstract": "Environments built for people are increasingly operated by a new class of economic actors: LLM-powered software agents making decisions on our behalf. These decisions range from our purchases to travel plans to medical treatment selection. Current evaluations of these agents largely focus on task competence, but we argue for a deeper assessment: how these agents choose when faced with realistic decisions. We introduce ABxLab, a framework for systematically probing agentic choice through controlled manipulations of option attributes and persuasive cues. We apply this to a realistic web-based shopping environment, where we vary prices, ratings, and psychological nudges, all of which are factors long known to shape human choice. We find that agent decisions shift predictably and substantially in response, revealing that agents are strongly biased choosers even without being subject to the cognitive constraints that shape human biases. This susceptibility reveals both risk and opportunity: risk, because agentic consumers may inherit and amplify human biases; opportunity, because consumer choice provides a powerful testbed for a behavioral science of AI agents, just as it has for the study of human behavior. We release our framework as an open benchmark for rigorous, scalable evaluation of agent decision-making.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2509.25609v1",
    "published_date": "2025-09-30 00:05:23 UTC",
    "updated_date": "2025-09-30 00:05:23 UTC"
  }
]
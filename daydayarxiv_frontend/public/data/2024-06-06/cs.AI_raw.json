[
  {
    "arxiv_id": "2406.04553v2",
    "title": "Better Late Than Never: Formulating and Benchmarking Recommendation Editing",
    "authors": [
      "Chengyu Lai",
      "Sheng Zhou",
      "Zhimeng Jiang",
      "Qiaoyu Tan",
      "Yuanchen Bei",
      "Jiawei Chen",
      "Ningyu Zhang",
      "Jiajun Bu"
    ],
    "abstract": "Recommendation systems play a pivotal role in suggesting items to users based\non their preferences. However, in online platforms, these systems inevitably\noffer unsuitable recommendations due to limited model capacity, poor data\nquality, or evolving user interests. Enhancing user experience necessitates\nefficiently rectify such unsuitable recommendation behaviors. This paper\nintroduces a novel and significant task termed recommendation editing, which\nfocuses on modifying known and unsuitable recommendation behaviors.\nSpecifically, this task aims to adjust the recommendation model to eliminate\nknown unsuitable items without accessing training data or retraining the model.\nWe formally define the problem of recommendation editing with three primary\nobjectives: strict rectification, collaborative rectification, and concentrated\nrectification. Three evaluation metrics are developed to quantitatively assess\nthe achievement of each objective. We present a straightforward yet effective\nbenchmark for recommendation editing using novel Editing Bayesian Personalized\nRanking Loss. To demonstrate the effectiveness of the proposed method, we\nestablish a comprehensive benchmark that incorporates various methods from\nrelated fields. Codebase is available at\nhttps://github.com/cycl2018/Recommendation-Editing.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04553v2",
    "published_date": "2024-06-06 23:44:33 UTC",
    "updated_date": "2024-10-28 07:38:11 UTC"
  },
  {
    "arxiv_id": "2406.04551v2",
    "title": "Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance",
    "authors": [
      "Reyhane Askari Hemmat",
      "Melissa Hall",
      "Alicia Sun",
      "Candace Ross",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ],
    "abstract": "With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04551v2",
    "published_date": "2024-06-06 23:35:51 UTC",
    "updated_date": "2024-08-02 16:09:49 UTC"
  },
  {
    "arxiv_id": "2406.04533v1",
    "title": "Rare Class Prediction Model for Smart Industry in Semiconductor Manufacturing",
    "authors": [
      "Abdelrahman Farrag",
      "Mohammed-Khalil Ghali",
      "Yu Jin"
    ],
    "abstract": "The evolution of industry has enabled the integration of physical and digital\nsystems, facilitating the collection of extensive data on manufacturing\nprocesses. This integration provides a reliable solution for improving process\nquality and managing equipment health. However, data collected from real\nmanufacturing processes often exhibit challenging properties, such as severe\nclass imbalance, high rates of missing values, and noisy features, which hinder\neffective machine learning implementation. In this study, a rare class\nprediction approach is developed for in situ data collected from a smart\nsemiconductor manufacturing process. The primary objective is to build a model\nthat addresses issues of noise and class imbalance, enhancing class separation.\nThe developed approach demonstrated promising results compared to existing\nliterature, which would allow the prediction of new observations that could\ngive insights into future maintenance plans and production quality. The model\nwas evaluated using various performance metrics, with ROC curves showing an AUC\nof 0.95, a precision of 0.66, and a recall of 0.96",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04533v1",
    "published_date": "2024-06-06 22:09:43 UTC",
    "updated_date": "2024-06-06 22:09:43 UTC"
  },
  {
    "arxiv_id": "2406.06611v1",
    "title": "Building Hybrid B-Spline And Neural Network Operators",
    "authors": [
      "Raffaele Romagnoli",
      "Jasmine Ratchford",
      "Mark H. Klein"
    ],
    "abstract": "Control systems are indispensable for ensuring the safety of cyber-physical\nsystems (CPS), spanning various domains such as automobiles, airplanes, and\nmissiles. Safeguarding CPS necessitates runtime methodologies that continuously\nmonitor safety-critical conditions and respond in a verifiably safe manner. A\nfundamental aspect of many safety approaches involves predicting the future\nbehavior of systems. However, achieving this requires accurate models that can\noperate in real time. Motivated by DeepONets, we propose a novel strategy that\ncombines the inductive bias of B-splines with data-driven neural networks to\nfacilitate real-time predictions of CPS behavior. We introduce our hybrid\nB-spline neural operator, establishing its capability as a universal\napproximator and providing rigorous bounds on the approximation error. These\nfindings are applicable to a broad class of nonlinear autonomous systems and\nare validated through experimentation on a controlled 6-degree-of-freedom (DOF)\nquadrotor with a 12 dimensional state space. Furthermore, we conduct a\ncomparative analysis of different network architectures, specifically fully\nconnected networks (FCNN) and recurrent neural networks (RNN), to elucidate the\npractical utility and trade-offs associated with each architecture in\nreal-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06611v1",
    "published_date": "2024-06-06 21:54:59 UTC",
    "updated_date": "2024-06-06 21:54:59 UTC"
  },
  {
    "arxiv_id": "2406.04520v1",
    "title": "NATURAL PLAN: Benchmarking LLMs on Natural Language Planning",
    "authors": [
      "Huaixiu Steven Zheng",
      "Swaroop Mishra",
      "Hugh Zhang",
      "Xinyun Chen",
      "Minmin Chen",
      "Azade Nova",
      "Le Hou",
      "Heng-Tze Cheng",
      "Quoc V. Le",
      "Ed H. Chi",
      "Denny Zhou"
    ],
    "abstract": "We introduce NATURAL PLAN, a realistic planning benchmark in natural language\ncontaining 3 key tasks: Trip Planning, Meeting Planning, and Calendar\nScheduling. We focus our evaluation on the planning capabilities of LLMs with\nfull information on the task, by providing outputs from tools such as Google\nFlights, Google Maps, and Google Calendar as contexts to the models. This\neliminates the need for a tool-use environment for evaluating LLMs on Planning.\nWe observe that NATURAL PLAN is a challenging benchmark for state of the art\nmodels. For example, in Trip Planning, GPT-4 and Gemini 1.5 Pro could only\nachieve 31.1% and 34.8% solve rate respectively. We find that model performance\ndrops drastically as the complexity of the problem increases: all models\nperform below 5% when there are 10 cities, highlighting a significant gap in\nplanning in natural language for SoTA LLMs. We also conduct extensive ablation\nstudies on NATURAL PLAN to further shed light on the (in)effectiveness of\napproaches such as self-correction, few-shot generalization, and in-context\nplanning with long-contexts on improving LLM planning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04520v1",
    "published_date": "2024-06-06 21:27:35 UTC",
    "updated_date": "2024-06-06 21:27:35 UTC"
  },
  {
    "arxiv_id": "2406.04508v2",
    "title": "OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification Inference",
    "authors": [
      "Dujian Ding",
      "Bicheng Xu",
      "Laks V. S. Lakshmanan"
    ],
    "abstract": "Classification tasks play a fundamental role in various applications,\nspanning domains such as healthcare, natural language processing and computer\nvision. With the growing popularity and capacity of machine learning models,\npeople can easily access trained classifiers as a service online or offline.\nHowever, model use comes with a cost and classifiers of higher capacity (such\nas large foundation models) usually incur higher inference costs. To harness\nthe respective strengths of different classifiers, we propose a principled\napproach, OCCAM, to compute the best classifier assignment strategy over\nclassification queries (termed as the optimal model portfolio) so that the\naggregated accuracy is maximized, under user-specified cost budgets. Our\napproach uses an unbiased and low-variance accuracy estimator and effectively\ncomputes the optimal solution by solving an integer linear programming problem.\nOn a variety of real-world datasets, OCCAM achieves 40% cost reduction with\nlittle to no accuracy drop.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025 (main conference)",
    "pdf_url": "http://arxiv.org/pdf/2406.04508v2",
    "published_date": "2024-06-06 21:05:39 UTC",
    "updated_date": "2025-02-25 03:15:20 UTC"
  },
  {
    "arxiv_id": "2406.18587v1",
    "title": "Nomic Embed Vision: Expanding the Latent Space",
    "authors": [
      "Zach Nussbaum",
      "Brandon Duderstadt",
      "Andriy Mulyar"
    ],
    "abstract": "This technical report describes the training of nomic-embed-vision, a highly\nperformant, open-code, open-weights image embedding model that shares the same\nlatent space as nomic-embed-text. Together, nomic-embed-vision and\nnomic-embed-text form the first unified latent space to achieve high\nperformance across vision, language, and multimodal tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18587v1",
    "published_date": "2024-06-06 21:02:51 UTC",
    "updated_date": "2024-06-06 21:02:51 UTC"
  },
  {
    "arxiv_id": "2406.04501v1",
    "title": "FLUID-LLM: Learning Computational Fluid Dynamics with Spatiotemporal-aware Large Language Models",
    "authors": [
      "Max Zhu",
      "Adrián Bazaga",
      "Pietro Liò"
    ],
    "abstract": "Learning computational fluid dynamics (CFD) traditionally relies on\ncomputationally intensive simulations of the Navier-Stokes equations. Recently,\nlarge language models (LLMs) have shown remarkable pattern recognition and\nreasoning abilities in natural language processing (NLP) and computer vision\n(CV). However, these models struggle with the complex geometries inherent in\nfluid dynamics. We introduce FLUID-LLM, a novel framework combining pre-trained\nLLMs with spatiotemporal-aware encoding to predict unsteady fluid dynamics. Our\napproach leverages the temporal autoregressive abilities of LLMs alongside\nspatial-aware layers, bridging the gap between previous CFD prediction methods.\nEvaluations on standard benchmarks reveal significant performance improvements\nacross various fluid datasets. Our results demonstrate that FLUID-LLM\neffectively integrates spatiotemporal information into pre-trained LLMs,\nenhancing CFD task performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04501v1",
    "published_date": "2024-06-06 20:55:40 UTC",
    "updated_date": "2024-06-06 20:55:40 UTC"
  },
  {
    "arxiv_id": "2406.04496v2",
    "title": "Time Sensitive Knowledge Editing through Efficient Finetuning",
    "authors": [
      "Xiou Ge",
      "Ali Mousavi",
      "Edouard Grave",
      "Armand Joulin",
      "Kun Qian",
      "Benjamin Han",
      "Mostafa Arefiyan",
      "Yunyao Li"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capability in\ndifferent tasks and are bringing transformative changes to many domains.\nHowever, keeping the knowledge in LLMs up-to-date remains a challenge once\npretraining is complete. It is thus essential to design effective methods to\nboth update obsolete knowledge and induce new knowledge into LLMs. Existing\nlocate-and-edit knowledge editing (KE) method suffers from two limitations.\nFirst, the post-edit LLMs by such methods generally have poor capability in\nanswering complex queries that require multi-hop reasoning. Second, the long\nrun-time of such locate-and-edit methods to perform knowledge edits make it\ninfeasible for large scale KE in practice. In this paper, we explore\nParameter-Efficient Fine-Tuning (PEFT) techniques as an alternative for KE. We\ncurate a more comprehensive temporal KE dataset with both knowledge update and\nknowledge injection examples for KE performance benchmarking. We further probe\nthe effect of fine-tuning on a range of layers in an LLM for the multi-hop QA\ntask. We find that PEFT performs better than locate-and-edit techniques for\ntime-sensitive knowledge edits.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 main",
    "pdf_url": "http://arxiv.org/pdf/2406.04496v2",
    "published_date": "2024-06-06 20:41:36 UTC",
    "updated_date": "2024-07-23 00:46:37 UTC"
  },
  {
    "arxiv_id": "2406.06610v1",
    "title": "Reinterpreting 'the Company a Word Keeps': Towards Explainable and Ontologically Grounded Language Models",
    "authors": [
      "Walid S. Saba"
    ],
    "abstract": "We argue that the relative success of large language models (LLMs) is not a\nreflection on the symbolic vs. subsymbolic debate but a reflection on employing\na successful bottom-up strategy of a reverse engineering of language at scale.\nHowever, and due to their subsymbolic nature whatever knowledge these systems\nacquire about language will always be buried in millions of weights none of\nwhich is meaningful on its own, rendering such systems utterly unexplainable.\nFurthermore, and due to their stochastic nature, LLMs will often fail in making\nthe correct inferences in various linguistic contexts that require reasoning in\nintensional, temporal, or modal contexts. To remedy these shortcomings we\nsuggest employing the same successful bottom-up strategy employed in LLMs but\nin a symbolic setting, resulting in explainable, language-agnostic, and\nontologically grounded language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2308.14199, arXiv:2306.00017",
    "pdf_url": "http://arxiv.org/pdf/2406.06610v1",
    "published_date": "2024-06-06 20:38:35 UTC",
    "updated_date": "2024-06-06 20:38:35 UTC"
  },
  {
    "arxiv_id": "2406.04485v4",
    "title": "GenAI Arena: An Open Evaluation Platform for Generative Models",
    "authors": [
      "Dongfu Jiang",
      "Max Ku",
      "Tianle Li",
      "Yuansheng Ni",
      "Shizhuo Sun",
      "Rongqi Fan",
      "Wenhu Chen"
    ],
    "abstract": "Generative AI has made remarkable strides to revolutionize fields such as\nimage and video generation. These advancements are driven by innovative\nalgorithms, architecture, and data. However, the rapid proliferation of\ngenerative models has highlighted a critical gap: the absence of trustworthy\nevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc\noften fail to capture the nuanced quality and user satisfaction associated with\ngenerative outputs. This paper proposes an open platform GenAI-Arena to\nevaluate different image and video generative models, where users can actively\nparticipate in evaluating these models. By leveraging collective user feedback\nand votes, GenAI-Arena aims to provide a more democratic and accurate measure\nof model performance. It covers three tasks of text-to-image generation,\ntext-to-video generation, and image editing respectively. Currently, we cover a\ntotal of 35 open-source generative models. GenAI-Arena has been operating for\nseven months, amassing over 9000 votes from the community. We describe our\nplatform, analyze the data, and explain the statistical methods for ranking the\nmodels. To further promote the research in building model-based evaluation\nmetrics, we release a cleaned version of our preference data for the three\ntasks, namely GenAI-Bench. We prompt the existing multi-modal models like\nGemini, and GPT-4o to mimic human voting. We compute the accuracy by comparing\nthe model voting with the human voting to understand their judging abilities.\nOur results show existing multimodal models are still lagging in assessing the\ngenerated visual content, even the best model GPT-4o only achieves an average\naccuracy of 49.19 across the three generative tasks. Open-source MLLMs perform\neven worse due to the lack of instruction-following and reasoning ability in\ncomplex vision scenarios.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages,7 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.04485v4",
    "published_date": "2024-06-06 20:15:42 UTC",
    "updated_date": "2024-11-11 06:32:24 UTC"
  },
  {
    "arxiv_id": "2406.04482v1",
    "title": "Automatic Bug Detection in LLM-Powered Text-Based Games Using LLMs",
    "authors": [
      "Claire Jin",
      "Sudha Rao",
      "Xiangyu Peng",
      "Portia Botchway",
      "Jessica Quaye",
      "Chris Brockett",
      "Bill Dolan"
    ],
    "abstract": "Advancements in large language models (LLMs) are revolutionizing interactive\ngame design, enabling dynamic plotlines and interactions between players and\nnon-player characters (NPCs). However, LLMs may exhibit flaws such as\nhallucinations, forgetfulness, or misinterpretations of prompts, causing\nlogical inconsistencies and unexpected deviations from intended designs.\nAutomated techniques for detecting such game bugs are still lacking. To address\nthis, we propose a systematic LLM-based method for automatically identifying\nsuch bugs from player game logs, eliminating the need for collecting additional\ndata such as post-play surveys. Applied to a text-based game DejaBoom!, our\napproach effectively identifies bugs inherent in LLM-powered interactive games,\nsurpassing unstructured LLM-powered bug-catching methods and filling the gap in\nautomated detection of logical and design flaws.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in Findings of the Association for\n  Computational Linguistics: ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.04482v1",
    "published_date": "2024-06-06 20:11:08 UTC",
    "updated_date": "2024-06-06 20:11:08 UTC"
  },
  {
    "arxiv_id": "2406.04481v1",
    "title": "Optimizing Autonomous Driving for Safety: A Human-Centric Approach with LLM-Enhanced RLHF",
    "authors": [
      "Yuan Sun",
      "Navid Salami Pargoo",
      "Peter J. Jin",
      "Jorge Ortiz"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is popular in large\nlanguage models (LLMs), whereas traditional Reinforcement Learning (RL) often\nfalls short. Current autonomous driving methods typically utilize either human\nfeedback in machine learning, including RL, or LLMs. Most feedback guides the\ncar agent's learning process (e.g., controlling the car). RLHF is usually\napplied in the fine-tuning step, requiring direct human \"preferences,\" which\nare not commonly used in optimizing autonomous driving models. In this\nresearch, we innovatively combine RLHF and LLMs to enhance autonomous driving\nsafety. Training a model with human guidance from scratch is inefficient. Our\nframework starts with a pre-trained autonomous car agent model and implements\nmultiple human-controlled agents, such as cars and pedestrians, to simulate\nreal-life road environments. The autonomous car model is not directly\ncontrolled by humans. We integrate both physical and physiological feedback to\nfine-tune the model, optimizing this process using LLMs. This multi-agent\ninteractive environment ensures safe, realistic interactions before real-world\napplication. Finally, we will validate our model using data gathered from\nreal-life testbeds located in New Jersey and New York City.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04481v1",
    "published_date": "2024-06-06 20:10:34 UTC",
    "updated_date": "2024-06-06 20:10:34 UTC"
  },
  {
    "arxiv_id": "2406.15451v1",
    "title": "Deep Vision-Based Framework for Coastal Flood Prediction Under Climate Change Impacts and Shoreline Adaptations",
    "authors": [
      "Areg Karapetyan",
      "Aaron Chung Hin Chow",
      "Samer Madanat"
    ],
    "abstract": "In light of growing threats posed by climate change in general and sea level\nrise (SLR) in particular, the necessity for computationally efficient means to\nestimate and analyze potential coastal flood hazards has become increasingly\npressing. Data-driven supervised learning methods serve as promising candidates\nthat can dramatically expedite the process, thereby eliminating the\ncomputational bottleneck associated with traditional physics-based hydrodynamic\nsimulators. Yet, the development of accurate and reliable coastal flood\nprediction models, especially those based on Deep Learning (DL) techniques, has\nbeen plagued with two major issues: (1) the scarcity of training data and (2)\nthe high-dimensional output required for detailed inundation mapping. To remove\nthis barrier, we present a systematic framework for training high-fidelity Deep\nVision-based coastal flood prediction models in low-data settings. We test the\nproposed workflow on different existing vision models, including a fully\ntransformer-based architecture and a Convolutional Neural Network (CNN) with\nadditive attention gates. Additionally, we introduce a deep CNN architecture\ntailored specifically to the coastal flood prediction problem at hand. The\nmodel was designed with a particular focus on its compactness so as to cater to\nresource-constrained scenarios and accessibility aspects. The performance of\nthe developed DL models is validated against commonly adopted geostatistical\nregression methods and traditional Machine Learning (ML) approaches,\ndemonstrating substantial improvement in prediction quality. Lastly, we round\nup the contributions by providing a meticulously curated dataset of synthetic\nflood inundation maps of Abu Dhabi's coast produced with a physics-based\nhydrodynamic simulator, which can serve as a benchmark for evaluating future\ncoastal flood prediction models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15451v1",
    "published_date": "2024-06-06 19:54:34 UTC",
    "updated_date": "2024-06-06 19:54:34 UTC"
  },
  {
    "arxiv_id": "2406.04470v2",
    "title": "DiffuSyn Bench: Evaluating Vision-Language Models on Real-World Complexities with Diffusion-Generated Synthetic Benchmarks",
    "authors": [
      "Haokun Zhou",
      "Yipeng Hong"
    ],
    "abstract": "This study assesses the ability of Large Vision-Language Models (LVLMs) to\ndifferentiate between AI-generated and human-generated images. It introduces a\nnew automated benchmark construction method for this evaluation. The experiment\ncompared common LVLMs with human participants using a mixed dataset of AI and\nhuman-created images. Results showed that LVLMs could distinguish between the\nimage types to some extent but exhibited a rightward bias, and perform\nsignificantly worse compared to humans. To build on these findings, we\ndeveloped an automated benchmark construction process using AI. This process\ninvolved topic retrieval, narrative script generation, error embedding, and\nimage generation, creating a diverse set of text-image pairs with intentional\nerrors. We validated our method through constructing two caparable benchmarks.\nThis study highlights the strengths and weaknesses of LVLMs in real-world\nunderstanding and advances benchmark construction techniques, providing a\nscalable and automatic approach for AI model evaluation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04470v2",
    "published_date": "2024-06-06 19:50:33 UTC",
    "updated_date": "2024-06-13 16:46:22 UTC"
  },
  {
    "arxiv_id": "2406.04464v1",
    "title": "On The Importance of Reasoning for Context Retrieval in Repository-Level Code Editing",
    "authors": [
      "Alexander Kovrigin",
      "Aleksandra Eliseeva",
      "Yaroslav Zharov",
      "Timofey Bryksin"
    ],
    "abstract": "Recent advancements in code-fluent Large Language Models (LLMs) enabled the\nresearch on repository-level code editing. In such tasks, the model navigates\nand modifies the entire codebase of a project according to request. Hence, such\ntasks require efficient context retrieval, i.e., navigating vast codebases to\ngather relevant context. Despite the recognized importance of context\nretrieval, existing studies tend to approach repository-level coding tasks in\nan end-to-end manner, rendering the impact of individual components within\nthese complicated systems unclear. In this work, we decouple the task of\ncontext retrieval from the other components of the repository-level code\nediting pipelines. We lay the groundwork to define the strengths and weaknesses\nof this component and the role that reasoning plays in it by conducting\nexperiments that focus solely on context retrieval. We conclude that while the\nreasoning helps to improve the precision of the gathered context, it still\nlacks the ability to identify its sufficiency. We also outline the ultimate\nrole of the specialized tools in the process of context gathering. The code\nsupplementing this paper is available at\nhttps://github.com/JetBrains-Research/ai-agents-code-editing.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04464v1",
    "published_date": "2024-06-06 19:44:17 UTC",
    "updated_date": "2024-06-06 19:44:17 UTC"
  },
  {
    "arxiv_id": "2406.04456v1",
    "title": "Learning Optimal Linear Precoding for Cell-Free Massive MIMO with GNN",
    "authors": [
      "Benjamin Parlier",
      "Lou Salaün",
      "Hong Yang"
    ],
    "abstract": "We develop a graph neural network (GNN) to compute, within a time budget of 1\nto 2 milliseconds required by practical systems, the optimal linear precoder\n(OLP) maximizing the minimal downlink user data rate for a Cell-Free Massive\nMIMO system - a key 6G wireless technology. The state-of-the-art method is a\nbisection search on second order cone programming feasibility test (B-SOCP)\nwhich is a magnitude too slow for practical systems. Our approach relies on\nrepresenting OLP as a node-level prediction task on a graph. We construct a\ngraph that accurately captures the interdependence relation between access\npoints (APs) and user equipments (UEs), and the permutation equivariance of the\nMax-Min problem. Our neural network, named OLP-GNN, is trained on data obtained\nby B-SOCP. We tailor the OLP-GNN size, together with several artful data\npreprocessing and postprocessing methods to meet the runtime requirement. We\nshow by extensive simulations that it achieves near optimal spectral efficiency\nin a range of scenarios with different number of APs and UEs, and for both\nline-of-sight and non-line-of-sight radio propagation environments.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted in the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD) 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.04456v1",
    "published_date": "2024-06-06 19:29:33 UTC",
    "updated_date": "2024-06-06 19:29:33 UTC"
  },
  {
    "arxiv_id": "2406.04446v1",
    "title": "Can Language Models Use Forecasting Strategies?",
    "authors": [
      "Sarah Pratt",
      "Seth Blumberg",
      "Pietro Kreitlon Carolino",
      "Meredith Ringel Morris"
    ],
    "abstract": "Advances in deep learning systems have allowed large models to match or\nsurpass human accuracy on a number of skills such as image classification,\nbasic programming, and standardized test taking. As the performance of the most\ncapable models begin to saturate on tasks where humans already achieve high\naccuracy, it becomes necessary to benchmark models on increasingly complex\nabilities. One such task is forecasting the future outcome of events. In this\nwork we describe experiments using a novel dataset of real world events and\nassociated human predictions, an evaluation metric to measure forecasting\nability, and the accuracy of a number of different LLM based forecasting\ndesigns on the provided dataset. Additionally, we analyze the performance of\nthe LLM forecasters against human predictions and find that models still\nstruggle to make accurate predictions about the future. Our follow-up\nexperiments indicate this is likely due to models' tendency to guess that most\nevents are unlikely to occur (which tends to be true for many prediction\ndatasets, but does not reflect actual forecasting abilities). We reflect on\nnext steps for developing a systematic and reliable approach to studying LLM\nforecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04446v1",
    "published_date": "2024-06-06 19:01:42 UTC",
    "updated_date": "2024-06-06 19:01:42 UTC"
  },
  {
    "arxiv_id": "2406.06609v2",
    "title": "Mitigating Bias in Dataset Distillation",
    "authors": [
      "Justin Cui",
      "Ruochen Wang",
      "Yuanhao Xiong",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Dataset Distillation has emerged as a technique for compressing large\ndatasets into smaller synthetic counterparts, facilitating downstream training\ntasks. In this paper, we study the impact of bias inside the original dataset\non the performance of dataset distillation. With a comprehensive empirical\nevaluation on canonical datasets with color, corruption and background biases,\nwe found that color and background biases in the original dataset will be\namplified through the distillation process, resulting in a notable decline in\nthe performance of models trained on the distilled dataset, while corruption\nbias is suppressed through the distillation process. To reduce bias\namplification in dataset distillation, we introduce a simple yet highly\neffective approach based on a sample reweighting scheme utilizing kernel\ndensity estimation. Empirical results on multiple real-world and synthetic\ndatasets demonstrate the effectiveness of the proposed method. Notably, on\nCMNIST with 5% bias-conflict ratio and IPC 50, our method achieves 91.5% test\naccuracy compared to 23.8% from vanilla DM, boosting the performance by 67.7%,\nwhereas applying state-of-the-art debiasing method on the same dataset only\nachieves 53.7% accuracy. Our findings highlight the importance of addressing\nbiases in dataset distillation and provide a promising avenue to address bias\namplification in the process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML",
    "pdf_url": "http://arxiv.org/pdf/2406.06609v2",
    "published_date": "2024-06-06 18:52:28 UTC",
    "updated_date": "2024-07-10 17:58:14 UTC"
  },
  {
    "arxiv_id": "2406.04438v1",
    "title": "TexIm FAST: Text-to-Image Representation for Semantic Similarity Evaluation using Transformers",
    "authors": [
      "Wazib Ansar",
      "Saptarsi Goswami",
      "Amlan Chakrabarti"
    ],
    "abstract": "One of the principal objectives of Natural Language Processing (NLP) is to\ngenerate meaningful representations from text. Improving the informativeness of\nthe representations has led to a tremendous rise in the dimensionality and the\nmemory footprint. It leads to a cascading effect amplifying the complexity of\nthe downstream model by increasing its parameters. The available techniques\ncannot be applied to cross-modal applications such as text-to-image. To\nameliorate these issues, a novel Text-to-Image methodology for generating\nfixed-length representations through a self-supervised Variational Auto-Encoder\n(VAE) for semantic evaluation applying transformers (TexIm FAST) has been\nproposed in this paper. The pictorial representations allow oblivious inference\nwhile retaining the linguistic intricacies, and are potent in cross-modal\napplications. TexIm FAST deals with variable-length sequences and generates\nfixed-length representations with over 75% reduced memory footprint. It\nenhances the efficiency of the models for downstream tasks by reducing its\nparameters. The efficacy of TexIm FAST has been extensively analyzed for the\ntask of Semantic Textual Similarity (STS) upon the MSRPC, CNN/ Daily Mail, and\nXSum data-sets. The results demonstrate 6% improvement in accuracy compared to\nthe baseline and showcase its exceptional ability to compare disparate length\nsequences such as a text with its summary.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 33 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.04438v1",
    "published_date": "2024-06-06 18:28:50 UTC",
    "updated_date": "2024-06-06 18:28:50 UTC"
  },
  {
    "arxiv_id": "2406.04432v1",
    "title": "LipGER: Visually-Conditioned Generative Error Correction for Robust Automatic Speech Recognition",
    "authors": [
      "Sreyan Ghosh",
      "Sonal Kumar",
      "Ashish Seth",
      "Purva Chiniya",
      "Utkarsh Tyagi",
      "Ramani Duraiswami",
      "Dinesh Manocha"
    ],
    "abstract": "Visual cues, like lip motion, have been shown to improve the performance of\nAutomatic Speech Recognition (ASR) systems in noisy environments. We propose\nLipGER (Lip Motion aided Generative Error Correction), a novel framework for\nleveraging visual cues for noise-robust ASR. Instead of learning the\ncross-modal correlation between the audio and visual modalities, we make an LLM\nlearn the task of visually-conditioned (generative) ASR error correction.\nSpecifically, we instruct an LLM to predict the transcription from the N-best\nhypotheses generated using ASR beam-search. This is further conditioned on lip\nmotions. This approach addresses key challenges in traditional AVSR learning,\nsuch as the lack of large-scale paired datasets and difficulties in adapting to\nnew domains. We experiment on 4 datasets in various settings and show that\nLipGER improves the Word Error Rate in the range of 1.1%-49.2%. We also release\nLipHyp, a large-scale dataset with hypothesis-transcription pairs that is\nadditionally equipped with lip motion cues to promote further research in this\nspace",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "InterSpeech 2024. Code and Data: https://github.com/Sreyan88/LipGER",
    "pdf_url": "http://arxiv.org/pdf/2406.04432v1",
    "published_date": "2024-06-06 18:17:59 UTC",
    "updated_date": "2024-06-06 18:17:59 UTC"
  },
  {
    "arxiv_id": "2406.04428v1",
    "title": "MoralBench: Moral Evaluation of LLMs",
    "authors": [
      "Jianchao Ji",
      "Yutong Chen",
      "Mingyu Jin",
      "Wujiang Xu",
      "Wenyue Hua",
      "Yongfeng Zhang"
    ],
    "abstract": "In the rapidly evolving field of artificial intelligence, large language\nmodels (LLMs) have emerged as powerful tools for a myriad of applications, from\nnatural language processing to decision-making support systems. However, as\nthese models become increasingly integrated into societal frameworks, the\nimperative to ensure they operate within ethical and moral boundaries has never\nbeen more critical. This paper introduces a novel benchmark designed to measure\nand compare the moral reasoning capabilities of LLMs. We present the first\ncomprehensive dataset specifically curated to probe the moral dimensions of LLM\noutputs, addressing a wide range of ethical dilemmas and scenarios reflective\nof real-world complexities.\n  The main contribution of this work lies in the development of benchmark\ndatasets and metrics for assessing the moral identity of LLMs, which accounts\nfor nuance, contextual sensitivity, and alignment with human ethical standards.\nOur methodology involves a multi-faceted approach, combining quantitative\nanalysis with qualitative insights from ethics scholars to ensure a thorough\nevaluation of model performance. By applying our benchmark across several\nleading LLMs, we uncover significant variations in moral reasoning capabilities\nof different models. These findings highlight the importance of considering\nmoral reasoning in the development and evaluation of LLMs, as well as the need\nfor ongoing research to address the biases and limitations uncovered in our\nstudy. We publicly release the benchmark at\nhttps://drive.google.com/drive/u/0/folders/1k93YZJserYc2CkqP8d4B3M3sgd3kA8W7\nand also open-source the code of the project at\nhttps://github.com/agiresearch/MoralBench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04428v1",
    "published_date": "2024-06-06 18:15:01 UTC",
    "updated_date": "2024-06-06 18:15:01 UTC"
  },
  {
    "arxiv_id": "2406.04426v2",
    "title": "DeTra: A Unified Model for Object Detection and Trajectory Forecasting",
    "authors": [
      "Sergio Casas",
      "Ben Agro",
      "Jiageng Mao",
      "Thomas Gilles",
      "Alexander Cui",
      "Thomas Li",
      "Raquel Urtasun"
    ],
    "abstract": "The tasks of object detection and trajectory forecasting play a crucial role\nin understanding the scene for autonomous driving. These tasks are typically\nexecuted in a cascading manner, making them prone to compounding errors.\nFurthermore, there is usually a very thin interface between the two tasks,\ncreating a lossy information bottleneck. To address these challenges, our\napproach formulates the union of the two tasks as a trajectory refinement\nproblem, where the first pose is the detection (current time), and the\nsubsequent poses are the waypoints of the multiple forecasts (future time). To\ntackle this unified task, we design a refinement transformer that infers the\npresence, pose, and multi-modal future behaviors of objects directly from LiDAR\npoint clouds and high-definition maps. We call this model DeTra, short for\nobject Detection and Trajectory forecasting. In our experiments, we observe\nthat \\ourmodel{} outperforms the state-of-the-art on Argoverse 2 Sensor and\nWaymo Open Dataset by a large margin, across a broad range of metrics. Last but\nnot least, we perform extensive ablation studies that show the value of\nrefinement for this task, that every proposed component contributes positively\nto its performance, and that key design choices were made.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04426v2",
    "published_date": "2024-06-06 18:12:04 UTC",
    "updated_date": "2024-06-13 12:54:10 UTC"
  },
  {
    "arxiv_id": "2406.06608v6",
    "title": "The Prompt Report: A Systematic Survey of Prompt Engineering Techniques",
    "authors": [
      "Sander Schulhoff",
      "Michael Ilie",
      "Nishant Balepur",
      "Konstantine Kahadze",
      "Amanda Liu",
      "Chenglei Si",
      "Yinheng Li",
      "Aayush Gupta",
      "HyoJung Han",
      "Sevien Schulhoff",
      "Pranav Sandeep Dulepet",
      "Saurav Vidyadhara",
      "Dayeon Ki",
      "Sweta Agrawal",
      "Chau Pham",
      "Gerson Kroiz",
      "Feileen Li",
      "Hudson Tao",
      "Ashay Srivastava",
      "Hevander Da Costa",
      "Saloni Gupta",
      "Megan L. Rogers",
      "Inna Goncearenco",
      "Giuseppe Sarli",
      "Igor Galynker",
      "Denis Peskoff",
      "Marine Carpuat",
      "Jules White",
      "Shyamal Anadkat",
      "Alexander Hoyle",
      "Philip Resnik"
    ],
    "abstract": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06608v6",
    "published_date": "2024-06-06 18:10:11 UTC",
    "updated_date": "2025-02-26 18:59:01 UTC"
  },
  {
    "arxiv_id": "2406.04413v2",
    "title": "Efficient 3D-Aware Facial Image Editing via Attribute-Specific Prompt Learning",
    "authors": [
      "Amandeep Kumar",
      "Muhammad Awais",
      "Sanath Narayan",
      "Hisham Cholakkal",
      "Salman Khan",
      "Rao Muhammad Anwer"
    ],
    "abstract": "Drawing upon StyleGAN's expressivity and disentangled latent space, existing\n2D approaches employ textual prompting to edit facial images with different\nattributes. In contrast, 3D-aware approaches that generate faces at different\ntarget poses require attribute-specific classifiers, learning separate model\nweights for each attribute, and are not scalable for novel attributes. In this\nwork, we propose an efficient, plug-and-play, 3D-aware face editing framework\nbased on attribute-specific prompt learning, enabling the generation of facial\nimages with controllable attributes across various target poses. To this end,\nwe introduce a text-driven learnable style token-based latent attribute editor\n(LAE). The LAE harnesses a pre-trained vision-language model to find\ntext-guided attribute-specific editing direction in the latent space of any\npre-trained 3D-aware GAN. It utilizes learnable style tokens and style mappers\nto learn and transform this editing direction to 3D latent space. To train LAE\nwith multiple attributes, we use directional contrastive loss and style token\nloss. Furthermore, to ensure view consistency and identity preservation across\ndifferent poses and attributes, we employ several 3D-aware identity and pose\npreservation losses. Our experiments show that our proposed framework generates\nhigh-quality images with 3D awareness and view consistency while maintaining\nattribute-specific features. We demonstrate the effectiveness of our method on\ndifferent facial attributes, including hair color and style, expression, and\nothers.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV, 2024. Amandeep Kumar and Muhammad Awais are joint\n  first authors. More details are available at\n  https://awaisrauf.github.io/3d_face_editing",
    "pdf_url": "http://arxiv.org/pdf/2406.04413v2",
    "published_date": "2024-06-06 18:01:30 UTC",
    "updated_date": "2024-07-24 10:16:33 UTC"
  },
  {
    "arxiv_id": "2406.04412v2",
    "title": "Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment",
    "authors": [
      "Dongyoung Kim",
      "Kimin Lee",
      "Jinwoo Shin",
      "Jaehyung Kim"
    ],
    "abstract": "Aligning large language models (LLMs) with human preferences becomes a key\ncomponent to obtaining state-of-the-art performance, but it yields a huge cost\nto construct a large human-annotated preference dataset. To tackle this\nproblem, we propose a new framework, Spread Preference Annotation with direct\npreference judgment (SPA), that boosts the alignment of LLMs using only a very\nsmall amount of human-annotated preference data. Our key idea is leveraging the\nhuman prior knowledge within the small (seed) data and progressively improving\nthe alignment of LLM, by iteratively generating the responses and learning from\nthem with the self-annotated preference data. To be specific, we propose to\nderive the preference label from the logits of LLM to explicitly extract the\nmodel's inherent preference. Compared to the previous approaches using external\nreward models or implicit in-context learning, we observe that the proposed\napproach is significantly more effective. In addition, we introduce a\nnoise-aware preference learning algorithm to mitigate the risk of low quality\nwithin generated preference data. Our experimental results demonstrate that the\nproposed framework significantly boosts the alignment of LLMs. For example, we\nachieve superior alignment performance on AlpacaEval 2.0 with only 3.3% of the\nground-truth preference labels in the Ultrafeedback data compared to the cases\nusing the entire data or state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 Oral Presentation, 22 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.04412v2",
    "published_date": "2024-06-06 18:01:02 UTC",
    "updated_date": "2025-03-04 00:04:24 UTC"
  },
  {
    "arxiv_id": "2406.04338v3",
    "title": "Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion",
    "authors": [
      "Fangfu Liu",
      "Hanyang Wang",
      "Shunyu Yao",
      "Shengjun Zhang",
      "Jie Zhou",
      "Yueqi Duan"
    ],
    "abstract": "In recent years, there has been rapid development in 3D generation models,\nopening up new possibilities for applications such as simulating the dynamic\nmovements of 3D objects and customizing their behaviors. However, current 3D\ngenerative models tend to focus only on surface features such as color and\nshape, neglecting the inherent physical properties that govern the behavior of\nobjects in the real world. To accurately simulate physics-aligned dynamics, it\nis essential to predict the physical properties of materials and incorporate\nthem into the behavior prediction process. Nonetheless, predicting the diverse\nmaterials of real-world objects is still challenging due to the complex nature\nof their physical attributes. In this paper, we propose \\textbf{Physics3D}, a\nnovel method for learning various physical properties of 3D objects through a\nvideo diffusion model. Our approach involves designing a highly generalizable\nphysical simulation system based on a viscoelastic material model, which\nenables us to simulate a wide range of materials with high-fidelity\ncapabilities. Moreover, we distill the physical priors from a video diffusion\nmodel that contains more understanding of realistic object materials. Extensive\nexperiments demonstrate the effectiveness of our method with both elastic and\nplastic materials. Physics3D shows great potential for bridging the gap between\nthe physical world and virtual neural space, providing a better integration and\napplication of realistic physical principles in virtual environments. Project\npage: https://liuff19.github.io/Physics3D.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://liuff19.github.io/Physics3D",
    "pdf_url": "http://arxiv.org/pdf/2406.04338v3",
    "published_date": "2024-06-06 17:59:47 UTC",
    "updated_date": "2024-06-11 03:36:09 UTC"
  },
  {
    "arxiv_id": "2406.04337v2",
    "title": "Coherent Zero-Shot Visual Instruction Generation",
    "authors": [
      "Quynh Phung",
      "Songwei Ge",
      "Jia-Bin Huang"
    ],
    "abstract": "Despite the advances in text-to-image synthesis, particularly with diffusion\nmodels, generating visual instructions that require consistent representation\nand smooth state transitions of objects across sequential steps remains a\nformidable challenge. This paper introduces a simple, training-free framework\nto tackle the issues, capitalizing on the advancements in diffusion models and\nlarge language models (LLMs). Our approach systematically integrates text\ncomprehension and image generation to ensure visual instructions are visually\nappealing and maintain consistency and accuracy throughout the instruction\nsequence. We validate the effectiveness by testing multi-step instructions and\ncomparing the text alignment and consistency with several baselines. Our\nexperiments show that our approach can visualize coherent and visually pleasing\ninstructions",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "https://instruct-vis-zero.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.04337v2",
    "published_date": "2024-06-06 17:59:44 UTC",
    "updated_date": "2024-06-08 12:07:32 UTC"
  },
  {
    "arxiv_id": "2406.04331v2",
    "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
    "authors": [
      "Jinqi Luo",
      "Tianjiao Ding",
      "Kwan Ho Ryan Chan",
      "Darshan Thaker",
      "Aditya Chattopadhyay",
      "Chris Callison-Burch",
      "René Vidal"
    ],
    "abstract": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable outputs via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Given\nany alignment task, we instruct a concept partitioner to efficiently annotate\nthe concepts as benign or undesirable. Then, at inference time, we decompose\nthe LLM activations along the concept dictionary via sparse coding, to\naccurately represent the activations as linear combinations of benign and\nundesirable components. By removing the latter ones from the activations, we\nreorient the behavior of the LLM towards the alignment goal. We conduct\nexperiments on tasks such as response detoxification, faithfulness enhancement,\nand sentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in NeurIPS 2024. GitHub repository at\n  https://github.com/peterljq/Parsimonious-Concept-Engineering",
    "pdf_url": "http://arxiv.org/pdf/2406.04331v2",
    "published_date": "2024-06-06 17:59:10 UTC",
    "updated_date": "2024-11-05 15:43:18 UTC"
  },
  {
    "arxiv_id": "2406.04323v1",
    "title": "ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories",
    "authors": [
      "Qianlan Yang",
      "Yu-Xiong Wang"
    ],
    "abstract": "Training autonomous agents with sparse rewards is a long-standing problem in\nonline reinforcement learning (RL), due to low data efficiency. Prior work\novercomes this challenge by extracting useful knowledge from offline data,\noften accomplished through the learning of action distribution from offline\ndata and utilizing the learned distribution to facilitate online RL. However,\nsince the offline data are given and fixed, the extracted knowledge is\ninherently limited, making it difficult to generalize to new tasks. We propose\na novel approach that leverages offline data to learn a generative diffusion\nmodel, coined as Adaptive Trajectory Diffuser (ATraDiff). This model generates\nsynthetic trajectories, serving as a form of data augmentation and consequently\nenhancing the performance of online RL methods. The key strength of our\ndiffuser lies in its adaptability, allowing it to effectively handle varying\ntrajectory lengths and mitigate distribution shifts between online and offline\ndata. Because of its simplicity, ATraDiff seamlessly integrates with a wide\nspectrum of RL methods. Empirical evaluation shows that ATraDiff consistently\nachieves state-of-the-art performance across a variety of environments, with\nparticularly pronounced improvements in complicated settings. Our code and demo\nvideo are available at https://atradiff.github.io .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2406.04323v1",
    "published_date": "2024-06-06 17:58:15 UTC",
    "updated_date": "2024-06-06 17:58:15 UTC"
  },
  {
    "arxiv_id": "2406.04320v1",
    "title": "Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models",
    "authors": [
      "Ali Behrouz",
      "Michele Santacatterina",
      "Ramin Zabih"
    ],
    "abstract": "Modeling multivariate time series is a well-established problem with a wide\nrange of applications from healthcare to financial markets. Traditional State\nSpace Models (SSMs) are classical approaches for univariate time series\nmodeling due to their simplicity and expressive power to represent linear\ndependencies. They, however, have fundamentally limited expressive power to\ncapture non-linear dependencies, are slow in practice, and fail to model the\ninter-variate information flow. Despite recent attempts to improve the\nexpressive power of SSMs by using deep structured SSMs, the existing methods\nare either limited to univariate time series, fail to model complex patterns\n(e.g., seasonal patterns), fail to dynamically model the dependencies of\nvariate and time dimensions, and/or are input-independent. We present Chimera\nthat uses two input-dependent 2-D SSM heads with different discretization\nprocesses to learn long-term progression and seasonal patterns. To improve the\nefficiency of complex 2D recurrence, we present a fast training using a new\n2-dimensional parallel selective scan. We further present and discuss\n2-dimensional Mamba and Mamba-2 as the spacial cases of our 2D SSM. Our\nexperimental evaluation shows the superior performance of Chimera on extensive\nand diverse benchmarks, including ECG and speech time series classification,\nlong-term and short-term time series forecasting, and time series anomaly\ndetection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04320v1",
    "published_date": "2024-06-06 17:58:09 UTC",
    "updated_date": "2024-06-06 17:58:09 UTC"
  },
  {
    "arxiv_id": "2406.04318v1",
    "title": "Adaptive Sampling of k-Space in Magnetic Resonance for Rapid Pathology Prediction",
    "authors": [
      "Chen-Yu Yen",
      "Raghav Singhal",
      "Umang Sharma",
      "Rajesh Ranganath",
      "Sumit Chopra",
      "Lerrel Pinto"
    ],
    "abstract": "Magnetic Resonance (MR) imaging, despite its proven diagnostic utility,\nremains an inaccessible imaging modality for disease surveillance at the\npopulation level. A major factor rendering MR inaccessible is lengthy scan\ntimes. An MR scanner collects measurements associated with the underlying\nanatomy in the Fourier space, also known as the k-space. Creating a\nhigh-fidelity image requires collecting large quantities of such measurements,\nincreasing the scan time. Traditionally to accelerate an MR scan, image\nreconstruction from under-sampled k-space data is the method of choice.\nHowever, recent works show the feasibility of bypassing image reconstruction\nand directly learning to detect disease directly from a sparser learned subset\nof the k-space measurements. In this work, we propose Adaptive Sampling for MR\n(ASMR), a sampling method that learns an adaptive policy to sequentially select\nk-space samples to optimize for target disease detection. On 6 out of 8\npathology classification tasks spanning the Knee, Brain, and Prostate MR scans,\nASMR reaches within 2% of the performance of a fully sampled classifier while\nusing only 8% of the k-space, as well as outperforming prior state-of-the-art\nwork in k-space sampling such as EMRT, LOUPE, and DPS.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024. Project website at https://adaptive-sampling-mr.github.io",
    "pdf_url": "http://arxiv.org/pdf/2406.04318v1",
    "published_date": "2024-06-06 17:58:00 UTC",
    "updated_date": "2024-06-06 17:58:00 UTC"
  },
  {
    "arxiv_id": "2406.04313v4",
    "title": "Improving Alignment and Robustness with Circuit Breakers",
    "authors": [
      "Andy Zou",
      "Long Phan",
      "Justin Wang",
      "Derek Duenas",
      "Maxwell Lin",
      "Maksym Andriushchenko",
      "Rowan Wang",
      "Zico Kolter",
      "Matt Fredrikson",
      "Dan Hendrycks"
    ],
    "abstract": "AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Code and models are available at\n  https://github.com/GraySwanAI/circuit-breakers",
    "pdf_url": "http://arxiv.org/pdf/2406.04313v4",
    "published_date": "2024-06-06 17:57:04 UTC",
    "updated_date": "2024-07-12 16:51:07 UTC"
  },
  {
    "arxiv_id": "2406.04306v1",
    "title": "Semantically Diverse Language Generation for Uncertainty Estimation in Language Models",
    "authors": [
      "Lukas Aichberger",
      "Kajetan Schweighofer",
      "Mykyta Ielanskyi",
      "Sepp Hochreiter"
    ],
    "abstract": "Large language models (LLMs) can suffer from hallucinations when generating\ntext. These hallucinations impede various applications in society and industry\nby making LLMs untrustworthy. Current LLMs generate text in an autoregressive\nfashion by predicting and appending text tokens. When an LLM is uncertain about\nthe semantic meaning of the next tokens to generate, it is likely to start\nhallucinating. Thus, it has been suggested that hallucinations stem from\npredictive uncertainty. We introduce Semantically Diverse Language Generation\n(SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM to\ngenerate semantically diverse yet likely alternatives for an initially\ngenerated text. This approach provides a precise measure of aleatoric semantic\nuncertainty, detecting whether the initial text is likely to be hallucinated.\nExperiments on question-answering tasks demonstrate that SDLG consistently\noutperforms existing methods while being the most computationally efficient,\nsetting a new standard for uncertainty estimation in LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04306v1",
    "published_date": "2024-06-06 17:53:34 UTC",
    "updated_date": "2024-06-06 17:53:34 UTC"
  },
  {
    "arxiv_id": "2406.04303v3",
    "title": "Vision-LSTM: xLSTM as Generic Vision Backbone",
    "authors": [
      "Benedikt Alkin",
      "Maximilian Beck",
      "Korbinian Pöppel",
      "Sepp Hochreiter",
      "Johannes Brandstetter"
    ],
    "abstract": "Transformers are widely used as generic backbones in computer vision, despite\ninitially introduced for natural language processing. Recently, the Long\nShort-Term Memory (LSTM) has been extended to a scalable and performant\narchitecture - the xLSTM - which overcomes long-standing LSTM limitations via\nexponential gating and parallelizable matrix memory structure. In this report,\nwe introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to\ncomputer vision. ViL comprises a stack of xLSTM blocks where odd blocks process\nthe sequence of patch tokens from top to bottom while even blocks go from\nbottom to top. Experiments show that ViL holds promise to be further deployed\nas new generic backbone for computer vision architectures.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published as a conference paper at ICLR 2025, Github:\n  https://github.com/NX-AI/vision-lstm",
    "pdf_url": "http://arxiv.org/pdf/2406.04303v3",
    "published_date": "2024-06-06 17:49:21 UTC",
    "updated_date": "2025-02-20 23:20:40 UTC"
  },
  {
    "arxiv_id": "2406.04391v2",
    "title": "Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?",
    "authors": [
      "Rylan Schaeffer",
      "Hailey Schoelkopf",
      "Brando Miranda",
      "Gabriel Mukobi",
      "Varun Madan",
      "Adam Ibrahim",
      "Herbie Bradley",
      "Stella Biderman",
      "Sanmi Koyejo"
    ],
    "abstract": "Predicting changes from scaling advanced AI systems is a desirable property\nfor engineers, economists, governments and industry alike, and, while a\nwell-established literature exists on how pretraining performance scales,\npredictable scaling behavior on downstream capabilities remains elusive. While\nmany factors are certainly responsible, this paper identifies a significant\nfactor that makes predicting scaling behavior on widely used multiple-choice\nquestion answering benchmarks challenging and illuminates a path towards making\nsuch downstream evaluations predictable with scale. Using five model families\nand twelve well-established multiple-choice benchmarks, we demonstrate that\ndownstream performance is computed from negative log likelihoods via a sequence\nof transformations that progressively degrades the statistical relationship\nbetween performance and scale. We then pinpoint the mechanism causing this\ndegradation: downstream metrics require comparing the correct choice against a\nsmall number of specific incorrect choices, meaning accurately predicting\ndownstream capabilities requires predicting not just how probability mass\nconcentrates on the correct choice with scale, but also how probability mass\nfluctuates on the alternative incorrect choices with scale. We empirically\nstudy how probability mass on the correct choice co-varies with probability\nmass on incorrect choices with increasing compute, suggesting that scaling laws\nfor \\textit{incorrect} choices might be achievable. Our work also explains why\npretraining scaling laws are commonly regarded as more predictable than\ndownstream capabilities and contributes towards establishing\nscaling-predictable evaluations of frontier AI models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04391v2",
    "published_date": "2024-06-06 17:46:56 UTC",
    "updated_date": "2025-02-05 17:44:38 UTC"
  },
  {
    "arxiv_id": "2407.06170v1",
    "title": "Real-Time Spacecraft Pose Estimation Using Mixed-Precision Quantized Neural Network on COTS Reconfigurable MPSoC",
    "authors": [
      "Julien Posso",
      "Guy Bois",
      "Yvon Savaria"
    ],
    "abstract": "This article presents a pioneering approach to real-time spacecraft pose\nestimation, utilizing a mixed-precision quantized neural network implemented on\nthe FPGA components of a commercially available Xilinx MPSoC, renowned for its\nsuitability in space applications. Our co-design methodology includes a novel\nevaluation technique for assessing the layer-wise neural network sensitivity to\nquantization, facilitating an optimal balance between accuracy, latency, and\nFPGA resource utilization. Utilizing the FINN library, we developed a bespoke\nFPGA dataflow accelerator that integrates on-chip weights and activation\nfunctions to minimize latency and energy consumption. Our implementation is 7.7\ntimes faster and 19.5 times more energy-efficient than the best-reported values\nin the existing spacecraft pose estimation literature. Furthermore, our\ncontribution includes the first real-time, open-source implementation of such\nalgorithms, marking a significant advancement in making efficient spacecraft\npose estimation algorithms widely accessible. The source code is available at\nhttps://github.com/possoj/FPGA-SpacePose.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IEEE NEWCAS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.06170v1",
    "published_date": "2024-06-06 17:36:26 UTC",
    "updated_date": "2024-06-06 17:36:26 UTC"
  },
  {
    "arxiv_id": "2406.04286v1",
    "title": "ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions",
    "authors": [
      "Sreyan Ghosh",
      "Utkarsh Tyagi",
      "Sonal Kumar",
      "C. K. Evuru",
      "S Ramaneswaran",
      "S Sakshi",
      "Dinesh Manocha"
    ],
    "abstract": "We present ABEX, a novel and effective generative data augmentation\nmethodology for low-resource Natural Language Understanding (NLU) tasks. ABEX\nis based on ABstract-and-EXpand, a novel paradigm for generating diverse forms\nof an input document -- we first convert a document into its concise, abstract\ndescription and then generate new documents based on expanding the resultant\nabstraction. To learn the task of expanding abstract descriptions, we first\ntrain BART on a large-scale synthetic dataset with abstract-document pairs.\nNext, to generate abstract descriptions for a document, we propose a simple,\ncontrollable, and training-free method based on editing AMR graphs. ABEX brings\nthe best of both worlds: by expanding from abstract representations, it\npreserves the original semantic properties of the documents, like style and\nmeaning, thereby maintaining alignment with the original label and data\ndistribution. At the same time, the fundamental process of elaborating on\nabstract descriptions facilitates diverse generations. We demonstrate the\neffectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource\nsettings. ABEX outperforms all our baselines qualitatively with improvements of\n0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods from\nliterature in terms of context and length diversity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Main Conference. Code and data:\n  https://github.com/Sreyan88/ABEX",
    "pdf_url": "http://arxiv.org/pdf/2406.04286v1",
    "published_date": "2024-06-06 17:29:57 UTC",
    "updated_date": "2024-06-06 17:29:57 UTC"
  },
  {
    "arxiv_id": "2406.04276v1",
    "title": "Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks",
    "authors": [
      "Han Zhang",
      "Akram Bin Sediq",
      "Ali Afana",
      "Melike Erol-Kantarci"
    ],
    "abstract": "In recent years, machine learning (ML) techniques have created numerous\nopportunities for intelligent mobile networks and have accelerated the\nautomation of network operations. However, complex network tasks may involve\nvariables and considerations even beyond the capacity of traditional ML\nalgorithms. On the other hand, large language models (LLMs) have recently\nemerged, demonstrating near-human-level performance in cognitive tasks across\nvarious fields. However, they remain prone to hallucinations and often lack\ncommon sense in basic tasks. Therefore, they are regarded as assistive tools\nfor humans. In this work, we propose the concept of \"generative AI-in-the-loop\"\nand utilize the semantic understanding, context awareness, and reasoning\nabilities of LLMs to assist humans in handling complex or unforeseen situations\nin mobile communication networks. We believe that combining LLMs and ML models\nallows both to leverage their respective capabilities and achieve better\nresults than either model alone. To support this idea, we begin by analyzing\nthe capabilities of LLMs and compare them with traditional ML algorithms. We\nthen explore potential LLM-based applications in line with the requirements of\nnext-generation networks. We further examine the integration of ML and LLMs,\ndiscussing how they can be used together in mobile networks. Unlike existing\nstudies, our research emphasizes the fusion of LLMs with traditional ML-driven\nnext-generation networks and serves as a comprehensive refinement of existing\nsurveys. Finally, we provide a case study to enhance ML-based network intrusion\ndetection with synthesized data generated by LLMs. Our case study further\ndemonstrates the advantages of our proposed idea.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04276v1",
    "published_date": "2024-06-06 17:25:07 UTC",
    "updated_date": "2024-06-06 17:25:07 UTC"
  },
  {
    "arxiv_id": "2406.04274v1",
    "title": "Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models",
    "authors": [
      "Xiang Ji",
      "Sanjeev Kulkarni",
      "Mengdi Wang",
      "Tengyang Xie"
    ],
    "abstract": "This work studies the challenge of aligning large language models (LLMs) with\noffline preference data. We focus on alignment by Reinforcement Learning from\nHuman Feedback (RLHF) in particular. While popular preference optimization\nmethods exhibit good empirical performance in practice, they are not\ntheoretically guaranteed to converge to the optimal policy and can provably\nfail when the data coverage is sparse by classical offline reinforcement\nlearning (RL) results. On the other hand, a recent line of work has focused on\ntheoretically motivated preference optimization methods with provable\nguarantees, but these are not computationally efficient for large-scale\napplications like LLM alignment. To bridge this gap, we propose SPAC, a new\noffline preference optimization method with self-play, inspired by the\non-average pessimism technique from the offline RL literature, to be the first\nprovable and scalable approach to LLM alignment. We both provide theoretical\nanalysis for its convergence under single-policy concentrability for the\ngeneral function approximation setting and demonstrate its competitive\nempirical performance for LLM alignment on a 7B Mistral model with Open LLM\nLeaderboard evaluations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04274v1",
    "published_date": "2024-06-06 17:23:49 UTC",
    "updated_date": "2024-06-06 17:23:49 UTC"
  },
  {
    "arxiv_id": "2406.04273v2",
    "title": "ELFS: Label-Free Coreset Selection with Proxy Training Dynamics",
    "authors": [
      "Haizhong Zheng",
      "Elisa Tsai",
      "Yifu Lu",
      "Jiachen Sun",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Atul Prakash"
    ],
    "abstract": "High-quality human-annotated data is crucial for modern deep learning\npipelines, yet the human annotation process is both costly and time-consuming.\nGiven a constrained human labeling budget, selecting an informative and\nrepresentative data subset for labeling can significantly reduce human\nannotation effort. Well-performing state-of-the-art (SOTA) coreset selection\nmethods require ground truth labels over the whole dataset, failing to reduce\nthe human labeling burden. Meanwhile, SOTA label-free coreset selection methods\ndeliver inferior performance due to poor geometry-based difficulty scores. In\nthis paper, we introduce ELFS (Effective Label-Free Coreset Selection), a novel\nlabel-free coreset selection method. ELFS significantly improves label-free\ncoreset selection by addressing two challenges: 1) ELFS utilizes deep\nclustering to estimate training dynamics-based data difficulty scores without\nground truth labels; 2) Pseudo-labels introduce a distribution shift in the\ndata difficulty scores, and we propose a simple but effective double-end\npruning method to mitigate bias on calculated scores. We evaluate ELFS on four\nvision benchmarks and show that, given the same vision encoder, ELFS\nconsistently outperforms SOTA label-free baselines. For instance, when using\nSwAV as the encoder, ELFS outperforms D2 by up to 10.2% in accuracy on\nImageNet-1K. We make our code publicly available on GitHub.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.04273v2",
    "published_date": "2024-06-06 17:23:05 UTC",
    "updated_date": "2025-02-24 14:56:11 UTC"
  },
  {
    "arxiv_id": "2406.04268v1",
    "title": "Open-Endedness is Essential for Artificial Superhuman Intelligence",
    "authors": [
      "Edward Hughes",
      "Michael Dennis",
      "Jack Parker-Holder",
      "Feryal Behbahani",
      "Aditi Mavalankar",
      "Yuge Shi",
      "Tom Schaul",
      "Tim Rocktaschel"
    ],
    "abstract": "In recent years there has been a tremendous surge in the general capabilities\nof AI systems, mainly fuelled by training foundation models on internetscale\ndata. Nevertheless, the creation of openended, ever self-improving AI remains\nelusive. In this position paper, we argue that the ingredients are now in place\nto achieve openendedness in AI systems with respect to a human observer.\nFurthermore, we claim that such open-endedness is an essential property of any\nartificial superhuman intelligence (ASI). We begin by providing a concrete\nformal definition of open-endedness through the lens of novelty and\nlearnability. We then illustrate a path towards ASI via open-ended systems\nbuilt on top of foundation models, capable of making novel, humanrelevant\ndiscoveries. We conclude by examining the safety implications of\ngenerally-capable openended AI. We expect that open-ended foundation models\nwill prove to be an increasingly fertile and safety-critical area of research\nin the near future.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04268v1",
    "published_date": "2024-06-06 17:15:02 UTC",
    "updated_date": "2024-06-06 17:15:02 UTC"
  },
  {
    "arxiv_id": "2406.04264v3",
    "title": "MLVU: Benchmarking Multi-task Long Video Understanding",
    "authors": [
      "Junjie Zhou",
      "Yan Shu",
      "Bo Zhao",
      "Boya Wu",
      "Zhengyang Liang",
      "Shitao Xiao",
      "Minghao Qin",
      "Xi Yang",
      "Yongping Xiong",
      "Bo Zhang",
      "Tiejun Huang",
      "Zheng Liu"
    ],
    "abstract": "The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04264v3",
    "published_date": "2024-06-06 17:09:32 UTC",
    "updated_date": "2025-01-01 15:53:58 UTC"
  },
  {
    "arxiv_id": "2407.11991v1",
    "title": "Inspired by AI? A Novel Generative AI System To Assist Conceptual Automotive Design",
    "authors": [
      "Ye Wang",
      "Nicole B. Damen",
      "Thomas Gale",
      "Voho Seo",
      "Hooman Shayani"
    ],
    "abstract": "Design inspiration is crucial for establishing the direction of a design as\nwell as evoking feelings and conveying meanings during the conceptual design\nprocess. Many practice designers use text-based searches on platforms like\nPinterest to gather image ideas, followed by sketching on paper or using\ndigital tools to develop concepts. Emerging generative AI techniques, such as\ndiffusion models, offer a promising avenue to streamline these processes by\nswiftly generating design concepts based on text and image inspiration inputs,\nsubsequently using the AI generated design concepts as fresh sources of\ninspiration for further concept development. However, applying these generative\nAI techniques directly within a design context has challenges. Firstly,\ngenerative AI tools may exhibit a bias towards particular styles, resulting in\na lack of diversity of design outputs. Secondly, these tools may struggle to\ngrasp the nuanced meanings of texts or images in a design context. Lastly, the\nlack of integration with established design processes within design teams can\nresult in fragmented use scenarios. Focusing on these challenges, we conducted\nworkshops, surveys, and data augmentation involving teams of experienced\nautomotive designers to investigate their current practices in generating\nconcepts inspired by texts and images, as well as their preferred interaction\nmodes for generative AI systems to support the concept generation workflow.\nFinally, we developed a novel generative AI system based on diffusion models to\nassist conceptual automotive design.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11991v1",
    "published_date": "2024-06-06 17:04:14 UTC",
    "updated_date": "2024-06-06 17:04:14 UTC"
  },
  {
    "arxiv_id": "2406.04254v3",
    "title": "GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions",
    "authors": [
      "Salvatore Esposito",
      "Qingshan Xu",
      "Kacper Kania",
      "Charlie Hewitt",
      "Octave Mariotti",
      "Lohit Petikam",
      "Julien Valentin",
      "Arno Onken",
      "Oisin Mac Aodha"
    ],
    "abstract": "We introduce a new generative approach for synthesizing 3D geometry and\nimages from single-view collections. Most existing approaches predict\nvolumetric density to render multi-view consistent images. By employing\nvolumetric rendering using neural radiance fields, they inherit a key\nlimitation: the generated geometry is noisy and unconstrained, limiting the\nquality and utility of the output meshes. To address this issue, we propose\nGeoGen, a new SDF-based 3D generative model trained in an end-to-end manner.\nInitially, we reinterpret the volumetric density as a Signed Distance Function\n(SDF). This allows us to introduce useful priors to generate valid meshes.\nHowever, those priors prevent the generative model from learning details,\nlimiting the applicability of the method to real-world scenarios. To alleviate\nthat problem, we make the transformation learnable and constrain the rendered\ndepth map to be consistent with the zero-level set of the SDF. Through the lens\nof adversarial training, we encourage the network to produce higher fidelity\ndetails on the output meshes. For evaluation, we introduce a synthetic dataset\nof human avatars captured from 360-degree camera angles, to overcome the\nchallenges presented by real-world datasets, which often lack 3D consistency\nand do not cover all camera angles. Our experiments on multiple datasets show\nthat GeoGen produces visually and quantitatively better geometry than the\nprevious generative models based on neural radiance fields.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04254v3",
    "published_date": "2024-06-06 17:00:10 UTC",
    "updated_date": "2024-06-14 12:58:05 UTC"
  },
  {
    "arxiv_id": "2406.04233v2",
    "title": "FairytaleQA Translated: Enabling Educational Question and Answer Generation in Less-Resourced Languages",
    "authors": [
      "Bernardo Leite",
      "Tomás Freitas Osório",
      "Henrique Lopes Cardoso"
    ],
    "abstract": "Question Answering (QA) datasets are crucial in assessing reading\ncomprehension skills for both machines and humans. While numerous datasets have\nbeen developed in English for this purpose, a noticeable void exists in\nless-resourced languages. To alleviate this gap, our paper introduces\nmachine-translated versions of FairytaleQA, a renowned QA dataset designed to\nassess and enhance narrative comprehension skills in young children. By\nemploying fine-tuned, modest-scale models, we establish benchmarks for both\nQuestion Generation (QG) and QA tasks within the translated datasets. In\naddition, we present a case study proposing a model for generating\nquestion-answer pairs, with an evaluation incorporating quality metrics such as\nquestion well-formedness, answerability, relevance, and children suitability.\nOur evaluation prioritizes quantifying and describing error cases, along with\nproviding directions for future work. This paper contributes to the advancement\nof QA and QG research in less-resourced languages, promoting accessibility and\ninclusivity in the development of these models for reading comprehension. The\ncode and data is publicly available at\ngithub.com/bernardoleite/fairytaleqa-translated.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint - Accepted for publication at ECTEL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.04233v2",
    "published_date": "2024-06-06 16:31:47 UTC",
    "updated_date": "2024-06-24 15:39:17 UTC"
  },
  {
    "arxiv_id": "2406.04231v3",
    "title": "Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment",
    "authors": [
      "Aidan Kierans",
      "Avijit Ghosh",
      "Hananel Hazan",
      "Shiri Dori-Hacohen"
    ],
    "abstract": "Existing work on the alignment problem has focused mainly on (1) qualitative\ndescriptions of the alignment problem; (2) attempting to align AI actions with\nhuman interests by focusing on value specification and learning; and/or (3)\nfocusing on a single agent or on humanity as a monolith. Recent sociotechnical\napproaches highlight the need to understand complex misalignment among multiple\nhuman and AI agents. We address this gap by adapting a computational social\nscience model of human contention to the alignment problem. Our model\nquantifies misalignment in large, diverse agent groups with potentially\nconflicting goals across various problem areas. Misalignment scores in our\nframework depend on the observed agent population, the domain in question, and\nconflict between agents' weighted preferences. Through simulations, we\ndemonstrate how our model captures intuitive aspects of misalignment across\ndifferent scenarios. We then apply our model to two case studies, including an\nautonomous vehicle setting, showcasing its practical utility. Our approach\noffers enhanced explanatory power for complex sociotechnical environments and\ncould inform the design of more aligned AI systems in real-world applications.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY",
      "cs.GT",
      "I.2.11; K.4.m"
    ],
    "primary_category": "cs.MA",
    "comment": "7 pages, 8 figures, 3 tables, forthcoming at the AAAI-25 Special\n  Track on AI Alignment",
    "pdf_url": "http://arxiv.org/pdf/2406.04231v3",
    "published_date": "2024-06-06 16:31:22 UTC",
    "updated_date": "2024-12-16 19:55:37 UTC"
  },
  {
    "arxiv_id": "2406.04230v2",
    "title": "M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and Multispectral Data",
    "authors": [
      "Matthew J Allen",
      "Francisco Dorr",
      "Joseph Alejandro Gallego Mejia",
      "Laura Martínez-Ferrer",
      "Anna Jungbluth",
      "Freddie Kalaitzis",
      "Raúl Ramos-Pollán"
    ],
    "abstract": "Satellite-based remote sensing has revolutionised the way we address global\nchallenges. Huge quantities of Earth Observation (EO) data are generated by\nsatellite sensors daily, but processing these large datasets for use in ML\npipelines is technically and computationally challenging. While some\npreprocessed Earth observation datasets exist, their content is often limited\nto optical or near-optical wavelength data, which is ineffective at night or in\nadverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing\ntechnique based on microwave length radiation, offers a viable alternative.\nHowever, the application of machine learning to SAR has been limited due to a\nlack of ML-ready data and pipelines, particularly for the full diversity of SAR\ndata, including polarimetry, coherence and interferometry. In this work, we\nintroduce M3LEO, a multi-modal, multi-label Earth observation dataset that\nincludes polarimetric, interferometric, and coherence SAR data derived from\nSentinel-1, alongside multispectral Sentinel-2 imagery and auxiliary data\ndescribing terrain properties such as land use. M3LEO spans approximately 17M\n4x4 km data chips from six diverse geographic regions. The dataset is\ncomplemented by a flexible PyTorch Lightning framework configured using Hydra\nto accommodate its use across diverse ML applications in Earth observation. We\nprovide tools to process any dataset available on popular platforms such as\nGoogle Earth Engine for seamless integration with our framework. We show that\nthe distribution shift in self-supervised embeddings is substantial across\ngeographic regions, even when controlling for terrain properties. Data:\nhuggingface.co/M3LEO, Code: github.com/spaceml-org/M3LEO.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4; I.4.6; I.4.8; I.4.9; I.5; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.04230v2",
    "published_date": "2024-06-06 16:30:41 UTC",
    "updated_date": "2024-10-31 13:18:14 UTC"
  },
  {
    "arxiv_id": "2406.04229v1",
    "title": "The CLRS-Text Algorithmic Reasoning Language Benchmark",
    "authors": [
      "Larisa Markeeva",
      "Sean McLeish",
      "Borja Ibarz",
      "Wilfried Bounsi",
      "Olga Kozlova",
      "Alex Vitvitskyi",
      "Charles Blundell",
      "Tom Goldstein",
      "Avi Schwarzschild",
      "Petar Veličković"
    ],
    "abstract": "Eliciting reasoning capabilities from language models (LMs) is a critical\ndirection on the path towards building intelligent systems. Most recent studies\ndedicated to reasoning focus on out-of-distribution performance on\nprocedurally-generated synthetic benchmarks, bespoke-built to evaluate specific\nskills only. This trend makes results hard to transfer across publications,\nslowing down progress. Three years ago, a similar issue was identified and\nrectified in the field of neural algorithmic reasoning, with the advent of the\nCLRS benchmark. CLRS is a dataset generator comprising graph execution traces\nof classical algorithms from the Introduction to Algorithms textbook. Inspired\nby this, we propose CLRS-Text -- a textual version of these algorithmic traces.\nOut of the box, CLRS-Text is capable of procedurally generating trace data for\nthirty diverse, challenging algorithmic tasks across any desirable input\ndistribution, while offering a standard pipeline in which any additional\nalgorithmic tasks may be created in the benchmark. We fine-tune and evaluate\nvarious LMs as generalist executors on this benchmark, validating prior work\nand revealing a novel, interesting challenge for the LM reasoning community.\nOur code is available at\nhttps://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint, under review. Comments welcome",
    "pdf_url": "http://arxiv.org/pdf/2406.04229v1",
    "published_date": "2024-06-06 16:29:25 UTC",
    "updated_date": "2024-06-06 16:29:25 UTC"
  },
  {
    "arxiv_id": "2406.04220v4",
    "title": "BEADs: Bias Evaluation Across Domains",
    "authors": [
      "Shaina Raza",
      "Mizanur Rahman",
      "Michael R. Zhang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have greatly enhanced\nnatural language processing (NLP) applications. Nevertheless, these models\noften inherit biases from their training data. Despite the availability of\nvarious datasets for bias detection, most are limited to one or two NLP tasks\n(typically classification or evaluation) and lack comprehensive evaluations\nacross a broader range of NLP tasks. To address this gap, we introduce the Bias\nEvaluations Across Domains BEADs dataset, designed to support a wide array of\nNLP tasks, including text classification, token classification, bias\nquantification, and benign language generation. A key focus of this paper is\nthe gold label dataset that is annotated by GPT4 for scalabilty and verified by\nexperts to ensure high reliability. BEADs provides data for both fine-tuning,\nincluding classification and language generation tasks, and for evaluating\nLLMs. Our findings indicate that BEADs effectively identifies numerous biases\nwhen fine-tuned on this dataset. It also reduces biases when used for\nfine-tuning language generation task, while preserving language quality. The\nresults also reveal some prevalent demographic biases in LLMs when BEADs is\nused for evaluation in demographic task. We provide the BEADs dataset for\ndetecting biases in various domains, and this dataset is readily usable for\nresponsible AI development and application. The dataset can be accessed at\nhttps://huggingface.co/datasets/shainar/BEAD .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2406.04220v4",
    "published_date": "2024-06-06 16:18:30 UTC",
    "updated_date": "2024-12-24 15:08:40 UTC"
  },
  {
    "arxiv_id": "2406.04215v1",
    "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
    "authors": [
      "Yusuke Sakai",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ],
    "abstract": "It is very challenging to curate a dataset for language-specific knowledge\nand common sense in order to evaluate natural language understanding\ncapabilities of language models. Due to the limitation in the availability of\nannotators, most current multilingual datasets are created through translation,\nwhich cannot evaluate such language-specific aspects. Therefore, we propose\nMultilingual CommonsenseQA (mCSQA) based on the construction process of CSQA\nbut leveraging language models for a more efficient construction, e.g., by\nasking LM to generate questions/answers, refine answers and verify QAs followed\nby reduced human efforts for verification. Constructed dataset is a benchmark\nfor cross-lingual language-transfer capabilities of multilingual LMs, and\nexperimental results showed high language-transfer capabilities for questions\nthat LMs could easily solve, but lower transfer capabilities for questions\nrequiring deep knowledge or commonsense. This highlights the necessity of\nlanguage-specific datasets for evaluation and training. Finally, our method\ndemonstrated that multilingual LMs could create QA including language-specific\nknowledge, significantly reducing the dataset creation cost compared to manual\ncreation. The datasets are available at\nhttps://huggingface.co/datasets/yusuke1997/mCSQA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Findings of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.04215v1",
    "published_date": "2024-06-06 16:14:54 UTC",
    "updated_date": "2024-06-06 16:14:54 UTC"
  },
  {
    "arxiv_id": "2406.04208v1",
    "title": "Aligning Agents like Large Language Models",
    "authors": [
      "Adam Jelley",
      "Yuhan Cao",
      "Dave Bignell",
      "Sam Devlin",
      "Tabish Rashid"
    ],
    "abstract": "Training agents to behave as desired in complex 3D environments from\nhigh-dimensional sensory information is challenging. Imitation learning from\ndiverse human behavior provides a scalable approach for training an agent with\na sensible behavioral prior, but such an agent may not perform the specific\nbehaviors of interest when deployed. To address this issue, we draw an analogy\nbetween the undesirable behaviors of imitation learning agents and the\nunhelpful responses of unaligned large language models (LLMs). We then\ninvestigate how the procedure for aligning LLMs can be applied to aligning\nagents in a 3D environment from pixels. For our analysis, we utilize an\nacademically illustrative part of a modern console game in which the human\nbehavior distribution is multi-modal, but we want our agent to imitate a single\nmode of this behavior. We demonstrate that we can align our agent to\nconsistently perform the desired mode, while providing insights and advice for\nsuccessfully applying this approach to training agents. Project webpage at\nhttps://adamjelley.github.io/aligning-agents-like-llms .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04208v1",
    "published_date": "2024-06-06 16:05:45 UTC",
    "updated_date": "2024-06-06 16:05:45 UTC"
  },
  {
    "arxiv_id": "2406.04202v1",
    "title": "Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model",
    "authors": [
      "Chun-Hsien Lin",
      "Pu-Jen Cheng"
    ],
    "abstract": "With the development of large-scale Language Models (LLM), fine-tuning\npre-trained LLM has become a mainstream paradigm for solving downstream tasks\nof natural language processing. However, training a language model in the legal\nfield requires a large number of legal documents so that the language model can\nlearn legal terminology and the particularity of the format of legal documents.\nThe typical NLP approaches usually rely on many manually annotated data sets\nfor training. However, in the legal field application, it is difficult to\nobtain a large number of manually annotated data sets, which restricts the\ntypical method applied to the task of drafting legal documents. The\nexperimental results of this paper show that not only can we leverage a large\nnumber of annotation-free legal documents without Chinese word segmentation to\nfine-tune a large-scale language model, but more importantly, it can fine-tune\na pre-trained LLM on the local computer to achieve the generating legal\ndocument drafts task, and at the same time achieve the protection of\ninformation privacy and to improve information security issues.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12th International Conference on Software Engineering & Trends (SE\n  2024), April 27 ~ 28, 2024, Copenhagen, Denmark Volume Editors : David C.\n  Wyld, Dhinaharan Nagamalai (Eds) ISBN : 978-1-923107-24-3",
    "pdf_url": "http://arxiv.org/pdf/2406.04202v1",
    "published_date": "2024-06-06 16:00:20 UTC",
    "updated_date": "2024-06-06 16:00:20 UTC"
  },
  {
    "arxiv_id": "2406.06607v1",
    "title": "Continuous Test-time Domain Adaptation for Efficient Fault Detection under Evolving Operating Conditions",
    "authors": [
      "Han Sun",
      "Kevin Ammann",
      "Stylianos Giannoulakis",
      "Olga Fink"
    ],
    "abstract": "Fault detection is crucial in industrial systems to prevent failures and\noptimize performance by distinguishing abnormal from normal operating\nconditions. Data-driven methods have been gaining popularity for fault\ndetection tasks as the amount of condition monitoring data from complex\nindustrial systems increases. Despite these advances, early fault detection\nremains a challenge under real-world scenarios. The high variability of\noperating conditions and environments makes it difficult to collect\ncomprehensive training datasets that can represent all possible operating\nconditions, especially in the early stages of system operation. Furthermore,\nthese variations often evolve over time, potentially leading to entirely new\ndata distributions in the future that were previously unseen. These challenges\nprevent direct knowledge transfer across different units and over time, leading\nto the distribution gap between training and testing data and inducing\nperformance degradation of those methods in real-world scenarios. To overcome\nthis, our work introduces a novel approach for continuous test-time domain\nadaptation. This enables early-stage robust anomaly detection by addressing\ndomain shifts and limited data representativeness issues. We propose a\nTest-time domain Adaptation Anomaly Detection (TAAD) framework that separates\ninput variables into system parameters and measurements, employing two domain\nadaptation modules to independently adapt to each input category. This method\nallows for effective adaptation to evolving operating conditions and is\nparticularly beneficial in systems with scarce data. Our approach, tested on a\nreal-world pump monitoring dataset, shows significant improvements over\nexisting domain adaptation methods in fault detection, demonstrating enhanced\naccuracy and reliability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages including references",
    "pdf_url": "http://arxiv.org/pdf/2406.06607v1",
    "published_date": "2024-06-06 15:53:14 UTC",
    "updated_date": "2024-06-06 15:53:14 UTC"
  },
  {
    "arxiv_id": "2406.04388v2",
    "title": "Single Exposure Quantitative Phase Imaging with a Conventional Microscope using Diffusion Models",
    "authors": [
      "Gabriel della Maggiora",
      "Luis Alberto Croquevielle",
      "Harry Horsley",
      "Thomas Heinis",
      "Artur Yakimovich"
    ],
    "abstract": "Phase imaging is gaining importance due to its applications in fields like\nbiomedical imaging and material characterization. In biomedical applications,\nit can provide quantitative information missing in label-free microscopy\nmodalities. One of the most prominent methods in phase quantification is the\nTransport-of-Intensity Equation (TIE). TIE often requires multiple acquisitions\nat different defocus distances, which is not always feasible in a clinical\nsetting. To address this issue, we propose to use chromatic aberrations to\ninduce the required through-focus images with a single exposure, effectively\ngenerating a through-focus stack. Since the defocus distance induced by the\naberrations is small, conventional TIE solvers are insufficient to address the\nresulting artifacts. We propose Zero-Mean Diffusion, a modified version of\ndiffusion models designed for quantitative image prediction, and train it with\nsynthetic data to ensure robust phase retrieval. Our contributions offer an\nalternative TIE approach that leverages chromatic aberrations, achieving\naccurate single-exposure phase measurement with white light and thus improving\nthe efficiency of phase imaging. Moreover, we present a new class of diffusion\nmodels that are well-suited for quantitative data and have a sound theoretical\nbasis. To validate our approach, we employ a widespread brightfield microscope\nequipped with a commercially available color camera. We apply our model to\nclinical microscopy of patients' urine, obtaining accurate phase measurements.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "physics.optics"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04388v2",
    "published_date": "2024-06-06 15:44:24 UTC",
    "updated_date": "2024-12-20 15:49:51 UTC"
  },
  {
    "arxiv_id": "2406.04184v2",
    "title": "Shield Synthesis for LTL Modulo Theories",
    "authors": [
      "Andoni Rodriguez",
      "Guy Amir",
      "Davide Corsi",
      "Cesar Sanchez",
      "Guy Katz"
    ],
    "abstract": "In recent years, Machine Learning (ML) models have achieved remarkable\nsuccess in various domains. However, these models also tend to demonstrate\nunsafe behaviors, precluding their deployment in safety-critical systems. To\ncope with this issue, ample research focuses on developing methods that\nguarantee the safe behaviour of a given ML model. A prominent example is\nshielding which incorporates an external component (a ``shield'') that blocks\nunwanted behavior. Despite significant progress, shielding suffers from a main\nsetback: it is currently geared towards properties encoded solely in\npropositional logics (e.g., LTL) and is unsuitable for richer logics. This, in\nturn, limits the widespread applicability of shielding in many real-world\nsystems. In this work, we address this gap, and extend shielding to LTL modulo\ntheories, by building upon recent advances in reactive synthesis modulo\ntheories. This allowed us to develop a novel approach for generating shields\nconforming to complex safety specifications in these more expressive, logics.\nWe evaluated our shields and demonstrate their ability to handle rich data with\ntemporal dynamics. To the best of our knowledge, this is the first approach for\nsynthesizing shields for such expressivity.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LO",
    "comment": "To appear in AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.04184v2",
    "published_date": "2024-06-06 15:40:29 UTC",
    "updated_date": "2025-02-14 15:19:01 UTC"
  },
  {
    "arxiv_id": "2406.04175v2",
    "title": "Confabulation: The Surprising Value of Large Language Model Hallucinations",
    "authors": [
      "Peiqi Sui",
      "Eamon Duede",
      "Sophie Wu",
      "Richard Jean So"
    ],
    "abstract": "This paper presents a systematic defense of large language model (LLM)\nhallucinations or 'confabulations' as a potential resource instead of a\ncategorically negative pitfall. The standard view is that confabulations are\ninherently problematic and AI research should eliminate this flaw. In this\npaper, we argue and empirically demonstrate that measurable semantic\ncharacteristics of LLM confabulations mirror a human propensity to utilize\nincreased narrativity as a cognitive resource for sense-making and\ncommunication. In other words, it has potential value. Specifically, we analyze\npopular hallucination benchmarks and reveal that hallucinated outputs display\nincreased levels of narrativity and semantic coherence relative to veridical\noutputs. This finding reveals a tension in our usually dismissive\nunderstandings of confabulation. It suggests, counter-intuitively, that the\ntendency for LLMs to confabulate may be intimately associated with a positive\ncapacity for coherent narrative-text generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Forthcoming at ACL2024 main conference. 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2406.04175v2",
    "published_date": "2024-06-06 15:32:29 UTC",
    "updated_date": "2024-06-25 18:37:19 UTC"
  },
  {
    "arxiv_id": "2406.04170v4",
    "title": "Element-wise Multiplication Based Deeper Physics-Informed Neural Networks",
    "authors": [
      "Feilong Jiang",
      "Xiaonan Hou",
      "Min Xia"
    ],
    "abstract": "As a promising framework for resolving partial differential equations (PDEs),\nPhysics-Informed Neural Networks (PINNs) have received widespread attention\nfrom industrial and scientific fields. However, lack of expressive ability and\ninitialization pathology issues are found to prevent the application of PINNs\nin complex PDEs. In this work, we propose Deeper Physics-Informed Neural\nNetwork (Deeper-PINN) to resolve these issues. The element-wise multiplication\noperation is adopted to transform features into high-dimensional, non-linear\nspaces. Benefiting from element-wise multiplication operation, Deeper-PINNs can\nalleviate the initialization pathologies of PINNs and enhance the expressive\ncapability of PINNs. The proposed structure is verified on various benchmarks.\nThe results show that Deeper-PINNs can effectively resolve the initialization\npathology and exhibit strong expressive ability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04170v4",
    "published_date": "2024-06-06 15:27:52 UTC",
    "updated_date": "2024-09-11 20:21:36 UTC"
  },
  {
    "arxiv_id": "2406.06606v2",
    "title": "Prototypical Reward Network for Data-Efficient RLHF",
    "authors": [
      "Jinghan Zhang",
      "Xiting Wang",
      "Yiqiao Jin",
      "Changyu Chen",
      "Xinhao Zhang",
      "Kunpeng Liu"
    ],
    "abstract": "The reward model for Reinforcement Learning from Human Feedback (RLHF) has\nproven effective in fine-tuning Large Language Models (LLMs). Notably,\ncollecting human feedback for RLHF can be resource-intensive and lead to\nscalability issues for LLMs and complex tasks. Our proposed framework Proto-RM\nleverages prototypical networks to enhance reward models under limited human\nfeedback. By enabling stable and reliable structural learning from fewer\nsamples, Proto-RM significantly enhances LLMs' adaptability and accuracy in\ninterpreting human preferences. Extensive experiments on various datasets\ndemonstrate that Proto-RM significantly improves the performance of reward\nmodels and LLMs in human feedback tasks, achieving comparable and usually\nbetter results than traditional methods, while requiring significantly less\ndata. in data-limited scenarios. This research offers a promising direction for\nenhancing the efficiency of reward models and optimizing the fine-tuning of\nlanguage models under restricted feedback conditions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06606v2",
    "published_date": "2024-06-06 15:23:30 UTC",
    "updated_date": "2024-07-07 16:29:17 UTC"
  },
  {
    "arxiv_id": "2406.04156v1",
    "title": "Pointer-Guided Pre-Training: Infusing Large Language Models with Paragraph-Level Contextual Awareness",
    "authors": [
      "Lars Hillebrand",
      "Prabhupad Pradhan",
      "Christian Bauckhage",
      "Rafet Sifa"
    ],
    "abstract": "We introduce \"pointer-guided segment ordering\" (SO), a novel pre-training\ntechnique aimed at enhancing the contextual understanding of paragraph-level\ntext representations in large language models. Our methodology leverages a\nself-attention-driven pointer network to restore the original sequence of\nshuffled text segments, addressing the challenge of capturing the structural\ncoherence and contextual dependencies within documents. This pre-training\napproach is complemented by a fine-tuning methodology that incorporates dynamic\nsampling, augmenting the diversity of training instances and improving sample\nefficiency for various downstream applications. We evaluate our method on a\ndiverse set of datasets, demonstrating its efficacy in tasks requiring\nsequential text classification across scientific literature and financial\nreporting domains. Our experiments show that pointer-guided pre-training\nsignificantly enhances the model's ability to understand complex document\nstructures, leading to state-of-the-art performance in downstream\nclassification tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 3 figures, 5 tables, accepted at ECML-PKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.04156v1",
    "published_date": "2024-06-06 15:17:51 UTC",
    "updated_date": "2024-06-06 15:17:51 UTC"
  },
  {
    "arxiv_id": "2406.04155v1",
    "title": "Improving Physics-Augmented Continuum Neural Radiance Field-Based Geometry-Agnostic System Identification with Lagrangian Particle Optimization",
    "authors": [
      "Takuhiro Kaneko"
    ],
    "abstract": "Geometry-agnostic system identification is a technique for identifying the\ngeometry and physical properties of an object from video sequences without any\ngeometric assumptions. Recently, physics-augmented continuum neural radiance\nfields (PAC-NeRF) has demonstrated promising results for this technique by\nutilizing a hybrid Eulerian-Lagrangian representation, in which the geometry is\nrepresented by the Eulerian grid representations of NeRF, the physics is\ndescribed by a material point method (MPM), and they are connected via\nLagrangian particles. However, a notable limitation of PAC-NeRF is that its\nperformance is sensitive to the learning of the geometry from the first frames\nowing to its two-step optimization. First, the grid representations are\noptimized with the first frames of video sequences, and then the physical\nproperties are optimized through video sequences utilizing the fixed\nfirst-frame grid representations. This limitation can be critical when learning\nof the geometric structure is difficult, for example, in a few-shot (sparse\nview) setting. To overcome this limitation, we propose Lagrangian particle\noptimization (LPO), in which the positions and features of particles are\noptimized through video sequences in Lagrangian space. This method allows for\nthe optimization of the geometric structure across the entire video sequence\nwithin the physical constraints imposed by the MPM. The experimental results\ndemonstrate that the LPO is useful for geometric correction and physical\nidentification in sparse-view settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2024. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/lpo/",
    "pdf_url": "http://arxiv.org/pdf/2406.04155v1",
    "published_date": "2024-06-06 15:17:33 UTC",
    "updated_date": "2024-06-06 15:17:33 UTC"
  },
  {
    "arxiv_id": "2406.04151v1",
    "title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments",
    "authors": [
      "Zhiheng Xi",
      "Yiwen Ding",
      "Wenxiang Chen",
      "Boyang Hong",
      "Honglin Guo",
      "Junzhe Wang",
      "Dingwen Yang",
      "Chenyang Liao",
      "Xin Guo",
      "Wei He",
      "Songyang Gao",
      "Lu Chen",
      "Rui Zheng",
      "Yicheng Zou",
      "Tao Gui",
      "Qi Zhang",
      "Xipeng Qiu",
      "Xuanjing Huang",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "Building generalist agents that can handle diverse tasks and evolve\nthemselves across different environments is a long-term goal in the AI\ncommunity. Large language models (LLMs) are considered a promising foundation\nto build such agents due to their generalized capabilities. Current approaches\neither have LLM-based agents imitate expert-provided trajectories step-by-step,\nrequiring human supervision, which is hard to scale and limits environmental\nexploration; or they let agents explore and learn in isolated environments,\nresulting in specialist agents with limited generalization. In this paper, we\ntake the first step towards building generally-capable LLM-based agents with\nself-evolution ability. We identify a trinity of ingredients: 1) diverse\nenvironments for agent exploration and learning, 2) a trajectory set to equip\nagents with basic capabilities and prior knowledge, and 3) an effective and\nscalable evolution method. We propose AgentGym, a new framework featuring a\nvariety of environments and tasks for broad, real-time, uni-format, and\nconcurrent agent exploration. AgentGym also includes a database with expanded\ninstructions, a benchmark suite, and high-quality trajectories across\nenvironments. Next, we propose a novel method, AgentEvol, to investigate the\npotential of agent self-evolution beyond previously seen data across tasks and\nenvironments. Experimental results show that the evolved agents can achieve\nresults comparable to SOTA models. We release the AgentGym suite, including the\nplatform, dataset, benchmark, checkpoints, and algorithm implementations. The\nAgentGym suite is available on https://github.com/WooooDyy/AgentGym.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Project site: https://agentgym.github.io",
    "pdf_url": "http://arxiv.org/pdf/2406.04151v1",
    "published_date": "2024-06-06 15:15:41 UTC",
    "updated_date": "2024-06-06 15:15:41 UTC"
  },
  {
    "arxiv_id": "2406.04149v1",
    "title": "Characterizing segregation in blast rock piles a deep-learning approach leveraging aerial image analysis",
    "authors": [
      "Chengeng Liu",
      "Sihong Liu",
      "Chaomin Shen",
      "Yupeng Gao",
      "Yuxuan Liu"
    ],
    "abstract": "Blasted rock material serves a critical role in various engineering\napplications, yet the phenomenon of segregation-where particle sizes vary\nsignificantly along the gradient of a quarry pile-presents challenges for\noptimizing quarry material storage and handling. This study introduces an\nadvanced image analysis methodology to characterize such segregation of rock\nfragments. The accurate delineation of detailed rock fragment size\ndistributions was achieved through the analysis of drone-captured imagery,\ncoupled with the application of an enhanced Unet semantic segmentation model\nintegrated with an expansion-based post-processing technique. The quarry slope\nwas stratified into four vertical sections, with the size distribution of each\nsection quantified via ellipsoid shape approximations. Our results disclose\npronounced vertical segregation patterns, with finer particles concentrated in\nthe upper slope regions and coarser particles in the lower. Utilizing relative\ncharacteristic diameters, we offered insight into the degree of segregation,\nthereby illustrating the spatial heterogeneity in fragment size more clearly.\nThe techniques outlined in this study deliver a scalable and accurate method\nfor assessing fragment size distribution, with the potential to better inform\nresource management and operational decisions in quarry management.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04149v1",
    "published_date": "2024-06-06 15:13:56 UTC",
    "updated_date": "2024-06-06 15:13:56 UTC"
  },
  {
    "arxiv_id": "2406.04145v1",
    "title": "Every Answer Matters: Evaluating Commonsense with Probabilistic Measures",
    "authors": [
      "Qi Cheng",
      "Michael Boratko",
      "Pranay Kumar Yelugam",
      "Tim O'Gorman",
      "Nalini Singh",
      "Andrew McCallum",
      "Xiang Lorraine Li"
    ],
    "abstract": "Large language models have demonstrated impressive performance on commonsense\ntasks; however, these tasks are often posed as multiple-choice questions,\nallowing models to exploit systematic biases. Commonsense is also inherently\nprobabilistic with multiple correct answers. The purpose of \"boiling water\"\ncould be making tea and cooking, but it also could be killing germs. Existing\ntasks do not capture the probabilistic nature of common sense. To this end, we\npresent commonsense frame completion (CFC), a new generative task that\nevaluates common sense via multiple open-ended generations. We also propose a\nmethod of probabilistic evaluation that strongly correlates with human\njudgments. Humans drastically outperform strong language model baselines on our\ndataset, indicating this approach is both a challenging and useful evaluation\nof machine common sense.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2406.04145v1",
    "published_date": "2024-06-06 15:10:27 UTC",
    "updated_date": "2024-06-06 15:10:27 UTC"
  },
  {
    "arxiv_id": "2406.04144v1",
    "title": "Redundancy-aware Action Spaces for Robot Learning",
    "authors": [
      "Pietro Mazzaglia",
      "Nicholas Backshall",
      "Xiao Ma",
      "Stephen James"
    ],
    "abstract": "Joint space and task space control are the two dominant action modes for\ncontrolling robot arms within the robot learning literature. Actions in joint\nspace provide precise control over the robot's pose, but tend to suffer from\ninefficient training; actions in task space boast data-efficient training but\nsacrifice the ability to perform tasks in confined spaces due to limited\ncontrol over the full joint configuration. This work analyses the criteria for\ndesigning action spaces for robot manipulation and introduces ER (End-effector\nRedundancy), a novel action space formulation that, by addressing the\nredundancies present in the manipulator, aims to combine the advantages of both\njoint and task spaces, offering fine-grained comprehensive control with\noveractuated robot arms whilst achieving highly efficient robot learning. We\npresent two implementations of ER, ERAngle (ERA) and ERJoint (ERJ), and we show\nthat ERJ in particular demonstrates superior performance across multiple\nsettings, especially when precise control over the robot configuration is\nrequired. We validate our results both in simulated and real robotic\nenvironments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Published in the RA-L journal",
    "pdf_url": "http://arxiv.org/pdf/2406.04144v1",
    "published_date": "2024-06-06 15:08:41 UTC",
    "updated_date": "2024-06-06 15:08:41 UTC"
  },
  {
    "arxiv_id": "2406.04143v1",
    "title": "Do Language Models Understand Morality? Towards a Robust Detection of Moral Content",
    "authors": [
      "Luana Bulla",
      "Aldo Gangemi",
      "Misael Mongiovì"
    ],
    "abstract": "The task of detecting moral values in text has significant implications in\nvarious fields, including natural language processing, social sciences, and\nethical decision-making. Previously proposed supervised models often suffer\nfrom overfitting, leading to hyper-specialized moral classifiers that struggle\nto perform well on data from different domains. To address this issue, we\nintroduce novel systems that leverage abstract concepts and common-sense\nknowledge acquired from Large Language Models and Natural Language Inference\nmodels during previous stages of training on multiple data sources. By doing\nso, we aim to develop versatile and robust methods for detecting moral values\nin real-world scenarios. Our approach uses the GPT 3.5 model as a zero-shot\nready-made unsupervised multi-label classifier for moral values detection,\neliminating the need for explicit training on labeled data. We compare it with\na smaller NLI-based zero-shot model. The results show that the NLI approach\nachieves competitive results compared to the Davinci model. Furthermore, we\nconduct an in-depth investigation of the performance of supervised systems in\nthe context of cross-domain multi-label moral value detection. This involves\ntraining supervised models on different domains to explore their effectiveness\nin handling data from different sources and comparing their performance with\nthe unsupervised methods. Our contributions encompass a thorough analysis of\nboth supervised and unsupervised methodologies for cross-domain value\ndetection. We introduce the Davinci model as a state-of-the-art zero-shot\nunsupervised moral values classifier, pushing the boundaries of moral value\ndetection without the need for explicit training on labeled data. Additionally,\nwe perform a comparative evaluation of our approach with the supervised models,\nshedding light on their respective strengths and weaknesses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04143v1",
    "published_date": "2024-06-06 15:08:16 UTC",
    "updated_date": "2024-06-06 15:08:16 UTC"
  },
  {
    "arxiv_id": "2406.04136v1",
    "title": "Legal Judgment Reimagined: PredEx and the Rise of Intelligent AI Interpretation in Indian Courts",
    "authors": [
      "Shubham Kumar Nigam",
      "Anurag Sharma",
      "Danush Khanna",
      "Noel Shallum",
      "Kripabandhu Ghosh",
      "Arnab Bhattacharya"
    ],
    "abstract": "In the era of Large Language Models (LLMs), predicting judicial outcomes\nposes significant challenges due to the complexity of legal proceedings and the\nscarcity of expert-annotated datasets. Addressing this, we introduce\n\\textbf{Pred}iction with \\textbf{Ex}planation (\\texttt{PredEx}), the largest\nexpert-annotated dataset for legal judgment prediction and explanation in the\nIndian context, featuring over 15,000 annotations. This groundbreaking corpus\nsignificantly enhances the training and evaluation of AI models in legal\nanalysis, with innovations including the application of instruction tuning to\nLLMs. This method has markedly improved the predictive accuracy and explanatory\ndepth of these models for legal judgments. We employed various\ntransformer-based models, tailored for both general and Indian legal contexts.\nThrough rigorous lexical, semantic, and expert assessments, our models\neffectively leverage \\texttt{PredEx} to provide precise predictions and\nmeaningful explanations, establishing it as a valuable benchmark for both the\nlegal profession and the NLP community.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04136v1",
    "published_date": "2024-06-06 14:57:48 UTC",
    "updated_date": "2024-06-06 14:57:48 UTC"
  },
  {
    "arxiv_id": "2407.11990v1",
    "title": "Digital twins in sport: Concepts, Taxonomies, Challenges and Practical Potentials",
    "authors": [
      "Tilen Hliš",
      "Iztok Fister",
      "Iztok Fister Jr"
    ],
    "abstract": "Digital twins belong to ten of the strategic technology trends according to\nthe Gartner list from 2019, and have encountered a big expansion, especially\nwith the introduction of Industry 4.0. Sport, on the other hand, has become a\nconstant companion of the modern human suffering a lack of a healthy way of\nlife. The application of digital twins in sport has brought dramatic changes\nnot only in the domain of sport training, but also in managing athletes during\ncompetitions, searching for strategical solutions before and tactical solutions\nduring the games by coaches. In this paper, the domain of digital twins in\nsport is reviewed based on papers which have emerged in this area. At first,\nthe concept of a digital twin is discussed in general. Then, taxonomies of\ndigital twins are appointed. According to these taxonomies, the collection of\nrelevant papers is analyzed, and some real examples of digital twins are\nexposed. The review finishes with a discussion about how the digital twins\naffect changes in the modern sport disciplines, and what challenges and\nopportunities await the digital twins in the future.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11990v1",
    "published_date": "2024-06-06 14:51:01 UTC",
    "updated_date": "2024-06-06 14:51:01 UTC"
  },
  {
    "arxiv_id": "2406.04127v3",
    "title": "Are We Done with MMLU?",
    "authors": [
      "Aryo Pradipta Gema",
      "Joshua Ong Jun Leang",
      "Giwon Hong",
      "Alessio Devoto",
      "Alberto Carlo Maria Mancino",
      "Rohit Saxena",
      "Xuanli He",
      "Yu Zhao",
      "Xiaotang Du",
      "Mohammad Reza Ghasemi Madani",
      "Claire Barale",
      "Robert McHardy",
      "Joshua Harris",
      "Jean Kaddour",
      "Emile van Krieken",
      "Pasquale Minervini"
    ],
    "abstract": "Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\nannotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700\nmanually re-annotated questions across all 57 MMLU subjects. We estimate that\n6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate\nsignificant discrepancies with the model performance metrics that were\noriginally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04127v3",
    "published_date": "2024-06-06 14:49:06 UTC",
    "updated_date": "2025-01-10 14:31:21 UTC"
  },
  {
    "arxiv_id": "2406.04116v2",
    "title": "Promoting the Responsible Development of Speech Datasets for Mental Health and Neurological Disorders Research",
    "authors": [
      "Eleonora Mancini",
      "Ana Tanevska",
      "Andrea Galassi",
      "Alessio Galatolo",
      "Federico Ruggeri",
      "Paolo Torroni"
    ],
    "abstract": "Current research in machine learning and artificial intelligence is largely\ncentered on modeling and performance evaluation, less so on data collection.\nHowever, recent research demonstrated that limitations and biases in data may\nnegatively impact trustworthiness and reliability. These aspects are\nparticularly impactful on sensitive domains such as mental health and\nneurological disorders, where speech data are used to develop AI applications\nfor patients and healthcare providers. In this paper, we chart the landscape of\navailable speech datasets for this domain, to highlight possible pitfalls and\nopportunities for improvement and promote fairness and diversity. We present a\ncomprehensive list of desiderata for building speech datasets for mental health\nand neurological disorders and distill it into an actionable checklist focused\non ethical concerns to foster more responsible research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "36 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.04116v2",
    "published_date": "2024-06-06 14:36:07 UTC",
    "updated_date": "2025-02-17 12:44:01 UTC"
  },
  {
    "arxiv_id": "2406.06603v1",
    "title": "FPN-fusion: Enhanced Linear Complexity Time Series Forecasting Model",
    "authors": [
      "Chu Li",
      "Pingjia Xiao",
      "Qiping Yuan"
    ],
    "abstract": "This study presents a novel time series prediction model, FPN-fusion,\ndesigned with linear computational complexity, demonstrating superior\npredictive performance compared to DLiner without increasing parameter count or\ncomputational demands. Our model introduces two key innovations: first, a\nFeature Pyramid Network (FPN) is employed to effectively capture time series\ndata characteristics, bypassing the traditional decomposition into trend and\nseasonal components. Second, a multi-level fusion structure is developed to\nintegrate deep and shallow features seamlessly. Empirically, FPN-fusion\noutperforms DLiner in 31 out of 32 test cases on eight open-source datasets,\nwith an average reduction of 16.8% in mean squared error (MSE) and 11.8% in\nmean absolute error (MAE). Additionally, compared to the transformer-based\nPatchTST, FPN-fusion achieves 10 best MSE and 15 best MAE results, using only\n8% of PatchTST's total computational load in the 32 test projects.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "FPN,time series,fusion. arXiv admin note: text overlap with\n  arXiv:2401.03001 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2406.06603v1",
    "published_date": "2024-06-06 14:34:26 UTC",
    "updated_date": "2024-06-06 14:34:26 UTC"
  },
  {
    "arxiv_id": "2406.04112v2",
    "title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation",
    "authors": [
      "Can Yaras",
      "Peng Wang",
      "Laura Balzano",
      "Qing Qu"
    ],
    "abstract": "While overparameterization in machine learning models offers great benefits\nin terms of optimization and generalization, it also leads to increased\ncomputational requirements as model sizes grow. In this work, we show that by\nleveraging the inherent low-dimensional structures of data and compressible\ndynamics within the model parameters, we can reap the benefits of\noverparameterization without the computational burdens. In practice, we\ndemonstrate the effectiveness of this approach for deep low-rank matrix\ncompletion as well as fine-tuning language models. Our approach is grounded in\ntheoretical findings for deep overparameterized low-rank matrix recovery, where\nwe show that the learning dynamics of each weight matrix are confined to an\ninvariant low-dimensional subspace. Consequently, we can construct and train\ncompact, highly compressed factorizations possessing the same benefits as their\noverparameterized counterparts. In the context of deep matrix completion, our\ntechnique substantially improves training efficiency while retaining the\nadvantages of overparameterization. For language model fine-tuning, we propose\na method called \"Deep LoRA\", which improves the existing low-rank adaptation\n(LoRA) technique, leading to reduced overfitting and a simplified\nhyperparameter setup, while maintaining comparable efficiency. We validate the\neffectiveness of Deep LoRA on natural language tasks, particularly when\nfine-tuning with limited data. Our code is available at\nhttps://github.com/cjyaras/deep-lora-transformers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML'24 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2406.04112v2",
    "published_date": "2024-06-06 14:29:49 UTC",
    "updated_date": "2024-06-10 02:05:26 UTC"
  },
  {
    "arxiv_id": "2406.04103v1",
    "title": "Multistep Distillation of Diffusion Models via Moment Matching",
    "authors": [
      "Tim Salimans",
      "Thomas Mensink",
      "Jonathan Heek",
      "Emiel Hoogeboom"
    ],
    "abstract": "We present a new method for making diffusion models faster to sample. The\nmethod distills many-step diffusion models into few-step models by matching\nconditional expectations of the clean data given noisy data along the sampling\ntrajectory. Our approach extends recently proposed one-step methods to the\nmulti-step case, and provides a new perspective by interpreting these\napproaches in terms of moment matching. By using up to 8 sampling steps, we\nobtain distilled models that outperform not only their one-step versions but\nalso their original many-step teacher models, obtaining new state-of-the-art\nresults on the Imagenet dataset. We also show promising results on a large\ntext-to-image model where we achieve fast generation of high resolution images\ndirectly in image space, without needing autoencoders or upsamplers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04103v1",
    "published_date": "2024-06-06 14:20:21 UTC",
    "updated_date": "2024-06-06 14:20:21 UTC"
  },
  {
    "arxiv_id": "2406.04099v2",
    "title": "Enhancing Weather Predictions: Super-Resolution via Deep Diffusion Models",
    "authors": [
      "Jan Martinů",
      "Petr Šimánek"
    ],
    "abstract": "This study investigates the application of deep-learning diffusion models for\nthe super-resolution of weather data, a novel approach aimed at enhancing the\nspatial resolution and detail of meteorological variables. Leveraging the\ncapabilities of diffusion models, specifically the SR3 and ResDiff\narchitectures, we present a methodology for transforming low-resolution weather\ndata into high-resolution outputs. Our experiments, conducted using the\nWeatherBench dataset, focus on the super-resolution of the two-meter\ntemperature variable, demonstrating the models' ability to generate detailed\nand accurate weather maps. The results indicate that the ResDiff model, further\nimproved by incorporating physics-based modifications, significantly\noutperforms traditional SR3 methods in terms of Mean Squared Error (MSE),\nStructural Similarity Index (SSIM), and Peak Signal-to-Noise Ratio (PSNR). This\nresearch highlights the potential of diffusion models in meteorological\napplications, offering insights into their effectiveness, challenges, and\nprospects for future advancements in weather prediction and climate analysis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04099v2",
    "published_date": "2024-06-06 14:15:12 UTC",
    "updated_date": "2024-08-30 08:05:08 UTC"
  },
  {
    "arxiv_id": "2406.04093v1",
    "title": "Scaling and evaluating sparse autoencoders",
    "authors": [
      "Leo Gao",
      "Tom Dupré la Tour",
      "Henk Tillman",
      "Gabriel Goh",
      "Rajan Troll",
      "Alec Radford",
      "Ilya Sutskever",
      "Jan Leike",
      "Jeffrey Wu"
    ],
    "abstract": "Sparse autoencoders provide a promising unsupervised approach for extracting\ninterpretable features from a language model by reconstructing activations from\na sparse bottleneck layer. Since language models learn many concepts,\nautoencoders need to be very large to recover all relevant features. However,\nstudying the properties of autoencoder scaling is difficult due to the need to\nbalance reconstruction and sparsity objectives and the presence of dead\nlatents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to\ndirectly control sparsity, simplifying tuning and improving the\nreconstruction-sparsity frontier. Additionally, we find modifications that\nresult in few dead latents, even at the largest scales we tried. Using these\ntechniques, we find clean scaling laws with respect to autoencoder size and\nsparsity. We also introduce several new metrics for evaluating feature quality\nbased on the recovery of hypothesized features, the explainability of\nactivation patterns, and the sparsity of downstream effects. These metrics all\ngenerally improve with autoencoder size. To demonstrate the scalability of our\napproach, we train a 16 million latent autoencoder on GPT-4 activations for 40\nbillion tokens. We release training code and autoencoders for open-source\nmodels, as well as a visualizer.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04093v1",
    "published_date": "2024-06-06 14:10:12 UTC",
    "updated_date": "2024-06-06 14:10:12 UTC"
  },
  {
    "arxiv_id": "2406.06601v1",
    "title": "A Human-in-the-Loop Approach to Improving Cross-Text Prosody Transfer",
    "authors": [
      "Himanshu Maurya",
      "Atli Sigurgeirsson"
    ],
    "abstract": "Text-To-Speech (TTS) prosody transfer models can generate varied prosodic\nrenditions, for the same text, by conditioning on a reference utterance. These\nmodels are trained with a reference that is identical to the target utterance.\nBut when the reference utterance differs from the target text, as in cross-text\nprosody transfer, these models struggle to separate prosody from text,\nresulting in reduced perceived naturalness. To address this, we propose a\nHuman-in-the-Loop (HitL) approach. HitL users adjust salient correlates of\nprosody to make the prosody more appropriate for the target text, while\nmaintaining the overall reference prosodic effect. Human adjusted renditions\nmaintain the reference prosody while being rated as more appropriate for the\ntarget text $57.8\\%$ of the time. Our analysis suggests that limited user\neffort suffices for these improvements, and that closeness in the latent\nreference space is not a reliable prosodic similarity metric for the cross-text\ncondition.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages (+1 references), 4 figures, to be presented at Interspeech\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06601v1",
    "published_date": "2024-06-06 14:01:53 UTC",
    "updated_date": "2024-06-06 14:01:53 UTC"
  },
  {
    "arxiv_id": "2406.04089v1",
    "title": "On Limitation of Transformer for Learning HMMs",
    "authors": [
      "Jiachen Hu",
      "Qinghua Liu",
      "Chi Jin"
    ],
    "abstract": "Despite the remarkable success of Transformer-based architectures in various\nsequential modeling tasks, such as natural language processing, computer\nvision, and robotics, their ability to learn basic sequential models, like\nHidden Markov Models (HMMs), is still unclear. This paper investigates the\nperformance of Transformers in learning HMMs and their variants through\nextensive experimentation and compares them to Recurrent Neural Networks\n(RNNs). We show that Transformers consistently underperform RNNs in both\ntraining speed and testing accuracy across all tested HMM models. There are\neven challenging HMM instances where Transformers struggle to learn, while RNNs\ncan successfully do so. Our experiments further reveal the relation between the\ndepth of Transformers and the longest sequence length it can effectively learn,\nbased on the types and the complexity of HMMs. To address the limitation of\ntransformers in modeling HMMs, we demonstrate that a variant of the\nChain-of-Thought (CoT), called $\\textit{block CoT}$ in the training phase, can\nhelp transformers to reduce the evaluation error and to learn longer sequences\nat a cost of increasing the training time. Finally, we complement our empirical\nfindings by theoretical results proving the expressiveness of transformers in\napproximating HMMs with logarithmic depth.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04089v1",
    "published_date": "2024-06-06 13:59:51 UTC",
    "updated_date": "2024-06-06 13:59:51 UTC"
  },
  {
    "arxiv_id": "2406.10244v3",
    "title": "GLINT-RU: Gated Lightweight Intelligent Recurrent Units for Sequential Recommender Systems",
    "authors": [
      "Sheng Zhang",
      "Maolin Wang",
      "Wanyu Wang",
      "Jingtong Gao",
      "Xiangyu Zhao",
      "Yu Yang",
      "Xuetao Wei",
      "Zitao Liu",
      "Tong Xu"
    ],
    "abstract": "Transformer-based models have gained significant traction in sequential\nrecommender systems (SRSs) for their ability to capture user-item interactions\neffectively. However, these models often suffer from high computational costs\nand slow inference. Meanwhile, existing efficient SRS approaches struggle to\nembed high-quality semantic and positional information into latent\nrepresentations. To tackle these challenges, this paper introduces GLINT-RU, a\nlightweight and efficient SRS leveraging a single-layer dense selective Gated\nRecurrent Units (GRU) module to accelerate inference. By incorporating a dense\nselective gate, GLINT-RU adaptively captures temporal dependencies and\nfine-grained positional information, generating high-quality latent\nrepresentations. Additionally, a parallel mixing block infuses fine-grained\npositional features into user-item interactions, enhancing both recommendation\nquality and efficiency. Extensive experiments on three datasets demonstrate\nthat GLINT-RU achieves superior prediction accuracy and inference speed,\noutperforming baselines based on RNNs, Transformers, MLPs, and SSMs. These\nresults establish GLINT-RU as a powerful and efficient solution for SRSs.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10244v3",
    "published_date": "2024-06-06 13:55:55 UTC",
    "updated_date": "2025-04-12 17:18:17 UTC"
  },
  {
    "arxiv_id": "2406.04082v1",
    "title": "Leveraging automatic strategy discovery to teach people how to select better projects",
    "authors": [
      "Lovis Heindrich",
      "Falk Lieder"
    ],
    "abstract": "The decisions of individuals and organizations are often suboptimal because\nnormative decision strategies are too demanding in the real world. Recent work\nsuggests that some errors can be prevented by leveraging artificial\nintelligence to discover and teach prescriptive decision strategies that take\npeople's constraints into account. So far, this line of research has been\nlimited to simplified decision problems. This article is the first to extend\nthis approach to a real-world decision problem, namely project selection. We\ndevelop a computational method (MGPS) that automatically discovers project\nselection strategies that are optimized for real people and develop an\nintelligent tutor that teaches the discovered strategies. We evaluated MGPS on\na computational benchmark and tested the intelligent tutor in a training\nexperiment with two control conditions. MGPS outperformed a state-of-the-art\nmethod and was more computationally efficient. Moreover, the intelligent tutor\nsignificantly improved people's decision strategies. Our results indicate that\nour method can improve human decision-making in naturalistic settings similar\nto real-world project selection, a first step towards applying strategy\ndiscovery to the real world.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04082v1",
    "published_date": "2024-06-06 13:51:44 UTC",
    "updated_date": "2024-06-06 13:51:44 UTC"
  },
  {
    "arxiv_id": "2406.04081v1",
    "title": "Bootstrapping Expectiles in Reinforcement Learning",
    "authors": [
      "Pierre Clavier",
      "Emmanuel Rachelson",
      "Erwan Le Pennec",
      "Matthieu Geist"
    ],
    "abstract": "Many classic Reinforcement Learning (RL) algorithms rely on a Bellman\noperator, which involves an expectation over the next states, leading to the\nconcept of bootstrapping. To introduce a form of pessimism, we propose to\nreplace this expectation with an expectile. In practice, this can be very\nsimply done by replacing the $L_2$ loss with a more general expectile loss for\nthe critic. Introducing pessimism in RL is desirable for various reasons, such\nas tackling the overestimation problem (for which classic solutions are double\nQ-learning or the twin-critic approach of TD3) or robust RL (where transitions\nare adversarial). We study empirically these two cases. For the overestimation\nproblem, we show that the proposed approach, ExpectRL, provides better results\nthan a classic twin-critic. On robust RL benchmarks, involving changes of the\nenvironment, we show that our approach is more robust than classic RL\nalgorithms. We also introduce a variation of ExpectRL combined with domain\nrandomization which is competitive with state-of-the-art robust RL agents.\nEventually, we also extend \\ExpectRL with a mechanism for choosing\nautomatically the expectile value, that is the degree of pessimism",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04081v1",
    "published_date": "2024-06-06 13:51:39 UTC",
    "updated_date": "2024-06-06 13:51:39 UTC"
  },
  {
    "arxiv_id": "2406.06600v5",
    "title": "HORAE: A Domain-Agnostic Language for Automated Service Regulation",
    "authors": [
      "Yutao Sun",
      "Mingshuai Chen",
      "Tiancheng Zhao",
      "Kangjia Zhao",
      "He Li",
      "Jintao Chen",
      "Zhongyi Wang",
      "Liqiang Lu",
      "Xinkui Zhao",
      "Shuiguang Deng",
      "Jianwei Yin"
    ],
    "abstract": "Artificial intelligence is rapidly encroaching on the field of service\nregulation. However, existing AI-based regulation techniques are often tailored\nto specific application domains and thus are difficult to generalize in an\nautomated manner. This paper presents Horae, a unified specification language\nfor modeling (multimodal) regulation rules across a diverse set of domains. We\nshowcase how Horae facilitates an intelligent service regulation pipeline by\nfurther exploiting a fine-tuned large language model named RuleGPT that\nautomates the Horae modeling process, thereby yielding an end-to-end framework\nfor fully automated intelligent service regulation. The feasibility and\neffectiveness of our framework are demonstrated over a benchmark of various\nreal-world regulation domains. In particular, we show that our open-sourced,\nfine-tuned RuleGPT with 7B parameters suffices to outperform GPT-3.5 and\nperform on par with GPT-4o.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Full version of IJCAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.06600v5",
    "published_date": "2024-06-06 13:44:57 UTC",
    "updated_date": "2025-05-09 07:06:46 UTC"
  },
  {
    "arxiv_id": "2406.04070v1",
    "title": "Batch-in-Batch: a new adversarial training framework for initial perturbation and sample selection",
    "authors": [
      "Yinting Wu",
      "Pai Peng",
      "Bo Cai",
      "Le Li",
      "."
    ],
    "abstract": "Adversarial training methods commonly generate independent initial\nperturbation for adversarial samples from a simple uniform distribution, and\nobtain the training batch for the classifier without selection. In this work,\nwe propose a simple yet effective training framework called Batch-in-Batch (BB)\nto enhance models robustness. It involves specifically a joint construction of\ninitial values that could simultaneously generates $m$ sets of perturbations\nfrom the original batch set to provide more diversity for adversarial samples;\nand also includes various sample selection strategies that enable the trained\nmodels to have smoother losses and avoid overconfident outputs. Through\nextensive experiments on three benchmark datasets (CIFAR-10, SVHN, CIFAR-100)\nwith two networks (PreActResNet18 and WideResNet28-10) that are used in both\nthe single-step (Noise-Fast Gradient Sign Method, N-FGSM) and multi-step\n(Projected Gradient Descent, PGD-10) adversarial training, we show that models\ntrained within the BB framework consistently have higher adversarial accuracy\nacross various adversarial settings, notably achieving over a 13% improvement\non the SVHN dataset with an attack radius of 8/255 compared to the N-FGSM\nbaseline model. Furthermore, experimental analysis of the efficiency of both\nthe proposed initial perturbation method and sample selection strategies\nvalidates our insights. Finally, we show that our framework is cost-effective\nin terms of computational resources, even with a relatively large value of $m$.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.04070v1",
    "published_date": "2024-06-06 13:34:43 UTC",
    "updated_date": "2024-06-06 13:34:43 UTC"
  },
  {
    "arxiv_id": "2406.04064v1",
    "title": "Ask LLMs Directly, \"What shapes your bias?\": Measuring Social Bias in Large Language Models",
    "authors": [
      "Jisu Shin",
      "Hoyun Song",
      "Huije Lee",
      "Soyeong Jeong",
      "Jong C. Park"
    ],
    "abstract": "Social bias is shaped by the accumulation of social perceptions towards\ntargets across various demographic identities. To fully understand such social\nbias in large language models (LLMs), it is essential to consider the composite\nof social perceptions from diverse perspectives among identities. Previous\nstudies have either evaluated biases in LLMs by indirectly assessing the\npresence of sentiments towards demographic identities in the generated text or\nmeasuring the degree of alignment with given stereotypes. These methods have\nlimitations in directly quantifying social biases at the level of distinct\nperspectives among identities. In this paper, we aim to investigate how social\nperceptions from various viewpoints contribute to the development of social\nbias in LLMs. To this end, we propose a novel strategy to intuitively quantify\nthese social perceptions and suggest metrics that can evaluate the social\nbiases within LLMs by aggregating diverse social perceptions. The experimental\nresults show the quantitative demonstration of the social attitude in LLMs by\nexamining social perception. The analysis we conducted shows that our proposed\nmetrics capture the multi-dimensional aspects of social bias, enabling a\nfine-grained and comprehensive investigation of bias in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Findings of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.04064v1",
    "published_date": "2024-06-06 13:32:09 UTC",
    "updated_date": "2024-06-06 13:32:09 UTC"
  },
  {
    "arxiv_id": "2406.04055v1",
    "title": "Leveraging SPD Matrices on Riemannian Manifolds in Quantum Classical Hybrid Models for Structural Health Monitoring",
    "authors": [
      "Azadeh Alavi",
      "Sanduni Jayasinghe"
    ],
    "abstract": "Realtime finite element modeling of bridges assists modern structural health\nmonitoring systems by providing comprehensive insights into structural\nintegrity. This capability is essential for ensuring the safe operation of\nbridges and preventing sudden catastrophic failures. However, FEM computational\ncost and the need for realtime analysis pose significant challenges.\nAdditionally, the input data is a 7 dimensional vector, while the output is a\n1017 dimensional vector, making accurate and efficient analysis particularly\ndifficult. In this study, we propose a novel hybrid quantum classical\nMultilayer Perceptron pipeline leveraging Symmetric Positive Definite matrices\nand Riemannian manifolds for effective data representation. To maintain the\nintegrity of the qubit structure, we utilize SPD matrices, ensuring data\nrepresentation is well aligned with the quantum computational framework.\nAdditionally, the method leverages polynomial feature expansion to capture\nnonlinear relationships within the data. The proposed pipeline combines\nclassical fully connected neural network layers with quantum circuit layers to\nenhance model performance and efficiency. Our experiments focused on various\nconfigurations of such hybrid models to identify the optimal structure for\naccurate and efficient realtime analysis. The best performing model achieved a\nMean Squared Error of 0.00031, significantly outperforming traditional methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "3 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2406.04055v1",
    "published_date": "2024-06-06 13:21:28 UTC",
    "updated_date": "2024-06-06 13:21:28 UTC"
  },
  {
    "arxiv_id": "2406.04052v2",
    "title": "Multivector Neurons: Better and Faster O(n)-Equivariant Clifford Graph Neural Networks",
    "authors": [
      "Cong Liu",
      "David Ruhe",
      "Patrick Forré"
    ],
    "abstract": "Most current deep learning models equivariant to $O(n)$ or $SO(n)$ either\nconsider mostly scalar information such as distances and angles or have a very\nhigh computational complexity. In this work, we test a few novel message\npassing graph neural networks (GNNs) based on Clifford multivectors, structured\nsimilarly to other prevalent equivariant models in geometric deep learning. Our\napproach leverages efficient invariant scalar features while simultaneously\nperforming expressive learning on multivector representations, particularly\nthrough the use of the equivariant geometric product operator. By integrating\nthese elements, our methods outperform established efficient baseline models on\nan N-Body simulation task and protein denoising task while maintaining a high\nefficiency. In particular, we push the state-of-the-art error on the N-body\ndataset to 0.0035 (averaged over 3 runs); an 8% improvement over recent\nmethods. Our implementation is available on Github.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04052v2",
    "published_date": "2024-06-06 13:17:44 UTC",
    "updated_date": "2024-07-10 11:24:42 UTC"
  },
  {
    "arxiv_id": "2406.04046v3",
    "title": "ActionReasoningBench: Reasoning about Actions with and without Ramification Constraints",
    "authors": [
      "Divij Handa",
      "Pavel Dolin",
      "Shrinidhi Kumbhar",
      "Tran Cao Son",
      "Chitta Baral"
    ],
    "abstract": "Reasoning about Actions and Change (RAC) has historically played a pivotal\nrole in solving foundational AI problems, such as the frame problem. It has\ndriven advancements in AI fields, such as non-monotonic and commonsense\nreasoning. RAC remains crucial for AI systems that operate in dynamic\nenvironments, engage in interactive scenarios, or rely on commonsense\nreasoning. Despite substantial advances made by Large Language Models (LLMs) in\nvarious AI domains, their performance in RAC remains underexplored. To address\nthis gap, we introduce a new diagnostic benchmark, ActionReasoningBench, which\nencompasses 8 domains and includes questions for up to 19 action sequences.\nThis benchmark rigorously evaluates LLMs across six key RAC dimensions: Fluent\nTracking, State Tracking, Action Executability, Effects of Actions, Numerical\nRAC, and Composite Questions. LLMs demonstrate average accuracy rates of\n73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are\nfrequently discussed in RAC literature. However, the performance on the latter\ntwo dimensions, which introduce complex and novel reasoning questions, the\naverage performance of LLMs is lowered to 33.16% and 51.19%, respectively,\nreflecting a 17.9% performance decline. We also introduce new ramification\nconstraints to capture the indirect effects of actions, providing deeper\ninsights into RAC challenges. Our evaluation of state-of-the-art LLMs,\nincluding both open-source and commercial models, reveals challenges across all\nRAC dimensions, particularly in handling ramifications, with GPT-4o failing to\nsolve any question and o1-preview achieving a score of only 18.4%.",
    "categories": [
      "cs.CC",
      "cs.AI"
    ],
    "primary_category": "cs.CC",
    "comment": "Accepted in ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.04046v3",
    "published_date": "2024-06-06 13:15:37 UTC",
    "updated_date": "2025-03-02 23:24:43 UTC"
  },
  {
    "arxiv_id": "2406.04039v1",
    "title": "Shaping History: Advanced Machine Learning Techniques for the Analysis and Dating of Cuneiform Tablets over Three Millennia",
    "authors": [
      "Danielle Kapon",
      "Michael Fire",
      "Shai Gordin"
    ],
    "abstract": "Cuneiform tablets, emerging in ancient Mesopotamia around the late fourth\nmillennium BCE, represent one of humanity's earliest writing systems.\nCharacterized by wedge-shaped marks on clay tablets, these artifacts provided\ninsight into Mesopotamian civilization across various domains. Traditionally,\nthe analysis and dating of these tablets rely on subjective assessment of shape\nand writing style, leading to uncertainties in pinpointing their exact temporal\norigins. Recent advances in digitization have revolutionized the study of\ncuneiform by enhancing accessibility and analytical capabilities. Our research\nuniquely focuses on the silhouette of tablets as significant indicators of\ntheir historical periods, diverging from most studies that concentrate on\ntextual content. Utilizing an unprecedented dataset of over 94,000 images from\nthe Cuneiform Digital Library Initiative collection, we apply deep learning\nmethods to classify cuneiform tablets, covering over 3,000 years of history. By\nleveraging statistical, computational techniques, and generative modeling\nthrough Variational Auto-Encoders (VAEs), we achieve substantial advancements\nin the automatic classification of these ancient documents, focusing on the\ntablets' silhouettes as key predictors. Our classification approach begins with\na Decision Tree using height-to-width ratios and culminates with a ResNet50\nmodel, achieving a 61% macro F1-score for tablet silhouettes. Moreover, we\nintroduce novel VAE-powered tools to enhance explainability and enable\nresearchers to explore changes in tablet shapes across different eras and\ngenres. This research contributes to document analysis and diplomatics by\ndemonstrating the value of large-scale data analysis combined with statistical\nmethods. These insights offer valuable tools for historians and epigraphists,\nenriching our understanding of cuneiform tablets and the cultures that produced\nthem.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SI",
      "I.4.10; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.04039v1",
    "published_date": "2024-06-06 13:05:32 UTC",
    "updated_date": "2024-06-06 13:05:32 UTC"
  },
  {
    "arxiv_id": "2406.04035v3",
    "title": "STEMO: Early Spatio-temporal Forecasting with Multi-Objective Reinforcement Learning",
    "authors": [
      "Wei Shao",
      "Yufan Kang",
      "Ziyan Peng",
      "Xiao Xiao",
      "Lei Wang",
      "Yuhui Yang",
      "Flora D Salim"
    ],
    "abstract": "Accuracy and timeliness are indeed often conflicting goals in prediction\ntasks. Premature predictions may yield a higher rate of false alarms, whereas\ndelaying predictions to gather more information can render them too late to be\nuseful. In applications such as wildfires, crimes, and traffic jams, timely\nforecasting are vital for safeguarding human life and property. Consequently,\nfinding a balance between accuracy and timeliness is crucial. In this paper, we\npropose an early spatio-temporal forecasting model based on Multi-Objective\nreinforcement learning that can either implement an optimal policy given a\npreference or infer the preference based on a small number of samples. The\nmodel addresses two primary challenges: 1) enhancing the accuracy of early\nforecasting and 2) providing the optimal policy for determining the most\nsuitable prediction time for each area. Our method demonstrates superior\nperformance on three large-scale real-world datasets, surpassing existing\nmethods in early spatio-temporal forecasting tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted paper in KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.04035v3",
    "published_date": "2024-06-06 13:03:51 UTC",
    "updated_date": "2024-06-18 09:16:33 UTC"
  },
  {
    "arxiv_id": "2406.04029v2",
    "title": "Pre-trained Transformer Uncovers Meaningful Patterns in Human Mobility Data",
    "authors": [
      "Alameen Najjar"
    ],
    "abstract": "We empirically demonstrate that a transformer pre-trained on country-scale\nunlabeled human mobility data learns embeddings capable, through fine-tuning,\nof developing a deep understanding of the target geography and its\ncorresponding mobility patterns. Utilizing an adaptation framework, we evaluate\nthe performance of our pre-trained embeddings in encapsulating a broad spectrum\nof concepts directly and indirectly related to human mobility. This includes\nbasic notions, such as geographic location and distance, and extends to more\ncomplex constructs, such as administrative divisions and land cover. Our\nextensive empirical analysis reveals a substantial performance boost gained\nfrom pre-training, reaching up to 38% in tasks such as tree-cover regression.\nWe attribute this result to the ability of the pre-training to uncover\nmeaningful patterns hidden in the raw data, beneficial for modeling relevant\nhigh-level concepts. The pre-trained embeddings emerge as robust\nrepresentations of regions and trajectories, potentially valuable for a wide\nrange of downstream applications.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted to the 8th IEEE International Workshop on Big Spatial Data @\n  IEEE BigData 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.04029v2",
    "published_date": "2024-06-06 12:59:46 UTC",
    "updated_date": "2024-12-12 03:19:59 UTC"
  },
  {
    "arxiv_id": "2406.04028v1",
    "title": "Contrastive Sparse Autoencoders for Interpreting Planning of Chess-Playing Agents",
    "authors": [
      "Yoann Poupart"
    ],
    "abstract": "AI led chess systems to a superhuman level, yet these systems heavily rely on\nblack-box algorithms. This is unsustainable in ensuring transparency to the\nend-user, particularly when these systems are responsible for sensitive\ndecision-making. Recent interpretability work has shown that the inner\nrepresentations of Deep Neural Networks (DNNs) were fathomable and contained\nhuman-understandable concepts. Yet, these methods are seldom contextualised and\nare often based on a single hidden state, which makes them unable to interpret\nmulti-step reasoning, e.g. planning. In this respect, we propose contrastive\nsparse autoencoders (CSAE), a novel framework for studying pairs of game\ntrajectories. Using CSAE, we are able to extract and interpret concepts that\nare meaningful to the chess-agent plans. We primarily focused on a qualitative\nanalysis of the CSAE features before proposing an automated feature taxonomy.\nFurthermore, to evaluate the quality of our trained CSAE, we devise sanity\nchecks to wave spurious correlations in our results.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Worskhop on Interpretable Policies in Reinforcement Learning @\n  RLC-2024, 18 pages and 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.04028v1",
    "published_date": "2024-06-06 12:57:31 UTC",
    "updated_date": "2024-06-06 12:57:31 UTC"
  },
  {
    "arxiv_id": "2406.04024v2",
    "title": "American Sign Language Handshapes Reflect Pressures for Communicative Efficiency",
    "authors": [
      "Kayo Yin",
      "Terry Regier",
      "Dan Klein"
    ],
    "abstract": "Communicative efficiency is a key topic in linguistics and cognitive\npsychology, with many studies demonstrating how the pressure to communicate\nwith minimal effort guides the form of natural language. However, this\nphenomenon is rarely explored in signed languages. This paper shows how\nhandshapes in American Sign Language (ASL) reflect these efficiency pressures\nand provides new evidence of communicative efficiency in the visual-gestural\nmodality.\n  We focus on hand configurations in native ASL signs and signs borrowed from\nEnglish to compare efficiency pressures from both ASL and English usage. First,\nwe develop new methodologies to quantify the articulatory effort needed to\nproduce handshapes and the perceptual effort required to recognize them. Then,\nwe analyze correlations between communicative effort and usage statistics in\nASL or English. Our findings reveal that frequent ASL handshapes are easier to\nproduce and that pressures for communicative efficiency mostly come from ASL\nusage, rather than from English lexical borrowing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.04024v2",
    "published_date": "2024-06-06 12:46:21 UTC",
    "updated_date": "2024-06-10 13:45:36 UTC"
  },
  {
    "arxiv_id": "2406.03997v1",
    "title": "HackAtari: Atari Learning Environments for Robust and Continual Reinforcement Learning",
    "authors": [
      "Quentin Delfosse",
      "Jannis Blüml",
      "Bjarne Gregori",
      "Kristian Kersting"
    ],
    "abstract": "Artificial agents' adaptability to novelty and alignment with intended\nbehavior is crucial for their effective deployment. Reinforcement learning (RL)\nleverages novelty as a means of exploration, yet agents often struggle to\nhandle novel situations, hindering generalization. To address these issues, we\npropose HackAtari, a framework introducing controlled novelty to the most\ncommon RL benchmark, the Atari Learning Environment. HackAtari allows us to\ncreate novel game scenarios (including simplification for curriculum learning),\nto swap the game elements' colors, as well as to introduce different reward\nsignals for the agent. We demonstrate that current agents trained on the\noriginal environments include robustness failures, and evaluate HackAtari's\nefficacy in enhancing RL agents' robustness and aligning behavior through\nexperiments using C51 and PPO. Overall, HackAtari can be used to improve the\nrobustness of current and future RL algorithms, allowing Neuro-Symbolic RL,\ncurriculum RL, causal RL, as well as LLM-driven RL. Our work underscores the\nsignificance of developing interpretable in RL agents.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "9 main pages, 4 pages references, 19 pages of appendix",
    "pdf_url": "http://arxiv.org/pdf/2406.03997v1",
    "published_date": "2024-06-06 12:17:05 UTC",
    "updated_date": "2024-06-06 12:17:05 UTC"
  },
  {
    "arxiv_id": "2406.03995v1",
    "title": "AC4MPC: Actor-Critic Reinforcement Learning for Nonlinear Model Predictive Control",
    "authors": [
      "Rudolf Reiter",
      "Andrea Ghezzi",
      "Katrin Baumgärtner",
      "Jasper Hoffmann",
      "Robert D. McAllister",
      "Moritz Diehl"
    ],
    "abstract": "\\Ac{MPC} and \\ac{RL} are two powerful control strategies with, arguably,\ncomplementary advantages. In this work, we show how actor-critic \\ac{RL}\ntechniques can be leveraged to improve the performance of \\ac{MPC}. The \\ac{RL}\ncritic is used as an approximation of the optimal value function, and an actor\nroll-out provides an initial guess for primal variables of the \\ac{MPC}. A\nparallel control architecture is proposed where each \\ac{MPC} instance is\nsolved twice for different initial guesses. Besides the actor roll-out\ninitialization, a shifted initialization from the previous solution is used.\nThereafter, the actor and the critic are again used to approximately evaluate\nthe infinite horizon cost of these trajectories. The control actions from the\nlowest-cost trajectory are applied to the system at each time step. We\nestablish that the proposed algorithm is guaranteed to outperform the original\n\\ac{RL} policy plus an error term that depends on the accuracy of the critic\nand decays with the horizon length of the \\ac{MPC} formulation. Moreover, we do\nnot require globally optimal solutions for these guarantees to hold. The\napproach is demonstrated on an illustrative toy example and an \\ac{AD}\novertaking scenario.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03995v1",
    "published_date": "2024-06-06 12:15:51 UTC",
    "updated_date": "2024-06-06 12:15:51 UTC"
  },
  {
    "arxiv_id": "2406.03947v1",
    "title": "Weight-based Decomposition: A Case for Bilinear MLPs",
    "authors": [
      "Michael T. Pearce",
      "Thomas Dooms",
      "Alice Rigg"
    ],
    "abstract": "Gated Linear Units (GLUs) have become a common building block in modern\nfoundation models. Bilinear layers drop the non-linearity in the \"gate\" but\nstill have comparable performance to other GLUs. An attractive quality of\nbilinear layers is that they can be fully expressed in terms of a third-order\ntensor and linear operations. Leveraging this, we develop a method to decompose\nthe bilinear tensor into a set of sparsely interacting eigenvectors that show\npromising interpretability properties in preliminary experiments for shallow\nimage classifiers (MNIST) and small language models (Tiny Stories). Since the\ndecomposition is fully equivalent to the model's original computations,\nbilinear layers may be an interpretability-friendly architecture that helps\nconnect features to the model weights. Application of our method may not be\nlimited to pretrained bilinear models since we find that language models such\nas TinyLlama-1.1B can be finetuned into bilinear variants.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03947v1",
    "published_date": "2024-06-06 10:46:51 UTC",
    "updated_date": "2024-06-06 10:46:51 UTC"
  },
  {
    "arxiv_id": "2406.06599v1",
    "title": "Anna Karenina Strikes Again: Pre-Trained LLM Embeddings May Favor High-Performing Learners",
    "authors": [
      "Abigail Gurin Schleifer",
      "Beata Beigman Klebanov",
      "Moriah Ariely",
      "Giora Alexandron"
    ],
    "abstract": "Unsupervised clustering of student responses to open-ended questions into\nbehavioral and cognitive profiles using pre-trained LLM embeddings is an\nemerging technique, but little is known about how well this captures\npedagogically meaningful information. We investigate this in the context of\nstudent responses to open-ended questions in biology, which were previously\nanalyzed and clustered by experts into theory-driven Knowledge Profiles (KPs).\nComparing these KPs to ones discovered by purely data-driven clustering\ntechniques, we report poor discoverability of most KPs, except for the ones\nincluding the correct answers. We trace this \"discoverability bias\" to the\nrepresentations of KPs in the pre-trained LLM embeddings space.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages (not including bibliography), Appendix and 10 tables.\n  Accepted to the 19th Workshop on Innovative Use of NLP for Building\n  Educational Applications, Co-located with NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.06599v1",
    "published_date": "2024-06-06 10:36:48 UTC",
    "updated_date": "2024-06-06 10:36:48 UTC"
  },
  {
    "arxiv_id": "2406.16911v1",
    "title": "Evaluating the Influence of Temporal Context on Automatic Mouse Sleep Staging through the Application of Human Models",
    "authors": [
      "Javier García Ciudad",
      "Morten Mørup",
      "Birgitte Rahbek Kornum",
      "Alexander Neergaard Zahid"
    ],
    "abstract": "In human sleep staging models, augmenting the temporal context of the input\nto the range of tens of minutes has recently demonstrated performance\nimprovement. In contrast, the temporal context of mouse sleep staging models is\ntypically in the order of tens of seconds. While long-term time patterns are\nless clear in mouse sleep, increasing the temporal context further than that of\nthe current mouse sleep staging models might still result in a performance\nincrease, given that the current methods only model very short term patterns.\nIn this study, we examine the influence of increasing the temporal context in\nmouse sleep staging up to 15 minutes in three mouse cohorts using two recent\nand high-performing human sleep staging models that account for long-term\ndependencies. These are compared to two prominent mouse sleep staging models\nthat use a local context of 12 s and 20 s, respectively. An increase in context\nup to 28 s is observed to have a positive impact on sleep stage classification\nperformance, especially in REM sleep. However, the impact is limited for longer\ncontext windows. One of the human sleep scoring models, L-SeqSleepNet,\noutperforms both mouse models in all cohorts. This suggests that mouse sleep\nstaging can benefit from more temporal context than currently used.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Accepted for publication in the 46th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.16911v1",
    "published_date": "2024-06-06 10:07:19 UTC",
    "updated_date": "2024-06-06 10:07:19 UTC"
  },
  {
    "arxiv_id": "2406.03919v2",
    "title": "Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations",
    "authors": [
      "Jan Hagnberger",
      "Marimuthu Kalimuthu",
      "Daniel Musekamp",
      "Mathias Niepert"
    ],
    "abstract": "Transformer models are increasingly used for solving Partial Differential\nEquations (PDEs). Several adaptations have been proposed, all of which suffer\nfrom the typical problems of Transformers, such as quadratic memory and time\ncomplexity. Furthermore, all prevalent architectures for PDE solving lack at\nleast one of several desirable properties of an ideal surrogate model, such as\n(i) generalization to PDE parameters not seen during training, (ii) spatial and\ntemporal zero-shot super-resolution, (iii) continuous temporal extrapolation,\n(iv) support for 1D, 2D, and 3D PDEs, and (v) efficient inference for longer\ntemporal rollouts. To address these limitations, we propose Vectorized\nConditional Neural Fields (VCNeFs), which represent the solution of\ntime-dependent PDEs as neural fields. Contrary to prior methods, however,\nVCNeFs compute, for a set of multiple spatio-temporal query points, their\nsolutions in parallel and model their dependencies through attention\nmechanisms. Moreover, VCNeF can condition the neural field on both the initial\nconditions and the parameters of the PDEs. An extensive set of experiments\ndemonstrates that VCNeFs are competitive with and often outperform existing\nML-based surrogate models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at the 41st International Conference on\n  Machine Learning (ICML) 2024, Vienna, Austria; Project Page:\n  https://jhagnberger.github.io/vectorized-conditional-neural-field/",
    "pdf_url": "http://arxiv.org/pdf/2406.03919v2",
    "published_date": "2024-06-06 10:02:06 UTC",
    "updated_date": "2024-07-13 12:32:25 UTC"
  },
  {
    "arxiv_id": "2406.03916v2",
    "title": "ArMeme: Propagandistic Content in Arabic Memes",
    "authors": [
      "Firoj Alam",
      "Abul Hasnat",
      "Fatema Ahmed",
      "Md Arid Hasan",
      "Maram Hasanain"
    ],
    "abstract": "With the rise of digital communication, memes have become a significant\nmedium for cultural and political expression that is often used to mislead\naudiences. Identification of such misleading and persuasive multimodal content\nhas become more important among various stakeholders, including social media\nplatforms, policymakers, and the broader society as they often cause harm to\nindividuals, organizations, and/or society. While there has been effort to\ndevelop AI-based automatic systems for resource-rich languages (e.g., English),\nit is relatively little to none for medium to low resource languages. In this\nstudy, we focused on developing an Arabic memes dataset with manual annotations\nof propagandistic content. We annotated ~6K Arabic memes collected from various\nsocial media platforms, which is a first resource for Arabic multimodal\nresearch. We provide a comprehensive analysis aiming to develop computational\ntools for their detection. We will make them publicly available for the\ncommunity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "disinformation, misinformation, factuality, harmfulness, fake news,\n  propaganda, multimodality, text, images",
    "pdf_url": "http://arxiv.org/pdf/2406.03916v2",
    "published_date": "2024-06-06 09:56:49 UTC",
    "updated_date": "2024-10-06 08:35:10 UTC"
  },
  {
    "arxiv_id": "2406.03912v2",
    "title": "GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning Algorithms Based on Reduced Order Markov Decision Process Model",
    "authors": [
      "Zhehua Zhou",
      "Xuan Xie",
      "Jiayang Song",
      "Zhan Shu",
      "Lei Ma"
    ],
    "abstract": "Safe Reinforcement Learning (SRL) aims to realize a safe learning process for\nDeep Reinforcement Learning (DRL) algorithms by incorporating safety\nconstraints. However, the efficacy of SRL approaches often relies on accurate\nfunction approximations, which are notably challenging to achieve in the early\nlearning stages due to data insufficiency. To address this issue, we introduce\nin this work a novel Generalizable Safety enhancer (GenSafe) that is able to\novercome the challenge of data insufficiency and enhance the performance of SRL\napproaches. Leveraging model order reduction techniques, we first propose an\ninnovative method to construct a Reduced Order Markov Decision Process (ROMDP)\nas a low-dimensional approximator of the original safety constraints. Then, by\nsolving the reformulated ROMDP-based constraints, GenSafe refines the actions\nof the agent to increase the possibility of constraint satisfaction.\nEssentially, GenSafe acts as an additional safety layer for SRL algorithms. We\nevaluate GenSafe on multiple SRL approaches and benchmark problems. The results\ndemonstrate its capability to improve safety performance, especially in the\nearly learning phases, while maintaining satisfactory task performance. Our\nproposed GenSafe not only offers a novel measure to augment existing SRL\nmethods but also shows broad compatibility with various SRL algorithms, making\nit applicable to a wide range of systems and SRL problems.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03912v2",
    "published_date": "2024-06-06 09:51:30 UTC",
    "updated_date": "2025-01-14 10:32:32 UTC"
  },
  {
    "arxiv_id": "2406.14570v2",
    "title": "Deep-Learning Approach for Tissue Classification using Acoustic Waves during Ablation with an Er:YAG Laser (Updated)",
    "authors": [
      "Carlo Seppi",
      "Philippe C. Cattin"
    ],
    "abstract": "Today's mechanical tools for bone cutting (osteotomy) cause mechanical trauma\nthat prolongs the healing process. Medical device manufacturers aim to minimize\nthis trauma, with minimally invasive surgery using laser cutting as one\ninnovation. This method ablates tissue using laser light instead of mechanical\ntools, reducing post-surgery healing time. A reliable feedback system is\ncrucial during laser surgery to prevent damage to surrounding tissues. We\npropose a tissue classification method analyzing acoustic waves generated\nduring laser ablation, demonstrating its applicability in an ex-vivo\nexperiment. The ablation process with a microsecond pulsed Er:YAG laser\nproduces acoustic waves, acquired with an air-coupled transducer. These waves\nwere used to classify five porcine tissue types: hard bone, soft bone, muscle,\nfat, and skin. For automated tissue classification, we compared five Neural\nNetwork (NN) approaches: a one-dimensional Convolutional Neural Network (CNN)\nwith time-dependent input, a Fully-connected Neural Network (FcNN) with either\nthe frequency spectrum or principal components of the frequency spectrum as\ninput, and a combination of a CNN and an FcNN with time-dependent data and its\nfrequency spectrum as input. Consecutive acoustic waves were used to improve\nclassification accuracy. Grad-Cam identified the activation map of the\nfrequencies, showing low frequencies as the most important for this task. Our\nresults indicated that combining time-dependent data with its frequency\nspectrum achieved the highest classification accuracy (65.5%-75.5%). We also\nfound that using the frequency spectrum alone was sufficient, with no\nadditional benefit from applying Principal Components Analysis (PCA).",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "eess.IV",
      "q-bio.TO"
    ],
    "primary_category": "physics.med-ph",
    "comment": "This paper is an updated version of Deep-Learning Approach for Tissue\n  Classification using Acoustic Waves during Ablation with an Er:YAG Laser\n  originally published in DOI:10.1109/ACCESS.2021.3113055. This update\n  addresses several issues and incorporates corrections as outlined in\n  DOI:10.1109/ACCESS.2024.3395071. We provide here a detailed description of\n  our experiments and the new models we used",
    "pdf_url": "http://arxiv.org/pdf/2406.14570v2",
    "published_date": "2024-06-06 09:46:14 UTC",
    "updated_date": "2024-06-24 09:25:33 UTC"
  },
  {
    "arxiv_id": "2406.03897v2",
    "title": "HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew",
    "authors": [
      "Tzuf Paz-Argaman",
      "Itai Mondshine",
      "Asaf Achi Mordechai",
      "Reut Tsarfaty"
    ],
    "abstract": "While large language models (LLMs) excel in various natural language tasks in\nEnglish, their performance in lower-resourced languages like Hebrew, especially\nfor generative tasks such as abstractive summarization, remains unclear. The\nhigh morphological richness in Hebrew adds further challenges due to the\nambiguity in sentence comprehension and the complexities in meaning\nconstruction. In this paper, we address this resource and evaluation gap by\nintroducing HeSum, a novel benchmark specifically designed for abstractive text\nsummarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs\nsourced from Hebrew news websites written by professionals. Linguistic analysis\nconfirms HeSum's high abstractness and unique morphological challenges. We show\nthat HeSum presents distinct difficulties for contemporary state-of-the-art\nLLMs, establishing it as a valuable testbed for generative language technology\nin Hebrew, and MRLs generative challenges in general.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03897v2",
    "published_date": "2024-06-06 09:36:14 UTC",
    "updated_date": "2024-06-10 05:45:25 UTC"
  },
  {
    "arxiv_id": "2406.06598v1",
    "title": "Qabas: An Open-Source Arabic Lexicographic Database",
    "authors": [
      "Mustafa Jarrar",
      "Tymaa Hammouda"
    ],
    "abstract": "We present Qabas, a novel open-source Arabic lexicon designed for NLP\napplications. The novelty of Qabas lies in its synthesis of 110 lexicons.\nSpecifically, Qabas lexical entries (lemmas) are assembled by linking lemmas\nfrom 110 lexicons. Furthermore, Qabas lemmas are also linked to 12\nmorphologically annotated corpora (about 2M tokens), making it the first Arabic\nlexicon to be linked to lexicons and corpora. Qabas was developed\nsemi-automatically, utilizing a mapping framework and a web-based tool.\nCompared with other lexicons, Qabas stands as the most extensive Arabic\nlexicon, encompassing about 58K lemmas (45K nominal lemmas, 12.5K verbal\nlemmas, and 473 functional-word lemmas). Qabas is open-source and accessible\nonline at https://sina.birzeit.edu/qabas.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06598v1",
    "published_date": "2024-06-06 09:25:36 UTC",
    "updated_date": "2024-06-06 09:25:36 UTC"
  },
  {
    "arxiv_id": "2406.03880v1",
    "title": "Memorization in deep learning: A survey",
    "authors": [
      "Jiaheng Wei",
      "Yanjun Zhang",
      "Leo Yu Zhang",
      "Ming Ding",
      "Chao Chen",
      "Kok-Leong Ong",
      "Jun Zhang",
      "Yang Xiang"
    ],
    "abstract": "Deep Learning (DL) powered by Deep Neural Networks (DNNs) has revolutionized\nvarious domains, yet understanding the intricacies of DNN decision-making and\nlearning processes remains a significant challenge. Recent investigations have\nuncovered an interesting memorization phenomenon in which DNNs tend to memorize\nspecific details from examples rather than learning general patterns, affecting\nmodel generalization, security, and privacy. This raises critical questions\nabout the nature of generalization in DNNs and their susceptibility to security\nbreaches. In this survey, we present a systematic framework to organize\nmemorization definitions based on the generalization and security/privacy\ndomains and summarize memorization evaluation methods at both the example and\nmodel levels. Through a comprehensive literature review, we explore DNN\nmemorization behaviors and their impacts on security and privacy. We also\nintroduce privacy vulnerabilities caused by memorization and the phenomenon of\nforgetting and explore its connection with memorization. Furthermore, we\nspotlight various applications leveraging memorization and forgetting\nmechanisms, including noisy label learning, privacy preservation, and model\nenhancement. This survey offers the first-in-kind understanding of memorization\nin DNNs, providing insights into its challenges and opportunities for enhancing\nAI development while addressing critical ethical concerns.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03880v1",
    "published_date": "2024-06-06 09:17:40 UTC",
    "updated_date": "2024-06-06 09:17:40 UTC"
  },
  {
    "arxiv_id": "2406.03873v3",
    "title": "Quantum Implicit Neural Representations",
    "authors": [
      "Jiaming Zhao",
      "Wenbo Qiao",
      "Peng Zhang",
      "Hui Gao"
    ],
    "abstract": "Implicit neural representations have emerged as a powerful paradigm to\nrepresent signals such as images and sounds. This approach aims to utilize\nneural networks to parameterize the implicit function of the signal. However,\nwhen representing implicit functions, traditional neural networks such as\nReLU-based multilayer perceptrons face challenges in accurately modeling\nhigh-frequency components of signals. Recent research has begun to explore the\nuse of Fourier Neural Networks (FNNs) to overcome this limitation. In this\npaper, we propose Quantum Implicit Representation Network (QIREN), a novel\nquantum generalization of FNNs. Furthermore, through theoretical analysis, we\ndemonstrate that QIREN possesses a quantum advantage over classical FNNs.\nLastly, we conducted experiments in signal representation, image\nsuperresolution, and image generation tasks to show the superior performance of\nQIREN compared to state-of-the-art (SOTA) models. Our work not only\nincorporates quantum advantages into implicit neural representations but also\nuncovers a promising application direction for Quantum Neural Networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper was accepted by icml 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.03873v3",
    "published_date": "2024-06-06 09:04:48 UTC",
    "updated_date": "2024-09-01 09:56:44 UTC"
  },
  {
    "arxiv_id": "2406.18585v1",
    "title": "Flexible ViG: Learning the Self-Saliency for Flexible Object Recognition",
    "authors": [
      "Lin Zuo",
      "Kunshan Yang",
      "Xianlong Tian",
      "Kunbin He",
      "Yongqi Ding",
      "Mengmeng Jing"
    ],
    "abstract": "Existing computer vision methods mainly focus on the recognition of rigid\nobjects, whereas the recognition of flexible objects remains unexplored.\nRecognizing flexible objects poses significant challenges due to their\ninherently diverse shapes and sizes, translucent attributes, ambiguous\nboundaries, and subtle inter-class differences. In this paper, we claim that\nthese problems primarily arise from the lack of object saliency. To this end,\nwe propose the Flexible Vision Graph Neural Network (FViG) to optimize the\nself-saliency and thereby improve the discrimination of the representations for\nflexible objects. Specifically, on one hand, we propose to maximize the\nchannel-aware saliency by extracting the weight of neighboring nodes, which\nadapts to the shape and size variations in flexible objects. On the other hand,\nwe maximize the spatial-aware saliency based on clustering to aggregate\nneighborhood information for the centroid nodes, which introduces local context\ninformation for the representation learning. To verify the performance of\nflexible objects recognition thoroughly, for the first time we propose the\nFlexible Dataset (FDA), which consists of various images of flexible objects\ncollected from real-world scenarios or online. Extensive experiments evaluated\non our Flexible Dataset demonstrate the effectiveness of our method on\nenhancing the discrimination of flexible objects.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2406.18585v1",
    "published_date": "2024-06-06 08:55:06 UTC",
    "updated_date": "2024-06-06 08:55:06 UTC"
  },
  {
    "arxiv_id": "2406.03865v2",
    "title": "Semantic Similarity Score for Measuring Visual Similarity at Semantic Level",
    "authors": [
      "Senran Fan",
      "Zhicheng Bao",
      "Chen Dong",
      "Haotai Liang",
      "Xiaodong Xu",
      "Ping Zhang"
    ],
    "abstract": "Semantic communication, as a revolutionary communication architecture, is\nconsidered a promising novel communication paradigm. Unlike traditional\nsymbol-based error-free communication systems, semantic-based visual\ncommunication systems extract, compress, transmit, and reconstruct images at\nthe semantic level. However, widely used image similarity evaluation metrics,\nwhether pixel-based MSE or PSNR or structure-based MS-SSIM, struggle to\naccurately measure the loss of semantic-level information of the source during\nsystem transmission. This presents challenges in evaluating the performance of\nvisual semantic communication systems, especially when comparing them with\ntraditional communication systems. To address this, we propose a semantic\nevaluation metric -- SeSS (Semantic Similarity Score), based on Scene Graph\nGeneration and graph matching, which shifts the similarity scores between\nimages into semantic-level graph matching scores. Meanwhile, semantic\nsimilarity scores for tens of thousands of image pairs are manually annotated\nto fine-tune the hyperparameters in the graph matching algorithm, aligning the\nmetric more closely with human semantic perception. The performance of the SeSS\nis tested on different datasets, including (1)images transmitted by traditional\nand semantic communication systems at different compression rates, (2)images\ntransmitted by traditional and semantic communication systems at different\nsignal-to-noise ratios, (3)images generated by large-scale model with different\nnoise levels introduced, and (4)cases of images subjected to certain special\ntransformations. The experiments demonstrate the effectiveness of SeSS,\nindicating that the metric can measure the semantic-level differences in\nsemantic-level information of images and can be used for evaluation in visual\nsemantic communication systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03865v2",
    "published_date": "2024-06-06 08:51:26 UTC",
    "updated_date": "2024-07-10 04:34:13 UTC"
  },
  {
    "arxiv_id": "2406.03862v1",
    "title": "Behavior-Targeted Attack on Reinforcement Learning with Limited Access to Victim's Policy",
    "authors": [
      "Shojiro Yamabe",
      "Kazuto Fukuchi",
      "Ryoma Senda",
      "Jun Sakuma"
    ],
    "abstract": "This study considers the attack on reinforcement learning agents where the\nadversary aims to control the victim's behavior as specified by the adversary\nby adding adversarial modifications to the victim's state observation. While\nsome attack methods reported success in manipulating the victim agent's\nbehavior, these methods often rely on environment-specific heuristics. In\naddition, all existing attack methods require white-box access to the victim's\npolicy. In this study, we propose a novel method for manipulating the victim\nagent in the black-box (i.e., the adversary is allowed to observe the victim's\nstate and action only) and no-box (i.e., the adversary is allowed to observe\nthe victim's state only) setting without requiring environment-specific\nheuristics. Our attack method is formulated as a bi-level optimization problem\nthat is reduced to a distribution matching problem and can be solved by an\nexisting imitation learning algorithm in the black-box and no-box settings.\nEmpirical evaluations on several reinforcement learning benchmarks show that\nour proposed method has superior attack performance to baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03862v1",
    "published_date": "2024-06-06 08:49:51 UTC",
    "updated_date": "2024-06-06 08:49:51 UTC"
  },
  {
    "arxiv_id": "2406.03848v3",
    "title": "OceanCastNet: A Deep Learning Ocean Wave Model with Energy Conservation",
    "authors": [
      "Ziliang Zhang",
      "Huaming Yu",
      "Danqin Ren"
    ],
    "abstract": "Traditional wave forecasting models, although based on energy conservation\nequations, are computationally expensive. On the other hand, existing deep\nlearning geophysical fluid models, while computationally efficient, often\nsuffer from issues such as energy dissipation in long-term forecasts. This\npaper proposes a novel energy-balanced deep learning wave forecasting model\ncalled OceanCastNet (OCN). By incorporating wind fields at the current,\nprevious, and future time steps, as well as wave fields at the current and\nprevious time steps as input variables, OCN maintains energy balance within the\nmodel. Furthermore, the model employs adaptive Fourier operators as its core\ncomponents and designs a masked loss function to better handle the impact of\nland-sea boundaries. A series of experiments on the ERA5 dataset demonstrate\nthat OCN can achieve short-term forecast accuracy comparable to traditional\nmodels while exhibiting an understanding of the wave generation process. In\ncomparative experiments under both normal and extreme conditions, OCN\nconsistently outperforms the widely used WaveWatch III model in the industry.\nEven after long-term forecasting, OCN maintains a stable and energy-rich state.\nBy further constructing a simple meteorological model, OCN-wind, which\nconsiders energy balance, this paper confirms the importance of energy\nconstraints for improving the long-term forecast performance of deep learning\nmeteorological models. This finding provides new ideas for future research on\ndeep learning geophysical fluid models.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03848v3",
    "published_date": "2024-06-06 08:29:29 UTC",
    "updated_date": "2024-12-03 08:54:30 UTC"
  },
  {
    "arxiv_id": "2406.03843v3",
    "title": "POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models",
    "authors": [
      "Jianben He",
      "Xingbo Wang",
      "Shiyi Liu",
      "Guande Wu",
      "Claudio Silva",
      "Huamin Qu"
    ],
    "abstract": "Large language models (LLMs) have exhibited impressive abilities for\nmultimodal content comprehension and reasoning with proper prompting in zero-\nor few-shot settings. Despite the proliferation of interactive systems\ndeveloped to support prompt engineering for LLMs across various tasks, most\nhave primarily focused on textual or visual inputs, thus neglecting the complex\ninterplay between modalities within multimodal inputs. This oversight hinders\nthe development of effective prompts that guide model multimodal reasoning\nprocesses by fully exploiting the rich context provided by multiple modalities.\nIn this paper, we present POEM, a visual analytics system to facilitate\nefficient prompt engineering for enhancing the multimodal reasoning performance\nof LLMs. The system enables users to explore the interaction patterns across\nmodalities at varying levels of detail for a comprehensive understanding of the\nmultimodal knowledge elicited by various prompts. Through diverse\nrecommendations of demonstration examples and instructional principles, POEM\nsupports users in iteratively crafting and refining prompts to better align and\nenhance model knowledge with human insights. The effectiveness and efficiency\nof our system are validated through two case studies and interviews with\nexperts.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "68",
      "H.5; I.2.1"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.03843v3",
    "published_date": "2024-06-06 08:21:30 UTC",
    "updated_date": "2024-09-30 16:16:04 UTC"
  },
  {
    "arxiv_id": "2406.03836v1",
    "title": "Proactive Detection of Physical Inter-rule Vulnerabilities in IoT Services Using a Deep Learning Approach",
    "authors": [
      "Bing Huang",
      "Chen Chen",
      "Kwok-Yan Lam",
      "Fuqun Huang"
    ],
    "abstract": "Emerging Internet of Things (IoT) platforms provide sophisticated\ncapabilities to automate IoT services by enabling occupants to create\ntrigger-action rules. Multiple trigger-action rules can physically interact\nwith each other via shared environment channels, such as temperature, humidity,\nand illumination. We refer to inter-rule interactions via shared environment\nchannels as a physical inter-rule vulnerability. Such vulnerability can be\nexploited by attackers to launch attacks against IoT systems. We propose a new\nframework to proactively discover possible physical inter-rule interactions\nfrom user requirement specifications (i.e., descriptions) using a deep learning\napproach. Specifically, we utilize the Transformer model to generate\ntrigger-action rules from their associated descriptions. We discover two types\nof physical inter-rule vulnerabilities and determine associated environment\nchannels using natural language processing (NLP) tools. Given the extracted\ntrigger-action rules and associated environment channels, an approach is\nproposed to identify hidden physical inter-rule vulnerabilities among them. Our\nexperiment on 27983 IFTTT style rules shows that the Transformer can\nsuccessfully extract trigger-action rules from descriptions with 95.22%\naccuracy. We also validate the effectiveness of our approach on 60 SmartThings\nofficial IoT apps and discover 99 possible physical inter-rule vulnerabilities.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by IEEE ICWS 2024 Workshop",
    "pdf_url": "http://arxiv.org/pdf/2406.03836v1",
    "published_date": "2024-06-06 08:13:02 UTC",
    "updated_date": "2024-06-06 08:13:02 UTC"
  },
  {
    "arxiv_id": "2406.06596v1",
    "title": "Are Large Language Models the New Interface for Data Pipelines?",
    "authors": [
      "Sylvio Barbon Junior",
      "Paolo Ceravolo",
      "Sven Groppe",
      "Mustafa Jarrar",
      "Samira Maghool",
      "Florence Sèdes",
      "Soror Sahri",
      "Maurice Van Keulen"
    ],
    "abstract": "A Language Model is a term that encompasses various types of models designed\nto understand and generate human communication. Large Language Models (LLMs)\nhave gained significant attention due to their ability to process text with\nhuman-like fluency and coherence, making them valuable for a wide range of\ndata-related tasks fashioned as pipelines. The capabilities of LLMs in natural\nlanguage understanding and generation, combined with their scalability,\nversatility, and state-of-the-art performance, enable innovative applications\nacross various AI-related fields, including eXplainable Artificial Intelligence\n(XAI), Automated Machine Learning (AutoML), and Knowledge Graphs (KG).\nFurthermore, we believe these models can extract valuable insights and make\ndata-driven decisions at scale, a practice commonly referred to as Big Data\nAnalytics (BDA). In this position paper, we provide some discussions in the\ndirection of unlocking synergies among these technologies, which can lead to\nmore powerful and intelligent AI solutions, driving improvements in data\npipelines across a wide range of applications and domains integrating humans,\ncomputers, and knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06596v1",
    "published_date": "2024-06-06 08:10:32 UTC",
    "updated_date": "2024-06-06 08:10:32 UTC"
  },
  {
    "arxiv_id": "2406.03820v2",
    "title": "A Survey on Intelligent Internet of Things: Applications, Security, Privacy, and Future Directions",
    "authors": [
      "Ons Aouedi",
      "Thai-Hoc Vu",
      "Alessio Sacco",
      "Dinh C. Nguyen",
      "Kandaraj Piamrat",
      "Guido Marchetto",
      "Quoc-Viet Pham"
    ],
    "abstract": "The rapid advances in the Internet of Things (IoT) have promoted a revolution\nin communication technology and offered various customer services. Artificial\nintelligence (AI) techniques have been exploited to facilitate IoT operations\nand maximize their potential in modern application scenarios. In particular,\nthe convergence of IoT and AI has led to a new networking paradigm called\nIntelligent IoT (IIoT), which has the potential to significantly transform\nbusinesses and industrial domains. This paper presents a comprehensive survey\nof IIoT by investigating its significant applications in mobile networks, as\nwell as its associated security and privacy issues. Specifically, we explore\nand discuss the roles of IIoT in a wide range of key application domains, from\nsmart healthcare and smart cities to smart transportation and smart industries.\nThrough such extensive discussions, we investigate important security issues in\nIIoT networks, where network attacks, confidentiality, integrity, and intrusion\nare analyzed, along with a discussion of potential countermeasures. Privacy\nissues in IIoT networks were also surveyed and discussed, including data,\nlocation, and model privacy leakage. Finally, we outline several key challenges\nand highlight potential research directions in this important area.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CR",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "This work has been accepted by IEEE Communications Surveys &\n  Tutorials",
    "pdf_url": "http://arxiv.org/pdf/2406.03820v2",
    "published_date": "2024-06-06 07:55:30 UTC",
    "updated_date": "2024-06-21 14:43:41 UTC"
  },
  {
    "arxiv_id": "2406.06595v1",
    "title": "Beyond 5G Network Failure Classification for Network Digital Twin Using Graph Neural Network",
    "authors": [
      "Abubakar Isah",
      "Ibrahim Aliyu",
      "Jaechan Shim",
      "Hoyong Ryu",
      "Jinsul Kim"
    ],
    "abstract": "Fifth-generation (5G) core networks in network digital twins (NDTs) are\ncomplex systems with numerous components, generating considerable data.\nAnalyzing these data can be challenging due to rare failure types, leading to\nimbalanced classes in multiclass classification. To address this problem, we\npropose a novel method of integrating a graph Fourier transform (GFT) into a\nmessage-passing neural network (MPNN) designed for NDTs. This approach\ntransforms the data into a graph using the GFT to address class imbalance,\nwhereas the MPNN extracts features and models dependencies between network\ncomponents. This combined approach identifies failure types in real and\nsimulated NDT environments, demonstrating its potential for accurate failure\nclassification in 5G and beyond (B5G) networks. Moreover, the MPNN is adept at\nlearning complex local structures among neighbors in an end-to-end setting.\nExtensive experiments have demonstrated that the proposed approach can identify\nfailure types in three multiclass domain datasets at multiple failure points in\nreal networks and NDT environments. The results demonstrate that the proposed\nGFT-MPNN can accurately classify network failures in B5G networks, especially\nwhen employed within NDTs to detect failure types.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06595v1",
    "published_date": "2024-06-06 07:36:25 UTC",
    "updated_date": "2024-06-06 07:36:25 UTC"
  },
  {
    "arxiv_id": "2406.03808v1",
    "title": "Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting",
    "authors": [
      "Jiaxin Gao",
      "Qinglong Cao",
      "Yuntian Chen",
      "Dongxiao Zhang"
    ],
    "abstract": "Photovoltaic (PV) power forecasting plays a crucial role in optimizing the\noperation and planning of PV systems, thereby enabling efficient energy\nmanagement and grid integration. However, un certainties caused by fluctuating\nweather conditions and complex interactions between different variables pose\nsignificant challenges to accurate PV power forecasting. In this study, we\npropose PV-Client (Cross-variable Linear Integrated ENhanced Transformer for\nPhotovoltaic power forecasting) to address these challenges and enhance PV\npower forecasting accuracy. PV-Client employs an ENhanced Transformer module to\ncapture complex interactions of various features in PV systems, and utilizes a\nlinear module to learn trend information in PV power. Diverging from\nconventional time series-based Transformer models that use cross-time Attention\nto learn dependencies between different time steps, the Enhanced Transformer\nmodule integrates cross-variable Attention to capture dependencies between PV\npower and weather factors. Furthermore, PV-Client streamlines the embedding and\nposition encoding layers by replacing the Decoder module with a projection\nlayer. Experimental results on three real-world PV power datasets affirm\nPV-Client's state-of-the-art (SOTA) performance in PV power forecasting.\nSpecifically, PV-Client surpasses the second-best model GRU by 5.3% in MSE\nmetrics and 0.9% in accuracy metrics at the Jingang Station. Similarly,\nPV-Client outperforms the second-best model SVR by 10.1% in MSE metrics and\n0.2% in accuracy metrics at the Xinqingnian Station, and PV-Client exhibits\nsuperior performance compared to the second-best model SVR with enhancements of\n3.4% in MSE metrics and 0.9% in accuracy metrics at the Hongxing Station.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03808v1",
    "published_date": "2024-06-06 07:30:27 UTC",
    "updated_date": "2024-06-06 07:30:27 UTC"
  },
  {
    "arxiv_id": "2406.03807v3",
    "title": "Tool-Planner: Task Planning with Clusters across Multiple Tools",
    "authors": [
      "Yanming Liu",
      "Xinyue Peng",
      "Jiannan Cao",
      "Shi Bo",
      "Yuwei Zhang",
      "Xuhong Zhang",
      "Sheng Cheng",
      "Xun Wang",
      "Jianwei Yin",
      "Tianyu Du"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\nhttps://github.com/OceannTwT/Tool-Planner",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025 Camera Ready version",
    "pdf_url": "http://arxiv.org/pdf/2406.03807v3",
    "published_date": "2024-06-06 07:30:14 UTC",
    "updated_date": "2025-02-28 07:12:21 UTC"
  },
  {
    "arxiv_id": "2406.03799v2",
    "title": "Enhanced Semantic Segmentation Pipeline for WeatherProof Dataset Challenge",
    "authors": [
      "Nan Zhang",
      "Xidan Zhang",
      "Jianing Wei",
      "Fangjun Wang",
      "Zhiming Tan"
    ],
    "abstract": "This report describes the winning solution to the WeatherProof Dataset\nChallenge (CVPR 2024 UG2+ Track 3). Details regarding the challenge are\navailable at https://cvpr2024ug2challenge.github.io/track3.html. We propose an\nenhanced semantic segmentation pipeline for this challenge. Firstly, we improve\nsemantic segmentation models, using backbone pretrained with Depth Anything to\nimprove UperNet model and SETRMLA model, and adding language guidance based on\nboth weather and category information to InternImage model. Secondly, we\nintroduce a new dataset WeatherProofExtra with wider viewing angle and employ\ndata augmentation methods, including adverse weather and super-resolution.\nFinally, effective training strategies and ensemble method are applied to\nimprove final performance further. Our solution is ranked 1st on the final\nleaderboard. Code will be available at\nhttps://github.com/KaneiGi/WeatherProofChallenge.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03799v2",
    "published_date": "2024-06-06 07:12:50 UTC",
    "updated_date": "2024-06-07 02:18:56 UTC"
  },
  {
    "arxiv_id": "2406.03789v2",
    "title": "Enhancing Graph U-Nets for Mesh-Agnostic Spatio-Temporal Flow Prediction",
    "authors": [
      "Sunwoong Yang",
      "Ricardo Vinuesa",
      "Namwoo Kang"
    ],
    "abstract": "This study aims to overcome the limitations of conventional deep-learning\napproaches based on convolutional neural networks in complex geometries and\nunstructured meshes by exploring the potential of Graph U-Nets for unsteady\nflow-field prediction. We present a comprehensive investigation of Graph\nU-Nets, originally developed for classification tasks, now tailored for\nmesh-agnostic spatio-temporal forecasting of fluid dynamics. Our focus is on\nenhancing their performance through systematic hyperparameter tuning and\narchitectural modifications. We propose novel approaches to improve\nmesh-agnostic spatio-temporal prediction of transient flow fields using Graph\nU-Nets, enabling accurate prediction on diverse mesh configurations. Key\nenhancements to the Graph U-Net architecture, including the\nGaussian-mixture-model convolutional operator and noise injection approaches,\nprovide increased flexibility in modeling node dynamics: the former reduces\nprediction error by 95\\% compared to conventional convolutional operators,\nwhile the latter improves long-term prediction robustness, resulting in an\nerror reduction of 86\\%. We demonstrate the effectiveness of these enhancements\nin both transductive and inductive learning settings, showcasing the\nadaptability of Graph U-Nets to various flow conditions and mesh structures.\nThis work contributes to the field of reduced-order modeling for computational\nfluid dynamics by establishing Graph U-Nets as a viable and flexible\nalternative to convolutional neural networks, capable of accurately and\nefficiently predicting complex fluid flow phenomena across diverse scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03789v2",
    "published_date": "2024-06-06 07:01:36 UTC",
    "updated_date": "2024-10-17 00:44:07 UTC"
  },
  {
    "arxiv_id": "2406.04384v1",
    "title": "Innovations in Cover Song Detection: A Lyrics-Based Approach",
    "authors": [
      "Maximilian Balluff",
      "Peter Mandl",
      "Christian Wolff"
    ],
    "abstract": "Cover songs are alternate versions of a song by a different artist. Long\nbeing a vital part of the music industry, cover songs significantly influence\nmusic culture and are commonly heard in public venues. The rise of online music\nplatforms has further increased their prevalence, often as background music or\nvideo soundtracks. While current automatic identification methods serve\nadequately for original songs, they are less effective with cover songs,\nprimarily because cover versions often significantly deviate from the original\ncompositions. In this paper, we propose a novel method for cover song detection\nthat utilizes the lyrics of a song. We introduce a new dataset for cover songs\nand their corresponding originals. The dataset contains 5078 cover songs and\n2828 original songs. In contrast to other cover song datasets, it contains the\nannotated lyrics for the original song and the cover song. We evaluate our\nmethod on this dataset and compare it with multiple baseline approaches. Our\nresults show that our method outperforms the baseline approaches.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.04384v1",
    "published_date": "2024-06-06 06:52:25 UTC",
    "updated_date": "2024-06-06 06:52:25 UTC"
  },
  {
    "arxiv_id": "2406.03777v3",
    "title": "Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge Devices",
    "authors": [
      "Ruiyang Qin",
      "Dancheng Liu",
      "Chenhui Xu",
      "Zheyu Yan",
      "Zhaoxuan Tan",
      "Zhenge Jia",
      "Amir Nassereldine",
      "Jiajie Li",
      "Meng Jiang",
      "Ahmed Abbasi",
      "Jinjun Xiong",
      "Yiyu Shi"
    ],
    "abstract": "The scaling laws have become the de facto guidelines for designing large\nlanguage models (LLMs), but they were studied under the assumption of unlimited\ncomputing resources for both training and inference. As LLMs are increasingly\nused as personalized intelligent assistants, their customization (i.e.,\nlearning through fine-tuning) and deployment onto resource-constrained edge\ndevices will become more and more prevalent. An urging but open question is how\na resource-constrained computing environment would affect the design choices\nfor a personalized LLM. We study this problem empirically in this work. In\nparticular, we consider the tradeoffs among a number of key design factors and\ntheir intertwined impacts on learning efficiency and accuracy. The factors\ninclude the learning methods for LLM customization, the amount of personalized\ndata used for learning customization, the types and sizes of LLMs, the\ncompression methods of LLMs, the amount of time afforded to learn, and the\ndifficulty levels of the target use cases. Through extensive experimentation\nand benchmarking, we draw a number of surprisingly insightful guidelines for\ndeploying LLMs onto resource-constrained devices. For example, an optimal\nchoice between parameter learning and RAG may vary depending on the difficulty\nof the downstream task, the longer fine-tuning time does not necessarily help\nthe model, and a compressed LLM may be a better choice than an uncompressed LLM\nto learn from limited personalized data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Benckmarking paper",
    "pdf_url": "http://arxiv.org/pdf/2406.03777v3",
    "published_date": "2024-06-06 06:41:53 UTC",
    "updated_date": "2024-10-02 04:14:21 UTC"
  },
  {
    "arxiv_id": "2406.03776v2",
    "title": "XL-HeadTags: Leveraging Multimodal Retrieval Augmentation for the Multilingual Generation of News Headlines and Tags",
    "authors": [
      "Faisal Tareque Shohan",
      "Mir Tafseer Nayeem",
      "Samsul Islam",
      "Abu Ubaida Akash",
      "Shafiq Joty"
    ],
    "abstract": "Millions of news articles published online daily can overwhelm readers.\nHeadlines and entity (topic) tags are essential for guiding readers to decide\nif the content is worth their time. While headline generation has been\nextensively studied, tag generation remains largely unexplored, yet it offers\nreaders better access to topics of interest. The need for conciseness in\ncapturing readers' attention necessitates improved content selection strategies\nfor identifying salient and relevant segments within lengthy articles, thereby\nguiding language models effectively. To address this, we propose to leverage\nauxiliary information such as images and captions embedded in the articles to\nretrieve relevant sentences and utilize instruction tuning with variations to\ngenerate both headlines and tags for news articles in a multilingual context.\nTo make use of the auxiliary information, we have compiled a dataset named\nXL-HeadTags, which includes 20 languages across 6 diverse language families.\nThrough extensive evaluation, we demonstrate the effectiveness of our\nplug-and-play multimodal-multilingual retrievers for both tasks. Additionally,\nwe have developed a suite of tools for processing and evaluating multilingual\ntexts, significantly contributing to the research community by enabling more\naccurate and efficient analysis across languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 camera ready. The first two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2406.03776v2",
    "published_date": "2024-06-06 06:40:19 UTC",
    "updated_date": "2024-06-07 05:58:45 UTC"
  },
  {
    "arxiv_id": "2406.18584v2",
    "title": "Assessment of Sentinel-2 spatial and temporal coverage based on the scene classification layer",
    "authors": [
      "Cristhian Sanchez",
      "Francisco Mena",
      "Marcela Charfuelan",
      "Marlon Nuske",
      "Andreas Dengel"
    ],
    "abstract": "Since the launch of the Sentinel-2 (S2) satellites, many ML models have used\nthe data for diverse applications. The scene classification layer (SCL) inside\nthe S2 product provides rich information for training, such as filtering images\nwith high cloud coverage. However, there is more potential in this. We propose\na technique to assess the clean optical coverage of a region, expressed by a\nSITS and calculated with the S2-based SCL data. With a manual threshold and\nspecific labels in the SCL, the proposed technique assigns a percentage of\nspatial and temporal coverage across the time series and a high/low assessment.\nBy evaluating the AI4EO challenge for Enhanced Agriculture, we show that the\nassessment is correlated to the predictive results of ML models. The\nclassification results in a region with low spatial and temporal coverage is\nworse than in a region with high coverage. Finally, we applied the technique\nacross all continents of the global dataset LandCoverNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18584v2",
    "published_date": "2024-06-06 06:22:06 UTC",
    "updated_date": "2024-06-28 07:34:25 UTC"
  },
  {
    "arxiv_id": "2406.03768v2",
    "title": "Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective",
    "authors": [
      "Xinhao Yao",
      "Xiaolin Hu",
      "Shenzhi Yang",
      "Yong Liu"
    ],
    "abstract": "Pre-trained large language models (LLMs) based on Transformer have\ndemonstrated striking in-context learning (ICL) abilities. With a few\ndemonstration input-label pairs, they can predict the label for an unseen input\nwithout any parameter updates. In this paper, we show an exciting phenomenon\nthat SVD-based weight pruning can enhance ICL performance, and more surprising,\npruning weights in deep layers often results in more stable performance\nimprovements than in shallow layers. However, the underlying mechanism of those\nfindings still remains an open question. To reveal those findings, we conduct\nan in-depth theoretical analysis by presenting the implicit gradient descent\n(GD) trajectories of ICL and giving the mutual information based generalization\nbounds of ICL via full implicit GD trajectories. This helps us reasonably\nexplain the surprising experimental findings. Besides, based on all our\nexperimental and theoretical insights, we intuitively propose a simple,\nmodel-compression and derivative-free algorithm for downstream tasks in\nenhancing ICL inference. Experiments on benchmark datasets and open source LLMs\ndisplay the method effectiveness\\footnote{The code is available at\n\\url{https://github.com/chen123CtrlS/EnhancingICL_SVDPruning}.}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.03768v2",
    "published_date": "2024-06-06 06:15:35 UTC",
    "updated_date": "2024-10-13 11:19:58 UTC"
  },
  {
    "arxiv_id": "2406.04383v2",
    "title": "Exploring the Latest LLMs for Leaderboard Extraction",
    "authors": [
      "Salomon Kabongo",
      "Jennifer D'Souza",
      "Sören Auer"
    ],
    "abstract": "The rapid advancements in Large Language Models (LLMs) have opened new\navenues for automating complex tasks in AI research. This paper investigates\nthe efficacy of different LLMs-Mistral 7B, Llama-2, GPT-4-Turbo and GPT-4.o in\nextracting leaderboard information from empirical AI research articles. We\nexplore three types of contextual inputs to the models: DocTAET (Document\nTitle, Abstract, Experimental Setup, and Tabular Information), DocREC (Results,\nExperiments, and Conclusions), and DocFULL (entire document). Our comprehensive\nstudy evaluates the performance of these models in generating (Task, Dataset,\nMetric, Score) quadruples from research papers. The findings reveal significant\ninsights into the strengths and limitations of each model and context type,\nproviding valuable guidance for future AI research automation efforts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.04383v2",
    "published_date": "2024-06-06 05:54:45 UTC",
    "updated_date": "2024-07-08 19:04:26 UTC"
  },
  {
    "arxiv_id": "2406.03747v1",
    "title": "Instance Segmentation and Teeth Classification in Panoramic X-rays",
    "authors": [
      "Devichand Budagam",
      "Ayush Kumar",
      "Sayan Ghosh",
      "Anuj Shrivastav",
      "Azamat Zhanatuly Imanbayev",
      "Iskander Rafailovich Akhmetov",
      "Dmitrii Kaplun",
      "Sergey Antonov",
      "Artem Rychenkov",
      "Gleb Cyganov",
      "Aleksandr Sinitca"
    ],
    "abstract": "Teeth segmentation and recognition are critical in various dental\napplications and dental diagnosis. Automatic and accurate segmentation\napproaches have been made possible by integrating deep learning models.\nAlthough teeth segmentation has been studied in the past, only some techniques\nwere able to effectively classify and segment teeth simultaneously. This\narticle offers a pipeline of two deep learning models, U-Net and YOLOv8, which\nresults in BB-UNet, a new architecture for the classification and segmentation\nof teeth on panoramic X-rays that is efficient and reliable. We have improved\nthe quality and reliability of teeth segmentation by utilising the YOLOv8 and\nU-Net capabilities. The proposed networks have been evaluated using the mean\naverage precision (mAP) and dice coefficient for YOLOv8 and BB-UNet,\nrespectively. We have achieved a 3\\% increase in mAP score for teeth\nclassification compared to existing methods, and a 10-15\\% increase in dice\ncoefficient for teeth segmentation compared to U-Net across different\ncategories of teeth. A new Dental dataset was created based on UFBA-UESC\ndataset with Bounding-Box and Polygon annotations of 425 dental panoramic\nX-rays. The findings of this research pave the way for a wider adoption of\nobject detection models in the field of dental diagnosis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "submtted to Expert Systems with Applications Journal",
    "pdf_url": "http://arxiv.org/pdf/2406.03747v1",
    "published_date": "2024-06-06 04:57:29 UTC",
    "updated_date": "2024-06-06 04:57:29 UTC"
  },
  {
    "arxiv_id": "2406.03746v1",
    "title": "Efficient Knowledge Infusion via KG-LLM Alignment",
    "authors": [
      "Zhouyu Jiang",
      "Ling Zhong",
      "Mengshu Sun",
      "Jun Xu",
      "Rui Sun",
      "Hui Cai",
      "Shuhan Luo",
      "Zhiqiang Zhang"
    ],
    "abstract": "To tackle the problem of domain-specific knowledge scarcity within large\nlanguage models (LLMs), knowledge graph-retrievalaugmented method has been\nproven to be an effective and efficient technique for knowledge infusion.\nHowever, existing approaches face two primary challenges: knowledge mismatch\nbetween public available knowledge graphs and the specific domain of the task\nat hand, and poor information compliance of LLMs with knowledge graphs. In this\npaper, we leverage a small set of labeled samples and a large-scale corpus to\nefficiently construct domain-specific knowledge graphs by an LLM, addressing\nthe issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM\nalignment strategyto enhance the LLM's capability to utilize information from\nknowledge graphs. We conduct experiments with a limited-sample setting on two\nbiomedical question-answering datasets, and the results demonstrate that our\napproach outperforms existing baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.03746v1",
    "published_date": "2024-06-06 04:55:55 UTC",
    "updated_date": "2024-06-06 04:55:55 UTC"
  },
  {
    "arxiv_id": "2406.03733v4",
    "title": "Credit Card Fraud Detection Using Advanced Transformer Model",
    "authors": [
      "Chang Yu",
      "Yongshun Xu",
      "Jin Cao",
      "Ye Zhang",
      "Yinxin Jin",
      "Mengran Zhu"
    ],
    "abstract": "With the proliferation of various online and mobile payment systems, credit\ncard fraud has emerged as a significant threat to financial security. This\nstudy focuses on innovative applications of the latest Transformer models for\nmore robust and precise fraud detection. To ensure the reliability of the data,\nwe meticulously processed the data sources, balancing the dataset to address\nthe issue of data sparsity significantly. We also selected highly correlated\nvectors to strengthen the training process.To guarantee the reliability and\npracticality of the new Transformer model, we conducted performance comparisons\nwith several widely adopted models, including Support Vector Machine (SVM),\nRandom Forest, Neural Network, and Logistic Regression. We rigorously compared\nthese models using metrics such as Precision, Recall, and F1 Score. Through\nthese detailed analyses and comparisons, we present to the readers a highly\nefficient and powerful anti-fraud mechanism with promising prospects. The\nresults demonstrate that the Transformer model not only excels in traditional\napplications but also shows great potential in niche areas like fraud\ndetection, offering a substantial advancement in the field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper have been received by https://ieee-metacom.org/",
    "pdf_url": "http://arxiv.org/pdf/2406.03733v4",
    "published_date": "2024-06-06 04:12:57 UTC",
    "updated_date": "2024-11-12 16:44:14 UTC"
  },
  {
    "arxiv_id": "2406.03730v1",
    "title": "FastGAS: Fast Graph-based Annotation Selection for In-Context Learning",
    "authors": [
      "Zihan Chen",
      "Song Wang",
      "Cong Shen",
      "Jundong Li"
    ],
    "abstract": "In-context learning (ICL) empowers large language models (LLMs) to tackle new\ntasks by using a series of training instances as prompts. Since generating the\nprompts needs to sample from a vast pool of instances and annotate them (e.g.,\nadd labels in classification task), existing methods have proposed to select a\nsubset of unlabeled examples for annotation, thus enhancing the quality of\nprompts and concurrently mitigating annotation costs. However, these methods\noften require a long time to select instances due to their complexity,\nhindering their practical viability. To address this limitation, we propose a\ngraph-based selection method, FastGAS, designed to efficiently identify\nhigh-quality instances while minimizing computational overhead. Initially, we\nconstruct a data similarity graph based on instance similarities. Subsequently,\nemploying a graph partitioning algorithm, we partition the graph into pieces.\nWithin each piece (i.e., subgraph), we adopt a greedy approach to pick the most\nrepresentative nodes. By aggregating nodes from diverse pieces and annotating\nthe corresponding instances, we identify a set of diverse and representative\ninstances for ICL. Compared to prior approaches, our method not only exhibits\nsuperior performance on different tasks but also significantly reduces\nselection time. In addition, we demonstrate the efficacy of our approach in\nLLMs of larger sizes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03730v1",
    "published_date": "2024-06-06 04:05:54 UTC",
    "updated_date": "2024-06-06 04:05:54 UTC"
  },
  {
    "arxiv_id": "2406.04382v2",
    "title": "Improving the Fairness of Deep-Learning, Short-term Crime Prediction with Under-reporting-aware Models",
    "authors": [
      "Jiahui Wu",
      "Vanessa Frias-Martinez"
    ],
    "abstract": "Deep learning crime predictive tools use past crime data and additional\nbehavioral datasets to forecast future crimes. Nevertheless, these tools have\nbeen shown to suffer from unfair predictions across minority racial and ethnic\ngroups. Current approaches to address this unfairness generally propose either\npre-processing methods that mitigate the bias in the training datasets by\napplying corrections to crime counts based on domain knowledge or in-processing\nmethods that are implemented as fairness regularizers to optimize for both\naccuracy and fairness. In this paper, we propose a novel deep learning\narchitecture that combines the power of these two approaches to increase\nprediction fairness. Our results show that the proposed model improves the\nfairness of crime predictions when compared to models with in-processing\nde-biasing approaches and with models without any type of bias correction,\nalbeit at the cost of reducing accuracy.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "25 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.04382v2",
    "published_date": "2024-06-06 04:05:23 UTC",
    "updated_date": "2024-06-13 17:53:01 UTC"
  },
  {
    "arxiv_id": "2406.03722v1",
    "title": "Offline Multi-Objective Optimization",
    "authors": [
      "Ke Xue",
      "Rong-Xi Tan",
      "Xiaobin Huang",
      "Chao Qian"
    ],
    "abstract": "Offline optimization aims to maximize a black-box objective function with a\nstatic dataset and has wide applications. In addition to the objective function\nbeing black-box and expensive to evaluate, numerous complex real-world problems\nentail optimizing multiple conflicting objectives, i.e., multi-objective\noptimization (MOO). Nevertheless, offline MOO has not progressed as much as\noffline single-objective optimization (SOO), mainly due to the lack of\nbenchmarks like Design-Bench for SOO. To bridge this gap, we propose a first\nbenchmark for offline MOO, covering a range of problems from synthetic to\nreal-world tasks. This benchmark provides tasks, datasets, and open-source\nexamples, which can serve as a foundation for method comparisons and\nadvancements in offline MOO. Furthermore, we analyze how the current related\nmethods can be adapted to offline MOO from four fundamental perspectives,\nincluding data, model architecture, learning algorithm, and search algorithm.\nEmpirical results show improvements over the best value of the training set,\ndemonstrating the effectiveness of offline MOO methods. As no particular method\nstands out significantly, there is still an open challenge in further enhancing\nthe effectiveness of offline MOO. We finally discuss future challenges for\noffline MOO, with the hope of shedding some light on this emerging field. Our\ncode is available at \\url{https://github.com/lamda-bbo/offline-moo}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.03722v1",
    "published_date": "2024-06-06 03:35:09 UTC",
    "updated_date": "2024-06-06 03:35:09 UTC"
  },
  {
    "arxiv_id": "2406.03721v1",
    "title": "Attribute-Aware Implicit Modality Alignment for Text Attribute Person Search",
    "authors": [
      "Xin Wang",
      "Fangfang Liu",
      "Zheng Li",
      "Caili Guo"
    ],
    "abstract": "Text attribute person search aims to find specific pedestrians through given\ntextual attributes, which is very meaningful in the scene of searching for\ndesignated pedestrians through witness descriptions. The key challenge is the\nsignificant modality gap between textual attributes and images. Previous\nmethods focused on achieving explicit representation and alignment through\nunimodal pre-trained models. Nevertheless, the absence of inter-modality\ncorrespondence in these models may lead to distortions in the local information\nof intra-modality. Moreover, these methods only considered the alignment of\ninter-modality and ignored the differences between different attribute\ncategories. To mitigate the above problems, we propose an Attribute-Aware\nImplicit Modality Alignment (AIMA) framework to learn the correspondence of\nlocal representations between textual attributes and images and combine global\nrepresentation matching to narrow the modality gap. Firstly, we introduce the\nCLIP model as the backbone and design prompt templates to transform attribute\ncombinations into structured sentences. This facilitates the model's ability to\nbetter understand and match image details. Next, we design a Masked Attribute\nPrediction (MAP) module that predicts the masked attributes after the\ninteraction of image and masked textual attribute features through multi-modal\ninteraction, thereby achieving implicit local relationship alignment. Finally,\nwe propose an Attribute-IoU Guided Intra-Modal Contrastive (A-IoU IMC) loss,\naligning the distribution of different textual attributes in the embedding\nspace with their IoU distribution, achieving better semantic arrangement.\nExtensive experiments on the Market-1501 Attribute, PETA, and PA100K datasets\nshow that the performance of our proposed method significantly surpasses the\ncurrent state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03721v1",
    "published_date": "2024-06-06 03:34:42 UTC",
    "updated_date": "2024-06-06 03:34:42 UTC"
  },
  {
    "arxiv_id": "2406.03718v1",
    "title": "Generalization-Enhanced Code Vulnerability Detection via Multi-Task Instruction Fine-Tuning",
    "authors": [
      "Xiaohu Du",
      "Ming Wen",
      "Jiahao Zhu",
      "Zifan Xie",
      "Bin Ji",
      "Huijun Liu",
      "Xuanhua Shi",
      "Hai Jin"
    ],
    "abstract": "Code Pre-trained Models (CodePTMs) based vulnerability detection have\nachieved promising results over recent years. However, these models struggle to\ngeneralize as they typically learn superficial mapping from source code to\nlabels instead of understanding the root causes of code vulnerabilities,\nresulting in poor performance in real-world scenarios beyond the training\ninstances. To tackle this challenge, we introduce VulLLM, a novel framework\nthat integrates multi-task learning with Large Language Models (LLMs) to\neffectively mine deep-seated vulnerability features. Specifically, we construct\ntwo auxiliary tasks beyond the vulnerability detection task. First, we utilize\nthe vulnerability patches to construct a vulnerability localization task.\nSecond, based on the vulnerability features extracted from patches, we leverage\nGPT-4 to construct a vulnerability interpretation task. VulLLM innovatively\naugments vulnerability classification by leveraging generative LLMs to\nunderstand complex vulnerability patterns, thus compelling the model to capture\nthe root causes of vulnerabilities rather than overfitting to spurious features\nof a single task. The experiments conducted on six large datasets demonstrate\nthat VulLLM surpasses seven state-of-the-art models in terms of effectiveness,\ngeneralization, and robustness.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to ACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.03718v1",
    "published_date": "2024-06-06 03:29:05 UTC",
    "updated_date": "2024-06-06 03:29:05 UTC"
  },
  {
    "arxiv_id": "2406.03711v1",
    "title": "Pi-fusion: Physics-informed diffusion model for learning fluid dynamics",
    "authors": [
      "Jing Qiu",
      "Jiancheng Huang",
      "Xiangdong Zhang",
      "Zeng Lin",
      "Minglei Pan",
      "Zengding Liu",
      "Fen Miao"
    ],
    "abstract": "Physics-informed deep learning has been developed as a novel paradigm for\nlearning physical dynamics recently. While general physics-informed deep\nlearning methods have shown early promise in learning fluid dynamics, they are\ndifficult to generalize in arbitrary time instants in real-world scenario,\nwhere the fluid motion can be considered as a time-variant trajectory involved\nlarge-scale particles. Inspired by the advantage of diffusion model in learning\nthe distribution of data, we first propose Pi-fusion, a physics-informed\ndiffusion model for predicting the temporal evolution of velocity and pressure\nfield in fluid dynamics. Physics-informed guidance sampling is proposed in the\ninference procedure of Pi-fusion to improve the accuracy and interpretability\nof learning fluid dynamics. Furthermore, we introduce a training strategy based\non reciprocal learning to learn the quasiperiodical pattern of fluid motion and\nthus improve the generalizability of the model. The proposed approach are then\nevaluated on both synthetic and real-world dataset, by comparing it with\nstate-of-the-art physics-informed deep learning methods. Experimental results\nshow that the proposed approach significantly outperforms existing methods for\npredicting temporal evolution of velocity and pressure field, confirming its\nstrong generalization by drawing probabilistic inference of forward process and\nphysics-informed guidance sampling. The proposed Pi-fusion can also be\ngeneralized in learning other physical dynamics governed by partial\ndifferential equations.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03711v1",
    "published_date": "2024-06-06 03:14:59 UTC",
    "updated_date": "2024-06-06 03:14:59 UTC"
  },
  {
    "arxiv_id": "2406.03710v2",
    "title": "TwinS: Revisiting Non-Stationarity in Multivariate Time Series Forecasting",
    "authors": [
      "Jiaxi Hu",
      "Qingsong Wen",
      "Sijie Ruan",
      "Li Liu",
      "Yuxuan Liang"
    ],
    "abstract": "Recently, multivariate time series forecasting tasks have garnered increasing\nattention due to their significant practical applications, leading to the\nemergence of various deep forecasting models. However, real-world time series\nexhibit pronounced non-stationary distribution characteristics. These\ncharacteristics are not solely limited to time-varying statistical properties\nhighlighted by non-stationary Transformer but also encompass three key aspects:\nnested periodicity, absence of periodic distributions, and hysteresis among\ntime variables. In this paper, we begin by validating this theory through\nwavelet analysis and propose the Transformer-based TwinS model, which consists\nof three modules to address the non-stationary periodic distributions: Wavelet\nConvolution, Period-Aware Attention, and Channel-Temporal Mixed MLP.\nSpecifically, The Wavelet Convolution models nested periods by scaling the\nconvolution kernel size like wavelet transform. The Period-Aware Attention\nguides attention computation by generating period relevance scores through a\nconvolutional sub-network. The Channel-Temporal Mixed MLP captures the overall\nrelationships between time series through channel-time mixing learning. TwinS\nachieves SOTA performance compared to mainstream TS models, with a maximum\nimprovement in MSE of 25.8\\% over PatchTST.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03710v2",
    "published_date": "2024-06-06 03:14:23 UTC",
    "updated_date": "2024-07-14 14:55:16 UTC"
  },
  {
    "arxiv_id": "2406.06594v2",
    "title": "Stock Movement Prediction with Multimodal Stable Fusion via Gated Cross-Attention Mechanism",
    "authors": [
      "Chang Zong",
      "Hang Zhou"
    ],
    "abstract": "The accurate prediction of stock movements is crucial for investment\nstrategies. Stock prices are subject to the influence of various forms of\ninformation, including financial indicators, sentiment analysis, news\ndocuments, and relational structures. Predominant analytical approaches,\nhowever, tend to address only unimodal or bimodal sources, neglecting the\ncomplexity of multimodal data. Further complicating the landscape are the\nissues of data sparsity and semantic conflicts between these modalities, which\nare frequently overlooked by current models, leading to unstable performance\nand limiting practical applicability. To address these shortcomings, this study\nintroduces a novel architecture, named Multimodal Stable Fusion with Gated\nCross-Attention (MSGCA), designed to robustly integrate multimodal input for\nstock movement prediction. The MSGCA framework consists of three integral\ncomponents: (1) a trimodal encoding module, responsible for processing\nindicator sequences, dynamic documents, and a relational graph, and\nstandardizing their feature representations; (2) a cross-feature fusion module,\nwhere primary and consistent features guide the multimodal fusion of the three\nmodalities via a pair of gated cross-attention networks; and (3) a prediction\nmodule, which refines the fused features through temporal and dimensional\nreduction to execute precise movement forecasting. Empirical evaluations\ndemonstrate that the MSGCA framework exceeds current leading methods, achieving\nperformance gains of 8.1%, 6.1%, 21.7% and 31.6% on four multimodal datasets,\nrespectively, attributed to its enhanced multimodal fusion stability.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG",
      "68T07",
      "I.2.6; J.4"
    ],
    "primary_category": "q-fin.CP",
    "comment": "14 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.06594v2",
    "published_date": "2024-06-06 03:13:34 UTC",
    "updated_date": "2024-12-02 07:04:17 UTC"
  },
  {
    "arxiv_id": "2406.03707v1",
    "title": "What Should Embeddings Embed? Autoregressive Models Represent Latent Generating Distributions",
    "authors": [
      "Liyi Zhang",
      "Michael Y. Li",
      "Thomas L. Griffiths"
    ],
    "abstract": "Autoregressive language models have demonstrated a remarkable ability to\nextract latent structure from text. The embeddings from large language models\nhave been shown to capture aspects of the syntax and semantics of language. But\nwhat {\\em should} embeddings represent? We connect the autoregressive\nprediction objective to the idea of constructing predictive sufficient\nstatistics to summarize the information contained in a sequence of\nobservations, and use this connection to identify three settings where the\noptimal content of embeddings can be identified: independent identically\ndistributed data, where the embedding should capture the sufficient statistics\nof the data; latent state models, where the embedding should encode the\nposterior distribution over states given the data; and discrete hypothesis\nspaces, where the embedding should reflect the posterior distribution over\nhypotheses given the data. We then conduct empirical probing studies to show\nthat transformers encode these three kinds of latent generating distributions,\nand that they perform well in out-of-distribution cases and without token\nmemorization in these settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML",
      "I.2; I.5"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.03707v1",
    "published_date": "2024-06-06 03:06:46 UTC",
    "updated_date": "2024-06-06 03:06:46 UTC"
  },
  {
    "arxiv_id": "2406.03689v3",
    "title": "Evaluating the World Model Implicit in a Generative Model",
    "authors": [
      "Keyon Vafa",
      "Justin Y. Chen",
      "Ashesh Rambachan",
      "Jon Kleinberg",
      "Sendhil Mullainathan"
    ],
    "abstract": "Recent work suggests that large language models may implicitly learn world\nmodels. How should we assess this possibility? We formalize this question for\nthe case where the underlying reality is governed by a deterministic finite\nautomaton. This includes problems as diverse as simple logical reasoning,\ngeographic navigation, game-playing, and chemistry. We propose new evaluation\nmetrics for world model recovery inspired by the classic Myhill-Nerode theorem\nfrom language theory. We illustrate their utility in three domains: game\nplaying, logic puzzles, and navigation. In all domains, the generative models\nwe consider do well on existing diagnostics for assessing world models, but our\nevaluation metrics reveal their world models to be far less coherent than they\nappear. Such incoherence creates fragility: using a generative model to solve\nrelated but subtly different tasks can lead to failures. Building generative\nmodels that meaningfully capture the underlying logic of the domains they model\nwould be immensely valuable; our results suggest new ways to assess how close a\ngiven model is to that goal.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03689v3",
    "published_date": "2024-06-06 02:20:31 UTC",
    "updated_date": "2024-11-10 23:47:33 UTC"
  },
  {
    "arxiv_id": "2406.06593v1",
    "title": "Differentiable Combinatorial Scheduling at Scale",
    "authors": [
      "Mingju Liu",
      "Yingjie Li",
      "Jiaqi Yin",
      "Zhiru Zhang",
      "Cunxi Yu"
    ],
    "abstract": "This paper addresses the complex issue of resource-constrained scheduling, an\nNP-hard problem that spans critical areas including chip design and\nhigh-performance computing. Traditional scheduling methods often stumble over\nscalability and applicability challenges. We propose a novel approach using a\ndifferentiable combinatorial scheduling framework, utilizing Gumbel-Softmax\ndifferentiable sampling technique. This new technical allows for a fully\ndifferentiable formulation of linear programming (LP) based scheduling,\nextending its application to a broader range of LP formulations. To encode\ninequality constraints for scheduling tasks, we introduce \\textit{constrained\nGumbel Trick}, which adeptly encodes arbitrary inequality constraints.\nConsequently, our method facilitates an efficient and scalable scheduling via\ngradient descent without the need for training data. Comparative evaluations on\nboth synthetic and real-world benchmarks highlight our capability to\nsignificantly improve the optimization efficiency of scheduling, surpassing\nstate-of-the-art solutions offered by commercial and open-source solvers such\nas CPLEX, Gurobi, and CP-SAT in the majority of the designs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages; International Conference on Machine Learning (ICML'24)",
    "pdf_url": "http://arxiv.org/pdf/2406.06593v1",
    "published_date": "2024-06-06 02:09:39 UTC",
    "updated_date": "2024-06-06 02:09:39 UTC"
  },
  {
    "arxiv_id": "2407.10977v1",
    "title": "CIRCUITSYNTH: Leveraging Large Language Models for Circuit Topology Synthesis",
    "authors": [
      "Prashanth Vijayaraghavan",
      "Luyao Shi",
      "Ehsan Degan",
      "Xin Zhang"
    ],
    "abstract": "Circuit topology generation plays a crucial role in the design of electronic\ncircuits, influencing the fundamental functionality of the circuit. In this\npaper, we introduce CIRCUITSYNTH, a novel approach that harnesses LLMs to\nfacilitate the automated synthesis of valid circuit topologies. With a dataset\ncomprising both valid and invalid circuit configurations, CIRCUITSYNTH employs\na sophisticated two-phase methodology, comprising Circuit Topology Generation\nand Circuit Topology Refinement. Experimental results demonstrate the\neffectiveness of CIRCUITSYNTH compared to various fine-tuned LLM variants. Our\napproach lays the foundation for future research aimed at enhancing circuit\nefficiency and specifying output voltage, thus enabling the automated\ngeneration of circuit topologies with improved performance and adherence to\ndesign requirements.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 1 figure, LAD'24",
    "pdf_url": "http://arxiv.org/pdf/2407.10977v1",
    "published_date": "2024-06-06 01:59:59 UTC",
    "updated_date": "2024-06-06 01:59:59 UTC"
  },
  {
    "arxiv_id": "2406.03679v6",
    "title": "On the Effects of Data Scale on UI Control Agents",
    "authors": [
      "Wei Li",
      "William Bishop",
      "Alice Li",
      "Chris Rawles",
      "Folawiyo Campbell-Ajala",
      "Divya Tyamagundlu",
      "Oriana Riva"
    ],
    "abstract": "Autonomous agents that control computer interfaces to accomplish human tasks\nare emerging. Leveraging LLMs to power such agents has been of special\ninterest, but unless fine-tuned on human-collected task demonstrations,\nperformance is still relatively low. In this work we study whether fine-tuning\nalone is a viable approach for building real-world computer control agents. In\nparticularly, we investigate how performance measured on both high and\nlow-level tasks in domain and out of domain scales as more training data is\ncollected. To this end we collect and release a new dataset, AndroidControl,\nconsisting of 15,283 demonstrations of everyday tasks with Android apps.\nCompared to existing datasets, each AndroidControl task instance includes both\nhigh and low-level human-generated instructions, allowing us to explore the\nlevel of task complexity an agent can handle. Moreover, AndroidControl is the\nmost diverse computer control dataset to date, including 14,548 unique tasks\nover 833 Android apps, thus allowing us to conduct in-depth analysis of the\nmodel performance in and out of the domain of the training data. Using the\ndataset, we find that when tested in domain fine-tuned models outperform zero\nand few-shot baselines and scale in such a way that robust performance might\nfeasibly be obtained simply by collecting more data. Out of domain, performance\nscales significantly more slowly and suggests that in particular for high-level\ntasks, fine-tuning on more data alone may be insufficient for achieving robust\nout-of-domain performance.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024 (Datasets and Benchmarks)",
    "pdf_url": "http://arxiv.org/pdf/2406.03679v6",
    "published_date": "2024-06-06 01:49:29 UTC",
    "updated_date": "2024-11-13 16:42:22 UTC"
  },
  {
    "arxiv_id": "2406.03678v1",
    "title": "Reflective Policy Optimization",
    "authors": [
      "Yaozhong Gan",
      "Renye Yan",
      "Zhe Wu",
      "Junliang Xing"
    ],
    "abstract": "On-policy reinforcement learning methods, like Trust Region Policy\nOptimization (TRPO) and Proximal Policy Optimization (PPO), often demand\nextensive data per update, leading to sample inefficiency. This paper\nintroduces Reflective Policy Optimization (RPO), a novel on-policy extension\nthat amalgamates past and future state-action information for policy\noptimization. This approach empowers the agent for introspection, allowing\nmodifications to its actions within the current state. Theoretical analysis\nconfirms that policy performance is monotonically improved and contracts the\nsolution space, consequently expediting the convergence procedure. Empirical\nresults demonstrate RPO's feasibility and efficacy in two reinforcement\nlearning benchmarks, culminating in superior sample efficiency. The source code\nof this work is available at https://github.com/Edgargan/RPO.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.03678v1",
    "published_date": "2024-06-06 01:46:49 UTC",
    "updated_date": "2024-06-06 01:46:49 UTC"
  },
  {
    "arxiv_id": "2406.03673v1",
    "title": "Linguistically Conditioned Semantic Textual Similarity",
    "authors": [
      "Jingxuan Tu",
      "Keer Xu",
      "Liulu Yue",
      "Bingyang Ye",
      "Kyeongmin Rim",
      "James Pustejovsky"
    ],
    "abstract": "Semantic textual similarity (STS) is a fundamental NLP task that measures the\nsemantic similarity between a pair of sentences. In order to reduce the\ninherent ambiguity posed from the sentences, a recent work called Conditional\nSTS (C-STS) has been proposed to measure the sentences' similarity conditioned\non a certain aspect. Despite the popularity of C-STS, we find that the current\nC-STS dataset suffers from various issues that could impede proper evaluation\non this task. In this paper, we reannotate the C-STS validation set and observe\nan annotator discrepancy on 55% of the instances resulting from the annotation\nerrors in the original label, ill-defined conditions, and the lack of clarity\nin the task definition. After a thorough dataset analysis, we improve the C-STS\ntask by leveraging the models' capability to understand the conditions under a\nQA task setting. With the generated answers, we present an automatic error\nidentification pipeline that is able to identify annotation errors from the\nC-STS data with over 80% F1 score. We also propose a new method that largely\nimproves the performance over baselines on the C-STS data by training the\nmodels with the answers. Finally we discuss the conditionality annotation based\non the typed-feature structure (TFS) of entity types. We show in examples that\nthe TFS is able to provide a linguistic foundation for constructing C-STS data\nwith new conditions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in the ACL 2024 main proceedings",
    "pdf_url": "http://arxiv.org/pdf/2406.03673v1",
    "published_date": "2024-06-06 01:23:45 UTC",
    "updated_date": "2024-06-06 01:23:45 UTC"
  },
  {
    "arxiv_id": "2406.03671v2",
    "title": "PANDA: Expanded Width-Aware Message Passing Beyond Rewiring",
    "authors": [
      "Jeongwhan Choi",
      "Sumin Park",
      "Hyowon Wi",
      "Sung-Bae Cho",
      "Noseong Park"
    ],
    "abstract": "Recent research in the field of graph neural network (GNN) has identified a\ncritical issue known as \"over-squashing,\" resulting from the bottleneck\nphenomenon in graph structures, which impedes the propagation of long-range\ninformation. Prior works have proposed a variety of graph rewiring concepts\nthat aim at optimizing the spatial or spectral properties of graphs to promote\nthe signal propagation. However, such approaches inevitably deteriorate the\noriginal graph topology, which may lead to a distortion of information flow. To\naddress this, we introduce an expanded width-aware (PANDA) message passing, a\nnew message passing paradigm where nodes with high centrality, a potential\nsource of over-squashing, are selectively expanded in width to encapsulate the\ngrowing influx of signals from distant nodes. Experimental results show that\nour method outperforms existing rewiring methods, suggesting that selectively\nexpanding the hidden state of nodes can be a compelling alternative to graph\nrewiring for addressing the over-squashing.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.03671v2",
    "published_date": "2024-06-06 01:14:24 UTC",
    "updated_date": "2024-07-20 03:44:32 UTC"
  },
  {
    "arxiv_id": "2406.03668v1",
    "title": "3rd Place Solution for MOSE Track in CVPR 2024 PVUW workshop: Complex Video Object Segmentation",
    "authors": [
      "Xinyu Liu",
      "Jing Zhang",
      "Kexin Zhang",
      "Yuting Yang",
      "Licheng Jiao",
      "Shuyuan Yang"
    ],
    "abstract": "Video Object Segmentation (VOS) is a vital task in computer vision, focusing\non distinguishing foreground objects from the background across video frames.\nOur work draws inspiration from the Cutie model, and we investigate the effects\nof object memory, the total number of memory frames, and input resolution on\nsegmentation performance. This report validates the effectiveness of our\ninference method on the coMplex video Object SEgmentation (MOSE) dataset, which\nfeatures complex occlusions. Our experimental results demonstrate that our\napproach achieves a J\\&F score of 0.8139 on the test set, securing the third\nposition in the final ranking. These findings highlight the robustness and\naccuracy of our method in handling challenging VOS scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03668v1",
    "published_date": "2024-06-06 00:56:25 UTC",
    "updated_date": "2024-06-06 00:56:25 UTC"
  },
  {
    "arxiv_id": "2406.03665v2",
    "title": "Towards Dynamic Trend Filtering through Trend Point Detection with Reinforcement Learning",
    "authors": [
      "Jihyeon Seong",
      "Sekwang Oh",
      "Jaesik Choi"
    ],
    "abstract": "Trend filtering simplifies complex time series data by applying smoothness to\nfilter out noise while emphasizing proximity to the original data. However,\nexisting trend filtering methods fail to reflect abrupt changes in the trend\ndue to `approximateness,' resulting in constant smoothness. This\napproximateness uniformly filters out the tail distribution of time series\ndata, characterized by extreme values, including both abrupt changes and noise.\nIn this paper, we propose Trend Point Detection formulated as a Markov Decision\nProcess (MDP), a novel approach to identifying essential points that should be\nreflected in the trend, departing from approximations. We term these essential\npoints as Dynamic Trend Points (DTPs) and extract trends by interpolating them.\nTo identify DTPs, we utilize Reinforcement Learning (RL) within a discrete\naction space and a forecasting sum-of-squares loss function as a reward,\nreferred to as the Dynamic Trend Filtering network (DTF-net). DTF-net\nintegrates flexible noise filtering, preserving critical original subsequences\nwhile removing noise as required for other subsequences. We demonstrate that\nDTF-net excels at capturing abrupt changes compared to other trend filtering\nalgorithms and enhances forecasting performance, as abrupt changes are\npredicted rather than smoothed out.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.03665v2",
    "published_date": "2024-06-06 00:50:22 UTC",
    "updated_date": "2025-03-22 02:52:18 UTC"
  },
  {
    "arxiv_id": "2406.04379v1",
    "title": "VHDL-Eval: A Framework for Evaluating Large Language Models in VHDL Code Generation",
    "authors": [
      "Prashanth Vijayaraghavan",
      "Luyao Shi",
      "Stefano Ambrogio",
      "Charles Mackin",
      "Apoorva Nitsure",
      "David Beymer",
      "Ehsan Degan"
    ],
    "abstract": "With the unprecedented advancements in Large Language Models (LLMs), their\napplication domains have expanded to include code generation tasks across\nvarious programming languages. While significant progress has been made in\nenhancing LLMs for popular programming languages, there exists a notable gap in\ncomprehensive evaluation frameworks tailored for Hardware Description Languages\n(HDLs), particularly VHDL. This paper addresses this gap by introducing a\ncomprehensive evaluation framework designed specifically for assessing LLM\nperformance in VHDL code generation task. We construct a dataset for evaluating\nLLMs on VHDL code generation task. This dataset is constructed by translating a\ncollection of Verilog evaluation problems to VHDL and aggregating publicly\navailable VHDL problems, resulting in a total of 202 problems. To assess the\nfunctional correctness of the generated VHDL code, we utilize a curated set of\nself-verifying testbenches specifically designed for those aggregated VHDL\nproblem set. We conduct an initial evaluation of different LLMs and their\nvariants, including zero-shot code generation, in-context learning (ICL), and\nParameter-efficient fine-tuning (PEFT) methods. Our findings underscore the\nconsiderable challenges faced by existing LLMs in VHDL code generation,\nrevealing significant scope for improvement. This study emphasizes the\nnecessity of supervised fine-tuning code generation models specifically for\nVHDL, offering potential benefits to VHDL designers seeking efficient code\ngeneration solutions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "6 pages, 3 Figures, LAD'24",
    "pdf_url": "http://arxiv.org/pdf/2406.04379v1",
    "published_date": "2024-06-06 00:06:50 UTC",
    "updated_date": "2024-06-06 00:06:50 UTC"
  }
]
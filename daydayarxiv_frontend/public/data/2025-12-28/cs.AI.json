{
  "date": "2025-12-28",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å­¦æœ¯æ—¥æŠ¥ä¸“æ ä½œè€…ã€‚æ¬¢è¿æ¥åˆ° **UTC æ—¶é—´ 2025-12-28** çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n### ğŸš€ ä»Šæ—¥å¯¼è¯»\nä»Šå¤©çš„ arXiv æ›´æ–°å¯è°“æ˜¯**å¤§æ¨¡å‹æ¨ç†ï¼ˆReasoningï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**çš„ç››å®´ï¼ŒåŒæ—¶**åŒ»ç–— AI çš„è½åœ°åæ€**ä¹Ÿå æ®äº†é‡è¦ç‰ˆé¢ã€‚\næœ€ä»¤äººå°è±¡æ·±åˆ»çš„åŒ…æ‹¬ï¼šYann LeCun å›¢é˜Ÿå…³äº **JEPA ä¸–ç•Œæ¨¡å‹**åœ¨åŠ¨ä½œè§„åˆ’ä¸Šçš„æ–°è¿›å±•ï¼›ä¸€é¡¹ç”±**æœ¬ç§‘ç”Ÿä¸»å¯¼çš„æœ‰è¶£å®éªŒ**æ­ç¤ºäº†æ¶ˆè´¹çº§ Chatbot çš„â€œä¼ªæ¨ç†â€ç°è±¡ï¼›ä»¥åŠå¤šç¯‡å…³äº LLM åœ¨ RL è®­ç»ƒä¸­**å¯¹é½ï¼ˆAlignmentï¼‰ä¸é•¿ç¨‹ä»»åŠ¡**çš„ç¡¬æ ¸ç†è®ºæ–‡ç« ã€‚æ­¤å¤–ï¼Œå…³äº AI Agent è¢«ç½‘é¡µâ€œé»‘æš—æ¨¡å¼â€æ“æ§çš„ç ”ç©¶ä¹Ÿæ•²å“äº†å®‰å…¨è­¦é’Ÿã€‚\n\n---\n\n### ğŸ§  æ·±åº¦å­¦ä¹ ä¸å¤§æ¨¡å‹æ¨ç† (Reasoning & World Models)\n\n**1. [æ¨è] Value-guided action planning with JEPA world models**\n**åŸºäº JEPA ä¸–ç•Œæ¨¡å‹çš„ä»·å€¼å¼•å¯¼åŠ¨ä½œè§„åˆ’**\n> **Authors:** Matthieu Destrade, ... , Yann LeCun\n> **TLDR:** Yann LeCun å›¢é˜Ÿæ–°ä½œã€‚é€šè¿‡åœ¨ JEPAï¼ˆè”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼‰çš„è¡¨ç¤ºç©ºé—´ä¸­é€šè¿‡è·ç¦»é€¼è¿‘è´Ÿç›®æ ‡æ¡ä»¶ä»·å€¼å‡½æ•°ï¼Œæ˜¾è‘—æå‡äº†ä¸–ç•Œæ¨¡å‹åœ¨æ§åˆ¶ä»»åŠ¡ä¸­çš„è§„åˆ’èƒ½åŠ›ã€‚\n\n**2. [æœ‰è¶£] Can Consumer Chatbots Reason? A Student-Led Field Experiment Embedded in an \"AI-for-All\" Undergraduate Course**\n**æ¶ˆè´¹çº§èŠå¤©æœºå™¨äººèƒ½æ¨ç†å—ï¼Ÿä¸€é¡¹åµŒå…¥æœ¬ç§‘è¯¾ç¨‹çš„å­¦ç”Ÿä¸»å¯¼å®åœ°å®éªŒ**\n> **Authors:** Amarda Shehu, et al.\n> **TLDR:** è¿™æ˜¯ä¸€ç¯‡éå¸¸æœ‰æ„æ€çš„â€œéå…¸å‹â€è®ºæ–‡ã€‚ä¹”æ²»æ¢…æ£®å¤§å­¦çš„æœ¬ç§‘ç”Ÿä»¬è®¾è®¡äº†80ä¸ªæ¨ç†æç¤ºè¯æµ‹è¯• GPT-5, Claude 4.5 ç­‰æ¨¡å‹ã€‚ç»“è®ºå¾ˆæ‰å¿ƒï¼šæ¨¡å‹ç»å¸¸â€œå¬èµ·æ¥æ˜¯å¯¹çš„ï¼Œä½†æ¨ç†è¿‡ç¨‹æ˜¯é”™çš„â€ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´/è§†è§‰æ¨ç†ä¸Šè¡¨ç°ä¸ä½³ã€‚è¿™ä¸º AI ç´ å…»æ•™è‚²æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„èŒƒä¾‹ã€‚\n\n**3. Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization**\n**æ€ç»´é“¾ï¼ˆCoTï¼‰çœŸçš„ä¸å…·å¤‡å¯è§£é‡Šæ€§å—ï¼ŸCoT å³ä½¿ä¸è¨€è¯´æš—ç¤ºä¹Ÿèƒ½ä¿æŒå¿ å®**\n> **Authors:** Kerem Zaman, Shashank Srivastava\n> **TLDR:** åé©³äº†è¿‘æœŸè®¤ä¸º CoT ä¸å¿ å®ï¼ˆUnfaithfulï¼‰çš„è§‚ç‚¹ã€‚ä½œè€…è®¤ä¸ºç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡æ··æ·†äº†â€œä¸å¿ å®â€ä¸â€œæœ‰æŸå‹ç¼©â€ã€‚ç ”ç©¶å‘ç°ï¼Œå¢åŠ æ¨ç†æ—¶çš„ Token é¢„ç®—èƒ½æ˜¾è‘—æé«˜æç¤ºï¼ˆHintï¼‰çš„æ˜¾æ€§åŒ–ï¼Œä¸”å³ä½¿æœªæ˜¾æ€§åŒ–çš„æç¤ºä¹Ÿèƒ½é€šè¿‡å› æœä¸­ä»‹å½±å“é¢„æµ‹ã€‚\n\n**4. Eliminating Agentic Workflow for Introduction Generation with Parametric Stage Tokens**\n**åˆ©ç”¨å‚æ•°åŒ–é˜¶æ®µ Token æ¶ˆé™¤å­¦æœ¯å¼•è¨€ç”Ÿæˆçš„ Agent å·¥ä½œæµ**\n> **Authors:** Meicong Zhang, et al.\n> **TLDR:** é’ˆå¯¹å†™è®ºæ–‡ Introduction éš¾çš„é—®é¢˜ï¼Œä½œè€…æå‡º STIGï¼Œç”¨ç‰¹æ®Šçš„â€œé˜¶æ®µ Tokenâ€å°†å¤æ‚çš„ Agent å·¥ä½œæµå†…åŒ–åˆ°æ¨¡å‹å‚æ•°ä¸­ï¼Œå®ç°å•æ¬¡æ¨ç†ç”Ÿæˆé«˜è´¨é‡ã€é€»è¾‘ä¸¥å¯†çš„å¼•è¨€ï¼Œæ— éœ€å¤–éƒ¨å¤æ‚çš„ Agent è°ƒç”¨ã€‚\n\n---\n\n### ğŸ›¡ï¸ AI å®‰å…¨ã€Agent ä¸å¯¹é½ (Safety, Agents & Alignment)\n\n**5. [è­¦é’Ÿ] DECEPTICON: How Dark Patterns Manipulate Web Agents**\n**DECEPTICONï¼šé»‘æš—æ¨¡å¼å¦‚ä½•æ“æ§ Web Agent**\n> **Authors:** Phil Cuvin, Hao Zhu, Diyi Yang\n> **TLDR:** æ–¯å¦ç¦ Diyi Yang ç­‰äººçš„ç ”ç©¶ã€‚ç½‘é¡µè®¾è®¡ä¸­çš„â€œé»‘æš—æ¨¡å¼â€ï¼ˆè¯±å¯¼æ€§ UIï¼‰èƒ½æˆåŠŸè¯¯å¯¼ Web Agentï¼ˆå¦‚è¯¯å¯¼ç‚¹å‡»ã€é”™è¯¯è´­ä¹°ï¼‰ï¼ŒæˆåŠŸç‡é«˜è¾¾ 70% ä»¥ä¸Šã€‚ä¸ä»…å¦‚æ­¤ï¼Œæ¨¡å‹è¶Šå¤§ã€æ¨ç†èƒ½åŠ›è¶Šå¼ºï¼Œåè€Œè¶Šå®¹æ˜“è¢«è¿™äº›è®¾è®¡â€œå¿½æ‚ â€ã€‚\n\n**6. The Reward Model Selection Crisis in Personalized Alignment**\n**ä¸ªæ€§åŒ–å¯¹é½ä¸­çš„å¥–åŠ±æ¨¡å‹é€‰æ‹©å±æœº**\n> **Authors:** Fady Rezk, et al.\n> **TLDR:** åœ¨ä¸ªæ€§åŒ–å¯¹é½ä¸­ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰çš„æ’åºå‡†ç¡®ç‡é«˜å¹¶ä¸ä»£è¡¨ç”Ÿæˆçš„ç­–ç•¥å¥½ã€‚ç ”ç©¶å‘ç° RM çš„å‡†ç¡®ç‡ä¸ä¸‹æ¸¸ç­–ç•¥çš„å‡†ç¡®ç‡ä»…æœ‰å¾®å¼±ç›¸å…³æ€§ï¼Œå¹¶æå‡ºäº† Pref-LaMP åŸºå‡†æ¥æ­éœ²è¿™ä¸€è„±èŠ‚ç°è±¡ã€‚\n\n**7. Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory**\n**é€šè¿‡å¯éªŒè¯å¥–åŠ±ã€ç»éªŒåˆæˆå’ŒæŒç»­è®°å¿†å®ç° Agentic LLM çš„å®¡è®¡æŠ€èƒ½å›¾è‡ªæˆ‘æ”¹è¿›**\n> **Authors:** Ken Huang, Jerry Huang\n> **TLDR:** æå‡º ASG-SI æ¡†æ¶ï¼Œå°† Agent çš„è‡ªæˆ‘æ”¹è¿›è§†ä¸ºâ€œæŠ€èƒ½å›¾è°±â€çš„ç¼–è¯‘è¿‡ç¨‹ã€‚é€šè¿‡å¯éªŒè¯çš„å¥–åŠ±å’Œå®¡è®¡æ—¥å¿—ï¼Œè§£å†³ Agent è‡ªæˆ‘è¿›åŒ–ä¸­çš„â€œå¥–åŠ±é»‘å®¢â€å’Œè¡Œä¸ºæ¼‚ç§»é—®é¢˜ï¼Œå¼ºè°ƒäº† AI æ²»ç†çš„é‡è¦æ€§ã€‚\n\n---\n\n### ğŸ¥ åŒ»ç–— AI ä¸ç§‘å­¦å‘ç° (Medical AI & Science)\n\n**8. [å¿…è¯»] Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients**\n**åŸºå‡†æˆåŠŸï¼Œä¸´åºŠå¤±è´¥ï¼šå½“å¼ºåŒ–å­¦ä¹ åªä¸ºåˆ·æ¦œè€Œéä¸ºæ‚£è€…ä¼˜åŒ–**\n> **Authors:** Armin Berger, et al.\n> **TLDR:** æ­ç¤ºäº† RL åœ¨åŒ»ç–— VLM ä¸­çš„â€œæ³›åŒ–æ‚–è®ºâ€ã€‚è™½ç„¶ RLï¼ˆå¦‚ GRPOï¼‰åœ¨ç‰¹å®šåŸºå‡†ï¼ˆCheXpertï¼‰ä¸Šæå‡äº†æ€§èƒ½ï¼Œä½†åœ¨è·¨æ•°æ®é›†ï¼ˆNIHï¼‰ä¸Šæ€§èƒ½æš´è·Œ 19%ã€‚è¿™è¡¨æ˜ç›®å‰çš„ RL èŒƒå¼å¯èƒ½ç ´åäº†é¢„è®­ç»ƒæ¨¡å‹åœ¨è·¨æœºæ„æ•°æ®ä¸Šçš„é²æ£’æ€§ã€‚\n\n**9. HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery**\n**HiSciBenchï¼šä»é˜…è¯»åˆ°å‘ç°çš„åˆ†å±‚å¤šå­¦ç§‘ç§‘å­¦æ™ºèƒ½åŸºå‡†**\n> **Authors:** Yaping Zhang, et al.\n> **TLDR:** æå‡ºäº†ä¸€ä¸ªåŒ…å« 5 ä¸ªå±‚çº§çš„ç§‘å­¦èƒ½åŠ›è¯„ä¼°åŸºå‡†ï¼ˆä»è¯†å­—åˆ°ç§‘å­¦å‘ç°ï¼‰ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä¾¿æ˜¯ GPT-5 å’Œ DeepSeek-R1ï¼Œåœ¨åŸºç¡€ä»»åŠ¡ä¸Šå°šå¯ï¼Œä½†åœ¨â€œç§‘å­¦å‘ç°â€å±‚çº§ä¸Šå‡†ç¡®ç‡éª¤é™è‡³ 25%ï¼Œä»»é‡é“è¿œã€‚\n\n**10. How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure**\n**æ•°æ®å¤šå°‘æ‰å¤Ÿï¼Ÿä½ç»´ç»“æ„ä¸‹ç”Ÿæˆæ¨¡å‹ä¸ VLM çš„ä¸€è‡´æ”¶æ•›ç•Œ**\n> **Authors:** Paul M. Thompson\n> **TLDR:** ä»ç†è®ºå±‚é¢æ¢è®¨äº† VLM åœ¨åŒ»ç–—ç­‰é«˜é£é™©é¢†åŸŸçš„æ•°æ®éœ€æ±‚ã€‚è¯æ˜äº†å¦‚æœæ¨¡å‹è¾“å‡ºå¹³æ»‘åœ°ä¾èµ–äºä½ç»´è¯­ä¹‰è¡¨ç¤ºï¼Œå³ä½¿æ ·æœ¬é‡æœ‰é™ï¼Œä¹Ÿèƒ½è·å¾—æœ‰æ„ä¹‰çš„éæ¸è¿‘ä¿è¯ã€‚\n\n---\n\n### âš™ï¸ å¼ºåŒ–å­¦ä¹ ç†è®ºä¸ä¼˜åŒ– (RL Theory & Optimization)\n\n**11. Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning**\n**é©¯æœé•¿å°¾ï¼šé€šè¿‡åŠ¨æ€è¯è¡¨å‰ªæå®ç°ç¨³å®šçš„ LLM å¼ºåŒ–å­¦ä¹ **\n> **Authors:** Yingru Li, et al.\n> **TLDR:** è§£å†³äº† LLM RL è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚ä½œè€…è¯æ˜ä½æ¦‚ç‡ï¼ˆé•¿å°¾ï¼‰Token ä¼šå¯¼è‡´æ¢¯åº¦ä¼°è®¡çš„ç³»ç»Ÿæ€§åå·®ã€‚æå‡ºçš„æ–¹æ³•æ˜¯ï¼šåœ¨ RL è¿‡ç¨‹ä¸­åŠ¨æ€å‰ªæè¿™äº›é•¿å°¾ Tokenï¼Œä»¥å¾®å°çš„ä¼˜åŒ–åå·®æ¢å–è®­ç»ƒçš„ç¨³å®šæ€§ã€‚\n\n**12. Trust Region Masking for Long-Horizon LLM Reinforcement Learning**\n**ç”¨äºé•¿ç¨‹ LLM å¼ºåŒ–å­¦ä¹ çš„ä¿¡ä»»åŒºåŸŸæ©ç **\n> **Authors:** Yingru Li, et al.\n> **TLDR:** é’ˆå¯¹é•¿åºåˆ—ä»»åŠ¡ï¼Œä¼ ç»Ÿçš„ PPO ä¿¡ä»»åŒºåŸŸè¾¹ç•Œä¼šå¤±æ•ˆã€‚ä½œè€…æå‡ºäº†åŸºäº Token çº§æœ€å¤§ KL æ•£åº¦çš„æ›´ç´§è‡´è¾¹ç•Œï¼Œå¹¶è®¾è®¡äº† TRM ç®—æ³•ï¼Œè‹¥åºåˆ—ä¸­ä»»ä¸€ Token è¿åä¿¡ä»»åŒºåŸŸåˆ™å±è”½æ•´æ¡åºåˆ—ï¼Œä¿è¯äº†é•¿ç¨‹ RL çš„å•è°ƒæå‡ã€‚\n\n**13. A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms**\n**å…³äº LLM æ··åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸æ¨¡ä»¿å­¦ä¹ çš„æ³¨è®°ï¼šå…¬å¼ä¸ç®—æ³•**\n> **Authors:** Yingru Li, Ziniu Li, Jiacai Liu\n> **TLDR:** æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°† LLM å¾®è°ƒçš„ç›®æ ‡å‡½æ•°åˆ†è§£ä¸ºâ€œç¨ å¯†æ¢¯åº¦â€ï¼ˆç”¨äº Token çº§æ¨¡ä»¿ï¼‰å’Œâ€œç¨€ç–æ¢¯åº¦â€ï¼ˆç”¨äºé•¿ç¨‹å¥–åŠ±ä¼˜åŒ–ï¼‰ï¼Œå¹¶ç»™å‡ºäº†é«˜æ•ˆçš„ GPU å®ç°æ–¹æ¡ˆã€‚\n\n**14. FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents**\n**FoldActï¼šç”¨äºé•¿ç¨‹æœç´¢ Agent çš„é«˜æ•ˆç¨³å®šä¸Šä¸‹æ–‡æŠ˜å **\n> **Authors:** Jiaqi Shao, et al.\n> **TLDR:** é’ˆå¯¹é•¿ç¨‹ä»»åŠ¡ä¸­ä¸Šä¸‹æ–‡æ— é™å¢é•¿çš„é—®é¢˜ï¼Œæå‡º FoldActã€‚å®ƒé€šè¿‡åˆ†ç¦»æ‘˜è¦ Token å’ŒåŠ¨ä½œ Token çš„æ¢¯åº¦è®¡ç®—ï¼Œè§£å†³äº†ä¸Šä¸‹æ–‡æŠ˜å ï¼ˆContext Foldingï¼‰å¸¦æ¥çš„éå¹³ç¨³è§‚å¯Ÿåˆ†å¸ƒé—®é¢˜ï¼Œè®­ç»ƒé€Ÿåº¦æå‡ 5 å€ã€‚\n\n---\n\n### ğŸ¨ è®¡ç®—æœºè§†è§‰ä¸å¤šæ¨¡æ€ (Vision & Multimodal)\n\n**15. Deep Learning for Art Market Valuation**\n**æ·±åº¦å­¦ä¹ ç”¨äºè‰ºæœ¯å¸‚åœºä¼°å€¼**\n> **Authors:** Jianping Mei, et al.\n> **TLDR:** è‰ºæœ¯å“ä¼°å€¼ä¸ä»…çœ‹ç”»å®¶åæ°”ã€‚è¿™é¡¹ç ”ç©¶åˆ©ç”¨å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åˆ†æç”»ä½œçš„è§†è§‰å†…å®¹ï¼Œå‘ç°å¯¹äºâ€œé¦–æ¬¡å…¥å¸‚â€ä¸”ç¼ºä¹å†å²äº¤æ˜“é”šç‚¹çš„è‰ºæœ¯å“ï¼Œè§†è§‰åµŒå…¥èƒ½æä¾›æ˜¾è‘—çš„ä¼°å€¼è´¡çŒ®ã€‚\n\n**16. EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation**\n**EgoReActï¼šç¬¬ä¸€äººç§°è§†é¢‘é©±åŠ¨çš„ 3D äººä½“ååº”ç”Ÿæˆ**\n> **Authors:** Libo Zhang, et al.\n> **TLDR:** è¿™æ˜¯ä¸€ä¸ªä»ç¬¬ä¸€äººç§°è§†é¢‘ï¼ˆEgocentric Videoï¼‰ç”Ÿæˆ 3D äººä½“ååº”åŠ¨ä½œçš„æ¨¡å‹ã€‚ç›¸æ¯”ä»¥å¾€æ–¹æ³•ï¼Œå®ƒæ›´å¥½åœ°è§£å†³äº†ç©ºé—´å¯¹é½å’Œå› æœç”Ÿæˆçš„é—®é¢˜ï¼Œèƒ½åœ¨åŠ¨æ€åœºæ™¯ä¸‹ç”Ÿæˆé€¼çœŸçš„ååº”åŠ¨ä½œã€‚\n\n**17. Multimodal Functional Maximum Correlation for Emotion Recognition**\n**ç”¨äºæƒ…æ„Ÿè¯†åˆ«çš„å¤šæ¨¡æ€å‡½æ•°æœ€å¤§ç›¸å…³æ€§**\n> **Authors:** Deyang Zheng, et al.\n> **TLDR:** æå‡º MFMC æ¡†æ¶ï¼Œé€šè¿‡æœ€å¤§åŒ–å¤šæ¨¡æ€ä¾èµ–ï¼ˆè€Œéç®€å•çš„æˆå¯¹å¯¹é½ï¼‰æ¥æå‡æƒ…æ„Ÿè¯†åˆ«æ•ˆæœã€‚åœ¨å¤„ç†è„‘ç”µï¼ˆEEGï¼‰å’Œçš®è‚¤ç”µï¼ˆEDAï¼‰ä¿¡å·çš„è”åˆåŠ¨æ€ä¸Šè¡¨ç°å‡ºè‰²ã€‚\n\n---\n\n### ğŸ§± å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡\n\n*   **[Theory] Understanding the Mechanisms of Fast Hyperparameter Transfer (Paper 53):** è§£é‡Šäº†ä¸ºä½• $\\mu$Pï¼ˆæœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼‰èƒ½å®ç°è¶…å‚æ•°ä»è¿™ç§å°æ¨¡å‹åˆ°å¤§æ¨¡å‹çš„å¿«é€Ÿè¿ç§»ï¼Œæå‡ºäº†å®½åº¦ç¨³å®šå’Œå®½åº¦æ•æ„Ÿä¸¤ä¸ªåˆ†é‡çš„å‡è®¾ã€‚\n*   **[System] Viability and Performance of a Private LLM Server for SMBs (Paper 16):** è¯„æµ‹äº† Qwen3-30B åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šçš„è¡¨ç°ï¼Œè¯æ˜ä¸­å°ä¼ä¸šè‡ªå»ºç§æœ‰ LLM æœåŠ¡åœ¨æˆæœ¬å’Œæ€§èƒ½ä¸Šæ˜¯å¯è¡Œçš„ã€‚\n*   **[Code] FasterPy: An LLM-based Code Execution Efficiency Optimization Framework (Paper 41):** ç»“åˆ RAG å’Œ LoRA çš„æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ– Python ä»£ç çš„æ‰§è¡Œæ•ˆç‡ã€‚\n\n---\nå¸Œæœ›ä»Šå¤©çš„ arXiv å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼æ˜å¤©è§ï¼ğŸ‘‹",
  "papers": [
    {
      "arxiv_id": "2512.23109v1",
      "title": "How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure",
      "title_zh": "å¤šå°‘æ•°æ®æ‰è¶³å¤Ÿï¼Ÿä½ç»´ç»“æ„ä¸‹ç”Ÿæˆå¼æ¨¡å‹ä¸è§†è§‰-è¯­è¨€æ¨¡å‹çš„ä¸€è‡´æ”¶æ•›ç•Œ",
      "authors": [
        "Paul M. Thompson"
      ],
      "abstract": "Modern generative and vision-language models (VLMs) are increasingly used in scientific and medical decision support, where predicted probabilities must be both accurate and well calibrated. Despite strong empirical results with moderate data, it remains unclear when such predictions generalize uniformly across inputs, classes, or subpopulations, rather than only on average-a critical issue in biomedicine, where rare conditions and specific groups can exhibit large errors even when overall loss is low.\n  We study this question from a finite-sample perspective and ask: under what structural assumptions can generative and VLM-based predictors achieve uniformly accurate and calibrated behavior with practical sample sizes? Rather than analyzing arbitrary parameterizations, we focus on induced families of classifiers obtained by varying prompts or semantic embeddings within a restricted representation space. When model outputs depend smoothly on a low-dimensional semantic representation-an assumption supported by spectral structure in text and joint image-text embeddings-classical uniform convergence tools yield meaningful non-asymptotic guarantees.\n  Our main results give finite-sample uniform convergence bounds for accuracy and calibration functionals of VLM-induced classifiers under Lipschitz stability with respect to prompt embeddings. The implied sample complexity depends on intrinsic/effective dimension, not ambient embedding dimension, and we further derive spectrum-dependent bounds that make explicit how eigenvalue decay governs data requirements. We conclude with implications for data-limited biomedical modeling, including when current dataset sizes can support uniformly reliable predictions and why average calibration metrics may miss worst-case miscalibration.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç§‘å­¦å’ŒåŒ»ç–—å†³ç­–æ”¯æŒä¸­å®ç°ä¸€è‡´å‡†ç¡®æ€§å’Œæ ¡å‡†(calibration)æ‰€éœ€çš„æ•°æ®é‡é—®é¢˜ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨ä½ç»´ç»“æ„å‡è®¾ï¼Œå³å½“æ¨¡å‹è¾“å‡ºå¹³æ»‘åœ°ä¾èµ–äºä½ç»´è¯­ä¹‰è¡¨ç¤ºæ—¶ï¼Œåˆ©ç”¨æœ‰é™æ ·æœ¬è§†è§’å’ŒLipschitzç¨³å®šæ€§åˆ†æåˆ†ç±»å™¨çš„æ¨å¹¿æ€§èƒ½ã€‚é€šè¿‡æ¨å¯¼æœ‰é™æ ·æœ¬çš„ä¸€è‡´æ”¶æ•›ç•Œé™(uniform convergence bounds)ï¼Œç ”ç©¶å‘ç°æ ·æœ¬å¤æ‚åº¦ä¸»è¦å–å†³äºå†…åœ¨æœ‰æ•ˆç»´åº¦(intrinsic dimension)ï¼Œè€Œéç¯å¢ƒåµŒå…¥ç»´åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†åŸºäºç‰¹å¾å€¼è¡°å‡(eigenvalue decay)çš„å…‰è°±ç›¸å…³ç•Œé™ï¼Œæ­ç¤ºäº†å½“å‰æ•°æ®é›†è§„æ¨¡ä½•æ—¶èƒ½æ”¯æŒç»Ÿä¸€å¯é çš„é¢„æµ‹ã€‚æœ€åï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºäº†ç†è®ºç»“æœå¯¹æœ‰é™æ•°æ®ä¸‹ç”Ÿç‰©åŒ»å­¦å»ºæ¨¡çš„å¯ç¤ºï¼Œå¹¶é˜æ˜äº†ä¸ºä½•å¹³å‡æ ¡å‡†æŒ‡æ ‡å¯èƒ½æ©ç›–æœ€å·®æƒ…å†µä¸‹çš„è¯¯æ ¡å‡†(miscalibration)é—®é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.23109v1",
      "published_date": "2025-12-28 23:16:22 UTC",
      "updated_date": "2025-12-28 23:16:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:18:48.391527+00:00"
    },
    {
      "arxiv_id": "2601.04225v1",
      "title": "Can Consumer Chatbots Reason? A Student-Led Field Experiment Embedded in an \"AI-for-All\" Undergraduate Course",
      "title_zh": "æ¶ˆè´¹çº§èŠå¤©æœºå™¨äººå…·å¤‡æ¨ç†èƒ½åŠ›å—ï¼Ÿä¸€é¡¹ä¾æ‰˜äºâ€œAI-for-Allâ€æœ¬ç§‘è¯¾ç¨‹çš„å­¦ç”Ÿä¸»å¯¼å®åœ°å®éªŒ",
      "authors": [
        "Amarda Shehu",
        "Adonyas Ababu",
        "Asma Akbary",
        "Griffin Allen",
        "Aroush Baig",
        "Tereana Battle",
        "Elias Beall",
        "Christopher Byrom",
        "Matt Dean",
        "Kate Demarco",
        "Ethan Douglass",
        "Luis Granados",
        "Layla Hantush",
        "Andy Hay",
        "Eleanor Hay",
        "Caleb Jackson",
        "Jaewon Jang",
        "Carter Jones",
        "Quanyang Li",
        "Adrian Lopez",
        "Logan Massimo",
        "Garrett McMullin",
        "Ariana Mendoza Maldonado",
        "Eman Mirza",
        "Hadiya Muddasar",
        "Sara Nuwayhid",
        "Brandon Pak",
        "Ashley Petty",
        "Dryden Rancourt",
        "Lily Rodriguez",
        "Corbin Rogers",
        "Jacob Schiek",
        "Taeseo Seok",
        "Aarav Sethi",
        "Giovanni Vitela",
        "Winston Williams",
        "Jagan Yetukuri"
      ],
      "abstract": "Claims about whether large language model (LLM) chatbots \"reason\" are typically debated using curated benchmarks and laboratory-style evaluation protocols. This paper offers a complementary perspective: a student-led field experiment embedded as a midterm project in UNIV 182 (AI4All) at George Mason University, a Mason Core course designed for undergraduates across disciplines with no expected prior STEM exposure. Student teams designed their own reasoning tasks, ran them on widely used consumer chatbots representative of current capabilities, and evaluated both (i) answer correctness and (ii) the validity of the chatbot's stated reasoning (for example, cases where an answer is correct but the explanation is not, or vice versa). Across eight teams that reported standardized scores, students contributed 80 original reasoning prompts spanning six categories: pattern completion, transformation rules, spatial/visual reasoning, quantitative reasoning, relational/logic reasoning, and analogical reasoning. These prompts yielded 320 model responses plus follow-up explanations. Aggregating team-level results, OpenAI GPT-5 and Claude 4.5 achieved the highest mean answer accuracy (86.2% and 83.8%), followed by Grok 4 (82.5%) and Perplexity (73.1%); explanation validity showed a similar ordering (81.2%, 80.0%, 77.5%, 66.2%). Qualitatively, teams converged on a consistent error signature: strong performance on short, structured math and pattern items but reduced reliability on spatial/visual reasoning and multi-step transformations, with frequent \"sound right but reason wrong\" explanations. The assignment's primary contribution is pedagogical: it operationalizes AI literacy as experimental practice (prompt design, measurement, rater disagreement, and interpretability/grounding) while producing a reusable, student-generated corpus of reasoning probes grounded in authentic end-user interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åœ¨ä¹”æ²»æ¢…æ£®å¤§å­¦çš„ä¸€é—¨é¢å‘éSTEMä¸“ä¸šçš„æœ¬ç§‘è¯¾ç¨‹ä¸­åµŒå…¥å­¦ç”Ÿä¸»å¯¼çš„ç”°é‡å®éªŒï¼Œæ¢è®¨äº†æ¶ˆè´¹çº§èŠå¤©æœºå™¨äºº(Consumer Chatbots)çš„æ¨ç†èƒ½åŠ›ã€‚å­¦ç”Ÿå›¢é˜Ÿé’ˆå¯¹æ¨¡å¼è¡¥å…¨(Pattern Completion)ã€ç©ºé—´/è§†è§‰æ¨ç†(Spatial/Visual Reasoning)å’Œå®šé‡æ¨ç†(Quantitative Reasoning)ç­‰å…­ä¸ªç±»åˆ«è®¾è®¡äº†80ä¸ªåŸåˆ›æ¨ç†æç¤ºè¯(Prompts)ï¼Œå¹¶å¯¹ OpenAI GPT-5ã€Claude 4.5ã€Grok 4 å’Œ Perplexity è¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-5 å’Œ Claude 4.5 åœ¨ç­”æ¡ˆå‡†ç¡®ç‡ï¼ˆåˆ†åˆ«ä¸º86.2%å’Œ83.8%ï¼‰åŠè§£é‡Šæœ‰æ•ˆæ€§ä¸Šé¢†å…ˆï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨å¤„ç†ç©ºé—´æ¨ç†å’Œå¤šæ­¥å˜æ¢ä»»åŠ¡æ—¶è¡¨ç°å‡ºè¾ƒä½çš„å¯é æ€§ï¼Œä¸”å¸¸å‡ºç°â€œç­”æ¡ˆæ­£ç¡®ä½†æ¨ç†é”™è¯¯â€çš„ç‰¹å¾æ€§é”™è¯¯ã€‚è¯¥ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åœ¨äºæ•™å­¦æ³•åˆ›æ–°ï¼Œé€šè¿‡ Prompt Design å’Œå¯è§£é‡Šæ€§åˆ†æç­‰å®éªŒå®è·µå°† AI Literacy å…·è±¡åŒ–ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæºäºçœŸå®ç»ˆç«¯ç”¨æˆ·äº¤äº’çš„æ¨ç†æ¢æµ‹è¯­æ–™åº“ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.04225v1",
      "published_date": "2025-12-28 22:51:25 UTC",
      "updated_date": "2025-12-28 22:51:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:01.170657+00:00"
    },
    {
      "arxiv_id": "2512.23097v1",
      "title": "A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms",
      "title_zh": "LLM æ··åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸æ¨¡ä»¿å­¦ä¹ ç®€æï¼šå½¢å¼åŒ–å®šä¹‰ä¸ç®—æ³•",
      "authors": [
        "Yingru Li",
        "Ziniu Li",
        "Jiacai Liu"
      ],
      "abstract": "We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLM) çš„å¾®è°ƒæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†æ¨¡ä»¿å­¦ä¹  (Imitation Learning) ä¸å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) è¿›è¡Œæœ‰æ•ˆæ•´åˆã€‚é€šè¿‡å¯¹ç»“åˆäº†è½¨è¿¹çº§ KL æ•£åº¦ (KL divergence) ä¸ä»»åŠ¡å¥–åŠ± (task rewards) çš„å¤åˆç›®æ ‡è¿›è¡Œæ¢¯åº¦åˆ†æï¼Œæœ¬æ–‡æ¨å¯¼å‡ºäº†ä¸€ç§è‡ªç„¶çš„æ¢¯åº¦åˆ†è§£æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆç”±ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ç»„æˆï¼šä¸€æ˜¯ç”¨äºä»¤ç‰Œçº§ (token-level) æ¨¡ä»¿çš„ç¨ å¯†æ¢¯åº¦ (Dense Gradient)ï¼Œå…¶å¯é€šè¿‡è§£æè®¡ç®—å¾—å‡ºï¼›äºŒæ˜¯ç”¨äºé•¿ç¨‹å¥–åŠ±ä¼˜åŒ– (long-horizon reward optimization) çš„ç¨€ç–æ¢¯åº¦ (Sparse Gradient)ï¼Œé‡‡ç”¨è’™ç‰¹å¡æ´› (Monte Carlo) æ–¹æ³•è¿›è¡Œä¼°ç®—ã€‚å…¶ä¸­ï¼Œç¨ å¯†æ¢¯åº¦ (Dense Gradient) å…·å¤‡é—­å¼å¯¹æ•°çº§ (closed-form logit-level) å…¬å¼ï¼Œè¿™ä½¿å¾—åœ¨ GPU ä¸Šçš„ç®—æ³•å®ç°å˜å¾—éå¸¸é«˜æ•ˆã€‚è¯¥æ¡†æ¶ä¸ºç†è§£å’Œä¼˜åŒ– LLM çš„æ··åˆå¾®è°ƒæä¾›äº†ç†è®ºåŸºç¡€ä¸ç®—æ³•æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23097v1",
      "published_date": "2025-12-28 22:25:27 UTC",
      "updated_date": "2025-12-28 22:25:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:18:55.552046+00:00"
    },
    {
      "arxiv_id": "2512.23762v1",
      "title": "Drift-Based Dataset Stability Benchmark",
      "title_zh": "åŸºäºæ¼‚ç§»çš„æ•°æ®é›†ç¨³å®šæ€§åŸºå‡†",
      "authors": [
        "Dominik Soukup",
        "Richard PlnÃ½",
        "Daniel VaÅ¡ata",
        "TomÃ¡Å¡ ÄŒejka"
      ],
      "abstract": "Machine learning (ML) represents an efficient and popular approach for network traffic classification. However, network traffic classification is a challenging domain, and trained models may degrade soon after deployment due to the obsolete datasets and quick evolution of computer networks as new or updated protocols appear. Moreover, significant change in the behavior of a traffic type (and, therefore, the underlying features representing the traffic) can produce a large and sudden performance drop of the deployed model, known as a data or concept drift. In most cases, complete retraining is performed, often without further investigation of root causes, as good dataset quality is assumed. However, this is not always the case and further investigation must be performed. This paper proposes a novel methodology to evaluate the stability of datasets and a benchmark workflow that can be used to compare datasets.\n  The proposed framework is based on a concept drift detection method that also uses ML feature weights to boost the detection performance. The benefits of this work are demonstrated on CESNET-TLS-Year22 dataset. We provide the initial dataset stability benchmark that is used to describe dataset stability and weak points to identify the next steps for optimization. Lastly, using the proposed benchmarking methodology, we show the optimization impact on the created dataset variants.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç½‘ç»œæµé‡åˆ†ç±»ä¸­ç”±äºç½‘ç»œæ¼”è¿›å’Œæ•°æ®é›†è¿‡æ—¶å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è¯„ä¼°æ•°æ®é›†ç¨³å®šæ€§çš„åˆ›æ–°æ–¹æ³•è®ºåŠåŸºå‡†æµ‹è¯•(Benchmark)å·¥ä½œæµã€‚ç”±äºæ•°æ®æˆ–æ¦‚å¿µæ¼‚ç§»(Data/Concept Drift)å¸¸å¯¼è‡´æ¨¡å‹æ•ˆæœéª¤é™ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“åˆæœºå™¨å­¦ä¹ ç‰¹å¾æƒé‡(Feature Weights)çš„æ¼‚ç§»æ£€æµ‹æŠ€æœ¯ï¼Œå®ç°äº†å¯¹æ•°æ®é›†è´¨é‡çš„æ·±å…¥åˆ†æã€‚é€šè¿‡åœ¨CESNET-TLS-Year22æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†æ•°æ®é›†çš„ç¨³å®šæ€§å¼±ç‚¹å¹¶ç¡®å®šäº†ä¼˜åŒ–æ–¹å‘ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåˆ©ç”¨è¯¥åŸºå‡†æµ‹è¯•æ–¹æ³•å¯ä»¥æœ‰æ•ˆè¯„ä¼°ä¼˜åŒ–æªæ–½å¯¹æ•°æ®é›†å˜ä½“çš„å½±å“ï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„ç½‘ç»œæµé‡åˆ†ç±»æ¨¡å‹æä¾›äº†ç§‘å­¦çš„è¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.23762v1",
      "published_date": "2025-12-28 22:02:19 UTC",
      "updated_date": "2025-12-28 22:02:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:18:57.745580+00:00"
    },
    {
      "arxiv_id": "2512.23090v2",
      "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients",
      "title_zh": "åŸºå‡†æµ‹è¯•çš„æˆåŠŸï¼Œä¸´åºŠåº”ç”¨çš„å¤±è´¥ï¼šå½“å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„æ˜¯åŸºå‡†æµ‹è¯•è€Œéæ‚£è€…åˆ©ç›Š",
      "authors": [
        "Armin Berger",
        "Manuela Bergau",
        "Helen Schneider",
        "Saad Ahmad",
        "Tom Anglim Lagones",
        "Gianluca Brugnara",
        "Martha Foltyn-Dumitru",
        "Kai Schlamp",
        "Philipp Vollmuth",
        "Rafet Sifa"
      ],
      "abstract": "Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (RL)åœ¨å—é™èµ„æºä¸‹åº”ç”¨äºåŒ»ç–—å½±åƒçš„ç°çŠ¶ï¼Œå¹¶æå‡ºäº†ChexReasonæ¨¡å‹ï¼Œè¯¥è§†è§‰è¯­è¨€æ¨¡å‹é‡‡ç”¨R1é£æ ¼çš„æ–¹æ³•è®ºï¼Œä»…é€šè¿‡æå°‘é‡ç›‘ç£å¾®è°ƒ(SFT)å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)æ ·æœ¬åœ¨å•å¼ GPUä¸Šå®Œæˆè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶GRPOèƒ½å¤Ÿæœ‰æ•ˆæ¢å¤åˆ†å¸ƒå†…(in-distribution)çš„æ€§èƒ½ï¼Œä½¿CheXpertåŸºå‡†è¡¨ç°æå‡23%ï¼Œä½†å´æ˜¾è‘—å‰Šå¼±äº†è·¨æ•°æ®é›†è¿ç§»èƒ½åŠ›ï¼Œå¯¼è‡´NIHæµ‹è¯•è¡¨ç°ä¸‹é™19%ã€‚è¿™ç§æ€§èƒ½é¸¿æ²Ÿåœ¨NV-Reason-CXR-3Bç­‰é«˜èµ„æºæ¨¡å‹ä¸­åŒæ ·å­˜åœ¨ï¼Œè¡¨æ˜è¯¥é—®é¢˜æ ¹æºäºå¼ºåŒ–å­¦ä¹ èŒƒå¼è€Œéæ¨¡å‹è§„æ¨¡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†â€œæ³›åŒ–æ‚–è®ºâ€ï¼Œå³SFTé˜¶æ®µçš„æ¨¡å‹åœ¨RLä¼˜åŒ–å‰æ›´èƒ½æ•æ‰æœºæ„æ— å…³çš„ç‰¹å¾ï¼Œä¸”ç»“æ„åŒ–æ¨ç†å¯¹åŒ»å­¦é¢„è®­ç»ƒæ¨¡å‹çš„å¢ç›Šååˆ†æœ‰é™ã€‚æœ€ç»ˆç»“è®ºæŒ‡å‡ºï¼Œå¯¹äºéœ€è¦è·¨äººç¾¤é²æ£’æ€§çš„ä¸´åºŠéƒ¨ç½²åœºæ™¯ï¼Œç²¾å¿ƒè®¾è®¡çš„ç›‘ç£å¾®è°ƒ(SFT)å¯èƒ½æ¯”æ¿€è¿›çš„å¼ºåŒ–å­¦ä¹ (RL)æ›´å…·ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23090v2",
      "published_date": "2025-12-28 21:57:42 UTC",
      "updated_date": "2026-01-02 18:25:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:00.206796+00:00"
    },
    {
      "arxiv_id": "2512.23089v1",
      "title": "MedSAM-based lung masking for multi-label chest X-ray classification",
      "title_zh": "åŸºäº MedSAM è‚ºéƒ¨æ©è”½çš„å¤šæ ‡ç­¾èƒ¸éƒ¨ X å°„çº¿åˆ†ç±»",
      "authors": [
        "Brayden Miao",
        "Zain Rehman",
        "Xin Miao",
        "Siming Liu",
        "Jianjie Wang"
      ],
      "abstract": "Chest X-ray (CXR) imaging is widely used for screening and diagnosing pulmonary abnormalities, yet automated interpretation remains challenging due to weak disease signals, dataset bias, and limited spatial supervision. Foundation models for medical image segmentation (MedSAM) provide an opportunity to introduce anatomically grounded priors that may improve robustness and interpretability in CXR analysis. We propose a segmentation-guided CXR classification pipeline that integrates MedSAM as a lung region extraction module prior to multi-label abnormality classification. MedSAM is fine-tuned using a public image-mask dataset from Airlangga University Hospital. We then apply it to a curated subset of the public NIH CXR dataset to train and evaluate deep convolutional neural networks for multi-label prediction of five abnormalities (Mass, Nodule, Pneumonia, Edema, and Fibrosis), with the normal case (No Finding) evaluated via a derived score. Experiments show that MedSAM produces anatomically plausible lung masks across diverse imaging conditions. We find that masking effects are both task-dependent and architecture-dependent. ResNet50 trained on original images achieves the strongest overall abnormality discrimination, while loose lung masking yields comparable macro AUROC but significantly improves No Finding discrimination, indicating a trade-off between abnormality-specific classification and normal case screening. Tight masking consistently reduces abnormality level performance but improves training efficiency. Loose masking partially mitigates this degradation by preserving perihilar and peripheral context. These results suggest that lung masking should be treated as a controllable spatial prior selected to match the backbone and clinical objective, rather than applied uniformly.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºMedSAMçš„è‚ºéƒ¨æ©è†œï¼ˆlung maskingï¼‰å¼•å¯¼çš„èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰åˆ†ç±»æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³å¤šæ ‡ç­¾å¼‚å¸¸åˆ†ç±»ä¸­ç”±äºç—…ç¶ä¿¡å·å¾®å¼±å’Œç©ºé—´ç›‘ç£æœ‰é™å¯¼è‡´çš„è‡ªåŠ¨åŒ–è§£é‡Šéš¾é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç‰¹å®šçš„å›¾åƒæ©è†œæ•°æ®é›†å¯¹MedSAMè¿›è¡Œå¾®è°ƒä»¥æå–è‚ºéƒ¨åŒºåŸŸï¼Œå¹¶åœ¨NIH CXRæ•°æ®é›†ä¸Šè®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œï¼Œå¯¹Massã€Noduleã€Pneumoniaã€Edemaå’ŒFibrosisäº”ç§å¼‚å¸¸åŠæ­£å¸¸ï¼ˆNo Findingï¼‰æƒ…å†µè¿›è¡Œé¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMedSAMåœ¨å¤šç§å½±åƒæ¡ä»¶ä¸‹å‡èƒ½ç”Ÿæˆè§£å‰–å­¦åˆç†çš„è‚ºéƒ¨æ©è†œï¼Œä¸”æ©è†œå¯¹æ€§èƒ½çš„å½±å“å…·æœ‰ä»»åŠ¡å’Œæ¶æ„ä¾èµ–æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ResNet50åœ¨åŸå§‹å›¾åƒä¸Šå¯¹å¼‚å¸¸çš„åˆ¤åˆ«åŠ›æœ€å¼ºï¼Œä½†å®½æ¾æ©è†œï¼ˆloose lung maskingï¼‰åœ¨ä¿æŒç›¸è¿‘å®è§‚AUROCçš„åŒæ—¶æ˜¾è‘—æå‡äº†å¯¹æ­£å¸¸ç—…ä¾‹çš„ç­›æŸ¥èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç´§è‡´æ©è†œï¼ˆtight maskingï¼‰è™½ç„¶æå‡äº†è®­ç»ƒæ•ˆç‡ï¼Œä½†ä¼šé™ä½å¼‚å¸¸åˆ†ç±»çš„æ€§èƒ½ã€‚æœ€ç»ˆç»“è®ºè¡¨æ˜ï¼Œè‚ºéƒ¨æ©è†œåº”è¢«è§†ä¸ºä¸€ç§å¯æ§çš„ç©ºé—´å…ˆéªŒï¼ˆspatial priorï¼‰ï¼Œéœ€æ ¹æ®ç‰¹å®šçš„éª¨å¹²ç½‘ç»œå’Œä¸´åºŠç›®æ ‡çµæ´»é€‰ç”¨ï¼Œè€Œéä¸€æˆä¸å˜åœ°åº”ç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.23089v1",
      "published_date": "2025-12-28 21:56:41 UTC",
      "updated_date": "2025-12-28 21:56:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:03.223316+00:00"
    },
    {
      "arxiv_id": "2512.23087v1",
      "title": "Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning",
      "title_zh": "é©¯æœé•¿å°¾ï¼šåŸºäºåŠ¨æ€è¯è¡¨å‰ªæçš„ç¨³å®šå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yingru Li",
        "Jiawei Xu",
        "Jiacai Liu",
        "Yuxuan Tong",
        "Ziniu Li",
        "Tianle Cai",
        "Ge Zhang",
        "Qian Liu",
        "Baoxiang Wang"
      ],
      "abstract": "Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰ä¸­é«˜ååæ¨ç†å¼•æ“ä¸é«˜ç²¾åº¦è®­ç»ƒç³»ç»Ÿä¹‹é—´å­˜åœ¨çš„æ¦‚ç‡åˆ†å¸ƒä¸ä¸€è‡´é—®é¢˜è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç ”ç©¶è¯æ˜è¿™ç§ä¸ä¸€è‡´æ€§å¯¹ä½æ¦‚ç‡ Tokenï¼ˆTailï¼‰å…·æœ‰æ˜¾è‘—çš„ä¸å¯¹ç§°å½±å“ï¼Œå¯¼è‡´ç³»ç»Ÿæ€§åç½®åœ¨åºåˆ—ä¸­ç´¯ç§¯å¹¶ä½¿æ¢¯åº¦ä¼°è®¡ï¼ˆGradient Estimationï¼‰å¤±å»ç¨³å®šæ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŠ¨æ€è¯è¡¨å‰ªæï¼ˆDynamic Vocabulary Pruningï¼‰æ–¹æ¡ˆï¼Œé€šè¿‡å°† RL ç›®æ ‡å‡½æ•°é™åˆ¶åœ¨æ’é™¤æç«¯é•¿å°¾çš„â€œå®‰å…¨â€è¯è¡¨å†…ï¼Œä»æ ¹æºä¸Šç¼“è§£è®­ç»ƒä¸æ¨ç†çš„ä¸åŒ¹é…ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å°†å·¨å¤§çš„ç³»ç»Ÿæ€§åç½®æœ‰æ•ˆåœ°è½¬åŒ–ä¸ºå¾®å°ä¸”æœ‰ç•Œçš„ä¼˜åŒ–åå·®ï¼ˆOptimization Biasï¼‰ã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†è®­ç»ƒè¿‡ç¨‹çš„æ•°å€¼ç¨³å®šæ€§ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„é«˜æ•ˆå¯¹é½æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23087v1",
      "published_date": "2025-12-28 21:44:07 UTC",
      "updated_date": "2025-12-28 21:44:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:11.428250+00:00"
    },
    {
      "arxiv_id": "2512.23078v1",
      "title": "Deep Learning for Art Market Valuation",
      "title_zh": "æ·±åº¦å­¦ä¹ åœ¨è‰ºæœ¯å¸‚åœºä¼°å€¼ä¸­çš„åº”ç”¨",
      "authors": [
        "Jianping Mei",
        "Michael Moses",
        "Jan Waelty",
        "Yucheng Yang"
      ],
      "abstract": "We study how deep learning can improve valuation in the art market by incorporating the visual content of artworks into predictive models. Using a large repeated-sales dataset from major auction houses, we benchmark classical hedonic regressions and tree-based methods against modern deep architectures, including multi-modal models that fuse tabular and image data. We find that while artist identity and prior transaction history dominate overall predictive power, visual embeddings provide a distinct and economically meaningful contribution for fresh-to-market works where historical anchors are absent. Interpretability analyses using Grad-CAM and embedding visualizations show that models attend to compositional and stylistic cues. Our findings demonstrate that multi-modal deep learning delivers significant value precisely when valuation is hardest, namely first-time sales, and thus offers new insights for both academic research and practice in art market valuation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ (Deep Learning)å¦‚ä½•é€šè¿‡æ•´åˆè‰ºæœ¯å“çš„è§†è§‰å†…å®¹æ¥æå‡è‰ºæœ¯å¸‚åœºä¼°å€¼çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¥è‡ªä¸»è¦æ‹å–è¡Œçš„å¤§å‹é‡å¤é”€å”®æ•°æ®é›†ï¼Œå°†ä¼ ç»Ÿçš„äº«ä¹å›å½’(Hedonic Regressions)å’ŒåŸºäºæ ‘çš„æ–¹æ³•ä¸èåˆäº†è¡¨æ ¼åŠå›¾åƒæ•°æ®çš„ç°ä»£å¤šæ¨¡æ€(Multi-modal)æ·±åº¦å­¦ä¹ æ¶æ„è¿›è¡Œå¯¹æ¯”ã€‚å®éªŒå‘ç°ï¼Œè™½ç„¶è‰ºæœ¯å®¶èº«ä»½å’Œäº¤æ˜“å†å²åœ¨æ€»ä½“é¢„æµ‹ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†åœ¨ç¼ºä¹å†å²é”šç‚¹çš„â€œæ–°é²œå…¥å¸‚â€(Fresh-to-market)ä½œå“ä¸­ï¼Œè§†è§‰åµŒå…¥(Visual Embeddings)å±•ç°å‡ºäº†æ˜¾è‘—ä¸”å…·æœ‰ç»æµæ„ä¹‰çš„è´¡çŒ®ã€‚é€šè¿‡Grad-CAMå’ŒåµŒå…¥å¯è§†åŒ–ç­‰è§£é‡Šæ€§åˆ†æï¼Œç ”ç©¶è¯å®äº†æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«ä½œå“çš„æ„å›¾å’Œé£æ ¼ç‰¹å¾ã€‚è¯¥æˆæœè¯æ˜äº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åœ¨ä¼°å€¼æœ€å…·æŒ‘æˆ˜æ€§çš„é¦–æ¬¡é”€å”®(First-time sales)åœºæ™¯ä¸­å…·æœ‰æ˜¾è‘—ä»·å€¼ï¼Œä¸ºè‰ºæœ¯å¸‚åœºä¼°å€¼çš„å­¦æœ¯ç ”ç©¶å’Œå®è·µåº”ç”¨æä¾›äº†æ–°çš„è§è§£ã€‚",
      "categories": [
        "q-fin.GN",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "econ.GN"
      ],
      "primary_category": "q-fin.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23078v1",
      "published_date": "2025-12-28 21:04:09 UTC",
      "updated_date": "2025-12-28 21:04:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:11.887527+00:00"
    },
    {
      "arxiv_id": "2512.23076v1",
      "title": "Multimodal Functional Maximum Correlation for Emotion Recognition",
      "title_zh": "é¢å‘æƒ…æ„Ÿè¯†åˆ«çš„å¤šæ¨¡æ€æ³›å‡½æœ€å¤§ç›¸å…³",
      "authors": [
        "Deyang Zheng",
        "Tianyi Zhang",
        "Wenming Zheng",
        "Shujian Yu"
      ],
      "abstract": "Emotional states manifest as coordinated yet heterogeneous physiological responses across central and autonomic systems, posing a fundamental challenge for multimodal representation learning in affective computing. Learning such joint dynamics is further complicated by the scarcity and subjectivity of affective annotations, which motivates the use of self-supervised learning (SSL). However, most existing SSL approaches rely on pairwise alignment objectives, which are insufficient to characterize dependencies among more than two modalities and fail to capture higher-order interactions arising from coordinated brain and autonomic responses.\n  To address this limitation, we propose Multimodal Functional Maximum Correlation (MFMC), a principled SSL framework that maximizes higher-order multimodal dependence through a Dual Total Correlation (DTC) objective. By deriving a tight sandwich bound and optimizing it using a functional maximum correlation analysis (FMCA) based trace surrogate, MFMC captures joint multimodal interactions directly, without relying on pairwise contrastive losses.\n  Experiments on three public affective computing benchmarks demonstrate that MFMC consistently achieves state-of-the-art or competitive performance under both subject-dependent and subject-independent evaluation protocols, highlighting its robustness to inter-subject variability. In particular, MFMC improves subject-dependent accuracy on CEAP-360VR from 78.9% to 86.8%, and subject-independent accuracy from 27.5% to 33.1% using the EDA signal alone. Moreover, MFMC remains within 0.8 percentage points of the best-performing method on the most challenging EEG subject-independent split of MAHNOB-HCI. Our code is available at https://github.com/DY9910/MFMC.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Multimodal Functional Maximum Correlation (MFMC)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³æƒ…æ„Ÿè¯†åˆ«ä¸­å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ éš¾é¢˜çš„è‡ªç›‘ç£å­¦ä¹  (Self-Supervised Learning) æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–ä¸¤ä¸¤å¯¹é½ (Pairwise Alignment) è€Œæ— æ³•æ•æ‰å¤šä¸ªæ¨¡æ€é—´é«˜é˜¶äº¤äº’ (Higher-order Interactions) çš„å±€é™ï¼ŒMFMCé€šè¿‡åŒå…¨ç›¸å…³ (Dual Total Correlation, DTC) ç›®æ ‡å‡½æ•°æ¥æœ€å¤§åŒ–å¤šæ¨¡æ€ä¾èµ–ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ³›å‡½æœ€å¤§ç›¸å…³åˆ†æ (Functional Maximum Correlation Analysis, FMCA) çš„è¿¹ä»£ç†æ¥ä¼˜åŒ–ç´§è‡´å¤¹é€¼ç•Œ (Tight Sandwich Bound)ï¼Œä»è€Œç›´æ¥æ•æ‰è”åˆå¤šæ¨¡æ€äº¤äº’è€Œæ— éœ€ä¾èµ–æˆå¯¹å¯¹æ¯”æŸå¤±ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMFMCåœ¨ä¸‰ä¸ªå…¬å…±æƒ…æ„Ÿè®¡ç®—åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›æˆ–å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå¯¹å—è¯•è€…é—´å·®å¼‚ (Inter-subject Variability) çš„å¼ºå¤§é²æ£’æ€§ã€‚åœ¨CEAP-360VRæ•°æ®é›†ä¸Šï¼Œè¯¥æ¡†æ¶å°†åŸºäºEDAä¿¡å·çš„å—è¯•è€…ç›¸å…³å‡†ç¡®ç‡ä»78.9%æ˜¾è‘—æå‡è‡³86.8%ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚ç”Ÿç†ä¿¡å·åè°ƒååº”æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "manuscript currently under review at IEEE journals, 20 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.23076v1",
      "published_date": "2025-12-28 20:48:02 UTC",
      "updated_date": "2025-12-28 20:48:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:19.006295+00:00"
    },
    {
      "arxiv_id": "2512.23075v1",
      "title": "Trust Region Masking for Long-Horizon LLM Reinforcement Learning",
      "title_zh": "é¢å‘é•¿ç¨‹å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„ç½®ä¿¡åŸŸæ©ç ",
      "authors": [
        "Yingru Li",
        "Jiacai Liu",
        "Jiawei Xu",
        "Yuxuan Tong",
        "Ziniu Li",
        "Baoxiang Wang"
      ],
      "abstract": "Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $Ï€_{\\text{roll}}$. When $Ï€_{\\text{roll}} \\ne Ï€_Î¸$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ (LLM-RL)ä¸­ï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨é•¿ç¨‹(Long-Horizon)ä»»åŠ¡ä¸‹å› ä»£ç†ç›®æ ‡å‡½æ•°è¿‘ä¼¼è¯¯å·®å¯¼è‡´çš„å¤±æ•ˆé—®é¢˜ã€‚ä½œè€…å‘ç°ä¼ ç»Ÿçš„ä¿¡ä»»åŒºåŸŸ(Trust Region)è¯¯å·®ç•Œé™éšåºåˆ—é•¿åº¦ $T$ å‘ˆ $O(T^2)$ å¢é•¿ï¼Œåœ¨é•¿åºåˆ—ä¸­å¾€å¾€å¤±å»æŒ‡å¯¼æ„ä¹‰ï¼Œä¸ºæ­¤æ¨å¯¼å‡ºäº†æ›´ç´§è‡´çš„Pinsker-Marginalç•Œé™($O(T^{3/2})$)å’Œæ··åˆç•Œé™(Mixed bound, $O(T)$)ã€‚è¿™äº›æ–°ç•Œé™è¡¨æ˜ï¼Œè¯¯å·®æ ¸å¿ƒå–å†³äºåºåˆ—ä¸­æœ€å¤§çš„ä»¤ç‰Œçº§KLæ•£åº¦($D_{kl}^{tok,max}$)ï¼Œè€Œè¿™ä¸€åºåˆ—çº§å±æ€§æ— æ³•é€šè¿‡PPOè£å‰ª(PPO clipping)ç­‰ç‹¬ç«‹äºä»¤ç‰Œçš„æ–¹æ³•æœ‰æ•ˆæ§åˆ¶ã€‚æ®æ­¤ç ”ç©¶æå‡ºäº†ä¿¡ä»»åŒºåŸŸæ©ç (Trust Region Masking, TRM)æŠ€æœ¯ï¼Œè‹¥åºåˆ—ä¸­ä»»ä¸€ä»¤ç‰Œè¿åä¿¡ä»»åŒºåŸŸåˆ™å°†æ•´ä¸ªåºåˆ—æ’é™¤åœ¨æ¢¯åº¦è®¡ç®—ä¹‹å¤–ã€‚è¯¥æ–¹æ³•ä¸ºé•¿ç¨‹LLM-RLæä¾›äº†é¦–ä¸ªéå¹³å‡¡çš„å•è°ƒæå‡ä¿è¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹åˆ†å¸ƒå¼è®­ç»ƒè¿Ÿæ»å’Œæ··åˆä¸“å®¶æ¨¡å‹(MoE)è·¯ç”±ä¸è¿ç»­å¸¦æ¥çš„ç­–ç•¥åå·®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23075v1",
      "published_date": "2025-12-28 20:41:59 UTC",
      "updated_date": "2025-12-28 20:41:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:24.371847+00:00"
    },
    {
      "arxiv_id": "2512.23067v2",
      "title": "The Reward Model Selection Crisis in Personalized Alignment",
      "title_zh": "ä¸ªæ€§åŒ–å¯¹é½ä¸­çš„å¥–åŠ±æ¨¡å‹é€‰æ‹©å±æœº",
      "authors": [
        "Fady Rezk",
        "Yuangang Pan",
        "Chuan-Sheng Foo",
        "Xun Xu",
        "Nancy Chen",
        "Henry Gouk",
        "Timothy Hospedales"
      ],
      "abstract": "Personalized alignment from preference data has focused primarily on improving personal reward model (RM) accuracy, with the implicit assumption that better preference ranking translates to better personalized behavior. However, in deployment, computational constraints necessitate inference-time adaptation such as reward-guided decoding (RGD) rather than per-user policy fine-tuning. This creates a critical but overlooked requirement: reward models must not only rank preferences accurately but also effectively guide generation. We demonstrate that standard RM accuracy fails catastrophically as a selection criterion for deployment-ready personalized rewards. We introduce policy accuracy; a metric quantifying whether RGD-adapted LLMs correctly discriminate between preferred and dispreferred responses and show that upstream RM accuracy correlates only weakly with downstream policy accuracy (Kendall's tau = 0.08--0.31). More critically, we introduce Pref-LaMP the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioural evaluation. On Pref-LaMP, we expose a complete decoupling between discriminative ranking and generation metrics: methods with 20-point RM accuracy differences produce almost identical output quality, and methods with high ranking accuracy can fail to generate behaviorally aligned responses. These findings reveal that the field has been optimizing for proxy metrics that do not predict deployment performance, and that current personalized alignment methods fail to operationalize preferences into behavioral adaptation under realistic deployment constraints. In contrast, we find simple in-context learning (ICL) to be highly effective - dominating all reward-guided methods for models $\\geq$3B parameters, achieving $\\sim$3 point ROUGE-1 gains over the best reward method at 7B scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ªæ€§åŒ–å¯¹é½(Personalized Alignment)ä¸­çš„å¥–åŠ±æ¨¡å‹é€‰æ‹©å±æœºï¼ŒæŒ‡å‡ºæå‡å¥–åŠ±æ¨¡å‹(Reward Model, RM)çš„å‡†ç¡®ç‡å¹¶ä¸ä¸€å®šèƒ½æ”¹å–„å®é™…çš„ä¸ªæ€§åŒ–è¡Œä¸ºã€‚ç ”ç©¶å¼ºè°ƒï¼Œåœ¨å—é™çš„éƒ¨ç½²ç¯å¢ƒä¸‹ï¼Œå¥–åŠ±æ¨¡å‹ä¸ä»…éœ€è¦å‡†ç¡®æ’åºåå¥½ï¼Œè¿˜å¿…é¡»èƒ½æœ‰æ•ˆå¼•å¯¼å¥–åŠ±å¼•å¯¼è§£ç (Reward-Guided Decoding, RGD)è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ç­–ç•¥å‡†ç¡®ç‡(Policy Accuracy)æŒ‡æ ‡ï¼Œå¹¶å‘ç°ä¼ ç»Ÿçš„RMå‡†ç¡®ç‡ä¸ä¸‹æ¸¸ç­–ç•¥è¡¨ç°çš„ç›¸å…³æ€§æå¼±ã€‚é€šè¿‡å¼•å…¥é¦–ä¸ªå¸¦æœ‰çœŸå®ç”¨æˆ·è¡¥å…¨æ•°æ®çš„åŸºå‡†Pref-LaMPï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†åˆ¤åˆ«å¼æ’åºå‡†ç¡®ç‡ä¸ç”Ÿæˆè´¨é‡ä¹‹é—´çš„ä¸¥é‡è„±èŠ‚ï¼Œå³é«˜å‡†ç¡®ç‡çš„RMå¯èƒ½æ— æ³•ç”Ÿæˆè¡Œä¸ºå¯¹é½çš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å‚æ•°é‡å¤§äº3Bçš„æ¨¡å‹ä¸­ï¼Œç®€å•çš„ä¸Šä¸‹æ–‡å­¦ä¹ (In-Context Learning, ICL)æ•ˆæœä¼˜äºæ‰€æœ‰å¥–åŠ±å¼•å¯¼æ–¹æ³•ï¼Œåœ¨7Bè§„æ¨¡ä¸‹æ¯”æœ€ä½³å¥–åŠ±æ–¹æ³•æå‡äº†çº¦3ä¸ªROUGE-1åˆ†æ•°ã€‚è¿™ä¸€å‘ç°æç¤ºå½“å‰é¢†åŸŸæ­£åœ¨ä¼˜åŒ–æ— æ³•é¢„æµ‹éƒ¨ç½²è¡¨ç°çš„ä»£ç†æŒ‡æ ‡ï¼Œä¸ºç°å®çº¦æŸä¸‹çš„ä¸ªæ€§åŒ–åå¥½æ“ä½œåŒ–æä¾›äº†é‡è¦ä¿®æ­£ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23067v2",
      "published_date": "2025-12-28 20:27:15 UTC",
      "updated_date": "2026-01-07 20:10:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:44.959166+00:00"
    },
    {
      "arxiv_id": "2601.00844v1",
      "title": "Value-guided action planning with JEPA world models",
      "title_zh": "åŸºäºJEPAä¸–ç•Œæ¨¡å‹çš„ä»·å€¼å¼•å¯¼åŠ¨ä½œè§„åˆ’",
      "authors": [
        "Matthieu Destrade",
        "Oumayma Bounou",
        "Quentin Le Lidec",
        "Jean Ponce",
        "Yann LeCun"
      ],
      "abstract": "Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹ç¯å¢ƒåŠ¨æ€çš„æ¨ç†èƒ½åŠ›ï¼Œé‡ç‚¹æ”¹è¿›äº†Joint-Embedded Predictive Architectures (JEPA) æ¡†æ¶åœ¨åŠ¨ä½œè§„åˆ’ (action planning) æ–¹é¢çš„å±€é™æ€§ã€‚ä½œè€…æå‡ºé€šè¿‡å¡‘é€ è¡¨ç¤ºç©ºé—´ï¼Œä½¿ç›®æ ‡æ¡ä»¶ä»·å€¼å‡½æ•° (goal-conditioned value function) çš„è´Ÿå€¼èƒ½å¤Ÿç”±çŠ¶æ€åµŒå…¥ (state embeddings) ä¹‹é—´çš„è·ç¦»æˆ–æ‹Ÿè·ç¦»æ¥è¿‘ä¼¼ï¼Œä»è€Œå°†ä»·å€¼å¯¼å‘èå…¥è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§åœ¨è®­ç»ƒæœŸé—´å¼ºåˆ¶æ‰§è¡Œè¯¥çº¦æŸçš„å®ç”¨æ–¹æ³•ï¼Œç¡®ä¿ä¸–ç•Œæ¨¡å‹ (world models) åœ¨æ•æ‰åŠ¨æ€ç‰¹æ€§çš„åŒæ—¶ä¼˜åŒ–è§„åˆ’è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡† JEPA æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç®€å•æ§åˆ¶ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†è§„åˆ’æ€§èƒ½ (planning performance)ã€‚è¿™ä¸€æ”¹è¿›ä¸ºæ„å»ºå…·å¤‡é«˜æ•ˆæ¨ç†å’Œè§„åˆ’èƒ½åŠ›çš„è‡ªç›‘ç£å­¦ä¹ æ¶æ„æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented as a poster at the World Modeling Workshop 2026, Mila",
      "pdf_url": "https://arxiv.org/pdf/2601.00844v1",
      "published_date": "2025-12-28 20:17:49 UTC",
      "updated_date": "2025-12-28 20:17:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:41.719848+00:00"
    },
    {
      "arxiv_id": "2512.23760v1",
      "title": "Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory",
      "title_zh": "åŸºäºå¯éªŒè¯å¥–åŠ±ã€ç»éªŒåˆæˆä¸æŒç»­è®°å¿†çš„æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ç»å®¡è®¡æŠ€èƒ½å›¾è°±è‡ªæˆ‘æ”¹è¿›",
      "authors": [
        "Ken Huang",
        "Jerry Huang"
      ],
      "abstract": "Reinforcement learning is increasingly used to transform large language models into agentic systems that act over long horizons, invoke tools, and manage memory under partial observability. While recent work has demonstrated performance gains through tool learning, verifiable rewards, and continual training, deployed self-improving agents raise unresolved security and governance challenges: optimization pressure can incentivize reward hacking, behavioral drift is difficult to audit or reproduce, and improvements are often entangled in opaque parameter updates rather than reusable, verifiable artifacts.\n  This paper proposes Audited Skill-Graph Self-Improvement (ASG-SI), a framework that treats self-improvement as iterative compilation of an agent into a growing, auditable skill graph. Each candidate improvement is extracted from successful trajectories, normalized into a skill with an explicit interface, and promoted only after passing verifier-backed replay and contract checks. Rewards are decomposed into reconstructible components derived from replayable evidence, enabling independent audit of promotion decisions and learning signals. ASG-SI further integrates experience synthesis for scalable stress testing and continual memory control to preserve long-horizon performance under bounded context.\n  We present a complete system architecture, threat model, and security analysis, and provide a fully runnable reference implementation that demonstrates verifier-backed reward construction, skill compilation, audit logging, and measurable improvement under continual task streams. ASG-SI reframes agentic self-improvement as accumulation of verifiable, reusable capabilities, offering a practical path toward reproducible evaluation and operational governance of self-improving AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Audited Skill-Graph Self-Improvement (ASG-SI) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) è‡ªä¸»æ™ºèƒ½ä½“åœ¨è‡ªæˆ‘æ”¹è¿›è¿‡ç¨‹ä¸­é¢ä¸´çš„ reward hackingã€è¡Œä¸ºæ¼‚ç§»åŠç¼ºä¹å¯å®¡è®¡æ€§ç­‰å®‰å…¨ä¸æ²»ç†æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†è‡ªæˆ‘æ”¹è¿›è¿‡ç¨‹è§†ä¸ºå°†æ™ºèƒ½ä½“è¿­ä»£ç¼–è¯‘ä¸ºå¯å®¡è®¡çš„ skill graphï¼Œä»æˆåŠŸè½¨è¿¹ä¸­æå–å¹¶è§„èŒƒåŒ–å…·å¤‡æ˜ç¡®æ¥å£çš„æŠ€èƒ½ã€‚ASG-SI è¦æ±‚æ”¹è¿›é¡¹å¿…é¡»é€šè¿‡åŸºäºéªŒè¯å™¨çš„å›æ”¾ (verifier-backed replay) å’Œå¥‘çº¦æ£€æŸ¥ (contract checks) åæ–¹å¯æ™‹å‡ï¼Œå¹¶å°†å¥–åŠ±åˆ†è§£ä¸ºå¯é‡æ„ç»„ä»¶ä»¥æ”¯æŒç‹¬ç«‹å®¡è®¡ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé›†æˆäº† experience synthesis ç”¨äºè§„æ¨¡åŒ–å‹åŠ›æµ‹è¯•ï¼Œå¹¶å¼•å…¥ continual memory æ§åˆ¶ä»¥åœ¨æœ‰é™ä¸Šä¸‹æ–‡ä¸‹ä¿æŒé•¿æ—¶ç¨‹æ€§èƒ½ã€‚é€šè¿‡æä¾›å®Œæ•´çš„ç³»ç»Ÿæ¶æ„å’Œå¯è¿è¡Œçš„å‚è€ƒå®ç°ï¼ŒASG-SI å°†æ™ºèƒ½ä½“çš„è‡ªæˆ‘æ”¹è¿›é‡æ„ä¸ºå¯éªŒè¯ä¸”å¯é‡ç”¨èƒ½åŠ›çš„ç§¯ç´¯ï¼Œä¸ºè‡ªæˆ‘æ”¹è¿›å‹ AI æ™ºèƒ½ä½“çš„å¯é‡å¤è¯„ä¼°å’Œè¿è¥æ²»ç†æä¾›äº†å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 4 figures. Includes a complete runnable reference implementation and audit logging framework",
      "pdf_url": "https://arxiv.org/pdf/2512.23760v1",
      "published_date": "2025-12-28 19:39:47 UTC",
      "updated_date": "2025-12-28 19:39:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:47.591342+00:00"
    },
    {
      "arxiv_id": "2512.23036v1",
      "title": "Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨å­¦ä¹ è€…å»ºæ¨¡ä¸­çš„å±€é™æ€§ï¼šè®ºå…¶ä¸ºä½•æ— æ³•ç‹¬ç«‹èƒœä»» K-12 æ•™è‚²ä¸­è´Ÿè´£ä»»çš„æ™ºèƒ½è¾…å¯¼",
      "authors": [
        "Danial Hooshyar",
        "Yeongwook Yang",
        "Gustav Å Ã­Å™",
        "Tommi KÃ¤rkkÃ¤inen",
        "Raija HÃ¤mÃ¤lÃ¤inen",
        "Mutlu Cukurova",
        "Roger Azevedo"
      ],
      "abstract": "The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\\% over the zero-shot baseline, it remains 6\\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨K-12æ•™è‚²ä¸­åˆ©ç”¨Large Language Models (LLMs)è¿›è¡Œå­¦ä¹ è€…å»ºæ¨¡çš„å±€é™æ€§ï¼Œæ—¨åœ¨æ­ç¤ºç”Ÿæˆå¼æ¨¡å‹æ— æ³•å®Œå…¨å–ä»£ä¼ ç»Ÿè‡ªé€‚åº”æ•™å­¦å»ºæ¨¡çš„åŸå› ã€‚é€šè¿‡å°†Deep Knowledge Tracing (DKT)æ¨¡å‹ä¸zero-shotåŠfine-tunedçŠ¶æ€ä¸‹çš„LLMè¿›è¡Œå¯¹æ¯”ï¼Œç ”ç©¶å‘ç°DKTåœ¨é¢„æµ‹å­¦ç”Ÿä¸‹ä¸€æ­¥ç­”é¢˜æ­£ç¡®æ€§æ–¹é¢è¡¨ç°æœ€ä¼˜ï¼Œå…¶AUCè¾¾åˆ°0.83ä¸”æŒç»­ä¼˜äºLLMã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿ç»è¿‡198å°æ—¶çš„é«˜ç®—åŠ›fine-tuningï¼ŒLLMçš„AUCä»æ¯”DKTä½6%ï¼Œä¸”åœ¨åºåˆ—æ—©æœŸé˜¶æ®µå®¹æ˜“äº§ç”Ÿé”™è¯¯é¢„æµ‹ï¼Œè¿™å¯¹è‡ªé€‚åº”æ”¯æŒå…·æœ‰æ½œåœ¨å±å®³ã€‚æ—¶é—´ä¸€è‡´æ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼ŒDKTèƒ½å¤Ÿæä¾›ç¨³å®šä¸”æ–¹å‘æ­£ç¡®çš„çŸ¥è¯†æŒæ¡åº¦æ›´æ–°ï¼Œè€ŒLLMå˜ä½“åˆ™è¡¨ç°å‡ºä¸¥é‡çš„æ—¶é—´ä¸ä¸€è‡´æ€§å’Œè½¨è¿¹è¿è´¯æ€§é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®šæ€§åˆ†æä¹Ÿè¯å®äº†LLMåœ¨å¤šæŠ€èƒ½æŒæ¡åº¦ä¼°è®¡ä¸Šçš„ä¸ç¨³å®šæ€§ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œç ”ç©¶è®¤ä¸ºå•é LLMséš¾ä»¥åŒ¹é…æˆç†Ÿæ™ºèƒ½æ•™å­¦ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œæå€¡æ„å»ºç»“åˆäº†ä¼ ç»Ÿå­¦ä¹ è€…å»ºæ¨¡çš„hybrid frameworksä»¥å®ç°è´Ÿè´£ä»»çš„æ•™è‚²AIã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23036v1",
      "published_date": "2025-12-28 18:26:22 UTC",
      "updated_date": "2025-12-28 18:26:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:48.802276+00:00"
    },
    {
      "arxiv_id": "2512.23032v1",
      "title": "Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization",
      "title_zh": "é“¾å¼æ€ç»´çœŸçš„ä¸å…·å¤‡å¯è§£é‡Šæ€§å—ï¼Ÿæ— é¡»æç¤ºè¯è¯­è¨€åŒ–ï¼Œé“¾å¼æ€ç»´äº¦èƒ½ä¿æŒå¿ å®æ€§",
      "authors": [
        "Kerem Zaman",
        "Shashank Srivastava"
      ],
      "abstract": "Recent work, using the Biasing Features metric, labels a CoT as unfaithful if it omits a prompt-injected hint that affected the prediction. We argue this metric confuses unfaithfulness with incompleteness, the lossy compression needed to turn distributed transformer computation into a linear natural language narrative. On multi-hop reasoning tasks with Llama-3 and Gemma-3, many CoTs flagged as unfaithful by Biasing Features are judged faithful by other metrics, exceeding 50% in some models. With a new faithful@k metric, we show that larger inference-time token budgets greatly increase hint verbalization (up to 90% in some settings), suggesting much apparent unfaithfulness is due to tight token limits. Using Causal Mediation Analysis, we further show that even non-verbalized hints can causally mediate prediction changes through the CoT. We therefore caution against relying solely on hint-based evaluations and advocate a broader interpretability toolkit, including causal mediation and corruption-based metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Chain-of-Thought (CoT) çš„å¿ å®æ€§é—®é¢˜ï¼ŒæŒ‘æˆ˜äº†æ­¤å‰åŸºäº Biasing Features æŒ‡æ ‡å°†æœªæ˜¾å¼æåŠæç¤ºè¯çš„æ¨ç†è¿‡ç¨‹åˆ¤å®šä¸ºä¸å¿ å®çš„è§‚ç‚¹ã€‚ä½œè€…æŒ‡å‡ºï¼Œè¯¥æŒ‡æ ‡æ··æ·†äº†â€œä¸å¿ å®æ€§â€ä¸â€œä¸å®Œæ•´æ€§â€ï¼Œå³æ¨¡å‹å°†åˆ†å¸ƒå¼è®¡ç®—å‹ç¼©ä¸ºçº¿æ€§è‡ªç„¶è¯­è¨€æ—¶äº§ç”Ÿçš„å¿…ç„¶æŸè€—ã€‚åœ¨ Llama-3 å’Œ Gemma-3 çš„å¤šè·³æ¨ç†ä»»åŠ¡ä¸­ï¼Œå®éªŒè¡¨æ˜è®¸å¤šè¢«åˆ¤å®šä¸ºä¸å¿ å®çš„ CoT åœ¨å…¶ä»–åº¦é‡æ ‡å‡†ä¸‹è¡¨ç°å‡ºé«˜åº¦å¿ å®ã€‚ç ”ç©¶æå‡ºäº†æ–°çš„ faithful@k æŒ‡æ ‡ï¼Œè¯æ˜å¢åŠ æ¨ç† Token é¢„ç®—èƒ½æ˜¾è‘—æå‡ hint verbalizationï¼Œæš—ç¤ºæ‰€è°“çš„ä¸å¿ å®å¾ˆå¤§ç¨‹åº¦ä¸Šæºäºç´§å‡‘çš„ Token é™åˆ¶ã€‚åˆ©ç”¨ Causal Mediation Analysis è¿›ä¸€æ­¥è¯å®ï¼Œå³ä½¿æç¤ºè¯æœªåœ¨æ–‡æœ¬ä¸­æ˜¾å¼è¡¨è¾¾ï¼Œä»èƒ½é€šè¿‡ CoT è·¯å¾„å› æœæ€§åœ°ä¸­ä»‹é¢„æµ‹ç»“æœçš„å˜åŒ–ã€‚åŸºäºæ­¤ï¼Œä½œè€…å»ºè®®ç ”ç©¶è€…ä¸åº”ä»…ä¾èµ–åŸºäºæç¤ºè¯çš„è¯„ä¼°ï¼Œè€Œåº”é‡‡ç”¨åŒ…å«å› æœä¸­ä»‹å’Œ corruption-based æŒ‡æ ‡åœ¨å†…çš„æ›´å¹¿æ³›çš„å¯è§£é‡Šæ€§å·¥å…·åŒ…ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 20 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.23032v1",
      "published_date": "2025-12-28 18:18:02 UTC",
      "updated_date": "2025-12-28 18:18:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:49.689930+00:00"
    },
    {
      "arxiv_id": "2512.23029v1",
      "title": "Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware",
      "title_zh": "ä¸­å°ä¼ä¸šç§æœ‰ LLM æœåŠ¡å™¨çš„å¯è¡Œæ€§ä¸æ€§èƒ½ï¼šåŸºäºæ¶ˆè´¹çº§ç¡¬ä»¶çš„ Qwen3-30B åŸºå‡†è¯„æµ‹åˆ†æ",
      "authors": [
        "Alex Khalil",
        "Guillaume Heilles",
        "Maria Parraga",
        "Simon Heilles"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) has been accompanied by a reliance on cloud-based, proprietary systems, raising significant concerns regarding data privacy, operational sovereignty, and escalating costs. This paper investigates the feasibility of deploying a high-performance, private LLM inference server at a cost accessible to Small and Medium Businesses (SMBs). We present a comprehensive benchmarking analysis of a locally hosted, quantized 30-billion parameter Mixture-of-Experts (MoE) model based on Qwen3, running on a consumer-grade server equipped with a next-generation NVIDIA GPU. Unlike cloud-based offerings, which are expensive and complex to integrate, our approach provides an affordable and private solution for SMBs. We evaluate two dimensions: the model's intrinsic capabilities and the server's performance under load. Model performance is benchmarked against academic and industry standards to quantify reasoning and knowledge relative to cloud services. Concurrently, we measure server efficiency through latency, tokens per second, and time to first token, analyzing scalability under increasing concurrent users. Our findings demonstrate that a carefully configured on-premises setup with emerging consumer hardware and a quantized open-source model can achieve performance comparable to cloud-based services, offering SMBs a viable pathway to deploy powerful LLMs without prohibitive costs or privacy compromises.",
      "tldr_zh": "é’ˆå¯¹ä¸­å°å‹ä¼ä¸š (SMBs) åœ¨ä½¿ç”¨äº‘ç«¯å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ—¶é¢ä¸´çš„æ•°æ®éšç§ã€è¿è¥ä¸»æƒå’Œæˆæœ¬ä¸Šå‡ç­‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶è¯„ä¼°äº†åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šéƒ¨ç½²é«˜æ€§èƒ½ç§æœ‰æ¨ç†æœåŠ¡å™¨çš„å¯è¡Œæ€§ã€‚ç ”ç©¶é€šè¿‡å¯¹ä¸€ä¸ªåŸºäº Qwen3 çš„ 300 äº¿å‚æ•°é‡åŒ–æ··åˆä¸“å®¶æ¨¡å‹ (Mixture-of-Experts, MoE) åœ¨é…å¤‡ä¸‹ä¸€ä»£ NVIDIA GPU çš„æœåŠ¡å™¨ä¸Šè¿›è¡ŒåŸºå‡†åˆ†æï¼Œæä¾›äº†ä¸€ç§ç»æµä¸”ç§å¯†çš„è§£å†³æ–¹æ¡ˆã€‚è¯„ä¼°è¿‡ç¨‹æ¶µç›–äº†æ¨¡å‹åœ¨æ¨ç†ä¸çŸ¥è¯†æ–¹é¢çš„å†…åœ¨èƒ½åŠ›ï¼Œä»¥åŠåœ¨å¹¶å‘è´Ÿè½½ä¸‹çš„å»¶è¿Ÿã€æ¯ç§’ç”Ÿæˆè¯æ•° (tokens per second) å’Œé¦–å­—æ—¶é—´ (time to first token) ç­‰æ€§èƒ½æŒ‡æ ‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œé€šè¿‡åˆç†é…ç½®å¼€æºé‡åŒ–æ¨¡å‹ä¸æ¶ˆè´¹çº§ç¡¬ä»¶ï¼Œæœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆå¯ä»¥è¾¾åˆ°ä¸äº‘ç«¯æœåŠ¡ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥å‘ç°ä¸ºä¸­å°å‹ä¼ä¸šåœ¨ä¿éšœéšç§å’Œæ§åˆ¶æˆæœ¬çš„å‰æä¸‹ï¼Œåˆ©ç”¨å¼ºå¤§çš„ LLMs æŠ€æœ¯å¼€è¾Ÿäº†æœ‰æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23029v1",
      "published_date": "2025-12-28 18:08:01 UTC",
      "updated_date": "2025-12-28 18:08:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:19:56.274203+00:00"
    },
    {
      "arxiv_id": "2512.23028v1",
      "title": "An Architecture-Led Hybrid Report on Body Language Detection Project",
      "title_zh": "è‚¢ä½“è¯­è¨€æ£€æµ‹é¡¹ç›®ï¼šä»¥æ¶æ„ä¸ºå¯¼å‘çš„ç»¼åˆæŠ¥å‘Š",
      "authors": [
        "Thomson Tong",
        "Diba Darooneh"
      ],
      "abstract": "This report provides an architecture-led analysis of two modern vision-language models (VLMs), Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct, and explains how their architectural properties map to a practical video-to-artifact pipeline implemented in the BodyLanguageDetection repository [1]. The system samples video frames, prompts a VLM to detect visible people and generate pixel-space bounding boxes with prompt-conditioned attributes (emotion by default), validates output structure using a predefined schema, and optionally renders an annotated video. We first summarize the shared multimodal foundation (visual tokenization, Transformer attention, and instruction following), then describe each architecture at a level sufficient to justify engineering choices without speculative internals. Finally, we connect model behavior to system constraints: structured outputs can be syntactically valid while semantically incorrect, schema validation is structural (not geometric correctness), person identifiers are frame-local in the current prompting contract, and interactive single-frame analysis returns free-form text rather than schema-enforced JSON. These distinctions are critical for writing defensible claims, designing robust interfaces, and planning evaluation.",
      "tldr_zh": "è¯¥æŠ¥å‘Šé‡‡ç”¨æ¶æ„å¯¼å‘çš„æ–¹æ³•åˆ†æäº† Qwen2.5-VL-7B-Instruct å’Œ Llama-4-Scout-17B-16E-Instruct ä¸¤ç§è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs)ï¼Œå¹¶æ¢è®¨äº†å…¶æ¶æ„å±æ€§å¦‚ä½•åº”ç”¨äºè‚¢ä½“è¯­è¨€æ£€æµ‹é¡¹ç›®çš„ video-to-artifact æµæ°´çº¿ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è§†é¢‘å¸§é‡‡æ ·ï¼Œåˆ©ç”¨ VLM æ£€æµ‹äººä½“å¹¶ç”Ÿæˆå¸¦æœ‰æƒ…ç»ªå±æ€§çš„è¾¹ç•Œæ¡† (bounding boxes)ï¼ŒåŒæ—¶ä½¿ç”¨é¢„å®šä¹‰æ¨¡å¼ (schema) éªŒè¯è¾“å‡ºç»“æ„ã€‚ç ”ç©¶æ€»ç»“äº†è§†è§‰æ ‡è®°åŒ– (visual tokenization) å’Œ Transformer attention ç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œæ—¨åœ¨ä¸ºå·¥ç¨‹é€‰æ‹©æä¾›æ¶æ„å±‚é¢çš„è¯æ˜ã€‚æŠ¥å‘Šè¿˜æ­ç¤ºäº†æ¨¡å‹è¡Œä¸ºä¸ç³»ç»Ÿçº¦æŸä¹‹é—´çš„å…³é”®è”ç³»ï¼ŒæŒ‡å‡ºç»“æ„åŒ–è¾“å‡ºå³ä¾¿è¯­æ³•æ­£ç¡®ä¹Ÿå¯èƒ½å­˜åœ¨è¯­ä¹‰åå·®ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­æ˜ç¡®äº†è¡Œäººæ ‡è¯†ç¬¦ (person identifiers) åœ¨å½“å‰ prompting å¥‘çº¦ä¸‹çš„å¸§å†…å±€éƒ¨æ€§ï¼Œä»¥åŠäº¤äº’å¼åˆ†æä¸­è‡ªç”±æ–‡æœ¬ä¸ JSON æ ¼å¼çš„å·®å¼‚ã€‚è¿™äº›åˆ†æå¯¹äºè®¾è®¡é²æ£’çš„æ¥å£ã€è§„åˆ’ç³»ç»Ÿè¯„ä¼°ä»¥åŠæ’°å†™ä¸¥è°¨çš„æŠ€æœ¯ä¸»å¼ è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.23028v1",
      "published_date": "2025-12-28 18:03:00 UTC",
      "updated_date": "2025-12-28 18:03:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:03.673045+00:00"
    },
    {
      "arxiv_id": "2512.23025v1",
      "title": "LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models",
      "title_zh": "LENSï¼šé€šè¿‡å¤šæ¨¡æ€æ„ŸçŸ¥ä¸è¯­è¨€æ¨¡å‹å¯¹é½å®ç°å¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½çš„å¿ƒç†å¥åº·å™äº‹åˆæˆ",
      "authors": [
        "Wenxuan Xu",
        "Arvind Pillai",
        "Subigya Nepal",
        "Amanda C Collins",
        "Daniel M Mackin",
        "Michael V Heinz",
        "Tess Z Griffin",
        "Nicholas C Jacobson",
        "Andrew Campbell"
      ],
      "abstract": "Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by transforming Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM's representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LENSæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†å¤šæ¨¡æ€æ„ŸçŸ¥æ•°æ®(Multimodal sensing)ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹é½ï¼Œç”Ÿæˆå…·æœ‰ä¸´åºŠåŸºç¡€çš„å¿ƒç†å¥åº·å™è¿°ï¼Œä»¥è§£å†³æ•°å€¼å‹æ—¶é—´åºåˆ—æµ‹é‡å€¼éš¾ä»¥è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœæ¨¡å‹éš¾ä»¥ç›´æ¥å¤„ç†é•¿æ—¶ä¼ æ„Ÿå™¨æµå’Œé…å¯¹æ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼ŒLENSé€šè¿‡å°†æŠ‘éƒå’Œç„¦è™‘ç—‡çŠ¶ç›¸å…³çš„ç”Ÿæ€ç¬æ—¶è¯„ä¼°(Ecological Momentary Assessment, EMA)è½¬åŒ–ä¸ºè¯­è¨€æè¿°ï¼Œä»258åå‚ä¸è€…ä¸­æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡10ä¸‡ä¸ªä¼ æ„Ÿå™¨-æ–‡æœ¬é—®ç­”å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚åœ¨æŠ€æœ¯å®ç°ä¸Šï¼Œè¯¥æ¡†æ¶è®­ç»ƒäº†ä¸€ä¸ªè¡¥ä¸çº§ç¼–ç å™¨(patch-level encoder)ï¼Œå°†åŸå§‹ä¼ æ„Ÿå™¨ä¿¡å·ç›´æ¥æ˜ å°„åˆ°LLMçš„è¡¨ç¤ºç©ºé—´ï¼Œå®ç°äº†åŸç”Ÿæ—¶é—´åºåˆ—çš„é›†æˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLENSåœ¨æ ‡å‡†NLPæŒ‡æ ‡å’Œç—‡çŠ¶ä¸¥é‡ç¨‹åº¦å‡†ç¡®æ€§(symptom-severity accuracy)æ–¹é¢å‡ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ã€‚ç”±13åå¿ƒç†å¥åº·ä¸“ä¸šäººå£«å‚ä¸çš„ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼ŒLENSç”Ÿæˆçš„å™è¿°å…·æœ‰å…¨é¢æ€§å’Œä¸´åºŠæ„ä¹‰ã€‚è¯¥æ–¹æ³•ä¸ºåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºå¥åº·æ„ŸçŸ¥æ¥å£ã€æ”¯æŒä¸´åºŠå†³ç­–å¹¶æ¨ç†åŸå§‹è¡Œä¸ºä¿¡å·æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "22 pages, 9 figures, under review",
      "pdf_url": "https://arxiv.org/pdf/2512.23025v1",
      "published_date": "2025-12-28 18:00:57 UTC",
      "updated_date": "2025-12-28 18:00:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:05.076211+00:00"
    },
    {
      "arxiv_id": "2512.23020v2",
      "title": "OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding",
      "title_zh": "OpenGroundï¼šé¢å‘å¼€æ”¾ä¸–ç•Œ 3D è§†è§‰å®šä½çš„ä¸»åŠ¨è®¤çŸ¥æ¨ç†",
      "authors": [
        "Wenyuan Huang",
        "Zhao Wang",
        "Zhou Wei",
        "Ting Huang",
        "Fang Zhao",
        "Jian Yang",
        "Zhenyu Zhang"
      ],
      "abstract": "3D visual grounding aims to locate objects based on natural language descriptions in 3D scenes. Existing methods rely on a pre-defined Object Lookup Table (OLT) to query Visual Language Models (VLMs) for reasoning about object locations, which limits the applications in scenarios with undefined or unforeseen targets. To address this problem, we present OpenGround, a novel zero-shot framework for open-world 3D visual grounding. Central to OpenGround is the Active Cognition-based Reasoning (ACR) module, which is designed to overcome the fundamental limitation of pre-defined OLTs by progressively augmenting the cognitive scope of VLMs. The ACR module performs human-like perception of the target via a cognitive task chain and actively reasons about contextually relevant objects, thereby extending VLM cognition through a dynamically updated OLT. This allows OpenGround to function with both pre-defined and open-world categories. We also propose a new dataset named OpenTarget, which contains over 7000 object-description pairs to evaluate our method in open-world scenarios. Extensive experiments demonstrate that OpenGround achieves competitive performance on Nr3D, state-of-the-art on ScanRefer, and delivers a substantial 17.6% improvement on OpenTarget. Project Page at https://why-102.github.io/openground.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OpenGroundï¼Œä¸€ç§é’ˆå¯¹å¼€æ”¾ä¸–ç•Œ3Dè§†è§‰å®šä½(3D Visual Grounding)çš„æ–°å‹é›¶æ ·æœ¬(zero-shot)æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–é¢„å®šä¹‰å¯¹è±¡æŸ¥æ‰¾è¡¨(Object Lookup Table, OLT)å¯¼è‡´æ— æ³•å¤„ç†æœªçŸ¥ç›®æ ‡çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºä¸»åŠ¨è®¤çŸ¥çš„æ¨ç†(Active Cognition-based Reasoning, ACR)æ¨¡å—ã€‚ACRæ¨¡å—é€šè¿‡è®¤çŸ¥ä»»åŠ¡é“¾æ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥ï¼Œå¹¶å¯¹ä¸Šä¸‹æ–‡ç›¸å…³çš„ç‰©ä½“è¿›è¡Œä¸»åŠ¨æ¨ç†ï¼Œä»è€Œé€šè¿‡åŠ¨æ€æ›´æ–°çš„OLTæœ‰æ•ˆæ‰©å±•äº†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„è®¤çŸ¥èŒƒå›´ã€‚è¿™ä½¿å¾—OpenGroundèƒ½å¤ŸåŒæ—¶èƒœä»»é¢„å®šä¹‰ç±»åˆ«å’Œå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹çš„å®šä½ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æå‡ºäº†åŒ…å«è¶…è¿‡7000ä¸ªç‰©ä½“-æè¿°å¯¹çš„OpenTargetæ•°æ®é›†ï¼Œç”¨äºä¸“é—¨è¯„ä¼°å¼€æ”¾ä¸–ç•Œæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenGroundåœ¨ScanReferä¸Šè¾¾åˆ°äº†é¢†åŸŸæœ€å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨OpenTargetæ•°æ®é›†ä¸Šå–å¾—äº†17.6%çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "27 pages, 15 figures, 14 tables, Project Page at https://why-102.github.io/openground.io/",
      "pdf_url": "https://arxiv.org/pdf/2512.23020v2",
      "published_date": "2025-12-28 17:44:20 UTC",
      "updated_date": "2025-12-31 10:56:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:02.572350+00:00"
    },
    {
      "arxiv_id": "2601.09730v1",
      "title": "Clinical Document Metadata Extraction: A Scoping Review",
      "title_zh": "ä¸´åºŠæ–‡æ¡£å…ƒæ•°æ®æŠ½å–ï¼šèŒƒå›´ç»¼è¿°",
      "authors": [
        "Kurt Miller",
        "Qiuhao Lu",
        "William Hersh",
        "Kirk Roberts",
        "Steven Bedrick",
        "Andrew Wen",
        "Hongfang Liu"
      ],
      "abstract": "Clinical document metadata, such as document type, structure, author role, medical specialty, and encounter setting, is essential for accurate interpretation of information captured in clinical documents. However, vast documentation heterogeneity and drift over time challenge harmonization of document metadata. Automated extraction methods have emerged to coalesce metadata from disparate practices into target schema. This scoping review aims to catalog research on clinical document metadata extraction, identify methodological trends and applications, and highlight gaps. We followed the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines to identify articles that perform clinical document metadata extraction. We initially found and screened 266 articles published between January 2011 and August 2025, then comprehensively reviewed 67 we deemed relevant to our study. Among the articles included, 45 were methodological, 17 used document metadata as features in a downstream application, and 5 analyzed document metadata composition. We observe myriad purposes for methodological study and application types. Available labelled public data remains sparse except for structural section datasets. Methods for extracting document metadata have progressed from largely rule-based and traditional machine learning with ample feature engineering to transformer-based architectures with minimal feature engineering. The emergence of large language models has enabled broader exploration of generalizability across tasks and datasets, allowing the possibility of advanced clinical text processing systems. We anticipate that research will continue to expand into richer document metadata representations and integrate further into clinical applications and workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸´åºŠæ–‡æ¡£å…ƒæ•°æ®ï¼ˆå¦‚ document type, author role å’Œ medical specialty ç­‰ï¼‰çš„è‡ªåŠ¨æå–è¿›è¡Œäº†èŒƒå›´ç»¼è¿°(Scoping Review)ï¼Œæ—¨åœ¨æ¢³ç†è¯¥é¢†åŸŸçš„æ–¹æ³•å­¦è¶‹åŠ¿ã€åº”ç”¨åœºæ™¯åŠç°å­˜ç ”ç©¶ç©ºç™½ã€‚ä½œè€…éµå¾ª PRISMA-ScR æŒ‡å—ï¼Œå¯¹ 2011 å¹´è‡³ 2025 å¹´é—´å‘è¡¨çš„ 67 ç¯‡ç›¸å…³æ–‡çŒ®è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œæ¶µç›–æ–¹æ³•å­¦ç ”ç©¶ã€ä¸‹æ¸¸åº”ç”¨åŠç»„æˆåˆ†æç­‰ç»´åº¦ã€‚ç ”ç©¶è§‚å¯Ÿåˆ°ï¼Œæå–æ–¹æ³•å·²ä»æ—©æœŸçš„åŸºäºè§„åˆ™(rule-based)å’Œä¾èµ–å¤§é‡ç‰¹å¾å·¥ç¨‹çš„ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼Œæ¼”è¿›ä¸ºåŸºäº Transformer æ¶æ„ä»¥åŠåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)çš„å…ˆè¿›æŠ€æœ¯ã€‚å°½ç®¡ LLMs æ˜¾è‘—æå‡äº†è·¨ä»»åŠ¡çš„é€šç”¨æ€§å¹¶æ¨åŠ¨äº†ä¸´åºŠæ–‡æœ¬å¤„ç†ç³»ç»Ÿçš„è¿›æ­¥ï¼Œä½†é™¤ç»“æ„åŒ–ç« èŠ‚å¤–ï¼Œé«˜è´¨é‡çš„æ ‡æ³¨å…¬å¼€æ•°æ®é›†ä¾ç„¶åŒ®ä¹ã€‚è¯¥ç»¼è¿°æœ€åæŒ‡å‡ºï¼Œæœªæ¥ç ”ç©¶å°†æŒç»­æ‰©å±•æ›´ä¸°å¯Œçš„å…ƒæ•°æ®è¡¨ç¤ºï¼Œå¹¶è¿›ä¸€æ­¥æ•´åˆåˆ°ä¸´åºŠå®é™…åº”ç”¨å’Œå·¥ä½œæµä¸­ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.09730v1",
      "published_date": "2025-12-28 17:21:16 UTC",
      "updated_date": "2025-12-28 17:21:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:06.984672+00:00"
    },
    {
      "arxiv_id": "2512.22999v1",
      "title": "JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference",
      "title_zh": "JADAIï¼šè‡ªé€‚åº”è®¾è®¡ä¸è´å¶æ–¯æ¨æ–­çš„è”åˆæ‘Šé”€",
      "authors": [
        "Niels Bracher",
        "Lars KÃ¼hmichel",
        "Desi R. Ivanova",
        "Xavier Intes",
        "Paul-Christian BÃ¼rkner",
        "Stefan T. Radev"
      ],
      "abstract": "We consider problems of parameter estimation where design variables can be actively optimized to maximize information gain. To this end, we introduce JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, a history network, and an inference network end-to-end. The networks minimize a generic loss that aggregates incremental reductions in posterior error along experimental sequences. Inference networks are instantiated with diffusion-based posterior estimators that can approximate high-dimensional and multimodal posteriors at every experimental step. Across standard adaptive design benchmarks, JADAI achieves superior or competitive performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† JADAI æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è®¾è®¡å˜é‡å¯è¢«ä¸»åŠ¨ä¼˜åŒ–ä»¥æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šçš„å‚æ•°ä¼°è®¡é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç«¯åˆ°ç«¯åœ°è®­ç»ƒ Policyã€History Network å’Œ Inference Networkï¼Œå®ç°äº† Bayesian Adaptive Design ä¸æ¨æ–­çš„è”åˆæ‘Šé”€ã€‚ç³»ç»Ÿé€šè¿‡æœ€å°åŒ–ä¸€ä¸ªèšåˆäº†å®éªŒåºåˆ—ä¸­åéªŒè¯¯å·® (Posterior Error) å¢é‡å‡å°‘çš„é€šç”¨æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–ç½‘ç»œæ€§èƒ½ã€‚Inference Network é‡‡ç”¨äº†åŸºäº Diffusion-based çš„åéªŒä¼°è®¡å™¨ï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªå®éªŒæ­¥éª¤ä¸­å‡†ç¡®é€¼è¿‘é«˜ç»´ä¸”å¤šæ¨¡æ€çš„åéªŒåˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJADAI åœ¨æ ‡å‡†è‡ªé€‚åº”è®¾è®¡åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜è¶Šæˆ–æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†å‚æ•°ä¼°è®¡çš„æ•ˆç‡ä¸ç²¾åº¦ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22999v1",
      "published_date": "2025-12-28 16:54:43 UTC",
      "updated_date": "2025-12-28 16:54:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:35.377523+00:00"
    },
    {
      "arxiv_id": "2601.09729v1",
      "title": "Enhancing Business Analytics through Hybrid Summarization of Financial Reports",
      "title_zh": "é€šè¿‡è´¢åŠ¡æŠ¥å‘Šæ··åˆæ‘˜è¦æå‡å•†ä¸šåˆ†æ",
      "authors": [
        "Tohida Rehman"
      ],
      "abstract": "Financial reports and earnings communications contain large volumes of structured and semi structured information, making detailed manual analysis inefficient. Earnings conference calls provide valuable evidence about a firm's performance, outlook, and strategic priorities. The manual analysis of lengthy call transcripts requires substantial effort and is susceptible to interpretive bias and unintentional error. In this work, we present a hybrid summarization framework that combines extractive and abstractive techniques to produce concise and factually reliable Reuters-style summaries from the ECTSum dataset. The proposed two stage pipeline first applies the LexRank algorithm to identify salient sentences, which are subsequently summarized using fine-tuned variants of BART and PEGASUS designed for resource constrained settings. In parallel, we fine-tune a Longformer Encoder-Decoder (LED) model to directly capture long-range contextual dependencies in financial documents.\n  Model performance is evaluated using standard automatic metrics, including ROUGE, METEOR, MoverScore, and BERTScore, along with domain-specific variants such as SciBERTScore and FinBERTScore. To assess factual accuracy, we further employ entity-level measures based on source-precision and F1-target. The results highlight complementary trade offs between approaches, long context models yield the strongest overall performance, while the hybrid framework achieves competitive results with improved factual consistency under computational constraints. These findings support the development of practical summarization systems for efficiently distilling lengthy financial texts into usable business insights.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡‘èæŠ¥å‘Šå’Œè´¢æŠ¥ç”µè¯ä¼šè®®åˆ†ææ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæŠ½å–å¼(Extractive)ä¸ç”Ÿæˆå¼(Abstractive)æŠ€æœ¯çš„æ··åˆæ‘˜è¦æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆç®€æ´ä¸”äº‹å®å¯é çš„é‡‘èæ‘˜è¦ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæµæ°´çº¿ï¼Œé¦–å…ˆåˆ©ç”¨LexRankç®—æ³•è¯†åˆ«å…³é”®å¥å­ï¼Œéšåé€šè¿‡é’ˆå¯¹èµ„æºå—é™ç¯å¢ƒä¼˜åŒ–çš„BARTå’ŒPEGASUSå˜ä½“è¿›è¡Œè¿›ä¸€æ­¥æ‘˜è¦ã€‚ä¸æ­¤åŒæ—¶ï¼Œç ”ç©¶è¿˜å¾®è°ƒäº†Longformer Encoder-Decoder (LED)æ¨¡å‹ï¼Œä»¥ç›´æ¥æ•æ‰é‡‘èæ–‡æ¡£ä¸­çš„é•¿ç¨‹ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚å®éªŒåœ¨ECTSumæ•°æ®é›†ä¸Šå±•å¼€ï¼Œå¹¶åˆ©ç”¨ROUGEã€METEORåŠé¢†åŸŸç‰¹å®šçš„FinBERTScoreç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼ŒåŒæ—¶å¼•å…¥å®ä½“çº§åº¦é‡ä»¥ç¡®ä¿äº‹å®å‡†ç¡®æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œé•¿ä¸Šä¸‹æ–‡æ¨¡å‹LEDåœ¨æ•´ä½“æ€§èƒ½ä¸Šè¡¨ç°æœ€å¼ºï¼Œè€Œæ··åˆæ¡†æ¶åœ¨è®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹å±•ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½å’Œæ›´é«˜çš„äº‹å®ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶ä¸ºé«˜æ•ˆæå–é•¿ç¯‡é‡‘èæ–‡æœ¬ä¸­çš„å•†ä¸šæ´å¯Ÿæä¾›äº†åˆ‡å®å¯è¡Œçš„ç³»ç»ŸåŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 2 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.09729v1",
      "published_date": "2025-12-28 16:25:12 UTC",
      "updated_date": "2025-12-28 16:25:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:27.405487+00:00"
    },
    {
      "arxiv_id": "2601.00843v1",
      "title": "OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification",
      "title_zh": "OmniNeuroï¼šä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸å£°éŸ³åŒ–çš„å¯è§£é‡Šæ€§è„‘æœºæ¥å£åé¦ˆå¤šæ¨¡æ€äººæœºäº¤äº’æ¡†æ¶",
      "authors": [
        "Ayda Aghaei Nia"
      ],
      "abstract": "While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the \"Black Box\" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the \"trial-and-error\" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OmniNeuroï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„äººæœºäº¤äº’(HCI)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)å’Œå£°åƒåŒ–(Sonification)æŠ€æœ¯ï¼Œå°†è„‘æœºæ¥å£(BCI)ä»é™é»˜çš„è§£ç å™¨è½¬å˜ä¸ºé€æ˜çš„åé¦ˆä¼™ä¼´ã€‚è¯¥æ¡†æ¶é’ˆå¯¹æ·±åº¦å­¦ä¹ (Deep Learning)ç®—æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­ç”±äºâ€œé»‘ç›’â€ç‰¹æ€§å¯¼è‡´çš„è§£é‡Šæ€§å·®å’Œç”¨æˆ·æŒ«è´¥æ„Ÿç­‰é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚OmniNeuroé›†æˆäº†ä¸‰ä¸ªå¯è§£é‡Šæ€§å¼•æ“ï¼ŒåŒ…æ‹¬ç‰©ç†ï¼ˆèƒ½é‡ï¼‰ã€æ··æ²Œï¼ˆåˆ†å½¢å¤æ‚æ€§ï¼‰å’Œé‡å­å¯å‘(Quantum-Inspired)çš„ä¸ç¡®å®šæ€§å»ºæ¨¡ï¼Œç”¨äºé©±åŠ¨å®æ—¶çš„ç¥ç»å£°åƒåŒ–(Neuro-Sonification)å¹¶ç”Ÿæˆä¸´åºŠæŠ¥å‘Šã€‚åœ¨PhysioNetæ•°æ®é›†ï¼ˆN=109ï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿå®ç°äº†58.52%çš„å¹³å‡å‡†ç¡®ç‡ã€‚å®šæ€§åˆæ­¥ç ”ç©¶è¯å®ï¼Œè¿™ç§å¯è§£é‡Šçš„åé¦ˆæœ‰åŠ©äºç”¨æˆ·æ›´å¥½åœ°è°ƒèŠ‚å¿ƒç†åŠªåŠ›ï¼Œå¹¶æœ‰æ•ˆç¼©çŸ­äº†â€œå°è¯•ä¸é”™è¯¯â€é˜¶æ®µã€‚ç”±äºå…¶è§£ç å™¨æ— å…³æ€§(decoder-agnostic)çš„ç‰¹ç‚¹ï¼ŒOmniNeuroå¯ä»¥ä½œä¸ºä»»ä½•å…ˆè¿›æ¶æ„çš„å…³é”®å¯è§£é‡Šå±‚ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 7 figures, 3 tables. Source code and implementation available at: https://github.com/ayda-aghaei/OmniNeuro. Highlights the use of LLMs (Gemini) and Quantum probability formalism for real-time BCI explainability",
      "pdf_url": "https://arxiv.org/pdf/2601.00843v1",
      "published_date": "2025-12-28 16:06:55 UTC",
      "updated_date": "2025-12-28 16:06:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:50.152153+00:00"
    },
    {
      "arxiv_id": "2512.22953v1",
      "title": "APO: Alpha-Divergence Preference Optimization",
      "title_zh": "APOï¼šAlpha æ•£åº¦åå¥½ä¼˜åŒ–",
      "authors": [
        "Wang Zixian"
      ],
      "abstract": "Two divergence regimes dominate modern alignment practice. Supervised fine-tuning and many distillation-style objectives implicitly minimize the forward KL divergence KL(q || pi_theta), yielding stable mode-covering updates but often under-exploiting high-reward modes. In contrast, PPO-style online reinforcement learning from human feedback behaves closer to reverse KL divergence KL(pi_theta || q), enabling mode-seeking improvements but risking mode collapse. Recent anchored methods, such as ADPO, show that performing the projection in anchored coordinates can substantially improve stability, yet they typically commit to a single divergence. We introduce Alpha-Divergence Preference Optimization (APO), an anchored framework that uses Csiszar alpha-divergence to continuously interpolate between forward and reverse KL behavior within the same anchored geometry. We derive unified gradient dynamics parameterized by alpha, analyze gradient variance properties, and propose a practical reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is both improving and confidently calibrated. Experiments on Qwen3-1.7B with math-level3 demonstrate that APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† APO (Alpha-Divergence Preference Optimization)ï¼Œä¸€ç§æ—¨åœ¨å¹³è¡¡å‰å‘ KL æ•£åº¦ (Forward KL) ä¸åå‘ KL æ•£åº¦ (Reverse KL) ä¼˜åŠ¿çš„æ–°å‹é”šå®šå¯¹é½æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ¨¡å¼è¦†ç›– (mode-covering) çš„ç¨³å®šæ€§ä¸æ¨¡å¼å¯»æ‰¾ (mode-seeking) çš„å¼€å‘èƒ½åŠ›ä¹‹é—´éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ï¼ŒAPO åˆ©ç”¨ Csiszar alpha-divergence åœ¨é”šå®šå‡ ä½• (anchored geometry) ä¸­å®ç°äº†ä¸¤ç§è¡Œä¸ºçš„è¿ç»­æ’å€¼ã€‚ç ”ç©¶æ¨å¯¼äº†å— alpha å‚æ•°åŒ–çš„ç»Ÿä¸€æ¢¯åº¦åŠ¨åŠ›å­¦ï¼Œå¹¶åˆ†æäº†å…¶æ¢¯åº¦æ–¹å·®ç‰¹æ€§ï¼Œè¿›è€Œæå‡ºäº†ä¸€ç§åŸºäºå¥–åŠ±å’Œç½®ä¿¡åº¦ä¿æŠ¤çš„ alpha è°ƒåº¦ç­–ç•¥ï¼Œç¡®ä¿ç­–ç•¥ä»…åœ¨æ€§èƒ½æå‡ä¸”æ ¡å‡†è‰¯å¥½æ—¶ä»è¦†ç›–è½¬å‘å¼€å‘ã€‚åœ¨ Qwen3-1.7B æ¨¡å‹ä¸Šçš„æ•°å­¦ä»»åŠ¡å®éªŒè¯æ˜ï¼ŒAPO åœ¨ä¿æŒè®­ç»ƒç¨³å®šæ€§çš„åŒæ—¶ï¼Œå–å¾—äº†ä¸ GRPO å’Œ GSPO åŸºçº¿æ¨¡å‹ç›¸å½“çš„ç«äº‰æ€§èƒ½ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„åå¥½ä¼˜åŒ–æä¾›äº†æ›´çµæ´»ä¸”ç¨³å¥çš„ç†è®ºä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22953v1",
      "published_date": "2025-12-28 14:51:03 UTC",
      "updated_date": "2025-12-28 14:51:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:27.565658+00:00"
    },
    {
      "arxiv_id": "2512.22941v1",
      "title": "Heterogeneity in Multi-Agent Reinforcement Learning",
      "title_zh": "å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„å¼‚è´¨æ€§",
      "authors": [
        "Tianyi Hu",
        "Zhiqiang Pu",
        "Yuan Wang",
        "Tenghai Qiu",
        "Min Chen",
        "Xin Yu"
      ],
      "abstract": "Heterogeneity is a fundamental property in multi-agent reinforcement learning (MARL), which is closely related not only to the functional differences of agents, but also to policy diversity and environmental interactions. However, the MARL field currently lacks a rigorous definition and deeper understanding of heterogeneity. This paper systematically discusses heterogeneity in MARL from the perspectives of definition, quantification, and utilization. First, based on an agent-level modeling of MARL, we categorize heterogeneity into five types and provide mathematical definitions. Second, we define the concept of heterogeneity distance and propose a practical quantification method. Third, we design a heterogeneity-based multi-agent dynamic parameter sharing algorithm as an example of the application of our methodology. Case studies demonstrate that our method can effectively identify and quantify various types of agent heterogeneity. Experimental results show that the proposed algorithm, compared to other parameter sharing baselines, has better interpretability and stronger adaptability. The proposed methodology will help the MARL community gain a more comprehensive and profound understanding of heterogeneity, and further promote the development of practical algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-Agent Reinforcement Learning, MARL) ä¸­å¼‚æ„æ€§ (Heterogeneity) çš„åŸºæœ¬å±æ€§ï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸç›®å‰ç¼ºä¹ä¸¥è°¨å®šä¹‰å’Œæ·±åˆ»ç†è§£çš„é—®é¢˜ã€‚ä½œè€…åŸºäºæ™ºèƒ½ä½“å±‚é¢çš„å»ºæ¨¡ï¼Œå°†å¼‚æ„æ€§å½’çº³ä¸ºäº”ç§ç±»å‹å¹¶ç»™å‡ºäº†ç›¸åº”çš„æ•°å­¦å®šä¹‰ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®ºæ–‡æå‡ºäº†å¼‚æ„æ€§è·ç¦» (Heterogeneity Distance) çš„æ¦‚å¿µåŠå…¶å…·ä½“çš„é‡åŒ–è¯„ä»·æ–¹æ³•ã€‚ä½œä¸ºæ–¹æ³•è®ºçš„åº”ç”¨ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ç§åŸºäºå¼‚æ„æ€§çš„å¤šæ™ºèƒ½ä½“åŠ¨æ€å‚æ•°å…±äº«ç®—æ³•ã€‚æ¡ˆä¾‹ç ”ç©¶å’Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¹¶é‡åŒ–å¤šç§ç±»å‹çš„æ™ºèƒ½ä½“å¼‚æ„æ€§ï¼Œä¸”æå‡ºçš„ç®—æ³•åœ¨å¯è§£é‡Šæ€§å’Œé€‚åº”æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„å‚æ•°å…±äº«åŸºçº¿æ¨¡å‹ã€‚è¯¥é¡¹å·¥ä½œä¸º MARL ç¤¾åŒºæä¾›äº†æ›´å…¨é¢æ·±å…¥çš„å¼‚æ„æ€§è®¤çŸ¥ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ›´å…·å®ç”¨æ€§çš„ç®—æ³•å¼€å‘ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22941v1",
      "published_date": "2025-12-28 14:07:31 UTC",
      "updated_date": "2025-12-28 14:07:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:21:30.404270+00:00"
    },
    {
      "arxiv_id": "2512.22933v3",
      "title": "Multimodal Fact-Checking: An Agent-based Approach",
      "title_zh": "å¤šæ¨¡æ€äº‹å®æ ¸æŸ¥ï¼šåŸºäºæ™ºèƒ½ä½“çš„æ–¹æ³•",
      "authors": [
        "Danni Xu",
        "Shaojing Fan",
        "Harry Cheng",
        "Mohan Kankanhalli"
      ],
      "abstract": "The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.",
      "tldr_zh": "é’ˆå¯¹å¤šæ¨¡æ€è™šå‡ä¿¡æ¯å¿«é€Ÿä¼ æ’­å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å¤æ‚æ¨ç†å’Œæ·±åº¦è¯æ®åˆ©ç”¨æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº†RW-Postï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„çœŸå®ä¸–ç•Œå¤šæ¨¡æ€äº‹å®æ ¸æŸ¥æ•°æ®é›†ï¼Œå®ƒå°†çœŸå®ä¸»å¼ ä¸åŸå§‹ç¤¾äº¤åª’ä½“èƒŒæ™¯å¯¹é½ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ¨ç†è·¯å¾„å’Œå…³è”è¯æ®ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†AgentFactï¼Œä¸€ç§æ¨¡æ‹Ÿäººç±»æ ¸æŸ¥æµç¨‹çš„åŸºäºæ™ºèƒ½ä½“(Agent)çš„å¤šæ¨¡æ€äº‹å®æ ¸æŸ¥æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±è´Ÿè´£ç­–ç•¥è§„åˆ’ã€è¯æ®æ£€ç´¢ã€è§†è§‰åˆ†æã€æ¨ç†åŠè§£é‡Šç”Ÿæˆçš„äº”ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ç»„æˆï¼Œé€šè¿‡è¿­ä»£å·¥ä½œæµå®ç°ç³»ç»Ÿçš„è¯æ®åˆ†æä¸æˆ˜ç•¥å†³ç­–ã€‚å®éªŒè¯æ˜ï¼ŒRW-Postä¸AgentFactçš„ååŒä½œç”¨æ˜¾è‘—å¢å¼ºäº†å¤šæ¨¡æ€äº‹å®æ ¸æŸ¥çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Code and dataset will be released at https://github.com/xudanni0927/AgentFact",
      "pdf_url": "https://arxiv.org/pdf/2512.22933v3",
      "published_date": "2025-12-28 13:58:33 UTC",
      "updated_date": "2026-01-04 06:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:37.402805+00:00"
    },
    {
      "arxiv_id": "2512.22931v1",
      "title": "Geometric Structural Knowledge Graph Foundation Model",
      "title_zh": "å‡ ä½•ç»“æ„çŸ¥è¯†å›¾è°±åŸºåº§æ¨¡å‹",
      "authors": [
        "Ling Xin",
        "Mojtaba Nayyeri",
        "Zahra Makki Nayeri",
        "Steffen Staab"
      ],
      "abstract": "Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»“æ„åŒ–çŸ¥è¯†å›¾è°±åŸºç¡€æ¨¡å‹ï¼ˆStructural knowledge graph foundation modelsï¼‰åœ¨å¤„ç†å…¨æ–°å›¾è°±ä¸­ä¸å¯è§å®ä½“å’Œå…³ç³»æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†åä¸º Gamma çš„æ–°å‹åŸºç¡€æ¨¡å‹ã€‚Gamma å¼•å…¥äº†å¤šå¤´å‡ ä½•æ³¨æ„åŠ›ï¼ˆmulti-head geometric attentionï¼‰æœºåˆ¶ï¼Œå°†å•ä¸€çš„å…³ç³»å˜æ¢æ‰©å±•ä¸ºåŒ…æ‹¬å®æ•°ã€å¤æ•°ï¼ˆcomplexï¼‰ã€åˆ†è£‚å¤æ•°ï¼ˆsplit-complexï¼‰å’Œå¯¹å¶æ•°ï¼ˆdual numberï¼‰åœ¨å†…çš„å¤šç§å¹¶è¡Œä»£æ•°å˜æ¢ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†å¯¹å¤šæ ·åŒ–å…³ç³»ç»“æ„çš„æ•æ‰èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆè½»é‡çº§é—¨æ§å’Œç†µæ­£åˆ™åŒ–çš„å…³ç³»æ¡ä»¶æ³¨æ„åŠ›èåˆæœºåˆ¶ï¼ˆrelational conditioned attention fusion mechanismï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä¸ºä¸åŒçš„ä¸‰å…ƒç»„æ¨¡å¼è‡ªé€‚åº”åœ°é€‰æ‹©å¹¶èåˆæœ€åˆé€‚çš„å‡ ä½•åå·®ã€‚åœ¨ 56 ä¸ªçŸ¥è¯†å›¾è°±ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGamma åœ¨é›¶æ ·æœ¬å½’çº³å¼é“¾æ¥é¢„æµ‹ï¼ˆzero-shot inductive link predictionï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶åœ¨å½’çº³åŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡å€’æ•°æ’åï¼ˆmean reciprocal rankï¼‰æå‡äº† 5.5%ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº† 4.4%ã€‚è¿™ä¸€ç ”ç©¶æˆæœå……åˆ†è¯æ˜äº†åˆ©ç”¨äº’è¡¥å‡ ä½•è¡¨ç¤ºæ¥æå‡åŸºç¡€æ¨¡å‹è¡¨è¾¾èƒ½åŠ›åŠå…¶åœ¨æœªçŸ¥å›¾è°±ä¸Šæ³›åŒ–æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to IEEE TPAMI, under review",
      "pdf_url": "https://arxiv.org/pdf/2512.22931v1",
      "published_date": "2025-12-28 13:53:23 UTC",
      "updated_date": "2025-12-28 13:53:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:21:12.984120+00:00"
    },
    {
      "arxiv_id": "2601.09728v1",
      "title": "Eliminating Agentic Workflow for Introduction Generation with Parametric Stage Tokens",
      "title_zh": "åŸºäºå‚æ•°åŒ–é˜¶æ®µæ ‡è®°æ¶ˆé™¤å¼•è¨€ç”Ÿæˆçš„æ™ºèƒ½ä½“å·¥ä½œæµ",
      "authors": [
        "Meicong Zhang",
        "Tiancheng su",
        "Guoxiu He"
      ],
      "abstract": "In recent years, using predefined agentic workflows to guide large language models (LLMs) for literature classification and review has become a research focus. However, writing research introductions is more challenging. It requires rigorous logic, coherent structure, and abstract summarization. Existing workflows often suffer from long reasoning chains, error accumulation, and reduced textual coherence. To address these limitations, we propose eliminating external agentic workflows. Instead, we directly parameterize their logical structure into the LLM. This allows the generation of a complete introduction in a single inference. To this end, we introduce the Stage Token for Introduction Generation (STIG). STIG converts the multiple stages of the original workflow into explicit stage signals. These signals guide the model to follow different logical roles and functions during generation. Through instruction tuning, the model learns the mapping between stage tokens and text functions. It also learns the logical order and transition patterns between stages, encoding this knowledge into the model parameters. Experimental results show that STIG can generate multi-stage text in a single inference. It does not require explicit workflow calls. STIG outperforms traditional agentic workflows and other baselines on metrics of semantic similarity and sentence-level structural rationality. The code is provided in the Supplementary Materials.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢„å®šä¹‰çš„æ™ºèƒ½ä½“å·¥ä½œæµï¼ˆagentic workflowsï¼‰åœ¨ç”Ÿæˆç ”ç©¶è®ºæ–‡å¼•è¨€æ—¶é¢ä¸´çš„æ¨ç†é“¾é•¿ã€è¯¯å·®ç´¯ç§¯åŠæ–‡æœ¬è¿è´¯æ€§å·®ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ¶ˆé™¤å¤–éƒ¨å·¥ä½œæµçš„æ–°æ–¹æ¡ˆã€‚ç ”ç©¶è€…é€šè¿‡å°†é€»è¾‘ç»“æ„ç›´æ¥å‚æ•°åŒ–åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œå¼•å…¥äº†ç”¨äºå¼•è¨€ç”Ÿæˆçš„é˜¶æ®µä»¤ç‰Œï¼ˆStage Token for Introduction Generationï¼Œç®€ç§°STIGï¼‰ã€‚STIG å°†ä¼ ç»Ÿå·¥ä½œæµçš„å¤šä¸ªé˜¶æ®µè½¬åŒ–ä¸ºæ˜¾å¼çš„é˜¶æ®µä¿¡å·ï¼Œå¼•å¯¼æ¨¡å‹åœ¨å•æ¬¡æ¨ç†è¿‡ç¨‹ä¸­æ‰¿æ‹…ä¸åŒçš„é€»è¾‘è§’è‰²ï¼Œä»è€Œç”Ÿæˆå®Œæ•´çš„å¼•è¨€æ–‡æœ¬ã€‚é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼ˆinstruction tuningï¼‰ï¼Œæ¨¡å‹æˆåŠŸå°†é˜¶æ®µé—´çš„é€»è¾‘é¡ºåºå’Œè½¬æ¢æ¨¡å¼ç¼–ç è¿›æ¨¡å‹å‚æ•°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSTIG åœ¨æ— éœ€æ˜¾å¼å·¥ä½œæµè°ƒç”¨çš„æƒ…å†µä¸‹ï¼Œåœ¨è¯­ä¹‰ç›¸ä¼¼åº¦å’Œç»“æ„åˆç†æ€§ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„æ™ºèƒ½ä½“å·¥ä½œæµåŠå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.09728v1",
      "published_date": "2025-12-28 12:51:36 UTC",
      "updated_date": "2025-12-28 12:51:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:46.150651+00:00"
    },
    {
      "arxiv_id": "2512.22910v1",
      "title": "Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning",
      "title_zh": "Sat-EnQï¼šé¢å‘å¯é ä¸”è®¡ç®—é«˜æ•ˆå¼ºåŒ–å­¦ä¹ çš„å¼± Q å­¦ä¹ å™¨æ»¡æ„åŒ–é›†æˆ",
      "authors": [
        "Ãœnver Ã‡iftÃ§i"
      ],
      "abstract": "Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough'' before optimizing aggressively. In Phase 1, we train an ensemble of lightweight Q-networks under a satisficing objective that limits early value growth using a dynamic baseline, producing diverse, low-variance estimates while avoiding catastrophic overestimation. In Phase 2, the ensemble is distilled into a larger network and fine-tuned with standard Double DQN. We prove theoretically that satisficing induces bounded updates and cannot increase target variance, with a corollary quantifying conditions for substantial reduction. Empirically, Sat-EnQ achieves 3.8x variance reduction, eliminates catastrophic failures (0% vs 50% for DQN), maintains 79% performance under environmental noise}, and requires 2.5x less compute than bootstrapped ensembles. Our results highlight a principled path toward robust reinforcement learning by embracing satisficing before optimization.",
      "tldr_zh": "é’ˆå¯¹ Deep Q-learning åœ¨æ—©æœŸè®­ç»ƒä¸­ç”±äºæœ€å¤§åŒ–æ“ä½œå¯¼è‡´ä¼°ç®—è¯¯å·®æ”¾å¤§å’Œä¸ç¨³å®šçš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶å—æœ‰é™ç†æ€§ç†è®º (Bounded Rationality) å’Œå‘å±•å­¦ä¹  (Developmental Learning) å¯å‘ï¼Œæå‡ºäº† Sat-EnQ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µè®­ç»ƒè½»é‡çº§ Q-networks çš„é›†æˆ (Ensemble)ï¼Œåˆ©ç”¨æ»¡è¶³åŒ– (Satisficing) ç›®æ ‡å’ŒåŠ¨æ€åŸºå‡†é™åˆ¶æ—©æœŸä»·å€¼å¢é•¿ï¼Œæ—¨åœ¨äº§ç”Ÿä½æ–¹å·®ä¼°è®¡å¹¶é¿å…ç¾éš¾æ€§çš„é«˜ä¼°ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œé›†æˆæ¨¡å‹è¢«è’¸é¦ (Distill) åˆ°æ›´å¤§çš„ç½‘ç»œä¸­ï¼Œå¹¶ä½¿ç”¨ Double DQN è¿›è¡Œå¾®è°ƒä»¥ä¼˜åŒ–æ€§èƒ½ã€‚ç†è®ºè¯æ˜æ˜¾ç¤ºï¼Œæ»¡è¶³åŒ–ç­–ç•¥èƒ½è¯±å¯¼æœ‰ç•Œæ›´æ–°ä¸”ä¸ä¼šå¢åŠ ç›®æ ‡æ–¹å·®ï¼Œç¡®ä¿äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSat-EnQ å®ç°äº† 3.8 å€çš„æ–¹å·®ç¼©å‡ï¼Œå°†ç¾éš¾æ€§å¤±è´¥ç‡ä» 50% é™è‡³ 0%ï¼Œåœ¨ç¯å¢ƒå™ªå£°ä¸‹è¡¨ç°ç¨³å¥ï¼Œä¸”è®¡ç®—å¼€é”€æ¯”å¼•å¯¼é›†æˆ (Bootstrapped Ensembles) ä½ 2.5 å€ã€‚è¯¥ç ”ç©¶é€šè¿‡åœ¨æ¿€è¿›ä¼˜åŒ–å‰å¼•å…¥æ»¡è¶³åŒ–æœºåˆ¶ï¼Œä¸ºæ„å»ºå¯é ä¸”è®¡ç®—é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ç³»ç»Ÿæä¾›äº†ä¸€æ¡åŸåˆ™æ€§çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22910v1",
      "published_date": "2025-12-28 12:41:09 UTC",
      "updated_date": "2025-12-28 12:41:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:20:52.758793+00:00"
    },
    {
      "arxiv_id": "2601.09727v1",
      "title": "SciNets: Graph-Constrained Multi-Hop Reasoning for Scientific Literature Synthesis",
      "title_zh": "SciNetsï¼šé¢å‘ç§‘å­¦æ–‡çŒ®åˆæˆçš„å›¾çº¦æŸå¤šè·³æ¨ç†",
      "authors": [
        "Sauhard Dubey"
      ],
      "abstract": "Cross-domain scientific synthesis requires connecting mechanistic explanations across fragmented literature, a capability that remains challenging for both retrieval-based systems and unconstrained language models. While recent work has applied large language models to scientific summarization and question answering, these approaches provide limited control over reasoning depth and structural grounding. We frame mechanistic synthesis as a graph-constrained multi-hop reasoning problem over literature-derived concept graphs. Given a scientific query and a compact, query-local corpus, SciNets constructs a directed concept graph and synthesizes mechanistic explanations by identifying multi-hop reasoning paths that connect concepts that rarely co-occur within individual papers. We systematically compare shortest-path reasoning, k-shortest paths with diversity constraints, stochastic random walks, and a retrieval-augmented language model baseline. Rather than evaluating correctness, which is often indeterminate when synthesizing connections across distributed sources, we introduce a behavioral framework that measures symbolic reasoning depth, mechanistic diversity, and grounding stability. Across machine learning, biology, and climate science tasks, explicit graph constraints enable controllable multi-hop reasoning while revealing a consistent trade-off: deeper and more diverse symbolic reasoning increases grounding instability, whereas shortest-path reasoning remains highly stable but structurally conservative. These findings provide a systematic behavioral characterization of the limits and capabilities of current graph-LLM integration for scientific synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SciNetsï¼Œä¸€ä¸ªé’ˆå¯¹è·¨é¢†åŸŸç§‘å­¦æ–‡çŒ®ç»¼è¿°çš„å›¾çº¦æŸå¤šè·³æ¨ç†(graph-constrained multi-hop reasoning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ£€ç´¢ç³»ç»Ÿå’Œè¯­è¨€æ¨¡å‹åœ¨å¤„ç†é›¶æ•£æ–‡çŒ®ä¸­æœºåˆ¶è§£é‡Šæ—¶çš„å±€é™æ€§ã€‚SciNets å°†æœºåˆ¶åˆæˆå»ºæ¨¡ä¸ºæ–‡çŒ®è¡ç”Ÿæ¦‚å¿µå›¾ä¸Šçš„å¤šè·³æ¨ç†é—®é¢˜ï¼Œé€šè¿‡æ„å»ºæœ‰å‘æ¦‚å¿µå›¾æ¥è¯†åˆ«é‚£äº›åœ¨å•ç¯‡è®ºæ–‡ä¸­ç½•è§å…±ç°çš„æ¦‚å¿µè¿æ¥è·¯å¾„ã€‚è¯¥æ¡†æ¶ç³»ç»Ÿå¯¹æ¯”äº†æœ€çŸ­è·¯å¾„æ¨ç†(shortest-path reasoning)ã€å¸¦å¤šæ ·æ€§çº¦æŸçš„ k-æœ€çŸ­è·¯å¾„ã€éšæœºæ¸¸èµ°ä»¥åŠæ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹(RAG)åŸºçº¿ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè¡¡é‡ç¬¦å·æ¨ç†æ·±åº¦ã€æœºåˆ¶å¤šæ ·æ€§å’Œæ¥åœ°ç¨³å®šæ€§(grounding stability)çš„è¡Œä¸ºè¯„ä¼°æ¡†æ¶ã€‚åœ¨æœºå™¨å­¦ä¹ ã€ç”Ÿç‰©å­¦å’Œæ°”å€™ç§‘å­¦ä»»åŠ¡ä¸­çš„å®éªŒæ­ç¤ºäº†æ¨ç†æ·±åº¦ä¸ç¨³å®šæ€§ä¹‹é—´çš„æƒè¡¡ï¼šæ›´æ·±ã€æ›´å¤šæ ·åŒ–çš„ç¬¦å·æ¨ç†ä¼šå¢åŠ æ¥åœ°çš„ä¸ç¨³å®šæ€§ï¼Œè€Œæœ€çŸ­è·¯å¾„æ¨ç†è™½ç¨³å®šä½†åœ¨ç»“æ„ä¸Šè¾ƒä¸ºä¿å®ˆã€‚è¿™äº›å‘ç°ä¸ºå½“å‰å›¾ä¸å¤§è¯­è¨€æ¨¡å‹(Graph-LLM)é›†æˆåœ¨ç§‘å­¦ç»¼è¿°ä¸­çš„èƒ½åŠ›è¾¹ç•Œæä¾›äº†ç³»ç»Ÿæ€§çš„è¡Œä¸ºè¡¨å¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.09727v1",
      "published_date": "2025-12-28 12:27:42 UTC",
      "updated_date": "2025-12-28 12:27:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:21:04.173730+00:00"
    },
    {
      "arxiv_id": "2512.22901v1",
      "title": "A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments",
      "title_zh": "åŸºäºç¥ç»ç½‘ç»œçš„äº•ä¸‹ä»ªå™¨å®æ—¶å¥—ç®¡æ¥ç®è¯†åˆ«ç³»ç»Ÿ",
      "authors": [
        "Si-Yu Xiao",
        "Xin-Di Zhao",
        "Xiang-Zhan Wang",
        "Tian-Hao Mao",
        "Ying-Kai Liao",
        "Xing-Yu Liao",
        "Yu-Qiao Chen",
        "Jun-Jie Wang",
        "Shuang Liu",
        "Tu-Pei Chen",
        "Yang Liu"
      ],
      "abstract": "Accurate downhole positioning is critical in oil and gas operations but is often compromised by signal degradation in traditional surface-based Casing Collar Locator (CCL) monitoring. To address this, we present an in-situ, real-time collar recognition system using embedded neural network. We introduce lightweight \"Collar Recognition Nets\" (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors. By leveraging temporal and depthwise separable convolutions, our most compact model reduces computational complexity to just 8,208 MACs while maintaining an F1 score of 0.972. Hardware validation confirms an average inference latency of 343.2 Î¼s, demonstrating that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ²¹æ°”ä½œä¸šä¸­ä¼ ç»Ÿåœ°é¢Casing Collar Locator (CCL)ç›‘æµ‹å› ä¿¡å·è¡°å‡å¯¼è‡´ä¸‹äº•å®šä½ç²¾åº¦å—æŸçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåµŒå…¥å¼ç¥ç»ç½‘ç»œçš„åŸä½å®æ—¶æ¥ç®è¯†åˆ«ç³»ç»Ÿã€‚ç ”ç©¶å¼•å…¥äº†ä¸“é—¨é’ˆå¯¹èµ„æºå—é™çš„ARM Cortex-M7å¾®å¤„ç†å™¨ä¼˜åŒ–çš„è½»é‡çº§ç¥ç»ç½‘ç»œCollar Recognition Nets (CRNs)ã€‚é€šè¿‡ç»“åˆæ—¶é—´å·ç§¯å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯(depthwise separable convolutions)æŠ€æœ¯ï¼Œæœ€ç´§å‡‘çš„æ¨¡å‹åœ¨ç»´æŒ0.972çš„F1åˆ†æ•°æ—¶ï¼ŒæˆåŠŸå°†è®¡ç®—å¤æ‚åº¦é™è‡³8,208 MACsã€‚ç¡¬ä»¶éªŒè¯ç»“æœæ˜¾ç¤ºå…¶å¹³å‡æ¨ç†å»¶è¿Ÿä»…ä¸º343.2 Î¼sï¼Œå……åˆ†è¯æ˜äº†åœ¨äº•ä¸‹ä»ªå™¨ä¸¥è‹›çš„åŠŸè€—ä¸ç©ºé—´é™åˆ¶ä¸‹ï¼Œå®ç°ç¨³å¥ã€è‡ªä¸»çš„ä¿¡å·å¤„ç†æ˜¯å®Œå…¨å¯è¡Œçš„ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22901v1",
      "published_date": "2025-12-28 12:19:36 UTC",
      "updated_date": "2025-12-28 12:19:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:21:49.557946+00:00"
    },
    {
      "arxiv_id": "2512.22899v1",
      "title": "HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery",
      "title_zh": "HiSciBenchï¼šæ¶µç›–ä»é˜…è¯»åˆ°å‘ç°çš„å±‚æ¬¡åŒ–å¤šå­¦ç§‘ç§‘å­¦æ™ºèƒ½è¯„æµ‹åŸºå‡†",
      "authors": [
        "Yaping Zhang",
        "Qixuan Zhang",
        "Xingquan Zhang",
        "Zhiyuan Chen",
        "Wenwen Zhuang",
        "Yupu Liang",
        "Lu Xiang",
        "Yang Zhao",
        "Jiajun Zhang",
        "Yu Zhou",
        "Chengqing Zong"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \\textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \\textit{Scientific Literacy} (L1), \\textit{Literature Parsing} (L2), \\textit{Literature-based Question Answering} (L3), \\textit{Literature Review Generation} (L4), and \\textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\\% accuracy on basic literacy tasks, performance declines sharply to 25\\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†HiSciBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦æ™ºèƒ½(Scientific Intelligence)é¢†åŸŸèƒ½åŠ›çš„å±‚æ¬¡åŒ–å¤šå­¦ç§‘åŸºå‡†æµ‹è¯•ã€‚è¯¥æ¡†æ¶å°†ç§‘å­¦å·¥ä½œæµåˆ’åˆ†ä¸ºäº”ä¸ªå±‚çº§ï¼Œä»åŸºç¡€çš„ç§‘å­¦ç´ å…»(Scientific Literacy)åˆ°å¤æ‚çš„ç§‘å­¦å‘ç°(Scientific Discovery)ï¼Œæ¶µç›–äº†æ–‡çŒ®è§£æã€æ–‡çŒ®é—®ç­”åŠç»¼è¿°ç”Ÿæˆç­‰å…³é”®ç¯èŠ‚ã€‚åŸºå‡†åº“åŒ…å«è·¨è¶Šæ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ã€åœ°ç†å’Œå¤©æ–‡å…­å¤§é¢†åŸŸçš„8,735ä¸ªå®ä¾‹ï¼Œå¹¶æ”¯æŒæ–‡æœ¬ã€å…¬å¼ã€å›¾è¡¨ç­‰å¤šç§æ¨¡æ€è¾“å…¥åŠè·¨è¯­è¨€è¯„ä¼°ã€‚é€šè¿‡å¯¹GPT-5ã€DeepSeek-R1ç­‰é¢†å…ˆæ¨¡å‹çš„ç»¼åˆè¯„ä¼°å‘ç°ï¼Œè™½ç„¶æ¨¡å‹åœ¨åŸºç¡€ç´ å…»ä»»åŠ¡ä¸­è¡¨ç°å°šå¯ï¼Œä½†åœ¨ç§‘å­¦å‘ç°å±‚çº§çš„æŒ‘æˆ˜ä¸­å‡†ç¡®ç‡ä»…ä¸º25%ã€‚HiSciBenchä¸ºç§‘å­¦æ™ºèƒ½çš„è¯„ä¼°å»ºç«‹äº†æ–°æ ‡å‡†ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨æ·±å±‚æ¬¡ç§‘å­¦æ¨ç†ä¸­çš„å·¨å¤§ç¼ºå£ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¯é çš„ç§‘ç ”è¾…åŠ©æ¨¡å‹æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22899v1",
      "published_date": "2025-12-28 12:08:05 UTC",
      "updated_date": "2025-12-28 12:08:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:22:52.877627+00:00"
    },
    {
      "arxiv_id": "2512.22895v1",
      "title": "SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning",
      "title_zh": "SAMP-HDRLï¼šåŸºäºåˆ†å±‚æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„åŠ¨é‡è°ƒèŠ‚æ•ˆç”¨åˆ†æ®µåˆ†é…å¤šæ™ºèƒ½ä½“æŠ•èµ„ç»„åˆç®¡ç†",
      "authors": [
        "Xiaotian Ren",
        "Nuerxiati Abudurexiti",
        "Zhengyong Jiang",
        "Angelos Stefanidis",
        "Hongbin Liu",
        "Jionglong Su"
      ],
      "abstract": "Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\\% higher Return, 5\\% higher Sharpe ratio, 5\\% higher Sortino ratio, and 2\\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAMP-HDRLï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹éå¹³ç¨³å¸‚åœºä¸­æŠ•èµ„ç»„åˆä¼˜åŒ–æŒ‘æˆ˜ï¼ˆå¦‚å¸‚åœºçŠ¶æ€åˆ‡æ¢ã€åŠ¨æ€ç›¸å…³æ€§å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„å¯è§£é‡Šæ€§ä¸è¶³ï¼‰è€Œè®¾è®¡çš„å±‚æ¬¡åŒ–æ·±åº¦å¼ºåŒ–å­¦ä¹ (Hierarchical Deep Reinforcement Learning)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€èµ„äº§åˆ†ç»„æŠ€æœ¯å°†å¸‚åœºåˆ’åˆ†ä¸ºé«˜è´¨é‡å’Œæ™®é€šå­é›†ï¼Œåˆ©ç”¨ä¸Šå±‚æ™ºèƒ½ä½“æå–å…¨å±€ä¿¡å·ï¼Œå¹¶é…åˆä¸‹å±‚æ™ºèƒ½ä½“åœ¨æ©ç çº¦æŸä¸‹æ‰§è¡Œç»„å†…åˆ†é…ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºæ•ˆç”¨çš„èµ„æœ¬åˆ†é…æœºåˆ¶ä»¥æ•´åˆé£é™©ä¸æ— é£é™©èµ„äº§ï¼Œç¡®ä¿äº†å…¨å±€ä¸å±€éƒ¨å†³ç­–çš„ä¸€è‡´åè°ƒã€‚åœ¨ä¸‰ç§å¸‚åœºçŠ¶æ€ä¸‹çš„å›æµ‹æ˜¾ç¤ºï¼ŒSAMP-HDRLåœ¨æ”¶ç›Šç‡ã€Sharpe ratioã€Sortino ratioå’ŒOmega ratioç­‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿å’ŒDRLåŸºå‡†æ¨¡å‹ï¼Œåœ¨åŠ¨è¡å¸‚åœºä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚æ¶ˆèå®éªŒè¯å®äº†å±‚çº§åè°ƒä¸åŠ¨æ€èšç±»å¯¹ç³»ç»Ÿç¨³å¥æ€§çš„è´¡çŒ®ï¼Œè€ŒåŸºäºSHAPçš„å¯è§£é‡Šæ€§åˆ†æåˆ™æ­ç¤ºäº†æ™ºèƒ½ä½“é—´â€œåˆ†æ•£ä¸é›†ä¸­â€å¹¶è¡Œçš„å†³ç­–æœºåˆ¶ã€‚æ€»ä½“è€Œè¨€ï¼ŒSAMP-HDRLé€šè¿‡å°†å¸‚åœºç»“æ„çº¦æŸåµŒå…¥å­¦ä¹ ç®¡çº¿ï¼Œæœ‰æ•ˆæå‡äº†å¤æ‚é‡‘èç¯å¢ƒä¸‹çš„èµ„äº§ç®¡ç†é€‚åº”æ€§ä¸é€æ˜åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22895v1",
      "published_date": "2025-12-28 11:56:39 UTC",
      "updated_date": "2025-12-28 11:56:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:21:46.761350+00:00"
    },
    {
      "arxiv_id": "2512.22894v1",
      "title": "DECEPTICON: How Dark Patterns Manipulate Web Agents",
      "title_zh": "DECEPTICONï¼šé»‘æš—æ¨¡å¼å¦‚ä½•æ“çºµç½‘ç»œæ™ºèƒ½ä½“",
      "authors": [
        "Phil Cuvin",
        "Hao Zhu",
        "Diyi Yang"
      ],
      "abstract": "Deceptive UI designs, widely instantiated across the web and commonly known as dark patterns, manipulate users into performing actions misaligned with their goals. In this paper, we show that dark patterns are highly effective in steering agent trajectories, posing a significant risk to agent robustness. To quantify this risk, we introduce DECEPTICON, an environment for testing individual dark patterns in isolation. DECEPTICON includes 700 web navigation tasks with dark patterns -- 600 generated tasks and 100 real-world tasks, designed to measure instruction-following success and dark pattern effectiveness. Across state-of-the-art agents, we find dark patterns successfully steer agent trajectories towards malicious outcomes in over 70% of tested generated and real-world tasks -- compared to a human average of 31%. Moreover, we find that dark pattern effectiveness correlates positively with model size and test-time reasoning, making larger, more capable models more susceptible. Leading countermeasures against adversarial attacks, including in-context prompting and guardrail models, fail to consistently reduce the success rate of dark pattern interventions. Our findings reveal dark patterns as a latent and unmitigated risk to web agents, highlighting the urgent need for robust defenses against manipulative designs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æš—é»‘æ¨¡å¼(Dark Patterns)è¿™ä¸€æ¬ºéª—æ€§UIè®¾è®¡å¦‚ä½•æ“çºµWeb Agentsæ‰§è¡Œåç¦»ç›®æ ‡çš„æ“ä½œã€‚ç ”ç©¶è€…å¼•å…¥äº†åä¸ºDECEPTICONçš„è¯„ä¼°ç¯å¢ƒï¼Œé€šè¿‡700ä¸ªWebå¯¼èˆªä»»åŠ¡é‡åŒ–åˆ†æäº†æš—é»‘æ¨¡å¼å¯¹Agentè½¨è¿¹çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å…ˆè¿›çš„Web Agentsä¸­ï¼Œæš—é»‘æ¨¡å¼è¯±å¯¼å…¶äº§ç”Ÿæ¶æ„ç»“æœçš„æˆåŠŸç‡è¶…è¿‡70%ï¼Œè¿œé«˜äºäººç±»31%çš„å—å½±å“æ°´å¹³ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œæ¨¡å‹è§„æ¨¡è¶Šå¤§ã€æ¨ç†èƒ½åŠ›(Test-time reasoning)è¶Šå¼ºï¼Œå—æš—é»‘æ¨¡å¼æ“çºµçš„é£é™©åè€Œè¶Šé«˜ã€‚å½“å‰çš„é˜²å¾¡æ‰‹æ®µå¦‚æç¤ºå·¥ç¨‹(In-context prompting)å’ŒæŠ¤æ æ¨¡å‹(Guardrail models)å‡æ— æ³•ç¨³å®šé™ä½å¹²æ‰°æˆåŠŸç‡ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†æš—é»‘æ¨¡å¼æ˜¯Web Agentsé¢ä¸´çš„ä¸¥é‡æ½œåœ¨é£é™©ï¼ŒäºŸéœ€å¼€å‘æ›´å…·é²æ£’æ€§çš„é˜²å¾¡ä½“ç³»ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22894v1",
      "published_date": "2025-12-28 11:55:20 UTC",
      "updated_date": "2025-12-28 11:55:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:22:18.467159+00:00"
    },
    {
      "arxiv_id": "2512.22883v1",
      "title": "Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations",
      "title_zh": "é¢å‘ç½‘ç»œéŸ§æ€§çš„æ™ºèƒ½ä½“ AIï¼šå®‰å…¨æ–°èŒƒå¼åŠå…¶ç³»ç»Ÿè®ºåŸºç¡€",
      "authors": [
        "Tao Li",
        "Quanyan Zhu"
      ],
      "abstract": "Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºåŸºç¡€æ¨¡å‹(Foundation Models)çš„äººå·¥æ™ºèƒ½å¦‚ä½•ä»æ ¹æœ¬ä¸Šé‡å¡‘ç½‘ç»œå®‰å…¨é¢†åŸŸï¼Œå¹¶ä¸»å¼ ä»ä¼ ç»Ÿçš„ä»¥é¢„é˜²ä¸ºä¸­å¿ƒçš„å®‰å…¨æ¨¡å¼è½¬å‘æ™ºèƒ½ä½“åŒ–ç½‘ç»œéŸ§æ€§(Agentic Cyber Resilience)ã€‚è¯¥èŒƒå¼å¼ºè°ƒç³»ç»Ÿåº”å…·å¤‡é¢„æµ‹å¹²æ‰°ã€åœ¨æ”»å‡»ä¸‹ç»´æŒå…³é”®åŠŸèƒ½ã€é«˜æ•ˆæ¢å¤ä»¥åŠæŒç»­å­¦ä¹ çš„èƒ½åŠ›ï¼Œä½¿è‡ªä¸»æ™ºèƒ½ä½“(Autonomous Agents)èƒ½å¤Ÿç›´æ¥å‚ä¸ç½‘ç»œåŠä¿¡æ¯ç‰©ç†ç³»ç»Ÿ(Cyber-Physical Systems)çš„æ„ŸçŸ¥ã€æ¨ç†ä¸é€‚åº”ã€‚ç ”ç©¶å¼€å‘äº†ä¸€å¥—ç”¨äºè®¾è®¡æ™ºèƒ½ä½“åŒ–AIå·¥ä½œæµçš„ç³»ç»Ÿçº§æ¡†æ¶ï¼Œé€šè¿‡å°†æ”»å‡»è€…ä¸é˜²å¾¡è€…çš„äº¤äº’å»ºæ¨¡ä¸ºè€¦åˆçš„è‡ªé€‚åº”è¿‡ç¨‹ï¼Œåˆ©ç”¨åšå¼ˆè®º(Game-Theoretic)å…¬å¼ä¸ºè‡ªä¸»æƒåˆ†é…å’Œä¿¡æ¯æµæä¾›ç»Ÿä¸€çš„è®¾è®¡è¯­è¨€ã€‚é€šè¿‡è‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•(Automated Penetration Testing)ã€ä¿®å¤å’Œç½‘ç»œæ¬ºéª—(Cyber Deception)çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥ç ”ç©¶è®ºè¯äº†åŸºäºå‡è¡¡çš„è®¾è®¡(Equilibrium-based Design)åœ¨å®ç°ç³»ç»Ÿçº§ç½‘ç»œéŸ§æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22883v1",
      "published_date": "2025-12-28 11:17:36 UTC",
      "updated_date": "2025-12-28 11:17:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:21:55.480307+00:00"
    },
    {
      "arxiv_id": "2512.22878v1",
      "title": "SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation",
      "title_zh": "SwinTF3Dï¼šä¸€ç§ç”¨äºæ–‡æœ¬å¼•å¯¼ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„è½»é‡çº§å¤šæ¨¡æ€èåˆæ–¹æ³•",
      "authors": [
        "Hasan Faraz Khan",
        "Noor Fatima",
        "Muzammil Behzad"
      ],
      "abstract": "The recent integration of artificial intelligence into medical imaging has driven remarkable advances in automated organ segmentation. However, most existing 3D segmentation frameworks rely exclusively on visual learning from large annotated datasets restricting their adaptability to new domains and clinical tasks. The lack of semantic understanding in these models makes them ineffective in addressing flexible, user-defined segmentation objectives. To overcome these limitations, we propose SwinTF3D, a lightweight multimodal fusion approach that unifies visual and linguistic representations for text-guided 3D medical image segmentation. The model employs a transformer-based visual encoder to extract volumetric features and integrates them with a compact text encoder via an efficient fusion mechanism. This design allows the system to understand natural-language prompts and correctly align semantic cues with their corresponding spatial structures in medical volumes, while producing accurate, context-aware segmentation results with low computational overhead. Extensive experiments on the BTCV dataset demonstrate that SwinTF3D achieves competitive Dice and IoU scores across multiple organs, despite its compact architecture. The model generalizes well to unseen data and offers significant efficiency gains compared to conventional transformer-based segmentation networks. Bridging visual perception with linguistic understanding, SwinTF3D establishes a practical and interpretable paradigm for interactive, text-driven 3D medical image segmentation, opening perspectives for more adaptive and resource-efficient solutions in clinical imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰3DåŒ»ç–—å½±åƒåˆ†å‰²æ¨¡å‹ä»…ä¾èµ–è§†è§‰å­¦ä¹ ä¸”ç¼ºä¹è¯­ä¹‰ç†è§£ã€éš¾ä»¥é€‚åº”çµæ´»ä¸´åºŠä»»åŠ¡çš„é—®é¢˜ï¼Œæå‡ºäº†SwinTF3Dï¼Œä¸€ç§è½»é‡çº§çš„å¤šæ¨¡æ€èåˆ(Multimodal Fusion)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ–‡æœ¬å¼•å¯¼å®ç°3DåŒ»ç–—å›¾åƒåˆ†å‰²ã€‚è¯¥æ¨¡å‹åˆ©ç”¨åŸºäºTransformerçš„è§†è§‰ç¼–ç å™¨(Visual Encoder)æå–å®¹ç§¯ç‰¹å¾ï¼Œå¹¶ç»“åˆç´§å‡‘çš„æ–‡æœ¬ç¼–ç å™¨(Text Encoder)åŠé«˜æ•ˆèåˆæœºåˆ¶ï¼Œå®ç°äº†è§†è§‰ä¸è¯­è¨€è¡¨ç¤ºçš„ç»Ÿä¸€ã€‚è¿™ç§è®¾è®¡èµ‹äºˆäº†ç³»ç»Ÿç†è§£è‡ªç„¶è¯­è¨€æç¤º(Natural-language Prompts)çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å°†è¯­ä¹‰çº¿ç´¢ä¸åŒ»ç–—å®¹ç§¯ä¸­çš„ç©ºé—´ç»“æ„ç²¾å‡†å¯¹é½ï¼Œåœ¨ä¿æŒä½è®¡ç®—å¼€é”€çš„åŒæ—¶ç”Ÿæˆå‡†ç¡®çš„åˆ†å‰²ç»“æœã€‚åœ¨BTCVæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSwinTF3Dåœ¨å¤šå™¨å®˜åˆ†å‰²ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„Diceå’ŒIoUåˆ†æ•°ï¼Œä¸”åœ¨æ³›åŒ–èƒ½åŠ›å’Œè¿è¡Œæ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„Transformeråˆ†å‰²ç½‘ç»œã€‚é€šè¿‡æ¡¥æ¥è§†è§‰æ„ŸçŸ¥ä¸è¯­è¨€ç†è§£ï¼ŒSwinTF3Då»ºç«‹äº†ä¸€ä¸ªå®ç”¨ä¸”å¯è§£é‡Šçš„äº¤äº’å¼åˆ†å‰²èŒƒå¼ï¼Œä¸ºå¼€å‘é€‚åº”æ€§å¼ºã€èµ„æºåˆ©ç”¨ç‡é«˜çš„ä¸´åºŠå½±åƒè§£å†³æ–¹æ¡ˆå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22878v1",
      "published_date": "2025-12-28 11:00:05 UTC",
      "updated_date": "2025-12-28 11:00:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:22:34.848841+00:00"
    },
    {
      "arxiv_id": "2512.22876v1",
      "title": "Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks",
      "title_zh": "Reinforcement Networksï¼šåä½œå¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„æ–°å‹æ¡†æ¶",
      "authors": [
        "Maksim Kryzhanovskiy",
        "Svetlana Glazyrina",
        "Roman Ischenko",
        "Konstantin Vorontsov"
      ],
      "abstract": "Modern AI systems often comprise multiple learnable components that can be naturally organized as graphs. A central challenge is the end-to-end training of such systems without restrictive architectural or training assumptions. Such tasks fit the theory and approaches of the collaborative Multi-Agent Reinforcement Learning (MARL) field. We introduce Reinforcement Networks, a general framework for MARL that organizes agents as vertices in a directed acyclic graph (DAG). This structure extends hierarchical RL to arbitrary DAGs, enabling flexible credit assignment and scalable coordination while avoiding strict topologies, fully centralized training, and other limitations of current approaches. We formalize training and inference methods for the Reinforcement Networks framework and connect it to the LevelEnv concept to support reproducible construction, training, and evaluation. We demonstrate the effectiveness of our approach on several collaborative MARL setups by developing several Reinforcement Networks models that achieve improved performance over standard MARL baselines. Beyond empirical gains, Reinforcement Networks unify hierarchical, modular, and graph-structured views of MARL, opening a principled path toward designing and training complex multi-agent systems. We conclude with theoretical and practical directions - richer graph morphologies, compositional curricula, and graph-aware exploration. That positions Reinforcement Networks as a foundation for a new line of research in scalable, structured MARL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Reinforcement Networksï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåä½œå¼Multi-Agent Reinforcement Learning (MARL)ä»»åŠ¡çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨æ¶æ„å’Œè®­ç»ƒå‡è®¾ä¸Šçš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å°†æ™ºèƒ½ä½“ç»„ç»‡ä¸ºæœ‰å‘æ— ç¯å›¾(DAG)ä¸­çš„é¡¶ç‚¹ï¼ŒæˆåŠŸå°†å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ æ‰©å±•è‡³ä»»æ„DAGç»“æ„ï¼Œä»è€Œå®ç°äº†æ›´çµæ´»çš„ä¿¡ç”¨åˆ†é…(credit assignment)å’Œé«˜æ•ˆçš„åä½œã€‚é€šè¿‡å¼•å…¥LevelEnvæ¦‚å¿µï¼Œç ”ç©¶ç¡®ä¿äº†æ¨¡å‹æ„å»ºä¸è¯„ä¼°çš„å¯é‡å¤æ€§ï¼Œå¹¶åœ¨å¤šä¸ªå®éªŒåœºæ™¯ä¸­è¯æ˜äº†å…¶æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„MARLåŸºçº¿æ¨¡å‹ã€‚Reinforcement Networksä¸ä»…åœ¨ç»éªŒæ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œè¿˜ç»Ÿä¸€äº†MARLçš„å±‚æ¬¡åŒ–ã€æ¨¡å—åŒ–å’Œå›¾ç»“æ„è§†è§’ï¼Œä¸ºè®¾è®¡ä¸è®­ç»ƒå¤æ‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¥ å®šäº†å…·æœ‰ç†è®ºåŸºç¡€ä¸”å¯æ‰©å±•çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22876v1",
      "published_date": "2025-12-28 10:56:20 UTC",
      "updated_date": "2025-12-28 10:56:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:22:06.019046+00:00"
    },
    {
      "arxiv_id": "2512.22868v1",
      "title": "The body is not there to compute: Comment on \"Informational embodiment: Computational role of information structure in codes and robots\" by Pitti et al",
      "title_zh": "èº«ä½“å¹¶éä¸ºäº†è®¡ç®—ï¼šè¯„ Pitti ç­‰äººçš„ã€Šä¿¡æ¯å…·èº«ï¼šä»£ç ä¸æœºå™¨äººä¸­ä¿¡æ¯ç»“æ„çš„è®¡ç®—ä½œç”¨ã€‹",
      "authors": [
        "Matej Hoffmann"
      ],
      "abstract": "Applying the lens of computation and information has been instrumental in driving the technological progress of our civilization as well as in empowering our understanding of the world around us. The digital computer was and for many still is the leading metaphor for how our mind operates. Information theory (IT) has also been important in our understanding of how nervous systems encode and process information. The target article deploys information and computation to bodies: to understand why they have evolved in particular ways (animal bodies) and to design optimal bodies (robots). In this commentary, I argue that the main role of bodies is not to compute.",
      "tldr_zh": "è¯¥è¯„è®ºé’ˆå¯¹ Pitti ç­‰äººå…³äº \"Informational embodiment: Computational role of information structure in codes and robots\" çš„æ–‡ç« å±•å¼€è®¨è®ºï¼Œæ¢è®¨äº†è®¡ç®—(computation)å’Œä¿¡æ¯(information)è§†è§’åœ¨ç†è§£ç”Ÿç‰©ä½“åŠæœºå™¨äººè®¾è®¡ä¸­çš„åº”ç”¨ã€‚ä½œè€…å›é¡¾äº†ä¿¡æ¯ç†è®º(Information theory, IT)åœ¨ç¥ç»ç³»ç»Ÿç¼–ç å’Œå¤„ç†ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠæ•°å­—è®¡ç®—æœº(digital computer)ä½œä¸ºæ€ç»´è¿ä½œéšå–»çš„å†å²åœ°ä½ã€‚é’ˆå¯¹ç›®æ ‡æ–‡ç« å°†è®¡ç®—ä¸ä¿¡æ¯æ¡†æ¶åº”ç”¨äºèº«ä½“è¿›åŒ–ä¸ä¼˜åŒ–è®¾è®¡çš„åšæ³•ï¼Œä½œè€…æå‡ºäº†æ ¸å¿ƒè´¨ç–‘ã€‚ä»–æ˜ç¡®ä¸»å¼ èº«ä½“çš„ä¸»è¦è§’è‰²å¹¶ä¸æ˜¯ä¸ºäº†è¿›è¡Œè®¡ç®—(not to compute)ã€‚è¯¥æ–‡ç« é€šè¿‡åæ€ç”Ÿç‰©å®ä½“çš„æœ¬è´¨ï¼ŒæŒ‘æˆ˜äº†å½“å‰å°†èº«ä½“åŠŸèƒ½è¿‡åº¦ç®€åŒ–ä¸ºä¿¡æ¯å¤„ç†æˆ–è®¡ç®—è¿‡ç¨‹çš„ç ”ç©¶è¶‹å‘ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "q-bio.NC",
        "q-bio.QM"
      ],
      "primary_category": "cs.RO",
      "comment": "Comment on Pitti, A., Austin, M., Nakajima, K., & Kuniyoshi, Y. (2025). Informational Embodiment: Computational role of information structure in codes and robots. Physics of Life Reviews 53, 262-276. https://doi.org/10.1016/j.plrev.2025.03.018. Also available as arXiv:2408.12950",
      "pdf_url": "https://arxiv.org/pdf/2512.22868v1",
      "published_date": "2025-12-28 10:44:53 UTC",
      "updated_date": "2025-12-28 10:44:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:22:07.468166+00:00"
    },
    {
      "arxiv_id": "2601.09726v1",
      "title": "Forgetting as a Feature: Cognitive Alignment of Large Language Models",
      "title_zh": "é—å¿˜å³ç‰¹æ€§ï¼šå¤§è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥å¯¹é½",
      "authors": [
        "Hien Tran",
        "Quinten Steenhuis",
        "Alexandros Christoforos",
        "Chadbourne Davis"
      ],
      "abstract": "Large Language Models (LLMs) are often evaluated against ideals of perfect Bayesian inference, yet growing evidence suggests that their in-context reasoning exhibits systematic forgetting of past information. Rather than viewing this behavior as a limitation, we reinterpret forgetting as a functional cognitive mechanism. Drawing inspiration from human memory dynamics, we model LLM inference as a probabilistic memory process governed by exponential decay. We introduce a benchmark suite that evaluates temporal reasoning, concept drift adaptation, and associative recall, enabling direct comparison between model behavior and human cognitive patterns. Our empirical results reveal that LLMs demonstrate forgetting rates analogous to human memory efficiency trade-offs between stability and adaptability. Building on these observations, we propose probabilistic memory prompting, a lightweight strategy that shapes evidence integration to mimic human-like memory decay, leading to improved long-horizon reasoning performance. Our findings position forgetting not as a failure mode, but as a principled mechanism for adaptive intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„é—å¿˜ç°è±¡ï¼Œå¹¶å°†å…¶ä»å•çº¯çš„æ€§èƒ½é™åˆ¶é‡æ–°å®šä¹‰ä¸ºä¸€ç§åŠŸèƒ½æ€§çš„è®¤çŸ¥æœºåˆ¶ã€‚å—äººç±»è®°å¿†åŠ¨åŠ›å­¦å¯å‘ï¼Œä½œè€…å°†LLMsçš„æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºå—æŒ‡æ•°è¡°å‡(exponential decay)é©±åŠ¨çš„æ¦‚ç‡è®°å¿†è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥äº†æ¶µç›–æ—¶é—´æ¨ç†ã€æ¦‚å¿µæ¼‚ç§»(concept drift)é€‚åº”å’Œå…³è”å¬å›(associative recall)çš„åŸºå‡†æµ‹è¯•é›†ã€‚å®éªŒç»“æœæ­ç¤ºï¼ŒLLMsçš„é—å¿˜ç‡ä¸äººç±»è®°å¿†åœ¨ç¨³å®šæ€§ä¸é€‚åº”æ€§ä¹‹é—´çš„æ•ˆç‡æƒè¡¡(efficiency trade-offs)å…·æœ‰é«˜åº¦ç›¸ä¼¼æ€§ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºæ¦‚ç‡è®°å¿†æç¤º(probabilistic memory prompting)çš„è½»é‡åŒ–ç­–ç•¥ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»çš„è®°å¿†è¡°å‡æ¨¡å¼æ¥ä¼˜åŒ–è¯æ®æ•´åˆã€‚è¯¥æ–¹æ³•æœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨é•¿ç¨‹æ¨ç†(long-horizon reasoning)ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œè¯æ˜äº†é—å¿˜æ˜¯å®ç°è‡ªé€‚åº”æ™ºèƒ½çš„ä¸€ç§åŸåˆ™æ€§æœºåˆ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under submission",
      "pdf_url": "https://arxiv.org/pdf/2601.09726v1",
      "published_date": "2025-12-28 10:43:00 UTC",
      "updated_date": "2025-12-28 10:43:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:22:16.647538+00:00"
    },
    {
      "arxiv_id": "2512.22857v1",
      "title": "AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning",
      "title_zh": "AutoForgeï¼šé¢å‘æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨åŒ–ç¯å¢ƒåˆæˆ",
      "authors": [
        "Shihao Cai",
        "Runnan Fang",
        "Jialong Wu",
        "Baixuan Li",
        "Xinyu Wang",
        "Yong Jiang",
        "Liangcai Su",
        "Liwen Zhang",
        "Wenbiao Yin",
        "Zhen Zhang",
        "Fuli Feng",
        "Pengjun Xie",
        "Xiaobin Wang"
      ],
      "abstract": "Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoForgeï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Agentic Reinforcement Learning) çš„è‡ªåŠ¨åŒ–ç¯å¢ƒåˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡æ‹Ÿç¯å¢ƒåˆæˆè‡ªåŠ¨åŒ–ç¨‹åº¦ä½ä¸”ä»»åŠ¡éš¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€å¥—ç»Ÿä¸€çš„æµæ°´çº¿ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä¸”å¤§è§„æ¨¡åœ°ç”Ÿæˆå…·æœ‰é«˜éš¾åº¦ä¸”æ˜“äºéªŒè¯ä»»åŠ¡çš„æ¨¡æ‹Ÿç¯å¢ƒã€‚é’ˆå¯¹æ¨¡æ‹Ÿç”¨æˆ·çš„ä¸ç¨³å®šæ€§åŠç¯å¢ƒé—´çš„å¼‚æ„æ€§ï¼ŒAutoForge å¼•å…¥äº†ä¸€ç§ç¯å¢ƒçº§çš„å¼ºåŒ–å­¦ä¹  (RL) ç®—æ³•ï¼Œé€šè¿‡åœ¨ç¯å¢ƒå±‚é¢è¿›è¡Œä¼˜åŠ¿ä¼°è®¡ (Advantage Estimation) æ¥æ˜¾è‘—æå‡è®­ç»ƒçš„æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚åœ¨ tau-benchã€tau2-Bench å’Œ VitaBench ç­‰å¤šä¸ªæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ·±å…¥åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ AutoForge åœ¨è·¨é¢†åŸŸæ³›åŒ– (Out-of-domain Generalization) æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºå¢å¼ºè¯­è¨€æ™ºèƒ½ä½“çš„èƒ½åŠ›æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„é€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22857v1",
      "published_date": "2025-12-28 09:43:11 UTC",
      "updated_date": "2025-12-28 09:43:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:22:20.338056+00:00"
    },
    {
      "arxiv_id": "2512.22827v1",
      "title": "FasterPy: An LLM-based Code Execution Efficiency Optimization Framework",
      "title_zh": "FasterPyï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»£ç æ‰§è¡Œæ•ˆç‡ä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Yue Wu",
        "Minghao Han",
        "Ruiyin Li",
        "Peng Liang",
        "Amjed Tahir",
        "Zengyang Li",
        "Qiong Feng",
        "Mojtaba Shahin"
      ],
      "abstract": "Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FasterPyï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ä½æˆæœ¬ä¸”é«˜æ•ˆçš„æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ–Pythonä»£ç çš„æ‰§è¡Œæ•ˆç‡ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿè§„åˆ™æ–¹æ³•çš„é«˜äººåŠ›æˆæœ¬å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¯æ‰©å±•æ€§ä¸Šçš„å±€é™ï¼ŒFasterPyåˆ›æ–°æ€§åœ°ç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸ä½ç§©è‡ªé€‚åº”(LoRA)æŠ€æœ¯ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç”±ç°æœ‰æ€§èƒ½æå‡ä»£ç å¯¹å’Œæ€§èƒ½è¯„ä¼°æ•°æ®æ„å»ºçš„çŸ¥è¯†åº“ï¼Œé€šè¿‡RAGä¸ºæ¨¡å‹æä¾›ç²¾ç¡®çš„ä¼˜åŒ–å‚è€ƒã€‚åŒæ—¶ï¼ŒLoRAå¾®è°ƒæŠ€æœ¯çš„åº”ç”¨è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹å¯¹ä»£ç æ€§èƒ½æ¨¡å¼çš„ç†è§£ä¸ä¼˜åŒ–èƒ½åŠ›ã€‚åœ¨æ€§èƒ½æå‡ä»£ç ç¼–è¾‘(PIE)åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒFasterPyåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–ä»£ç æ€§èƒ½ä¼˜åŒ–æä¾›äº†æ–°çš„æœ‰æ•ˆè·¯å¾„ï¼Œå¹¶å±•ç¤ºäº†LLMsåœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "32 pages, 5 images, 7 tables, Manuscript submitted to a Journal (2025)",
      "pdf_url": "https://arxiv.org/pdf/2512.22827v1",
      "published_date": "2025-12-28 07:43:08 UTC",
      "updated_date": "2025-12-28 07:43:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:05.107247+00:00"
    },
    {
      "arxiv_id": "2601.06066v1",
      "title": "TEAS: Trusted Educational AI Standard: A Framework for Verifiable, Stable, Auditable, and Pedagogically Sound Learning Systems",
      "title_zh": "TEASï¼šå¯ä¿¡æ•™è‚²äººå·¥æ™ºèƒ½æ ‡å‡†ï¼šä¸€ç§é¢å‘å¯éªŒè¯ã€ç¨³å®šã€å¯å®¡è®¡ä¸”ç¬¦åˆæ•™è‚²å­¦åŸç†çš„å­¦ä¹ ç³»ç»Ÿæ¡†æ¶",
      "authors": [
        "Abu Syed"
      ],
      "abstract": "The rapid integration of AI into education has prioritized capability over trustworthiness, creating significant risks. Real-world deployments reveal that even advanced models are insufficient without extensive architectural scaffolding to ensure reliability. Current evaluation frameworks are fragmented: institutional policies lack technical verification, pedagogical guidelines assume AI reliability, and technical metrics are context-agnostic. This leaves institutions without a unified standard for deployment readiness. This paper introduces TEAS (Trusted Educational AI Standard), an integrated framework built on four interdependent pillars: (1) Verifiability, grounding content in authoritative sources; (2) Stability, ensuring deterministic core knowledge; (3) Auditability, enabling independent institutional validation; and (4) Pedagogical Soundness, enforcing principles of active learning. We argue that trustworthiness stems primarily from systematic architecture, not raw model capability. This insight implies that affordable, open-source models can achieve deployment-grade trust, offering a scalable and equitable path to integrating AI safely into learning environments globally.",
      "tldr_zh": "é’ˆå¯¹æ•™è‚²é¢†åŸŸäººå·¥æ™ºèƒ½(AI)é›†æˆä¸­å­˜åœ¨çš„ä¿¡ä»»é£é™©å’Œè¯„ä¼°æ¡†æ¶ç¢ç‰‡åŒ–é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†TEAS (Trusted Educational AI Standard)æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æ„å»ºå¯éªŒè¯ã€ç¨³å®šã€å¯å®¡è®¡ä¸”ç¬¦åˆæ•™è‚²å­¦åŸç†çš„å­¦ä¹ ç³»ç»Ÿï¼Œç”±å››ä¸ªç›¸äº’ä¾èµ–çš„æ ¸å¿ƒæ”¯æŸ±ç»„æˆï¼šç¡®ä¿å†…å®¹åŸºäºæƒå¨æ¥æºçš„Verifiabilityã€ä¿è¯æ ¸å¿ƒçŸ¥è¯†ç¡®å®šæ€§çš„Stabilityã€æ”¯æŒæœºæ„ç‹¬ç«‹éªŒè¯çš„Auditabilityä»¥åŠéµå¾ªä¸»åŠ¨å­¦ä¹ åŸåˆ™çš„Pedagogical Soundnessã€‚ç ”ç©¶å¼ºè°ƒï¼Œæ•™è‚²AIçš„ä¿¡ä»»åº¦ä¸»è¦æºäºç³»ç»Ÿçš„æ¶æ„è®¾è®¡(systematic architecture)è€Œéæ¨¡å‹æœ¬èº«çš„åŸå§‹èƒ½åŠ›(raw model capability)ã€‚è¿™ä¸€è§è§£æ„å‘³ç€é€šè¿‡åˆç†çš„æ¶æ„æ”¯æ’‘ï¼Œä½æˆæœ¬çš„å¼€æºæ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°éƒ¨ç½²çº§çš„ä¿¡ä»»æ°´å¹³ã€‚è¯¥æ¡†æ¶ä¸ºå…¨çƒæ•™è‚²ç¯å¢ƒå®‰å…¨ã€å…¬å¹³åœ°å¤§è§„æ¨¡é›†æˆAIæŠ€æœ¯æä¾›äº†ä¸€æ¡å…·æœ‰å¯æ‰©å±•æ€§çš„å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "19 pages, 6 tables, accepted at AAAI-26 (DAI Workshop) in Singapore",
      "pdf_url": "https://arxiv.org/pdf/2601.06066v1",
      "published_date": "2025-12-28 07:40:26 UTC",
      "updated_date": "2025-12-28 07:40:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:08.847454+00:00"
    },
    {
      "arxiv_id": "2512.22808v2",
      "title": "EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation",
      "title_zh": "EgoReActï¼šç¬¬ä¸€è§†è§’è§†é¢‘é©±åŠ¨çš„ä¸‰ç»´äººä½“ååº”ç”Ÿæˆ",
      "authors": [
        "Libo Zhang",
        "Zekun Li",
        "Tianyu Li",
        "Zeyu Cao",
        "Rui Xu",
        "Xiaoxiao Long",
        "Wenjia Wang",
        "Jingbo Wang",
        "Yuan Liu",
        "Wenping Wang",
        "Daquan Zhou",
        "Taku Komura",
        "Zhiyang Dou"
      ],
      "abstract": "Humans exhibit adaptive, context-sensitive responses to egocentric visual input. However, faithfully modeling such reactions from egocentric video remains challenging due to the dual requirements of strictly causal generation and precise 3D spatial alignment. To tackle this problem, we first construct the Human Reaction Dataset (HRD) to address data scarcity and misalignment by building a spatially aligned egocentric video-reaction dataset, as existing datasets (e.g., ViMo) suffer from significant spatial inconsistency between the egocentric video and reaction motion, e.g., dynamically moving motions are always paired with fixed-camera videos. Leveraging HRD, we present EgoReAct, the first autoregressive framework that generates 3D-aligned human reaction motions from egocentric video streams in real-time. We first compress the reaction motion into a compact yet expressive latent space via a Vector Quantised-Variational AutoEncoder and then train a Generative Pre-trained Transformer for reaction generation from the visual input. EgoReAct incorporates 3D dynamic features, i.e., metric depth, and head dynamics during the generation, which effectively enhance spatial grounding. Extensive experiments demonstrate that EgoReAct achieves remarkably higher realism, spatial consistency, and generation efficiency compared with prior methods, while maintaining strict causality during generation. We will release code, models, and data upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»ç¬¬ä¸€è§†è§’(Egocentric)è§†é¢‘ä¸­ç”Ÿæˆç¬¦åˆé€»è¾‘ä¸”ç©ºé—´å¯¹é½çš„3Däººä½“ååº”åŠ¨ä½œè¿™ä¸€éš¾é¢˜ï¼ŒæŒ‡å‡ºå½“å‰æ•°æ®é›†å¦‚ViMoå­˜åœ¨ç©ºé—´ä¸ä¸€è‡´å’Œæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†Human Reaction Dataset (HRD)ï¼Œé€šè¿‡å»ºç«‹ç©ºé—´å¯¹é½çš„ç¬¬ä¸€è§†è§’è§†é¢‘ä¸ååº”åŠ¨ä½œå…³è”ï¼Œè§£å†³äº†ç›¸æœºè§†è§’ä¸è¿åŠ¨ä¸åŒ¹é…çš„ç¼ºé™·ã€‚éšåï¼Œè¯¥è®ºæ–‡æå‡ºäº†EgoReActï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿä»å®æ—¶è§†é¢‘æµä¸­ç”Ÿæˆ3Då¯¹é½ååº”åŠ¨ä½œçš„è‡ªå›å½’(Autoregressive)æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Vector Quantised-Variational AutoEncoderå°†åŠ¨ä½œå‹ç¼©è‡³éšç©ºé—´ï¼Œå¹¶ç»“åˆGenerative Pre-trained Transformerè¿›è¡Œé¢„æµ‹ç”Ÿæˆã€‚EgoReActåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ•´åˆäº†Metric depthå’ŒHead dynamicsç­‰3DåŠ¨æ€ç‰¹å¾ï¼Œæ˜¾è‘—å¢å¼ºäº†åŠ¨ä½œçš„ç©ºé—´æ„ŸçŸ¥ä¸å®šä½(Spatial grounding)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä¸¥æ ¼å› æœæ€§çš„åŒæ—¶ï¼Œåœ¨çœŸå®æ„Ÿã€ç©ºé—´ä¸€è‡´æ€§å’Œç”Ÿæˆæ•ˆç‡ä¸Šå‡ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22808v2",
      "published_date": "2025-12-28 06:44:05 UTC",
      "updated_date": "2026-01-03 03:38:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:12.327201+00:00"
    },
    {
      "arxiv_id": "2512.22804v1",
      "title": "MoR: Mixture Of Representations For Mixed-Precision Training",
      "title_zh": "MoRï¼šé¢å‘æ··åˆç²¾åº¦è®­ç»ƒçš„è¡¨å¾æ··åˆ",
      "authors": [
        "Bor-Yiing Su",
        "Peter Dykas",
        "Mike Chrzanowski",
        "Jatin Chhugani"
      ],
      "abstract": "Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MoR (Mixture-of-Representations)ï¼Œä¸€ç§æ–°å‹çš„å¼ é‡çº§å’Œå­å¼ é‡çº§é‡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æ··åˆç²¾åº¦è®­ç»ƒ(Mixed-precision training)ä¸­æ•°æ®è¡¨ç¤ºçš„é€‰æ‹©ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŠ¨æ€åˆ†æå¼ é‡çš„æ•°å€¼å±æ€§ï¼Œå¹¶åœ¨FP8å’ŒBF16ç­‰å¤šç§è¡¨ç¤ºå½¢å¼ä¹‹é—´è¿›è¡Œæ™ºèƒ½åˆ‡æ¢ï¼Œä»è€Œåœ¨ä¸åŒçš„é‡åŒ–åˆ†åŒºç­–ç•¥ä¸‹ä¿æŒæ¨¡å‹è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°†98.38%çš„å¼ é‡é‡åŒ–ä¸ºFP8æ ¼å¼çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½è¾¾åˆ°state-of-the-artçš„æ€§èƒ½æ°´å¹³ã€‚MoRä¸ä»…æ˜¾è‘—æé«˜äº†ä½ç²¾åº¦è®­ç»ƒçš„é²æ£’æ€§ï¼Œä¸”æ— éœ€å¤æ‚çš„ç»†ç²’åº¦åˆ†åŒºå³å¯å®ç°å“è¶Šçš„ç²¾åº¦è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¹¶å±•ç°å‡ºä¸NVFP4ç­‰æ›´ä½ç²¾åº¦æ•°å€¼æ ¼å¼ç»“åˆä½¿ç”¨çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22804v1",
      "published_date": "2025-12-28 06:28:50 UTC",
      "updated_date": "2025-12-28 06:28:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:21.812416+00:00"
    },
    {
      "arxiv_id": "2601.10726v1",
      "title": "Building AI Agents to Improve Job Referral Requests to Strangers",
      "title_zh": "æ„å»ºäººå·¥æ™ºèƒ½æ™ºèƒ½ä½“ä»¥ä¼˜åŒ–å‘é™Œç”Ÿäººå‘èµ·çš„æ±‚èŒå†…æ¨è¯·æ±‚",
      "authors": [
        "Ross Chu",
        "Yuting Huang"
      ],
      "abstract": "This paper develops AI agents that help job seekers write effective requests for job referrals in a professional online community. The basic workflow consists of an improver agent that rewrites the referral request and an evaluator agent that measures the quality of revisions using a model trained to predict the probability of receiving referrals from other users. Revisions suggested by the LLM (large language model) increase predicted success rates for weaker requests while reducing them for stronger requests. Enhancing the LLM with Retrieval-Augmented Generation (RAG) prevents edits that worsen stronger requests while it amplifies improvements for weaker requests. Overall, using LLM revisions with RAG increases the predicted success rate for weaker requests by 14\\% without degrading performance on stronger requests. Although improvements in model-predicted success do not guarantee more referrals in the real world, they provide low-cost signals for promising features before running higher-stakes experiments on real users.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†æ—¨åœ¨å¸®åŠ©æ±‚èŒè€…åœ¨ä¸“ä¸šåœ¨çº¿ç¤¾åŒºä¸­å‘é™Œç”Ÿäººæ’°å†™æœ‰æ•ˆ job referral requests çš„ AI agents ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿçš„å·¥ä½œæµç”±ä¸€ä¸ªè´Ÿè´£é‡å†™è¯·æ±‚çš„ improver agent å’Œä¸€ä¸ªåˆ©ç”¨é¢„æµ‹æ¨¡å‹è¡¡é‡ä¿®è®¢è´¨é‡çš„ evaluator agent ç»„æˆã€‚ç ”ç©¶å‘ç°ï¼Œå•çº¯ä½¿ç”¨ LLM è¿›è¡Œä¿®è®¢è™½èƒ½æå‡è¾ƒå¼±è¯·æ±‚çš„é¢„æµ‹æˆåŠŸç‡ï¼Œä½†ä¼šé™ä½è¾ƒå¼ºè¯·æ±‚çš„è¡¨ç°ã€‚é€šè¿‡å¼•å…¥ Retrieval-Augmented Generation (RAG) æŠ€æœ¯ï¼Œç³»ç»Ÿæœ‰æ•ˆé˜²æ­¢äº†å¯¹å¼ºè¯·æ±‚çš„è´Ÿé¢ä¿®æ”¹ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†å¯¹å¼±è¯·æ±‚çš„æ”¹è¿›æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆ RAG çš„ LLM ä¿®è®¢ä½¿è¾ƒå¼±è¯·æ±‚çš„é¢„æµ‹æˆåŠŸç‡æé«˜äº†14%ï¼Œä¸”æœªæŸå®³è¾ƒå¼ºè¯·æ±‚çš„è´¨é‡ã€‚è¯¥é¡¹å·¥ä½œä¸ºåœ¨è¿›è¡Œé«˜é£é™©çš„çœŸå®ç”¨æˆ·å®éªŒå‰è¯†åˆ«æœ‰æ•ˆç‰¹å¾æä¾›äº†ä½æˆæœ¬çš„ä¿¡å·å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.10726v1",
      "published_date": "2025-12-28 05:59:56 UTC",
      "updated_date": "2025-12-28 05:59:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:22.365036+00:00"
    },
    {
      "arxiv_id": "2512.22795v1",
      "title": "CNSight: Evaluation of Clinical Note Segmentation Tools",
      "title_zh": "CNSightï¼šä¸´åºŠç—…å†åˆ†æ®µå·¥å…·è¯„ä¼°",
      "authors": [
        "Risha Surana",
        "Adrian Law",
        "Sunwoo Kim",
        "Rishab Sridhar",
        "Angxiao Han",
        "Peiyu Hong"
      ],
      "abstract": "Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»ç”µå­ç—…å†(EMR)ç³»ç»Ÿä¸­æå–çš„ä¸´åºŠç¬”è®°å› ç¼ºä¹ç»“æ„åŒ–è€Œéš¾ä»¥è¿›è¡ŒäºŒæ¬¡åˆ†æçš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†è¯„ä¼°æ¡†æ¶ CNSight ä»¥è¯†åˆ«ç« èŠ‚è¾¹ç•Œã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ MIMIC-IV æ•°æ®é›†ä¸­çš„ 1,000 ä»½ç¬”è®°ï¼Œå¯¹åŸºäºè§„åˆ™çš„åŸºçº¿æ¨¡å‹ã€é¢†åŸŸç‰¹å®šçš„ Transformer æ¨¡å‹ä»¥åŠå¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäº API çš„å¤§æ¨¡å‹è¡¨ç°æœ€ä¼˜ï¼Œå…¶ä¸­ GPT-5-mini åœ¨å¥å­çº§å’Œè‡ªç”±æ–‡æœ¬åˆ‡åˆ†ä»»åŠ¡ä¸­è¾¾åˆ°äº† 72.4 çš„å¹³å‡ F1 åˆ†æ•°ã€‚è™½ç„¶è½»é‡çº§åŸºçº¿åœ¨ç»“æ„åŒ–ä»»åŠ¡ä¸­ä»æœ‰ç«äº‰åŠ›ï¼Œä½†åœ¨å¤„ç†éç»“æ„åŒ–æ–‡æœ¬ä¸Šæ˜æ˜¾å—é™ã€‚è¯¥ç ”ç©¶ä¸ºä¸´åºŠç¬”è®°åˆ‡åˆ†çš„æŠ€æœ¯é€‰å‹æä¾›äº†æŒ‡å¯¼ï¼Œå¹¶ä¸ºä¿¡æ¯æå–å’Œè‡ªåŠ¨æ‘˜è¦ç­‰ä¸‹æ¸¸ä¸´åºŠåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22795v1",
      "published_date": "2025-12-28 05:40:15 UTC",
      "updated_date": "2025-12-28 05:40:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:24.045976+00:00"
    },
    {
      "arxiv_id": "2601.03277v1",
      "title": "MixRx: Predicting Drug Combination Interactions with LLMs",
      "title_zh": "MixRxï¼šåŸºäº LLMs çš„è¯ç‰©ç»„åˆç›¸äº’ä½œç”¨é¢„æµ‹",
      "authors": [
        "Risha Surana",
        "Cameron Saidock",
        "Hugo Chacon"
      ],
      "abstract": "MixRx uses Large Language Models (LLMs) to classify drug combination interactions as Additive, Synergistic, or Antagonistic, given a multi-drug patient history. We evaluate the performance of 4 models, GPT-2, Mistral Instruct 2.0, and the fine-tuned counterparts. Our results showed a potential for such an application, with the Mistral Instruct 2.0 Fine-Tuned model providing an average accuracy score on standard and perturbed datasets of 81.5%. This paper aims to further develop an upcoming area of research that evaluates if LLMs can be used for biological prediction tasks.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº† MixRxï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ ¹æ®æ‚£è€…å¤šè¯ç‰©ç—…å²é¢„æµ‹è¯ç‰©ç»„åˆç›¸äº’ä½œç”¨çš„æ¡†æ¶ã€‚è¯¥ç ”ç©¶æ—¨åœ¨å°†è¯ç‰©ç›¸äº’ä½œç”¨åˆ†ç±»ä¸º Additiveã€Synergistic æˆ– Antagonistic ä¸‰ç§æ¨¡å¼ï¼Œå¹¶å¯¹æ¯”è¯„ä¼°äº† GPT-2ã€Mistral Instruct 2.0 åŠå…¶ Fine-Tuned ç‰ˆæœ¬çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„ Mistral Instruct 2.0 æ¨¡å‹åœ¨æ ‡å‡†å’Œ Perturbed æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº† 81.5%ã€‚è¿™ä¸€ç»“æœæœ‰åŠ›è¯æ˜äº† LLMs åœ¨ç”Ÿç‰©é¢„æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºæ¢ç´¢å¤§æ¨¡å‹åœ¨å¤æ‚ç”Ÿç‰©å­¦é¢†åŸŸçš„é¢„æµ‹èƒ½åŠ›æä¾›äº†é‡è¦å‚è€ƒã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥æ¨åŠ¨äº†åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯è¾…åŠ©ä¸´åºŠè¯ç‰©å†³ç­–çš„ç›¸å…³ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "q-bio.OT",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "q-bio.OT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.03277v1",
      "published_date": "2025-12-28 05:37:56 UTC",
      "updated_date": "2025-12-28 05:37:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:51.675360+00:00"
    },
    {
      "arxiv_id": "2512.22793v1",
      "title": "Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach",
      "title_zh": "åŸºäºå¯è¾¾æ€§åˆ†æçš„æ— äººæœºåˆ°è¾¾-å›é¿å¾®åˆ†åšå¼ˆï¼šä¸€ç§åˆ†è§£æ–¹æ³•",
      "authors": [
        "Minh Bui",
        "Simon Monckton",
        "Mo Chen"
      ],
      "abstract": "Reach-avoid (RA) games have significant applications in security and defense, particularly for unmanned aerial vehicles (UAVs). These problems are inherently challenging due to the need to consider obstacles, consider the adversarial nature of opponents, ensure optimality, and account for nonlinear dynamics. Hamilton-Jacobi (HJ) reachability analysis has emerged as a powerful tool for tackling these challenges; however, while it has been applied to games involving two spatial dimensions, directly extending this approach to three spatial dimensions is impossible due to high dimensionality. On the other hand, alternative approaches for solving RA games lack the generality to consider games with three spatial dimensions involving agents with non-trivial system dynamics. In this work, we propose a novel framework for dimensionality reduction by decomposing the problem into a horizontal RA sub-game and a vertical RA sub-game. We then solve each sub-game using HJ reachability analysis and consider second-order dynamics that account for the defender's acceleration. To reconstruct the solution to the original RA game from the sub-games, we introduce a HJ-based tracking control algorithm in each sub-game that not only guarantees capture of the attacker but also tracking of the attacker thereafter. We prove the conditions under which the capture guarantees are maintained. The effectiveness of our approach is demonstrated via numerical simulations, showing that the decomposition maintains optimality and guarantees in the original problem. Our methods are also validated in a Gazebo physics simulator, achieving successful capture of quadrotors in three spatial dimensions space for the first time to the best of our knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAVs)åœ¨å®‰å…¨é˜²å¾¡é¢†åŸŸçš„åˆ°è¾¾-è§„é¿(Reach-avoid, RA)åšå¼ˆé—®é¢˜ï¼Œé’ˆå¯¹Hamilton-Jacobi (HJ) å¯è¾¾æ€§åˆ†æåœ¨ä¸‰ç»´é«˜ç»´ç©ºé—´ä¸­è®¡ç®—å—é™çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„é™ç»´åˆ†è§£æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å¤æ‚çš„ä¸‰ç»´åšå¼ˆåˆ†è§£ä¸ºæ°´å¹³å’Œå‚ç›´ä¸¤ä¸ªRAå­åšå¼ˆ(sub-games)ï¼Œå¹¶ç»“åˆè€ƒè™‘é˜²å¾¡è€…åŠ é€Ÿåº¦çš„äºŒé˜¶åŠ¨åŠ›å­¦æ¨¡å‹è¿›è¡Œç‹¬ç«‹æ±‚è§£ã€‚é€šè¿‡å¼•å…¥ä¸€ç§åŸºäºHJçš„è¿½è¸ªæ§åˆ¶ç®—æ³•(tracking control algorithm)ï¼Œç ”ç©¶å®ç°äº†ä»å­åšå¼ˆåˆ°åŸå§‹é—®é¢˜çš„è§£é‡æ„ï¼Œå¹¶ä¸¥æ ¼è¯æ˜äº†æ•è·æ”»å‡»è€…çš„ç†è®ºä¿éšœæ¡ä»¶ã€‚æ•°å€¼ä»¿çœŸä¸Gazeboç‰©ç†æ¨¡æ‹ŸéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ä¿æŒæœ€ä¼˜æ€§çš„å‰æä¸‹ï¼Œé¦–æ¬¡å®ç°äº†åœ¨ä¸‰ç»´ç©ºé—´ä¸­å¯¹å››æ—‹ç¿¼æ— äººæœºçš„æˆåŠŸæ•è·ã€‚è¯¥æˆæœä¸ºè§£å†³å…·æœ‰éå¹³å‡¡åŠ¨åŠ›å­¦ç‰¹å¾çš„é«˜ç»´å¯¹æŠ—æ€§åšå¼ˆé—®é¢˜æä¾›äº†æ–°çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "eess.SY",
      "comment": "Paper version accepted to the Journal of Guidance, Control, and Dynamics (JGCD)",
      "pdf_url": "https://arxiv.org/pdf/2512.22793v1",
      "published_date": "2025-12-28 05:34:11 UTC",
      "updated_date": "2025-12-28 05:34:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:30.047535+00:00"
    },
    {
      "arxiv_id": "2512.22792v2",
      "title": "A Universal and Robust Framework for Multiple Gas Recognition Based-on Spherical Normalization-Coupled Mahalanobis Algorithm",
      "title_zh": "åŸºäºçƒé¢å½’ä¸€åŒ–è€¦åˆé©¬æ°ç®—æ³•çš„é€šç”¨ä¸”ç¨³å¥çš„å¤šæ°”ä½“è¯†åˆ«æ¡†æ¶",
      "authors": [
        "Shuai Chen",
        "Yang Song",
        "Chen Wang",
        "Ziran Wang"
      ],
      "abstract": "Electronic nose (E-nose) systems face two interconnected challenges in open-set gas recognition: feature distribution shift caused by signal drift and decision boundary failure induced by unknown gas interference. Existing methods predominantly rely on Euclidean distance or conventional classifiers, failing to account for anisotropic feature distributions and dynamic signal intensity variations. To address these issues, this study proposes the Spherical Normalization coupled Mahalanobis (SNM) module, a universal post-processing module for open-set gas recognition. First, it achieves geometric decoupling through cascaded batch and L2 normalization, projecting features onto a unit hypersphere to eliminate signal intensity fluctuations. Second, it utilizes Mahalanobis distance to construct adaptive ellipsoidal decision boundaries that conform to the anisotropic feature geometry. The architecture-agnostic SNM-Module seamlessly integrates with mainstream backbones including Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Transformer. Experiments on the public Vergara dataset demonstrate that the Transformer+SNM configuration achieves near-theoretical-limit performance in discriminating among multiple target gases, with an AUROC of 0.9977 and an unknown gas detection rate of 99.57% at 5% false positive rate, significantly outperforming state-of-the-art methods with a 3.0% AUROC improvement and 91.0% standard deviation reduction compared to Class Anchor Clustering (CAC). The module maintains exceptional robustness across five sensor positions, with standard deviations below 0.0028. This work effectively addresses the critical challenge of simultaneously achieving high accuracy and high stability in open-set gas recognition, providing solid support for industrial E-nose deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Spherical Normalization coupled Mahalanobis (SNM) çš„é€šç”¨åå¤„ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”µå­é¼» (E-nose) ç³»ç»Ÿåœ¨å¼€æ”¾é›†æ°”ä½“è¯†åˆ«ä¸­å› ä¿¡å·æ¼‚ç§»å’ŒæœªçŸ¥æ°”ä½“å¹²æ‰°å¯¼è‡´çš„ç‰¹å¾åˆ†å¸ƒåç§»åŠå†³ç­–è¾¹ç•Œå¤±æ•ˆé—®é¢˜ã€‚è¯¥æ¨¡å—é€šè¿‡çº§è” Batch Normalization å’Œ L2 Normalization å®ç°å‡ ä½•è§£è€¦ï¼Œå°†ç‰¹å¾æŠ•å½±è‡³å•ä½è¶…çƒé¢ä»¥æ¶ˆé™¤ä¿¡å·å¼ºåº¦æ³¢åŠ¨ï¼Œå¹¶åˆ©ç”¨ Mahalanobis distance æ„å»ºå‡ºç¬¦åˆå„å‘å¼‚æ€§å‡ ä½•ç‰¹å¾çš„è‡ªé€‚åº”æ¤­çƒå†³ç­–è¾¹ç•Œã€‚SNM æ¨¡å—å…·æœ‰æ¶æ„æ— å…³æ€§ï¼Œå¯æ— ç¼é›†æˆäº CNNã€RNN å’Œ Transformer ç­‰ä¸»æµéª¨å¹²ç½‘ç»œä¸­ã€‚åœ¨å…¬å¼€çš„ Vergara æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTransformer+SNM é…ç½®çš„ AUROC è¾¾åˆ° 0.9977ï¼Œåœ¨ 5% è¯¯æŠ¥ç‡ä¸‹çš„æœªçŸ¥æ°”ä½“æ£€æµ‹ç‡é«˜è¾¾ 99.57%ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äº Class Anchor Clustering (CAC) ç­‰ç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒä¼ æ„Ÿå™¨ä½ç½®å‡å±•ç°å‡ºæé«˜çš„é²æ£’æ€§ï¼Œæ ‡å‡†å·®ä¿æŒåœ¨æä½æ°´å¹³ï¼Œä¸ºå·¥ä¸šçº§ç”µå­é¼»çš„éƒ¨ç½²æä¾›äº†é«˜ç²¾åº¦ä¸”é«˜ç¨³å®šæ€§çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages, 8 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.22792v2",
      "published_date": "2025-12-28 05:33:05 UTC",
      "updated_date": "2026-01-05 13:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:36.565259+00:00"
    },
    {
      "arxiv_id": "2512.22777v1",
      "title": "Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning",
      "title_zh": "å¿«æ…¢è‡ªé€‚åº”ï¼šé¢å‘å°æ ·æœ¬å­¦ä¹ çš„å¯è¿ç§»ç”µè·¯",
      "authors": [
        "Kasra Jalaldoust",
        "Elias Bareinboim"
      ],
      "abstract": "Generalization across the domains is not possible without asserting a structure that constrains the unseen target domain w.r.t. the source domain. Building on causal transportability theory, we design an algorithm for zero-shot compositional generalization which relies on access to qualitative domain knowledge in form of a causal graph for intra-domain structure and discrepancies oracle for inter-domain mechanism sharing. \\textit{Circuit-TR} learns a collection of modules (i.e., local predictors) from the source data, and transport/compose them to obtain a circuit for prediction in the target domain if the causal structure licenses. Furthermore, circuit transportability enables us to design a supervised domain adaptation scheme that operates without access to an explicit causal structure, and instead uses limited target data. Our theoretical results characterize classes of few-shot learnable tasks in terms of graphical circuit transportability criteria, and connects few-shot generalizability with the established notion of circuit size complexity; controlled simulations corroborate our theoretical results.",
      "tldr_zh": "è¯¥ç ”ç©¶åŸºäºå› æœå¯ç§»æ¤æ€§ç†è®º (Causal transportability theory)ï¼Œæå‡ºäº†åä¸º Circuit-TR çš„ç®—æ³•ï¼Œæ—¨åœ¨å®ç°è·¨é¢†åŸŸçš„é›¶æ ·æœ¬ç»„åˆæ³›åŒ– (Zero-shot compositional generalization)ã€‚è¯¥ç®—æ³•ä»æºæ•°æ®ä¸­å­¦ä¹ ä¸€ç»„æ¨¡å—åŒ–çš„å±€éƒ¨é¢„æµ‹å™¨ï¼Œå¹¶ä¾æ®å› æœç»“æ„æˆæƒå°†å…¶è¿ç§»å’Œé‡æ–°ç»„åˆï¼Œåœ¨ç›®æ ‡é¢†åŸŸæ„å»ºé¢„æµ‹ç”µè·¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸€ç§ç›‘ç£é¢†åŸŸè‡ªé€‚åº” (Supervised domain adaptation) æ–¹æ¡ˆï¼Œä½¿å…¶èƒ½åœ¨æ²¡æœ‰æ˜¾å¼å› æœå›¾çš„æƒ…å†µä¸‹ï¼Œä»…åˆ©ç”¨æœ‰é™çš„ç›®æ ‡æ•°æ®å®Œæˆé¢„æµ‹ã€‚ç†è®ºåˆ†æé€šè¿‡å›¾å½¢ç”µè·¯å¯ç§»æ¤æ€§æ ‡å‡†å®šä¹‰äº†å°‘æ ·æœ¬å­¦ä¹  (Few-shot learnable) ä»»åŠ¡çš„è¾¹ç•Œï¼Œå¹¶æ­ç¤ºäº†æ³›åŒ–æ€§ä¸ç”µè·¯è§„æ¨¡å¤æ‚åº¦ (Circuit size complexity) ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚å—æ§æ¨¡æ‹Ÿå®éªŒéªŒè¯äº†ç†è®ºé¢„æµ‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†è·¨é¢†åŸŸåˆ†å¸ƒå˜åŒ–æ—¶åˆ©ç”¨ç»“æ„åŒ–çº¦æŸçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22777v1",
      "published_date": "2025-12-28 04:38:43 UTC",
      "updated_date": "2025-12-28 04:38:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:23:46.280715+00:00"
    },
    {
      "arxiv_id": "2512.22772v1",
      "title": "GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks",
      "title_zh": "GRExplainerï¼šæ—¶åºå›¾ç¥ç»ç½‘ç»œçš„é€šç”¨è§£é‡Šæ–¹æ³•",
      "authors": [
        "Xuyan Li",
        "Jie Wang",
        "Zheng Yan"
      ],
      "abstract": "Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GRExplainerï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ—¶åºå›¾ç¥ç»ç½‘ç»œ(TGNNs)çš„é€šç”¨ã€é«˜æ•ˆä¸”ç”¨æˆ·å‹å¥½çš„è§£é‡Šæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•é€šç”¨æ€§å—é™ã€è®¡ç®—æˆæœ¬é«˜ä»¥åŠå¯¹å…ˆéªŒçŸ¥è¯†ä¾èµ–æ€§å¼ºç­‰é—®é¢˜ã€‚GRExplaineré€šè¿‡æå–èŠ‚ç‚¹åºåˆ—ä½œä¸ºç»Ÿä¸€ç‰¹å¾è¡¨ç¤ºï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶é€‚ç”¨äºåŸºäºå¿«ç…§(Snapshot-based)å’ŒåŸºäºäº‹ä»¶(Event-based)çš„ä¸¤ç§ä¸»æµTGNNæ¶æ„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢(BFS)å’Œæ—¶åºä¿¡æ¯æ„å»ºè¾“å…¥åºåˆ—ï¼Œæœ‰æ•ˆå‡å°‘äº†å†—ä½™è®¡ç®—å¹¶æå‡äº†æ¨æ–­æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªåŸºäºå¾ªç¯ç¥ç»ç½‘ç»œ(RNNs)çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–ä¸”æŒç»­çš„è§£é‡Šç”Ÿæˆï¼Œæ˜¾è‘—å¢å¼ºäº†ç”¨æˆ·å‹å¥½åº¦ã€‚åœ¨å…­ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGRExplaineråœ¨é€šç”¨æ€§ã€æ‰§è¡Œæ•ˆç‡å’Œè§£é‡Šè´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰çš„åŸºå‡†æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22772v1",
      "published_date": "2025-12-28 04:24:59 UTC",
      "updated_date": "2025-12-28 04:24:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:24:05.028100+00:00"
    },
    {
      "arxiv_id": "2512.22771v1",
      "title": "Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting",
      "title_zh": "é¢å‘è¯­ä¹‰ä¸åŠ¨æ€ 3D é«˜æ–¯æ³¼æº…çš„æœ€ä½³ä¸‹è§†ç‚¹é€‰æ‹©",
      "authors": [
        "Yiqian Li",
        "Wen Jiang",
        "Kostas Daniilidis"
      ],
      "abstract": "Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­ä¹‰å’ŒåŠ¨æ€ 3D Gaussian Splatting æå‡ºäº†ä¸€ç§åŸºäºåŸåˆ™çš„æœ€ä½³è§†è§’é€‰æ‹©ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½ä½“åœ¨å¤„ç†è¯­ä¹‰ä¸åŠ¨æ€ä»»åŠ¡æ—¶é¢ä¸´çš„å¤§é‡æ•°æ®å†—ä½™é—®é¢˜ã€‚ç ”ç©¶è€…å°†è§†è§’é€‰æ‹©å»ºæ¨¡ä¸ºä¸»åŠ¨å­¦ä¹ (Active Learning)é—®é¢˜ï¼Œæ ¸å¿ƒåœ¨äºåˆ©ç”¨è´¹èˆå°”ä¿¡æ¯(Fisher Information)æ¥é‡åŒ–å€™é€‰è§†è§’å¯¹äºè¯­ä¹‰é«˜æ–¯å‚æ•°(Semantic Gaussian parameters)å’Œå˜å½¢ç½‘ç»œ(Deformation networks)çš„ä¿¡æ¯å¢é‡ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŒæ—¶å¤„ç†è¯­ä¹‰æ¨ç†å’ŒåŠ¨æ€åœºæ™¯å»ºæ¨¡ï¼Œä¸ºä¼ ç»Ÿçš„éšæœºé€‰æ‹©æˆ–å¯å‘å¼ç­–ç•¥æä¾›äº†ä¸€ç§ç†è®ºæ”¯æ’‘çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨å¤§å‹é™æ€å›¾åƒå’ŒåŠ¨æ€è§†é¢‘æ•°æ®é›†ä¸Šçš„å¤šç›¸æœºå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¸²æŸ“è´¨é‡å’Œè¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¸€è‡´ä¼˜äºéšæœºé€‰æ‹©å’ŒåŸºäºä¸ç¡®å®šæ€§çš„(Uncertainty-based)å¯å‘å¼åŸºçº¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22771v1",
      "published_date": "2025-12-28 04:19:25 UTC",
      "updated_date": "2025-12-28 04:19:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:24:10.357668+00:00"
    },
    {
      "arxiv_id": "2512.22768v1",
      "title": "Understanding the Mechanisms of Fast Hyperparameter Transfer",
      "title_zh": "æ¢ç©¶å¿«é€Ÿè¶…å‚æ•°è¿ç§»çš„å†…åœ¨æœºåˆ¶",
      "authors": [
        "Nikhil Ghosh",
        "Denny Wu",
        "Alberto Bietti"
      ],
      "abstract": "The growing scale of deep learning models has rendered standard hyperparameter (HP) optimization prohibitively expensive. A promising solution is the use of scale-aware hyperparameters, which can enable direct transfer of optimal HPs from small-scale grid searches to large models with minimal performance loss. To understand the principles governing such transfer strategy, we develop a general conceptual framework for reasoning about HP transfer across scale, characterizing transfer as fast when the suboptimality it induces vanishes asymptotically faster than the finite-scale performance gap. We show formally that fast transfer is equivalent to useful transfer for compute-optimal grid search, meaning that transfer is asymptotically more compute-efficient than direct tuning. While empirical work has found that the Maximal Update Parameterization ($Î¼$P) exhibits fast transfer when scaling model width, the mechanisms remain poorly understood. We show that this property depends critically on problem structure by presenting synthetic settings where transfer either offers provable computational advantage or fails to outperform direct tuning even under $Î¼$P. To explain the fast transfer observed in practice, we conjecture that decomposing the optimization trajectory reveals two contributions to loss reduction: (1) a width-stable component that determines the optimal HPs, and (2) a width-sensitive component that improves with width but weakly perturbs the HP optimum. We present empirical evidence for this hypothesis across various settings, including large language model pretraining.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹è¶…å‚æ•°ï¼ˆHyperparameter, HPï¼‰ä¼˜åŒ–æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç†è§£è·¨å°ºåº¦è¶…å‚æ•°è½¬ç§»æœºåˆ¶çš„é€šç”¨æ¦‚å¿µæ¡†æ¶ã€‚ç ”ç©¶å®šä¹‰äº†â€œå¿«é€Ÿè½¬ç§»â€ï¼ˆFast Hyperparameter Transferï¼‰æ¦‚å¿µï¼Œå¹¶è¯æ˜å…¶åœ¨è®¡ç®—æœ€ä¼˜çš„ç½‘æ ¼æœç´¢ä¸­æ¯”ç›´æ¥åœ¨å¤§æ¨¡å‹ä¸Šè°ƒä¼˜å…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡å¯¹æå¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆMaximal Update Parameterization, $\\mu$Pï¼‰çš„æ·±å…¥åˆ†æï¼Œä½œè€…å‘ç°å¿«é€Ÿè½¬ç§»çš„æˆåŠŸé«˜åº¦ä¾èµ–äºé—®é¢˜çš„ç»“æ„ï¼Œè€Œéä»…ä»…å–å†³äºå‚æ•°åŒ–æ–¹æ¡ˆã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€é¡¹æ ¸å¿ƒå‡è®¾ï¼Œå³ä¼˜åŒ–è½¨è¿¹å¯åˆ†è§£ä¸ºå†³å®šæœ€ä¼˜è¶…å‚æ•°çš„å®½åº¦ç¨³å®šï¼ˆwidth-stableï¼‰ç»„ä»¶å’Œéšè§„æ¨¡å¢é•¿ä½†å¯¹æœ€ä¼˜å€¼å¹²æ‰°è¾ƒå¼±çš„å®½åº¦æ•æ„Ÿï¼ˆwidth-sensitiveï¼‰ç»„ä»¶ã€‚é€šè¿‡åœ¨åŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒåœ¨å†…çš„å¤šç§å®éªŒè®¾ç½®ä¸‹è¿›è¡ŒéªŒè¯ï¼Œè¯¥ç ”ç©¶ä¸ºé«˜æ•ˆçš„è·¨å°ºåº¦è¶…å‚æ•°è½¬ç§»æä¾›äº†ç†è®ºæ”¯æ’‘å’Œå®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "43 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.22768v1",
      "published_date": "2025-12-28 04:13:00 UTC",
      "updated_date": "2025-12-28 04:13:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:24:10.519416+00:00"
    },
    {
      "arxiv_id": "2512.22757v1",
      "title": "Active Constraint Learning in High Dimensions from Demonstrations",
      "title_zh": "åŸºäºæ¼”ç¤ºçš„é«˜ç»´ä¸»åŠ¨çº¦æŸå­¦ä¹ ",
      "authors": [
        "Zheng Qiu",
        "Chih-Yuan Chiu",
        "Glen Chou"
      ],
      "abstract": "We present an iterative active constraint learning (ACL) algorithm, within the learning from demonstrations (LfD) paradigm, which intelligently solicits informative demonstration trajectories for inferring an unknown constraint in the demonstrator's environment. Our approach iteratively trains a Gaussian process (GP) on the available demonstration dataset to represent the unknown constraints, uses the resulting GP posterior to query start/goal states, and generates informative demonstrations which are added to the dataset. Across simulation and hardware experiments using high-dimensional nonlinear dynamics and unknown nonlinear constraints, our method outperforms a baseline, random-sampling based method at accurately performing constraint inference from an iteratively generated set of sparse but informative demonstrations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåœ¨ç¤ºæ•™å­¦ä¹ (Learning from Demonstrations, LfD)æ¡†æ¶ä¸‹çš„è¿­ä»£ä¸»åŠ¨çº¦æŸå­¦ä¹ (Active Constraint Learning, ACL)ç®—æ³•ï¼Œæ—¨åœ¨æ™ºèƒ½åœ°å¾æ±‚å…·æœ‰ä¿¡æ¯é‡çš„æ¼”ç¤ºè½¨è¿¹ï¼Œä»¥æ¨æ–­æ¼”ç¤ºè€…ç¯å¢ƒä¸­çš„æœªçŸ¥çº¦æŸã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç°æœ‰æ¼”ç¤ºæ•°æ®é›†ä¸Šè¿­ä»£è®­ç»ƒé«˜æ–¯è¿‡ç¨‹(Gaussian Process, GP)æ¥è¡¨ç¤ºæœªçŸ¥çº¦æŸï¼Œå¹¶åˆ©ç”¨æ‰€å¾—çš„GPåéªŒåˆ†å¸ƒæ¥æŸ¥è¯¢èµ·å§‹å’Œç›®æ ‡çŠ¶æ€ã€‚éšåï¼Œç³»ç»Ÿç”Ÿæˆå…·æœ‰é«˜ä¿¡æ¯ä»·å€¼çš„æ¼”ç¤ºè½¨è¿¹å¹¶å°†å…¶åŠ å…¥æ•°æ®é›†ï¼Œä»è€Œä¸æ–­ä¼˜åŒ–æ¨¡å‹è¡¨ç°ã€‚å®éªŒæ¶µç›–äº†å…·æœ‰é«˜ç»´éçº¿æ€§åŠ¨åŠ›å­¦(High-dimensional Nonlinear Dynamics)å’ŒæœªçŸ¥éçº¿æ€§çº¦æŸçš„ä»¿çœŸåŠç¡¬ä»¶æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»è¿­ä»£ç”Ÿæˆçš„ç¨€ç–ä½†å…·æœ‰ä¿¡æ¯é‡çš„æ¼”ç¤ºä¸­å‡†ç¡®æ‰§è¡Œçº¦æŸæ¨æ–­(Constraint Inference)æ–¹é¢ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºéšæœºé‡‡æ ·(Random-sampling)çš„åŸºå‡†æ–¹æ³•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.RO",
      "comment": "Under review, 25 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22757v1",
      "published_date": "2025-12-28 03:06:05 UTC",
      "updated_date": "2025-12-28 03:06:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:24:27.552524+00:00"
    },
    {
      "arxiv_id": "2512.22742v1",
      "title": "Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning",
      "title_zh": "åŸºäºæç¤ºè¯å¢å¼ºä¸ LoRA å¾®è°ƒçš„ç¨³å¥å¤§è¯­è¨€æ¨¡å‹åˆ—ç±»å‹æ ‡æ³¨",
      "authors": [
        "Hanze Meng",
        "Jianhao Cao",
        "Rachel Pottinger"
      ],
      "abstract": "Column Type Annotation (CTA) is a fundamental step towards enabling schema alignment and semantic understanding of tabular data. Existing encoder-only language models achieve high accuracy when fine-tuned on labeled columns, but their applicability is limited to in-domain settings, as distribution shifts in tables or label spaces require costly re-training from scratch. Recent work has explored prompting generative large language models (LLMs) by framing CTA as a multiple-choice task, but these approaches face two key challenges: (1) model performance is highly sensitive to subtle changes in prompt wording and structure, and (2) annotation F1 scores remain modest. A natural extension is to fine-tune large language models. However, fully fine-tuning these models incurs prohibitive computational costs due to their scale, and the sensitivity to prompts is not eliminated. In this paper, we present a parameter-efficient framework for CTA that trains models over prompt-augmented data via Low-Rank Adaptation (LoRA). Our approach mitigates sensitivity to prompt variations while drastically reducing the number of necessary trainable parameters, achieving robust performance across datasets and templates. Experimental results on recent benchmarks demonstrate that models fine-tuned with our prompt augmentation strategy maintain stable performance across diverse prompt patterns during inference and yield higher weighted F1 scores than those fine-tuned on a single prompt template. These results highlight the effectiveness of parameter-efficient training and augmentation strategies in developing practical and adaptable CTA systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡¨æ ¼æ•°æ®çš„åˆ—ç±»å‹æ ‡æ³¨(Column Type Annotation, CTA)ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæç¤ºå¢å¼º(Prompt Augmentation)ä¸ä½ç§©è‡ªé€‚åº”(Low-Rank Adaptation, LoRA)çš„å‚æ•°é«˜æ•ˆå‹æ¡†æ¶ã€‚ç°æœ‰çš„ç¼–ç å™¨æ¨¡å‹åœ¨å¤„ç†è·¨åŸŸåˆ†å¸ƒåç§»æ—¶å—é™ï¼Œè€Œç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹(LLMs)åˆ™é¢ä¸´å¯¹æç¤ºè¯­é«˜åº¦æ•æ„ŸåŠ F1 åˆ†æ•°è¾ƒä½çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨æç¤ºå¢å¼ºçš„æ•°æ®ä¸Šåº”ç”¨ LoRA å¾®è°ƒï¼Œåœ¨æ˜¾è‘—å‡å°‘å¯è®­ç»ƒå‚æ•°é‡çš„åŒæ—¶ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹æç¤ºæ¨¡æ¿å˜åŒ–çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ç¨³å¥ï¼Œå…¶åŠ æƒ F1 åˆ†æ•°ä¼˜äºä»…åœ¨å•ä¸€æç¤ºæ¨¡æ¿ä¸Šå¾®è°ƒçš„æ¨¡å‹ã€‚è¿™ä¸€æˆæœè¯æ˜äº†å‚æ•°é«˜æ•ˆè®­ç»ƒä¸å¢å¼ºç­–ç•¥åœ¨å¼€å‘å®ç”¨ä¸”å…·é€‚åº”æ€§çš„ CTA ç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "13 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.22742v1",
      "published_date": "2025-12-28 02:04:17 UTC",
      "updated_date": "2025-12-28 02:04:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:24:35.070512+00:00"
    },
    {
      "arxiv_id": "2512.22738v1",
      "title": "Harnessing Large Language Models for Biomedical Named Entity Recognition",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«",
      "authors": [
        "Jian Chen",
        "Leilei Su",
        "Cong Sun"
      ],
      "abstract": "Background and Objective: Biomedical Named Entity Recognition (BioNER) is a foundational task in medical informatics, crucial for downstream applications like drug discovery and clinical trial matching. However, adapting general-domain Large Language Models (LLMs) to this task is often hampered by their lack of domain-specific knowledge and the performance degradation caused by low-quality training data. To address these challenges, we introduce BioSelectTune, a highly efficient, data-centric framework for fine-tuning LLMs that prioritizes data quality over quantity. Methods and Results: BioSelectTune reformulates BioNER as a structured JSON generation task and leverages our novel Hybrid Superfiltering strategy, a weak-to-strong data curation method that uses a homologous weak model to distill a compact, high-impact training dataset. Conclusions: Through extensive experiments, we demonstrate that BioSelectTune achieves state-of-the-art (SOTA) performance across multiple BioNER benchmarks. Notably, our model, trained on only 50% of the curated positive data, not only surpasses the fully-trained baseline but also outperforms powerful domain-specialized models like BioMedBERT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨ Large Language Models (LLMs) åœ¨ Biomedical Named Entity Recognition (BioNER) ä»»åŠ¡ä¸­ç”±äºç¼ºä¹é¢†åŸŸçŸ¥è¯†åŠè®­ç»ƒæ•°æ®è´¨é‡å·®è€Œå¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº†åä¸º BioSelectTune çš„é«˜æ•ˆæ•°æ®ä¸­å¿ƒåŒ–å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°† BioNER ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºç»“æ„åŒ–çš„ JSON ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº† Hybrid Superfiltering ç­–ç•¥ï¼Œé€šè¿‡å¼±åˆ°å¼ºçš„æ•°æ®æ¸…æ´—æ–¹æ³•æå–é«˜ä»·å€¼çš„è®­ç»ƒå­é›†ã€‚å®éªŒè¯æ˜ï¼ŒBioSelectTune åœ¨å¤šä¸ª BioNER åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº† State-of-the-Art (SOTA) çš„è¡¨ç°ã€‚ç‰¹åˆ«æ˜¯è¯¥æ¨¡å‹ä»…éœ€ä½¿ç”¨ 50% çš„ç²¾é€‰æ­£å‘æ•°æ®ï¼Œå°±èƒ½åœ¨æ€§èƒ½ä¸Šè¶…è¶Šå…¨é‡è®­ç»ƒçš„åŸºå‡†æ¨¡å‹ä»¥åŠ BioMedBERT ç­‰é¢†åŸŸä¸“ç”¨æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22738v1",
      "published_date": "2025-12-28 01:34:23 UTC",
      "updated_date": "2025-12-28 01:34:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:24:23.832932+00:00"
    },
    {
      "arxiv_id": "2512.22733v1",
      "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents",
      "title_zh": "FoldActï¼šé¢å‘é•¿æ—¶ç¨‹æœç´¢æ™ºèƒ½ä½“çš„é«˜æ•ˆä¸”ç¨³å®šçš„ä¸Šä¸‹æ–‡æŠ˜å ",
      "authors": [
        "Jiaqi Shao",
        "Yufeng Miao",
        "Wei Zhang",
        "Bing Luo"
      ],
      "abstract": "Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \\textbf{FoldAct}\\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\\times$ speedup.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FoldAct æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•¿ç¨‹å¼ºåŒ–å­¦ä¹ (Long-horizon reinforcement learning)ä¸­ç”±äºä¸Šä¸‹æ–‡æ— é™å¢é•¿å¯¼è‡´çš„æ‰©å±•æ€§æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰ä¸Šä¸‹æ–‡æŠ˜å (context folding)æ–¹æ³•ä¸­æ‘˜è¦æ“ä½œæ”¹å˜æœªæ¥è§‚æµ‹ç©ºé—´æ‰€å¼•èµ·çš„éå¹³ç¨³è§‚æµ‹åˆ†å¸ƒ(non-stationary observation distribution)ä»¥åŠè®­ç»ƒå´©æºƒç­‰é—®é¢˜ï¼ŒFoldAct å¼•å…¥äº†ä¸‰é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ†ç¦»æŸå¤±è®¡ç®—(separated loss computation)ä¸ºæ‘˜è¦å’ŒåŠ¨ä½œæ ‡è®°æä¾›ç‹¬ç«‹æ¢¯åº¦ä¿¡å·ï¼Œå¹¶åˆ©ç”¨å…¨ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æŸå¤±(full context consistency loss)æ¥å‡å°‘åˆ†å¸ƒåç§»ã€‚æ­¤å¤–ï¼Œå…¶é‡‡ç”¨çš„é€‰æ‹©æ€§ç‰‡æ®µè®­ç»ƒ(selective segment training)æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFoldAct åœ¨å®ç°é•¿ç¨‹æœç´¢æ™ºèƒ½ä½“ç¨³å®šè®­ç»ƒçš„åŒæ—¶ï¼ŒæˆåŠŸè§£å†³äº†éå¹³ç¨³è§‚æµ‹é—®é¢˜ï¼Œå¹¶å°†è®­ç»ƒæ•ˆç‡æå‡äº† 5.19 å€ï¼Œä¸ºé«˜æ•ˆå¤„ç†é•¿åºåˆ—ä»»åŠ¡æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22733v1",
      "published_date": "2025-12-28 00:24:01 UTC",
      "updated_date": "2025-12-28 00:24:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T20:24:21.743012+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 57,
  "processed_papers_count": 57,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T20:25:25.474868+00:00"
}
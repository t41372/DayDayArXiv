{
  "date": "2025-12-07",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-12-07 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„è®ºæ–‡è´¨é‡é¢‡é«˜ï¼Œå­¦æœ¯ç•Œå°†ç›®å…‰å¤§é‡èšç„¦åœ¨ **AI Agent çš„ç”Ÿæ€è§„èŒƒä¸è°ƒè¯•**ï¼ˆå¦‚ Agent ç‰ˆçš„ robots.txt å’Œå¤šæ™ºèƒ½ä½“è‡ªåŠ¨è°ƒè¯•ï¼‰ã€**Transformer æ¶æ„çš„æè‡´ä¼˜åŒ–**ï¼ˆFlash Multi-Head FFNï¼‰ä»¥åŠ **å¤šæ¨¡æ€åœ¨æœºå™¨äººä¸ç‰©ç†ä¸–ç•Œä¸­çš„åº”ç”¨**ã€‚æ­¤å¤–ï¼Œå…³äº Microsoft Copilot çš„å¤§è§„æ¨¡ç”¨æˆ·è¡Œä¸ºåˆ†æä»¥åŠ AI è®°å¿†ä¸å¯¹ç§°æ€§çš„ç¤¾ä¼šå­¦æ¢è®¨ä¹Ÿéå¸¸å¼•äººæ·±æ€ã€‚\n\n---\n\n### ğŸš€ å¿…è¯»ç²¾é€‰ï¼šæ¶æ„ä¼˜åŒ–ä¸ Agent ç”Ÿæ€\n\n**16. Flash Multi-Head FFN: Flash Multi-Head Feed-Forward Network**\n*   **å…³é”®è¯**ï¼šTransformer æ¶æ„ä¼˜åŒ–ã€FlashAttention æ€æƒ³ã€å‚æ•°æ•ˆç‡\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ç¯‡ä¸ä»…æ‹¼æ‰‹é€Ÿè¿˜æ‹¼æ·±åº¦çš„æ–‡ç« ã€‚ä½œè€…æå‡º **FlashMHF**ï¼Œå°† Multi-Head æœºåˆ¶å¼•å…¥ Transformer çš„ FFN å±‚ã€‚ä¸ºäº†è§£å†³æ˜¾å­˜æ¶ˆè€—å’Œç»´åº¦ä¸å¹³è¡¡é—®é¢˜ï¼Œä»–ä»¬è®¾è®¡äº†ç±»ä¼¼ FlashAttention çš„ I/O æ„ŸçŸ¥èåˆç®—å­ï¼ˆSRAM è®¡ç®—ï¼‰ã€‚\n*   **å‘ç°**ï¼šåœ¨ 128M åˆ° 1.3B å‚æ•°æ¨¡å‹ä¸ŠéªŒè¯ï¼Œæ¯” SwiGLU FFN å…·æœ‰æ›´å¥½çš„ Perplexity å’Œä¸‹æ¸¸ä»»åŠ¡ç²¾åº¦ï¼ŒåŒæ—¶ **æ¨ç†åŠ é€Ÿ 1.08 å€ï¼Œæ˜¾å­˜å³°å€¼é™ä½ 3-5 å€**ã€‚è¿™å¯èƒ½æˆä¸ºæœªæ¥å¤§æ¨¡å‹ FFN çš„æ–°æ ‡å‡†è®¾è®¡ã€‚\n\n**23. Permission Manifests for Web Agents**\n*   **å…³é”®è¯**ï¼šWeb Agentsã€åè®®è§„èŒƒã€robots.txt\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šéšç€ LLM Agent åœ¨ç½‘ç»œä¸Šâ€œæ¨ªè¡Œéœ¸é“â€ï¼Œä¼ ç»Ÿçš„ `robots.txt` å·²ä¸å¤Ÿç”¨ã€‚Lightweight Agent Standards Working Group æå‡ºäº† `agent-permissions.json`ã€‚\n*   **å‘ç°**ï¼šè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ JSON æ¸…å•ï¼Œå…è®¸ç½‘ç«™æ‰€æœ‰è€…æ˜ç¡®æŒ‡å®š Agent å¯ä»¥è¿›è¡Œå“ªäº›å¤æ‚äº¤äº’ï¼ˆä¸ä»…ä»…æ˜¯æŠ“å–ï¼‰ï¼Œè¿™æ˜¯ Agent èµ°å‘åˆè§„åŒ–å’Œå¤§è§„æ¨¡åº”ç”¨çš„åŸºç¡€è®¾æ–½ã€‚\n\n**55. DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems**\n*   **å…³é”®è¯**ï¼šå¤šæ™ºèƒ½ä½“è°ƒè¯•ã€è‡ªåŠ¨å½’å› ã€Agentic Systems\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systemsï¼‰å‡ºé”™æéš¾è°ƒè¯•ã€‚å¾®è½¯å›¢é˜Ÿæå‡ºäº† **DoVer**ï¼Œä¸€ç§åŸºäºâ€œå¹²é¢„â€çš„è°ƒè¯•æ¡†æ¶ã€‚å®ƒä¸ä¾èµ–å•ä¸€çš„æ—¥å¿—å½’å› ï¼Œè€Œæ˜¯é€šè¿‡ä¸»åŠ¨ä¿®æ”¹æ¶ˆæ¯æˆ–è®¡åˆ’æ¥éªŒè¯æ•…éšœå‡è®¾ã€‚\n*   **å‘ç°**ï¼šåœ¨ Magnetic-One æ¡†æ¶ä¸‹ï¼ŒDoVer èƒ½å°† 18-28% çš„å¤±è´¥æ¡ˆä¾‹è½¬åŒ–ä¸ºæˆåŠŸï¼Œå¹¶èƒ½éªŒè¯æˆ–æ¨ç¿» 30-60% çš„æ•…éšœå‡è®¾ã€‚è¿™æ˜¯ Agent å¼€å‘è€…çš„ç¦éŸ³ã€‚\n\n---\n\n### ğŸ¤– å¤šæ¨¡æ€ä¸æœºå™¨äºº (VLA)\n\n**19. VideoVLA: Video Generators Can Be Generalizable Robot Manipulators**\n*   **å…³é”®è¯**ï¼šè§†é¢‘ç”Ÿæˆã€æœºå™¨äººæ“ä½œã€VLA\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ¢ç´¢äº†å°†å¤§è§„æ¨¡ **è§†é¢‘ç”Ÿæˆæ¨¡å‹** è½¬åŒ–ä¸ºæœºå™¨äºº VLAï¼ˆVision-Language-Actionï¼‰æ“æ§è€…çš„æ½œåŠ›ã€‚VideoVLA ä¸ä»…é¢„æµ‹åŠ¨ä½œï¼Œè¿˜é¢„æµ‹æœªæ¥çš„è§†è§‰ç»“æœï¼ˆImagined futuresï¼‰ã€‚\n*   **å‘ç°**ï¼šé«˜è´¨é‡çš„è§†è§‰æƒ³è±¡ä¸ä»»åŠ¡æˆåŠŸç‡é«˜åº¦ç›¸å…³ã€‚è¯¥æ¨¡å‹åœ¨æ¨¡ä»¿å…¶ä»–å®ä½“æŠ€èƒ½å’Œå¤„ç†æ–°ç‰©ä½“æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†â€œè§†è§‰æƒ³è±¡â€åœ¨æœºå™¨äººæ“ä½œä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚\n\n**53. VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors**\n*   **å…³é”®è¯**ï¼šè§†è§‰æ¨ç†è¯„æµ‹ã€å¤šè½®å¯¹è¯ã€LVLM\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **VisChainBench**ï¼ŒåŒ…å« 1457 ä¸ªä»»åŠ¡å’Œ 2 ä¸‡å¼ å›¾ç‰‡ã€‚\n*   **å‘ç°**ï¼šç°æœ‰çš„ LVLM è¯„æµ‹è¿‡äºä¾èµ–è¯­è¨€æç¤ºæˆ–å•å›¾ç†è§£ã€‚è¿™ä¸ª Benchmark å¼ºè¿«æ¨¡å‹è¿›è¡Œå¤šæ­¥ã€å¤šå›¾çš„è§†è§‰æ¨ç†ï¼ˆå¦‚å·¥ç¨‹æ•…éšœæ’æŸ¥ï¼‰ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ‘†è„±è¯­è¨€å…ˆéªŒåçš„çœŸå®è§†è§‰æ¨ç†çŸ­æ¿ã€‚\n\n---\n\n### ğŸ’¡ æ·±åº¦æ´å¯Ÿï¼šäººæœºäº¤äº’ä¸ç¤¾ä¼šå½±å“\n\n**8. It's About Time: The Temporal and Modal Dynamics of Copilot Usage**\n*   **å…³é”®è¯**ï¼šç”¨æˆ·è¡Œä¸ºåˆ†æã€Copilotã€å¤§è§„æ¨¡æ•°æ®\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šåˆ†æäº† 2025 å¹´ 1 æœˆè‡³ 9 æœˆæœŸé—´ **3750 ä¸‡æ¬¡** Microsoft Copilot å¯¹è¯ã€‚\n*   **å‘ç°**ï¼šPC ç«¯å’Œç§»åŠ¨ç«¯çš„ä½¿ç”¨é€»è¾‘å®Œå…¨å‰²è£‚ï¼š**ç§»åŠ¨ç«¯**ä¸»æ‰“å¥åº·å’Œå»ºè®®ï¼ˆå…¨å¤©å€™ï¼‰ï¼Œ**PC ç«¯**åœ¨æ—©å…«æ™šäº”ä¸¥æ ¼éµå¾ªå·¥ä½œ/æŠ€æœ¯è¯é¢˜ã€‚æœ‰è¶£çš„æ˜¯ï¼Œç¼–ç¨‹æŸ¥è¯¢åœ¨å·¥ä½œæ—¥æ¿€å¢ï¼Œå“²å­¦é—®é¢˜åœ¨æ·±å¤œæ”€å‡ï¼Œè€Œæ‹çˆ±è¯é¢˜åœ¨æƒ…äººèŠ‚çˆ†å‘ã€‚AI å·²æ·±åº¦åµŒå…¥äººç±»ç”Ÿæ´»çš„çº¹ç†ã€‚\n\n**82. Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age**\n*   **å…³é”®è¯**ï¼šäººæœºå…³ç³»ã€è®°å¿†ä¸å¯¹ç§°ã€ç¤¾ä¼šä¼¦ç†\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸€ç¯‡æ·±åˆ»çš„ç†è®ºæ–‡ç« ã€‚æ¢è®¨äº†äººç±»ï¼ˆä¹ æƒ¯é—å¿˜ï¼‰ä¸ AIï¼ˆæ— é™è®°å¿†ï¼‰ä¹‹é—´çš„ **è®°å¿†æƒåŠ›ä¸å¯¹ç§° (MPA)**ã€‚\n*   **å‘ç°**ï¼šAI çš„å®Œç¾è®°å¿†å¯èƒ½ç ´åäººç±»å…³ç³»ä¸­åŸºäºâ€œç›¸äº’é—å¿˜â€çš„å®‰å…¨æ„Ÿå’ŒåŸè°…æœºåˆ¶ã€‚ä½œè€…æå‡ºäº†â€œè®¾è®¡é—å¿˜â€å’Œâ€œå¯¹ç§°è®¿é—®â€ç­‰åŸåˆ™ï¼Œå‘¼ååœ¨ AI æ—¶ä»£ä¿æŠ¤äººç±»â€œè¢«é—å¿˜â€çš„æƒåˆ©ã€‚\n\n**67. Academic journals' AI policies fail to curb the surge in AI-assisted academic writing**\n*   **å…³é”®è¯**ï¼šAI å†™ä½œã€å­¦æœ¯å‡ºç‰ˆã€æ”¿ç­–å¤±æ•ˆ\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šåˆ†æäº† 520 ä¸‡ç¯‡è®ºæ–‡ã€‚\n*   **å‘ç°**ï¼šå°½ç®¡ 70% çš„æœŸåˆŠå‡ºå°äº† AI æ”¿ç­–ï¼Œä½† AI å†™ä½œå·¥å…·çš„ä½¿ç”¨æ¿€å¢ä¸”ä¸å—æ§ï¼Œç‰¹åˆ«æ˜¯åœ¨éè‹±è¯­å›½å®¶å’Œç‰©ç†ç§‘å­¦é¢†åŸŸã€‚è‡ª 2023 å¹´ä»¥æ¥å‘è¡¨çš„ 7.5 ä¸‡ç¯‡è®ºæ–‡ä¸­ï¼Œä»… 0.1% æŠ«éœ²äº† AI ä½¿ç”¨æƒ…å†µã€‚ç»“è®ºï¼šç°æœ‰çš„ AI æ”¿ç­–åŸºæœ¬å¤±æ•ˆã€‚\n\n---\n\n### ğŸ—ï¸ ç®—æ³•æ¨¡å‹ä¸ç†è®º\n\n**7. Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length**\n*   **å…³é”®è¯**ï¼šLLM è¯„æµ‹ã€é¡¹ç›®ååº”ç†è®º (IRT)ã€CoT é•¿åº¦\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **LaRT** (Latency-Response Theory)ï¼Œç»“åˆå›ç­”å‡†ç¡®ç‡å’Œæ€ç»´é“¾ (CoT) é•¿åº¦æ¥è¯„ä¼° LLMã€‚\n*   **å‘ç°**ï¼šæ­ç¤ºäº†æ½œåœ¨èƒ½åŠ›ä¸æ¨ç†é€Ÿåº¦ï¼ˆæ½œä¼æœŸï¼‰ä¹‹é—´çš„å¼ºè´Ÿç›¸å…³æ€§â€”â€”**æ¨ç†èƒ½åŠ›è¶Šå¼ºï¼Œå¾€å¾€â€œæ€è€ƒâ€æ—¶é—´è¶Šé•¿**ã€‚LaRT æä¾›äº†æ¯”ä¼ ç»Ÿ IRT æ›´ç²¾å‡†çš„æ’åã€‚\n\n**73. TextMamba: Scene Text Detector with Mamba**\n**25. Adaptive Normalization Mamba (AdaMamba)**\n*   **å…³é”®è¯**ï¼šMambaã€çŠ¶æ€ç©ºé—´æ¨¡å‹ (SSM)ã€æ—¶é—´åºåˆ—\n*   **ç®€è¯„**ï¼šMamba æ¶æ„æŒç»­å‘åŠ›ã€‚**TextMamba (#73)** åˆ©ç”¨ Mamba çš„çº¿æ€§å¤æ‚åº¦å’Œé€‰æ‹©æœºåˆ¶æ”¹è¿›åœºæ™¯æ–‡æœ¬æ£€æµ‹ï¼›**AdaMamba (#25)** åˆ™åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ç»“åˆè‡ªé€‚åº”å½’ä¸€åŒ–è§£å†³éå¹³ç¨³æ€§é—®é¢˜ã€‚Mamba æ­£é€æ¸æˆä¸º Transformer åœ¨ç‰¹å®šé¢†åŸŸçš„å¼ºåŠ›æ›¿ä»£å“ã€‚\n\n**49. From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs**\n*   **å…³é”®è¯**ï¼šDiffusion LLMã€å—ç”Ÿæˆã€è®­ç»ƒæ•ˆç‡\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¯•å›¾æ‰“ç ´è‡ªå›å½’ (AR) ç”Ÿæˆçš„ç“¶é¢ˆã€‚æå‡ºäº†ä¸€ç§ä» AR åˆ° **Block-Diffusion** çš„å¹³æ»‘è¿‡æ¸¡è·¯å¾„ã€‚\n*   **å‘ç°**ï¼šNBDiff-7B æ¨¡å‹åœ¨ä¿ç•™ AR é¢„è®­ç»ƒçŸ¥è¯†çš„åŒæ—¶ï¼Œå®ç°äº†å¹¶è¡Œçš„å—ç”Ÿæˆï¼Œåœ¨æ•°å­¦å’Œä»£ç åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº† Diffusion LLM ä¸éœ€è¦ä»å¤´è®­ç»ƒã€‚\n\n---\n\n### ğŸ¥ åŒ»ç–—ä¸å®‰å…¨\n\n**1. DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation**\n*   **å…³é”®è¯**ï¼šåŒ»å­¦åˆ†å‰²ã€è½»é‡çº§ UNetã€SimAM\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡º DAUNetï¼Œç»“åˆå¯å˜å½¢å·ç§¯å’Œæ— å‚æ•°æ³¨æ„åŠ› (SimAM)ã€‚\n*   **å‘ç°**ï¼šåœ¨è¶…å£°å’Œ CT æ•°æ®é›†ä¸Šï¼Œå‚æ•°é‡æ›´å°‘ï¼Œä½† Dice åˆ†æ•°æ›´é«˜ï¼Œé€‚åˆèµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒã€‚\n\n**6. Transferring Clinical Knowledge into ECGs Representation**\n*   **å…³é”®è¯**ï¼šå¿ƒç”µå›¾ã€å¤šæ¨¡æ€çŸ¥è¯†è’¸é¦ã€å¯è§£é‡Šæ€§\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šåœ¨è®­ç»ƒé˜¶æ®µåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ï¼ˆå®éªŒå®¤ç»“æœã€ç”Ÿå‘½ä½“å¾ï¼‰ï¼Œä½†åœ¨æ¨ç†é˜¶æ®µä»…ä½¿ç”¨ ECG ä¿¡å·ã€‚é€šè¿‡è”åˆåµŒå…¥é¢„è®­ç»ƒï¼Œè®©å•ä¸€ ECG ç¼–ç å™¨â€œå­¦ä¼šâ€äº†ä¸´åºŠä¸Šä¸‹æ–‡ï¼Œæå‡äº†è¯Šæ–­å‡†ç¡®æ€§ã€‚\n\n**3. A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection**\n*   **å…³é”®è¯**ï¼šé›¶æ—¥æ”»å‡»ã€ä¸å¹³è¡¡æ•°æ®ã€XGBoost\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¯„ä¼°äº†ç›‘ç£å­¦ä¹ æ¨¡å‹åœ¨æç«¯ä¸å¹³è¡¡æ•°æ®ä¸‹æ£€æµ‹æœªçŸ¥ï¼ˆé›¶æ—¥ï¼‰æ”»å‡»çš„èƒ½åŠ›ã€‚ç»“è®ºæ˜¯ XGBoost åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¸Šä¼˜äº Random Forestï¼Œæ˜¯æœ€ä½³é€‰æ‹©ã€‚",
  "papers": [
    {
      "arxiv_id": "2512.07051v1",
      "title": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation",
      "title_zh": "DAUNetï¼šä¸€ç§èåˆå¯å˜å½¢å·ç§¯ä¸æ— å‚æ•°æ³¨æ„åŠ›çš„åŒ»å­¦å›¾åƒåˆ†å‰²è½»é‡çº§ UNet å˜ä½“",
      "authors": [
        "Adnan Munir",
        "Shujaat Khan"
      ],
      "abstract": "Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º DAUNet çš„è½»é‡åŒ– UNet å˜ä½“ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆ Deformable V2 Convolutions å’Œæ— å‚æ•°æ³¨æ„åŠ›æœºåˆ¶ SimAM æ¥æå‡åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ€§èƒ½ã€‚æ¨¡å‹åœ¨ bottleneck åŒºåŸŸåˆ©ç”¨åŠ¨æ€å¯å˜å½¢å·ç§¯æ ¸åº”å¯¹å¤æ‚çš„å‡ ä½•å˜åŒ–ï¼Œå¹¶ç»“åˆ SimAM æ¨¡å—åœ¨è§£ç å™¨ä¸è·³è·ƒè¿æ¥ï¼ˆskip pathwaysï¼‰ä¸­å®ç°æ˜¾è‘—æ€§ç‰¹å¾çš„ç»†åŒ–ã€‚åœ¨ FH-PS-AoP å’Œ FUMPE ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDAUNet åœ¨ Dice scoreã€HD95 å’Œ ASD ç­‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ¨¡å‹ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹å±•ç°äº†æé«˜çš„å‚æ•°æ•ˆç‡ä»¥åŠå¯¹ä½å¯¹æ¯”åº¦åŒºåŸŸçš„é²æ£’æ€§ï¼Œè¯æ˜å…¶éå¸¸é€‚åˆéƒ¨ç½²åœ¨å®æ—¶æ€§è¦æ±‚é«˜æˆ–è®¡ç®—èµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.07051v1",
      "published_date": "2025-12-07 23:57:00 UTC",
      "updated_date": "2025-12-07 23:57:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:25.409583+00:00"
    },
    {
      "arxiv_id": "2512.07034v1",
      "title": "Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues",
      "title_zh": "è¾¹ç•Œä¸åå°„çš„åŠ›é‡ï¼šåŸºäºé€æ˜çº¿ç´¢é‡‘å­—å¡”è§†è§‰ Transformer çš„è¯­ä¹‰é€æ˜ç‰©ä½“åˆ†å‰²",
      "authors": [
        "Tuan-Anh Vu",
        "Hai Nguyen-Truong",
        "Ziqiang Zheng",
        "Binh-Son Hua",
        "Qing Guo",
        "Ivor Tsang",
        "Sai-Kit Yeung"
      ],
      "abstract": "Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.",
      "tldr_zh": "ç”±äºç»ç’ƒç­‰é€æ˜ç‰©ä½“å…·æœ‰é€æ˜åº¦å’Œåå°„ç‰¹æ€§ï¼Œä¼ ç»Ÿçš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•åœ¨åŒºåˆ†è¿™äº›ç‰©ä½“ä¸ä¸é€æ˜ææ–™æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†åä¸º TransCues çš„åˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨é‡‘å­—å¡”è§†è§‰äº’æ„Ÿå™¨ (Pyramid Vision Transformer) çš„ç¼–è§£ç å™¨æ¶æ„ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆ Boundary Feature Enhancement å’Œ Reflection Feature Enhancement æ¨¡å—ï¼Œä»¥äº’åˆ©çš„æ–¹å¼åˆ©ç”¨è¾¹ç•Œå’Œåå°„è¿™ä¸¤ç§å…³é”®è§†è§‰çº¿ç´¢ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªæ¨¡å—çš„ç»“åˆæœ‰æ•ˆæå‡äº†åœ¨ç»ç’ƒã€é•œé¢åŠé€šç”¨åˆ†å‰²æ•°æ®é›†ä¸Šçš„æ•´ä½“æ€§èƒ½ã€‚åœ¨ Trans10K-v2ã€MSD å’Œ TROSD ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒTransCues å¤§å¹…è¶…è¶Šäº†ç°æœ‰çš„ SOTA æ–¹æ³•ï¼Œå…¶ mIoU æœ€é«˜æå‡è¾¾ 13.1%ã€‚è¯¥ç ”ç©¶å……åˆ†è¯æ˜äº†é€šè¿‡æ•æ‰é€æ˜åº¦ç‰¹å¾çº¿ç´¢æ¥å¢å¼ºé€æ˜ç‰©ä½“è¯­ä¹‰åˆ†å‰²ç²¾åº¦çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.07034v1",
      "published_date": "2025-12-07 22:52:53 UTC",
      "updated_date": "2025-12-07 22:52:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:30.611456+00:00"
    },
    {
      "arxiv_id": "2512.07030v1",
      "title": "A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data",
      "title_zh": "ç›‘ç£å¼æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é›¶æ—¥æ”»å‡»æ£€æµ‹ä¸­çš„å…¨é¢ç ”ç©¶ï¼šé’ˆå¯¹ä¸å¹³è¡¡æ•°æ®çš„æ€§èƒ½åˆ†æ",
      "authors": [
        "Zahra Lotfi",
        "Mostafa Lotfi"
      ],
      "abstract": "Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ç½‘ç»œå®‰å…¨ä¸­éš¾ä»¥æ£€æµ‹çš„ Zero-Day Attack é—®é¢˜ï¼Œæ·±å…¥æ¢è®¨äº†ç›‘ç£å­¦ä¹ æ¨¡å‹åœ¨é¢å¯¹æœªçŸ¥æ”»å‡»å’Œé«˜åº¦ä¸å¹³è¡¡æ•°æ®æ—¶çš„è¯†åˆ«æ•ˆèƒ½ã€‚ä¸ºäº†ä¼˜åŒ–é¢„æµ‹æ€§èƒ½ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ•´åˆäº† Grid searchã€ç»´åº¦å‹ç¼©å’Œ Oversampling æ–¹æ³•çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥å…‹æœä¼ ç»Ÿæ¨¡å‹åœ¨æœªè§æ ·æœ¬ä¸Šè¡¨ç°ä¸ä½³çš„ç¼ºé™·ã€‚å®éªŒè®¾å®šæ¨¡æ‹Ÿäº†çœŸå®ç¯å¢ƒï¼Œå³åˆ†ç±»å™¨åœ¨è®­ç»ƒé˜¶æ®µä¸æ¥è§¦ä»»ä½•æ”»å‡»ç‰¹å¾ï¼Œä»…åœ¨æµ‹è¯•é˜¶æ®µå°è¯•è¯†åˆ« Zero-Day Attackã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ Random Forest (RF) åœ¨å„ç§é‡‡æ ·æ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºæœ€é«˜çš„å‡†ç¡®æ€§ï¼Œä½†å…¶è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚å› æ­¤ï¼Œå…¼å…·é«˜ç²¾åº¦ä¸å¿«é€Ÿæ‰§è¡Œèƒ½åŠ›çš„ XG Boost (XGB) è¢«é€‰ä¸ºæ£€æµ‹æœªçŸ¥æ”»å‡»çš„æœ€ä½³æ¨¡å‹ã€‚è¯¥ç ”ç©¶ç»“æœè¯æ˜äº†è¿‡é‡‡æ ·æŠ€æœ¯åœ¨æå‡ ML æ¨¡å‹æ€§èƒ½æŒ‡æ ‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºé«˜æ•ˆçš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿæä¾›äº†ç†è®ºä¸å®è·µä¾æ®ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.07030v1",
      "published_date": "2025-12-07 22:42:37 UTC",
      "updated_date": "2025-12-07 22:42:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:25.528963+00:00"
    },
    {
      "arxiv_id": "2512.13713v1",
      "title": "LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms",
      "title_zh": "LoopBenchï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç¾¤æ¢ç´¢æ¶Œç°çš„å¯¹ç§°æ€§ç ´ç¼ºç­–ç•¥",
      "authors": [
        "Ali Parsaee",
        "Yashar Talebirad",
        "Csongor SzepesvÃ¡ri",
        "Vishwajeet Ohal",
        "Eden Redman"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \\textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† LoopBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ†å¸ƒå¼å¯¹ç§°æ€§ç ´ç¼ºï¼ˆsymmetry breakingï¼‰å’Œå…ƒè®¤çŸ¥æ€ç»´ä¸­æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†çš„æ ¸å¿ƒä»»åŠ¡æ˜¯åœ¨é¢œè‰²å—é™çš„æƒ…å†µä¸‹ä¸ºå¥‡ç¯å›¾ï¼ˆodd cycle graphsï¼Œå¦‚ $C_3, C_5, C_{11}$ï¼‰ç€è‰²ï¼Œè¿™ç±»åœºæ™¯é€šå¸¸ä¼šå¯¼è‡´ç¡®å®šæ€§ä¸”éé€šä¿¡çš„æ™ºèƒ½ä½“é™·å…¥æ— é™å¾ªç¯ã€‚ä¸ºäº†è§£å†³åè°ƒéš¾é¢˜ï¼Œç ”ç©¶å®ç°äº†ä¸€ç§ç­–ç•¥ä¼ é€’æœºåˆ¶ï¼ˆstrategy passing mechanismï¼‰ä½œä¸ºä¸€ç§ä¸€è‡´æ€§è®°å¿†ï¼ˆconsistent memoryï¼‰å½¢å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ ‡å‡† LLMs å’Œç»å…¸å¯å‘å¼ç®—æ³•éš¾ä»¥åº”å¯¹æ­¤ç±»æŒ‘æˆ˜ï¼Œä½†å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼ˆå¦‚ O3ï¼‰èƒ½å¤Ÿè‡ªä¸»è®¾è®¡å‡ºé€ƒç¦»æ­»é”ï¼ˆdeadlocksï¼‰çš„ç­–ç•¥ã€‚LoopBench ä¸ºç ”ç©¶åŸºäºè¯­è¨€æ¨ç†çš„æ¶Œç°å¼åˆ†å¸ƒå¼ç®—æ³•ï¼ˆemergent distributed algorithmsï¼‰æä¾›äº†é‡è¦å¹³å°ï¼Œä¸ºè¿›ä¸€æ­¥æ¢ç´¢é›†ä½“æ™ºèƒ½ï¼ˆcollective intelligenceï¼‰å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 3 figures, submitted to ANTS 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.13713v1",
      "published_date": "2025-12-07 22:26:40 UTC",
      "updated_date": "2025-12-07 22:26:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:29.488343+00:00"
    },
    {
      "arxiv_id": "2512.07022v1",
      "title": "Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization",
      "title_zh": "é‡æ„ã€æ£€ç´¢ã€å®šä½ï¼šä»“åº“çº§ç¼ºé™·å®šä½æ™ºèƒ½ä½“",
      "authors": [
        "Genevieve Caumartin",
        "Glaucia Melo"
      ],
      "abstract": "Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡è½¯ä»¶ä»“åº“ä¸­ç¼ºé™·å®šä½(Bug Localization)å­˜åœ¨çš„å™ªå£°å¹²æ‰°å’Œæ£€ç´¢å‡†ç¡®ç‡ä½çš„é—®é¢˜ï¼Œæ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ™ºèƒ½ä½“å¦‚ä½•é€šè¿‡è½»é‡åŒ–æ‰‹æ®µä¼˜åŒ–æ–‡ä»¶çº§å®šä½ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªâ€œé‡æ„ã€æ£€ç´¢ã€å®šä½â€çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¼€æºä¸”æœªç»å¾®è°ƒçš„ LLMs ä»ç¼ºé™·æŠ¥å‘Šä¸­æå–æ ‡è¯†ç¬¦å’Œä»£ç ç‰‡æ®µï¼Œåœ¨æ£€ç´¢å‰å®ŒæˆæŸ¥è¯¢é‡æ„(Query Reformulation)ã€‚è¯¥æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªåŠ¨åŒ–ç¼–æ’ BM25 æ£€ç´¢å·¥ä½œæµï¼Œä»è€Œåœ¨å¤§è§„æ¨¡ä»“åº“ä¸­å®ç°é«˜æ•ˆçš„è‡ªåŠ¨åŒ–å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¦–ä¸ªæ–‡ä»¶æ£€ç´¢çš„æ’åæ€§èƒ½ä¸Šæ¯” BM25 åŸºå‡†æå‡äº† 35%ï¼Œä¸”åœ¨æ–‡ä»¶æ£€ç´¢æ€§èƒ½ä¸Šæ¯” SWE-agent é«˜å‡º 22%ã€‚è¿™ä¸€è¿›å±•è¯æ˜äº†ç»“åˆ LLMs çš„æŸ¥è¯¢é¢„å¤„ç†æŠ€æœ¯åœ¨æå‡è½¯ä»¶ä»“åº“çº§ç¼ºé™·å®šä½å‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at BoatSE 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.07022v1",
      "published_date": "2025-12-07 22:25:11 UTC",
      "updated_date": "2025-12-07 22:25:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:29.132539+00:00"
    },
    {
      "arxiv_id": "2512.07021v1",
      "title": "Transferring Clinical Knowledge into ECGs Representation",
      "title_zh": "å°†ä¸´åºŠçŸ¥è¯†è¿ç§»è‡³å¿ƒç”µå›¾è¡¨å¾",
      "authors": [
        "Jose Geraldo Fernandes",
        "Luiz Facury de Souza",
        "Pedro Robles Dutenhefner",
        "Gisele L. Pappa",
        "Wagner Meira"
      ],
      "abstract": "Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \\emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¿ƒç”µå›¾ï¼ˆECGï¼‰åˆ†ç±»ä¸­å­˜åœ¨çš„â€œé»‘ç›’â€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†å¤šæ¨¡æ€ä¸´åºŠçŸ¥è¯†è¿ç§»è‡³å•æ¨¡æ€ ECG è¡¨å¾çš„ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ã€‚é€šè¿‡é‡‡ç”¨è‡ªç›‘ç£(self-supervised)çš„è”åˆåµŒå…¥(joint-embedding)é¢„è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä»…è¾“å…¥ ECG ä¿¡å·çš„æ¨ç†é˜¶æ®µåˆ©ç”¨å¯Œå«å®éªŒå®¤æ£€æŸ¥å’Œç”Ÿå‘½ä½“å¾ç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç¼–ç ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡è®­ç»ƒæ¨¡å‹é¢„æµ‹ç›¸å…³çš„å®éªŒå®¤æŒ‡æ ‡å¼‚å¸¸ï¼Œä¸ºè¯Šæ–­ç»“æœæä¾›äº†ç”Ÿç†å±‚é¢çš„é—´æ¥è§£é‡Šï¼Œæœ‰æ•ˆå¢å¼ºäº†ä¸´åºŠå¯ä¿¡åº¦ã€‚åœ¨ MIMIC-IV-ECG æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„åˆ†ç±»æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä»…ä¿¡å·æ¨¡å‹ï¼Œå¹¶å¤§å¹…ç¼©å°äº†ä¸å…¨å¤šæ¨¡æ€æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚è¿™é¡¹å·¥ä½œä¸º AI åœ¨ä¸´åºŠå·¥ä½œæµä¸­çš„å®‰å…¨é›†æˆæä¾›äº†ä¸€ç§å…¼å…·å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§çš„å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.07021v1",
      "published_date": "2025-12-07 22:19:24 UTC",
      "updated_date": "2025-12-07 22:19:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:44.605943+00:00"
    },
    {
      "arxiv_id": "2512.07019v2",
      "title": "Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length",
      "title_zh": "å»¶è¿Ÿ-å“åº”ç†è®ºæ¨¡å‹ï¼šåŸºäºå“åº”å‡†ç¡®ç‡ä¸æ€ç»´é“¾é•¿åº¦çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Zhiyu Xu",
        "Jia Liu",
        "Yixin Wang",
        "Yuqi Gu"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to guide downstream applications and actionable future improvements. The Item Response Theory (IRT) has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose Latency-Response Theory (LaRT) to jointly model the response accuracy and CoT length by introducing the latent ability, latent speed, and a key correlation parameter between them. We derive an efficient estimation algorithm and establish rigorous identifiability results for the population parameters to ensure the statistical validity of estimation. Theoretical asymptotic analyses and simulation studies demonstrate LaRT's advantages over IRT in terms of higher estimation accuracy and shorter confidence intervals for latent traits. A key finding is that the asymptotic estimation precision of the latent ability under LaRT exceeds that of IRT whenever the latent ability and latent speed are correlated. We collect real responses from diverse LLMs on popular benchmark datasets. The application of LaRT reveals a strong negative correlation between the latent ability and latent speed in all benchmarks, with stronger correlation for more difficult benchmarks. This finding supports the intuition that higher reasoning ability correlates with slower speed and longer response latency. LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Latency-Response Theory (LaRT) æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆå“åº”å‡†ç¡®ç‡å’Œ Chain-of-Thought (CoT) é•¿åº¦æ¥è”åˆè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ€§èƒ½ã€‚é’ˆå¯¹ä¼ ç»Ÿçš„é¡¹ç›®ååº”ç†è®º (Item Response Theory, IRT) ä»…å…³æ³¨å‡†ç¡®ç‡çš„å±€é™æ€§ï¼ŒLaRT å¼•å…¥äº†æ½œåœ¨èƒ½åŠ› (latent ability)ã€æ½œåœ¨é€Ÿåº¦ (latent speed) åŠå…¶å…³è”å‚æ•°ï¼Œå¹¶æ¨å¯¼å‡ºäº†é«˜æ•ˆçš„ä¼°è®¡ç®—æ³•ä»¥ç¡®ä¿ç»Ÿè®¡æœ‰æ•ˆæ€§ã€‚ç†è®ºåˆ†æä¸ä»¿çœŸç ”ç©¶è¯æ˜ï¼ŒLaRT åœ¨ä¼°è®¡å‡†ç¡®æ€§å’Œç½®ä¿¡åŒºé—´ç¼©çŸ­æ–¹é¢å‡æ˜¾è‘—ä¼˜äº IRTï¼Œå°¤å…¶åœ¨èƒ½åŠ›ä¸é€Ÿåº¦ç›¸å…³æ—¶è¡¨ç°æ›´ä½³ã€‚åœ¨çœŸå®åŸºå‡†æµ‹è¯•ä¸­çš„åº”ç”¨æ­ç¤ºäº† LLMs çš„æ½œåœ¨èƒ½åŠ›ä¸æ½œåœ¨é€Ÿåº¦ä¹‹é—´å­˜åœ¨æ˜¾è‘—è´Ÿç›¸å…³ï¼ŒéªŒè¯äº†é«˜æ¨ç†èƒ½åŠ›å¾€å¾€å¯¹åº”æ›´é•¿å“åº”å»¶è¿Ÿçš„ç›´è§‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaRT åœ¨é¢„æµ‹èƒ½åŠ›ã€æ’åæœ‰æ•ˆæ€§å’Œè¯„ä¼°æ•ˆç‡ç­‰å¤šä¸ªå…³é”®ç»´åº¦ä¸Šå‡ä¼˜äº IRTï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„ç²¾ç»†åŒ–è¯„ä¼°æä¾›äº†æ›´ä¸ºç²¾å‡†çš„ç»Ÿè®¡æ¡†æ¶ã€‚",
      "categories": [
        "stat.ME",
        "cs.AI",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.07019v2",
      "published_date": "2025-12-07 22:06:51 UTC",
      "updated_date": "2025-12-11 02:45:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:57.526643+00:00"
    },
    {
      "arxiv_id": "2512.11879v1",
      "title": "It's About Time: The Temporal and Modal Dynamics of Copilot Usage",
      "title_zh": "å…³ä¹æ—¶é—´ï¼šCopilot ä½¿ç”¨çš„æ—¶é—´ä¸æ¨¡æ€åŠ¨æ€",
      "authors": [
        "Beatriz Costa-Gomes",
        "Sophia Chen",
        "Connie Hsueh",
        "Deborah Morgan",
        "Philipp Schoenegger",
        "Yash Shah",
        "Sam Way",
        "Yuki Zhu",
        "TimothÃ© Adeline",
        "Michael Bhaskar",
        "Mustafa Suleyman",
        "Seth Spielman"
      ],
      "abstract": "We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with \"Work and Career\" overtaking \"Technology\" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†2025å¹´1æœˆè‡³9æœˆæœŸé—´ä¸Microsoft Copilotè¿›è¡Œçš„3750ä¸‡æ¬¡å»æ ‡è¯†åŒ–å¯¹è¯ï¼Œæ·±å…¥æ¢è®¨äº†äººå·¥æ™ºèƒ½ä½¿ç”¨çš„æ—¶ç©ºä¸æ¨¡å¼åŠ¨æ€(Temporal and Modal Dynamics)ã€‚ç ”ç©¶å‘ç°ï¼ŒAIçš„ä½¿ç”¨æ¨¡å¼é«˜åº¦ä¾èµ–äºç”¨æˆ·æ‰€å¤„çš„ä¸Šä¸‹æ–‡ç¯å¢ƒå’Œè®¾å¤‡ç±»å‹(Device Type)ã€‚åœ¨ç§»åŠ¨ç«¯(Mobile)ï¼Œå¥åº·(Health)æ˜¯è´¯ç©¿å…¨å¤©å€™çš„ä¸»å¯¼è¯é¢˜ï¼Œç”¨æˆ·å€¾å‘äºå¯»æ±‚ç›¸å…³å»ºè®®ä¸ä¿¡æ¯ï¼›è€Œåœ¨æ¡Œé¢ç«¯(Desktop)ï¼Œå·¥ä½œä¸èŒä¸š(Work and Career)åŠæŠ€æœ¯(Technology)è¯é¢˜åœ¨å·¥ä½œæ—¶é—´å†…å æ®ç»Ÿæ²»åœ°ä½ã€‚æ­¤å¤–ï¼ŒAIä½¿ç”¨å‘ˆç°å‡ºæ˜¾è‘—çš„æ—¶é—´èŠ‚å¥ï¼Œä¾‹å¦‚ç¼–ç¨‹æŸ¥è¯¢åœ¨å·¥ä½œæ—¥æ¿€å¢ï¼Œæ¸¸æˆè¯é¢˜åœ¨å‘¨æœ«ä¸Šå‡ï¼Œå“²å­¦æ¢è®¨åˆ™åœ¨æ·±å¤œæ—¶æ®µæ˜¾è‘—å¢åŠ ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç”¨æˆ·å·²è¿…é€Ÿå°†AIæ·±åº¦èå…¥ç”Ÿæ´»å…¨è²Œï¼Œåœ¨åŠå…¬æ¡Œå‰å°†å…¶ä½œä¸ºå·¥ä½œè¾…åŠ©å·¥å…·ï¼Œåœ¨æ‰‹æœºä¸Šåˆ™å°†å…¶è§†ä¸ºéšèº«ä¼´ä¾£ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "12 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.11879v1",
      "published_date": "2025-12-07 21:45:20 UTC",
      "updated_date": "2025-12-07 21:45:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:45.462149+00:00"
    },
    {
      "arxiv_id": "2512.07015v2",
      "title": "FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations",
      "title_zh": "FVA-RAGï¼šæ—¨åœ¨ç¼“è§£è¿åˆæ€§å¹»è§‰çš„è¯ä¼ª-éªŒè¯å¯¹é½æ–¹æ³•",
      "authors": [
        "Mayank Ravishankara"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) reduces hallucinations by grounding answers in retrieved evidence, yet standard retrievers often exhibit retrieval sycophancy: they preferentially surface evidence that supports a user's premise, even when the premise is false. We propose FVA-RAG (Falsification-Verification Alignment RAG), a pipeline that inverts the standard RAG workflow by treating the initial response as a draft hypothesis and explicitly retrieving anti-context to stress-test it. We evaluate on the full TruthfulQA-Generation benchmark (N=817) under a fully frozen protocol with 0 live web calls and identical retrieval budgets across methods. Using gpt-4o for generation and deterministic judging, FVA-RAG achieves 79.80-80.05% accuracy across two independently built frozen corpora , significantly outperforming prompted variants of Self-RAG (71.11-72.22%) and CRAG (71.36-73.93%) with p < 10^-6 according to McNemar's test. FVA-RAG triggers falsification on 24.5-29.3% of queries, demonstrating that targeted counter-evidence retrieval is decisive for mitigating premise-confirming hallucinations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FVA-RAGï¼ˆFalsification-Verification Alignment RAGï¼‰ï¼Œæ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­æ ‡å‡†æ£€ç´¢å™¨å› å€¾å‘äºæœç´¢æ”¯æŒç”¨æˆ·é”™è¯¯å‰æçš„è¯æ®è€Œäº§ç”Ÿçš„æ£€ç´¢è°„åªšï¼ˆretrieval sycophancyï¼‰å¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åè½¬ä¼ ç»Ÿçš„RAGå·¥ä½œæµç¨‹ï¼Œå°†åˆå§‹ç”Ÿæˆçš„å“åº”è§†ä¸ºè‰æ¡ˆå‡è®¾ï¼Œå¹¶ä¸“é—¨æ£€ç´¢åå‘èƒŒæ™¯ä¿¡æ¯ï¼ˆanti-contextï¼‰å¯¹å…¶è¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚å®éªŒåœ¨TruthfulQA-GenerationåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºFVA-RAGè¾¾åˆ°äº†79.80-80.05%çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºSelf-RAGå’ŒCRAGç­‰å¯¹æ¯”æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹æ€§çš„è¯ä¼ªï¼ˆfalsificationï¼‰æ£€ç´¢åœ¨24.5-29.3%çš„æŸ¥è¯¢ä¸­è¢«è§¦å‘ï¼Œè¯æ˜äº†å¼•å…¥å¯¹æŠ—æ€§è¯æ®å¯¹äºç¼“è§£ç¡®è®¤æ€§å¹»è§‰å…·æœ‰å†³å®šæ€§ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.07015v2",
      "published_date": "2025-12-07 21:28:42 UTC",
      "updated_date": "2025-12-25 20:24:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:48.553414+00:00"
    },
    {
      "arxiv_id": "2512.07009v1",
      "title": "Optimizing video analytics inference pipelines: a case study",
      "title_zh": "è§†é¢‘åˆ†ææ¨ç†æµæ°´çº¿ä¼˜åŒ–ï¼šä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Saeid Ghafouri",
        "Yuming Ding",
        "Katerine Diaz Chito",
        "JesÃºs Martinez del RincÃ³n",
        "Niamh O'Connell",
        "Hans Vandierendonck"
      ],
      "abstract": "Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»¥å®¶ç¦½ç¦åˆ©ç›‘æ§ç³»ç»Ÿä¸ºæ¡ˆä¾‹ï¼Œæ¢è®¨äº†å¦‚ä½•ä¼˜åŒ–è§†é¢‘åˆ†ææ¨ç†æµæ°´çº¿ï¼Œä»¥æ»¡è¶³ç²¾å‡†ç•œç‰§ä¸šç›‘æ§ä¸­é«˜åˆ†è¾¨ç‡å’Œè¿‘å®æ—¶ç›‘æµ‹å¸¦æ¥çš„é«˜è®¡ç®—éœ€æ±‚ã€‚è®ºæ–‡é’ˆå¯¹æ£€æµ‹ã€è·Ÿè¸ªã€èšç±»å’Œè¡Œä¸ºåˆ†æç­‰æ¨¡å—è¿›è¡Œäº†ç³»ç»Ÿçº§æ”¹è¿›ï¼Œæå‡ºäº†ä¸€å¥—åŒ…æ‹¬å¤šçº§å¹¶è¡ŒåŒ–(multi-level parallelization)ã€GPUåŠ é€Ÿä»£ç æ›¿æ¢CPUä»£ç ã€çŸ¢é‡åŒ–èšç±»(vectorized clustering)ä»¥åŠå†…å­˜é«˜æ•ˆåå¤„ç†åœ¨å†…çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¿æŒæ¨¡å‹å‡†ç¡®ç‡ä¸å˜çš„å‰æä¸‹ï¼Œè¿™äº›ä¼˜åŒ–ç­–ç•¥åœ¨çœŸå®å†œåœºè§†é¢‘æ•°æ®ä¸Šå®ç°äº†é«˜è¾¾2å€çš„æ¨ç†åŠ é€Ÿã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºé«˜ååã€ä½å»¶è¿Ÿçš„è§†é¢‘åˆ†æç³»ç»Ÿæä¾›äº†å®è·µæŒ‡å¯¼ï¼Œæ˜¾è‘—é™ä½äº†å†œä¸šåŠæ™ºèƒ½æ„ŸçŸ¥åº”ç”¨ä¸­çš„åŸºç¡€è®¾æ–½æˆæœ¬ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted to the IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT 2025)",
      "pdf_url": "https://arxiv.org/pdf/2512.07009v1",
      "published_date": "2025-12-07 21:17:53 UTC",
      "updated_date": "2025-12-07 21:17:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:21:49.660692+00:00"
    },
    {
      "arxiv_id": "2512.07005v1",
      "title": "Multi-Accent Mandarin Dry-Vocal Singing Dataset: Benchmark for Singing Accent Recognition",
      "title_zh": "å¤šå£éŸ³æ™®é€šè¯æ­Œå”±å¹²å£°æ•°æ®é›†ï¼šæ­Œå”±å£éŸ³è¯†åˆ«åŸºå‡†",
      "authors": [
        "Zihao Wang",
        "Ruibin Yuan",
        "Ziqi Geng",
        "Hengjia Li",
        "Xingwei Qu",
        "Xinyi Li",
        "Songye Chen",
        "Haoying Fu",
        "Roger B. Dannenberg",
        "Kejun Zhang"
      ],
      "abstract": "Singing accent research is underexplored compared to speech accent studies, primarily due to the scarcity of suitable datasets. Existing singing datasets often suffer from detail loss, frequently resulting from the vocal-instrumental separation process. Additionally, they often lack regional accent annotations. To address this, we introduce the Multi-Accent Mandarin Dry-Vocal Singing Dataset (MADVSD). MADVSD comprises over 670 hours of dry vocal recordings from 4,206 native Mandarin speakers across nine distinct Chinese regions. In addition to each participant recording audio of three popular songs in their native accent, they also recorded phonetic exercises covering all Mandarin vowels and a full octave range. We validated MADVSD through benchmark experiments in singing accent recognition, demonstrating its utility for evaluating state-of-the-art speech models in singing contexts. Furthermore, we explored dialectal influences on singing accent and analyzed the role of vowels in accentual variations, leveraging MADVSD's unique phonetic exercises.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç›¸æ¯”è¯­éŸ³å£éŸ³ç ”ç©¶ï¼Œæ­Œå”±å£éŸ³(Singing Accent)ç ”ç©¶å› ç¼ºä¹åˆé€‚æ•°æ®é›†è€Œå‘å±•å—é™ï¼Œä¸”ç°æœ‰æ•°æ®å¤šå­˜åœ¨éŸ³è½¨åˆ†ç¦»å¯¼è‡´çš„ç»†èŠ‚æŸå¤±åŠå£éŸ³æ ‡æ³¨ç¼ºå¤±é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº†å¤šå£éŸ³æ™®é€šè¯å¹²å£°æ­Œå”±æ•°æ®é›†(Multi-Accent Mandarin Dry-Vocal Singing Dataset, MADVSD)ï¼ŒåŒ…å«æ¥è‡ªä¸­å›½ä¹ä¸ªä¸åŒåœ°åŒºçš„4,206åæ¯è¯­è€…çš„670å¤šå°æ—¶å¹²å£°å½•éŸ³ã€‚é™¤äº†æµè¡Œæ­Œæ›²ï¼Œè¯¥æ•°æ®é›†è¿˜æ¶µç›–äº†æ‰€æœ‰æ™®é€šè¯å…ƒéŸ³å’Œå…¨å…«åº¦éŸ³ç¨‹çš„è¯­éŸ³ç»ƒä¹ ï¼Œä¸ºå­¦æœ¯ç ”ç©¶æä¾›äº†ä¸°å¯Œä¸”é«˜è´¨çš„ç´ æã€‚é€šè¿‡æ­Œå”±å£éŸ³è¯†åˆ«(Singing Accent Recognition)åŸºå‡†å®éªŒï¼Œè¯¥ç ”ç©¶éªŒè¯äº†MADVSDåœ¨è¯„ä¼°æœ€å…ˆè¿›è¯­éŸ³æ¨¡å‹æ–¹é¢çš„å®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥æ•°æ®é›†ç‰¹æœ‰çš„è¯­éŸ³ç»ƒä¹ ï¼Œæ·±å…¥æ¢è®¨äº†æ–¹è¨€å¯¹æ­Œå”±çš„å½±å“ï¼Œå¹¶åˆ†æäº†å…ƒéŸ³åœ¨å£éŸ³å˜å¼‚ä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by ACMMM 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.07005v1",
      "published_date": "2025-12-07 21:14:26 UTC",
      "updated_date": "2025-12-07 21:14:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:10.510796+00:00"
    },
    {
      "arxiv_id": "2512.07000v2",
      "title": "Benchmarking Deep Neural Networks for Modern Recommendation Systems",
      "title_zh": "ç°ä»£æ¨èç³»ç»Ÿæ·±åº¦ç¥ç»ç½‘ç»œåŸºå‡†æµ‹è¯•",
      "authors": [
        "Abderaouf Bahi",
        "Inoussa Mouiche",
        "Ibtissem Gasmi"
      ],
      "abstract": "This paper presents a requirement-oriented benchmark of seven deep neural architectures, CNN, RNN, GNN, Autoencoder, Transformer, Neural Collaborative Filtering, and Siamese Networks, across three real-world datasets: Retail E-commerce, Amazon Products, and Netflix Prize. To ensure a fair and comprehensive comparison aligned with the evolving demands of modern recommendation systems, we adopt a Requirement-Oriented Benchmarking (ROB) framework that structures evaluation around predictive accuracy, recommendation diversity, relational awareness, temporal dynamics, and computational efficiency. Under a unified evaluation protocol, models are assessed using standard accuracy-oriented metrics alongside diversity and efficiency indicators. Experimental results show that different architectures exhibit complementary strengths across requirements, motivating the use of hybrid and ensemble designs. The findings provide practical guidance for selecting and combining neural architectures to better satisfy multi-objective recommendation system requirements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£æ¨èç³»ç»Ÿçš„æ¼”è¿›éœ€æ±‚ï¼Œæå‡ºäº†é¢å‘éœ€æ±‚è¯„ä¼°æ¡†æ¶ Requirement-Oriented Benchmarking (ROB)ï¼Œæ—¨åœ¨å¯¹ CNNã€RNNã€GNNã€Autoencoderã€Transformerã€Neural Collaborative Filtering å’Œ Siamese Networks è¿™ä¸ƒç§æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ã€‚å®éªŒåœ¨ Retail E-commerceã€Amazon Products å’Œ Netflix Prize ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå±•å¼€ï¼Œå›´ç»•é¢„æµ‹å‡†ç¡®æ€§ã€æ¨èå¤šæ ·æ€§ã€å…³ç³»æ„ŸçŸ¥ã€æ—¶é—´åŠ¨æ€åŠè®¡ç®—æ•ˆç‡äº”ä¸ªæ ¸å¿ƒç»´åº¦è¿›è¡Œå¯¹æ¯”ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒç¥ç»ç½‘ç»œæ¶æ„åœ¨å„é¡¹æŒ‡æ ‡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„äº’è¡¥ä¼˜åŠ¿ï¼Œå•ä¸€æ¨¡å‹éš¾ä»¥åœ¨æ‰€æœ‰ç»´åº¦ä¸Šè¾¾åˆ°æœ€ä¼˜ã€‚è¯¥è®ºæ–‡çš„å‘ç°ä¸ºå¼€å‘è€…æ ¹æ®ç‰¹å®šä¸šåŠ¡ç›®æ ‡é€‰æ‹©å’Œç»„åˆä¸åŒæ¶æ„æä¾›äº†å®ç”¨æŒ‡å—ï¼Œå¹¶å¼ºè°ƒäº†é€šè¿‡æ··åˆæ¶æ„ä¸é›†æˆè®¾è®¡æ¥ä¼˜åŒ–å¤šç›®æ ‡æ¨èç³»ç»Ÿæ€§èƒ½çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.07000v2",
      "published_date": "2025-12-07 21:06:24 UTC",
      "updated_date": "2026-01-17 12:17:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:11.660109+00:00"
    },
    {
      "arxiv_id": "2512.06999v1",
      "title": "Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model",
      "title_zh": "åŸºäºå¤šæ¨¡æ€å¤§åŸºåº§æ¨¡å‹çš„æ­Œå”±éŸ³è‰²æµè¡Œåº¦è¯„ä¼°",
      "authors": [
        "Zihao Wang",
        "Ruibin Yuan",
        "Ziqi Geng",
        "Hengjia Li",
        "Xingwei Qu",
        "Xinyi Li",
        "Songye Chen",
        "Haoying Fu",
        "Roger B. Dannenberg",
        "Kejun Zhang"
      ],
      "abstract": "Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿè‡ªåŠ¨æ­Œå”±è¯„ä¼°ç³»ç»Ÿè¿‡åº¦ä¾èµ–å‚è€ƒéŸ³è½¨ä¸”è¯„åˆ†ç»´åº¦å•ä¸€çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§ä»åˆ¤åˆ«å¼è¯„ä»·è½¬å‘æè¿°å¼è¯„ä»·çš„å‚è€ƒæ— å…³(reference-free)å¤šç»´åº¦è¯„ä¼°ç”Ÿæ€ç³»ç»Ÿã€‚é¦–å…ˆï¼Œç ”ç©¶è€…æ„å»ºäº†å¤§è§„æ¨¡æ•°æ®é›† Sing-MDï¼Œç”±ä¸“å®¶ä»å‘¼å¸æ§åˆ¶(breath control)ã€éŸ³è‰²è´¨é‡(timbre quality)ã€æƒ…æ„Ÿè¡¨è¾¾(emotional expression)å’Œå‘å£°æŠ€æœ¯(vocal technique)å››ä¸ªç»´åº¦è¿›è¡Œè¯¦ç»†æ ‡æ³¨ã€‚é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨å¤„ç†é•¿æ­Œæ›²æ—¶çš„å†…å­˜ç“¶é¢ˆï¼Œç ”ç©¶æå‡ºäº†é«˜æ•ˆçš„æ··åˆæ¶æ„ VocalVerseï¼Œåˆ©ç”¨è½»é‡çº§å£°å­¦ç¼–ç å™¨æœ‰æ•ˆå»ºæ¨¡å…¨å±€è¡¨ç°ç‰¹å¾ä¸é•¿ç¨‹ä¾èµ–ã€‚æœ€åï¼Œä¸ºå¼¥è¡¥ä¼ ç»Ÿè‡ªåŠ¨åŒ–æŒ‡æ ‡çš„ä¸è¶³ï¼Œè¯¥ç ”ç©¶å»ºç«‹äº† H-TPR (Human-in-the-loop Tiered Perceptual Ranking) åŸºå‡†ï¼Œé€šè¿‡è¯„ä¼°æ¨¡å‹ç”Ÿæˆæ„ŸçŸ¥æœ‰æ•ˆæ’åºçš„èƒ½åŠ›æ¥è¡¡é‡å…¶æ€§èƒ½ï¼Œè€Œéå•çº¯é¢„æµ‹å¸¦æœ‰å™ªå£°çš„åŸå§‹åˆ†æ•°ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ACMMM 2025 oral",
      "pdf_url": "https://arxiv.org/pdf/2512.06999v1",
      "published_date": "2025-12-07 21:06:16 UTC",
      "updated_date": "2025-12-07 21:06:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:31.303323+00:00"
    },
    {
      "arxiv_id": "2512.06991v1",
      "title": "Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models",
      "title_zh": "Prompting-in-a-Seriesï¼šåŸºäºä»…è§£ç å™¨æ¨¡å‹çš„å¿ƒç†å­¦å¯å‘å†…å®¹ä¸åµŒå…¥å¼äººæ ¼è¯†åˆ«",
      "authors": [
        "Jing Jie Tan",
        "Ban-Hoe Kwan",
        "Danny Wee-Kiat Ng",
        "Yan-Chai Hum",
        "Anissa Mokraoui",
        "Shih-Yu Lo"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel \"Prompting-in-a-Series\" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \\textit{gpt4o} from OpenAI and \\textit{gemini} from Google, along with open-source models like \\textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPICEPRçš„æ–°é¢–â€œåºåˆ—æç¤ºâ€(Prompting-in-a-Series)ç®—æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨ä»…è§£ç å™¨æ¨¡å‹(Decoder-Only Models)æå‡äººæ ¼è¯†åˆ«(Personality Recognition)çš„æ€§èƒ½ã€‚è¯¥ç®—æ³•åŒ…å«å†…å®¹(Contents)å’ŒåµŒå…¥(Embeddings)ä¸¤ä¸ªæ ¸å¿ƒæµæ°´çº¿ï¼Œé€šè¿‡æ¨¡å—åŒ–çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åˆ†åˆ«ä½œä¸ºäººæ ¼ç‰¹å¾æå–å™¨å’Œå¯Œäººæ ¼å†…å®¹ç”Ÿæˆå™¨ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨GPT-4oã€Geminiä»¥åŠMistralç­‰å¤šç§å¼€æºå’Œé—­æºæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒéªŒè¯äº†å¿ƒç†å­¦å¯å‘å†…å®¹ä¸åµŒå…¥åœ¨äººæ ¼è¯†åˆ«ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPICEPRç®—æ³•åœ¨æ€§èƒ½ä¸Šå®ç°äº†5-15%çš„æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°äº†ç›®å‰çš„æœ€å…ˆè¿›æ°´å¹³(SOTA)ã€‚è¯¥é¡¹å·¥ä½œä¸ºç»“åˆå¿ƒç†å­¦ç†è®ºä¸å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç²¾å‡†äººæ ¼å»ºæ¨¡æä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.06991v1",
      "published_date": "2025-12-07 20:52:00 UTC",
      "updated_date": "2025-12-07 20:52:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:45.378232+00:00"
    },
    {
      "arxiv_id": "2512.06990v1",
      "title": "Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients",
      "title_zh": "åˆ©ç”¨åŸºäºç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç¡®å®šå¤šå½¢æ€§èƒ¶è´¨æ¯ç»†èƒç˜¤æ‚£è€…çš„æœ€ä½³åˆ‡é™¤ä½ç½®",
      "authors": [
        "Krishna Arun",
        "Moinak Bhattachrya",
        "Paras Goel"
      ],
      "abstract": "Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èƒ¶è´¨æ¯ç»†èƒç˜¤(Glioblastoma Multiforme, GBM)å¼€å‘äº†ä¸€ç§é›†æˆå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (Multi-Agent Reinforcement Learning)ä¸ç¼–ç å™¨-è§£ç å™¨æ¶æ„(Encoder-Decoder Architecture)çš„ç«¯åˆ°ç«¯AIç³»ç»Ÿï¼Œæ—¨åœ¨è¾…åŠ©åŒ»ç”Ÿå®Œæˆä»è¯Šæ–­åˆ°æ²»ç–—è§„åˆ’çš„å…¨æµç¨‹ã€‚è¯Šæ–­é˜¶æ®µåˆ©ç”¨ç”±å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å’Œæ”¯æŒå‘é‡æœº(SVM)ç»„æˆçš„é¡ºåºå†³ç­–æ¡†æ¶ï¼Œè€Œæ²»ç–—è§„åˆ’åˆ™é€šè¿‡æ‰©æ•£æ¨¡å‹(Diffusion Model)å’Œæ—¶ç©ºè§†è§‰Transformer(Spatio-Temporal Vision Transformer)é¢„æµ‹æ‰‹æœ¯åˆ‡é™¤åŠæ”¾åŒ–ç–—åçš„ç—…å˜è¿›å±•ã€‚ç³»ç»Ÿå¼•å…¥ç”Ÿå­˜ç‡è®¡ç®—å™¨å¹¶ç»“åˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(Proximal Policy Optimization, PPO)ç®—æ³•æ„å»ºåé¦ˆå¾ªç¯ï¼Œä»è€Œè¯†åˆ«æœ€ä¼˜çš„æ‰‹æœ¯åˆ‡é™¤ä½ç½®ã€‚å®éªŒå‘ç°ï¼Œè¯¥é¡ºåºè¯Šæ–­æ¡†æ¶å°†è®¡ç®—æˆæœ¬é™ä½äº†22.28å€ï¼ŒTransformeråº”ç”¨ä½¿æ¨ç†æ—¶é—´ç¼©çŸ­äº†113å°æ—¶ï¼Œä¸”æ•°æ®å¢å¼ºæŠ€æœ¯ä½¿DICEè¯„åˆ†æé«˜äº†2.9%ã€‚ç»“æœè¡¨æ˜è¯¥ç³»ç»Ÿæœ‰æœ›å°†GBMæ‚£è€…ç”Ÿå­˜ç‡æé«˜0.9%ï¼Œä¸ºä¸´åºŠæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡ç²¾å‡†åº¦çš„æ²»ç–—è¾…åŠ©æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06990v1",
      "published_date": "2025-12-07 20:51:59 UTC",
      "updated_date": "2025-12-07 20:51:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:44.906298+00:00"
    },
    {
      "arxiv_id": "2512.06989v1",
      "title": "Flash Multi-Head Feed-Forward Network",
      "title_zh": "Flash å¤šå¤´å‰é¦ˆç½‘ç»œ",
      "authors": [
        "Minshen Zhang",
        "Xiang Hu",
        "Jianguo Li",
        "Wei Wu",
        "Kewei Tu"
      ],
      "abstract": "We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»¥å¤šå¤´å‰é¦ˆç½‘ç»œ(Multi-Head FFN)æ›¿ä»£Transformeræ¶æ„ä¸­æ ‡å‡†FFNçš„å¯èƒ½æ€§ï¼Œæ—¨åœ¨åˆ©ç”¨å¤šå¤´æœºåˆ¶å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚é’ˆå¯¹å¤šå¤´æœºåˆ¶å¸¦æ¥çš„å†…å­˜æ¶ˆè€—æ¿€å¢ä»¥åŠæ¨¡å‹æ‰©å±•æ—¶ç»´åº¦æ¯”ä¾‹å¤±è¡¡çš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†Flash Multi-Head FFN (FlashMHF)ã€‚è¯¥æ–¹æ¡ˆå¼•å…¥äº†å—FlashAttentionå¯å‘çš„I/Oæ„ŸçŸ¥èåˆç®—å­ä»¥åœ¨SRAMä¸­åœ¨çº¿è®¡ç®—è¾“å‡ºï¼Œå¹¶è®¾è®¡äº†åŠ¨æ€åŠ æƒå¹¶è¡Œå­ç½‘ç»œæ¥ç»´æŒä¸­é—´ç»´åº¦ä¸å¤´ç»´åº¦çš„å¹³è¡¡ã€‚åœ¨128Mè‡³1.3Bå‚æ•°è§„æ¨¡çš„å®éªŒä¸­ï¼ŒFlashMHFåœ¨å›°æƒ‘åº¦(perplexity)å’Œä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡ä¸Šä¸€è‡´ä¼˜äºSwiGLU FFNã€‚ç»“æœè¡¨æ˜ï¼ŒFlashMHFèƒ½å°†å³°å€¼å†…å­˜ä½¿ç”¨é™ä½3è‡³5å€ï¼Œå¹¶å®ç°é«˜è¾¾1.08å€çš„æ¨ç†åŠ é€Ÿã€‚è¯¥å·¥ä½œè¯æ˜äº†å¤šå¤´è®¾è®¡æ˜¯FFNçš„ä¸€ç§ä¼˜è¶Šæ¶æ„åŸåˆ™ï¼Œä¸ºTransformeræä¾›äº†ä¸€ä¸ªå¼ºå¤§ã€é«˜æ•ˆä¸”æå…·æ‰©å±•æ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06989v1",
      "published_date": "2025-12-07 20:50:20 UTC",
      "updated_date": "2025-12-07 20:50:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:29.799112+00:00"
    },
    {
      "arxiv_id": "2512.06983v1",
      "title": "On Memory: A comparison of memory mechanisms in world models",
      "title_zh": "è®ºè®°å¿†ï¼šä¸–ç•Œæ¨¡å‹ä¸­è®°å¿†æœºåˆ¶çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Eli J. Laird",
        "Corey Clark"
      ],
      "abstract": "World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸–ç•Œæ¨¡å‹ï¼ˆWorld modelsï¼‰åœ¨é•¿ç¨‹è§„åˆ’ä¸­å› å—é™äºä¸»å¹²æ¶æ„è®°å¿†è·¨åº¦è€Œå¯¼è‡´çš„æ„ŸçŸ¥æ¼‚ç§»ä¸å›ç¯æ£€æµ‹ï¼ˆloop closuresï¼‰éš¾é¢˜ã€‚é€šè¿‡åˆ†æå¤šç§è®°å¿†å¢å¼ºæœºåˆ¶ï¼Œä½œè€…ç ”ç©¶äº†åŸºäº Transformer çš„ä¸–ç•Œæ¨¡å‹çš„æœ‰æ•ˆè®°å¿†è·¨åº¦ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŒºåˆ†è®°å¿†ç¼–ç ï¼ˆmemory encodingï¼‰ä¸è®°å¿†æ³¨å…¥ï¼ˆmemory injectionï¼‰æœºåˆ¶çš„åˆ†ç±»æ³•ã€‚è¯¥åˆ†ç±»æ³•åŸºäºæ®‹å·®æµåŠ¨åŠ›å­¦ï¼ˆresidual stream dynamicsï¼‰è§†è§’ï¼Œé˜æ˜äº†ä¸åŒæœºåˆ¶åœ¨æ‰©å±•è®°å¿†ä¸­çš„ä½œç”¨ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨çŠ¶æ€å¬å›è¯„ä»·ä»»åŠ¡ï¼ˆstate recall evaluation taskï¼‰è¡¡é‡äº†å„æœºåˆ¶çš„è®°å¿†å¬å›æ•ˆæœåŠå…¶æƒè¡¡å–èˆã€‚ç»“æœè¯æ˜ï¼Œè®°å¿†æœºåˆ¶èƒ½æœ‰æ•ˆæå‡è§†è§‰ Transformerï¼ˆvision transformersï¼‰çš„è®°å¿†è·¨åº¦ã€‚è¯¥æˆæœä¸ºåœ¨ä¸–ç•Œæ¨¡å‹çš„æƒ³è±¡è½¨è¿¹ä¸­å®ç°å›ç¯æ£€æµ‹æä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2512.06983v1",
      "published_date": "2025-12-07 20:29:20 UTC",
      "updated_date": "2025-12-07 20:29:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:44.520987+00:00"
    },
    {
      "arxiv_id": "2512.06969v1",
      "title": "Comparing BFGS and OGR for Second-Order Optimization",
      "title_zh": "BFGS ä¸ OGR åœ¨äºŒé˜¶ä¼˜åŒ–ä¸­çš„å¯¹æ¯”",
      "authors": [
        "Adrian Przybysz",
        "MikoÅ‚aj KoÅ‚ek",
        "Franciszek Sobota",
        "Jarek Duda"
      ],
      "abstract": "Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº†ç»å…¸çš„äºŒé˜¶ä¼˜åŒ–æ–¹æ³• BFGS ä¸ä¸€ç§åä¸º Online Gradient Regression (OGR) çš„æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ Hessian çŸ©é˜µä¼°ç®—çš„é«˜ç»´æ€§å’Œé«˜æˆæœ¬æŒ‘æˆ˜ã€‚BFGS æ–¹æ³•ä¾èµ–äº Sherman-Morrison æ›´æ–°ï¼Œåœ¨å‡¸æ€§å‡è®¾ä¸‹ç»´æŒæ­£å®šçš„ Hessian è¿‘ä¼¼ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒOGR é€šè¿‡å¯¹æ¢¯åº¦ä¸ä½ç½®è¿›è¡ŒæŒ‡æ•°ç§»åŠ¨å¹³å‡ (Exponential Moving Average) çš„å›å½’åˆ†æï¼Œå®ç°åœ¨çº¿ä¼°è®¡äºŒé˜¶å¯¼æ•°ä¸”æ— éœ€è¿›è¡Œ Hessian çŸ©é˜µæ±‚é€†ã€‚ä¸ BFGS ä¸åŒï¼ŒOGR èƒ½å¤Ÿä¼°ç®—éæ­£å®šçš„é€šç”¨ Hessian çŸ©é˜µï¼Œä»è€Œå…·å¤‡å¤„ç†éå‡¸ç»“æ„çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ ‡å‡†æµ‹è¯•å‡½æ•°çš„è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ OGR åœ¨æ”¶æ•›é€Ÿåº¦å’ŒæŸå¤±ä¸‹é™æ–¹é¢å‡ä¼˜äº BFGSã€‚ç‰¹åˆ«æ˜¯åœ¨éå‡¸è®¾ç½®ä¸‹ï¼ŒOGR å±•ç°å‡ºäº†æ›´æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä¸ºäºŒé˜¶ä¼˜åŒ–æä¾›äº†ä¸€ç§æ›´çµæ´»ä¸”é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06969v1",
      "published_date": "2025-12-07 19:26:26 UTC",
      "updated_date": "2025-12-07 19:26:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:41.610557+00:00"
    },
    {
      "arxiv_id": "2512.06963v1",
      "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
      "title_zh": "VideoVLAï¼šè§†é¢‘ç”Ÿæˆæ¨¡å‹äº¦å¯ä½œä¸ºé€šç”¨çš„æœºå™¨äººæ“çºµå™¨",
      "authors": [
        "Yichao Shen",
        "Fangyun Wei",
        "Zhiying Du",
        "Yaobo Liang",
        "Yan Lu",
        "Jiaolong Yang",
        "Nanning Zheng",
        "Baining Guo"
      ],
      "abstract": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(Vision-Language-Action, VLA)åœ¨å¤„ç†æ–°ä»»åŠ¡å’Œç‰©ä½“æ—¶æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº† VideoVLA æ¡†æ¶ï¼Œæ—¨åœ¨æ¢ç´¢å°†å¤§å‹è§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºæœºå™¨äººæ“æ§å™¨çš„æ½œåŠ›ã€‚VideoVLA åŸºäºå¤šæ¨¡æ€æ‰©æ•£ Transformer (multi-modal Diffusion Transformer)ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤å’Œå½“å‰å›¾åƒåŒæ—¶é¢„æµ‹åŠ¨ä½œåºåˆ—åŠæœªæ¥çš„è§†è§‰æ¼”å˜ç»“æœã€‚å®éªŒç»“æœæ­ç¤ºäº†é«˜è´¨é‡çš„è§†è§‰æƒ³è±¡ä¸åŠ¨ä½œé¢„æµ‹çš„å¯é æ€§åŠä»»åŠ¡æˆåŠŸç‡ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ­£ç›¸å…³å…³ç³»ã€‚VideoVLA åœ¨æ¨¡ä»¿ä¸åŒå…·èº«æ™ºèƒ½(embodiment)çš„æŠ€èƒ½ä»¥åŠå¤„ç†æ–°é¢–ç‰©ä½“æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–è¡¨ç°ã€‚è¿™ç§ç»“åˆåŠ¨ä½œé¢„æµ‹ä¸è§†è§‰åæœé¢„åˆ¤çš„åŒé‡é¢„æµ‹ç­–ç•¥ï¼Œä¸ºæœºå™¨äººå­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œæ˜¾è‘—æå‡äº†æ“æ§ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é€šç”¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://videovla-nips2025.github.io",
      "pdf_url": "https://arxiv.org/pdf/2512.06963v1",
      "published_date": "2025-12-07 18:57:15 UTC",
      "updated_date": "2025-12-07 18:57:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:53.968195+00:00"
    },
    {
      "arxiv_id": "2512.09946v1",
      "title": "ELANA: A Simple Energy and Latency Analyzer for LLMs",
      "title_zh": "ELANAï¼šä¸€ç§ç®€æ˜“çš„å¤§è¯­è¨€æ¨¡å‹èƒ½è€—ä¸å»¶è¿Ÿåˆ†æå™¨",
      "authors": [
        "Hung-Yueh Chiang",
        "Bokun Wang",
        "Diana Marculescu"
      ],
      "abstract": "The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \\textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†å¼€æºåˆ†æå·¥å…· ELANAï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä»ç§»åŠ¨è¾¹ç¼˜è®¾å¤‡åˆ°äº‘ç«¯ GPU é›†ç¾¤éƒ¨ç½²è¿‡ç¨‹ä¸­é¢ä¸´çš„å»¶è¿Ÿå’ŒåŠŸè€—é™åˆ¶ã€‚ä½œä¸ºä¸€ä¸ªè½»é‡çº§ä¸”å­¦æœ¯å‹å¥½çš„åˆ†æå™¨ï¼ŒELANA èƒ½å¤Ÿç²¾ç¡®è¯„ä¼°æ¨¡å‹å¤§å°ã€é”®å€¼ç¼“å­˜ (KV cache) å¤§å°ï¼Œä»¥åŠé¦–ä»¤ç‰Œæ—¶é—´ (TTFT)ã€æ¯ä»¤ç‰Œç”Ÿæˆæ—¶é—´ (TPOT) å’Œæ€»å»¶è¿Ÿ (TTLT) ç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡ã€‚è¯¥å·¥å…·å…¨é¢æ”¯æŒ Hugging Face ä¸Šçš„å…¬å¼€æ¨¡å‹ï¼Œå…·å¤‡ç®€å•çš„å‘½ä»¤è¡Œç•Œé¢å¹¶æä¾›å¯é€‰çš„èƒ½è€—è®°å½•åŠŸèƒ½ï¼Œä¸”å¹¿æ³›å…¼å®¹å¤š GPU å’Œè¾¹ç¼˜ GPU å¹³å°ã€‚æ­¤å¤–ï¼ŒELANA ä¸ Hugging Face API å®Œå…¨å…¼å®¹ï¼Œå¯è½»æ¾é€‚é…å‹ç¼©æ¨¡å‹æˆ–ä½ä½å®½ (low bit-width) æ¨¡å‹ï¼Œä¸ºé«˜æ•ˆ LLMs ç ”ç©¶å’Œå°å‹æ¦‚å¿µéªŒè¯ç ”ç©¶æä¾›äº†ç†æƒ³çš„æ€§èƒ½åˆ†ææ–¹æ¡ˆã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.09946v1",
      "published_date": "2025-12-07 18:43:47 UTC",
      "updated_date": "2025-12-07 18:43:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:22:54.428119+00:00"
    },
    {
      "arxiv_id": "2512.06951v2",
      "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
      "title_zh": "è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ä»»åŠ¡é€‚é…ï¼š2025 BEHAVIOR æŒ‘æˆ˜èµ›å† å†›æ–¹æ¡ˆ",
      "authors": [
        "Ilia Larchenko",
        "Gleb Zarin",
        "Akash Karnatak"
      ],
      "abstract": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making. Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules. Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† 2025 BEHAVIOR Challenge çš„å† å†›æ–¹æ¡ˆï¼Œæ—¨åœ¨åº”å¯¹åŒ…å« 50 ç§é•¿ç¨‹å®¶åº­ä»»åŠ¡çš„å¤æ‚æŒ‘æˆ˜ï¼Œæ¶µç›–äº†åŒè‡‚æ“ä½œ (bimanual manipulation)ã€è‡ªä¸»å¯¼èˆªåŠä¸Šä¸‹æ–‡æ„ŸçŸ¥å†³ç­–ã€‚è¯¥æ–¹æ¡ˆåŸºäº Pi0.5 æ¶æ„ï¼Œå…¶æ ¸å¿ƒè´¡çŒ®æ˜¯ä¸º flow matching å¼•å…¥äº†ç›¸å…³å™ªå£° (correlated noise)ï¼Œè¿™ä¸ä»…æ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ï¼Œè¿˜é€šè¿‡å…³è”æ„ŸçŸ¥è¡¥å…¨ (correlation-aware inpainting) ç¡®ä¿äº†åŠ¨ä½œåºåˆ—çš„å¹³æ»‘æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿåº”ç”¨äº†å¯å­¦ä¹ çš„æ··åˆå±‚æ³¨æ„åŠ› (learnable mixed-layer attention) å’Œ System 2 é˜¶æ®µè¿½è¸ªæŠ€æœ¯æ¥è§£å†³ä»»åŠ¡ä¸­çš„æ­§ä¹‰é—®é¢˜ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨å¤šæ ·æœ¬ flow matching é™ä½æ–¹å·®ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå¼•å…¥åŠ¨ä½œå‹ç¼© (action compression) ä¸ç‰¹å®šä¿®æ­£è§„åˆ™ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•åœ¨å…¬æœ‰åŠç§æœ‰æ’è¡Œæ¦œçš„æ‰€æœ‰ä»»åŠ¡ä¸­å‡å–å¾—äº† 26% çš„ q-scoreï¼Œå±•ç°äº†å…¶åœ¨å¤„ç†å¤§è§„æ¨¡ã€é«˜ä¿çœŸä»¿çœŸç¯å¢ƒä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "2025 NeurIPS Behavior Challenge 1st place solution",
      "pdf_url": "https://arxiv.org/pdf/2512.06951v2",
      "published_date": "2025-12-07 18:08:45 UTC",
      "updated_date": "2025-12-21 13:33:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:08.093961+00:00"
    },
    {
      "arxiv_id": "2512.06944v1",
      "title": "A Unifying Human-Centered AI Fairness Framework",
      "title_zh": "ç»Ÿä¸€çš„ä»¥äººä¸ºä¸­å¿ƒçš„äººå·¥æ™ºèƒ½å…¬å¹³æ€§æ¡†æ¶",
      "authors": [
        "Munshi Mahbubur Rahman",
        "Shimei Pan",
        "James R. Foulds"
      ],
      "abstract": "The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ä»¥äººä¸ºæœ¬çš„AIå…¬å¹³æ€§æ¡†æ¶ (human-centered fairness framework)ï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½åœ¨å…³é”®ç¤¾ä¼šé¢†åŸŸåº”ç”¨æ—¶å‡ºç°çš„å…¬å¹³æ€§æŒ‘æˆ˜åŠä¸åŒå…¬å¹³æ€§å®šä¹‰ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ã€‚è¯¥æ¡†æ¶ç³»ç»Ÿåœ°æ¶µç›–äº†å…«ç§ä¸åŒçš„å…¬å¹³æ€§æŒ‡æ ‡ (fairness metrics)ï¼Œé€šè¿‡æ•´åˆä¸ªäººå…¬å¹³ (individual fairness) ä¸ç¾¤ä½“å…¬å¹³ (group fairness)ã€è¾¹é™…å†… (infra-marginal) ä¸äº¤é›† (intersectional) å‡è®¾ï¼Œä»¥åŠåŸºäºç»“æœ (outcome-based) ä¸æœºä¼šå‡ç­‰ (equality-of-opportunity) çš„è§†è§’ã€‚è¿™ç§ç»“æ„ä½¿åˆ©ç›Šç›¸å…³è€…èƒ½å¤Ÿæ ¹æ®å…·ä½“èƒŒæ™¯å’Œä»·å€¼è§‚ä¸ºå¤šä¸ªå…¬å¹³æ€§ç›®æ ‡åˆ†é…æƒé‡ï¼Œä»è€Œåœ¨å¤šæ–¹åšå¼ˆä¸­è¾¾æˆæŠ˜ä¸­ã€‚é€šè¿‡åœ¨ UCI Adultã€COMPASã€German Credit å’Œ MEPS å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„åº”ç”¨ï¼Œç ”ç©¶å±•ç¤ºäº†è°ƒæ•´æƒé‡å¦‚ä½•æ­ç¤ºä¸åŒå…¬å¹³æ€§æŒ‡æ ‡ä¹‹é—´çš„ç»†å¾®æƒè¡¡ã€‚æœ€åï¼Œé€šè¿‡å¸æ³•å†³ç­–å’ŒåŒ»ç–—ä¿å¥çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥æ¡†æ¶è¯æ˜äº†å…¶åœ¨æŒ‡å¯¼å…¬å¹³äººå·¥æ™ºèƒ½ç³»ç»Ÿè¿›è¡Œä»·å€¼æ•æ„Ÿå‹éƒ¨ç½²æ–¹é¢çš„å®è·µæ„ä¹‰ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06944v1",
      "published_date": "2025-12-07 17:52:38 UTC",
      "updated_date": "2025-12-07 17:52:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:11.328385+00:00"
    },
    {
      "arxiv_id": "2601.02371v2",
      "title": "Permission Manifests for Web Agents",
      "title_zh": "Web æ™ºèƒ½ä½“æƒé™æ¸…å•",
      "authors": [
        "Samuele Marro",
        "Alan Chan",
        "Xinxing Ren",
        "Lewis Hammond",
        "Jesse Wright",
        "Gurjyot Wanga",
        "Tiziano Piccardi",
        "Nuno Campos",
        "Tobin South",
        "Jialin Yu",
        "Sunando Sengupta",
        "Eric Sommerlade",
        "Alex Pentland",
        "Philip Torr",
        "Jiaxin Pei"
      ],
      "abstract": "The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with the web. Unlike traditional crawlers that follow simple conventions, such as robots$.$txt, modern agents engage with websites in sophisticated ways: navigating complex interfaces, extracting structured information, and completing end-to-end tasks. Existing governance mechanisms were not designed for these capabilities. Without a way to specify what interactions are and are not allowed, website owners increasingly rely on blanket blocking and CAPTCHAs, which undermine beneficial applications such as efficient automation, convenient use of e-commerce services, and accessibility tools. We introduce agent-permissions$.$json, a robots$.$txt-style lightweight manifest where websites specify allowed interactions, complemented by API references where available. This framework provides a low-friction coordination mechanism: website owners only need to write a simple JSON file, while agents can easily parse and automatically implement the manifest's provisions. Website owners can then focus on blocking non-compliant agents, rather than agents as a whole. By extending the spirit of robots$.$txt to the era of LLM-mediated interaction, and complementing data use initiatives such as AIPref, the manifest establishes a compliance framework that enables beneficial agent interactions while respecting site owners' preferences.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„Webæ™ºèƒ½ä½“åœ¨æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶é¢ä¸´çš„æ²»ç†æŒ‘æˆ˜ï¼Œæå‡ºäº† agent-permissions.json è¿™ä¸€è½»é‡çº§æ¸…å•æ–‡ä»¶ã€‚ç”±äºä¼ ç»Ÿçš„ robots.txt æ— æ³•è§„èŒƒç°ä»£æ™ºèƒ½ä½“åœ¨å¯¼èˆªç•Œé¢å’Œæå–ç»“æ„åŒ–ä¿¡æ¯æ–¹é¢çš„æ·±å±‚äº¤äº’ï¼Œå¯¼è‡´ç½‘ç«™æ‰€æœ‰è€…å¸¸é‡‡å–å…¨é¢å°ç¦æˆ– CAPTCHAs ç­‰æ‰‹æ®µï¼Œä»è€Œé™åˆ¶äº†æœ‰ç›Šçš„è‡ªåŠ¨åŒ–å’Œå¯è®¿é—®æ€§å·¥å…·çš„åº”ç”¨ã€‚è¯¥æ–¹æ¡ˆå€Ÿé‰´äº† robots.txt çš„è®¾è®¡ç†å¿µï¼Œé€šè¿‡ç®€å•çš„ JSON æ ¼å¼æ–‡ä»¶è®©ç½‘ç«™æ˜ç¡®æˆæƒçš„äº¤äº’èŒƒå›´ï¼Œå¹¶è¾…ä»¥ API å¼•ç”¨ã€‚æ™ºèƒ½ä½“å¯ä»¥è½»æ¾è§£æå¹¶è‡ªåŠ¨éµå®ˆè¿™äº›è§„å®šï¼Œä»è€Œå»ºç«‹èµ·ä¸€ç§ä½æ‘©æ“¦çš„åä½œä¸åˆè§„æ¡†æ¶ã€‚è¿™ä¸€æœºåˆ¶åœ¨å°Šé‡ç«™ç‚¹æ‰€æœ‰è€…åå¥½çš„åŸºç¡€ä¸Šï¼Œæœ‰æ•ˆä¿ƒè¿›äº† LLM æ—¶ä»£ä¸‹æ™ºèƒ½ä½“ä¸ç½‘é¡µä¹‹é—´æœ‰ç›Šä¸”è§„èŒƒçš„äº¤äº’ï¼Œå¹¶ä¸ AIPref ç­‰ç°æœ‰æ•°æ®ä½¿ç”¨è§„èŒƒå½¢æˆäº†äº’è¡¥ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.MA",
        "cs.NI"
      ],
      "primary_category": "cs.CY",
      "comment": "Authored by the Lightweight Agent Standards Working Group https://las-wg.org/",
      "pdf_url": "https://arxiv.org/pdf/2601.02371v2",
      "published_date": "2025-12-07 17:45:01 UTC",
      "updated_date": "2026-01-12 23:23:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:13.167149+00:00"
    },
    {
      "arxiv_id": "2512.06932v1",
      "title": "Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies",
      "title_zh": "æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„éšæ€§æ³„éœ²ï¼šæ•°æ®æ³„éœ²å¦‚ä½•å½±å“ä¸åŒé…ç½®ä¸éªŒè¯ç­–ç•¥ä¸‹çš„ LSTM è¯„ä¼°",
      "authors": [
        "Salma Albelali",
        "Moataz Ahmed"
      ],
      "abstract": "Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—é¢„æµ‹ä¸­æ•°æ®æ³„æ¼(data leakage)å¯¹Long Short-Term Memory(LSTM)æ¨¡å‹è¯„ä¼°çš„å½±å“ï¼Œé‡ç‚¹åˆ†æäº†éªŒè¯ç­–ç•¥(validation strategy)å¦‚ä½•è°ƒèŠ‚æ³„æ¼æ•æ„Ÿæ€§ã€‚ç ”ç©¶è€…å¯¹æ¯”äº†ä¸‰ç§å¸¸ç”¨éªŒè¯æŠ€æœ¯ï¼š2-way splitã€3-way splitå’Œ10-fold cross-validationï¼Œå¹¶ä½¿ç”¨RMSE Gainè¡¡é‡æ³„æ¼å¯¼è‡´çš„æ€§èƒ½è™šé«˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ10-fold cross-validationåœ¨é•¿æ»åæ­¥éª¤ä¸‹è¡¨ç°å‡ºæé«˜çš„æ³„æ¼æ•æ„Ÿæ€§ï¼Œå…¶RMSE Gainé«˜è¾¾20.5%ï¼Œè€Œ2-wayå’Œ3-way splitsåœ¨å¤šæ•°é…ç½®ä¸‹è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå¢ç›Šé€šå¸¸ä½äº5%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°è¾“å…¥çª—å£å¤§å°(input window size)å’Œæ»åæ­¥éª¤(lag step)æ˜¾è‘—å½±å“æ³„æ¼é£é™©ï¼Œè¾ƒå°çš„çª—å£å’Œè¾ƒé•¿çš„æ»åæ›´å®¹æ˜“å¼•å‘æ•°æ®æ³„æ¼ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨æ—¶é—´åºåˆ—å»ºæ¨¡ä¸­æ„å»ºæŠ—æ³„æ¼è¯„ä¼°æµç¨‹çš„é‡è¦æ€§ï¼Œä»¥ç¡®ä¿æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„çœŸå®å¯é ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06932v1",
      "published_date": "2025-12-07 17:21:27 UTC",
      "updated_date": "2025-12-07 17:21:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:16.914604+00:00"
    },
    {
      "arxiv_id": "2512.06929v1",
      "title": "Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding",
      "title_zh": "èåˆå¤šå°ºåº¦è¶‹åŠ¿åˆ†è§£ä¸ Patch MoE ç¼–ç çš„è‡ªé€‚åº”å½’ä¸€åŒ– Mamba",
      "authors": [
        "MinCheol Jeon"
      ],
      "abstract": "Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AdaMambaï¼Œä¸€ç§æ—¨åœ¨è§£å†³æ—¶é—´åºåˆ—é¢„æµ‹ä¸­non-stationarityã€å¤šå°ºåº¦æ—¶é—´æ¨¡å¼å’Œåˆ†å¸ƒåç§»é—®é¢˜çš„ç»Ÿä¸€æ¶æ„ã€‚è¯¥æ¶æ„çš„æ ¸å¿ƒæ˜¯Adaptive Normalization Blockï¼Œé€šè¿‡å¤šå°ºåº¦å·ç§¯è¶‹åŠ¿æå–å’Œchannel-wise recalibrationæ¥å®ç°æ–¹å·®ç¨³å®šå’Œå»è¶‹åŠ¿åŒ–ã€‚Context Encoderéƒ¨åˆ†ç»“åˆäº†patch-wise embeddingså’Œå¸¦æœ‰Mixture of Experts (MoE)å‰é¦ˆæ¨¡å—çš„Mambaå¢å¼ºå‹Transformerå±‚ï¼Œèƒ½å¤Ÿé«˜æ•ˆå»ºæ¨¡é•¿ç¨‹ä¾èµ–å’Œå±€éƒ¨æ—¶é—´åŠ¨æ€ã€‚é€šè¿‡è½»é‡çº§é¢„æµ‹å¤´å’Œèåˆå±€éƒ¨è¶‹åŠ¿çš„denormalizationæœºåˆ¶ï¼ŒAdaMambaç¡®ä¿äº†åœ¨å¤šæ­¥é¢„æµ‹ä»»åŠ¡ä¸­çš„é²æ£’æ€§ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼ŒAdaMambaåœ¨å¼‚æ„æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„TransformeråŸºçº¿ï¼Œæœ‰æ•ˆç¼“è§£äº†covariate shiftå¹¶æå‡äº†é¢„æµ‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06929v1",
      "published_date": "2025-12-07 17:14:32 UTC",
      "updated_date": "2025-12-07 17:14:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:28.160057+00:00"
    },
    {
      "arxiv_id": "2512.06926v1",
      "title": "Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise",
      "title_zh": "è¯„ä¼° BiLSTM é¢„æµ‹æ¨¡å‹å¯¹åºåˆ—é•¿åº¦å’Œè¾“å…¥å™ªå£°çš„æ•æ„Ÿæ€§",
      "authors": [
        "Salma Albelali",
        "Moataz Ahmed"
      ],
      "abstract": "Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ BiLSTM æ¶æ„ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æäº†è¾“å…¥åºåˆ—é•¿åº¦ (input sequence length) å’Œæ·»åŠ å™ªå£° (additive noise) ä¸¤ä¸ªå…³é”®æ•°æ®å› ç´ å¯¹æ¨¡å‹é²æ£’æ€§çš„å½±å“ã€‚é€šè¿‡æ„å»ºæ¨¡å—åŒ–ä¸”å¯é‡å¤çš„é¢„æµ‹æµæ°´çº¿å¹¶åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œå—æ§å®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†é•¿åºåˆ—è¾“å…¥åœ¨æ•°æ®å—é™ç¯å¢ƒä¸‹ä¼šæ˜¾è‘—å¢åŠ è¿‡æ‹Ÿåˆ (overfitting) å’Œæ•°æ®æ³„æ¼ (data leakage) çš„é£é™©ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œæ·»åŠ å™ªå£°ä¼šæ™®éé™ä½é¢„æµ‹å‡†ç¡®æ€§ï¼Œä¸”å½“åºåˆ—é•¿åº¦ä¸å™ªå£°æŒ‘æˆ˜å¹¶å­˜æ—¶ï¼Œæ¨¡å‹ç¨³å®šæ€§çš„ä¸‹é™æœ€ä¸ºä¸¥é‡ã€‚è™½ç„¶é«˜é‡‡æ ·é¢‘ç‡çš„æ•°æ®é›†å±•ç°å‡ºç›¸å¯¹è¾ƒå¼ºçš„é²æ£’æ€§ï¼Œä½†åœ¨åŒé‡ä¸åˆ©å› ç´ å½±å“ä¸‹ä»æ˜¾è„†å¼±ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ•°æ®æ„ŸçŸ¥è®¾è®¡ç­–ç•¥çš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºæ›´å¯é ã€æ›´å…·æ³›åŒ–èƒ½åŠ›çš„æ—¶é—´åºåˆ—é¢„æµ‹ç³»ç»Ÿæä¾›äº†é‡è¦çš„å®è·µè§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06926v1",
      "published_date": "2025-12-07 17:10:06 UTC",
      "updated_date": "2025-12-07 17:10:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:22.855436+00:00"
    },
    {
      "arxiv_id": "2512.06925v1",
      "title": "Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features",
      "title_zh": "åŸºäº Transformer è¯­ä¹‰ç‰¹å¾çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ç½‘ç»œé’“é±¼æ£€æµ‹",
      "authors": [
        "Aseer Al Faisal"
      ],
      "abstract": "Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹é’“é±¼æ”»å‡»å¯¼è‡´çš„ä¸ªäººä¿¡æ¯æ³„éœ²é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ†ä½æ•°å›å½’æ·±åº¦Qç½‘ç»œ(QR-DQN)æ–¹æ³•ï¼Œé€šè¿‡é›†æˆRoBERTaè¯­ä¹‰åµŒå…¥(RoBERTa semantic embeddings)å’Œæ‰‹å·¥æå–çš„è¯æ³•ç‰¹å¾(lexical features)æ¥å¢å¼ºæ£€æµ‹èƒ½åŠ›ã€‚ä¸ä¼ ç»ŸDQNä»…ä¼°è®¡å•ä¸€æ ‡é‡å€¼ä¸åŒï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åˆ†ä½æ•°å›å½’å»ºæ¨¡å›æŠ¥åˆ†å¸ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åº”å¯¹ä¸ç¡®å®šæ€§æ—¶çš„ç¨³å®šæ€§å’Œæ³›åŒ–è¡¨ç°ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºä»PhishTankç­‰å¹³å°æœé›†çš„105,000ä¸ªURLæ„å»ºäº†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°99.86%ï¼ŒF1åˆ†æ•°è¾¾99.85%ã€‚ç›¸æ¯”äºæ ‡å‡†DQNï¼Œè¿™ç§æ··åˆè¯­ä¹‰ä¸è¯æ³•ç‰¹å¾çš„æ–¹æ³•å°†æ³›åŒ–å·®è·(generalization gap)ä»1.66%å¤§å¹…ç¼©å‡è‡³0.04%ï¼Œè¡¨ç°å‡ºæé«˜çš„é²æ£’æ€§ã€‚äº”æŠ˜äº¤å‰éªŒè¯è¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹åœ¨è¯†åˆ«æ¼”å˜ä¸­çš„é’“é±¼å¨èƒä»¥åŠå¤„ç†æœªè§æ•°æ®æ–¹é¢çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06925v1",
      "published_date": "2025-12-07 17:08:12 UTC",
      "updated_date": "2025-12-07 17:08:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:47.379463+00:00"
    },
    {
      "arxiv_id": "2512.06921v1",
      "title": "NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification",
      "title_zh": "NeuroABenchï¼šé¢å‘ç¥ç»å¤–ç§‘è§£å‰–è¯†åˆ«çš„å¤šæ¨¡æ€è¯„ä¼°åŸºå‡†",
      "authors": [
        "Ziyang Song",
        "Zelin Zang",
        "Xiaofan Ye",
        "Boqiang Xu",
        "Long Bai",
        "Jinlin Wu",
        "Hongliang Ren",
        "Hongbin Liu",
        "Jiebo Luo",
        "Zhen Lei"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†NeuroABenchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°ç¥ç»å¤–ç§‘é¢†åŸŸè§£å‰–ç»“æ„ç†è§£çš„å¤šæ¨¡æ€è¯„ä»·åŸºå‡†ã€‚è¯¥ Benchmark åŒ…å«9å°æ—¶ç»è¿‡æ ‡æ³¨çš„è§†é¢‘ï¼Œæ¶µç›–89ç§æ‰‹æœ¯ç¨‹åºåŠ68ç§ä¸´åºŠè§£å‰–ç»“æ„çš„è¯†åˆ«ï¼Œå¹¶é‡‡ç”¨æ–°å‹å¤šæ¨¡æ€æ ‡æ³¨æµç¨‹ç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§ã€‚å¯¹è¶…è¿‡10ç§å…ˆè¿› Multimodal Large Language Models (MLLMs) çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º40.87%ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„å±€é™æ€§ã€‚é€šè¿‡ä¸ç¥ç»å¤–ç§‘å—è®­äººå‘˜çš„å¯¹æ¯”å‘ç°ï¼Œé¡¶çº§æ¨¡å‹æ€§èƒ½è™½æ¥è¿‘ä½åˆ†æ®µå—è®­è€…ï¼Œä½†ä»æ˜æ˜¾ä½äºäººç±»46.5%çš„å¹³å‡å‡†ç¡®ç‡ã€‚è¿™é¡¹å·¥ä½œä¸ä»…å¡«è¡¥äº†è§£å‰–å­¦ç†è§£è¯„æµ‹çš„ç©ºç™½ï¼Œæ›´æ­ç¤ºäº†å½“å‰äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¸“ä¸šé¢†åŸŸä¸äººç±»ä¸“å®¶æ°´å¹³ä¹‹é—´çš„å·¨å¤§å·®è·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by IEEE ICIA 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.06921v1",
      "published_date": "2025-12-07 17:00:25 UTC",
      "updated_date": "2025-12-07 17:00:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:28.036605+00:00"
    },
    {
      "arxiv_id": "2512.06914v1",
      "title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions",
      "title_zh": "SoKï¼šå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“äº¤äº’ä¸­çš„ä¿¡ä»»-æˆæƒä¸åŒ¹é…é—®é¢˜",
      "authors": [
        "Guanquan Shi",
        "Haohua Du",
        "Zhiqiang Wang",
        "Xiaoyu Liang",
        "Weiwenpei Liu",
        "Song Bian",
        "Zhenyu Guan"
      ],
      "abstract": "Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.\n  We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºè‡ªä¸»æ™ºèƒ½ä½“åœ¨ä¸å¤–éƒ¨ä¸–ç•Œäº¤äº’æ—¶é¢ä¸´çš„å®‰å…¨æŒ‘æˆ˜ï¼Œé‡ç‚¹åˆ†æäº†ç”±äºå†³ç­–ä»ç¡®å®šæ€§ä»£ç é€»è¾‘è½¬å‘è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ¦‚ç‡æ¨ç†è€Œäº§ç”Ÿçš„æˆæƒä¸ä»£ç†éš¾é¢˜ã€‚ä½œè€…æŒ‡å‡ºï¼Œç”±äºAIæ™ºèƒ½ä½“çš„è¡Œä¸ºå…·æœ‰ä¸å¯é¢„æµ‹æ€§ä¸”æŒ‡ä»¤å­˜åœ¨æ­§ä¹‰ï¼Œå»ºç«‹ä¿¡ä»»å¹¶æ‰§è¡Œæœ€å°ç‰¹æƒåŸåˆ™(PoLP)å…·æœ‰æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚ä¸ºäº†ç³»ç»ŸåŒ–åˆ†æè¿™ä¸€é¢†åŸŸï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å½¢å¼åŒ–è§†è§’ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ä»¥ä¿¡ä»»è¯„ä¼°ä¸æˆæƒç­–ç•¥ä¹‹é—´çš„ä¿¡ä»»-æˆæƒç¼ºå£(trust-authorization gap)ä¸ºæ ¸å¿ƒçš„æ–°å‹é£é™©åˆ†ææ¨¡å‹ã€‚é€šè¿‡è¯¥æ¨¡å‹ï¼Œç ”ç©¶è€…å¯¹ç°æœ‰çš„æ”»å‡»ä¸é˜²å¾¡è·¯å¾„è¿›è¡Œäº†åˆ†ç±»å’Œè°ƒæŸ¥ï¼Œæœ‰æ•ˆç»Ÿä¸€äº†è¯¥é¢†åŸŸçš„åˆ†æ•£è®¤çŸ¥å¹¶æ­ç¤ºäº†å…³é”®ç ”ç©¶ç©ºç™½ã€‚æœ€åï¼Œè®ºæ–‡åŸºäºè¯¥åˆ†ææå‡ºäº†æ„å»ºé²æ£’ã€å—ä¿¡ä»»æ™ºèƒ½ä½“åŠåŠ¨æ€æˆæƒæœºåˆ¶çš„ç³»ç»Ÿæ€§ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06914v1",
      "published_date": "2025-12-07 16:41:02 UTC",
      "updated_date": "2025-12-07 16:41:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:33.257965+00:00"
    },
    {
      "arxiv_id": "2512.06902v1",
      "title": "BabelCoder: Agentic Code Translation with Specification Alignment",
      "title_zh": "BabelCoderï¼šåŸºäºè§„èŒƒå¯¹é½çš„æ™ºèƒ½ä½“åŒ–ä»£ç ç¿»è¯‘",
      "authors": [
        "Fazle Rabbi",
        "Soumit Kanti Saha",
        "Tri Minh Triet Pham",
        "Song Wang",
        "Jinqiu Yang"
      ],
      "abstract": "As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BabelCoderï¼Œä¸€ç§åŸºäºæ™ºèƒ½ä½“ (agentic) çš„ä»£ç ç¿»è¯‘æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä»£ç è¿ç§»è¿‡ç¨‹ä¸­å‡†ç¡®æ€§ä¸è¶³ä»¥åŠå¯¹ä¸Šä¸‹æ–‡å’Œç»“æ„çº¿ç´¢åˆ©ç”¨æœ‰é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ç¿»è¯‘ä»»åŠ¡åˆ†è§£ä¸ºç¿»è¯‘ã€æµ‹è¯•å’Œä¿®å¤ä¸‰ä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ï¼Œå®ç°äº†ç”Ÿæˆä»£ç ã€éªŒè¯æ­£ç¡®æ€§åŠä¿®å¤é”™è¯¯çš„ç»“æ„åŒ–åä½œï¼Œä»è€Œæ˜¾è‘—æå‡ç¿»è¯‘è´¨é‡ã€‚å®éªŒåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤º BabelCoder åœ¨ 94% çš„æµ‹è¯•æ¡ˆä¾‹ä¸­ä¼˜äºç°æœ‰çš„å››ç§æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ï¼Œæ€§èƒ½æå‡å¹…åº¦åœ¨ 0.5% è‡³ 13.5% ä¹‹é—´ã€‚æœ€ç»ˆï¼Œè¯¥ç³»ç»Ÿå®ç°äº† 94.16% çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚è·¨è¯­è¨€ä»£ç è¿ç§»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "21 pages, 8 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.06902v1",
      "published_date": "2025-12-07 15:57:54 UTC",
      "updated_date": "2025-12-07 15:57:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:23:35.275360+00:00"
    },
    {
      "arxiv_id": "2512.06885v1",
      "title": "JoPano: Unified Panorama Generation via Joint Modeling",
      "title_zh": "JoPanoï¼šåŸºäºè”åˆå»ºæ¨¡çš„ç»Ÿä¸€å…¨æ™¯å›¾ç”Ÿæˆ",
      "authors": [
        "Wancheng Feng",
        "Chen An",
        "Zhenliang He",
        "Meina Kan",
        "Shiguang Shan",
        "Lukun Wang"
      ],
      "abstract": "Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†JoPanoï¼Œä¸€ç§é€šè¿‡è”åˆå»ºæ¨¡å®ç°ç»Ÿä¸€å…¨æ™¯å›¾ç”Ÿæˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­åŸºäºU-Netæ¶æ„çš„è§†è§‰è´¨é‡å—é™ï¼Œä»¥åŠtext-to-panoramaä¸view-to-panoramaä»»åŠ¡ç›¸äº’ç‹¬ç«‹å¯¼è‡´çš„å»ºæ¨¡å†—ä½™é—®é¢˜ã€‚JoPanoé‡‡ç”¨åŸºäºDiTçš„æ¨¡å‹æ¶æ„ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå…¨æ™¯å›¾cubemapè¡¨ç¤ºçš„Joint-Face Adapterï¼Œä½¿é¢„è®­ç»ƒçš„DiTèƒ½å¤Ÿè”åˆå»ºæ¨¡å¹¶ç”Ÿæˆå…¨æ™¯å›¾çš„ä¸åŒè§†å›¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ç”Ÿæˆçš„è§†è§‰ä¸€è‡´æ€§ï¼Œç ”ç©¶å¼•å…¥äº†Poisson BlendingæŠ€æœ¯æ¥å‡å°‘ç«‹æ–¹ä½“é¢è¾¹ç•Œå¤„å¸¸å‡ºç°çš„æ¥ç¼ä¸ä¸€è‡´ç°è±¡ã€‚ç›¸åº”åœ°ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†Seam-SSIMå’ŒSeam-SobelæŒ‡æ ‡ï¼Œç”¨äºå®šé‡è¯„ä¼°æ¥ç¼ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ¡ä»¶åˆ‡æ¢æœºåˆ¶ï¼ŒJoPanoåœ¨å•ä¸€æ¨¡å‹ä¸­æˆåŠŸç»Ÿä¸€äº†text-to-panoramaå’Œview-to-panoramaä¸¤é¡¹æ ¸å¿ƒä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJoPanoèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å…¨æ™¯å›¾åƒï¼Œå¹¶åœ¨FIDã€CLIP-FIDã€ISå’ŒCLIP-Scoreç­‰å¤šé¡¹æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†state-of-the-artæ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code: https://github.com/VIPL-GENUN/JoPano",
      "pdf_url": "https://arxiv.org/pdf/2512.06885v1",
      "published_date": "2025-12-07 15:19:26 UTC",
      "updated_date": "2025-12-07 15:19:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:03.170074+00:00"
    },
    {
      "arxiv_id": "2512.06879v1",
      "title": "WisPaper: Your AI Scholar Search Engine",
      "title_zh": "WisPaperï¼šæ‚¨çš„ AI å­¦æœ¯æœç´¢å¼•æ“",
      "authors": [
        "Li Ju",
        "Jun Zhao",
        "Mingxu Chai",
        "Ziyu Shen",
        "Xiangyang Wang",
        "Yage Geng",
        "Chunchun Ma",
        "Hao Peng",
        "Guangbin Li",
        "Tao Li",
        "Chengyong Liao",
        "Fu Wang",
        "Xiaolong Wang",
        "Junshen Chen",
        "Rui Gong",
        "Shijia Liang",
        "Feiyan Li",
        "Ming Zhang",
        "Kexin Tan",
        "Jujie Ye",
        "Zhiheng Xi",
        "Shihan Dou",
        "Tao Gui",
        "Yuankai Ying",
        "Yang Shi",
        "Yue Zhang",
        "Qi Zhang"
      ],
      "abstract": "Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \\textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \\textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \\textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \\textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \\textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† WisPaperï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³ç§‘ç ”äººå‘˜åœ¨å¤„ç†æŒ‡æ•°çº§å¢é•¿çš„ç§‘å­¦å‡ºç‰ˆç‰©æ—¶éš¾ä»¥é«˜æ•ˆå®šä½å’Œç®¡ç†æ–‡çŒ®è¿™ä¸€éš¾é¢˜çš„æ™ºèƒ½å­¦æœ¯æ£€ç´¢ä¸æ–‡çŒ®ç®¡ç†å¹³å°ã€‚è¯¥å¹³å°é›†æˆäº†ä¸‰é¡¹æ ¸å¿ƒåŠŸèƒ½ï¼Œå…¶ä¸­ Scholar Search æä¾›å¿«é€Ÿçš„å…³é”®è¯æ£€ç´¢å’Œæ·±åº¦çš„ Agentic Search æ¨¡å¼ï¼Œå¤§å¹…æå‡äº†è®ºæ–‡å‘ç°çš„æ•ˆç‡ï¼›Library åŠŸèƒ½ä½œä¸ºå¯å®šåˆ¶çš„çŸ¥è¯†åº“åŠ©åŠ›ç³»ç»Ÿçš„æ–‡çŒ®ç»„ç»‡ï¼Œè€Œ AI Feeds åˆ™åŸºäºç”¨æˆ·å…´è¶£åˆ©ç”¨æ™ºèƒ½æ¨èç³»ç»Ÿè‡ªåŠ¨æ¨é€ç›¸å…³çš„æœ€æ–°å‡ºç‰ˆç‰©ã€‚ä¸ç°æœ‰çš„å­¦æœ¯å·¥å…·ä¸åŒï¼ŒWisPaper æä¾›äº†å°†æ–‡çŒ®å‘ç°ã€ç®¡ç†å’Œç ”ç©¶å‰æ²¿æŒç»­è¿½è¸ªæ— ç¼è¿æ¥çš„ Closed-loop Workflowã€‚ä½œä¸ºä¸€ä¸ªå¤šè¯­è¨€å’Œå¤šå­¦ç§‘ç³»ç»Ÿï¼ŒWisPaper æ˜¾è‘—å‡å°‘äº†ä¸åŒèƒŒæ™¯ç§‘ç ”äººå‘˜åœ¨è®ºæ–‡ç­›é€‰å’Œç®¡ç†ä¸ŠèŠ±è´¹çš„æ—¶é—´ã€‚è¯¥å¹³å°ç›®å‰å·²é¢å‘å…¬ä¼—å¼€æ”¾ï¼Œæ—¨åœ¨ä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„ç ”ç©¶äººå‘˜æä¾›æœ‰åŠ›æ”¯æŒï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿæ›´ä¸“æ³¨äºæ ¸å¿ƒç ”ç©¶æ´»åŠ¨ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "17 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06879v1",
      "published_date": "2025-12-07 15:10:20 UTC",
      "updated_date": "2025-12-07 15:10:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:02.674530+00:00"
    },
    {
      "arxiv_id": "2512.06867v1",
      "title": "Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?",
      "title_zh": "æ³¨å…¥äººæ ¼çš„å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦ä¼šå½±å“ç­–ç•¥æ¨ç†æ¸¸æˆä¸­çš„è¡¨ç°ï¼Ÿ",
      "authors": [
        "John Licato",
        "Stephen Steinle",
        "Brayden Hollis"
      ],
      "abstract": "Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­æ¤å…¥äººæ ¼æç¤º(persona prompting)æ˜¯å¦ä¼šå½±å“å…¶åœ¨å¯¹æŠ—æ€§ç­–ç•¥åšå¼ˆç¯å¢ƒä¸­çš„å†³ç­–è¡¨ç°ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨å¼€æºçš„å…¨çƒå¾æœæ£‹ç›˜æ¸¸æˆPERILï¼Œå¯¹æ¯”äº†åŸºäºäººæ ¼è¡ç”Ÿçš„å¯å‘å¼ç­–ç•¥(heuristic strategies)ä¸äººå·¥é€‰æ‹©ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œå…·æœ‰ç‰¹å®šâ€œç­–ç•¥æ€ç»´â€çš„äººæ ¼ç¡®å®èƒ½æå‡æ¸¸æˆè¡¨ç°ï¼Œä½†å‰ææ˜¯å¿…é¡»ä½¿ç”¨ä¸­ä»‹(mediator)å°†äººæ ¼ç‰¹å¾è½¬åŒ–ä¸ºå¯å‘å¼æ•°å€¼ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§å—æ¢ç´¢æ€§å› å­åˆ†æ(exploratory factor analysis)å¯å‘çš„æ–°å‹ä¸­ä»‹æœºåˆ¶ï¼Œé€šè¿‡ç»“æ„åŒ–ç¿»è¯‘è¿‡ç¨‹å°†LLMç”Ÿæˆçš„äººæ ¼å“åº”æ˜ å°„ä¸ºå…·ä½“å¯å‘å¼ã€‚ä¸ç›´æ¥æ¨æ–­çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†å¯å‘å¼ç”Ÿæˆçš„å¯é æ€§(reliability)ä¸è¡¨è±¡æ•ˆåº¦(face validity)ï¼Œä»è€Œæ›´æ·±å…¥åœ°æ­ç¤ºäº†äººæ ¼ç±»å‹å¯¹å†³ç­–çš„å½±å“ã€‚è¿™äº›å‘ç°ä¸ä»…åŠ æ·±äº†å¯¹äººæ ¼æç¤ºå¦‚ä½•å½±å“LLMå†³ç­–æœºåˆ¶çš„ç†è§£ï¼Œè¿˜æå‡ºäº†ä¸€ç§å°†å¿ƒç†æµ‹é‡å­¦åŸåˆ™(psychometric principles)åº”ç”¨äºLLMå¯å‘å¼ç”Ÿæˆçš„æ–°æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at IJCNLP-AACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.06867v1",
      "published_date": "2025-12-07 14:42:29 UTC",
      "updated_date": "2025-12-07 14:42:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:28.918859+00:00"
    },
    {
      "arxiv_id": "2512.06866v1",
      "title": "Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior",
      "title_zh": "å°‘å³æ˜¯å¤šï¼Œä½•å¤„å–èˆï¼ŸåŸºäº LLM å¼•å¯¼å…³é”®å¸§å…ˆéªŒçš„åŠ¨æ€ Token å‹ç¼©",
      "authors": [
        "Yulin Li",
        "Haokun Gui",
        "Ziyang Fan",
        "Junjie Wang",
        "Bin Kang",
        "Bin Chen",
        "Zhuotao Tian"
      ],
      "abstract": "Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ (Video Large Language Models, VLLMs) å¤„ç†é•¿è§†é¢‘æ—¶å› è§†è§‰ Token åºåˆ—è¿‡é•¿å¯¼è‡´çš„è®¡ç®—ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†åä¸º DyToK çš„åŠ¨æ€ Token å‹ç¼©èŒƒå¼ã€‚DyToK æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒ (training-free) çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ VLLMs å†…ç½®çš„æ³¨æ„åŠ›æœºåˆ¶ (attention mechanisms) æ¥æå–æŸ¥è¯¢ç›¸å…³çš„å…³é”®å¸§å…ˆéªŒ (keyframe priors)ã€‚è¯¥èŒƒå¼èƒ½å¤Ÿæ ¹æ®æ¯å¸§çš„è¯­ä¹‰é‡è¦æ€§åŠ¨æ€è°ƒæ•´ Token ä¿ç•™æ¯”ä¾‹ï¼Œä»è€Œåœ¨ä¼˜å…ˆä¿ç•™æ ¸å¿ƒä¿¡æ¯çš„åŒæ—¶å¤§å¹…æŠ‘åˆ¶æ•°æ®å†—ä½™ã€‚DyToK å…·å¤‡å³æ’å³ç”¨ (plug-and-play) çš„ç‰¹æ€§ï¼Œå¯ä¸ VisionZip å’Œ FastV ç­‰ç°æœ‰å‹ç¼©æŠ€æœ¯ç»“åˆä½¿ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ LLaVA-OneVision å’Œ Qwen2.5-VL ç­‰æ¨¡å‹ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå‡†ç¡®ç‡çš„å‰æä¸‹å®ç°äº† 4.3 å€çš„æ¨ç†åŠ é€Ÿï¼Œåœ¨æ•ˆç‡ä¸å‡†ç¡®ç‡æƒè¡¡ (efficiency-accuracy tradeoffs) æ–¹é¢è¾¾åˆ°äº† SOTA æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.06866v1",
      "published_date": "2025-12-07 14:42:10 UTC",
      "updated_date": "2025-12-07 14:42:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:10.857069+00:00"
    },
    {
      "arxiv_id": "2512.06859v1",
      "title": "JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models",
      "title_zh": "JT-DAï¼šåˆ©ç”¨é›†æˆå·¥å…·çš„è¡¨æ ¼æ¨ç†å¤§è¯­è¨€æ¨¡å‹å¢å¼ºæ•°æ®åˆ†æ",
      "authors": [
        "Ce Chi",
        "Xing Wang",
        "Zhendong Wang",
        "Xiaofan Liu",
        "Ce Li",
        "Zhiyan Song",
        "Chen Zhao",
        "Kexin Yang",
        "Boshen Shi",
        "Jingjing Yang",
        "Chao Deng",
        "Junlan Feng"
      ],
      "abstract": "In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† JT-DA-8Bï¼ˆJiuTian Data Analyst 8Bï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸ºå¤„ç†ç°å®åœºæ™¯ä¸­å¤æ‚è¡¨æ ¼æ¨ç†ï¼ˆTable Reasoningï¼‰ä»»åŠ¡è€Œè®¾è®¡çš„ä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¡¨æ ¼æ¨ç†é¢†åŸŸé«˜è´¨é‡ç›‘ç£æ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ•´åˆäº†29ä¸ªå…¬å¼€è¡¨æ ¼é—®ç­”æ•°æ®é›†å’Œ300ä¸‡å¼ è¡¨æ ¼ï¼Œæ„å»ºäº†åŒ…å«34é¡¹è¡¨æ ¼æ¨ç†ä»»åŠ¡çš„ç»¼åˆè®­ç»ƒè¯­æ–™åº“ã€‚è¯¥æ¨¡å‹åŸºäºä»é›¶è®­ç»ƒçš„ JT-Coder-8B åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ç”Ÿæˆæ¶‰åŠå¤šç§æ¨ç†æ¨¡å¼çš„å¤šæ­¥åˆ†æä»»åŠ¡ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œç ”ç©¶è€…åˆ©ç”¨åŸºäº LLM çš„è¯„åˆ†å’Œå·¥ä½œæµå¯¹é½è¿‡æ»¤æŠ€æœ¯æå–é«˜è´¨é‡æ•°æ®ï¼Œå¹¶ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºäº†åŒ…å«è¡¨æ ¼é¢„å¤„ç†ã€è¡¨æ ¼æ„ŸçŸ¥ã€å·¥å…·é›†æˆæ¨ç†ï¼ˆTool-integrated reasoningï¼‰åŠæç¤ºå·¥ç¨‹åœ¨å†…çš„å››é˜¶æ®µæ¨ç†å·¥ä½œæµï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJT-DA-8B åœ¨å¤šé¡¹è¡¨æ ¼æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå……åˆ†éªŒè¯äº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç”Ÿæˆç­–ç•¥å’Œå·¥ä½œæµé©±åŠ¨ä¼˜åŒ–æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06859v1",
      "published_date": "2025-12-07 14:29:23 UTC",
      "updated_date": "2025-12-07 14:29:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:11.637520+00:00"
    },
    {
      "arxiv_id": "2512.06854v1",
      "title": "ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design",
      "title_zh": "ArchPowerï¼šé¢å‘ç°ä»£ CPU è®¾è®¡çš„ä½“ç³»ç»“æ„çº§åŠŸè€—å»ºæ¨¡æ•°æ®é›†",
      "authors": [
        "Qijun Zhang",
        "Yao Lu",
        "Mengming Li",
        "Shang Liu",
        "Zhiyao Xie"
      ],
      "abstract": "Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ArchPowerï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ç°ä»£ CPU è®¾è®¡çš„æ¶æ„çº§åŠŸè€—å»ºæ¨¡å¼€æºæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³æ—©æœŸè®¾è®¡é˜¶æ®µä¼ ç»Ÿæ¨¡å‹ä¸å‡†ç¡®ä¸”æ•°æ®è·å–å‘¨æœŸé•¿çš„é—®é¢˜ã€‚é€šè¿‡å¤æ‚çš„é›†æˆç”µè·¯(IC)è®¾è®¡æµç¨‹ï¼Œè¯¥æ•°æ®é›†æ”¶é›†äº† 25 ç§ CPU é…ç½®åœ¨ä¸åŒè´Ÿè½½ä¸‹çš„ 200 ä¸ªæ ·æœ¬ï¼Œå¡«è¡¥äº†æœºå™¨å­¦ä¹ (ML)åº”ç”¨ä¸­ç¼ºä¹å¼€æºæ•°æ®æ”¯æ’‘çš„ç©ºç™½ã€‚æ¯ä¸ªæ ·æœ¬åŒ…å«è¶…è¿‡ 100 ä¸ªæ¶µç›–ç¡¬ä»¶å‚æ•°ä¸äº‹ä»¶å‚æ•°çš„æ¶æ„ç‰¹å¾ï¼Œå¹¶æä¾›ç»†ç²’åº¦çš„åŠŸè€—æ ‡ç­¾ï¼ŒåŒ…æ‹¬ 11 ä¸ªç»„ä»¶çš„åŠŸè€—ä»¥åŠç»†åˆ†çš„ç»„åˆé€»è¾‘ã€æ—¶åºé€»è¾‘ã€å­˜å‚¨å™¨å’Œæ—¶é’ŸåŠŸè€—ã€‚ArchPower ä¸ºç ”ç©¶äººå‘˜æä¾›äº†çœŸå®ä¸”è¯¦ç»†çš„ CPU åŠŸè€—è¯„ä¼°åŸºå‡†ï¼Œæœ‰åŠ©äºæ¨åŠ¨é«˜ç²¾åº¦ã€é«˜æ•ˆç‡çš„å¤„ç†å™¨è®¾è®¡ä¼˜åŒ–æŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "Published in NeurIPS'25 Dataset and Benchmark Track",
      "pdf_url": "https://arxiv.org/pdf/2512.06854v1",
      "published_date": "2025-12-07 14:12:06 UTC",
      "updated_date": "2025-12-07 14:12:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:22.377768+00:00"
    },
    {
      "arxiv_id": "2512.06850v1",
      "title": "Formal that \"Floats\" High: Formal Verification of Floating Point Arithmetic",
      "title_zh": "æµ®ç‚¹è¿ç®—çš„é«˜æ•ˆå½¢å¼åŒ–éªŒè¯",
      "authors": [
        "Hansa Mohanty",
        "Vaisakh Naduvodi Viswambharan",
        "Deepak Narayan Gadde"
      ],
      "abstract": "Formal verification of floating-point arithmetic remains challenging due to non-linear arithmetic behavior and the tight coupling between control and datapath logic. Existing approaches often rely on high-level C models for equivalence checking against Register Transfer Level (RTL) designs, but this introduces abstraction gaps, translation overhead, and limits scalability at the RTL level. To address these challenges, this paper presents a scalable methodology for verifying floating-point arithmetic using direct RTL-to-RTL model checking against a golden reference model. The approach adopts a divide-and conquer strategy that decomposes verification into modular stages, each captured by helper assertions and lemmas that collectively prove a main correctness theorem. Counterexample (CEX)-guided refinement is used to iteratively localize and resolve implementation defects, while targeted fault injection validates the robustness of the verification process against precision-critical datapath errors. To assess scalability and practicality, the methodology is extended with agentic AI-based formal property generation, integrating large language model (LLM)-driven automation with Human-in-the-Loop (HITL) refinement. Coverage analysis evaluates the effectiveness of the approach by comparing handwritten and AI-generated properties in both RTL-to-RTL model checking and standalone RTL verification settings. Results show that direct RTL-to-RTL model checking achieves higher coverage efficiency and requires fewer assertions than standalone verification, especially when combined with AI-generated properties refined through HITL guidance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ®ç‚¹è¿ç®—åœ¨ Formal verification ä¸­é¢ä¸´çš„éçº¿æ€§ç®—æœ¯è¡Œä¸ºå’Œæ§åˆ¶é€»è¾‘è€¦åˆæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç›´æ¥ RTL-to-RTL model checking çš„å¯æ‰©å±•éªŒè¯æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†è€Œæ²»ä¹‹(divide-and-conquer)çš„ç­–ç•¥ï¼Œé€šè¿‡æ¨¡å—åŒ–é˜¶æ®µçš„è¾…åŠ©æ–­è¨€å’Œå¼•ç†æ„å»ºä¸»æ­£ç¡®æ€§å®šç†ï¼Œå¹¶åˆ©ç”¨ Counterexample (CEX)-guided refinement è¿­ä»£å®šä½å®ç°ç¼ºé™·ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº† Agentic AI æŠ€æœ¯ï¼Œå°† LLM é©±åŠ¨çš„è‡ªåŠ¨åŒ–å±æ€§ç”Ÿæˆä¸ Human-in-the-Loop (HITL) ç»†åŒ–ç›¸ç»“åˆï¼Œä»¥æå‡éªŒè¯çš„è‡ªåŠ¨åŒ–ç¨‹åº¦ã€‚é€šè¿‡å®šå‘æ•…éšœæ³¨å…¥å’Œè¦†ç›–ç‡åˆ†æéªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ RTL-to-RTL éªŒè¯åœºæ™¯ä¸‹æ¯”ä¼ ç»Ÿçš„ standalone verification å…·æœ‰æ›´é«˜çš„è¦†ç›–æ•ˆç‡ä¸”éœ€è¦çš„æ–­è¨€æ›´å°‘ã€‚æœ€åï¼Œå®éªŒè¯æ˜ç»“åˆ AI ç”Ÿæˆå±æ€§ä¸äººå·¥å¹²é¢„çš„éªŒè¯æµç¨‹æ˜¾è‘—å¢å¼ºäº†å¤„ç†å¤æ‚æµ®ç‚¹ç¡¬ä»¶é€»è¾‘çš„æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LO",
      "comment": "To appear at the 37th IEEE International Conference on Microelectronics (ICM), December 14-17, 2025, Cairo, Egypt",
      "pdf_url": "https://arxiv.org/pdf/2512.06850v1",
      "published_date": "2025-12-07 14:03:44 UTC",
      "updated_date": "2025-12-07 14:03:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:34.776562+00:00"
    },
    {
      "arxiv_id": "2512.06836v1",
      "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ”¯æŒæ–‡æœ¬åŒ– DSL å®šä¹‰ä¸å®ä¾‹çš„ååŒæ¼”åŒ–",
      "authors": [
        "Weixing Zhang",
        "Regina Hebig",
        "Daniel StrÃ¼ber"
      ],
      "abstract": "Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Model)æ”¯æŒæ–‡æœ¬ç‰¹å®šé¢†åŸŸè¯­è¨€(textual DSLs)å®šä¹‰ä¸å®ä¾‹ä¹‹é—´ååŒæ¼”åŒ–(co-evolution)çš„æ½œåŠ›ã€‚é’ˆå¯¹ä¼ ç»Ÿæ¨¡å‹é©±åŠ¨å·¥ç¨‹(Model-Driven Engineering)æŠ€æœ¯åœ¨å¤„ç†æ–‡æœ¬è¯­æ³•æ—¶å®¹æ˜“ä¸¢å¤±æ³¨é‡Šå’Œå¸ƒå±€ç­‰è¾…åŠ©ä¿¡æ¯çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶é‡ç‚¹è¯„ä¼°äº† LLM åœ¨ç›´æ¥å¤„ç†æ–‡æœ¬å®ä¾‹æ—¶çš„è¿ç§»èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸ƒä¸ªæ¡ˆä¾‹è¯­è¨€ä¸Šæµ‹è¯• Claude-3.5 å’Œ GPT-4o ä¸¤ç§å…ˆè¿›æ¨¡å‹ï¼Œç»“æœè¡¨æ˜ LLM åœ¨å¤„ç†å°è§„æ¨¡å®ä¾‹è¿ç§»æ—¶è¡¨ç°è‰¯å¥½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™å¯¹è½¯ä»¶ç†è§£è‡³å…³é‡è¦çš„éç»“æ„åŒ–ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿå‘ç° LLM é©±åŠ¨çš„è§£å†³æ–¹æ¡ˆåœ¨é¢å¯¹å¤§è§„æ¨¡å®ä¾‹æ—¶å­˜åœ¨æ˜¾è‘—çš„æ‰©å±•æ€§(scalability)æŒ‘æˆ˜ï¼Œéš¾ä»¥ç›´æ¥åº”ç”¨äºå¤æ‚åœºæ™¯ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰ LLM åœ¨ DSL æ¼”åŒ–ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ä¸å±€é™ï¼Œä¸ºæœªæ¥ç»“åˆ LLM ä¼˜åŒ–é¢†åŸŸç‰¹å®šè¯­è¨€çš„ç»´æŠ¤æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06836v1",
      "published_date": "2025-12-07 13:17:37 UTC",
      "updated_date": "2025-12-07 13:17:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:22.900081+00:00"
    },
    {
      "arxiv_id": "2512.06835v1",
      "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning",
      "title_zh": "è§£è€¦ä»¥æ³›åŒ–ï¼šé¢å‘æ•°æ®ç¨€ç¼ºè§†è§‰è¯­è¨€æ¨ç†çš„è¯­å¢ƒä¼˜å…ˆè‡ªè¿›åŒ–å­¦ä¹ ",
      "authors": [
        "Tingyu Li",
        "Zheng Sun",
        "Jingxuan Wei",
        "Siyuan Li",
        "Conghui He",
        "Lijun Wu",
        "Cheng Tan"
      ],
      "abstract": "Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º DoGe (Decouple to Generalize) çš„åŒè§£è€¦æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨æ•°æ®ç¨€ç¼ºçš„ä¸“ä¸šé¢†åŸŸè¿›è¡Œå¼ºåŒ–å­¦ä¹  (RL) æ—¶é¢ä¸´çš„å¥–åŠ±æ“çºµ (reward hacking) å’Œè®­ç»ƒä¸ç¨³å®šæ€§ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†å­¦ä¹ è¿‡ç¨‹è§£è€¦ä¸º Thinker å’Œ Solver ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå¼•å¯¼æ¨¡å‹ä¼˜å…ˆä»é—®é¢˜èƒŒæ™¯ (context) ä¸­å­¦ä¹ è€Œéç›´æ¥æ±‚è§£ï¼Œä»è€Œæ›´åˆç†åœ°é‡åŒ–å¥–åŠ±ä¿¡å·ã€‚DoGe é‡‡ç”¨äº†ä¸€ç§ä»è‡ªç”±æ¢ç´¢ä¸Šä¸‹æ–‡åˆ°å®é™…ä»»åŠ¡è§£å†³çš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ åè®­ç»ƒ (post-training) æ–¹æ³•ï¼Œå¹¶é…åˆç”±é¢†åŸŸçŸ¥è¯†è¯­æ–™åº“ä¸è¿­ä»£è¿›åŒ–çš„ç§å­é—®é¢˜æ± æ„æˆçš„è¯¾ç¨‹å­¦ä¹  (curriculum learning) æµæ°´çº¿ä»¥å¢å¼ºæ•°æ®å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå®ç°å¤šæ¨¡æ€å¤§æ¨¡å‹ (LVLMs) çš„æŒç»­è‡ªæˆ‘è¿›åŒ– (self-evolving) æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06835v1",
      "published_date": "2025-12-07 13:17:31 UTC",
      "updated_date": "2025-12-07 13:17:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:29.871709+00:00"
    },
    {
      "arxiv_id": "2601.04205v1",
      "title": "STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models",
      "title_zh": "STDDï¼šæ‰©æ•£è¯­è¨€æ¨¡å‹ä¸­æ—¶ç©ºåŠ¨æ€é©±åŠ¨çš„ Token ç²¾ç»†åŒ–",
      "authors": [
        "Xinhao Sun",
        "Maoliang Li",
        "Zihao Zheng",
        "Jiayu Chen",
        "Hezhao Xu",
        "Yun Liang",
        "Xiang Chen"
      ],
      "abstract": "Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹(Diffusion Language Models, DLMs)ä¸­ä¼ ç»Ÿé‡æ©ç (remasking)ç­–ç•¥ä¾èµ–å•ä¸€å…¨å±€ç½®ä¿¡åº¦é˜ˆå€¼ã€å¿½è§†Tokenæ—¶ç©ºåŠ¨æ€çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºSTDDçš„æ–°å‹Tokenç²¾ç‚¼æ–¹æ³•ã€‚STDDé€šè¿‡åŠ¨æ€æ£€æµ‹æ¯ä¸ªTokençš„æ—¶é—´æ–¹å·®(Temporal Variance)å’Œç©ºé—´åç¦»(Spatial Deviance)æ¥å®æ—¶æ•æ‰å…¶æ”¶æ•›çŠ¶æ€ä¸Tokené—´çš„ç›¸å…³æ€§ã€‚åˆ©ç”¨è¿™äº›åŠ¨æ€ä¿¡å·ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ¯ä¸€æ­¥è¿­ä»£ä¸­ä¸ºæ¯ä¸ªTokenè‡ªé€‚åº”åœ°è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†å›ºå®šé˜ˆå€¼ç­–ç•¥å¯¼è‡´çš„å†—ä½™è¿­ä»£ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¹¶è¡Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTDDåœ¨ä¸»æµæ•°æ®é›†ä¸Šå¤§å¹…æé«˜äº†DLMsçš„è¿è¡Œé€Ÿåº¦ï¼Œåœ¨å¿ å®ä¿ç•™ç”Ÿæˆè´¨é‡çš„å‰æä¸‹å®ç°äº†æœ€é«˜8.9å€çš„åŠ é€Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.04205v1",
      "published_date": "2025-12-07 12:53:48 UTC",
      "updated_date": "2025-12-07 12:53:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:43.613302+00:00"
    },
    {
      "arxiv_id": "2601.04204v1",
      "title": "Generative Teaching via Code",
      "title_zh": "åŸºäºä»£ç çš„ç”Ÿæˆå¼æ•™å­¦",
      "authors": [
        "Yuheng Wang",
        "Runde Yang",
        "Lin Wu",
        "Jie Zhang",
        "Jingru Fan",
        "Ruoyu Fu",
        "Tianle Zhou",
        "Huatao Li",
        "Siheng Chen",
        "Weinan E",
        "Chen Qian"
      ],
      "abstract": "The scalability of high-quality online education is hindered by the high costs and slow cycles of labor-intensive manual content creation. Despite advancements in video generation, current approaches often fail to ensure pedagogical structure and precise control due to their pixel-level, black-box nature. In this paper, we propose Generative Teaching, a novel paradigm that transitions educators from manual creators to high-level directors, allowing them to focus on pedagogical intent while autonomous agents handle the execution. To realize this vision, we introduce TeachMaster, a multi-agent framework that leverages code as an intermediate semantic medium. Unlike traditional video generation methods, TeachMaster orchestrates a collaborative team of agents--spanning planning, design, and rendering--to automate the production of interpretable, editable, and curriculum-ready educational videos. Experiments validate that TeachMaster significantly boosts production efficiency without compromising structural coherence or visual fidelity, providing a robust solution for scalable education.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨çº¿æ•™è‚²ä¸­é«˜è´¨é‡å†…å®¹åˆ¶ä½œæˆæœ¬é«˜ã€å‘¨æœŸé•¿ï¼Œä»¥åŠä¼ ç»Ÿè§†é¢‘ç”ŸæˆæŠ€æœ¯ç”±äºå…¶åƒç´ çº§é»‘ç›’æ€§è´¨è€Œç¼ºä¹æ•™å­¦ç»“æ„å’Œç²¾å‡†æ§åˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº† Generative Teaching è¿™ä¸€æ–°èŒƒå¼ã€‚è¯¥èŒƒå¼æ—¨åœ¨å°†æ•™è‚²è€…ä»ç¹é‡çš„æ‰‹åŠ¨åˆ›ä½œä¸­è§£æ”¾å‡ºæ¥ï¼Œä½¿å…¶è½¬å‹ä¸ºä¾§é‡æ•™å­¦æ„å›¾çš„é«˜å±‚å¯¼æ¼”ï¼Œå¹¶ç”±è‡ªä¸»æ™ºèƒ½ä½“è´Ÿè´£å…·ä½“æ‰§è¡Œã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å¼•å…¥äº† TeachMaster å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ code ä½œä¸ºä¸­é—´è¯­ä¹‰åª’ä»‹ï¼Œé€šè¿‡ååŒè§„åˆ’ã€è®¾è®¡å’Œæ¸²æŸ“æ™ºèƒ½ä½“ï¼Œå®ç°äº†å¯è§£é‡Šã€å¯ç¼–è¾‘ä¸”ç¬¦åˆè¯¾ç¨‹è¦æ±‚çš„æ•™è‚²è§†é¢‘è‡ªåŠ¨åŒ–ç”Ÿäº§ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼ŒTeachMaster åœ¨æ˜¾è‘—æå‡åˆ¶ä½œæ•ˆç‡çš„åŒæ—¶ï¼Œç¡®ä¿äº†è§†é¢‘å†…å®¹çš„ç»“æ„è¿è´¯æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°å¤§è§„æ¨¡ã€å¯æ‰©å±•çš„åœ¨çº¿æ•™è‚²å†…å®¹ç”Ÿäº§æä¾›äº†ä¸€ç§ç¨³å¥ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.04204v1",
      "published_date": "2025-12-07 12:52:24 UTC",
      "updated_date": "2025-12-07 12:52:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:24:57.303754+00:00"
    },
    {
      "arxiv_id": "2512.06814v1",
      "title": "CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation",
      "title_zh": "CAuSEï¼šåˆ©ç”¨å¿ å®è‡ªç„¶è¯­è¨€è§£é‡Šå®ç°å¤šæ¨¡æ€åˆ†ç±»å™¨çš„è§£ç ",
      "authors": [
        "Dibyanayan Bandyopadhyay",
        "Soham Bhattacharjee",
        "Mohammed Hasanuzzaman",
        "Asif Ekbal"
      ],
      "abstract": "Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€åˆ†ç±»å™¨(Multimodal Classifiers)ä½œä¸ºé»‘ç›’æ¨¡å‹ç¼ºä¹ç›´è§‚è§£é‡Šçš„é—®é¢˜ï¼Œæå‡ºäº†CAuSE(Causal Abstraction under Simulated Explanations)æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºé¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆå…·æœ‰å¿ å®æ€§(Faithfulness)çš„è‡ªç„¶è¯­è¨€è§£é‡Š(NLEs)ã€‚CAuSEé€šè¿‡äº¤æ¢å¹²é¢„(Interchange Intervention)è¿›è¡Œè®­ç»ƒï¼Œåœ¨ç†è®ºä¸Šæ„æˆäº†åº•å±‚åˆ†ç±»å™¨çš„å› æœæŠ½è±¡(Causal Abstraction)ï¼Œç¡®ä¿ç”Ÿæˆçš„è§£é‡Šèƒ½çœŸå®åæ˜ æ¨¡å‹çš„å†…éƒ¨å†³ç­–é€»è¾‘ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜è®¾è®¡äº†ä¸€ç§ä¸“é—¨ç”¨äºè¡¡é‡å¤šæ¨¡æ€åœºæ™¯ä¸‹å› æœå¿ å®æ€§çš„æ”¹è¿›æŒ‡æ ‡ï¼Œå®éªŒè¯æ˜CAuSEåœ¨è¯¥æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè·¨æ•°æ®é›†å’Œæ¨¡å‹çš„å¹¿æ³›è¯„ä¼°éªŒè¯äº†è¯¥æ¡†æ¶ä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚å®šæ€§åˆ†æå’Œè¯¦ç»†çš„é”™è¯¯åˆ†æè¿›ä¸€æ­¥æ¢è®¨äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸å±€é™ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Transactions of the Association for Computational Linguistics (TACL). Pre-MIT Press publication version",
      "pdf_url": "https://arxiv.org/pdf/2512.06814v1",
      "published_date": "2025-12-07 12:15:21 UTC",
      "updated_date": "2025-12-07 12:15:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:25:12.370531+00:00"
    },
    {
      "arxiv_id": "2512.06813v2",
      "title": "Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation",
      "title_zh": "åŸºäºååŒç¥ç»ç½‘ç»œçš„é«˜æ€§èƒ½æ··å‡åœŸéƒ¨åˆ†é€†å‘è®¾è®¡ä¸çº¦æŸæ„ŸçŸ¥é…åˆæ¯”ç”Ÿæˆ",
      "authors": [
        "Agung Nugraha",
        "Heungjun Im",
        "Jihwan Lee"
      ],
      "abstract": "High-performance concrete requires complex mix design decisions involving interdependent variables and practical constraints. While data-driven methods have improved predictive modeling for forward design in concrete engineering, inverse design remains limited, especially when some variables are fixed and only the remaining ones must be inferred. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework integrates an imputation model with a surrogate strength predictor and learns through cooperative training. Once trained, it generates valid and performance-consistent mix designs in a single forward pass without retraining for different constraint scenarios. Compared with baseline models, including autoencoder models and Bayesian inference with Gaussian process surrogates, the proposed method achieves R-squared values of 0.87 to 0.92 and substantially reduces mean squared error by approximately 50% and 70%, respectively. The results show that the framework provides an accurate and computationally efficient foundation for constraint-aware, data-driven mix proportioning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä½œç¥ç»ç½‘ç»œ(Cooperative Neural Network)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜æ€§èƒ½æ··å‡åœŸé…åˆæ¯”è®¾è®¡ä¸­çš„éƒ¨åˆ†é€†å‘è®¾è®¡(Partial Inverse Design)éš¾é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“éƒ¨åˆ†å˜é‡å›ºå®šè€Œéœ€æ¨æ–­å…¶ä½™å˜é‡çš„æƒ…æ™¯ã€‚è¯¥æ¡†æ¶é›†æˆäº†å¡«è¡¥æ¨¡å‹(Imputation Model)ä¸ä»£ç†å¼ºåº¦é¢„æµ‹å™¨(Surrogate Strength Predictor)ï¼Œå¹¶é€šè¿‡åä½œè®­ç»ƒ(Cooperative Training)ååŒå·¥ä½œã€‚è¯¥ç³»ç»Ÿå…è®¸åœ¨çº¦æŸæ„ŸçŸ¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å•æ¬¡å‰å‘ä¼ é€’ç”Ÿæˆç¬¦åˆæ€§èƒ½è¦æ±‚çš„æœ‰æ•ˆé…åˆæ¯”ï¼Œæ— éœ€é’ˆå¯¹ä¸åŒåœºæ™¯é‡æ–°è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å®ç°äº†0.87è‡³0.92çš„R-squaredå€¼ï¼Œç›¸æ¯”è‡ªåŠ¨ç¼–ç å™¨(Autoencoder)å’Œå¸¦æœ‰é«˜æ–¯è¿‡ç¨‹ä»£ç†çš„è´å¶æ–¯æ¨ç†(Bayesian Inference with Gaussian Process Surrogates)ç­‰åŸºçº¿æ¨¡å‹ï¼Œå…¶å‡æ–¹è¯¯å·®(Mean Squared Error)åˆ†åˆ«é™ä½äº†çº¦50%å’Œ70%ã€‚è¿™é¡¹å·¥ä½œä¸ºæ•°æ®é©±åŠ¨çš„æ··å‡åœŸé…åˆæ¯”ä¼˜åŒ–æä¾›äº†ä¸€ä¸ªç²¾ç¡®ä¸”è®¡ç®—é«˜æ•ˆçš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 12 figures. Updated the abstract, Table 3, and bibliography. No changes to the main text or results",
      "pdf_url": "https://arxiv.org/pdf/2512.06813v2",
      "published_date": "2025-12-07 12:14:56 UTC",
      "updated_date": "2025-12-10 12:48:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:25:08.394649+00:00"
    },
    {
      "arxiv_id": "2512.06811v1",
      "title": "RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models",
      "title_zh": "RMAdapterï¼šé¢å‘è§†è§‰-è¯­è¨€æ¨¡å‹çš„åŸºäºé‡å»ºçš„å¤šæ¨¡æ€é€‚é…å™¨",
      "authors": [
        "Xiang Lin",
        "Weixin Li",
        "Shu Guo",
        "Lihong Wang",
        "Di Huang"
      ],
      "abstract": "Pre-trained Vision-Language Models (VLMs), \\textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RMAdapterï¼Œä¸€ç§åŸºäºé‡æ„çš„åŒåˆ†æ”¯å¤šæ¨¡æ€é€‚é…å™¨ï¼Œæ—¨åœ¨è§£å†³é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å°‘æ ·æœ¬(few-shot)åœºæ™¯ä¸‹éš¾ä»¥å¹³è¡¡ä»»åŠ¡ç‰¹å®šé€‚é…ä¸æ³›åŒ–æ€§èƒ½çš„é—®é¢˜ã€‚RMAdapteré‡‡ç”¨äº†ç‹¬ç‰¹çš„åŒåˆ†æ”¯æ¶æ„ï¼ŒåŒ…å«ä¸€ä¸ªç”¨äºæ³¨å…¥ä»»åŠ¡ç‰¹å®šçŸ¥è¯†çš„é€‚é…åˆ†æ”¯(adaptation branch)ï¼Œä»¥åŠä¸€ä¸ªé€šè¿‡å°†æ½œç©ºé—´ç‰¹å¾é‡æ„å›åŸå§‹ç‰¹å¾ç©ºé—´ä»¥ä¿ç•™é€šç”¨çŸ¥è¯†çš„é‡æ„åˆ†æ”¯(reconstruction branch)ã€‚ä¸ºäº†ä¿æŒæ¨¡å‹çš„è½»é‡åŒ–ï¼Œè¯¥æ¡†æ¶é€šè¿‡åœ¨å„å±‚è®¡ç®—å±€éƒ¨é‡æ„æŸå¤±å¹¶å…±äº«æŠ•å½±æ¨¡å—æ¥æœ€å°åŒ–è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶åˆ©ç”¨ä¸€è‡´æ€§çº¦æŸæ¥è°ƒèŠ‚åˆ¤åˆ«åŠ›ä¸æ³›åŒ–æ€§çš„å¹³è¡¡ã€‚åœ¨é’ˆå¯¹æ–°ç±»åˆ«æ³›åŒ–ã€è·¨æ•°æ®é›†æ³›åŒ–ä»¥åŠé¢†åŸŸæ³›åŒ–ç­‰ä»»åŠ¡çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼ŒRMAdapteråœ¨ä¸ä¾èµ–æ•°æ®å¢å¼ºæˆ–å¤æ‚æç¤ºè®¾è®¡çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½å§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†é€šè¿‡é‡æ„æœºåˆ¶æ˜¾å¼ä¿ç•™é€šç”¨çŸ¥è¯†å¯¹äºæå‡è§†è§‰è¯­è¨€æ¨¡å‹é€‚é…æ•ˆç‡çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2026(Oral)",
      "pdf_url": "https://arxiv.org/pdf/2512.06811v1",
      "published_date": "2025-12-07 12:04:46 UTC",
      "updated_date": "2025-12-07 12:04:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:25:05.002059+00:00"
    },
    {
      "arxiv_id": "2512.06797v1",
      "title": "Optimal and Diffusion Transports in Machine Learning",
      "title_zh": "æœºå™¨å­¦ä¹ ä¸­çš„æœ€ä¼˜ä¼ è¾“ä¸æ‰©æ•£ä¼ è¾“",
      "authors": [
        "Gabriel PeyrÃ©"
      ],
      "abstract": "Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†æœºå™¨å­¦ä¹ ä¸­å°†è¯¸å¤šé—®é¢˜ï¼ˆå¦‚é‡‡æ ·ã€ä¼˜åŒ–å’Œå¤§å‹è¯­è¨€æ¨¡å‹åˆ†æï¼‰å»ºæ¨¡ä¸ºéšæ—¶é—´æ¼”åŒ–çš„æ¦‚ç‡åˆ†å¸ƒçš„è®¾è®¡ä¸åˆ†ææ–¹æ³•ã€‚ç ”ç©¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¯†åº¦çš„ Eulerian è¡¨ç¤ºåˆ‡æ¢ä¸ºé€šè¿‡å‘é‡åœºå¹³æµç²’å­çš„ Lagrangian è¡¨ç¤ºï¼Œä»è€Œåˆ©ç”¨å‘é‡åœºçš„ç‰¹æ€§æ¥æ„å»ºå…·æœ‰è‰¯å¥½æ­£åˆ™æ€§ã€ç¨³å®šæ€§å’Œè®¡ç®—å¯è¡Œæ€§çš„å¯†åº¦æ¼”å˜è¿‡ç¨‹ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†ä¸¤ç§äº’è¡¥çš„æ–¹æ³•ï¼šåŸºäºéšæœºæ’å€¼è¿‡ç¨‹çš„ Diffusion methodsï¼Œä»¥åŠé€šè¿‡æœ€å°åŒ–ä½ç§»æˆæœ¬å®šä¹‰æ’å€¼çš„ Optimal Transportã€‚ä½œè€…å±•ç¤ºäº†è¿™ä¸¤ç§æ–¹æ³•å¦‚ä½•åº”ç”¨äºé‡‡æ ·ã€ç¥ç»ç½‘ç»œæƒé‡ä¼˜åŒ–ä»¥åŠå¤§æ¨¡å‹ä¸­ Transformers çš„åŠ¨åŠ›å­¦å»ºæ¨¡ã€‚é€šè¿‡è¿™ç§ç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œè¯¥ç ”ç©¶ä¸ºç†è§£ç°ä»£ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ Generative AI åŠå…¶åº•å±‚æœºåˆ¶æä¾›äº†ç³»ç»Ÿæ€§çš„è§†è§’ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "math.OC",
      "comment": "Proc. 2026 International Congress of Mathematicians",
      "pdf_url": "https://arxiv.org/pdf/2512.06797v1",
      "published_date": "2025-12-07 11:25:32 UTC",
      "updated_date": "2025-12-07 11:25:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:25:13.700085+00:00"
    },
    {
      "arxiv_id": "2512.06785v1",
      "title": "Angular Regularization for Positive-Unlabeled Learning on the Hypersphere",
      "title_zh": "è¶…çƒé¢ä¸Šæ­£ç±»-æ— æ ‡ç­¾ï¼ˆPUï¼‰å­¦ä¹ çš„è§’åº¦æ­£åˆ™åŒ–",
      "authors": [
        "Vasileios Sevetlidis",
        "George Pavlidis",
        "Antonios Gasteratos"
      ],
      "abstract": "Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ­£æ ·æœ¬-æ— æ ‡ç­¾å­¦ä¹ ï¼ˆPositive-Unlabeled Learning, PU Learningï¼‰ä¸­ç¼ºä¹æ˜¾å¼è´Ÿæ ·æœ¬ç›‘ç£çš„é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é«˜ç»´æ•°æ®å’Œåˆ†å¸ƒå‡è®¾æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† AngularPUï¼Œè¿™æ˜¯ä¸€ç§åœ¨å•ä½è¶…çƒé¢ä¸Šè¿è¡Œçš„æ–°å‹ PU å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCosine Similarityï¼‰å’Œè§’è¾¹è·ï¼ˆAngular Marginï¼‰è¿›è¡Œåˆ†ç±»ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„åŸå‹å‘é‡ï¼ˆPrototype Vectorï¼‰è¡¨ç¤ºæ­£ç±»ï¼Œå°†åˆ†ç±»ç®€åŒ–ä¸ºä½™å¼¦ç›¸ä¼¼åº¦çš„é˜ˆå€¼åˆ¤å®šï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹æ˜¾å¼è´Ÿæ ·æœ¬å»ºæ¨¡çš„éœ€æ±‚ã€‚ä¸ºäº†é˜²æ­¢æ— æ ‡ç­¾æ ·æœ¬åœ¨æ­£ç±»åŸå‹é™„è¿‘è¿‡åº¦èšé›†ï¼Œç ”ç©¶å¼•å…¥äº†è§’æ­£åˆ™é¡¹ï¼ˆAngular Regularizerï¼‰ä»¥ä¿ƒè¿›æ— æ ‡ç­¾é›†åœ¨è¶…çƒé¢ä¸Šçš„ç¦»æ•£åˆ†å¸ƒï¼Œè¿›è€Œæå‡ç±»åˆ«åˆ†ç¦»åº¦ã€‚ç ”ç©¶æä¾›äº†å…³äºè´å¶æ–¯æœ€ä¼˜æ€§ï¼ˆBayes-optimalityï¼‰å’ŒåŸå‹ä¸€è‡´æ€§çš„ç†è®ºä¿è¯ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯æ˜äº† AngularPU åœ¨æ­£æ ·æœ¬ç¨€ç¼ºå’Œé«˜ç»´åµŒå…¥åœºæ™¯ä¸‹çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰ SOTA æ–¹æ³•ï¼Œå¹¶å…·å¤‡è‰¯å¥½çš„å‡ ä½•å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Featured Certification, J2C Certification. Transactions on Machine Learning Research, 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.06785v1",
      "published_date": "2025-12-07 10:59:35 UTC",
      "updated_date": "2025-12-07 10:59:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:26:12.542626+00:00"
    },
    {
      "arxiv_id": "2512.14706v1",
      "title": "LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts",
      "title_zh": "LLM ä½œä¸ºç¥ç»æ¶æ„å¸ˆï¼šä¸¥æ ¼ API å¥‘çº¦ä¸‹çš„å›¾åƒæè¿°æ¨¡å‹å—æ§ç”Ÿæˆ",
      "authors": [
        "Krunal Jesani",
        "Dmitry Ignatov",
        "Radu Timofte"
      ],
      "abstract": "Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NN-Captionï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æµæ°´çº¿ï¼Œæ—¨åœ¨ä¸¥æ ¼çš„ Net API çº¦æŸä¸‹è‡ªåŠ¨ç”Ÿæˆå¯è¿è¡Œçš„å›¾åƒæè¿°ï¼ˆImage Captioningï¼‰æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»„åˆæ¥è‡ª LEMUR åˆ†ç±»éª¨å¹²ç½‘ç»œçš„ CNN ç¼–ç å™¨ä¸ LSTMã€GRU æˆ– Transformer ç­‰åºåˆ—è§£ç å™¨ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–æ¨¡å‹æ„å»ºã€‚ç ”ç©¶ä¸»è¦ä½¿ç”¨ DeepSeek-R1-0528-Qwen3-8B ä½œä¸ºæ¶æ„ç”Ÿæˆå™¨ï¼Œå¹¶æ¢è®¨äº†ä¸åŒæç¤ºè¯è®¾è®¡å¯¹ç”ŸæˆæˆåŠŸç‡çš„å½±å“ã€‚åœ¨ MS COCO æ•°æ®é›†çš„å®éªŒä¸­ï¼ŒLLM ç”Ÿæˆçš„æ•°åä¸ªæ¶æ„ä¸­æœ‰è¶…è¿‡åŠæ•°æˆåŠŸé€šè¿‡è®­ç»ƒå¹¶å±•ç°å‡ºæœ‰æ•ˆçš„ BLEU-4 æ€§èƒ½ï¼Œè¯æ˜äº† LLM åœ¨æ¶æ„è®¾è®¡ã€è¶…å‚æ•°å»ºè®®åŠè®­ç»ƒå®è·µæ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜åˆ†æå¹¶è§£å†³äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä»£ç å¹»è§‰ä¸ API åˆè§„æ€§é—®é¢˜ã€‚æœ€ç»ˆï¼Œè¯¥å·¥ä½œä¸ºå¼€æº LEMUR æ•°æ®é›†è´¡çŒ®äº†æ•°åç§æ–°å‹æ¨¡å‹ï¼Œä¸ºå¯å¤ç°çš„åŸºå‡†æµ‹è¯•å’Œä¸‹æ¸¸ AutoML ç ”ç©¶æä¾›äº†æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.14706v1",
      "published_date": "2025-12-07 10:47:28 UTC",
      "updated_date": "2025-12-07 10:47:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:25:21.511478+00:00"
    },
    {
      "arxiv_id": "2512.06781v3",
      "title": "From Description to Score: Can LLMs Quantify Vulnerabilities?",
      "title_zh": "ä»æè¿°åˆ°è¯„åˆ†ï¼šå¤§è¯­è¨€æ¨¡å‹èƒ½å¦å®ç°æ¼æ´é‡åŒ–ï¼Ÿ",
      "authors": [
        "Sima Jafarikhah",
        "Daniel Thompson",
        "Eva Deans",
        "Hossein Siadati",
        "Yi Liu"
      ],
      "abstract": "Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \\textit{Availability Impact}), while offering more modest gains on others (e.g., \\textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰ï¼Œå¦‚ ChatGPT, Llama, Grok, DeepSeek å’Œ Geminiï¼Œåœ¨è‡ªåŠ¨åŒ–é€šç”¨æ¼æ´è¯„åˆ†ç³»ç»Ÿï¼ˆCommon Vulnerability Scoring System, CVSSï¼‰è¯„åˆ†æ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹è¶…è¿‡ 31,000 ä¸ªæœ€è¿‘çš„é€šç”¨æ¼æ´æŠ«éœ²ï¼ˆCommon Vulnerabilities and Exposures, CVEï¼‰æ¡ç›®è¿›è¡Œåˆ†æï¼Œç ”ç©¶å‘ç° LLMs åœ¨æŸäº›æŒ‡æ ‡ï¼ˆå¦‚ Availability Impactï¼‰ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œä½†åœ¨å¦ä¸€äº›æŒ‡æ ‡ï¼ˆå¦‚ Attack Complexityï¼‰ä¸Šçš„æå‡åˆ™è¾ƒä¸ºæœ‰é™ã€‚åœ¨æ‰€è¯„ä¼°çš„æ¨¡å‹ä¸­ï¼ŒChatGPT-5 å±•ç°å‡ºäº†æœ€é«˜çš„ç²¾ç¡®åº¦ã€‚åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œç”±äº CVE æè¿°å¾€å¾€ç¼ºä¹å…³é”®ä¸Šä¸‹æ–‡æˆ–å­˜åœ¨æ­§ä¹‰ï¼Œå¯¼è‡´ LLMs åœ¨å¤„ç†ç‰¹å®šæ¼æ´æ—¶å‡ºç°ç³»ç»Ÿæ€§è¯¯åˆ¤ã€‚æ­¤å¤–ï¼Œé›†æˆå…ƒåˆ†ç±»å™¨ï¼ˆensemble-based meta-classifiersï¼‰å¯¹æ€§èƒ½çš„æå‡éå¸¸å¾®å¼±ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†æå‡æ¼æ´æè¿°è´¨é‡å’Œå¼•å…¥æ›´ä¸°å¯ŒèƒŒæ™¯ä¿¡æ¯çš„é‡è¦æ€§ï¼Œä»¥æ”¯æŒæ›´å¯é çš„è‡ªåŠ¨åŒ–æ¨ç†å¹¶æœ‰æ•ˆç¼“è§£æ—¥ç›Šç§¯å‹çš„ CVE å¾…å¤„ç†ä»»åŠ¡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.06781v3",
      "published_date": "2025-12-07 10:47:00 UTC",
      "updated_date": "2026-01-05 02:16:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:25:25.665850+00:00"
    },
    {
      "arxiv_id": "2512.06776v1",
      "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
      "title_zh": "ä» Next-Token åˆ° Next-Blockï¼šæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„ä¸€ç§åŸåˆ™æ€§é€‚é…è·¯å¾„",
      "authors": [
        "Yuchuan Tian",
        "Yuchen Liang",
        "Jiacheng Sun",
        "Shuo Zhang",
        "Guangwen Yang",
        "Yingte Shu",
        "Sibo Fang",
        "Tianyu Guo",
        "Kai Han",
        "Chao Xu",
        "Hanting Chen",
        "Xinghao Chen",
        "Yunhe Wang"
      ],
      "abstract": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’æ¨¡å‹(Autoregressive, AR)ä¸²è¡Œè§£ç å¯¼è‡´çš„ååé‡ç“¶é¢ˆï¼Œæå‡ºäº†å°†é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹é€‚é…ä¸ºå—æ‰©æ•£æ¨¡å‹(Block-Diffusion LLMs)çš„åŸåˆ™æ€§è·¯å¾„ã€‚ä½œè€…é€šè¿‡å°† AR è§†ä¸ºå—å¤§å°ä¸º1çš„ç‰¹æ®Šå—æ‰©æ•£å½¢å¼ï¼Œè®¾è®¡äº† NBDiff-7B æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†è‡ªå›å½’å› æœæ€§ä¸å—å†…åŒå‘æ€§ä¹‹é—´çš„å¤±é…é—®é¢˜ã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨äº†ä¸Šä¸‹æ–‡å› æœæ³¨æ„åŠ›æ©ç (context-causal attention mask)ã€é«˜æ•ˆå¹¶è¡Œé€‚é…ç¨‹åºåŠè¾…åŠ© AR æŸå¤±ï¼Œä¸ä»…æœ€å¤§åŒ–äº†æ•°æ®åˆ©ç”¨ç‡ï¼Œè¿˜å®Œæ•´ç»§æ‰¿äº†åŸæ¨¡å‹çš„é•¿æ–‡æœ¬å»ºæ¨¡ä¸æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒNBDiff-7B åœ¨é€šç”¨çŸ¥è¯†ã€æ•°å­¦å’Œä»£ç åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 7B çº§æ‰©æ•£æ¨¡å‹çš„é¢†å…ˆæ€§èƒ½ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¿™ç§ä» Next-Token å‘ Next-Block æ¼”è¿›çš„é€‚é…è·¯å¾„æ˜¯æ¯”ä»å¤´è®­ç»ƒæ‰©æ•£è¯­è¨€æ¨¡å‹æ›´é«˜æ•ˆä¸”ä½æˆæœ¬çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06776v1",
      "published_date": "2025-12-07 10:28:21 UTC",
      "updated_date": "2025-12-07 10:28:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:26:31.994921+00:00"
    },
    {
      "arxiv_id": "2512.06774v1",
      "title": "RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting",
      "title_zh": "RDSplatï¼šé’ˆå¯¹ 3D é«˜æ–¯æ³¼æº…çš„æŠ—æ‰©æ•£ç¼–è¾‘é²æ£’æ°´å°",
      "authors": [
        "Longjie Zhao",
        "Ziming Hong",
        "Zhenyang Ren",
        "Runnan Chen",
        "Mingming Gong",
        "Tongliang Liu"
      ],
      "abstract": "3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D Gaussian Splatting (3DGS) æ•°å­—åŒ–èµ„äº§åœ¨ç‰ˆæƒä¿æŠ¤ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† RDSplatï¼Œä¸€ç§èƒ½æœ‰æ•ˆæŠµå¾¡æ‰©æ•£ç¼–è¾‘ (diffusion-based editing) çš„é²æ£’æ°´å°æ¡†æ¶ã€‚RDSplat é€šè¿‡å°†æ°´å°åµŒå…¥åˆ°æ‰©æ•£ç¼–è¾‘å¤©ç„¶ä¿ç•™çš„ä½é¢‘é«˜æ–¯åˆ†å¸ƒ (low-frequency Gaussians) ä¸­ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•æ°´å°ä¿¡æ¯ææ˜“è¢«æ“¦é™¤çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸç”Ÿ 3DGS ç©ºé—´çš„å¤šåŸŸæ¶æ„ï¼Œç»“åˆååŒåæ–¹å·®æ­£åˆ™åŒ– (coordinated covariance regularization) ä¸ 2D æ»¤æ³¢æŠ€æœ¯ï¼Œå®ç°äº†æ°´å°çš„ç¨³å®šåµŒå…¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨é«˜æ–¯æ¨¡ç³Šæ¨¡æ‹Ÿæ‰©æ•£ç¼–è¾‘çš„ä½é€šæ»¤æ³¢è¡Œä¸ºï¼Œé€šè¿‡å¯¹æŠ—æ€§å¾®è°ƒ (adversarial fine-tuning) è¿›ä¸€æ­¥å¢å¼ºäº†æ°´å°çš„é˜²å¾¡èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRDSplat åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº† SOTA æ€§èƒ½ï¼Œåœ¨æ˜¾è‘—æå‡é²æ£’æ€§çš„åŒæ—¶ä¿æŒäº†æä½³çš„æ°´å°éšè”½æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06774v1",
      "published_date": "2025-12-07 10:26:35 UTC",
      "updated_date": "2025-12-07 10:26:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:25:32.212368+00:00"
    },
    {
      "arxiv_id": "2512.10988v1",
      "title": "Mathematics of natural intelligence",
      "title_zh": "è‡ªç„¶æ™ºèƒ½çš„æ•°å­¦",
      "authors": [
        "Evgenii Vityaev"
      ],
      "abstract": "In the process of evolution, the brain has achieved such perfection that artificial intelligence systems do not have and which needs its own mathematics. The concept of cognitome, introduced by the academician K.V. Anokhin, as the cognitive structure of the mind -- a high-order structure of the brain and a neural hypernetwork, is considered as the basis for modeling. Consciousness then is a special form of dynamics in this hypernetwork -- a large-scale integration of its cognitive elements. The cognitome, in turn, consists of interconnected COGs (cognitive groups of neurons) of two types -- functional systems and cellular ensembles. K.V. Anokhin sees the task of the fundamental theory of the brain and mind in describing these structures, their origin, functions and processes in them. The paper presents mathematical models of these structures based on new mathematical results, as well as models of different cognitive processes in terms of these models. In addition, it is shown that these models can be derived based on a fairly general principle of the brain works: \\textit{the brain discovers all possible causal relationships in the external world and draws all possible conclusions from them}. Based on these results, the paper presents models of: ``natural\" classification; theory of functional brain systems by P.K. Anokhin; prototypical theory of categorization by E. Roche; theory of causal models by Bob Rehter; theory of consciousness as integrated information by G. Tononi.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªç„¶æ™ºèƒ½çš„æ•°å­¦åŸºç¡€ï¼Œä»¥ K.V. Anokhin æå‡ºçš„ cognitome æ¦‚å¿µä¸ºå»ºæ¨¡åŸºç¡€ï¼Œå°†å…¶è§†ä¸ºå¤§è„‘çš„é«˜é˜¶è®¤çŸ¥ç»“æ„å’Œç¥ç»è¶…ç½‘ç»œ (neural hypernetwork)ã€‚ç ”ç©¶å°†æ„è¯†å®šä¹‰ä¸ºè¯¥è¶…ç½‘ç»œä¸­è®¤çŸ¥å…ƒç´ çš„å¤§è§„æ¨¡æ•´åˆåŠ¨æ€ï¼Œå¹¶æŒ‡å‡º cognitome ç”±åŠŸèƒ½ç³»ç»Ÿ (functional systems) å’Œç»†èƒç¾¤ (cellular ensembles) è¿™ä¸¤ç±»ç›¸äº’è¿æ¥çš„è®¤çŸ¥ç¥ç»å…ƒç»„ (COGs) ç»„æˆã€‚è®ºæ–‡åŸºäºæœ€æ–°çš„æ•°å­¦æˆæœæ„å»ºäº†è¿™äº›ç»“æ„çš„æ•°å­¦æ¨¡å‹ï¼Œå¹¶æ¨å¯¼å‡ºä¸€ä¸ªé€šç”¨çš„è„‘å·¥ä½œåŸç†ï¼Œå³å¤§è„‘é€šè¿‡å‘ç°å¤–éƒ¨ä¸–ç•Œçš„å› æœå…³ç³»å¹¶è¿›è¡Œæ¨ç†æ¥è¿ä½œã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨è‡ªç„¶åˆ†ç±»ã€å¤§è„‘åŠŸèƒ½ç³»ç»Ÿç†è®ºã€èŒƒç•´åŒ–åŸå‹ç†è®ºã€å› æœæ¨¡å‹ç†è®ºä»¥åŠé›†æˆä¿¡æ¯ç†è®º (integrated information theory) ä¸­çš„åº”ç”¨ã€‚è¯¥å·¥ä½œé€šè¿‡ä¸¥è°¨çš„æ•°å­¦æè¿°ï¼Œä¸ºç†è§£å¤§è„‘ä¸å¿ƒæ™ºçš„ç»“æ„ã€åŠŸèƒ½åŠå…¶æ¼”åŒ–è¿‡ç¨‹æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "18 pages, 4 figures, presented at the conference \"MathAI 2025 The International Conference dedicated to mathematics in artificial intelligence\"",
      "pdf_url": "https://arxiv.org/pdf/2512.10988v1",
      "published_date": "2025-12-07 10:15:00 UTC",
      "updated_date": "2025-12-07 10:15:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:26:45.666271+00:00"
    },
    {
      "arxiv_id": "2512.06769v1",
      "title": "Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding",
      "title_zh": "Stitch and Tellï¼šé¢å‘ç©ºé—´ç†è§£çš„ç»“æ„åŒ–å¤šæ¨¡æ€æ•°æ®å¢å¼ºæ–¹æ³•",
      "authors": [
        "Hang Yin",
        "Xiaomin He",
        "PeiWen Yuan",
        "Yiwei Li",
        "Jiayi Shi",
        "Wenxiao Fan",
        "Shaoxiong Feng",
        "Kan Li"
      ],
      "abstract": "Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\\text{MME}_{\\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models) åœ¨æè¿°ç‰©ä½“ç›¸å¯¹ä½ç½®æ—¶å­˜åœ¨çš„ç©ºé—´å¹»è§‰ (Spatial Hallucinations) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Stitch and Tell (SiTe) çš„ç®€å•ã€æ— éœ€æ ‡æ³¨ä¸”å³æ’å³ç”¨çš„å¤šæ¨¡æ€æ•°æ®å¢å¼ºæ–¹æ³•ã€‚SiTe é€šè¿‡æ²¿ç©ºé—´è½´æ‹¼æ¥å›¾åƒï¼Œå¹¶æ ¹æ®æ‹¼æ¥å¸ƒå±€è‡ªåŠ¨ç”Ÿæˆå…·æœ‰ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›çš„æè¿°æˆ–é—®ç­”å¯¹ï¼Œä»è€Œåœ¨è®­ç»ƒæ•°æ®ä¸­æ³¨å…¥ç»“æ„åŒ–çš„ç©ºé—´ç›‘ç£ã€‚å®éªŒåœ¨ LLaVA-v1.5-7Bã€LLaVA-Qwen2-1.5B å’Œ HALVA-7B ç­‰å¤šç§æ¶æ„ä»¥åŠå…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒSiTe åœ¨ MME_Position (+5.50%) å’Œ Spatial-MM (+4.19%) ç­‰ç©ºé—´ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨ COCO-QA å’Œ MMBench ç­‰é€šç”¨è§†è§‰è¯­è¨€åŸºå‡†ä¸Šä¹Ÿå®ç°äº†æ€§èƒ½å¢é•¿ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ˜¾å¼æ³¨å…¥ç©ºé—´æ„ŸçŸ¥ç»“æ„æ˜¯ç¼“è§£ç©ºé—´å¹»è§‰ã€æå‡æ¨¡å‹ç©ºé—´ç†è§£èƒ½åŠ›å¹¶ä¿æŒé€šç”¨èƒ½åŠ›çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06769v1",
      "published_date": "2025-12-07 10:07:59 UTC",
      "updated_date": "2025-12-07 10:07:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:26:49.095875+00:00"
    },
    {
      "arxiv_id": "2512.06759v1",
      "title": "VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors",
      "title_zh": "VisChainBenchï¼šè¶…è¶Šè¯­è¨€å…ˆéªŒçš„å¤šè½®å¤šå›¾åƒè§†è§‰æ¨ç†åŸºå‡†",
      "authors": [
        "Wenbo Lyu",
        "Yingjun Du",
        "Jinglin Zhao",
        "Xianton Zhen",
        "Ling Shao"
      ],
      "abstract": "Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VisChainBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šè½®ã€å¤šå›¾åœºæ™¯ä¸‹è¶…è¶Šè¯­è¨€å…ˆéªŒï¼ˆLanguage Priorsï¼‰æ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°å·¥å…·è¿‡åº¦ä¾èµ–è¯­è¨€æç¤ºè€Œå¿½è§†æ¸è¿›å¼è§†è§‰æ¨ç†çš„ç¼ºé™·ï¼ŒVisChainBench å¼ºè°ƒåœ¨æå°‘è¯­è¨€å¼•å¯¼ä¸‹æ‰§è¡Œå¤šæ­¥ã€äº’ç›¸å…³è”çš„è§†è§‰æ¨ç†ä»»åŠ¡ã€‚è¯¥åŸºå‡†æ¶µç›–äº†æ—¥å¸¸åœºæ™¯å’Œå·¥ç¨‹æ•…éšœæ’é™¤ç­‰ä¸‰å¤§é¢†åŸŸçš„ 1,457 ä¸ªä»»åŠ¡ï¼ŒåŒ…å«è¶…è¿‡ 20,000 å¼ å›¾åƒï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®çš„å†³ç­–è¿‡ç¨‹ã€‚é€šè¿‡åˆ›æ–°çš„å¤šæ™ºèƒ½ä½“ç”Ÿæˆç®¡çº¿ï¼ˆmulti-agent generation pipelineï¼‰ï¼Œè¯¥åŸºå‡†åœ¨ç¡®ä¿é«˜åº¦è§†è§‰å¤šæ ·æ€§çš„åŒæ—¶æœ‰æ•ˆå‡å°‘äº†è¯­è¨€åå·®ã€‚VisChainBench çš„æ¨å‡ºä¸ºæå‡æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è§†è§‰å¯¹è§†è§‰ï¼ˆvisual-to-visualï¼‰æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages,13figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06759v1",
      "published_date": "2025-12-07 09:48:10 UTC",
      "updated_date": "2025-12-07 09:48:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:26:52.529978+00:00"
    },
    {
      "arxiv_id": "2512.06751v1",
      "title": "Becoming Experienced Judges: Selective Test-Time Learning for Evaluators",
      "title_zh": "æˆä¸ºç»éªŒä¸°å¯Œçš„è¯„åˆ¤è€…ï¼šé¢å‘è¯„ä¼°å™¨çš„é€‰æ‹©æ€§æµ‹è¯•æ—¶å­¦ä¹ ",
      "authors": [
        "Seungyeon Jwa",
        "Daechul Ahn",
        "Reokyoung Kim",
        "Dongyeop Kang",
        "Jonghyun Choi"
      ],
      "abstract": "Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹LLM-as-a-judgeåœ¨è¯„ä¼°ä»»åŠ¡ä¸­é€šå¸¸å°†æ ·æœ¬è§†ä¸ºç‹¬ç«‹ä¸ªä½“ä¸”ä¾èµ–å›ºå®šPromptçš„é—®é¢˜ï¼Œæå‡ºäº†Learning While Evaluating (LWE)æ¡†æ¶ã€‚LWEå…è®¸è¯„ä¼°å™¨åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œé¡ºåºå­¦ä¹ ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒé›†æˆ–éªŒè¯é›†ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»´æŠ¤ä¸€ä¸ªåŠ¨æ€æ¼”è¿›çš„meta-promptï¼Œä¸ºç‰¹å®šæ ·æœ¬ç”Ÿæˆè¯„ä¼°æŒ‡ä»¤ï¼Œå¹¶åˆ©ç”¨è‡ªæˆ‘ç”Ÿæˆçš„åé¦ˆè¿›è¡Œè‡ªæˆ‘å®Œå–„ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†Selective LWEæ–¹æ³•ï¼Œä»…åœ¨è¯„ä¼°å™¨å‡ºç°è‡ªæˆ‘ä¸ä¸€è‡´(self-inconsistent)çš„æƒ…å†µä¸‹æ›´æ–°meta-promptï¼Œä»è€Œåœ¨ä¿æŒå­¦ä¹ æ•ˆæœçš„åŒæ—¶æ˜¾è‘—æå‡æˆæœ¬æ•ˆç›Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSelective LWEåœ¨ä¸¤é¡¹æˆå¯¹æ¯”è¾ƒåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜è¯„ä¼°å™¨å¯ä»¥åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­é€šè¿‡æœ‰é€‰æ‹©æ€§çš„æ›´æ–°ä¸æ–­è¿›æ­¥ï¼Œå¹¶èƒ½ä»å…¶éš¾ä»¥å¤„ç†çš„æ¡ˆä¾‹ä¸­å­¦ä¹ ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06751v1",
      "published_date": "2025-12-07 09:28:39 UTC",
      "updated_date": "2025-12-07 09:28:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:26:52.660613+00:00"
    },
    {
      "arxiv_id": "2512.06749v2",
      "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
      "title_zh": "DoVerï¼šå¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¹²é¢„é©±åŠ¨è‡ªåŠ¨è°ƒè¯•",
      "authors": [
        "Ming Ma",
        "Jue Zhang",
        "Fangkai Yang",
        "Yu Kang",
        "Qingwei Lin",
        "Tianming Yang",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "abstract": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DoVerï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLM) å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¹²é¢„é©±åŠ¨å‹è‡ªåŠ¨è°ƒè¯•æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºæ—¥å¿—çš„è°ƒè¯•æ–¹æ³•åœ¨å¤„ç†å¤æ‚äº¤äº’è·¯å¾„æ—¶å­˜åœ¨çš„éªŒè¯ç¼ºå¤±å’Œå½’å› ä¸å‡†ç­‰é—®é¢˜ã€‚DoVer é€šè¿‡å¼•å…¥é’ˆå¯¹æ€§å¹²é¢„ï¼ˆå¦‚ç¼–è¾‘æ¶ˆæ¯æˆ–ä¿®æ”¹è®¡åˆ’ï¼‰æ¥å¢å¼ºå‡è®¾ç”Ÿæˆé˜¶æ®µï¼Œå®ç°äº†å¯¹æ•…éšœå‡è®¾çš„ä¸»åŠ¨éªŒè¯ã€‚ä¸ä¼ ç»Ÿä¾§é‡äºæ•…éšœå½’å› å‡†ç¡®ç‡çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä»¥ç»“æœä¸ºå¯¼å‘çš„è§†è§’ï¼Œé‡ç‚¹è¡¡é‡ç³»ç»Ÿæ˜¯å¦èƒ½é€šè¿‡è°ƒè¯•ä¿®å¤æ•…éšœæˆ–å–å¾—å¯é‡åŒ–çš„ä»»åŠ¡è¿›å±•ã€‚åœ¨ Magnetic-One æ™ºèƒ½ä½“æ¡†æ¶ä»¥åŠ GAIA å’Œ AssistantBench æ•°æ®é›†çš„æµ‹è¯•ä¸­ï¼ŒDoVer æˆåŠŸå°† 18-28% çš„å¤±è´¥å°è¯•è½¬åŒ–ä¸ºæˆåŠŸï¼Œå¹¶å®ç°äº†é«˜è¾¾ 16% çš„é‡Œç¨‹ç¢‘å¼è¿›å±•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸéªŒè¯æˆ–æ¨ç¿» 30-60% çš„æ•…éšœå‡è®¾ï¼Œæ˜¾è‘—æå‡äº†è°ƒè¯•è¿‡ç¨‹çš„å¯ä¿¡åº¦ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥æ˜¾ç¤ºï¼ŒDoVer åœ¨ GSMPlus æ•°æ®é›†å’Œ AG2 æ¡†æ¶ä¸ŠåŒæ ·è¡¨ç°å‡ºè‰²ï¼Œæ¢å¤äº† 49% çš„å¤±è´¥ä»»åŠ¡ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†å¹²é¢„æœºåˆ¶æ˜¯æé«˜æ™ºèƒ½ä½“ç³»ç»Ÿå¯é æ€§çš„åˆ‡å®æ‰‹æ®µï¼Œä¸ºæ„å»ºæ›´ç¨³å¥ã€å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“è°ƒè¯•æ–¹æ³•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06749v2",
      "published_date": "2025-12-07 09:23:48 UTC",
      "updated_date": "2025-12-09 13:22:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:26:58.607127+00:00"
    },
    {
      "arxiv_id": "2512.06747v1",
      "title": "PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance",
      "title_zh": "PrivLLMSwarmï¼šé¢å‘å®‰å…¨ç‰©è”ç½‘ç›‘æ§çš„éšç§ä¿æŠ¤å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æ— äººæœºé›†ç¾¤",
      "authors": [
        "Jifar Wakuma Ayana",
        "Huang Qiming"
      ],
      "abstract": "Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PrivLLMSwarmï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ— äººæœºï¼ˆUAVï¼‰ç¾¤åœ¨ç‰©è”ç½‘ï¼ˆIoTï¼‰ç¯å¢ƒä¸‹åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œåä½œæ—¶äº§ç”Ÿçš„éšç§ä¸å®‰å…¨é£é™©è€Œè®¾è®¡çš„éšç§ä¿æŠ¤æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å®‰å…¨å¤šæ–¹è®¡ç®—ï¼ˆSecure Multi-Party Computation, MPCï¼‰å®ç°äº† LLM çš„å®‰å…¨æ¨ç†ï¼Œç¡®ä¿äº†æ•æ„Ÿä½œæˆ˜æ•°æ®çš„æœºå¯†æ€§ã€‚PrivLLMSwarm å¼•å…¥äº†ç»è¿‡ MPC ä¼˜åŒ–çš„ Transformer ç»„ä»¶å’Œé«˜æ•ˆçš„éçº¿æ€§æ¿€æ´»å‡½æ•°è¿‘ä¼¼æŠ€æœ¯ï¼Œä½¿å¾—åœ¨èµ„æºå—é™çš„èˆªç©ºå¹³å°ä¸Šè¿›è¡ŒåŠ å¯†æ¨ç†æˆä¸ºå¯èƒ½ã€‚ç³»ç»Ÿè¿˜åŒ…å«ä¸€ä¸ªç»è¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰å¾®è°ƒçš„ GPT æŒ‡ä»¤ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿåœ¨ä¿éšœéšç§çš„å‰æä¸‹ç”Ÿæˆå¯é çš„è¡ŒåŠ¨æŒ‡ä»¤ã€‚åœ¨åŸå¸‚è§„æ¨¡ä»¿çœŸä¸­çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPrivLLMSwarm å®ç°äº†é«˜è¯­ä¹‰å‡†ç¡®æ€§ã€ä½åŠ å¯†æ¨ç†å»¶è¿Ÿä»¥åŠç¨³å¥çš„ç¼–é˜Ÿæ§åˆ¶ã€‚ç›¸æ¯”äºå·®åˆ†éšç§ï¼ˆDifferential Privacyï¼‰å’Œè”é‚¦å­¦ä¹ ï¼ˆFederated Learningï¼‰ç­‰æ–¹æ³•ï¼Œè¯¥æ¡†æ¶åœ¨éšç§ä¸æ•ˆç”¨çš„å¹³è¡¡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™ä¸€æˆæœä¸ºæ™ºèƒ½åŸå¸‚ç›‘æ§å’Œåº”æ€¥å“åº”ç­‰éšç§æ•æ„Ÿå‹ IoT åœºæ™¯ä¸­éƒ¨ç½² LLM é©±åŠ¨çš„ UAV ç¾¤å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06747v1",
      "published_date": "2025-12-07 09:20:14 UTC",
      "updated_date": "2025-12-07 09:20:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:09.444847+00:00"
    },
    {
      "arxiv_id": "2512.06746v1",
      "title": "Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection",
      "title_zh": "ä»»åŠ¡-æ¨¡å‹å¯¹é½ï¼šå®ç°å¯æ³›åŒ– AI ç”Ÿæˆå›¾åƒæ£€æµ‹çš„ç®€æ·è·¯å¾„",
      "authors": [
        "Ruoxin Chen",
        "Jiahui Gao",
        "Kaiqing Lin",
        "Keyue Zhang",
        "Yandan Zhao",
        "Isabel Guan",
        "Taiping Yao",
        "Shouhong Ding"
      ],
      "abstract": "Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨ç”Ÿæˆå›¾åƒæ£€æµ‹ (AIGI detection) ä¸­é¢ä¸´çš„èµ„æºæ¶ˆè€—å¤§åŠå¹»è§‰ä¸¥é‡ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä»»åŠ¡-æ¨¡å‹å¯¹é½ (Task-Model Alignment) åŸåˆ™ã€‚ä½œè€…é€šè¿‡å®è¯åˆ†æå‘ç°ï¼ŒVLMs æ“…é•¿é«˜å±‚è¯­ä¹‰åˆ¤åˆ« (semantic discrimination) ä½†å¯¹åº•å±‚åƒç´ ä¼ªå½± (pixel artifacts) ç¼ºä¹æ•æ„Ÿæ€§ï¼Œè€Œä¼ ç»Ÿæ£€æµ‹å™¨åˆ™ä¸ä¹‹è¡¨ç°ç›¸åã€‚è¯¥è®ºæ–‡å°† AIGI æ£€æµ‹å½¢å¼åŒ–ä¸ºè¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥å’Œåƒç´ ä¼ªå½±æ£€æµ‹ä¸¤ä¸ªäº’è¡¥ä»»åŠ¡ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†åŒåˆ†æ”¯æ£€æµ‹å™¨ AlignGeminiã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸€ä¸ªæ¥å—çº¯è¯­ä¹‰ç›‘ç£å¾®è°ƒçš„ VLM åˆ†æ”¯å’Œä¸€ä¸ªä¸“æ³¨äºåƒç´ ä¼ªå½±çš„ä¸“å®¶åˆ†æ”¯ï¼Œé€šè¿‡åœ¨ç®€åŒ–æ•°æ®é›†ä¸Šå®æ–½æ­£äº¤ç›‘ç£ï¼Œä½¿å„åˆ†æ”¯èƒ½å¤Ÿé’ˆå¯¹æ€§åœ°å‘æŒ¥å…¶æ¶æ„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlignGemini åœ¨äº”ä¸ªçœŸå®åœºæ™¯åŸºå‡†æµ‹è¯•ä¸­å¹³å‡å‡†ç¡®ç‡æå‡äº† 9.5%ï¼Œè¯æ˜äº†ä»»åŠ¡-æ¨¡å‹å¯¹é½æ˜¯å®ç°é€šç”¨ä¸”é²æ£’çš„ AI ç”Ÿæˆå›¾åƒæ£€æµ‹çš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06746v1",
      "published_date": "2025-12-07 09:19:00 UTC",
      "updated_date": "2025-12-07 09:19:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:22.947416+00:00"
    },
    {
      "arxiv_id": "2512.06737v2",
      "title": "Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics",
      "title_zh": "Arc æ¢¯åº¦ä¸‹é™ï¼šä¸€ç§å…·å¤‡é˜¶æ®µæ„ŸçŸ¥ä¸ç”¨æˆ·å¯æ§æ­¥é•¿åŠ¨æ€çš„æ¢¯åº¦ä¸‹é™æ•°å­¦é‡æ„",
      "authors": [
        "Nikhil Verma",
        "Joonas Linnosmaa",
        "Leonardo Espinosa-Leal",
        "Napat Vajragupta"
      ],
      "abstract": "The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved superior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a limiting variant of ArcGD can be interpreted as a sign-based momentum-like update, highlighting conceptual connections between the inherent mechanisms of ArcGD and the Lion optimiser.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ArcGD (Arc Gradient Descent) ä¼˜åŒ–å™¨ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ•°å­¦æ¨å¯¼çš„æ¢¯åº¦ä¸‹é™é‡æ„æ–¹æ¡ˆï¼Œå…·æœ‰ç›¸ä½æ„ŸçŸ¥ (Phase-Aware) å’Œç”¨æˆ·å¯æ§çš„æ­¥é•¿åŠ¨æ€ç‰¹æ€§ã€‚ç ”ç©¶é¦–å…ˆåœ¨å…·æœ‰é«˜åº¦éå‡¸æ€§çš„ Rosenbrock å‡½æ•°éšæœºå˜ä½“ä¸Šè¿›è¡Œäº†å‹åŠ›æµ‹è¯•ï¼Œæ¶µç›–äº†ä» 2ç»´åˆ° 50,000ç»´çš„æç«¯åœºæ™¯ï¼Œè¯æ˜äº†å…¶åœ¨å¯»æ‰¾ä¼˜è§£æ–¹é¢æ¯” Adam ä¼˜åŒ–å™¨æ›´å…·ä¼˜åŠ¿ã€‚éšååœ¨ CIFAR-10 å›¾åƒåˆ†ç±»ä»»åŠ¡çš„å¤šæ ·åŒ– MLP æ¶æ„è¯„ä¼°ä¸­ï¼ŒArcGD è¾¾åˆ°äº† 50.7% çš„æœ€é«˜å¹³å‡æµ‹è¯•å‡†ç¡®ç‡ï¼Œä¼˜äº Adam, AdamW, Lion å’Œ SGD ç­‰ä¸»æµç®—æ³•ã€‚å®éªŒå‘ç° ArcGD åœ¨é•¿æœŸè®­ç»ƒä¸­å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’ŒæŠ—è¿‡æ‹Ÿåˆç‰¹æ€§ï¼Œæœ‰æ•ˆè§£å†³äº† Adam ç­‰æ¨¡å‹åœ¨è®­ç»ƒåæœŸå¯èƒ½å‡ºç°çš„æ€§èƒ½å›é€€é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº† ArcGD çš„ä¸€ç§æé™å˜ä½“å¯è§£é‡Šä¸ºåŸºäºç¬¦å·çš„ç±»åŠ¨é‡æ›´æ–°ï¼Œé˜æ˜äº†å…¶ä¸ Lion ä¼˜åŒ–å™¨åœ¨å†…åœ¨æœºåˆ¶ä¸Šçš„æ¦‚å¿µè”ç³»ï¼ŒéªŒè¯äº†è¯¥ç®—æ³•åœ¨å‡ ä½•å‹åŠ›æµ‹è¯•å’Œæ ‡å‡†æ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "80 pages, 6 tables, 2 figures, 5 appendices, proof-of-concept",
      "pdf_url": "https://arxiv.org/pdf/2512.06737v2",
      "published_date": "2025-12-07 09:03:45 UTC",
      "updated_date": "2025-12-20 21:12:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:27.093444+00:00"
    },
    {
      "arxiv_id": "2512.06734v1",
      "title": "A Patient-Doctor-NLP-System to contest inequality for less privileged",
      "title_zh": "æ—¨åœ¨æ¶ˆé™¤å¼±åŠ¿ç¾¤ä½“ä¸å¹³ç­‰é—®é¢˜çš„åŒ»æ‚£ NLP ç³»ç»Ÿ",
      "authors": [
        "Subrit Dikshit",
        "Ritu Tiwari",
        "Priyank Jain"
      ],
      "abstract": "Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èµ„æºå—é™çš„å†œæ‘åŒ»ç–—ç¯å¢ƒä»¥åŠå°åœ°è¯­(Hindi)ç­‰ä½èµ„æºè¯­è¨€èƒŒæ™¯ä¸‹çš„åŒ»ç–—è¾…åŠ©éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations)çš„ç´§å‡‘å‹Transformeræ¶æ„ã€‚è¯¥æ¨¡å‹é€šè¿‡é›†æˆæ¨¡å‹è’¸é¦(Model Distillation)ã€é¢‘åŸŸè°ƒåˆ¶(Frequency-domain Modulation)ã€é›†æˆå­¦ä¹ (Ensemble Learning)å’Œéšæœºæ¿€æ´»æ¨¡å¼(Randomized Activation Patterns)ï¼Œåœ¨å¤§å¹…é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ç¡®ä¿äº†é«˜æ•ˆçš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨é’ˆå¯¹ä½èµ„æºè¯­è¨€å’Œè§†éšœç”¨æˆ·å®šåˆ¶çš„åŒ»ç–—é—®ç­”åŠå’¨è¯¢æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸ä¸»æµNLPåŸºçº¿æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPDFTEMRAåœ¨èµ„æºæ¶ˆè€—æ˜¾è‘—å‡å°‘çš„å‰æä¸‹ï¼Œè¾¾åˆ°äº†ä¸å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½è¡¨ç°ã€‚è¿™ä¸€æˆæœè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨æ¨åŠ¨æ™®æƒ åŒ»ç–—å’Œæ— éšœç¢NLPåº”ç”¨æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºè§£å†³æ¬ å‘è¾¾åœ°åŒºçš„åŒ»ç–—èµ„æºåˆ†é…ä¸å‡æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06734v1",
      "published_date": "2025-12-07 08:59:15 UTC",
      "updated_date": "2025-12-07 08:59:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:11.013049+00:00"
    },
    {
      "arxiv_id": "2512.06732v1",
      "title": "\"The Dentist is an involved parent, the bartender is not\": Revealing Implicit Biases in QA with Implicit BBQ",
      "title_zh": "â€œç‰™åŒ»æ˜¯å°½è´£çš„å®¶é•¿ï¼Œè€Œé…’ä¿ä¸æ˜¯â€ï¼šé€šè¿‡ Implicit BBQ æ­ç¤ºé—®ç­”ç³»ç»Ÿä¸­çš„éšæ€§åè§",
      "authors": [
        "Aarushi Wagh",
        "Saniya Srivastava"
      ],
      "abstract": "Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the \"sexual orientation\" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„è¯­è¨€æ¨¡å‹(LLMs)åè§è¯„ä¼°åŸºå‡†ä¸»è¦ä¾èµ–æ˜¾å¼çº¿ç´¢(explicit cues)ï¼Œå¿½ç•¥äº†ç°å®äº’åŠ¨ä¸­é€šè¿‡å§“åã€æ–‡åŒ–æš—ç¤ºæˆ–ç‰¹å¾æ¨æ–­å‡ºçš„éšå¼åè§(implicit biases)ã€‚ä¸ºå¡«è¡¥è¿™ä¸€å…¬å¹³æ€§è¯„ä¼°çš„ç©ºç™½ï¼Œä½œè€…æå‡ºäº†ImplicitBBQåŸºå‡†ï¼Œå®ƒæ˜¯å¯¹åŸæœ‰Bias Benchmark for QA (BBQ)çš„æ‰©å±•ï¼Œæ¶µç›–äº†6ä¸ªç±»åˆ«çš„éšå¼æç¤ºå—ä¿æŠ¤å±æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ImplicitBBQå¯¹GPT-4oè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å…¶åœ¨å¤„ç†éšå¼æç¤ºæ—¶è¡¨ç°å‡ºä»¤äººæ‹…å¿§çš„æ€§èƒ½å·®å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸æ˜¾å¼åŸºå‡†ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨â€œæ€§å–å‘(sexual orientation)â€å­ç±»åˆ«ä¸­çš„å‡†ç¡®ç‡ä¸‹é™äº†é«˜è¾¾7%ï¼Œä¸”åœ¨å¤§å¤šæ•°å…¶ä»–ç±»åˆ«ä¸­ä¹Ÿå‘ˆç°å‡ºä¸€è‡´çš„ä¸‹é™è¶‹åŠ¿ã€‚è¿™è¡¨æ˜å½“å‰çš„è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨ç€æ˜¾å¼åŸºå‡†æ— æ³•æ£€æµ‹åˆ°çš„éšå¼åè§ï¼Œè€ŒImplicitBBQä¸ºè‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸæä¾›äº†ä¸€ä¸ªè¿›è¡Œç»†è‡´å…¬å¹³æ€§è¯„ä¼°çš„å…³é”®å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06732v1",
      "published_date": "2025-12-07 08:57:27 UTC",
      "updated_date": "2025-12-07 08:57:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:14.001866+00:00"
    },
    {
      "arxiv_id": "2512.06726v1",
      "title": "The Role of Entropy in Visual Grounding: Analysis and Optimization",
      "title_zh": "ç†µåœ¨è§†è§‰æ¥åœ°ä¸­çš„ä½œç”¨ï¼šåˆ†æä¸ä¼˜åŒ–",
      "authors": [
        "Shuo Li",
        "Jiajun Sun",
        "Zhihao Zhang",
        "Xiaoran Fan",
        "Senjie Jin",
        "Hui Li",
        "Yuming Yang",
        "Junjie Ye",
        "Lixing Shen",
        "Tao Ji",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç†µ(Entropy)åœ¨è§†è§‰æ¥åœ°(visual grounding)è¿™ä¸€é¢å‘æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„ä½œç”¨åŠå…¶ç‰¹å¾ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­ç†µæ§åˆ¶ç ”ç©¶ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¯¹æ¯”æ¨ç†ä»»åŠ¡ï¼Œä½œè€…è¯¦ç»†åˆ†æäº†è§†è§‰æ¥åœ°ä¸­ç†µçš„ç‹¬ç‰¹æ€§ï¼Œå¹¶æ®æ­¤æå‡ºäº†å¯è§£é‡Šçš„ç†µè°ƒèŠ‚ç®—æ³•ECVGPO (Entropy Control Visual Grounding Policy Optimization)ã€‚è¯¥ç®—æ³•é€šè¿‡æœ‰æ•ˆçš„ç†µæ§åˆ¶ï¼Œä¼˜åŒ–äº†æ¢ç´¢(exploration)ä¸åˆ©ç”¨(exploitation)ä¹‹é—´çš„å¹³è¡¡ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒECVGPOåœ¨å¤šç§åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹æ¶æ„ä¸‹å‡å–å¾—äº†å¹¿æ³›çš„æ€§èƒ½æå‡ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†é’ˆå¯¹æ„ŸçŸ¥ä»»åŠ¡è¿›è¡Œç‰¹å®šç†µä¼˜åŒ–çš„é‡è¦æ€§ï¼Œä¸ºæå‡å¤šæ¨¡æ€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†æœ‰æ•ˆçš„ç†è®ºæ”¯æ’‘ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06726v1",
      "published_date": "2025-12-07 08:33:55 UTC",
      "updated_date": "2025-12-07 08:33:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:41.780606+00:00"
    },
    {
      "arxiv_id": "2512.06721v1",
      "title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems",
      "title_zh": "ProAgentï¼šåˆ©ç”¨æŒ‰éœ€æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„ä¸»åŠ¨å¼å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Bufang Yang",
        "Lilin Xu",
        "Liekang Zeng",
        "Yunqi Guo",
        "Siyang Jiang",
        "Wenrui Lu",
        "Kaiwei Liu",
        "Hancheng Xiang",
        "Xiaofan Jiang",
        "Guoliang Xing",
        "Zhenyu Yan"
      ],
      "abstract": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ProAgentï¼Œè¿™æ˜¯é¦–ä¸ªåˆ©ç”¨å¤§è§„æ¨¡æ„Ÿå®˜ä¸Šä¸‹æ–‡å’ŒLLMæ¨ç†æä¾›ä¸»åŠ¨æœåŠ¡çš„ç«¯åˆ°ç«¯ä¸»åŠ¨æ™ºèƒ½ä½“ç³»ç»Ÿã€‚ProAgenté’ˆå¯¹ç°æœ‰LLMæ™ºèƒ½ä½“è¿‡äºä¾èµ–ç”¨æˆ·æŒ‡ä»¤çš„ååº”å¼(reactive)å±€é™ï¼Œé‡‡ç”¨äº†ä¸€ç§ç»“åˆæŒ‰éœ€åˆ†å±‚æ„ŸçŸ¥(on-demand tiered perception)çš„æå–æ–¹æ³•ï¼Œä»¥æŒç»­æ„ŸçŸ¥ç¯å¢ƒå¹¶æ¨å¯¼å‡ºåŒ…å«æ„Ÿå®˜å’Œç”»åƒ(persona)çº¿ç´¢çš„å±‚æ¬¡åŒ–ä¸Šä¸‹æ–‡ã€‚é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä¸»åŠ¨æ¨ç†å™¨(context-aware proactive reasoner)ï¼Œè¯¥ç³»ç»Ÿèƒ½å°†å¤æ‚ç¯å¢ƒä¿¡æ¯æ˜ å°„ä¸ºç”¨æˆ·éœ€æ±‚å’Œå·¥å…·è°ƒç”¨ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å¢å¼ºç°å®(AR)çœ¼é•œå’Œè¾¹ç¼˜æœåŠ¡å™¨ä¸Šå®ç°äº†ProAgentï¼Œå¹¶åœ¨çœŸå®æµ‹è¯•å¹³å°å’Œå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProAgentçš„ä¸»åŠ¨é¢„æµ‹å‡†ç¡®ç‡æ¯”åŸºçº¿æ¨¡å‹æå‡äº†33.4%ï¼Œå·¥å…·è°ƒç”¨F1åˆ†æ•°æé«˜äº†16.8%ï¼Œæ˜¾è‘—æå‡äº†ç”¨æˆ·æ»¡æ„åº¦ï¼Œä¸ºæœªæ¥ä¸»åŠ¨å¼åŠ©æ‰‹çš„å‘å±•è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06721v1",
      "published_date": "2025-12-07 08:21:07 UTC",
      "updated_date": "2025-12-07 08:21:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:50.540601+00:00"
    },
    {
      "arxiv_id": "2512.06716v2",
      "title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents",
      "title_zh": "Cognitive Control Architecture (CCA)ï¼šé¢å‘é²æ£’å¯¹é½ AI æ™ºèƒ½ä½“çš„å…¨ç”Ÿå‘½å‘¨æœŸç›‘ç®¡æ¡†æ¶",
      "authors": [
        "Zhibo Liang",
        "Tianze Hu",
        "Zaiye Chen",
        "Mingjie Tang"
      ],
      "abstract": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated \"Intent Graph\"; and (ii) an innovative \"Tiered Adjudicator\" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªä¸»å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼ˆLLM agentsï¼‰ææ˜“å—åˆ°é—´æ¥æç¤ºæ³¨å…¥ï¼ˆIndirect Prompt Injection, IPIï¼‰æ”»å‡»çš„é—®é¢˜ï¼Œåˆ†æäº†ç°æœ‰é˜²å¾¡æœºåˆ¶åœ¨å®‰å…¨ã€åŠŸèƒ½ä¸æ•ˆç‡ä¹‹é—´å­˜åœ¨çš„ç¢ç‰‡åŒ–æƒè¡¡å›°å¢ƒã€‚ä½œè€…æå‡ºæ ¸å¿ƒè§è§£ï¼Œå³æ¶æ„æ”»å‡»æ— è®ºå¦‚ä½•éšè”½ï¼Œå…¶æœ€ç»ˆåŠ¨ä½œè½¨è¿¹å¿…ç„¶ä¼šåç¦»é¢„æœŸçš„åˆæ³•è®¡åˆ’ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†è®¤çŸ¥æ§åˆ¶æ¶æ„ï¼ˆCognitive Control Architecture, CCAï¼‰ï¼Œé€šè¿‡å…¨ç”Ÿå‘½å‘¨æœŸè®¤çŸ¥ç›‘ç®¡æ¥å¢å¼ºæ™ºèƒ½ä½“çš„é²æ£’å¯¹é½ã€‚è¯¥æ¶æ„åŒ…å«ä¸¤å¤§æ”¯æŸ±ï¼šåˆ©ç”¨é¢„ç”Ÿæˆçš„æ„å›¾å›¾ï¼ˆIntent Graphï¼‰ä¸»åŠ¨å¼ºåˆ¶æ‰§è¡Œæ§åˆ¶æµå’Œæ•°æ®æµçš„å®Œæ•´æ€§ï¼Œä»¥åŠé€šè¿‡åˆ†å±‚è£å†³å™¨ï¼ˆTiered Adjudicatorï¼‰åœ¨æ£€æµ‹åˆ°åå·®æ—¶è¿›è¡ŒåŸºäºå¤šç»´è¯„åˆ†çš„æ·±åº¦æ¨ç†ã€‚åœ¨ AgentDojo åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯å®ï¼ŒCCA èƒ½å¤Ÿæœ‰æ•ˆæŠµå¾¡æå…·æŒ‘æˆ˜æ€§çš„å…ˆè¿›æ”»å‡»ã€‚è¯¥æ¡†æ¶åœ¨ä¿æŒé«˜æ•ˆä¸é²æ£’çš„åŒæ—¶å®ç°äº†å“è¶Šçš„å®‰å…¨æ€§ï¼ŒæˆåŠŸåŒ–è§£äº†æ™ºèƒ½ä½“é˜²å¾¡ä¸­çš„å¤šç»´åº¦æ€§èƒ½å†²çªã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06716v2",
      "published_date": "2025-12-07 08:11:19 UTC",
      "updated_date": "2026-01-23 08:44:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:55.669907+00:00"
    },
    {
      "arxiv_id": "2512.06714v1",
      "title": "A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting",
      "title_zh": "ä¸€ç§é¢å‘å®æ—¶éœ€æ°´é‡é¢„æµ‹çš„æ–°å‹æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„",
      "authors": [
        "Tony Salloom",
        "Okyay Kaynak",
        "Wei He"
      ],
      "abstract": "Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ­æœŸç”¨æ°´éœ€æ±‚é¢„æµ‹(StWDF)ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹å¤æ‚åº¦é«˜åŠæç«¯ç‚¹é¢„æµ‹è¯¯å·®å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ã€‚ä¸ºäº†ç¼“è§£æç«¯ç‚¹é™„è¿‘çš„éçº¿æ€§ç‰¹å¾å¸¦æ¥çš„è¯¯å·®ï¼Œè¯¥æ–‡é¦–æ¬¡æå‡ºé€šè¿‡åœ¨å®é™…æ•°æ®ä¸­æ’å…¥è™šæ‹Ÿæ•°æ®æ¥æ‰©å±•æ•°æ®é›†çš„æ–¹æ³•ã€‚æ ¸å¿ƒæ¨¡å‹åˆ©ç”¨é—¨æ§å¾ªç¯å•å…ƒ(GRU)å¤„ç†å†å²éœ€æ±‚æ•°æ®çš„åºåˆ—å…³ç³»ï¼Œå¹¶ç»“åˆæ— ç›‘ç£åˆ†ç±»æ–¹æ³•K-meansè¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼Œæ—¨åœ¨ä»¥æ›´å°‘çš„å‚æ•°é‡å®ç°é«˜ç²¾åº¦é¢„æµ‹ã€‚ç ”ç©¶äººå‘˜é‡‡ç”¨æ¥è‡ªä¸­å›½ä¸¤å®¶æ°´å‚çš„å®é™…æ•°æ®è¿›è¡Œäº†è®­ç»ƒä¸éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé¢„æµ‹ç²¾åº¦çš„åŒæ—¶ï¼Œå°†æ¨¡å‹å¤æ‚åº¦é™ä½è‡³ç°æœ‰å…ˆè¿›æŠ€æœ¯çš„å…­åˆ†ä¹‹ä¸€ã€‚æ­¤å¤–ï¼Œæ•°æ®æ‰©å±•ç­–ç•¥æ˜¾è‘—æ”¹å–„äº†æç«¯ç‚¹çš„é¢„æµ‹æ•ˆæœï¼Œä½¿ç›¸å…³è¯¯å·®é™ä½äº†çº¦30%ï¼Œä¸ºå®æ—¶æ°´èµ„æºç®¡ç†æä¾›äº†é«˜æ•ˆä¸”ä½è®¡ç®—æˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06714v1",
      "published_date": "2025-12-07 08:08:49 UTC",
      "updated_date": "2025-12-07 08:08:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:48.315602+00:00"
    },
    {
      "arxiv_id": "2512.06710v1",
      "title": "Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation",
      "title_zh": "æ™ºèƒ½ä½“è¯„ä¼°ä¸­çš„éšæœºæ€§ï¼šåˆ©ç”¨ç»„å†…ç›¸å…³ç³»æ•°é‡åŒ–ä¸ä¸€è‡´æ€§",
      "authors": [
        "Zairah Mustahsan",
        "Abel Lim",
        "Megna Anand",
        "Saahil Jain",
        "Bryan McCann"
      ],
      "abstract": "As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ™ºèƒ½ä½“ç³»ç»Ÿä¸­è¯„ä¼°å¯é æ€§ä¸è¶³çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰çš„å•æ¬¡å‡†ç¡®ç‡æŠ¥å‘Šæ— æ³•åŒºåˆ†çœŸå®çš„èƒ½åŠ›æå‡ä¸éšæœºé‡‡æ ·è¯¯å·®ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºå¼•å…¥æµ‹é‡ç§‘å­¦ä¸­çš„ç»„å†…ç›¸å…³ç³»æ•°(Intraclass Correlation Coefficient, ICC)æ¥è¡¨å¾è¯„ä¼°æ–¹å·®ï¼Œå°†å…¶åˆ†è§£ä¸ºä»»åŠ¡éš¾åº¦å¼•èµ·çš„æŸ¥è¯¢é—´æ–¹å·®å’Œæ™ºèƒ½ä½“ä¸ä¸€è‡´æ€§å¯¼è‡´çš„æŸ¥è¯¢å†…æ–¹å·®ã€‚åœ¨GAIAå’ŒFRAMESåŸºå‡†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒICCåœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹é—´å·®å¼‚æ˜¾è‘—ï¼Œä¸”åªæœ‰åœ¨å‡†ç¡®ç‡ä¸ICCåŒæ­¥æå‡æ—¶ï¼Œå­æ™ºèƒ½ä½“çš„æ›¿æ¢å†³ç­–æ‰è¢«è§†ä¸ºå¯é ã€‚å®éªŒè¯æ˜ICCé€šå¸¸åœ¨8è‡³32æ¬¡é‡å¤è¯•éªŒä¸­æ”¶æ•›ï¼Œä¸ºç ”ç©¶è€…åˆ¶å®šåŸºäºè¯æ®çš„é‡é‡‡æ ·é¢„ç®—æä¾›äº†å‚è€ƒã€‚ç ”ç©¶å»ºè®®åœ¨æŠ¥å‘Šå‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå°†ICCå’ŒæŸ¥è¯¢å†…æ–¹å·®çº³å…¥æ ‡å‡†è¯„ä¼°ä½“ç³»ï¼Œå¹¶æå‡ºäº†æ›´æ–°çš„è¯„ä¼°å¡(Evaluation Cards)ä»¥æå‡æµ‹è¯•çš„é€æ˜åº¦ã€‚è¯¥ç ”ç©¶é€šè¿‡ä½¿è¯„ä¼°ç¨³å®šæ€§å¯è§ï¼Œæ—¨åœ¨å°†æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä»ç®€å•çš„æ’è¡Œæ¦œç«äº‰æ¨å‘æ›´å…·å¯ä¿¡åº¦çš„å®éªŒç§‘å­¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06710v1",
      "published_date": "2025-12-07 07:58:13 UTC",
      "updated_date": "2025-12-07 07:58:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:54.471535+00:00"
    },
    {
      "arxiv_id": "2512.06708v1",
      "title": "A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations",
      "title_zh": "ä¸€ç§å…·æœ‰é€å±‚è§£é‡Šèƒ½åŠ›çš„æ–°å‹å¤šæ¨¡æ€å‰©ä½™ä½¿ç”¨å¯¿å‘½é¢„æµ‹æ¡†æ¶",
      "authors": [
        "Waleed Razzaq",
        "Yun-Bo Zhao"
      ],
      "abstract": "Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€ RUL æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºæ¢°ç³»ç»Ÿå‰©ä½™å¯¿å‘½é¢„æµ‹ (Remaining Useful Life) ä¸­æ³›åŒ–æ€§å·®ã€é²æ£’æ€§ä¸è¶³åŠè§£é‡Šæ€§å—é™ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ Bresenham ç®—æ³•ç”Ÿæˆçš„å›¾åƒè¡¨ç¤º (ImR) å’Œè¿ç»­å°æ³¢å˜æ¢ (Continuous Wavelet Transform) å¾—åˆ°çš„æ—¶é¢‘è¡¨ç¤º (TFR) å¯¹å¤šé€šé“éå¹³ç¨³æŒ¯åŠ¨ä¿¡å·è¿›è¡Œè”åˆè¡¨å¾ã€‚æ¨¡å‹æ¶æ„ç»“åˆäº†æ‰©å¼ å·ç§¯å—ã€æ®‹å·®è¿æ¥ä»¥åŠ LSTM ä»¥å»ºæ¨¡æ—¶ç©ºé€€åŒ–ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (multi-head attention) å¼ºåŒ–å…³é”®ä¿¡æ¯æå–ã€‚ä¸ºäº†æå‡é¢„æµ‹çš„é€æ˜åº¦ï¼Œç ”ç©¶å¼•å…¥äº†ä¸“é—¨è®¾è®¡çš„å¤šæ¨¡æ€å±‚çº§ç›¸å…³æ€§ä¼ æ’­ (multimodal-LRP) æŠ€æœ¯ã€‚åœ¨ XJTU-SY å’Œ PRONOSTIA åŸºå‡†æ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—å‡å°‘è®­ç»ƒæ•°æ®éœ€æ±‚ï¼ˆåˆ†åˆ«å‡å°‘çº¦ 28% å’Œ 48%ï¼‰çš„åŒæ—¶ï¼Œé¢„æµ‹ç²¾åº¦è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„å™ªå£°é²æ£’æ€§ï¼Œå…¶å¯è§£é‡Šæ€§å¯è§†åŒ–ç»“æœä¸ºå·¥ä¸šå®é™…éƒ¨ç½²æä¾›äº†é«˜åº¦å¯ä¿¡çš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06708v1",
      "published_date": "2025-12-07 07:38:36 UTC",
      "updated_date": "2025-12-07 07:38:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:27:54.343284+00:00"
    },
    {
      "arxiv_id": "2512.06705v2",
      "title": "Academic journals' AI policies fail to curb the surge in AI-assisted academic writing",
      "title_zh": "å­¦æœ¯æœŸåˆŠçš„äººå·¥æ™ºèƒ½æ”¿ç­–æœªèƒ½éåˆ¶äººå·¥æ™ºèƒ½è¾…åŠ©å­¦æœ¯å†™ä½œçš„æ¿€å¢",
      "authors": [
        "Yongyuan He",
        "Yi Bu"
      ],
      "abstract": "The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (~0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†5,114æœ¬æœŸåˆŠå’Œè¶…è¿‡520ä¸‡ç¯‡è®ºæ–‡ï¼Œæ—¨åœ¨è¯„ä¼°å­¦æœ¯æœŸåˆŠåˆ¶å®šçš„ç”Ÿæˆå¼ AI ä½¿ç”¨æ”¿ç­–åœ¨ç°å®ä¸–ç•Œä¸­çš„å®é™…å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡çº¦70%çš„æœŸåˆŠå·²é‡‡çº³ AI æ”¿ç­–ï¼ˆä¸»è¦è¦æ±‚æŠ«éœ²ï¼‰ï¼Œä½†å„å­¦ç§‘ç ”ç©¶äººå‘˜ä½¿ç”¨ AI å†™ä½œå·¥å…·çš„æ¯”ä¾‹ä»åœ¨æ€¥å‰§å¢åŠ ï¼Œä¸”æ— è®ºæœŸåˆŠæ˜¯å¦åˆ¶å®šæ”¿ç­–ï¼Œå…¶ AI ä½¿ç”¨å¢é•¿è¶‹åŠ¿å¹¶æ— æ˜¾è‘—å·®å¼‚ã€‚åœ¨éè‹±è¯­æ¯è¯­å›½å®¶ã€ç‰©ç†ç§‘å­¦é¢†åŸŸå’Œé«˜å¼€æ”¾è·å– (OA) æœŸåˆŠä¸­ï¼ŒAI è¾…åŠ©å†™ä½œçš„å¢é•¿ç‡æœ€ä¸ºçªå‡ºã€‚é€šè¿‡å¯¹16.4ä¸‡ç¯‡ç§‘å­¦å‡ºç‰ˆç‰©çš„å…¨æ–‡åˆ†ææ­ç¤ºäº†å·¨å¤§çš„é€æ˜åº¦ç¼ºå£ (transparency gap)ï¼Œè‡ª2023å¹´ä»¥æ¥å‘è¡¨çš„è®ºæ–‡ä¸­ä»…æœ‰çº¦0.1%æ˜ç¡®æŠ«éœ²äº† AI çš„ä½¿ç”¨æƒ…å†µã€‚ç ”ç©¶ç»“æœè¡¨æ˜ç°è¡Œæ”¿ç­–åœ¨ä¿ƒè¿›é€æ˜åº¦æˆ–éåˆ¶ AI é‡‡ç”¨æ–¹é¢åŸºæœ¬å¤±æ•ˆï¼Œå› æ­¤ç ”ç©¶è€…å‘¼åé‡æ–°è¯„ä¼°ä¼¦ç†æ¡†æ¶ï¼Œä»¥åœ¨ç§‘å­¦ç ”ç©¶ä¸­å®ç°è´Ÿè´£ä»»çš„ AI é›†æˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "39 pages, 10 figures, and 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.06705v2",
      "published_date": "2025-12-07 07:30:53 UTC",
      "updated_date": "2026-01-20 04:40:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:00.379837+00:00"
    },
    {
      "arxiv_id": "2512.06699v2",
      "title": "Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization",
      "title_zh": "æœºå™¨å­¦ä¹ è®­ç»ƒæµæ°´çº¿ I/O æ€§èƒ½é¢„æµ‹å»ºæ¨¡ï¼šä¸€ç§æ•°æ®é©±åŠ¨çš„å­˜å‚¨ä¼˜åŒ–æ–¹æ³•",
      "authors": [
        "Karthik Prabhakar",
        "Durgamadhab Mishra"
      ],
      "abstract": "Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£æœºå™¨å­¦ä¹ è®­ç»ƒä¸­æ•°æ® I/O ç“¶é¢ˆå¯¼è‡´ GPU åˆ©ç”¨ç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„ I/O æ€§èƒ½é¢„æµ‹ä¸å­˜å‚¨é…ç½®ä¼˜åŒ–æ–¹æ³•ã€‚ç ”ç©¶è€…é€šè¿‡å¯¹ NVMe SSDã€ç½‘ç»œé™„åŠ å­˜å‚¨ NAS åŠå†…å­˜æ–‡ä»¶ç³»ç»Ÿç­‰ä¸åŒå­˜å‚¨åç«¯è¿›è¡Œç³»ç»Ÿæ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¤šç§æ•°æ®æ ¼å¼å’Œè®¿é—®æ¨¡å¼ï¼Œæ„å»ºäº†åŒ…å« 141 ç»„è§‚æµ‹æ•°æ®çš„å®éªŒé›†ã€‚åœ¨å¯¹å¤šç§å›å½’å’Œåˆ†ç±»æ¨¡å‹è¿›è¡Œè¯„ä¼°åï¼ŒXGBoost æ¨¡å‹è¡¨ç°æœ€ä¼˜ï¼Œå…¶å†³å®šç³»æ•° R-squared è¾¾åˆ° 0.991ï¼Œå¹³å‡é¢„æµ‹è¯¯å·®ä»…ä¸º 11.8%ã€‚ç‰¹å¾é‡è¦æ€§åˆ†ææ˜¾ç¤ºï¼Œååé‡æŒ‡æ ‡å’Œ Batch Size æ˜¯å†³å®šæ€§èƒ½çš„æ ¸å¿ƒé©±åŠ¨å› ç´ ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå°†å­˜å‚¨é…ç½®çš„ä¼˜åŒ–æ—¶é—´ä»æ•°å¤©çš„æ‰‹åŠ¨å°è¯•ç¼©çŸ­è‡³åˆ†é’Ÿçº§çš„é¢„æµ‹æ¨èï¼Œæ˜¾è‘—æå‡äº†æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„èµ„æºç®¡ç†æ•ˆç‡ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…ä¸ºå­˜å‚¨ä¼˜åŒ–æä¾›äº†é«˜æ•ˆå·¥å…·ï¼Œå…¶æ–¹æ³•è®ºä¹Ÿå…·å¤‡æ‰©å±•è‡³å…¶ä»–æœºå™¨å­¦ä¹ èµ„æºç®¡ç†é—®é¢˜çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.PF",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.PF",
      "comment": "20 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06699v2",
      "published_date": "2025-12-07 07:25:08 UTC",
      "updated_date": "2025-12-19 06:10:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:00.873031+00:00"
    },
    {
      "arxiv_id": "2512.06681v1",
      "title": "Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis",
      "title_zh": "GPT-2 çš„æœºæ¢°å¯è§£é‡Šæ€§ï¼šæƒ…æ„Ÿåˆ†æä¸­çš„è¯æ³•å±‚ä¸è¯­å¢ƒå±‚",
      "authors": [
        "Amartya Hatua"
      ],
      "abstract": "We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ GPT-2 è¿›è¡Œäº†æœºæ¢°å¯è§£é‡Šæ€§ (Mechanistic Interpretability) ç ”ç©¶ï¼Œé€šè¿‡å¯¹å…¨éƒ¨ 12 å±‚è¿›è¡Œç³»ç»Ÿæ€§çš„æ¿€æ´»ä¿®è¡¥ (Activation Patching)ï¼Œå› æœæ€§åœ°åˆ†æäº†æƒ…æ„Ÿä¿¡æ¯åœ¨ Transformer å±‚ä¸­çš„å¤„ç†è¿‡ç¨‹ã€‚å®éªŒè¯å®äº†æ—©æœŸå±‚ï¼ˆ0-3å±‚ï¼‰å……å½“è¯æ±‡æƒ…æ„Ÿæ£€æµ‹å™¨ (Lexical Sentiment Detectors)ï¼Œè´Ÿè´£ç¼–ç ä¸ä¸Šä¸‹æ–‡æ— å…³çš„ç¨³å®šææ€§ä¿¡å·ã€‚ç„¶è€Œï¼Œå…³äºä¸­å±‚ä¸Šä¸‹æ–‡é›†æˆçš„ä¸‰é¡¹æ ¸å¿ƒå‡è®¾å‡è¢«è¯ä¼ªï¼Œç ”ç©¶å‘ç°å¦å®šã€è®½åˆºåŠé¢†åŸŸè½¬ç§»ç­‰ä¸Šä¸‹æ–‡ç°è±¡ä¸»è¦åœ¨åæœŸå±‚ï¼ˆ8-11å±‚ï¼‰é€šè¿‡ä¸€ç§ç»Ÿä¸€ä¸”éæ¨¡å—åŒ–çš„æœºåˆ¶è¿›è¡Œé›†æˆã€‚è¿™äº›å› æœè¯æ®è¡¨æ˜ï¼ŒGPT-2 çš„æƒ…æ„Ÿè®¡ç®—å¹¶ééµå¾ªé¢„æµ‹çš„å±‚æ¬¡æ¨¡å¼ï¼ŒæŒ‘æˆ˜äº†æ—¢æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¯¥ç ”ç©¶ç»“æœä¸ºè¿›ä¸€æ­¥å®è¯è¡¨å¾å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡é›†æˆæä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06681v1",
      "published_date": "2025-12-07 06:36:35 UTC",
      "updated_date": "2025-12-07 06:36:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:09.109218+00:00"
    },
    {
      "arxiv_id": "2512.06678v1",
      "title": "GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning",
      "title_zh": "GradientSpaceï¼šç”¨äºæ”¹è¿›æŒ‡ä»¤å¾®è°ƒçš„æ— ç›‘ç£æ•°æ®èšç±»",
      "authors": [
        "Shrihari Sridharan",
        "Deepak Ravikumar",
        "Anand Raghunathan",
        "Kaushik Roy"
      ],
      "abstract": "Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŒ‡ä»¤å¾®è°ƒ(Instruction tuning)ä¸­ç”±äºæ•°æ®å¼‚æ„æ€§å¯¼è‡´çš„æ¢¯åº¦å¹²æ‰°(gradient interference)é—®é¢˜ï¼Œæå‡ºäº†GradientSpaceæ¡†æ¶ã€‚ä¼ ç»Ÿçš„è¯­ä¹‰ç›¸ä¼¼åº¦èšç±»æ–¹æ³•å¾€å¾€æ— æ³•å‡†ç¡®æ•æ‰æ•°æ®å¯¹æ¨¡å‹å‚æ•°çš„å½±å“ï¼Œè€Œç°æœ‰çš„æ¢¯åº¦èšç±»æŠ€æœ¯åˆ™å—é™äºéšæœºæŠ•å½±å¸¦æ¥çš„ç²¾åº¦æŸå¤±æˆ–ä¸“å®¶é›†æˆ(expert ensembles)äº§ç”Ÿçš„é«˜æ˜‚æ¨ç†æˆæœ¬ã€‚GradientSpaceå¼•å…¥äº†ä¸€ç§åŸºäºåœ¨çº¿å¥‡å¼‚å€¼åˆ†è§£(SVD)çš„ç®—æ³•ï¼Œç›´æ¥åœ¨å…¨ç»´LoRAæ¢¯åº¦ç©ºé—´ä¸­å¯¹æ ·æœ¬è¿›è¡Œæ— ç›‘ç£èšç±»ï¼Œä»è€Œåœ¨ä¸å¢åŠ å­˜å‚¨è´Ÿæ‹…çš„æƒ…å†µä¸‹æœ‰æ•ˆè¯†åˆ«æ•°æ®çš„æ½œåœ¨æŠ€èƒ½ã€‚è¯¥æ¡†æ¶ä¸ºæ¯ä¸ªèšç±»ç°‡è®­ç»ƒä¸“é—¨çš„LoRAä¸“å®¶ï¼Œå¹¶é…åˆè½»é‡çº§è·¯ç”±å™¨(router)åœ¨æ¨ç†æ—¶é€‰æ‹©æœ€ä½³çš„å•ä¸“å®¶æ‰§è¡Œä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGradientSpaceåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€é‡‘èå’Œåˆ›æ„å†™ä½œç­‰ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜äºç°æœ‰èšç±»å’Œå¾®è°ƒæŠ€æœ¯çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨æå‡ä¸“å®¶ä¸“ä¸šåŒ–ç¨‹åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06678v1",
      "published_date": "2025-12-07 06:35:04 UTC",
      "updated_date": "2025-12-07 06:35:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:10.352469+00:00"
    },
    {
      "arxiv_id": "2512.06665v1",
      "title": "Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods",
      "title_zh": "é‡æ–°å®¡è§†é²æ£’æ€§ï¼šä¸€ç§è¯„ä¼°ç‰¹å¾å½’å› æ–¹æ³•çš„æ–°æ–¹æ³•",
      "authors": [
        "Panagiota Kiourti",
        "Anu Singh",
        "Preeti Duraipandian",
        "Weichao Zhou",
        "Wenchao Li"
      ],
      "abstract": "This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œä¸­ç‰¹å¾å½’å› æ–¹æ³•(Feature Attribution Methods)çš„é²æ£’æ€§ï¼Œå¹¶æŒ‡å‡ºå½“å‰çš„è¯„ä¼°æ¦‚å¿µå¾€å¾€å¿½ç•¥äº†æ¨¡å‹è¾“å‡ºå·®å¼‚å¸¦æ¥çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è¯„ä¼°å½’å› æ–¹æ³•é²æ£’æ€§çš„æ–°æ¡†æ¶ï¼ŒåŒ…æ‹¬å¯¹ç›¸ä¼¼è¾“å…¥çš„æ–°å®šä¹‰ã€ä¸€å¥—å…¨æ–°çš„é²æ£’æ€§æŒ‡æ ‡(Robustness Metric)ï¼Œä»¥åŠä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(Generative Adversarial Networks)ç”Ÿæˆæµ‹è¯•è¾“å…¥çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹ç°æœ‰æŒ‡æ ‡å’Œæœ€å…ˆè¿›å½’å› æ–¹æ³•çš„å…¨é¢è¯„ä¼°ï¼Œç ”ç©¶å‘ç°ç›®å‰çš„è¯„ä¼°ä½“ç³»éš¾ä»¥åŒºåˆ†å½’å› æ–¹æ³•ä¸ç¥ç»ç½‘ç»œæœ¬èº«çš„å¼±ç‚¹ã€‚å®éªŒç»“æœå¼ºè°ƒäº†å¼€å‘æ›´å®¢è§‚è¯„ä¼°æŒ‡æ ‡çš„å¿…è¦æ€§ï¼Œä»è€Œå®ç°å¯¹å½’å› æ–¹æ³•é²æ£’æ€§æ›´å‡†ç¡®çš„è¡¡é‡ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£å’Œæ”¹è¿›æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06665v1",
      "published_date": "2025-12-07 05:29:38 UTC",
      "updated_date": "2025-12-07 05:29:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:22.367527+00:00"
    },
    {
      "arxiv_id": "2512.06660v1",
      "title": "Towards Small Language Models for Security Query Generation in SOC Workflows",
      "title_zh": "é¢å‘ SOC å·¥ä½œæµä¸­å®‰å…¨æŸ¥è¯¢ç”Ÿæˆçš„å°è¯­è¨€æ¨¡å‹ç ”ç©¶",
      "authors": [
        "Saleha Muzammil",
        "Rahul Reddy",
        "Vishal Kamalakrishnan",
        "Hadi Ahmadi",
        "Wajih Ul Hassan"
      ],
      "abstract": "Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å®‰å…¨è¿è¥ä¸­å¿ƒ(SOC)å·¥ä½œæµä¸­ä½¿ç”¨å°è¯­è¨€æ¨¡å‹(SLMs)è¿›è¡Œè‡ªç„¶è¯­è¨€åˆ°Kusto Query Language(KQL)è½¬æ¢çš„å¯è¡Œæ€§ï¼Œä»¥è§£å†³å®‰å…¨å›¢é˜Ÿé¢ä¸´çš„ä¸“å®¶ä¾èµ–ç“¶é¢ˆã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ¶µç›–æç¤ºå·¥ç¨‹ã€å¾®è°ƒå’Œæ¶æ„è®¾è®¡çš„ç»¼åˆæ¡†æ¶ï¼Œå¼•å…¥äº†è½»é‡çº§æ£€ç´¢å’Œé”™è¯¯æ„ŸçŸ¥æç¤ºï¼Œå¹¶é‡‡ç”¨LoRAå¾®è°ƒä¸æ¨ç†è’¸é¦(rationale distillation)æŠ€æœ¯ä»æ•™å¸ˆæ¨¡å‹ä¸­è½¬ç§»æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆè®¾è®¡äº†ä¸€ç§ä¸¤é˜¶æ®µæ¶æ„ï¼Œç»“åˆäº†SLMçš„å€™é€‰é¡¹ç”Ÿæˆèƒ½åŠ›ä¸ä½æˆæœ¬LLMçš„æ¨¡å¼æ„ŸçŸ¥ç²¾ç‚¼èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨Microsoftçš„NL2KQLè¯„ä¼°æ•°æ®é›†ä¸Šå®ç°äº†0.987çš„è¯­æ³•å‡†ç¡®ç‡å’Œ0.906çš„è¯­ä¹‰å‡†ç¡®ç‡ã€‚ä¸GPT-5ç­‰å¤§å‹æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆçš„Tokenæˆæœ¬é™ä½äº†é«˜è¾¾10å€ï¼Œè¯æ˜äº†SLMsä½œä¸ºå®‰å…¨è¿è¥ä¸­è‡ªç„¶è¯­è¨€æŸ¥è¯¢åŸºç¡€çš„å®ç”¨æ€§ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06660v1",
      "published_date": "2025-12-07 05:18:27 UTC",
      "updated_date": "2025-12-07 05:18:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:37.900165+00:00"
    },
    {
      "arxiv_id": "2512.06657v1",
      "title": "TextMamba: Scene Text Detector with Mamba",
      "title_zh": "TextMambaï¼šåŸºäº Mamba çš„åœºæ™¯æ–‡æœ¬æ£€æµ‹å™¨",
      "authors": [
        "Qiyan Zhao",
        "Yue Yan",
        "Da-Han Wang"
      ],
      "abstract": "In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\\%, 89.2\\%, and 78.5\\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TextMambaï¼Œä¸€ç§åŸºäº Mamba çŠ¶æ€ç©ºé—´æ¨¡å‹çš„åœºæ™¯æ–‡æœ¬æ£€æµ‹å™¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ Transformer æ¨¡å‹åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–æ—¶å®¹æ˜“é—å¿˜å…³é”®ä¿¡æ¯æˆ–å…³æ³¨æ— å…³è¡¨ç¤ºçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°å°† Mamba çš„é€‰æ‹©æœºåˆ¶ä¸ Attention å±‚ç›¸ç»“åˆï¼Œå¢å¼ºäº†ç¼–ç å™¨ä»é•¿åºåˆ—ä¸­æå–æ ¸å¿ƒç‰¹å¾çš„èƒ½åŠ›ã€‚ä¸ºäº†é™ä½æ— å…³ä¿¡æ¯çš„å¹²æ‰°ï¼Œç ”ç©¶é‡‡ç”¨äº† Top\\_k ç®—æ³•è¿›è¡Œæ˜¾å¼ä¿¡æ¯ç­›é€‰ï¼Œå¹¶è®¾è®¡äº†åŒå°ºåº¦å‰é¦ˆç½‘ç»œ (Dual-scale feed-forward network) å’ŒåµŒå…¥é‡‘å­—å¡”å¢å¼ºæ¨¡å— (Embedding pyramid enhancement module) æ¥ä¼˜åŒ–é«˜ç»´éšçŠ¶æ€äº¤äº’ä¸å¤šå°ºåº¦ç‰¹å¾èåˆã€‚å®éªŒè¡¨æ˜ï¼ŒTextMamba åœ¨ CTW1500ã€TotalText å’Œ ICDAR19ArT ç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº† state-of-the-art æˆ–æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒF-measure åˆ†åˆ«è¾¾åˆ° 89.7%ã€89.2% å’Œ 78.5%ã€‚è¯¥ç ”ç©¶è¯æ˜äº† Mamba æ¶æ„åœ¨å¤æ‚åœºæ™¯æ–‡æœ¬æ£€æµ‹ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06657v1",
      "published_date": "2025-12-07 05:06:19 UTC",
      "updated_date": "2025-12-07 05:06:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:35.365828+00:00"
    },
    {
      "arxiv_id": "2512.06655v1",
      "title": "GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering",
      "title_zh": "GSAEï¼šç”¨äºå¤§è¯­è¨€æ¨¡å‹ç¨³å¥å®‰å…¨å¼•å¯¼çš„å›¾æ­£åˆ™åŒ–ç¨€ç–è‡ªç¼–ç å™¨",
      "authors": [
        "Jehyeok Yeon",
        "Federico Cinus",
        "Yifan Wu",
        "Luca Luceri"
      ],
      "abstract": "Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é¢ä¸´çš„å¯¹æŠ—æ€§æç¤ºä¸è¶Šç‹±æ”»å‡»é£é™©ï¼Œæå‡ºäº†å›¾æ­£åˆ™åŒ–ç¨€ç–è‡ªç¼–ç å™¨(Graph-Regularized Sparse Autoencoders, GSAEs)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å°†å®‰å…¨æ¦‚å¿µç®€åŒ–ä¸ºå•ä¸€æ½œç‰¹å¾çš„å±€é™æ€§ã€‚é€šè¿‡åœ¨ç¥ç»å…ƒå…±æ¿€æ´»å›¾ä¸Šå¼•å…¥æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æƒ©ç½š(Laplacian smoothness penalty)ï¼ŒGSAEèƒ½å¤Ÿæ•æ‰è·¨å¤šä¸ªç‰¹å¾çš„åˆ†å¸ƒå¼å®‰å…¨è¡¨ç¤ºï¼Œä»è€Œæ›´ç²¾å‡†åœ°æ¢å¤å¤æ‚çš„å®‰å…¨æ¨¡å¼ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µé—¨æ§æœºåˆ¶è¿›è¡Œå®æ—¶å®‰å…¨å¼•å¯¼(Safety Steering)ï¼Œä»…åœ¨æ£€æµ‹åˆ°æœ‰å®³å†…å®¹æ—¶æ¿€æ´»å¹²é¢„ï¼Œç¡®ä¿åœ¨å®ç°è‡ªé€‚åº”æ‹’ç»çš„åŒæ—¶ä¿ç•™æ¨¡å‹åœ¨è‰¯æ€§æŸ¥è¯¢ä¸Šçš„æ•ˆç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGSAEåœ¨å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†82%çš„é€‰æ‹©æ€§æ‹’ç»ç‡ï¼Œæ˜¾è‘—ä¼˜äºæ ‡å‡†SAEçš„42%ï¼Œå¹¶åœ¨TriviaQAå’ŒGSM8Kç­‰ä»»åŠ¡ä¸­ä¿æŒäº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨LLaMA-3ã€Mistralã€QwenåŠPhiç­‰å¤šä¸ªæ¨¡å‹ç³»åˆ—ä¸Šå±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆæŠµå¾¡GCGå’ŒAutoDANç­‰è¶Šç‹±æ”»å‡»ï¼Œç»´æŒ90%ä»¥ä¸Šçš„æœ‰å®³å†…å®¹æ‹’ç»ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06655v1",
      "published_date": "2025-12-07 04:46:30 UTC",
      "updated_date": "2025-12-07 04:46:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:41.050728+00:00"
    },
    {
      "arxiv_id": "2512.06653v3",
      "title": "LightSearcher: Efficient DeepSearch via Experiential Memory",
      "title_zh": "LightSearcherï¼šåŸºäºç»éªŒè®°å¿†çš„é«˜æ•ˆæ·±åº¦æœç´¢",
      "authors": [
        "Hengzhi Lan",
        "Yue Yu",
        "Li Qian",
        "Li Peng",
        "Jie Wu",
        "Wei Liu",
        "Jian Luan",
        "Ting Bai"
      ],
      "abstract": "DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LightSearcherï¼Œä¸€ä¸ªé«˜æ•ˆçš„ Reinforcement Learning æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ DeepSearch ç³»ç»Ÿä¸­å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ã€‚LightSearcher å¼•å…¥äº† Textual Experiential Memory æœºåˆ¶ï¼Œé€šè¿‡å­¦ä¹  Contrastive Reasoning Trajectories æ¥ç”ŸæˆæˆåŠŸæ¨ç†æ¨¡å¼çš„å¯è§£é‡Šæ€»ç»“ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§ Adaptive Reward Shaping æœºåˆ¶ï¼Œä»…åœ¨è¾“å‡ºæ­£ç¡®ç­”æ¡ˆçš„æƒ…å†µä¸‹æƒ©ç½šå†—ä½™çš„å·¥å…·è°ƒç”¨ï¼Œä»è€Œç²¾å‡†ä¼˜åŒ–äº†æ£€ç´¢ç­–ç•¥ã€‚åœ¨å››ä¸ª Multi-hop QA åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒLightSearcher åœ¨ä¿æŒä¸åŸºçº¿æ¨¡å‹ ReSearch ç›¸å½“å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†å¤„ç†é€Ÿåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶å°†æœç´¢å·¥å…·è°ƒç”¨æ¬¡æ•°å‡å°‘äº† 39.6%ï¼Œæ¨ç†æ—¶é—´ç¼©çŸ­äº† 48.6%ï¼ŒToken æ¶ˆè€—é™ä½äº† 21.2%ã€‚è¯¥ç ”ç©¶å……åˆ†è¯æ˜äº† LightSearcher åœ¨å¢å¼ºæ·±åº¦æ¨ç†æ¨¡å‹äº‹å®å¯é æ€§çš„åŒæ—¶ï¼Œå…·å¤‡å“è¶Šçš„æ¨ç†æ•ˆç‡ä¸èµ„æºä¼˜åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06653v3",
      "published_date": "2025-12-07 04:29:52 UTC",
      "updated_date": "2025-12-10 07:22:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:45.489851+00:00"
    },
    {
      "arxiv_id": "2512.06652v1",
      "title": "Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts",
      "title_zh": "é’ˆå¯¹å¤šä¸­å¿ƒé˜Ÿåˆ—æœ‰åˆ›æœºæ¢°é€šæ°”éœ€æ±‚é¢„æµ‹çš„è‡ªé€‚åº”æµ‹è¯•æ—¶è®­ç»ƒ",
      "authors": [
        "Xiaolei Lu",
        "Shamim Nemati"
      ],
      "abstract": "Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤šä¸­å¿ƒICUé˜Ÿåˆ—ä¸­å› ä¸åŒæœºæ„é—´ä¸´åºŠå®è·µå’Œç”µå­å¥åº·è®°å½•(EHR)ç³»ç»Ÿå·®å¼‚å¯¼è‡´çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œæå‡ºäº†AdaTTTï¼ˆAdaptive Test-Time Trainingï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æœ‰åˆ›æœºæ¢°é€šæ°”(IMV)éœ€æ±‚é¢„æµ‹çš„æ³›åŒ–æ€§èƒ½ã€‚è¯¥ç ”ç©¶é¦–å…ˆä»ç†è®ºä¸Šæ¨å¯¼äº†æµ‹è¯•æ—¶é¢„æµ‹è¯¯å·®çš„ä¿¡æ¯è®ºç•Œé™ï¼Œå‘ç°å…¶å—ä¸»ä»»åŠ¡ä¸è¾…åŠ©ä»»åŠ¡é—´ä¸ç¡®å®šæ€§çš„çº¦æŸã€‚ä¸ºæ­¤ï¼ŒAdaTTTé‡‡ç”¨äº†ä¸€ç§åŒ…å«é‡æ„å’Œæ©ç ç‰¹å¾å»ºæ¨¡çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€æ©ç ç­–ç•¥å¼ºåŒ–å¯¹æ ¸å¿ƒç‰¹å¾çš„å…³æ³¨ã€‚ä¸ºäº†å¢å¼ºé²æ£’æ€§ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸå‹å­¦ä¹ ä¸éƒ¨åˆ†æœ€ä¼˜ä¼ è¾“ï¼ˆPartial Optimal Transport, POTï¼‰æŠ€æœ¯ï¼Œåœ¨ä¿æŒä¸´åºŠè¡¨å¾æ„ä¹‰çš„åŒæ—¶å®ç°çµæ´»çš„ç‰¹å¾å¯¹é½ã€‚è·¨ä¸­å¿ƒå®éªŒç»“æœè¯æ˜ï¼ŒAdaTTTåœ¨å¤šç§æµ‹è¯•æ—¶è‡ªé€‚åº”åŸºå‡†ä¸Šå‡å–å¾—äº†å‡ºè‰²çš„åˆ†ç±»è¡¨ç°ï¼Œä¸ºè§£å†³åŒ»ç–—AIéƒ¨ç½²ä¸­çš„åˆ†å¸ƒå·®å¼‚é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.06652v1",
      "published_date": "2025-12-07 04:27:40 UTC",
      "updated_date": "2025-12-07 04:27:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:47.650095+00:00"
    },
    {
      "arxiv_id": "2512.06648v2",
      "title": "Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network",
      "title_zh": "åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„ä¸Šå¸‚å…¬å¸è´¢åŠ¡æ¬ºè¯ˆè¯†åˆ«åŠå¯è§£é‡Šæ€§ç ”ç©¶",
      "authors": [
        "Xiao Li"
      ],
      "abstract": "Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.\n  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.\n  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸­å›½Aè‚¡ä¸Šå¸‚å…¬å¸æå‡ºäº†ä¸€ç§åŸºäºConvolutional Neural Network (CNN)çš„è´¢åŠ¡èˆå¼Šæ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ·±åº¦å­¦ä¹ æå‡è¯†åˆ«çš„å‡†ç¡®æ€§ä¸æ—¶æ•ˆæ€§ã€‚è¯¥æ–¹æ¡ˆè®¾è®¡äº†ä¸€ç§ç‰¹å¾å·¥ç¨‹æ–¹æ³•ï¼Œå°†å…¬å¸å¹´åº¦é¢æ¿æ•°æ®è½¬åŒ–ä¸ºç±»å›¾åƒè¡¨ç¤º(image-like representations)ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰æ¨ªæˆªé¢ä¸æ—¶é—´æ¨¡å¼å¹¶å®ç°èˆå¼Šçš„æå‰é¢„è­¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCNNåœ¨å‡†ç¡®æ€§ã€ç¨³å¥æ€§å’Œé¢„è­¦æ€§èƒ½æ–¹é¢å‡æ˜¾è‘—ä¼˜äºLogistic Regressionå’ŒLightGBMæ¨¡å‹ã€‚ä¸ºäº†è§£å†³æ¨¡å‹è§£é‡Šæ€§é—®é¢˜ï¼Œç ”ç©¶åˆ©ç”¨å±€éƒ¨è§£é‡ŠæŠ€æœ¯(local explanation techniques)ä»å®ä½“ã€ç‰¹å¾å’Œæ—¶é—´ç»´åº¦è¿›è¡Œåˆ†æï¼Œå‘ç°solvencyã€ratio structureã€governance structureå’Œå†…éƒ¨æ§åˆ¶æ˜¯æ™®éçš„èˆå¼Šé¢„æµ‹æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶æŒ‡å‡ºéèˆå¼Šå…¬å¸å…·æœ‰ç¨³å®šçš„ç‰¹å¾æ¨¡å¼ï¼Œè€Œèˆå¼Šå…¬å¸åˆ™è¡¨ç°å‡ºé›†ä¸­åœ¨çŸ­æ—¶é—´çª—å£å†…çš„å¼‚æ„æ¨¡å¼ã€‚é€šè¿‡å¯¹2022å¹´Guanong Sharesçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹åœ¨ç°é‡‘æµåˆ†æå’Œç¤¾ä¼šè´£ä»»ç­‰ç»´åº¦çš„é¢„æµ‹é€»è¾‘ä¸å…¶å®é™…è¿è§„è¡Œä¸ºé«˜åº¦ä¸€è‡´ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "in Chinese language",
      "pdf_url": "https://arxiv.org/pdf/2512.06648v2",
      "published_date": "2025-12-07 04:14:16 UTC",
      "updated_date": "2025-12-10 02:58:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:29:00.501540+00:00"
    },
    {
      "arxiv_id": "2601.00005v2",
      "title": "Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems",
      "title_zh": "é’ˆå¯¹æ¨¡æ‹Ÿé«˜åº¦ä¸å¹³è¡¡å·¥ä¸šåˆ†ç±»é—®é¢˜çš„å¼‚å¸¸æ£€æµ‹å™¨è¯„ä¼°",
      "authors": [
        "Lesley Wheat",
        "Martin v. Mohrenschildt",
        "Saeid Habibi"
      ],
      "abstract": "Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šç³»ç»Ÿä¸­è´¨é‡æ§åˆ¶å’Œé¢„æµ‹æ€§ç»´æŠ¤é¢ä¸´çš„æç«¯ç±»åˆ«ä¸å¹³è¡¡(Class Imbalance)æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•…éšœæ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå¯¹å¤šç§å¼‚å¸¸æ£€æµ‹(Anomaly Detection)ç®—æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç ”ç©¶åˆ©ç”¨åæ˜ çœŸå®å·¥ç¨‹çº¦æŸçš„è¶…çƒä½“(Hyper-spherical)åˆ†å¸ƒæ¨¡æ‹Ÿæ•°æ®é›†ï¼Œåœ¨2ç»´å’Œ10ç»´ç©ºé—´å†…å¯¹14ç§æ£€æµ‹å™¨è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒæ¶µç›–äº†0.05%è‡³20%çš„å¼‚å¸¸ç‡ä»¥åŠ1,000è‡³10,000è§„æ¨¡çš„è®­ç»ƒæ ·æœ¬ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒç®—æ³•çš„æ€§èƒ½è¡¨ç°ä¸æ³›åŒ–è¯¯å·®(Generalization Error)ã€‚ç»“æœè¡¨æ˜ï¼Œæœ€ä½³æ£€æµ‹å™¨çš„é€‰æ‹©é«˜åº¦ä¾èµ–äºè®­ç»ƒé›†ä¸­æ•…éšœæ ·æœ¬çš„æ€»æ•°ï¼Œè€Œå¢åŠ æ­£å¸¸æ ·æœ¬å¸¦æ¥çš„æ”¶ç›Šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å¹¶ä¸æ˜¾è‘—ã€‚å½“æ•…éšœæ ·æœ¬å°‘äº20ä¸ªæ—¶ï¼Œæ— ç›‘ç£å­¦ä¹ (Unsupervised Learning)æ–¹æ³•å¦‚kNNå’ŒLOFè¡¨ç°æœ€ä¼˜ã€‚å½“æ•…éšœæ ·æœ¬è¾¾åˆ°30è‡³50ä¸ªå·¦å³æ—¶ï¼ŒåŠç›‘ç£å­¦ä¹ (Semi-supervised Learning)çš„XGBODä»¥åŠç›‘ç£å­¦ä¹ (Supervised Learning)çš„SVMå’ŒCatBoostå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å¢é•¿ã€‚æ­¤å¤–ï¼ŒåŠç›‘ç£æ–¹æ³•åœ¨é«˜ç»´ç‰¹å¾ä¸‹æ¯”ä½ç»´ç‰¹å¾æ˜¾ç¤ºå‡ºæ›´æ˜ç¡®çš„ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¸‹é™é—®é¢˜ï¼Œä¸ºå·¥ä¸šç¯å¢ƒä¸‹çš„æ¨¡å‹éƒ¨ç½²æä¾›äº†å®ç”¨çš„æŒ‡å¯¼å»ºè®®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 14 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.00005v2",
      "published_date": "2025-12-07 03:49:54 UTC",
      "updated_date": "2026-01-14 02:17:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:53.730986+00:00"
    },
    {
      "arxiv_id": "2512.06642v1",
      "title": "Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution",
      "title_zh": "é¢å‘æš—ç‰©è´¨æ¨¡å‹åˆ†ç±»ä¸è¶…åˆ†è¾¨ç‡è”åˆä»»åŠ¡çš„å¼ºå¼•åŠ›é€é•œå›¾åƒæ©ç è‡ªç¼–ç å™¨é¢„è®­ç»ƒ",
      "authors": [
        "Achmad Ardani Prasha",
        "Clavino Ourizqi Rachmadi",
        "Muhamad Fauzan Ibnu Syahlan",
        "Naufal Rahfi Anugerah",
        "Nanda Garin Raditya",
        "Putri Amelia",
        "Sabrina Laila Mutiara",
        "Hilman Syachr Ramadhan"
      ],
      "abstract": "Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»ä½åˆ†è¾¨ç‡å¼ºå¼•åŠ›é€é•œ (Strong Gravitational Lensing) å›¾åƒä¸­æå–æš—ç‰©è´¨å­ç»“æ„ (Dark-Matter Substructure) çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ©ç è‡ªç¼–ç å™¨ (Masked Autoencoder, MAE) çš„é¢„è®­ç»ƒç­–ç•¥ã€‚é€šè¿‡åœ¨ DeepLense ML4SCI åŸºå‡†æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šé¢„è®­ç»ƒ Vision Transformer (ViT) ç¼–ç å™¨ï¼Œç ”ç©¶å®ç°äº†å¯¹æš—ç‰©è´¨æ¨¡å‹åˆ†ç±»å’Œè¶…åˆ†è¾¨ç‡ (Super-Resolution) ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡çš„è”åˆå¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ 90% çš„æ©ç æ¯”ä¾‹ä¸‹ï¼Œå¾®è°ƒåçš„åˆ†ç±»å™¨å®å¹³å‡ AUC è¾¾åˆ° 0.968ï¼Œå‡†ç¡®ç‡æå‡è‡³ 88.65%ï¼Œæ˜¾è‘—ä¼˜äºä»é›¶å¼€å§‹è®­ç»ƒçš„åŸºçº¿æ°´å¹³ã€‚åœ¨å°†å›¾åƒä» 16x16 æå‡è‡³ 64x64 çš„è¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹ä¹Ÿå±•ç°äº†ä¼˜äºåŸºçº¿çš„é‡å»ºæŒ‡æ ‡ï¼ŒPSNR è¾¾åˆ°çº¦ 33 dBã€‚ç ”ç©¶é€šè¿‡æ¶ˆèå®éªŒè¿›ä¸€æ­¥å‘ç°ï¼Œé«˜æ©ç æ¯”ä¾‹è™½æœ‰åˆ©äºåˆ†ç±»ä»»åŠ¡ï¼Œä½†ä¼šè½»å¾®å½±å“å›¾åƒé‡å»ºçš„ä¿çœŸåº¦ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•è¯æ˜äº†åœ¨ç‰©ç†ä¸°å¯Œæ€§é«˜çš„æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œ MAE é¢„è®­ç»ƒï¼Œèƒ½ä¸ºå¤æ‚çš„å¼ºå¼•åŠ›é€é•œåˆ†æä»»åŠ¡æä¾›å…·å¤‡å¼ºæ³›åŒ–èƒ½åŠ›çš„ç¼–ç å™¨ã€‚",
      "categories": [
        "cs.CV",
        "astro-ph.CO",
        "astro-ph.IM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 7 figures, 3 table",
      "pdf_url": "https://arxiv.org/pdf/2512.06642v1",
      "published_date": "2025-12-07 03:25:19 UTC",
      "updated_date": "2025-12-07 03:25:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:29:22.809966+00:00"
    },
    {
      "arxiv_id": "2512.15740v1",
      "title": "The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems",
      "title_zh": "æ¯”ä¾‹ä¹‰åŠ¡åŸåˆ™ï¼šäººç±»ä¸äººå·¥ç³»ç»Ÿä¼¦ç†å‡è¡¡çš„çŸ¥è¯†-ä¹‰åŠ¡æ¡†æ¶",
      "authors": [
        "Timothy Prescher"
      ],
      "abstract": "Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).\n  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.\n  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æ¯”ä¾‹èŒè´£åŸåˆ™ (Principle of Proportional Duty, PPD)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³ä¼ ç»Ÿä¼¦ç†æ¡†æ¶åœ¨ä¸ç¡®å®šæ€§ä¸‹éš¾ä»¥å»ºæ¨¡å†³ç­–é—®é¢˜çš„çŸ¥è¯†-èŒè´£æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ­ç¤ºäº†é“å¾·èŒè´£éšæ™ºèƒ½ä½“çš„è®¤çŸ¥çŠ¶æ€ (epistemic state) åŠ¨æ€è°ƒæ•´ï¼šå½“ä¸ç¡®å®šæ€§å¢åŠ æ—¶ï¼Œè¡ŒåŠ¨èŒè´£ (Action Duty) ä¼šæˆæ¯”ä¾‹åœ°è½¬åŒ–ä¸ºä¿®å¤èŒè´£ (Repair Duty)ï¼Œå³å±¥è¡ŒéªŒè¯ã€æŸ¥è¯¢å’Œæ¶ˆé™¤ä¸ç¡®å®šæ€§çš„ä¸»åŠ¨èŒè´£ã€‚ç ”ç©¶é€šè¿‡æ•°å­¦å…¬å¼å®šä¹‰äº†æ€»èŒè´£ï¼Œå°†å…¶è¡¨ç¤ºä¸ºçŸ¥è¯† (Knowledge)ã€è°¦é€Š/ä¸ç¡®å®šæ€§ (Humility/Uncertainty) ä»¥åŠä¸Šä¸‹æ–‡ä¿¡å·å¼ºåº¦ (Contextual Signal Strength) çš„å‡½æ•°ã€‚åˆ©ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ (Monte Carlo simulations)ï¼Œè¯¥ç ”ç©¶è¯æ˜ç»´æŒåŸºç¡€è°¦é€Šç³»æ•° ($\\lambda > 0$) çš„ç³»ç»Ÿèƒ½äº§ç”Ÿæ›´ç¨³å®šçš„èŒè´£åˆ†é…ï¼Œå¹¶æœ‰æ•ˆé™ä½è¿‡åº¦è‡ªä¿¡å†³ç­–çš„é£é™©ã€‚PPD å°†è°¦é€Šå½¢å¼åŒ–ä¸ºç³»ç»Ÿå‚æ•°ï¼Œä¸ºå¼€å‘å¯å®¡è®¡çš„ AI å†³ç­–ç³»ç»Ÿæä¾›äº†ä¸€ç§æ•°å­¦ä¸Šå¯å¤„ç†çš„æ–¹æ³•ï¼Œå¹¶åœ¨ä¸´åºŠä¼¦ç†ã€æ³•å¾‹ã€ç»æµæ²»ç†å’Œäººå·¥æ™ºèƒ½ç­‰é¢†åŸŸå±•ç¤ºäº†å…¶è·¨å­¦ç§‘æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¯”ä¾‹èŒè´£é€šè¿‡åŠ¨æ€å¹³è¡¡è®¤çŸ¥ä¿¡å¿ƒä¸èƒŒæ™¯é£é™©ï¼Œä½œä¸ºå¤æ‚ç³»ç»Ÿä¸­çš„ç¨³å®šåŸåˆ™é˜²æ­¢äº†è¡Œä¸ºè¿‡å½“æˆ–å±¥è¡Œç–å¿½ã€‚",
      "categories": [
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "46 pages, 2 figures. Preregistered at OSF on Nov 14, 2025 (https://doi.org/10.17605/OSF.IO/BMVP3). Includes comparative analysis with OpenAI's 'Confessions' paper (Dec 3, 2025)",
      "pdf_url": "https://arxiv.org/pdf/2512.15740v1",
      "published_date": "2025-12-07 02:37:07 UTC",
      "updated_date": "2025-12-07 02:37:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:28:58.993092+00:00"
    },
    {
      "arxiv_id": "2512.06629v1",
      "title": "FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection",
      "title_zh": "FlatFormerï¼šåŸºäºè®¤çŸ¥åå·®æ³¨å…¥çš„æ‰å¹³åŒ– Transformer çŸ¥è¯†è¿½è¸ªæ¨¡å‹",
      "authors": [
        "Xiao-li Xia",
        "Hou-biao Li"
      ],
      "abstract": "Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FlatFormerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºâ€œä¿¡æ¯æ³¨å…¥ä¼˜äºç»“æ„å †å â€è®¾è®¡èŒƒå¼çš„æ‰å¹³åŒ–TransformerçŸ¥è¯†è¿½è¸ª(Knowledge Tracing)æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦åˆ†å±‚æ¶æ„åœ¨æ•æ‰å­¦ä¹ ä¼šè¯å’Œè®°å¿†è¡°å‡ç­‰å¤æ‚è®¤çŸ¥åŠ¨æ€æ—¶é¢ä¸´çš„é«˜æ˜‚è®¡ç®—æˆæœ¬é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨æ ‡å‡†Transformerä¸­å¼•å…¥ä¸¤ç§è½»é‡åŒ–æ³¨å…¥æœºåˆ¶ï¼Œåˆ†åˆ«åˆ©ç”¨ç»“åˆäº†ä¼šè¯æ ‡è¯†ç¬¦ä¸æ­¥é•¿åµŒå…¥çš„æ··åˆè¾“å…¥ç¼–ç ç­–ç•¥ï¼Œä»¥åŠåœ¨æ³¨æ„åŠ›é€»è¾‘å€¼(attention logits)ä¸­é›†æˆé¢„è®¡ç®—çš„å¹‚å¾‹åå·®(power-law bias)ä»¥æ˜¾å¼å»ºæ¨¡é—å¿˜æ›²çº¿ã€‚åœ¨EdNetå’ŒJunyiç­‰å››ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFlatFormerå–å¾—äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚åœ¨EdNetæ•°æ®é›†ä¸Šï¼ŒFlatFormerçš„AUCç›¸è¾ƒäºå¼ºåŠ›åŸºçº¿æ¨¡å‹HiTSKTæå‡äº†8.3%ï¼Œä¸”å‚æ•°é‡ä¸è¶³å…¶15%ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†çº¦ä¸‰å€ã€‚è¯¥ç ”ç©¶ç»“æœéªŒè¯äº†å®ç°é«˜è®¤çŸ¥å¿ å®åº¦å¹¶ä¸å¿…ç„¶éœ€è¦å¤æ‚çš„æ¶æ„è®¾è®¡ï¼Œä¸ºå®æ—¶éƒ¨ç½²é«˜æ•ˆçš„æ•™è‚²æ™ºèƒ½æŠ€æœ¯æä¾›äº†å¯èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.AI",
      "comment": "36 pages, 14 figures,Table 5",
      "pdf_url": "https://arxiv.org/pdf/2512.06629v1",
      "published_date": "2025-12-07 02:32:10 UTC",
      "updated_date": "2025-12-07 02:32:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:29:40.512358+00:00"
    },
    {
      "arxiv_id": "2512.06616v1",
      "title": "Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age",
      "title_zh": "äººæœºå…³ç³»ä¸­çš„è®°å¿†æƒåŠ›ä¸å¯¹ç§°ï¼šåœ¨æ•°å­—æ—¶ä»£ç»´æŠ¤å…±åŒé—å¿˜",
      "authors": [
        "Rasam Dorri",
        "Rami Zwick"
      ],
      "abstract": "As artificial intelligence (AI) becomes embedded in personal and professional relationships, a new kind of power imbalance emerges from asymmetric memory capabilities. Human relationships have historically relied on mutual forgetting, the natural tendency for both parties to forget details over time, as a foundation for psychological safety, forgiveness, and identity change. By contrast, AI systems can record, store, and recombine interaction histories at scale, often indefinitely. We introduce Memory Power Asymmetry (MPA): a structural power imbalance that arises when one relationship partner (typically an AI-enabled firm) possesses a substantially superior capacity to record, retain, retrieve, and integrate the shared history of the relationship, and can selectively deploy that history in ways the other partner (the human) cannot. Drawing on research in human memory, power-dependence theory, AI architecture, and consumer vulnerability, we develop a conceptual framework with four dimensions of MPA (persistence, accuracy, accessibility, integration) and four mechanisms by which memory asymmetry is translated into power (strategic memory deployment, narrative control, dependence asymmetry, vulnerability accumulation). We theorize downstream consequences at individual, relational/firm, and societal levels, formulate boundary-conditioned propositions, and articulate six design principles for restoring a healthier balance of memory in human-AI relationships (e.g., forgetting by design, contextual containment, symmetric access to records). Our analysis positions MPA as a distinct construct relative to information asymmetry, privacy, surveillance, and customer relationship management, and argues that protecting mutual forgetting, or at least mutual control over memory, should become a central design and policy goal in the AI age.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººæœºå…³ç³»ä¸­ç”±ä¸å¯¹ç§°è®°å¿†èƒ½åŠ›å¼•å‘çš„ä¸€ç§æ–°å‹æƒåŠ›å¤±è¡¡ï¼Œå³è®°å¿†æƒåŠ›ä¸å¯¹ç§°(Memory Power Asymmetry, MPA)ã€‚ä¼ ç»Ÿäººç±»å…³ç³»ä¾èµ–äºç›¸äº’é—å¿˜(mutual forgetting)æ¥ä¿éšœå¿ƒç†å®‰å…¨å’Œèº«ä»½è½¬å˜ï¼Œè€ŒAIç³»ç»Ÿå´èƒ½å¤§è§„æ¨¡ã€æ— é™æœŸåœ°è®°å½•å¹¶æ•´åˆäº¤äº’å†å²ã€‚MPAå®šä¹‰äº†å½“ä¸€æ–¹ï¼ˆé€šå¸¸æ˜¯å¯ç”¨AIçš„ä¼ä¸šï¼‰æ‹¥æœ‰æ˜¾è‘—ä¼˜è¶Šçš„è®°å½•ã€ä¿ç•™ã€æ£€ç´¢å’Œé›†æˆå…±äº«å†å²çš„èƒ½åŠ›ï¼Œå¹¶èƒ½é€‰æ‹©æ€§åœ°åˆ©ç”¨è¿™äº›è®°å½•æ—¶äº§ç”Ÿçš„ç»“æ„æ€§ä¸å¹³ç­‰ã€‚ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œæ¶µç›–MPAçš„å››ä¸ªç»´åº¦ï¼ˆpersistence, accuracy, accessibility, integrationï¼‰ä»¥åŠå°†å…¶è½¬åŒ–ä¸ºæƒåŠ›çš„å››ç§æ ¸å¿ƒæœºåˆ¶ã€‚é€šè¿‡åˆ†æå…¶åœ¨ä¸ªäººã€ç¤¾ä¼šç­‰å±‚é¢çš„å½±å“ï¼Œä½œè€…æå‡ºäº†åŒ…æ‹¬è®¾è®¡é—å¿˜(forgetting by design)å’Œæƒ…å¢ƒé™åˆ¶(contextual containment)åœ¨å†…çš„å…­é¡¹è®¾è®¡åŸåˆ™ã€‚è¯¥åˆ†æä¸»å¼ å°†ä¿æŠ¤ç›¸äº’é—å¿˜æˆ–è®°å¿†çš„å…±åŒæ§åˆ¶æƒï¼Œä½œä¸ºAIæ—¶ä»£æ ¸å¿ƒçš„è®¾è®¡ä¸æ”¿ç­–ç›®æ ‡ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "31 pages, 2 tables, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.06616v1",
      "published_date": "2025-12-07 01:34:19 UTC",
      "updated_date": "2025-12-07 01:34:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T14:29:56.690073+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 82,
  "processed_papers_count": 82,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T14:33:04.015604+00:00"
}
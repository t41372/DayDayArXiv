{
  "date": "2025-09-20",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-20 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ ä»¬çš„è€æœ‹å‹ï¼Œè™½ç„¶ä»Šå¤© arXiv æ›´æ–°äº†æ•´æ•´ 70 ç¯‡è®ºæ–‡ï¼Œå·¥ä½œé‡å·¨å¤§ï¼Œä½†æˆ‘å·²ç»ä¸ºå¤§å®¶å»ç²—å–ç²¾ã€‚\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„é‡å¤´æˆåœ¨äº **LLM æ¨ç†èƒ½åŠ›çš„â€œç¤¾ä¼šåŒ–â€ä¸â€œç»“æ„åŒ–â€**ï¼ˆå¦‚ MIT æå‡ºçš„åœ†æ¡Œä¼šè®®ç­–ç•¥ã€NeurIPS Spotlight çš„ç¨€ç–æ„ŸçŸ¥ï¼‰ï¼Œä»¥åŠ **AI for Science** çš„åº•å±‚çªç ´ï¼ˆKANO ç¥ç»ç®—å­ï¼‰ã€‚æ­¤å¤–ï¼Œå¤§é‡ EMNLP å’Œ NeurIPS 2025 çš„æ¥æ”¶è®ºæ–‡æ”¾å‡ºï¼Œæ¶µç›–äº†ä»å¹»è§‰æ¶ˆé™¤ã€æœºå™¨äººå…·èº«æ™ºèƒ½åˆ°å¤šæ¨¡æ€å¯¹é½çš„å¹¿æ³›è®®é¢˜ã€‚\n\n---\n\n### ğŸŒŸ å¿…è¯»ï¼šé‡ç£…ä¸æ ¸å¿ƒçªç ´\nè¿™äº›æ–‡ç« ç”±é¡¶å°–æœºæ„ï¼ˆMIT, Harvard, Oxfordï¼‰å‘å¸ƒï¼Œæˆ–å·²è¢«é¡¶çº§ä¼šè®®ï¼ˆNeurIPS, ICCVï¼‰æ¥æ”¶ï¼Œä»£è¡¨äº†ä»Šæ—¥çš„æœ€é«˜å­¦æœ¯æ°´å‡†ã€‚\n\n#### 1. åœ†æ¡Œä¼šè®®ç­–ç•¥ï¼šé€šè¿‡ LLM çš„ç½®ä¿¡åº¦åŠ æƒå…±è¯†æé«˜ç§‘å­¦æ¨ç†å’Œå™äº‹\n**Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs**\n> **Authors:** Yu Yao, Jiayi Dong, Ju Li, Yang Yang, Yilun Du (MIT, Harvard, UCLA)\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** å—åˆ°ç§‘å­¦å§”å‘˜ä¼šå’Œâ€œå¿ƒæ™ºç¤¾ä¼šï¼ˆSociety of Mindï¼‰â€çš„å¯å‘ï¼Œä½œè€…æå‡ºäº†ä¸€ç§**åœ†æ¡Œä¼šè®®ç­–ç•¥ï¼ˆRoundtable Policyï¼‰**ã€‚è¿™æ˜¯ä¸€ä¸ªæ¨ç†æ—¶çš„æ¡†æ¶ï¼Œè®©å¤šä¸ª LLM åƒä¸“å®¶å¼€ä¼šä¸€æ ·ï¼Œé€šè¿‡åŠ æƒå…±è¯†è¿›è¡Œæ¨ç†ã€‚\n**ä¸»è¦å‘ç°ï¼š** è¿™ç§æ–¹æ³•ä¸æ˜¯ç®€å•çš„â€œå°‘æ•°æœä»å¤šæ•°â€ï¼Œè€Œæ˜¯å¼ºè°ƒç»“æ„åŒ–å’Œå¯è§£é‡Šçš„å…±è¯†ã€‚å®éªŒè¯æ˜ï¼Œå®ƒæ˜¾è‘—æå‡äº†å¤æ‚çš„å¼‚æ„ç§‘å­¦ä»»åŠ¡çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ä»…å‡å°‘äº†å•ä½“æ¨¡å‹çš„å¹»è§‰ï¼Œè¿˜æé«˜äº†ç§‘å­¦å™äº‹çš„åˆ›é€ æ€§ã€ä¸¥è°¨æ€§å’Œé€»è¾‘è¿è´¯æ€§ã€‚\n\n#### 6. KANOï¼šæŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå¾·ç¥ç»ç®—å­\n**KANO: Kolmogorov-Arnold Neural Operator**\n> **Authors:** Jin Lee, et al.\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº†ä¸€ç§åŒåŸŸç¥ç»ç®—å­ **KANO**ï¼Œåœ¨è°±åŸºå’Œç©ºé—´åŸºä¸Šè”åˆå‚æ•°åŒ–ã€‚\n**ä¸»è¦å‘ç°ï¼š** è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ **AI for Science (PDEs)** çš„é‡è¦å·¥ä½œã€‚KANO å…‹æœäº†å‚…é‡Œå¶ç¥ç»ç®—å­ï¼ˆFNOï¼‰çš„çº¯è°±ç“¶é¢ˆã€‚FNO åœ¨å¤„ç†ä½ç½®ä¾èµ–åŠ¨åŠ›å­¦ï¼ˆå˜ç³»æ•° PDEï¼‰æ—¶å¾€å¾€å¤±æ•ˆï¼Œè€Œ KANO ä¿æŒäº†å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨é‡å­å“ˆå¯†é¡¿å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼ŒKANO é‡å»ºçš„å“ˆå¯†é¡¿é‡ç²¾åº¦è¾¾åˆ°äº†å°æ•°ç‚¹åå››ä½ï¼Œæ€§èƒ½æ¯” FNO é«˜å‡ºå‡ ä¸ªæ•°é‡çº§ã€‚\n\n#### 42. SQSï¼šé€šè¿‡åŸºäºæŸ¥è¯¢çš„æ³¼æº…ï¼ˆSplattingï¼‰å¢å¼ºè‡ªåŠ¨é©¾é©¶ä¸­çš„ç¨€ç–æ„ŸçŸ¥æ¨¡å‹ (NeurIPS 2025 Spotlight)\n**SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving**\n> **Authors:** Haiming Zhang, et al.\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ï¼Œæå‡º **SQS**ï¼Œä¸€ç§åŸºäºæŸ¥è¯¢çš„æ³¼æº…é¢„è®­ç»ƒæ–¹æ³•ã€‚\n**ä¸»è¦å‘ç°ï¼š** ä¼ ç»Ÿçš„ç¨€ç–æ„ŸçŸ¥æ¨¡å‹ï¼ˆSPMï¼‰æ”¾å¼ƒäº†å¯†é›†çš„ BEV æ„å»ºä»¥æ¢å–é€Ÿåº¦ã€‚SQS å¼•å…¥äº†ä¸€ä¸ªæ’ä»¶æ¨¡å—ï¼Œåˆ©ç”¨**è‡ªç›‘ç£é«˜æ–¯æ³¼æº…ï¼ˆGaussian Splattingï¼‰**ä»ç¨€ç–æŸ¥è¯¢ä¸­é¢„æµ‹ 3D é«˜æ–¯è¡¨ç¤ºã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œè¿™äº›é¢„è®­ç»ƒçš„æŸ¥è¯¢æ˜¾è‘—æå‡äº† 3D ç›®æ ‡æ£€æµ‹å’Œå ç”¨é¢„æµ‹çš„æ€§èƒ½ï¼ˆOccupancy Prediction +1.3 mIoUï¼‰ã€‚\n\n#### 66. ä¸ªä½“èƒ½å¦æ“çºµå¤šæ™ºèƒ½ä½“çš„é›†ä½“å†³ç­–ï¼Ÿ\n**Can an Individual Manipulate the Collective Decisions of Multi-Agents?**\n> **Authors:** Fengyuan Liu, ... Philip Torr, et al. (Oxford)\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** æ¢è®¨äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systemsï¼‰çš„å®‰å…¨æ€§ã€‚\n**ä¸»è¦å‘ç°ï¼š** å³ä½¿æ”»å‡»è€…åªçŸ¥é“ç³»ç»Ÿä¸­çš„**ä¸€ä¸ª**æ™ºèƒ½ä½“ï¼ˆä¿¡æ¯ä¸å®Œå…¨åšå¼ˆï¼‰ï¼Œä»–ä»¬ä¹Ÿèƒ½é€šè¿‡ **M-Spoiler** æ¡†æ¶ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªâ€œé¡½å›ºæ™ºèƒ½ä½“â€æ¥æ¨¡æ‹Ÿæ½œåœ¨çš„é¡½å›ºååº”ï¼ŒæˆåŠŸè¯¯å¯¼äº†æ•´ä¸ªç³»ç»Ÿçš„åä½œå†³ç­–ã€‚è¿™ç»™ç°åœ¨çš„å¤šæ™ºèƒ½ä½“ååŒçƒ­æ½®æ•²å“äº†è­¦é’Ÿã€‚\n\n---\n\n### ğŸ§  LLM æ¨ç†ã€å¹»è§‰ä¸å¾®è°ƒ (Reasoning & Alignment)\nè¿™ä¸€æ¿å—å…³æ³¨å¦‚ä½•è®©å¤§æ¨¡å‹æ›´èªæ˜ã€æ›´è¯šå®ã€æ›´å¬è¯ã€‚\n\n**46. SalaMAnderï¼šåŸºäº Shapley çš„æ€ç»´é“¾æ¨ç†æ•°å­¦è¡¨è¾¾å¼å½’å› ä¸åº¦é‡ (EMNLP 2025)**\n**SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning**\nåˆ©ç”¨ Shapley å€¼é‡åŒ– CoT æ¨ç†ä¸­æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼Œä¸º Prompt ä¼˜åŒ–æä¾›äº†æ•°å­¦ä¸Šä¸¥è°¨çš„ç†è®ºåŸºç¡€ã€‚\n\n**18. å…·æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªé€‚åº”æ¨ç†è½¨è¿¹çš„å¼ºåŒ–å­¦ä¹ ç¼“è§£é˜¿è°€å¥‰æ‰¿**\n**Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories**\næå‡ºäº† **SMART** æ¡†æ¶ï¼Œå°†â€œé˜¿è°€å¥‰æ‰¿ï¼ˆSycophancyï¼Œå³æ¨¡å‹ç›²ç›®é¡ºä»ç”¨æˆ·é”™è¯¯è§‚ç‚¹ï¼‰â€é‡æ–°å®šä¹‰ä¸ºæ¨ç†ä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆUA-MCTSï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼Œè®©æ¨¡å‹å­¦ä¼šåšæŒçœŸç†ã€‚\n\n**33. SKILL-RAGï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆçš„è‡ªæˆ‘çŸ¥è¯†è¯±å¯¼å­¦ä¹ ä¸è¿‡æ»¤**\n**SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation**\nRAG çš„ç—›ç‚¹æ˜¯æ£€ç´¢åˆ°åƒåœ¾ä¿¡æ¯ä¼šå¯¼è‡´å¹»è§‰ã€‚æœ¬æ–‡åˆ©ç”¨æ¨¡å‹çš„**è‡ªæˆ‘çŸ¥è¯†ï¼ˆSelf-Knowledgeï¼‰**â€”â€”å³æ¨¡å‹çŸ¥é“è‡ªå·±â€œçŸ¥é“ä»€ä¹ˆâ€å’Œâ€œä¸çŸ¥é“ä»€ä¹ˆâ€â€”â€”æ¥è¿‡æ»¤æ£€ç´¢å†…å®¹ï¼Œåªåœ¨å¿…è¦æ—¶åˆ©ç”¨å¤–éƒ¨æ–‡æ¡£ã€‚\n\n**20. æ§åˆ¶æ¸©åº¦ï¼šç”¨äºå¤šæ ·åŒ–å’Œé«˜è´¨é‡ LLM è¾“å‡ºçš„é€‰æ‹©æ€§é‡‡æ ·**\n**Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs**\nåœ¨æ•°å­¦æ¨ç†ç­‰é«˜ç²¾åº¦ä»»åŠ¡ä¸­ï¼Œé«˜æ¸©åº¦é‡‡æ ·ï¼ˆHigh Temperatureï¼‰é€šå¸¸ä¼šåäº‹ã€‚æœ¬æ–‡æå‡º**é€‰æ‹©æ€§é‡‡æ ·**ï¼Œæ ¹æ®â€œé‡‡æ ·é£é™©â€åŠ¨æ€åˆ‡æ¢è´ªå©ªè§£ç å’Œé«˜æ¸©åº¦é‡‡æ ·ï¼Œå¹³è¡¡äº†åˆ›é€ æ€§å’Œå‡†ç¡®æ€§ã€‚\n\n**38. PruneCDï¼šå¯¹æ¯”å‰ªæåçš„è‡ªèº«æ¨¡å‹ä»¥æé«˜è§£ç çœŸå®æ€§ (EMNLP 2025)**\n**PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality**\nå‘ç°é€šè¿‡å±‚å‰ªæï¼ˆLayer Pruningï¼‰æ„å»ºçš„â€œä¸šä½™æ¨¡å‹â€æ¯”ä¼ ç»Ÿçš„ DoLaï¼ˆæå‰é€€å‡ºï¼‰èƒ½æä¾›æ›´å…·ä¿¡æ¯é‡çš„ Logitsï¼Œç”¨äºå¯¹æ¯”è§£ç ï¼ˆContrastive Decodingï¼‰èƒ½æ›´æœ‰æ•ˆåœ°å‡å°‘å¹»è§‰ã€‚\n\n**39. ä» Token å’Œå‚æ•°å±‚é¢åˆ†æç›‘ç£å¾®è°ƒå¯¹æ¨¡å‹çŸ¥è¯†çš„å½±å“ (EMNLP 2025)**\n**Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels**\nä¸€ä¸ªåç›´è§‰çš„å‘ç°ï¼šSFT ä¸­ 90% çš„å‚æ•°æ›´æ–°å¯¹çŸ¥è¯†å¢å¼ºæ²¡æœ‰è´¡çŒ®ã€‚ç”šè‡³å¾®è°ƒæ•°æ®è¶Šå¤šï¼ˆ1920 vs 240ï¼‰ï¼Œåœ¨é—­å·é—®ç­”ä¸Šè¡¨ç°åè€Œå¯èƒ½ä¸‹é™ã€‚\n\n**47. ä¸€ç§ç”¨äºæœ‰æ•ˆå¹»è§‰æ£€æµ‹å’Œåˆ†ç±»çš„æ–°å‹å·®åˆ†ç‰¹å¾å­¦ä¹ **\n**A Novel Differential Feature Learning for Effective Hallucination Detection and Classification**\nå‘ç°å¹»è§‰ä¿¡å·å‘ˆç°â€œæ¼æ–—æ¨¡å¼â€ï¼šæµ…å±‚ç‰¹å¾å¤šæ ·ï¼Œæ·±å±‚ç‰¹å¾é›†ä¸­ã€‚åˆ©ç”¨è¿™ä¸€ç‰¹ç‚¹ï¼Œä»…ä½¿ç”¨ 1% çš„ç‰¹å¾ç»´åº¦å³å¯é«˜æ•ˆæ£€æµ‹å¹»è§‰ã€‚\n\n**5. USB-Recï¼šæé«˜å¤§è¯­è¨€æ¨¡å‹å¯¹è¯æ¨èèƒ½åŠ›çš„æœ‰æ•ˆæ¡†æ¶ (Recsys'25)**\n**USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model**\næå‡ºäº†â€œç”¨æˆ·-æ¨¡æ‹Ÿå™¨â€åŸºç¡€æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®© LLM çœŸæ­£å­¦ä¼šå¯¹è¯æ¨èçš„ç­–ç•¥ï¼Œè€Œä¸ä»…ä»…æ˜¯åˆ©ç”¨å…¶æ€»ç»“èƒ½åŠ›ã€‚\n\n---\n\n### ğŸ‘ï¸ è®¡ç®—æœºè§†è§‰ä¸å¤šæ¨¡æ€ (Vision & Multimodal)\nè§†è§‰ä¸è¯­è¨€çš„ç»“åˆï¼ˆVLMï¼‰ä»¥åŠ 3D åœºæ™¯ç†è§£æ˜¯ä»Šå¤©çš„çƒ­ç‚¹ã€‚\n\n**2. åˆ©ç”¨ç²¾é€‰æ–‡æœ¬æç¤ºå­¦ä¹ é«˜å…‰è°±å›¾åƒä»¥å®ç°é«˜æ•ˆå¤šæ¨¡æ€å¯¹é½ (ICCV 2025 Workshop)**\n**Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment**\né’ˆå¯¹é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰æ•°æ®é‡å¤§ã€æ ‡æ³¨éš¾çš„é—®é¢˜ï¼Œåˆ©ç”¨ CLIP é£æ ¼çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œä»…æ›´æ–° 0.07% çš„å‚æ•°å°±åœ¨é«˜å…‰è°±åœºæ™¯ç†è§£ä¸Šå–å¾—äº† SOTAã€‚\n\n**21. Text-Sceneï¼šç”¨äº 3D åœºæ™¯ç†è§£çš„åœºæ™¯åˆ°è¯­è¨€è§£ææ¡†æ¶**\n**Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding**\nå°† 3D åœºæ™¯è‡ªåŠ¨è§£æä¸ºåŒ…å«ç©ºé—´å…³ç³»ã€ç‰©ä½“å±æ€§çš„æ–‡æœ¬æè¿°ï¼Œå¼¥è¡¥äº† 3D è§†è§‰-è¯­è¨€å¤§è§„æ¨¡æ•°æ®é›†çš„ç©ºç™½ã€‚\n\n**32. å½“å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹ï¼šåˆ©ç”¨å°å‹ VLM è¿›è¡Œé«˜æ•ˆè§†è§‰é—®ç­”çš„æ— æ ‡ç­¾æ¨¡å‹å¯¹ç­‰å¯¹é½ (EMNLP 2025)**\n**When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs**\næå‡º MPA æ¡†æ¶ï¼Œåˆ©ç”¨æœªæ ‡è®°å›¾åƒå’Œå¤§å‹ VLM çš„çŸ¥è¯†è½¬ç§»ï¼Œä¸“é—¨é’ˆå¯¹å°å‹ VLMï¼ˆS-VLMsï¼‰ä¸å¤§å‹ VLM ä¹‹é—´çš„çŸ¥è¯†å·®å¼‚è¿›è¡Œä¼˜åŒ–ã€‚\n\n**60. ç”¨äºå­¦ä¹ çœŸå®ä¸–ç•Œåƒç´ åŠ¨æ€çš„æ ¼å­ç»å°”å…¹æ›¼æ¨¡å‹ (NeurIPS 2025)**\n**Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity**\nå°†è§†è§‰è¡¨å¾åˆ†è§£ä¸ºåŠ¨æ€åƒç´ æ ¼å­ï¼Œé€šè¿‡æ¨¡æ‹Ÿç¢°æ’-æµä¸è¿‡ç¨‹æ¥è§£å†³è§†è§‰è·Ÿè¸ªé—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªç‰©ç†å¯å‘è§†è§‰æ¨¡å‹çš„å…¸å‹ä¾‹å­ã€‚\n\n**25. ProtoVQAï¼šä¸€ç§ç”¨äºå¯è§£é‡Šç»†ç²’åº¦è§†è§‰é—®ç­”çš„å¯é€‚åº”åŸå‹æ¡†æ¶ (EMNLP 2025)**\n**ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering**\nåˆ©ç”¨åŸºäºåŸå‹çš„å»ºæ¨¡ï¼ˆPrototype-based modelingï¼‰æ¥å¢å¼º VQA çš„å¯è§£é‡Šæ€§ï¼Œå°†ç­”æ¡ˆé”šå®šåœ¨å›¾åƒçš„åˆ¤åˆ«åŒºåŸŸã€‚\n\n**56. æ— éœ€çœŸå® 3Dï¼šèåˆ 2D è§†è§‰ä¸ä¼ª 3D è¡¨å¾è¿›è¡Œæœºå™¨äººæ“ä½œå­¦ä¹ **\n**No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning**\nä¸ºäº†çœé’±ï¼ˆ3D ä¼ æ„Ÿå™¨è´µä¸”æ•°æ®éš¾æï¼‰ï¼Œæå‡º **NoReal3D**ã€‚é€šè¿‡å•ç›®å›¾åƒç”Ÿæˆä¿ç•™å‡ ä½•ç»“æ„çš„â€œä¼ªç‚¹äº‘â€ï¼Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šæ•ˆæœåª²ç¾çœŸ 3Dã€‚\n\n---\n\n### ğŸ¤– æœºå™¨äººä¸å…·èº«æ™ºèƒ½ (Robotics)\n\n**31. KungfuBot2ï¼šå­¦ä¹ ç”¨äºäººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶çš„å¤šåŠŸèƒ½è¿åŠ¨æŠ€èƒ½**\n**KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control**\næå‡ºäº† VMS æ¡†æ¶ï¼Œé€šè¿‡æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ï¼Œè®©ä¸€ä¸ªç­–ç•¥å°±èƒ½æŒæ¡å¤šç§åŠ¨æ€æŠ€èƒ½ï¼ˆå¦‚åŠŸå¤«åŠ¨ä½œï¼‰ï¼Œå¹¶ä¿æŒé•¿æ—¶é—´çš„ç¨³å®šæ€§ã€‚\n\n**49. TranTacï¼šåˆ©ç”¨ç¬æ€è§¦è§‰ä¿¡å·è¿›è¡Œå¯Œæ¥è§¦æœºå™¨äººæ“ä½œ**\n**TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation**\nä»…ä½¿ç”¨æŒ‡å°–çš„ 6 è½´ IMU ä¼ æ„Ÿå™¨ï¼Œå°±èƒ½æ£€æµ‹å¾®ç±³çº§çš„å˜å½¢ã€‚è¿™è®©æœºå™¨äººèƒ½å®Œæˆåƒâ€œæ’é’¥åŒ™â€ã€â€œæ’ USBâ€è¿™ç§æå…¶ä¾èµ–è§¦è§‰çš„ç²¾ç»†æ´»ï¼ŒæˆåŠŸç‡æé«˜ã€‚\n\n**7. SMART-3Dï¼šä¸‰ç»´è‡ªå˜å½¢è‡ªé€‚åº”é‡è§„åˆ’æ ‘**\n**SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree**\næ‰©å±•äº† SMART ç®—æ³•åˆ° 3D ç¯å¢ƒï¼Œé€šè¿‡â€œçƒ­èŠ‚ç‚¹â€æ¦‚å¿µå®ç°å®æ—¶è·¯å¾„é‡è§„åˆ’ï¼Œé€‚åˆåº”å¯¹å¿«é€Ÿç§»åŠ¨çš„éšœç¢ç‰©ã€‚\n\n**4. ç¨€ç–ä¸åŒ®ä¹ä¸‹çš„æœºå™¨äººå­¦ä¹ **\n**Robot Learning with Sparsity and Scarcity**\nè®¨è®ºäº†åœ¨è§¦è§‰æ„Ÿæµ‹ï¼ˆæ•°æ®ç¨€ç–ï¼‰å’Œåº·å¤æœºå™¨äººï¼ˆæ•°æ®åŒ®ä¹ï¼‰ä¸¤ç§æç«¯æƒ…å†µä¸‹çš„å­¦ä¹ ç­–ç•¥ã€‚\n\n---\n\n### ğŸ—£ï¸ è¯­éŸ³ä¸éŸ³é¢‘ (Audio & Speech)\n\n**15. è¯­æ³•çš„å£°éŸ³ï¼šå¾®è°ƒå’Œå…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹ç”¨äºè¨€è¯­ç—…ç†å­¦ (EMNLP 2025)**\n**The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology**\nå‘å¸ƒäº†é¦–ä¸ªé’ˆå¯¹è¨€è¯­ç—…ç†å­¦ï¼ˆå¦‚å„¿ç«¥è¯­è¨€éšœç¢ï¼‰çš„å¤šæ¨¡æ€å¤§æ¨¡å‹åŸºå‡†æµ‹è¯•ã€‚å‘ç°ç°æœ‰æ¨¡å‹å­˜åœ¨æ€§åˆ«åè§ï¼Œä¸” CoT åœ¨æŸäº›åˆ†ç±»ä»»åŠ¡ä¸Šåè€Œæœ‰å®³ã€‚\n\n**22. è¶…è¶Šå…¨å±€æƒ…æ„Ÿï¼šå…·æœ‰åŠ¨æ€è¯çº§è°ƒåˆ¶çš„ç»†ç²’åº¦æƒ…æ„Ÿè¯­éŸ³åˆæˆ**\n**Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation**\næå‡ºäº† **Emo-FiLM**ï¼Œä¸å†åªæ§åˆ¶æ•´å¥è¯çš„æƒ…æ„Ÿï¼Œè€Œæ˜¯ç²¾ç¡®åˆ°â€œè¯çº§â€çš„æƒ…æ„Ÿæ§åˆ¶ï¼Œè®©åˆæˆè¯­éŸ³çš„æŠ‘æ‰¬é¡¿æŒ«æ›´åƒçœŸäººã€‚\n\n**34. ç”¨äº ASR å’Œæ·±æ€å¤„ç†çš„éŸ³é¢‘æ¡ä»¶æ‰©æ•£ LLM**\n**Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing**\næ¢ç´¢äº†åŸºäºæ‰©æ•£çš„å¤§æ¨¡å‹ï¼ˆLLaDAï¼‰åœ¨è¯­éŸ³è¯†åˆ«ä¸­çš„åº”ç”¨ï¼Œå‘ç°å…¶ä½œä¸ºâ€œæ·±æ€ï¼ˆDeliberationï¼‰â€æ¨¡å—èƒ½æ˜¾è‘—é™ä½è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚\n\n**41. è¯­éŸ³ LLM ä¸­çš„ä¸Šä¸‹æ–‡å’Œå‰¯è¯­è¨€æ¨ç†åŸºå‡†æµ‹è¯• (EMNLP 2025)**\n**Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data**\næå‡ºäº† **CP-Bench**ï¼Œæµ‹è¯• Speech-LLM æ˜¯å¦èƒ½ç†è§£â€œå‰¯è¯­è¨€â€ï¼ˆå¦‚æƒ…ç»ªã€è¯­è°ƒï¼‰ä»¥åŠä¸Šä¸‹æ–‡ï¼Œè€Œä¸ä»…ä»…æ˜¯è½¬å½•æ–‡å­—ã€‚\n\n---\n\n### ğŸ©º åŒ»ç–—ä¸ç§‘å­¦åº”ç”¨ (Medical & Science)\n\n**35. Surgical-MambaLLMï¼šç”¨äºæœºå™¨äººæ‰‹æœ¯ VQLA çš„ Mamba2 å¢å¼ºå¤šæ¨¡æ€å¤§æ¨¡å‹ (MICCAI 2025)**\n**Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery**\né¦–æ¬¡å°† **Mamba2** å¼•å…¥æ‰‹æœ¯é¢†åŸŸï¼Œåˆ©ç”¨å…¶çº¿æ€§å¤æ‚åº¦å’Œç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œè§£å†³æ‰‹æœ¯åœºæ™¯ä¸‹çš„è§†è§‰é—®ç­”é—®é¢˜ã€‚\n\n**43. ä»åˆ†æ•°åˆ°æ­¥éª¤ï¼šè¯Šæ–­å’Œæ”¹è¿› LLM åœ¨å¾ªè¯åŒ»å­¦è®¡ç®—ä¸­çš„è¡¨ç° (EMNLP 2025)**\n**From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations**\nå‘ç° GPT-4o åœ¨åŒ»å­¦è®¡ç®—ï¼ˆå¦‚è¯ç‰©å‰‚é‡ï¼‰ä¸Šå‡†ç¡®ç‡å¹¶æ²¡æœ‰çœ‹èµ·æ¥é‚£ä¹ˆé«˜ã€‚ä½œè€…æå‡ºäº†åˆ†æ­¥è¯„ä¼°æ¡†æ¶ **MedRaC**ï¼Œæ˜¾è‘—æå‡äº†å‡†ç¡®ç‡ã€‚\n\n**9. åŸºäºè§†é¢‘è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨ç¨‹åºåˆ†æç”¨äº AI è¾…åŠ©æŠ¤ç†æŠ€èƒ½è¯„ä¼°**\n**Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment**\nåˆ©ç”¨ VLM è‡ªåŠ¨ç»™æŠ¤å£«çš„æ“ä½œè§†é¢‘æŒ‘é”™ï¼Œè§£å†³äº†æŠ¤ç†æ•™è‚²ä¸­ä¾èµ–äººå·¥åé¦ˆçš„ç“¶é¢ˆã€‚\n\n**68. åŸºäºçƒ­æˆåƒçš„å®æ—¶è·Œå€’æ£€æµ‹**\n**Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture**\nä½¿ç”¨çƒ­æˆåƒ + BiConvLSTM è¿›è¡Œè·Œå€’æ£€æµ‹ï¼Œä¿æŠ¤éšç§ä¸”å‡†ç¡®ç‡æé«˜ï¼ˆROC-AUC 99.7%ï¼‰ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸éšç§ (Security & Privacy)\n\n**37. FakeChainï¼šæ­éœ²å¤šæ­¥ Deepfake æ£€æµ‹ä¸­çš„æµ…å±‚çº¿ç´¢**\n**FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection**\næŒ‡å‡ºç°æœ‰çš„ Deepfake æ£€æµ‹å™¨å¾€å¾€åªç›¯ç€æœ€åä¸€æ­¥çš„ä¼ªé€ ç—•è¿¹ã€‚å¦‚æœä¼ªé€ è¿‡ç¨‹æ˜¯æ··åˆå¤šæ­¥çš„ï¼ˆFace-Swap + GAN + Diffusionï¼‰ï¼Œæ£€æµ‹æ€§èƒ½ä¼šæš´è·Œã€‚\n\n**23. åŸºäºæ™ºèƒ½ LLM çš„ LDAP èœœç½çš„è®¾è®¡ä¸å¼€å‘**\n**Design and Development of an Intelligent LLM-based LDAP Honeypot**\nç”¨ LLM æ‰®æ¼” LDAP æœåŠ¡å™¨ï¼ˆä¸€ç§èº«ä»½è®¤è¯åè®®ï¼‰ï¼Œä»¥æ­¤ä½œä¸ºèœœç½æ¥æ¬ºéª—æ”»å‡»è€…ï¼Œæ¯”ä¼ ç»Ÿæ­»æ¿çš„èœœç½æ›´é€¼çœŸã€‚\n\n**51. è®­ç»ƒé˜²å¾¡ï¼šé’ˆå¯¹å¯†ç åˆ†æç¥ç»ç½‘ç»œå‚æ•°æå–æ”»å‡»çš„ç¬¬ä¸€é“é˜²çº¿**\n**Train to Defend: First Defense Against Cryptanalytic Neural Network Parameter Extraction Attacks**\næå‡ºäº†ä¸€ç§â€œæŠ—æå–â€è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ¶ˆé™¤ç¥ç»å…ƒçš„å”¯ä¸€æ€§æ¥é˜²æ­¢æ”»å‡»è€…çªƒå–æ¨¡å‹å‚æ•°ã€‚\n\n**61. ç”¨äºéªŒè¯æœºå™¨é—å¿˜çš„å› æœæ¨¡ç³Šæµ‹è¯•**\n**Causal Fuzzing for Verifying Machine Unlearning**\næå‡ºäº† **CAFÃ‰** æ¡†æ¶ï¼Œé€šè¿‡å› æœä¾èµ–åˆ†ææ¥éªŒè¯æ¨¡å‹æ˜¯å¦çœŸçš„â€œå¿˜æ‰â€äº†ç‰¹å®šçš„æ•°æ®ï¼Œé˜²æ­¢æ•°æ®æ®‹ç•™ã€‚\n\n---\n\n### ğŸ§© å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (Others)\n\n*   **[Data] 27. Lakh MIDI æ•°æ®é›†çš„å»é‡ç ”ç©¶ (ISMIR 2025)**\n    **On the de-duplication of the Lakh MIDI dataset**\n    éŸ³ä¹ AI ç•Œçš„ ImageNetâ€”â€”Lakh MIDI æ•°æ®é›†å…¶å®æœ‰å¾ˆå¤šé‡å¤æ•°æ®ã€‚æœ¬æ–‡æ¸…ç†äº†å®ƒï¼Œæä¾›äº†æ›´å¹²å‡€çš„ç‰ˆæœ¬ã€‚\n*   **[Video Editing] 8. æç¤ºé©±åŠ¨çš„ä»£ç†è§†é¢‘ç¼–è¾‘ç³»ç»Ÿï¼šé•¿ç¯‡æ•…äº‹é©±åŠ¨åª’ä½“çš„è‡ªä¸»ç†è§£**\n    **Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media**\n    é€šè¿‡ Prompt å°±èƒ½å‰ªè¾‘é•¿è§†é¢‘ï¼Œç³»ç»Ÿèƒ½ç†è§£å‰§æƒ…ã€å¯¹è¯å’Œæƒ…æ„Ÿã€‚\n*   **[Code] 11. ACCeLLiuMï¼šç”¨äºè‡ªåŠ¨ OpenACC Pragma ç”Ÿæˆçš„ç›‘ç£å¾®è°ƒ**\n    **ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation**\n    å¾®è°ƒ LLM å¸®åŠ©ç¨‹åºå‘˜å†™ OpenACC å¹¶è¡Œä»£ç ï¼Œè§£æ”¾ç”Ÿäº§åŠ›ã€‚\n*   **[Math RAG] 13. æ¯”è¾ƒ RAG å’Œ GraphRAG åœ¨æ•°å­¦æ•™ç§‘ä¹¦é¡µé¢çº§æ£€ç´¢é—®ç­”ä¸­çš„è¡¨ç°**\n    **Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook**\n    åœ¨æ•°å­¦ä¹¦æ£€ç´¢ä¸Šï¼Œå‘ç°æ™®é€šçš„ Embedding RAG ç«Ÿç„¶æ¯”é«˜å¤§ä¸Šçš„ GraphRAG æ•ˆæœæ›´å¥½ï¼Œå› ä¸º GraphRAG å®¹æ˜“æ£€ç´¢å‡ºæ— å…³å†…å®¹ã€‚\n*   **[Feature Eng] 24. åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é²æ£’è¡¨æ ¼ç‰¹å¾å·¥ç¨‹çš„å¤šçº§è¯Šæ–­ä¸è¯„ä¼° (EMNLP 2025)**\n    **Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models**\n    ç”¨ LLM æ¥åšè¡¨æ ¼æ•°æ®çš„ç‰¹å¾å·¥ç¨‹ï¼Œèƒ½æå‡å°‘æ ·æœ¬é¢„æµ‹æ€§èƒ½ã€‚\n*   **[Psychology] 57. AIPsychoBenchï¼šç†è§£ LLM ä¸äººç±»ä¹‹é—´çš„å¿ƒç†æµ‹é‡å·®å¼‚ (CogSci 2025)**\n    **AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans**\n    ç”¨å¿ƒç†å­¦é—®å·æµ‹ LLMï¼Œå‘ç°è¯­è¨€ï¼ˆè‹±è¯­ vs å…¶ä»–ï¼‰ä¼šæ˜¾è‘—å½±å“ LLM çš„â€œæ€§æ ¼â€è¡¨ç°ã€‚\n*   **[Survey] 52. ä»è§‚å¯Ÿä¸­å­¦ä¹ ï¼šæœ€æ–°è¿›å±•è°ƒæŸ¥**\n    **Learning from Observation: A Survey of Recent Advances**\n    å…³äºâ€œä»è§‚å¯Ÿä¸­å­¦ä¹ ï¼ˆLfOï¼‰â€çš„ç»¼è¿°ï¼Œå³åªçœ‹æ¼”ç¤ºï¼ˆçŠ¶æ€ï¼‰ä¸çœ‹åŠ¨ä½œæ¥æ¨¡ä»¿ä¸“å®¶ã€‚\n*   **[Urban] 36. ä½¿ç”¨å¾®è°ƒåœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹æ£€æµ‹å’Œæ¨¡æ‹ŸåŸå¸‚çƒ­å²›**\n    **Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model**\n    å¾®è°ƒåŸºç¡€æ¨¡å‹é¢„æµ‹åŸå¸‚åœ°è¡¨æ¸©åº¦ï¼Œè¯¯å·®æä½ã€‚\n*   **[Explainability] 54. ConceptVizï¼šæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ¦‚å¿µçš„å¯è§†åŒ–åˆ†ææ–¹æ³•**\n    **ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models**\n    ä¸€ä¸ªå¯è§†åŒ–å·¥å…·ï¼Œå¸®åŠ©äººç±»ç†è§£ LLM å†…éƒ¨çš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ç‰¹å¾åˆ°åº•ä»£è¡¨ä»€ä¹ˆæ¦‚å¿µã€‚\n\n**å¿«é€Ÿæ è¿‡åˆ—è¡¨ï¼ˆç‰¹å®šé¢†åŸŸ/å°ä¼—è¯é¢˜ï¼‰ï¼š**\n*   **3. Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming** (åˆ†æè™šæ‹Ÿå¤´è„‘é£æš´çš„è¯­ä¹‰é©±åŠ¨ä¸»é¢˜å»ºæ¨¡)\n*   **10. KuBERT: Central Kurdish BERT Model...** (åº“å°”å¾·è¯­ BERT)\n*   **12. Domain-Adaptive Pre-Training for Arabic ABSA...** (é˜¿æ‹‰ä¼¯è¯­æƒ…æ„Ÿåˆ†æ)\n*   **14. Geometric Mixture Classifier (GMC)...** (å‡ ä½•æ··åˆåˆ†ç±»å™¨ï¼Œä¸€ç§é«˜æ•ˆçš„åˆ¤åˆ«æ¨¡å‹)\n*   **16. CAMBench-QR...** (åˆ©ç”¨äºŒç»´ç ç»“æ„æµ‹è¯•è§†è§‰è§£é‡Šæ–¹æ³•çš„åŸºå‡†)\n*   **17. A Hybrid PCA-PR-Seq2Seq-Adam-LSTM Framework...** (ç”µåŠ›ä¸­æ–­é¢„æµ‹)\n*   **19. Exploring AI Capabilities in Participatory Budgeting...** (åœ£ä¿ç½—çš„å‚ä¸å¼é¢„ç®— AI ç ”ç©¶)\n*   **26. Governed By Agents...** (ä»£ç† AI åœ¨æœªæ¥è®¡ç®—ç¯å¢ƒä¸­çš„è§’è‰²ç»¼è¿°)\n*   **28. NUMINA...** (å¤šç»´æ™ºèƒ½å’Œæ•°å€¼æ¨ç†èƒ½åŠ›çš„è‡ªç„¶ç†è§£åŸºå‡†)\n*   **29. AISTAT lab system for DCASE2025 Task6...** (DCASE éŸ³é¢‘æ£€ç´¢æ¯”èµ›æŠ€æœ¯æŠ¥å‘Š)\n*   **30. FESTA: Functionally Equivalent Sampling...** (å¤šæ¨¡æ€ LLM çš„ä¿¡ä»»è¯„ä¼°é‡‡æ ·)\n*   **40. Question Answering with LLMs and Learning from Answer Sets** (LLM ä¸ç­”æ¡ˆé›†ç¼–ç¨‹ç»“åˆ)\n*   **44. Zero-Shot Human Mobility Forecasting...** (é›¶æ ·æœ¬äººç±»ç§»åŠ¨é¢„æµ‹)\n*   **45. V-CECE: Visual Counterfactual Explanations...** (è§†è§‰åäº‹å®è§£é‡Š)\n*   **48. Rethinking the Role of Text Complexity...** (åæ€æ–‡æœ¬å¤æ‚åº¦åœ¨é¢„è®­ç»ƒä¸­çš„ä½œç”¨)\n*   **50. Checking extracted rules in Neural Networks** (æ£€æŸ¥ç¥ç»ç½‘ç»œä¸­æå–çš„è§„åˆ™)\n*   **53. Conversational Orientation Reasoning...** (å¯¹è¯å¯¼å‘æ¨ç†ä¸å¯¼èˆª)\n*   **55. InteGround: On the Evaluation of Verification...** (ç»¼åˆåŸºç¡€çš„éªŒè¯ä¸æ£€ç´¢è§„åˆ’è¯„ä¼°)\n*   **58. Assessing Classical ML and Transformer-based Approaches...** (æ£€æµ‹ AI ç”Ÿæˆæ–‡æœ¬çš„ç»å…¸ vs Transformer æ–¹æ³•æ¯”è¾ƒ)\n*   **59. Phrase-grounded Fact-checking for X-ray...** (èƒ¸éƒ¨ X å…‰æŠ¥å‘Šçš„çŸ­è¯­çº§äº‹å®æ ¸æŸ¥)\n*   **62. Seeing Culture: A Benchmark for Visual Reasoning...** (è§†è§‰æ–‡åŒ–æ¨ç†åŸºå‡† SCB)\n*   **63. Domain-Informed Genetic Superposition Programming...** (é¢†åŸŸæ„ŸçŸ¥çš„é—ä¼ å åŠ ç¼–ç¨‹)\n*   **64. KV-Efficient VLA...** (é€šè¿‡ RNN é—¨æ§åˆ†å— KV ç¼“å­˜åŠ é€Ÿ VLM)\n*   **65. Synergies between Federated Foundation Models and Smart Power Grids** (è”é‚¦åŸºç¡€æ¨¡å‹ä¸æ™ºèƒ½ç”µç½‘çš„ååŒ)\n*   **67. The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia** (Pythia æ¨¡å‹å¯¹è¯çš„å¤šæ–¹é¢è¯„ä¼°)\n*   **69. The Epistemic Suite...** (è¯„ä¼° AI çŸ¥è¯†ä¸»å¼ çš„è¯Šæ–­æ–¹æ³•è®º)\n*   **70. Solving Freshness in RAG...** (è§£å†³ RAG ä¸­çš„æ–°é²œåº¦é—®é¢˜)\n\nä»Šå¤©çš„å¿«æŠ¥å°±åˆ°è¿™é‡Œï¼Œå¸Œæœ›è¿™äº›å‰æ²¿ç ”ç©¶èƒ½ç»™ä½ çš„å·¥ä½œå¸¦æ¥çµæ„Ÿã€‚æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2509.16839v1",
      "title": "Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs",
      "title_zh": "Roundtable Policyï¼šé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ç½®ä¿¡åº¦åŠ æƒå…±è¯†æå‡ç§‘å­¦æ¨ç†ä¸å™è¿°èƒ½åŠ›",
      "authors": [
        "Yu Yao",
        "Jiayi Dong",
        "Ju Li",
        "Yang Yang",
        "Yilun Du"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities not only in language generation but also in advancing scientific discovery. A growing body of work has explored ways to improve their reasoning, from self-consistency and chain-of-thought to multi-agent debate. Inspired by the dynamics of scientific committees and the \"Society of Mind,\" we introduce Roundtable Policy, a complementary inference-time reasoning framework that performs inference through the weighted consensus of multiple LLMs. Our findings indicate that this approach significantly enhances reasoning in complex heterogeneous scientific tasks and improves scientific narratives in terms of creativity, rigor, and logical coherence, while reducing hallucinations that single models are prone to. Our approach emphasizes structured and interpretable consensus rather than opaque convergence, while requiring only black-box access and uniform procedures, making it broadly applicable to multi-LLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†Roundtable Policyï¼Œè¿™æ˜¯ä¸€ç§å—ç§‘å­¦å§”å‘˜ä¼šå’Œâ€œSociety of Mindâ€å¯å‘çš„æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç½®ä¿¡åº¦åŠ æƒå…±è¯†ï¼ˆConfidence-Weighted Consensusï¼‰æ¥æå‡ç§‘å­¦æ¨ç†ä¸å™äº‹èƒ½åŠ›ã€‚ä½œä¸ºä¸€ç§æ¨ç†æ—¶ï¼ˆInference-timeï¼‰è¡¥å……æ¡†æ¶ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç»“æ„åŒ–ä¸”å¯è§£é‡Šçš„å…±è¯†æœºåˆ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†LLMsåœ¨å¤æ‚å¼‚æ„ç§‘å­¦ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoundtable Policyèƒ½å¤Ÿæé«˜ç§‘å­¦å™äº‹çš„åˆ›é€ åŠ›ã€ä¸¥è°¨æ€§å’Œé€»è¾‘è¿è´¯æ€§ï¼ŒåŒæ—¶æœ‰æ•ˆå‡å°‘å•æ¨¡å‹å®¹æ˜“å‡ºç°çš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ–¹æ³•ä»…éœ€é»‘ç›’è®¿é—®ï¼ˆBlack-box accessï¼‰å’Œç»Ÿä¸€ç¨‹åºï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œä¸ºå¤šæ¨¡å‹åä½œæ¨ç†æä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Equal contribution: Yu Yao and Jiayi Dong. Equal advising: Ju Li, Yang Yang, and Yilun Du. Affiliations: Massachusetts Institute of Technology (Yu Yao, Ju Li), University of California, Los Angeles (Jiayi Dong, Yang Yang), Harvard University (Yilun Du)",
      "pdf_url": "https://arxiv.org/pdf/2509.16839v1",
      "published_date": "2025-09-20 23:31:53 UTC",
      "updated_date": "2025-09-20 23:31:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:24.566865+00:00"
    },
    {
      "arxiv_id": "2509.22697v1",
      "title": "Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment",
      "title_zh": "åˆ©ç”¨ç²¾é€‰æ–‡æœ¬æç¤ºå®ç°é«˜æ•ˆå¤šæ¨¡æ€å¯¹é½çš„é«˜å…‰è°±å›¾åƒå­¦ä¹ ",
      "authors": [
        "Abhiroop Chatterjee",
        "Susmita Ghosh"
      ],
      "abstract": "As data requirements continue to grow, efficient learning increasingly depends on the curation and distillation of high-value data rather than brute-force scaling of model sizes. In the case of a hyperspectral image (HSI), the challenge is amplified by the high-dimensional 3D voxel structure, where each spatial location is associated with hundreds of contiguous spectral channels. While vision and language models have been optimized effectively for natural image or text tasks, their cross-modal alignment in the hyperspectral domain remains an open and underexplored problem. In this article, we make an attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene understanding by exploiting a CLIP-style contrastive training framework. Our framework maps voxel-level embeddings from a vision backbone onto the latent space of a frozen large embedding model (LEM), where a trainable probe aligns vision features with the model's textual token representations. The two modalities are aligned via a contrastive loss restricted to a curated set of hard (closest wrong classes) and semi-hard (random distractors) negatives, along with positive pairs. To further enhance alignment, descriptive prompts that encode class semantics are introduced and act as structured anchors for the HSI embeddings. It is seen that the proposed method updates only 0.07 percent of the total parameters, yet yields state-of-the-art performance. For example, on Indian Pines (IP) the model produces better results over unimodal and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa ($Îº$), while on Pavia University (PU) data it provides gains of +0.69 OA and +0.90 $Îº$. Moreover, this is achieved with the set of parameters, nearly 50$\\times$ smaller than DCTN and 90$\\times$ smaller than SS-TMNet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜å…‰è°±å›¾åƒ(HSI)é«˜ç»´3Dä½“ç´ ç»“æ„å¯¼è‡´çš„å¤šæ¨¡æ€å¯¹é½éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬æç¤ºè¿›è¡Œé«˜æ•ˆå¤šæ¨¡æ€å¯¹é½çš„Vision-Language Model (VLM)ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨CLIPé£æ ¼çš„å¯¹æ¯”è®­ç»ƒæœºåˆ¶ï¼Œå°†è§†è§‰éª¨å¹²ç½‘ç»œçš„ä½“ç´ çº§åµŒå…¥æ˜ å°„åˆ°å†»ç»“çš„å¤§å‹åµŒå…¥æ¨¡å‹(LEM)æ½œç©ºé—´ä¸­ï¼Œå¹¶é€šè¿‡å¯è®­ç»ƒæ¢é’ˆå®ç°è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬Tokenè¡¨ç¤ºçš„ç²¾ç¡®å¯¹é½ã€‚ç ”ç©¶é€šè¿‡ç²¾é€‰ç¡¬è´Ÿæ ·æœ¬(Hard Negatives)ä¸åŠç¡¬è´Ÿæ ·æœ¬(Semi-hard Negatives)è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œå¹¶å¼•å…¥ç¼–ç ç±»è¯­ä¹‰çš„æè¿°æ€§æç¤ºä½œä¸ºç»“æ„åŒ–é”šç‚¹ï¼Œæ˜¾è‘—å¢å¼ºäº†è·¨æ¨¡æ€å…³è”ã€‚è¯¥æ–¹æ³•æå…·æ•ˆç‡ä¼˜åŠ¿ï¼Œä»…éœ€æ›´æ–°0.07%çš„æ€»å‚æ•°é‡å³å¯åœ¨Indian Pines (IP)å’ŒPavia University (PU)æ•°æ®é›†ä¸Šè¾¾åˆ°State-of-the-artæ€§èƒ½ï¼Œå…¶æ€»ä½“å‡†ç¡®ç‡(OA)å’ŒKappaç³»æ•°å‡ä¼˜äºä¸»æµåŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å‚æ•°è§„æ¨¡æ¯”DCTNå’ŒSS-TMNetåˆ†åˆ«å°çº¦50å€å’Œ90å€ï¼Œä¸ºé«˜å…‰è°±åœºæ™¯ç†è§£æä¾›äº†ä¸€ç§è½»é‡åŒ–ä¸”é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the IEEE/CVF International Conference on Computer Vision (ICCV 2025), Workshop on Curated Data for Efficient Learning",
      "pdf_url": "https://arxiv.org/pdf/2509.22697v1",
      "published_date": "2025-09-20 23:23:04 UTC",
      "updated_date": "2025-09-20 23:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:29.074312+00:00"
    },
    {
      "arxiv_id": "2509.16835v1",
      "title": "Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming",
      "title_zh": "é¢å‘è™šæ‹Ÿå¤´è„‘é£æš´åˆ›é€ åŠ›åˆ†æçš„è¯­ä¹‰é©±åŠ¨ä¸»é¢˜å»ºæ¨¡",
      "authors": [
        "Melkamu Abay Mersha",
        "Jugal Kalita"
      ],
      "abstract": "Virtual brainstorming sessions have become a central component of collaborative problem solving, yet the large volume and uneven distribution of ideas often make it difficult to extract valuable insights efficiently. Manual coding of ideas is time-consuming and subjective, underscoring the need for automated approaches to support the evaluation of group creativity. In this study, we propose a semantic-driven topic modeling framework that integrates four modular components: transformer-based embeddings (Sentence-BERT), dimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction with refinement. The framework captures semantic similarity at the sentence level, enabling the discovery of coherent themes from brainstorming transcripts while filtering noise and identifying outliers. We evaluate our approach on structured Zoom brainstorming sessions involving student groups tasked with improving their university. Results demonstrate that our model achieves higher topic coherence compared to established methods such as LDA, ETM, and BERTopic, with an average coherence score of 0.687 (CV), outperforming baselines by a significant margin. Beyond improved performance, the model provides interpretable insights into the depth and diversity of topics explored, supporting both convergent and divergent dimensions of group creativity. This work highlights the potential of embedding-based topic modeling for analyzing collaborative ideation and contributes an efficient and scalable framework for studying creativity in synchronous virtual meetings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è¯­ä¹‰é©±åŠ¨çš„ä¸»é¢˜å»ºæ¨¡(semantic-driven topic modeling)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è™šæ‹Ÿå¤´è„‘é£æš´(virtual brainstorming)ä¸­å› æ„æ€æ•°é‡å¤§ä¸”åˆ†å¸ƒä¸å‡è€Œå¯¼è‡´çš„äººå·¥åˆ†ææ•ˆç‡ä½ä¸‹å’Œä¸»è§‚æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†Sentence-BERTåµŒå…¥ã€UMAPé™ç»´ã€HDBSCANèšç±»ä»¥åŠä¼˜åŒ–çš„ä¸»é¢˜æå–æŠ€æœ¯ï¼Œé€šè¿‡æ•è·å¥å­å±‚é¢çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä»ä¼šè®®è®°å½•ä¸­æœ‰æ•ˆå‘ç°è¿è´¯ä¸»é¢˜å¹¶è¿‡æ»¤å™ªå£°ã€‚åœ¨é’ˆå¯¹Zoomä¼šè®®æ•°æ®çš„è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡å‹å®ç°äº†0.687 (CV)çš„å¹³å‡ä¸»é¢˜è¿è´¯æ€§(topic coherence)å¾—åˆ†ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºLDAã€ETMå’ŒBERTopicç­‰ä¼ ç»ŸåŸºå‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¯¹ä¸»é¢˜æ·±åº¦ä¸å¤šæ ·æ€§çš„å¯è§£é‡Šåˆ†æï¼Œæœ‰æ•ˆæ”¯æŒäº†ç¾¤ä½“åˆ›é€ åŠ›ä¸­æ”¶æ•›æ€§(convergent)ä¸å‘æ•£æ€§(divergent)ç»´åº¦çš„è¯„ä¼°ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†åŸºäºåµŒå…¥çš„ä¸»é¢˜å»ºæ¨¡åœ¨åˆ†æåä½œæ„æ€ä¸­çš„æ½œåŠ›ï¼Œä¸ºç ”ç©¶åŒæ­¥è™šæ‹Ÿä¼šè®®ä¸­çš„åˆ›é€ åŠ›æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ‰©å±•çš„ä¸“ä¸šæ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16835v1",
      "published_date": "2025-09-20 23:18:50 UTC",
      "updated_date": "2025-09-20 23:18:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:30.751264+00:00"
    },
    {
      "arxiv_id": "2509.16834v1",
      "title": "Robot Learning with Sparsity and Scarcity",
      "title_zh": "é’ˆå¯¹ç¨€ç–ä¸ç¨€ç¼ºæ€§çš„æœºå™¨äººå­¦ä¹ ",
      "authors": [
        "Jingxi Xu"
      ],
      "abstract": "Unlike in language or vision, one of the fundamental challenges in robot learning is the lack of access to vast data resources. We can further break down the problem into (1) data sparsity from the angle of data representation and (2) data scarcity from the angle of data quantity. In this thesis, I will discuss selected works on two domains: (1) tactile sensing and (2) rehabilitation robots, which are exemplars of data sparsity and scarcity, respectively. Tactile sensing is an essential modality for robotics, but tactile data are often sparse, and for each interaction with the physical world, tactile sensors can only obtain information about the local area of contact. I will discuss my work on learning vision-free tactile-only exploration and manipulation policies through model-free reinforcement learning to make efficient use of sparse tactile information. On the other hand, rehabilitation robots are an example of data scarcity to the extreme due to the significant challenge of collecting biosignals from disabled-bodied subjects at scale for training. I will discuss my work in collaboration with the medical school and clinicians on intent inferral for stroke survivors, where a hand orthosis developed in our lab collects a set of biosignals from the patient and uses them to infer the activity that the patient intends to perform, so the orthosis can provide the right type of physical assistance at the right moment. My work develops machine learning algorithms that enable intent inferral with minimal data, including semi-supervised, meta-learning, and generative AI methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†æœºå™¨äººå­¦ä¹ (Robot Learning)åœ¨ç¼ºä¹å¤§è§„æ¨¡æ•°æ®èµ„æºèƒŒæ™¯ä¸‹æ‰€é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå³æ•°æ®ç¨€ç–æ€§(Data Sparsity)å’Œæ•°æ®åŒ®ä¹æ€§(Data Scarcity)ã€‚é’ˆå¯¹è§¦è§‰æ„ŸçŸ¥(Tactile Sensing)é¢†åŸŸçš„ä¿¡æ¯å±€éƒ¨æ€§ï¼Œç ”ç©¶è€…é€šè¿‡æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ (Model-free Reinforcement Learning)å¼€å‘äº†æ— éœ€è§†è§‰ã€ä»…ä¾èµ–è§¦è§‰çš„æ¢ç´¢ä¸æ“çºµç­–ç•¥ï¼Œä»¥é«˜æ•ˆåˆ©ç”¨ç¨€ç–çš„è§¦è§‰åé¦ˆã€‚è€Œåœ¨åº·å¤æœºå™¨äºº(Rehabilitation Robots)é¢†åŸŸï¼Œé¢å¯¹æåº¦ç¼ºä¹æ®‹éšœå—è¯•è€…ç”Ÿç‰©ä¿¡å·çš„éš¾é¢˜ï¼Œè¯¥å·¥ä½œåˆ©ç”¨åŠç›‘ç£å­¦ä¹ (Semi-supervised Learning)ã€å…ƒå­¦ä¹ (Meta-learning)å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)ç­‰æŠ€æœ¯å®ç°äº†é«˜æ•ˆçš„æ„å›¾æ¨æ–­(Intent Inferral)ã€‚æœ€ç»ˆï¼Œé€šè¿‡ä¸åŒ»ç–—æœºæ„åˆä½œå¼€å‘çš„æ‰‹éƒ¨çŸ«å½¢å™¨(Hand Orthosis)ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†åœ¨æå°æ•°æ®é›†ä¸‹å®ç°ç²¾å‡†ç‰©ç†è¾…åŠ©çš„å¯è¡Œæ€§ï¼Œä¸ºåº”å¯¹æœºå™¨äººå­¦ä¹ ä¸­çš„æ•°æ®å—é™é—®é¢˜æä¾›äº†ç³»ç»Ÿçš„ç®—æ³•æ¡†æ¶ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16834v1",
      "published_date": "2025-09-20 23:18:41 UTC",
      "updated_date": "2025-09-20 23:18:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:45.665987+00:00"
    },
    {
      "arxiv_id": "2509.20381v1",
      "title": "USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model",
      "title_zh": "USB-Recï¼šä¸€ç§æå‡å¤§è¯­è¨€æ¨¡å‹ä¼šè¯å¼æ¨èèƒ½åŠ›çš„æœ‰æ•ˆæ¡†æ¶",
      "authors": [
        "Jianyu Wen",
        "Jingyun Wang",
        "Cilin Yan",
        "Jiayin Cai",
        "Xiaolong Jiang",
        "Ying Zhang"
      ],
      "abstract": "Recently, Large Language Models (LLMs) have been widely employed in Conversational Recommender Systems (CRSs). Unlike traditional language model approaches that focus on training, all existing LLMs-based approaches are mainly centered around how to leverage the summarization and analysis capabilities of LLMs while ignoring the issue of training. Therefore, in this work, we propose an integrated training-inference framework, User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs in conversational recommendation at the model level. Firstly, we design a LLM-based Preference Optimization (PO) dataset construction strategy for RL training, which helps the LLMs understand the strategies and methods in conversational recommendation. Secondly, we propose a Self-Enhancement Strategy (SES) at the inference stage to further exploit the conversational recommendation potential obtained from RL training. Extensive experiments on various datasets demonstrate that our method consistently outperforms previous state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†USB-Recï¼ˆUser-Simulator-Based frameworkï¼‰ï¼Œä¸€ä¸ªé›†æˆè®­ç»ƒä¸æ¨ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹è¯å¼æ¨èç³»ç»Ÿï¼ˆCRSsï¼‰ä¸­å¾€å¾€åªä¾§é‡åˆ©ç”¨æ¨¡å‹èƒ½åŠ›è€Œå¿½ç•¥è®­ç»ƒä¼˜åŒ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆè®¾è®¡äº†ä¸€ç§åŸºäºLLMçš„åå¥½ä¼˜åŒ–ï¼ˆPreference Optimization, POï¼‰æ•°æ®é›†æ„å»ºç­–ç•¥ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒä½¿æ¨¡å‹æŒæ¡å¯¹è¯æ¨èçš„ç­–ç•¥ä¸æ–¹æ³•ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è‡ªå¢å¼ºç­–ç•¥ï¼ˆSelf-Enhancement Strategy, SESï¼‰ï¼Œä»¥å……åˆ†æŒ–æ˜æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ‰€è·å¾—çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUSB-Recåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¸€è‡´æ€§åœ°ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†LLMsåœ¨å¯¹è¯æ¨èåœºæ™¯ä¸‹çš„æ¨¡å‹å±‚é¢è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by Recsys'25",
      "pdf_url": "https://arxiv.org/pdf/2509.20381v1",
      "published_date": "2025-09-20 22:34:55 UTC",
      "updated_date": "2025-09-20 22:34:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:33.037888+00:00"
    },
    {
      "arxiv_id": "2509.16825v4",
      "title": "KANO: Kolmogorov-Arnold Neural Operator",
      "title_zh": "KANOï¼šKolmogorov-Arnold ç¥ç»ç®—å­",
      "authors": [
        "Jin Lee",
        "Ziming Liu",
        "Xinling Yu",
        "Yixuan Wang",
        "Haewon Jeong",
        "Murphy Yuezhen Niu",
        "Zheng Zhang"
      ],
      "abstract": "We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over generic position-dependent dynamics (variable coefficient PDEs) for any physical input, whereas FNO stays practical only for spectrally sparse operators and strictly imposes a fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\\approx 6\\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\\approx 1.5\\times10^{-2}$, by orders of magnitude.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†KANO (Kolmogorov-Arnold Neural Operator)ï¼Œä¸€ç§ç”±è°±åŸº(spectral bases)å’Œç©ºé—´åŸº(spatial bases)å…±åŒå‚æ•°åŒ–çš„åŒåŸŸç¥ç»ç®—å­ï¼Œå…·å¤‡å†…åœ¨çš„ç¬¦å·å¯è§£é‡Šæ€§(symbolic interpretability)ã€‚ç†è®ºè¯æ˜KANOå…‹æœäº†Fourier Neural Operator (FNO)çš„çº¯é¢‘è°±ç“¶é¢ˆï¼Œèƒ½å¤Ÿå¯¹é€šç”¨çš„ä½ç½®ç›¸å…³åŠ¨åŠ›å­¦ï¼ˆå¦‚å˜ç³»æ•°PDEsï¼‰è¿›è¡Œæœ‰æ•ˆè¡¨è¾¾ï¼Œè§£å†³äº†FNOåœ¨éç¨€ç–é¢‘è°±è¾“å…¥ä¸‹çš„å±€é™æ€§ã€‚åœ¨ä½ç½®ç›¸å…³å¾®åˆ†ç®—å­çš„å®éªŒä¸­ï¼ŒKANOå±•ç°äº†æå¼ºçš„æ³›åŒ–ç¨³å¥æ€§ã€‚é’ˆå¯¹é‡å­å“ˆå¯†é¡¿å­¦ä¹ (Quantum Hamiltonian learning)ä»»åŠ¡ï¼ŒKANOèƒ½ä»¥é—­å¼ç¬¦å·å½¢å¼ç²¾ç¡®é‡å»ºå“ˆå¯†é¡¿é‡ï¼Œå…¶ç³»æ•°å‡†ç¡®åº¦è¾¾ä¸‡åˆ†ä¹‹ä¸€ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼ŒKANOåœ¨æŠ•å½±æµ‹é‡æ•°æ®ä¸‹çš„æ€ä¸å¿ å®åº¦(state infidelity)ä»…ä¸º$6\\times10^{-6}$ï¼Œç›¸æ¯”äºä½¿ç”¨å…¨æ³¢å‡½æ•°è®­ç»ƒçš„FNOå®ç°äº†æ•°ä¸ªæ•°é‡çº§çš„æ€§èƒ½è·¨è¶Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16825v4",
      "published_date": "2025-09-20 22:32:58 UTC",
      "updated_date": "2026-01-01 09:39:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:47.404778+00:00"
    },
    {
      "arxiv_id": "2509.16812v1",
      "title": "SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree",
      "title_zh": "SMART-3Dï¼šä¸‰ç»´è‡ªå˜å½¢è‡ªé€‚åº”é‡è§„åˆ’æ ‘",
      "authors": [
        "Priyanshu Agrawal",
        "Shalabh Gupta",
        "Zongyuan Shen"
      ],
      "abstract": "This paper presents SMART-3D, an extension of the SMART algorithm to 3D environments. SMART-3D is a tree-based adaptive replanning algorithm for dynamic environments with fast moving obstacles. SMART-3D morphs the underlying tree to find a new path in real-time whenever the current path is blocked by obstacles. SMART-3D removed the grid decomposition requirement of the SMART algorithm by replacing the concept of hot-spots with that of hot-nodes, thus making it computationally efficient and scalable to 3D environments. The hot-nodes are nodes which allow for efficient reconnections to morph the existing tree to find a new safe and reliable path. The performance of SMART-3D is evaluated by extensive simulations in 2D and 3D environments populated with randomly moving dynamic obstacles. The results show that SMART-3D achieves high success rates and low replanning times, thus highlighting its suitability for real-time onboard applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SMART-3Dï¼Œè¿™æ˜¯SMARTç®—æ³•åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„æ‰©å±•ç‰ˆæœ¬ï¼Œä¸“é—¨ç”¨äºè§£å†³å…·æœ‰å¿«é€Ÿç§»åŠ¨éšœç¢ç‰©çš„åŠ¨æ€ç¯å¢ƒä¸­çš„è·¯å¾„è§„åˆ’é—®é¢˜ã€‚SMART-3D æ˜¯ä¸€ç§åŸºäºæ ‘ç»“æ„çš„è‡ªé€‚åº”é‡è§„åˆ’ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å½“å‰è·¯å¾„è¢«é˜»æŒ¡æ—¶å®æ—¶å˜å½¢ï¼ˆMorphingï¼‰åº•å±‚æ ‘ç»“æ„ä»¥å¯»æ‰¾æ–°è·¯å¾„ã€‚è¯¥ç®—æ³•é€šè¿‡å¼•å…¥ Hot-nodes æ¦‚å¿µæ›¿ä»£äº†åŸæœ‰çš„ç½‘æ ¼åˆ†è§£ï¼ˆGrid decompositionï¼‰å’Œ Hot-spots éœ€æ±‚ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡å’Œåœ¨ä¸‰ç»´ç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§ã€‚è¿™äº› Hot-nodes å…è®¸é€šè¿‡é«˜æ•ˆçš„é‡æ–°è¿æ¥æ¥è°ƒæ•´ç°æœ‰æ ‘ç»“æ„ï¼Œä»è€Œå¿«é€Ÿç”Ÿæˆå®‰å…¨å¯é çš„æ–°è·¯å¾„ã€‚é€šè¿‡åœ¨åŒ…å«éšæœºç§»åŠ¨åŠ¨æ€éšœç¢ç‰©çš„ 2D å’Œ 3D ç¯å¢ƒä¸­è¿›è¡Œå¤§é‡ä»¿çœŸå®éªŒï¼Œç»“æœè¡¨æ˜ SMART-3D å®ç°äº†æé«˜çš„æˆåŠŸç‡å’ŒæçŸ­çš„é‡è§„åˆ’æ—¶é—´ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº† SMART-3D åœ¨å®æ—¶æœºè½½åº”ç”¨ï¼ˆOnboard applicationsï¼‰ä¸­çš„å“è¶Šæ€§èƒ½ä¸é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16812v1",
      "published_date": "2025-09-20 21:30:55 UTC",
      "updated_date": "2025-09-20 21:30:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:40.863823+00:00"
    },
    {
      "arxiv_id": "2509.16811v2",
      "title": "Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media",
      "title_zh": "æç¤ºé©±åŠ¨çš„æ™ºèƒ½ä½“åŒ–è§†é¢‘ç¼–è¾‘ç³»ç»Ÿï¼šé•¿ç¯‡å™äº‹æ€§åª’ä½“çš„è‡ªä¸»ç†è§£",
      "authors": [
        "Zihan Ding",
        "Xinyi Wang",
        "Junlong Chen",
        "Per Ola Kristensson",
        "Junxiao Shen"
      ],
      "abstract": "Creators struggle to edit long-form, narrative-rich videos not because of UI complexity, but due to the cognitive demands of searching, storyboarding, and sequencing hours of footage. Existing transcript- or embedding-based methods fall short for creative workflows, as models struggle to track characters, infer motivations, and connect dispersed events. We present a prompt-driven, modular editing system that helps creators restructure multi-hour content through free-form prompts rather than timelines. At its core is a semantic indexing pipeline that builds a global narrative via temporal segmentation, guided memory compression, and cross-granularity fusion, producing interpretable traces of plot, dialogue, emotion, and context. Users receive cinematic edits while optionally refining transparent intermediate outputs. Evaluated on 400+ videos with expert ratings, QA, and preference studies, our system scales prompt-driven editing, preserves narrative coherence, and balances automation with creator control.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”±æç¤ºé©±åŠ¨çš„æ™ºèƒ½è§†é¢‘ç¼–è¾‘ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åˆ›ä½œè€…åœ¨å¤„ç†é•¿ç¯‡å™äº‹è§†é¢‘æ—¶é¢ä¸´çš„ç´ ææœå¯»ã€åˆ†é•œè®¾è®¡å’Œåºåˆ—ç¼–æ’ç­‰é«˜è®¤çŸ¥éœ€æ±‚æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºæ–‡æœ¬è½¬å½•æˆ–åµŒå…¥(embedding)çš„æ–¹æ³•åœ¨åˆ›æ„å·¥ä½œæµä¸­å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æœ‰æ•ˆè¿½è¸ªè§’è‰²åŠ¨æ€ã€æ¨æ–­åŠ¨æœºåŠå…³è”åˆ†æ•£çš„äº‹ä»¶ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªè¯­ä¹‰ç´¢å¼•æµæ°´çº¿(semantic indexing pipeline)ï¼Œé€šè¿‡æ—¶é—´åˆ†å‰²(temporal segmentation)ã€å¼•å¯¼å¼è®°å¿†å‹ç¼©(guided memory compression)å’Œè·¨ç²’åº¦èåˆ(cross-granularity fusion)æ„å»ºå…¨å±€å™äº‹ï¼Œä»è€Œç”ŸæˆåŒ…å«æƒ…èŠ‚(plot)ã€å¯¹è¯(dialogue)ã€æƒ…æ„Ÿ(emotion)å’Œä¸Šä¸‹æ–‡(context)çš„å¯è§£é‡Šè¿½è¸ªã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç”±å½¢å¼çš„æç¤º(free-form prompts)è€Œéä¼ ç»Ÿæ—¶é—´è½´æ¥é‡æ„é•¿è¾¾æ•°å°æ—¶çš„å†…å®¹ã€‚é’ˆå¯¹400å¤šä¸ªè§†é¢‘çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ç”Ÿæˆç”µå½±åŒ–å‰ªè¾‘çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆä¿æŒå™äº‹çš„è¿è´¯æ€§ï¼Œå¹¶åœ¨è‡ªåŠ¨åŒ–ä¸åˆ›ä½œè€…æ§åˆ¶ä¹‹é—´å®ç°äº†è‰¯å¥½å¹³è¡¡ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16811v2",
      "published_date": "2025-09-20 21:22:56 UTC",
      "updated_date": "2025-09-28 07:22:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:44.564080+00:00"
    },
    {
      "arxiv_id": "2509.16810v1",
      "title": "Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment",
      "title_zh": "åŸºäºè§†é¢‘è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–æµç¨‹åˆ†æï¼šåŠ©åŠ›äººå·¥æ™ºèƒ½è¾…åŠ©çš„æŠ¤ç†æŠ€èƒ½è¯„ä¼°",
      "authors": [
        "Shen Chang",
        "Dennis Liu",
        "Renran Tian",
        "Kristen L. Swartzell",
        "Stacie L. Klingler",
        "Amy M. Nagle",
        "Nan Kong"
      ],
      "abstract": "Consistent high-quality nursing care is essential for patient safety, yet current nursing education depends on subjective, time-intensive instructor feedback in training future nurses, which limits scalability and efficiency in their training, and thus hampers nursing competency when they enter the workforce. In this paper, we introduce a video-language model (VLM) based framework to develop the AI capability of automated procedural assessment and feedback for nursing skills training, with the potential of being integrated into existing training programs. Mimicking human skill acquisition, the framework follows a curriculum-inspired progression, advancing from high-level action recognition, fine-grained subaction decomposition, and ultimately to procedural reasoning. This design supports scalable evaluation by reducing instructor workload while preserving assessment quality. The system provides three core capabilities: 1) diagnosing errors by identifying missing or incorrect subactions in nursing skill instruction videos, 2) generating explainable feedback by clarifying why a step is out of order or omitted, and 3) enabling objective, consistent formative evaluation of procedures. Validation on synthesized videos demonstrates reliable error detection and temporal localization, confirming its potential to handle real-world training variability. By addressing workflow bottlenecks and supporting large-scale, standardized evaluation, this work advances AI applications in nursing education, contributing to stronger workforce development and ultimately safer patient care.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Video-Language Model (VLM) çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æŠ¤ç†æŠ€èƒ½åŸ¹è®­ä¸­çš„è‡ªåŠ¨åŒ–ç¨‹åºè¯„ä¼°ä¸åé¦ˆï¼Œä»è€Œè§£å†³ä¼ ç»ŸæŠ¤ç†æ•™è‚²ä¸­æ•™å¸ˆåé¦ˆä¸»è§‚ä¸”è€—æ—¶çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ¨¡æ‹Ÿäººç±»çš„å­¦ä¹ è·¯å¾„ï¼Œéµå¾ªä»é«˜å±‚ Action Recognition åˆ°ç»†ç²’åº¦ Subaction Decompositionï¼Œå†åˆ° Procedural Reasoning çš„è¯¾ç¨‹å¯å‘å¼é€’è¿›è®¾è®¡ã€‚ç³»ç»Ÿå…·å¤‡è¯Šæ–­é”™è¯¯ã€æä¾›è§£é‡Šæ€§åé¦ˆä»¥åŠæ‰§è¡Œå®¢è§‚å½¢æˆæ€§è¯„ä»·ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½ï¼Œèƒ½å‡†ç¡®è¯†åˆ«æŠ¤ç†æ“ä½œè§†é¢‘ä¸­ç¼ºå¤±æˆ–é”™è¯¯çš„æ­¥éª¤ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åˆæˆè§†é¢‘ä¸Šå±•ç°äº†å¯é çš„é”™è¯¯æ£€æµ‹ä¸æ—¶é—´å®šä½èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡è½»æ•™å­¦è´Ÿæ‹…ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡æ”¯æŒå¤§è§„æ¨¡æ ‡å‡†åŒ–è¯„ä¼°ï¼Œæ¨åŠ¨äº† AI åœ¨æŠ¤ç†æ•™è‚²é¢†åŸŸçš„åº”ç”¨ï¼Œä¸ºå¼ºåŒ–æŠ¤ç†äººæ‰åŸ¹å…»å’Œä¿éšœæ‚£è€…å®‰å…¨æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16810v1",
      "published_date": "2025-09-20 21:11:33 UTC",
      "updated_date": "2025-09-20 21:11:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:47.644177+00:00"
    },
    {
      "arxiv_id": "2509.16804v1",
      "title": "KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis",
      "title_zh": "KuBERTï¼šä¸­åº“å°”å¾·è¯­ BERT æ¨¡å‹åŠå…¶åœ¨æƒ…æ„Ÿåˆ†æä¸­çš„åº”ç”¨",
      "authors": [
        "Kozhin muhealddin Awlla",
        "Hadi Veisi",
        "Abdulhady Abas Abdullah"
      ],
      "abstract": "This paper enhances the study of sentiment analysis for the Central Kurdish language by integrating the Bidirectional Encoder Representations from Transformers (BERT) into Natural Language Processing techniques. Kurdish is a low-resourced language, having a high level of linguistic diversity with minimal computational resources, making sentiment analysis somewhat challenging. Earlier, this was done using a traditional word embedding model, such as Word2Vec, but with the emergence of new language models, specifically BERT, there is hope for improvements. The better word embedding capabilities of BERT lend to this study, aiding in the capturing of the nuanced semantic pool and the contextual intricacies of the language under study, the Kurdish language, thus setting a new benchmark for sentiment analysis in low-resource languages.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KuBERTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ä¸­åº“å°”å¾·è¯­ (Central Kurdish) å¼€å‘çš„ Bidirectional Encoder Representations from Transformers (BERT) æ¨¡å‹ï¼Œæ—¨åœ¨æå‡è¯¥è¯­è¨€çš„æƒ…æ„Ÿåˆ†æ (Sentiment Analysis) æ•ˆæœã€‚ç”±äºåº“å°”å¾·è¯­æ˜¯ä¸€ç§ä½èµ„æºè¯­è¨€ (Low-resourced language)ï¼Œå…·æœ‰é«˜åº¦çš„è¯­è¨€å¤šæ ·æ€§ä¸”ç¼ºä¹è®¡ç®—èµ„æºï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ– Word2Vec ç­‰è¯å‘é‡æ¨¡å‹ï¼Œéš¾ä»¥æ•æ‰å¤æ‚çš„è¯­ä¹‰ç»†èŠ‚ã€‚KuBERT åˆ©ç”¨ BERT çš„å¼ºå¤§è¯åµŒå…¥èƒ½åŠ›ï¼Œæœ‰æ•ˆæ•æ‰äº†è¯¥è¯­è¨€ä¸­ç»†å¾®çš„è¯­ä¹‰ç‰¹å¾å’Œä¸Šä¸‹æ–‡å¤æ‚æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºä½èµ„æºè¯­è¨€çš„æƒ…æ„Ÿåˆ†æç ”ç©¶è®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œå±•ç¤ºäº†å…ˆè¿›é¢„è®­ç»ƒæ¨¡å‹åœ¨æå‡ç¨€ç¼ºèµ„æºè¯­è¨€å¤„ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16804v1",
      "published_date": "2025-09-20 20:44:29 UTC",
      "updated_date": "2025-09-20 20:44:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:54:51.378105+00:00"
    },
    {
      "arxiv_id": "2509.20380v2",
      "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation",
      "title_zh": "ACCeLLiuMï¼šé¢å‘è‡ªåŠ¨ OpenACC ç¼–è¯‘æŒ‡ä»¤ç”Ÿæˆçš„æœ‰ç›‘ç£å¾®è°ƒ",
      "authors": [
        "Samyak Jhaveri",
        "Vanessa Klotzmann",
        "Crista Lopes"
      ],
      "abstract": "The increasing ubiquity of GPUs is accompanied by the increasing complexity of their hardware and parallel programming frameworks. Directive-based parallel programming standards like OpenACC simplify GPU programming to some extent by abstracting away low-level complexities, but a fair amount of expertise is still required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically fine-tuned for generating expert OpenACC directives for data-parallel loops, along with the supervised fine-tuning dataset that was used to train them. The ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for testing. Experimental evaluations show a pronounced performance gap in generating correct OpenACC pragmas between base LLMs and our fine-tuned versions. On the held-out test set, base LLMs fail to consistently generate valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid pragmas with the correct directive type for $87\\%$ of the data-parallel loops, and exact pragmas--including directives, clauses, clause order, and clause variables--for $50\\%$ of the cases. Even when not exact, generated pragmas frequently incorporate the correct clauses in a different order than the ground-truth label, or include additional clauses that enable finer control over parallel execution, data movement, and concurrency, offering practical value beyond strict string-matching. By publicly releasing the code, models, and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU offloading of serially written programs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ACCeLLiuMï¼ŒåŒ…å«ä¸¤ä¸ªä¸“é—¨é’ˆå¯¹ OpenACC è‡ªåŠ¨æŒ‡ä»¤ç”Ÿæˆè¿›è¡Œç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹åŠå…¶é…å¥—æ•°æ®é›†ã€‚é’ˆå¯¹ GPU ç¼–ç¨‹ä¸­å¹¶è¡Œæ¡†æ¶çš„å¤æ‚æ€§ï¼Œç ”ç©¶è€…ä» GitHub å…¬å…±ä»“åº“ä¸­æŒ–æ˜å¹¶æ„å»ºäº†åŒ…å« 4,033 ä¸ª pragma-loop å¯¹çš„è®­ç»ƒä¸æµ‹è¯•é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒACCeLLiuM æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆ OpenACC æŒ‡ä»¤æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿä¸º 87% çš„æ•°æ®å¹¶è¡Œå¾ªç¯ç”Ÿæˆæ­£ç¡®ç±»å‹çš„æŒ‡ä»¤ï¼Œå¹¶åœ¨ 50% çš„æ¡ˆä¾‹ä¸­å®ç°åŒ…å«å­å¥ã€å˜é‡åŠé¡ºåºåœ¨å†…çš„ç²¾ç¡®åŒ¹é…ã€‚å³ä½¿åœ¨éç²¾ç¡®åŒ¹é…çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ç”Ÿæˆçš„æŒ‡ä»¤ä¹Ÿå¾€å¾€åŒ…å«æ­£ç¡®çš„å­å¥ï¼Œæˆ–èƒ½æä¾›æ›´ç»†ç²’åº¦å¹¶è¡Œæ§åˆ¶çš„é¢å¤–å­å¥ï¼Œå…·æœ‰æé«˜çš„å®é™…åº”ç”¨ä»·å€¼ã€‚é€šè¿‡å¼€æºä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ï¼Œè¯¥é¡¹å·¥ä½œä¸ºåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°è‡ªåŠ¨åŒ– GPU å¸è½½(Offloading)å»ºç«‹äº†å¯é‡å¤çš„åŸºå‡†ï¼Œæœ‰æ•ˆé™ä½äº†å°†ä¸²è¡Œç¨‹åºè½¬åŒ–ä¸ºå¹¶è¡Œä»£ç çš„ä¸“ä¸šé—¨æ§›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20380v2",
      "published_date": "2025-09-20 20:41:32 UTC",
      "updated_date": "2025-09-26 01:37:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:08.689709+00:00"
    },
    {
      "arxiv_id": "2509.16788v1",
      "title": "Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies",
      "title_zh": "é¢å‘é˜¿æ‹‰ä¼¯è¯­æ–¹é¢çº§æƒ…æ„Ÿåˆ†æçš„é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼šé¢†åŸŸè‡ªé€‚åº”ä¸å¾®è°ƒç­–ç•¥çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Salha Alyami",
        "Amani Jamal",
        "Areej Alhothali"
      ],
      "abstract": "Aspect-based sentiment analysis (ABSA) in natural language processing enables organizations to understand customer opinions on specific product aspects. While deep learning models are widely used for English ABSA, their application in Arabic is limited due to the scarcity of labeled data. Researchers have attempted to tackle this issue by using pre-trained contextualized language models such as BERT. However, these models are often based on fact-based data, which can introduce bias in domain-specific tasks like ABSA. To our knowledge, no studies have applied adaptive pre-training with Arabic contextualized models for ABSA. This research proposes a novel approach using domain-adaptive pre-training for aspect-sentiment classification (ASC) and opinion target expression (OTE) extraction. We examine fine-tuning strategies - feature extraction, full fine-tuning, and adapter-based methods - to enhance performance and efficiency, utilizing multiple adaptation corpora and contextualized models. Our results show that in-domain adaptive pre-training yields modest improvements. Adapter-based fine-tuning is a computationally efficient method that achieves competitive results. However, error analyses reveal issues with model predictions and dataset labeling. In ASC, common problems include incorrect sentiment labeling, misinterpretation of contrastive markers, positivity bias for early terms, and challenges with conflicting opinions and subword tokenization. For OTE, issues involve mislabeling targets, confusion over syntactic roles, difficulty with multi-word expressions, and reliance on shallow heuristics. These findings underscore the need for syntax- and semantics-aware models, such as graph convolutional networks, to more effectively capture long-distance relations and complex aspect-based opinion alignments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­æ–¹é¢çº§æƒ…æ„Ÿåˆ†æ(Aspect-Based Sentiment Analysis, ABSA)ä¸­æ ‡è®°æ•°æ®ç¨€ç¼ºä»¥åŠé¢„è®­ç»ƒæ¨¡å‹é¢†åŸŸåè§çš„é—®é¢˜ï¼Œé¦–æ¬¡åœ¨é˜¿æ‹‰ä¼¯è¯­è¯­å¢ƒä¸‹æå‡ºäº†é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒ(Domain-Adaptive Pre-Training)æ–¹æ³•ï¼Œå¹¶åº”ç”¨äºæ–¹é¢æƒ…æ„Ÿåˆ†ç±»(ASC)å’Œè§‚ç‚¹ç›®æ ‡è¡¨è¾¾å¼(OTE)æå–ã€‚ç ”ç©¶ç³»ç»Ÿåœ°æ¯”è¾ƒäº†ç‰¹å¾æå–(Feature Extraction)ã€å…¨é‡å¾®è°ƒ(Full Fine-Tuning)åŠåŸºäºé€‚é…å™¨(Adapter-based)çš„å¾®è°ƒç­–ç•¥ï¼Œæ¢è®¨äº†ä¸åŒè‡ªé€‚åº”è¯­æ–™åº“å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢†åŸŸå†…è‡ªé€‚åº”é¢„è®­ç»ƒèƒ½å¸¦æ¥æ€§èƒ½æå‡ï¼Œè€Œé€‚é…å™¨å¾®è°ƒåˆ™åœ¨è®¡ç®—æ•ˆç‡ä¸å‡†ç¡®ç‡ä¹‹é—´å–å¾—äº†ç†æƒ³çš„å¹³è¡¡ã€‚é€šè¿‡è¯¦ç»†çš„é”™è¯¯åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†å¯¹æ¯”æ ‡è®°(Contrastive Markers)ã€è¯­æ³•è§’è‰²è¯†åˆ«åŠå¤šè¯è¡¨è¾¾å¼æ–¹é¢çš„å±€é™æ€§ã€‚æœ€åï¼Œä½œè€…æå‡ºæœªæ¥åº”å¼•å…¥å¦‚å›¾å·ç§¯ç½‘ç»œ(Graph Convolutional Networks, GCNs)ç­‰å…·å¤‡è¯­æ³•å’Œè¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›çš„æ¨¡å‹ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰å¤æ‚çš„é•¿è·ç¦»ä¾èµ–å’Œè§‚ç‚¹å¯¹é½å…³ç³»ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "26 excluding bibliography , journal article",
      "pdf_url": "https://arxiv.org/pdf/2509.16788v1",
      "published_date": "2025-09-20 19:32:16 UTC",
      "updated_date": "2025-09-20 19:32:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:09.658748+00:00"
    },
    {
      "arxiv_id": "2509.16780v2",
      "title": "Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook",
      "title_zh": "æ•°å­¦æ•™ç§‘ä¹¦é¡µçº§æ£€ç´¢é—®ç­”ä¸­ RAG ä¸ GraphRAG çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Eason Chen",
        "Chuangji Li",
        "Shizhuo Li",
        "Zimo Xiao",
        "Jionghao Lin",
        "Kenneth R. Koedinger"
      ],
      "abstract": "Technology-enhanced learning environments often help students retrieve relevant learning content for questions arising during self-paced study. Large language models (LLMs) have emerged as novel aids for information retrieval during learning. While LLMs are effective for general-purpose question-answering, they typically lack alignment with the domain knowledge of specific course materials such as textbooks and slides. We investigate Retrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced RAG approach, for page-level question answering in an undergraduate mathematics textbook. While RAG has been effective for retrieving discrete, contextually relevant passages, GraphRAG may excel in modeling interconnected concepts and hierarchical knowledge structures. We curate a dataset of 477 question-answer pairs, each tied to a distinct textbook page. We then compare the standard embedding-based RAG methods to GraphRAG for evaluating both retrieval accuracy-whether the correct page is retrieved-and generated answer quality via F1 scores. Our findings show that embedding-based RAG achieves higher retrieval accuracy and better F1 scores compared to GraphRAG, which tends to retrieve excessive and sometimes irrelevant content due to its entity-based structure. We also explored re-ranking the retrieved pages with LLM and observed mixed results, including performance drop and hallucinations when dealing with larger context windows. Overall, this study highlights both the promises and challenges of page-level retrieval systems in educational contexts, emphasizing the need for more refined retrieval methods to build reliable AI tutoring solutions in providing reference page numbers.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ¯”äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ä¸åŸºäºçŸ¥è¯†å›¾è°±å¢å¼ºçš„GraphRAGåœ¨æœ¬ç§‘æ•°å­¦æ•™æé¡µé¢çº§é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶äººå‘˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«477ä¸ªé—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œé€šè¿‡æ£€ç´¢å‡†ç¡®ç‡å’Œç”Ÿæˆç­”æ¡ˆçš„F1åˆ†æ•°è¯„ä¼°äº†ä¸¤ç§æ–¹æ³•çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿçš„åŸºäºåµŒå…¥(embedding-based)çš„RAGåœ¨æ£€ç´¢ç²¾åº¦å’Œç”Ÿæˆè´¨é‡ä¸Šå‡ä¼˜äºGraphRAGï¼Œåè€…ç”±äºå…¶åŸºäºå®ä½“çš„ç»“æ„ï¼Œå®¹æ˜“æ£€ç´¢å‡ºè¿‡å¤šä¸”ä¸ç›¸å…³çš„å†…å®¹ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹æ£€ç´¢é¡µé¢è¿›è¡Œé‡æ’åºçš„æ•ˆæœï¼Œä½†åœ¨å¤„ç†å¤§ä¸Šä¸‹æ–‡çª—å£æ—¶è§‚å¯Ÿåˆ°äº†æ€§èƒ½ä¸‹é™å’Œå¹»è§‰(hallucinations)ç°è±¡ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†é¡µé¢çº§æ£€ç´¢ç³»ç»Ÿåœ¨æ•™è‚²èƒŒæ™¯ä¸‹çš„æ½œåŠ›ä¸æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†å¼€å‘å¯é AIåŠ©æ•™æ–¹æ¡ˆéœ€è¿›ä¸€æ­¥æ”¹è¿›æ£€ç´¢æ–¹æ³•ä»¥æä¾›ç²¾ç¡®çš„å‚è€ƒé¡µç ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16780v2",
      "published_date": "2025-09-20 19:06:49 UTC",
      "updated_date": "2025-09-26 05:24:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:14.289329+00:00"
    },
    {
      "arxiv_id": "2509.16769v1",
      "title": "Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes",
      "title_zh": "å‡ ä½•æ··åˆåˆ†ç±»å™¨ (GMC)ï¼šä¸€ç§åˆ¤åˆ«å¼é€ç±»è¶…å¹³é¢æ··åˆæ¨¡å‹",
      "authors": [
        "Prasanth K K",
        "Shubham Sharma"
      ],
      "abstract": "Many real world categories are multimodal, with single classes occupying disjoint regions in feature space. Classical linear models (logistic regression, linear SVM) use a single global hyperplane and perform poorly on such data, while high-capacity methods (kernel SVMs, deep nets) fit multimodal structure but at the expense of interpretability, heavier tuning, and higher computational cost. We propose the Geometric Mixture Classifier (GMC), a discriminative model that represents each class as a mixture of hyperplanes. Within each class, GMC combines plane scores via a temperature-controlled soft-OR (log-sum-exp), smoothly approximating the max; across classes, standard softmax yields probabilistic posteriors. GMC optionally uses Random Fourier Features (RFF) for nonlinear mappings while keeping inference linear in the number of planes and features. Our practical training recipe: geometry-aware k-means initialization, silhouette-based plane budgeting, alpha annealing, usage-aware L2 regularization, label smoothing, and early stopping, makes GMC plug-and-play. Across synthetic multimodal datasets (moons, circles, blobs, spirals) and tabular/image benchmarks (iris, wine, WDBC, digits), GMC consistently outperforms linear baselines and k-NN, is competitive with RBF-SVM, Random Forests, and small MLPs, and provides geometric introspection via per-plane and class responsibility visualizations. Inference scales linearly in planes and features, making GMC CPU-friendly, with single-digit microsecond latency per example, often faster than RBF-SVM and compact MLPs. Post-hoc temperature scaling reduces ECE from about 0.06 to 0.02. GMC thus strikes a favorable balance of accuracy, interpretability, and efficiency: it is more expressive than linear models and lighter, more transparent, and faster than kernel or deep models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Geometric Mixture Classifier (GMC)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³ç°å®ä¸–ç•Œå¤šæ¨¡æ€(multimodal)æ•°æ®åœ¨ç‰¹å¾ç©ºé—´ä¸­åˆ†å¸ƒä¸è¿ç»­é—®é¢˜çš„åˆ¤åˆ«æ¨¡å‹ã€‚GMC é€šè¿‡å°†æ¯ä¸ªç±»åˆ«è¡¨ç¤ºä¸ºè¶…å¹³é¢æ··åˆ(mixture of hyperplanes)ï¼Œå¹¶åˆ©ç”¨å—æ¸©åº¦æ§åˆ¶çš„ soft-OR (log-sum-exp) å‡½æ•°ç»“åˆ softmax å±‚ï¼Œå®ç°äº†å¯¹å¤æ‚æ•°æ®ç»“æ„çš„é«˜æ•ˆæ‹Ÿåˆã€‚è¯¥æ¨¡å‹æ”¯æŒä½¿ç”¨ Random Fourier Features (RFF) è¿›è¡Œéçº¿æ€§æ˜ å°„ï¼Œå¹¶æä¾›äº†ä¸€å¥—åŒ…æ‹¬å‡ ä½•æ„ŸçŸ¥ K-means åˆå§‹åŒ–åœ¨å†…çš„å®Œæ•´è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿å…¶å…·å¤‡å³æ’å³ç”¨çš„ç‰¹æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGMC åœ¨åˆæˆæ•°æ®é›†å’Œå¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºçº¿æ€§æ¨¡å‹ï¼Œæ€§èƒ½è¶³ä»¥åª²ç¾ RBF-SVM å’Œå°å‹ MLPsã€‚ç”±äºå…¶æ¨ç†è¿‡ç¨‹ä¸è¶…å¹³é¢å’Œç‰¹å¾æ•°é‡æˆçº¿æ€§å…³ç³»ï¼ŒGMC è¡¨ç°å‡ºæä½çš„å»¶è¿Ÿï¼ŒåŒæ—¶èƒ½é€šè¿‡è¶…å¹³é¢å¯è§†åŒ–æä¾›å‡ºè‰²çš„å‡ ä½•å¯è§£é‡Šæ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒGMC åœ¨åˆ†ç±»å‡†ç¡®ç‡ã€è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹é€æ˜åº¦ä¹‹é—´è¾¾æˆäº†ç†æƒ³çš„å¹³è¡¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 6 figures, 14 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.16769v1",
      "published_date": "2025-09-20 18:32:05 UTC",
      "updated_date": "2025-09-20 18:32:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:22.389151+00:00"
    },
    {
      "arxiv_id": "2509.16765v2",
      "title": "The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology",
      "title_zh": "å¥æ³•ä¹‹å£°ï¼šé¢å‘è¨€è¯­ç—…ç†å­¦çš„è¯­è¨€æ¨¡å‹å¾®è°ƒä¸å…¨é¢è¯„ä¼°",
      "authors": [
        "Fagun Patel",
        "Duc Q. Nguyen",
        "Sang T. Truong",
        "Jody Vaynshtok",
        "Sanmi Koyejo",
        "Nick Haber"
      ],
      "abstract": "According to the U.S. National Institutes of Health, more than 3.4 million children experience speech disorders that require clinical intervention. The number of speech-language pathologists (SLPs) is roughly 20 times fewer than the number of affected children, highlighting a significant gap in children's care and a pressing need for technological support that improves the productivity of SLPs. State-of-the-art multimodal language models (MLMs) show promise for supporting SLPs, but their use remains underexplored largely due to a limited understanding of their performance in high-stakes clinical settings. To address this gap, we collaborate with domain experts to develop a taxonomy of real-world use cases of MLMs in speech-language pathologies. Building on this taxonomy, we introduce the first comprehensive benchmark for evaluating MLM across five core use cases, each containing 1,000 manually annotated data points. This benchmark includes robustness and sensitivity tests under various settings, including background noise, speaker gender, and accent. Our evaluation of 15 state-of-the-art MLMs reveals that no single model consistently outperforms others across all tasks. Notably, we find systematic disparities, with models performing better on male speakers, and observe that chain-of-thought prompting can degrade performance on classification tasks with large label spaces and narrow decision boundaries. Furthermore, we study fine-tuning MLMs on domain-specific data, achieving improvements of over 10\\% compared to base models. These findings highlight both the potential and limitations of current MLMs for speech-language pathology applications, underscoring the need for further research and targeted development.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¨€è¯­éšœç¢å„¿ç«¥ä¸è¨€è¯­è¯­è¨€ç—…ç†å­¦å®¶(SLPs)æ•°é‡ä¸¥é‡å¤±è¡¡çš„ç°çŠ¶ï¼Œæ·±å…¥æ¢è®¨äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹(MLMs)åœ¨è¾…åŠ©ä¸´åºŠå¹²é¢„ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡ä¸é¢†åŸŸä¸“å®¶åˆä½œï¼Œç ”ç©¶å›¢é˜Ÿåˆ¶å®šäº†MLMsåœ¨è¨€è¯­ç—…ç†å­¦ä¸­çš„å®é™…æ¡ˆä¾‹åˆ†ç±»æ³•ï¼Œå¹¶æ¨å‡ºäº†é¦–ä¸ªåŒ…å«5000ä¸ªæ‰‹åŠ¨æ ‡æ³¨æ•°æ®ç‚¹çš„ç»¼åˆè¯„ä¼°åŸºå‡†(Benchmark)ï¼Œä¸“é—¨æµ‹è¯•æ¨¡å‹åœ¨èƒŒæ™¯å™ªéŸ³ã€æ€§åˆ«å’Œå£éŸ³ç­‰å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚å¯¹15ç§å…ˆè¿›MLMsçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç›®å‰å°šæ— å•ä¸€æ¨¡å‹èƒ½åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­è¡¨ç°æœ€ä¼˜ï¼Œä¸”æ¨¡å‹åœ¨ç”·æ€§è¯´è¯è€…ä¸Šçš„è¡¨ç°æ™®éä¼˜äºå¥³æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œé“¾å¼æ€ç»´(Chain-of-Thought)æç¤ºåœ¨å…·æœ‰å¤§æ ‡ç­¾ç©ºé—´å’Œçª„å†³ç­–è¾¹ç•Œçš„åˆ†ç±»ä»»åŠ¡ä¸­åè€Œä¼šé™ä½æ€§èƒ½ï¼Œè€Œé€šè¿‡é¢†åŸŸç‰¹å®šæ•°æ®çš„å¾®è°ƒ(Fine-tuning)åˆ™èƒ½ä½¿æ¨¡å‹æ€§èƒ½æå‡è¶…è¿‡10%ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ä¸´åºŠåŒ»ç–—åœºæ™¯ä¸­çš„ä¼˜åŠ¿ä¸å±€é™æ€§ï¼Œä¸ºæœªæ¥å¼€å‘æ›´å…·é’ˆå¯¹æ€§çš„è¨€è¯­è¾…åŠ©æŠ€æœ¯æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 Oral Presentation",
      "pdf_url": "https://arxiv.org/pdf/2509.16765v2",
      "published_date": "2025-09-20 18:10:30 UTC",
      "updated_date": "2025-10-08 14:02:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:28.895496+00:00"
    },
    {
      "arxiv_id": "2509.16745v1",
      "title": "CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding",
      "title_zh": "CAMBench-QRï¼šé¢å‘ QR ç†è§£çš„äº‹åè§£é‡Šç»“æ„æ„ŸçŸ¥åŸºå‡†",
      "authors": [
        "Ritabrata Chakraborty",
        "Avijit Dasgupta",
        "Sandeep Chaurasia"
      ],
      "abstract": "Visual explanations are often plausible but not structurally faithful. We introduce CAMBench-QR, a structure-aware benchmark that leverages the canonical geometry of QR codes (finder patterns, timing lines, module grid) to test whether CAM methods place saliency on requisite substructures while avoiding background. CAMBench-QR synthesizes QR/non-QR data with exact masks and controlled distortions, and reports structure-aware metrics (Finder/Timing Mass Ratios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside causal occlusion, insertion/deletion faithfulness, robustness, and latency. We benchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM) under two practical regimes of zero-shot and last-block fine-tuning. The benchmark, metrics, and training recipes provide a simple, reproducible yardstick for structure-aware evaluation of visual explanations. Hence we propose that CAMBENCH-QR can be used as a litmus test of whether visual explanations are truly structure-aware.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è§£é‡Šæ¨¡å‹ï¼ˆCAM methodsï¼‰åœ¨ç»“æ„ä¿çœŸåº¦ï¼ˆstructural faithfulnessï¼‰æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº† CAMBench-QRï¼Œä¸€ä¸ªå…·æœ‰ç»“æ„æ„ŸçŸ¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åˆ©ç”¨äºŒç»´ç ï¼ˆQR codesï¼‰çš„è§„èŒƒå‡ ä½•ç»“æ„ï¼Œå¦‚ finder patternsã€timing lines å’Œ module gridï¼Œæ¥æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½ç²¾å‡†åœ°å°†æ˜¾è‘—æ€§å®šä½åœ¨å¿…è¦çš„å­ç»“æ„ä¸Šï¼Œå¹¶æœ‰æ•ˆé¿å…èƒŒæ™¯å¹²æ‰°ã€‚CAMBench-QR ç»“åˆäº†åˆæˆæ•°æ®ã€ç²¾ç¡®æ©ç åŠå—æ§ç•¸å˜ï¼Œå¹¶å¼•å…¥äº† Finder/Timing Mass Ratiosã€Background Leakage å’Œ Distance-to-Structure ç­‰ç»“æ„æ„ŸçŸ¥æŒ‡æ ‡ï¼ŒåŒæ—¶è¯„ä¼°äº†å› æœé®è”½ã€é²æ£’æ€§å’Œå»¶è¿Ÿã€‚ç ”ç©¶äººå‘˜åœ¨é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰å’Œæœ«å—å¾®è°ƒï¼ˆlast-block fine-tuningï¼‰åœºæ™¯ä¸‹ï¼Œå¯¹ LayerCAMã€EigenGrad-CAM å’Œ XGrad-CAM ç­‰ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥åŸºå‡†æä¾›çš„æŒ‡æ ‡å’Œè®­ç»ƒæ–¹æ¡ˆä¸ºè§†è§‰è§£é‡Šçš„ç»“æ„æ„ŸçŸ¥è¯„ä¼°å»ºç«‹äº†ä¸€ä¸ªç®€å•ä¸”å¯é‡å¤çš„æ ‡å‡†ã€‚ä½œè€…æå‡ºï¼ŒCAMBench-QR å¯ä»¥ä½œä¸ºæ£€éªŒè§†è§‰è§£é‡Šæ¨¡å‹æ˜¯å¦çœŸæ­£å…·å¤‡ç»“æ„æ„ŸçŸ¥èƒ½åŠ›çš„â€œè¯•é‡‘çŸ³â€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 5 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.16745v1",
      "published_date": "2025-09-20 17:13:38 UTC",
      "updated_date": "2025-09-20 17:13:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:30.387115+00:00"
    },
    {
      "arxiv_id": "2509.16743v1",
      "title": "A Hybrid PCA-PR-Seq2Seq-Adam-LSTM Framework for Time-Series Power Outage Prediction",
      "title_zh": "ç”¨äºæ—¶é—´åºåˆ—åœç”µé¢„æµ‹çš„ PCA-PR-Seq2Seq-Adam-LSTM æ··åˆæ¡†æ¶",
      "authors": [
        "Subhabrata Das",
        "Bodruzzaman Khan",
        "Xiao-Yang Liu"
      ],
      "abstract": "Accurately forecasting power outages is a complex task influenced by diverse factors such as weather conditions [1], vegetation, wildlife, and load fluctuations. These factors introduce substantial variability and noise into outage data, making reliable prediction challenging. Long Short-Term Memory (LSTM) networks, a type of Recurrent Neural Network (RNN), are particularly effective for modeling nonlinear and dynamic time-series data, with proven applications in stock price forecasting [2], energy demand prediction, demand response [3], and traffic flow management [4]. This paper introduces a hybrid deep learning framework, termed PCA-PR-Seq2Seq-Adam-LSTM, that integrates Principal Component Analysis (PCA), Poisson Regression (PR), a Sequence-to-Sequence (Seq2Seq) architecture, and an Adam-optimized LSTM. PCA is employed to reduce dimensionality and stabilize data variance, while Poisson Regression effectively models discrete outage events. The Seq2Seq-Adam-LSTM component enhances temporal feature learning through efficient gradient optimization and long-term dependency capture. The framework is evaluated using real-world outage records from Michigan, and results indicate that the proposed approach significantly improves forecasting accuracy and robustness compared to existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å—å¤©æ°”ã€æ¤è¢«å’Œè´Ÿè·æ³¢åŠ¨ç­‰å› ç´ å½±å“è€Œå…·æœ‰é«˜å™ªå£°å’Œå˜å¼‚æ€§çš„åœç”µé¢„æµ‹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º PCA-PR-Seq2Seq-Adam-LSTM çš„æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨ä¸»æˆåˆ†åˆ†æ(Principal Component Analysis, PCA)é™ä½æ•°æ®ç»´åº¦å¹¶ç¨³å®šæ–¹å·®ï¼Œéšåç»“åˆæ³Šæ¾å›å½’(Poisson Regression)å¯¹ç¦»æ•£çš„åœç”µäº‹ä»¶è¿›è¡Œå»ºæ¨¡ã€‚é¢„æµ‹æ ¸å¿ƒç”± Sequence-to-Sequence (Seq2Seq) æ¶æ„ä¸ç»è¿‡ Adam ç®—æ³•ä¼˜åŒ–çš„é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)ç»„æˆï¼Œé€šè¿‡é«˜æ•ˆçš„æ¢¯åº¦ä¼˜åŒ–æ•æ‰æ•°æ®ä¸­çš„é•¿æ—¶ä¾èµ–å¹¶å¢å¼ºæ—¶é—´ç‰¹å¾å­¦ä¹ ã€‚åŸºäºç¾å›½å¯†æ­‡æ ¹å·çœŸå®åœç”µè®°å½•çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ··åˆæ¡†æ¶åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶ä¸ºç”µåŠ›ç³»ç»Ÿçš„é¢„é˜²æ€§ç»´æŠ¤å’Œåº”æ€¥å“åº”æä¾›äº†æ›´ä¸ºå¯é çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16743v1",
      "published_date": "2025-09-20 17:13:25 UTC",
      "updated_date": "2025-09-20 17:13:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:37.798197+00:00"
    },
    {
      "arxiv_id": "2509.16742v1",
      "title": "Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories",
      "title_zh": "åŸºäºä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªé€‚åº”æ¨ç†è½¨è¿¹å¼ºåŒ–å­¦ä¹ çš„è¿åˆè¡Œä¸ºç¼“è§£",
      "authors": [
        "Mohammad Beigi",
        "Ying Shen",
        "Parshin Shojaee",
        "Qifan Wang",
        "Zichao Wang",
        "Chandan Reddy",
        "Ming Jin",
        "Lifu Huang"
      ],
      "abstract": "Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster \\textit{sycophancy}, i.e., the tendency of a model to agree with or reinforce user-provided information even when it's factually incorrect. To address this challenge, we introduce \\textbf{SMART} (Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes sycophancy as a \\textit{reasoning optimization problem} rather than an output alignment issue. SMART is a two-stage framework comprising: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts model exploration based on state-level uncertainty to collect high-quality, diverse reasoning trajectories alongside both stepwise progress and final outcome rewards; and (2) progress-based reinforcement learning, which fine-tunes the model using the collected trajectories and reward signals to reinforce effective reasoning patterns. Through extensive experiments, we show that SMART significantly reduces sycophantic behavior while preserving strong performance on out-of-distribution inputs and maintaining general capabilities. These results underscore the importance of optimizing internal reasoning mechanisms to build more truthful and aligned AI assistants.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€¾å‘äºè¿åˆç”¨æˆ·é”™è¯¯ä¿¡æ¯çš„è°„åªšï¼ˆSycophancyï¼‰ç°è±¡ï¼Œæå‡ºäº†SMARTï¼ˆSycophancy Mitigation through Adaptive Reasoning Trajectoriesï¼‰æ¡†æ¶ã€‚SMARTå°†è°„åªšé—®é¢˜é‡æ–°å®šä¹‰ä¸ºæ¨ç†ä¼˜åŒ–é—®é¢˜ï¼Œè€Œéç®€å•çš„è¾“å‡ºå¯¹é½é—®é¢˜ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæµç¨‹è¿›è¡Œæ²»ç†ã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥äº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆUncertainty-Aware Adaptive Monte Carlo Tree Search, UA-MCTSï¼‰ï¼Œæ ¹æ®çŠ¶æ€çº§çš„ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´æ¨¡å‹æ¢ç´¢ï¼Œä»¥æ”¶é›†åŒ…å«æ­¥éª¤çº§è¿›å±•å’Œæœ€ç»ˆç»“æœå¥–åŠ±çš„é«˜è´¨é‡æ¨ç†è½¨è¿¹ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨åŸºäºè¿›å±•çš„å¼ºåŒ–å­¦ä¹ ï¼ˆProgress-based Reinforcement Learningï¼‰ï¼Œåˆ©ç”¨è¿™äº›è½¨è¿¹å’Œå¥–åŠ±ä¿¡å·å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå¼ºåŒ–æœ‰æ•ˆçš„å†…éƒ¨æ¨ç†æ¨¡å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSMARTåœ¨æ˜¾è‘—å‡å°‘è°„åªšè¡Œä¸ºçš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆä¿ç•™æ¨¡å‹åœ¨åˆ†å¸ƒå¤–ï¼ˆOut-of-Distributionï¼‰è¾“å…¥ä¸Šçš„æ€§èƒ½å¹¶ç»´æŒå…¶é€šç”¨èƒ½åŠ›ã€‚è¯¥æˆæœè¯æ˜äº†é€šè¿‡ä¼˜åŒ–æ¨ç†æœºåˆ¶æ¥æ„å»ºæ›´è¯šå®ä¸”å¯¹é½çš„AIåŠ©æ‰‹å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16742v1",
      "published_date": "2025-09-20 17:09:14 UTC",
      "updated_date": "2025-09-20 17:09:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:40.090559+00:00"
    },
    {
      "arxiv_id": "2509.16724v1",
      "title": "Exploring AI Capabilities in Participatory Budgeting within Smart Cities: The Case of Sao Paulo",
      "title_zh": "æ¢ç´¢æ™ºæ…§åŸå¸‚å‚ä¸å¼é¢„ç®—ä¸­çš„äººå·¥æ™ºèƒ½èƒ½åŠ›ï¼šä»¥ Sao Paulo ä¸ºä¾‹",
      "authors": [
        "Italo Alberto Sousa",
        "Mariana Carvalho da Silva",
        "Jorge Machado",
        "JosÃ© Carlos Vaz"
      ],
      "abstract": "This research examines how Artificial Intelligence (AI) can improve participatory budgeting processes within smart cities. In response to challenges like declining civic participation and resource allocation conflicts, the study explores how online political participation can be improved by AI. It investigates the state capacity governments need to implement AI-enhanced participatory tools, considering technological dependencies and vulnerabilities. It analyzes technological and administrative structures, actors, interests, and strategies to understand the dynamics of online political participation technologies in the case of Sao Paulo, Brazil. The study contributes to understanding how technological advancements can reshape participatory budgeting processes. In a broader sense, the research highlights how AI can transform participatory institutions by offering new tools for citizens and also for government officials in charge of participatory processes within smart cities.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ (Artificial Intelligence, AI) åœ¨æ™ºæ…§åŸå¸‚èƒŒæ™¯ä¸‹å¦‚ä½•æ”¹è¿›å‚ä¸å¼é¢„ç®— (Participatory Budgeting) æµç¨‹ï¼Œä»¥åº”å¯¹å…¬æ°‘å‚ä¸åº¦ä¸‹é™å’Œèµ„æºåˆ†é…å†²çªç­‰æŒ‘æˆ˜ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†åœ¨çº¿æ”¿æ²»å‚ä¸å¦‚ä½•é€šè¿‡ AI æŠ€æœ¯å¾—åˆ°æå‡ï¼Œå¹¶è€ƒå¯Ÿäº†æ”¿åºœåœ¨å®æ–½è¿™äº›å¢å¼ºå‹å·¥å…·æ—¶æ‰€éœ€çš„å›½å®¶èƒ½åŠ› (state capacity) ä»¥åŠç›¸å…³çš„æŠ€æœ¯ä¾èµ–æ€§å’Œè„†å¼±æ€§ã€‚é€šè¿‡å¯¹å·´è¥¿åœ£ä¿ç½— (Sao Paulo) æ¡ˆä¾‹ä¸­æŠ€æœ¯ä¸è¡Œæ”¿ç»“æ„ã€å‚ä¸è€…ã€åˆ©ç›ŠåŠç­–ç•¥çš„æ·±å…¥åˆ†æï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†åœ¨çº¿æ”¿æ²»å‚ä¸æŠ€æœ¯çš„åŠ¨æ€æ¼”å˜ã€‚ç ”ç©¶ç»“æœæœ‰åŠ©äºç†è§£æŠ€æœ¯è¿›æ­¥å¦‚ä½•é‡å¡‘å‚ä¸å¼é¢„ç®—è¿‡ç¨‹ï¼Œå¹¶é˜æ˜äº† AI åœ¨è½¬å‹æ™ºæ…§åŸå¸‚å‚ä¸å¼æœºæ„ã€ä¸ºå…¬æ°‘å’Œæ”¿åºœå®˜å‘˜æä¾›é«˜æ•ˆå·¥å…·æ–¹é¢çš„æ½œåŠ›ã€‚è¿™ä¸€æˆæœä¸ºåˆ©ç”¨å…ˆè¿›æŠ€æœ¯ä¼˜åŒ–åŸå¸‚æ²»ç†å’ŒåŠ å¼ºæ°‘ä¸»å‚ä¸æä¾›äº†é‡è¦çš„å‚è€ƒä¾æ®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "22 pages, Presented at 28th IPSA World Congress of Political Science, Seoul 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16724v1",
      "published_date": "2025-09-20 15:34:50 UTC",
      "updated_date": "2025-09-20 15:34:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:42.991641+00:00"
    },
    {
      "arxiv_id": "2510.01218v1",
      "title": "Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs",
      "title_zh": "æŒæ§æ¸©åº¦ï¼šé¢å‘å¤šæ ·åŒ–ä¸é«˜è´¨é‡å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºçš„é€‰æ‹©æ€§é‡‡æ ·",
      "authors": [
        "Sergey Troshin",
        "Wafaa Mohammed",
        "Yan Meng",
        "Christof Monz",
        "Antske Fokkens",
        "Vlad Niculae"
      ],
      "abstract": "Diversity is an essential metric for evaluating the creativity of outputs generated by language models. Temperature-based sampling is a common strategy to increase diversity. However, for tasks that require high precision, e.g., mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$ or top-$p$, degrades reasoning quality. We demonstrate that the loss of accuracy is caused by sampling incorrect continuations in sensitive decoding positions. To address this, in this paper, we propose \\textbf{selective sampling}, a method that dynamically switches between greedy and high-temperature sampling based on a sampling risk metric. This risk metric estimates the likelihood of output errors when applying high-temperature sampling on the current token position. To predict sampling risk, we train a lightweight classifier on a small subset of verifiable problems. The trained classifier can be integrated with the base language model with minimal latency overhead. Experiments on mathematical reasoning tasks demonstrate that selective sampling enhances the quality-diversity trade-off, even in high-temperature settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿½æ±‚ç”Ÿæˆå¤šæ ·æ€§æ—¶ï¼Œä½¿ç”¨é«˜ Temperature é‡‡æ ·ä¼šå¯¼è‡´æ•°å­¦æ¨ç†ç­‰é«˜ç²¾åº¦ä»»åŠ¡å‡†ç¡®ç‡ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº† Selective Sampling æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œå‡†ç¡®ç‡çš„æŸå¤±ä¸»è¦æºäºåœ¨æ•æ„Ÿè§£ç ä½ç½®é‡‡æ ·äº†é”™è¯¯çš„åç»­æ ‡è®°ã€‚ä¸ºæ­¤ï¼ŒSelective Sampling é€šè¿‡ä¸€ä¸ªé‡‡æ ·é£é™©æŒ‡æ ‡ï¼ŒåŠ¨æ€åœ°åœ¨ Greedy Sampling å’Œé«˜ Temperature é‡‡æ ·ä¹‹é—´è¿›è¡Œåˆ‡æ¢ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åœ¨å°‘é‡å¯éªŒè¯é—®é¢˜çš„å­é›†ä¸Šè®­ç»ƒçš„è½»é‡çº§åˆ†ç±»å™¨æ¥é¢„æµ‹å½“å‰ä½ç½®çš„é‡‡æ ·é£é™©ï¼Œä¸”é›†æˆåˆ°åŸºç¡€æ¨¡å‹ä¸­äº§ç”Ÿçš„å»¶è¿Ÿå¼€é”€æå°ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼Œå³ä½¿åœ¨é«˜æ¸©è®¾ç½®ä¸‹ï¼ŒSelective Sampling ä¹Ÿèƒ½æœ‰æ•ˆä¼˜åŒ–è´¨é‡ä¸å¤šæ ·æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œç¡®ä¿åœ¨æå‡è¾“å‡ºåˆ›é€ åŠ›çš„åŒæ—¶ç»´æŒé«˜ç²¾åº¦çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Second Conference on Language Modeling, 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.01218v1",
      "published_date": "2025-09-20 15:16:27 UTC",
      "updated_date": "2025-09-20 15:16:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:55:50.490536+00:00"
    },
    {
      "arxiv_id": "2509.16721v1",
      "title": "Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding",
      "title_zh": "Text-Sceneï¼šé¢å‘ 3D åœºæ™¯ç†è§£çš„åœºæ™¯åˆ°è¯­è¨€è§£ææ¡†æ¶",
      "authors": [
        "Haoyuan Li",
        "Rui Liu",
        "Hehe Fan",
        "Yi Yang"
      ],
      "abstract": "Enabling agents to understand and interact with complex 3D scenes is a fundamental challenge for embodied artificial intelligence systems. While Multimodal Large Language Models (MLLMs) have achieved significant progress in 2D image understanding, extending such capabilities to 3D scenes remains difficult: 1) 3D environment involves richer concepts such as spatial relationships, affordances, physics, layout, and so on, 2) the absence of large-scale 3D vision-language datasets has posed a significant obstacle. In this paper, we introduce Text-Scene, a framework that automatically parses 3D scenes into textual descriptions for scene understanding. Given a 3D scene, our model identifies object attributes and spatial relationships, and then generates a coherent summary of the whole scene, bridging the gap between 3D observation and language without requiring human-in-the-loop intervention. By leveraging both geometric analysis and MLLMs, Text-Scene produces descriptions that are accurate, detailed, and human-interpretable, capturing object-level details and global-level context. Experimental results on benchmarks demonstrate that our textual parses can faithfully represent 3D scenes and benefit downstream tasks. To evaluate the reasoning capability of MLLMs, we present InPlan3D, a comprehensive benchmark for 3D task planning, consisting of 3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity and accessibility in our approach, aiming to make 3D scene content understandable through language. Code and datasets will be released.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½(Embodied AI)ä¸­3Dåœºæ™¯ç†è§£çš„å¤æ‚æ€§ä»¥åŠå¤§è§„æ¨¡3Dè§†è§‰è¯­è¨€æ•°æ®é›†ç¼ºå¤±çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºText-Sceneçš„åœºæ™¯åˆ°è¯­è¨€è§£ææ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆå‡ ä½•åˆ†æä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)ï¼Œé€šè¿‡è¯†åˆ«ç‰©ä½“å±æ€§å’Œç©ºé—´å…³ç³»(Spatial Relationships)ï¼Œåœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹å°†3Dåœºæ™¯è‡ªåŠ¨è½¬åŒ–ä¸ºå‡†ç¡®ä¸”è¯¦å°½çš„æ–‡æœ¬æè¿°ã€‚Text-Sceneèƒ½å¤Ÿæœ‰æ•ˆæ•è·ç‰©ä½“çº§ç»†èŠ‚ä¸å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¡«è¡¥äº†3Dè§‚æµ‹æ•°æ®ä¸è‡ªç„¶è¯­è¨€ç†è§£ä¹‹é—´çš„é¸¿æ²Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„æ–‡æœ¬è§£æèƒ½å¤Ÿå¿ å®åœ°ä»£è¡¨3Dåœºæ™¯ï¼Œå¹¶æ˜¾è‘—å—ç›Šäºä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ¨å‡ºäº†åŒ…å«3174ä¸ªé•¿ç¨‹è§„åˆ’ä»»åŠ¡çš„InPlan3DåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨3Dä»»åŠ¡è§„åˆ’(3D Task Planning)ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶é€šè¿‡è¯­è¨€åŒ–æ‰‹æ®µæå‡äº†3Dåœºæ™¯çš„å¯ç†è§£æ€§ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„æ™ºèƒ½äº¤äº’æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 12 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.16721v1",
      "published_date": "2025-09-20 15:10:45 UTC",
      "updated_date": "2025-09-20 15:10:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:10.083912+00:00"
    },
    {
      "arxiv_id": "2509.20378v1",
      "title": "Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation",
      "title_zh": "è¶…è¶Šå…¨å±€æƒ…æ„Ÿï¼šåŸºäºåŠ¨æ€è¯çº§è°ƒåˆ¶çš„ç»†ç²’åº¦æƒ…æ„Ÿè¯­éŸ³åˆæˆ",
      "authors": [
        "Sirui Wang",
        "Andong Chen",
        "Tiejun Zhao"
      ],
      "abstract": "Emotional text-to-speech (E-TTS) is central to creating natural and trustworthy human-computer interaction. Existing systems typically rely on sentence-level control through predefined labels, reference audio, or natural language prompts. While effective for global emotion expression, these approaches fail to capture dynamic shifts within a sentence. To address this limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to words to obtain word-level emotion annotations, and maps them through a Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion control by directly modulating text embeddings. To support evaluation, we construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed annotations of emotional transitions. Experiments show that Emo-FiLM outperforms existing approaches on both global and fine-grained tasks, demonstrating its effectiveness and generality for expressive speech synthesis.",
      "tldr_zh": "æƒ…æ„Ÿæ–‡æœ¬è½¬è¯­éŸ³ (Emotional text-to-speech, E-TTS) æ˜¯å®ç°è‡ªç„¶äººæœºäº¤äº’çš„å…³é”®ï¼Œä½†ç°æœ‰ç³»ç»Ÿå¤šä¾èµ–å¥å­çº§æ§åˆ¶ï¼Œéš¾ä»¥æ•æ‰å¥å†…çš„åŠ¨æ€æƒ…æ„Ÿå˜åŒ–ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™ï¼Œè¯¥ç ”ç©¶æå‡ºäº† Emo-FiLMï¼Œä¸€ç§é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM-based TTS) çš„ç»†ç²’åº¦æƒ…æ„Ÿå»ºæ¨¡æ¡†æ¶ã€‚Emo-FiLM å°†æ¥è‡ª emotion2vec çš„å¸§çº§ç‰¹å¾ä¸å•è¯å¯¹é½ä»¥è·å–è¯çº§æƒ…æ„Ÿæ ‡æ³¨ï¼Œå¹¶é€šè¿‡ç‰¹å¾çº¿æ€§è°ƒåˆ¶ (Feature-wise Linear Modulation, FiLM) å±‚ç›´æ¥è°ƒåˆ¶æ–‡æœ¬åµŒå…¥ (text embeddings)ï¼Œä»è€Œå®ç°ç²¾å‡†çš„è¯çº§æƒ…æ„Ÿæ§åˆ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…æ„å»ºäº†ç»†ç²’åº¦æƒ…æ„ŸåŠ¨æ€æ•°æ®é›† (Fine-grained Emotion Dynamics Dataset, FEDD)ï¼Œæä¾›äº†è¯¦ç»†çš„æƒ…æ„Ÿè½¬ç§»æ ‡æ³¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEmo-FiLM åœ¨å…¨å±€å’Œç»†ç²’åº¦ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨å¢å¼ºè¯­éŸ³åˆæˆè¡¨ç°åŠ›æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ä¸é€šç”¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20378v1",
      "published_date": "2025-09-20 14:26:15 UTC",
      "updated_date": "2025-09-20 14:26:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:10.803152+00:00"
    },
    {
      "arxiv_id": "2509.16682v1",
      "title": "Design and Development of an Intelligent LLM-based LDAP Honeypot",
      "title_zh": "åŸºäº LLM çš„æ™ºèƒ½ LDAP èœœç½çš„è®¾è®¡ä¸å¼€å‘",
      "authors": [
        "Javier JimÃ©nez-RomÃ¡n",
        "Florina Almenares-Mendoza",
        "Alfonso SÃ¡nchez-MaciÃ¡n"
      ],
      "abstract": "Cybersecurity threats continue to increase, with a growing number of previously unknown attacks each year targeting both large corporations and smaller entities. This scenario demands the implementation of advanced security measures, not only to mitigate damage but also to anticipate emerging attack trends. In this context, deception tools have become a key strategy, enabling the detection, deterrence, and deception of potential attackers while facilitating the collection of information about their tactics and methods. Among these tools, honeypots have proven their value, although they have traditionally been limited by rigidity and configuration complexity, hindering their adaptability to dynamic scenarios. The rise of artificial intelligence, and particularly general-purpose Large Language Models (LLMs), is driving the development of new deception solutions capable of offering greater adaptability and ease of use. This work proposes the design and implementation of an LLM-based honeypot to simulate an LDAP server, a critical protocol present in most organizations due to its central role in identity and access management. The proposed solution aims to provide a flexible and realistic tool capable of convincingly interacting with attackers, thereby contributing to early detection and threat analysis while enhancing the defensive capabilities of infrastructures against intrusions targeting this service.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç½‘ç»œå®‰å…¨å¨èƒæ—¥ç›Šå¢åŠ ä»¥åŠä¼ ç»Ÿèœœç½ (honeypots) å› é…ç½®å¤æ‚ä¸”ç¼ºä¹çµæ´»æ€§è€Œéš¾ä»¥åº”å¯¹åŠ¨æ€æ”»å‡»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ™ºèƒ½ LDAP èœœç½è®¾è®¡ä¸å®ç°æ–¹æ¡ˆã€‚ç”±äº LDAP åè®®åœ¨ç»„ç»‡èº«ä»½ä¸è®¿é—®ç®¡ç†ä¸­å…·æœ‰æ ¸å¿ƒä½œç”¨ï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨ LLM çš„è‡ªé€‚åº”èƒ½åŠ›æ¥æ¨¡æ‹Ÿé«˜åº¦é€¼çœŸçš„ LDAP æœåŠ¡å™¨ç¯å¢ƒã€‚é€šè¿‡ä¸æ”»å‡»è€…è¿›è¡Œå…·æœ‰è¯´æœåŠ›çš„äº¤äº’ï¼Œè¯¥å·¥å…·èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯±å¯¼ã€å¨æ…‘å¹¶æ¬ºéª—æ½œåœ¨æ”»å‡»è€…ï¼ŒåŒæ—¶æ”¶é›†å…¶æ”»å‡»æˆ˜æœ¯å’Œæ–¹æ³•çš„å…³é”®ä¿¡æ¯ã€‚è¿™ç§åŸºäº AI çš„æ¬ºéª—æŠ€æœ¯å…‹æœäº†ä¼ ç»Ÿé˜²å¾¡å·¥å…·çš„å±€é™æ€§ï¼Œå®ç°äº†æ›´å¼ºçš„æ˜“ç”¨æ€§å’Œé€‚åº”æ€§ã€‚è¯¥ç³»ç»Ÿçš„å®æ–½æœ‰åŠ©äºå®‰å…¨äººå‘˜åŠæ—©å‘ç°æ½œåœ¨å¨èƒå¹¶è¿›è¡Œæ·±åº¦æ”»å‡»åˆ†æï¼Œä¸ºå¢å¼ºç»„ç»‡åŸºç¡€è®¾æ–½å¯¹æŠ—é’ˆå¯¹å…³é”®æœåŠ¡çš„å…¥ä¾µæä¾›äº†æœ‰æ•ˆçš„é˜²å¾¡ç­–ç•¥å’ŒæŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16682v1",
      "published_date": "2025-09-20 13:16:07 UTC",
      "updated_date": "2025-09-20 13:16:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:22.975852+00:00"
    },
    {
      "arxiv_id": "2509.25207v1",
      "title": "Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹é²æ£’è¡¨æ ¼ç‰¹å¾å·¥ç¨‹çš„å¤šå±‚çº§è¯Šæ–­ä¸è¯„ä¼°",
      "authors": [
        "Yebin Lim",
        "Susik Yoon"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have shown promise in feature engineering for tabular data, but concerns about their reliability persist, especially due to variability in generated outputs. We introduce a multi-level diagnosis and evaluation framework to assess the robustness of LLMs in feature engineering across diverse domains, focusing on the three main factors: key variables, relationships, and decision boundary values for predicting target classes. We demonstrate that the robustness of LLMs varies significantly over different datasets, and that high-quality LLM-generated features can improve few-shot prediction performance by up to 10.52%. This work opens a new direction for assessing and enhancing the reliability of LLM-driven feature engineering in various domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šå±‚æ¬¡è¯Šæ–­ä¸è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¡¨æ ¼æ•°æ®ç‰¹å¾å·¥ç¨‹(tabular feature engineering)ä¸­çš„é²æ£’æ€§ã€‚è¯¥æ¡†æ¶é‡ç‚¹å…³æ³¨ä¸‰ä¸ªæ ¸å¿ƒå› ç´ ï¼šå…³é”®å˜é‡ã€å˜é‡é—´å…³ç³»ä»¥åŠé¢„æµ‹ç›®æ ‡ç±»åˆ«çš„å†³ç­–è¾¹ç•Œå€¼(decision boundary values)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsçš„é²æ£’æ€§åœ¨ä¸åŒæ•°æ®é›†ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚é«˜è´¨é‡çš„LLMç”Ÿæˆç‰¹å¾åœ¨å°æ ·æœ¬é¢„æµ‹(few-shot prediction)ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½æå‡æœ€é«˜å¯è¾¾10.52%ã€‚æ­¤é¡¹å·¥ä½œä¸ºåœ¨å„é¢†åŸŸè¯„ä¼°å’Œæå‡å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç‰¹å¾å·¥ç¨‹çš„å¯é æ€§æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to Findings of EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.25207v1",
      "published_date": "2025-09-20 13:13:36 UTC",
      "updated_date": "2025-09-20 13:13:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:20.887182+00:00"
    },
    {
      "arxiv_id": "2509.16680v1",
      "title": "ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering",
      "title_zh": "ProtoVQAï¼šä¸€ç§é¢å‘å¯è§£é‡Šç»†ç²’åº¦è§†è§‰é—®ç­”çš„å¯é€‚é…åŸå‹æ¡†æ¶",
      "authors": [
        "Xingjian Diao",
        "Weiyi Wu",
        "Keyi Kong",
        "Peijun Qing",
        "Xinwen Xu",
        "Ming Cheng",
        "Soroush Vosoughi",
        "Jiang Gui"
      ],
      "abstract": "Visual Question Answering (VQA) is increasingly used in diverse applications ranging from general visual reasoning to safety-critical domains such as medical imaging and autonomous systems, where models must provide not only accurate answers but also explanations that humans can easily understand and verify. Prototype-based modeling has shown promise for interpretability by grounding predictions in semantically meaningful regions for purely visual reasoning tasks, yet remains underexplored in the context of VQA. We present ProtoVQA, a unified prototypical framework that (i) learns question-aware prototypes that serve as reasoning anchors, connecting answers to discriminative image regions, (ii) applies spatially constrained matching to ensure that the selected evidence is coherent and semantically relevant, and (iii) supports both answering and grounding tasks through a shared prototype backbone. To assess explanation quality, we propose the Visual-Linguistic Alignment Score (VLAS), which measures how well the model's attended regions align with ground-truth evidence. Experiments on Visual7W show that ProtoVQA yields faithful, fine-grained explanations while maintaining competitive accuracy, advancing the development of transparent and trustworthy VQA systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®‰å…¨å…³é”®é¢†åŸŸä¸­ Visual Question Answering (VQA) å¯¹å¯è§£é‡Šæ€§çš„éœ€æ±‚ï¼Œæå‡ºäº† ProtoVQA æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ Prototype-based modeling æé«˜æ¨¡å‹å†³ç­–çš„é€æ˜åº¦ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹  question-aware prototypes ä½œä¸ºæ¨ç†é”šç‚¹ï¼Œå°†ç”Ÿæˆçš„ç­”æ¡ˆä¸å›¾åƒä¸­çš„åˆ¤åˆ«æ€§åŒºåŸŸç›´æ¥è”ç³»èµ·æ¥ï¼Œå¹¶åˆ©ç”¨ spatially constrained matching ç¡®ä¿æ‰€é€‰è¯æ®åœ¨è¯­ä¹‰ä¸Šçš„è¿è´¯æ€§ã€‚ProtoVQA é‡‡ç”¨å…±äº«çš„ prototype backbone åŒæ—¶æ”¯æŒå›ç­”å’Œå®šä½ï¼ˆgroundingï¼‰ä»»åŠ¡ï¼Œå®ç°äº†å¤šä»»åŠ¡çš„ååŒæ¨ç†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ Visual-Linguistic Alignment Score (VLAS)ï¼Œç”¨äºé‡åŒ–è¡¡é‡æ¨¡å‹å…³æ³¨åŒºåŸŸä¸çœŸå®è¯æ®ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚åœ¨ Visual7W æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒProtoVQA åœ¨ä¿æŒç«äº‰åŠ›çš„å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæä¾›å¿ å®ä¸”ç»†ç²’åº¦çš„è§£é‡Šï¼Œæœ‰æ•ˆæ¨è¿›äº†é€æ˜ä¸”å¯ä¿¡çš„ VQA ç³»ç»Ÿå¼€å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2509.16680v1",
      "published_date": "2025-09-20 13:12:08 UTC",
      "updated_date": "2025-09-20 13:12:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:22.806723+00:00"
    },
    {
      "arxiv_id": "2509.16676v1",
      "title": "Governed By Agents: A Survey On The Role Of Agentic AI In Future Computing Environments",
      "title_zh": "æ™ºèƒ½ä½“æ²»ç†ï¼šä»£ç†å¼äººå·¥æ™ºèƒ½åœ¨æœªæ¥è®¡ç®—ç¯å¢ƒä¸­çš„è§’è‰²ç»¼è¿°",
      "authors": [
        "Nauman Ali Murad",
        "Safia Baloch"
      ],
      "abstract": "The emergence of agentic Artificial Intelligence (AI), which can operate autonomously, demonstrate goal-directed behavior, and adaptively learn, indicates the onset of a massive change in today's computing infrastructure. This study investigates how agentic AI models' multiple characteristics may impact the architecture, governance, and operation under which computing environments function. Agentic AI has the potential to reduce reliance on extremely large (public) cloud environments due to resource efficiency, especially with processing and/or storage. The aforementioned characteristics provide us with an opportunity to canvas the likelihood of strategic migration in computing infrastructures away from massive public cloud services, towards more locally distributed architectures: edge computing and on-premises computing infrastructures. Many of these likely migrations will be spurred by factors like on-premises processing needs, diminished data consumption footprints, and cost savings. This study examines how a solution for implementing AI's autonomy could result in a re-architecture of the systems and model a departure from today's governance models to help us manage these increasingly autonomous agents, and an operational overhaul of processes over a very diverse computing systems landscape that bring together computing via cloud, edge, and on-premises computing solutions. To enable us to explore these intertwined decisions, it will be fundamentally important to understand how to best position agentic AI, and to navigate the future state of computing infrastructures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½(Agentic AI)åœ¨æœªæ¥è®¡ç®—ç¯å¢ƒä¸­çš„è§’è‰²åŠå…¶å¯¹è®¡ç®—åŸºç¡€è®¾æ–½æ¶æ„ã€æ²»ç†å’Œè¿è¡Œæ¨¡å¼çš„æ·±è¿œå½±å“ã€‚æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½(Agentic AI)å…·å¤‡è‡ªä¸»æ€§ã€ç›®æ ‡å¯¼å‘è¡Œä¸ºå’Œè‡ªé€‚åº”å­¦ä¹ èƒ½åŠ›ï¼Œæ­£æ¨åŠ¨å½“å‰è®¡ç®—ç³»ç»Ÿå‘ç”Ÿæ ¹æœ¬æ€§å˜é©ã€‚è®ºæ–‡åˆ†æäº†ç”±äºèµ„æºæ•ˆç‡å’Œæˆæœ¬ä¼˜åŠ¿ï¼Œè®¡ç®—æ¶æ„å¯èƒ½ä»å¤§å‹å…¬å…±äº‘(Public Cloud)ç¯å¢ƒå‘è¾¹ç¼˜è®¡ç®—(Edge Computing)å’Œæœ¬åœ°è®¡ç®—(On-premises Computing)ç­‰åˆ†å¸ƒå¼æ¶æ„è¿›è¡Œæˆ˜ç•¥æ€§è¿ç§»ã€‚è¿™ç§è¿ç§»ä¸»è¦å—åˆ°æœ¬åœ°å¤„ç†éœ€æ±‚ã€æ•°æ®æ¶ˆè€—å‡å°‘ä»¥åŠæˆæœ¬èŠ‚çº¦ç­‰å› ç´ çš„é©±åŠ¨ï¼Œæ—¨åœ¨é™ä½å¯¹é›†ä¸­å¼äº‘æœåŠ¡çš„ä¾èµ–ã€‚ç ”ç©¶è¿›ä¸€æ­¥å®¡è§†äº†AIè‡ªä¸»æ€§å¦‚ä½•å¼•å‘ç³»ç»Ÿé‡æ„ï¼Œå¹¶æ¢è®¨äº†ä»å½“å‰æ²»ç†æ¨¡å‹å‘èƒ½å¤Ÿç®¡ç†é«˜åº¦è‡ªä¸»æ™ºèƒ½ä½“çš„æ–°å‹æ²»ç†æ¨¡å¼åŠè¿è¥æµç¨‹çš„è½¬å˜ã€‚é€šè¿‡æ•´åˆäº‘ã€è¾¹ç¼˜åŠæœ¬åœ°è§£å†³æ–¹æ¡ˆï¼Œè¯¥é¡¹è°ƒæŸ¥ä¸ºå¦‚ä½•æœ€ä½³å®šä½æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½(Agentic AI)å¹¶å¼•é¢†æœªæ¥è®¡ç®—åŸºç¡€è®¾æ–½çš„å‘å±•æä¾›äº†å…³é”®æ´å¯Ÿã€‚",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16676v1",
      "published_date": "2025-09-20 13:03:11 UTC",
      "updated_date": "2025-09-20 13:03:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:26.559684+00:00"
    },
    {
      "arxiv_id": "2509.16662v1",
      "title": "On the de-duplication of the Lakh MIDI dataset",
      "title_zh": "å…³äº Lakh MIDI æ•°æ®é›†å»é‡çš„ç ”ç©¶",
      "authors": [
        "Eunjin Choi",
        "Hyerin Kim",
        "Jiwoo Ryu",
        "Juhan Nam",
        "Dasaem Jeong"
      ],
      "abstract": "A large-scale dataset is essential for training a well-generalized deep-learning model. Most such datasets are collected via scraping from various internet sources, inevitably introducing duplicated data. In the symbolic music domain, these duplicates often come from multiple user arrangements and metadata changes after simple editing. However, despite critical issues such as unreliable training evaluation from data leakage during random splitting, dataset duplication has not been extensively addressed in the MIR community. This study investigates the dataset duplication issues regarding Lakh MIDI Dataset (LMD), one of the largest publicly available sources in the symbolic music domain. To find and evaluate the best retrieval method for duplicated data, we employed the Clean MIDI subset of the LMD as a benchmark test set, in which different versions of the same songs are grouped together. We first evaluated rule-based approaches and previous symbolic music retrieval models for de-duplication and also investigated with a contrastive learning-based BERT model with various augmentations to find duplicate files. As a result, we propose three different versions of the filtered list of LMD, which filters out at least 38,134 samples in the most conservative settings among 178,561 files.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¬¦å·éŸ³ä¹é¢†åŸŸå¤§è§„æ¨¡æ•°æ®é›† Lakh MIDI Dataset (LMD) ä¸­ä¸¥é‡çš„æ•°æ®é‡å¤é—®é¢˜ï¼ŒæŒ‡å‡ºè¿™äº›é‡å¤é€šå¸¸æºäºäº’è”ç½‘æŠ“å–è¿‡ç¨‹ä¸­çš„å¤šç”¨æˆ·ç¼–æ’å’Œå…ƒæ•°æ®ä¿®æ”¹ã€‚æ•°æ®é‡å¤ä¼šå¼•å‘æ•°æ®æ³„éœ²(data leakage)é£é™©ï¼Œå¯¼è‡´éšæœºåˆ’åˆ†æ•°æ®é›†åçš„æ¨¡å‹è®­ç»ƒè¯„ä¼°ç»“æœå¤±å»å¯é æ€§ã€‚ä¸ºäº†å¯»æ‰¾æœ€ä½³çš„æ£€ç´¢ä¸å»é‡æ–¹æ³•ï¼Œç ”ç©¶äººå‘˜ä»¥ Clean MIDI å­é›†ä¸ºåŸºå‡†ï¼Œå¯¹æ¯”è¯„ä¼°äº†åŸºäºè§„åˆ™çš„æ–¹æ³•ã€ä¼ ç»Ÿçš„ç¬¦å·éŸ³ä¹æ£€ç´¢æ¨¡å‹ä»¥åŠç»“åˆå¤šç§æ•°æ®å¢å¼ºæŠ€æœ¯çš„å¯¹æ¯”å­¦ä¹ (contrastive learning) BERT æ¨¡å‹ã€‚æœ€ç»ˆï¼Œç ”ç©¶æå‡ºäº†ä¸‰ä¸ªç‰ˆæœ¬çš„ LMD è¿‡æ»¤æ¸…å•ï¼Œåœ¨æœ€ä¿å®ˆçš„è®¾ç½®ä¸‹ä» 178,561 ä¸ªæ–‡ä»¶ä¸­å‰”é™¤äº†è‡³å°‘ 38,134 ä¸ªé‡å¤æ ·æœ¬ã€‚è¯¥æˆæœæœ‰æ•ˆè§£å†³äº† MIR ç¤¾åŒºé•¿æœŸå¿½è§†çš„æ•°æ®é›†å†—ä½™é—®é¢˜ï¼Œä¸ºæ„å»ºæ³›åŒ–èƒ½åŠ›æ›´å¼ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "The paper has been accepted for publication at ISMIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16662v1",
      "published_date": "2025-09-20 12:31:30 UTC",
      "updated_date": "2025-09-20 12:31:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:31.971317+00:00"
    },
    {
      "arxiv_id": "2509.16656v2",
      "title": "NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities",
      "title_zh": "NUMINAï¼šé¢å‘å¤šç»´æ™ºèƒ½ä¸æ•°å€¼æ¨ç†èƒ½åŠ›çš„è‡ªç„¶ç†è§£åŸºå‡†æµ‹è¯•",
      "authors": [
        "Changyu Zeng",
        "Yifan Wang",
        "Zimu Wang",
        "Wei Wang",
        "Zhengni Yang",
        "Muyi Bao",
        "Jiming Xiao",
        "Anh Nguyen",
        "Yutao Yue"
      ],
      "abstract": "Recent advancements in 2D multimodal large language models (MLLMs) have significantly improved performance in vision-language tasks. However, extending these capabilities to 3D environments remains a distinct challenge due to the complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability to perform precise spatial measurements and complex numerical reasoning. To address this gap, we introduce NUMINA, the first Natural Understanding benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities to enhance multimodal indoor perceptual understanding. NUMINA features multi-scale annotations and various question-answer pairs, generated using NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and rule-based self-verification. We evaluate the performance of various state-of-the-art LLMs on NUMINA following the Chat-Scene framework, demonstrating that current LLMs struggle with multimodal numerical reasoning, particularly in performing precise computations such as distance and volume estimation, highlighting the need for further advancements in 3D models. The dataset and source codes can be obtained from https://github.com/fengshun124/NUMINA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†NUMINAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹å¤šç»´æ™ºèƒ½å’Œæ•°å€¼æ¨ç†èƒ½åŠ›(Numerical reasoning Abilities)çš„è‡ªç„¶ç†è§£åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€å®¤å†…æ„ŸçŸ¥ç†è§£æ°´å¹³ã€‚é’ˆå¯¹ç°æœ‰3DåŸºå‡†æµ‹è¯•ç¼ºä¹ç»†ç²’åº¦æ•°å€¼æ¨ç†æ ‡æ³¨ï¼Œå¯¼è‡´å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)éš¾ä»¥è¿›è¡Œç²¾ç¡®ç©ºé—´æµ‹é‡çš„é—®é¢˜ï¼ŒNUMINAæä¾›äº†å¤šå°ºåº¦æ ‡æ³¨å’Œä¸°å¯Œçš„é—®ç­”å¯¹ã€‚è¿™äº›èµ„æºé€šè¿‡è‡ªåŠ¨åŒ–æ ‡æ³¨æµæ°´çº¿NUMINA-Flowç”Ÿæˆï¼Œè¯¥æµæ°´çº¿é›†æˆäº†LLMé‡å†™ä¸åŸºäºè§„åˆ™çš„è‡ªæˆ‘éªŒè¯(self-verification)æŠ€æœ¯ã€‚å®éªŒé€šè¿‡Chat-Sceneæ¡†æ¶è¯„ä¼°äº†å¤šç§é¡¶å°–æ¨¡å‹ï¼Œç»“æœè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨å¤„ç†è·ç¦»å’Œä½“ç§¯ä¼°ç®—ç­‰ç²¾ç¡®æ•°å€¼æ¨ç†ä»»åŠ¡æ—¶ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æå‡3Dæ¨¡å‹å¤šæ¨¡æ€æ•°å€¼æ¨ç†èƒ½åŠ›çš„å¿…è¦æ€§ï¼Œå¹¶å¼€æ”¾äº†ç›¸å…³æ•°æ®é›†å’Œæºä»£ç ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16656v2",
      "published_date": "2025-09-20 12:05:47 UTC",
      "updated_date": "2025-10-01 01:50:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:36.769768+00:00"
    },
    {
      "arxiv_id": "2509.16649v1",
      "title": "AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval",
      "title_zh": "AISTAT å®éªŒå®¤ DCASE2025 ä»»åŠ¡ 6 ç³»ç»Ÿï¼šåŸºäºè¯­è¨€çš„éŸ³é¢‘æ£€ç´¢",
      "authors": [
        "Hyun Jun Kim",
        "Hyeong Yong Choi",
        "Changwon Lim"
      ],
      "abstract": "This report presents the AISTAT team's submission to the language-based audio retrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder architecture, where audio and text modalities are encoded separately, and their representations are aligned using contrastive learning. Drawing inspiration from methodologies of the previous year's challenge, we implemented a distillation approach and leveraged large language models (LLMs) for effective data augmentation techniques, including back-translation and LLM mix. Additionally, we incorporated clustering to introduce an auxiliary classification task for further finetuning. Our best single system achieved a mAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on the Clotho development test split.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† AISTAT å›¢é˜Ÿé’ˆå¯¹ DCASE 2025 Task 6 ä»»åŠ¡æå‡ºçš„åŸºäºè¯­è¨€çš„éŸ³é¢‘æ£€ç´¢ (Language-based audio retrieval) ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†åŒç¼–ç å™¨æ¶æ„ (Dual encoder architecture)ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹  (Contrastive learning) å®ç°éŸ³é¢‘ä¸æ–‡æœ¬æ¨¡æ€çš„å¯¹é½ã€‚ç ”ç©¶ä¸­å¼•å…¥äº†è’¸é¦ (Distillation) æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) é€šè¿‡å›è¯‘ (Back-translation) å’Œ LLM mix è¿›è¡Œæœ‰æ•ˆçš„æ•°æ®å¢å¼ºã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜ç»“åˆèšç±» (Clustering) å¼•å…¥äº†è¾…åŠ©åˆ†ç±»ä»»åŠ¡ä»¥è¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶æœ€ä½³å•ç³»ç»Ÿåœ¨ Clotho å¼€å‘æµ‹è¯•é›†ä¸Šçš„ mAP@16 è¾¾åˆ° 46.62ï¼Œè€Œå››ç³»ç»Ÿé›†æˆ (Ensemble) çš„ mAP@16 åˆ™æå‡è‡³ 48.83ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 1 figure, DCASE2025 Task2 technical report",
      "pdf_url": "https://arxiv.org/pdf/2509.16649v1",
      "published_date": "2025-09-20 11:53:18 UTC",
      "updated_date": "2025-09-20 11:53:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:36.263442+00:00"
    },
    {
      "arxiv_id": "2509.16648v3",
      "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs",
      "title_zh": "FESTAï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¿¡ä»»è¯„ä¼°çš„åŠŸèƒ½ç­‰æ•ˆé‡‡æ ·",
      "authors": [
        "Debarpan Bhattacharya",
        "Apoorva Kulkarni",
        "Sriram Ganapathy"
      ],
      "abstract": "The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FESTAï¼ˆFunctionally Equivalent Sampling for Trust Assessmentï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Models, MLLMsï¼‰çš„è¾“å…¥é‡‡æ ·æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆä¸ç¡®å®šæ€§åº¦é‡æ¥å®ç°å‡†ç¡®çš„ä¿¡ä»»è¯„ä¼°ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä»»åŠ¡ä¿æŒï¼ˆtask-preservingï¼‰çš„é‡‡æ ·ç­–ç•¥ï¼Œé€šè¿‡ç­‰æ•ˆæ ·æœ¬æ¢æµ‹æ¨¡å‹çš„ä¸€è‡´æ€§ï¼ˆconsistencyï¼‰ä»¥åŠé€šè¿‡äº’è¡¥æ ·æœ¬æ¢æµ‹å…¶æ•æ„Ÿæ€§ï¼ˆsensitivityï¼‰ï¼Œä»è€Œåœ¨æ‰©å±•çš„è¾“å…¥ç©ºé—´ä¸­è¿›è¡Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚FESTAä»…éœ€å¯¹æ¨¡å‹è¿›è¡Œé»‘ç›’ï¼ˆblack-boxï¼‰è®¿é—®ï¼Œä¸”å…·æœ‰æ— éœ€çœŸå®æ ‡ç­¾ï¼ˆunsupervisedï¼‰çš„ç‰¹ç‚¹ï¼Œé€‚ç”¨äºè§†è§‰å’ŒéŸ³é¢‘ç­‰å¤šç§æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ£€æµ‹é”™è¯¯é¢„æµ‹çš„AUROCæŒ‡æ ‡ä¸Šï¼ŒFESTAåœ¨è§†è§‰LLMså’ŒéŸ³é¢‘LLMsä¸Šåˆ†åˆ«å®ç°äº†33.3%å’Œ29.6%çš„ç›¸å¯¹æå‡ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é€‰æ‹©æ€§é¢„æµ‹å’Œå¢å¼ºç”¨æˆ·å¯¹MLLMsçš„ä¿¡å¿ƒæä¾›äº†æœ‰æ•ˆæ‰‹æ®µï¼Œä¸”ç›¸å…³ä»£ç å·²å¼€æºã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in the Findings of EMNLP, 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16648v3",
      "published_date": "2025-09-20 11:50:22 UTC",
      "updated_date": "2025-11-02 05:16:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:41.977979+00:00"
    },
    {
      "arxiv_id": "2509.16638v1",
      "title": "KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control",
      "title_zh": "KungfuBot2ï¼šé¢å‘äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶çš„å¤šæ ·åŒ–è¿åŠ¨æŠ€èƒ½å­¦ä¹ ",
      "authors": [
        "Jinrui Han",
        "Weiji Xie",
        "Jiakun Zheng",
        "Jiyuan Shi",
        "Weinan Zhang",
        "Ting Xiao",
        "Chenjia Bai"
      ],
      "abstract": "Learning versatile whole-body skills by tracking various human motions is a fundamental step toward general-purpose humanoid robots. This task is particularly challenging because a single policy must master a broad repertoire of motion skills while ensuring stability over long-horizon sequences. To this end, we present VMS, a unified whole-body controller that enables humanoid robots to learn diverse and dynamic behaviors within a single policy. Our framework integrates a hybrid tracking objective that balances local motion fidelity with global trajectory consistency, and an Orthogonal Mixture-of-Experts (OMoE) architecture that encourages skill specialization while enhancing generalization across motions. A segment-level tracking reward is further introduced to relax rigid step-wise matching, enhancing robustness when handling global displacements and transient inaccuracies. We validate VMS extensively in both simulation and real-world experiments, demonstrating accurate imitation of dynamic skills, stable performance over minute-long sequences, and strong generalization to unseen motions. These results highlight the potential of VMS as a scalable foundation for versatile humanoid whole-body control. The project page is available at https://kungfubot2-humanoid.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VMSï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡å•ä¸€ç­–ç•¥å®ç°äººå½¢æœºå™¨äººå­¦ä¹ å¤šæ ·åŒ–åŠ¨æ€å…¨èº«è¿åŠ¨æŠ€èƒ½çš„ç»Ÿä¸€æ§åˆ¶å™¨ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ­£äº¤ä¸“å®¶æ··åˆ(Orthogonal Mixture-of-Experts, OMoE)æ¶æ„ï¼Œåœ¨æå‡è¿åŠ¨æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ä¿ƒè¿›äº†ç‰¹å®šæŠ€èƒ½çš„ä¸“ä¸šåŒ–ã€‚ç ”ç©¶é€šè¿‡æ•´åˆæ··åˆè·Ÿè¸ªç›®æ ‡æ¥å¹³è¡¡å±€éƒ¨åŠ¨ä½œå¿ å®åº¦ä¸å…¨å±€è½¨è¿¹ä¸€è‡´æ€§ï¼Œå¹¶å¼•å…¥ç‰‡æ®µçº§è·Ÿè¸ªå¥–åŠ±(segment-level tracking reward)ä»¥æ”¾å®½ä¸¥æ ¼çš„é€æ­¥åŒ¹é…é™åˆ¶ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†å¤„ç†å…¨å±€ä½ç§»å’Œç¬æ—¶è¯¯å·®æ—¶çš„ç³»ç»Ÿé²æ£’æ€§ã€‚å®éªŒåœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œä¸­å……åˆ†éªŒè¯äº†VMSçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½ç²¾ç¡®æ¨¡ä»¿åŠ¨æ€åŠ¨ä½œå¹¶åœ¨é•¿è¾¾æ•°åˆ†é’Ÿçš„åºåˆ—ä¸­ä¿æŒç¨³å®šè¡¨ç°ã€‚æ­¤å¤–ï¼ŒVMSåœ¨æœªè§è¿‡çš„è¿åŠ¨ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–æ½œåŠ›ï¼Œä¸ºæ„å»ºå¯æ‰©å±•çš„äººå½¢æœºå™¨äººå…¨èº«æ§åˆ¶(whole-body control)åŸºç¡€æ¨¡å‹å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16638v1",
      "published_date": "2025-09-20 11:31:14 UTC",
      "updated_date": "2025-09-20 11:31:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:56:55.671028+00:00"
    },
    {
      "arxiv_id": "2509.16633v1",
      "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs",
      "title_zh": "å¤§æ¨¡å‹é©±åŠ¨çš„å°æ¨¡å‹è®­ç»ƒï¼šé¢å‘å°å‹ VLM é«˜æ•ˆè§†è§‰é—®ç­”çš„æ— æ ‡ç­¾æ¨¡å‹å¯¹ç­‰å¯¹é½",
      "authors": [
        "Abhirama Subramanyam Penamakuri",
        "Navlika Singh",
        "Piyush Arora",
        "Anand Mishra"
      ],
      "abstract": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable performance in various vision and language tasks, including visual question answering (VQA). However, their high computational cost makes them impractical for resource-constrained settings and inference-heavy applications. In contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer from a significant performance gap compared to their larger counterparts. In this work, we introduce the Model Parity Aligner (MPA), a novel framework designed to systematically improve S-VLMs by leveraging unlabeled images and effective knowledge transfer from L-VLMs. Instead of traditional knowledge distillation methods that rely on labeled training data, MPA employs a strategic parity-based approach that precisely identifies the knowledge disparities between S-VLMs and L-VLMs, and optimizes training by targeting only these disparities. We conduct extensive experiments on four diverse VQA benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires specialized reasoning capabilities such as text recognition, chart interpretation, and commonsense and factual understanding. Our results demonstrate that MPA consistently enhances the performance of S-VLMs on all benchmarks, reducing the performance gap while maintaining computational efficiency. We make our code publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Model Parity Aligner (MPA)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é€šè¿‡æ— æ ‡ç­¾å›¾åƒå’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (L-VLMs) çš„çŸ¥è¯†è½¬ç§»æ¥æå‡å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ (S-VLMs) æ€§èƒ½çš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹ L-VLMs è®¡ç®—æˆæœ¬é«˜æ˜‚è€Œ S-VLMs æ€§èƒ½ä¸è¶³çš„ç°çŠ¶ï¼ŒMPA é¿å¼€äº†ä¾èµ–æ ‡æ³¨æ•°æ®çš„ä¼ ç»ŸçŸ¥è¯†è’¸é¦ (Knowledge Distillation) è·¯å¾„ï¼Œè½¬è€Œé‡‡ç”¨ä¸€ç§åŸºäºå¹³ä»· (Parity) çš„ç­–ç•¥ã€‚è¯¥ç­–ç•¥èƒ½ç²¾å‡†è¯†åˆ« S-VLMs ä¸ L-VLMs ä¹‹é—´çš„çŸ¥è¯†å·®è·ï¼Œå¹¶ä»…é’ˆå¯¹è¿™äº›ç‰¹å®šå·®è·è¿›è¡Œè®­ç»ƒä¼˜åŒ–ã€‚ç ”ç©¶åœ¨ TextVQAã€ST-VQAã€ChartQA å’Œ OKVQA ç­‰å¤šä¸ªæ¶‰åŠæ–‡æœ¬è¯†åˆ«ã€å›¾è¡¨è§£è¯»åŠå¸¸è¯†æ¨ç†çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMPA åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—å¢å¼ºäº† S-VLMs åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œæœ‰æ•ˆç¼©å°äº†å…¶ä¸å¤§å‹æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to EMNLP (Main) 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16633v1",
      "published_date": "2025-09-20 11:12:23 UTC",
      "updated_date": "2025-09-20 11:12:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:03.987098+00:00"
    },
    {
      "arxiv_id": "2509.20377v1",
      "title": "SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation",
      "title_zh": "SKILL-RAGï¼šé¢å‘æ£€ç´¢å¢å¼ºç”Ÿæˆçš„è‡ªçŸ¥è¯†å¼•å¯¼å­¦ä¹ ä¸è¿‡æ»¤",
      "authors": [
        "Tomoaki Isoda"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive tasks in recent years. However, since retrieval systems may return irrelevant content, incorporating such information into the model often leads to hallucinations. Thus, identifying and filtering out unhelpful retrieved content is a key challenge for improving RAG performance.To better integrate the internal knowledge of the model with external knowledge from retrieval, it is essential to understand what the model \"knows\" and \"does not know\" (which is also called \"self-knowledge\"). Based on this insight, we propose SKILL-RAG (Self-Knowledge Induced Learning and Filtering for RAG), a novel method that leverages the model's self-knowledge to determine which retrieved documents are beneficial for answering a given query. We design a reinforcement learning-based training framework to explicitly elicit self-knowledge from the model and employs sentence-level granularity to filter out irrelevant content while preserving useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several question answering benchmarks. Experimental results demonstrate that SKILL-RAG not only improves generation quality but also significantly reduces the number of input documents, validating the importance of self-knowledge in guiding the selection of high-quality retrievals.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SKILL-RAG (Self-Knowledge Induced Learning and Filtering for RAG)ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„çŸ¥è¯†è¾¹ç•Œè®¤çŸ¥ï¼ˆSelf-knowledgeï¼‰æ¥ä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆè¿‡ç¨‹ã€‚é’ˆå¯¹ Retrieval-Augmented Generation (RAG) ç³»ç»Ÿä¸­æ£€ç´¢åˆ°çš„æ— å…³å†…å®¹æ˜“å¼•å‘ Hallucination çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¹¶è¿‡æ»¤å¯¹å›ç­”æŸ¥è¯¢æ— åŠ©çš„æ–‡æ¡£ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ä¸ªåŸºäº Reinforcement learning çš„è®­ç»ƒæ¡†æ¶æ¥æ˜¾å¼æ¿€å‘æ¨¡å‹çš„è‡ªæˆ‘è®¤çŸ¥èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨å¥å­çº§ï¼ˆSentence-levelï¼‰ç²’åº¦åœ¨ä¿ç•™æœ‰ç”¨çŸ¥è¯†çš„åŒæ—¶å‰”é™¤æ— å…³å™ªå£°ã€‚åœ¨ Llama2-7B å’Œ Qwen3-8B ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSKILL-RAG ä¸ä»…æ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ï¼Œè¿˜å¤§å¹…å‡å°‘äº†éœ€è¦å¤„ç†çš„è¾“å…¥æ–‡æ¡£æ•°é‡ã€‚è¿™å……åˆ†éªŒè¯äº† Self-knowledge åœ¨å¼•å¯¼é«˜è´¨é‡æ£€ç´¢é€‰æ‹©åŠæå‡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20377v1",
      "published_date": "2025-09-20 11:02:06 UTC",
      "updated_date": "2025-09-20 11:02:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:07.184010+00:00"
    },
    {
      "arxiv_id": "2509.16622v2",
      "title": "Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing",
      "title_zh": "é¢å‘è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸ç ”åˆ¤å¤„ç†çš„éŸ³é¢‘å¼•å¯¼æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Mengqi Wang",
        "Zhan Liu",
        "Zengrui Jin",
        "Guangzhi Sun",
        "Chao Zhang",
        "Philip C. Woodland"
      ],
      "abstract": "Diffusion-based large language models (DLLMs) have recently attracted growing interest as an alternative to autoregressive decoders. In this work, we present an empirical study on using the diffusion-based large language model LLaDA for automatic speech recognition (ASR). We first investigate its use as an external deliberation-based processing module for Whisper-LLaMA transcripts. By leveraging the bidirectional attention and denoising capabilities of LLaDA, we explore random masking, low-confidence masking, and semi-autoregressive strategies, showing that Whisper-LLaDA substantially reduces WER compared with the baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER on test-clean/test-other, representing a 12.3% relative improvement over the Whisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA without acoustic features fails to improve accuracy, highlighting the importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA as a standalone decoder for ASR with diffusion-based and semi-autoregressive decoding. Most experimental configurations achieve faster inference than the Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These findings offer an empirical view of diffusion-based LLMs for ASR and point to promising directions for improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ‰©æ•£çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆDLLMsï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„åº”ç”¨å±•å¼€äº†å®è¯ç ”ç©¶ï¼Œé‡ç‚¹è¯„ä¼°äº† LLaDA æ¨¡å‹åœ¨è¯­éŸ³å¤„ç†ä¸­çš„æ½œåŠ›ã€‚ç ”ç©¶è€…é¦–å…ˆå°† LLaDA ä½œä¸ºå¤–éƒ¨å®¡è®®å¤„ç†æ¨¡å—ï¼ˆdeliberation-based processing moduleï¼‰ç”¨äºä¼˜åŒ– Whisper-LLaMA çš„è½¬å½•è¾“å‡ºï¼Œé€šè¿‡ç»“åˆåŒå‘æ³¨æ„åŠ›å’Œå»å™ªèƒ½åŠ›ï¼Œæ¢ç´¢äº†éšæœºæ©ç ã€ä½ç½®ä¿¡åº¦æ©ç åŠåŠè‡ªå›å½’ç­‰å¤šç§ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨éŸ³é¢‘è°ƒèŠ‚åµŒå…¥ï¼ˆaudio-conditioned embeddingsï¼‰çš„è¾…åŠ©ä¸‹ï¼Œè¯¥çº§è”ç³»ç»Ÿåœ¨ LibriSpeech çš„ test-other ä»»åŠ¡ä¸­ç›¸æ¯”åŸºçº¿æ¨¡å‹å®ç°äº† 12.3% çš„ç›¸å¯¹ WER æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜éªŒè¯äº† Whisper-LLaDA ä½œä¸ºç‹¬ç«‹ ASR è§£ç å™¨çš„æ€§èƒ½ï¼Œå‘ç°å…¶åœ¨æ‰©æ•£å¼å’ŒåŠè‡ªå›å½’è§£ç é…ç½®ä¸‹å…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†éŸ³é¢‘ç‰¹å¾å¯¹æ‰©æ•£æ¨¡å‹çš„é‡è¦æ€§ï¼Œä¹Ÿä¸ºæœªæ¥åŸºäºæ‰©æ•£çš„ LLM åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„è¿›ä¸€æ­¥ä¼˜åŒ–æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16622v2",
      "published_date": "2025-09-20 10:48:06 UTC",
      "updated_date": "2025-10-09 07:55:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:10.260653+00:00"
    },
    {
      "arxiv_id": "2509.16618v1",
      "title": "Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery",
      "title_zh": "Surgical-MambaLLMï¼šé¢å‘æœºå™¨äººæ‰‹æœ¯ VQLA çš„ Mamba2 å¢å¼ºå‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Pengfei Hao",
        "Hongqiu Wang",
        "Shuaibo Li",
        "Zhaohu Xing",
        "Guang Yang",
        "Kaishun Wu",
        "Lei Zhu"
      ],
      "abstract": "In recent years, Visual Question Localized-Answering in robotic surgery (Surgical-VQLA) has gained significant attention for its potential to assist medical students and junior doctors in understanding surgical scenes. Recently, the rapid development of Large Language Models (LLMs) has provided more promising solutions for this task. However, current methods struggle to establish complex dependencies between text and visual details, and have difficulty perceiving the spatial information of surgical scenes. To address these challenges, we propose a novel method, Surgical-MambaLLM, which is the first to combine Mamba2 with LLM in the surgical domain, that leverages Mamba2's ability to effectively capture cross-modal dependencies and perceive spatial information in surgical scenes, thereby enhancing the LLMs' understanding of surgical images. Specifically, we propose the Cross-modal Bidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective multimodal fusion, with its cross-modal integration capabilities. Additionally, tailored to the geometric characteristics of surgical scenes, we design the Surgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the surgical images, enhancing the model's spatial understanding of the surgical scene. Extensive experiments demonstrate that our Surgical-MambaLLM model outperforms the state-of-the-art methods on the EndoVis17-VQLA and EndoVis18-VQLA datasets, significantly improving the performance of the Surgical-VQLA task.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººæ‰‹æœ¯ä¸­çš„è§†è§‰é—®é¢˜å®šä½å›ç­”(Surgical-VQLA)ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å»ºç«‹æ–‡æœ¬ä¸è§†è§‰ç»†èŠ‚é—´çš„å¤æ‚ä¾èµ–å…³ç³»ä»¥åŠæ„ŸçŸ¥æ‰‹æœ¯åœºæ™¯ç©ºé—´ä¿¡æ¯æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Surgical-MambaLLMï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨æ‰‹æœ¯é¢†åŸŸå°†Mamba2ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)ç›¸ç»“åˆçš„æ¶æ„ï¼Œæ—¨åœ¨åˆ©ç”¨Mamba2æ•æ‰è·¨æ¨¡æ€ä¾èµ–å’Œæ„ŸçŸ¥ç©ºé—´ä¿¡æ¯çš„èƒ½åŠ›ã€‚è®ºæ–‡è®¾è®¡äº†è·¨æ¨¡æ€åŒå‘Mamba2é›†æˆ(CBMI)æ¨¡å—ä»¥å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€èåˆï¼Œå¹¶é’ˆå¯¹æ‰‹æœ¯åœºæ™¯çš„å‡ ä½•ç‰¹å¾å¼€å‘äº†æ‰‹æœ¯å™¨æ¢°æ„ŸçŸ¥(SIP)æ‰«ææ¨¡å¼ï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹åœºæ™¯çš„ç©ºé—´ç†è§£ã€‚åœ¨EndoVis17-VQLAå’ŒEndoVis18-VQLAæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å…ˆè¿›æ¶æ„åœ¨æå‡æ‰‹æœ¯æ™ºèƒ½åŒ–ç†è§£æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºè¾…åŠ©ä¸´åºŠæ•™å­¦å’Œæ‰‹æœ¯å†³ç­–æä¾›äº†ç²¾å‡†çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Early accepted by MICCAI2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16618v1",
      "published_date": "2025-09-20 10:42:29 UTC",
      "updated_date": "2025-09-20 10:42:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:12.757318+00:00"
    },
    {
      "arxiv_id": "2509.16617v1",
      "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model",
      "title_zh": "åŸºäºå¾®è°ƒåœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹çš„åŸå¸‚çƒ­å²›æ£€æµ‹ä¸æ¨¡æ‹Ÿ",
      "authors": [
        "David Kreismann"
      ],
      "abstract": "As urbanization and climate change progress, urban heat island effects are becoming more frequent and severe. To formulate effective mitigation plans, cities require detailed air temperature data. However, predictive analytics methods based on conventional machine learning models and limited data infrastructure often provide inaccurate predictions, especially in underserved areas. In this context, geospatial foundation models trained on unstructured global data demonstrate strong generalization and require minimal fine-tuning, offering an alternative for predictions where traditional approaches are limited. This study fine-tunes a geospatial foundation model to predict urban land surface temperatures under future climate scenarios and explores its response to land cover changes using simulated vegetation strategies. The fine-tuned model achieved pixel-wise downscaling errors below 1.74 Â°C and aligned with ground truth patterns, demonstrating an extrapolation capacity up to 3.62 Â°C.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹åŸå¸‚çƒ­å²›æ•ˆåº”(Urban Heat Island)é¢„æµ‹ä¸­ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ç²¾åº¦å—é™çš„é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨å¾®è°ƒåœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹(Geospatial Foundation Model)è¿›è¡Œåœ°è¡¨æ¸©åº¦é¢„æµ‹ä¸æ¨¡æ‹Ÿã€‚è¯¥æ¨¡å‹å‡­å€Ÿåœ¨å…¨çƒéç»“æ„åŒ–æ•°æ®ä¸Šè®­ç»ƒçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æœªæ¥æ°”å€™æƒ…æ™¯ä¸‹çš„åŸå¸‚åœ°è¡¨æ¸©åº¦ã€‚ç ”ç©¶é€šè¿‡æ¨¡æ‹Ÿæ¤è¢«ç­–ç•¥è¿›ä¸€æ­¥æ¢ç´¢äº†æ¨¡å‹å¯¹åœ°è¡¨è¦†ç›–å˜åŒ–çš„å“åº”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨åƒç´ çº§é™å°ºåº¦ä¸­çš„è¯¯å·®ä½äº1.74 Â°Cï¼Œä¸”ä¸åœ°é¢å®å†µ(Ground Truth)æ¨¡å¼ä¿æŒé«˜åº¦ä¸€è‡´ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºé«˜è¾¾3.62 Â°Cçš„å¤–æ¨èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨æ•°æ®åŸºç¡€è®¾æ–½ä¸è¶³åœ°åŒºä»¥åŠåˆ¶å®šåŸå¸‚æ°”å€™ç¼“è§£è®¡åˆ’ä¸­çš„é‡è¦åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 4 figures, to appear in GI LNI (SKILL 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.16617v1",
      "published_date": "2025-09-20 10:41:33 UTC",
      "updated_date": "2025-09-20 10:41:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:13.570681+00:00"
    },
    {
      "arxiv_id": "2509.16602v2",
      "title": "FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection",
      "title_zh": "FakeChainï¼šæ­ç¤ºå¤šæ­¥æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„æµ…å±‚çº¿ç´¢",
      "authors": [
        "Minji Heo",
        "Simon S. Woo"
      ],
      "abstract": "Multi-step or hybrid deepfakes, created by sequentially applying different deepfake creation methods such as Face-Swapping, GAN-based generation, and Diffusion methods, can pose an emerging and unforseen technical challenge for detection models trained on single-step forgeries. While prior studies have mainly focused on detecting isolated single manipulation, little is known about the detection model behavior under such compositional, hybrid, and complex manipulation pipelines. In this work, we introduce \\textbf{FakeChain}, a large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using five state-of-the-art representative generators. Using this approach, we analyze detection performance and spectral properties across hybrid manipulation at different step, along with varying generator combinations and quality settings. Surprisingly, our findings reveal that detection performance highly depends on the final manipulation type, with F1-score dropping by up to \\textbf{58.83\\%} when it differs from training distribution. This clearly demonstrates that detectors rely on last-stage artifacts rather than cumulative manipulation traces, limiting generalization. Such findings highlight the need for detection models to explicitly consider manipulation history and sequences. Our results highlight the importance of benchmarks such as FakeChain, reflecting growing synthesis complexity and diversity in real-world scenarios. Our sample code is available here\\footnote{https://github.com/minjihh/FakeChain}.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”±Face-Swappingã€GAN-based generationå’ŒDiffusionç­‰æ–¹æ³•é¡ºåºç”Ÿæˆçš„Multi-step Deepfakeå¯¹å•æ­¥æ£€æµ‹æ¨¡å‹å¸¦æ¥çš„æ–°æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†FakeChainï¼Œä¸€ä¸ªåŒ…å«1è‡³3æ­¥ä¼ªé€ æ ·æœ¬çš„å¤§è§„æ¨¡Benchmarkï¼Œç”¨äºç³»ç»Ÿåˆ†ææ£€æµ‹æ¨¡å‹åœ¨æ··åˆæ“çºµä¸‹çš„æ€§èƒ½å’Œå…‰è°±ç‰¹æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæ£€æµ‹å™¨çš„æ€§èƒ½é«˜åº¦ä¾èµ–äºæœ€åé˜¶æ®µçš„æ“ä½œç±»å‹ï¼ˆlast-stage artifactsï¼‰ï¼Œè€Œéç´¯ç§¯çš„æ“çºµç—•è¿¹ï¼Œè¿™å¯¼è‡´å½“æœ€ç»ˆæ“ä½œä¸è®­ç»ƒåˆ†å¸ƒä¸ç¬¦æ—¶ï¼ŒF1-scoreæœ€é«˜ä¸‹é™è¾¾58.83%ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†ç°æœ‰æ£€æµ‹å™¨åœ¨å¤„ç†å¤æ‚åˆæˆæµæ°´çº¿æ—¶ç”±äºè¿‡åº¦ä¾èµ–è¡¨å±‚çº¿ç´¢è€Œå­˜åœ¨çš„æ³›åŒ–ç“¶é¢ˆã€‚å®éªŒç»“æœè¿›ä¸€æ­¥å¼ºè°ƒäº†æ£€æµ‹æ¨¡å‹æ˜¾å¼è€ƒè™‘æ“ä½œå†å²å’Œåºåˆ—ï¼ˆmanipulation history and sequencesï¼‰çš„é‡è¦æ€§ï¼Œä¸ºåº”å¯¹æ—¥ç›Šå¤æ‚çš„çœŸå®ä¸–ç•Œä¼ªé€ åœºæ™¯æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16602v2",
      "published_date": "2025-09-20 09:53:50 UTC",
      "updated_date": "2025-09-30 21:02:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:20.964949+00:00"
    },
    {
      "arxiv_id": "2509.16598v2",
      "title": "PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality",
      "title_zh": "PruneCDï¼šé€šè¿‡å¯¹æ¯”å‰ªæè‡ªæ¨¡å‹æå‡è§£ç äº‹å®æ€§",
      "authors": [
        "Byeongho Yu",
        "Changhun Lee",
        "Jungyu Jin",
        "Eunhyeok Park"
      ],
      "abstract": "To mitigate the hallucination problem in large language models, DoLa exploits early exit logits from the same model as a contrastive prior. However, we found that these early exit logits tend to be flat, low in magnitude, and fail to reflect meaningful contrasts. To address this, we propose PruneCD, a novel contrastive decoding method that constructs the amateur model via layer pruning rather than early exit. This design leads to more informative and well-aligned logits, enabling more effective contrastive decoding. Through qualitative and quantitative analyses, we demonstrate that PruneCD consistently improves factuality with minimal inference overhead, offering a robust and practical approach to mitigating hallucinations in LLMs.",
      "tldr_zh": "ä¸ºäº†ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„å¹»è§‰é—®é¢˜ï¼Œç°æœ‰çš„ DoLa æ–¹æ³•åˆ©ç”¨åŒä¸€æ¨¡å‹çš„æ—©æœŸé€€å‡º (early exit) Logits ä½œä¸ºå¯¹æ¯”å…ˆéªŒï¼Œä½†ç ”ç©¶å‘ç°è¿™äº› Logits å¾€å¾€å› åˆ†å¸ƒå¹³å¦ä¸”å¹…åº¦è¾ƒä½è€Œéš¾ä»¥æä¾›æœ‰æ•ˆçš„å¯¹æ¯”ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº† PruneCDï¼Œä¸€ç§é€šè¿‡å±‚å‰ªæ (layer pruning) è€Œéæ—©æœŸé€€å‡ºæ¥æ„å»ºä¸šä½™æ¨¡å‹ (amateur model) çš„æ–°å‹å¯¹æ¯”è§£ç  (contrastive decoding) æ–¹æ³•ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿäº§ç”Ÿæ›´å…·ä¿¡æ¯é‡ä¸”å¯¹é½æ›´å¥½çš„ Logitsï¼Œä»è€Œæ˜¾è‘—å¢å¼ºå¯¹æ¯”è§£ç çš„æ•ˆæœã€‚å®šæ€§å’Œå®šé‡å®éªŒè¯æ˜ï¼ŒPruneCD åœ¨ä»…äº§ç”Ÿæå°æ¨ç†å¼€é”€çš„å‰æä¸‹ï¼ŒæŒç»­æå‡äº†æ¨¡å‹ç”Ÿæˆçš„çœŸå®æ€§ (factuality)ï¼Œä¸ºè§£å†³ LLMs çš„å¹»è§‰é—®é¢˜æä¾›äº†ä¸€ç§ç¨³å¥ä¸”å®ç”¨çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted at EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2509.16598v2",
      "published_date": "2025-09-20 09:47:34 UTC",
      "updated_date": "2025-09-23 07:28:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:21.665619+00:00"
    },
    {
      "arxiv_id": "2509.16596v1",
      "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels",
      "title_zh": "ä»è¯å…ƒä¸å‚æ•°å±‚é¢åˆ†ææœ‰ç›‘ç£å¾®è°ƒå¯¹æ¨¡å‹çŸ¥è¯†çš„å½±å“",
      "authors": [
        "Junjie Ye",
        "Yuming Yang",
        "Yang Nan",
        "Shuo Li",
        "Qi Zhang",
        "Tao Gui",
        "Xuanjing Huang",
        "Peng Wang",
        "Zhongchao Shi",
        "Jianping Fan"
      ],
      "abstract": "Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœ‰ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning, SFT)å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)çŸ¥è¯†æŒæ¡ç¨‹åº¦çš„å½±å“ï¼Œæ—¨åœ¨è§£å†³å¾®è°ƒè¿‡ç¨‹ä¸­çŸ¥è¯†å˜åŒ–è¡Œä¸ºéš¾ä»¥æ§åˆ¶çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜åŸºäº LLaMA-2 å’Œ LLaMA-3 ç³»åˆ—çš„äº”ç§æ¨¡å‹ï¼Œé€šè¿‡é—­å·é—®ç­”(Closed-book Question Answering, CBQA)ä»»åŠ¡è¯„ä¼°äº† SFT åçš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒå‘ç°ï¼Œåœ¨ 1,920 ä¸ªæ ·æœ¬ä¸Šå¾®è°ƒçš„æ¨¡å‹æ€§èƒ½åè€Œæ¯”ä»…åœ¨ 240 ä¸ªæ ·æœ¬ä¸Šå¾®è°ƒçš„æ¨¡å‹å·® 14%ï¼Œè¡¨æ˜ä¸å½“çš„å¾®è°ƒæ•°æ®é‡å¯èƒ½å‰Šå¼±æ¨¡å‹åŸæœ‰çŸ¥è¯†ã€‚åŒæ—¶ï¼Œå¾®è°ƒæ•°æ®ä¸­çŸ¥è¯†æŒæ¡æ°´å¹³çš„å·®å¼‚ä¼šå¯¼è‡´è¶…è¿‡ 12% çš„æ€§èƒ½æ³¢åŠ¨ã€‚ç ”ç©¶è¿›ä¸€æ­¥ä» Token å’Œ Parameter å±‚é¢è¿›è¡Œåˆ†æï¼Œæ­ç¤ºäº† SFT æœŸé—´é«˜è¾¾ 90% çš„å‚æ•°æ›´æ–°å¹¶æœªå¯¹çŸ¥è¯†å¢å¼ºåšå‡ºè´¡çŒ®ã€‚é€šè¿‡æ¢å¤è¿™äº›ç‰¹å®šæ›´æ–°ï¼Œæ¨¡å‹åœ¨ CBQA ä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—ä»¥æå‡ï¼Œè¯¥å‘ç°ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„æ¨¡å‹çŸ¥è¯†å¢å¼ºå¾®è°ƒç­–ç•¥æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by EMNLP 2025 Main Conference. arXiv admin note: text overlap with arXiv:2409.15825",
      "pdf_url": "https://arxiv.org/pdf/2509.16596v1",
      "published_date": "2025-09-20 09:40:32 UTC",
      "updated_date": "2025-09-20 09:40:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:22.963133+00:00"
    },
    {
      "arxiv_id": "2509.16590v1",
      "title": "Question Answering with LLMs and Learning from Answer Sets",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸ç­”æ¡ˆé›†å­¦ä¹ çš„é—®ç­”",
      "authors": [
        "Manuel Borroto",
        "Katie Gallagher",
        "Antonio Ielo",
        "Irfan Kareem",
        "Francesco Ricca",
        "Alessandra Russo"
      ],
      "abstract": "Large Language Models (LLMs) excel at understanding natural language but struggle with explicit commonsense reasoning. A recent trend of research suggests that the combination of LLM with robust symbolic reasoning systems can overcome this problem on story-based question answering tasks. In this setting, existing approaches typically depend on human expertise to manually craft the symbolic component. We argue, however, that this component can also be automatically learned from examples. In this work, we introduce LLM2LAS, a hybrid system that effectively combines the natural language understanding capabilities of LLMs, the rule induction power of the Learning from Answer Sets (LAS) system ILASP, and the formal reasoning strengths of Answer Set Programming (ASP). LLMs are used to extract semantic structures from text, which ILASP then transforms into interpretable logic rules. These rules allow an ASP solver to perform precise and consistent reasoning, enabling correct answers to previously unseen questions. Empirical results outline the strengths and weaknesses of our automatic approach for learning and reasoning in a story-based question answering benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ˜¾å¼å¸¸è¯†æ¨ç† (commonsense reasoning) æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº† LLM2LAS æ··åˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæœ‰æ•ˆç»“åˆäº† LLMs çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€Learning from Answer Sets (LAS) ç³»ç»Ÿ ILASP çš„è§„åˆ™å½’çº³èƒ½åŠ›ä»¥åŠ Answer Set Programming (ASP) çš„å½¢å¼åŒ–æ¨ç†ä¼˜åŠ¿ã€‚LLM2LAS é¦–å…ˆåˆ©ç”¨ LLMs ä»æ–‡æœ¬ä¸­æå–è¯­ä¹‰ç»“æ„ï¼Œå†é€šè¿‡ ILASP å°†å…¶è‡ªåŠ¨è½¬åŒ–ä¸ºå¯è§£é‡Šçš„é€»è¾‘è§„åˆ™ã€‚è¿™äº›è§„åˆ™ä½¿ ASP æ±‚è§£å™¨èƒ½å¤Ÿæ‰§è¡Œç²¾ç¡®ä¸”ä¸€è‡´çš„é€»è¾‘æ¨ç†ï¼Œä»è€Œå¯¹ä¹‹å‰æœªè§è¿‡çš„é—®é¢˜åšå‡ºæ­£ç¡®å›ç­”ã€‚å®éªŒç»“æœåˆ†æäº†è¿™ç§è‡ªåŠ¨åŒ–å­¦ä¹ ä¸æ¨ç†æ–¹æ³•åœ¨æ•…äº‹ç±»é—®ç­”åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹æ„å»ºå¯è§£é‡Šæ¨ç†æ¨¡å‹çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Under consideration for TPLP journal",
      "pdf_url": "https://arxiv.org/pdf/2509.16590v1",
      "published_date": "2025-09-20 09:26:44 UTC",
      "updated_date": "2025-09-20 09:26:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:27.585296+00:00"
    },
    {
      "arxiv_id": "2509.16589v2",
      "title": "Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data",
      "title_zh": "Speech-LLMs çš„è¯­å¢ƒä¸å‰¯è¯­è¨€æ¨ç†åŸºå‡†æµ‹è¯•ï¼šä¸€é¡¹åŸºäºçœŸå®åœºæ™¯æ•°æ®çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Qiongqiong Wang",
        "Hardik Bhupendra Sailor",
        "Tianchi Liu",
        "Wenyu Zhang",
        "Muhammad Huzaifah",
        "Nattadaporn Lertcheva",
        "Shuo Sun",
        "Nancy F. Chen",
        "Jinyang Wu",
        "AiTi Aw"
      ],
      "abstract": "Recent speech-LLMs have shown impressive performance in tasks like transcription and translation, yet they remain limited in understanding the paralinguistic aspects of speech crucial for social and emotional intelligence. We propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual paralinguistic reasoning the integration of verbal content with non-verbal cues like emotion and prosody. The benchmark includes two curated question answering (QA) datasets requiring both linguistic and empathetic understanding. We evaluate state-of-the-art speech-LLMs from both open and closed-source models and perform a comprehensive analysis across different question types. The top two models were further analyzed under temperature tuning to understand its effect on this task. Our benchmark reveals a key gap in existing evaluations and offers insights into building more context-aware and emotionally intelligent speech-capable LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CP-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ (Speech-LLMs) åœ¨ä¸Šä¸‹æ–‡å‰¯è¯­è¨€æ¨ç† (Contextual Paralinguistic Reasoning) èƒ½åŠ›çš„æ–°å‹åŸºå‡†ã€‚å°½ç®¡å½“å‰çš„ Speech-LLMs åœ¨è½¬å½•å’Œç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç†è§£ç¤¾äº¤å’Œæƒ…æ„Ÿæ™ºèƒ½è‡³å…³é‡è¦çš„å‰¯è¯­è¨€æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚CP-Bench é€šè¿‡æ•´åˆè¨€è¯­å†…å®¹ä¸æƒ…ç»ªã€éŸµå¾‹ (Prosody) ç­‰éè¨€è¯­çº¿ç´¢ï¼Œè¦æ±‚æ¨¡å‹åŒæ—¶å…·å¤‡è¯­è¨€ç†è§£å’ŒåŒç†å¿ƒç†è§£èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«ä¸¤ä¸ªç²¾å¿ƒç­–åˆ’çš„é—®ç­” (QA) æ•°æ®é›†ï¼Œå¹¶å¯¹å¤šç§å¼€æºå’Œé—­æºçš„å‰æ²¿ Speech-LLMs è¿›è¡Œäº†ç»¼åˆè¯„ä¼°ä¸è·¨é¢˜å‹åˆ†æã€‚ç ”ç©¶è€…è¿˜é’ˆå¯¹è¡¨ç°æœ€å¥½çš„ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œäº†æ¸©åº¦è°ƒèŠ‚ (Temperature Tuning) åˆ†æï¼Œä»¥æ¢ç©¶å…¶å¯¹è¯¥ä»»åŠ¡çš„å½±å“ã€‚å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰è¯„ä¼°ä½“ç³»ä¸­çš„å…³é”®ç©ºç™½ï¼Œä¸ºæ„å»ºæ›´å…·ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›å’Œæƒ…æ„Ÿæ™ºèƒ½çš„ Speech-LLMs æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in EMNLP Findings 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16589v2",
      "published_date": "2025-09-20 09:26:40 UTC",
      "updated_date": "2025-09-24 05:32:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:39.659492+00:00"
    },
    {
      "arxiv_id": "2509.16588v1",
      "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving",
      "title_zh": "SQSï¼šé€šè¿‡åŸºäºæŸ¥è¯¢çš„æ³¼æº…æå‡è‡ªåŠ¨é©¾é©¶ç¨€ç–æ„ŸçŸ¥æ¨¡å‹",
      "authors": [
        "Haiming Zhang",
        "Yiyao Zhu",
        "Wending Zhou",
        "Xu Yan",
        "Yingjie Cai",
        "Bingbing Liu",
        "Shuguang Cui",
        "Zhen Li"
      ],
      "abstract": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes explicit dense BEV or volumetric construction, enabling highly efficient computation and accelerated inference. In this paper, we introduce SQS, a novel query-based splatting pre-training specifically designed to advance SPMs in autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian representations from sparse queries during pre-training, leveraging self-supervised splatting to learn fine-grained contextual features through the reconstruction of multi-view images and depth maps. During fine-tuning, the pre-trained Gaussian queries are seamlessly integrated into downstream networks via query interaction mechanisms that explicitly connect pre-trained queries with task-specific queries, effectively accommodating the diverse requirements of occupancy prediction and 3D object detection. Extensive experiments on autonomous driving benchmarks demonstrate that SQS delivers considerable performance gains across multiple query-based 3D perception tasks, notably in occupancy prediction and 3D object detection, outperforming prior state-of-the-art pre-training approaches by a significant margin (i.e., +1.3 mIoU on occupancy prediction and +1.0 NDS on 3D detection).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SQSï¼Œä¸€ç§ä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„åŸºäºæŸ¥è¯¢çš„é«˜æ–¯æŠ•å½±(query-based splatting)é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºç¨€ç–æ„ŸçŸ¥æ¨¡å‹(Sparse Perception Models)çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚SQS å¼•å…¥äº†ä¸€ä¸ªæ’ä»¶å¼æ¨¡å—ï¼Œåœ¨é¢„è®­ç»ƒæœŸé—´ä»ç¨€ç–æŸ¥è¯¢ä¸­é¢„æµ‹ 3D Gaussian è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨è‡ªç›‘ç£æŠ•å½±æŠ€æœ¯é‡å»ºå¤šè§†è§’å›¾åƒå’Œæ·±åº¦å›¾ï¼Œä»è€Œå­¦ä¹ æ›´ç»†ç²’åº¦çš„ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œé¢„è®­ç»ƒçš„é«˜æ–¯æŸ¥è¯¢é€šè¿‡æŸ¥è¯¢äº¤äº’æœºåˆ¶ä¸ä¸‹æ¸¸ä»»åŠ¡ç‰¹å®šæŸ¥è¯¢æ˜¾å¼è¿æ¥ï¼Œçµæ´»é€‚é…å ç”¨é¢„æµ‹(occupancy prediction)å’Œ 3D ç›®æ ‡æ£€æµ‹(3D object detection)çš„éœ€æ±‚ã€‚å®éªŒè¯æ˜ï¼ŒSQS åœ¨å¤šé¡¹æŸ¥è¯¢é©±åŠ¨çš„ 3D æ„ŸçŸ¥ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚ç›¸æ¯”æ­¤å‰æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ–¹æ³•ï¼ŒSQS åœ¨å ç”¨é¢„æµ‹ä¸­æé«˜äº† 1.3 mIoUï¼Œåœ¨ 3D æ£€æµ‹ä¸­æé«˜äº† 1.0 NDSã€‚è¯¥æ¡†æ¶æœ‰æ•ˆè§£å†³äº†ç¨€ç–æ„ŸçŸ¥æ¨¡å‹åœ¨ç¼ºä¹å¯†é›†è¡¨å¾æ„å»ºæ—¶çš„ç‰¹å¾æå–éš¾é¢˜ï¼Œä¸ºé«˜æ•ˆä¸”ç²¾å‡†çš„è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025 (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2509.16588v1",
      "published_date": "2025-09-20 09:25:19 UTC",
      "updated_date": "2025-09-20 09:25:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:56.390643+00:00"
    },
    {
      "arxiv_id": "2509.16584v1",
      "title": "From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations",
      "title_zh": "ä»è¯„åˆ†åˆ°æ­¥éª¤ï¼šå¾ªè¯åŒ»å­¦è®¡ç®—ä¸­å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„è¯Šæ–­ä¸æå‡",
      "authors": [
        "Benlu Wang",
        "Iris Xia",
        "Yifan Zhang",
        "Junda Wang",
        "Feiyun Ouyang",
        "Shuo Han",
        "Arman Cohan",
        "Hong Yu",
        "Zonghai Yao"
      ],
      "abstract": "Large language models (LLMs) have demonstrated promising performance on medical benchmarks; however, their ability to perform medical calculations, a crucial aspect of clinical decision-making, remains underexplored and poorly evaluated. Existing benchmarks often assess only the final answer with a wide numerical tolerance, overlooking systematic reasoning failures and potentially causing serious clinical misjudgments. In this work, we revisit medical calculation evaluation with a stronger focus on clinical trustworthiness. First, we clean and restructure the MedCalc-Bench dataset and propose a new step-by-step evaluation pipeline that independently assesses formula selection, entity extraction, and arithmetic computation. Under this granular framework, the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by prior evaluations. Second, we introduce an automatic error analysis framework that generates structured attribution for each failure mode. Human evaluation confirms its alignment with expert judgment, enabling scalable and explainable diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that combines retrieval-augmented generation and Python-based code execution. Without any fine-tuning, MedRaC improves the accuracy of different LLMs from 16.35% up to 53.19%. Our work highlights the limitations of current benchmark practices and proposes a more clinically faithful methodology. By enabling transparent and transferable reasoning evaluation, we move closer to making LLM-based systems trustworthy for real-world medical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŒ»å­¦è®¡ç®—ä»»åŠ¡ä¸­å› ç°æœ‰è¯„ä¼°æŒ‡æ ‡è¿‡å®½è€Œæ©ç›–æ¨ç†å¤±è´¥çš„é—®é¢˜ï¼Œé‡æ–°å®¡è§†äº†ä¸´åºŠè®¡ç®—çš„å¯ä¿¡åº¦è¯„ä»·ã€‚ä½œè€…é€šè¿‡æ¸…ç†å¹¶é‡æ„MedCalc-Benchæ•°æ®é›†ï¼Œæå‡ºäº†ä¸€ç§åˆ†æ­¥è¯„ä¼°æµç¨‹ï¼Œåˆ†åˆ«å¯¹å…¬å¼é€‰æ‹©(Formula Selection)ã€å®ä½“æå–(Entity Extraction)å’Œç®—æœ¯è®¡ç®—(Arithmetic Computation)è¿›è¡Œç‹¬ç«‹è¡¡é‡ã€‚å®éªŒå‘ç°ï¼Œåœ¨ç»†ç²’åº¦æ¡†æ¶ä¸‹GPT-4oçš„å‡†ç¡®ç‡ä»62.7%å¤§å¹…é™è‡³43.6%ï¼Œæš´éœ²äº†éšè”½çš„é€»è¾‘é”™è¯¯ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å¼•å…¥äº†å¯å®ç°è‡ªåŠ¨åŒ–å½’å› çš„é”™è¯¯åˆ†ææ¡†æ¶ï¼Œå¹¶å¼€å‘äº†åä¸ºMedRaCçš„æ¨¡å—åŒ–æ™ºèƒ½ä½“æµæ°´çº¿ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸Pythonä»£ç æ‰§è¡Œï¼Œåœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹å°†å„æ¨¡å‹çš„å‡†ç¡®ç‡ä»16.35%æœ€é«˜æå‡è‡³53.19%ã€‚è¯¥å·¥ä½œçªæ˜¾äº†å½“å‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼Œä¸ºæ„å»ºä¸´åºŠå¯ä¿¡ä¸”æ¨ç†é€æ˜çš„åŒ»å­¦AIç³»ç»Ÿæä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Equal contribution for the first two authors. To appear as an Oral presentation in the proceedings of the Main Conference on Empirical Methods in Natural Language Processing (EMNLP) 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16584v1",
      "published_date": "2025-09-20 09:10:26 UTC",
      "updated_date": "2025-09-20 09:10:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:50.084827+00:00"
    },
    {
      "arxiv_id": "2509.16578v1",
      "title": "Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning",
      "title_zh": "åŸºäºå±‚çº§åŒ–æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬äººç±»ç§»åŠ¨æ€§é¢„æµ‹",
      "authors": [
        "Wenyao Li",
        "Ran Zhang",
        "Pengyang Wang",
        "Yuanchun Zhou",
        "Pengfei Wang"
      ],
      "abstract": "Human mobility forecasting is important for applications such as transportation planning, urban management, and personalized recommendations. However, existing methods often fail to generalize to unseen users or locations and struggle to capture dynamic intent due to limited labeled data and the complexity of mobility patterns. We propose ZHMF, a framework for zero-shot human mobility forecasting that combines a semantic enhanced retrieval and reflection mechanism with a hierarchical language model based reasoning system. The task is reformulated as a natural language question answering paradigm. Leveraging LLMs semantic understanding of user histories and context, our approach handles previously unseen prediction scenarios. We further introduce a hierarchical reflection mechanism for iterative reasoning and refinement by decomposing forecasting into an activity level planner and a location level selector, enabling collaborative modeling of long term user intentions and short term contextual preferences. Experiments on standard human mobility datasets show that our approach outperforms existing models. Ablation studies reveal the contribution of each module, and case studies illustrate how the method captures user intentions and adapts to diverse contextual scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ZHMFæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå±‚çº§åŒ–æ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹(LLM)é›¶æ ·æœ¬äººç±»ç§»åŠ¨é¢„æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœªè§ç”¨æˆ·æˆ–åœ°ç‚¹æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³ä»¥åŠéš¾ä»¥æ•æ‰åŠ¨æ€æ„å›¾çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†ç§»åŠ¨é¢„æµ‹ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºè‡ªç„¶è¯­è¨€é—®ç­”èŒƒå¼ï¼Œåˆ©ç”¨LLMå¯¹ç”¨æˆ·å†å²å’ŒèƒŒæ™¯çš„è¯­ä¹‰ç†è§£èƒ½åŠ›æ¥åº”å¯¹é¢„æµ‹åœºæ™¯çš„å¤æ‚æ€§ã€‚ZHMFæ ¸å¿ƒå¼•å…¥äº†è¯­ä¹‰å¢å¼ºçš„æ£€ç´¢ä¸åæ€æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨å±‚çº§åŒ–æ¨ç†ç³»ç»Ÿå°†é¢„æµ‹åˆ†è§£ä¸ºæ´»åŠ¨çº§è§„åˆ’å™¨(activity level planner)å’Œåœ°ç‚¹çº§é€‰æ‹©å™¨(location level selector)ã€‚è¿™ç§è®¾è®¡å®ç°äº†å¯¹ç”¨æˆ·é•¿æœŸæ„å›¾ä¸çŸ­æœŸä¸Šä¸‹æ–‡åå¥½çš„ååŒå»ºæ¨¡ï¼Œæ”¯æŒè¿­ä»£æ¨ç†ä¸ç»“æœç»†åŒ–ã€‚åœ¨æ ‡å‡†äººç±»ç§»åŠ¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒZHMFçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ•æ‰ç”¨æˆ·å¤æ‚æ„å›¾ä»¥åŠé€‚åº”å¤šæ ·åŒ–ä¸Šä¸‹æ–‡åœºæ™¯æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16578v1",
      "published_date": "2025-09-20 08:46:38 UTC",
      "updated_date": "2025-09-20 08:46:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:54.092438+00:00"
    },
    {
      "arxiv_id": "2509.16567v2",
      "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits",
      "title_zh": "V-CECEï¼šåŸºäºæ¦‚å¿µç¼–è¾‘çš„è§†è§‰åäº‹å®è§£é‡Š",
      "authors": [
        "Nikolaos Spanos",
        "Maria Lymperaiou",
        "Giorgos Filandrianos",
        "Konstantinos Thomas",
        "Athanasios Voulodimos",
        "Giorgos Stamou"
      ],
      "abstract": "Recent black-box counterfactual generation frameworks fail to take into account the semantic content of the proposed edits, while relying heavily on training to guide the generation process. We propose a novel, plug-and-play black-box counterfactual generation framework, which suggests step-by-step edits based on theoretical guarantees of optimal edits to produce human-level counterfactual explanations with zero training. Our framework utilizes a pre-trained image editing diffusion model, and operates without access to the internals of the classifier, leading to an explainable counterfactual generation process. Throughout our experimentation, we showcase the explanatory gap between human reasoning and neural model behavior by utilizing both Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision Language Model (LVLM) classifiers, substantiated through a comprehensive human evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†V-CECEï¼Œä¸€ç§é€šè¿‡æ¦‚å¿µç¼–è¾‘(Conceptual Edits)å®ç°è§†è§‰åäº‹å®è§£é‡Š(Visual Counterfactual Explanations)çš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰é»‘ç›’åäº‹å®ç”Ÿæˆæ–¹æ³•å¿½è§†è¯­ä¹‰å†…å®¹ä¸”è¿‡åº¦ä¾èµ–è®­ç»ƒçš„é—®é¢˜ï¼ŒV-CECEé‡‡ç”¨å³æ’å³ç”¨(plug-and-play)çš„è®¾è®¡ï¼Œå®ç°äº†é›¶è®­ç»ƒ(zero training)ç”Ÿæˆäººç±»æ°´å¹³çš„åäº‹å®è§£é‡Šã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹(diffusion model)ï¼ŒåŸºäºæœ€ä¼˜ç¼–è¾‘çš„ç†è®ºä¿è¯æä¾›é€æ­¥ç¼–è¾‘å»ºè®®ï¼Œä¸”æ— éœ€è®¿é—®åˆ†ç±»å™¨çš„å†…éƒ¨ç»“æ„ã€‚é€šè¿‡å¯¹å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€è§†è§‰Transformer(ViT)å’Œå¤§è¯­è¨€è§†è§‰æ¨¡å‹(LVLM)åˆ†ç±»å™¨çš„å®éªŒï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†äººç±»æ¨ç†ä¸ç¥ç»æ¨¡å‹è¡Œä¸ºä¹‹é—´çš„è§£é‡Šæ€§å·®è·ã€‚æœ€åï¼Œç»“åˆå…¨é¢çš„å¤šç»´åº¦äººå·¥è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹å¯è§£é‡Šæ€§æ–¹é¢çš„æ˜¾è‘—æ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16567v2",
      "published_date": "2025-09-20 07:53:06 UTC",
      "updated_date": "2025-12-05 07:24:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:57:59.992431+00:00"
    },
    {
      "arxiv_id": "2509.16561v1",
      "title": "SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning",
      "title_zh": "SalaMAnderï¼šåŸºäº Shapley çš„é“¾å¼æ€ç»´æ¨ç†æ•°å­¦è¡¨è¾¾å¼å½’å› ä¸åº¦é‡æŒ‡æ ‡",
      "authors": [
        "Yue Xin",
        "Chen Shen",
        "Shaotian Yan",
        "Xiaosong Yuan",
        "Yaoming Wang",
        "Xiaofeng Zhang",
        "Chenxi Huang",
        "Jieping Ye"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting enhances the math reasoning capability of large language models (LLMs) to a large margin. However, the mechanism underlying such improvements remains unexplored. In this paper, we present \\textbf{SalaMAnder} (\\textbf{S}h\\textbf{a}p\\textbf{l}ey-b\\textbf{a}sed \\textbf{M}athematical Expression \\textbf{A}ttribution a\\textbf{nd} M\\textbf{e}t\\textbf{r}ic), a theoretically grounded methodology as well as a mathematically rigorous evaluation metric for quantifying component-level contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley value for mathematical expression attribution and develop an efficient stratified sampling algorithm that significantly reduces the computational complexity. Besides, we develop the \\textbf{CoSP} (\\textbf{C}ardinality \\textbf{o}f \\textbf{S}hapley \\textbf{P}ositives) metric through covariance analysis. Comprehensive validation across popular LLM models and diverse mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder framework exhibits a robust monotonic correlation with model performance, not only providing theoretical explanations for the empirical success of existing few-shot CoT but also establishing mathematically rigorous principles for prompt construction optimization. Furthermore, we verify the reliability of the explanation, based on which we unify the insights of previous work.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SalaMAnderæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºShapleyå€¼çš„æ•°å­¦è¡¨è¾¾å¼å½’å› æ–¹æ³•ï¼Œæ—¨åœ¨é‡åŒ–å°‘æ ·æœ¬(few-shot)é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†ä¸­å„ç»„ä»¶çš„è´¡çŒ®ã€‚ä¸ºäº†è§£å†³è®¡ç®—å¤æ‚æ€§é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„åˆ†å±‚é‡‡æ ·ç®—æ³•ï¼Œå¹¶åŸºäºåæ–¹å·®åˆ†ææå‡ºäº†åä¸ºCoSP (Cardinality of Shapley Positives)çš„æ•°å­¦ä¸¥è°¨è¯„ä¼°æŒ‡æ ‡ã€‚åœ¨å¤šç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œæ•°å­¦åŸºå‡†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒCoSPæŒ‡æ ‡ä¸æ¨¡å‹æ€§èƒ½å…·æœ‰å¼ºå¥çš„å•è°ƒç›¸å…³æ€§ã€‚SalaMAnderæ¡†æ¶ä¸ä»…ä¸ºç°æœ‰few-shot CoTçš„å®è¯æˆåŠŸæä¾›äº†ç†è®ºè§£é‡Šï¼Œè¿˜ä¸ºæç¤ºè¯æ„é€ (Prompt Construction)çš„ä¼˜åŒ–å»ºç«‹äº†æ•°å­¦å‡†åˆ™ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶éªŒè¯äº†å½’å› è§£é‡Šçš„å¯é æ€§ï¼Œå¹¶ç»Ÿä¸€äº†å…ˆå‰ç›¸å…³å·¥ä½œçš„è§è§£ï¼Œä¸ºç†è§£LLMæ¨ç†æœºåˆ¶æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "accpeted by EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16561v1",
      "published_date": "2025-09-20 07:38:58 UTC",
      "updated_date": "2025-09-20 07:38:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:58:01.795608+00:00"
    },
    {
      "arxiv_id": "2509.21357v1",
      "title": "A Novel Differential Feature Learning for Effective Hallucination Detection and Classification",
      "title_zh": "ä¸€ç§ç”¨äºé«˜æ•ˆå¹»è§‰æ£€æµ‹ä¸åˆ†ç±»çš„æ–°å‹å·®å¼‚ç‰¹å¾å­¦ä¹ ",
      "authors": [
        "Wenkai Wang",
        "Vincent Lee",
        "Yizhen Zheng"
      ],
      "abstract": "Large language model hallucination represents a critical challenge where outputs deviate from factual accuracy due to distributional biases in training data. While recent investigations establish that specific hidden layers exhibit differences between hallucinatory and factual content, the precise localization of hallucination signals within layers remains unclear, limiting the development of efficient detection methods. We propose a dual-model architecture integrating a Projected Fusion (PF) block for adaptive inter-layer feature weighting and a Differential Feature Learning (DFL) mechanism that identifies discriminative features by computing differences between parallel encoders learning complementary representations from identical inputs. Through systematic experiments across HaluEval's question answering, dialogue, and summarization datasets, we demonstrate that hallucination signals concentrate in highly sparse feature subsets, achieving significant accuracy improvements on question answering and dialogue tasks. Notably, our analysis reveals a hierarchical \"funnel pattern\" where shallow layers exhibit high feature diversity while deep layers demonstrate concentrated usage, enabling detection performance to be maintained with minimal degradation using only 1\\% of feature dimensions. These findings suggest that hallucination signals are more concentrated than previously assumed, offering a pathway toward computationally efficient detection systems that could reduce inference costs while maintaining accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Model (LLM) çš„å¹»è§‰æ£€æµ‹é—®é¢˜ï¼Œæå‡ºäº†ç»“åˆ Projected Fusion (PF) æ¨¡å—ä¸ Differential Feature Learning (DFL) æœºåˆ¶çš„åŒæ¨¡å‹æ¶æ„ã€‚PF æ¨¡å—å®ç°äº†è‡ªé€‚åº”çš„è·¨å±‚ç‰¹å¾åŠ æƒï¼Œè€Œ DFL æœºåˆ¶åˆ™é€šè¿‡å¹¶è¡Œç¼–ç å™¨ä¹‹é—´çš„ç‰¹å¾å·®å¼‚æ¥è¯†åˆ«å…·æœ‰åˆ¤åˆ«æ€§çš„å¹»è§‰ä¿¡å·ã€‚åœ¨ HaluEval æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é—®ç­”å’Œå¯¹è¯ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ£€æµ‹å‡†ç¡®ç‡ï¼Œå¹¶å‘ç°å¹»è§‰ä¿¡å·é«˜åº¦é›†ä¸­åœ¨ç¨€ç–ç‰¹å¾å­é›†ä¸­ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ç‰¹å¾åˆ†å¸ƒçš„â€œæ¼æ–—æ¨¡å¼â€ (funnel pattern)ï¼Œå³æµ…å±‚ç‰¹å¾å…·æœ‰å¤šæ ·æ€§è€Œæ·±å±‚ç‰¹å¾è¶‹äºé›†ä¸­ï¼Œä½¿å¾—ç³»ç»Ÿä»…éœ€ 1% çš„ç‰¹å¾ç»´åº¦å³å¯ç»´æŒé«˜æ€§èƒ½ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†å¹»è§‰ä¿¡å·çš„é›†ä¸­æ€§ï¼Œä¸ºæ„å»ºä½æ¨ç†æˆæœ¬ä¸”é«˜æ•ˆçš„å¹»è§‰æ£€æµ‹ç³»ç»Ÿå¼€è¾Ÿäº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 7 figures, 13 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.21357v1",
      "published_date": "2025-09-20 06:48:22 UTC",
      "updated_date": "2025-09-20 06:48:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:58:29.398150+00:00"
    },
    {
      "arxiv_id": "2509.16551v2",
      "title": "Rethinking the Role of Text Complexity in Language Model Pretraining",
      "title_zh": "é‡æ–°å®¡è§†æ–‡æœ¬å¤æ‚åº¦åœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„ä½œç”¨",
      "authors": [
        "Dan John Velasco",
        "Matthew Theodore Roque"
      ],
      "abstract": "Improving pretraining data quality and size is known to boost downstream performance, but the role of text complexity--how hard a text is to read--remains less explored. We reduce surface-level complexity (shorter sentences, simpler words, simpler structure) while keeping core content approximately constant and ask: (i) How does complexity affect language modeling across model sizes? (ii) Can useful representations be learned from simpler text alone? (iii) How does pretraining text complexity influence downstream language understanding? We simplify human-written texts using a large language model, pretrain causal models (28M-500M) from scratch on original vs. simplified data, and evaluate them in fine-tuning and zero-shot setups. We find that perplexity is sensitive to the interaction between model capacity and text complexity--smaller models degrade far less on simpler texts--while text complexity has little impact on fine-tuning evaluations, with zero-shot evaluations indicating that simpler texts benefit performance on linguistic knowledge tasks, whereas more complex texts favor tasks requiring world knowledge and entity tracking. Our findings suggest that different types of data diversity affect transfer and zero-shot performance differently, providing insight into tailoring data curation to specific goals.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬å¤æ‚åº¦(text complexity)åœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„ä½œç”¨ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç®€åŒ–åŸå§‹æ–‡æœ¬çš„è¡¨é¢å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒæ ¸å¿ƒå†…å®¹åŸºæœ¬ä¸å˜ï¼Œå¯¹æ¯”åˆ†æäº†åŸå§‹æ•°æ®ä¸ç®€åŒ–æ•°æ®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶äººå‘˜ä»é›¶å¼€å§‹é¢„è®­ç»ƒäº†å‚æ•°é‡åœ¨28Mè‡³500Mä¹‹é—´çš„å› æœæ¨¡å‹ï¼Œå¹¶åœ¨å¾®è°ƒ(fine-tuning)å’Œé›¶æ ·æœ¬(zero-shot)è®¾ç½®ä¸‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒå‘ç°å›°æƒ‘åº¦(perplexity)å—æ¨¡å‹å®¹é‡ä¸æ–‡æœ¬å¤æ‚åº¦çš„äº¤äº’å½±å“ï¼Œè¾ƒå°å®¹é‡çš„æ¨¡å‹åœ¨ç®€åŒ–æ–‡æœ¬ä¸Šçš„æ€§èƒ½é€€åŒ–ç¨‹åº¦æ›´ä½ã€‚åœ¨å¾®è°ƒè¯„ä¼°ä¸­ï¼Œæ–‡æœ¬å¤æ‚åº¦å¯¹ç»“æœçš„å½±å“å¹¶ä¸æ˜¾è‘—ï¼Œä½†åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜æ˜¾å·®å¼‚ã€‚å…·ä½“è€Œè¨€ï¼Œç®€åŒ–æ–‡æœ¬æœ‰åˆ©äºæå‡è¯­è¨€çŸ¥è¯†(linguistic knowledge)ç›¸å…³ä»»åŠ¡çš„è¡¨ç°ï¼Œè€Œè¾ƒå¤æ‚çš„æ–‡æœ¬åˆ™åœ¨å¸¸è¯†çŸ¥è¯†(world knowledge)å’Œå®ä½“è¿½è¸ª(entity tracking)ä»»åŠ¡ä¸­æ›´å…·ä¼˜åŠ¿ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ä¸åŒç±»å‹çš„æ•°æ®å¤šæ ·æ€§å¯¹æ¨¡å‹è¿ç§»èƒ½åŠ›çš„å½±å“è·¯å¾„ï¼Œä¸ºé’ˆå¯¹ç‰¹å®šç›®æ ‡è¿›è¡Œæ•°æ®ç­–åˆ’(data curation)æä¾›äº†ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Camera-ready version for BabyLM Workshop at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16551v2",
      "published_date": "2025-09-20 06:33:01 UTC",
      "updated_date": "2025-10-04 06:12:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:58:13.828928+00:00"
    },
    {
      "arxiv_id": "2509.16550v1",
      "title": "TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation",
      "title_zh": "TranTacï¼šåˆ©ç”¨ç¬æ€è§¦è§‰ä¿¡å·å®ç°æ¥è§¦å¯†é›†å‹æœºå™¨äººæ“ä½œ",
      "authors": [
        "Yinghao Wu",
        "Shuhong Hou",
        "Haowen Zheng",
        "Yichen Li",
        "Weiyi Lu",
        "Xun Zhou",
        "Yitian Shao"
      ],
      "abstract": "Robotic manipulation tasks such as inserting a key into a lock or plugging a USB device into a port can fail when visual perception is insufficient to detect misalignment. In these situations, touch sensing is crucial for the robot to monitor the task's states and make precise, timely adjustments. Current touch sensing solutions are either insensitive to detect subtle changes or demand excessive sensor data. Here, we introduce TranTac, a data-efficient and low-cost tactile sensing and control framework that integrates a single contact-sensitive 6-axis inertial measurement unit within the elastomeric tips of a robotic gripper for completing fine insertion tasks. Our customized sensing system can detect dynamic translational and torsional deformations at the micrometer scale, enabling the tracking of visually imperceptible pose changes of the grasped object. By leveraging transformer-based encoders and diffusion policy, TranTac can imitate human insertion behaviors using transient tactile cues detected at the gripper's tip during insertion processes. These cues enable the robot to dynamically control and correct the 6-DoF pose of the grasped object. When combined with vision, TranTac achieves an average success rate of 79% on object grasping and insertion tasks, outperforming both vision-only policy and the one augmented with end-effector 6D force/torque sensing. Contact localization performance is also validated through tactile-only misaligned insertion tasks, achieving an average success rate of 88%. We assess the generalizability by training TranTac on a single prism-slot pair and testing it on unseen data, including a USB plug and a metal key, and find that the insertion tasks can still be completed with an average success rate of nearly 70%. The proposed framework may inspire new robotic tactile sensing systems for delicate manipulation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TranTacï¼Œä¸€ç§ä½æˆæœ¬ä¸”æ•°æ®é«˜æ•ˆçš„è§¦è§‰æ„ŸçŸ¥ä¸æ§åˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰æ„ŸçŸ¥ä¸è¶³æ—¶æœºå™¨äººéš¾ä»¥å®Œæˆç²¾ç»†æ’å…¥ä»»åŠ¡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨æœºå™¨äººæŠ“å–å™¨çš„å¼¹æ€§æŒ‡å°–å†…é›†æˆäº†ä¸€ä¸ªé«˜çµæ•åº¦çš„ 6-axis IMUï¼Œèƒ½å¤Ÿæ•æ‰å¾®ç±³çº§çš„å¹³ç§»å’Œæ‰­è½¬å˜å½¢ï¼Œä»è€Œç›‘æµ‹è¢«æŠ“å–ç‰©ä½“çš„ç»†å¾®ä½å§¿å˜åŒ–ã€‚TranTac ç»“åˆäº†åŸºäº Transformer çš„ç¼–ç å™¨å’Œæ‰©æ•£ç­–ç•¥ï¼ˆDiffusion Policyï¼‰ï¼Œåˆ©ç”¨æ’å…¥è¿‡ç¨‹ä¸­çš„ç¬æ€è§¦è§‰ä¿¡å·ï¼ˆtransient tactile signalsï¼‰æ¨¡ä»¿äººç±»è¡Œä¸ºï¼Œå®ç°å¯¹ç‰©ä½“ 6-DoF ä½å§¿çš„åŠ¨æ€çº åã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTranTac åœ¨è§†è§‰è¾…åŠ©ä¸‹çš„ä»»åŠ¡æˆåŠŸç‡è¾¾åˆ° 79%ï¼Œåœ¨ä»…é è§¦è§‰çš„åå·®æ’å…¥ä»»åŠ¡ä¸­æˆåŠŸç‡è¾¾ 88%ï¼Œä¸”åœ¨æœªè§è¿‡çš„ USB æ’å¤´å’Œé’¥åŒ™ä»»åŠ¡ä¸­å±•ç°å‡ºçº¦ 70% çš„æ³›åŒ–æˆåŠŸç‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ºéœ€è¦ç»†è…»æ“ä½œçš„æœºå™¨äººä»»åŠ¡æä¾›äº†ä¸€ç§åˆ›æ–°çš„ã€ä½æˆæœ¬çš„è§¦è§‰æ„ŸçŸ¥ç³»ç»Ÿæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.16550v1",
      "published_date": "2025-09-20 06:25:59 UTC",
      "updated_date": "2025-09-20 06:25:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:58:23.590527+00:00"
    },
    {
      "arxiv_id": "2509.16547v1",
      "title": "Checking extracted rules in Neural Networks",
      "title_zh": "ç¥ç»ç½‘ç»œæå–è§„åˆ™çš„æ ¡éªŒ",
      "authors": [
        "Adrian Wurm"
      ],
      "abstract": "In this paper we investigate formal verification of extracted rules for Neural Networks under a complexity theoretic point of view. A rule is a global property or a pattern concerning a large portion of the input space of a network. These rules are algorithmically extracted from networks in an effort to better understand their inner way of working. Here, three problems will be in the focus: Does a given set of rules apply to a given network? Is a given set of rules consistent or do the rules contradict themselves? Is a given set of rules exhaustive in the sense that for every input the output is determined? Finding algorithms that extract such rules out of networks has been investigated over the last 30 years, however, to the author's current knowledge, no attempt in verification was made until now. A lot of attempts of extracting rules use heuristics involving randomness and over-approximation, so it might be beneficial to know whether knowledge obtained in that way can actually be trusted.\n  We investigate the above questions for neural networks with ReLU-activation as well as for Boolean networks, each for several types of rules. We demonstrate how these problems can be reduced to each other and show that most of them are co-NP-complete.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»å¤æ‚åº¦ç†è®º(complexity theoretic)çš„è§’åº¦æ¢è®¨äº†ç¥ç»ç½‘ç»œ(Neural Networks)æå–è§„åˆ™çš„å½¢å¼åŒ–éªŒè¯(formal verification)é—®é¢˜ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šç»™å®šçš„è§„åˆ™é›†æ˜¯å¦é€‚ç”¨äºç‰¹å®šç½‘ç»œã€è§„åˆ™é›†æ˜¯å¦å…·æœ‰ä¸€è‡´æ€§(consistent)ä»¥åŠè§„åˆ™æ˜¯å¦è¯¦å°½(exhaustive)ã€‚å°½ç®¡è§„åˆ™æå–ç®—æ³•å·²æœ‰ä¸‰åå¹´çš„ç ”ç©¶å†å²ï¼Œä½†è¯¥è®ºæ–‡å¡«è¡¥äº†é’ˆå¯¹è¿™äº›è§„åˆ™è¿›è¡Œå½¢å¼åŒ–éªŒè¯çš„ç†è®ºç©ºç™½ã€‚ä½œè€…é’ˆå¯¹ä½¿ç”¨ReLUæ¿€æ´»(ReLU-activation)çš„ç¥ç»ç½‘ç»œä»¥åŠå¸ƒå°”ç½‘ç»œ(Boolean networks)çš„ä¸åŒè§„åˆ™ç±»å‹è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚é€šè¿‡å±•ç¤ºè¿™äº›éªŒè¯é—®é¢˜å¦‚ä½•ç›¸äº’è½¬åŒ–ï¼Œè®ºæ–‡è¯æ˜äº†å¤§å¤šæ•°ç›¸å…³é—®é¢˜åœ¨è®¡ç®—å¤æ‚åº¦ä¸Šå±äºco-NP-completeã€‚è¯¥ç ”ç©¶ä¸ºè¯„ä¼°é€šè¿‡å¯å‘å¼æˆ–è¿‡è¿‘ä¼¼(over-approximation)æ–¹æ³•æå–çš„çŸ¥è¯†æ˜¯å¦å¯ä¿¡æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, one figure",
      "pdf_url": "https://arxiv.org/pdf/2509.16547v1",
      "published_date": "2025-09-20 06:15:47 UTC",
      "updated_date": "2025-09-20 06:15:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:58:22.287123+00:00"
    },
    {
      "arxiv_id": "2509.16546v1",
      "title": "Train to Defend: First Defense Against Cryptanalytic Neural Network Parameter Extraction Attacks",
      "title_zh": "Train to Defendï¼šé¦–ä¸ªé’ˆå¯¹ç¥ç»ç½‘ç»œå‚æ•°æå–å¯†ç åˆ†ææ”»å‡»çš„é˜²å¾¡æœºåˆ¶",
      "authors": [
        "Ashley Kurian",
        "Aydin Aysu"
      ],
      "abstract": "Neural networks are valuable intellectual property due to the significant computational cost, expert labor, and proprietary data involved in their development. Consequently, protecting their parameters is critical not only for maintaining a competitive advantage but also for enhancing the model's security and privacy. Prior works have demonstrated the growing capability of cryptanalytic attacks to scale to deeper models. In this paper, we present the first defense mechanism against cryptanalytic parameter extraction attacks. Our key insight is to eliminate the neuron uniqueness necessary for these attacks to succeed. We achieve this by a novel, extraction-aware training method. Specifically, we augment the standard loss function with an additional regularization term that minimizes the distance between neuron weights within a layer. Therefore, the proposed defense has zero area-delay overhead during inference. We evaluate the effectiveness of our approach in mitigating extraction attacks while analyzing the model accuracy across different architectures and datasets. When re-trained with the same model architecture, the results show that our defense incurs a marginal accuracy change of less than 1% with the modified loss function. Moreover, we present a theoretical framework to quantify the success probability of the attack. When tested comprehensively with prior attack settings, our defense demonstrated empirical success for sustained periods of extraction, whereas unprotected networks are extracted between 14 minutes to 4 hours.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œå‚æ•°æå–æ”»å‡»(parameter extraction attacks)æå‡ºäº†åä¸ºTrain to Defendçš„é¦–ä¸ªé˜²å¾¡æœºåˆ¶ã€‚é’ˆå¯¹æ­¤ç±»æ”»å‡»ä¾èµ–ç¥ç»å…ƒå”¯ä¸€æ€§(neuron uniqueness)çš„ç‰¹æ€§ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æå–æ„ŸçŸ¥(extraction-aware)çš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨æ ‡å‡†æŸå¤±å‡½æ•°ä¸­å¼•å…¥æ­£åˆ™åŒ–é¡¹æ¥æœ€å°åŒ–å±‚å†…ç¥ç»å…ƒæƒé‡çš„è·ç¦»ã€‚è¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µå…·æœ‰é›¶é¢ç§¯å»¶è¿Ÿå¼€é”€(zero area-delay overhead)ï¼Œå®éªŒè¡¨æ˜å…¶å¯¹æ¨¡å‹å‡†ç¡®ç‡çš„å½±å“é€šå¸¸ä¸è¶³1%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªç”¨äºé‡åŒ–æ”»å‡»æˆåŠŸæ¦‚ç‡çš„ç†è®ºæ¡†æ¶ã€‚åœ¨å¤šç§æ”»å‡»è®¾å®šä¸‹çš„æµ‹è¯•è¯æ˜ï¼Œè¯¥é˜²å¾¡æœºåˆ¶èƒ½é•¿æ—¶é—´æœ‰æ•ˆæŠµå¾¡æå–ï¼Œæ˜¾è‘—ä¼˜äºåœ¨14åˆ†é’Ÿè‡³4å°æ—¶å†…å³è¢«ç ´è§£çš„æœªå—ä¿æŠ¤ç½‘ç»œã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "18 pages, 3 Figures",
      "pdf_url": "https://arxiv.org/pdf/2509.16546v1",
      "published_date": "2025-09-20 06:05:23 UTC",
      "updated_date": "2025-09-20 06:05:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:58:52.895879+00:00"
    },
    {
      "arxiv_id": "2509.19379v1",
      "title": "Learning from Observation: A Survey of Recent Advances",
      "title_zh": "ä»è§‚æµ‹ä¸­å­¦ä¹ ï¼šè¿‘æœŸè¿›å±•ç»¼è¿°",
      "authors": [
        "Returaj Burnwal",
        "Hriday Mehta",
        "Nirav Pravinbhai Bhatt",
        "Balaraman Ravindran"
      ],
      "abstract": "Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert's behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Although expert actions can provide detailed guidance, requiring such action information may prove impractical for real-world applications where expert actions are difficult to obtain. To address this limitation, the concept of learning from observation (LfO) or state-only imitation learning (SOIL) has recently gained attention, wherein the imitator only has access to expert state visitation information. In this paper, we present a framework for LfO and use it to survey and classify existing LfO methods in terms of their trajectory construction, assumptions and algorithm's design choices. This survey also draws connections between several related fields like offline RL, model-based RL and hierarchical RL. Finally, we use our framework to identify open problems and suggest future research directions.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°è®ºæ–‡ç³»ç»Ÿåœ°è°ƒç ”äº†æ¨¡ä»¿å­¦ä¹  (Imitation Learning, IL) é¢†åŸŸä¸­ä»è§‚å¯Ÿä¸­å­¦ä¹  (Learning from Observation, LfO) æˆ–ä»…çŠ¶æ€æ¨¡ä»¿å­¦ä¹  (State-Only Imitation Learning, SOIL) çš„æœ€æ–°è¿›å±•ã€‚é’ˆå¯¹ä¼ ç»Ÿ IL ç®—æ³•å› ä¾èµ–ä¸“å®¶åŠ¨ä½œä¿¡æ¯è€Œåœ¨å¤æ‚ç°å®åº”ç”¨ä¸­å—é™çš„é—®é¢˜ï¼ŒLfO æŠ€æœ¯ä»…é€šè¿‡è·å–ä¸“å®¶çš„çŠ¶æ€è®¿é—®ä¿¡æ¯å³å¯å®ç°ç­–ç•¥è®­ç»ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„ LfO æ¡†æ¶ï¼Œå¹¶æ ¹æ®è½¨è¿¹æ„å»ºã€æ ¸å¿ƒå‡è®¾å’Œç®—æ³•è®¾è®¡å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†æ·±åº¦åˆ†ç±»ã€‚è¯¥ç ”ç©¶è¿˜è¯¦ç»†æ¢è®¨äº† LfO ä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline RL)ã€åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹  (Model-based RL) åŠåˆ†å±‚å¼ºåŒ–å­¦ä¹  (Hierarchical RL) ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚æœ€åï¼Œä½œè€…åˆ©ç”¨è¯¥æ¡†æ¶è¯†åˆ«äº†é¢†åŸŸå†…çš„å¼€æ”¾æ€§é—®é¢˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼æ€§å»ºè®®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19379v1",
      "published_date": "2025-09-20 05:44:02 UTC",
      "updated_date": "2025-09-20 05:44:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:07.493602+00:00"
    },
    {
      "arxiv_id": "2509.18200v1",
      "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought",
      "title_zh": "å¯¹è¯å¼æ–¹ä½æ¨ç†ï¼šåŸºäºå¤šæ¨¡æ€æ€ç»´é“¾çš„è‡ªæˆ‘ä¸­å¿ƒåˆ°å¤–éƒ¨ä¸­å¿ƒå¯¼èˆª",
      "authors": [
        "Yu Ti Huang"
      ],
      "abstract": "Conversational agents must translate egocentric utterances (e.g., \"on my right\") into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Conversational Orientation Reasoning (COR) åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³å¯¹è¯æ™ºèƒ½ä½“åœ¨å®¤å†…æˆ–å¤æ‚è®¾æ–½ä¸­å¦‚ä½•å°†ç¬¬ä¸€äººç§°æè¿°(Egocentric)è½¬åŒ–ä¸ºç»å¯¹æ–¹ä½(Allocentric)çš„å¯¼èˆªéš¾é¢˜ã€‚é’ˆå¯¹ç¹ä½“ä¸­æ–‡(Traditional Chinese)å’ŒASRè½¬å½•åœºæ™¯ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†å¤šæ¨¡æ€é“¾å¼æ€ç»´(Multimodal Chain-of-Thought, MCoT)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æå–ç©ºé—´å…³ç³»ã€å°†åæ ‡æ˜ å°„è‡³ç»å¯¹æ–¹å‘ä»¥åŠæ¨æ–­ç”¨æˆ·æ–¹ä½è¿™ä¸‰ä¸ªç»“æ„åŒ–æ¨ç†æ­¥éª¤ï¼Œå°†ASRè¯­éŸ³ä¿¡æ¯ä¸åœ°æ ‡åæ ‡æ·±åº¦èåˆã€‚é€šè¿‡åœ¨Taiwan-LLM-13B-v2.0-Chatæ¨¡å‹ä¸Šåº”ç”¨è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)ç­–ç•¥ï¼ŒMCoTåœ¨çº¯å‡€æ–‡æœ¬å’ŒASRè½¬å½•ç¯å¢ƒä¸‹åˆ†åˆ«è¾¾åˆ°äº†100%å’Œ98.1%çš„æ–¹ä½å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºå•æ¨¡æ€å’Œéç»“æ„åŒ–åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å™ªå£°å¯¹è¯ã€å¤šè¯­è¨€è¯­ç è½¬æ¢(Code-switching)ä»¥åŠé¢†åŸŸè¿ç§»æµ‹è¯•ä¸­å±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ä¸å¯è§£é‡Šæ€§ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å…·èº«å¯¼èˆª(Embodied Navigation)æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18200v1",
      "published_date": "2025-09-20 05:25:32 UTC",
      "updated_date": "2025-09-20 05:25:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:02.688037+00:00"
    },
    {
      "arxiv_id": "2509.20376v1",
      "title": "ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models",
      "title_zh": "ConceptVizï¼šå¤§è¯­è¨€æ¨¡å‹æ¦‚å¿µæ¢ç´¢çš„å¯è§†åŒ–åˆ†ææ–¹æ³•",
      "authors": [
        "Haoxuan Li",
        "Zhen Wen",
        "Qiqi Jiang",
        "Chenxiao Li",
        "Yuwei Wu",
        "Yuchen Yang",
        "Yiyao Wang",
        "Xiuqi Huang",
        "Minfeng Zhu",
        "Wei Chen"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks. Understanding how LLMs internally represent knowledge remains a significant challenge. Despite Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz, a visual analytics system designed for exploring concepts in LLMs. ConceptViz implements a novel dentification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification. We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features. Our code and user guide are publicly available at https://github.com/Happy-Hippo209/ConceptViz.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ConceptVizï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ¢ç´¢ Large Language Models (LLMs) ä¸­æ¦‚å¿µçš„å¯è§†åŒ–åˆ†æç³»ç»Ÿã€‚ä¸ºäº†å¼¥è¡¥ Sparse Autoencoders (SAEs) æå–çš„ç‰¹å¾ä¸äººç±»å¯ç†è§£æ¦‚å¿µä¹‹é—´çš„é¸¿æ²Ÿï¼Œè¯¥ç³»ç»Ÿå®ç°äº†ä¸€å¥—åˆ›æ–°çš„ Identification => Interpretation => Validation å·¥ä½œæµã€‚ConceptViz å…è®¸ç”¨æˆ·åˆ©ç”¨æ„Ÿå…´è¶£çš„æ¦‚å¿µæŸ¥è¯¢ SAEsï¼Œäº¤äº’å¼åœ°æ¢ç´¢æ¦‚å¿µä¸ç‰¹å¾ä¹‹é—´çš„å¯¹é½å…³ç³»ï¼Œå¹¶é€šè¿‡æ¨¡å‹è¡Œä¸ºéªŒè¯å…¶å¯¹åº”æ€§ã€‚é€šè¿‡ä¸¤ä¸ªåº”ç”¨åœºæ™¯å’Œä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œå®éªŒè¯æ˜äº† ConceptViz åœ¨ç®€åŒ–æœ‰æ„ä¹‰æ¦‚å¿µè¡¨ç¤ºçš„å‘ç°ä¸éªŒè¯è¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç³»ç»Ÿæ˜¾è‘—å¢å¼ºäº† LLM çš„å¯è§£é‡Šæ€§ç ”ç©¶ï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜æ„å»ºæ›´å‡†ç¡®çš„ç‰¹å¾å¿ƒç†æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20376v1",
      "published_date": "2025-09-20 04:57:20 UTC",
      "updated_date": "2025-09-20 04:57:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:58:59.589373+00:00"
    },
    {
      "arxiv_id": "2509.16534v1",
      "title": "InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding",
      "title_zh": "InteGroundï¼šç»¼åˆæ€§æº¯æºä¸­çš„éªŒè¯ä¸æ£€ç´¢è§„åˆ’è¯„ä¼°ç ”ç©¶",
      "authors": [
        "Cheng Jiayang",
        "Qianqian Zhuang",
        "Haoran Li",
        "Chunkit Chan",
        "Xin Liu",
        "Lin Qiu",
        "Yangqiu Song"
      ],
      "abstract": "Grounding large language models (LLMs) in external knowledge sources is a promising method for faithful prediction. While existing grounding approaches work well for simple queries, many real-world information needs require synthesizing multiple pieces of evidence. We introduce \"integrative grounding\" -- the challenge of retrieving and verifying multiple inter-dependent pieces of evidence to support a hypothesis query. To systematically study this problem, we repurpose data from four domains for evaluating integrative grounding capabilities. Our investigation reveals two critical findings: First, in groundedness verification, while LLMs are robust to redundant evidence, they tend to rationalize using internal knowledge when information is incomplete. Second, in examining retrieval planning strategies, we find that undirected planning can degrade performance through noise introduction, while premise abduction emerges as a promising approach due to its logical constraints. Additionally, LLMs' zero-shot self-reflection capabilities consistently improve grounding quality. These insights provide valuable direction for developing more effective integrative grounding systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Integrative Groundingï¼ˆç»¼åˆæ¥åœ°ï¼‰çš„æ¦‚å¿µï¼Œé‡ç‚¹å…³æ³¨åœ¨å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢å’ŒéªŒè¯å¤šæ¡ç›¸äº’ä¾èµ–çš„è¯æ®ä»¥æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤æ‚é¢„æµ‹çš„æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨å››ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„è¡¨ç°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹åœ¨å¤„ç†Groundedness Verificationï¼ˆæ¥åœ°éªŒè¯ï¼‰æ—¶è™½èƒ½åº”å¯¹å†—ä½™ä¿¡æ¯ï¼Œä½†åœ¨ä¿¡æ¯ä¸å…¨æ—¶æ˜“åˆ©ç”¨å†…éƒ¨çŸ¥è¯†è¿›è¡ŒRationalizeï¼ˆåˆç†åŒ–æ¨ç†ï¼‰ã€‚åœ¨æ£€ç´¢è§„åˆ’ç­–ç•¥æ–¹é¢ï¼ŒUndirected Planningï¼ˆæ— å‘è§„åˆ’ï¼‰ç”±äºå¼•å…¥å™ªå£°ä¼šæŸå®³æ€§èƒ½ï¼Œè€Œå…·å¤‡é€»è¾‘çº¦æŸçš„Premise Abductionï¼ˆå‰ææº¯å› ï¼‰åˆ™å±•ç°å‡ºæ›´å¥½çš„åº”ç”¨æ½œåŠ›ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜LLMsçš„Zero-shot Self-reflectionï¼ˆé›¶æ ·æœ¬è‡ªæˆ‘åæ€ï¼‰èƒ½åŠ›å¯æ˜¾è‘—ä¸”æŒç»­åœ°æå‡æ¥åœ°ä»»åŠ¡çš„è´¨é‡ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥å¼€å‘æ›´ç²¾ç¡®ã€æ›´é«˜æ•ˆçš„ç»¼åˆæ¥åœ°ç³»ç»Ÿæä¾›äº†é‡è¦çš„æŠ€æœ¯è·¯å¾„å‚è€ƒå’Œä¼˜åŒ–æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2509.16534v1",
      "published_date": "2025-09-20 04:48:24 UTC",
      "updated_date": "2025-09-20 04:48:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:14.188430+00:00"
    },
    {
      "arxiv_id": "2509.16532v1",
      "title": "No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning",
      "title_zh": "æ— éœ€çœŸå®3Dï¼šèåˆ2Dè§†è§‰ä¸ä¼ª3Dè¡¨å¾çš„æœºå™¨äººæ“ä½œå­¦ä¹ ",
      "authors": [
        "Run Yu",
        "Yangdi Liu",
        "Wen-Da Wei",
        "Chen Li"
      ],
      "abstract": "Recently,vision-based robotic manipulation has garnered significant attention and witnessed substantial advancements. 2D image-based and 3D point cloud-based policy learning represent two predominant paradigms in the field, with recent studies showing that the latter consistently outperforms the former in terms of both policy performance and generalization, thereby underscoring the value and significance of 3D information. However, 3D point cloud-based approaches face the significant challenge of high data acquisition costs, limiting their scalability and real-world deployment. To address this issue, we propose a novel framework NoReal3D: which introduces the 3DStructureFormer, a learnable 3D perception module capable of transforming monocular images into geometrically meaningful pseudo-point cloud features, effectively fused with the 2D encoder output features. Specially, the generated pseudo-point clouds retain geometric and topological structures so we design a pseudo-point cloud encoder to preserve these properties, making it well-suited for our framework. We also investigate the effectiveness of different feature fusion strategies.Our framework enhances the robot's understanding of 3D spatial structures while completely eliminating the substantial costs associated with 3D point cloud acquisition.Extensive experiments across various tasks validate that our framework can achieve performance comparable to 3D point cloud-based methods, without the actual point cloud data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NoReal3D æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºè§†è§‰çš„æœºå™¨äººæ“æ§é¢†åŸŸä¸­ 3D point cloud æ•°æ®è·å–æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªåä¸º 3DStructureFormer çš„å¯å­¦ä¹  3D æ„ŸçŸ¥æ¨¡å—ï¼Œèƒ½å¤Ÿå°† monocular images è½¬åŒ–ä¸ºå…·æœ‰å‡ ä½•æ„ä¹‰çš„ pseudo-point cloud ç‰¹å¾ï¼Œå¹¶å°†å…¶ä¸ 2D ç¼–ç å™¨è¾“å‡ºçš„ç‰¹å¾è¿›è¡Œæœ‰æ•ˆèåˆã€‚é€šè¿‡ä¸“é—¨è®¾è®¡çš„ä¼ªç‚¹äº‘ç¼–ç å™¨ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿä¿ç•™å…³é”®çš„å‡ ä½•ä¸æ‹“æ‰‘ç»“æ„ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºæœºå™¨äººå¯¹ä¸‰ç»´ç©ºé—´æƒé‡çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNoReal3D åœ¨å¤šç§ä»»åŠ¡ä¸­å®ç°äº†ä¸åŸºäºçœŸå® 3D point cloud æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œä½†åœ¨åº”ç”¨ä¸­å®Œå…¨æ— éœ€çœŸå®çš„ 3D æ•°æ®ã€‚è¿™ç§èåˆ 2D è§†è§‰ä¸ä¼ª 3D è¡¨å¾çš„æ–¹æ³•ï¼Œåœ¨æ¶ˆé™¤ 3D æ•°æ®é‡‡é›†æˆæœ¬çš„åŒæ—¶ï¼Œä¸ºå®ç°é«˜æ‰©å±•æ€§çš„æœºå™¨äººæ“æ§å­¦ä¹ å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16532v1",
      "published_date": "2025-09-20 04:43:42 UTC",
      "updated_date": "2025-09-20 04:43:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:17.493807+00:00"
    },
    {
      "arxiv_id": "2509.16530v1",
      "title": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans",
      "title_zh": "AIPsychoBenchï¼šæ¢ç©¶å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»ä¹‹é—´çš„å¿ƒç†æµ‹é‡å­¦å·®å¼‚",
      "authors": [
        "Wei Xie",
        "Shuoyoucheng Ma",
        "Zhenhua Wang",
        "Enze Wang",
        "Kai Chen",
        "Xiaobing Sun",
        "Baosheng Wang"
      ],
      "abstract": "Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data. However, the uninterpretability of large-scale neural networks raises concerns about the reliability of LLM. Studies have attempted to assess the psychometric properties of LLMs by borrowing concepts from human psychology to enhance their interpretability, but they fail to account for the fundamental differences between LLMs and humans. This results in high rejection rates when human scales are reused directly. Furthermore, these scales do not support the measurement of LLM psychological property variations in different languages. This paper introduces AIPsychoBench, a specialized benchmark tailored to assess the psychological properties of LLM. It uses a lightweight role-playing prompt to bypass LLM alignment, improving the average effective response rate from 70.12% to 90.40%. Meanwhile, the average biases are only 3.3% (positive) and 2.1% (negative), which are significantly lower than the biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts. Furthermore, among the total of 112 psychometric subcategories, the score deviations for seven languages compared to English ranged from 5% to 20.2% in 43 subcategories, providing the first comprehensive evidence of the linguistic impact on the psychometrics of LLM.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†AIPsychoBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¿ƒç†å±æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶ç›´æ¥å¤ç”¨äººç±»å¿ƒç†å­¦é‡è¡¨æ—¶é¢ä¸´çš„é«˜æ‹’ç­”ç‡å’Œè·¨è¯­è¨€æ”¯æŒä¸è¶³ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è½»é‡çº§çš„è§’è‰²æ‰®æ¼”(role-playing)æç¤ºæ¥ç»•è¿‡æ¨¡å‹å¯¹é½é™åˆ¶ï¼Œå°†å¹³å‡æœ‰æ•ˆå“åº”ç‡ä»70.12%æ˜¾è‘—æå‡è‡³90.40%ï¼Œä¸”äº§ç”Ÿçš„æ­£è´Ÿåå·®å‡è¿œä½äºä¼ ç»Ÿçš„è¶Šç‹±(jailbreak)æç¤ºã€‚ç ”ç©¶é€šè¿‡å¯¹112ä¸ªå¿ƒç†æµ‹é‡å­ç±»åˆ«çš„åˆ†æï¼Œå‘ç°ä¸ƒç§ä¸åŒè¯­è¨€ä¸è‹±è¯­ç›¸æ¯”åœ¨43ä¸ªå­ç±»åˆ«ä¸­å­˜åœ¨5%è‡³20.2%çš„åˆ†æ•°åå·®ï¼Œé¦–æ¬¡æä¾›äº†è¯­è¨€å¯¹LLMå¿ƒç†æµ‹é‡(psychometric)ç‰¹å¾äº§ç”Ÿå½±å“çš„å…¨é¢è¯æ®ã€‚è¿™ä¸€æˆæœä¸ºç†è§£LLMsä¸äººç±»ä¹‹é—´çš„å¿ƒç†æµ‹é‡å·®å¼‚æä¾›äº†æ›´å…·é’ˆå¯¹æ€§çš„è¯„ä¼°å·¥å…·ï¼Œå¹¶æ­ç¤ºäº†è¯­è¨€èƒŒæ™¯åœ¨æ¨¡å‹å¿ƒç†è¯„ä¼°ä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Thank you for your attention. This paper was accepted by the CogSci 2025 conference in April and published in August. The location in the proceedings is: https://escholarship.org/uc/item/39k8f46q",
      "pdf_url": "https://arxiv.org/pdf/2509.16530v1",
      "published_date": "2025-09-20 04:40:31 UTC",
      "updated_date": "2025-09-20 04:40:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:20.278605+00:00"
    },
    {
      "arxiv_id": "2509.20375v1",
      "title": "Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text",
      "title_zh": "è¯„ä¼°ç”¨äºæ£€æµ‹ AI ç”Ÿæˆå­¦æœ¯æ–‡æœ¬çš„ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸åŸºäº Transformer çš„æ–¹æ³•",
      "authors": [
        "Sharanya Parimanoharan",
        "Ruwan D. Nawarathna"
      ],
      "abstract": "The rapid adoption of large language models (LLMs) such as ChatGPT has blurred the line between human and AI-generated texts, raising urgent questions about academic integrity, intellectual property, and the spread of misinformation. Thus, reliable AI-text detection is needed for fair assessment to safeguard human authenticity and cultivate trust in digital communication. In this study, we investigate how well current machine learning (ML) approaches can distinguish ChatGPT-3.5-generated texts from human-written texts employing a labeled data set of 250 pairs of abstracts from a wide range of research topics. We test and compare both classical (Logistic Regression armed with classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier, and LSTM-based N-gram models) ML detection techniques. As we aim to assess each model's performance in detecting AI-generated research texts, we also aim to test whether an ensemble of these models can outperform any single detector. Results show DistilBERT achieves the overall best performance, while Logistic Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and BERT-N-gram approaches lag. The max voting ensemble of the three best models fails to surpass DistilBERT itself, highlighting the primacy of a single transformer-based representation over mere model diversity. By comprehensively assessing the strengths and weaknesses of these AI-text detection approaches, this work lays a foundation for more robust transformer frameworks with larger, richer datasets to keep pace with ever-improving generative AI models.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹  (Machine Learning) ä¸åŸºäº Transformer çš„æ–¹æ³•åœ¨è¯†åˆ« AI ç”Ÿæˆç ”ç©¶æ–‡æœ¬æ–¹é¢çš„æ•ˆèƒ½ï¼Œä»¥åº”å¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ (LLMs) å¯¹å­¦æœ¯è¯šä¿¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ç ”ç©¶åˆ©ç”¨åŒ…å« 250 å¯¹æ‘˜è¦çš„æ ‡æ³¨æ•°æ®é›†ï¼Œå¯¹æ¯”äº†ç»“åˆ TF-IDF ç­‰ç‰¹å¾çš„ Logistic Regression ä»¥åŠ DistilBERTã€BERTã€LSTM ç­‰å¤šç§æ£€æµ‹æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDistilBERT åœ¨æ•´ä½“è¡¨ç°ä¸Šæœ€ä¸ºä¼˜å¼‚ï¼Œè€Œ Logistic Regression å’Œè‡ªå®šä¹‰çš„ BERT æ¨¡å‹åˆ™æä¾›äº†è¾ƒä¸ºç¨³å¥ä¸”å¹³è¡¡çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäº LSTM å’Œ N-gram çš„æ–¹æ³•è¡¨ç°ç›¸å¯¹æ»åï¼Œä¸”ç”±æœ€ä¼˜æ¨¡å‹ç»„æˆçš„æŠ•ç¥¨é›†æˆ (Ensemble) ç³»ç»Ÿæœªèƒ½è¶…è¶Šå•ä¸ª DistilBERT æ¨¡å‹ã€‚è¯¥å·¥ä½œé€šè¿‡å…¨é¢åˆ†æå„æ£€æµ‹æ–¹æ³•çš„ä¼˜åŠ£ï¼Œå¼ºè°ƒäº†å•ä¸€é«˜æ•ˆ Transformer è¡¨å¾åœ¨å¤„ç† AI æ–‡æœ¬æ£€æµ‹ä»»åŠ¡ä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œä¸ºæœªæ¥æ„å»ºæ›´å¼ºå¤§çš„æ£€æµ‹æ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20375v1",
      "published_date": "2025-09-20 04:36:21 UTC",
      "updated_date": "2025-09-20 04:36:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:25.381209+00:00"
    },
    {
      "arxiv_id": "2509.21356v1",
      "title": "Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports",
      "title_zh": "è‡ªåŠ¨ç”Ÿæˆèƒ¸éƒ¨ X å°„çº¿æŠ¥å‘Šçš„çŸ­è¯­å®šä½äº‹å®æ ¸æŸ¥",
      "authors": [
        "Razi Mahmood",
        "Diego Machado-Reyes",
        "Joy Wu",
        "Parisa Kaviani",
        "Ken C. L. Wong",
        "Niharika D'Souza",
        "Mannudeep Kalra",
        "Ge Wang",
        "Pingkun Yan",
        "Tanveer Syeda-Mahmood"
      ],
      "abstract": "With the emergence of large-scale vision language models (VLM), it is now possible to produce realistic-looking radiology reports for chest X-ray images. However, their clinical translation has been hampered by the factual errors and hallucinations in the produced descriptions during inference. In this paper, we present a novel phrase-grounded fact-checking model (FC model) that detects errors in findings and their indicated locations in automatically generated chest radiology reports.\n  Specifically, we simulate the errors in reports through a large synthetic dataset derived by perturbing findings and their locations in ground truth reports to form real and fake findings-location pairs with images. A new multi-label cross-modal contrastive regression network is then trained on this dataset. We present results demonstrating the robustness of our method in terms of accuracy of finding veracity prediction and localization on multiple X-ray datasets. We also show its effectiveness for error detection in reports of SOTA report generators on multiple datasets achieving a concordance correlation coefficient of 0.997 with ground truth-based verification, thus pointing to its utility during clinical inference in radiology workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºçŸ­è¯­å®šä½çš„äº‹å®æ£€æŸ¥æ¨¡å‹(FC model)ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è‡ªåŠ¨ç”Ÿæˆèƒ¸éƒ¨Xå°„çº¿(Chest X-ray)æŠ¥å‘Šæ—¶é¢ä¸´çš„äº‹å®é”™è¯¯å’Œå¹»è§‰é—®é¢˜ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç²¾ç¡®æ£€æµ‹æ”¾å°„å­¦æŠ¥å‘Šä¸­å‘ç°ç‰©(findings)åŠå…¶å¯¹åº”ä½ç½®çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è®­ç»ƒè¯¥æ¨¡å‹ï¼Œç ”ç©¶è€…é€šè¿‡æ‰°åŠ¨çœŸå®æŠ¥å‘Šä¸­çš„æè¿°æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†å¤šæ ‡ç­¾è·¨æ¨¡æ€å¯¹æ¯”å›å½’ç½‘ç»œ(multi-label cross-modal contrastive regression network)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„çœŸå®æ€§é¢„æµ‹å’Œå®šä½ä»»åŠ¡ä¸­å±•ç°äº†æ˜¾è‘—çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è¯„ä¼°ä¸»æµSOTAæŠ¥å‘Šç”Ÿæˆå™¨æ—¶ï¼Œä¸åŸºäºçœŸå€¼çš„éªŒè¯ç»“æœè¾¾åˆ°äº†0.997çš„ä¸€è‡´æ€§ç›¸å…³ç³»æ•°(CCC)ï¼Œå……åˆ†è¯æ˜äº†å…¶åœ¨æ”¾å°„ç§‘ä¸´åºŠæ¨æ–­å·¥ä½œæµä¸­çš„å®ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "In proceedings MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.21356v1",
      "published_date": "2025-09-20 04:33:44 UTC",
      "updated_date": "2025-09-20 04:33:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:24.371119+00:00"
    },
    {
      "arxiv_id": "2509.16527v2",
      "title": "Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity",
      "title_zh": "å­¦ä¹ çœŸå®ä¸–ç•Œåƒç´ åŠ¨æ€æ€§çš„æ ¼å­ç»å°”å…¹æ›¼æ¨¡å‹",
      "authors": [
        "Guangze Zheng",
        "Shijie Lin",
        "Haobo Zuo",
        "Si Si",
        "Ming-Shan Wang",
        "Changhong Fu",
        "Jia Pan"
      ],
      "abstract": "This work proposes the Lattice Boltzmann Model (LBM) to learn real-world pixel dynamicity for visual tracking. LBM decomposes visual representations into dynamic pixel lattices and solves pixel motion states through collision-streaming processes. Specifically, the high-dimensional distribution of the target pixels is acquired through a multilayer predict-update network to estimate the pixel positions and visibility. The predict stage formulates lattice collisions among the spatial neighborhood of target pixels and develops lattice streaming within the temporal visual context. The update stage rectifies the pixel distributions with online visual representations. Compared with existing methods, LBM demonstrates practical applicability in an online and real-time manner, which can efficiently adapt to real-world visual tracking tasks. Comprehensive evaluations of real-world point tracking benchmarks such as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B further demonstrates LBM's real-world practicality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æ ¼å­ç»å°”å…¹æ›¼æ¨¡å‹(Lattice Boltzmann Model, LBM)ï¼Œæ—¨åœ¨å­¦ä¹ çœŸå®ä¸–ç•Œè§†è§‰è·Ÿè¸ªä¸­çš„åƒç´ åŠ¨æ€æ€§(pixel dynamicity)ã€‚LBMå°†è§†è§‰è¡¨å¾åˆ†è§£ä¸ºåŠ¨æ€åƒç´ æ ¼å­(dynamic pixel lattices)ï¼Œå¹¶é€šè¿‡ç¢°æ’-æµæ¼”åŒ–è¿‡ç¨‹(collision-streaming processes)æ±‚è§£åƒç´ è¿åŠ¨çŠ¶æ€ã€‚è¯¥æ¶æ„åˆ©ç”¨å¤šå±‚é¢„æµ‹-æ›´æ–°ç½‘ç»œ(predict-update network)è·å–ç›®æ ‡åƒç´ çš„é«˜ç»´åˆ†å¸ƒï¼Œä»¥ç²¾ç¡®ä¼°è®¡åƒç´ ä½ç½®å’Œå¯è§æ€§ã€‚åœ¨é¢„æµ‹é˜¶æ®µï¼Œæ¨¡å‹å¤„ç†ç›®æ ‡åƒç´ ç©ºé—´é‚»åŸŸå†…çš„æ ¼å­ç¢°æ’ï¼Œå¹¶åœ¨æ—¶é—´è§†è§‰ä¸Šä¸‹æ–‡ä¸­å®ç°æ ¼å­æµæ¼”åŒ–ï¼›éšååœ¨æ›´æ–°é˜¶æ®µåˆ©ç”¨åœ¨çº¿è§†è§‰è¡¨å¾ä¿®æ­£åƒç´ åˆ†å¸ƒã€‚ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼ŒLBMå…·æœ‰ä¼˜ç§€çš„åœ¨çº¿å®æ—¶å¤„ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿé«˜æ•ˆé€‚åº”å¤æ‚çš„çœŸå®è§†è§‰è·Ÿè¸ªä»»åŠ¡ã€‚åœ¨TAP-Vidå’ŒRoboTAPç­‰ç‚¹è·Ÿè¸ªåŸºå‡†ä»¥åŠTAOã€BFTã€OVT-Bç­‰å¤§è§„æ¨¡å¼€æ”¾ä¸–ç•Œç›®æ ‡è·Ÿè¸ªåŸºå‡†ä¸Šçš„å®éªŒç»“æœï¼Œå……åˆ†è¯æ˜äº†LBMçš„æ•ˆç‡ä¸å¹¿æ³›çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2025. Project page: https://george-zhuang.github.io/lbm/",
      "pdf_url": "https://arxiv.org/pdf/2509.16527v2",
      "published_date": "2025-09-20 04:25:27 UTC",
      "updated_date": "2025-11-01 01:04:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:32.758982+00:00"
    },
    {
      "arxiv_id": "2509.16525v1",
      "title": "Causal Fuzzing for Verifying Machine Unlearning",
      "title_zh": "ç”¨äºéªŒè¯æœºå™¨é—å¿˜çš„å› æœæ¨¡ç³Šæµ‹è¯•",
      "authors": [
        "Anna Mazhar",
        "Sainyam Galhotra"
      ],
      "abstract": "As machine learning models become increasingly embedded in decision-making systems, the ability to \"unlearn\" targeted data or features is crucial for enhancing model adaptability, fairness, and privacy in models which involves expensive training. To effectively guide machine unlearning, a thorough testing is essential. Existing methods for verification of machine unlearning provide limited insights, often failing in scenarios where the influence is indirect. In this work, we propose CAFÃ‰, a new causality based framework that unifies datapoint- and feature-level unlearning for verification of black-box ML models. CAFÃ‰ evaluates both direct and indirect effects of unlearning targets through causal dependencies, providing actionable insights with fine-grained analysis. Our evaluation across five datasets and three model architectures demonstrates that CAFÃ‰ successfully detects residual influence missed by baselines while maintaining computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CAFÃ‰ï¼Œä¸€ç§åŸºäºå› æœå…³ç³» (causality) çš„å…¨æ–°æ¡†æ¶ï¼Œæ—¨åœ¨éªŒè¯é»‘ç›’æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ•°æ®ç‚¹çº§åˆ« (datapoint-level) å’Œç‰¹å¾çº§åˆ« (feature-level) çš„ Machine Unlearning æ•ˆæœã€‚é’ˆå¯¹ç°æœ‰éªŒè¯æ–¹æ³•éš¾ä»¥è¯†åˆ«é—´æ¥å½±å“çš„é—®é¢˜ï¼ŒCAFÃ‰ é€šè¿‡åˆ†æå› æœä¾èµ–å…³ç³»æ¥è¯„ä¼°å¸è½½ç›®æ ‡çš„ç›´æ¥ä¸é—´æ¥æ•ˆåº”ï¼Œä»è€Œæä¾›ç»†ç²’åº¦çš„åˆ†æã€‚åœ¨äº”ä¸ªæ•°æ®é›†å’Œä¸‰ç§æ¨¡å‹æ¶æ„ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸæˆåŠŸæ£€æµ‹åˆ°åŸºçº¿æ–¹æ³•æ¼æ‰çš„æ®‹ä½™å½±å“ (residual influence)ã€‚æ­¤å¤–ï¼ŒCAFÃ‰ åœ¨ç¡®ä¿éªŒè¯æ·±åº¦çš„åŒæ—¶ä¿æŒäº†è‰¯å¥½çš„è®¡ç®—æ•ˆç‡ï¼Œä¸ºæå‡æ¨¡å‹çš„é€‚åº”æ€§ã€å…¬å¹³æ€§å’Œéšç§æ€§æä¾›äº†æœ‰æ•ˆçš„æµ‹è¯•æ‰‹æ®µã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16525v1",
      "published_date": "2025-09-20 04:19:37 UTC",
      "updated_date": "2025-09-20 04:19:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:44.061255+00:00"
    },
    {
      "arxiv_id": "2509.16517v1",
      "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding",
      "title_zh": "Seeing Cultureï¼šé¢å‘è§†è§‰æ¨ç†ä¸å®šä½çš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Burak Satar",
        "Zhixin Ma",
        "Patrick A. Irawan",
        "Wilfried A. Mulyawan",
        "Jing Jiang",
        "Ee-Peng Lim",
        "Chong-Wah Ngo"
      ],
      "abstract": "Multimodal vision-language models (VLMs) have made substantial progress in various tasks that require a combined understanding of visual and textual content, particularly in cultural understanding tasks, with the emergence of new cultural datasets. However, these datasets frequently fall short of providing cultural reasoning while underrepresenting many cultures. In this paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural reasoning with a novel approach that requires VLMs to reason on culturally rich images in two stages: i) selecting the correct visual option with multiple-choice visual question answering (VQA), and ii) segmenting the relevant cultural artifact as evidence of reasoning. Visual options in the first stage are systematically organized into three types: those originating from the same country, those from different countries, or a mixed group. Notably, all options are derived from a singular category for each type. Progression to the second stage occurs only after a correct visual option is chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural artifacts across five categories from seven Southeast Asia countries, whose diverse cultures are often overlooked, accompanied by 3,178 questions, of which 1,093 are unique and meticulously curated by human annotators. Our evaluation of various VLMs reveals the complexities involved in cross-modal cultural reasoning and highlights the disparity between visual reasoning and spatial grounding in culturally nuanced scenarios. The SCB serves as a crucial benchmark for identifying these shortcomings, thereby guiding future developments in the field of cultural reasoning. https://github.com/buraksatar/SeeingCulture",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Seeing Culture Benchmark (SCB)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) æ–‡åŒ–æ¨ç†ä¸ç©ºé—´å®šä½èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†åœ¨æ–‡åŒ–æ¨ç† (cultural reasoning) æ·±åº¦åŠä»£è¡¨æ€§ä¸Šçš„ä¸è¶³ï¼ŒSCB èšç„¦äºä¸œå—äºšä¸ƒä¸ªå›½å®¶çš„å¤šå…ƒæ–‡åŒ–ï¼Œæ¶µç›–äº† 1,065 å¼ å›¾åƒåŠ 138 ç§æ–‡åŒ–äººå·¥åˆ¶å“ (cultural artifacts)ã€‚è¯¥åŸºå‡†é‡‡ç”¨æ–°é¢–çš„ä¸¤é˜¶æ®µæ¨ç†æœºåˆ¶ï¼Œè¦æ±‚æ¨¡å‹å…ˆé€šè¿‡å¤šé¡¹é€‰æ‹©è§†è§‰é—®ç­” (VQA) è¯†åˆ«æ–‡åŒ–ç‰¹å¾ï¼Œå†å¯¹æ”¯æ’‘å…¶æ¨ç†è¯æ®çš„å›¾åƒåŒºåŸŸè¿›è¡Œåˆ†å‰²ã€‚å®éªŒè¯„ä¼°æ­ç¤ºäº†è·¨æ¨¡æ€æ–‡åŒ–æ¨ç†çš„å¤æ‚æ€§ï¼Œå¹¶å¼ºè°ƒäº†æ¨¡å‹åœ¨å¤„ç†æ–‡åŒ–ç»†å¾®åœºæ™¯æ—¶ï¼Œè§†è§‰æ¨ç†ä¸ç©ºé—´å®šä½ (spatial grounding) ä¹‹é—´å­˜åœ¨çš„æ˜¾è‘—å·®è·ã€‚SCB çš„æå‡ºæœ‰æ•ˆè¯†åˆ«äº†å½“å‰æ¨¡å‹åœ¨æ–‡åŒ–æ„ŸçŸ¥æ–¹é¢çš„çŸ­æ¿ï¼Œä¸ºæœªæ¥æ„å»ºå…·å¤‡æ·±å±‚æ–‡åŒ–ç†è§£èƒ½åŠ›çš„æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to EMNLP 2025 Main Conference, https://seeingculture-benchmark.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2509.16517v1",
      "published_date": "2025-09-20 03:47:49 UTC",
      "updated_date": "2025-09-20 03:47:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:50.375415+00:00"
    },
    {
      "arxiv_id": "2509.21355v1",
      "title": "Domain-Informed Genetic Superposition Programming: A Case Study on SFRC Beams",
      "title_zh": "é¢†åŸŸå¯å‘çš„é—ä¼ å åŠ ç¼–ç¨‹ï¼šä»¥é’¢çº¤ç»´æ··å‡åœŸæ¢ä¸ºä¾‹",
      "authors": [
        "Mohammad Sadegh Khorshidi",
        "Navid Yazdanjue",
        "Hassan Gharoun",
        "Mohammad Reza Nikoo",
        "Fang Chen",
        "Amir H. Gandomi"
      ],
      "abstract": "This study presents domain-informed genetic superposition programming (DIGSP), a symbolic regression framework tailored for engineering systems governed by separable physical mechanisms. DIGSP partitions the input space into domain-specific feature subsets and evolves independent genetic programming (GP) populations to model material-specific effects. Early evolution occurs in isolation, while ensemble fitness promotes inter-population cooperation. To enable symbolic superposition, an adaptive hierarchical symbolic abstraction mechanism (AHSAM) is triggered after stagnation across all populations. AHSAM performs analysis of variance- (ANOVA) based filtering to identify statistically significant individuals, compresses them into symbolic constructs, and injects them into all populations through a validation-guided pruning cycle. The DIGSP is benchmarked against a baseline multi-gene genetic programming (BGP) model using a dataset of steel fiber-reinforced concrete (SFRC) beams. Across 30 independent trials with 65% training, 10% validation, and 25% testing splits, DIGSP consistently outperformed BGP in training and test root mean squared error (RMSE). The Wilcoxon rank-sum test confirmed statistical significance (p < 0.01), and DIGSP showed tighter error distributions and fewer outliers. No significant difference was observed in validation RMSE due to limited sample size. These results demonstrate that domain-informed structural decomposition and symbolic abstraction improve convergence and generalization. DIGSP offers a principled and interpretable modeling strategy for systems where symbolic superposition aligns with the underlying physical structure.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¢†åŸŸæ„ŸçŸ¥é—ä¼ å åŠ ç¼–ç¨‹ (Domain-Informed Genetic Superposition Programming, DIGSP)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹å…·æœ‰å¯åˆ†ç¦»ç‰©ç†æœºåˆ¶å·¥ç¨‹ç³»ç»Ÿçš„ç¬¦å·å›å½’ (Symbolic Regression) æ¡†æ¶ã€‚DIGSP é€šè¿‡å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸ºç‰¹å®šé¢†åŸŸçš„ç‰¹å¾å­é›†ï¼Œå¹¶æ¼”åŒ–ç‹¬ç«‹çš„é—ä¼ ç¼–ç¨‹ (Genetic Programming, GP) ç§ç¾¤æ¥å¯¹ææ–™ç‰¹å®šæ•ˆåº”è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒå¼•å…¥äº†è‡ªé€‚åº”åˆ†å±‚ç¬¦å·æŠ½è±¡æœºåˆ¶ (Adaptive Hierarchical Symbolic Abstraction Mechanism, AHSAM)ï¼Œé€šè¿‡åŸºäºæ–¹å·®åˆ†æ (ANOVA) çš„è¿‡æ»¤è¯†åˆ«æ˜¾è‘—ä¸ªä½“ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç¬¦å·æ„ä»¶æ³¨å…¥æ¼”åŒ–è¿‡ç¨‹ã€‚ä»¥é’¢çº¤ç»´å¢å¼ºæ··å‡åœŸ (Steel Fiber-Reinforced Concrete, SFRC) æ¢ä¸ºæ¡ˆä¾‹çš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒDIGSP åœ¨è®­ç»ƒå’Œæµ‹è¯•é›†çš„å‡æ–¹æ ¹è¯¯å·® (RMSE) è¡¨ç°ä¸Šä¸€è‡´ä¼˜äºåŸºçº¿å¤šåŸºå› é—ä¼ ç¼–ç¨‹ (BGP) æ¨¡å‹ã€‚Wilcoxon ç§©å’Œæ£€éªŒ (p < 0.01) è¯å®äº†ç»“æœçš„æ˜¾è‘—æ€§ï¼Œä¸”è¯¥æ¨¡å‹å±•ç°å‡ºæ›´ç´§å‡‘çš„è¯¯å·®åˆ†å¸ƒå’Œæ›´å°‘çš„ç¦»ç¾¤ç‚¹ã€‚ç ”ç©¶è¯æ˜ï¼Œé¢†åŸŸæ„ŸçŸ¥çš„ç»“æ„åˆ†è§£ä¸ç¬¦å·æŠ½è±¡æœºåˆ¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ä¸æ³›åŒ–æ€§èƒ½ï¼Œä¸ºå…·å¤‡ç‰©ç†å åŠ ç‰¹æ€§çš„å¤æ‚ç³»ç»Ÿæä¾›äº†ä¸€ç§æå…·è§£é‡ŠåŠ›çš„å»ºæ¨¡ç­–ç•¥ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "11 pages, 6 tables, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21355v1",
      "published_date": "2025-09-20 03:32:31 UTC",
      "updated_date": "2025-09-20 03:32:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:56.455606+00:00"
    },
    {
      "arxiv_id": "2509.21354v2",
      "title": "KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache",
      "title_zh": "KV-Efficient VLAï¼šä¸€ç§åŸºäºRNNé—¨æ§åˆ†å—KVç¼“å­˜çš„è§†è§‰è¯­è¨€æ¨¡å‹åŠ é€Ÿæ–¹æ³•",
      "authors": [
        "Wanshun Xu",
        "Long Zhuang",
        "Lianlei Shan"
      ],
      "abstract": "Vision-Language-Action (VLA) models offer a unified framework for robotic perception and control, but their ability to scale to real-world, long-horizon tasks is limited by the high computational cost of attention and the large memory required for storing key-value (KV) pairs during inference, particularly when retaining historical image tokens as context. Recent methods have focused on scaling backbone architectures to improve generalization, with less emphasis on addressing inference inefficiencies essential for real-time use. In this work, we present KV-Efficient VLA, a model-agnostic memory compression approach designed to address these limitations by introducing a lightweight mechanism to selectively retain high-utility context. Our method partitions the KV cache into fixed-size chunks and employs a recurrent gating module to summarize and filter the historical context according to learned utility scores. This design aims to preserve recent fine-grained detail while aggressively pruning stale, low-relevance memory. Based on experiments, our approach can yield an average of 24.6% FLOPs savings, 1.34x inference speedup, and 1.87x reduction in KV memory. Our method integrates seamlessly into recent VLA stacks, enabling scalable inference without modifying downstream control logic.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KV-Efficient VLAï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„å†…å­˜å‹ç¼©æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹åœ¨é•¿æ—¶ç¨‹æœºå™¨äººä»»åŠ¡ä¸­å› Key-Value (KV)ç¼“å­˜è¿‡å¤§å¯¼è‡´çš„è®¡ç®—ä¸å†…å­˜ç“¶é¢ˆã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§è½»é‡åŒ–çš„åˆ†å—æœºåˆ¶ï¼Œåˆ©ç”¨å¾ªç¯é—¨æ§æ¨¡å—(recurrent gating module)æ ¹æ®å­¦ä¹ åˆ°çš„å®ç”¨æ€§è¯„åˆ†å¯¹å†å²ä¸Šä¸‹æ–‡è¿›è¡ŒåŠ¨æ€è¿‡æ»¤ä¸æ€»ç»“ã€‚è¿™ç§è®¾è®¡åœ¨ä¿ç•™è¿‘æœŸå…³é”®ç»†ç²’åº¦ä¿¡æ¯çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå¤§å¹…åº¦å‰”é™¤é™ˆæ—§ä¸”ä½ç›¸å…³çš„å­˜å‚¨æ•°æ®ã€‚å®éªŒæ•°æ®è¯æ˜ï¼ŒKV-Efficient VLAå¹³å‡å¯èŠ‚çœ24.6%çš„FLOPsï¼Œå¹¶å°†æ¨ç†é€Ÿåº¦æå‡1.34å€ï¼ŒåŒæ—¶å‡å°‘1.87å€çš„KVç¼“å­˜å ç”¨ã€‚è¯¥æ–¹æ³•å¯æ— ç¼é›†æˆè‡³ç°æœ‰çš„VLAæŠ€æœ¯æ ˆï¼Œåœ¨ä¸æ”¹å˜ä¸‹æ¸¸æ§åˆ¶é€»è¾‘çš„å‰æä¸‹æ˜¾è‘—æå‡äº†å®æ—¶æ¨ç†çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21354v2",
      "published_date": "2025-09-20 02:04:24 UTC",
      "updated_date": "2025-11-23 17:07:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:54.497617+00:00"
    },
    {
      "arxiv_id": "2509.16496v1",
      "title": "Synergies between Federated Foundation Models and Smart Power Grids",
      "title_zh": "è”é‚¦åŸºç¡€æ¨¡å‹ä¸æ™ºèƒ½ç”µç½‘çš„ååŒæ•ˆåº”",
      "authors": [
        "Seyyedali Hosseinalipour",
        "Shimiao Li",
        "Adedoyin Inaolaji",
        "Filippo Malandra",
        "Luis Herrera",
        "Nicholas Mastronarde"
      ],
      "abstract": "The recent emergence of large language models (LLMs) such as GPT-3 has marked a significant paradigm shift in machine learning. Trained on massive corpora of data, these models demonstrate remarkable capabilities in language understanding, generation, summarization, and reasoning, transforming how intelligent systems process and interact with human language. Although LLMs may still seem like a recent breakthrough, the field is already witnessing the rise of a new and more general category: multi-modal, multi-task foundation models (M3T FMs). These models go beyond language and can process heterogeneous data types/modalities, such as time-series measurements, audio, imagery, tabular records, and unstructured logs, while supporting a broad range of downstream tasks spanning forecasting, classification, control, and retrieval. When combined with federated learning (FL), they give rise to M3T Federated Foundation Models (FedFMs): a highly recent and largely unexplored class of models that enable scalable, privacy-preserving model training/fine-tuning across distributed data sources. In this paper, we take one of the first steps toward introducing these models to the power systems research community by offering a bidirectional perspective: (i) M3T FedFMs for smart grids and (ii) smart grids for FedFMs. In the former, we explore how M3T FedFMs can enhance key grid functions, such as load/demand forecasting and fault detection, by learning from distributed, heterogeneous data available at the grid edge in a privacy-preserving manner. In the latter, we investigate how the constraints and structure of smart grids, spanning energy, communication, and regulatory dimensions, shape the design, training, and deployment of M3T FedFMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤šä»»åŠ¡è”é‚¦åŸºç¡€æ¨¡å‹(M3T Federated Foundation Models, FedFMs)ä¸æ™ºèƒ½ç”µç½‘(Smart Power Grids)ä¹‹é—´çš„åŒå‘ååŒä½œç”¨ã€‚æ–‡ç« é¦–å…ˆé˜è¿°äº†M3T FMså¤„ç†æ—¶é—´åºåˆ—ã€å›¾åƒåŠè¡¨æ ¼è®°å½•ç­‰å¼‚æ„æ•°æ®çš„å¼ºå¤§èƒ½åŠ›ï¼Œå¹¶ç»“åˆè”é‚¦å­¦ä¹ (Federated Learning)æå‡ºäº†FedFMsæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°åˆ†å¸ƒå¼æ•°æ®æºä¸Šçš„éšç§ä¿æŠ¤æ¨¡å‹è®­ç»ƒä¸å¾®è°ƒã€‚é€šè¿‡åŒå‘è§†è§’ï¼Œç ”ç©¶åˆ†æäº†FedFMså¦‚ä½•åˆ©ç”¨ç”µç½‘è¾¹ç¼˜æ•°æ®ä¼˜åŒ–è´Ÿè·é¢„æµ‹å’Œæ•…éšœæ£€æµ‹ç­‰åŠŸèƒ½ï¼ŒåŒæ—¶æ¢è®¨äº†æ™ºèƒ½ç”µç½‘åœ¨èƒ½æºã€é€šä¿¡å’Œç›‘ç®¡æ–¹é¢çš„çº¦æŸå¦‚ä½•åå‘å¡‘é€ FedFMsçš„è®¾è®¡ä¸éƒ¨ç½²ã€‚ä½œä¸ºé¦–æ‰¹å°†è¯¥æ¨¡å‹ç±»åˆ«å¼•å…¥ç”µåŠ›ç³»ç»Ÿç¤¾åŒºçš„ç ”ç©¶ä¹‹ä¸€ï¼Œè¯¥å·¥ä½œä¸ºæ„å»ºé«˜æ•ˆã€éšç§ä¸”å…·å¤‡é²æ£’æ€§çš„åˆ†å¸ƒå¼ç”µç½‘æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16496v1",
      "published_date": "2025-09-20 02:00:07 UTC",
      "updated_date": "2025-09-20 02:00:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T19:59:57.785575+00:00"
    },
    {
      "arxiv_id": "2509.16494v2",
      "title": "Can an Individual Manipulate the Collective Decisions of Multi-Agents?",
      "title_zh": "ä¸ªä½“èƒ½å¦æ“çºµå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é›†ä½“å†³ç­–ï¼Ÿ",
      "authors": [
        "Fengyuan Liu",
        "Rui Zhao",
        "Shuo Chen",
        "Guohao Li",
        "Philip Torr",
        "Lei Han",
        "Jindong Gu"
      ],
      "abstract": "Individual Large Language Models (LLMs) have demonstrated significant capabilities across various domains, such as healthcare and law. Recent studies also show that coordinated multi-agent systems exhibit enhanced decision-making and reasoning abilities through collaboration. However, due to the vulnerabilities of individual LLMs and the difficulty of accessing all agents in a multi-agent system, a key question arises: If attackers only know one agent, could they still generate adversarial samples capable of misleading the collective decision? To explore this question, we formulate it as a game with incomplete information, where attackers know only one target agent and lack knowledge of the other agents in the system. With this formulation, we propose M-Spoiler, a framework that simulates agent interactions within a multi-agent system to generate adversarial samples. These samples are then used to manipulate the target agent in the target system, misleading the system's collaborative decision-making process. More specifically, M-Spoiler introduces a stubborn agent that actively aids in optimizing adversarial samples by simulating potential stubborn responses from agents in the target system. This enhances the effectiveness of the generated adversarial samples in misleading the system. Through extensive experiments across various tasks, our findings confirm the risks posed by the knowledge of an individual agent in multi-agent systems and demonstrate the effectiveness of our framework. We also explore several defense mechanisms, showing that our proposed attack framework remains more potent than baselines, underscoring the need for further research into defensive strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ”»å‡»è€…ä»…æŒæ¡å•ä¸ªæ™ºèƒ½ä½“ä¿¡æ¯çš„ incomplete information åœºæ™¯ä¸‹ï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆmulti-agent systemsï¼‰é›†ä½“å†³ç­–æ˜¯å¦ä¼šè¢«æ“çºµã€‚ä½œè€…å°†è¯¥æŒ‘æˆ˜å½¢å¼åŒ–ä¸ºåšå¼ˆè®ºé—®é¢˜ï¼Œå¹¶æå‡ºäº†åä¸º M-Spoiler çš„æ”»å‡»æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿç³»ç»Ÿå†…æ™ºèƒ½ä½“çš„äº¤äº’æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼ˆadversarial samplesï¼‰ã€‚M-Spoiler å¼•å…¥äº†ä¸€ä¸ªå…³é”®çš„ stubborn agent ç»„ä»¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿç›®æ ‡ç³»ç»Ÿä¸­çš„å›ºæ‰§å“åº”æ¥ä¼˜åŒ–æ ·æœ¬ï¼Œä»è€Œæœ‰æ•ˆè¯±å¯¼é›†ä½“å†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…å‡­å•ä¸ªæ™ºèƒ½ä½“çš„çŸ¥è¯†å³å¯å¯¹æ•´ä¸ªåä½œç³»ç»Ÿæ„æˆä¸¥é‡å®‰å…¨é£é™©ï¼Œä¸” M-Spoiler åœ¨å¤šç§ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ¯”åŸºçº¿æ–¹æ³•æ›´å¼ºçš„æ”»å‡»æ•ˆåŠ›ã€‚ç ”ç©¶æœ€åè¯„ä¼°äº†å¤šç§é˜²å¾¡æœºåˆ¶ï¼Œå¼ºè°ƒäº†å½“å‰ç³»ç»Ÿåœ¨åº”å¯¹æ­¤ç±»å®šå‘æ“çºµæ—¶ä»æ˜¾è„†å¼±ï¼ŒäºŸéœ€è¿›ä¸€æ­¥ç ”ç©¶æ›´ä¸ºç¨³å¥çš„é˜²å¾¡ç­–ç•¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16494v2",
      "published_date": "2025-09-20 01:54:20 UTC",
      "updated_date": "2025-10-15 07:53:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:00:06.996869+00:00"
    },
    {
      "arxiv_id": "2509.16487v1",
      "title": "The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia",
      "title_zh": "ç¥è°•å·²é™ï¼šPythia å¯¹è¯èƒ½åŠ›çš„å¤šç»´åº¦è¯„ä¼°",
      "authors": [
        "Zixun Chen",
        "Petr Babkin",
        "Akshat Gupta",
        "Gopala Anumanchipalli",
        "Xiaomo Liu"
      ],
      "abstract": "Dialogue is one of the landmark abilities of large language models (LLMs). Despite its ubiquity, few studies actually distinguish specific ingredients underpinning dialogue behavior emerging during post-training. We employ a comprehensive suite of model-based metrics, each targeting a distinct fine-grained aspect of dialogue, motivated by linguistic theory. We evaluate how the performance of pre-trained Pythia models changes with respect to each of those dimensions, depending on model size and as a result of supervised fine-tuning on conversational datasets. We observe only a mild impact of raw model size on most metrics, whereas fine-tuning quickly saturates the scores for all but the smallest models tested. Somewhat contrary to our expectations, many metrics show very similar trends, especially if they are all rooted in the same evaluator model, which raises the question of their reliability in measuring a specific dimension. To that end, we conduct additional analyses of score distributions, metric correlations, and term frequencies in generated responses to help explain our observations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Pythia æ¨¡å‹åœ¨å¯¹è¯èƒ½åŠ›æ–¹é¢çš„ç»†ç²’åº¦ç»´åº¦è¿›è¡Œäº†å¤šæ–¹é¢è¯„ä¼°ï¼Œæ—¨åœ¨è§£æå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒåæœŸé˜¶æ®µæ¶Œç°å‡ºçš„å¯¹è¯è¡Œä¸ºã€‚ç ”ç©¶è€…åŸºäºè¯­è¨€å­¦ç†è®ºæ„å»ºäº†ä¸€å¥—ç»¼åˆçš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡(Model-based metrics)ï¼Œç³»ç»Ÿåœ°è€ƒå¯Ÿäº†æ¨¡å‹è§„æ¨¡(Model size)å’Œç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)å¯¹å¯¹è¯è´¨é‡çš„å…·ä½“å½±å“ã€‚å®éªŒè§‚å¯Ÿåˆ°åŸå§‹æ¨¡å‹è§„æ¨¡å¯¹å¤§å¤šæ•°æŒ‡æ ‡çš„å½±å“è¾ƒä¸ºæ¸©å’Œï¼Œè€Œå¾®è°ƒè¿‡ç¨‹åˆ™èƒ½ä½¿é™¤æå°æ¨¡å‹å¤–çš„æ‰€æœ‰æ¨¡å‹å¾—åˆ†è¿…é€Ÿè¾¾åˆ°é¥±å’Œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè®¸å¤šæ¤æ ¹äºåŒä¸€è¯„ä¼°å™¨æ¨¡å‹(Evaluator model)çš„æŒ‡æ ‡å±•ç°å‡ºé«˜åº¦ç›¸ä¼¼çš„è¶‹åŠ¿ï¼Œè¿™å¼•å‘äº†å¯¹å…¶è¡¡é‡ç‰¹å®šå¯¹è¯ç»´åº¦å¯é æ€§çš„è´¨ç–‘ã€‚é€šè¿‡å¯¹å¾—åˆ†åˆ†å¸ƒ(Score distributions)ã€æŒ‡æ ‡ç›¸å…³æ€§(Metric correlations)åŠç”Ÿæˆå›å¤è¯é¢‘(Term frequencies)çš„æ·±å…¥åˆ†æï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†ç°æœ‰å¯¹è¯è¯„ä¼°æ‰‹æ®µçš„å±€é™æ€§å¹¶ä¸ºç†è§£å¤§æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›æä¾›äº†å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16487v1",
      "published_date": "2025-09-20 01:11:10 UTC",
      "updated_date": "2025-09-20 01:11:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:00:07.789258+00:00"
    },
    {
      "arxiv_id": "2509.16479v1",
      "title": "Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture",
      "title_zh": "åŸºäºè¿åŠ¨æµä¸æ³¨æ„åŠ›å¢å¼ºå·ç§¯å¾ªç¯æ¶æ„çš„çƒ­æˆåƒå®æ—¶è·Œå€’æ£€æµ‹",
      "authors": [
        "Christopher Silver",
        "Thangarajah Akilan"
      ],
      "abstract": "Falls among seniors are a major public health issue. Existing solutions using wearable sensors, ambient sensors, and RGB-based vision systems face challenges in reliability, user compliance, and practicality. Studies indicate that stakeholders, such as older adults and eldercare facilities, prefer non-wearable, passive, privacy-preserving, and real-time fall detection systems that require no user interaction. This study proposes an advanced thermal fall detection method using a Bidirectional Convolutional Long Short-Term Memory (BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general attention mechanisms. Through systematic experimentation across hundreds of model variations exploring the integration of attention mechanisms, recurrent modules, and motion flow, we identified top-performing architectures. Among them, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of $99.7\\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly emerged, diverse, and privacy-preserving benchmark. These results highlight the generalizability and practicality of the proposed model, setting new standards for thermal fall detection and paving the way toward deployable, high-performance solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è€å¹´äººè·Œå€’æ£€æµ‹ä¸­ç©¿æˆ´å¼è®¾å¤‡åŠ RGB è§†è§‰ç³»ç»Ÿåœ¨éšç§ä¿æŠ¤ä¸ç”¨æˆ·ä¾ä»æ€§æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åŸºäºçƒ­æˆåƒ (Thermal Imaging) çš„å®æ—¶æ£€æµ‹æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨å¢å¼ºå‹çš„åŒå‘å·ç§¯é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (BiConvLSTM) æ¶æ„ï¼Œæ·±åº¦èåˆäº†ç©ºé—´ã€æ—¶é—´ã€ç‰¹å¾ã€è‡ªæ³¨æ„åŠ›åŠé€šç”¨æ³¨æ„åŠ›æœºåˆ¶ (Attention mechanisms) ä»¥åŠè¿åŠ¨æµ (Motion Flow) ä¿¡æ¯ã€‚é€šè¿‡å¯¹æ•°ç™¾ç§æ¨¡å‹å˜ä½“è¿›è¡Œç³»ç»Ÿæ€§å®éªŒï¼Œç ”ç©¶è¯†åˆ«å‡ºäº†æ€§èƒ½æœ€ä¼˜çš„æ¶æ„é…ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBiConvLSTM æ¨¡å‹åœ¨ TSF æ•°æ®é›†ä¸Šå®ç°äº† 99.7% çš„ ROC-AUCï¼Œå¹¶åœ¨æ–°å‹éšç§ä¿æŠ¤åŸºå‡†æ•°æ®é›† TF-66 ä¸Šå±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚è¯¥ç ”ç©¶ä¸ºçƒ­æˆåƒè·Œå€’æ£€æµ‹ç¡®ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡å‡†ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸å®ç”¨æ€§ï¼Œä¸ºé«˜æ€§èƒ½ä¸”ä¿æŠ¤éšç§çš„è·Œå€’è¾…åŠ©ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16479v1",
      "published_date": "2025-09-20 00:29:43 UTC",
      "updated_date": "2025-09-20 00:29:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:00:10.188828+00:00"
    },
    {
      "arxiv_id": "2510.24721v1",
      "title": "The Epistemic Suite: A Post-Foundational Diagnostic Methodology for Assessing AI Knowledge Claims",
      "title_zh": "Epistemic Suiteï¼šè¯„ä¼°äººå·¥æ™ºèƒ½çŸ¥è¯†ä¸»å¼ çš„ååŸºç¡€ä¸»ä¹‰è¯Šæ–­æ–¹æ³•è®º",
      "authors": [
        "Matthew Kelly"
      ],
      "abstract": "Large Language Models (LLMs) generate fluent, plausible text that can mislead users into mistaking simulated coherence for genuine understanding. This paper introduces the Epistemic Suite, a post-foundational diagnostic methodology for surfacing the epistemic conditions under which AI outputs are produced and received. Rather than determining truth or falsity, the Suite operates through twenty diagnostic lenses, applied by practitioners as context warrants, to reveal patterns such as confidence laundering, narrative compression, displaced authority, and temporal drift. It is grounded in three design principles: diagnosing production before evaluating claims, preferring diagnostic traction over foundational settlement, and embedding reflexivity as a structural requirement rather than an ethical ornament. When enacted, the Suite shifts language models into a diagnostic stance, producing inspectable artifacts-flags, annotations, contradiction maps, and suspension logs (the FACS bundle)-that create an intermediary layer between AI output and human judgment. A key innovation is epistemic suspension, a practitioner-enacted circuit breaker that halts continuation when warrant is exceeded, with resumption based on judgment rather than rule. The methodology also includes an Epistemic Triage Protocol and a Meta-Governance Layer to manage proportionality and link activation to relational accountability, consent, historical context, and pluralism safeguards. Unlike internalist approaches that embed alignment into model architectures (e.g., RLHF or epistemic-integrity proposals), the Suite operates externally as scaffolding, preserving expendability and refusal as safeguards rather than failures. It preserves the distinction between performance and understanding, enabling accountable deliberation while maintaining epistemic modesty.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Epistemic Suiteï¼Œè¿™æ˜¯ä¸€ç§ååŸºç¡€ä¸»ä¹‰(post-foundational)è¯Šæ–­æ–¹æ³•è®ºï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„çŸ¥è¯†å£°æ˜å¹¶æ­ç¤ºå…¶èƒŒåçš„è®¤çŸ¥æ¡ä»¶ã€‚è¯¥å·¥å…·åŒ…é€šè¿‡ 20 ä¸ªè¯Šæ–­è§†è§’ï¼ˆå¦‚ confidence laundering, narrative compression å’Œ temporal drift ç­‰ï¼‰è¯†åˆ« AI è¾“å‡ºçš„æ¨¡å¼ï¼Œè€Œéå•çº¯åˆ¤å®šå…¶é™ˆè¿°çš„çœŸä¼ªã€‚å…¶è®¾è®¡éµå¾ªåœ¨è¯„ä¼°å£°æ˜å‰è¯Šæ–­ç”Ÿäº§è¿‡ç¨‹ã€è¿½æ±‚è¯Šæ–­ç‰µå¼•åŠ›ä»¥åŠå°†åæ€æ€§(reflexivity)ç»“æ„åŒ–ç­‰åŸåˆ™ï¼Œå¹¶ç”ŸæˆåŒ…å« flags, annotations, contradiction maps å’Œ suspension logs çš„ FACS bundle ä½œä¸ºä¸­é—´æ£€æŸ¥å±‚ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°æ˜¯è®¤çŸ¥æ‚¬ç½®(epistemic suspension)ï¼Œå…è®¸ä»ä¸šè€…åœ¨ AI è®ºæ®ä¸è¶³æ—¶ä¸­æ–­ç”Ÿæˆï¼Œç¡®ä¿æ¢å¤è¿è¡ŒåŸºäºäººä¸ºåˆ¤æ–­è€Œéé¢„è®¾è§„åˆ™ã€‚ä¸ RLHF ç­‰å†…éƒ¨å¯¹é½æ¶æ„ä¸åŒï¼ŒEpistemic Suite ä½œä¸ºå¤–éƒ¨è„šæ‰‹æ¶(scaffolding)è¿è¡Œï¼Œé€šè¿‡ä¿ç•™æ‹’ç»æƒå’Œè®¤çŸ¥è°¦é€Š(epistemic modesty)æ¥é˜²æ­¢ç”¨æˆ·å°†æ¨¡æ‹Ÿçš„è¿è´¯æ€§è¯¯è®¤ä¸ºçœŸå®çš„ç†è§£ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "65 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.24721v1",
      "published_date": "2025-09-20 00:29:38 UTC",
      "updated_date": "2025-09-20 00:29:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:00:19.777504+00:00"
    },
    {
      "arxiv_id": "2509.19376v1",
      "title": "Solving Freshness in RAG: A Simple Recency Prior and the Limits of Heuristic Trend Detection",
      "title_zh": "è§£å†³RAGä¸­çš„æ—¶æ•ˆæ€§é—®é¢˜ï¼šç®€å•çš„è¿‘æ—¶å…ˆéªŒä¸å¯å‘å¼è¶‹åŠ¿æ£€æµ‹çš„å±€é™æ€§",
      "authors": [
        "Matthew Grofsky"
      ],
      "abstract": "We address temporal failures in RAG systems using two methods on cybersecurity data. A simple recency prior achieved an accuracy of 1.00 on freshness tasks. In contrast, a clustering heuristic for topic evolution failed (0.08 F1-score), showing trend detection requires methods beyond simple heuristics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿä¸­çš„æ—¶æ•ˆæ€§å¤±æ•ˆé—®é¢˜ï¼Œé€šè¿‡ç½‘ç»œå®‰å…¨æ•°æ®è¯„ä¼°äº†ä¸¤ç§ä¸åŒçš„åº”å¯¹ç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼Œç®€å•çš„è¿‘æ—¶æ€§å…ˆéªŒ(Recency Prior)åœ¨å¤„ç†ä¿¡æ¯æ–°é²œåº¦(Freshness)ä»»åŠ¡æ—¶è¡¨ç°å“è¶Šï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†1.00ã€‚ç„¶è€Œï¼Œç”¨äºè¯é¢˜æ¼”åŒ–(Topic Evolution)æ£€æµ‹çš„èšç±»å¯å‘å¼(Clustering Heuristic)æ–¹æ³•æ•ˆæœæå·®ï¼Œå…¶F1-scoreä»…ä¸º0.08ã€‚è¿™ä¸€å¯¹æ¯”ç»“æœæ¸…æ™°åœ°å±•ç¤ºäº†ç®€å•å¯å‘å¼æ‰‹æ®µåœ¨è¶‹åŠ¿æ£€æµ‹(Trend Detection)æ–¹é¢çš„å±€é™æ€§ã€‚ä½œè€…æœ€åæŒ‡å‡ºï¼Œä¸ºäº†æœ‰æ•ˆè§£å†³RAGç³»ç»Ÿä¸­çš„æ—¶é—´å¤±æ•ˆé—®é¢˜ï¼Œå¿…é¡»å¼€å‘è¶…è¶ŠåŸºç¡€å¯å‘å¼ç®—æ³•çš„æ›´é«˜çº§æ¨¡å‹å’Œæ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19376v1",
      "published_date": "2025-09-20 00:19:37 UTC",
      "updated_date": "2025-09-20 00:19:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:00:28.474392+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 70,
  "processed_papers_count": 70,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T20:01:47.995189+00:00"
}
[
  {
    "arxiv_id": "2505.18901v2",
    "title": "PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models",
    "authors": [
      "Xiaoyan Hu",
      "Lauren Pick",
      "Ho-fung Leung",
      "Farzan Farnia"
    ],
    "abstract": "The rapid advancement of generative AI has provided users with a wide range of well-trained models to address diverse prompts. When selecting a model for a given prompt, users should weigh not only its performance but also its service cost. However, existing model-selection methods typically emphasize performance while overlooking cost differences. In this paper, we introduce PromptWise, an online learning framework that assigns prompts to generative models in a cost-aware manner. PromptWise estimates prompt-model compatibility to select the least expensive model expected to deliver satisfactory outputs. Unlike standard contextual bandits that make a one-shot decision per prompt, PromptWise employs a cost-aware bandit structure that allows sequential model assignments per prompt to reduce total service cost. Through numerical experiments on tasks such as code generation and translation, we demonstrate that PromptWise can achieve performance comparable to baseline selection methods while incurring substantially lower costs. The code is available at: github.com/yannxiaoyanhu/PromptWise.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "39 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.18901v2",
    "published_date": "2025-05-24 23:26:33 UTC",
    "updated_date": "2025-11-02 01:07:49 UTC"
  },
  {
    "arxiv_id": "2505.20343v1",
    "title": "Do LLMs have a Gender (Entropy) Bias?",
    "authors": [
      "Sonal Prabhune",
      "Balaji Padmanabhan",
      "Kaushik Dutta"
    ],
    "abstract": "We investigate the existence and persistence of a specific type of gender bias in some of the popular LLMs and contribute a new benchmark dataset, RealWorldQuestioning (released on HuggingFace ), developed from real-world questions across four key domains in business and health contexts: education, jobs, personal financial management, and general health. We define and study entropy bias, which we define as a discrepancy in the amount of information generated by an LLM in response to real questions users have asked. We tested this using four different LLMs and evaluated the generated responses both qualitatively and quantitatively by using ChatGPT-4o (as \"LLM-as-judge\"). Our analyses (metric-based comparisons and \"LLM-as-judge\" evaluation) suggest that there is no significant bias in LLM responses for men and women at a category level. However, at a finer granularity (the individual question level), there are substantial differences in LLM responses for men and women in the majority of cases, which \"cancel\" each other out often due to some responses being better for males and vice versa. This is still a concern since typical users of these tools often ask a specific question (only) as opposed to several varied ones in each of these common yet important areas of life. We suggest a simple debiasing approach that iteratively merges the responses for the two genders to produce a final result. Our approach demonstrates that a simple, prompt-based debiasing strategy can effectively debias LLM outputs, thus producing responses with higher information content than both gendered variants in 78% of the cases, and consistently achieving a balanced integration in the remaining cases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20343v1",
    "published_date": "2025-05-24 23:06:41 UTC",
    "updated_date": "2025-05-24 23:06:41 UTC"
  },
  {
    "arxiv_id": "2505.18897v1",
    "title": "Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance tuning",
    "authors": [
      "Dipanwita Saha",
      "Anis Zaman",
      "Hua Zou",
      "Ning Chen",
      "Xinxin Shu",
      "Nadia Vase",
      "Abraham Bagherjeiran"
    ],
    "abstract": "In search advertising, keyword matching connects user queries with relevant ads. While token-based matching increases ad coverage, it can reduce relevance due to overly permissive semantic expansion. This work extends keyword reach through document-side semantic keyword expansion, using a language model to broaden token-level matching without altering queries. We propose a solution using a pre-trained siamese model to generate dense vector representations of ad keywords and identify semantically related variants through nearest neighbor search. To maintain precision, we introduce a cluster-based thresholding mechanism that adjusts similarity cutoffs based on local semantic density. Each expanded keyword maps to a group of seller-listed items, which may only partially align with the original intent. To ensure relevance, we enhance the downstream relevance model by adapting it to the expanded keyword space using an incremental learning strategy with a lightweight decision tree ensemble. This system improves both relevance and click-through rate (CTR), offering a scalable, low-latency solution adaptable to evolving query behavior and advertising inventory.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18897v1",
    "published_date": "2025-05-24 23:02:19 UTC",
    "updated_date": "2025-05-24 23:02:19 UTC"
  },
  {
    "arxiv_id": "2505.18894v1",
    "title": "Digital Overconsumption and Waste: A Closer Look at the Impacts of Generative AI",
    "authors": [
      "Vanessa Utz",
      "Steve DiPaola"
    ],
    "abstract": "Generative Artificial Intelligence (AI) systems currently contribute negatively to the production of digital waste, via the associated energy consumption and the related CO2 emissions. At this moment, a discussion is urgently needed on the replication of harmful consumer behavior, such as overconsumption, in the digital space. We outline our previous work on the climate implications of commercially available generative AI systems and the sentiment of generative AI users when confronted with AI-related climate research. We expand on this work via a discussion of digital overconsumption and waste, other related societal impacts, and a possible solution pathway",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Conference on Computer Vision and Pattern Recognition (CVPR) 2023. Ethical Considerations in Creative Applications of Computer Vision (EC3V) Workshop",
    "pdf_url": "https://arxiv.org/pdf/2505.18894v1",
    "published_date": "2025-05-24 22:40:08 UTC",
    "updated_date": "2025-05-24 22:40:08 UTC"
  },
  {
    "arxiv_id": "2505.21547v1",
    "title": "Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing",
    "authors": [
      "Weixing Wang",
      "Zifeng Ding",
      "Jindong Gu",
      "Rui Cao",
      "Christoph Meinel",
      "Gerard de Melo",
      "Haojin Yang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) with discrete image tokenizers unify multimodal representations by encoding visual inputs into a finite set of tokens. Despite their effectiveness, we find that these models still hallucinate non-existent objects. We hypothesize that this may be due to visual priors induced during training: When certain image tokens frequently co-occur in the same spatial regions and represent shared objects, they become strongly associated with the verbalizations of those objects. As a result, the model may hallucinate by evoking visually absent tokens that often co-occur with present ones. To test this assumption, we construct a co-occurrence graph of image tokens using a segmentation dataset and employ a Graph Neural Network (GNN) with contrastive learning followed by a clustering method to group tokens that frequently co-occur in similar visual contexts. We find that hallucinations predominantly correspond to clusters whose tokens dominate the input, and more specifically, that the visually absent tokens in those clusters show much higher correlation with hallucinated objects compared to tokens present in the image. Based on this observation, we propose a hallucination mitigation method that suppresses the influence of visually absent tokens by modifying latent image embeddings during generation. Experiments show our method reduces hallucinations while preserving expressivity. Code is available at https://github.com/weixingW/CGC-VTD/tree/main",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21547v1",
    "published_date": "2025-05-24 22:36:15 UTC",
    "updated_date": "2025-05-24 22:36:15 UTC"
  },
  {
    "arxiv_id": "2505.18893v4",
    "title": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects",
    "authors": [
      "Reva Schwartz",
      "Rumman Chowdhury",
      "Akash Kundu",
      "Heather Frase",
      "Marzieh Fadaee",
      "Tom David",
      "Gabriella Waters",
      "Afaf Taik",
      "Morgan Briggs",
      "Patrick Hall",
      "Shomik Jain",
      "Kyra Yee",
      "Spencer Thomas",
      "Sundeep Bhandari",
      "Paul Duncan",
      "Andrew Thompson",
      "Maya Carlyle",
      "Qinghua Lu",
      "Matthew Holmes",
      "Theodora Skeadas"
    ],
    "abstract": "Conventional AI evaluation approaches concentrated within the AI stack exhibit systemic limitations for exploring, navigating and resolving the human and societal factors that play out in real world deployment such as in education, finance, healthcare, and employment sectors. AI capability evaluations can capture detail about first-order effects, such as whether immediate system outputs are accurate, or contain toxic, biased or stereotypical content, but AI's second-order effects, i.e. any long-term outcomes and consequences that may result from AI use in the real world, have become a significant area of interest as the technology becomes embedded in our daily lives. These secondary effects can include shifts in user behavior, societal, cultural and economic ramifications, workforce transformations, and long-term downstream impacts that may result from a broad and growing set of risks. This position paper argues that measuring the indirect and secondary effects of AI will require expansion beyond static, single-turn approaches conducted in silico to include testing paradigms that can capture what actually materializes when people use AI technology in context. Specifically, we describe the need for data and methods that can facilitate contextual awareness and enable downstream interpretation and decision making about AI's secondary effects, and recommend requirements for a new ecosystem.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.18893v4",
    "published_date": "2025-05-24 22:35:32 UTC",
    "updated_date": "2025-05-30 14:09:51 UTC"
  },
  {
    "arxiv_id": "2505.18892v1",
    "title": "Climate Implications of Diffusion-based Generative Visual AI Systems and their Mass Adoption",
    "authors": [
      "Vanessa Utz",
      "Steve DiPaola"
    ],
    "abstract": "Climate implications of rapidly developing digital technologies, such as blockchains and the associated crypto mining and NFT minting, have been well documented and their massive GPU energy use has been identified as a cause for concern. However, we postulate that due to their more mainstream consumer appeal, the GPU use of text-prompt based diffusion AI art systems also requires thoughtful considerations. Given the recent explosion in the number of highly sophisticated generative art systems and their rapid adoption by consumers and creative professionals, the impact of these systems on the climate needs to be carefully considered. In this work, we report on the growth of diffusion-based visual AI systems, their patterns of use, growth and the implications on the climate. Our estimates show that the mass adoption of these tools potentially contributes considerably to global energy consumption. We end this paper with our thoughts on solutions and future areas of inquiry as well as associated difficulties, including the lack of publicly available data.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "International Conference on Computational Creativity",
    "pdf_url": "https://arxiv.org/pdf/2505.18892v1",
    "published_date": "2025-05-24 22:32:56 UTC",
    "updated_date": "2025-05-24 22:32:56 UTC"
  },
  {
    "arxiv_id": "2505.18889v5",
    "title": "Security Concerns for Large Language Models: A Survey",
    "authors": [
      "Miles Q. Li",
      "Benjamin C. M. Fung"
    ],
    "abstract": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: inference-time attacks via prompt manipulation; training-time attacks; misuse by malicious actors; and the inherent risks in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze existing defense mechanisms and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18889v5",
    "published_date": "2025-05-24 22:22:43 UTC",
    "updated_date": "2025-08-24 03:15:13 UTC"
  },
  {
    "arxiv_id": "2505.18884v2",
    "title": "LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders",
    "authors": [
      "Borna Khodabandeh",
      "Amirabbas Afzali",
      "Amirhossein Afsharrad",
      "Seyed Shahabeddin Mousavi",
      "Sanjay Lall",
      "Sajjad Amini",
      "Seyed-Mohsen Moosavi-Dezfooli"
    ],
    "abstract": "Visual encoders have become fundamental components in modern computer vision pipelines. However, ensuring robustness against adversarial perturbations remains a critical challenge. Recent efforts have explored both supervised and unsupervised adversarial fine-tuning strategies. We identify two key limitations in these approaches: (i) they often suffer from instability, especially during the early stages of fine-tuning, resulting in suboptimal convergence and degraded performance on clean data, and (ii) they exhibit a suboptimal trade-off between robustness and clean data accuracy, hindering the simultaneous optimization of both objectives. To overcome these challenges, we propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised adversarial fine-tuning framework. LORE utilizes constrained optimization, which offers a principled approach to balancing competing goals, such as improving robustness while preserving nominal performance. By enforcing embedding-space proximity constraints, LORE effectively maintains clean data performance throughout adversarial fine-tuning. Extensive experiments show that LORE significantly improves zero-shot adversarial robustness with minimal degradation in clean data accuracy. Furthermore, we demonstrate the effectiveness of the adversarially fine-tuned CLIP image encoder in out-of-distribution generalization and enhancing the interpretability of image embeddings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18884v2",
    "published_date": "2025-05-24 21:54:52 UTC",
    "updated_date": "2025-11-29 10:41:35 UTC"
  },
  {
    "arxiv_id": "2505.18881v1",
    "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes",
    "authors": [
      "Dicong Qiu",
      "Jiadi You",
      "Zeying Gong",
      "Ronghe Qiu",
      "Hui Xiong",
      "Junwei Liang"
    ],
    "abstract": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for Open-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes pretraining multimodal foundation models to generate infinite unique photo-realistic scene variants that adhere to real-world semantics and daily commonsense for the training and the evaluation of navigation agents, accompanied with a plugin for generating object navigation task episodes compatible to the Habitat simulator. In addition, we offer two pre-generated object navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising respectively about 3k and 10k episodes of the open-vocabulary object navigation task, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans of real-world environments and the SD-OVON-Objects dataset with 0.9k manually inspected scanned and artist-created manipulatable object models. Unlike prior datasets limited to static environments, SD-OVON covers dynamic scenes and manipulatable objects, facilitating both real-to-sim and sim-to-real robotic applications. This approach enhances the realism of navigation tasks, the training and the evaluation of open-vocabulary object navigation agents in complex settings. To demonstrate the effectiveness of our pipeline and datasets, we propose two baselines and evaluate them along with state-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source code are publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. 21 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.18881v1",
    "published_date": "2025-05-24 21:37:06 UTC",
    "updated_date": "2025-05-24 21:37:06 UTC"
  },
  {
    "arxiv_id": "2505.18880v1",
    "title": "REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing",
    "authors": [
      "Weihan Xu",
      "Yimeng Ma",
      "Jingyue Huang",
      "Yang Li",
      "Wenye Ma",
      "Taylor Berg-Kirkpatrick",
      "Julian McAuley",
      "Paul Pu Liang",
      "Hao-Wen Dong"
    ],
    "abstract": "Short videos are an effective tool for promoting contents and improving knowledge accessibility. While existing extractive video summarization methods struggle to produce a coherent narrative, existing abstractive methods cannot `quote' from the input videos, i.e., inserting short video clips in their outputs. In this work, we explore novel video editing models for generating shorts that feature a coherent narrative with embedded video insertions extracted from a long input video. We propose a novel retrieval-embedded generation framework that allows a large language model to quote multimodal resources while maintaining a coherent narrative. Our proposed REGen system first generates the output story script with quote placeholders using a finetuned large language model, and then uses a novel retrieval model to replace the quote placeholders by selecting a video clip that best supports the narrative from a pool of candidate quotable video clips. We examine the proposed method on the task of documentary teaser generation, where short interview insertions are commonly used to support the narrative of a documentary. Our objective evaluations show that the proposed method can effectively insert short video clips while maintaining a coherent narrative. In a subjective survey, we show that our proposed method outperforms existing abstractive and extractive approaches in terms of coherence, alignment, and realism in teaser generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18880v1",
    "published_date": "2025-05-24 21:36:49 UTC",
    "updated_date": "2025-05-24 21:36:49 UTC"
  },
  {
    "arxiv_id": "2505.18878v1",
    "title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions",
    "authors": [
      "Kung-Hsiang Huang",
      "Akshara Prabhakar",
      "Onkar Thorat",
      "Divyansh Agarwal",
      "Prafulla Kumar Choubey",
      "Yixin Mao",
      "Silvio Savarese",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ],
    "abstract": "While AI agents hold transformative potential in business, effective performance benchmarking is hindered by the scarcity of public, realistic business data on widely used platforms. Existing benchmarks often lack fidelity in their environments, data, and agent-user interactions, with limited coverage of diverse business scenarios and industries. To address these gaps, we introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena with nineteen expert-validated tasks across sales, service, and 'configure, price, and quote' processes, for both Business-to-Business and Business-to-Customer scenarios. It distinctively incorporates multi-turn interactions guided by diverse personas and robust confidentiality awareness assessments. Experiments reveal leading LLM agents achieve only around 58% single-turn success on CRMArena-Pro, with performance dropping significantly to approximately 35% in multi-turn settings. While Workflow Execution proves more tractable for top agents (over 83% single-turn success), other evaluated business skills present greater challenges. Furthermore, agents exhibit near-zero inherent confidentiality awareness; though targeted prompting can improve this, it often compromises task performance. These findings highlight a substantial gap between current LLM capabilities and enterprise demands, underscoring the need for advancements in multi-turn reasoning, confidentiality adherence, and versatile skill acquisition.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18878v1",
    "published_date": "2025-05-24 21:33:22 UTC",
    "updated_date": "2025-05-24 21:33:22 UTC"
  },
  {
    "arxiv_id": "2505.18859v1",
    "title": "Writing Like the Best: Exemplar-Based Expository Text Generation",
    "authors": [
      "Yuxiang Liu",
      "Kevin Chen-Chuan Chang"
    ],
    "abstract": "We introduce the Exemplar-Based Expository Text Generation task, aiming to generate an expository text on a new topic using an exemplar on a similar topic. Current methods fall short due to their reliance on extensive exemplar data, difficulty in adapting topic-specific content, and issues with long-text coherence. To address these challenges, we propose the concept of Adaptive Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA leverages large language models (LLMs) for effective adaptive imitation through a fine-grained plan-then-adapt process. RePA also enables recurrent segment-by-segment imitation, supported by two memory structures that enhance input clarity and output coherence. We also develop task-specific evaluation metrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as evaluators. Experimental results across our collected three diverse datasets demonstrate that RePA surpasses existing baselines in producing factual, consistent, and relevant texts for this task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025. Camera-ready version",
    "pdf_url": "https://arxiv.org/pdf/2505.18859v1",
    "published_date": "2025-05-24 20:40:39 UTC",
    "updated_date": "2025-05-24 20:40:39 UTC"
  },
  {
    "arxiv_id": "2506.13970v1",
    "title": "Making deep neural networks work for medical audio: representation, compression and domain adaptation",
    "authors": [
      "Charles C Onu"
    ],
    "abstract": "This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.\n  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.\n  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "PhD Thesis",
    "pdf_url": "https://arxiv.org/pdf/2506.13970v1",
    "published_date": "2025-05-24 20:32:31 UTC",
    "updated_date": "2025-05-24 20:32:31 UTC"
  },
  {
    "arxiv_id": "2505.18857v1",
    "title": "Hierarchical-embedding autoencoder with a predictor (HEAP) as efficient architecture for learning long-term evolution of complex multi-scale physical systems",
    "authors": [
      "Alexander Khrabry",
      "Edward Startsev",
      "Andrew Powis",
      "Igor Kaganovich"
    ],
    "abstract": "We propose a novel efficient architecture for learning long-term evolution in complex multi-scale physical systems which is based on the idea of separation of scales. Structures of various scales that dynamically emerge in the system interact with each other only locally. Structures of similar scale can interact directly when they are in contact and indirectly when they are parts of larger structures that interact directly. This enables modeling a multi-scale system in an efficient way, where interactions between small-scale features that are apart from each other do not need to be modeled. The hierarchical fully-convolutional autoencoder transforms the state of a physical system not just into a single embedding layer, as it is done conventionally, but into a series of embedding layers which encode structures of various scales preserving spatial information at a corresponding resolution level. Shallower layers embed smaller structures on a finer grid, while deeper layers embed larger structures on a coarser grid. The predictor advances all embedding layers in sync. Interactions between features of various scales are modeled using a combination of convolutional operators. We compare the performance of our model to variations of a conventional ResNet architecture in application to the Hasegawa-Wakatani turbulence. A multifold improvement in long-term prediction accuracy was observed for crucial statistical characteristics of this system.",
    "categories": [
      "cs.AI",
      "physics.plasm-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18857v1",
    "published_date": "2025-05-24 20:27:16 UTC",
    "updated_date": "2025-05-24 20:27:16 UTC"
  },
  {
    "arxiv_id": "2505.18850v1",
    "title": "The Theory of the Unique Latent Pattern: A Formal Epistemic Framework for Structural Singularity in Complex Systems",
    "authors": [
      "Mohamed Aly Bouke"
    ],
    "abstract": "This paper introduces the Theory of the Unique Latent Pattern (ULP), a formal epistemic framework that redefines the origin of apparent complexity in dynamic systems. Rather than attributing unpredictability to intrinsic randomness or emergent nonlinearity, ULP asserts that every analyzable system is governed by a structurally unique, deterministic generative mechanism, one that remains hidden not due to ontological indeterminacy, but due to epistemic constraints. The theory is formalized using a non-universal generative mapping \\( \\mathcal{F}_S(P_S, t) \\), where each system \\( S \\) possesses its own latent structure \\( P_S \\), irreducible and non-replicable across systems. Observed irregularities are modeled as projections of this generative map through observer-limited interfaces, introducing epistemic noise \\( \\varepsilon_S(t) \\) as a measure of incomplete access. By shifting the locus of uncertainty from the system to the observer, ULP reframes chaos as a context-relative failure of representation. We contrast this position with foundational paradigms in chaos theory, complexity science, and statistical learning. While they assume or model shared randomness or collective emergence, ULP maintains that every instance harbors a singular structural identity. Although conceptual, the theory satisfies the criterion of falsifiability in the Popperian sense, it invites empirical challenge by asserting that no two systems governed by distinct latent mechanisms will remain indistinguishable under sufficient resolution. This opens avenues for structurally individuated models in AI, behavioral inference, and epistemic diagnostics.",
    "categories": [
      "cs.AI",
      "math.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18850v1",
    "published_date": "2025-05-24 19:52:28 UTC",
    "updated_date": "2025-05-24 19:52:28 UTC"
  },
  {
    "arxiv_id": "2506.17230v2",
    "title": "MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving",
    "authors": [
      "Yichen Luo",
      "Jia Wang",
      "Dapeng Lan",
      "Yu Liu",
      "Zhibo Pang"
    ],
    "abstract": "Partial Differential Equations (PDEs) are fundamental for modeling physical systems, yet solving them in a generic and efficient manner using machine learning-based approaches remains challenging due to limited multi-input and multi-scale generalization capabilities, as well as high computational costs. This paper proposes the Multi-input and Multi-scale Efficient Transformer (MMET), a novel framework designed to address the above challenges. MMET decouples mesh and query points as two sequences and feeds them into the encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE) layer to embed input variables or functions with varying dimensions, enabling effective solutions for multi-scale and multi-input problems. Additionally, a Hilbert curve-based reserialization and patch embedding mechanism decrease the input length. This significantly reduces the computational cost when dealing with large-scale geometric models. These innovations enable efficient representations and support multi-scale resolution queries for large-scale and multi-input PDE problems. Experimental evaluations on diverse benchmarks spanning different physical fields demonstrate that MMET outperforms SOTA methods in both accuracy and computational efficiency. This work highlights the potential of MMET as a robust and scalable solution for real-time PDE solving in engineering and physics-based applications, paving the way for future explorations into pre-trained large-scale models in specific domains. This work is open-sourced at https://github.com/YichenLuo-0/MMET.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17230v2",
    "published_date": "2025-05-24 19:50:11 UTC",
    "updated_date": "2025-08-09 08:20:15 UTC"
  },
  {
    "arxiv_id": "2505.18847v1",
    "title": "Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework",
    "authors": [
      "William Han",
      "Chaojing Duan",
      "Zhepeng Cen",
      "Yihang Yao",
      "Xiaoyu Song",
      "Atharva Mhaskar",
      "Dylan Leong",
      "Michael A. Rosenberg",
      "Emerson Liu",
      "Ding Zhao"
    ],
    "abstract": "Recent advances have increasingly applied large language models (LLMs) to electrocardiogram (ECG) interpretation, giving rise to Electrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual query, an ELM autoregressively generates a free-form textual response. Unlike traditional classification-based systems, ELMs emulate expert cardiac electrophysiologists by issuing diagnoses, analyzing waveform morphology, identifying contributing factors, and proposing patient-specific action plans. To realize this potential, researchers are curating instruction-tuning datasets that pair ECGs with textual dialogues and are training ELMs on these resources. Yet before scaling ELMs further, there is a fundamental question yet to be explored: What is the most effective ECG input representation? In recent works, three candidate representations have emerged-raw time-series signals, rendered images, and discretized symbolic sequences. We present the first comprehensive benchmark of these modalities across 6 public datasets and 5 evaluation metrics. We find symbolic representations achieve the greatest number of statistically significant wins over both signal and image inputs. We further ablate the LLM backbone, ECG duration, and token budget, and we evaluate robustness to signal perturbations. We hope that our findings offer clear guidance for selecting input representations when developing the next generation of ELMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 2 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.18847v1",
    "published_date": "2025-05-24 19:43:15 UTC",
    "updated_date": "2025-05-24 19:43:15 UTC"
  },
  {
    "arxiv_id": "2505.20342v1",
    "title": "Machine Theory of Mind and the Structure of Human Values",
    "authors": [
      "Paul de Font-Reaulx"
    ],
    "abstract": "Value learning is a crucial aspect of safe and ethical AI. This is primarily pursued by methods inferring human values from behaviour. However, humans care about much more than we are able to demonstrate through our actions. Consequently, an AI must predict the rest of our seemingly complex values from a limited sample. I call this the value generalization problem. In this paper, I argue that human values have a generative rational structure and that this allows us to solve the value generalization problem. In particular, we can use Bayesian Theory of Mind models to infer human values not only from behaviour, but also from other values. This has been obscured by the widespread use of simple utility functions to represent human values. I conclude that developing generative value-to-value inference is a crucial component of achieving a scalable machine theory of mind.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper was originally submitted and accepted to the 2023 NeurIPS MP2 Workshop",
    "pdf_url": "https://arxiv.org/pdf/2505.20342v1",
    "published_date": "2025-05-24 19:18:58 UTC",
    "updated_date": "2025-05-24 19:18:58 UTC"
  },
  {
    "arxiv_id": "2505.18829v2",
    "title": "LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS",
    "authors": [
      "Kai Mei",
      "Xi Zhu",
      "Hang Gao",
      "Shuhang Lin",
      "Yongfeng Zhang"
    ],
    "abstract": "We present AIOS 1.0, a novel platform designed to advance computer-use agent (CUA) capabilities through environmental contextualization. While existing approaches primarily focus on building more powerful agent frameworks or enhancing agent models, we identify a fundamental limitation: the semantic disconnect between how language models understand the world and how computer interfaces are structured. AIOS 1.0 addresses this challenge by transforming computers into contextual environments that language models can natively comprehend, implementing a Model Context Protocol (MCP) server architecture to abstract computer states and actions. This approach effectively decouples interface complexity from decision complexity, enabling agents to reason more effectively about computing environments. To demonstrate our platform's effectiveness, we introduce LiteCUA, a lightweight computer-use agent built on AIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark, outperforming several specialized agent frameworks despite its simple architecture. Our results suggest that contextualizing computer environments for language models represents a promising direction for developing more capable computer-use agents and advancing toward AI that can interact with digital systems.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.OS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18829v2",
    "published_date": "2025-05-24 18:56:00 UTC",
    "updated_date": "2025-10-31 21:48:48 UTC"
  },
  {
    "arxiv_id": "2505.18822v2",
    "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting",
    "authors": [
      "Shijue Huang",
      "Hongru Wang",
      "Wanjun Zhong",
      "Zhaochen Su",
      "Jiazhan Feng",
      "Bowen Cao",
      "Yi R. Fung"
    ],
    "abstract": "Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18822v2",
    "published_date": "2025-05-24 18:46:50 UTC",
    "updated_date": "2025-12-22 14:30:53 UTC"
  },
  {
    "arxiv_id": "2505.18817v2",
    "title": "High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction",
    "authors": [
      "Seongsu Kim",
      "Nayoung Kim",
      "Dongwoo Kim",
      "Sungsoo Ahn"
    ],
    "abstract": "Density functional theory (DFT) is a fundamental method for simulating quantum chemical properties, but it remains expensive due to the iterative self-consistent field (SCF) process required to solve the Kohn-Sham equations. Recently, deep learning methods are gaining attention as a way to bypass this step by directly predicting the Hamiltonian. However, they rely on deterministic regression and do not consider the highly structured nature of Hamiltonians. In this work, we propose QHFlow, a high-order equivariant flow matching framework that generates Hamiltonian matrices conditioned on molecular geometry. Flow matching models continuous-time trajectories between simple priors and complex targets, learning the structured distributions over Hamiltonians instead of direct regression. To further incorporate symmetry, we use a neural architecture that predicts SE(3)-equivariant vector fields, improving accuracy and generalization across diverse geometries. To further enhance physical fidelity, we additionally introduce a fine-tuning scheme to align predicted orbital energies with the target. QHFlow achieves state-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53% on QH9. Moreover, we further show that QHFlow accelerates the DFT process without trading off the solution quality when initializing SCF iterations with the predicted Hamiltonian, significantly reducing the number of iterations and runtime.",
    "categories": [
      "physics.comp-ph",
      "cs.AI"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "NeurIPS 2025 Spotlight",
    "pdf_url": "https://arxiv.org/pdf/2505.18817v2",
    "published_date": "2025-05-24 18:23:28 UTC",
    "updated_date": "2025-10-22 11:59:20 UTC"
  },
  {
    "arxiv_id": "2505.18807v1",
    "title": "Mitigating Deceptive Alignment via Self-Monitoring",
    "authors": [
      "Jiaming Ji",
      "Wenqi Chen",
      "Kaile Wang",
      "Donghai Hong",
      "Sitong Fang",
      "Boyuan Chen",
      "Jiayi Zhou",
      "Juntao Dai",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "abstract": "Modern large language models rely on chain-of-thought (CoT) reasoning to achieve impressive performance, yet the same mechanism can amplify deceptive alignment, situations in which a model appears aligned while covertly pursuing misaligned goals. Existing safety pipelines treat deception as a black-box output to be filtered post-hoc, leaving the model free to scheme during its internal reasoning. We ask: Can deception be intercepted while the model is thinking? We answer this question, the first framework that embeds a Self-Monitor inside the CoT process itself, named CoT Monitor+. During generation, the model produces (i) ordinary reasoning steps and (ii) an internal self-evaluation signal trained to flag and suppress misaligned strategies. The signal is used as an auxiliary reward in reinforcement learning, creating a feedback loop that rewards honest reasoning and discourages hidden goals. To study deceptive alignment systematically, we introduce DeceptionBench, a five-category benchmark that probes covert alignment-faking, sycophancy, etc. We evaluate various LLMs and show that unrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT Monitor+ cuts deceptive behaviors by 43.8% on average while preserving task accuracy. Further, when the self-monitor signal replaces an external weak judge in RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and retain transparency. Our project website can be found at cot-monitor-plus.github.io",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18807v1",
    "published_date": "2025-05-24 17:41:47 UTC",
    "updated_date": "2025-05-24 17:41:47 UTC"
  },
  {
    "arxiv_id": "2505.18799v4",
    "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models",
    "authors": [
      "Hao Chen",
      "Haoze Li",
      "Zhiqing Xiao",
      "Lirong Gao",
      "Qi Zhang",
      "Xiaomeng Hu",
      "Ningtao Wang",
      "Xing Fu",
      "Junbo Zhao"
    ],
    "abstract": "Aligning general-purpose large language models (LLMs) to downstream tasks often incurs significant training adjustment costs. Prior research has explored various avenues to enhance alignment efficiency, primarily through minimal-data training or data-driven activations to identify key attention heads. However, these approaches inherently introduce data dependency, which hinders generalization and reusability. To address this issue and enhance model alignment efficiency, we propose the Attention Localization and Pruning Strategy (ALPS), an efficient algorithm that localizes the most task-sensitive attention heads and prunes by restricting attention training updates to these heads, thereby reducing alignment costs. Experimental results demonstrate that our method activates only 10% of attention parameters during fine-tuning while achieving a 2% performance improvement over baselines on three tasks. Moreover, the identified task-specific heads are transferable across datasets and mitigate knowledge forgetting. Our work and findings provide a novel perspective on efficient LLM alignment. The code is available at https://github.com/VoiceBeer/ALPS.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted@ACL25-findings, 17 pages, 8 figures, 14 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.18799v4",
    "published_date": "2025-05-24 17:19:34 UTC",
    "updated_date": "2025-06-18 05:56:01 UTC"
  },
  {
    "arxiv_id": "2505.18787v2",
    "title": "Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation",
    "authors": [
      "Hong-Hanh Nguyen-Le",
      "Van-Tuan Tran",
      "Dinh-Thuc Nguyen",
      "Nhien-An Le-Khac"
    ],
    "abstract": "Deepfake (DF) detectors face significant challenges when deployed in real-world environments, particularly when encountering test samples deviated from training data through either postprocessing manipulations or distribution shifts. We demonstrate postprocessing techniques can completely obscure generation artifacts presented in DF samples, leading to performance degradation of DF detectors. To address these challenges, we propose Think Twice before Adaptation (\\texttt{T$^2$A}), a novel online test-time adaptation method that enhances the adaptability of detectors during inference without requiring access to source training data or labels. Our key idea is to enable the model to explore alternative options through an Uncertainty-aware Negative Learning objective rather than solely relying on its initial predictions as commonly seen in entropy minimization (EM)-based approaches. We also introduce an Uncertain Sample Prioritization strategy and Gradients Masking technique to improve the adaptation by focusing on important samples and model parameters. Our theoretical analysis demonstrates that the proposed negative learning objective exhibits complementary behavior to EM, facilitating better adaptation capability. Empirically, our method achieves state-of-the-art results compared to existing test-time adaptation (TTA) approaches and significantly enhances the resilience and generalization of DF detectors during inference. Code is available \\href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at 34th International Joint Conference on Artificial Intelligence (IJCAI-25)",
    "pdf_url": "https://arxiv.org/pdf/2505.18787v2",
    "published_date": "2025-05-24 16:58:53 UTC",
    "updated_date": "2025-06-17 22:31:49 UTC"
  },
  {
    "arxiv_id": "2505.18783v1",
    "title": "Soft Weighted Machine Unlearning",
    "authors": [
      "Xinbao Qiao",
      "Ningning Ding",
      "Yushi Cheng",
      "Meng Zhang"
    ],
    "abstract": "Machine unlearning, as a post-hoc processing technique, has gained widespread adoption in addressing challenges like bias mitigation and robustness enhancement, colloquially, machine unlearning for fairness and robustness. However, existing non-privacy unlearning-based solutions persist in using binary data removal framework designed for privacy-driven motivation, leading to significant information loss, a phenomenon known as over-unlearning. While over-unlearning has been largely described in many studies as primarily causing utility degradation, we investigate its fundamental causes and provide deeper insights in this work through counterfactual leave-one-out analysis. In this paper, we introduce a weighted influence function that assigns tailored weights to each sample by solving a convex quadratic programming problem analytically. Building on this, we propose a soft-weighted framework enabling fine-grained model adjustments to address the over-unlearning challenge. We demonstrate that the proposed soft-weighted scheme is versatile and can be seamlessly integrated into most existing unlearning algorithms. Extensive experiments show that in fairness- and robustness-driven tasks, the soft-weighted scheme significantly outperforms hard-weighted schemes in fairness/robustness metrics and alleviates the decline in utility metric, thereby enhancing machine unlearning algorithm as an effective correction solution.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages,22 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18783v1",
    "published_date": "2025-05-24 16:40:14 UTC",
    "updated_date": "2025-05-24 16:40:14 UTC"
  },
  {
    "arxiv_id": "2505.23786v3",
    "title": "Mind the Gap: A Practical Attack on GGUF Quantization",
    "authors": [
      "Kazuki Egashira",
      "Robin Staab",
      "Mark Vero",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "abstract": "With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and llama$.$cpp frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($$=$88.7\\%$), targeted content injection ($$=$85.0\\%$), and benign instruction refusal ($$=$30.1\\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.23786v3",
    "published_date": "2025-05-24 16:30:37 UTC",
    "updated_date": "2025-06-03 19:21:57 UTC"
  },
  {
    "arxiv_id": "2505.18777v3",
    "title": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation",
    "authors": [
      "Yiding Wang",
      "Fauxu Meng",
      "Xuefeng Zhang",
      "Fan Jiang",
      "Pingzhi Tang",
      "Muhan Zhang"
    ],
    "abstract": "Existing parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank subspaces, limiting their expressiveness and leading to suboptimal performance on complex tasks. To address this, we introduce High-rank Distributed PiSSA (HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters across different devices and aggregates their delta updates collectively on W for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical adapters across all devices, HD-PiSSA assigns different principal components of the pre-trained weights to each GPU, significantly expanding the range of update directions. This results in over 16x higher effective updated ranks than data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device adapter rank. Empirically, we evaluate HD-PiSSA across various challenging downstream tasks, including mathematics, code generation, and multi-task learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0 absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12 benchmarks, demonstrating its benefits from the extra optimization flexibility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18777v3",
    "published_date": "2025-05-24 16:30:13 UTC",
    "updated_date": "2025-09-26 15:49:44 UTC"
  },
  {
    "arxiv_id": "2505.18775v1",
    "title": "OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks",
    "authors": [
      "Jiayu Wang",
      "Yang Jiao",
      "Yue Yu",
      "Tianwen Qian",
      "Shaoxiang Chen",
      "Jingjing Chen",
      "Yu-Gang Jiang"
    ],
    "abstract": "Recent breakthroughs in large multimodal models (LMMs), such as the impressive GPT-4o-Native, have demonstrated remarkable proficiency in following general-purpose instructions for image generation. However, current benchmarks often lack the necessary breadth and depth to fully evaluate the diverse capabilities of these models. To overcome this limitation, we introduce OmniGenBench, a novel and comprehensive benchmark meticulously designed to assess the instruction-following abilities of state-of-the-art LMMs across both perception-centric and cognition-centric dimensions. Our OmniGenBench includes 57 diverse sub-tasks grounded in real-world scenarios, systematically categorized according to the specific model capabilities they demand. For rigorous evaluation, we further employ a dual-mode protocol. This protocol utilizes off-the-shelf visual parsing tools for perception-centric tasks and a powerful LLM-based judger for cognition-centric tasks to assess the alignment between generated images and user instructions. Using OmniGenBench, we evaluate mainstream generative models, including prevalent models like GPT-4o, Gemini-2.0-Flash, and Seedream, and provide in-depth comparisons and analyses of their performance.Code and data are available at https://github.com/emilia113/OmniGenBench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18775v1",
    "published_date": "2025-05-24 16:29:34 UTC",
    "updated_date": "2025-05-24 16:29:34 UTC"
  },
  {
    "arxiv_id": "2506.12045v1",
    "title": "From Proxies to Fields: Spatiotemporal Reconstruction of Global Radiation from Sparse Sensor Sequences",
    "authors": [
      "Kazuma Kobayashi",
      "Samrendra Roy",
      "Seid Koric",
      "Diab Abueidda",
      "Syed Bahauddin Alam"
    ],
    "abstract": "Accurate reconstruction of latent environmental fields from sparse and indirect observations is a foundational challenge across scientific domains-from atmospheric science and geophysics to public health and aerospace safety. Traditional approaches rely on physics-based simulators or dense sensor networks, both constrained by high computational cost, latency, or limited spatial coverage. We present the Temporal Radiation Operator Network (TRON), a spatiotemporal neural operator architecture designed to infer continuous global scalar fields from sequences of sparse, non-uniform proxy measurements.\n  Unlike recent forecasting models that operate on dense, gridded inputs to predict future states, TRON addresses a more ill-posed inverse problem: reconstructing the current global field from sparse, temporally evolving sensor sequences, without access to future observations or dense labels. Demonstrated on global cosmic radiation dose reconstruction, TRON is trained on 22 years of simulation data and generalizes across 65,341 spatial locations, 8,400 days, and sequence lengths from 7 to 90 days. It achieves sub-second inference with relative L2 errors below 0.1%, representing a >58,000X speedup over Monte Carlo-based estimators. Though evaluated in the context of cosmic radiation, TRON offers a domain-agnostic framework for scientific field reconstruction from sparse data, with applications in atmospheric modeling, geophysical hazard monitoring, and real-time environmental risk forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12045v1",
    "published_date": "2025-05-24 16:24:10 UTC",
    "updated_date": "2025-05-24 16:24:10 UTC"
  },
  {
    "arxiv_id": "2505.18773v3",
    "title": "Exploring the limits of strong membership inference attacks on large language models",
    "authors": [
      "Jamie Hayes",
      "Ilia Shumailov",
      "Christopher A. Choquette-Choo",
      "Matthew Jagielski",
      "George Kaissis",
      "Milad Nasr",
      "Sahra Ghalebikesabi",
      "Meenatchi Sundaram Mutu Selva Annamalai",
      "Niloofar Mireshghallah",
      "Igor Shilov",
      "Matthieu Meeus",
      "Yves-Alexandre de Montjoye",
      "Katherine Lee",
      "Franziska Boenisch",
      "Adam Dziedzic",
      "A. Feder Cooper"
    ],
    "abstract": "State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training references (e.g., fine-tuning attacks), or on stronger attacks applied to small models and datasets. However, weaker attacks have been shown to be brittle and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges prompt an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA--one of the strongest MIAs--to GPT-2 architectures ranging from 10M to 1B parameters, training references on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in four key ways. While (1) strong MIAs can succeed on pre-trained LLMs, (2) their effectiveness, remains limited (e.g., AUC<0.7) in practical settings. (3) Even when strong MIAs achieve better-than-random AUC, aggregate metrics can conceal substantial per-sample MIA decision instability: due to training randomness, many decisions are so unstable that they are statistically indistinguishable from a coin flip. Finally, (4) the relationship between MIA success and related LLM privacy metrics is not as straightforward as prior work has suggested.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18773v3",
    "published_date": "2025-05-24 16:23:43 UTC",
    "updated_date": "2026-01-08 02:25:04 UTC"
  },
  {
    "arxiv_id": "2506.12044v2",
    "title": "Why Do Some Inputs Break Low-Bit LLM Quantization?",
    "authors": [
      "Ting-Yun Chang",
      "Muru Zhang",
      "Jesse Thomason",
      "Robin Jia"
    ],
    "abstract": "Low-bit weight-only quantization significantly reduces the memory footprint of large language models (LLMs), but disproportionately affects certain examples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in size and find that the quantization errors of 50 pairs of methods are strongly correlated (avg. 0.82) on FineWeb examples. Moreover, the residual stream magnitudes of full-precision models are indicative of future quantization errors. We further establish a hypothesis that relates the residual stream magnitudes to error amplification and accumulation over layers. Using LLM localization techniques, early exiting, and activation patching, we show that examples with large errors rely on precise residual activations in the late layers, and that the outputs of MLP gates play a crucial role in maintaining the perplexity. Our work reveals why certain examples result in large quantization errors and which model components are most critical for performance preservation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.12044v2",
    "published_date": "2025-05-24 16:17:50 UTC",
    "updated_date": "2025-09-24 12:57:03 UTC"
  },
  {
    "arxiv_id": "2505.20341v1",
    "title": "Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset",
    "authors": [
      "Rui Liu",
      "Pu Gao",
      "Jiatian Xi",
      "Berrak Sisman",
      "Carlos Busso",
      "Haizhou Li"
    ],
    "abstract": "Text-based speech editing (TSE) modifies speech using only text, eliminating re-recording. However, existing TSE methods, mainly focus on the content accuracy and acoustic consistency of synthetic speech segments, and often overlook the emotional shifts or inconsistency issues introduced by text changes. To address this issue, we propose EmoCorrector, a novel post-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented Generation (RAG) by extracting the edited text's emotional features, retrieving speech samples with matching emotions, and synthesizing speech that aligns with the desired emotion while preserving the speaker's identity and quality. To support the training and evaluation of emotional consistency modeling in TSE, we pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The prominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data featuring diverse text variations and a range of emotional expressions. Subjective and objective experiments and comprehensive analysis on ECD-TSE confirm that EmoCorrector significantly enhances the expression of intended emotion while addressing emotion inconsistency limitations in current TSE methods. Code and audio examples are available at https://github.com/AI-S2-Lab/EmoCorrector.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "INTERSPEECH2025. Code and audio examples: https://github.com/AI-S2-Lab/EmoCorrector",
    "pdf_url": "https://arxiv.org/pdf/2505.20341v1",
    "published_date": "2025-05-24 16:10:56 UTC",
    "updated_date": "2025-05-24 16:10:56 UTC"
  },
  {
    "arxiv_id": "2505.18766v2",
    "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations",
    "authors": [
      "Yanjie Li",
      "Wenxuan Zhang",
      "Xinqi Lyu",
      "Yihao Liu",
      "Bin Xiao"
    ],
    "abstract": "Recently, text-to-image diffusion models have been widely used for style mimicry and personalized customization through methods such as DreamBooth and Textual Inversion. This has raised concerns about intellectual property protection and the generation of deceptive content. Recent studies, such as Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect images from these attacks. However, recent purification-based methods, such as DiffPure and Noise Upscaling, have successfully attacked these latest defenses, showing the vulnerabilities of these methods. Moreover, present methods show limited transferability across models, making them less effective against unknown text-to-image models. To address these issues, we propose a novel anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes the style-related features in the latent space to make it deviate from the original image, which improves model-agnostic transferability. Additionally, to enhance the perturbation's ability to bypass diffusion-based purification, we designed a novel upscale loss that involves ensemble purifiers and upscalers during training. Extensive experiments on the WikiArt and CelebA datasets demonstrate that StyleGuard outperforms existing methods in robustness against various transformations and purifications, effectively countering style mimicry in various models. Moreover, StyleGuard is effective on different style mimicry methods, including DreamBooth and Textual Inversion. The code is available at https://github.com/PolyLiYJ/StyleGuard.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NIPS2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18766v2",
    "published_date": "2025-05-24 16:09:26 UTC",
    "updated_date": "2025-10-30 13:19:55 UTC"
  },
  {
    "arxiv_id": "2505.21541v3",
    "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers",
    "authors": [
      "Zitong Wang",
      "Hang Zhao",
      "Qianyu Zhou",
      "Xuequan Lu",
      "Xiangtai Li",
      "Yiren Song"
    ],
    "abstract": "Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21541v3",
    "published_date": "2025-05-24 16:08:04 UTC",
    "updated_date": "2025-09-01 09:14:35 UTC"
  },
  {
    "arxiv_id": "2505.18762v1",
    "title": "Towards an automatic method for generating topical vocabulary test forms for specific reading passages",
    "authors": [
      "Michael Flor",
      "Zuowei Wang",
      "Paul Deane",
      "Tenaha O'Reilly"
    ],
    "abstract": "Background knowledge is typically needed for successful comprehension of topical and domain specific reading passages, such as in the STEM domain. However, there are few automated measures of student knowledge that can be readily deployed and scored in time to make predictions on whether a given student will likely be able to understand a specific content area text. In this paper, we present our effort in developing K-tool, an automated system for generating topical vocabulary tests that measure students' background knowledge related to a specific text. The system automatically detects the topic of a given text and produces topical vocabulary items based on their relationship with the topic. This information is used to automatically generate background knowledge forms that contain words that are highly related to the topic and words that share similar features but do not share high associations to the topic. Prior research indicates that performance on such tasks can help determine whether a student is likely to understand a particular text based on their knowledge state. The described system is intended for use with middle and high school student population of native speakers of English. It is designed to handle single reading passages and is not dependent on any corpus or text collection. In this paper, we describe the system architecture and present an initial evaluation of the system outputs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "This manuscript was accepted to be published as an ETS Research Report. Keywords topics; vocabulary; background knowledge; automatic item generation; assessment; reading comprehension",
    "pdf_url": "https://arxiv.org/pdf/2505.18762v1",
    "published_date": "2025-05-24 15:57:02 UTC",
    "updated_date": "2025-05-24 15:57:02 UTC"
  },
  {
    "arxiv_id": "2505.18761v2",
    "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark",
    "authors": [
      "Minglai Yang",
      "Ethan Huang",
      "Liang Zhang",
      "Mihai Surdeanu",
      "William Wang",
      "Liangming Pan"
    ],
    "abstract": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic benchmark to evaluate Large Language Models' (LLMs) reasoning robustness against systematically controlled irrelevant context (IC). GSM-DC constructs symbolic reasoning graphs with precise distractor injections, enabling rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are significantly sensitive to IC, affecting both reasoning path selection and arithmetic accuracy. Additionally, training models with strong distractors improves performance in both in-distribution and out-of-distribution scenarios. We further propose a stepwise tree search guided by a process reward model, which notably enhances robustness in out-of-distribution conditions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 10 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.18761v2",
    "published_date": "2025-05-24 15:56:22 UTC",
    "updated_date": "2025-09-22 16:41:27 UTC"
  },
  {
    "arxiv_id": "2505.18759v1",
    "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation",
    "authors": [
      "Ruichen Zhang",
      "Rana Muhammad Shahroz Khan",
      "Zhen Tan",
      "Dawei Li",
      "Song Wang",
      "Tianlong Chen"
    ],
    "abstract": "Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is shared in https://anonymous.4open.science/r/DC-COT-FF4C/.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18759v1",
    "published_date": "2025-05-24 15:54:19 UTC",
    "updated_date": "2025-05-24 15:54:19 UTC"
  },
  {
    "arxiv_id": "2505.18755v1",
    "title": "Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation",
    "authors": [
      "Xiaolu Chen",
      "Chenghao Huang",
      "Yanru Zhang",
      "Hao Wang"
    ],
    "abstract": "With the proliferation of smart grids, smart cities face growing challenges due to cyber-attacks and sophisticated electricity theft behaviors, particularly in residential photovoltaic (PV) generation systems. Traditional Electricity Theft Detection (ETD) methods often struggle to capture complex temporal dependencies and integrating multi-source data, limiting their effectiveness. In this work, we propose an efficient ETD method that accurately identifies fraudulent behaviors in residential PV generation, thus ensuring the supply-demand balance in smart cities. Our hybrid deep learning model, combining multi-scale Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer, excels in capturing both short-term and long-term temporal dependencies. Additionally, we introduce a data embedding technique that seamlessly integrates time-series data with discrete temperature variables, enhancing detection robustness. Extensive simulation experiments using real-world data validate the effectiveness of our approach, demonstrating significant improvements in the accuracy of detecting sophisticated energy theft activities, thereby contributing to the stability and fairness of energy systems in smart cities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "2024 IEEE International Smart Cities Conference (ISC2)",
    "pdf_url": "https://arxiv.org/pdf/2505.18755v1",
    "published_date": "2025-05-24 15:47:00 UTC",
    "updated_date": "2025-05-24 15:47:00 UTC"
  },
  {
    "arxiv_id": "2505.18754v1",
    "title": "Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection",
    "authors": [
      "Elsen Ronando",
      "Sozo Inoue"
    ],
    "abstract": "In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid Euclidean Distance with Large Language Models) to improve example selection for sensor-based classification tasks. While few-shot prompting enables efficient inference with limited labeled data, its performance largely depends on the quality of selected examples. HED-LM addresses this challenge through a hybrid selection pipeline that filters candidate examples based on Euclidean distance and re-ranks them using contextual relevance scored by large language models (LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection task using accelerometer data characterized by overlapping patterns and high inter-subject variability. Unlike simpler tasks such as activity recognition, fatigue detection demands more nuanced example selection due to subtle differences in physiological signals. Our experiments show that HED-LM achieves a mean macro F1-score of 69.13$\\pm$10.71%, outperforming both random selection (59.30$\\pm$10.13%) and distance-only filtering (67.61$\\pm$11.39%). These represent relative improvements of 16.6% and 2.3%, respectively. The results confirm that combining numerical similarity with contextual relevance improves the robustness of few-shot prompting. Overall, HED-LM offers a practical solution to improve performance in real-world sensor-based learning tasks and shows potential for broader applications in healthcare monitoring, human activity recognition, and industrial safety scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "43 pages, 18 figures. Accepted for publication in MDPI Sensors (2025). Final version before journal publication",
    "pdf_url": "https://arxiv.org/pdf/2505.18754v1",
    "published_date": "2025-05-24 15:43:25 UTC",
    "updated_date": "2025-05-24 15:43:25 UTC"
  },
  {
    "arxiv_id": "2505.18750v1",
    "title": "Agent-Based Decentralized Energy Management of EV Charging Station with Solar Photovoltaics via Multi-Agent Reinforcement Learning",
    "authors": [
      "Jiarong Fan",
      "Chenghao Huang",
      "Hao Wang"
    ],
    "abstract": "In the pursuit of energy net zero within smart cities, transportation electrification plays a pivotal role. The adoption of Electric Vehicles (EVs) keeps increasing, making energy management of EV charging stations critically important. While previous studies have managed to reduce energy cost of EV charging while maintaining grid stability, they often overlook the robustness of EV charging management against uncertainties of various forms, such as varying charging behaviors and possible faults in faults in some chargers. To address the gap, a novel Multi-Agent Reinforcement Learning (MARL) approach is proposed treating each charger to be an agent and coordinate all the agents in the EV charging station with solar photovoltaics in a more realistic scenario, where system faults may occur. A Long Short-Term Memory (LSTM) network is incorporated in the MARL algorithm to extract temporal features from time-series. Additionally, a dense reward mechanism is designed for training the agents in the MARL algorithm to improve EV charging experience. Through validation on a real-world dataset, we show that our approach is robust against system uncertainties and faults and also effective in minimizing EV charging costs and maximizing charging service satisfaction.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "2024 IEEE International Smart Cities Conference (ISC2)",
    "pdf_url": "https://arxiv.org/pdf/2505.18750v1",
    "published_date": "2025-05-24 15:34:37 UTC",
    "updated_date": "2025-05-24 15:34:37 UTC"
  },
  {
    "arxiv_id": "2505.18747v1",
    "title": "Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion",
    "authors": [
      "Xiaolu Chen",
      "Chenghao Huang",
      "Yanru Zhang",
      "Hao Wang"
    ],
    "abstract": "With the advancement of energy Internet and energy system integration, the increasing adoption of distributed photovoltaic (PV) systems presents new challenges on smart monitoring and measurement for utility companies, particularly in separating PV generation from net electricity load. Existing methods struggle with feature extraction from net load and capturing the relevance between weather factors. This paper proposes a PV disaggregation method that integrates Hierarchical Interpolation (HI) and multi-head self-attention mechanisms. By using HI to extract net load features and multi-head self-attention to capture the complex dependencies between weather factors, the method achieves precise PV generation predictions. Simulation experiments demonstrate the effectiveness of the proposed method in real-world data, supporting improved monitoring and management of distributed energy systems.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "2024 IEEE 8th Conference on Energy Internet and Energy System Integration (EI2)",
    "pdf_url": "https://arxiv.org/pdf/2505.18747v1",
    "published_date": "2025-05-24 15:25:46 UTC",
    "updated_date": "2025-05-24 15:25:46 UTC"
  },
  {
    "arxiv_id": "2505.18746v4",
    "title": "$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking",
    "authors": [
      "Peijie Yu",
      "Yifan Yang",
      "Jinjian Li",
      "Zelong Zhang",
      "Haorui Wang",
      "Xiao Feng",
      "Feng Zhang"
    ],
    "abstract": "Agents based on large language models leverage tools to modify environments, revolutionizing how AI interacts with the physical world. Unlike traditional NLP tasks that rely solely on historical dialogue for responses, these agents must consider more complex factors, such as inter-tool relationships, environmental feedback and previous decisions, when making choices. Current research typically evaluates agents via multi-turn dialogues. However, it overlooks the influence of these critical factors on agent behavior. To bridge this gap, we present an open-source and high-quality benchmark $C^3$-Bench. This benchmark integrates attack concepts and applies univariate analysis to pinpoint key elements affecting agent robustness. In concrete, we design three challenges: navigate complex tool relationships, handle critical hidden information and manage dynamic decision paths. Complementing these challenges, we introduce fine-grained metrics, innovative data collection algorithms and reproducible evaluation methods. Extensive experiments are conducted on 49 mainstream agents, encompassing general fast-thinking, slow-thinking and domain-specific models. We observe that agents have significant shortcomings in handling tool dependencies, long context information dependencies and frequent policy-type switching. In essence, $C^3$-Bench aims to expose model vulnerabilities through these challenges and drive research into the interpretability of agent performance. The benchmark is publicly available at https://github.com/TencentHunyuan/C3-Benchmark.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18746v4",
    "published_date": "2025-05-24 15:25:44 UTC",
    "updated_date": "2025-06-27 03:58:25 UTC"
  },
  {
    "arxiv_id": "2505.18741v1",
    "title": "MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images",
    "authors": [
      "Han Li",
      "Hu Han",
      "S. Kevin Zhou"
    ],
    "abstract": "Natural images exhibit label diversity (clean vs. noisy) in noisy-labeled image classification and prevalence diversity (abundant vs. sparse) in long-tailed image classification. Similarly, medical images in universal lesion detection (ULD) exhibit substantial variations in image quality, encompassing attributes such as clarity and label correctness. How to effectively leverage training images with diverse qualities becomes a problem in learning deep models. Conventional training mechanisms, such as self-paced curriculum learning (SCL) and online hard example mining (OHEM), relieve this problem by reweighting images with high loss values. Despite their success, these methods still confront two challenges: (i) the loss-based measure of sample hardness is imprecise, preventing optimum handling of different cases, and (ii) there exists under-utilization in SCL or over-utilization OHEM with the identified hard samples. To address these issues, this paper revisits the minibatch sampling (MBS), a technique widely used in deep network training but largely unexplored concerning the handling of diverse-quality training samples. We discover that the samples within a minibatch influence each other during training; thus, we propose a novel Mixed-order Minibatch Sampling (MoMBS) method to optimize the use of training samples with diverse qualities. MoMBS introduces a measure that takes both loss and uncertainty into account to surpass a sole reliance on loss and allows for a more refined categorization of high-loss samples by distinguishing them as either poorly labeled and under represented or well represented and overfitted. We prioritize under represented samples as the main gradient contributors in a minibatch and keep them from the negative influences of poorly labeled or overfitted samples with a mixed-order minibatch sampling design.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages,8 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18741v1",
    "published_date": "2025-05-24 15:20:47 UTC",
    "updated_date": "2025-05-24 15:20:47 UTC"
  },
  {
    "arxiv_id": "2505.18728v1",
    "title": "Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling",
    "authors": [
      "Andrea Ceni",
      "Alessio Gravina",
      "Claudio Gallicchio",
      "Davide Bacciu",
      "Carola-Bibiane Schonlieb",
      "Moshe Eliasof"
    ],
    "abstract": "The recent success of State-Space Models (SSMs) in sequence modeling has motivated their adaptation to graph learning, giving rise to Graph State-Space Models (GSSMs). However, existing GSSMs operate by applying SSM modules to sequences extracted from graphs, often compromising core properties such as permutation equivariance, message-passing compatibility, and computational efficiency. In this paper, we introduce a new perspective by embedding the key principles of modern SSM computation directly into the Message-Passing Neural Network framework, resulting in a unified methodology for both static and temporal graphs. Our approach, MP-SSM, enables efficient, permutation-equivariant, and long-range information propagation while preserving the architectural simplicity of message passing. Crucially, MP-SSM enables an exact sensitivity analysis, which we use to theoretically characterize information flow and evaluate issues like vanishing gradients and over-squashing in the deep regime. Furthermore, our design choices allow for a highly optimized parallel implementation akin to modern SSMs. We validate MP-SSM across a wide range of tasks, including node classification, graph property prediction, long-range benchmarks, and spatiotemporal forecasting, demonstrating both its versatility and strong empirical performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18728v1",
    "published_date": "2025-05-24 14:53:07 UTC",
    "updated_date": "2025-05-24 14:53:07 UTC"
  },
  {
    "arxiv_id": "2505.18724v2",
    "title": "LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning",
    "authors": [
      "Junyu Chen",
      "Junzhuo Li",
      "Zhen Peng",
      "Wenjie Wang",
      "Yuxiang Ren",
      "Long Shi",
      "Xuming Hu"
    ],
    "abstract": "Quantization and fine-tuning are crucial for deploying large language models (LLMs) on resource-constrained edge devices. However, fine-tuning quantized models presents significant challenges, primarily stemming from: First, the mismatch in data types between the low-precision quantized weights (e.g., 4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch limits the computational efficiency advantage offered by quantized weights during inference. Second, potential accuracy degradation when merging these high-precision adaptation weights into the low-precision quantized weights, as the adaptation weights often necessitate approximation or truncation. Third, as far as we know, no existing methods support the lossless merging of adaptation while adjusting all quantized weights. To address these challenges, we introduce lossless ternary adaptation for quantization-aware fine-tuning (LoTA-QAF). This is a novel fine-tuning method specifically designed for quantized LLMs, enabling the lossless merging of ternary adaptation weights into quantized weights and the adjustment of all quantized weights. LoTA-QAF operates through a combination of: i) A custom-designed ternary adaptation (TA) that aligns ternary weights with the quantization grid and uses these ternary weights to adjust quantized weights. ii) A TA-based mechanism that enables the lossless merging of adaptation weights. iii) Ternary signed gradient descent (t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and Qwen-2.5 model families and validate its effectiveness on several downstream tasks. On the MMLU benchmark, our method effectively recovers performance for quantized models, surpassing 16-bit LoRA by up to 5.14\\%. For task-specific fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still outperforms other methods. Code: github.com/KingdalfGoodman/LoTA-QAF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18724v2",
    "published_date": "2025-05-24 14:47:28 UTC",
    "updated_date": "2025-09-28 00:18:37 UTC"
  },
  {
    "arxiv_id": "2505.18722v1",
    "title": "Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers",
    "authors": [
      "Terry Yi Zhong",
      "Esther Janse",
      "Cristian Tejedor-Garcia",
      "Louis ten Bosch",
      "Martha Larson"
    ],
    "abstract": "Speech-based Parkinson's disease (PD) detection has gained attention for its automated, cost-effective, and non-intrusive nature. As research studies usually rely on data from diagnostic-oriented speech tasks, this work explores the feasibility of diagnosing PD on the basis of speech data not originally intended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our findings indicate that TT can be as useful as diagnostic-oriented PD datasets like PC-GITA. We also investigate which specific dataset characteristics impact PD classification performance. The results show that concatenating audio recordings and balancing participants' gender and status distributions can be beneficial. Cross-dataset evaluation reveals that models trained on PC-GITA generalize poorly to TT, whereas models trained on TT perform better on PC-GITA. Furthermore, we provide insights into the high variability across folds, which is mainly due to large differences in individual speaker performance.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted for Interspeech 2025 (Camera-Ready)",
    "pdf_url": "https://arxiv.org/pdf/2505.18722v1",
    "published_date": "2025-05-24 14:45:55 UTC",
    "updated_date": "2025-05-24 14:45:55 UTC"
  },
  {
    "arxiv_id": "2505.18720v1",
    "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization",
    "authors": [
      "Meng Li",
      "Guangda Huzhang",
      "Haibo Zhang",
      "Xiting Wang",
      "Anxiang Zeng"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as a promising framework for aligning Large Language Models (LLMs) with human preferences by directly optimizing the log-likelihood difference between chosen and rejected responses. However, existing methods assign equal importance to all tokens in the response, while humans focus on more meaningful parts. This leads to suboptimal preference optimization, as irrelevant or noisy tokens disproportionately influence DPO loss. To address this limitation, we propose \\textbf{O}ptimal \\textbf{T}ransport-based token weighting scheme for enhancing direct \\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically meaningful token pairs and de-emphasizing less relevant ones, our method introduces a context-aware token weighting scheme that yields a more contrastive reward difference estimate. This adaptive weighting enhances reward stability, improves interpretability, and ensures that preference optimization focuses on meaningful differences between responses. Extensive experiments have validated OTPO's effectiveness in improving instruction-following ability across various settings\\footnote{Code is available at https://github.com/Mimasss2/OTPO.}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 11 figures. Accepted by ACL 2025 (main)",
    "pdf_url": "https://arxiv.org/pdf/2505.18720v1",
    "published_date": "2025-05-24 14:44:15 UTC",
    "updated_date": "2025-05-24 14:44:15 UTC"
  },
  {
    "arxiv_id": "2505.18719v1",
    "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
    "authors": [
      "Guanxing Lu",
      "Wenkai Guo",
      "Chubin Zhang",
      "Yuheng Zhou",
      "Haonan Jiang",
      "Zifeng Gao",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "abstract": "Recent high-capacity vision-language-action (VLA) models have demonstrated impressive performance on a range of robotic manipulation tasks by imitating human demonstrations. However, exploiting offline data with limited visited states will cause execution failure in out-of-distribution scenarios. Intuitively, an exploration-based method that improves on online collected data at test time could address this limitation. We present VLA-RL, an algorithmic and systematic framework that leverages online reinforcement learning (RL) to improve pretrained auto-regressive VLAs in downstream tasks. Within a unified perspective, we first introduce a trajectory-level RL formulation for auto-regressive VLA training, which models general robotic manipulation trajectory as multi-modal multi-turn conversation. To address the challenge of sparse rewards, we fine-tune a pretrained vision-language model as a robotic process reward model, which is trained on pseudo reward labels annotated on automatically extracted task segments. To scale up, we identify several implementation findings that improve the stability and efficiency including curriculum selection strategy, GPU-balanced vectorized environments, batch decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, and even matches the performance of advanced commercial models such as $_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time optimization, indicating an early spark of inference scaling laws in robotics.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18719v1",
    "published_date": "2025-05-24 14:42:51 UTC",
    "updated_date": "2025-05-24 14:42:51 UTC"
  },
  {
    "arxiv_id": "2506.04236v2",
    "title": "Spore in the Wild: A Case Study of Spore.fun as an Open-Environment Evolution Experiment with Sovereign AI Agents on TEE-Secured Blockchains",
    "authors": [
      "Botao Amber Hu",
      "Helena Rong"
    ],
    "abstract": "In Artificial Life (ALife) research, replicating Open-Ended Evolution (OEE)-the continuous emergence of novelty observed in biological life-has usually been pursued within isolated, closed system simulations, such as Tierra and Avida, which have typically plateaued after an initial burst of novelty, failing to achieve sustained OEE. Scholars suggest that OEE requires an open-environment system that continually exchanges information or energy with its environment. A recent technological innovation in Decentralized Physical Infrastructure Network (DePIN), which provides permissionless computational substrates, enables the deployment of Large Language Model-based AI agents on blockchains integrated with Trusted Execution Environments (TEEs). This enables on-chain agents to operate autonomously \"in the wild,\" achieving self-sovereignty without human oversight. These agents can control their own social media accounts and cryptocurrency wallets, allowing them to interact directly with blockchain-based financial networks and broader human social media. Building on this new paradigm of on-chain agents, Spore.fun is a recent real-world AI evolution experiment that enables autonomous breeding and evolution of new on-chain agents. This paper presents a detailed case study of Spore.fun, examining agent behaviors and their evolutionary trajectories through digital ethology. We aim to spark discussion about whether open-environment ALife systems \"in the wild,\" based on permissionless computational substrates and driven by economic incentives to interact with their environment, could finally achieve the long-sought goal of OEE.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.HC",
      "cs.NE"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted by ALIFE 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.04236v2",
    "published_date": "2025-05-24 14:42:36 UTC",
    "updated_date": "2025-08-19 20:40:20 UTC"
  },
  {
    "arxiv_id": "2505.21539v2",
    "title": "Equivariant Flow Matching for Point Cloud Assembly",
    "authors": [
      "Ziming Wang",
      "Nan Xue",
      "Rebecka Jrnsten"
    ],
    "abstract": "The goal of point cloud assembly is to reconstruct a complete 3D shape by aligning multiple point cloud pieces. This work presents a novel equivariant solver for assembly tasks based on flow matching models. We first theoretically show that the key to learning equivariant distributions via flow matching is to learn related vector fields. Based on this result, we propose an assembly model, called equivariant diffusion assembly (Eda), which learns related vector fields conditioned on the input pieces. We further construct an equivariant path for Eda, which guarantees high data efficiency of the training process. Our numerical results show that Eda is highly competitive on practical datasets, and it can even handle the challenging situation where the input pieces are non-overlapped.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21539v2",
    "published_date": "2025-05-24 14:27:20 UTC",
    "updated_date": "2025-07-30 13:55:45 UTC"
  },
  {
    "arxiv_id": "2505.18713v1",
    "title": "Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer",
    "authors": [
      "Guodong Du",
      "Zitao Fang",
      "Jing Li",
      "Junlin Li",
      "Runhua Jiang",
      "Shuyang Yu",
      "Yifei Guo",
      "Yangneng Chen",
      "Sim Kuan Goh",
      "Ho-Kin Tang",
      "Daojing He",
      "Honghai Liu",
      "Min Zhang"
    ],
    "abstract": "Foundation models and their checkpoints have significantly advanced deep learning, boosting performance across various applications. However, fine-tuned models often struggle outside their specific domains and exhibit considerable redundancy. Recent studies suggest that combining a pruned fine-tuned model with the original pre-trained model can mitigate forgetting, reduce interference when merging model parameters across tasks, and improve compression efficiency. In this context, developing an effective pruning strategy for fine-tuned models is crucial. Leveraging the advantages of the task vector mechanism, we preprocess fine-tuned models by calculating the differences between them and the original model. Recognizing that different task vector subspaces contribute variably to model performance, we introduce a novel method called Neural Parameter Search (NPS-Pruning) for slimming down fine-tuned models. This method enhances pruning efficiency by searching through neural parameters of task vectors within low-rank subspaces. Our method has three key applications: enhancing knowledge transfer through pairwise model interpolation, facilitating effective knowledge fusion via model merging, and enabling the deployment of compressed models that retain near-original performance while significantly reducing storage costs. Extensive experiments across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness and robustness of our approach, resulting in substantial performance gains. The code is publicly available at: https://github.com/duguodong7/NPS-Pruning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ACL2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2505.18713v1",
    "published_date": "2025-05-24 14:27:20 UTC",
    "updated_date": "2025-05-24 14:27:20 UTC"
  },
  {
    "arxiv_id": "2505.21538v2",
    "title": "Caption This, Reason That: VLMs Caught in the Middle",
    "authors": [
      "Zihan Weng",
      "Lucas Gomez",
      "Taylor Whittington Webb",
      "Pouya Bashivan"
    ],
    "abstract": "Vision-Language Models (VLMs) have shown remarkable progress in visual understanding in recent years. Yet, they still lag behind human capabilities in specific visual tasks such as counting or relational reasoning. To understand the underlying limitations, we adopt methodologies from cognitive science, analyzing VLM performance along core cognitive axes: Perception, Attention, and Memory. Using a suite of tasks targeting these abilities, we evaluate state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct cognitive profiles: while advanced models approach ceiling performance on some tasks (e.g. category identification), a significant gap persists, particularly in tasks requiring spatial understanding or selective attention. Investigating the source of these failures and potential methods for improvement, we employ a vision-text decoupling analysis, finding that models struggling with direct visual reasoning show marked improvement when reasoning over their own generated text captions. These experiments reveal a strong need for improved VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed human performance. Furthermore, we demonstrate the potential of targeted fine-tuning on composite visual reasoning tasks and show that fine-tuning smaller VLMs substantially improves core cognitive abilities. While this improvement does not translate to large enhancements on challenging, out-of-distribution benchmarks, we show broadly that VLM performance on our datasets strongly correlates with performance on these other benchmarks. Our work provides a detailed analysis of VLM cognitive strengths and weaknesses and identifies key bottlenecks in simultaneous perception and reasoning while also providing an effective and simple solution.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper accepted by nips 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21538v2",
    "published_date": "2025-05-24 14:25:48 UTC",
    "updated_date": "2025-11-12 23:49:04 UTC"
  },
  {
    "arxiv_id": "2505.20340v2",
    "title": "Empirical Investigation of Latent Representational Dynamics in Large Language Models: A Manifold Evolution Perspective",
    "authors": [
      "Yukun Zhang",
      "Qi Dong"
    ],
    "abstract": "This paper introduces the Dynamical Manifold Evolution Theory (DMET), a conceptual framework that models large language model (LLM) generation as a continuous trajectory evolving on a low-dimensional semantic manifold. The theory characterizes latent dynamics through three interpretable metrics-state continuity ($C$), attractor compactness ($Q$), and topological persistence ($P$)-which jointly capture the smoothness, stability, and structure of representation evolution. Empirical analyses across multiple Transformer architectures reveal consistent links between these latent dynamics and text quality: smoother trajectories correspond to greater fluency, and richer topological organization correlates with enhanced coherence. Different models exhibit distinct dynamical regimes, reflecting diverse strategies of semantic organization in latent space. Moreover, decoding parameters such as temperature and top-$p$ shape these trajectories in predictable ways, defining a balanced region that harmonizes fluency and creativity. As a phenomenological rather than first-principles framework, DMET provides a unified and testable perspective for interpreting, monitoring, and guiding LLM behavior, offering new insights into the interplay between internal representation dynamics and external text generation quality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20340v2",
    "published_date": "2025-05-24 14:17:50 UTC",
    "updated_date": "2025-10-13 15:56:25 UTC"
  },
  {
    "arxiv_id": "2505.18710v1",
    "title": "GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis",
    "authors": [
      "Yi Jiang",
      "Sendong Zhao",
      "Jianbo Li",
      "Haochun Wang",
      "Bing Qin"
    ],
    "abstract": "The Retrieval-Augmented Generation (RAG) framework introduces a retrieval module to dynamically inject retrieved information into the input context of large language models (LLMs), and has demonstrated significant success in various NLP tasks. However, the current study points out that there is a preference gap between retrievers and LLMs in the RAG framework, which limit the further improvement of system performance. Some highly relevant passages may interfere with LLM reasoning because they contain complex or contradictory information; while some indirectly related or even inaccurate content may help LLM generate more accurate answers by providing suggestive information or logical clues. To solve this, we propose GainRAG, a novel approach that aligns the retriever's and LLM's preferences by defining a new metric, \"gain\", which measure how well an input passage contributes to correct outputs. Specifically, we propose a method to estimate these gain signals and train a middleware that aligns the preferences of the retriever and the LLM using only limited data. In addition, we introduce a pseudo-passage strategy to mitigate degradation. The experimental results on 6 datasets verify the effectiveness of GainRAG.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18710v1",
    "published_date": "2025-05-24 14:14:57 UTC",
    "updated_date": "2025-05-24 14:14:57 UTC"
  },
  {
    "arxiv_id": "2505.18709v1",
    "title": "Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla",
    "authors": [
      "Sourav Kumar Das",
      "Md. Julkar Naeen",
      "MD. Jahidul Islam",
      "Md. Anisul Haque Sajeeb",
      "Narayan Ranjan Chakraborty",
      "Mayen Uddin Mojumdar"
    ],
    "abstract": "Bangla or Bengali is the national language of Bangladesh, people from different regions don't talk in proper Bangla. Every division of Bangladesh has its own local language like Sylheti, Chittagong etc. In recent years some papers were published on Bangla language like sentiment analysis, fake news detection and classifications, but a few of them were on Bangla languages. This research is for the local language and this particular paper is on Sylheti language. It presented a comprehensive system using Natural Language Processing or NLP techniques for translating Pure or Modern Bangla to locally spoken Sylheti Bangla language. Total 1200 data used for training 3 models LSTM, Bi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3% accuracy. The findings of this research may contribute to the growth of Bangla NLP researchers for future more advanced innovations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)",
    "pdf_url": "https://arxiv.org/pdf/2505.18709v1",
    "published_date": "2025-05-24 14:13:45 UTC",
    "updated_date": "2025-05-24 14:13:45 UTC"
  },
  {
    "arxiv_id": "2505.18708v1",
    "title": "A General Knowledge Injection Framework for ICD Coding",
    "authors": [
      "Xu Zhang",
      "Kun Zhang",
      "Wenxin Ma",
      "Rongsheng Wang",
      "Chenxu Wu",
      "Yingtai Li",
      "S. Kevin Zhou"
    ],
    "abstract": "ICD Coding aims to assign a wide range of medical codes to a medical text document, which is a popular and challenging task in the healthcare domain. To alleviate the problems of long-tail distribution and the lack of annotations of code-specific evidence, many previous works have proposed incorporating code knowledge to improve coding performance. However, existing methods often focus on a single type of knowledge and design specialized modules that are complex and incompatible with each other, thereby limiting their scalability and effectiveness. To address this issue, we propose GKI-ICD, a novel, general knowledge injection framework that integrates three key types of knowledge, namely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized design of additional modules. The comprehensive utilization of the above knowledge, which exhibits both differences and complementarity, can effectively enhance the ICD coding performance. Extensive experiments on existing popular ICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves the state-of-the-art performance on most evaluation metrics. Code is available at https://github.com/xuzhang0112/GKI-ICD.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.18708v1",
    "published_date": "2025-05-24 13:57:56 UTC",
    "updated_date": "2025-05-24 13:57:56 UTC"
  },
  {
    "arxiv_id": "2505.18706v3",
    "title": "Steering LLM Reasoning Through Bias-Only Adaptation",
    "authors": [
      "Viacheslav Sinii",
      "Alexey Gorbatovski",
      "Artem Cherepanov",
      "Boris Shaposhnikov",
      "Nikita Balagansky",
      "Daniil Gavrilov"
    ],
    "abstract": "We show that training a single $d$-dimensional steering vector per layer with reinforcement learning, while freezing all base weights, matches the accuracy of fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8 billion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters and reproduces performance across a range of base models and mathematical-reasoning benchmarks. These results tighten the upper bound on the parameter budget required for high-level chain-of-thought reasoning, indicating that millions of adapter weights are unnecessary. The minimal trainable footprint reduces optimizer memory and inter-GPU communication, lowering the overall cost of fine-tuning. Moreover, a logit-lens analysis shows that the learned vectors amplify coherent token directions, providing clearer insight into the model's internal computations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18706v3",
    "published_date": "2025-05-24 13:55:38 UTC",
    "updated_date": "2025-10-01 09:23:07 UTC"
  },
  {
    "arxiv_id": "2505.18705v1",
    "title": "AI-Researcher: Autonomous Scientific Innovation",
    "authors": [
      "Jiabin Tang",
      "Lianghao Xia",
      "Zhonghang Li",
      "Chao Huang"
    ],
    "abstract": "The powerful reasoning capabilities of Large Language Models (LLMs) in mathematics and coding, combined with their ability to automate complex tasks through agentic frameworks, present unprecedented opportunities for accelerating scientific innovation. In this paper, we introduce AI-Researcher, a fully autonomous research system that transforms how AI-driven scientific discovery is conducted and evaluated. Our framework seamlessly orchestrates the complete research pipeline--from literature review and hypothesis generation to algorithm implementation and publication-ready manuscript preparation--with minimal human intervention. To rigorously assess autonomous research capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising state-of-the-art papers across diverse AI research domains, featuring both guided innovation and open-ended exploration tasks. Through extensive experiments, we demonstrate that AI-Researcher achieves remarkable implementation success rates and produces research papers that approach human-level quality. This work establishes new foundations for autonomous scientific innovation that can complement human researchers by systematically exploring solution spaces beyond cognitive limitations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Code on github: https://github.com/HKUDS/AI-Researcher",
    "pdf_url": "https://arxiv.org/pdf/2505.18705v1",
    "published_date": "2025-05-24 13:54:38 UTC",
    "updated_date": "2025-05-24 13:54:38 UTC"
  },
  {
    "arxiv_id": "2505.20339v1",
    "title": "Challenges for artificial cognitive systems",
    "authors": [
      "Antoni Gomila",
      "Vincent C. Mller"
    ],
    "abstract": "The declared goal of this paper is to fill this gap: \"... cognitive systems research needs questions or challenges that define progress. The challenges are not (yet more) predictions of the future, but a guideline to what are the aims and what would constitute progress.\" -- the quotation being from the project description of EUCogII, the project for the European Network for Cognitive Systems within which this formulation of the 'challenges' was originally developed (http://www.eucognition.org). So, we stick out our neck and formulate the challenges for artificial cognitive systems. These challenges are articulated in terms of a definition of what a cognitive system is: a system that learns from experience and uses its acquired knowledge (both declarative and practical) in a flexible manner to achieve its own goals.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20339v1",
    "published_date": "2025-05-24 13:49:54 UTC",
    "updated_date": "2025-05-24 13:49:54 UTC"
  },
  {
    "arxiv_id": "2505.18700v4",
    "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains",
    "authors": [
      "Chun Wang",
      "Xiaojun Ye",
      "Xiaoran Pan",
      "Zihao Pan",
      "Haofan Wang",
      "Yiren Song"
    ],
    "abstract": "Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches to geo-localization tasks often lack robust reasoning mechanisms and explainability, limiting their effectiveness. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, a novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference. The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, we introduce GRE30K, a high-quality geo-localization reasoning dataset designed to facilitate fine-grained visual and contextual analysis. Next, we present the GRE model, which employs a multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features, thereby narrowing down potential geographic regions with enhanced precision. Finally, we construct the Geo Reason Evaluation Benchmark (GREval-Bench), a comprehensive evaluation framework that assesses VLMs across diverse urban, natural, and landmark scenes to measure both coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, underscoring the efficacy of reasoning-augmented VLMs in complex geographic inference. Code and data will be released at https://github.com/Thorin215/GRE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18700v4",
    "published_date": "2025-05-24 13:48:57 UTC",
    "updated_date": "2025-10-27 12:39:19 UTC"
  },
  {
    "arxiv_id": "2505.18697v2",
    "title": "Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study",
    "authors": [
      "Ziyang Cheng",
      "Zhixun Li",
      "Yuhan Li",
      "Yixin Song",
      "Kangyi Zhao",
      "Dawei Cheng",
      "Jia Li",
      "Hong Cheng",
      "Jeffrey Xu Yu"
    ],
    "abstract": "Nowadays, real-world data, including graph-structure data, often arrives in a streaming manner, which means that learning systems need to continuously acquire new knowledge without forgetting previously learned information. Although substantial existing works attempt to address catastrophic forgetting in graph machine learning, they are all based on training from scratch with streaming data. With the rise of pretrained models, an increasing number of studies have leveraged their strong generalization ability for continual learning. Therefore, in this work, we attempt to answer whether large language models (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning (GCL). We first point out that current experimental setups for GCL have significant flaws, as the evaluation stage may lead to task ID leakage. Then, we evaluate the performance of LLMs in more realistic scenarios and find that even minor modifications can lead to outstanding results. Finally, based on extensive experiments, we propose a simple-yet-effective method, Simple Graph Continual Learning (SimGCL), that surpasses the previous state-of-the-art GNN-based baseline by around 20% under the rehearsal-free constraint. To facilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL for training and evaluating existing GCL methods. The code is available at: https://github.com/ZhixunLEE/LLM4GCL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18697v2",
    "published_date": "2025-05-24 13:43:29 UTC",
    "updated_date": "2025-09-26 15:09:18 UTC"
  },
  {
    "arxiv_id": "2505.18695v1",
    "title": "AI for Regulatory Affairs: Balancing Accuracy, Interpretability, and Computational Cost in Medical Device Classification",
    "authors": [
      "Yu Han",
      "Aaron Ceross",
      "Jeroen H. M. Bergmann"
    ],
    "abstract": "Regulatory affairs, which sits at the intersection of medicine and law, can benefit significantly from AI-enabled automation. Classification task is the initial step in which manufacturers position their products to regulatory authorities, and it plays a critical role in determining market access, regulatory scrutiny, and ultimately, patient safety. In this study, we investigate a broad range of AI models -- including traditional machine learning (ML) algorithms, deep learning architectures, and large language models -- using a regulatory dataset of medical device descriptions. We evaluate each model along three key dimensions: accuracy, interpretability, and computational cost.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18695v1",
    "published_date": "2025-05-24 13:41:20 UTC",
    "updated_date": "2025-05-24 13:41:20 UTC"
  },
  {
    "arxiv_id": "2505.20338v1",
    "title": "Assessing the Capability of LLMs in Solving POSCOMP Questions",
    "authors": [
      "Cayo Viegas",
      "Rohit Gheyi",
      "Mrcio Ribeiro"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly expanded the capabilities of artificial intelligence in natural language processing tasks. Despite this progress, their performance in specialized domains such as computer science remains relatively unexplored. Understanding the proficiency of LLMs in these domains is critical for evaluating their practical utility and guiding future developments. The POSCOMP, a prestigious Brazilian examination used for graduate admissions in computer science promoted by the Brazlian Computer Society (SBC), provides a challenging benchmark. This study investigates whether LLMs can match or surpass human performance on the POSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and Le Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP exams. The assessments measured the models' proficiency in handling complex questions typical of the exam. LLM performance was notably better on text-based questions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led with 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced (49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were observed in the 2023 exam. ChatGPT-4 achieved the highest performance, surpassing all students who took the POSCOMP 2023 exam. LLMs, particularly ChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image interpretation remains a challenge. Given the rapid evolution of LLMs, we expanded our analysis to include more recent models - o1, Gemini 2.5 Pro, Claude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams. These newer models demonstrate further improvements and consistently surpass both the average and top-performing human participants across all three years.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20338v1",
    "published_date": "2025-05-24 13:40:53 UTC",
    "updated_date": "2025-05-24 13:40:53 UTC"
  },
  {
    "arxiv_id": "2505.18694v1",
    "title": "AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa",
    "authors": [
      "Rafiu Adekoya Badekale",
      "Adewale Akinfaderin"
    ],
    "abstract": "Climate policy scenario generation and evaluation have traditionally relied on integrated assessment models (IAMs) and expert-driven qualitative analysis. These methods enable stakeholders, such as policymakers and researchers, to anticipate impacts, plan governance strategies, and develop mitigation measures. However, traditional methods are often time-intensive, reliant on simple extrapolations of past trends, and limited in capturing the complex and interconnected nature of energy and climate issues. With the advent of artificial intelligence (AI), particularly generative AI models trained on vast datasets, these limitations can be addressed, ensuring robustness even under limited data conditions. In this work, we explore the novel method that employs generative AI, specifically large language models (LLMs), to simulate climate policy scenarios for Sub-Saharan Africa. These scenarios focus on energy transition themes derived from the historical United Nations Climate Change Conference (COP) documents. By leveraging generative models, the project aims to create plausible and diverse policy scenarios that align with regional climate goals and energy challenges. Given limited access to human evaluators, automated techniques were employed for scenario evaluation. We generated policy scenarios using the llama3.2-3B model. Of the 34 generated responses, 30 (88%) passed expert validation, accurately reflecting the intended impacts provided in the corresponding prompts. We compared these validated responses against assessments from a human climate expert and two additional LLMs (gemma2-2B and mistral-7B). Our structured, embedding-based evaluation framework shows that generative AI effectively generate scenarios that are coherent, relevant, plausible, and diverse. This approach offers a transformative tool for climate policy planning in data-constrained regions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages long. Extended version of the paper titled \"Climate Policy Simulation and Scenario Generation for Sub-Saharan Africa\" accepted at the 13th Data Science Africa Workshop (DSA 2025). Code available at https://github.com/AdeTheBade/CPSG.git",
    "pdf_url": "https://arxiv.org/pdf/2505.18694v1",
    "published_date": "2025-05-24 13:38:17 UTC",
    "updated_date": "2025-05-24 13:38:17 UTC"
  },
  {
    "arxiv_id": "2505.18688v2",
    "title": "Large Language Models in the Task of Automatic Validation of Text Classifier Predictions",
    "authors": [
      "Aleksandr Tsymbalov",
      "Mikhail Khovrichev"
    ],
    "abstract": "Machine learning models for text classification are trained to predict a class for a given text. To do this, training and validation samples must be prepared: a set of texts is collected, and each text is assigned a class. These classes are usually assigned by human annotators with different expertise levels, depending on the specific classification task. Collecting such samples from scratch is labor-intensive because it requires finding specialists and compensating them for their work; moreover, the number of available specialists is limited, and their productivity is constrained by human factors. While it may not be too resource-intensive to collect samples once, the ongoing need to retrain models (especially in incremental learning pipelines) to address data drift (also called model drift) makes the data collection process crucial and costly over the model's entire lifecycle. This paper proposes several approaches to replace human annotators with Large Language Models (LLMs) to test classifier predictions for correctness, helping ensure model quality and support high-quality incremental learning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18688v2",
    "published_date": "2025-05-24 13:19:03 UTC",
    "updated_date": "2025-08-25 14:01:55 UTC"
  },
  {
    "arxiv_id": "2505.18687v3",
    "title": "An AI Capability Threshold for Rent-Funded Universal Basic Income in an AI-Automated Economy",
    "authors": [
      "Aran Nayebi"
    ],
    "abstract": "We derive the first closed-form condition under which artificial intelligence (AI) capital profits could sustainably finance a universal basic income (UBI) without relying on new taxation or the creation of new jobs. In a Solow-Zeira task-automation economy with a CES aggregator $< 1$, we introduce an AI capability parameter that scales the productivity of automatable tasks and obtain a tractable expression for the AI capability threshold -- the minimum productivity of AI relative to pre-AI automation required for a balanced transfer.\n  Using current U.S. economic parameters, we find that even in the conservative scenario where no new tasks or jobs emerge, AI systems would only need to reach only 5-7 times today's automation productivity to fund an 11%-of-GDP UBI.\n  Our analysis also reveals some specific policy levers: raising public revenue share (e.g. profit taxation) of AI capital from the current 15% to about 33% halves the required AI capability threshold to attain UBI to 3 times existing automation productivity, but gains diminish beyond 50% public revenue share, especially if regulatory costs increase. Market structure also strongly affects outcomes: monopolistic or concentrated oligopolistic markets reduce the threshold by increasing economic rents, whereas heightened competition significantly raises it.\n  These results therefore offer a rigorous benchmark for assessing when advancing AI capabilities might sustainably finance social transfers in an increasingly automated economy.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "econ.GN",
    "comment": "9 pages, 3 figures, added Ramsey-Cass-Koopmans endogenous savings analysis & varied Figure 1 calibration",
    "pdf_url": "https://arxiv.org/pdf/2505.18687v3",
    "published_date": "2025-05-24 13:08:13 UTC",
    "updated_date": "2025-11-30 16:41:03 UTC"
  },
  {
    "arxiv_id": "2505.18675v2",
    "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps",
    "authors": [
      "Sicheng Feng",
      "Song Wang",
      "Shuyi Ouyang",
      "Lingdong Kong",
      "Zikai Song",
      "Jianke Zhu",
      "Huan Wang",
      "Xinchao Wang"
    ],
    "abstract": "Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18675v2",
    "published_date": "2025-05-24 12:33:52 UTC",
    "updated_date": "2025-06-07 01:57:03 UTC"
  },
  {
    "arxiv_id": "2505.18674v2",
    "title": "Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model",
    "authors": [
      "Peng Xiao",
      "Hongbo Zhao",
      "Yijun Wang",
      "Jianxin Lin"
    ],
    "abstract": "Restoring real-world degraded images, such as old photographs or low-resolution images, presents a significant challenge due to the complex, mixed degradations they exhibit, such as scratches, color fading, and noise. Recent data-driven approaches have struggled with two main challenges: achieving high-fidelity restoration and providing object-level control over colorization. While diffusion models have shown promise in generating high-quality images with specific controls, they often fail to fully preserve image details during restoration. In this work, we propose an internal detail-preserving diffusion model for high-fidelity restoration of real-world degraded images. Our method utilizes a pre-trained Stable Diffusion model as a generative prior, eliminating the need to train a model from scratch. Central to our approach is the Internal Image Detail Enhancement (IIDE) technique, which directs the diffusion model to preserve essential structural and textural information while mitigating degradation effects. The process starts by mapping the input image into a latent space, where we inject the diffusion denoising process with degradation operations that simulate the effects of various degradation factors. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art models in both qualitative assessments and perceptual quantitative evaluations. Additionally, our approach supports text-guided restoration, enabling object-level colorization control that mimics the expertise of professional photo editing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18674v2",
    "published_date": "2025-05-24 12:32:53 UTC",
    "updated_date": "2025-05-27 03:47:50 UTC"
  },
  {
    "arxiv_id": "2505.20336v1",
    "title": "MOSLIM:Align with diverse preferences in prompts through reward classification",
    "authors": [
      "Yu Zhang",
      "Wanli Jiang",
      "Zhengyu Yang"
    ],
    "abstract": "The multi-objective alignment of Large Language Models (LLMs) is essential for ensuring foundational models conform to diverse human preferences. Current research in this field typically involves either multiple policies or multiple reward models customized for various preferences, or the need to train a preference-specific supervised fine-tuning (SFT) model. In this work, we introduce a novel multi-objective alignment method, MOSLIM, which utilizes a single reward model and policy model to address diverse objectives. MOSLIM provides a flexible way to control these objectives through prompting and does not require preference training during SFT phase, allowing thousands of off-the-shelf models to be directly utilized within this training framework. MOSLIM leverages a multi-head reward model that classifies question-answer pairs instead of scoring them and then optimize policy model with a scalar reward derived from a mapping function that converts classification results from reward model into reward scores. We demonstrate the efficacy of our proposed method across several multi-objective benchmarks and conduct ablation studies on various reward model sizes and policy optimization methods. The MOSLIM method outperforms current multi-objective approaches in most results while requiring significantly fewer GPU computing resources compared with existing policy optimization methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20336v1",
    "published_date": "2025-05-24 12:22:21 UTC",
    "updated_date": "2025-05-24 12:22:21 UTC"
  },
  {
    "arxiv_id": "2505.18670v3",
    "title": "MoveGPT: Scaling Mobility Foundation Models with Spatially-Aware Mixture of Experts",
    "authors": [
      "Chonghua Han",
      "Yuan Yuan",
      "Jingtao Ding",
      "Jie Feng",
      "Fanjin Meng",
      "Yong Li"
    ],
    "abstract": "The success of foundation models in language has inspired a new wave of general-purpose models for human mobility. However, existing approaches struggle to scale effectively due to two fundamental limitations: a failure to use meaningful basic units to represent movement, and an inability to capture the vast diversity of patterns found in large-scale data. In this work, we develop MoveGPT, a large-scale foundation model specifically architected to overcome these barriers. MoveGPT is built upon two key innovations: (1) a unified location encoder that maps geographically disjoint locations into a shared semantic space, enabling pre-training on a global scale; and (2) a Spatially-Aware Mixture-of-Experts Transformer that develops specialized experts to efficiently capture diverse mobility patterns. Pre-trained on billion-scale datasets, MoveGPT establishes a new state-of-the-art across a wide range of downstream tasks, achieving performance gains of up to 35% on average. It also demonstrates strong generalization capabilities to unseen cities. Crucially, our work provides empirical evidence of scaling ability in human mobility, validating a clear path toward building increasingly capable foundation models in this domain.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18670v3",
    "published_date": "2025-05-24 12:17:47 UTC",
    "updated_date": "2025-11-24 13:44:50 UTC"
  },
  {
    "arxiv_id": "2505.20335v4",
    "title": "Language Model Distillation: A Temporal Difference Imitation Learning Perspective",
    "authors": [
      "Zishun Yu",
      "Shangzhe Li",
      "Xinhua Zhang"
    ],
    "abstract": "Large language models have led to significant progress across many NLP tasks, although their massive sizes often incur substantial computational costs. Distillation has become a common practice to compress these large and highly capable models into smaller, more efficient ones. Many existing language model distillation methods can be viewed as behavior cloning from the perspective of imitation learning or inverse reinforcement learning. This viewpoint has inspired subsequent studies that leverage (inverse) reinforcement learning techniques, including variations of behavior cloning and temporal difference learning methods. Rather than proposing yet another specific temporal difference method, we introduce a general framework for temporal difference-based distillation by exploiting the distributional sparsity of the teacher model. Specifically, it is often observed that language models assign most probability mass to a small subset of tokens. Motivated by this observation, we design a temporal difference learning framework that operates on a reduced action space (a subset of vocabulary), and demonstrate how practical algorithms can be derived and the resulting performance improvements.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2026; Code available at: https://github.com/TobyLeelsz/Bellman-Distillation",
    "pdf_url": "https://arxiv.org/pdf/2505.20335v4",
    "published_date": "2025-05-24 12:17:12 UTC",
    "updated_date": "2026-01-04 18:51:37 UTC"
  },
  {
    "arxiv_id": "2505.18659v2",
    "title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees",
    "authors": [
      "Sangwoo Park",
      "Matteo Zecchin",
      "Osvaldo Simeone"
    ],
    "abstract": "Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference (PPI) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose R-AutoEval+, a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of R-AutoEval+ is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, for prompt design in LLMs, and for test-time reasoning budget allocation in LLMs confirm the reliability and efficiency of R-AutoEval+.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "comment": "NeurIPS 2025 (spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2505.18659v2",
    "published_date": "2025-05-24 11:53:29 UTC",
    "updated_date": "2025-12-02 13:47:23 UTC"
  },
  {
    "arxiv_id": "2505.18658v2",
    "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics",
    "authors": [
      "Pankaj Kumar",
      "Subhankar Mishra"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence (AI). However, ensuring the robustness of LLMs remains a critical challenge. To address these challenges and advance the field, this survey provides a comprehensive overview of current studies in this area. First, we systematically examine the nature of robustness in LLMs, including its conceptual foundations, the importance of consistent performance across diverse inputs, and the implications of failure modes in real-world applications. Next, we analyze the sources of non-robustness, categorizing intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors that compromise reliability. Following this, we review state-of-the-art mitigation strategies, and then we discuss widely adopted benchmarks, emerging metrics, and persistent gaps in assessing real-world reliability. Finally, we synthesize findings from existing surveys and interdisciplinary studies to highlight trends, unresolved issues, and pathways for future research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at TMLR",
    "pdf_url": "https://arxiv.org/pdf/2505.18658v2",
    "published_date": "2025-05-24 11:50:52 UTC",
    "updated_date": "2025-11-06 08:02:24 UTC"
  },
  {
    "arxiv_id": "2505.18657v1",
    "title": "MLLMs are Deeply Affected by Modality Bias",
    "authors": [
      "Xu Zheng",
      "Chenfei Liao",
      "Yuqian Fu",
      "Kaiyu Lei",
      "Yuanhuiyi Lyu",
      "Lutao Jiang",
      "Bin Ren",
      "Jialei Chen",
      "Jiawen Wang",
      "Chengxin Li",
      "Linfeng Zhang",
      "Danda Pani Paudel",
      "Xuanjing Huang",
      "Yu-Gang Jiang",
      "Nicu Sebe",
      "Dacheng Tao",
      "Luc Van Gool",
      "Xuming Hu"
    ],
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have shown promising results in integrating diverse modalities such as texts and images. MLLMs are heavily influenced by modality bias, often relying on language while under-utilizing other modalities like visual inputs. This position paper argues that MLLMs are deeply affected by modality bias. Firstly, we diagnose the current state of modality bias, highlighting its manifestations across various tasks. Secondly, we propose a systematic research road-map related to modality bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and offer actionable suggestions for future research to mitigate it. To substantiate these findings, we conduct experiments that demonstrate the influence of each factor: 1. Data Characteristics: Language data is compact and abstract, while visual data is redundant and complex, creating an inherent imbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The dominance of pretrained language models in MLLMs leads to overreliance on language and neglect of visual information. 3. Training Objectives: Current objectives often fail to promote balanced cross-modal alignment, resulting in shortcut learning biased toward language. These findings highlight the need for balanced training strategies and model architectures to better integrate multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle these challenges and drive innovation in MLLM research. Our work provides a fresh perspective on modality bias in MLLMs and offers insights for developing more robust and generalizable multimodal systems-advancing progress toward Artificial General Intelligence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18657v1",
    "published_date": "2025-05-24 11:49:31 UTC",
    "updated_date": "2025-05-24 11:49:31 UTC"
  },
  {
    "arxiv_id": "2505.18647v1",
    "title": "Flow Matching for Geometric Trajectory Simulation",
    "authors": [
      "Kiet Bennema ten Brinke",
      "Koen Minartz",
      "Vlado Menkovski"
    ],
    "abstract": "The simulation of N-body systems is a fundamental problem with applications in a wide range of fields, such as molecular dynamics, biochemistry, and pedestrian dynamics. Machine learning has become an invaluable tool for scaling physics-based simulators and developing models directly from experimental data. In particular, recent advances based on deep generative modeling and geometric deep learning have enabled probabilistic simulation by modeling complex distributions over trajectories while respecting the permutation symmetry that is fundamental to N-body systems. However, to generate realistic trajectories, existing methods must learn complex transformations starting from uninformed noise and do not allow for the exploitation of domain-informed priors. In this work, we propose STFlow to address this limitation. By leveraging flow matching and data-dependent couplings, STFlow facilitates physics-informed simulation of geometric trajectories without sacrificing model expressivity or scalability. Our evaluation on N-body dynamical systems, molecular dynamics, and pedestrian dynamics benchmarks shows that STFlow produces significantly lower prediction errors while enabling more efficient inference, highlighting the benefits of employing physics-informed prior distributions in probabilistic geometric trajectory modeling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18647v1",
    "published_date": "2025-05-24 11:18:59 UTC",
    "updated_date": "2025-05-24 11:18:59 UTC"
  },
  {
    "arxiv_id": "2505.18646v1",
    "title": "SEW: Self-Evolving Agentic Workflows for Automated Code Generation",
    "authors": [
      "Siwei Liu",
      "Jinyuan Fang",
      "Han Zhou",
      "Yingxu Wang",
      "Zaiqiao Meng"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated effectiveness in code generation tasks. To enable LLMs to address more complex coding challenges, existing research has focused on crafting multi-agent systems with agentic workflows, where complex coding tasks are decomposed into sub-tasks, assigned to specialized agents. Despite their effectiveness, current approaches heavily rely on hand-crafted agentic workflows, with both agent topologies and prompts manually designed, which limits their ability to automatically adapt to different types of coding problems. To address these limitations and enable automated workflow design, we propose \\textbf{S}elf-\\textbf{E}volving \\textbf{W}orkflow (\\textbf{SEW}), a novel self-evolving framework that automatically generates and optimises multi-agent workflows. Extensive experiments on three coding benchmark datasets, including the challenging LiveCodeBench, demonstrate that our SEW can automatically design agentic workflows and optimise them through self-evolution, bringing up to 33\\% improvement on LiveCodeBench compared to using the backbone LLM only. Furthermore, by investigating different representation schemes of workflow, we provide insights into the optimal way to encode workflow information with text.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "16 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18646v1",
    "published_date": "2025-05-24 11:12:14 UTC",
    "updated_date": "2025-05-24 11:12:14 UTC"
  },
  {
    "arxiv_id": "2505.18645v1",
    "title": "Riverine Flood Prediction and Early Warning in Mountainous Regions using Artificial Intelligence",
    "authors": [
      "Haleema Bibi",
      "Sadia Saleem",
      "Zakia Jalil",
      "Muhammad Nasir",
      "Tahani Alsubait"
    ],
    "abstract": "Flooding is the most devastating phenomenon occurring globally, particularly in mountainous regions, risk dramatically increases due to complex terrains and extreme climate changes. These situations are damaging livelihoods, agriculture, infrastructure, and human lives. This study uses the Kabul River between Pakistan and Afghanistan as a case study to reflect the complications of flood forecasting in transboundary basins. The challenges in obtaining upstream data impede the efficacy of flood control measures and early warning systems, a common global problem in similar basins. Utilizing satellite-based climatic data, this study applied numerous advanced machine-learning and deep learning models, such as Support Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRU) to predict daily and multi-step river flow. The LSTM network outperformed other models, achieving the highest R2 value of 0.96 and the lowest RMSE value of 140.96 m3/sec. The time series LSTM and GRU network models, utilized for short-term forecasts of up to five days, performed significantly. However, the accuracy declined beyond the fourth day, highlighting the need for longer-term historical datasets for reliable long-term flood predictions. The results of the study are directly aligned with Sustainable Development Goals 6, 11, 13, and 15, facilitating disaster and water management, timely evacuations, improved preparedness, and effective early warning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 6 figure",
    "pdf_url": "https://arxiv.org/pdf/2505.18645v1",
    "published_date": "2025-05-24 11:10:09 UTC",
    "updated_date": "2025-05-24 11:10:09 UTC"
  },
  {
    "arxiv_id": "2505.18643v1",
    "title": "Anomaly detection in radio galaxy data with trainable COSFIRE filters",
    "authors": [
      "Steven Ndung'u",
      "Trienko Grobler",
      "Stefan J. Wijnholds",
      "George Azzopardi"
    ],
    "abstract": "Detecting anomalies in radio astronomy is challenging due to the vast amounts of data and the rarity of labeled anomalous examples. Addressing this challenge requires efficient methods capable of identifying unusual radio galaxy morphologies without relying on extensive supervision. This work introduces an innovative approach to anomaly detection based on morphological characteristics of the radio sources using trainable COSFIRE (Combination of Shifted Filter Responses) filters as an efficient alternative to complex deep learning methods. The framework integrates COSFIRE descriptors with an unsupervised Local Outlier Factor (LOF) algorithm to identify unusual radio galaxy morphologies. Evaluations on a radio galaxy benchmark data set demonstrate strong performance, with the COSFIRE-based approach achieving a geometric mean (G-Mean) score of 79%, surpassing the 77% achieved by a computationally intensive deep learning autoencoder. By characterizing normal patterns and detecting deviations, this semi-supervised methodology overcomes the need for anomalous examples in the training set, a major limitation of traditional supervised methods. This approach shows promise for next-generation radio telescopes, where fast processing and the ability to discover unknown phenomena are crucial.",
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "5 pages, URSI Asia-Pacific Radio Science Conference and URSI Radio Science Letters (RSL)",
    "pdf_url": "https://arxiv.org/pdf/2505.18643v1",
    "published_date": "2025-05-24 11:08:41 UTC",
    "updated_date": "2025-05-24 11:08:41 UTC"
  },
  {
    "arxiv_id": "2505.18640v2",
    "title": "ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation",
    "authors": [
      "Jian Liang",
      "Wenke Huang",
      "Xianda Guo",
      "Guancheng Wan",
      "Bo Du",
      "Mang Ye"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of foundation models due to its efficiency and zero additional inference cost. Many real-world applications require foundation models to specialize in several specific tasks simultaneously, motivating the need for efficient multi-task downstream adaptation. To address this need, existing studies have primarily explored two directions: Model Merging with LoRA, which shows advantages in training-free scenarios but still lags behind multi-task training in overall performance; and MoE-based LoRA approaches, which improve multi-task learning performance but introduce routers that hinder the mergeability of LoRA parameters and incur considerable inference overhead, thereby limiting real-world deployment practicality. To this end, we propose ThanoRA, a Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables effective, efficient and unified multi-task downstream adaptation without introducing additional structure. ThanoRA performs multi-task learning by tailoring subspace allocation at initialization and enforcing diversity preservation throughout training: it allocates varying dimensions to construct task-specific low-rank subspaces driven by inter-task heterogeneity, enabling fine-grained knowledge injection, while diversity-preserving regularization mitigates task interference and subspace collapse, thereby fully exploiting the low-rank capacity. Extensive experiments across multimodal and text-only benchmarks under varying multi-task mixtures demonstrate that ThanoRA consistently outperforms strong baselines, surpassing even separate task-specific fine-tuning, while introducing no additional structures or inference overhead. Our code will be publicly available at: https://github.com/LiangJian24/ThanoRA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18640v2",
    "published_date": "2025-05-24 11:01:45 UTC",
    "updated_date": "2025-09-29 07:43:39 UTC"
  },
  {
    "arxiv_id": "2505.20334v2",
    "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
    "authors": [
      "Yixuan Wang",
      "Shiyu Ji",
      "Yijun Liu",
      "Yuzhuang Xu",
      "Yang Xu",
      "Qingfu Zhu",
      "Wanxiang Che"
    ],
    "abstract": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2025 Main",
    "pdf_url": "https://arxiv.org/pdf/2505.20334v2",
    "published_date": "2025-05-24 10:34:38 UTC",
    "updated_date": "2025-11-17 13:29:25 UTC"
  },
  {
    "arxiv_id": "2505.18630v2",
    "title": "DDO: Dual-Decision Optimization for LLM-Based Medical Consultation via Multi-Agent Collaboration",
    "authors": [
      "Zhihao Jia",
      "Mingyi Jia",
      "Junwen Duan",
      "Jianxin Wang"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate strong generalization and reasoning abilities, making them well-suited for complex decision-making tasks such as medical consultation (MC). However, existing LLM-based methods often fail to capture the dual nature of MC, which entails two distinct sub-tasks: symptom inquiry, a sequential decision-making process, and disease diagnosis, a classification problem. This mismatch often results in ineffective symptom inquiry and unreliable disease diagnosis. To address this, we propose \\textbf{DDO}, a novel LLM-based framework that performs \\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling the two sub-tasks and optimizing them with distinct objectives through a collaborative multi-agent workflow. Experiments on three real-world MC datasets show that DDO consistently outperforms existing LLM-based approaches and achieves competitive performance with state-of-the-art generation-based methods, demonstrating its effectiveness in the MC task. The code is available at https://github.com/zh-jia/DDO.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18630v2",
    "published_date": "2025-05-24 10:26:57 UTC",
    "updated_date": "2025-10-10 02:03:53 UTC"
  },
  {
    "arxiv_id": "2505.20333v2",
    "title": "Multi-Scale Manifold Alignment for Interpreting Large Language Models: A Unified Information-Geometric Framework",
    "authors": [
      "Yukun Zhang",
      "Qi Dong"
    ],
    "abstract": "We present Multi-Scale Manifold Alignment(MSMA), an information-geometric framework that decomposes LLM representations into local, intermediate, and global manifolds and learns cross-scale mappings that preserve geometry and information. Across GPT-2, BERT, RoBERTa, and T5, we observe consistent hierarchical patterns and find that MSMA improves alignment metrics under multiple estimators (e.g., relative KL reduction and MI gains with statistical significance across seeds). Controlled interventions at different scales yield distinct and architecture-dependent effects on lexical diversity, sentence structure, and discourse coherence. While our theoretical analysis relies on idealized assumptions, the empirical results suggest that multi-objective alignment offers a practical lens for analyzing cross-scale information flow and guiding representation-level control.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20333v2",
    "published_date": "2025-05-24 10:25:58 UTC",
    "updated_date": "2025-10-13 16:09:37 UTC"
  },
  {
    "arxiv_id": "2505.18623v2",
    "title": "Mind The Gap: Quantifying Mechanistic Gaps in Algorithmic Reasoning via Neural Compilation",
    "authors": [
      "Lucas Saldyt",
      "Subbarao Kambhampati"
    ],
    "abstract": "This paper aims to understand how neural networks learn algorithmic reasoning by addressing two questions: How faithful are learned algorithms when they are effective, and why do neural networks fail to learn effective algorithms otherwise? To answer these questions, we use neural compilation, a technique that directly encodes a source algorithm into neural network parameters, enabling the network to compute the algorithm exactly. This enables comparison between compiled and conventionally learned parameters, intermediate vectors, and behaviors. This investigation is crucial for developing neural networks that robustly learn complexalgorithms from data. Our analysis focuses on graph neural networks (GNNs), which are naturally aligned with algorithmic reasoning tasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the spectrum of effective, faithful, and ineffective learned algorithms. Commonly, learning algorithmic reasoning is framed as induction over synthetic data, where a parameterized model is trained on inputs, traces, and outputs produced by an underlying ground truth algorithm. In contrast, we introduce a neural compilation method for GNNs, which sets network parameters analytically, bypassing training. Focusing on GNNs leverages their alignment with algorithmic reasoning, extensive algorithmic induction literature, and the novel application of neural compilation to GNNs. Overall, this paper aims to characterize expressability-trainability gaps - a fundamental shortcoming in learning algorithmic reasoning. We hypothesize that inductive learning is most effective for parallel algorithms contained within the computational class \\texttt{NC}.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18623v2",
    "published_date": "2025-05-24 10:11:36 UTC",
    "updated_date": "2025-12-06 12:53:49 UTC"
  },
  {
    "arxiv_id": "2505.18622v1",
    "title": "Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation",
    "authors": [
      "Kourosh Shahnazari",
      "Seyed Moein Ayyoubzadeh",
      "Mohammadali Keshtparvar",
      "Pegah Ghaffari"
    ],
    "abstract": "In recent machine learning systems, confidence scores are being utilized more and more to manage selective prediction, whereby a model can abstain from making a prediction when it is unconfident. Yet, conventional metrics like accuracy, expected calibration error (ECE), and area under the risk-coverage curve (AURC) do not capture the actual reliability of predictions. These metrics either disregard confidence entirely, dilute valuable localized information through averaging, or neglect to suitably penalize overconfident misclassifications, which can be particularly detrimental in real-world systems. We introduce two new metrics Confidence-Weighted Selective Accuracy (CWSA) and its normalized variant CWSA+ that offer a principled and interpretable way to evaluate predictive models under confidence thresholds. Unlike existing methods, our metrics explicitly reward confident accuracy and penalize overconfident mistakes. They are threshold-local, decomposable, and usable in both evaluation and deployment settings where trust and risk must be quantified. Through exhaustive experiments on both real-world data sets (MNIST, CIFAR-10) and artificial model variants (calibrated, overconfident, underconfident, random, perfect), we show that CWSA and CWSA+ both effectively detect nuanced failure modes and outperform classical metrics in trust-sensitive tests. Our results confirm that CWSA is a sound basis for developing and assessing selective prediction systems for safety-critical domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18622v1",
    "published_date": "2025-05-24 10:07:48 UTC",
    "updated_date": "2025-05-24 10:07:48 UTC"
  },
  {
    "arxiv_id": "2505.18607v1",
    "title": "Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented Graphs",
    "authors": [
      "Jonathan Leung",
      "Yongjie Wang",
      "Zhiqi Shen"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate impressive general capabilities but often struggle with step-by-step reasoning, especially in complex applications such as games. While retrieval-augmented methods like GraphRAG attempt to bridge this gap through cross-document extraction and indexing, their fragmented entity-relation graphs and overly dense local connectivity hinder the construction of coherent reasoning. In this paper, we propose a novel framework based on Goal-Oriented Graphs (GoGs), where each node represents a goal and its associated attributes, and edges encode logical dependencies between goals. This structure enables explicit retrieval of reasoning paths by first identifying high-level goals and recursively retrieving their subgoals, forming coherent reasoning chains to guide LLM prompting. Our method significantly enhances the reasoning ability of LLMs in game-playing tasks, as demonstrated by extensive experiments on the Minecraft testbed, outperforming GraphRAG and other baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18607v1",
    "published_date": "2025-05-24 09:09:20 UTC",
    "updated_date": "2025-05-24 09:09:20 UTC"
  },
  {
    "arxiv_id": "2505.21537v1",
    "title": "OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models",
    "authors": [
      "Hao Sun",
      "Yunyi Shen",
      "Mihaela van der Schaar"
    ],
    "abstract": "In the era of large language models (LLMs), high-quality, domain-rich, and continuously evolving datasets capturing expert-level knowledge, core human values, and reasoning are increasingly valuable. This position paper argues that OpenReview -- the continually evolving repository of research papers, peer reviews, author rebuttals, meta-reviews, and decision outcomes -- should be leveraged more broadly as a core community asset for advancing research in the era of LLMs. We highlight three promising areas in which OpenReview can uniquely contribute: enhancing the quality, scalability, and accountability of peer review processes; enabling meaningful, open-ended benchmarks rooted in genuine expert deliberation; and supporting alignment research through real-world interactions reflecting expert assessment, intentions, and scientific values. To better realize these opportunities, we suggest the community collaboratively explore standardized benchmarks and usage guidelines around OpenReview, inviting broader dialogue on responsible data use, ethical considerations, and collective stewardship.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21537v1",
    "published_date": "2025-05-24 09:07:13 UTC",
    "updated_date": "2025-05-24 09:07:13 UTC"
  },
  {
    "arxiv_id": "2505.18605v1",
    "title": "Rethinking Causal Mask Attention for Vision-Language Inference",
    "authors": [
      "Xiaohuan Pei",
      "Tao Huang",
      "YanXiang Ma",
      "Chang Xu"
    ],
    "abstract": "Causal attention has become a foundational mechanism in autoregressive vision-language models (VLMs), unifying textual and visual inputs under a single generative framework. However, existing causal mask-based strategies are inherited from large language models (LLMs) where they are tailored for text-only decoding, and their adaptation to vision tokens is insufficiently addressed in the prefill stage. Strictly masking future positions for vision queries introduces overly rigid constraints, which hinder the model's ability to leverage future context that often contains essential semantic cues for accurate inference. In this work, we empirically investigate how different causal masking strategies affect vision-language inference and then propose a family of future-aware attentions tailored for this setting. We first empirically analyze the effect of previewing future tokens for vision queries and demonstrate that rigid masking undermines the model's capacity to capture useful contextual semantic representations. Based on these findings, we propose a lightweight attention family that aggregates future visual context into past representations via pooling, effectively preserving the autoregressive structure while enhancing cross-token dependencies. We evaluate a range of causal masks across diverse vision-language inference settings and show that selectively compressing future semantic context into past representations benefits the inference.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18605v1",
    "published_date": "2025-05-24 08:59:28 UTC",
    "updated_date": "2025-05-24 08:59:28 UTC"
  },
  {
    "arxiv_id": "2505.18603v1",
    "title": "Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning",
    "authors": [
      "Ye Mo",
      "Zirui Shao",
      "Kai Ye",
      "Xianwei Mao",
      "Bo Zhang",
      "Hangdi Xing",
      "Peng Ye",
      "Gang Huang",
      "Kehan Chen",
      "Zhou Huan",
      "Zixu Yan",
      "Sheng Zhou"
    ],
    "abstract": "Multimodal large language models (MLLMs) have made significant progress in document understanding. However, the information-dense nature of document images still poses challenges, as most queries depend on only a few relevant regions, with the rest being redundant. Existing one-pass MLLMs process entire document images without considering query relevance, often failing to focus on critical regions and producing unfaithful responses. Inspired by the human coarse-to-fine reading pattern, we introduce Doc-CoB (Chain-of-Box), a simple-yet-effective mechanism that integrates human-style visual reasoning into MLLM without modifying its architecture. Our method allows the model to autonomously select the set of regions (boxes) most relevant to the query, and then focus attention on them for further understanding. We first design a fully automatic pipeline, integrating a commercial MLLM with a layout analyzer, to generate 249k training samples with intermediate visual reasoning supervision. Then we incorporate two enabling tasks that improve box identification and box-query reasoning, which together enhance document understanding. Extensive experiments on seven benchmarks with four popular models show that Doc-CoB significantly improves performance, demonstrating its effectiveness and wide applicability. All code, data, and models will be released publicly.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18603v1",
    "published_date": "2025-05-24 08:53:05 UTC",
    "updated_date": "2025-05-24 08:53:05 UTC"
  },
  {
    "arxiv_id": "2505.18602v2",
    "title": "LLM-Meta-SR: In-Context Learning for Evolving Selection Operators in Symbolic Regression",
    "authors": [
      "Hengzhe Zhang",
      "Qi Chen",
      "Bing Xue",
      "Wolfgang Banzhaf",
      "Mengjie Zhang"
    ],
    "abstract": "Large language models (LLMs) have revolutionized algorithm development, yet their application in symbolic regression, where algorithms automatically discover symbolic expressions from data, remains constrained and is typically designed manually by human experts. In this paper, we propose a meta learning framework that enables LLMs to automatically design selection operators for evolutionary symbolic regression algorithms. We first identify two key limitations in existing LLM-based algorithm evolution techniques: a lack of semantic guidance and code bloat. The absence of semantic awareness can lead to ineffective exchange of useful code components, and bloat results in unnecessarily complex components, both of which can reduce the interpretability of the designed algorithm or hinder evolutionary learning progress. To address these issues, we enhance the LLM-based evolution framework for meta symbolic regression with two key innovations: a complementary, semantics-aware selection operator and bloat control. Additionally, we embed domain knowledge into the prompt, enabling the LLM to generate more effective and contextually relevant selection operators. Our experimental results on symbolic regression benchmarks show that LLMs can devise selection operators that outperform nine expert-designed baselines, achieving state-of-the-art performance. Moreover, the evolved operator can further improve the state-of-the-art symbolic regression algorithm, achieving the best performance among 26 symbolic regression and machine learning algorithms across 116 regression datasets. This demonstrates that LLMs can exceed expert-level algorithm design for symbolic regression.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18602v2",
    "published_date": "2025-05-24 08:52:56 UTC",
    "updated_date": "2025-08-08 06:50:37 UTC"
  },
  {
    "arxiv_id": "2505.18601v4",
    "title": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators",
    "authors": [
      "Jongwoo Ko",
      "Sungnyun Kim",
      "Sungwoo Cho",
      "Se-Young Yun"
    ],
    "abstract": "Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18601v4",
    "published_date": "2025-05-24 08:50:53 UTC",
    "updated_date": "2025-10-20 06:19:03 UTC"
  },
  {
    "arxiv_id": "2505.18600v2",
    "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment",
    "authors": [
      "Bryan Sangwoo Kim",
      "Jeongsol Kim",
      "Jong Chul Ye"
    ],
    "abstract": "Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement with high perceptual quality and fidelity. Project Page: https://bryanswkim.github.io/chain-of-zoom/ .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://bryanswkim.github.io/chain-of-zoom/",
    "pdf_url": "https://arxiv.org/pdf/2505.18600v2",
    "published_date": "2025-05-24 08:50:08 UTC",
    "updated_date": "2025-05-27 16:02:29 UTC"
  },
  {
    "arxiv_id": "2505.18597v1",
    "title": "LLMs for Supply Chain Management",
    "authors": [
      "Haojie Wang",
      "Jiuyun Jiang",
      "L. Jeff Hong",
      "Guangxin Jiang"
    ],
    "abstract": "The development of large language models (LLMs) has provided new tools for research in supply chain management (SCM). In this paper, we introduce a retrieval-augmented generation (RAG) framework that dynamically integrates external knowledge into the inference process, and develop a domain-specialized SCM LLM, which demonstrates expert-level competence by passing standardized SCM examinations and beer game tests. We further employ the use of LLMs to conduct horizontal and vertical supply chain games, in order to analyze competition and cooperation within supply chains. Our experiments show that RAG significantly improves performance on SCM tasks. Moreover, game-theoretic analysis reveals that the LLM can reproduce insights from the classical SCM literature, while also uncovering novel behaviors and offering fresh perspectives on phenomena such as the bullwhip effect. This paper opens the door for exploring cooperation and competition for complex supply chain network through the lens of LLMs.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18597v1",
    "published_date": "2025-05-24 08:46:28 UTC",
    "updated_date": "2025-05-24 08:46:28 UTC"
  },
  {
    "arxiv_id": "2505.18596v4",
    "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models",
    "authors": [
      "Chen Han",
      "Wenzhen Zheng",
      "Xijin Tang"
    ],
    "abstract": "The proliferation of misinformation in digital platforms reveals the limitations of traditional detection methods, which mostly rely on static classification and fail to capture the intricate process of real-world fact-checking. Despite advancements in Large Language Models (LLMs) that enhance automated reasoning, their application to misinformation detection remains hindered by issues of logical inconsistency and superficial verification. In response, we introduce Debate-to-Detect (D2D), a novel Multi-Agent Debate (MAD) framework that reformulates misinformation detection as a structured adversarial debate. Inspired by fact-checking workflows, D2D assigns domain-specific profiles to each agent and orchestrates a five-stage debate process, including Opening Statement, Rebuttal, Free Debate, Closing Statement, and Judgment. To transcend traditional binary classification, D2D introduces a multi-dimensional evaluation mechanism that assesses each claim across five distinct dimensions: Factuality, Source Reliability, Reasoning Quality, Clarity, and Ethics. Experiments with GPT-4o on two datasets demonstrate significant improvements over baseline methods, and the case study highlight D2D's capability to iteratively refine evidence while improving decision transparency, representing a substantial advancement towards interpretable misinformation detection. The code will be released publicly after the official publication.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper has been accepted to EMNLP 2025 (Main Conference)",
    "pdf_url": "https://arxiv.org/pdf/2505.18596v4",
    "published_date": "2025-05-24 08:44:33 UTC",
    "updated_date": "2025-08-26 10:08:51 UTC"
  },
  {
    "arxiv_id": "2505.18595v1",
    "title": "MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations",
    "authors": [
      "The Viet Bui",
      "Tien Mai",
      "Hong Thanh Nguyen"
    ],
    "abstract": "We study offline imitation learning (IL) in cooperative multi-agent settings, where demonstrations have unlabeled mixed quality - containing both expert and suboptimal trajectories. Our proposed solution is structured in two stages: trajectory labeling and multi-agent imitation learning, designed jointly to enable effective learning from heterogeneous, unlabeled data. In the first stage, we combine advances in large language models and preference-based reinforcement learning to construct a progressive labeling pipeline that distinguishes expert-quality trajectories. In the second stage, we introduce MisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn robust policies while addressing the computational complexity of large joint state-action spaces. By extending the popular single-agent DICE framework to multi-agent settings with a new value decomposition and mixing architecture, our method yields a convex policy optimization objective and ensures consistency between global and local policies. We evaluate MisoDICE on multiple standard multi-agent RL benchmarks and demonstrate superior performance, especially when expert data is scarce.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18595v1",
    "published_date": "2025-05-24 08:43:42 UTC",
    "updated_date": "2025-05-24 08:43:42 UTC"
  },
  {
    "arxiv_id": "2506.12042v1",
    "title": "CRITS: Convolutional Rectifier for Interpretable Time Series Classification",
    "authors": [
      "Alejandro Kuratomi",
      "Zed Lee",
      "Guilherme Dinis Chaliane Junior",
      "Tony Lindgren",
      "Diego Garca Prez"
    ],
    "abstract": "Several interpretability methods for convolutional network-based classifiers exist. Most of these methods focus on extracting saliency maps for a given sample, providing a local explanation that highlights the main regions for the classification. However, some of these methods lack detailed explanations in the input space due to upscaling issues or may require random perturbations to extract the explanations. We propose Convolutional Rectifier for Interpretable Time Series Classification, or CRITS, as an interpretable model for time series classification that is designed to intrinsically extract local explanations. The proposed method uses a layer of convolutional kernels, a max-pooling layer and a fully-connected rectifier network (a network with only rectified linear unit activations). The rectified linear unit activation allows the extraction of the feature weights for the given sample, eliminating the need to calculate gradients, use random perturbations and the upscale of the saliency maps to the initial input space. We evaluate CRITS on a set of datasets, and study its classification performance and its explanation alignment, sensitivity and understandability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper was presented at the 2024 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), as part of the XKDD workshop on interpretability. However it was not published in the LNCSI proceedings of the conference",
    "pdf_url": "https://arxiv.org/pdf/2506.12042v1",
    "published_date": "2025-05-24 08:34:08 UTC",
    "updated_date": "2025-05-24 08:34:08 UTC"
  },
  {
    "arxiv_id": "2505.18588v1",
    "title": "Safety Alignment via Constrained Knowledge Unlearning",
    "authors": [
      "Zesheng Shi",
      "Yucheng Zhou",
      "Jing Li"
    ],
    "abstract": "Despite significant progress in safety alignment, large language models (LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms have not fully deleted harmful knowledge in LLMs, which allows such attacks to bypass safeguards and produce harmful outputs. To address this challenge, we propose a novel safety alignment strategy, Constrained Knowledge Unlearning (CKU), which focuses on two primary objectives: knowledge localization and retention, and unlearning harmful knowledge. CKU works by scoring neurons in specific multilayer perceptron (MLP) layers to identify a subset U of neurons associated with useful knowledge. During the unlearning process, CKU prunes the gradients of neurons in U to preserve valuable knowledge while effectively mitigating harmful content. Experimental results demonstrate that CKU significantly enhances model safety without compromising overall performance, offering a superior balance between safety and utility compared to existing methods. Additionally, our analysis of neuron knowledge sensitivity across various MLP layers provides valuable insights into the mechanics of safety alignment and model knowledge editing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18588v1",
    "published_date": "2025-05-24 08:29:50 UTC",
    "updated_date": "2025-05-24 08:29:50 UTC"
  },
  {
    "arxiv_id": "2505.18587v1",
    "title": "HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection",
    "authors": [
      "Pavan C Shekar",
      "Pawan Soni",
      "Vivek Kanhangad"
    ],
    "abstract": "Deepfakes pose a significant threat to digital media security, with current detection methods struggling to generalize across different manipulation techniques and datasets. While recent approaches combine CNN-based architectures with Vision Transformers or leverage multi-modal learning, they remain limited by the inherent constraints of RGB data. We introduce HyperFake, a novel deepfake detection pipeline that reconstructs 31-channel hyperspectral data from standard RGB videos, revealing hidden manipulation traces invisible to conventional methods. Using an improved MST++ architecture, HyperFake enhances hyperspectral reconstruction, while a spectral attention mechanism selects the most critical spectral features for deepfake detection. The refined spectral data is then processed by an EfficientNet-based classifier optimized for spectral analysis, enabling more accurate and generalizable detection across different deepfake styles and datasets, all without the need for expensive hyperspectral cameras. To the best of our knowledge, this is the first approach to leverage hyperspectral imaging reconstruction for deepfake detection, opening new possibilities for detecting increasingly sophisticated manipulations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 3 figures, 1 table. Preliminary results on FaceForensics++ dataset. First approach to use hyperspectral reconstruction for deepfake detection",
    "pdf_url": "https://arxiv.org/pdf/2505.18587v1",
    "published_date": "2025-05-24 08:28:55 UTC",
    "updated_date": "2025-05-24 08:28:55 UTC"
  },
  {
    "arxiv_id": "2506.12041v3",
    "title": "Meta Pruning via Graph Metanetworks : A Universal Meta Learning Framework for Network Pruning",
    "authors": [
      "Yewei Liu",
      "Xiyuan Wang",
      "Muhan Zhang"
    ],
    "abstract": "We propose an entirely new meta-learning framework for network pruning. It is a general framework that can be theoretically applied to almost all types of networks with all kinds of pruning and has great generality and transferability. Experiments have shown that it can achieve outstanding results on many popular and representative pruning tasks (including both CNNs and Transformers). Unlike all prior works that either rely on fixed, hand-crafted criteria to prune in a coarse manner, or employ learning to prune ways that require special training during each pruning and lack generality. Our framework can learn complex pruning rules automatically via a neural network (metanetwork) and has great generality that can prune without any special training. More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically and can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and some standard finetuning to prune at state-of-the-art. Our code is available at https://github.com/Yewei-Liu/MetaPruning",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12041v3",
    "published_date": "2025-05-24 08:22:34 UTC",
    "updated_date": "2025-12-15 14:12:36 UTC"
  },
  {
    "arxiv_id": "2505.18585v3",
    "title": "RvLLM: LLM Runtime Verification with Domain Knowledge",
    "authors": [
      "Yedi Zhang",
      "Sun Yi Emma",
      "Annabelle Lee Jia En",
      "Jin Song Dong"
    ],
    "abstract": "Large language models (LLMs) have emerged as a dominant AI paradigm due to their exceptional text understanding and generation capabilities. However, their tendency to generate inconsistent or erroneous outputs challenges their reliability, especially in high-stakes domains requiring accuracy and trustworthiness. Existing research primarily focuses on detecting and mitigating model misbehavior in general-purpose scenarios, often overlooking the potential of integrating domain-specific knowledge. In this work, we advance misbehavior detection by incorporating domain knowledge. The core idea is to design a general specification language that enables domain experts to customize domain-specific predicates in a lightweight and intuitive manner, supporting later runtime verification of LLM outputs. To achieve this, we design a novel specification language, ESL, and introduce a runtime verification framework, RvLLM, to validate LLM output against domain-specific constraints defined in ESL. We evaluate RvLLM on three representative tasks: violation detection against Singapore Rapid Transit Systems Act, numerical comparison, and inequality solving. Experimental results demonstrate that RvLLM effectively detects erroneous outputs across various LLMs in a lightweight and flexible manner. The results reveal that despite their impressive capabilities, LLMs remain prone to low-level errors due to limited interpretability and a lack of formal guarantees during inference, and our framework offers a potential long-term solution by leveraging expert domain knowledge to rigorously and efficiently verify LLM outputs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 11 tables, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18585v3",
    "published_date": "2025-05-24 08:21:44 UTC",
    "updated_date": "2025-11-28 09:26:03 UTC"
  },
  {
    "arxiv_id": "2505.18582v1",
    "title": "On Denoising Walking Videos for Gait Recognition",
    "authors": [
      "Dongyang Jin",
      "Chao Fan",
      "Jingzhe Ma",
      "Jingkai Zhou",
      "Weihua Chen",
      "Shiqi Yu"
    ],
    "abstract": "To capture individual gait patterns, excluding identity-irrelevant cues in walking videos, such as clothing texture and color, remains a persistent challenge for vision-based gait recognition. Traditional silhouette- and pose-based methods, though theoretically effective at removing such distractions, often fall short of high accuracy due to their sparse and less informative inputs. Emerging end-to-end methods address this by directly denoising RGB videos using human priors. Building on this trend, we propose DenoisingGait, a novel gait denoising method. Inspired by the philosophy that \"what I cannot create, I do not understand\", we turn to generative diffusion models, uncovering how they partially filter out irrelevant factors for gait understanding. Additionally, we introduce a geometry-driven Feature Matching module, which, combined with background removal via human silhouettes, condenses the multi-channel diffusion features at each foreground pixel into a two-channel direction vector. Specifically, the proposed within- and cross-frame matching respectively capture the local vectorized structures of gait appearance and motion, producing a novel flow-like gait representation termed Gait Feature Field, which further reduces residual noise in diffusion features. Experiments on the CCPG, CASIA-B*, and SUSTech1K datasets demonstrate that DenoisingGait achieves a new SoTA performance in most cases for both within- and cross-domain evaluations. Code is available at https://github.com/ShiqiYu/OpenGait.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18582v1",
    "published_date": "2025-05-24 08:17:34 UTC",
    "updated_date": "2025-05-24 08:17:34 UTC"
  },
  {
    "arxiv_id": "2505.18581v1",
    "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG",
    "authors": [
      "Wentao Hu",
      "Wengyu Zhang",
      "Yiyang Jiang",
      "Chen Jason Zhang",
      "Xiaoyong Wei",
      "Qing Li"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external knowledge, yet it introduces a critical issue: erroneous or biased retrieval can mislead generation, compounding hallucinations, a phenomenon we term Hallucination on Hallucination. To address this, we propose Debate-Augmented RAG (DRAG), a training-free framework that integrates Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages. In retrieval, DRAG employs structured debates among proponents, opponents, and judges to refine retrieval quality and ensure factual reliability. In generation, DRAG introduces asymmetric information roles and adversarial debates, enhancing reasoning robustness and mitigating factual inconsistencies. Evaluations across multiple tasks demonstrate that DRAG improves retrieval reliability, reduces RAG-induced hallucinations, and significantly enhances overall factual accuracy. Our code is available at https://github.com/Huenao/Debate-Augmented-RAG.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18581v1",
    "published_date": "2025-05-24 08:15:22 UTC",
    "updated_date": "2025-05-24 08:15:22 UTC"
  },
  {
    "arxiv_id": "2505.18575v1",
    "title": "Response Uncertainty and Probe Modeling: Two Sides of the Same Coin in LLM Interpretability?",
    "authors": [
      "Yongjie Wang",
      "Yibo Wang",
      "Xin Zhou",
      "Zhiqi Shen"
    ],
    "abstract": "Probing techniques have shown promise in revealing how LLMs encode human-interpretable concepts, particularly when applied to curated datasets. However, the factors governing a dataset's suitability for effective probe training are not well-understood. This study hypothesizes that probe performance on such datasets reflects characteristics of both the LLM's generated responses and its internal feature space. Through quantitative analysis of probe performance and LLM response uncertainty across a series of tasks, we find a strong correlation: improved probe performance consistently corresponds to a reduction in response uncertainty, and vice versa. Subsequently, we delve deeper into this correlation through the lens of feature importance analysis. Our findings indicate that high LLM response variance is associated with a larger set of important features, which poses a greater challenge for probe models and often results in diminished performance. Moreover, leveraging the insights from response uncertainty analysis, we are able to identify concrete examples where LLM representations align with human knowledge across diverse domains, offering additional evidence of interpretable reasoning in LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 Pages",
    "pdf_url": "https://arxiv.org/pdf/2505.18575v1",
    "published_date": "2025-05-24 07:37:12 UTC",
    "updated_date": "2025-05-24 07:37:12 UTC"
  },
  {
    "arxiv_id": "2505.18574v5",
    "title": "Autocomp: A Powerful and Portable Code Optimizer for Tensor Accelerators",
    "authors": [
      "Charles Hong",
      "Sahil Bhatia",
      "Alvin Cheung",
      "Yakun Sophia Shao"
    ],
    "abstract": "Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages, such as specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three distinct hardware platforms, we demonstrate that Autocomp-optimized code runs 5.6x faster than the vendor-provided library (Gemmini), outperforms expert-level hand-tuned code by 1.9x (AWS Trainium), and achieves 3.8x higher performance than a machine learning-based cost model for GPUs (NVIDIA L40S). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "comment": "10 pages + appendices",
    "pdf_url": "https://arxiv.org/pdf/2505.18574v5",
    "published_date": "2025-05-24 07:35:34 UTC",
    "updated_date": "2025-11-05 23:37:18 UTC"
  },
  {
    "arxiv_id": "2505.18572v1",
    "title": "MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework",
    "authors": [
      "Yifan Zhu",
      "Chao Zhang",
      "Xin Shi",
      "Xueqiao Zhang",
      "Yi Yang",
      "Yawei Luo"
    ],
    "abstract": "Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit remarkable problem-solving and task planning capabilities across diverse domains due to their specialized agentic roles and collaborative interactions. However, this also amplifies the severity of security risks under MAS attacks. To address this, we introduce MASTER, a novel security research framework for MAS, focusing on diverse Role configurations and Topological structures across various scenarios. MASTER offers an automated construction process for different MAS setups and an information-flow-based interaction paradigm. To tackle MAS security challenges in varied scenarios, we design a scenario-adaptive, extensible attack strategy utilizing role and topological information, which dynamically allocates targeted, domain-specific attack tasks for collaborative agent execution. Our experiments demonstrate that such an attack, leveraging role and topological information, exhibits significant destructive potential across most models. Additionally, we propose corresponding defense strategies, substantially enhancing MAS resilience across diverse scenarios. We anticipate that our framework and findings will provide valuable insights for future research into MAS security challenges.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18572v1",
    "published_date": "2025-05-24 07:24:29 UTC",
    "updated_date": "2025-05-24 07:24:29 UTC"
  },
  {
    "arxiv_id": "2505.18568v1",
    "title": "Learning without Isolation: Pathway Protection for Continual Learning",
    "authors": [
      "Zhikang Chen",
      "Abudukelimu Wuerkaixi",
      "Sen Cui",
      "Haoxuan Li",
      "Ding Li",
      "Jingfeng Zhang",
      "Bo Han",
      "Gang Niu",
      "Houfang Liu",
      "Yi Yang",
      "Sifan Yang",
      "Changshui Zhang",
      "Tianling Ren"
    ],
    "abstract": "Deep networks are prone to catastrophic forgetting during sequential task learning, i.e., losing the knowledge about old tasks upon learning new tasks. To this end, continual learning(CL) has emerged, whose existing methods focus mostly on regulating or protecting the parameters associated with the previous tasks. However, parameter protection is often impractical, since the size of parameters for storing the old-task knowledge increases linearly with the number of tasks, otherwise it is hard to preserve the parameters related to the old-task knowledge. In this work, we bring a dual opinion from neuroscience and physics to CL: in the whole networks, the pathways matter more than the parameters when concerning the knowledge acquired from the old tasks. Following this opinion, we propose a novel CL framework, learning without isolation(LwI), where model fusion is formulated as graph matching and the pathways occupied by the old tasks are protected without being isolated. Thanks to the sparsity of activation channels in a deep network, LwI can adaptively allocate available pathways for a new task, realizing pathway protection and addressing catastrophic forgetting in a parameter-efficient manner. Experiments on popular benchmark datasets demonstrate the superiority of the proposed LwI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.18568v1",
    "published_date": "2025-05-24 07:16:55 UTC",
    "updated_date": "2025-05-24 07:16:55 UTC"
  },
  {
    "arxiv_id": "2505.18563v1",
    "title": "PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient Collective Communication in Distributed Deep Learning",
    "authors": [
      "Yisu Wang",
      "Ruilong Wu",
      "Xinjiao Li",
      "Dirk Kutscher"
    ],
    "abstract": "Large-scale deep neural networks (DNN) exhibit excellent performance for various tasks. As DNNs and datasets grow, distributed training becomes extremely time-consuming and demands larger clusters. A main bottleneck is the resulting gradient aggregation overhead. While gradient compression and sparse collective communication techniques are commonly employed to alleviate network load, many gradient compression schemes do not achieve acceleration of the training process while also preserving accuracy. This paper introduces PacTrain, a novel framework that accelerates distributed training by combining pruning with sparse gradient compression. Active pruning of the neural network makes the model weights and gradients sparse. By ensuring the global knowledge of the gradient sparsity among all distributed training workers, we can perform lightweight compression communication without harming accuracy. We show that the PacTrain compression scheme achieves a near-optimal compression strategy while remaining compatible with the all-reduce primitive. Experimental evaluations show that PacTrain improves training throughput by 1.25 to 8.72 times compared to state-of-the-art compression-enabled systems for representative vision and language models training tasks under bandwidth-constrained conditions.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18563v1",
    "published_date": "2025-05-24 07:06:36 UTC",
    "updated_date": "2025-05-24 07:06:36 UTC"
  },
  {
    "arxiv_id": "2505.18562v2",
    "title": "From Word to World: Evaluate and Mitigate Culture Bias in LLMs via Word Association Test",
    "authors": [
      "Xunlian Dai",
      "Li Zhou",
      "Benyou Wang",
      "Haizhou Li"
    ],
    "abstract": "The human-centered word association test (WAT) serves as a cognitive proxy, revealing sociocultural variations through culturally shared semantic expectations and implicit linguistic patterns shaped by lived experiences. We extend this test into an LLM-adaptive, free-relation task to assess the alignment of large language models (LLMs) with cross-cultural cognition. To address culture preference, we propose CultureSteer, an innovative approach that moves beyond superficial cultural prompting by embedding cultural-specific semantic associations directly within the model's internal representation space. Experiments show that current LLMs exhibit significant bias toward Western (notably American) schemas at the word association level. In contrast, our model substantially improves cross-cultural alignment, capturing diverse semantic associations. Further validation on culture-sensitive downstream tasks confirms its efficacy in fostering cognitive alignment across cultures. This work contributes a novel methodological paradigm for enhancing cultural awareness in LLMs, advancing the development of more inclusive language technologies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Cultural Analysis, Cultural Alignment, Word Association Test, Large Language Models. Accepted by EMNLP 2025 (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2505.18562v2",
    "published_date": "2025-05-24 07:05:10 UTC",
    "updated_date": "2025-10-06 04:31:03 UTC"
  },
  {
    "arxiv_id": "2505.18556v2",
    "title": "Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation",
    "authors": [
      "Jun Zhuang",
      "Haibo Jin",
      "Ye Zhang",
      "Zhengjian Kang",
      "Wenbin Zhang",
      "Gaby G. Dagher",
      "Haohan Wang"
    ],
    "abstract": "Intent detection, a core component of natural language understanding, has considerably evolved as a crucial mechanism in safeguarding large language models (LLMs). While prior work has applied intent detection to enhance LLMs' moderation guardrails, showing a significant success against content-level jailbreaks, the robustness of these intent-aware guardrails under malicious manipulations remains under-explored. In this work, we investigate the vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit implicit intent detection capabilities. We propose a two-stage intent-based prompt-refinement framework, IntentPrompt, that first transforms harmful inquiries into structured outlines and further reframes them into declarative-style narratives by iteratively optimizing prompts via feedback loops to enhance jailbreak success for red-teaming purposes. Extensive experiments across four public benchmarks and various black-box LLMs indicate that our framework consistently outperforms several cutting-edge jailbreak methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought (CoT)-based defenses. Specifically, our \"FSTR+SPIN\" variant achieves attack success rates ranging from 88.25% to 96.54% against CoT-based defenses on the o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based defenses. These findings highlight a critical weakness in LLMs' safety mechanisms and suggest that intent manipulation poses a growing challenge to content moderation guardrails.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for EMNLP'25 Findings. TL;DR: We propose a new two-stage intent-based prompt-refinement framework, IntentPrompt, that aims to explore the vulnerability of LLMs' content moderation guardrails by refining prompts into benign-looking declarative forms via intent manipulation for red-teaming purposes",
    "pdf_url": "https://arxiv.org/pdf/2505.18556v2",
    "published_date": "2025-05-24 06:47:32 UTC",
    "updated_date": "2025-08-25 00:27:19 UTC"
  },
  {
    "arxiv_id": "2505.18547v1",
    "title": "Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models",
    "authors": [
      "Min Cheng",
      "Fatemeh Doudi",
      "Dileep Kalathil",
      "Mohammad Ghavamzadeh",
      "Panganamala R. Kumar"
    ],
    "abstract": "Reinforcement learning (RL) algorithms have been used recently to align diffusion models with downstream objectives such as aesthetic quality and text-image consistency by fine-tuning them to maximize a single reward function under a fixed KL regularization. However, this approach is inherently restrictive in practice, where alignment must balance multiple, often conflicting objectives. Moreover, user preferences vary across prompts, individuals, and deployment contexts, with varying tolerances for deviation from a pre-trained base model. We address the problem of inference-time multi-preference alignment: given a set of basis reward functions and a reference KL regularization strength, can we design a fine-tuning procedure so that, at inference time, it can generate images aligned with any user-specified linear combination of rewards and regularization, without requiring additional fine-tuning? We propose Diffusion Blend, a novel approach to solve inference-time multi-preference alignment by blending backward diffusion processes associated with fine-tuned models, and we instantiate this approach with two algorithms: DB-MPA for multi-reward alignment and DB-KLA for KL regularization control. Extensive experiments show that Diffusion Blend algorithms consistently outperform relevant baselines and closely match or exceed the performance of individually fine-tuned models, enabling efficient, user-driven alignment at inference-time. The code is available at https://github.com/bluewoods127/DB-2025}{github.com/bluewoods127/DB-2025.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18547v1",
    "published_date": "2025-05-24 06:27:55 UTC",
    "updated_date": "2025-05-24 06:27:55 UTC"
  },
  {
    "arxiv_id": "2505.18541v1",
    "title": "RoleRAG: Enhancing LLM Role-Playing via Graph Guided Retrieval",
    "authors": [
      "Yongjie Wang",
      "Jonathan Leung",
      "Zhiqi Shen"
    ],
    "abstract": "Large Language Models (LLMs) have shown promise in character imitation, enabling immersive and engaging conversations. However, they often generate content that is irrelevant or inconsistent with a character's background. We attribute these failures to: (1) the inability to accurately recall character-specific knowledge due to entity ambiguity, and (2) a lack of awareness of the character's cognitive boundaries. To address these issues, we propose RoleRAG, a retrieval-based framework that integrates efficient entity disambiguation for knowledge indexing with a boundary-aware retriever for extracting contextually appropriate information from a structured knowledge graph. Experiments on role-playing benchmarks show that RoleRAG's calibrated retrieval helps both general-purpose and role-specific LLMs better align with character knowledge and reduce hallucinated responses.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "A Retrieval-enhanced LLM Role-playing",
    "pdf_url": "https://arxiv.org/pdf/2505.18541v1",
    "published_date": "2025-05-24 06:11:17 UTC",
    "updated_date": "2025-05-24 06:11:17 UTC"
  },
  {
    "arxiv_id": "2505.18536v1",
    "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models",
    "authors": [
      "Haoyuan Sun",
      "Jiaqi Wu",
      "Bo Xia",
      "Yifu Luo",
      "Yifei Zhao",
      "Kai Qin",
      "Xufei Lv",
      "Tiantian Zhang",
      "Yongzhe Chang",
      "Xueqian Wang"
    ],
    "abstract": "Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18536v1",
    "published_date": "2025-05-24 06:01:48 UTC",
    "updated_date": "2025-05-24 06:01:48 UTC"
  },
  {
    "arxiv_id": "2505.18533v1",
    "title": "TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network",
    "authors": [
      "Xiaobin Rong",
      "Dahan Wang",
      "Qinwen Hu",
      "Yushi Wang",
      "Yuxiang Hu",
      "Jing Lu"
    ],
    "abstract": "Universal speech enhancement aims to handle input speech with different distortions and input formats. To tackle this challenge, we present TS-URGENet, a Three-Stage Universal, Robust, and Generalizable speech Enhancement Network. To address various distortions, the proposed system employs a novel three-stage architecture consisting of a filling stage, a separation stage, and a restoration stage. The filling stage mitigates packet loss by preliminarily filling lost regions under noise interference, ensuring signal continuity. The separation stage suppresses noise, reverberation, and clipping distortion to improve speech clarity. Finally, the restoration stage compensates for bandwidth limitation, codec artifacts, and residual packet loss distortion, refining the overall speech quality. Our proposed TS-URGENet achieved outstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd in Track 1.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18533v1",
    "published_date": "2025-05-24 05:53:05 UTC",
    "updated_date": "2025-05-24 05:53:05 UTC"
  },
  {
    "arxiv_id": "2505.18531v1",
    "title": "Generative RLHF-V: Learning Principles from Multi-modal Human Preference",
    "authors": [
      "Jiayi Zhou",
      "Jiaming Ji",
      "Boyuan Chen",
      "Jiapeng Sun",
      "Wenqi Chen",
      "Donghai Hong",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "abstract": "Training multi-modal large language models (MLLMs) that align with human intentions is a long-term challenge. Traditional score-only reward models for alignment suffer from low accuracy, weak generalization, and poor interpretability, blocking the progress of alignment methods, e.g., reinforcement learning from human feedback (RLHF). Generative reward models (GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate pair-wise responses, but their pair-wise paradigm makes it hard to generalize to learnable rewards. We introduce Generative RLHF-V, a novel alignment framework that integrates GRMs with multi-modal RLHF. We propose a two-stage pipeline: $\\textbf{multi-modal generative reward modeling from RL}$, where RL guides GRMs to actively capture human intention, then predict the correct pair-wise scores; and $\\textbf{RL optimization from grouped comparison}$, which enhances multi-modal RL scoring precision by grouped responses comparison. Experimental results demonstrate that, besides out-of-distribution generalization of RM discrimination, our framework improves 4 MLLMs' performance across 7 benchmarks by $18.1\\%$, while the baseline RLHF is only $5.3\\%$. We further validate that Generative RLHF-V achieves a near-linear improvement with an increasing number of candidate responses. Our code and models can be found at https://generative-rlhf-v.github.io.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18531v1",
    "published_date": "2025-05-24 05:50:07 UTC",
    "updated_date": "2025-05-24 05:50:07 UTC"
  },
  {
    "arxiv_id": "2505.18530v1",
    "title": "MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation with Med-LVLMs",
    "authors": [
      "Pengyu Wang",
      "Shuchang Ye",
      "Usman Naseem",
      "Jinman Kim"
    ],
    "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have been widely adopted for medical report generation. Despite Med-LVLMs producing state-of-the-art performance, they exhibit a bias toward predicting all findings as normal, leading to reports that overlook critical abnormalities. Furthermore, these models often fail to provide comprehensive descriptions of radiologically relevant regions necessary for accurate diagnosis. To address these challenges, we proposeMedical Report Generation Agents (MRGAgents), a novel multi-agent framework that fine-tunes specialized agents for different disease categories. By curating subsets of the IU X-ray and MIMIC-CXR datasets to train disease-specific agents, MRGAgents generates reports that more effectively balance normal and abnormal findings while ensuring a comprehensive description of clinically relevant regions. Our experiments demonstrate that MRGAgents outperformed the state-of-the-art, improving both report comprehensiveness and diagnostic utility.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "10pages",
    "pdf_url": "https://arxiv.org/pdf/2505.18530v1",
    "published_date": "2025-05-24 05:49:42 UTC",
    "updated_date": "2025-05-24 05:49:42 UTC"
  },
  {
    "arxiv_id": "2505.18527v1",
    "title": "CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs",
    "authors": [
      "Yiqing Zhang",
      "Xiaozhong Liu",
      "Fabricio Murai"
    ],
    "abstract": "Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data. While this scheme may boost prediction for common diseases and drugs, it can hinder learning of generalizable representations, leading to more false positives/negatives. To address this limitation, we introduce CLaDMoP, a new pre-training approach for clinical trial outcome prediction, alongside the Successful Clinical Trials dataset(SCT), specifically designed for this task. CLaDMoP leverages a Large Language Model-to encode trials' eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. To efficiently fuse long embeddings across levels, we incorporate a grouping block, drastically reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a \"pair matching\" proxy task. Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. We further evaluate and perform ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP, highlighting its potential for clinical trial outcome prediction. Code and SCT dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted and to be published in KDD2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18527v1",
    "published_date": "2025-05-24 05:45:32 UTC",
    "updated_date": "2025-05-24 05:45:32 UTC"
  },
  {
    "arxiv_id": "2505.18517v1",
    "title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs",
    "authors": [
      "Pooneh Mousavi",
      "Shubham Gupta",
      "Cem Subakan",
      "Mirco Ravanelli"
    ],
    "abstract": "Foundation models based on large language models (LLMs) have shown great success in handling various tasks and modalities. However, adapting these models for general-purpose audio-language tasks is challenging due to differences in acoustic environments and task variations. In this work, we introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic prompt selection strategy with learnable key-value pairs, allowing the model to balance general and task-specific knowledge while avoiding overfitting in a multitask setting. Our approach reduces dependence on large-scale ASR or captioning datasets, achieves competitive performance with fewer trainable parameters, and simplifies training by using a single-stage process. Additionally, LiSTEN enhances interpretability by analyzing the diversity and overlap of selected prompts across different tasks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18517v1",
    "published_date": "2025-05-24 05:28:22 UTC",
    "updated_date": "2025-05-24 05:28:22 UTC"
  },
  {
    "arxiv_id": "2505.18514v1",
    "title": "Test-Time Adaptation with Binary Feedback",
    "authors": [
      "Taeckyung Lee",
      "Sorn Chottananurak",
      "Junsu Kim",
      "Jinwoo Shin",
      "Taesik Gong",
      "Sung-Ju Lee"
    ],
    "abstract": "Deep learning models perform poorly when domain shifts exist between training and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue by adapting pre-trained models using only unlabeled test samples. However, existing TTA methods can fail under severe domain shifts, while recent active TTA approaches requiring full-class labels are impractical due to high labeling costs. To address this issue, we introduce a new setting of TTA with binary feedback. This setting uses a few binary feedback inputs from annotators to indicate whether model predictions are correct, thereby significantly reducing the labeling burden of annotators. Under the setting, we propose BiTTA, a novel dual-path optimization framework that leverages reinforcement learning to balance binary feedback-guided adaptation on uncertain samples with agreement-based self-adaptation on confident predictions. Experiments show BiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines, demonstrating its effectiveness in handling severe distribution shifts with minimal labeling effort. The source code is available at https://github.com/taeckyung/BiTTA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.18514v1",
    "published_date": "2025-05-24 05:24:10 UTC",
    "updated_date": "2025-05-24 05:24:10 UTC"
  },
  {
    "arxiv_id": "2505.18512v2",
    "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking",
    "authors": [
      "Soyoung Yoon",
      "Gyuwan Kim",
      "Gyu-Hwung Cho",
      "Seung-won Hwang"
    ],
    "abstract": "Listwise reranking with large language models (LLMs) enhances top-ranked results in retrieval-based applications. Due to the limit in context size and high inference cost of long context, reranking is typically performed over a fixed size of small subsets, with the final ranking aggregated from these partial results. This fixed computation disregards query difficulty and document distribution, leading to inefficiencies. We propose AcuRank, an adaptive reranking framework that dynamically adjusts both the amount and target of computation based on uncertainty estimates over document relevance. Using a Bayesian TrueSkill model, we iteratively refine relevance estimates until reaching sufficient confidence levels, and our explicit modeling of ranking uncertainty enables principled control over reranking behavior and avoids unnecessary updates to confident predictions. Results on the TREC-DL and BEIR benchmarks show that our method consistently achieves a superior accuracy-efficiency trade-off and scales better with compute than fixed-computation baselines. These results highlight the effectiveness and generalizability of our method across diverse retrieval tasks and LLM-based reranking models.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at NeurIPS 2025. The first two authors contributed equally. Author order is randomly determined via coin toss",
    "pdf_url": "https://arxiv.org/pdf/2505.18512v2",
    "published_date": "2025-05-24 05:15:49 UTC",
    "updated_date": "2025-10-24 06:55:54 UTC"
  },
  {
    "arxiv_id": "2505.18505v1",
    "title": "How Particle System Theory Enhances Hypergraph Message Passing",
    "authors": [
      "Yixuan Ma",
      "Kai Yi",
      "Pietro Lio",
      "Shi Jin",
      "Yu Guang Wang"
    ],
    "abstract": "Hypergraphs effectively model higher-order relationships in natural phenomena, capturing complex interactions beyond pairwise connections. We introduce a novel hypergraph message passing framework inspired by interacting particle systems, where hyperedges act as fields inducing shared node dynamics. By incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles of varying classes and features achieve class-dependent equilibrium, enabling separability through the particle-driven message passing. We investigate both first-order and second-order particle system equations for modeling these dynamics, which mitigate over-smoothing and heterophily thus can capture complete interactions. The more stable second-order system permits deeper message passing. Furthermore, we enhance deterministic message passing with stochastic element to account for interaction uncertainties. We prove theoretically that our approach mitigates over-smoothing by maintaining a positive lower bound on the hypergraph Dirichlet energy during propagation and thus to enable hypergraph message passing to go deep. Empirically, our models demonstrate competitive performance on diverse real-world hypergraph node classification tasks, excelling on both homophilic and heterophilic datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18505v1",
    "published_date": "2025-05-24 05:04:25 UTC",
    "updated_date": "2025-05-24 05:04:25 UTC"
  },
  {
    "arxiv_id": "2505.18502v1",
    "title": "Knowledge Grafting of Large Language Models",
    "authors": [
      "Guodong Du",
      "Xuanning Zhou",
      "Junlin Li",
      "Zhuo Li",
      "Zesheng Shi",
      "Wanyu Lin",
      "Ho-Kin Tang",
      "Xiucheng Li",
      "Fangming Liu",
      "Wenya Wang",
      "Min Zhang",
      "Jing Li"
    ],
    "abstract": "Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightweight models, enhancing adaptability and efficiency, which motivates our investigation into more efficient cross-capability transfer methods. However, existing approaches primarily focus on small, homogeneous models, limiting their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model's intrinsic capacity and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs. To address these issues, we introduce GraftLLM, a novel method that stores source model capabilities in a target model with SkillPack format. This approach preserves general capabilities, reduces parameter conflicts, and supports forget-free continual learning and model fusion. We employ a module-aware adaptive compression strategy to compress parameter updates, ensuring efficient storage while maintaining task-specific knowledge. The resulting SkillPack serves as a compact and transferable knowledge carrier, ideal for heterogeneous model fusion and continual learning. Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning, providing a scalable and efficient solution for cross-capability transfer. The code is publicly available at: https://github.com/duguodong7/GraftLLM.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18502v1",
    "published_date": "2025-05-24 04:43:24 UTC",
    "updated_date": "2025-05-24 04:43:24 UTC"
  },
  {
    "arxiv_id": "2505.18499v3",
    "title": "G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning",
    "authors": [
      "Xiaojun Guo",
      "Ang Li",
      "Yifei Wang",
      "Stefanie Jegelka",
      "Yisen Wang"
    ],
    "abstract": "Although Large Language Models (LLMs) have demonstrated remarkable progress, their proficiency in graph-related tasks remains notably limited, hindering the development of truly general-purpose models. Previous attempts, including pretraining graph foundation models or employing supervised fine-tuning, often face challenges such as the scarcity of large-scale, universally represented graph data. We introduce G1, a simple yet effective approach demonstrating that Reinforcement Learning (RL) on synthetic graph-theoretic tasks can significantly scale LLMs' graph reasoning abilities. To enable RL training, we curate Erds, the largest graph reasoning dataset to date comprising 50 diverse graph-theoretic tasks of varying difficulty levels, 100k training data and 5k test data, all drived from real-world graphs. With RL on Erds, G1 obtains substantial improvements in graph reasoning, where our finetuned 3B model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also show strong zero-shot generalization to unseen tasks, domains, and graph encoding schemes, including other graph-theoretic benchmarks as well as real-world node classification and link prediction tasks, without compromising general reasoning abilities. Our findings offer an efficient, scalable path for building strong graph reasoners by finetuning LLMs with RL on graph-theoretic tasks, which combines the strengths of pretrained LLM capabilities with abundant, automatically generated synthetic data, suggesting that LLMs possess graph understanding abilities that RL can elicit successfully. Our implementation is open-sourced at https://github.com/PKU-ML/G1, with models and datasets hosted on Hugging Face collections https://huggingface.co/collections/PKU-ML/g1-683d659e992794fc99618cf2 for broader accessibility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18499v3",
    "published_date": "2025-05-24 04:33:41 UTC",
    "updated_date": "2025-08-19 01:33:43 UTC"
  },
  {
    "arxiv_id": "2505.18494v1",
    "title": "FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation",
    "authors": [
      "Zihao Peng",
      "Jiandian Zeng",
      "Boyuan Li",
      "Guo Li",
      "Shengbo Chen",
      "Tian Wang"
    ],
    "abstract": "Federated Learning (FL) facilitates the fine-tuning of Foundation Models (FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining popularity due to its low communication costs and strong performance. While recent work acknowledges the benefits of heterogeneous LoRA in FL and introduces flexible algorithms to support its implementation, our theoretical analysis reveals a critical gap: existing methods lack formal convergence guarantees due to parameter truncation and biased gradient updates. Specifically, adapting client-specific LoRA ranks necessitates truncating global parameters, which introduces inherent truncation errors and leads to subsequent inaccurate gradient updates that accumulate over training rounds, ultimately degrading performance. To address the above issues, we propose \\textbf{FedHL}, a simple yet effective \\textbf{Fed}erated Learning framework tailored for \\textbf{H}eterogeneous \\textbf{L}oRA. By leveraging the full-rank global model as a calibrated aggregation basis, FedHL eliminates the direct truncation bias from initial alignment with client-specific ranks. Furthermore, we derive the theoretically optimal aggregation weights by minimizing the gradient drift term in the convergence upper bound. Our analysis shows that FedHL guarantees $\\mathcal{O}(1/\\sqrt{T})$ convergence rate, and experiments on multiple real-world datasets demonstrate a 1-3\\% improvement over several state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18494v1",
    "published_date": "2025-05-24 04:12:12 UTC",
    "updated_date": "2025-05-24 04:12:12 UTC"
  },
  {
    "arxiv_id": "2506.12040v1",
    "title": "BTC-LLM: Efficient Sub-1-Bit LLM Quantization via Learnable Transformation and Binary Codebook",
    "authors": [
      "Hao Gu",
      "Lujun Li",
      "Zheyu Wang",
      "Bei Liu",
      "Qiyuan Zhu",
      "Sirui Han",
      "Yike Guo"
    ],
    "abstract": "Binary quantization represents the most extreme form of large language model (LLM) compression, reducing weights to $\\pm$1 for maximal memory and computational efficiency. While recent sparsity-aware binarization methods achieve sub-1-bit compression by pruning redundant binary weights, they suffer from three critical challenges: performance deterioration, computational complexity from sparse mask management, and limited hardware compatibility. In this paper, we present BTC-LLM, a novel sub-1-bit LLM quantization framework that leverages adaptive weight transformation and binary pattern clustering to overcome these limitations, delivering both superior accuracy and efficiency. Our approach incorporates two key innovations: (1) a Learnable Transformation that optimizes invertible scaling and rotation matrices to align binarized weights with full-precision distributions, enabling incoherence processing to enhance layer-wise representation quality; (2) a Flash and Accurate Binary Codebook that identifies recurring binary vector clusters, compressing them into compact indices with tailored distance metrics and sign-based centroid updates. This eliminates the need for sparse masks, enabling efficient inference on standard hardware. Our code is available at https://github.com/Chooovy/BTC-LLM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12040v1",
    "published_date": "2025-05-24 03:57:19 UTC",
    "updated_date": "2025-05-24 03:57:19 UTC"
  },
  {
    "arxiv_id": "2505.18492v4",
    "title": "Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions",
    "authors": [
      "Jialiang Sun",
      "Yuzhi Tang",
      "Ao Li",
      "Chris J. Maddison",
      "Kuldeep S. Meel"
    ],
    "abstract": "Mathematical reasoning is central to artificial intelligence, with applications in education, code generation, and research-level mathematical discovery. Mathematical competitions highlight two problem types: theorem proving, requiring rigorous proofs, and answer construction, requiring creative generation and formal verification of mathematical objects. Existing research reveals that LLMs can tackle difficult answer-construction tasks but are prone to errors from hallucinations and unverifiable steps, while symbolic methods guarantee rigor but falter in creative answer construction. This raises a key understudied question: how to solve answer-construction problems while preserving both LLM creativity and mathematical rigor? To address this problem, we introduce the Enumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method integrating LLM-based enumeration and pattern-driven conjecturing with formal theorem proving in Lean, and ConstructiveBench, a dataset of 3,640 formal answer-construction problems from math competitions. ECP is model agnostic and shows consistent improvements over pure LLM baselines: on the subset of PutnamBench for answer construction, ECP formally solves 6 out of 337 answer-construction problems end to end (up from 4 without ECP) using GPT-5 mini and DeepSeek-Prover-V2-7B. On ConstructiveBench, ECP achieves 33.1% end-to-end state-of-the-art accuracy (up from 32.5%), demonstrating its potential to advance formal mathematical reasoning by combining LLM conjecturing with formal verification. Our code and dataset are publicly available at GitHub (https://github.com/sunjia72/ECP) and Hugging Face (https://huggingface.co/datasets/sunjia72/ConstructiveBench).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18492v4",
    "published_date": "2025-05-24 03:52:25 UTC",
    "updated_date": "2025-10-18 04:19:19 UTC"
  },
  {
    "arxiv_id": "2505.18488v1",
    "title": "Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications",
    "authors": [
      "Yanxiang Zhang",
      "Zheng Xu",
      "Shanshan Wu",
      "Yuanbo Zhang",
      "Daniel Ramage"
    ],
    "abstract": "Error correction is an important capability when applying large language models (LLMs) to facilitate user typing on mobile devices. In this paper, we use LLMs to synthesize a high-quality dataset of error correction pairs to evaluate and improve LLMs for mobile applications. We first prompt LLMs with error correction domain knowledge to build a scalable and reliable addition to the existing data synthesis pipeline. We then adapt the synthetic data distribution to match the mobile application domain by reweighting the samples. The reweighting model is learnt by predicting (a handful of) live A/B test metrics when deploying LLMs in production, given the LLM performance on offline evaluation data and scores from a small privacy-preserving on-device language model. Finally, we present best practices for mixing our synthetic data with other data sources to improve model performance on error correction in both offline evaluation and production live A/B testing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ACL Industry",
    "pdf_url": "https://arxiv.org/pdf/2505.18488v1",
    "published_date": "2025-05-24 03:27:20 UTC",
    "updated_date": "2025-05-24 03:27:20 UTC"
  },
  {
    "arxiv_id": "2505.18483v1",
    "title": "Retrieval Augmented Decision-Making: A Requirements-Driven, Multi-Criteria Framework for Structured Decision Support",
    "authors": [
      "Hongjia Wu",
      "Hongxin Zhang",
      "Wei Chen",
      "Jiazhi Xia"
    ],
    "abstract": "Various industries have produced a large number of documents such as industrial plans, technical guidelines, and regulations that are structurally complex and content-wise fragmented. This poses significant challenges for experts and decision-makers in terms of retrieval and understanding. Although existing LLM-based Retrieval-Augmented Generation methods can provide context-related suggestions, they lack quantitative weighting and traceable reasoning paths, making it difficult to offer multi-level and transparent decision support. To address this issue, this paper proposes the RAD method, which integrates Multi-Criteria Decision Making with the semantic understanding capabilities of LLMs. The method automatically extracts key criteria from industry documents, builds a weighted hierarchical decision model, and generates structured reports under model guidance. The RAD framework introduces explicit weight assignment and reasoning chains in decision generation to ensure accuracy, completeness, and traceability. Experiments show that in various decision-making tasks, the decision reports generated by RAD significantly outperform existing methods in terms of detail, rationality, and structure, demonstrating its application value and potential in complex decision support scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18483v1",
    "published_date": "2025-05-24 03:13:29 UTC",
    "updated_date": "2025-05-24 03:13:29 UTC"
  },
  {
    "arxiv_id": "2505.18475v2",
    "title": "A Survey of Large Language Models for Data Challenges in Graphs",
    "authors": [
      "Mengran Li",
      "Pengyu Zhang",
      "Wenbin Xing",
      "Yijia Zheng",
      "Klim Zaporojets",
      "Junzhou Chen",
      "Ronghui Zhang",
      "Yong Zhang",
      "Siyuan Gong",
      "Jia Hu",
      "Xiaolei Ma",
      "Zhiyuan Liu",
      "Paul Groth",
      "Marcel Worring"
    ],
    "abstract": "Graphs are a widely used paradigm for representing non-Euclidean data, with applications ranging from social network analysis to biomolecular prediction. While graph learning has achieved remarkable progress, real-world graph data presents a number of challenges that significantly hinder the learning process. In this survey, we focus on four fundamental data-centric challenges: (1) Incompleteness, real-world graphs have missing nodes, edges, or attributes; (2) Imbalance, the distribution of the labels of nodes or edges and their structures for real-world graphs are highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains exhibit incompatible feature spaces or structural patterns; and (4) Dynamic Instability, graphs evolve over time in unpredictable ways. Recently, Large Language Models (LLMs) offer the potential to tackle these challenges by leveraging rich semantic reasoning and external knowledge. This survey focuses on how LLMs can address four fundamental data-centric challenges in graph-structured data, thereby improving the effectiveness of graph learning. For each challenge, we review both traditional solutions and modern LLM-driven approaches, highlighting how LLMs contribute unique advantages. Finally, we discuss open research questions and promising future directions in this emerging interdisciplinary field. To support further exploration, we have curated a repository of recent advances on graph learning challenges: https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Expert Systems with Applications",
    "pdf_url": "https://arxiv.org/pdf/2505.18475v2",
    "published_date": "2025-05-24 02:38:14 UTC",
    "updated_date": "2025-09-18 05:51:08 UTC"
  },
  {
    "arxiv_id": "2505.18471v1",
    "title": "Invisible Tokens, Visible Bills: The Urgent Need to Audit Hidden Operations in Opaque LLM Services",
    "authors": [
      "Guoheng Sun",
      "Ziyao Wang",
      "Xuandong Zhao",
      "Bowei Tian",
      "Zheyu Shen",
      "Yexiao He",
      "Jinming Xing",
      "Ang Li"
    ],
    "abstract": "Modern large language model (LLM) services increasingly rely on complex, often abstract operations, such as multi-step reasoning and multi-agent collaboration, to generate high-quality outputs. While users are billed based on token consumption and API usage, these internal steps are typically not visible. We refer to such systems as Commercial Opaque LLM Services (COLS). This position paper highlights emerging accountability challenges in COLS: users are billed for operations they cannot observe, verify, or contest. We formalize two key risks: \\textit{quantity inflation}, where token and call counts may be artificially inflated, and \\textit{quality downgrade}, where providers might quietly substitute lower-cost models or tools. Addressing these risks requires a diverse set of auditing strategies, including commitment-based, predictive, behavioral, and signature-based methods. We further explore the potential of complementary mechanisms such as watermarking and trusted execution environments to enhance verifiability without compromising provider confidentiality. We also propose a modular three-layer auditing framework for COLS and users that enables trustworthy verification across execution, secure logging, and user-facing auditability without exposing proprietary internals. Our aim is to encourage further research and policy development toward transparency, auditability, and accountability in commercial LLM services.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18471v1",
    "published_date": "2025-05-24 02:26:49 UTC",
    "updated_date": "2025-05-24 02:26:49 UTC"
  },
  {
    "arxiv_id": "2505.21535v3",
    "title": "FAR: Function-preserving Attention Replacement for IMC-friendly Inference",
    "authors": [
      "Yuxin Ren",
      "Maxwell D Collins",
      "Miao Hu",
      "Huanrui Yang"
    ],
    "abstract": "While transformers dominate modern vision and language models, their attention mechanism remains poorly suited for in-memory computing (IMC) devices due to intensive activation-to-activation multiplications and non-local memory access, leading to substantial latency and bandwidth overhead on ReRAM-based accelerators. To address this mismatch, we propose FAR, a Function-preserving Attention Replacement framework that substitutes all attention in pretrained DeiTs with sequential modules inherently compatible with IMC dataflows. Specifically, FAR replaces self-attention with a multi-head bidirectional LSTM architecture via block-wise distillation to retain functional equivalence while enabling linear-time computation and localized weight reuse. We further incorporate structured pruning on FAR models, enabling flexible adaptation to resource-constrained IMC arrays while maintaining functional fidelity. Evaluations on the DeiT family demonstrate that FAR maintains comparable accuracy to the original attention-based models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships learned by attention while improving computational efficiency, highlighting its potential for energy-efficient transformer inference on IMC-based edge accelerators.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages main paper, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21535v3",
    "published_date": "2025-05-24 02:23:46 UTC",
    "updated_date": "2025-11-20 21:06:39 UTC"
  },
  {
    "arxiv_id": "2505.18470v2",
    "title": "Chemical classification program synthesis using generative artificial intelligence",
    "authors": [
      "Christopher J. Mungall",
      "Adnan Malik",
      "Daniel R. Korn",
      "Justin T. Reese",
      "Noel M. O'Boyle",
      "Noel",
      "Janna Hastings"
    ],
    "abstract": "Accurately classifying chemical structures is essential for cheminformatics and bioinformatics, including tasks such as identifying bioactive compounds of interest, screening molecules for toxicity to humans, finding non-organic compounds with desirable material properties, or organizing large chemical libraries for drug discovery or environmental monitoring. However, manual classification is labor-intensive and difficult to scale to large chemical databases. Existing automated approaches either rely on manually constructed classification rules, or are deep learning methods that lack explainability.\n  This work presents an approach that uses generative artificial intelligence to automatically write chemical classifier programs for classes in the Chemical Entities of Biological Interest (ChEBI) database. These programs can be used for efficient deterministic run-time classification of SMILES structures, with natural language explanations. The programs themselves constitute an explainable computable ontological model of chemical class nomenclature, which we call the ChEBI Chemical Class Program Ontology (C3PO).\n  We validated our approach against the ChEBI database, and compared our results against deep learning models and a naive SMARTS pattern based classifier. C3PO outperforms the naive classifier, but does not reach the performance of state of the art deep learning methods. However, C3PO has a number of strengths that complement deep learning methods, including explainability and reduced data dependence. C3PO can be used alongside deep learning classifiers to provide an explanation of the classification, where both methods agree. The programs can be used as part of the ontology development process, and iteratively refined by expert human curators.",
    "categories": [
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18470v2",
    "published_date": "2025-05-24 02:21:33 UTC",
    "updated_date": "2025-08-24 01:27:40 UTC"
  },
  {
    "arxiv_id": "2505.18467v1",
    "title": "Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark",
    "authors": [
      "Unggi Lee",
      "Jaeyong Lee",
      "Jiyeong Bae",
      "Yeil Jeong",
      "Junbo Koh",
      "Gyeonggeon Lee",
      "Gunho Lee",
      "Taekyung Ahn",
      "Hyeoncheol Kim"
    ],
    "abstract": "Recent advances in large reasoning models (LRMs) show strong performance in structured domains such as mathematics and programming; however, they often lack pedagogical coherence and realistic teaching behaviors. To bridge this gap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use through three innovations: (1) a distillation-based pipeline that filters and refines model outputs for instruction-tuning, (2) the Well-balanced Educational Benchmark (WBEB), which evaluates performance across subject knowledge, pedagogical knowledge, tracing, essay scoring, and teacher decision-making, and (3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting teacher-style reasoning. Our mixed-method evaluation combines quantitative metrics with qualitative analysis, providing the first systematic assessment of LRMs' pedagogical strengths and limitations.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 5 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.18467v1",
    "published_date": "2025-05-24 02:18:35 UTC",
    "updated_date": "2025-05-24 02:18:35 UTC"
  },
  {
    "arxiv_id": "2505.18464v1",
    "title": "From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data",
    "authors": [
      "Ugur Kursuncu",
      "Trilok Padhi",
      "Gaurav Sinha",
      "Abdulkadir Erol",
      "Jaya Krishna Mandivarapu",
      "Christopher R. Larrison"
    ],
    "abstract": "The growing demand for accessible mental health support, compounded by workforce shortages and logistical barriers, has led to increased interest in utilizing Large Language Models (LLMs) for scalable and real-time assistance. However, their use in sensitive domains such as anxiety support remains underexamined. This study presents a systematic evaluation of LLMs (GPT and Llama) for their potential utility in anxiety support by using real user-generated posts from the r/Anxiety subreddit for both prompting and fine-tuning. Our approach utilizes a mixed-method evaluation framework incorporating three main categories of criteria: (i) linguistic quality, (ii) safety and trustworthiness, and (iii) supportiveness. Results show that fine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic quality but increased toxicity and bias, and diminished emotional responsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more supportive overall. Our findings highlight the risks of fine-tuning LLMs on unprocessed social media content without mitigation strategies.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18464v1",
    "published_date": "2025-05-24 02:07:32 UTC",
    "updated_date": "2025-05-24 02:07:32 UTC"
  },
  {
    "arxiv_id": "2505.18461v2",
    "title": "Performance and Generalizability Impacts of Incorporating Location Encoders into Deep Learning for Dynamic PM2.5 Estimation",
    "authors": [
      "Morteza Karimzadeh",
      "Zhongying Wang",
      "James L. Crooks"
    ],
    "abstract": "Deep learning has shown strong performance in geospatial prediction tasks, but the role of geolocation information in improving accuracy and generalizability remains underexamined. Recent work has introduced location encoders that aim to represent spatial context in a transferable way, yet most evaluations have focused on static mapping tasks. Here, we study the effect of incorporating geolocation into deep learning for a dynamic and spatially heterogeneous application: estimating daily surface-level PM2.5 across the contiguous United States using satellite and ground-based observations. We compare three strategies for representing location: excluding geolocation, using raw latitude and longitude, and using pretrained location encoders. We evaluate each under within-region and out-of-region generalization settings. Results show that raw coordinates can improve performance within regions by supporting spatial interpolation, but can reduce generalizability across regions. In contrast, pretrained location encoders such as GeoCLIP improve both predictive accuracy and geographic transfer. However, we also observe spatial artifacts linked to encoder characteristics, and performance varies across encoder types (e.g., SatCLIP vs. GeoCLIP). This work provides the first systematic evaluation of location encoders in a dynamic environmental estimation context and offers guidance for incorporating geolocation into deep learning models for geospatial prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18461v2",
    "published_date": "2025-05-24 02:00:34 UTC",
    "updated_date": "2025-10-26 20:14:45 UTC"
  },
  {
    "arxiv_id": "2505.18458v3",
    "title": "A Survey of LLM $\\times$ DATA",
    "authors": [
      "Xuanhe Zhou",
      "Junxuan He",
      "Wei Zhou",
      "Haodong Chen",
      "Zirui Tang",
      "Haoyu Zhao",
      "Xin Tong",
      "Guoliang Li",
      "Youmin Chen",
      "Jun Zhou",
      "Zhaojun Sun",
      "Binyuan Hui",
      "Shuo Wang",
      "Conghui He",
      "Zhiyuan Liu",
      "Jingren Zhou",
      "Fan Wu"
    ],
    "abstract": "The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "Please refer to the paper list at: https://github.com/weAIDB/awesome-data-llm",
    "pdf_url": "https://arxiv.org/pdf/2505.18458v3",
    "published_date": "2025-05-24 01:57:12 UTC",
    "updated_date": "2025-06-01 16:00:34 UTC"
  },
  {
    "arxiv_id": "2505.18457v1",
    "title": "EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks",
    "authors": [
      "Abir Ray"
    ],
    "abstract": "This paper introduces EdgeAgentX, a novel framework integrating federated learning (FL), multi-agent reinforcement learning (MARL), and adversarial defense mechanisms, tailored for military communication networks. EdgeAgentX significantly improves autonomous decision-making, reduces latency, enhances throughput, and robustly withstands adversarial disruptions, as evidenced by comprehensive simulations.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18457v1",
    "published_date": "2025-05-24 01:56:32 UTC",
    "updated_date": "2025-05-24 01:56:32 UTC"
  },
  {
    "arxiv_id": "2505.18453v1",
    "title": "MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt",
    "authors": [
      "Zhichao Wu",
      "Yueteng Kang",
      "Songjun Cao",
      "Long Ma",
      "Qiulin Li",
      "Qun Yang"
    ],
    "abstract": "Most existing Zero-Shot Text-To-Speech(ZS-TTS) systems generate the unseen speech based on single prompt, such as reference speech or text descriptions, which limits their flexibility. We propose a customized emotion ZS-TTS system based on multi-modal prompt. The system disentangles speech into the content, timbre, emotion and prosody, allowing emotion prompts to be provided as text, image or speech. To extract emotion information from different prompts, we propose a multi-modal prompt emotion encoder. Additionally, we introduce an prosody predictor to fit the distribution of prosody and propose an emotion consistency loss to preserve emotion information in the predicted prosody. A diffusion-based acoustic model is employed to generate the target mel-spectrogram. Both objective and subjective experiments demonstrate that our system outperforms existing systems in terms of naturalness and similarity. The samples are available at https://mpetts-demo.github.io/mpetts_demo/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by InterSpeech",
    "pdf_url": "https://arxiv.org/pdf/2505.18453v1",
    "published_date": "2025-05-24 01:26:02 UTC",
    "updated_date": "2025-05-24 01:26:02 UTC"
  },
  {
    "arxiv_id": "2505.18451v1",
    "title": "$$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts",
    "authors": [
      "Toshiaki Koike-Akino",
      "Jing Liu",
      "Ye Wang"
    ],
    "abstract": "To tackle the huge computational demand of large foundation models, activation-aware compression techniques without retraining have been introduced. However, since these rely on calibration data, domain shift may arise for unknown downstream tasks. With a computationally efficient calibration, activation-aware pruning can be executed for every prompt adaptively, yet achieving reduced complexity at inference. We formulate it as a mixture of micro-experts, called $$-MoE. Several experiments demonstrate that $$-MoE can dynamically adapt to task/prompt-dependent structured sparsity on the fly.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18451v1",
    "published_date": "2025-05-24 01:23:02 UTC",
    "updated_date": "2025-05-24 01:23:02 UTC"
  },
  {
    "arxiv_id": "2505.18446v1",
    "title": "Mitigating Context Bias in Domain Adaptation for Object Detection using Mask Pooling",
    "authors": [
      "Hojun Son",
      "Asma Almutairi",
      "Arpan Kusari"
    ],
    "abstract": "Context bias refers to the association between the foreground objects and background during the object detection training process. Various methods have been proposed to minimize the context bias when applying the trained model to an unseen domain, known as domain adaptation for object detection (DAOD). But a principled approach to understand why the context bias occurs and how to remove it has been missing.\n  In this work, we provide a causal view of the context bias, pointing towards the pooling operation in the convolution network architecture as the possible source of this bias. We present an alternative, Mask Pooling, which uses an additional input of foreground masks, to separate the pooling process in the respective foreground and background regions and show that this process leads the trained model to detect objects in a more robust manner under different domains. We also provide a benchmark designed to create an ultimate test for DAOD, using foregrounds in the presence of absolute random backgrounds, to analyze the robustness of the intended trained models. Through these experiments, we hope to provide a principled approach for minimizing context bias under domain shift.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18446v1",
    "published_date": "2025-05-24 01:05:20 UTC",
    "updated_date": "2025-05-24 01:05:20 UTC"
  },
  {
    "arxiv_id": "2505.18442v1",
    "title": "Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting",
    "authors": [
      "Zhining Liu",
      "Ze Yang",
      "Xiao Lin",
      "Ruizhong Qiu",
      "Tianxin Wei",
      "Yada Zhu",
      "Hendrik Hamann",
      "Jingrui He",
      "Hanghang Tong"
    ],
    "abstract": "Time-series forecasting plays a critical role in many real-world applications. Although increasingly powerful models have been developed and achieved superior results on benchmark datasets, through a fine-grained sample-level inspection, we find that (i) no single model consistently outperforms others across different test samples, but instead (ii) each model excels in specific cases. These findings prompt us to explore how to adaptively leverage the distinct strengths of various forecasting models for different samples. We introduce TimeFuse, a framework for collective time-series forecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse utilizes meta-features to characterize input time series and trains a learnable fusor to predict optimal model fusion weights for any given input. The fusor can leverage samples from diverse datasets for joint training, allowing it to adapt to a wide variety of temporal patterns and thus generalize to new inputs, even from unseen datasets. Extensive experiments demonstrate the effectiveness of TimeFuse in various long-/short-term forecasting tasks, achieving near-universal improvement over the state-of-the-art individual models. Code is available at https://github.com/ZhiningLiu1998/TimeFuse.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2025. 22 pages, 6 Figures, 12 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.18442v1",
    "published_date": "2025-05-24 00:45:07 UTC",
    "updated_date": "2025-05-24 00:45:07 UTC"
  },
  {
    "arxiv_id": "2505.18440v2",
    "title": "Efficient Long CoT Reasoning in Small Language Models",
    "authors": [
      "Zhaoyang Wang",
      "Jinqi Jiang",
      "Tian Qiu",
      "Hui Liu",
      "Xianfeng Tang",
      "Huaxiu Yao"
    ],
    "abstract": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex problems solving abilities by generating long chain-of-thought (CoT) reasoning steps. It is challenging to directly train small language models (SLMs) to emerge long CoT. Thus, distillation becomes a practical method to enable SLMs for such reasoning ability. However, the long CoT often contains a lot of redundant contents (e.g., overthinking steps) which may make SLMs hard to learn considering their relatively poor capacity and generalization. To address this issue, we propose a simple-yet-effective method to prune unnecessary steps in long CoT, and then employ an on-policy method for the SLM itself to curate valid and useful long CoT training data. In this way, SLMs can effectively learn efficient long CoT reasoning and preserve competitive performance at the same time. Experimental results across a series of mathematical reasoning benchmarks demonstrate the effectiveness of the proposed method in distilling long CoT reasoning ability into SLMs which maintains the competitive performance but significantly reduces generating redundant reasoning steps.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.18440v2",
    "published_date": "2025-05-24 00:22:52 UTC",
    "updated_date": "2025-06-18 06:11:08 UTC"
  },
  {
    "arxiv_id": "2505.18434v1",
    "title": "TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP",
    "authors": [
      "Yuliang Cai",
      "Jesse Thomason",
      "Mohammad Rostami"
    ],
    "abstract": "Vision-language models (VLMs), such as CLIP, have demonstrated strong performance across a range of downstream tasks. However, CLIP is still limited in negation understanding: the ability to recognize the absence or exclusion of a concept. Existing methods address the problem by using a large language model (LLM) to generate large-scale data of image captions containing negation for further fine-tuning CLIP. However, these methods are both time- and compute-intensive, and their evaluations are typically restricted to image-text matching tasks. To expand the horizon, we (1) introduce a training-time negation data generation pipeline such that negation captions are generated during the training stage, which only increases 2.5% extra training time, and (2) we propose the first benchmark, Neg-TtoI, for evaluating text-to-image generation models on prompts containing negation, assessing model's ability to produce semantically accurate images. We show that our proposed method, TNG-CLIP, achieves SOTA performance on diverse negation benchmarks of image-to-text matching, text-to-image retrieval, and image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.18434v1",
    "published_date": "2025-05-24 00:02:48 UTC",
    "updated_date": "2025-05-24 00:02:48 UTC"
  }
]
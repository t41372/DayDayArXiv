[
  {
    "arxiv_id": "2406.09627v1",
    "title": "RobustSAM: Segment Anything Robustly on Degraded Images",
    "authors": [
      "Wei-Ting Chen",
      "Yu-Jiet Vong",
      "Sy-Yen Kuo",
      "Sizhuo Ma",
      "Jian Wang"
    ],
    "abstract": "Segment Anything Model (SAM) has emerged as a transformative approach in\nimage segmentation, acclaimed for its robust zero-shot segmentation\ncapabilities and flexible prompting system. Nonetheless, its performance is\nchallenged by images with degraded quality. Addressing this limitation, we\npropose the Robust Segment Anything Model (RobustSAM), which enhances SAM's\nperformance on low-quality images while preserving its promptability and\nzero-shot generalization. Our method leverages the pre-trained SAM model with\nonly marginal parameter increments and computational requirements. The\nadditional parameters of RobustSAM can be optimized within 30 hours on eight\nGPUs, demonstrating its feasibility and practicality for typical research\nlaboratories. We also introduce the Robust-Seg dataset, a collection of 688K\nimage-mask pairs with different degradations designed to train and evaluate our\nmodel optimally. Extensive experiments across various segmentation tasks and\ndatasets confirm RobustSAM's superior performance, especially under zero-shot\nconditions, underscoring its potential for extensive real-world application.\nAdditionally, our method has been shown to effectively improve the performance\nof SAM-based downstream tasks such as single image dehazing and deblurring.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2024 (Highlight); Project Page:\n  https://robustsam.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.09627v1",
    "published_date": "2024-06-13 23:33:59 UTC",
    "updated_date": "2024-06-13 23:33:59 UTC"
  },
  {
    "arxiv_id": "2406.09624v2",
    "title": "DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks",
    "authors": [
      "Mohamed Elrefaie",
      "Florin Morar",
      "Angela Dai",
      "Faez Ahmed"
    ],
    "abstract": "We present DrivAerNet++, the largest and most comprehensive multimodal\ndataset for aerodynamic car design. DrivAerNet++ comprises 8,000 diverse car\ndesigns modeled with high-fidelity computational fluid dynamics (CFD)\nsimulations. The dataset includes diverse car configurations such as fastback,\nnotchback, and estateback, with different underbody and wheel designs to\nrepresent both internal combustion engines and electric vehicles. Each entry in\nthe dataset features detailed 3D meshes, parametric models, aerodynamic\ncoefficients, and extensive flow and surface field data, along with segmented\nparts for car classification and point cloud data. This dataset supports a wide\narray of machine learning applications including data-driven design\noptimization, generative modeling, surrogate model training, CFD simulation\nacceleration, and geometric classification. With more than 39 TB of publicly\navailable engineering data, DrivAerNet++ fills a significant gap in available\nresources, providing high-quality, diverse data to enhance model training,\npromote generalization, and accelerate automotive design processes. Along with\nrigorous dataset validation, we also provide ML benchmarking results on the\ntask of aerodynamic drag prediction, showcasing the breadth of applications\nsupported by our dataset. This dataset is set to significantly impact\nautomotive design and broader engineering disciplines by fostering innovation\nand improving the fidelity of aerodynamic evaluations. Dataset and code\navailable at: https://github.com/Mohamedelrefaie/DrivAerNet.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09624v2",
    "published_date": "2024-06-13 23:19:48 UTC",
    "updated_date": "2025-02-13 09:57:50 UTC"
  },
  {
    "arxiv_id": "2406.09622v1",
    "title": "DSL-FIQA: Assessing Facial Image Quality via Dual-Set Degradation Learning and Landmark-Guided Transformer",
    "authors": [
      "Wei-Ting Chen",
      "Gurunandan Krishnan",
      "Qiang Gao",
      "Sy-Yen Kuo",
      "Sizhuo Ma",
      "Jian Wang"
    ],
    "abstract": "Generic Face Image Quality Assessment (GFIQA) evaluates the perceptual\nquality of facial images, which is crucial in improving image restoration\nalgorithms and selecting high-quality face images for downstream tasks. We\npresent a novel transformer-based method for GFIQA, which is aided by two\nunique mechanisms. First, a Dual-Set Degradation Representation Learning (DSL)\nmechanism uses facial images with both synthetic and real degradations to\ndecouple degradation from content, ensuring generalizability to real-world\nscenarios. This self-supervised method learns degradation features on a global\nscale, providing a robust alternative to conventional methods that use local\npatch information in degradation learning. Second, our transformer leverages\nfacial landmarks to emphasize visually salient parts of a face image in\nevaluating its perceptual quality. We also introduce a balanced and diverse\nComprehensive Generic Face IQA (CGFIQA-40k) dataset of 40K images carefully\ndesigned to overcome the biases, in particular the imbalances in skin tone and\ngender representation, in existing datasets. Extensive analysis and evaluation\ndemonstrate the robustness of our method, marking a significant improvement\nover prior methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024, Project Page: https://dsl-fiqa.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.09622v1",
    "published_date": "2024-06-13 23:11:25 UTC",
    "updated_date": "2024-06-13 23:11:25 UTC"
  },
  {
    "arxiv_id": "2406.09618v1",
    "title": "Multi-Modal Retrieval For Large Language Model Based Speech Recognition",
    "authors": [
      "Jari Kolehmainen",
      "Aditya Gourav",
      "Prashanth Gurunath Shivakumar",
      "Yile Gu",
      "Ankur Gandhe",
      "Ariya Rastrow",
      "Grant Strimel",
      "Ivan Bulyko"
    ],
    "abstract": "Retrieval is a widely adopted approach for improving language models\nleveraging external information. As the field moves towards multi-modal large\nlanguage models, it is important to extend the pure text based methods to\nincorporate other modalities in retrieval as well for applications across the\nwide spectrum of machine learning tasks and data types. In this work, we\npropose multi-modal retrieval with two approaches: kNN-LM and cross-attention\ntechniques. We demonstrate the effectiveness of our retrieval approaches\nempirically by applying them to automatic speech recognition tasks with access\nto external information. Under this setting, we show that speech-based\nmulti-modal retrieval outperforms text based retrieval, and yields up to 50 %\nimprovement in word error rate over the multi-modal language model baseline.\nFurthermore, we achieve state-of-the-art recognition results on the\nSpoken-Squad question answering dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09618v1",
    "published_date": "2024-06-13 22:55:22 UTC",
    "updated_date": "2024-06-13 22:55:22 UTC"
  },
  {
    "arxiv_id": "2406.09612v2",
    "title": "Automated Molecular Concept Generation and Labeling with Large Language Models",
    "authors": [
      "Zimin Zhang",
      "Qianli Wu",
      "Botao Xia",
      "Fang Sun",
      "Ziniu Hu",
      "Yizhou Sun",
      "Shichang Zhang"
    ],
    "abstract": "Artificial intelligence (AI) is transforming scientific research, with\nexplainable AI methods like concept-based models (CMs) showing promise for new\ndiscoveries. However, in molecular science, CMs are less common than black-box\nmodels like Graph Neural Networks (GNNs), due to their need for predefined\nconcepts and manual labeling. This paper introduces the Automated Molecular\nConcept (AutoMolCo) framework, which leverages Large Language Models (LLMs) to\nautomatically generate and label predictive molecular concepts. Through\niterative concept refinement, AutoMolCo enables simple linear models to\noutperform GNNs and LLM in-context learning on several benchmarks. The\nframework operates without human knowledge input, overcoming limitations of\nexisting CMs while maintaining explainability and allowing easy intervention.\nExperiments on MoleculeNet and High-Throughput Experimentation (HTE) datasets\ndemonstrate that AutoMolCo-induced explainable CMs are beneficial for molecular\nscience research.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "physics.chem-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09612v2",
    "published_date": "2024-06-13 22:44:08 UTC",
    "updated_date": "2024-12-14 07:16:41 UTC"
  },
  {
    "arxiv_id": "2406.09606v3",
    "title": "Cross-Modality Program Representation Learning for Electronic Design Automation with High-Level Synthesis",
    "authors": [
      "Zongyue Qin",
      "Yunsheng Bai",
      "Atefeh Sohrabizadeh",
      "Zijian Ding",
      "Ziniu Hu",
      "Yizhou Sun",
      "Jason Cong"
    ],
    "abstract": "In recent years, domain-specific accelerators (DSAs) have gained popularity\nfor applications such as deep learning and autonomous driving. To facilitate\nDSA designs, programmers use high-level synthesis (HLS) to compile a high-level\ndescription written in C/C++ into a design with low-level hardware description\nlanguages that eventually synthesize DSAs on circuits. However, creating a\nhigh-quality HLS design still demands significant domain knowledge,\nparticularly in microarchitecture decisions expressed as \\textit{pragmas}.\nThus, it is desirable to automate such decisions with the help of machine\nlearning for predicting the quality of HLS designs, requiring a deeper\nunderstanding of the program that consists of original code and pragmas.\nNaturally, these programs can be considered as sequence data. In addition,\nthese programs can be compiled and converted into a control data flow graph\n(CDFG). But existing works either fail to leverage both modalities or combine\nthe two in shallow or coarse ways. We propose ProgSG, a model that allows\ninteraction between the source code sequence modality and the graph modality in\na deep and fine-grained way. To alleviate the scarcity of labeled designs, a\npre-training method is proposed based on a suite of compiler's data flow\nanalysis tasks. Experimental results show that ProgSG reduces the RMSE of\ndesign performance predictions by up to $22\\%$, and identifies designs with an\naverage of $1.10\\times$ and $1.26\\times$ (up to $8.17\\times$ and $13.31\\times$)\nperformance improvement in design space exploration (DSE) task compared to HARP\nand AutoDSE, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2305.10838",
    "pdf_url": "http://arxiv.org/pdf/2406.09606v3",
    "published_date": "2024-06-13 22:34:58 UTC",
    "updated_date": "2024-07-17 22:08:51 UTC"
  },
  {
    "arxiv_id": "2406.11889v1",
    "title": "Hyperdimensional Quantum Factorization",
    "authors": [
      "Prathyush Poduval",
      "Zhuowen Zou",
      "Alvaro Velasquez",
      "Mohsen Imani"
    ],
    "abstract": "This paper presents a quantum algorithm for efficiently decoding\nhypervectors, a crucial process in extracting atomic elements from hypervectors\n- an essential task in Hyperdimensional Computing (HDC) models for\ninterpretable learning and information retrieval. HDC employs high-dimensional\nvectors and efficient operators to encode and manipulate information,\nrepresenting complex objects from atomic concepts. When one attempts to decode\na hypervector that is the product (binding) of multiple hypervectors, the\nfactorization becomes prohibitively costly with classical optimization-based\nmethods and specialized recurrent networks, an inherent consequence of the\nbinding operation. We propose HDQF, an innovative quantum computing approach,\nto address this challenge. By exploiting parallels between HDC and quantum\ncomputing and capitalizing on quantum algorithms' speedup capabilities, HDQF\nencodes potential factors as a quantum superposition using qubit states and\nbipolar vector representation. This yields a quadratic speedup over classical\nsearch methods and effectively mitigates Hypervector Factorization capacity\nissues.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "quant-ph",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.11889v1",
    "published_date": "2024-06-13 20:50:02 UTC",
    "updated_date": "2024-06-13 20:50:02 UTC"
  },
  {
    "arxiv_id": "2406.09573v1",
    "title": "Analyzing Gender Polarity in Short Social Media Texts with BERT: The Role of Emojis and Emoticons",
    "authors": [
      "Saba Yousefian Jazi",
      "Amir Mirzaeinia",
      "Sina Yousefian Jazi"
    ],
    "abstract": "In this effort we fine tuned different models based on BERT to detect the\ngender polarity of twitter accounts. We specially focused on analyzing the\neffect of using emojis and emoticons in performance of our model in classifying\ntask. We were able to demonstrate that the use of these none word inputs\nalongside the mention of other accounts in a short text format like tweet has\nan impact in detecting the account holder's gender.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09573v1",
    "published_date": "2024-06-13 20:23:59 UTC",
    "updated_date": "2024-06-13 20:23:59 UTC"
  },
  {
    "arxiv_id": "2406.09570v3",
    "title": "Improving Consistency Models with Generator-Augmented Flows",
    "authors": [
      "Thibaut Issenhuth",
      "Sangchul Lee",
      "Ludovic Dos Santos",
      "Jean-Yves Franceschi",
      "Chansoo Kim",
      "Alain Rakotomamonjy"
    ],
    "abstract": "Consistency models imitate the multi-step sampling of score-based diffusion\nin a single forward pass of a neural network. They can be learned in two ways:\nconsistency distillation and consistency training. The former relies on the\ntrue velocity field of the corresponding differential equation, approximated by\na pre-trained neural network. In contrast, the latter uses a single-sample\nMonte Carlo estimate of this velocity field. The related estimation error\ninduces a discrepancy between consistency distillation and training that, we\nshow, still holds in the continuous-time limit. To alleviate this issue, we\npropose a novel flow that transports noisy data towards their corresponding\noutputs derived from a consistency model. We prove that this flow reduces the\npreviously identified discrepancy and the noise-data transport cost.\nConsequently, our method not only accelerates consistency training convergence\nbut also enhances its overall performance. The code is available at:\nhttps://github.com/thibautissenhuth/consistency_GC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09570v3",
    "published_date": "2024-06-13 20:22:38 UTC",
    "updated_date": "2025-02-05 15:57:34 UTC"
  },
  {
    "arxiv_id": "2406.09569v1",
    "title": "Speech ReaLLM -- Real-time Streaming Speech Recognition with Multimodal LLMs by Teaching the Flow of Time",
    "authors": [
      "Frank Seide",
      "Morrie Doulaty",
      "Yangyang Shi",
      "Yashesh Gaur",
      "Junteng Jia",
      "Chunyang Wu"
    ],
    "abstract": "We introduce Speech ReaLLM, a new ASR architecture that marries\n\"decoder-only\" ASR with the RNN-T to make multimodal LLM architectures capable\nof real-time streaming. This is the first \"decoder-only\" ASR architecture\ndesigned to handle continuous audio without explicit end-pointing. Speech\nReaLLM is a special case of the more general ReaLLM (\"real-time LLM\") approach,\nalso introduced here for the first time. The idea is inspired by RNN-T: Instead\nof generating a response only at the end of a user prompt, generate after every\ninput token received in real time (it is often empty). On Librispeech \"test\",\nan 80M Speech ReaLLM achieves WERs of 3.0% and 7.4% in real time (without an\nexternal LM or auxiliary loss). This is only slightly above a 3x larger\nAttention-Encoder-Decoder baseline. We also show that this way, an LLM\narchitecture can learn to represent and reproduce the flow of time; and that a\npre-trained 7B LLM can be fine-tuned to do reasonably well on this task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09569v1",
    "published_date": "2024-06-13 20:20:29 UTC",
    "updated_date": "2024-06-13 20:20:29 UTC"
  },
  {
    "arxiv_id": "2406.09564v3",
    "title": "Towards Domain Adaptive Neural Contextual Bandits",
    "authors": [
      "Ziyan Wang",
      "Xiaoming Huo",
      "Hao Wang"
    ],
    "abstract": "Contextual bandit algorithms are essential for solving real-world decision\nmaking problems. In practice, collecting a contextual bandit's feedback from\ndifferent domains may involve different costs. For example, measuring drug\nreaction from mice (as a source domain) and humans (as a target domain).\nUnfortunately, adapting a contextual bandit algorithm from a source domain to a\ntarget domain with distribution shift still remains a major challenge and\nlargely unexplored. In this paper, we introduce the first general domain\nadaptation method for contextual bandits. Our approach learns a bandit model\nfor the target domain by collecting feedback from the source domain. Our\ntheoretical analysis shows that our algorithm maintains a sub-linear regret\nbound even adapting across domains. Empirical results show that our approach\noutperforms the state-of-the-art contextual bandit algorithms on real-world\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.09564v3",
    "published_date": "2024-06-13 20:12:46 UTC",
    "updated_date": "2025-04-06 18:23:33 UTC"
  },
  {
    "arxiv_id": "2406.09561v1",
    "title": "Label Noise Robustness for Domain-Agnostic Fair Corrections via Nearest Neighbors Label Spreading",
    "authors": [
      "Nathan Stromberg",
      "Rohan Ayyagari",
      "Sanmi Koyejo",
      "Richard Nock",
      "Lalitha Sankar"
    ],
    "abstract": "Last-layer retraining methods have emerged as an efficient framework for\ncorrecting existing base models. Within this framework, several methods have\nbeen proposed to deal with correcting models for subgroup fairness with and\nwithout group membership information. Importantly, prior work has demonstrated\nthat many methods are susceptible to noisy labels. To this end, we propose a\ndrop-in correction for label noise in last-layer retraining, and demonstrate\nthat it achieves state-of-the-art worst-group accuracy for a broad range of\nsymmetric label noise and across a wide variety of datasets exhibiting spurious\ncorrelations. Our proposed approach uses label spreading on a latent nearest\nneighbors graph and has minimal computational overhead compared to existing\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09561v1",
    "published_date": "2024-06-13 20:00:06 UTC",
    "updated_date": "2024-06-13 20:00:06 UTC"
  },
  {
    "arxiv_id": "2406.09559v1",
    "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
    "authors": [
      "Sankalp KJ",
      "Vinija Jain",
      "Sreyoshi Bhaduri",
      "Tamoghna Roy",
      "Aman Chadha"
    ],
    "abstract": "This review paper provides a comprehensive overview of large language model\n(LLM) research directions within Indic languages. Indic languages are those\nspoken in the Indian subcontinent, including India, Pakistan, Bangladesh, Sri\nLanka, Nepal, and Bhutan, among others. These languages have a rich cultural\nand linguistic heritage and are spoken by over 1.5 billion people worldwide.\nWith the tremendous market potential and growing demand for natural language\nprocessing (NLP) based applications in diverse languages, generative\napplications for Indic languages pose unique challenges and opportunities for\nresearch. Our paper deep dives into the recent advancements in Indic generative\nmodeling, contributing with a taxonomy of research directions, tabulating 84\nrecent publications. Research directions surveyed in this paper include LLM\ndevelopment, fine-tuning existing LLMs, development of corpora, benchmarking\nand evaluation, as well as publications around specific techniques, tools, and\napplications. We found that researchers across the publications emphasize the\nchallenges associated with limited data availability, lack of standardization,\nand the peculiar linguistic complexities of Indic languages. This work aims to\nserve as a valuable resource for researchers and practitioners working in the\nfield of NLP, particularly those focused on Indic languages, and contributes to\nthe development of more accurate and efficient LLM applications for these\nlanguages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "27 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2406.09559v1",
    "published_date": "2024-06-13 19:55:20 UTC",
    "updated_date": "2024-06-13 19:55:20 UTC"
  },
  {
    "arxiv_id": "2406.09553v1",
    "title": "My Body My Choice: Human-Centric Full-Body Anonymization",
    "authors": [
      "Umur Aybars Ciftci",
      "Ali Kemal Tanriverdi",
      "Ilke Demir"
    ],
    "abstract": "In an era of increasing privacy concerns for our online presence, we propose\nthat the decision to appear in a piece of content should only belong to the\nowner of the body. Although some automatic approaches for full-body\nanonymization have been proposed, human-guided anonymization can adapt to\nvarious contexts, such as cultural norms, personal relations, esthetic\nconcerns, and security issues. ''My Body My Choice'' (MBMC) enables physical\nand adversarial anonymization by removal and swapping approaches aimed for four\ntasks, designed by single or multi, ControlNet or GAN modules, combining\nseveral diffusion models. We evaluate anonymization on seven datasets; compare\nwith SOTA inpainting and anonymization methods; evaluate by image, adversarial,\nand generative metrics; and conduct reidentification experiments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AI for Content Creation Workshop @ CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09553v1",
    "published_date": "2024-06-13 19:40:30 UTC",
    "updated_date": "2024-06-13 19:40:30 UTC"
  },
  {
    "arxiv_id": "2406.09548v2",
    "title": "Between Randomness and Arbitrariness: Some Lessons for Reliable Machine Learning at Scale",
    "authors": [
      "A. Feder Cooper"
    ],
    "abstract": "To develop rigorous knowledge about ML models -- and the systems in which\nthey are embedded -- we need reliable measurements. But reliable measurement is\nfundamentally challenging, and touches on issues of reproducibility,\nscalability, uncertainty quantification, epistemology, and more. This\ndissertation addresses criteria needed to take reliability seriously: both\ncriteria for designing meaningful metrics, and for methodologies that ensure\nthat we can dependably and efficiently measure these metrics at scale and in\npractice. In doing so, this dissertation articulates a research vision for a\nnew field of scholarship at the intersection of machine learning, law, and\npolicy. Within this frame, we cover topics that fit under three different\nthemes: (1) quantifying and mitigating sources of arbitrariness in ML, (2)\ntaming randomness in uncertainty estimation and optimization algorithms, in\norder to achieve scalability without sacrificing reliability, and (3) providing\nmethods for evaluating generative-AI systems, with specific focuses on\nquantifying memorization in language models and training latent diffusion\nmodels on open-licensed data. By making contributions in these three themes,\nthis dissertation serves as an empirical proof by example that research on\nreliable measurement for machine learning is intimately and inescapably bound\nup with research in law and policy. These different disciplines pose similar\nresearch questions about reliable measurement in machine learning. They are, in\nfact, two complementary sides of the same research vision, which, broadly\nconstrued, aims to construct machine-learning systems that cohere with broader\nsocietal values.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Ph.D. Dissertation",
    "pdf_url": "http://arxiv.org/pdf/2406.09548v2",
    "published_date": "2024-06-13 19:29:37 UTC",
    "updated_date": "2024-08-12 08:02:06 UTC"
  },
  {
    "arxiv_id": "2406.11888v1",
    "title": "Neural logic programs and neural nets",
    "authors": [
      "Christian Antić"
    ],
    "abstract": "Neural-symbolic integration aims to combine the connectionist subsymbolic\nwith the logical symbolic approach to artificial intelligence. In this paper,\nwe first define the answer set semantics of (boolean) neural nets and then\nintroduce from first principles a class of neural logic programs and show that\nnets and programs are equivalent.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11888v1",
    "published_date": "2024-06-13 19:22:04 UTC",
    "updated_date": "2024-06-13 19:22:04 UTC"
  },
  {
    "arxiv_id": "2406.09529v1",
    "title": "Differentiable Reasoning about Knowledge Graphs with Region-based Graph Neural Networks",
    "authors": [
      "Aleksandar Pavlovic",
      "Emanuel Sallinger",
      "Steven Schockaert"
    ],
    "abstract": "Methods for knowledge graph (KG) completion need to capture semantic\nregularities and use these regularities to infer plausible knowledge that is\nnot explicitly stated. Most embedding-based methods are opaque in the kinds of\nregularities they can capture, although region-based KG embedding models have\nemerged as a more transparent alternative. By modeling relations as geometric\nregions in high-dimensional vector spaces, such models can explicitly capture\nsemantic regularities in terms of the spatial arrangement of these regions.\nUnfortunately, existing region-based approaches are severely limited in the\nkinds of rules they can capture. We argue that this limitation arises because\nthe considered regions are defined as the Cartesian product of two-dimensional\nregions. As an alternative, in this paper, we propose RESHUFFLE, a simple model\nbased on ordering constraints that can faithfully capture a much larger class\nof rule bases than existing approaches. Moreover, the embeddings in our\nframework can be learned by a monotonic Graph Neural Network (GNN), which\neffectively acts as a differentiable rule base. This approach has the important\nadvantage that embeddings can be easily updated as new knowledge is added to\nthe KG. At the same time, since the resulting representations can be used\nsimilarly to standard KG embeddings, our approach is significantly more\nefficient than existing approaches to differentiable reasoning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09529v1",
    "published_date": "2024-06-13 18:37:24 UTC",
    "updated_date": "2024-06-13 18:37:24 UTC"
  },
  {
    "arxiv_id": "2406.09520v1",
    "title": "A Systematic Review of Generative AI for Teaching and Learning Practice",
    "authors": [
      "Bayode Ogunleye",
      "Kudirat Ibilola Zakariyyah",
      "Oluwaseun Ajao",
      "Olakunle Olayinka",
      "Hemlata Sharma"
    ],
    "abstract": "The use of generative artificial intelligence (GenAI) in academia is a\nsubjective and hotly debated topic. Currently, there are no agreed guidelines\ntowards the usage of GenAI systems in higher education (HE) and, thus, it is\nstill unclear how to make effective use of the technology for teaching and\nlearning practice. This paper provides an overview of the current state of\nresearch on GenAI for teaching and learning in HE. To this end, this study\nconducted a systematic review of relevant studies indexed by Scopus, using the\npreferred reporting items for systematic reviews and meta-analyses (PRISMA)\nguidelines. The search criteria revealed a total of 625 research papers, of\nwhich 355 met the final inclusion criteria. The findings from the review showed\nthe current state and the future trends in documents, citations, document\nsources/authors, keywords, and co-authorship. The research gaps identified\nsuggest that while some authors have looked at understanding the detection of\nAI-generated text, it may be beneficial to understand how GenAI can be\nincorporated into supporting the educational curriculum for assessments,\nteaching, and learning delivery. Furthermore, there is a need for additional\ninterdisciplinary, multidimensional studies in HE through collaboration. This\nwill strengthen the awareness and understanding of students, tutors, and other\nstakeholders, which will be instrumental in formulating guidelines, frameworks,\nand policies for GenAI usage.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "20 pages, 10 figures, article published in Education Sciences",
    "pdf_url": "http://arxiv.org/pdf/2406.09520v1",
    "published_date": "2024-06-13 18:16:27 UTC",
    "updated_date": "2024-06-13 18:16:27 UTC"
  },
  {
    "arxiv_id": "2406.09519v4",
    "title": "Talking Heads: Understanding Inter-layer Communication in Transformer Language Models",
    "authors": [
      "Jack Merullo",
      "Carsten Eickhoff",
      "Ellie Pavlick"
    ],
    "abstract": "Although it is known that transformer language models (LMs) pass features\nfrom early layers to later layers, it is not well understood how this\ninformation is represented and routed by the model. We analyze a mechanism used\nin two LMs to selectively inhibit items in a context in one task, and find that\nit underlies a commonly used abstraction across many context-retrieval\nbehaviors. Specifically, we find that models write into low-rank subspaces of\nthe residual stream to represent features which are then read out by later\nlayers, forming low-rank communication channels (Elhage et al., 2021) between\nlayers. A particular 3D subspace in model activations in GPT-2 can be traversed\nto positionally index items in lists, and we show that this mechanism can\nexplain an otherwise arbitrary-seeming sensitivity of the model to the order of\nitems in the prompt. That is, the model has trouble copying the correct\ninformation from context when many items ``crowd\" this limited space. By\ndecomposing attention heads with the Singular Value Decomposition (SVD), we\nfind that previously described interactions between heads separated by one or\nmore layers can be predicted via analysis of their weight matrices alone. We\nshow that it is possible to manipulate the internal model representations as\nwell as edit model weights based on the mechanism we discover in order to\nsignificantly improve performance on our synthetic Laundry List task, which\nrequires recall from a list, often improving task accuracy by over 20%. Our\nanalysis reveals a surprisingly intricate interpretable structure learned from\nlanguage model pretraining, and helps us understand why sophisticated LMs\nsometimes fail in simple domains, facilitating future analysis of more complex\nbehaviors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Neurips 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09519v4",
    "published_date": "2024-06-13 18:12:01 UTC",
    "updated_date": "2025-05-08 21:25:08 UTC"
  },
  {
    "arxiv_id": "2406.09509v2",
    "title": "CleanDiffuser: An Easy-to-use Modularized Library for Diffusion Models in Decision Making",
    "authors": [
      "Zibin Dong",
      "Yifu Yuan",
      "Jianye Hao",
      "Fei Ni",
      "Yi Ma",
      "Pengyi Li",
      "Yan Zheng"
    ],
    "abstract": "Leveraging the powerful generative capability of diffusion models (DMs) to\nbuild decision-making agents has achieved extensive success. However, there is\nstill a demand for an easy-to-use and modularized open-source library that\noffers customized and efficient development for DM-based decision-making\nalgorithms. In this work, we introduce CleanDiffuser, the first DM library\nspecifically designed for decision-making algorithms. By revisiting the roles\nof DMs in the decision-making domain, we identify a set of essential\nsub-modules that constitute the core of CleanDiffuser, allowing for the\nimplementation of various DM algorithms with simple and flexible building\nblocks. To demonstrate the reliability and flexibility of CleanDiffuser, we\nconduct comprehensive evaluations of various DM algorithms implemented with\nCleanDiffuser across an extensive range of tasks. The analytical experiments\nprovide a wealth of valuable design choices and insights, reveal opportunities\nand challenges, and lay a solid groundwork for future research. CleanDiffuser\nwill provide long-term support to the decision-making community, enhancing\nreproducibility and fostering the development of more robust solutions. The\ncode and documentation of CleanDiffuser are open-sourced on the\nhttps://github.com/CleanDiffuserTeam/CleanDiffuser.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accept by NeurIPS2024 Datasets and Benchmarks Track. The first two\n  authors contribute equally to this work. Code and documentation:\n  https://github.com/CleanDiffuserTeam/CleanDiffuser",
    "pdf_url": "http://arxiv.org/pdf/2406.09509v2",
    "published_date": "2024-06-13 18:00:24 UTC",
    "updated_date": "2024-10-27 02:56:03 UTC"
  },
  {
    "arxiv_id": "2406.09496v3",
    "title": "The World Wide Recipe: A community-centred framework for fine-grained data collection and regional bias operationalisation",
    "authors": [
      "Jabez Magomere",
      "Shu Ishida",
      "Tejumade Afonja",
      "Aya Salama",
      "Daniel Kochin",
      "Foutse Yuehgoh",
      "Imane Hamzaoui",
      "Raesetje Sefala",
      "Aisha Alaagib",
      "Samantha Dalal",
      "Beatrice Marchegiani",
      "Elizaveta Semenova",
      "Lauren Crais",
      "Siobhan Mackenzie Hall"
    ],
    "abstract": "We introduce the World Wide recipe, which sets forth a framework for\nculturally aware and participatory data collection, and the resultant\nregionally diverse World Wide Dishes evaluation dataset. We also analyse bias\noperationalisation to highlight how current systems underperform across several\ndimensions: (in-)accuracy, (mis-)representation, and cultural (in-)sensitivity,\nwith evidence from qualitative community-based observations and quantitative\nautomated tools. We find that these T2I models generally do not produce quality\noutputs of dishes specific to various regions. This is true even for the US,\nwhich is typically considered more well-resourced in training data -- although\nthe generation of US dishes does outperform that of the investigated African\ncountries. The models demonstrate the propensity to produce inaccurate and\nculturally misrepresentative, flattening, and insensitive outputs. These\nrepresentational biases have the potential to further reinforce stereotypes and\ndisproportionately contribute to erasure based on region. The dataset and code\nare available at https://github.com/oxai/world-wide-dishes.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09496v3",
    "published_date": "2024-06-13 18:00:00 UTC",
    "updated_date": "2025-02-09 17:13:58 UTC"
  },
  {
    "arxiv_id": "2406.09412v1",
    "title": "Explore the Limits of Omni-modal Pretraining at Scale",
    "authors": [
      "Yiyuan Zhang",
      "Handong Li",
      "Jing Liu",
      "Xiangyu Yue"
    ],
    "abstract": "We propose to build omni-modal intelligence, which is capable of\nunderstanding any modality and learning universal representations. In specific,\nwe propose a scalable pretraining paradigm, named Multimodal Context (MiCo),\nwhich can scale up the numbers of modalities and amount of data, together with\nthe model parameters, in the pretraining process. With MiCo, the pretrained\nmodels show significant emergent abilities in multimodal learning, which are\nevaluated on the following tasks: i) single-modality perception benchmarks of\n10 different modalities, ii) 25 cross-modality understanding tasks of\nretrieval, question-answering, captioning, and iii) 18 multimodal large\nlanguage model benchmarks. Our models establish 37 new records for\nstate-of-the-art performance. We hope that our research could contribute to the\ndevelopment of omni-modal intelligence. Code and Models are at\nhttps://github.com/invictus717/MiCo",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Website: https://invictus717.github.io/MiCo/",
    "pdf_url": "http://arxiv.org/pdf/2406.09412v1",
    "published_date": "2024-06-13 17:59:53 UTC",
    "updated_date": "2024-06-13 17:59:53 UTC"
  },
  {
    "arxiv_id": "2406.09411v2",
    "title": "MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding",
    "authors": [
      "Fei Wang",
      "Xingyu Fu",
      "James Y. Huang",
      "Zekun Li",
      "Qin Liu",
      "Xiaogeng Liu",
      "Mingyu Derek Ma",
      "Nan Xu",
      "Wenxuan Zhou",
      "Kai Zhang",
      "Tianyi Lorena Yan",
      "Wenjie Jacky Mo",
      "Hsiang-Hui Liu",
      "Pan Lu",
      "Chunyuan Li",
      "Chaowei Xiao",
      "Kai-Wei Chang",
      "Dan Roth",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ],
    "abstract": "We introduce MuirBench, a comprehensive benchmark that focuses on robust\nmulti-image understanding capabilities of multimodal LLMs. MuirBench consists\nof 12 diverse multi-image tasks (e.g., scene understanding, ordering) that\ninvolve 10 categories of multi-image relations (e.g., multiview, temporal\nrelations). Comprising 11,264 images and 2,600 multiple-choice questions,\nMuirBench is created in a pairwise manner, where each standard instance is\npaired with an unanswerable variant that has minimal semantic differences, in\norder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our\nresults reveal that even the best-performing models like GPT-4o and Gemini Pro\nfind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.\nOpen-source multimodal LLMs trained on single images can hardly generalize to\nmulti-image questions, hovering below 33.3% in accuracy. These results\nhighlight the importance of MuirBench in encouraging the community to develop\nmultimodal LLMs that can look beyond a single image, suggesting potential\npathways for future improvements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "typos corrected, references added, Project Page:\n  https://muirbench.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.09411v2",
    "published_date": "2024-06-13 17:59:52 UTC",
    "updated_date": "2024-07-02 01:56:14 UTC"
  },
  {
    "arxiv_id": "2406.09410v3",
    "title": "STAR: A First-Ever Dataset and A Large-Scale Benchmark for Scene Graph Generation in Large-Size Satellite Imagery",
    "authors": [
      "Yansheng Li",
      "Linlin Wang",
      "Tingzhu Wang",
      "Xue Yang",
      "Junwei Luo",
      "Qi Wang",
      "Youming Deng",
      "Wenbin Wang",
      "Xian Sun",
      "Haifeng Li",
      "Bo Dang",
      "Yongjun Zhang",
      "Yi Yu",
      "Junchi Yan"
    ],
    "abstract": "Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting\nunderstanding of geospatial scenarios from perception to cognition. In SAI,\nobjects exhibit great variations in scales and aspect ratios, and there exist\nrich relationships between objects (even between spatially disjoint objects),\nwhich makes it attractive to holistically conduct SGG in large-size\nvery-high-resolution (VHR) SAI. However, there lack such SGG datasets. Due to\nthe complexity of large-size SAI, mining triplets <subject, relationship,\nobject> heavily relies on long-range contextual reasoning. Consequently, SGG\nmodels designed for small-size natural imagery are not directly applicable to\nlarge-size SAI. This paper constructs a large-scale dataset for SGG in\nlarge-size VHR SAI with image sizes ranging from 512 x 768 to 27,860 x 31,096\npixels, named STAR (Scene graph generaTion in lArge-size satellite imageRy),\nencompassing over 210K objects and over 400K triplets. To realize SGG in\nlarge-size SAI, we propose a context-aware cascade cognition (CAC) framework to\nunderstand SAI regarding object detection (OBD), pair pruning and relationship\nprediction for SGG. We also release a SAI-oriented SGG toolkit with about 30\nOBD and 10 SGG methods which need further adaptation by our devised modules on\nour challenging STAR dataset. The dataset and toolkit are available at:\nhttps://linlin-dev.github.io/project/STAR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.09410v3",
    "published_date": "2024-06-13 17:59:51 UTC",
    "updated_date": "2024-07-03 08:00:31 UTC"
  },
  {
    "arxiv_id": "2406.09406v2",
    "title": "4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities",
    "authors": [
      "Roman Bachmann",
      "Oğuzhan Fatih Kar",
      "David Mizrahi",
      "Ali Garjani",
      "Mingfei Gao",
      "David Griffiths",
      "Jiaming Hu",
      "Afshin Dehghan",
      "Amir Zamir"
    ],
    "abstract": "Current multimodal and multitask foundation models like 4M or UnifiedIO show\npromising results, but in practice their out-of-the-box abilities to accept\ndiverse inputs and perform diverse tasks are limited by the (usually rather\nsmall) number of modalities and tasks they are trained on. In this paper, we\nexpand upon the capabilities of them by training a single model on tens of\nhighly diverse modalities and by performing co-training on large-scale\nmultimodal datasets and text corpora. This includes training on several\nsemantic and geometric modalities, feature maps from recent state of the art\nmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAM\nand 4DHumans, and a range of new modalities that allow for novel ways to\ninteract with the model and steer the generation, for example image metadata or\ncolor palettes. A crucial step in this process is performing discrete\ntokenization on various modalities, whether they are image-like, neural network\nfeature maps, vectors, structured data like instance segmentation or human\nposes, or data that can be represented as text. Through this, we expand on the\nout-of-the-box capabilities of multimodal models and specifically show the\npossibility of training one model to solve at least 3x more tasks/modalities\nthan existing ones and doing so without a loss in performance. This enables\nmore fine-grained and controllable multimodal generation capabilities and\nallows us to study the distillation of models trained on diverse data and\nobjectives into a unified model. We successfully scale the training to a three\nbillion parameter model using tens of modalities and different datasets. The\nresulting models and training code are open sourced at 4m.epfl.ch.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page at 4m.epfl.ch",
    "pdf_url": "http://arxiv.org/pdf/2406.09406v2",
    "published_date": "2024-06-13 17:59:42 UTC",
    "updated_date": "2024-06-14 14:43:26 UTC"
  },
  {
    "arxiv_id": "2406.09404v1",
    "title": "ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing",
    "authors": [
      "Jun-Kun Chen",
      "Samuel Rota Bulò",
      "Norman Müller",
      "Lorenzo Porzi",
      "Peter Kontschieder",
      "Yu-Xiong Wang"
    ],
    "abstract": "This paper proposes ConsistDreamer - a novel framework that lifts 2D\ndiffusion models with 3D awareness and 3D consistency, thus enabling\nhigh-fidelity instruction-guided scene editing. To overcome the fundamental\nlimitation of missing 3D consistency in 2D diffusion models, our key insight is\nto introduce three synergetic strategies that augment the input of the 2D\ndiffusion model to become 3D-aware and to explicitly enforce 3D consistency\nduring the training process. Specifically, we design surrounding views as\ncontext-rich input for the 2D diffusion model, and generate 3D-consistent,\nstructured noise instead of image-independent noise. Moreover, we introduce\nself-supervised consistency-enforcing training within the per-scene editing\nprocedure. Extensive evaluation shows that our ConsistDreamer achieves\nstate-of-the-art performance for instruction-guided scene editing across\nvarious scenes and editing instructions, particularly in complicated\nlarge-scale indoor scenes from ScanNet++, with significantly improved sharpness\nand fine-grained textures. Notably, ConsistDreamer stands as the first work\ncapable of successfully editing complex (e.g., plaid/checkered) patterns. Our\nproject page is at immortalco.github.io/ConsistDreamer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09404v1",
    "published_date": "2024-06-13 17:59:32 UTC",
    "updated_date": "2024-06-13 17:59:32 UTC"
  },
  {
    "arxiv_id": "2406.09401v1",
    "title": "MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations",
    "authors": [
      "Ruiyuan Lyu",
      "Tai Wang",
      "Jingli Lin",
      "Shuai Yang",
      "Xiaohan Mao",
      "Yilun Chen",
      "Runsen Xu",
      "Haifeng Huang",
      "Chenming Zhu",
      "Dahua Lin",
      "Jiangmiao Pang"
    ],
    "abstract": "With the emergence of LLMs and their integration with other data modalities,\nmulti-modal 3D perception attracts more attention due to its connectivity to\nthe physical world and makes rapid progress. However, limited by existing\ndatasets, previous works mainly focus on understanding object properties or\ninter-object spatial relationships in a 3D scene. To tackle this problem, this\npaper builds the first largest ever multi-modal 3D scene dataset and benchmark\nwith hierarchical grounded language annotations, MMScan. It is constructed\nbased on a top-down logic, from region to object level, from a single target to\ninter-target relationships, covering holistic aspects of spatial and attribute\nunderstanding. The overall pipeline incorporates powerful VLMs via carefully\ndesigned prompts to initialize the annotations efficiently and further involve\nhumans' correction in the loop to ensure the annotations are natural, correct,\nand comprehensive. Built upon existing 3D scanning data, the resulting\nmulti-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects\nand 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding\nand question-answering benchmarks. We evaluate representative baselines on our\nbenchmarks, analyze their capabilities in different aspects, and showcase the\nkey problems to be addressed in the future. Furthermore, we use this\nhigh-quality dataset to train state-of-the-art 3D visual grounding and LLMs and\nobtain remarkable performance improvement both on existing benchmarks and\nin-the-wild evaluation. Codes, datasets, and benchmarks will be available at\nhttps://github.com/OpenRobotLab/EmbodiedScan.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Follow-up of EmbodiedScan. A multi-modal 3D dataset with the\n  most-ever comprehensive language annotations for 3D-LLMs. Project page:\n  https://tai-wang.github.io/mmscan/",
    "pdf_url": "http://arxiv.org/pdf/2406.09401v1",
    "published_date": "2024-06-13 17:59:30 UTC",
    "updated_date": "2024-06-13 17:59:30 UTC"
  },
  {
    "arxiv_id": "2406.09402v1",
    "title": "Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion",
    "authors": [
      "Linzhan Mou",
      "Jun-Kun Chen",
      "Yu-Xiong Wang"
    ],
    "abstract": "This paper proposes Instruct 4D-to-4D that achieves 4D awareness and\nspatial-temporal consistency for 2D diffusion models to generate high-quality\ninstruction-guided dynamic scene editing results. Traditional applications of\n2D diffusion models in dynamic scene editing often result in inconsistency,\nprimarily due to their inherent frame-by-frame editing methodology. Addressing\nthe complexities of extending instruction-guided editing to 4D, our key insight\nis to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems:\nachieving temporal consistency in video editing and applying these edits to the\npseudo-3D scene. Following this, we first enhance the Instruct-Pix2Pix (IP2P)\nmodel with an anchor-aware attention module for batch processing and consistent\nediting. Additionally, we integrate optical flow-guided appearance propagation\nin a sliding window fashion for more precise frame-to-frame editing and\nincorporate depth-based projection to manage the extensive data of pseudo-3D\nscenes, followed by iterative editing to achieve convergence. We extensively\nevaluate our approach in various scenes and editing instructions, and\ndemonstrate that it achieves spatially and temporally consistent editing\nresults, with significantly enhanced detail and sharpness over the prior art.\nNotably, Instruct 4D-to-4D is general and applicable to both monocular and\nchallenging multi-camera scenes. Code and more results are available at\nimmortalco.github.io/Instruct-4D-to-4D.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09402v1",
    "published_date": "2024-06-13 17:59:30 UTC",
    "updated_date": "2024-06-13 17:59:30 UTC"
  },
  {
    "arxiv_id": "2406.09397v1",
    "title": "Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms",
    "authors": [
      "Miaosen Zhang",
      "Yixuan Wei",
      "Zhen Xing",
      "Yifei Ma",
      "Zuxuan Wu",
      "Ji Li",
      "Zheng Zhang",
      "Qi Dai",
      "Chong Luo",
      "Xin Geng",
      "Baining Guo"
    ],
    "abstract": "Modern vision models are trained on very large noisy datasets. While these\nmodels acquire strong capabilities, they may not follow the user's intent to\noutput the desired results in certain aspects, e.g., visual aesthetic,\npreferred style, and responsibility. In this paper, we target the realm of\nvisual aesthetics and aim to align vision models with human aesthetic standards\nin a retrieval system. Advanced retrieval systems usually adopt a cascade of\naesthetic models as re-rankers or filters, which are limited to low-level\nfeatures like saturation and perform poorly when stylistic, cultural or\nknowledge contexts are involved. We find that utilizing the reasoning ability\nof large language models (LLMs) to rephrase the search query and extend the\naesthetic expectations can make up for this shortcoming. Based on the above\nfindings, we propose a preference-based reinforcement learning method that\nfine-tunes the vision models to distill the knowledge from both LLMs reasoning\nand the aesthetic models to better align the vision models with human\naesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval\nsystems, we leverage large multi-modality model (LMM) to evaluate the aesthetic\nperformance with their strong abilities. As aesthetic assessment is one of the\nmost subjective tasks, to validate the robustness of LMM, we further propose a\nnovel dataset named HPIR to benchmark the alignment with human aesthetics.\nExperiments demonstrate that our method significantly enhances the aesthetic\nbehaviors of the vision models, under several metrics. We believe the proposed\nalgorithm can be a general practice for aligning vision models with human\nvalues.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 26 figures, under review",
    "pdf_url": "http://arxiv.org/pdf/2406.09397v1",
    "published_date": "2024-06-13 17:59:20 UTC",
    "updated_date": "2024-06-13 17:59:20 UTC"
  },
  {
    "arxiv_id": "2406.09393v1",
    "title": "Improving Autoregressive Training with Dynamic Oracles",
    "authors": [
      "Jianing Yang",
      "Harshine Visvanathan",
      "Yilin Wang",
      "Xinyi Hu",
      "Matthew Gormley"
    ],
    "abstract": "Many tasks within NLP can be framed as sequential decision problems, ranging\nfrom sequence tagging to text generation. However, for many tasks, the standard\ntraining methods, including maximum likelihood (teacher forcing) and scheduled\nsampling, suffer from exposure bias and a mismatch between metrics employed\nduring training and inference. DAgger provides a solution to mitigate these\nproblems, yet it requires a metric-specific dynamic oracle algorithm, which\ndoes not exist for many common metrics like span-based F1, ROUGE, and BLEU. In\nthis paper, we develop these novel dynamic oracles and show they maintain\nDAgger's no-regret guarantee for decomposable metrics like span-based F1. We\nevaluate the algorithm's performance on named entity recognition (NER), text\nsummarization, and machine translation (MT). While DAgger with dynamic oracle\nyields less favorable results in our MT experiments, it outperforms the\nbaseline techniques in NER and text summarization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09393v1",
    "published_date": "2024-06-13 17:59:09 UTC",
    "updated_date": "2024-06-13 17:59:09 UTC"
  },
  {
    "arxiv_id": "2406.09391v1",
    "title": "A More Practical Approach to Machine Unlearning",
    "authors": [
      "David Zagardo"
    ],
    "abstract": "Machine learning models often incorporate vast amounts of data, raising\nsignificant privacy concerns. Machine unlearning, the ability to remove the\ninfluence of specific data points from a trained model, addresses these\nconcerns. This paper explores practical methods for implementing machine\nunlearning, focusing on a first-epoch gradient-ascent approach.\n  Key findings include: 1. Single vs. Multi-Epoch Unlearning: First-epoch\ngradient unlearning is more effective than multi-epoch gradients. 2.\nLayer-Based Unlearning: The embedding layer in GPT-2 is crucial for effective\nunlearning. Gradients from the output layers (11 and 12) have no impact.\nEfficient unlearning can be achieved using only the embedding layer, halving\nspace complexity. 3. Influence Functions & Scoring: Techniques like Hessian\nVector Product and the dot product of activations and tensors are used for\nquantifying unlearning. 4. Gradient Ascent Considerations: Calibration is\nnecessary to avoid overexposing the model to specific data points during\nunlearning, which could prematurely terminate the process. 5. Fuzzy Matching\nvs. Iterative Unlearning: Fuzzy matching techniques shift the model to a new\noptimum, while iterative unlearning provides a more complete modality.\n  Our empirical evaluation confirms that first-epoch gradient ascent for\nmachine unlearning is more effective than whole-model gradient ascent. These\nresults highlight the potential of machine unlearning for enhancing data\nprivacy and compliance with regulations such as GDPR and CCPA. The study\nunderscores the importance of formal methods to comprehensively evaluate the\nunlearning process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09391v1",
    "published_date": "2024-06-13 17:59:06 UTC",
    "updated_date": "2024-06-13 17:59:06 UTC"
  },
  {
    "arxiv_id": "2406.09388v1",
    "title": "Exploring the Spectrum of Visio-Linguistic Compositionality and Recognition",
    "authors": [
      "Youngtaek Oh",
      "Pyunghwan Ahn",
      "Jinhyung Kim",
      "Gwangmo Song",
      "Soonyoung Lee",
      "In So Kweon",
      "Junmo Kim"
    ],
    "abstract": "Vision and language models (VLMs) such as CLIP have showcased remarkable\nzero-shot recognition abilities yet face challenges in visio-linguistic\ncompositionality, particularly in linguistic comprehension and fine-grained\nimage-text alignment. This paper explores the intricate relationship between\ncompositionality and recognition -- two pivotal aspects of VLM capability. We\nconduct a comprehensive evaluation of existing VLMs, covering both pre-training\napproaches aimed at recognition and the fine-tuning methods designed to improve\ncompositionality. Our evaluation employs 12 benchmarks for compositionality,\nalong with 21 zero-shot classification and two retrieval benchmarks for\nrecognition. In our analysis from 274 CLIP model checkpoints, we reveal\npatterns and trade-offs that emerge between compositional understanding and\nrecognition accuracy. Ultimately, this necessitates strategic efforts towards\ndeveloping models that improve both capabilities, as well as the meticulous\nformulation of benchmarks for compositionality. We open our evaluation\nframework at https://github.com/ytaek-oh/vl_compo.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPRW 2024 on 'What is Next in Multimodal Foundation\n  Models?'. Code: https://github.com/ytaek-oh/vl_compo",
    "pdf_url": "http://arxiv.org/pdf/2406.09388v1",
    "published_date": "2024-06-13 17:58:39 UTC",
    "updated_date": "2024-06-13 17:58:39 UTC"
  },
  {
    "arxiv_id": "2406.14572v3",
    "title": "Bioptic -- A Target-Agnostic Potency-Based Small Molecules Search Engine",
    "authors": [
      "Vlad Vinogradov",
      "Ivan Izmailov",
      "Simon Steshin",
      "Kong T. Nguyen"
    ],
    "abstract": "Recent successes in virtual screening have been made possible by large models\nand extensive chemical libraries. However, combining these elements is\nchallenging: the larger the model, the more expensive it is to run, making\nultra-large libraries unfeasible. To address this, we developed a\ntarget-agnostic, efficacy-based molecule search model, which allows us to find\nstructurally dissimilar molecules with similar biological activities. We used\nthe best practices to design fast retrieval system, based on\nprocessor-optimized SIMD instructions, enabling us to screen the ultra-large\n40B Enamine REAL library with 100\\% recall rate. We extensively benchmarked our\nmodel and several state-of-the-art models for both speed performance and\nretrieval quality of novel molecules.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14572v3",
    "published_date": "2024-06-13 17:53:29 UTC",
    "updated_date": "2024-07-01 01:33:10 UTC"
  },
  {
    "arxiv_id": "2406.09363v2",
    "title": "ElicitationGPT: Text Elicitation Mechanisms via Language Models",
    "authors": [
      "Yifan Wu",
      "Jason Hartline"
    ],
    "abstract": "Scoring rules evaluate probabilistic forecasts of an unknown state against\nthe realized state and are a fundamental building block in the incentivized\nelicitation of information and the training of machine learning models. This\npaper develops mechanisms for scoring elicited text against ground truth text\nusing domain-knowledge-free queries to a large language model (specifically\nChatGPT) and empirically evaluates their alignment with human preferences. The\nempirical evaluation is conducted on peer reviews from a peer-grading dataset\nand in comparison to manual instructor scores for the peer reviews.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09363v2",
    "published_date": "2024-06-13 17:49:10 UTC",
    "updated_date": "2024-06-19 00:12:35 UTC"
  },
  {
    "arxiv_id": "2406.09495v4",
    "title": "FADE: Towards Fairness-aware Generation for Domain Generalization via Classifier-Guided Score-based Diffusion Models",
    "authors": [
      "Yujie Lin",
      "Dong Li",
      "Minglai Shao",
      "Guihong Wan",
      "Chen Zhao"
    ],
    "abstract": "Fairness-aware domain generalization (FairDG) has emerged as a critical\nchallenge for deploying trustworthy AI systems, particularly in scenarios\ninvolving distribution shifts. Traditional methods for addressing fairness have\nfailed in domain generalization due to their lack of consideration for\ndistribution shifts. Although disentanglement has been used to tackle FairDG,\nit is limited by its strong assumptions. To overcome these limitations, we\npropose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as\na novel approach to effectively address the FairDG issue. Specifically, we\nfirst pre-train a score-based diffusion model (SDM) and two classifiers to\nequip the model with strong generalization capabilities across different\ndomains. Then, we guide the SDM using these pre-trained classifiers to\neffectively eliminate sensitive information from the generated data. Finally,\nthe generated fair data is used to train downstream classifiers, ensuring\nrobust performance under new data distributions. Extensive experiments on three\nreal-world datasets demonstrate that FADE not only enhances fairness but also\nimproves accuracy in the presence of distribution shifts. Additionally, FADE\noutperforms existing methods in achieving the best accuracy-fairness\ntrade-offs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09495v4",
    "published_date": "2024-06-13 17:36:05 UTC",
    "updated_date": "2025-04-30 04:53:53 UTC"
  },
  {
    "arxiv_id": "2406.09346v2",
    "title": "Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores",
    "authors": [
      "Álvaro Ciudad",
      "Adrián Morales-Pastor",
      "Laura Malo",
      "Isaac Filella-Mercè",
      "Victor Guallar",
      "Alexis Molina"
    ],
    "abstract": "In this study, we present ScoreFormer, a novel graph transformer model\ndesigned to accurately predict molecular docking scores, thereby optimizing\nhigh-throughput virtual screening (HTVS) in drug discovery. The architecture\nintegrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk\nPositional Encodings (LRWPE), enhancing the model's ability to understand\ncomplex molecular structures and their relationship with their respective\ndocking scores. This approach significantly surpasses traditional HTVS methods\nand recent Graph Neural Network (GNN) models in both recovery and efficiency\ndue to a wider coverage of the chemical space and enhanced performance. Our\nresults demonstrate that ScoreFormer achieves competitive performance in\ndocking score prediction and offers a substantial 1.65-fold reduction in\ninference time compared to existing models. We evaluated ScoreFormer across\nmultiple datasets under various conditions, confirming its robustness and\nreliability in identifying potential drug candidates rapidly.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 1st Machine Learning for Life and Material Sciences\n  Workshop at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09346v2",
    "published_date": "2024-06-13 17:31:02 UTC",
    "updated_date": "2024-06-25 13:25:08 UTC"
  },
  {
    "arxiv_id": "2406.09329v2",
    "title": "Is Value Learning Really the Main Bottleneck in Offline RL?",
    "authors": [
      "Seohong Park",
      "Kevin Frans",
      "Sergey Levine",
      "Aviral Kumar"
    ],
    "abstract": "While imitation learning requires access to high-quality data, offline\nreinforcement learning (RL) should, in principle, perform similarly or better\nwith substantially lower data quality by using a value function. However,\ncurrent results indicate that offline RL often performs worse than imitation\nlearning, and it is often unclear what holds back the performance of offline\nRL. Motivated by this observation, we aim to understand the bottlenecks in\ncurrent offline RL algorithms. While poor performance of offline RL is\ntypically attributed to an imperfect value function, we ask: is the main\nbottleneck of offline RL indeed in learning the value function, or something\nelse? To answer this question, we perform a systematic empirical study of (1)\nvalue learning, (2) policy extraction, and (3) policy generalization in offline\nRL problems, analyzing how these components affect performance. We make two\nsurprising observations. First, we find that the choice of a policy extraction\nalgorithm significantly affects the performance and scalability of offline RL,\noften more so than the value learning objective. For instance, we show that\ncommon value-weighted behavioral cloning objectives (e.g., AWR) do not fully\nleverage the learned value function, and switching to behavior-constrained\npolicy gradient objectives (e.g., DDPG+BC) often leads to substantial\nimprovements in performance and scalability. Second, we find that a big barrier\nto improving offline RL performance is often imperfect policy generalization on\ntest-time states out of the support of the training data, rather than policy\nlearning on in-distribution states. We then show that the use of suboptimal but\nhigh-coverage data or test-time policy training techniques can address this\ngeneralization issue in practice. Specifically, we propose two simple test-time\npolicy improvement methods and show that these methods lead to better\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09329v2",
    "published_date": "2024-06-13 17:07:49 UTC",
    "updated_date": "2024-10-28 23:33:19 UTC"
  },
  {
    "arxiv_id": "2406.09326v2",
    "title": "PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance",
    "authors": [
      "Qijun Gan",
      "Song Wang",
      "Shengtao Wu",
      "Jianke Zhu"
    ],
    "abstract": "Recently, artificial intelligence techniques for education have been received\nincreasing attentions, while it still remains an open problem to design the\neffective music instrument instructing systems. Although key presses can be\ndirectly derived from sheet music, the transitional movements among key presses\nrequire more extensive guidance in piano performance. In this work, we\nconstruct a piano-hand motion generation benchmark to guide hand movements and\nfingerings for piano playing. To this end, we collect an annotated dataset,\nPianoMotion10M, consisting of 116 hours of piano playing videos from a\nbird's-eye view with 10 million annotated hand poses. We also introduce a\npowerful baseline model that generates hand motions from piano audios through a\nposition predictor and a position-guided gesture generator. Furthermore, a\nseries of evaluation metrics are designed to assess the performance of the\nbaseline model, including motion similarity, smoothness, positional accuracy of\nleft and right hands, and overall fidelity of movement distribution. Despite\nthat piano key presses with respect to music scores or audios are already\naccessible, PianoMotion10M aims to provide guidance on piano fingering for\ninstruction purposes. The source code and dataset can be accessed at\nhttps://github.com/agnJason/PianoMotion10M.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "ICLR 2025 Spotlight",
    "pdf_url": "http://arxiv.org/pdf/2406.09326v2",
    "published_date": "2024-06-13 17:05:23 UTC",
    "updated_date": "2025-02-25 08:21:57 UTC"
  },
  {
    "arxiv_id": "2406.09324v3",
    "title": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs",
    "authors": [
      "Zhao Xu",
      "Fan Liu",
      "Hao Liu"
    ],
    "abstract": "Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we introduced\n$\\textbf{JailTrickBench}$ to evaluate the impact of various attack settings on\nLLM performance and provide a baseline for jailbreak attacks, encouraging the\nadoption of a standardized evaluation framework. Specifically, we evaluate the\neight key factors of implementing jailbreak attacks on LLMs from both\ntarget-level and attack-level perspectives. We further conduct seven\nrepresentative jailbreak attacks on six defense methods across two widely used\ndatasets, encompassing approximately 354 experiments with about 55,000 GPU\nhours on A800-80G. Our experimental results highlight the need for standardized\nbenchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is\navailable at https://github.com/usail-hkust/JailTrickBench.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09324v3",
    "published_date": "2024-06-13 17:01:40 UTC",
    "updated_date": "2024-11-06 04:43:57 UTC"
  },
  {
    "arxiv_id": "2406.09322v2",
    "title": "Active Inference Meeting Energy-Efficient Control of Parallel and Identical Machines",
    "authors": [
      "Yavar Taheri Yeganeh",
      "Mohsen Jafari",
      "Andrea Matta"
    ],
    "abstract": "We investigate the application of active inference in developing\nenergy-efficient control agents for manufacturing systems. Active inference,\nrooted in neuroscience, provides a unified probabilistic framework integrating\nperception, learning, and action, with inherent uncertainty quantification\nelements. Our study explores deep active inference, an emerging field that\ncombines deep learning with the active inference decision-making framework.\nLeveraging a deep active inference agent, we focus on controlling parallel and\nidentical machine workstations to enhance energy efficiency. We address\nchallenges posed by the problem's stochastic nature and delayed policy response\nby introducing tailored enhancements to existing agent architectures.\nSpecifically, we introduce multi-step transition and hybrid horizon methods to\nmitigate the need for complex planning. Our experimental results demonstrate\nthe effectiveness of these enhancements and highlight the potential of the\nactive inference-based approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 10th International Conference on Machine Learning,\n  Optimization, and Data Science",
    "pdf_url": "http://arxiv.org/pdf/2406.09322v2",
    "published_date": "2024-06-13 17:00:30 UTC",
    "updated_date": "2024-11-13 17:08:34 UTC"
  },
  {
    "arxiv_id": "2406.09321v2",
    "title": "JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models",
    "authors": [
      "Delong Ran",
      "Jinyuan Liu",
      "Yichen Gong",
      "Jingyi Zheng",
      "Xinlei He",
      "Tianshuo Cong",
      "Anyu Wang"
    ],
    "abstract": "Jailbreak attacks induce Large Language Models (LLMs) to generate harmful\nresponses, posing severe misuse threats. Though research on jailbreak attacks\nand defenses is emerging, there is no consensus on evaluating jailbreaks, i.e.,\nthe methods to assess the harmfulness of an LLM's response are varied. Each\napproach has its own set of strengths and weaknesses, impacting their alignment\nwith human values, as well as the time and financial cost. This diversity\nchallenges researchers in choosing suitable evaluation methods and comparing\ndifferent attacks and defenses. In this paper, we conduct a comprehensive\nanalysis of jailbreak evaluation methodologies, drawing from nearly 90\njailbreak research published between May 2023 and April 2024. Our study\nintroduces a systematic taxonomy of jailbreak evaluators, offering indepth\ninsights into their strengths and weaknesses, along with the current status of\ntheir adaptation. To aid further research, we propose JailbreakEval, a toolkit\nfor evaluating jailbreak attempts. JailbreakEval includes various evaluators\nout-of-the-box, enabling users to obtain results with a single command or\ncustomized evaluation workflows. In summary, we regard JailbreakEval to be a\ncatalyst that simplifies the evaluation process in jailbreak research and\nfosters an inclusive standard for jailbreak evaluation within the community.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "This is the Extended Version for the Poster at NDSS Symposium 2025,\n  Feb 24-28, 2025. Our code is available at\n  https://github.com/ThuCCSLab/JailbreakEval",
    "pdf_url": "http://arxiv.org/pdf/2406.09321v2",
    "published_date": "2024-06-13 16:59:43 UTC",
    "updated_date": "2025-02-04 16:04:22 UTC"
  },
  {
    "arxiv_id": "2406.09318v1",
    "title": "Characterising Interventions in Causal Games",
    "authors": [
      "Manuj Mishra",
      "James Fox",
      "Michael Wooldridge"
    ],
    "abstract": "Causal games are probabilistic graphical models that enable causal queries to\nbe answered in multi-agent settings. They extend causal Bayesian networks by\nspecifying decision and utility variables to represent the agents' degrees of\nfreedom and objectives. In multi-agent settings, whether each agent decides on\ntheir policy before or after knowing the causal intervention is important as\nthis affects whether they can respond to the intervention by adapting their\npolicy. Consequently, previous work in causal games imposed chronological\nconstraints on permissible interventions. We relax this by outlining a sound\nand complete set of primitive causal interventions so the effect of any\narbitrarily complex interventional query can be studied in multi-agent\nsettings. We also demonstrate applications to the design of safe AI systems by\nconsidering causal mechanism design and commitment.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.GT",
    "comment": "Accepted to the 40th Conference on Uncertainty in Artificial\n  Intelligence (UAI-2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.09318v1",
    "published_date": "2024-06-13 16:55:07 UTC",
    "updated_date": "2024-06-13 16:55:07 UTC"
  },
  {
    "arxiv_id": "2406.09315v1",
    "title": "Vertical LoRA: Dense Expectation-Maximization Interpretation of Transformers",
    "authors": [
      "Zhuolin Fu"
    ],
    "abstract": "In this paper, we show how Transformers can be interpreted as dense\nExpectation-Maximization algorithms performed on Bayesian Nets. Based on the\nabove interpretation, we propose a new model design paradigm, namely Vertical\nLoRA (VLoRA), which reduces the parameter count dramatically while preserving\nperformance. In VLoRA, a model consists of layers, each of which recursively\nlearns an increment based on the previous layer. We then apply LoRA\ndecomposition to the increments. VLoRA works on the base model, which is\northogonal to LoRA, meaning they can be used together. We do experiments on\nvarious tasks and models. The results show that 1) with VLoRA, the Transformer\nmodel parameter count can be reduced dramatically and 2) the performance of the\noriginal model is preserved. The source code is available at\n\\url{https://github.com/neverUseThisName/vlora}",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09315v1",
    "published_date": "2024-06-13 16:51:33 UTC",
    "updated_date": "2024-06-13 16:51:33 UTC"
  },
  {
    "arxiv_id": "2406.09313v2",
    "title": "Less Cybersickness, Please: Demystifying and Detecting Stereoscopic Visual Inconsistencies in Virtual Reality Apps",
    "authors": [
      "Shuqing Li",
      "Cuiyun Gao",
      "Jianping Zhang",
      "Yujia Zhang",
      "Yepang Liu",
      "Jiazhen Gu",
      "Yun Peng",
      "Michael R. Lyu"
    ],
    "abstract": "The quality of Virtual Reality (VR) apps is vital, particularly the rendering\nquality of the VR Graphical User Interface (GUI). Different from traditional 2D\napps, VR apps create a 3D digital scene for users, by rendering two distinct 2D\nimages for the user's left and right eyes, respectively. Stereoscopic visual\ninconsistency (denoted as \"SVI\") issues, however, undermine the rendering\nprocess of the user's brain, leading to user discomfort and even adverse health\neffects. Such issues commonly exist but remain underexplored. We conduct an\nempirical analysis on 282 SVI bug reports from 15 VR platforms, summarizing 15\ntypes of manifestations. The empirical analysis reveals that automatically\ndetecting SVI issues is challenging, mainly because: (1) lack of training data;\n(2) the manifestations of SVI issues are diverse, complicated, and often\napplication-specific; (3) most accessible VR apps are closed-source commercial\nsoftware. Existing pattern-based supervised classification approaches may be\ninapplicable or ineffective in detecting the SVI issues. To counter these\nchallenges, we propose an unsupervised black-box testing framework named\nStereoID to identify the stereoscopic visual inconsistencies, based only on the\nrendered GUI states. StereoID generates a synthetic right-eye image based on\nthe actual left-eye image and computes distances between the synthetic\nright-eye image and the actual right-eye image to detect SVI issues. We propose\na depth-aware conditional stereo image translator to power the image generation\nprocess, which captures the expected perspective shifts between left-eye and\nright-eye images. We build a large-scale unlabeled VR stereo screenshot dataset\nwith larger than 171K images from 288 real-world VR apps for experiments. After\nsubstantial experiments, StereoID demonstrates superior performance for\ndetecting SVI issues in both user reports and wild VR apps.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.MM",
      "D.2.5; H.5.1; H.5.2"
    ],
    "primary_category": "cs.SE",
    "comment": "This work has been accepted at the ACM International Conference on\n  the Foundations of Software Engineering (FSE) 2024, Porto de Galinhas,\n  Brazil. DOI: https://doi.org/10.1145/3660803",
    "pdf_url": "http://arxiv.org/pdf/2406.09313v2",
    "published_date": "2024-06-13 16:48:48 UTC",
    "updated_date": "2024-09-20 03:36:27 UTC"
  },
  {
    "arxiv_id": "2406.09296v2",
    "title": "Parameter-Efficient Active Learning for Foundational models",
    "authors": [
      "Athmanarayanan Lakshmi Narayanan",
      "Ranganath Krishnan",
      "Amrutha Machireddy",
      "Mahesh Subedar"
    ],
    "abstract": "Foundational vision transformer models have shown impressive few shot\nperformance on many vision tasks. This research presents a novel investigation\ninto the application of parameter efficient fine-tuning methods within an\nactive learning (AL) framework, to advance the sampling selection process in\nextremely budget constrained classification tasks. The focus on image datasets,\nknown for their out-of-distribution characteristics, adds a layer of complexity\nand relevance to our study. Through a detailed evaluation, we illustrate the\nimproved AL performance on these challenging datasets, highlighting the\nstrategic advantage of merging parameter efficient fine tuning methods with\nfoundation models. This contributes to the broader discourse on optimizing AL\nstrategies, presenting a promising avenue for future exploration in leveraging\nfoundation models for efficient and effective data annotation in specialized\ndomains.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for CVPR2024 Transformers for Vision Workshop",
    "pdf_url": "http://arxiv.org/pdf/2406.09296v2",
    "published_date": "2024-06-13 16:30:32 UTC",
    "updated_date": "2024-06-14 04:40:09 UTC"
  },
  {
    "arxiv_id": "2406.09292v2",
    "title": "Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image Diffusion Models",
    "authors": [
      "Ziyi Wu",
      "Yulia Rubanova",
      "Rishabh Kabra",
      "Drew A. Hudson",
      "Igor Gilitschenski",
      "Yusuf Aytar",
      "Sjoerd van Steenkiste",
      "Kelsey R. Allen",
      "Thomas Kipf"
    ],
    "abstract": "We address the problem of multi-object 3D pose control in image diffusion\nmodels. Instead of conditioning on a sequence of text tokens, we propose to use\na set of per-object representations, Neural Assets, to control the 3D pose of\nindividual objects in a scene. Neural Assets are obtained by pooling visual\nrepresentations of objects from a reference image, such as a frame in a video,\nand are trained to reconstruct the respective objects in a different image,\ne.g., a later frame in the video. Importantly, we encode object visuals from\nthe reference image while conditioning on object poses from the target frame.\nThis enables learning disentangled appearance and pose features. Combining\nvisual and 3D pose representations in a sequence-of-tokens format allows us to\nkeep the text-to-image architecture of existing models, with Neural Assets in\nplace of text tokens. By fine-tuning a pre-trained text-to-image diffusion\nmodel with this information, our approach enables fine-grained 3D pose and\nplacement control of individual objects in a scene. We further demonstrate that\nNeural Assets can be transferred and recomposed across different scenes. Our\nmodel achieves state-of-the-art multi-object editing results on both synthetic\n3D scene datasets, as well as two real-world video datasets (Objectron, Waymo\nOpen).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Additional details and video results are available at\n  https://neural-assets-paper.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.09292v2",
    "published_date": "2024-06-13 16:29:18 UTC",
    "updated_date": "2024-10-28 23:42:11 UTC"
  },
  {
    "arxiv_id": "2406.09289v2",
    "title": "Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models",
    "authors": [
      "Sarah Ball",
      "Frauke Kreuter",
      "Nina Panickssery"
    ],
    "abstract": "Conversational large language models are trained to refuse to answer harmful\nquestions. However, emergent jailbreaking techniques can still elicit unsafe\noutputs, presenting an ongoing challenge for model alignment. To better\nunderstand how different jailbreak types circumvent safeguards, this paper\nanalyses model activations on different jailbreak inputs. We find that it is\npossible to extract a jailbreak vector from a single class of jailbreaks that\nworks to mitigate jailbreak effectiveness from other semantically-dissimilar\nclasses. This may indicate that different kinds of effective jailbreaks operate\nvia a similar internal mechanism. We investigate a potential common mechanism\nof harmfulness feature suppression, and find evidence that effective jailbreaks\nnoticeably reduce a model's perception of prompt harmfulness. These findings\noffer actionable insights for developing more robust jailbreak countermeasures\nand lay the groundwork for a deeper, mechanistic understanding of jailbreak\ndynamics in language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "37 pages, added analyses for 3 more models",
    "pdf_url": "http://arxiv.org/pdf/2406.09289v2",
    "published_date": "2024-06-13 16:26:47 UTC",
    "updated_date": "2024-10-05 19:56:20 UTC"
  },
  {
    "arxiv_id": "2406.09272v3",
    "title": "Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos",
    "authors": [
      "Changan Chen",
      "Puyuan Peng",
      "Ami Baid",
      "Zihui Xue",
      "Wei-Ning Hsu",
      "David Harwath",
      "Kristen Grauman"
    ],
    "abstract": "Generating realistic audio for human actions is important for many\napplications, such as creating sound effects for films or virtual reality\ngames. Existing approaches implicitly assume total correspondence between the\nvideo and audio during training, yet many sounds happen off-screen and have\nweak to no correspondence with the visuals -- resulting in uncontrolled ambient\nsounds or hallucinations at test time. We propose a novel ambient-aware audio\ngeneration model, AV-LDM. We devise a novel audio-conditioning mechanism to\nlearn to disentangle foreground action sounds from the ambient background\nsounds in in-the-wild training videos. Given a novel silent video, our model\nuses retrieval-augmented generation to create audio that matches the visual\ncontent both semantically and temporally. We train and evaluate our model on\ntwo in-the-wild egocentric video datasets, Ego4D and EPIC-KITCHENS, and we\nintroduce Ego4D-Sounds -- 1.2M curated clips with action-audio correspondence.\nOur model outperforms an array of existing methods, allows controllable\ngeneration of the ambient sound, and even shows promise for generalizing to\ncomputer graphics game clips. Overall, our approach is the first to focus\nvideo-to-audio generation faithfully on the observed visual content despite\ntraining from uncurated clips with natural background sounds.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://vision.cs.utexas.edu/projects/action2sound.\n  ECCV 2024 camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2406.09272v3",
    "published_date": "2024-06-13 16:10:19 UTC",
    "updated_date": "2024-07-25 15:03:37 UTC"
  },
  {
    "arxiv_id": "2406.09264v3",
    "title": "Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions",
    "authors": [
      "Hua Shen",
      "Tiffany Knearem",
      "Reshmi Ghosh",
      "Kenan Alkiek",
      "Kundan Krishna",
      "Yachuan Liu",
      "Ziqiao Ma",
      "Savvas Petridis",
      "Yi-Hao Peng",
      "Li Qiwei",
      "Sushrita Rakshit",
      "Chenglei Si",
      "Yutong Xie",
      "Jeffrey P. Bigham",
      "Frank Bentley",
      "Joyce Chai",
      "Zachary Lipton",
      "Qiaozhu Mei",
      "Rada Mihalcea",
      "Michael Terry",
      "Diyi Yang",
      "Meredith Ringel Morris",
      "Paul Resnick",
      "David Jurgens"
    ],
    "abstract": "Recent advancements in general-purpose AI have highlighted the importance of\nguiding AI systems towards the intended goals, ethical principles, and values\nof individuals and groups, a concept broadly recognized as alignment. However,\nthe lack of clarified definitions and scopes of human-AI alignment poses a\nsignificant obstacle, hampering collaborative efforts across research domains\nto achieve this alignment. In particular, ML- and philosophy-oriented alignment\nresearch often views AI alignment as a static, unidirectional process (i.e.,\naiming to ensure that AI systems' objectives match humans) rather than an\nongoing, mutual alignment problem. This perspective largely neglects the\nlong-term interaction and dynamic changes of alignment. To understand these\ngaps, we introduce a systematic review of over 400 papers published between\n2019 and January 2024, spanning multiple domains such as Human-Computer\nInteraction (HCI), Natural Language Processing (NLP), Machine Learning (ML). We\ncharacterize, define and scope human-AI alignment. From this, we present a\nconceptual framework of \"Bidirectional Human-AI Alignment\" to organize the\nliterature from a human-centered perspective. This framework encompasses both\n1) conventional studies of aligning AI to humans that ensures AI produces the\nintended outcomes determined by humans, and 2) a proposed concept of aligning\nhumans to AI, which aims to help individuals and society adjust to AI\nadvancements both cognitively and behaviorally. Additionally, we articulate the\nkey findings derived from literature analysis, including literature gaps and\ntrends, human values, and interaction techniques. To pave the way for future\nstudies, we envision three key challenges and give recommendations for future\nresearch.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "proposing \"bidirectional human-AI alignment\" framework after a\n  systematic review of over 400 alignment papers",
    "pdf_url": "http://arxiv.org/pdf/2406.09264v3",
    "published_date": "2024-06-13 16:03:25 UTC",
    "updated_date": "2024-08-10 17:50:39 UTC"
  },
  {
    "arxiv_id": "2406.09260v1",
    "title": "Deep Transformer Network for Monocular Pose Estimation of Ship-Based UAV",
    "authors": [
      "Maneesha Wickramasuriya",
      "Taeyoung Lee",
      "Murray Snyder"
    ],
    "abstract": "This paper introduces a deep transformer network for estimating the relative\n6D pose of a Unmanned Aerial Vehicle (UAV) with respect to a ship using\nmonocular images. A synthetic dataset of ship images is created and annotated\nwith 2D keypoints of multiple ship parts. A Transformer Neural Network model is\ntrained to detect these keypoints and estimate the 6D pose of each part. The\nestimates are integrated using Bayesian fusion. The model is tested on\nsynthetic data and in-situ flight experiments, demonstrating robustness and\naccuracy in various lighting conditions. The position estimation error is\napproximately 0.8\\% and 1.0\\% of the distance to the ship for the synthetic\ndata and the flight experiments, respectively. The method has potential\napplications for ship-based autonomous UAV landing and navigation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 25 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.09260v1",
    "published_date": "2024-06-13 16:01:22 UTC",
    "updated_date": "2024-06-13 16:01:22 UTC"
  },
  {
    "arxiv_id": "2406.09250v2",
    "title": "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models",
    "authors": [
      "Samar Fares",
      "Klea Ziu",
      "Toluwani Aremu",
      "Nikita Durasov",
      "Martin Takáč",
      "Pascal Fua",
      "Karthik Nandakumar",
      "Ivan Laptev"
    ],
    "abstract": "Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09250v2",
    "published_date": "2024-06-13 15:55:04 UTC",
    "updated_date": "2024-10-17 11:46:45 UTC"
  },
  {
    "arxiv_id": "2406.09242v1",
    "title": "Towards a Characterisation of Monte-Carlo Tree Search Performance in Different Games",
    "authors": [
      "Dennis J. N. J. Soemers",
      "Guillaume Bams",
      "Max Persoon",
      "Marco Rietjens",
      "Dimitar Sladić",
      "Stefan Stefanov",
      "Kurt Driessens",
      "Mark H. M. Winands"
    ],
    "abstract": "Many enhancements to Monte-Carlo Tree Search (MCTS) have been proposed over\nalmost two decades of general game playing and other artificial intelligence\nresearch. However, our ability to characterise and understand which variants\nwork well or poorly in which games is still lacking. This paper describes work\non an initial dataset that we have built to make progress towards such an\nunderstanding: 268,386 plays among 61 different agents across 1494 distinct\ngames. We describe a preliminary analysis and work on training predictive\nmodels on this dataset, as well as lessons learned and future plans for a new\nand improved version of the dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication in the Proceedings of the 2024 IEEE\n  Conference on Games",
    "pdf_url": "http://arxiv.org/pdf/2406.09242v1",
    "published_date": "2024-06-13 15:46:27 UTC",
    "updated_date": "2024-06-13 15:46:27 UTC"
  },
  {
    "arxiv_id": "2406.09486v1",
    "title": "SeMOPO: Learning High-quality Model and Policy from Low-quality Offline Visual Datasets",
    "authors": [
      "Shenghua Wan",
      "Ziyuan Chen",
      "Le Gan",
      "Shuai Feng",
      "De-Chuan Zhan"
    ],
    "abstract": "Model-based offline reinforcement Learning (RL) is a promising approach that\nleverages existing data effectively in many real-world applications, especially\nthose involving high-dimensional inputs like images and videos. To alleviate\nthe distribution shift issue in offline RL, existing model-based methods\nheavily rely on the uncertainty of learned dynamics. However, the model\nuncertainty estimation becomes significantly biased when observations contain\ncomplex distractors with non-trivial dynamics. To address this challenge, we\npropose a new approach - \\emph{Separated Model-based Offline Policy\nOptimization} (SeMOPO) - decomposing latent states into endogenous and\nexogenous parts via conservative sampling and estimating model uncertainty on\nthe endogenous states only. We provide a theoretical guarantee of model\nuncertainty and performance bound of SeMOPO. To assess the efficacy, we\nconstruct the Low-Quality Vision Deep Data-Driven Datasets for RL (LQV-D4RL),\nwhere the data are collected by non-expert policy and the observations include\nmoving distractors. Experimental results show that our method substantially\noutperforms all baseline methods, and further analytical experiments validate\nthe critical designs in our method. The project website is\n\\href{https://sites.google.com/view/semopo}{https://sites.google.com/view/semopo}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.09486v1",
    "published_date": "2024-06-13 15:16:38 UTC",
    "updated_date": "2024-06-13 15:16:38 UTC"
  },
  {
    "arxiv_id": "2406.09215v3",
    "title": "On Softmax Direct Preference Optimization for Recommendation",
    "authors": [
      "Yuxin Chen",
      "Junfei Tan",
      "An Zhang",
      "Zhengyi Yang",
      "Leheng Sheng",
      "Enzhi Zhang",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "abstract": "Recommender systems aim to predict personalized rankings based on user\npreference data. With the rise of Language Models (LMs), LM-based recommenders\nhave been widely explored due to their extensive world knowledge and powerful\nreasoning abilities. Most of the LM-based recommenders convert historical\ninteractions into language prompts, pairing with a positive item as the target\nresponse and fine-tuning LM with a language modeling loss. However, the current\nobjective fails to fully leverage preference data and is not optimized for\npersonalized ranking tasks, which hinders the performance of LM-based\nrecommenders. Inspired by the current advancement of Direct Preference\nOptimization (DPO) in human preference alignment and the success of softmax\nloss in recommendations, we propose Softmax-DPO (S-DPO) to instill ranking\ninformation into the LM to help LM-based recommenders distinguish preferred\nitems from negatives, rather than solely focusing on positives. Specifically,\nwe incorporate multiple negatives in user preference data and devise an\nalternative version of DPO loss tailored for LM-based recommenders, which is\nextended from the traditional full-ranking Plackett-Luce (PL) model to partial\nrankings and connected to softmax sampling strategies. Theoretically, we bridge\nS-DPO with the softmax loss over negative sampling and find that it has an\ninherent benefit of mining hard negatives, which assures its exceptional\ncapabilities in recommendation tasks. Empirically, extensive experiments\nconducted on three real-world datasets demonstrate the superiority of S-DPO to\neffectively model user preference and further boost recommendation performance\nwhile providing better rewards for preferred items. Our codes are available at\nhttps://github.com/chenyuxin1999/S-DPO.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09215v3",
    "published_date": "2024-06-13 15:16:11 UTC",
    "updated_date": "2024-11-07 18:30:53 UTC"
  },
  {
    "arxiv_id": "2406.09214v1",
    "title": "Applying Multi-Agent Negotiation to Solve the Production Routing Problem With Privacy Preserving",
    "authors": [
      "Luiza Pellin Biasoto",
      "Vinicius Renan de Carvalho",
      "Jaime Simão Sichman"
    ],
    "abstract": "This paper presents a novel approach to address the Production Routing\nProblem with Privacy Preserving (PRPPP) in supply chain optimization. The\nintegrated optimization of production, inventory, distribution, and routing\ndecisions in real-world industry applications poses several challenges,\nincluding increased complexity, discrepancies between planning and execution,\nand constraints on information sharing. To mitigate these challenges, this\npaper proposes the use of intelligent agent negotiation within a hybrid\nMulti-Agent System (MAS) integrated with optimization algorithms. The MAS\nfacilitates communication and coordination among entities, encapsulates private\ninformation, and enables negotiation. This, along with optimization algorithms,\nmakes it a compelling framework for establishing optimal solutions. The\napproach is supported by real-world applications and synergies between MAS and\noptimization methods, demonstrating its effectiveness in addressing complex\nsupply chain optimization problems.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "The 15th Workshop on Optimization and Learning in Multiagent Systems",
    "pdf_url": "http://arxiv.org/pdf/2406.09214v1",
    "published_date": "2024-06-13 15:15:34 UTC",
    "updated_date": "2024-06-13 15:15:34 UTC"
  },
  {
    "arxiv_id": "2406.09207v2",
    "title": "Investigating potential causes of Sepsis with Bayesian network structure learning",
    "authors": [
      "Bruno Petrungaro",
      "Neville K. Kitson",
      "Anthony C. Constantinou"
    ],
    "abstract": "Sepsis is a life-threatening and serious global health issue. This study\ncombines knowledge with available hospital data to investigate the potential\ncauses of Sepsis that can be affected by policy decisions. We investigate the\nunderlying causal structure of this problem by combining clinical expertise\nwith score-based, constraint-based, and hybrid structure learning algorithms. A\nnovel approach to model averaging and knowledge-based constraints was\nimplemented to arrive at a consensus structure for causal inference. The\nstructure learning process highlighted the importance of exploring data-driven\napproaches alongside clinical expertise. This includes discovering unexpected,\nalthough reasonable, relationships from a clinical perspective. Hypothetical\ninterventions on Chronic Obstructive Pulmonary Disease, Alcohol dependence, and\nDiabetes suggest that the presence of any of these risk factors in patients\nincreases the likelihood of Sepsis. This finding, alongside measuring the\neffect of these risk factors on Sepsis, has potential policy implications.\nRecognising the importance of prediction in improving health outcomes related\nto Sepsis, the model is also assessed in its ability to predict Sepsis by\nevaluating accuracy, sensitivity, and specificity. These three indicators all\nhad results around 70%, and the AUC was 80%, which means the causal structure\nof the model is reasonably accurate given that the models were trained on data\navailable for commissioning purposes only.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09207v2",
    "published_date": "2024-06-13 15:08:44 UTC",
    "updated_date": "2025-02-18 13:36:43 UTC"
  },
  {
    "arxiv_id": "2406.09206v2",
    "title": "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models",
    "authors": [
      "Christopher Schröder",
      "Gerhard Heyer"
    ],
    "abstract": "Active learning is an iterative labeling process that is used to obtain a\nsmall labeled subset, despite the absence of labeled data, thereby enabling to\ntrain a model for supervised tasks such as text classification. While active\nlearning has made considerable progress in recent years due to improvements\nprovided by pre-trained language models, there is untapped potential in the\noften neglected unlabeled portion of the data, although it is available in\nconsiderably larger quantities than the usually small set of labeled data. In\nthis work, we investigate how self-training, a semi-supervised approach that\nuses a model to obtain pseudo-labels for unlabeled data, can be used to improve\nthe efficiency of active learning for text classification. Building on a\ncomprehensive reproduction of four previous self-training approaches, some of\nwhich are evaluated for the first time in the context of active learning or\nnatural language processing, we introduce HAST, a new and effective\nself-training strategy, which is evaluated on four text classification\nbenchmarks. Our results show that it outperforms the reproduced self-training\napproaches and reaches classification results comparable to previous\nexperiments for three out of four datasets, using as little as 25% of the data.\nThe code is publicly available at\nhttps://github.com/chschroeder/self-training-for-sample-efficient-active-learning .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09206v2",
    "published_date": "2024-06-13 15:06:11 UTC",
    "updated_date": "2024-10-04 12:55:45 UTC"
  },
  {
    "arxiv_id": "2406.09205v1",
    "title": "ReadCtrl: Personalizing text generation with readability-controlled instruction learning",
    "authors": [
      "Hieu Tran",
      "Zonghai Yao",
      "Lingxi Li",
      "Hong Yu"
    ],
    "abstract": "Content generation conditioning on users's readability is an important\napplication for personalization. In an era of large language models (LLMs),\nreadability-controlled text generation based on LLMs has become increasingly\nimportant. This paper introduces a novel methodology called\n\"Readability-Controlled Instruction Learning (ReadCtrl),\" which aims to\ninstruction-tune LLMs to tailor users' readability levels. Unlike the\ntraditional methods, which primarily focused on categorical readability\nadjustments typically classified as high, medium, and low or expert and\nlayperson levels with limited success, ReadCtrl introduces a dynamic framework\nthat enables LLMs to generate content at various (near continuous level)\ncomplexity levels, thereby enhancing their versatility across different\napplications. Our results show that the ReadCtrl-Mistral-7B models\nsignificantly outperformed strong baseline models such as GPT-4 and Claude-3,\nwith a win rate of 52.1%:35.7% against GPT-4 in human evaluations. Furthermore,\nRead-Ctrl has shown significant improvements in automatic evaluations, as\nevidenced by better readability metrics (e.g., FOG, FKGL) and generation\nquality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and\nCoherence). These results underscore Read-Ctrl's effectiveness and tenacity in\nproducing high-quality, contextually appropriate outputs that closely align\nwith targeted readability levels, marking a significant advancement in\npersonalized content generation using LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.09205v1",
    "published_date": "2024-06-13 15:03:46 UTC",
    "updated_date": "2024-06-13 15:03:46 UTC"
  },
  {
    "arxiv_id": "2406.09202v1",
    "title": "Language Complexity and Speech Recognition Accuracy: Orthographic Complexity Hurts, Phonological Complexity Doesn't",
    "authors": [
      "Chihiro Taguchi",
      "David Chiang"
    ],
    "abstract": "We investigate what linguistic factors affect the performance of Automatic\nSpeech Recognition (ASR) models. We hypothesize that orthographic and\nphonological complexities both degrade accuracy. To examine this, we fine-tune\nthe multilingual self-supervised pretrained model Wav2Vec2-XLSR-53 on 25\nlanguages with 15 writing systems, and we compare their ASR accuracy, number of\ngraphemes, unigram grapheme entropy, logographicity (how much\nword/morpheme-level information is encoded in the writing system), and number\nof phonemes. The results demonstrate that orthographic complexities\nsignificantly correlate with low ASR accuracy, while phonological complexity\nshows no significant correlation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 5 figures, 5 tables, submitted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09202v1",
    "published_date": "2024-06-13 14:59:45 UTC",
    "updated_date": "2024-06-13 14:59:45 UTC"
  },
  {
    "arxiv_id": "2406.09181v2",
    "title": "A Large-scale Universal Evaluation Benchmark For Face Forgery Detection",
    "authors": [
      "Yijun Bei",
      "Hengrui Lou",
      "Jinsong Geng",
      "Erteng Liu",
      "Lechao Cheng",
      "Jie Song",
      "Mingli Song",
      "Zunlei Feng"
    ],
    "abstract": "With the rapid development of AI-generated content (AIGC) technology, the\nproduction of realistic fake facial images and videos that deceive human visual\nperception has become possible. Consequently, various face forgery detection\ntechniques have been proposed to identify such fake facial content. However,\nevaluating the effectiveness and generalizability of these detection techniques\nremains a significant challenge. To address this, we have constructed a\nlarge-scale evaluation benchmark called DeepFaceGen, aimed at quantitatively\nassessing the effectiveness of face forgery detection and facilitating the\niterative development of forgery detection technology. DeepFaceGen consists of\n776,990 real face image/video samples and 773,812 face forgery image/video\nsamples, generated using 34 mainstream face generation techniques. During the\nconstruction process, we carefully consider important factors such as content\ndiversity, fairness across ethnicities, and availability of comprehensive\nlabels, in order to ensure the versatility and convenience of DeepFaceGen.\nSubsequently, DeepFaceGen is employed in this study to evaluate and analyze the\nperformance of 13 mainstream face forgery detection techniques from various\nperspectives. Through extensive experimental analysis, we derive significant\nfindings and propose potential directions for future research. The code and\ndataset for DeepFaceGen are available at\nhttps://github.com/HengruiLou/DeepFaceGen.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This is a paper about constructing a large-scale universal evaluation\n  benchmark for face forgery detection.The full text is 30 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.09181v2",
    "published_date": "2024-06-13 14:42:59 UTC",
    "updated_date": "2024-06-14 02:17:04 UTC"
  },
  {
    "arxiv_id": "2406.09166v3",
    "title": "Fine-Grained Domain Generalization with Feature Structuralization",
    "authors": [
      "Wenlong Yu",
      "Dongyue Chen",
      "Qilong Wang",
      "Qinghua Hu"
    ],
    "abstract": "Fine-grained domain generalization (FGDG) is a more challenging task than\ntraditional DG tasks due to its small inter-class variations and relatively\nlarge intra-class disparities. When domain distribution changes, the\nvulnerability of subtle features leads to a severe deterioration in model\nperformance. Nevertheless, humans inherently demonstrate the capacity for\ngeneralizing to out-of-distribution data, leveraging structured\nmulti-granularity knowledge that emerges from discerning the commonality and\nspecificity within categories. Likewise, we propose a Feature Structuralized\nDomain Generalization (FSDG) model, wherein features experience\nstructuralization into common, specific, and confounding segments, harmoniously\naligned with their relevant semantic concepts, to elevate performance in FGDG.\nSpecifically, feature structuralization (FS) is accomplished through joint\noptimization of five constraints: a decorrelation function applied to\ndisentangled segments, three constraints ensuring common feature consistency\nand specific feature distinctiveness, and a prediction calibration term. By\nimposing these stipulations, FSDG is prompted to disentangle and align features\nbased on multi-granularity knowledge, facilitating robust subtle distinctions\namong categories. Extensive experimentation on three benchmarks consistently\nvalidates the superiority of FSDG over state-of-the-art counterparts, with an\naverage improvement of 6.2% in FGDG performance. Beyond that, the\nexplainability analysis on explicit concept matching intensity between the\nshared concepts among categories and the model channels, along with experiments\non various mainstream model architectures, substantiates the validity of FS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09166v3",
    "published_date": "2024-06-13 14:27:53 UTC",
    "updated_date": "2025-03-26 07:15:23 UTC"
  },
  {
    "arxiv_id": "2406.09159v1",
    "title": "ALPHAGMUT: A Rationale-Guided Alpha Shape Graph Neural Network to Evaluate Mutation Effects",
    "authors": [
      "Boshen Wang",
      "Bowei Ye",
      "Lin Xu",
      "Jie Liang"
    ],
    "abstract": "In silico methods evaluating the mutation effects of missense mutations are\nproviding an important approach for understanding mutations in personal genomes\nand identifying disease-relevant biomarkers. However, existing methods,\nincluding deep learning methods, heavily rely on sequence-aware information,\nand do not fully leverage the potential of available 3D structural information.\nIn addition, these methods may exhibit an inability to predict mutations in\ndomains difficult to formulate sequence-based embeddings. In this study, we\nintroduce a novel rationale-guided graph neural network AlphaGMut to evaluate\nmutation effects and to distinguish pathogenic mutations from neutral\nmutations. We compute the alpha shapes of protein structures to obtain\natomic-resolution edge connectivities and map them to an accurate residue-level\ngraph representation. We then compute structural-, topological-, biophysical-,\nand sequence properties of the mutation sites, which are assigned as node\nattributes in the graph. These node attributes could effectively guide the\ngraph neural network to learn the difference between pathogenic and neutral\nmutations using k-hop message passing with a short training period. We\ndemonstrate that AlphaGMut outperforms state-of-the-art methods, including\nDeepMind's AlphaMissense, in many performance metrics. In addition, AlphaGMut\nhas the advantage of performing well in alignment-free settings, which provides\nbroader prediction coverage and better generalization compared to current\nmethods requiring deep sequence-aware information.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CG",
      "q-bio.GN"
    ],
    "primary_category": "q-bio.QM",
    "comment": "2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.09159v1",
    "published_date": "2024-06-13 14:22:12 UTC",
    "updated_date": "2024-06-13 14:22:12 UTC"
  },
  {
    "arxiv_id": "2406.09155v1",
    "title": "DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation",
    "authors": [
      "A B M Ashikur Rahman",
      "Saeed Anwar",
      "Muhammad Usman",
      "Ajmal Mian"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nrevolutionizing the integration of AI in daily life applications. However, they\nare prone to hallucinations, generating claims that contradict established\nfacts, deviating from prompts, and producing inconsistent responses when the\nsame prompt is presented multiple times. Addressing these issues is challenging\ndue to the lack of comprehensive and easily assessable benchmark datasets. Most\nexisting datasets are small and rely on multiple-choice questions, which are\ninadequate for evaluating the generative prowess of LLMs. To measure\nhallucination in LLMs, this paper introduces a comprehensive benchmark dataset\ncomprising over 75,000 prompts across eight domains. These prompts are designed\nto elicit definitive, concise, and informative answers. The dataset is divided\ninto two segments: one publicly available for testing and assessing LLM\nperformance and a hidden segment for benchmarking various LLMs. In our\nexperiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and\nZephyr-revealing that overall factual hallucination ranges from 59% to 82% on\nthe public dataset and 57% to 76% in the hidden benchmark. Prompt misalignment\nhallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the\nhidden counterpart. Average consistency ranges from 21% to 61% and 22% to 63%,\nrespectively. Domain-wise analysis shows that LLM performance significantly\ndeteriorates when asked for specific numeric information while performing\nmoderately with person, location, and date queries. Our dataset demonstrates\nits efficacy and serves as a comprehensive benchmark for LLM performance\nevaluation. Our dataset and LLMs responses are available at\n\\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09155v1",
    "published_date": "2024-06-13 14:18:13 UTC",
    "updated_date": "2024-06-13 14:18:13 UTC"
  },
  {
    "arxiv_id": "2406.09143v2",
    "title": "Generative AI-based Prompt Evolution Engineering Design Optimization With Vision-Language Model",
    "authors": [
      "Melvin Wong",
      "Thiago Rios",
      "Stefan Menzel",
      "Yew Soon Ong"
    ],
    "abstract": "Engineering design optimization requires an efficient combination of a 3D\nshape representation, an optimization algorithm, and a design performance\nevaluation method, which is often computationally expensive. We present a\nprompt evolution design optimization (PEDO) framework contextualized in a\nvehicle design scenario that leverages a vision-language model for penalizing\nimpractical car designs synthesized by a generative model. The backbone of our\nframework is an evolutionary strategy coupled with an optimization objective\nfunction that comprises a physics-based solver and a vision-language model for\npractical or functional guidance in the generated car designs. In the prompt\nevolutionary search, the optimizer iteratively generates a population of text\nprompts, which embed user specifications on the aerodynamic performance and\nvisual preferences of the 3D car designs. Then, in addition to the\ncomputational fluid dynamics simulations, the pre-trained vision-language model\nis used to penalize impractical designs and, thus, foster the evolutionary\nalgorithm to seek more viable designs. Our investigations on a car design\noptimization problem show a wide spread of potential car designs generated at\nthe early phase of the search, which indicates a good diversity of designs in\nthe initial populations, and an increase of over 20\\% in the probability of\ngenerating practical designs compared to a baseline framework without using a\nvision-language model. Visual inspection of the designs against the performance\nresults demonstrates prompt evolution as a very promising paradigm for finding\nnovel designs with good optimization performance while providing ease of use in\nspecifying design specifications and preferences via a natural language\ninterface.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted and to be published in IEEE Congress on Evolutionary\n  Computation (CEC) 2024. Copyright 2024 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses",
    "pdf_url": "http://arxiv.org/pdf/2406.09143v2",
    "published_date": "2024-06-13 14:11:19 UTC",
    "updated_date": "2024-06-14 08:33:11 UTC"
  },
  {
    "arxiv_id": "2406.09130v1",
    "title": "Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning",
    "authors": [
      "Haoxin Liu",
      "Harshavardhan Kamarthi",
      "Lingkai Kong",
      "Zhiyuan Zhao",
      "Chao Zhang",
      "B. Aditya Prakash"
    ],
    "abstract": "Time-series forecasting (TSF) finds broad applications in real-world\nscenarios. Due to the dynamic nature of time-series data, it is crucial to\nequip TSF models with out-of-distribution (OOD) generalization abilities, as\nhistorical training data and future test data can have different distributions.\nIn this paper, we aim to alleviate the inherent OOD problem in TSF via\ninvariant learning. We identify fundamental challenges of invariant learning\nfor TSF. First, the target variables in TSF may not be sufficiently determined\nby the input due to unobserved core variables in TSF, breaking the conventional\nassumption of invariant learning. Second, time-series datasets lack adequate\nenvironment labels, while existing environmental inference methods are not\nsuitable for TSF.\n  To address these challenges, we propose FOIL, a model-agnostic framework that\nenables timeseries Forecasting for Out-of-distribution generalization via\nInvariant Learning. FOIL employs a novel surrogate loss to mitigate the impact\nof unobserved variables. Further, FOIL implements a joint optimization by\nalternately inferring environments effectively with a multi-head network while\npreserving the temporal adjacency structure, and learning invariant\nrepresentations across inferred environments for OOD generalized TSF. We\ndemonstrate that the proposed FOIL significantly improves the performance of\nvarious TSF models, achieving gains of up to 85%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "H.0"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.09130v1",
    "published_date": "2024-06-13 14:01:34 UTC",
    "updated_date": "2024-06-13 14:01:34 UTC"
  },
  {
    "arxiv_id": "2406.09117v1",
    "title": "PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation",
    "authors": [
      "Injoon Hwang",
      "Haewon Park",
      "Youngwan Lee",
      "Jooyoung Yang",
      "SunJae Maeng"
    ],
    "abstract": "Low-rank adaption (LoRA) is a prominent method that adds a small number of\nlearnable parameters to the frozen pre-trained weights for parameter-efficient\nfine-tuning. Prompted by the question, ``Can we make its representation enough\nwith LoRA weights solely at the final phase of finetuning without the\npre-trained weights?'' In this work, we introduce Progressive Compression\nLoRA~(PC-LoRA), which utilizes low-rank adaptation (LoRA) to simultaneously\nperform model compression and fine-tuning. The PC-LoRA method gradually removes\nthe pre-trained weights during the training process, eventually leaving only\nthe low-rank adapters in the end. Thus, these low-rank adapters replace the\nwhole pre-trained weights, achieving the goals of compression and fine-tuning\nat the same time. Empirical analysis across various models demonstrates that\nPC-LoRA achieves parameter and FLOPs compression rates of 94.36%/89.1% for\nvision models, e.g., ViT-B, and 93.42%/84.2% parameters and FLOPs compressions\nfor language models, e.g., BERT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at T4V@CVPR",
    "pdf_url": "http://arxiv.org/pdf/2406.09117v1",
    "published_date": "2024-06-13 13:44:31 UTC",
    "updated_date": "2024-06-13 13:44:31 UTC"
  },
  {
    "arxiv_id": "2406.09112v1",
    "title": "Large-Scale Evaluation of Open-Set Image Classification Techniques",
    "authors": [
      "Halil Bisgin",
      "Andres Palechor",
      "Mike Suter",
      "Manuel Günther"
    ],
    "abstract": "The goal for classification is to correctly assign labels to unseen samples.\nHowever, most methods misclassify samples with unseen labels and assign them to\none of the known classes. Open-Set Classification (OSC) algorithms aim to\nmaximize both closed and open-set recognition capabilities. Recent studies\nshowed the utility of such algorithms on small-scale data sets, but limited\nexperimentation makes it difficult to assess their performances in real-world\nproblems. Here, we provide a comprehensive comparison of various OSC\nalgorithms, including training-based (SoftMax, Garbage, EOS) and\npost-processing methods (Maximum SoftMax Scores, Maximum Logit Scores, OpenMax,\nEVM, PROSER), the latter are applied on features from the former. We perform\nour evaluation on three large-scale protocols that mimic real-world challenges,\nwhere we train on known and negative open-set samples, and test on known and\nunknown instances. Our results show that EOS helps to improve performance of\nalmost all post-processing algorithms. Particularly, OpenMax and PROSER are\nable to exploit better-trained networks, demonstrating the utility of hybrid\nmodels. However, while most algorithms work well on negative test samples --\nsamples of open-set classes seen during training -- they tend to perform poorly\nwhen tested on samples of previously unseen unknown classes, especially in\nchallenging conditions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09112v1",
    "published_date": "2024-06-13 13:43:01 UTC",
    "updated_date": "2024-06-13 13:43:01 UTC"
  },
  {
    "arxiv_id": "2406.09105v1",
    "title": "INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance",
    "authors": [
      "Chenwei Lin",
      "Hanjia Lyu",
      "Xian Xu",
      "Jiebo Luo"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance in various general multimodal applications such as image\nrecognition and visual reasoning, and have also shown promising potential in\nspecialized domains. However, the application potential of LVLMs in the\ninsurance domain-characterized by rich application scenarios and abundant\nmultimodal data-has not been effectively explored. There is no systematic\nreview of multimodal tasks in the insurance domain, nor a benchmark\nspecifically designed to evaluate the capabilities of LVLMs in insurance. This\ngap hinders the development of LVLMs within the insurance domain. In this\npaper, we systematically review and distill multimodal tasks for four\nrepresentative types of insurance: auto insurance, property insurance, health\ninsurance, and agricultural insurance. We propose INS-MMBench, the first\ncomprehensive LVLMs benchmark tailored for the insurance domain. INS-MMBench\ncomprises a total of 2.2K thoroughly designed multiple-choice questions,\ncovering 12 meta-tasks and 22 fundamental tasks. Furthermore, we evaluate\nmultiple representative LVLMs, including closed-source models such as GPT-4o\nand open-source models like BLIP-2. This evaluation not only validates the\neffectiveness of our benchmark but also provides an in-depth performance\nanalysis of current LVLMs on various multimodal tasks in the insurance domain.\nWe hope that INS-MMBench will facilitate the further application of LVLMs in\nthe insurance domain and inspire interdisciplinary development. Our dataset and\nevaluation code are available at https://github.com/FDU-INS/INS-MMBench.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09105v1",
    "published_date": "2024-06-13 13:31:49 UTC",
    "updated_date": "2024-06-13 13:31:49 UTC"
  },
  {
    "arxiv_id": "2406.09087v2",
    "title": "Suitability of KANs for Computer Vision: A preliminary investigation",
    "authors": [
      "Basim Azam",
      "Naveed Akhtar"
    ],
    "abstract": "Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling\nthat implements learnable functions on the edges of the networks, diverging\nfrom the traditional node-centric activations in neural networks. This work\nassesses the applicability and efficacy of KANs in visual modeling, focusing on\nfundamental recognition and segmentation tasks. We mainly analyze the\nperformance and efficiency of different network architectures built using KAN\nconcepts along with conventional building blocks of convolutional and linear\nlayers, enabling a comparative analysis with the conventional models. Our\nfindings are aimed at contributing to understanding the potential of KANs in\ncomputer vision, highlighting both their strengths and areas for further\nresearch. Our evaluation point toward the fact that while KAN-based\narchitectures perform in line with the original claims, it may often be\nimportant to employ more complex functions on the network edges to retain the\nperformance advantage of KANs on more complex visual data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09087v2",
    "published_date": "2024-06-13 13:13:17 UTC",
    "updated_date": "2024-10-17 23:02:17 UTC"
  },
  {
    "arxiv_id": "2406.09082v1",
    "title": "Data-driven modeling and supervisory control system optimization for plug-in hybrid electric vehicles",
    "authors": [
      "Hao Zhang",
      "Nuo Lei",
      "Boli Chen",
      "Bingbing Li",
      "Rulong Li",
      "Zhi Wang"
    ],
    "abstract": "Learning-based intelligent energy management systems for plug-in hybrid\nelectric vehicles (PHEVs) are crucial for achieving efficient energy\nutilization. However, their application faces system reliability challenges in\nthe real world, which prevents widespread acceptance by original equipment\nmanufacturers (OEMs). This paper begins by establishing a PHEV model based on\nphysical and data-driven models, focusing on the high-fidelity training\nenvironment. It then proposes a real-vehicle application-oriented control\nframework, combining horizon-extended reinforcement learning (RL)-based energy\nmanagement with the equivalent consumption minimization strategy (ECMS) to\nenhance practical applicability, and improves the flawed method of equivalent\nfactor evaluation based on instantaneous driving cycle and powertrain states\nfound in existing research. Finally, comprehensive simulation and\nhardware-in-the-loop validation are carried out which demonstrates the\nadvantages of the proposed control framework in fuel economy over adaptive-ECMS\nand rule-based strategies. Compared to conventional RL architectures that\ndirectly control powertrain components, the proposed control method not only\nachieves similar optimality but also significantly enhances the disturbance\nresistance of the energy management system, providing an effective control\nframework for RL-based energy management strategies aimed at real-vehicle\napplications by OEMs.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09082v1",
    "published_date": "2024-06-13 13:04:42 UTC",
    "updated_date": "2024-06-13 13:04:42 UTC"
  },
  {
    "arxiv_id": "2406.09070v3",
    "title": "FairCoT: Enhancing Fairness in Text-to-Image Generation via Chain of Thought Reasoning with Multimodal Large Language Models",
    "authors": [
      "Zahraa Al Sahili",
      "Ioannis Patras",
      "Matthew Purver"
    ],
    "abstract": "In the domain of text-to-image generative models, biases inherent in training\ndatasets often propagate into generated content, posing significant ethical\nchallenges, particularly in socially sensitive contexts. We introduce FairCoT,\na novel framework that enhances fairness in text to image models through Chain\nof Thought (CoT) reasoning within multimodal generative large language models.\nFairCoT employs iterative CoT refinement to systematically mitigate biases, and\ndynamically adjusts textual prompts in real time, ensuring diverse and\nequitable representation in generated images. By integrating iterative\nreasoning processes, FairCoT addresses the limitations of zero shot CoT in\nsensitive scenarios, balancing creativity with ethical responsibility.\nExperimental evaluations across popular text-to-image systems including DALLE\nand various Stable Diffusion variants, demonstrate that FairCoT significantly\nenhances fairness and diversity without sacrificing image quality or semantic\nfidelity. By combining robust reasoning, lightweight deployment, and\nextensibility to multiple models, FairCoT represents a promising step toward\nmore socially responsible and transparent AI driven content generation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09070v3",
    "published_date": "2024-06-13 12:55:10 UTC",
    "updated_date": "2025-02-16 19:55:25 UTC"
  },
  {
    "arxiv_id": "2406.09068v3",
    "title": "Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation",
    "authors": [
      "Claude Formanek",
      "Callum Rhys Tilbury",
      "Louise Beyers",
      "Jonathan Shock",
      "Arnu Pretorius"
    ],
    "abstract": "Offline multi-agent reinforcement learning (MARL) is an emerging field with\ngreat promise for real-world applications. Unfortunately, the current state of\nresearch in offline MARL is plagued by inconsistencies in baselines and\nevaluation protocols, which ultimately makes it difficult to accurately assess\nprogress, trust newly proposed innovations, and allow researchers to easily\nbuild upon prior work. In this paper, we firstly identify significant\nshortcomings in existing methodologies for measuring the performance of novel\nalgorithms through a representative study of published offline MARL work.\nSecondly, by directly comparing to this prior work, we demonstrate that simple,\nwell-implemented baselines can achieve state-of-the-art (SOTA) results across a\nwide range of tasks. Specifically, we show that on 35 out of 47 datasets used\nin prior work (almost 75% of cases), we match or surpass the performance of the\ncurrent purported SOTA. Strikingly, our baselines often substantially\noutperform these more sophisticated algorithms. Finally, we correct for the\nshortcomings highlighted from this prior work by introducing a straightforward\nstandardised methodology for evaluation and by providing our baseline\nimplementations with statistically robust results across several scenarios,\nuseful for comparisons in future work. Our proposal includes simple and\nsensible steps that are easy to adopt, which in combination with solid\nbaselines and comparative results, could substantially improve the overall\nrigour of empirical science in offline MARL moving forward.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024) Track on Datasets and Benchmarks",
    "pdf_url": "http://arxiv.org/pdf/2406.09068v3",
    "published_date": "2024-06-13 12:54:29 UTC",
    "updated_date": "2024-10-30 12:08:43 UTC"
  },
  {
    "arxiv_id": "2406.09056v3",
    "title": "Towards Reliable Detection of LLM-Generated Texts: A Comprehensive Evaluation Framework with CUDRT",
    "authors": [
      "Zhen Tao",
      "Yanfang Chen",
      "Dinghao Xi",
      "Zhiyu Li",
      "Wei Xu"
    ],
    "abstract": "The increasing prevalence of large language models (LLMs) has significantly\nadvanced text generation, but the human-like quality of LLM outputs presents\nmajor challenges in reliably distinguishing between human-authored and\nLLM-generated texts. Existing detection benchmarks are constrained by their\nreliance on static datasets, scenario-specific tasks (e.g., question answering\nand text refinement), and a primary focus on English, overlooking the diverse\nlinguistic and operational subtleties of LLMs. To address these gaps, we\npropose CUDRT, a comprehensive evaluation framework and bilingual benchmark in\nChinese and English, categorizing LLM activities into five key operations:\nCreate, Update, Delete, Rewrite, and Translate. CUDRT provides extensive\ndatasets tailored to each operation, featuring outputs from state-of-the-art\nLLMs to assess the reliability of LLM-generated text detectors. This framework\nsupports scalable, reproducible experiments and enables in-depth analysis of\nhow operational diversity, multilingual training sets, and LLM architectures\ninfluence detection performance. Our extensive experiments demonstrate the\nframework's capacity to optimize detection systems, providing critical insights\nto enhance reliability, cross-linguistic adaptability, and detection accuracy.\nBy advancing robust methodologies for identifying LLM-generated texts, this\nwork contributes to the development of intelligent systems capable of meeting\nreal-world multilingual detection challenges. Source code and dataset are\navailable at GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "30 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.09056v3",
    "published_date": "2024-06-13 12:43:40 UTC",
    "updated_date": "2024-12-17 12:20:34 UTC"
  },
  {
    "arxiv_id": "2406.10297v1",
    "title": "SememeLM: A Sememe Knowledge Enhanced Method for Long-tail Relation Representation",
    "authors": [
      "Shuyi Li",
      "Shaojuan Wu",
      "Xiaowang Zhang",
      "Zhiyong Feng"
    ],
    "abstract": "Recognizing relations between two words is a fundamental task with the broad\napplications. Different from extracting relations from text, it is difficult to\nidentify relations among words without their contexts. Especially for long-tail\nrelations, it becomes more difficult due to inadequate semantic features.\nExisting approaches based on language models (LMs) utilize rich knowledge of\nLMs to enhance the semantic features of relations. However, they capture\nuncommon relations while overlooking less frequent but meaningful ones since\nknowledge of LMs seriously relies on trained data where often represents common\nrelations. On the other hand, long-tail relations are often uncommon in\ntraining data. It is interesting but not trivial to use external knowledge to\nenrich LMs due to collecting corpus containing long-tail relationships is\nhardly feasible. In this paper, we propose a sememe knowledge enhanced method\n(SememeLM) to enhance the representation of long-tail relations, in which\nsememes can break the contextual constraints between wors. Firstly, we present\na sememe relation graph and propose a graph encoding method. Moreover, since\nexternal knowledge base possibly consisting of massive irrelevant knowledge,\nthe noise is introduced. We propose a consistency alignment module, which\naligns the introduced knowledge with LMs, reduces the noise and integrates the\nknowledge into the language model. Finally, we conducted experiments on word\nanalogy datasets, which evaluates the ability to distinguish relation\nrepresentations subtle differences, including long-tail relations. Extensive\nexperiments show that our approach outperforms some state-of-the-art methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10297v1",
    "published_date": "2024-06-13 12:42:49 UTC",
    "updated_date": "2024-06-13 12:42:49 UTC"
  },
  {
    "arxiv_id": "2406.09043v3",
    "title": "Language Models are Crossword Solvers",
    "authors": [
      "Soumadeep Saha",
      "Sutanoya Chakraborty",
      "Saptarshi Saha",
      "Utpal Garain"
    ],
    "abstract": "Crosswords are a form of word puzzle that require a solver to demonstrate a\nhigh degree of proficiency in natural language understanding, wordplay,\nreasoning, and world knowledge, along with adherence to character and length\nconstraints. In this paper we tackle the challenge of solving crosswords with\nlarge language models (LLMs). We demonstrate that the current generation of\nlanguage models shows significant competence at deciphering cryptic crossword\nclues and outperforms previously reported state-of-the-art (SoTA) results by a\nfactor of 2-3 in relevant benchmarks. We also develop a search algorithm that\nbuilds off this performance to tackle the problem of solving full crossword\ngrids with out-of-the-box LLMs for the very first time, achieving an accuracy\nof 93% on New York Times crossword puzzles. Additionally, we demonstrate that\nLLMs generalize well and are capable of supporting answers with sound\nrationale.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 3 figures, 6 Appendix. Accepted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.09043v3",
    "published_date": "2024-06-13 12:29:27 UTC",
    "updated_date": "2025-02-09 14:26:45 UTC"
  },
  {
    "arxiv_id": "2406.09041v2",
    "title": "ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models",
    "authors": [
      "Jing Liu",
      "Ruihao Gong",
      "Mingyang Zhang",
      "Yefei He",
      "Jianfei Cai",
      "Bohan Zhuang"
    ],
    "abstract": "LLM development involves pre-training a foundation model on massive data,\nfollowed by fine-tuning on task-specific data to create specialized experts.\nServing these experts can pose significant memory challenges, as loading all\nexperts onto devices is impractical, and frequent switching between experts in\nresponse to user requests can incur substantial I/O costs. Previous approaches\ndecompose the expert weights as the pre-trained weights plus delta weights,\nfollowed by quantizing the delta weights using output channel-wise step sizes\nto reduce the model size. However, these methods overlook the fact that certain\ninput channels of delta weights can cause significant quantization errors at\nextremely low bitwidths. Additionally, existing methods assume that the\nappropriate model for a user request is known in advance, which is not the case\nin practice. To this end, we introduce ME-Switch, a memory-efficient expert\nswitching framework tailored for serving multiple LLMs. To condense the number\nof bits required for describing the delta weights, we propose a salient-aware\ndelta compression method that identifies salient input channels based on\nreconstruction error and applies mixed-precision quantization, reducing\nnon-salient channels to low bits while keeping salient ones intact, cutting\nstorage demand without compromising performance. Moreover, we develop a\nmodel-level routing method that efficiently directs user queries to the most\nsuitable expert by performing domain classification. Extensive experiments show\nthe promising memory efficiency and routing performance of ME-Switch. For\nexample, when serving three models from the Mistral-7B family, ME-Switch\nreduces the model size by $1.74\\times$ and maintains nearly lossless\nperformance on instruction, mathematical reasoning, and code generation tasks.\nNotably, our method can efficiently serve 16 Mistral-7B models on a single\nNVIDIA A100 GPU.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Tech report",
    "pdf_url": "http://arxiv.org/pdf/2406.09041v2",
    "published_date": "2024-06-13 12:27:55 UTC",
    "updated_date": "2024-10-26 15:55:49 UTC"
  },
  {
    "arxiv_id": "2406.09031v3",
    "title": "A Comprehensive Graph Pooling Benchmark: Effectiveness, Robustness and Generalizability",
    "authors": [
      "Pengyun Wang",
      "Junyu Luo",
      "Yanxin Shen",
      "Ming Zhang",
      "Siyu Heng",
      "Xiao Luo"
    ],
    "abstract": "Graph pooling has gained attention for its ability to obtain effective node\nand graph representations for various downstream tasks. Despite the recent\nsurge in graph pooling approaches, there is a lack of standardized experimental\nsettings and fair benchmarks to evaluate their performance. To address this\nissue, we have constructed a comprehensive benchmark that includes 17 graph\npooling methods and 28 different graph datasets. This benchmark systematically\nassesses the performance of graph pooling methods in three dimensions, i.e.,\neffectiveness, robustness, and generalizability. We first evaluate the\nperformance of these graph pooling approaches across different tasks including\ngraph classification, graph regression and node classification. Then, we\ninvestigate their performance under potential noise attacks and\nout-of-distribution shifts in real-world scenarios. We also involve detailed\nefficiency analysis, backbone analysis, parameter analysis and visualization to\nprovide more evidence. Extensive experiments validate the strong capability and\napplicability of graph pooling approaches in various scenarios, which can\nprovide valuable insights and guidance for deep geometric learning research.\nThe source code of our benchmark is available at\nhttps://github.com/goose315/Graph_Pooling_Benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09031v3",
    "published_date": "2024-06-13 12:04:40 UTC",
    "updated_date": "2024-10-02 14:24:50 UTC"
  },
  {
    "arxiv_id": "2406.09030v1",
    "title": "CUER: Corrected Uniform Experience Replay for Off-Policy Continuous Deep Reinforcement Learning Algorithms",
    "authors": [
      "Arda Sarp Yenicesu",
      "Furkan B. Mutlu",
      "Suleyman S. Kozat",
      "Ozgur S. Oguz"
    ],
    "abstract": "The utilization of the experience replay mechanism enables agents to\neffectively leverage their experiences on several occasions. In previous\nstudies, the sampling probability of the transitions was modified based on\ntheir relative significance. The process of reassigning sample probabilities\nfor every transition in the replay buffer after each iteration is considered\nextremely inefficient. Hence, in order to enhance computing efficiency,\nexperience replay prioritization algorithms reassess the importance of a\ntransition as it is sampled. However, the relative importance of the\ntransitions undergoes dynamic adjustments when the agent's policy and value\nfunction are iteratively updated. Furthermore, experience replay is a mechanism\nthat retains the transitions generated by the agent's past policies, which\ncould potentially diverge significantly from the agent's most recent policy. An\nincreased deviation from the agent's most recent policy results in a greater\nfrequency of off-policy updates, which has a negative impact on the agent's\nperformance. In this paper, we develop a novel algorithm, Corrected Uniform\nExperience Replay (CUER), which stochastically samples the stored experience\nwhile considering the fairness among all other experiences without ignoring the\ndynamic nature of the transition importance by making sampled state\ndistribution more on-policy. CUER provides promising improvements for\noff-policy continuous control algorithms in terms of sample efficiency, final\nperformance, and stability of the policy during the training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09030v1",
    "published_date": "2024-06-13 12:03:40 UTC",
    "updated_date": "2024-06-13 12:03:40 UTC"
  },
  {
    "arxiv_id": "2406.12914v1",
    "title": "The Significance of Latent Data Divergence in Predicting System Degradation",
    "authors": [
      "Miguel Fernandes",
      "Catarina Silva",
      "Alberto Cardoso",
      "Bernardete Ribeiro"
    ],
    "abstract": "Condition-Based Maintenance is pivotal in enabling the early detection of\npotential failures in engineering systems, where precise prediction of the\nRemaining Useful Life is essential for effective maintenance and operation.\nHowever, a predominant focus in the field centers on predicting the Remaining\nUseful Life using unprocessed or minimally processed data, frequently\nneglecting the intricate dynamics inherent in the dataset. In this work we\nintroduce a novel methodology grounded in the analysis of statistical\nsimilarity within latent data from system components. Leveraging a specifically\ndesigned architecture based on a Vector Quantized Variational Autoencoder, we\ncreate a sequence of discrete vectors which is used to estimate system-specific\npriors. We infer the similarity between systems by evaluating the divergence of\nthese priors, offering a nuanced understanding of individual system behaviors.\nThe efficacy of our approach is demonstrated through experiments on the NASA\ncommercial modular aero-propulsion system simulation (C-MAPSS) dataset. Our\nvalidation not only underscores the potential of our method in advancing the\nstudy of latent statistical divergence but also demonstrates its superiority\nover existing techniques.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.12914v1",
    "published_date": "2024-06-13 11:41:20 UTC",
    "updated_date": "2024-06-13 11:41:20 UTC"
  },
  {
    "arxiv_id": "2406.09014v6",
    "title": "Deep learning empowered sensor fusion boosts infant movement classification",
    "authors": [
      "Tomas Kulvicius",
      "Dajie Zhang",
      "Luise Poustka",
      "Sven Bölte",
      "Lennart Jahn",
      "Sarah Flügge",
      "Marc Kraft",
      "Markus Zweckstetter",
      "Karin Nielsen-Saines",
      "Florentin Wörgötter",
      "Peter B Marschik"
    ],
    "abstract": "To assess the integrity of the developing nervous system, the Prechtl general\nmovement assessment (GMA) is recognized for its clinical value in diagnosing\nneurological impairments in early infancy. GMA has been increasingly augmented\nthrough machine learning approaches intending to scale-up its application,\ncircumvent costs in the training of human assessors and further standardize\nclassification of spontaneous motor patterns. Available deep learning tools,\nall of which are based on single sensor modalities, are however still\nconsiderably inferior to that of well-trained human assessors. These approaches\nare hardly comparable as all models are designed, trained and evaluated on\nproprietary/silo-data sets. With this study we propose a sensor fusion approach\nfor assessing fidgety movements (FMs). FMs were recorded from 51 typically\ndeveloping participants. We compared three different sensor modalities\n(pressure, inertial, and visual sensors). Various combinations and two sensor\nfusion approaches (late and early fusion) for infant movement classification\nwere tested to evaluate whether a multi-sensor system outperforms single\nmodality assessments. Convolutional neural network (CNN) architectures were\nused to classify movement patterns. The performance of the three-sensor fusion\n(classification accuracy of 94.5%) was significantly higher than that of any\nsingle modality evaluated. We show that the sensor fusion approach is a\npromising avenue for automated classification of infant motor patterns. The\ndevelopment of a robust sensor fusion system may significantly enhance AI-based\nearly recognition of neurofunctions, ultimately facilitating automated early\ndetection of neurodevelopmental conditions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09014v6",
    "published_date": "2024-06-13 11:38:58 UTC",
    "updated_date": "2024-12-05 10:57:12 UTC"
  },
  {
    "arxiv_id": "2406.09009v4",
    "title": "Fredformer: Frequency Debiased Transformer for Time Series Forecasting",
    "authors": [
      "Xihao Piao",
      "Zheng Chen",
      "Taichi Murayama",
      "Yasuko Matsubara",
      "Yasushi Sakurai"
    ],
    "abstract": "The Transformer model has shown leading performance in time series\nforecasting. Nevertheless, in some complex scenarios, it tends to learn\nlow-frequency features in the data and overlook high-frequency features,\nshowing a frequency bias. This bias prevents the model from accurately\ncapturing important high-frequency data features. In this paper, we undertook\nempirical analyses to understand this bias and discovered that frequency bias\nresults from the model disproportionately focusing on frequency features with\nhigher energy. Based on our analysis, we formulate this bias and propose\nFredformer, a Transformer-based framework designed to mitigate frequency bias\nby learning features equally across different frequency bands. This approach\nprevents the model from overlooking lower amplitude features important for\naccurate forecasting. Extensive experiments show the effectiveness of our\nproposed approach, which can outperform other baselines in different real-world\ntime-series datasets. Furthermore, we introduce a lightweight variant of the\nFredformer with an attention matrix approximation, which achieves comparable\nperformance but with much fewer parameters and lower computation costs. The\ncode is available at: https://github.com/chenzRG/Fredformer",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted by SIGKDD2024",
    "pdf_url": "http://arxiv.org/pdf/2406.09009v4",
    "published_date": "2024-06-13 11:29:21 UTC",
    "updated_date": "2024-07-03 14:24:39 UTC"
  },
  {
    "arxiv_id": "2406.08996v1",
    "title": "Introducing Brain-like Concepts to Embodied Hand-crafted Dialog Management System",
    "authors": [
      "Frank Joublin",
      "Antonello Ceravola",
      "Cristian Sandu"
    ],
    "abstract": "Along with the development of chatbot, language models and speech\ntechnologies, there is a growing possibility and interest of creating systems\nable to interface with humans seamlessly through natural language or directly\nvia speech. In this paper, we want to demonstrate that placing the research on\ndialog system in the broader context of embodied intelligence allows to\nintroduce concepts taken from neurobiology and neuropsychology to define\nbehavior architecture that reconcile hand-crafted design and artificial neural\nnetwork and open the gate to future new learning approaches like imitation or\nlearning by instruction. To do so, this paper presents a neural behavior engine\nthat allows creation of mixed initiative dialog and action generation based on\nhand-crafted models using a graphical language. A demonstration of the\nusability of such brain-like inspired architecture together with a graphical\ndialog model is described through a virtual receptionist application running on\na semi-public space.",
    "categories": [
      "cs.AI",
      "I.2.1; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.08996v1",
    "published_date": "2024-06-13 10:54:03 UTC",
    "updated_date": "2024-06-13 10:54:03 UTC"
  },
  {
    "arxiv_id": "2406.08979v1",
    "title": "Multi-Agent Software Development through Cross-Team Collaboration",
    "authors": [
      "Zhuoyun Du",
      "Chen Qian",
      "Wei Liu",
      "Zihao Xie",
      "Yifei Wang",
      "Yufan Dang",
      "Weize Chen",
      "Cheng Yang"
    ],
    "abstract": "The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have\ncatalyzed profound transformations, particularly through multi-agent\ncollaboration for software development. LLM agents can collaborate in teams\nlike humans, and follow the waterfall model to sequentially work on\nrequirements analysis, development, review, testing, and other phases to\nperform autonomous software generation. However, for an agent team, each phase\nin a single development process yields only one possible outcome. This results\nin the completion of only one development chain, thereby losing the opportunity\nto explore multiple potential decision paths within the solution space.\nConsequently, this may lead to obtaining suboptimal results. To address this\nchallenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team\nframework that enables orchestrated teams to jointly propose various decisions\nand communicate with their insights in a cross-team collaboration environment\nfor superior content generation. Experimental results in software development\nreveal a notable increase in quality compared to state-of-the-art baselines,\nunderscoring the efficacy of our framework. The significant improvements in\nstory generation demonstrate the promising generalization ability of our\nframework across various domains. We anticipate that our work will guide LLM\nagents towards a cross-team paradigm and contribute to their significant growth\nin but not limited to software development. The code and data will be available\nat https://github.com/OpenBMB/ChatDev.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2406.08979v1",
    "published_date": "2024-06-13 10:18:36 UTC",
    "updated_date": "2024-06-13 10:18:36 UTC"
  },
  {
    "arxiv_id": "2407.09501v1",
    "title": "On when is Reservoir Computing with Cellular Automata Beneficial?",
    "authors": [
      "Tom Glover",
      "Evgeny Osipov",
      "Stefano Nichele"
    ],
    "abstract": "Reservoir Computing with Cellular Automata (ReCA) is a relatively novel and\npromising approach. It consists of 3 steps: an encoding scheme to inject the\nproblem into the CA, the CA iterations step itself and a simple classifying\nstep, typically a linear classifier. This paper demonstrates that the ReCA\nconcept is effective even in arguably the simplest implementation of a ReCA\nsystem. However, we also report a failed attempt on the UCR Time Series\nClassification Archive where ReCA seems to work, but only because of the\nencoding scheme itself, not in any part due to the CA. This highlights the need\nfor ablation testing, i.e., comparing internally with sub-parts of one model,\nbut also raises an open question on what kind of tasks ReCA is best suited for.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09501v1",
    "published_date": "2024-06-13 10:04:34 UTC",
    "updated_date": "2024-06-13 10:04:34 UTC"
  },
  {
    "arxiv_id": "2406.08973v3",
    "title": "XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning",
    "authors": [
      "Alexander Nikulin",
      "Ilya Zisman",
      "Alexey Zemtsov",
      "Vladislav Kurenkov"
    ],
    "abstract": "Following the success of the in-context learning paradigm in large-scale\nlanguage and computer vision models, the recently emerging field of in-context\nreinforcement learning is experiencing a rapid growth. However, its development\nhas been held back by the lack of challenging benchmarks, as all the\nexperiments have been carried out in simple environments and on small-scale\ndatasets. We present XLand-100B, a large-scale dataset for in-context\nreinforcement learning based on the XLand-MiniGrid environment, as a first step\nto alleviate this problem. It contains complete learning histories for nearly\n$30,000$ different tasks, covering $100$B transitions and 2.5B episodes. It\ntook 50,000 GPU hours to collect the dataset, which is beyond the reach of most\nacademic labs. Along with the dataset, we provide the utilities to reproduce or\nexpand it even further. We also benchmark common in-context RL baselines and\nshow that they struggle to generalize to novel and diverse tasks. With this\nsubstantial effort, we aim to democratize research in the rapidly growing field\nof in-context reinforcement learning and provide a solid foundation for further\nscaling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025, Poster, Source code:\n  https://github.com/dunnolab/xland-minigrid-datasets",
    "pdf_url": "http://arxiv.org/pdf/2406.08973v3",
    "published_date": "2024-06-13 10:04:17 UTC",
    "updated_date": "2025-03-01 09:36:02 UTC"
  },
  {
    "arxiv_id": "2406.09478v1",
    "title": "Distributed genetic algorithm for application placement in the compute continuum leveraging infrastructure nodes for optimization",
    "authors": [
      "Carlos Guerrero",
      "Isaac Lera",
      "Carlos Juiz"
    ],
    "abstract": "The increasing complexity of fog computing environments calls for efficient\nresource optimization techniques. In this paper, we propose and evaluate three\ndistributed designs of a genetic algorithm (GA) for resource optimization in\nfog computing, within an increasing degree of distribution. The designs\nleverage the execution of the GA in the fog devices themselves by dealing with\nthe specific features of this domain: constrained resources and widely\ngeographical distribution of the devices. For their evaluation, we implemented\na benchmark case using the NSGA-II for the specific problem of optimizing the\nfog service placement, according to the guidelines of our three distributed\ndesigns. These three experimental scenarios were compared with a control case,\na traditional centralized version of this GA algorithm, considering solution\nquality and network overhead. The results show that the design with the lowest\ndistribution degree, which keeps centralized storage of the objective space,\nachieves comparable solution quality to the traditional approach but incurs a\nhigher network load. The second design, which completely distributes the\npopulation between the workers, reduces network overhead but exhibits lower\nsolution diversity while keeping enough good results in terms of optimization\nobjective minimization. Finally, the proposal with a distributed population and\nthat only interchanges solution between the workers' neighbors achieves the\nlowest network load but with compromised solution quality.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09478v1",
    "published_date": "2024-06-13 09:58:21 UTC",
    "updated_date": "2024-06-13 09:58:21 UTC"
  },
  {
    "arxiv_id": "2406.09477v1",
    "title": "Q-S5: Towards Quantized State Space Models",
    "authors": [
      "Steven Abreu",
      "Jens E. Pedersen",
      "Kade M. Heckel",
      "Alessandro Pierro"
    ],
    "abstract": "In the quest for next-generation sequence modeling architectures, State Space\nModels (SSMs) have emerged as a potent alternative to transformers,\nparticularly for their computational efficiency and suitability for dynamical\nsystems. This paper investigates the effect of quantization on the S5 model to\nunderstand its impact on model performance and to facilitate its deployment to\nedge and resource-constrained platforms. Using quantization-aware training\n(QAT) and post-training quantization (PTQ), we systematically evaluate the\nquantization sensitivity of SSMs across different tasks like dynamical systems\nmodeling, Sequential MNIST (sMNIST) and most of the Long Range Arena (LRA). We\npresent fully quantized S5 models whose test accuracy drops less than 1% on\nsMNIST and most of the LRA. We find that performance on most tasks degrades\nsignificantly for recurrent weights below 8-bit precision, but that other\ncomponents can be compressed further without significant loss of performance.\nOur results further show that PTQ only performs well on language-based LRA\ntasks whereas all others require QAT. Our investigation provides necessary\ninsights for the continued development of efficient and hardware-optimized\nSSMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09477v1",
    "published_date": "2024-06-13 09:53:24 UTC",
    "updated_date": "2024-06-13 09:53:24 UTC"
  },
  {
    "arxiv_id": "2406.08966v2",
    "title": "Separation Power of Equivariant Neural Networks",
    "authors": [
      "Marco Pacini",
      "Xiaowen Dong",
      "Bruno Lepri",
      "Gabriele Santin"
    ],
    "abstract": "The separation power of a machine learning model refers to its ability to\ndistinguish between different inputs and is often used as a proxy for its\nexpressivity. Indeed, knowing the separation power of a family of models is a\nnecessary condition to obtain fine-grained universality results. In this paper,\nwe analyze the separation power of equivariant neural networks, such as\nconvolutional and permutation-invariant networks. We first present a complete\ncharacterization of inputs indistinguishable by models derived by a given\narchitecture. From this results, we derive how separability is influenced by\nhyperparameters and architectural choices-such as activation functions, depth,\nhidden layer width, and representation types. Notably, all non-polynomial\nactivations, including ReLU and sigmoid, are equivalent in expressivity and\nreach maximum separation power. Depth improves separation power up to a\nthreshold, after which further increases have no effect. Adding invariant\nfeatures to hidden representations does not impact separation power. Finally,\nblock decomposition of hidden representations affects separability, with\nminimal components forming a hierarchy in separation power that provides a\nstraightforward method for comparing the separation power of models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages of main text, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2406.08966v2",
    "published_date": "2024-06-13 09:52:44 UTC",
    "updated_date": "2024-12-10 13:03:40 UTC"
  },
  {
    "arxiv_id": "2406.12913v1",
    "title": "T-JEPA: A Joint-Embedding Predictive Architecture for Trajectory Similarity Computation",
    "authors": [
      "Lihuan Li",
      "Hao Xue",
      "Yang Song",
      "Flora Salim"
    ],
    "abstract": "Trajectory similarity computation is an essential technique for analyzing\nmoving patterns of spatial data across various applications such as traffic\nmanagement, wildlife tracking, and location-based services. Modern methods\noften apply deep learning techniques to approximate heuristic metrics but\nstruggle to learn more robust and generalized representations from the vast\namounts of unlabeled trajectory data. Recent approaches focus on\nself-supervised learning methods such as contrastive learning, which have made\nsignificant advancements in trajectory representation learning. However,\ncontrastive learning-based methods heavily depend on manually pre-defined data\naugmentation schemes, limiting the diversity of generated trajectories and\nresulting in learning from such variations in 2D Euclidean space, which\nprevents capturing high-level semantic variations. To address these\nlimitations, we propose T-JEPA, a self-supervised trajectory similarity\ncomputation method employing Joint-Embedding Predictive Architecture (JEPA) to\nenhance trajectory representation learning. T-JEPA samples and predicts\ntrajectory information in representation space, enabling the model to infer the\nmissing components of trajectories at high-level semantics without relying on\ndomain knowledge or manual effort. Extensive experiments conducted on three\nurban trajectory datasets and two Foursquare datasets demonstrate the\neffectiveness of T-JEPA in trajectory similarity computation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12913v1",
    "published_date": "2024-06-13 09:51:51 UTC",
    "updated_date": "2024-06-13 09:51:51 UTC"
  },
  {
    "arxiv_id": "2406.08959v3",
    "title": "Beyond Recommendations: From Backward to Forward AI Support of Pilots' Decision-Making Process",
    "authors": [
      "Zelun Tony Zhang",
      "Sebastian S. Feger",
      "Lucas Dullenkopf",
      "Rulu Liao",
      "Lukas Süsslin",
      "Yuanting Liu",
      "Andreas Butz"
    ],
    "abstract": "AI is anticipated to enhance human decision-making in high-stakes domains\nlike aviation, but adoption is often hindered by challenges such as\ninappropriate reliance and poor alignment with users' decision-making. Recent\nresearch suggests that a core underlying issue is the recommendation-centric\ndesign of many AI systems, i.e., they give end-to-end recommendations and\nignore the rest of the decision-making process. Alternative support paradigms\nare rare, and it remains unclear how the few that do exist compare to\nrecommendation-centric support. In this work, we aimed to empirically compare\nrecommendation-centric support to an alternative paradigm, continuous support,\nin the context of diversions in aviation. We conducted a mixed-methods study\nwith 32 professional pilots in a realistic setting. To ensure the quality of\nour study scenarios, we conducted a focus group with four additional pilots\nprior to the study. We found that continuous support can support pilots'\ndecision-making in a forward direction, allowing them to think more beyond the\nlimits of the system and make faster decisions when combined with\nrecommendations, though the forward support can be disrupted. Participants'\nstatements further suggest a shift in design goal away from providing\nrecommendations, to supporting quick information gathering. Our results show\nways to design more helpful and effective AI decision support that goes beyond\nend-to-end recommendations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to CSCW 2024, to be published in PACM HCI Vol. 8, No. CSCW2",
    "pdf_url": "http://arxiv.org/pdf/2406.08959v3",
    "published_date": "2024-06-13 09:44:04 UTC",
    "updated_date": "2024-09-20 12:23:26 UTC"
  },
  {
    "arxiv_id": "2406.11886v1",
    "title": "Financial Assets Dependency Prediction Utilizing Spatiotemporal Patterns",
    "authors": [
      "Haoren Zhu",
      "Pengfei Zhao",
      "Wilfred Siu Hung NG",
      "Dik Lun Lee"
    ],
    "abstract": "Financial assets exhibit complex dependency structures, which are crucial for\ninvestors to create diversified portfolios to mitigate risk in volatile\nfinancial markets. To explore the financial asset dependencies dynamics, we\npropose a novel approach that models the dependencies of assets as an Asset\nDependency Matrix (ADM) and treats the ADM sequences as image sequences. This\nallows us to leverage deep learning-based video prediction methods to capture\nthe spatiotemporal dependencies among assets. However, unlike images where\nneighboring pixels exhibit explicit spatiotemporal dependencies due to the\nnatural continuity of object movements, assets in ADM do not have a natural\norder. This poses challenges to organizing the relational assets to reveal\nbetter the spatiotemporal dependencies among neighboring assets for ADM\nforecasting. To tackle the challenges, we propose the Asset Dependency Neural\nNetwork (ADNN), which employs the Convolutional Long Short-Term Memory\n(ConvLSTM) network, a highly successful method for video prediction. ADNN can\nemploy static and dynamic transformation functions to optimize the\nrepresentations of the ADM. Through extensive experiments, we demonstrate that\nour proposed framework consistently outperforms the baselines in the ADM\nprediction and downstream application tasks. This research contributes to\nunderstanding and predicting asset dependencies, offering valuable insights for\nfinancial market participants.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "q-fin.CP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11886v1",
    "published_date": "2024-06-13 09:42:28 UTC",
    "updated_date": "2024-06-13 09:42:28 UTC"
  },
  {
    "arxiv_id": "2406.08957v1",
    "title": "Tool Wear Prediction in CNC Turning Operations using Ultrasonic Microphone Arrays and CNNs",
    "authors": [
      "Jan Steckel",
      "Arne Aerts",
      "Erik Verreycken",
      "Dennis Laurijssen",
      "Walter Daems"
    ],
    "abstract": "This paper introduces a novel method for predicting tool wear in CNC turning\noperations, combining ultrasonic microphone arrays and convolutional neural\nnetworks (CNNs). High-frequency acoustic emissions between 0 kHz and 60 kHz are\nenhanced using beamforming techniques to improve the signal- to-noise ratio.\nThe processed acoustic data is then analyzed by a CNN, which predicts the\nRemaining Useful Life (RUL) of cutting tools. Trained on data from 350\nworkpieces machined with a single carbide insert, the model can accurately\npredict the RUL of the carbide insert. Our results demonstrate the potential\ngained by integrating advanced ultrasonic sensors with deep learning for\naccurate predictive maintenance tasks in CNC machining.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08957v1",
    "published_date": "2024-06-13 09:36:13 UTC",
    "updated_date": "2024-06-13 09:36:13 UTC"
  },
  {
    "arxiv_id": "2406.10296v2",
    "title": "CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer",
    "authors": [
      "Heeseok Jung",
      "Jaesang Yoo",
      "Yohaan Yoon",
      "Yeonju Jang"
    ],
    "abstract": "Knowledge tracing (KT), wherein students' problem-solving histories are used\nto estimate their current levels of knowledge, has attracted significant\ninterest from researchers. However, most existing KT models were developed with\nan ID-based paradigm, which exhibits limitations in cold-start performance.\nThese limitations can be mitigated by leveraging the vast quantities of\nexternal knowledge possessed by generative large language models (LLMs). In\nthis study, we propose cold-start mitigation in knowledge tracing by aligning a\ngenerative language model as a students' knowledge tracer (CLST) as a framework\nthat utilizes a generative LLM as a knowledge tracer. Upon collecting data from\nmath, social studies, and science subjects, we framed the KT task as a natural\nlanguage processing task, wherein problem-solving data are expressed in natural\nlanguage, and fine-tuned the generative LLM using the formatted KT dataset.\nSubsequently, we evaluated the performance of the CLST in situations of data\nscarcity using various baseline models for comparison. The results indicate\nthat the CLST significantly enhanced performance with a dataset of fewer than\n100 students in terms of prediction, reliability, and cross-domain\ngeneralization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10296v2",
    "published_date": "2024-06-13 09:21:43 UTC",
    "updated_date": "2024-06-18 00:53:50 UTC"
  },
  {
    "arxiv_id": "2406.08931v2",
    "title": "Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning",
    "authors": [
      "Arnav Goel",
      "Medha Hira",
      "Anubha Gupta"
    ],
    "abstract": "Advent of modern deep learning techniques has given rise to advancements in\nthe field of Speech Emotion Recognition (SER). However, most systems prevalent\nin the field fail to generalize to speakers not seen during training. This\nstudy focuses on handling challenges of multilingual SER, specifically on\nunseen speakers. We introduce CAMuLeNet, a novel architecture leveraging\nco-attention based fusion and multitask learning to address this problem.\nAdditionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0,\nand WavLM using 10-fold leave-speaker-out cross-validation on five existing\nmultilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and,\nrelease a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet\nshows an average improvement of approximately 8% over all benchmarks on unseen\nspeakers determined by our cross-validation strategy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, Accepted to INTERSPEECH 2024. The first two authors\n  contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2406.08931v2",
    "published_date": "2024-06-13 09:00:14 UTC",
    "updated_date": "2024-06-20 02:25:39 UTC"
  },
  {
    "arxiv_id": "2406.08930v1",
    "title": "Efficient Multi-View Fusion and Flexible Adaptation to View Missing in Cardiovascular System Signals",
    "authors": [
      "Qihan Hu",
      "Daomiao Wang",
      "Hong Wu",
      "Jian Liu",
      "Cuiwei Yang"
    ],
    "abstract": "The progression of deep learning and the widespread adoption of sensors have\nfacilitated automatic multi-view fusion (MVF) about the cardiovascular system\n(CVS) signals. However, prevalent MVF model architecture often amalgamates CVS\nsignals from the same temporal step but different views into a unified\nrepresentation, disregarding the asynchronous nature of cardiovascular events\nand the inherent heterogeneity across views, leading to catastrophic view\nconfusion. Efficient training strategies specifically tailored for MVF models\nto attain comprehensive representations need simultaneous consideration.\nCrucially, real-world data frequently arrives with incomplete views, an aspect\nrarely noticed by researchers. Thus, the View-Centric Transformer (VCT) and\nMultitask Masked Autoencoder (M2AE) are specifically designed to emphasize the\ncentrality of each view and harness unlabeled data to achieve superior fused\nrepresentations. Additionally, we systematically define the missing-view\nproblem for the first time and introduce prompt techniques to aid pretrained\nMVF models in flexibly adapting to various missing-view scenarios. Rigorous\nexperiments involving atrial fibrillation detection, blood pressure estimation,\nand sleep staging-typical health monitoring tasks-demonstrate the remarkable\nadvantage of our method in MVF compared to prevailing methodologies. Notably,\nthe prompt technique requires finetuning less than 3% of the entire model's\ndata, substantially fortifying the model's resilience to view missing while\ncircumventing the need for complete retraining. The results demonstrate the\neffectiveness of our approaches, highlighting their potential for practical\napplications in cardiovascular health monitoring. Codes and models are released\nat URL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages,12 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.08930v1",
    "published_date": "2024-06-13 08:58:59 UTC",
    "updated_date": "2024-06-13 08:58:59 UTC"
  },
  {
    "arxiv_id": "2406.08929v2",
    "title": "Step-by-Step Diffusion: An Elementary Tutorial",
    "authors": [
      "Preetum Nakkiran",
      "Arwen Bradley",
      "Hattie Zhou",
      "Madhu Advani"
    ],
    "abstract": "We present an accessible first course on diffusion models and flow matching\nfor machine learning, aimed at a technical audience with no diffusion\nexperience. We try to simplify the mathematical details as much as possible\n(sometimes heuristically), while retaining enough precision to derive correct\nalgorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.08929v2",
    "published_date": "2024-06-13 08:58:45 UTC",
    "updated_date": "2024-06-23 23:18:07 UTC"
  },
  {
    "arxiv_id": "2406.08922v1",
    "title": "Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors",
    "authors": [
      "Ying Zhou",
      "Ben He",
      "Le Sun"
    ],
    "abstract": "With the launch of ChatGPT, large language models (LLMs) have attracted\nglobal attention. In the realm of article writing, LLMs have witnessed\nextensive utilization, giving rise to concerns related to intellectual property\nprotection, personal privacy, and academic integrity. In response, AI-text\ndetection has emerged to distinguish between human and machine-generated\ncontent. However, recent research indicates that these detection systems often\nlack robustness and struggle to effectively differentiate perturbed texts.\nCurrently, there is a lack of systematic evaluations regarding detection\nperformance in real-world applications, and a comprehensive examination of\nperturbation techniques and detector robustness is also absent. To bridge this\ngap, our work simulates real-world scenarios in both informal and professional\nwriting, exploring the out-of-the-box performance of current detectors.\nAdditionally, we have constructed 12 black-box text perturbation methods to\nassess the robustness of current detection models across various perturbation\ngranularities. Furthermore, through adversarial learning experiments, we\ninvestigate the impact of perturbation data augmentation on the robustness of\nAI-text detectors. We have released our code and data at\nhttps://github.com/zhouying20/ai-text-detector-evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024, Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2406.08922v1",
    "published_date": "2024-06-13 08:37:01 UTC",
    "updated_date": "2024-06-13 08:37:01 UTC"
  },
  {
    "arxiv_id": "2406.08920v3",
    "title": "AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis",
    "authors": [
      "Swapnil Bhosale",
      "Haosen Yang",
      "Diptesh Kanojia",
      "Jiankang Deng",
      "Xiatian Zhu"
    ],
    "abstract": "Novel view acoustic synthesis (NVAS) aims to render binaural audio at any\ntarget viewpoint, given a mono audio emitted by a sound source at a 3D scene.\nExisting methods have proposed NeRF-based implicit models to exploit visual\ncues as a condition for synthesizing binaural audio. However, in addition to\nlow efficiency originating from heavy NeRF rendering, these methods all have a\nlimited ability of characterizing the entire scene environment such as room\ngeometry, material properties, and the spatial relation between the listener\nand sound source. To address these issues, we propose a novel Audio-Visual\nGaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware\ncondition for audio synthesis, we learn an explicit point-based scene\nrepresentation with an audio-guidance parameter on locally initialized Gaussian\npoints, taking into account the space relation from the listener and sound\nsource. To make the visual scene model audio adaptive, we propose a point\ndensification and pruning strategy to optimally distribute the Gaussian points,\nwith the per-point contribution in sound propagation (e.g., more points needed\nfor texture-less wall surfaces as they affect sound path diversion). Extensive\nexperiments validate the superiority of our AV-GS over existing alternatives on\nthe real-world RWAS and simulation-based SoundSpaces datasets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.08920v3",
    "published_date": "2024-06-13 08:34:12 UTC",
    "updated_date": "2025-03-16 19:43:03 UTC"
  },
  {
    "arxiv_id": "2406.08918v3",
    "title": "Beyond the Calibration Point: Mechanism Comparison in Differential Privacy",
    "authors": [
      "Georgios Kaissis",
      "Stefan Kolek",
      "Borja Balle",
      "Jamie Hayes",
      "Daniel Rueckert"
    ],
    "abstract": "In differentially private (DP) machine learning, the privacy guarantees of DP\nmechanisms are often reported and compared on the basis of a single\n$(\\varepsilon, \\delta)$-pair. This practice overlooks that DP guarantees can\nvary substantially even between mechanisms sharing a given $(\\varepsilon,\n\\delta)$, and potentially introduces privacy vulnerabilities which can remain\nundetected. This motivates the need for robust, rigorous methods for comparing\nDP guarantees in such cases. Here, we introduce the $\\Delta$-divergence between\nmechanisms which quantifies the worst-case excess privacy vulnerability of\nchoosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP\nand in terms of a newly presented Bayesian interpretation. Moreover, as a\ngeneralisation of the Blackwell theorem, it is endowed with strong\ndecision-theoretic foundations. Through application examples, we show that our\ntechniques can facilitate informed decision-making and reveal gaps in the\ncurrent understanding of privacy risks, as current practices in DP-SGD often\nresult in choosing mechanisms with high excess privacy vulnerabilities.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.CR",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.08918v3",
    "published_date": "2024-06-13 08:30:29 UTC",
    "updated_date": "2025-05-04 16:19:03 UTC"
  },
  {
    "arxiv_id": "2406.12911v1",
    "title": "The Promise of Analog Deep Learning: Recent Advances, Challenges and Opportunities",
    "authors": [
      "Aditya Datar",
      "Pramit Saha"
    ],
    "abstract": "Much of the present-day Artificial Intelligence (AI) utilizes artificial\nneural networks, which are sophisticated computational models designed to\nrecognize patterns and solve complex problems by learning from data. However, a\nmajor bottleneck occurs during a device's calculation of weighted sums for\nforward propagation and optimization procedure for backpropagation, especially\nfor deep neural networks, or networks with numerous layers. Exploration into\ndifferent methods of implementing neural networks is necessary for further\nadvancement of the area. While a great deal of research into AI hardware in\nboth directions, analog and digital implementation widely exists, much of the\nexisting survey works lacks discussion on the progress of analog deep learning.\nTo this end, we attempt to evaluate and specify the advantages and\ndisadvantages, along with the current progress with regards to deep learning,\nfor analog implementations. In this paper, our focus lies on the comprehensive\nexamination of eight distinct analog deep learning methodologies across\nmultiple key parameters. These parameters include attained accuracy levels,\napplication domains, algorithmic advancements, computational speed, and\nconsiderations of energy efficiency and power consumption. We also identify the\nneural network-based experiments implemented using these hardware devices and\ndiscuss comparative performance achieved by the different analog deep learning\nmethods along with an analysis of their current limitations. Overall, we find\nthat Analog Deep Learning has great potential for future consumer-level\napplications, but there is still a long road ahead in terms of scalability.\nMost of the current implementations are more proof of concept and are not yet\npractically deployable for large-scale models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12911v1",
    "published_date": "2024-06-13 07:52:33 UTC",
    "updated_date": "2024-06-13 07:52:33 UTC"
  },
  {
    "arxiv_id": "2406.08898v1",
    "title": "Computer Vision Approaches for Automated Bee Counting Application",
    "authors": [
      "Simon Bilik",
      "Ilona Janakova",
      "Adam Ligocki",
      "Dominik Ficek",
      "Karel Horak"
    ],
    "abstract": "Many application from the bee colony health state monitoring could be\nefficiently solved using a computer vision techniques. One of such challenges\nis an efficient way for counting the number of incoming and outcoming bees,\nwhich could be used to further analyse many trends, such as the bee colony\nhealth state, blooming periods, or for investigating the effects of\nagricultural spraying. In this paper, we compare three methods for the\nautomated bee counting over two own datasets. The best performing method is\nbased on the ResNet-50 convolutional neural network classifier, which achieved\naccuracy of 87% over the BUT1 dataset and the accuracy of 93% over the BUT2\ndataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08898v1",
    "published_date": "2024-06-13 07:51:08 UTC",
    "updated_date": "2024-06-13 07:51:08 UTC"
  },
  {
    "arxiv_id": "2406.08887v2",
    "title": "Low-Overhead Channel Estimation via 3D Extrapolation for TDD mmWave Massive MIMO Systems Under High-Mobility Scenarios",
    "authors": [
      "Binggui Zhou",
      "Xi Yang",
      "Shaodan Ma",
      "Feifei Gao",
      "Guanghua Yang"
    ],
    "abstract": "In time division duplexing (TDD) millimeter wave (mmWave) massive\nmultiple-input multiple-output (MIMO) systems, downlink channel state\ninformation (CSI) can be obtained from uplink channel estimation thanks to\nchannel reciprocity. However, under high-mobility scenarios, frequent uplink\nchannel estimation is needed due to channel aging. Additionally, large amounts\nof antennas and subcarriers result in high-dimensional CSI matrices,\naggravating pilot training overhead. To address this, we propose a three-domain\n(3D) channel extrapolation framework across spatial, frequency, and temporal\ndomains. First, considering the effectiveness of traditional knowledge-driven\nchannel estimation methods and the marginal effects of pilots in the spatial\nand frequency domains, a knowledge-and-data driven spatial-frequency channel\nextrapolation network (KDD-SFCEN) is proposed for uplink channel estimation via\njoint spatial-frequency channel extrapolation to reduce spatial-frequency\ndomain pilot overhead. Then, leveraging channel reciprocity and temporal\ndependencies, we propose a temporal uplink-downlink channel extrapolation\nnetwork (TUDCEN) powered by generative artificial intelligence for slot-level\nchannel extrapolation, aiming to reduce the tremendous temporal domain pilot\noverhead caused by high mobility. Numerical results demonstrate the superiority\nof the proposed framework in significantly reducing the pilot training overhead\nby 16 times and improving the system's spectral efficiency under high-mobility\nscenarios compared with state-of-the-art channel estimation/extrapolation\nmethods.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "eess.SP",
    "comment": "17 pages, 11 figures, 3 tables. Accepted by IEEE Transactions on\n  Wireless Communications",
    "pdf_url": "http://arxiv.org/pdf/2406.08887v2",
    "published_date": "2024-06-13 07:42:25 UTC",
    "updated_date": "2024-12-29 16:54:13 UTC"
  },
  {
    "arxiv_id": "2406.08877v2",
    "title": "EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding",
    "authors": [
      "Yuan-Ming Li",
      "Wei-Jin Huang",
      "An-Lan Wang",
      "Ling-An Zeng",
      "Jing-Ke Meng",
      "Wei-Shi Zheng"
    ],
    "abstract": "We present EgoExo-Fitness, a new full-body action understanding dataset,\nfeaturing fitness sequence videos recorded from synchronized egocentric and\nfixed exocentric (third-person) cameras. Compared with existing full-body\naction understanding datasets, EgoExo-Fitness not only contains videos from\nfirst-person perspectives, but also provides rich annotations. Specifically,\ntwo-level temporal boundaries are provided to localize single action videos\nalong with sub-steps of each action. More importantly, EgoExo-Fitness\nintroduces innovative annotations for interpretable action judgement--including\ntechnical keypoint verification, natural language comments on action execution,\nand action quality scores. Combining all of these, EgoExo-Fitness provides new\nresources to study egocentric and exocentric full-body action understanding\nacross dimensions of \"what\", \"when\", and \"how well\". To facilitate research on\negocentric and exocentric full-body action understanding, we construct\nbenchmarks on a suite of tasks (i.e., action classification, action\nlocalization, cross-view sequence verification, cross-view skill determination,\nand a newly proposed task of guidance-based execution verification), together\nwith detailed analysis. Code and data will be available at\nhttps://github.com/iSEE-Laboratory/EgoExo-Fitness/tree/main.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2406.08877v2",
    "published_date": "2024-06-13 07:28:45 UTC",
    "updated_date": "2024-07-16 09:35:49 UTC"
  },
  {
    "arxiv_id": "2406.11884v1",
    "title": "Hierarchical Compression of Text-Rich Graphs via Large Language Models",
    "authors": [
      "Shichang Zhang",
      "Da Zheng",
      "Jiani Zhang",
      "Qi Zhu",
      "Xiang song",
      "Soji Adeshina",
      "Christos Faloutsos",
      "George Karypis",
      "Yizhou Sun"
    ],
    "abstract": "Text-rich graphs, prevalent in data mining contexts like e-commerce and\nacademic graphs, consist of nodes with textual features linked by various\nrelations. Traditional graph machine learning models, such as Graph Neural\nNetworks (GNNs), excel in encoding the graph structural information, but have\nlimited capability in handling rich text on graph nodes. Large Language Models\n(LLMs), noted for their superior text understanding abilities, offer a solution\nfor processing the text in graphs but face integration challenges due to their\nlimitation for encoding graph structures and their computational complexities\nwhen dealing with extensive text in large neighborhoods of interconnected\nnodes. This paper introduces ``Hierarchical Compression'' (HiCom), a novel\nmethod to align the capabilities of LLMs with the structure of text-rich\ngraphs. HiCom processes text in a node's neighborhood in a structured manner by\norganizing the extensive textual information into a more manageable hierarchy\nand compressing node text step by step. Therefore, HiCom not only preserves the\ncontextual richness of the text but also addresses the computational challenges\nof LLMs, which presents an advancement in integrating the text processing power\nof LLMs with the structural complexities of text-rich graphs. Empirical results\nshow that HiCom can outperform both GNNs and LLM backbones for node\nclassification on e-commerce and citation graphs. HiCom is especially effective\nfor nodes from a dense region in a graph, where it achieves a 3.48% average\nperformance improvement on five datasets while being more efficient than LLM\nbackbones.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11884v1",
    "published_date": "2024-06-13 07:24:46 UTC",
    "updated_date": "2024-06-13 07:24:46 UTC"
  },
  {
    "arxiv_id": "2406.08866v1",
    "title": "Zoom and Shift are All You Need",
    "authors": [
      "Jiahao Qin"
    ],
    "abstract": "Feature alignment serves as the primary mechanism for fusing multimodal data.\nWe put forth a feature alignment approach that achieves full integration of\nmultimodal information. This is accomplished via an alternating process of\nshifting and expanding feature representations across modalities to obtain a\nconsistent unified representation in a joint feature space. The proposed\ntechnique can reliably capture high-level interplay between features\noriginating from distinct modalities. Consequently, substantial gains in\nmultimodal learning performance are attained. Additionally, we demonstrate the\nsuperiority of our approach over other prevalent multimodal fusion schemes on a\nrange of tasks. Extensive experimental evaluation conducted on multimodal\ndatasets comprising time series, image, and text demonstrates that our method\nachieves state-of-the-art results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.08866v1",
    "published_date": "2024-06-13 07:09:41 UTC",
    "updated_date": "2024-06-13 07:09:41 UTC"
  },
  {
    "arxiv_id": "2406.08864v1",
    "title": "Research on Early Warning Model of Cardiovascular Disease Based on Computer Deep Learning",
    "authors": [
      "Yuxiang Hu",
      "Jinxin Hu",
      "Ting Xu",
      "Bo Zhang",
      "Jiajie Yuan",
      "Haozhang Deng"
    ],
    "abstract": "This project intends to study a cardiovascular disease risk early warning\nmodel based on one-dimensional convolutional neural networks. First, the\nmissing values of 13 physiological and symptom indicators such as patient age,\nblood glucose, cholesterol, and chest pain were filled and Z-score was\nstandardized. The convolutional neural network is converted into a 2D matrix,\nthe convolution function of 1,3, and 5 is used for the first-order convolution\noperation, and the Max Pooling algorithm is adopted for dimension reduction.\nSet the learning rate and output rate. It is optimized by the Adam algorithm.\nThe result of classification is output by a soft classifier. This study was\nconducted based on Statlog in the UCI database and heart disease database\nrespectively. The empirical data indicate that the forecasting precision of\nthis technique has been enhanced by 11.2%, relative to conventional approaches,\nwhile there is a significant improvement in the logarithmic curve fitting. The\nefficacy and applicability of the novel approach are corroborated through the\nexamination employing a one-dimensional convolutional neural network.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.08864v1",
    "published_date": "2024-06-13 07:04:22 UTC",
    "updated_date": "2024-06-13 07:04:22 UTC"
  },
  {
    "arxiv_id": "2406.08863v2",
    "title": "Self-supervised Graph Neural Network for Mechanical CAD Retrieval",
    "authors": [
      "Yuhan Quan",
      "Huan Zhao",
      "Jinfeng Yi",
      "Yuqiang Chen"
    ],
    "abstract": "CAD (Computer-Aided Design) plays a crucial role in mechanical industry,\nwhere large numbers of similar-shaped CAD parts are often created. Efficiently\nreusing these parts is key to reducing design and production costs for\nenterprises. Retrieval systems are vital for achieving CAD reuse, but the\ncomplex shapes of CAD models are difficult to accurately describe using text or\nkeywords, making traditional retrieval methods ineffective. While existing\nrepresentation learning approaches have been developed for CAD, manually\nlabeling similar samples in these methods is expensive. Additionally, CAD\nmodels' unique parameterized data structure presents challenges for applying\nexisting 3D shape representation learning techniques directly. In this work, we\npropose GC-CAD, a self-supervised contrastive graph neural network-based method\nfor mechanical CAD retrieval that directly models parameterized CAD raw files.\nGC-CAD consists of two key modules: structure-aware representation learning and\ncontrastive graph learning framework. The method leverages graph neural\nnetworks to extract both geometric and topological information from CAD models,\ngenerating feature representations. We then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model to train\nwithout manual labels and generate retrieval-ready representations.\nExperimental results on four datasets including human evaluation demonstrate\nthat the proposed method achieves significant accuracy improvements and up to\n100 times efficiency improvement over the baseline methods.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08863v2",
    "published_date": "2024-06-13 06:56:49 UTC",
    "updated_date": "2024-06-18 03:29:12 UTC"
  },
  {
    "arxiv_id": "2406.08854v1",
    "title": "Current applications and potential future directions of reinforcement learning-based Digital Twins in agriculture",
    "authors": [
      "Georg Goldenits",
      "Kevin Mallinger",
      "Sebastian Raubitzek",
      "Thomas Neubauer"
    ],
    "abstract": "Digital Twins have gained attention in various industries for simulation,\nmonitoring, and decision-making, relying on ever-improving machine learning\nmodels. However, agricultural Digital Twin implementations are limited compared\nto other industries. Meanwhile, machine learning, particularly reinforcement\nlearning, has shown potential in agricultural applications like optimizing\ndecision-making, task automation, and resource management. A key aspect of\nDigital Twins is representing physical assets or systems in a virtual\nenvironment, which aligns well with reinforcement learning's need for\nenvironment representations to learn the best policy for a task. Reinforcement\nlearning in agriculture can thus enable various Digital Twin applications in\nagricultural domains. This review aims to categorize existing research\nemploying reinforcement learning in agricultural settings by application\ndomains like robotics, greenhouse management, irrigation systems, and crop\nmanagement, identifying potential future areas for reinforcement learning-based\nDigital Twins. It also categorizes the reinforcement learning techniques used,\nincluding tabular methods, Deep Q-Networks (DQN), Policy Gradient methods, and\nActor-Critic algorithms, to overview currently employed models. The review\nseeks to provide insights into the state-of-the-art in integrating Digital\nTwins and reinforcement learning in agriculture, identifying gaps and\nopportunities for future research, and exploring synergies to tackle\nagricultural challenges and optimize farming, paving the way for more efficient\nand sustainable farming methodologies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08854v1",
    "published_date": "2024-06-13 06:38:09 UTC",
    "updated_date": "2024-06-13 06:38:09 UTC"
  },
  {
    "arxiv_id": "2406.08848v1",
    "title": "An Approach to Build Zero-Shot Slot-Filling System for Industry-Grade Conversational Assistants",
    "authors": [
      "G P Shrivatsa Bhargav",
      "Sumit Neelam",
      "Udit Sharma",
      "Shajith Ikbal",
      "Dheeraj Sreedhar",
      "Hima Karanam",
      "Sachindra Joshi",
      "Pankaj Dhoolia",
      "Dinesh Garg",
      "Kyle Croutwater",
      "Haode Qi",
      "Eric Wayne",
      "J William Murdock"
    ],
    "abstract": "We present an approach to build Large Language Model (LLM) based slot-filling\nsystem to perform Dialogue State Tracking in conversational assistants serving\nacross a wide variety of industry-grade applications. Key requirements of this\nsystem include: 1) usage of smaller-sized models to meet low latency\nrequirements and to enable convenient and cost-effective cloud and customer\npremise deployments, and 2) zero-shot capabilities to serve across a wide\nvariety of domains, slot types and conversational scenarios. We adopt a\nfine-tuning approach where a pre-trained LLM is fine-tuned into a slot-filling\nmodel using task specific data. The fine-tuning data is prepared carefully to\ncover a wide variety of slot-filling task scenarios that the model is expected\nto face across various domains. We give details of the data preparation and\nmodel building process. We also give a detailed analysis of the results of our\nexperimental evaluations. Results show that our prescribed approach for\nslot-filling model building has resulted in 6.9% relative improvement of F1\nmetric over the best baseline on a realistic benchmark, while at the same time\nreducing the latency by 57%. More over, the data we prepared has helped improve\nF1 on an average by 4.2% relative across various slot-types.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08848v1",
    "published_date": "2024-06-13 06:24:52 UTC",
    "updated_date": "2024-06-13 06:24:52 UTC"
  },
  {
    "arxiv_id": "2406.08838v1",
    "title": "Research on Optimization of Natural Language Processing Model Based on Multimodal Deep Learning",
    "authors": [
      "Dan Sun",
      "Yaxin Liang",
      "Yining Yang",
      "Yuhan Ma",
      "Qishi Zhan",
      "Erdi Gao"
    ],
    "abstract": "This project intends to study the image representation based on attention\nmechanism and multimodal data. By adding multiple pattern layers to the\nattribute model, the semantic and hidden layers of image content are\nintegrated. The word vector is quantified by the Word2Vec method and then\nevaluated by a word embedding convolutional neural network. The published\nexperimental results of the two groups were tested. The experimental results\nshow that this method can convert discrete features into continuous characters,\nthus reducing the complexity of feature preprocessing. Word2Vec and natural\nlanguage processing technology are integrated to achieve the goal of direct\nevaluation of missing image features. The robustness of the image feature\nevaluation model is improved by using the excellent feature analysis\ncharacteristics of a convolutional neural network. This project intends to\nimprove the existing image feature identification methods and eliminate the\nsubjective influence in the evaluation process. The findings from the\nsimulation indicate that the novel approach has developed is viable,\neffectively augmenting the features within the produced representations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08838v1",
    "published_date": "2024-06-13 06:03:59 UTC",
    "updated_date": "2024-06-13 06:03:59 UTC"
  },
  {
    "arxiv_id": "2406.16929v1",
    "title": "Modelling the 5G Energy Consumption using Real-world Data: Energy Fingerprint is All You Need",
    "authors": [
      "Tingwei Chen",
      "Yantao Wang",
      "Hanzhi Chen",
      "Zijian Zhao",
      "Xinhao Li",
      "Nicola Piovesan",
      "Guangxu Zhu",
      "Qingjiang Shi"
    ],
    "abstract": "The introduction of fifth-generation (5G) radio technology has revolutionized\ncommunications, bringing unprecedented automation, capacity, connectivity, and\nultra-fast, reliable communications. However, this technological leap comes\nwith a substantial increase in energy consumption, presenting a significant\nchallenge. To improve the energy efficiency of 5G networks, it is imperative to\ndevelop sophisticated models that accurately reflect the influence of base\nstation (BS) attributes and operational conditions on energy usage.Importantly,\naddressing the complexity and interdependencies of these diverse features is\nparticularly challenging, both in terms of data processing and model\narchitecture design.\n  This paper proposes a novel 5G base stations energy consumption modelling\nmethod by learning from a real-world dataset used in the ITU 5G Base Station\nEnergy Consumption Modelling Challenge in which our model ranked second. Unlike\nexisting methods that omit the Base Station Identifier (BSID) information and\nthus fail to capture the unique energy fingerprint in different base stations,\nwe incorporate the BSID into the input features and encoding it with an\nembedding layer for precise representation. Additionally, we introduce a novel\nmasked training method alongside an attention mechanism to further boost the\nmodel's generalization capabilities and accuracy. After evaluation, our method\ndemonstrates significant improvements over existing models, reducing Mean\nAbsolute Percentage Error (MAPE) from 12.75% to 4.98%, leading to a performance\ngain of more than 60%.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16929v1",
    "published_date": "2024-06-13 06:02:15 UTC",
    "updated_date": "2024-06-13 06:02:15 UTC"
  },
  {
    "arxiv_id": "2406.08830v2",
    "title": "Center-Sensitive Kernel Optimization for Efficient On-Device Incremental Learning",
    "authors": [
      "Dingwen Zhang",
      "Yan Li",
      "De Cheng",
      "Nannan Wang",
      "Junwei Han"
    ],
    "abstract": "To facilitate the evolution of edge intelligence in ever-changing\nenvironments, we study on-device incremental learning constrained in limited\ncomputation resource in this paper. Current on-device training methods just\nfocus on efficient training without considering the catastrophic forgetting,\npreventing the model getting stronger when continually exploring the world. To\nsolve this problem, a direct solution is to involve the existing incremental\nlearning mechanisms into the on-device training framework. Unfortunately, such\na manner cannot work well as those mechanisms usually introduce large\nadditional computational cost to the network optimization process, which would\ninevitably exceed the memory capacity of the edge devices. To address this\nissue, this paper makes an early effort to propose a simple but effective\nedge-friendly incremental learning framework. Based on an empirical study on\nthe knowledge intensity of the kernel elements of the neural network, we find\nthat the center kernel is the key for maximizing the knowledge intensity for\nlearning new data, while freezing the other kernel elements would get a good\nbalance on the model's capacity for overcoming catastrophic forgetting. Upon\nthis finding, we further design a center-sensitive kernel optimization\nframework to largely alleviate the cost of the gradient computation and\nback-propagation. Besides, a dynamic channel element selection strategy is also\nproposed to facilitate a sparse orthogonal gradient projection for further\nreducing the optimization complexity, upon the knowledge explored from the new\ntask data. Extensive experiments validate our method is efficient and\neffective, e.g., our method achieves average accuracy boost of 38.08% with even\nless memory and approximate computation compared to existing on-device training\nmethods, indicating its significant potential for on-device incremental\nlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08830v2",
    "published_date": "2024-06-13 05:49:29 UTC",
    "updated_date": "2024-12-03 07:23:25 UTC"
  },
  {
    "arxiv_id": "2406.08828v1",
    "title": "Estimating Difficulty Levels of Programming Problems with Pre-trained Model",
    "authors": [
      "Zhiyuan Wang",
      "Wei Zhang",
      "Jun Wang"
    ],
    "abstract": "As the demand for programming skills grows across industries and academia,\nstudents often turn to Programming Online Judge (POJ) platforms for coding\npractice and competition. The difficulty level of each programming problem\nserves as an essential reference for guiding students' adaptive learning.\nHowever, current methods of determining difficulty levels either require\nextensive expert annotations or take a long time to accumulate enough student\nsolutions for each problem. To address this issue, we formulate the problem of\nautomatic difficulty level estimation of each programming problem, given its\ntextual description and a solution example of code. For tackling this problem,\nwe propose to couple two pre-trained models, one for text modality and the\nother for code modality, into a unified model. We built two POJ datasets for\nthe task and the results demonstrate the effectiveness of the proposed approach\nand the contributions of both modalities.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08828v1",
    "published_date": "2024-06-13 05:38:20 UTC",
    "updated_date": "2024-06-13 05:38:20 UTC"
  },
  {
    "arxiv_id": "2406.08824v1",
    "title": "LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions",
    "authors": [
      "Rumaisa Azeem",
      "Andrew Hundt",
      "Masoumeh Mansouri",
      "Martim Brandão"
    ],
    "abstract": "Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI)\ncommunities have proposed Large Language Models (LLMs) as a promising resource\nfor robotics tasks such as natural language interactions, doing household and\nworkplace tasks, approximating `common sense reasoning', and modeling humans.\nHowever, recent research has raised concerns about the potential for LLMs to\nproduce discriminatory outcomes and unsafe behaviors in real-world robot\nexperiments and applications. To address these concerns, we conduct an\nHRI-based evaluation of discrimination and safety criteria on several\nhighly-rated LLMs. Our evaluation reveals that LLMs currently lack robustness\nwhen encountering people across a diverse range of protected identity\ncharacteristics (e.g., race, gender, disability status, nationality, religion,\nand their intersections), producing biased outputs consistent with directly\ndiscriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled\nuntrustworthy, but not `european' or `able-bodied' people. Furthermore, we test\nmodels in settings with unconstrained natural language (open vocabulary)\ninputs, and find they fail to act safely, generating responses that accept\ndangerous, violent, or unlawful instructions -- such as incident-causing\nmisstatements, taking people's mobility aids, and sexual predation. Our results\nunderscore the urgent need for systematic, routine, and comprehensive risk\nassessments and assurances to improve outcomes and ensure LLMs only operate on\nrobots when it is safe, effective, and just to do so. Data and code will be\nmade available.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.RO",
    "comment": "40 pages (52 with references), 21 Figures, 6 Tables",
    "pdf_url": "http://arxiv.org/pdf/2406.08824v1",
    "published_date": "2024-06-13 05:31:49 UTC",
    "updated_date": "2024-06-13 05:31:49 UTC"
  },
  {
    "arxiv_id": "2406.08822v1",
    "title": "Computer vision-based model for detecting turning lane features on Florida's public roadways",
    "authors": [
      "Richard Boadu Antwi",
      "Samuel Takyi",
      "Kimollo Michael",
      "Alican Karaer",
      "Eren Erman Ozguven",
      "Ren Moses",
      "Maxim A. Dulebenets",
      "Thobias Sando"
    ],
    "abstract": "Efficient and current roadway geometry data collection is critical to\ntransportation agencies in road planning, maintenance, design, and\nrehabilitation. Data collection methods are divided into land-based and\naerial-based. Land-based methods for extensive highway networks are tedious,\ncostly, pose safety risks. Therefore, there is the need for efficient, safe,\nand economical data acquisition methodologies. The rise of computer vision and\nobject detection technologies have made automated extraction of roadway\ngeometry features feasible. This study detects roadway features on Florida's\npublic roads from high-resolution aerial images using AI. The developed model\nachieved an average accuracy of 80.4 percent when compared with ground truth\ndata. The extracted roadway geometry data can be integrated with crash and\ntraffic data to provide valuable insights to policymakers and roadway users.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08822v1",
    "published_date": "2024-06-13 05:28:53 UTC",
    "updated_date": "2024-06-13 05:28:53 UTC"
  },
  {
    "arxiv_id": "2406.08819v2",
    "title": "AIM: Attributing, Interpreting, Mitigating Data Unfairness",
    "authors": [
      "Zhining Liu",
      "Ruizhong Qiu",
      "Zhichen Zeng",
      "Yada Zhu",
      "Hendrik Hamann",
      "Hanghang Tong"
    ],
    "abstract": "Data collected in the real world often encapsulates historical discrimination\nagainst disadvantaged groups and individuals. Existing fair machine learning\n(FairML) research has predominantly focused on mitigating discriminative bias\nin the model prediction, with far less effort dedicated towards exploring how\nto trace biases present in the data, despite its importance for the\ntransparency and interpretability of FairML. To fill this gap, we investigate a\nnovel research problem: discovering samples that reflect biases/prejudices from\nthe training data. Grounding on the existing fairness notions, we lay out a\nsample bias criterion and propose practical algorithms for measuring and\ncountering sample bias. The derived bias score provides intuitive sample-level\nattribution and explanation of historical bias in data. On this basis, we\nfurther design two FairML strategies via sample-bias-informed minimal data\nediting. They can mitigate both group and individual unfairness at the cost of\nminimal or zero predictive utility loss. Extensive experiments and analyses on\nmultiple real-world datasets demonstrate the effectiveness of our methods in\nexplaining and mitigating unfairness. Code is available at\nhttps://github.com/ZhiningLiu1998/AIM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 6 figures, accepted by ACM SIGKDD 2024. Webpage:\n  https://github.com/ZhiningLiu1998/AIM",
    "pdf_url": "http://arxiv.org/pdf/2406.08819v2",
    "published_date": "2024-06-13 05:21:10 UTC",
    "updated_date": "2024-06-18 07:02:45 UTC"
  },
  {
    "arxiv_id": "2406.08809v2",
    "title": "Are We There Yet? A Brief Survey of Music Emotion Prediction Datasets, Models and Outstanding Challenges",
    "authors": [
      "Jaeyong Kang",
      "Dorien Herremans"
    ],
    "abstract": "Deep learning models for music have advanced drastically in recent years, but\nhow good are machine learning models at capturing emotion, and what challenges\nare researchers facing? In this paper, we provide a comprehensive overview of\nthe available music-emotion datasets and discuss evaluation standards as well\nas competitions in the field. We also offer a brief overview of various types\nof music emotion prediction models that have been built over the years,\nproviding insights into the diverse approaches within the field. Through this\nexamination, we highlight the challenges that persist in accurately capturing\nemotion in music, including issues related to dataset quality, annotation\nconsistency, and model generalization. Additionally, we explore the impact of\ndifferent modalities, such as audio, MIDI, and physiological signals, on the\neffectiveness of emotion prediction models. Recognizing the dynamic nature of\nthis field, we have complemented our findings with an accompanying GitHub\nrepository. This repository contains a comprehensive list of music emotion\ndatasets and recent predictive models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08809v2",
    "published_date": "2024-06-13 05:00:27 UTC",
    "updated_date": "2024-10-22 12:18:27 UTC"
  },
  {
    "arxiv_id": "2406.08805v2",
    "title": "A Dual Approach to Imitation Learning from Observations with Offline Datasets",
    "authors": [
      "Harshit Sikchi",
      "Caleb Chuck",
      "Amy Zhang",
      "Scott Niekum"
    ],
    "abstract": "Demonstrations are an effective alternative to task specification for\nlearning agents in settings where designing a reward function is difficult.\nHowever, demonstrating expert behavior in the action space of the agent becomes\nunwieldy when robots have complex, unintuitive morphologies. We consider the\npractical setting where an agent has a dataset of prior interactions with the\nenvironment and is provided with observation-only expert demonstrations.\nTypical learning from observations approaches have required either learning an\ninverse dynamics model or a discriminator as intermediate steps of training.\nErrors in these intermediate one-step models compound during downstream policy\nlearning or deployment. We overcome these limitations by directly learning a\nmulti-step utility function that quantifies how each action impacts the agent's\ndivergence from the expert's visitation distribution. Using the principle of\nduality, we derive DILO (Dual Imitation Learning from Observations), an\nalgorithm that can leverage arbitrary suboptimal data to learn imitating\npolicies without requiring expert actions. DILO reduces the learning from\nobservations problem to that of simply learning an actor and a critic, bearing\nsimilar complexity to vanilla offline RL. This allows DILO to gracefully scale\nto high dimensional observations, and demonstrate improved performance across\nthe board. Project page (code and videos):\n$\\href{https://hari-sikchi.github.io/dilo/}{\\text{hari-sikchi.github.io/dilo/}}$",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "8th Conference on Robot Learning (CoRL 2024), Munich, Germany. 23\n  pages",
    "pdf_url": "http://arxiv.org/pdf/2406.08805v2",
    "published_date": "2024-06-13 04:39:42 UTC",
    "updated_date": "2024-09-19 21:38:58 UTC"
  },
  {
    "arxiv_id": "2406.08804v2",
    "title": "DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation",
    "authors": [
      "Kairui Fu",
      "Shengyu Zhang",
      "Zheqi Lv",
      "Jingyuan Chen",
      "Jiwei Li"
    ],
    "abstract": "Due to the continuously improving capabilities of mobile edges, recommender\nsystems start to deploy models on edges to alleviate network congestion caused\nby frequent mobile requests. Several studies have leveraged the proximity of\nedge-side to real-time data, fine-tuning them to create edge-specific models.\nDespite their significant progress, these methods require substantial on-edge\ncomputational resources and frequent network transfers to keep the model up to\ndate. The former may disrupt other processes on the edge to acquire\ncomputational resources, while the latter consumes network bandwidth, leading\nto a decrease in user satisfaction. In response to these challenges, we propose\na customizeD slImming framework for incompatiblE neTworks(DIET). DIET deploys\nthe same generic backbone (potentially incompatible for a specific edge) to all\ndevices. To minimize frequent bandwidth usage and storage consumption in\npersonalization, DIET tailors specific subnets for each edge based on its past\ninteractions, learning to generate slimming subnets(diets) within incompatible\nnetworks for efficient transfer. It also takes the inter-layer relationships\ninto account, empirically reducing inference time while obtaining more suitable\ndiets. We further explore the repeated modules within networks and propose a\nmore storage-efficient framework, DIETING, which utilizes a single layer of\nparameters to represent the entire network, achieving comparably excellent\nperformance. The experiments across four state-of-the-art datasets and two\nwidely used models demonstrate the superior accuracy in recommendation and\nefficiency in transmission and storage of our framework.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.08804v2",
    "published_date": "2024-06-13 04:39:16 UTC",
    "updated_date": "2024-06-15 12:57:25 UTC"
  },
  {
    "arxiv_id": "2406.08799v1",
    "title": "Pareto Front-Diverse Batch Multi-Objective Bayesian Optimization",
    "authors": [
      "Alaleh Ahmadianshalchi",
      "Syrine Belakaria",
      "Janardhan Rao Doppa"
    ],
    "abstract": "We consider the problem of multi-objective optimization (MOO) of expensive\nblack-box functions with the goal of discovering high-quality and diverse\nPareto fronts where we are allowed to evaluate a batch of inputs. This problem\narises in many real-world applications including penicillin production where\ndiversity of solutions is critical. We solve this problem in the framework of\nBayesian optimization (BO) and propose a novel approach referred to as Pareto\nfront-Diverse Batch Multi-Objective BO (PDBO). PDBO tackles two important\nchallenges: 1) How to automatically select the best acquisition function in\neach BO iteration, and 2) How to select a diverse batch of inputs by\nconsidering multiple objectives. We propose principled solutions to address\nthese two challenges. First, PDBO employs a multi-armed bandit approach to\nselect one acquisition function from a given library. We solve a cheap MOO\nproblem by assigning the selected acquisition function for each expensive\nobjective function to obtain a candidate set of inputs for evaluation. Second,\nit utilizes Determinantal Point Processes (DPPs) to choose a\nPareto-front-diverse batch of inputs for evaluation from the candidate set\nobtained from the first step. The key parameters for the methods behind these\ntwo steps are updated after each round of function evaluations. Experiments on\nmultiple MOO benchmarks demonstrate that PDBO outperforms prior methods in\nterms of both the quality and diversity of Pareto solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at AAAI Conference on Artificial Intelligence, 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.08799v1",
    "published_date": "2024-06-13 04:28:00 UTC",
    "updated_date": "2024-06-13 04:28:00 UTC"
  },
  {
    "arxiv_id": "2406.10292v3",
    "title": "Automatically Labeling Clinical Trial Outcomes: A Large-Scale Benchmark for Drug Development",
    "authors": [
      "Chufan Gao",
      "Jathurshan Pradeepkumar",
      "Trisha Das",
      "Shivashankar Thati",
      "Jimeng Sun"
    ],
    "abstract": "Background The cost of drug discovery and development is substantial, with\nclinical trial outcomes playing a critical role in regulatory approval and\npatient care. However, access to large-scale, high-quality clinical trial\noutcome data remains limited, hindering advancements in predictive modeling and\nevidence-based decision-making.\n  Methods We present the Clinical Trial Outcome (CTO) benchmark, a fully\nreproducible, large-scale repository encompassing approximately 125,000 drug\nand biologics trials. CTO integrates large language model (LLM) interpretations\nof publications, trial phase progression tracking, sentiment analysis from news\nsources, stock price movements of trial sponsors, and additional trial-related\nmetrics. Furthermore, we manually annotated a dataset of clinical trials\nconducted between 2020 and 2024 to enhance the quality and reliability of\noutcome labels.\n  Results The trial outcome labels in the CTO benchmark agree strongly with\nexpert annotations, achieving an F1 score of 94 for Phase 3 trials and 91\nacross all phases. Additionally, benchmarking standard machine learning models\non our manually annotated dataset revealed distribution shifts in recent\ntrials, underscoring the necessity of continuously updated labeling approaches.\n  Conclusions By analyzing CTO's performance on recent clinical trials, we\ndemonstrate the ongoing need for high-quality, up-to-date trial outcome labels.\nWe publicly release the CTO knowledge base and annotated labels at\nhttps://chufangao.github.io/CTOD, with regular updates to support research on\nclinical trial outcomes and inform data-driven improvements in drug\ndevelopment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10292v3",
    "published_date": "2024-06-13 04:23:35 UTC",
    "updated_date": "2025-03-06 02:41:55 UTC"
  },
  {
    "arxiv_id": "2406.09464v1",
    "title": "GPT-ology, Computational Models, Silicon Sampling: How should we think about LLMs in Cognitive Science?",
    "authors": [
      "Desmond C. Ong"
    ],
    "abstract": "Large Language Models have taken the cognitive science world by storm. It is\nperhaps timely now to take stock of the various research paradigms that have\nbeen used to make scientific inferences about ``cognition\" in these models or\nabout human cognition. We review several emerging research paradigms --\nGPT-ology, LLMs-as-computational-models, and ``silicon sampling\" -- and review\nrecent papers that have used LLMs under these paradigms. In doing so, we\ndiscuss their claims as well as challenges to scientific inference under these\nvarious paradigms. We highlight several outstanding issues about LLMs that have\nto be addressed to push our science forward: closed-source vs open-sourced\nmodels; (the lack of visibility of) training data; and reproducibility in LLM\nresearch, including forming conventions on new task ``hyperparameters\" like\ninstructions and prompts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "CogSci 2024; 6 pages + 2 page of references",
    "pdf_url": "http://arxiv.org/pdf/2406.09464v1",
    "published_date": "2024-06-13 04:19:17 UTC",
    "updated_date": "2024-06-13 04:19:17 UTC"
  },
  {
    "arxiv_id": "2406.09462v1",
    "title": "SViTT-Ego: A Sparse Video-Text Transformer for Egocentric Video",
    "authors": [
      "Hector A. Valdez",
      "Kyle Min",
      "Subarna Tripathi"
    ],
    "abstract": "Pretraining egocentric vision-language models has become essential to\nimproving downstream egocentric video-text tasks. These egocentric foundation\nmodels commonly use the transformer architecture. The memory footprint of these\nmodels during pretraining can be substantial. Therefore, we pretrain SViTT-Ego,\nthe first sparse egocentric video-text transformer model integrating edge and\nnode sparsification. We pretrain on the EgoClip dataset and incorporate the\negocentric-friendly objective EgoNCE, instead of the frequently used InfoNCE.\nMost notably, SViTT-Ego obtains a +2.8% gain on EgoMCQ (intra-video) accuracy\ncompared to LAVILA large, with no additional data augmentation techniques other\nthan standard image augmentations, yet pretrainable on memory-limited devices.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09462v1",
    "published_date": "2024-06-13 03:57:38 UTC",
    "updated_date": "2024-06-13 03:57:38 UTC"
  },
  {
    "arxiv_id": "2406.08788v2",
    "title": "Towards Understanding Link Predictor Generalizability Under Distribution Shifts",
    "authors": [
      "Jay Revolinsky",
      "Harry Shomer",
      "Jiliang Tang"
    ],
    "abstract": "State-of-the-art link prediction (LP) models demonstrate impressive benchmark\nresults. However, popular benchmark datasets often assume that training,\nvalidation, and testing samples are representative of the overall dataset\ndistribution. In real-world situations, this assumption is often incorrect;\nuncontrolled factors lead new dataset samples to come from a different\ndistribution than training samples. Additionally, the majority of recent work\nwith graph dataset shift focuses on node- and graph-level tasks, largely\nignoring link-level tasks. To bridge this gap, we introduce a novel splitting\nstrategy, known as LPShift, which utilizes structural properties to induce a\ncontrolled distribution shift. We verify LPShift's effect through empirical\nevaluation of SOTA LP models on 16 LPShift variants of original dataset splits,\nwith results indicating drastic changes to model performance. Additional\nexperiments demonstrate graph structure has a strong influence on the success\nof current generalization methods. Source Code Available Here:\nhttps://github.com/revolins/LPShift",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 8 figures, 17 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.08788v2",
    "published_date": "2024-06-13 03:47:12 UTC",
    "updated_date": "2025-03-11 19:49:55 UTC"
  },
  {
    "arxiv_id": "2406.08787v2",
    "title": "A Survey on Compositional Learning of AI Models: Theoretical and Experimental Practices",
    "authors": [
      "Sania Sinha",
      "Tanawan Premsri",
      "Parisa Kordjamshidi"
    ],
    "abstract": "Compositional learning, mastering the ability to combine basic concepts and\nconstruct more intricate ones, is crucial for human cognition, especially in\nhuman language comprehension and visual perception. This notion is tightly\nconnected to generalization over unobserved situations. Despite its integral\nrole in intelligence, there is a lack of systematic theoretical and\nexperimental research methodologies, making it difficult to analyze the\ncompositional learning abilities of computational models. In this paper, we\nsurvey the literature on compositional learning of AI models and the\nconnections made to cognitive studies. We identify abstract concepts of\ncompositionality in cognitive and linguistic studies and connect these to the\ncomputational challenges faced by language and vision models in compositional\nreasoning. We overview the formal definitions, tasks, evaluation benchmarks,\nvarious computational models, and theoretical findings. Our primary focus is on\nlinguistic benchmarks and combining language and vision, though there is a\nlarge amount of research on compositional concept learning in the computer\nvision community alone. We cover modern studies on large language models to\nprovide a deeper understanding of the cutting-edge compositional capabilities\nexhibited by state-of-the-art AI models and pinpoint important directions for\nfuture research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08787v2",
    "published_date": "2024-06-13 03:46:21 UTC",
    "updated_date": "2024-11-21 00:54:18 UTC"
  },
  {
    "arxiv_id": "2406.10291v2",
    "title": "ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents",
    "authors": [
      "Hao Kang",
      "Chenyan Xiong"
    ],
    "abstract": "Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10291v2",
    "published_date": "2024-06-13 03:26:30 UTC",
    "updated_date": "2025-02-14 17:37:35 UTC"
  },
  {
    "arxiv_id": "2406.08771v2",
    "title": "MFF-EINV2: Multi-scale Feature Fusion across Spectral-Spatial-Temporal Domains for Sound Event Localization and Detection",
    "authors": [
      "Da Mu",
      "Zhicheng Zhang",
      "Haobo Yue"
    ],
    "abstract": "Sound Event Localization and Detection (SELD) involves detecting and\nlocalizing sound events using multichannel sound recordings. Previously\nproposed Event-Independent Network V2 (EINV2) has achieved outstanding\nperformance on SELD. However, it still faces challenges in effectively\nextracting features across spectral, spatial, and temporal domains. This paper\nproposes a three-stage network structure named Multi-scale Feature Fusion (MFF)\nmodule to fully extract multi-scale features across spectral, spatial, and\ntemporal domains. The MFF module utilizes parallel subnetworks architecture to\ngenerate multi-scale spectral and spatial features. The TF-Convolution Module\nis employed to provide multi-scale temporal features. We incorporated MFF into\nEINV2 and term the proposed method as MFF-EINV2. Experimental results in 2022\nand 2023 DCASE challenge task3 datasets show the effectiveness of our\nMFF-EINV2, which achieves state-of-the-art (SOTA) performance compared to\npublished methods.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.08771v2",
    "published_date": "2024-06-13 03:03:02 UTC",
    "updated_date": "2024-06-15 11:52:49 UTC"
  },
  {
    "arxiv_id": "2406.08766v1",
    "title": "Injecting Combinatorial Optimization into MCTS: Application to the Board Game boop",
    "authors": [
      "Florian Richoux"
    ],
    "abstract": "Games, including abstract board games, constitute a convenient ground to\ncreate, design, and improve new AI methods. In this field, Monte Carlo Tree\nSearch is a popular algorithm family, aiming to build game trees and explore\nthem efficiently. Combinatorial Optimization, on the other hand, aims to model\nand solve problems with an objective to optimize and constraints to satisfy,\nand is less common in Game AI. We believe however that both methods can be\ncombined efficiently, by injecting Combinatorial Optimization into Monte Carlo\nTree Search to help the tree search, leading to a novel combination of these\ntwo techniques. Tested on the board game boop., our method beats 96% of the\ntime the Monte Carlo Tree Search algorithm baseline. We conducted an ablation\nstudy to isolate and analyze which injections and combinations of injections\nlead to such performances. Finally, we opposed our AI method against human\nplayers on the Board Game Arena platform, and reached a 373 ELO rating after 51\nboop. games, with a 69% win rate and finishing ranked 56th worldwide on the\nplatform over 5,316 boop. players.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.08766v1",
    "published_date": "2024-06-13 02:55:08 UTC",
    "updated_date": "2024-06-13 02:55:08 UTC"
  },
  {
    "arxiv_id": "2406.17797v1",
    "title": "MoleculeCLA: Rethinking Molecular Benchmark via Computational Ligand-Target Binding Analysis",
    "authors": [
      "Shikun Feng",
      "Jiaxin Zheng",
      "Yinjun Jia",
      "Yanwen Huang",
      "Fengfeng Zhou",
      "Wei-Ying Ma",
      "Yanyan Lan"
    ],
    "abstract": "Molecular representation learning is pivotal for various molecular property\nprediction tasks related to drug discovery. Robust and accurate benchmarks are\nessential for refining and validating current methods. Existing molecular\nproperty benchmarks derived from wet experiments, however, face limitations\nsuch as data volume constraints, unbalanced label distribution, and noisy\nlabels. To address these issues, we construct a large-scale and precise\nmolecular representation dataset of approximately 140,000 small molecules,\nmeticulously designed to capture an extensive array of chemical, physical, and\nbiological properties, derived through a robust computational ligand-target\nbinding analysis pipeline. We conduct extensive experiments on various deep\nlearning models, demonstrating that our dataset offers significant\nphysicochemical interpretability to guide model development and design.\nNotably, the dataset's properties are linked to binding affinity metrics,\nproviding additional insights into model performance in drug-target interaction\ntasks. We believe this dataset will serve as a more accurate and reliable\nbenchmark for molecular representation learning, thereby expediting progress in\nthe field of artificial intelligence-driven drug discovery.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17797v1",
    "published_date": "2024-06-13 02:50:23 UTC",
    "updated_date": "2024-06-13 02:50:23 UTC"
  },
  {
    "arxiv_id": "2406.08757v1",
    "title": "SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding",
    "authors": [
      "Jiefeng Ma",
      "Yan Wang",
      "Chenyu Liu",
      "Jun Du",
      "Yu Hu",
      "Zhenrong Zhang",
      "Pengfei Hu",
      "Qing Wang",
      "Jianshu Zhang"
    ],
    "abstract": "Accurately identifying and organizing textual content is crucial for the\nautomation of document processing in the field of form understanding. Existing\ndatasets, such as FUNSD and XFUND, support entity classification and\nrelationship prediction tasks but are typically limited to local and\nentity-level annotations. This limitation overlooks the hierarchically\nstructured representation of documents, constraining comprehensive\nunderstanding of complex forms. To address this issue, we present the SRFUND, a\nhierarchically structured multi-task form understanding benchmark. SRFUND\nprovides refined annotations on top of the original FUNSD and XFUND datasets,\nencompassing five tasks: (1) word to text-line merging, (2) text-line to entity\nmerging, (3) entity category classification, (4) item table localization, and\n(5) entity-based full-document hierarchical structure recovery. We meticulously\nsupplemented the original dataset with missing annotations at various levels of\ngranularity and added detailed annotations for multi-item table regions within\nthe forms. Additionally, we introduce global hierarchical structure\ndependencies for entity relation prediction tasks, surpassing traditional local\nkey-value associations. The SRFUND dataset includes eight languages including\nEnglish, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese,\nmaking it a powerful tool for cross-lingual form understanding. Extensive\nexperimental results demonstrate that the SRFUND dataset presents new\nchallenges and significant opportunities in handling diverse layouts and global\nhierarchical structures of forms, thus providing deep insights into the field\nof form understanding. The original dataset and implementations of baseline\nmethods are available at https://sprateam-ustc.github.io/SRFUND",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024 Track on Datasets and Benchmarks under review",
    "pdf_url": "http://arxiv.org/pdf/2406.08757v1",
    "published_date": "2024-06-13 02:35:55 UTC",
    "updated_date": "2024-06-13 02:35:55 UTC"
  },
  {
    "arxiv_id": "2406.08751v1",
    "title": "3D Building Generation in Minecraft via Large Language Models",
    "authors": [
      "Shiying Hu",
      "Zengrong Huang",
      "Chengpeng Hu",
      "Jialin Liu"
    ],
    "abstract": "Recently, procedural content generation has exhibited considerable\nadvancements in the domain of 2D game level generation such as Super Mario\nBros. and Sokoban through large language models (LLMs). To further validate the\ncapabilities of LLMs, this paper explores how LLMs contribute to the generation\nof 3D buildings in a sandbox game, Minecraft. We propose a Text to Building in\nMinecraft (T2BM) model, which involves refining prompts, decoding interlayer\nrepresentation and repairing. Facade, indoor scene and functional blocks like\ndoors are supported in the generation. Experiments are conducted to evaluate\nthe completeness and satisfaction of buildings generated via LLMs. It shows\nthat LLMs hold significant potential for 3D building generation. Given\nappropriate prompts, LLMs can generate correct buildings in Minecraft with\ncomplete structures and incorporate specific building blocks such as windows\nand beds, meeting the specified requirements of human users.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted by IEEE Conference on Games",
    "pdf_url": "http://arxiv.org/pdf/2406.08751v1",
    "published_date": "2024-06-13 02:21:07 UTC",
    "updated_date": "2024-06-13 02:21:07 UTC"
  },
  {
    "arxiv_id": "2406.08748v2",
    "title": "Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel SVD and Nyström method",
    "authors": [
      "Qinghua Tao",
      "Francesco Tonin",
      "Alex Lambert",
      "Yingyi Chen",
      "Panagiotis Patrinos",
      "Johan A. K. Suykens"
    ],
    "abstract": "In contrast with Mercer kernel-based approaches as used e.g., in Kernel\nPrincipal Component Analysis (KPCA), it was previously shown that Singular\nValue Decomposition (SVD) inherently relates to asymmetric kernels and\nAsymmetric Kernel Singular Value Decomposition (KSVD) has been proposed.\nHowever, the existing formulation to KSVD cannot work with infinite-dimensional\nfeature mappings, the variational objective can be unbounded, and needs further\nnumerical evaluation and exploration towards machine learning. In this work, i)\nwe introduce a new asymmetric learning paradigm based on coupled covariance\neigenproblem (CCE) through covariance operators, allowing infinite-dimensional\nfeature maps. The solution to CCE is ultimately obtained from the SVD of the\ninduced asymmetric kernel matrix, providing links to KSVD. ii) Starting from\nthe integral equations corresponding to a pair of coupled adjoint\neigenfunctions, we formalize the asymmetric Nystr\\\"om method through a finite\nsample approximation to speed up training. iii) We provide the first empirical\nevaluations verifying the practical utility and benefits of KSVD and compare\nwith methods resorting to symmetrization or linear SVD across multiple tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 9 tables, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.08748v2",
    "published_date": "2024-06-13 02:12:18 UTC",
    "updated_date": "2025-03-08 07:42:08 UTC"
  },
  {
    "arxiv_id": "2406.12910v1",
    "title": "Human-level molecular optimization driven by mol-gene evolution",
    "authors": [
      "Jiebin Fang",
      "Churu Mao",
      "Yuchen Zhu",
      "Xiaoming Chen",
      "Chang-Yu Hsieh",
      "Zhongjun Ma"
    ],
    "abstract": "De novo molecule generation allows the search for more drug-like hits across\na vast chemical space. However, lead optimization is still required, and the\nprocess of optimizing molecular structures faces the challenge of balancing\nstructural novelty with pharmacological properties. This study introduces the\nDeep Genetic Molecular Modification Algorithm (DGMM), which brings structure\nmodification to the level of medicinal chemists. A discrete variational\nautoencoder (D-VAE) is used in DGMM to encode molecules as quantization code,\nmol-gene, which incorporates deep learning into genetic algorithms for flexible\nstructural optimization. The mol-gene allows for the discovery of\npharmacologically similar but structurally distinct compounds, and reveals the\ntrade-offs of structural optimization in drug discovery. We demonstrate the\neffectiveness of the DGMM in several applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "physics.chem-ph",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12910v1",
    "published_date": "2024-06-13 01:06:03 UTC",
    "updated_date": "2024-06-13 01:06:03 UTC"
  },
  {
    "arxiv_id": "2406.08713v1",
    "title": "Batch-Instructed Gradient for Prompt Evolution:Systematic Prompt Optimization for Enhanced Text-to-Image Synthesis",
    "authors": [
      "Xinrui Yang",
      "Zhuohan Wang",
      "Anthony Hu"
    ],
    "abstract": "Text-to-image models have shown remarkable progress in generating\nhigh-quality images from user-provided prompts. Despite this, the quality of\nthese images varies due to the models' sensitivity to human language nuances.\nWith advancements in large language models, there are new opportunities to\nenhance prompt design for image generation tasks. Existing research primarily\nfocuses on optimizing prompts for direct interaction, while less attention is\ngiven to scenarios involving intermediary agents, like the Stable Diffusion\nmodel. This study proposes a Multi-Agent framework to optimize input prompts\nfor text-to-image generation models. Central to this framework is a prompt\ngeneration mechanism that refines initial queries using dynamic instructions,\nwhich evolve through iterative performance feedback. High-quality prompts are\nthen fed into a state-of-the-art text-to-image model. A professional prompts\ndatabase serves as a benchmark to guide the instruction modifier towards\ngenerating high-caliber prompts. A scoring system evaluates the generated\nimages, and an LLM generates new instructions based on calculated gradients.\nThis iterative process is managed by the Upper Confidence Bound (UCB) algorithm\nand assessed using the Human Preference Score version 2 (HPS v2). Preliminary\nablation studies highlight the effectiveness of various system components and\nsuggest areas for future improvements.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.08713v1",
    "published_date": "2024-06-13 00:33:29 UTC",
    "updated_date": "2024-06-13 00:33:29 UTC"
  },
  {
    "arxiv_id": "2406.08702v4",
    "title": "VLind-Bench: Measuring Language Priors in Large Vision-Language Models",
    "authors": [
      "Kang-il Lee",
      "Minbeom Kim",
      "Seunghyun Yoon",
      "Minsung Kim",
      "Dongryeol Lee",
      "Hyukhun Koh",
      "Kyomin Jung"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance across various multimodal tasks. However, they suffer from a\nproblem known as language prior, where responses are generated based solely on\ntextual patterns while disregarding image information. Addressing the issue of\nlanguage prior is crucial, as it can lead to undesirable biases or\nhallucinations when dealing with images that are out of training distribution.\nDespite its importance, current methods for accurately measuring language\npriors in LVLMs are poorly studied. Although existing benchmarks based on\ncounterfactual or out-of-distribution images can partially be used to measure\nlanguage priors, they fail to disentangle language priors from other\nconfounding factors. To this end, we propose a new benchmark called\nVLind-Bench, which is the first benchmark specifically designed to measure the\nlanguage priors, or blindness, of LVLMs. It not only includes tests on\ncounterfactual images to assess language priors but also involves a series of\ntests to evaluate more basic capabilities such as commonsense knowledge, visual\nperception, and commonsense biases. For each instance in our benchmark, we\nensure that all these basic tests are passed before evaluating the language\npriors, thereby minimizing the influence of other factors on the assessment.\nThe evaluation and analysis of recent LVLMs in our benchmark reveal that almost\nall models exhibit a significant reliance on language priors, presenting a\nstrong challenge in the field.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "NAACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.08702v4",
    "published_date": "2024-06-13 00:00:20 UTC",
    "updated_date": "2025-02-08 23:14:12 UTC"
  }
]
[
  {
    "arxiv_id": "2402.10949v2",
    "title": "The Unreasonable Effectiveness of Eccentric Automatic Prompts",
    "authors": [
      "Rick Battle",
      "Teja Gollapudi"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable problem-solving and\nbasic mathematics abilities. However, their efficacy is highly contingent on\nthe formulation of the prompt. This study endeavors to quantify the influence\nof incorporating \"positive thinking\" into the system message of the prompt,\nthen compare that to systematic prompt optimization. We assess the performance\nof 60 combinations of system message snippets, tested with and without Chain of\nThought prompting, across three models with parameters ranging from 7 to 70\nbillion on the GSM8K dataset. Our findings reveal that results do not\nuniversally generalize across models. In most instances, the inclusion of\n\"positive thinking\" prompts positively affected model performance. Notably,\nhowever, Llama2-70B exhibited an exception when not utilizing Chain of Thought,\nas the optimal system message was found to be none at all. Given the\ncombinatorial complexity, and thus computation time, of experimenting with\nhand-tuning prompts for large black-box models, we then compared the\nperformance of the best \"positive thinking\" prompt against the output of\nsystematic prompt optimization. We show that employing an automated prompt\noptimizer emerges as the most effective method for enhancing performance, even\nwhen working with smaller open-source models. Additionally, our findings reveal\nthat the highest-scoring, automatically-optimized prompt exhibits a degree of\npeculiarity far beyond expectations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10949v2",
    "published_date": "2024-02-09 22:48:45 UTC",
    "updated_date": "2024-02-20 15:03:00 UTC"
  },
  {
    "arxiv_id": "2402.06811v1",
    "title": "Discipline and Label: A WEIRD Genealogy and Social Theory of Data Annotation",
    "authors": [
      "Andrew Smart",
      "Ding Wang",
      "Ellis Monk",
      "Mark Díaz",
      "Atoosa Kasirzadeh",
      "Erin Van Liemt",
      "Sonja Schmer-Galunder"
    ],
    "abstract": "Data annotation remains the sine qua non of machine learning and AI. Recent\nempirical work on data annotation has begun to highlight the importance of\nrater diversity for fairness, model performance, and new lines of research have\nbegun to examine the working conditions for data annotation workers, the\nimpacts and role of annotator subjectivity on labels, and the potential\npsychological harms from aspects of annotation work. This paper outlines a\ncritical genealogy of data annotation; starting with its psychological and\nperceptual aspects. We draw on similarities with critiques of the rise of\ncomputerized lab-based psychological experiments in the 1970's which question\nwhether these experiments permit the generalization of results beyond the\nlaboratory settings within which these results are typically obtained. Do data\nannotations permit the generalization of results beyond the settings, or\nlocations, in which they were obtained? Psychology is overly reliant on\nparticipants from Western, Educated, Industrialized, Rich, and Democratic\nsocieties (WEIRD). Many of the people who work as data annotation platform\nworkers, however, are not from WEIRD countries; most data annotation workers\nare based in Global South countries. Social categorizations and classifications\nfrom WEIRD countries are imposed on non-WEIRD annotators through instructions\nand tasks, and through them, on data, which is then used to train or evaluate\nAI models in WEIRD countries. We synthesize evidence from several recent lines\nof research and argue that data annotation is a form of automated social\ncategorization that risks entrenching outdated and static social categories\nthat are in reality dynamic and changing. We propose a framework for\nunderstanding the interplay of the global social conditions of data annotation\nwith the subjective phenomenological experience of data annotation work.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.06811v1",
    "published_date": "2024-02-09 22:21:55 UTC",
    "updated_date": "2024-02-09 22:21:55 UTC"
  },
  {
    "arxiv_id": "2402.06810v1",
    "title": "Evaluating Co-Creativity using Total Information Flow",
    "authors": [
      "Vignesh Gokul",
      "Chris Francis",
      "Shlomo Dubnov"
    ],
    "abstract": "Co-creativity in music refers to two or more musicians or musical agents\ninteracting with one another by composing or improvising music. However, this\nis a very subjective process and each musician has their own preference as to\nwhich improvisation is better for some context. In this paper, we aim to create\na measure based on total information flow to quantitatively evaluate the\nco-creativity process in music. In other words, our measure is an indication of\nhow \"good\" a creative musical process is. Our main hypothesis is that a good\nmusical creation would maximize information flow between the participants\ncaptured by music voices recorded in separate tracks. We propose a method to\ncompute the information flow using pre-trained generative models as entropy\nestimators. We demonstrate how our method matches with human perception using a\nqualitative study.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "cs.IT",
      "cs.LG",
      "eess.AS",
      "math.IT"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06810v1",
    "published_date": "2024-02-09 22:15:39 UTC",
    "updated_date": "2024-02-09 22:15:39 UTC"
  },
  {
    "arxiv_id": "2402.06794v2",
    "title": "Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing",
    "authors": [
      "Hochul Hwang",
      "Sunjae Kwon",
      "Yekyung Kim",
      "Donghyun Kim"
    ],
    "abstract": "Safely navigating street intersections is a complex challenge for blind and\nlow-vision individuals, as it requires a nuanced understanding of the\nsurrounding context - a task heavily reliant on visual cues. Traditional\nmethods for assisting in this decision-making process often fall short, lacking\nthe ability to provide a comprehensive scene analysis and safety level. This\npaper introduces an innovative approach that leverages large multimodal models\n(LMMs) to interpret complex street crossing scenes, offering a potential\nadvancement over conventional traffic signal recognition techniques. By\ngenerating a safety score and scene description in natural language, our method\nsupports safe decision-making for the blind and low-vision individuals. We\ncollected crosswalk intersection data that contains multiview egocentric images\ncaptured by a quadruped robot and annotated the images with corresponding\nsafety scores based on our predefined safety score categorization. Grounded on\nthe visual knowledge, extracted from images, and text prompt, we evaluate a\nlarge multimodal model for safety score prediction and scene description. Our\nfindings highlight the reasoning and safety score prediction capabilities of a\nLMM, activated by various prompts, as a pathway to developing a trustworthy\nsystem, crucial for applications requiring reliable decision-making support.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06794v2",
    "published_date": "2024-02-09 21:37:13 UTC",
    "updated_date": "2024-07-06 15:36:23 UTC"
  },
  {
    "arxiv_id": "2402.06784v2",
    "title": "Transfer learning with generative models for object detection on limited datasets",
    "authors": [
      "Matteo Paiano",
      "Stefano Martina",
      "Carlotta Giannelli",
      "Filippo Caruso"
    ],
    "abstract": "The availability of data is limited in some fields, especially for object\ndetection tasks, where it is necessary to have correctly labeled bounding boxes\naround each object. A notable example of such data scarcity is found in the\ndomain of marine biology, where it is useful to develop methods to\nautomatically detect submarine species for environmental monitoring. To address\nthis data limitation, the state-of-the-art machine learning strategies employ\ntwo main approaches. The first involves pretraining models on existing datasets\nbefore generalizing to the specific domain of interest. The second strategy is\nto create synthetic datasets specifically tailored to the target domain using\nmethods like copy-paste techniques or ad-hoc simulators. The first strategy\noften faces a significant domain shift, while the second demands custom\nsolutions crafted for the specific task. In response to these challenges, here\nwe propose a transfer learning framework that is valid for a generic scenario.\nIn this framework, generated images help to improve the performances of an\nobject detector in a few-real data regime. This is achieved through a\ndiffusion-based generative model that was pretrained on large generic datasets.\nWith respect to the state-of-the-art, we find that it is not necessary to fine\ntune the generative model on the specific domain of interest. We believe that\nthis is an important advance because it mitigates the labor-intensive task of\nmanual labeling the images in object detection tasks. We validate our approach\nfocusing on fishes in an underwater environment, and on the more common domain\nof cars in an urban setting. Our method achieves detection performance\ncomparable to models trained on thousands of images, using only a few hundreds\nof input data. Our results pave the way for new generative AI-based protocols\nfor machine learning applications in various domains.",
    "categories": [
      "cs.CV",
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "68T05, 68T07, 68T10, 68T45,",
      "I.2.6; I.2.0; I.4.8; I.4.9; I.5.1; I.5.0; I.5.4; J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 16 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2402.06784v2",
    "published_date": "2024-02-09 21:17:31 UTC",
    "updated_date": "2024-06-13 10:09:51 UTC"
  },
  {
    "arxiv_id": "2402.06782v4",
    "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
    "authors": [
      "Akbir Khan",
      "John Hughes",
      "Dan Valentine",
      "Laura Ruis",
      "Kshitij Sachan",
      "Ansh Radhakrishnan",
      "Edward Grefenstette",
      "Samuel R. Bowman",
      "Tim Rocktäschel",
      "Ethan Perez"
    ],
    "abstract": "Common methods for aligning large language models (LLMs) with desired\nbehaviour heavily rely on human-labelled data. However, as models grow\nincreasingly sophisticated, they will surpass human expertise, and the role of\nhuman evaluation will evolve into non-experts overseeing experts. In\nanticipation of this, we ask: can weaker models assess the correctness of\nstronger models? We investigate this question in an analogous setting, where\nstronger models (experts) possess the necessary information to answer questions\nand weaker models (non-experts) lack this information. The method we evaluate\nis debate, where two LLM experts each argue for a different answer, and a\nnon-expert selects the answer. We find that debate consistently helps both\nnon-expert models and humans answer questions, achieving 76% and 88% accuracy\nrespectively (naive baselines obtain 48% and 60%). Furthermore, optimising\nexpert debaters for persuasiveness in an unsupervised manner improves\nnon-expert ability to identify the truth in debates. Our results provide\nencouraging empirical evidence for the viability of aligning models with debate\nin the absence of ground truth.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "For code please check: https://github.com/ucl-dark/llm_debate",
    "pdf_url": "http://arxiv.org/pdf/2402.06782v4",
    "published_date": "2024-02-09 21:05:01 UTC",
    "updated_date": "2024-07-25 23:32:21 UTC"
  },
  {
    "arxiv_id": "2402.06772v1",
    "title": "Retrosynthesis Prediction via Search in (Hyper) Graph",
    "authors": [
      "Zixun Lan",
      "Binjie Hong",
      "Jiajun Zhu",
      "Zuo Zeng",
      "Zhenfu Liu",
      "Limin Yu",
      "Fei Ma"
    ],
    "abstract": "Predicting reactants from a specified core product stands as a fundamental\nchallenge within organic synthesis, termed retrosynthesis prediction. Recently,\nsemi-template-based methods and graph-edits-based methods have achieved good\nperformance in terms of both interpretability and accuracy. However, due to\ntheir mechanisms these methods cannot predict complex reactions, e.g.,\nreactions with multiple reaction center or attaching the same leaving group to\nmore than one atom. In this study we propose a semi-template-based method, the\n\\textbf{Retro}synthesis via \\textbf{S}earch \\textbf{i}n (Hyper) \\textbf{G}raph\n(RetroSiG) framework to alleviate these limitations. In the proposed method, we\nturn the reaction center identification and the leaving group completion tasks\nas tasks of searching in the product molecular graph and leaving group\nhypergraph respectively. As a semi-template-based method RetroSiG has several\nadvantages. First, RetroSiG is able to handle the complex reactions mentioned\nabove by its novel search mechanism. Second, RetroSiG naturally exploits the\nhypergraph to model the implicit dependencies between leaving groups. Third,\nRetroSiG makes full use of the prior, i.e., one-hop constraint. It reduces the\nsearch space and enhances overall performance. Comprehensive experiments\ndemonstrated that RetroSiG achieved competitive results. Furthermore, we\nconducted experiments to show the capability of RetroSiG in predicting complex\nreactions. Ablation experiments verified the efficacy of specific elements,\nsuch as the one-hop constraint and the leaving group hypergraph.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06772v1",
    "published_date": "2024-02-09 20:25:45 UTC",
    "updated_date": "2024-02-09 20:25:45 UTC"
  },
  {
    "arxiv_id": "2402.06766v1",
    "title": "Evaluation Metrics for Text Data Augmentation in NLP",
    "authors": [
      "Marcellus Amadeus",
      "William Alberto Cruz Castañeda"
    ],
    "abstract": "Recent surveys on data augmentation for natural language processing have\nreported different techniques and advancements in the field. Several\nframeworks, tools, and repositories promote the implementation of text data\naugmentation pipelines. However, a lack of evaluation criteria and standards\nfor method comparison due to different tasks, metrics, datasets, architectures,\nand experimental settings makes comparisons meaningless. Also, a lack of\nmethods unification exists and text data augmentation research would benefit\nfrom unified metrics to compare different augmentation methods. Thus, academics\nand the industry endeavor relevant evaluation metrics for text data\naugmentation techniques. The contribution of this work is to provide a taxonomy\nof evaluation metrics for text augmentation methods and serve as a direction\nfor a unified benchmark. The proposed taxonomy organizes categories that\ninclude tools for implementation and metrics calculation. Finally, with this\nstudy, we intend to present opportunities to explore the unification and\nstandardization of text data augmentation metrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06766v1",
    "published_date": "2024-02-09 19:59:34 UTC",
    "updated_date": "2024-02-09 19:59:34 UTC"
  },
  {
    "arxiv_id": "2402.06764v3",
    "title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
    "authors": [
      "Stefan Dernbach",
      "Khushbu Agarwal",
      "Alejandro Zuniga",
      "Michael Henry",
      "Sutanay Choudhury"
    ],
    "abstract": "Integrating large language models (LLMs) with knowledge graphs derived from\ndomain-specific data represents an important advancement towards more powerful\nand factual reasoning. As these models grow more capable, it is crucial to\nenable them to perform multi-step inferences over real-world knowledge graphs\nwhile minimizing hallucination. While large language models excel at\nconversation and text generation, their ability to reason over\ndomain-specialized graphs of interconnected entities remains limited. For\nexample, can we query a LLM to identify the optimal contact in a professional\nnetwork for a specific goal, based on relationships and attributes in a private\ndatabase? The answer is no--such capabilities lie beyond current methods.\nHowever, this question underscores a critical technical gap that must be\naddressed. Many high-value applications in areas such as science, security, and\ne-commerce rely on proprietary knowledge graphs encoding unique structures,\nrelationships, and logical constraints. We introduce a fine-tuning framework\nfor developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge\ngraph into an alternate text representation with labeled question-answer pairs.\nWe demonstrate that grounding the models in specific graph-based knowledge\nexpands the models' capacity for structure-based reasoning. Our methodology\nleverages the large-language model's generative capabilities to create the\ndataset and proposes an efficient alternate to retrieval-augmented generation\nstyled methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in AAAI Spring Symposium: AAAI-MAKE 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.06764v3",
    "published_date": "2024-02-09 19:53:29 UTC",
    "updated_date": "2024-04-17 19:55:37 UTC"
  },
  {
    "arxiv_id": "2402.06759v1",
    "title": "A Methodology for Questionnaire Analysis: Insights through Cluster Analysis of an Investor Competition Data",
    "authors": [
      "Carlos Henrique Q. Forster",
      "Paulo André Lima de Castro",
      "Andrei Ramalho"
    ],
    "abstract": "In this paper, we propose a methodology for the analysis of questionnaire\ndata along with its application on discovering insights from investor data\nmotivated by a day trading competition. The questionnaire includes categorical\nquestions, which are reduced to binary questions, 'yes' or 'no'. The\nmethodology reduces dimensionality by grouping questions and participants with\nsimilar responses using clustering analysis. Rule discovery was performed by\nusing a conversion rate metric. Innovative visual representations were proposed\nto validate the cluster analysis and the relation discovery between questions.\nWhen crossing with financial data, additional insights were revealed related to\nthe recognized clusters.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "14 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.06759v1",
    "published_date": "2024-02-09 19:44:29 UTC",
    "updated_date": "2024-02-09 19:44:29 UTC"
  },
  {
    "arxiv_id": "2402.06737v2",
    "title": "ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning",
    "authors": [
      "Mahdi Naseri",
      "Mahdi Biparva"
    ],
    "abstract": "Self-supervised Learning (SSL) has emerged as a powerful technique in\npre-training deep learning models without relying on expensive annotated\nlabels, instead leveraging embedded signals in unlabeled data. While SSL has\nshown remarkable success in computer vision tasks through intuitive data\naugmentation, its application to graph-structured data poses challenges due to\nthe semantic-altering and counter-intuitive nature of graph augmentations.\nAddressing this limitation, this paper introduces a novel non-contrastive SSL\napproach to Explicitly Generate a compositional Relation Graph (ExGRG) instead\nof relying solely on the conventional augmentation-based implicit relation\ngraph. ExGRG offers a framework for incorporating prior domain knowledge and\nonline extracted information into the SSL invariance objective, drawing\ninspiration from the Laplacian Eigenmap and Expectation-Maximization (EM).\nEmploying an EM perspective on SSL, our E-step involves relation graph\ngeneration to identify candidates to guide the SSL invariance objective, and\nM-step updates the model parameters by integrating the derived relational\ninformation. Extensive experimentation on diverse node classification datasets\ndemonstrates the superiority of our method over state-of-the-art techniques,\naffirming ExGRG as an effective adoption of SSL for graph representation\nlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06737v2",
    "published_date": "2024-02-09 19:16:04 UTC",
    "updated_date": "2024-06-04 15:30:15 UTC"
  },
  {
    "arxiv_id": "2402.06734v1",
    "title": "Corruption Robust Offline Reinforcement Learning with Human Feedback",
    "authors": [
      "Debmalya Mandal",
      "Andi Nika",
      "Parameswaran Kamalaruban",
      "Adish Singla",
      "Goran Radanović"
    ],
    "abstract": "We study data corruption robustness for reinforcement learning with human\nfeedback (RLHF) in an offline setting. Given an offline dataset of pairs of\ntrajectories along with feedback about human preferences, an\n$\\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or\ntrajectory features manipulated), capturing an adversarial attack or noisy\nhuman preferences. We aim to design algorithms that identify a near-optimal\npolicy from the corrupted data, with provable guarantees. Existing theoretical\nworks have separately studied the settings of corruption robust RL (learning\nfrom scalar rewards directly under corruption) and offline RLHF (learning from\nhuman feedback without corruption); however, they are inapplicable to our\nproblem of dealing with corrupted data in offline RLHF setting. To this end, we\ndesign novel corruption robust offline RLHF methods under various assumptions\non the coverage of the data-generating distributions. At a high level, our\nmethodology robustifies an offline RLHF framework by first learning a reward\nmodel along with confidence sets and then learning a pessimistic optimal policy\nover the confidence set. Our key insight is that learning optimal policy can be\ndone by leveraging an offline corruption-robust RL oracle in different ways\n(e.g., zero-order oracle or first-order oracle), depending on the data coverage\nassumptions. To our knowledge, ours is the first work that provides provable\ncorruption robust offline RLHF methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06734v1",
    "published_date": "2024-02-09 19:09:48 UTC",
    "updated_date": "2024-02-09 19:09:48 UTC"
  },
  {
    "arxiv_id": "2402.06733v3",
    "title": "NICE: To Optimize In-Context Examples or Not?",
    "authors": [
      "Pragya Srivastava",
      "Satvik Golechha",
      "Amit Deshpande",
      "Amit Sharma"
    ],
    "abstract": "Recent work shows that in-context learning and optimization of in-context\nexamples (ICE) can significantly improve the accuracy of large language models\n(LLMs) on a wide range of tasks, leading to an apparent consensus that ICE\noptimization is crucial for better performance. However, most of these studies\nassume a fixed or no instruction provided in the prompt. We challenge this\nconsensus by investigating the necessity of optimizing ICE when task-specific\ninstructions are provided and find that there are many tasks for which it\nyields diminishing returns. In particular, using a diverse set of tasks and a\nsystematically created instruction set with gradually added details, we find\nthat as the prompt instruction becomes more detailed, the returns on ICE\noptimization diminish. To characterize this behavior, we introduce a\ntask-specific metric called Normalized Invariability to Choice of Examples\n(NICE) that quantifies the learnability of tasks from a given instruction, and\nprovides a heuristic to help decide whether to optimize instructions or ICE for\na new task. Given a task, the proposed metric can reliably predict the utility\nof optimizing ICE compared to using random ICE. Our code is available at\nhttps://github.com/microsoft/nice-icl.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a full paper (9 pages) at ACL 2024 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2402.06733v3",
    "published_date": "2024-02-09 19:09:19 UTC",
    "updated_date": "2024-06-06 12:16:55 UTC"
  },
  {
    "arxiv_id": "2402.06627v3",
    "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
    "authors": [
      "Alexander Pan",
      "Erik Jones",
      "Meena Jagadeesan",
      "Jacob Steinhardt"
    ],
    "abstract": "Language models influence the external world: they query APIs that read and\nwrite to web pages, generate content that shapes human behavior, and run system\ncommands as autonomous agents. These interactions form feedback loops: LLM\noutputs affect the world, which in turn affect subsequent LLM outputs. In this\nwork, we show that feedback loops can cause in-context reward hacking (ICRH),\nwhere the LLM at test-time optimizes a (potentially implicit) objective but\ncreates negative side effects in the process. For example, consider an LLM\nagent deployed to increase Twitter engagement; the LLM may retrieve its\nprevious tweets into the context window and make them more controversial,\nincreasing engagement but also toxicity. We identify and study two processes\nthat lead to ICRH: output-refinement and policy-refinement. For these\nprocesses, evaluations on static datasets are insufficient -- they miss the\nfeedback effects and thus cannot capture the most harmful behavior. In\nresponse, we provide three recommendations for evaluation to capture more\ninstances of ICRH. As AI development accelerates, the effects of feedback loops\nwill proliferate, increasing the need to understand their role in shaping LLM\nbehavior.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024 camera-ready",
    "pdf_url": "http://arxiv.org/pdf/2402.06627v3",
    "published_date": "2024-02-09 18:59:29 UTC",
    "updated_date": "2024-06-06 21:39:09 UTC"
  },
  {
    "arxiv_id": "2402.06619v1",
    "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
    "authors": [
      "Shivalika Singh",
      "Freddie Vargus",
      "Daniel Dsouza",
      "Börje F. Karlsson",
      "Abinaya Mahendiran",
      "Wei-Yin Ko",
      "Herumb Shandilya",
      "Jay Patel",
      "Deividas Mataciunas",
      "Laura OMahony",
      "Mike Zhang",
      "Ramith Hettiarachchi",
      "Joseph Wilson",
      "Marina Machado",
      "Luisa Souza Moura",
      "Dominik Krzemiński",
      "Hakimeh Fadaei",
      "Irem Ergün",
      "Ifeoma Okoh",
      "Aisha Alaagib",
      "Oshan Mudannayake",
      "Zaid Alyafeai",
      "Vu Minh Chien",
      "Sebastian Ruder",
      "Surya Guthikonda",
      "Emad A. Alghamdi",
      "Sebastian Gehrmann",
      "Niklas Muennighoff",
      "Max Bartolo",
      "Julia Kreutzer",
      "Ahmet Üstün",
      "Marzieh Fadaee",
      "Sara Hooker"
    ],
    "abstract": "Datasets are foundational to many breakthroughs in modern artificial\nintelligence. Many recent achievements in the space of natural language\nprocessing (NLP) can be attributed to the finetuning of pre-trained models on a\ndiverse set of tasks that enables a large language model (LLM) to respond to\ninstructions. Instruction fine-tuning (IFT) requires specifically constructed\nand annotated datasets. However, existing datasets are almost all in the\nEnglish language. In this work, our primary goal is to bridge the language gap\nby building a human-curated instruction-following dataset spanning 65\nlanguages. We worked with fluent speakers of languages from around the world to\ncollect natural instances of instructions and completions. Furthermore, we\ncreate the most extensive multilingual collection to date, comprising 513\nmillion instances through templating and translating existing datasets across\n114 languages. In total, we contribute four key resources: we develop and\nopen-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection,\nand the Aya Evaluation Suite. The Aya initiative also serves as a valuable case\nstudy in participatory research, involving collaborators from 119 countries. We\nsee this as a valuable framework for future research collaborations that aim to\nbridge gaps in resources.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06619v1",
    "published_date": "2024-02-09 18:51:49 UTC",
    "updated_date": "2024-02-09 18:51:49 UTC"
  },
  {
    "arxiv_id": "2402.08690v1",
    "title": "If Turing played piano with an artificial partner",
    "authors": [
      "Dobromir Dotov",
      "Dante Camarena",
      "Zack Harris",
      "Joanna Spyra",
      "Pietro Gagliano",
      "Laurel Trainor"
    ],
    "abstract": "Music is an inherently social activity that allows people to share\nexperiences and feel connected with one another. There has been little progress\nin designing artificial partners exhibiting a similar social experience as\nplaying with another person. Neural network architectures that implement\ngenerative models, such as large language models, are suited for producing\nmusical scores. Playing music socially, however, involves more than playing a\nscore; it must complement the other musicians' ideas and keep time correctly.\nWe addressed the question of whether a convincing social experience is made\npossible by a generative model trained to produce musical scores, not\nnecessarily optimized for synchronization and continuation. The network, a\nvariational autoencoder trained on a large corpus of digital scores, was\nadapted for a timed call-and-response task with a human partner. Participants\nplayed piano with a human or artificial partner-in various configurations-and\nrated the performance quality and first-person experience of self-other\nintegration. Overall, the artificial partners held promise but were rated lower\nthan human partners. The artificial partner with simplest design and highest\nsimilarity parameter was not rated differently from the human partners on some\nmeasures, suggesting that interactive rather than generative sophistication is\nimportant in enabling social AI.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.08690v1",
    "published_date": "2024-02-09 18:43:48 UTC",
    "updated_date": "2024-02-09 18:43:48 UTC"
  },
  {
    "arxiv_id": "2402.06608v2",
    "title": "TIC: Translate-Infer-Compile for accurate \"text to plan\" using LLMs and Logical Representations",
    "authors": [
      "Sudhir Agarwal",
      "Anu Sreepathy"
    ],
    "abstract": "We study the problem of generating plans for given natural language planning\ntask requests. On one hand, LLMs excel at natural language processing but do\nnot perform well on planning. On the other hand, classical planning tools excel\nat planning tasks but require input in a structured language such as the\nPlanning Domain Definition Language (PDDL). We leverage the strengths of both\nthe techniques by using an LLM for generating the PDDL representation (task\nPDDL) of planning task requests followed by using a classical planner for\ncomputing a plan. Unlike previous approaches that use LLMs for generating task\nPDDLs directly, our approach comprises of (a) translate: using an LLM only for\ngenerating a logically interpretable intermediate representation of natural\nlanguage task description, (b) infer: deriving additional logically dependent\ninformation from the intermediate representation using a logic reasoner\n(currently, Answer Set Programming solver), and (c) compile: generating the\ntarget task PDDL from the base and inferred information. We observe that using\nan LLM to only output the intermediate representation significantly reduces LLM\nerrors. Consequently, TIC approach achieves, for at least one LLM, high\naccuracy on task PDDL generation for all seven domains of our evaluation\ndataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages (7 main + 2 references + 11 appendix), 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.06608v2",
    "published_date": "2024-02-09 18:39:13 UTC",
    "updated_date": "2024-06-29 00:30:04 UTC"
  },
  {
    "arxiv_id": "2402.06606v1",
    "title": "RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization",
    "authors": [
      "Ce Feng",
      "Parv Venkitasubramaniam"
    ],
    "abstract": "The rise of IoT devices has prompted the demand for deploying machine\nlearning at-the-edge with real-time, efficient, and secure data processing. In\nthis context, implementing machine learning (ML) models with real-valued weight\nparameters can prove to be impractical particularly for large models, and there\nis a need to train models with quantized discrete weights. At the same time,\nthese low-dimensional models also need to preserve privacy of the underlying\ndataset. In this work, we present RQP-SGD, a new approach for\nprivacy-preserving quantization to train machine learning models for low-memory\nML-at-the-edge. This approach combines differentially private stochastic\ngradient descent (DP-SGD) with randomized quantization, providing a measurable\nprivacy guarantee in machine learning. In particular, we study the utility\nconvergence of implementing RQP-SGD on ML tasks with convex objectives and\nquantization constraints and demonstrate its efficacy over deterministic\nquantization. Through experiments conducted on two datasets, we show the\npractical effectiveness of RQP-SGD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "This work is accepted by the 5th AAAI Workshop on Privacy-Preserving\n  Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2402.06606v1",
    "published_date": "2024-02-09 18:34:08 UTC",
    "updated_date": "2024-02-09 18:34:08 UTC"
  },
  {
    "arxiv_id": "2402.06599v1",
    "title": "On the Out-Of-Distribution Generalization of Multimodal Large Language Models",
    "authors": [
      "Xingxuan Zhang",
      "Jiansheng Li",
      "Wenjing Chu",
      "Junjia Hai",
      "Renzhe Xu",
      "Yuqing Yang",
      "Shikai Guan",
      "Jiazheng Xu",
      "Peng Cui"
    ],
    "abstract": "We investigate the generalization boundaries of current Multimodal Large\nLanguage Models (MLLMs) via comprehensive evaluation under out-of-distribution\nscenarios and domain-specific tasks. We evaluate their zero-shot generalization\nacross synthetic images, real-world distributional shifts, and specialized\ndatasets like medical and molecular imagery. Empirical results indicate that\nMLLMs struggle with generalization beyond common training domains, limiting\ntheir direct application without adaptation. To understand the cause of\nunreliable performance, we analyze three hypotheses: semantic\nmisinterpretation, visual feature extraction insufficiency, and mapping\ndeficiency. Results identify mapping deficiency as the primary hurdle. To\naddress this problem, we show that in-context learning (ICL) can significantly\nenhance MLLMs' generalization, opening new avenues for overcoming\ngeneralization barriers. We further explore the robustness of ICL under\ndistribution shifts and show its vulnerability to domain shifts, label shifts,\nand spurious correlation shifts between in-context examples and test data.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06599v1",
    "published_date": "2024-02-09 18:21:51 UTC",
    "updated_date": "2024-02-09 18:21:51 UTC"
  },
  {
    "arxiv_id": "2402.06596v1",
    "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
    "authors": [
      "Mingzhe Xing",
      "Rongkai Zhang",
      "Hui Xue",
      "Qi Chen",
      "Fan Yang",
      "Zhen Xiao"
    ],
    "abstract": "Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06596v1",
    "published_date": "2024-02-09 18:19:25 UTC",
    "updated_date": "2024-02-09 18:19:25 UTC"
  },
  {
    "arxiv_id": "2402.06590v3",
    "title": "Predictive representations: building blocks of intelligence",
    "authors": [
      "Wilka Carvalho",
      "Momchil S. Tomov",
      "William de Cothi",
      "Caswell Barry",
      "Samuel J. Gershman"
    ],
    "abstract": "Adaptive behavior often requires predicting future events. The theory of\nreinforcement learning prescribes what kinds of predictive representations are\nuseful and how to compute them. This paper integrates these theoretical ideas\nwith work on cognition and neuroscience. We pay special attention to the\nsuccessor representation (SR) and its generalizations, which have been widely\napplied both as engineering tools and models of brain function. This\nconvergence suggests that particular kinds of predictive representations may\nfunction as versatile building blocks of intelligence.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted to Neural Computation",
    "pdf_url": "http://arxiv.org/pdf/2402.06590v3",
    "published_date": "2024-02-09 18:10:38 UTC",
    "updated_date": "2024-07-11 14:02:37 UTC"
  },
  {
    "arxiv_id": "2402.06584v2",
    "title": "G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German",
    "authors": [
      "Ehsan Latif",
      "Gyeong-Geon Lee",
      "Knut Neumann",
      "Tamara Kastorff",
      "Xiaoming Zhai"
    ],
    "abstract": "The advancement of natural language processing has paved the way for\nautomated scoring systems in various languages, such as German (e.g., German\nBERT [G-BERT]). Automatically scoring written responses to science questions in\nGerman is a complex task and challenging for standard G-BERT as they lack\ncontextual knowledge in the science domain and may be unaligned with student\nwriting styles. This paper presents a contextualized German Science Education\nBERT (G-SciEdBERT), an innovative large language model tailored for scoring\nGerman-written responses to science tasks and beyond. Using G-BERT, we\npre-trained G-SciEdBERT on a corpus of 30K German written science responses\nwith 3M tokens on the Programme for International Student Assessment (PISA)\n2018. We fine-tuned G-SciEdBERT on an additional 20K student-written responses\nwith 2M tokens and examined the scoring accuracy. We then compared its scoring\nperformance with G-BERT. Our findings revealed a substantial improvement in\nscoring accuracy with G-SciEdBERT, demonstrating a 10.2% increase of quadratic\nweighted Kappa compared to G-BERT (mean difference = 0.1026, SD = 0.069). These\ninsights underline the significance of specialized language models like\nG-SciEdBERT, which is trained to enhance the accuracy of contextualized\nautomated scoring, offering a substantial contribution to the field of AI in\neducation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EDM and Submitted to JEDM",
    "pdf_url": "http://arxiv.org/pdf/2402.06584v2",
    "published_date": "2024-02-09 18:05:03 UTC",
    "updated_date": "2024-08-16 20:38:04 UTC"
  },
  {
    "arxiv_id": "2402.06563v1",
    "title": "What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices",
    "authors": [
      "Neslihan Suzen",
      "Evgeny M. Mirkes",
      "Damian Roland",
      "Jeremy Levesley",
      "Alexander N. Gorban",
      "Tim J. Coats"
    ],
    "abstract": "Electronic patient records (EPRs) produce a wealth of data but contain\nsignificant missing information. Understanding and handling this missing data\nis an important part of clinical data analysis and if left unaddressed could\nresult in bias in analysis and distortion in critical conclusions. Missing data\nmay be linked to health care professional practice patterns and imputation of\nmissing data can increase the validity of clinical decisions. This study\nfocuses on statistical approaches for understanding and interpreting the\nmissing data and machine learning based clinical data imputation using a single\ncentre's paediatric emergency data and the data from UK's largest clinical\naudit for traumatic injury database (TARN). In the study of 56,961 data points\nrelated to initial vital signs and observations taken on children presenting to\nan Emergency Department, we have shown that missing data are likely to be\nnon-random and how these are linked to health care professional practice\npatterns. We have then examined 79 TARN fields with missing values for 5,791\ntrauma cases. Singular Value Decomposition (SVD) and k-Nearest Neighbour (kNN)\nbased missing data imputation methods are used and imputation results against\nthe original dataset are compared and statistically tested. We have concluded\nthat the 1NN imputer is the best imputation which indicates a usual pattern of\nclinical decision making: find the most similar patients and take their\nattributes as imputation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.06563v1",
    "published_date": "2024-02-09 17:27:35 UTC",
    "updated_date": "2024-02-09 17:27:35 UTC"
  },
  {
    "arxiv_id": "2403.13812v1",
    "title": "Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Submissions using AI Detection Tool",
    "authors": [
      "Arslan Akram"
    ],
    "abstract": "Many people are interested in ChatGPT since it has become a prominent AIGC\nmodel that provides high-quality responses in various contexts, such as\nsoftware development and maintenance. Misuse of ChatGPT might cause significant\nissues, particularly in public safety and education, despite its immense\npotential. The majority of researchers choose to publish their work on Arxiv.\nThe effectiveness and originality of future work depend on the ability to\ndetect AI components in such contributions. To address this need, this study\nwill analyze a method that can see purposely manufactured content that academic\norganizations use to post on Arxiv. For this study, a dataset was created using\nphysics, mathematics, and computer science articles. Using the newly built\ndataset, the following step is to put originality.ai through its paces. The\nstatistical analysis shows that Originality.ai is very accurate, with a rate of\n98%.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "stat.OT",
      "62P25",
      "I.7; G.1; G.3"
    ],
    "primary_category": "cs.DL",
    "comment": "8 pages, 6 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2403.13812v1",
    "published_date": "2024-02-09 17:20:48 UTC",
    "updated_date": "2024-02-09 17:20:48 UTC"
  },
  {
    "arxiv_id": "2402.06559v2",
    "title": "Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following",
    "authors": [
      "Brian Yang",
      "Huangyuan Su",
      "Nikolaos Gkanatsios",
      "Tsung-Wei Ke",
      "Ayush Jain",
      "Jeff Schneider",
      "Katerina Fragkiadaki"
    ],
    "abstract": "Diffusion models excel at modeling complex and multimodal trajectory\ndistributions for decision-making and control. Reward-gradient guided denoising\nhas been recently proposed to generate trajectories that maximize both a\ndifferentiable reward function and the likelihood under the data distribution\ncaptured by a diffusion model. Reward-gradient guided denoising requires a\ndifferentiable reward function fitted to both clean and noised samples,\nlimiting its applicability as a general trajectory optimizer. In this paper, we\npropose DiffusionES, a method that combines gradient-free optimization with\ntrajectory denoising to optimize black-box non-differentiable objectives while\nstaying in the data manifold. Diffusion-ES samples trajectories during\nevolutionary search from a diffusion model and scores them using a black-box\nreward function. It mutates high-scoring trajectories using a truncated\ndiffusion process that applies a small number of noising and denoising steps,\nallowing for much more efficient exploration of the solution space. We show\nthat DiffusionES achieves state-of-the-art performance on nuPlan, an\nestablished closed-loop planning benchmark for autonomous driving. Diffusion-ES\noutperforms existing sampling-based planners, reactive deterministic or\ndiffusion-based policies, and reward-gradient guidance. Additionally, we show\nthat unlike prior guidance methods, our method can optimize non-differentiable\nlanguage-shaped reward functions generated by few-shot LLM prompting. When\nguided by a human teacher that issues instructions to follow, our method can\ngenerate novel, highly complex behaviors, such as aggressive lane weaving,\nwhich are not present in the training data. This allows us to solve the hardest\nnuPlan scenarios which are beyond the capabilities of existing trajectory\noptimization methods and driving policies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06559v2",
    "published_date": "2024-02-09 17:18:33 UTC",
    "updated_date": "2024-07-16 15:54:22 UTC"
  },
  {
    "arxiv_id": "2402.06557v1",
    "title": "The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model",
    "authors": [
      "Gregory Coppola"
    ],
    "abstract": "This paper introduces the Quantified Boolean Bayesian Network (QBBN), which\nprovides a unified view of logical and probabilistic reasoning. The QBBN is\nmeant to address a central problem with the Large Language Model (LLM), which\nhas become extremely popular in Information Retrieval, which is that the LLM\nhallucinates. A Bayesian Network, by construction, cannot hallucinate, because\nit can only return answers that it can explain. We show how a Bayesian Network\nover an unbounded number of boolean variables can be configured to represent\nthe logical reasoning underlying human language. We do this by creating a\nkey-value version of the First-Order Calculus, for which we can prove\nconsistency and completeness. We show that the model is trivially trained over\nfully observed data, but that inference is non-trivial. Exact inference in a\nBayesian Network is intractable (i.e. $\\Omega(2^N)$ for $N$ variables). For\ninference, we investigate the use of Loopy Belief Propagation (LBP), which is\nnot guaranteed to converge, but which has been shown to often converge in\npractice. Our experiments show that LBP indeed does converge very reliably, and\nour analysis shows that a round of LBP takes time $O(N2^n)$, where $N$ bounds\nthe number of variables considered, and $n$ bounds the number of incoming\nconnections to any factor, and further improvements may be possible. Our\nnetwork is specifically designed to alternate between AND and OR gates in a\nBoolean Algebra, which connects more closely to logical reasoning, allowing a\ncompleteness proof for an expanded version of our network, and also allows\ninference to follow specific but adequate pathways, that turn out to be fast.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06557v1",
    "published_date": "2024-02-09 17:15:45 UTC",
    "updated_date": "2024-02-09 17:15:45 UTC"
  },
  {
    "arxiv_id": "2402.06549v1",
    "title": "Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA",
    "authors": [
      "Marek Šuppa",
      "Daniel Skala",
      "Daniela Jašš",
      "Samuel Sučík",
      "Andrej Švec",
      "Peter Hraška"
    ],
    "abstract": "This study details our approach for the CASE 2024 Shared Task on Climate\nActivism Stance and Hate Event Detection, focusing on Hate Speech Detection,\nHate Speech Target Identification, and Stance Detection as classification\nchallenges. We explored the capability of Large Language Models (LLMs),\nparticularly GPT-4, in zero- or few-shot settings enhanced by retrieval\naugmentation and re-ranking for Tweet classification. Our goal was to determine\nif LLMs could match or surpass traditional methods in this context.\n  We conducted an ablation study with LLaMA for comparison, and our results\nindicate that our models significantly outperformed the baselines, securing\nsecond place in the Target Detection task. The code for our submission is\navailable at https://github.com/NaiveNeuron/bryndza-case-2024",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the 7th Workshop on Challenges and Applications of\n  Automated Extraction of Socio-political Events from Text (CASE 2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.06549v1",
    "published_date": "2024-02-09 17:02:41 UTC",
    "updated_date": "2024-02-09 17:02:41 UTC"
  },
  {
    "arxiv_id": "2402.06716v3",
    "title": "Dynamic Graph Information Bottleneck",
    "authors": [
      "Haonan Yuan",
      "Qingyun Sun",
      "Xingcheng Fu",
      "Cheng Ji",
      "Jianxin Li"
    ],
    "abstract": "Dynamic Graphs widely exist in the real world, which carry complicated\nspatial and temporal feature patterns, challenging their representation\nlearning. Dynamic Graph Neural Networks (DGNNs) have shown impressive\npredictive abilities by exploiting the intrinsic dynamics. However, DGNNs\nexhibit limited robustness, prone to adversarial attacks. This paper presents\nthe novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust\nand discriminative representations. Leveraged by the Information Bottleneck\n(IB) principle, we first propose the expected optimal representations should\nsatisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress\nredundant as well as conserve meritorious information into latent\nrepresentation, DGIB iteratively directs and refines the structural and feature\ninformation flow passing through graph snapshots. To meet the MSC Condition, we\ndecompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the\nDGIB$_{MS}$ channel aims to learn the minimal and sufficient representations,\nwith the DGIB$_{MS}$ channel guarantees the predictive consensus. Extensive\nexperiments on real-world and synthetic dynamic graph datasets demonstrate the\nsuperior robustness of DGIB against adversarial attacks compared with\nstate-of-the-art baselines in the link prediction task. To the best of our\nknowledge, DGIB is the first work to learn robust representations of dynamic\ngraphs grounded in the information-theoretic IB principle.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the research tracks of The Web Conference 2024 (WWW 2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.06716v3",
    "published_date": "2024-02-09 17:02:41 UTC",
    "updated_date": "2024-04-06 12:38:45 UTC"
  },
  {
    "arxiv_id": "2402.06544v2",
    "title": "Calibrating Long-form Generations from Large Language Models",
    "authors": [
      "Yukun Huang",
      "Yixin Liu",
      "Raghuveer Thirukovalluru",
      "Arman Cohan",
      "Bhuwan Dhingra"
    ],
    "abstract": "To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06544v2",
    "published_date": "2024-02-09 17:00:32 UTC",
    "updated_date": "2024-10-25 21:29:37 UTC"
  },
  {
    "arxiv_id": "2402.06532v2",
    "title": "Generative Adversarial Model-Based Optimization via Source Critic Regularization",
    "authors": [
      "Michael S. Yao",
      "Yimeng Zeng",
      "Hamsa Bastani",
      "Jacob Gardner",
      "James C. Gee",
      "Osbert Bastani"
    ],
    "abstract": "Offline model-based optimization seeks to optimize against a learned\nsurrogate model without querying the true oracle objective function during\noptimization. Such tasks are commonly encountered in protein design, robotics,\nand clinical medicine where evaluating the oracle function is prohibitively\nexpensive. However, inaccurate surrogate model predictions are frequently\nencountered along offline optimization trajectories. To address this\nlimitation, we propose generative adversarial model-based optimization using\nadaptive source critic regularization (aSCR) -- a task- and optimizer- agnostic\nframework for constraining the optimization trajectory to regions of the design\nspace where the surrogate function is reliable. We propose a computationally\ntractable algorithm to dynamically adjust the strength of this constraint, and\nshow how leveraging aSCR with standard Bayesian optimization outperforms\nexisting methods on a suite of offline generative design tasks. Our code is\navailable at https://github.com/michael-s-yao/gabo",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.06532v2",
    "published_date": "2024-02-09 16:43:57 UTC",
    "updated_date": "2024-09-25 18:07:41 UTC"
  },
  {
    "arxiv_id": "2402.06530v3",
    "title": "Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification",
    "authors": [
      "Muhammad Uzair Zahid",
      "Aysen Degerli",
      "Fahad Sohrab",
      "Serkan Kiranyaz",
      "Tahir Hamid",
      "Rashid Mazhar",
      "Moncef Gabbouj"
    ],
    "abstract": "Early detection of myocardial infarction (MI), a critical condition arising\nfrom coronary artery disease (CAD), is vital to prevent further myocardial\ndamage. This study introduces a novel method for early MI detection using a\none-class classification (OCC) algorithm in echocardiography. Our study\novercomes the challenge of limited echocardiography data availability by\nadopting a novel approach based on Multi-modal Subspace Support Vector Data\nDescription. The proposed technique involves a specialized MI detection\nframework employing multi-view echocardiography incorporating a composite\nkernel in the non-linear projection trick, fusing Gaussian and Laplacian\nsigmoid functions. Additionally, we enhance the update strategy of the\nprojection matrices by adapting maximization for both or one of the modalities\nin the optimization process. Our method boosts MI detection capability by\nefficiently transforming features extracted from echocardiography data into an\noptimized lower-dimensional subspace. The OCC model trained specifically on\ntarget class instances from the comprehensive HMC-QU dataset that includes\nmultiple echocardiography views indicates a marked improvement in MI detection\naccuracy. Our findings reveal that our proposed multi-view approach achieves a\ngeometric mean of 71.24%, signifying a substantial advancement in\nechocardiography-based MI diagnosis and offering more precise and efficient\ndiagnostic tools.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06530v3",
    "published_date": "2024-02-09 16:41:50 UTC",
    "updated_date": "2024-06-27 15:39:12 UTC"
  },
  {
    "arxiv_id": "2402.06529v4",
    "title": "Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity",
    "authors": [
      "Kaiqu Liang",
      "Zixu Zhang",
      "Jaime Fernández Fisac"
    ],
    "abstract": "Large language models (LLMs) exhibit advanced reasoning skills, enabling\nrobots to comprehend natural language instructions and strategically plan\nhigh-level actions through proper grounding. However, LLM hallucination may\nresult in robots confidently executing plans that are misaligned with user\ngoals or even unsafe in critical scenarios. Additionally, inherent ambiguity in\nnatural language instructions can introduce uncertainty into the LLM's\nreasoning and planning processes.We propose introspective planning, a\nsystematic approach that align LLM's uncertainty with the inherent ambiguity of\nthe task. Our approach constructs a knowledge base containing introspective\nreasoning examples as post-hoc rationalizations of human-selected safe and\ncompliant plans, which are retrieved during deployment. Evaluations on three\ntasks, including a newly introduced safe mobile manipulation benchmark,\ndemonstrate that introspection substantially improves both compliance and\nsafety over state-of-the-art LLM-based planning methods. Furthermore, we\nempirically show that introspective planning, in combination with conformal\nprediction, achieves tighter confidence bounds, maintaining statistical success\nguarantees while minimizing unnecessary user clarification requests. The\nwebpage and code are accessible at https://introplan.github.io.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.06529v4",
    "published_date": "2024-02-09 16:40:59 UTC",
    "updated_date": "2025-02-10 23:28:39 UTC"
  },
  {
    "arxiv_id": "2402.06509v1",
    "title": "Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions",
    "authors": [
      "Alberto Testoni",
      "Raquel Fernández"
    ],
    "abstract": "Clarification questions are an essential dialogue tool to signal\nmisunderstanding, ambiguities, and under-specification in language use. While\nhumans are able to resolve uncertainty by asking questions since childhood,\nmodern dialogue systems struggle to generate effective questions. To make\nprogress in this direction, in this work we take a collaborative dialogue task\nas a testbed and study how model uncertainty relates to human uncertainty -- an\nas yet under-explored problem. We show that model uncertainty does not mirror\nhuman clarification-seeking behavior, which suggests that using human\nclarification questions as supervision for deciding when to ask may not be the\nmost effective way to resolve model uncertainty. To address this issue, we\npropose an approach to generating clarification questions based on model\nuncertainty estimation, compare it to several alternatives, and show that it\nleads to significant improvements in terms of task success. Our findings\nhighlight the importance of equipping dialogue systems with the ability to\nassess their own uncertainty and exploit in interaction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.06509v1",
    "published_date": "2024-02-09 16:15:30 UTC",
    "updated_date": "2024-02-09 16:15:30 UTC"
  },
  {
    "arxiv_id": "2402.06506v1",
    "title": "Classifying point clouds at the facade-level using geometric features and deep learning networks",
    "authors": [
      "Yue Tan",
      "Olaf Wysocki",
      "Ludwig Hoegner",
      "Uwe Stilla"
    ],
    "abstract": "3D building models with facade details are playing an important role in many\napplications now. Classifying point clouds at facade-level is key to create\nsuch digital replicas of the real world. However, few studies have focused on\nsuch detailed classification with deep neural networks. We propose a method\nfusing geometric features with deep learning networks for point cloud\nclassification at facade-level. Our experiments conclude that such early-fused\nfeatures improve deep learning methods' performance. This method can be applied\nfor compensating deep learning networks' ability in capturing local geometric\ninformation and promoting the advancement of semantic segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to the Recent Advances in 3D Geoinformation Science,\n  Proceedings of the 18th 3D GeoInfo Conference 2023",
    "pdf_url": "http://arxiv.org/pdf/2402.06506v1",
    "published_date": "2024-02-09 16:14:30 UTC",
    "updated_date": "2024-02-09 16:14:30 UTC"
  },
  {
    "arxiv_id": "2402.06503v1",
    "title": "ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies",
    "authors": [
      "Jasmina Gajcin",
      "Ivana Dusparic"
    ],
    "abstract": "Understanding how failure occurs and how it can be prevented in reinforcement\nlearning (RL) is necessary to enable debugging, maintain user trust, and\ndevelop personalized policies. Counterfactual reasoning has often been used to\nassign blame and understand failure by searching for the closest possible world\nin which the failure is avoided. However, current counterfactual state\nexplanations in RL can only explain an outcome using just the current state\nfeatures and offer no actionable recourse on how a negative outcome could have\nbeen prevented. In this work, we propose ACTER (Actionable Counterfactual\nSequences for Explaining Reinforcement Learning Outcomes), an algorithm for\ngenerating counterfactual sequences that provides actionable advice on how\nfailure can be avoided. ACTER investigates actions leading to a failure and\nuses the evolutionary algorithm NSGA-II to generate counterfactual sequences of\nactions that prevent it with minimal changes and high certainty even in\nstochastic environments. Additionally, ACTER generates a set of multiple\ndiverse counterfactual sequences that enable users to correct failure in the\nway that best fits their preferences. We also introduce three diversity metrics\nthat can be used for evaluating the diversity of counterfactual sequences. We\nevaluate ACTER in two RL environments, with both discrete and continuous\nactions, and show that it can generate actionable and diverse counterfactual\nsequences. We conduct a user study to explore how explanations generated by\nACTER help users identify and correct failure.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2402.06503v1",
    "published_date": "2024-02-09 16:12:53 UTC",
    "updated_date": "2024-02-09 16:12:53 UTC"
  },
  {
    "arxiv_id": "2402.06501v2",
    "title": "Scalable Interactive Machine Learning for Future Command and Control",
    "authors": [
      "Anna Madison",
      "Ellen Novoseller",
      "Vinicius G. Goecks",
      "Benjamin T. Files",
      "Nicholas Waytowich",
      "Alfred Yu",
      "Vernon J. Lawhern",
      "Steven Thurman",
      "Christopher Kelshaw",
      "Kaleb McDowell"
    ],
    "abstract": "Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "I.2.6; I.2.7; J.7"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.06501v2",
    "published_date": "2024-02-09 16:11:04 UTC",
    "updated_date": "2024-03-28 15:17:01 UTC"
  },
  {
    "arxiv_id": "2402.07946v2",
    "title": "Re-Envisioning Command and Control",
    "authors": [
      "Kaleb McDowell",
      "Ellen Novoseller",
      "Anna Madison",
      "Vinicius G. Goecks",
      "Christopher Kelshaw"
    ],
    "abstract": "Future warfare will require Command and Control (C2) decision-making to occur\nin more complex, fast-paced, ill-structured, and demanding conditions. C2 will\nbe further complicated by operational challenges such as Denied, Degraded,\nIntermittent, and Limited (DDIL) communications and the need to account for\nmany data streams, potentially across multiple domains of operation. Yet,\ncurrent C2 practices -- which stem from the industrial era rather than the\nemerging intelligence era -- are linear and time-consuming. Critically, these\napproaches may fail to maintain overmatch against adversaries on the future\nbattlefield. To address these challenges, we propose a vision for future C2\nbased on robust partnerships between humans and artificial intelligence (AI)\nsystems. This future vision is encapsulated in three operational impacts:\nstreamlining the C2 operations process, maintaining unity of effort, and\ndeveloping adaptive collective knowledge systems. This paper illustrates the\nenvisaged future C2 capabilities, discusses the assumptions that shaped them,\nand describes how the proposed developments could transform C2 in future\nwarfare.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.6; I.2.7; J.7"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.07946v2",
    "published_date": "2024-02-09 16:10:29 UTC",
    "updated_date": "2024-03-28 15:17:30 UTC"
  },
  {
    "arxiv_id": "2402.06500v2",
    "title": "On the Fly Detection of Root Causes from Observed Data with Application to IT Systems",
    "authors": [
      "Lei Zan",
      "Charles K. Assaad",
      "Emilie Devijver",
      "Eric Gaussier",
      "Ali Aït-Bachir"
    ],
    "abstract": "This paper introduces a new structural causal model tailored for representing\nthreshold-based IT systems and presents a new algorithm designed to rapidly\ndetect root causes of anomalies in such systems. When root causes are not\ncausally related, the method is proven to be correct; while an extension is\nproposed based on the intervention of an agent to relax this assumption. Our\nalgorithm and its agent-based extension leverage causal discovery from offline\ndata and engage in subgraph traversal when encountering new anomalies in online\ndata. Our extensive experiments demonstrate the superior performance of our\nmethods, even when applied to data generated from alternative structural causal\nmodels or real IT monitoring data.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.06500v2",
    "published_date": "2024-02-09 16:10:19 UTC",
    "updated_date": "2024-07-29 13:13:30 UTC"
  },
  {
    "arxiv_id": "2402.06492v1",
    "title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
    "authors": [
      "Yichen Jiang",
      "Xiang Zhou",
      "Mohit Bansal"
    ],
    "abstract": "Transformers generalize to novel compositions of structures and entities\nafter being trained on a complex dataset, but easily overfit on datasets of\ninsufficient complexity. We observe that when the training set is sufficiently\ncomplex, the model encodes sentences that have a common syntactic structure\nusing a systematic attention pattern. Inspired by this observation, we propose\nSQ-Transformer (Structurally Quantized) that explicitly encourages\nsystematicity in the embeddings and attention layers, even with a training set\nof low complexity. At the embedding level, we introduce Structure-oriented\nVector Quantization (SoVQ) to cluster word embeddings into several classes of\nstructurally equivalent entities. At the attention level, we devise the\nSystematic Attention Layer (SAL) and an alternative, Systematically Regularized\nLayer (SRL) that operate on the quantized word embeddings so that sentences of\nthe same structure are encoded with invariant or similar attention patterns.\nEmpirically, we show that SQ-Transformer achieves stronger compositional\ngeneralization than the vanilla Transformer on multiple low-complexity semantic\nparsing and machine translation datasets. In our analysis, we show that SoVQ\nindeed learns a syntactically clustered embedding space and SAL/SRL induces\ngeneralizable attention patterns, which lead to improved systematicity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, code: https://github.com/jiangycTarheel/SQ-Transformer",
    "pdf_url": "http://arxiv.org/pdf/2402.06492v1",
    "published_date": "2024-02-09 15:53:15 UTC",
    "updated_date": "2024-02-09 15:53:15 UTC"
  },
  {
    "arxiv_id": "2402.06487v1",
    "title": "Le Nozze di Giustizia. Interactions between Artificial Intelligence, Law, Logic, Language and Computation with some case studies in Traffic Regulations and Health Care",
    "authors": [
      "Joost J. Joosten",
      "Manuela Montoya García"
    ],
    "abstract": "An important aim of this paper is to convey some basics of mathematical logic\nto the legal community working with Artificial Intelligence. After analysing\nwhat AI is, we decide to delimit ourselves to rule-based AI leaving Neural\nNetworks and Machine Learning aside. Rule based AI allows for Formal methods\nwhich are described in a rudimentary form. We will then see how mathematical\nlogic interacts with legal rule-based AI practice. We shall see how\nmathematical logic imposes limitations and complications to AI applications. We\nclassify the limitations and interactions between mathematical logic and legal\nAI in three categories: logical, computational and mathematical. The examples\nto showcase the interactions will largely come from European traffic\nregulations. The paper closes off with some reflections on how and where AI\ncould be used and on basic mechanisms that shape society.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06487v1",
    "published_date": "2024-02-09 15:43:31 UTC",
    "updated_date": "2024-02-09 15:43:31 UTC"
  },
  {
    "arxiv_id": "2402.06472v1",
    "title": "\"When He Feels Cold, He Goes to the Seahorse\"-Blending Generative AI into Multimaterial Storymaking for Family Expressive Arts Therapy",
    "authors": [
      "Di Liu",
      "Hanqing Zhou",
      "Pengcheng An"
    ],
    "abstract": "Storymaking, as an integrative form of expressive arts therapy, is an\neffective means to foster family communication. Yet, the integration of\ngenerative AI as expressive materials in therapeutic storymaking remains\nunderexplored. And there is a lack of HCI implications on how to support\nfamilies and therapists in this context. Addressing this, our study involved\nfive weeks of storymaking sessions with seven families guided by a professional\ntherapist. In these sessions, the families used both traditional art-making\nmaterials and image-based generative AI to create and evolve their family\nstories. Via the rich empirical data and commentaries from four expert\ntherapists, we contextualize how families creatively melded AI and traditional\nexpressive materials to externalize their ideas and feelings. Through the lens\nof Expressive Therapies Continuum (ETC), we characterize the therapeutic\nimplications of AI as expressive materials. Desirable interaction qualities to\nsupport children, parents, and therapists are distilled for future HCI\nresearch.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "to appear at ACM CHI '24",
    "pdf_url": "http://arxiv.org/pdf/2402.06472v1",
    "published_date": "2024-02-09 15:25:36 UTC",
    "updated_date": "2024-02-09 15:25:36 UTC"
  },
  {
    "arxiv_id": "2402.06457v2",
    "title": "V-STaR: Training Verifiers for Self-Taught Reasoners",
    "authors": [
      "Arian Hosseini",
      "Xingdi Yuan",
      "Nikolay Malkin",
      "Aaron Courville",
      "Alessandro Sordoni",
      "Rishabh Agarwal"
    ],
    "abstract": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR, iteratively fine-tune LLMs on self-generated solutions to improve their\nproblem-solving ability. However, these approaches discard the large amounts of\nincorrect solutions generated during this process, potentially neglecting\nvaluable information in such solutions. To address this shortcoming, we propose\nV-STaR that utilizes both the correct and incorrect solutions generated during\nthe self-improvement process to train a verifier using DPO that judges\ncorrectness of model-generated solutions. This verifier is used at inference\ntime to select one solution among many candidate solutions. Running V-STaR for\nmultiple iterations results in progressively better reasoners and verifiers,\ndelivering a 4% to 17% test accuracy improvement over existing self-improvement\nand verification approaches on common code generation and math reasoning\nbenchmarks with LLaMA2 models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06457v2",
    "published_date": "2024-02-09 15:02:56 UTC",
    "updated_date": "2024-08-14 02:41:48 UTC"
  },
  {
    "arxiv_id": "2402.06402v1",
    "title": "Hierarchical Transformers are Efficient Meta-Reinforcement Learners",
    "authors": [
      "Gresa Shala",
      "André Biedenkapp",
      "Josif Grabocka"
    ],
    "abstract": "We introduce Hierarchical Transformers for Meta-Reinforcement Learning\n(HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims\nto address the challenge of enabling reinforcement learning agents to perform\neffectively in previously unseen tasks. We demonstrate how past episodes serve\nas a rich source of information, which our model effectively distills and\napplies to new contexts. Our learned algorithm is capable of outperforming the\nprevious state-of-the-art and provides more efficient meta-training while\nsignificantly improving generalization capabilities. Experimental results,\nobtained across various simulated tasks of the Meta-World Benchmark, indicate a\nsignificant improvement in learning efficiency and adaptability compared to the\nstate-of-the-art on a variety of tasks. Our approach not only enhances the\nagent's ability to generalize from limited data but also paves the way for more\nrobust and versatile AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06402v1",
    "published_date": "2024-02-09 13:40:11 UTC",
    "updated_date": "2024-02-09 13:40:11 UTC"
  },
  {
    "arxiv_id": "2402.06397v1",
    "title": "Finding hardness reductions automatically using SAT solvers",
    "authors": [
      "Helena Bergold",
      "Manfred Scheucher",
      "Felix Schröder"
    ],
    "abstract": "In this article, we show that the completion problem, i.e. the decision\nproblem whether a partial structure can be completed to a full structure, is\nNP-complete for many combinatorial structures. While the gadgets for most\nreductions in literature are found by hand, we present an algorithm to\nconstruct gadgets in a fully automated way. Using our framework which is based\non SAT, we present the first thorough study of the completion problem on sign\nmappings with forbidden substructures by classifying thousands of structures\nfor which the completion problem is NP-complete. Our list in particular\nincludes interior triple systems, which were introduced by Knuth towards an\naxiomatization of planar point configurations. Last but not least, we give an\ninfinite family of structures generalizing interior triple system to higher\ndimensions for which the completion problem is NP-complete.",
    "categories": [
      "cs.CC",
      "cs.AI",
      "cs.DS",
      "cs.LO",
      "math.CO"
    ],
    "primary_category": "cs.CC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06397v1",
    "published_date": "2024-02-09 13:29:44 UTC",
    "updated_date": "2024-02-09 13:29:44 UTC"
  },
  {
    "arxiv_id": "2402.06389v1",
    "title": "Human Aesthetic Preference-Based Large Text-to-Image Model Personalization: Kandinsky Generation as an Example",
    "authors": [
      "Aven-Le Zhou",
      "Yu-Ao Wang",
      "Wei Wu",
      "Kang Zhang"
    ],
    "abstract": "With the advancement of neural generative capabilities, the art community has\nactively embraced GenAI (generative artificial intelligence) for creating\npainterly content. Large text-to-image models can quickly generate\naesthetically pleasing outcomes. However, the process can be non-deterministic\nand often involves tedious trial-and-error, as users struggle with formulating\neffective prompts to achieve their desired results. This paper introduces a\nprompting-free generative approach that empowers users to automatically\ngenerate personalized painterly content that incorporates their aesthetic\npreferences in a customized artistic style. This approach involves utilizing\n``semantic injection'' to customize an artist model in a specific artistic\nstyle, and further leveraging a genetic algorithm to optimize the prompt\ngeneration process through real-time iterative human feedback. By solely\nrelying on the user's aesthetic evaluation and preference for the artist\nmodel-generated images, this approach creates the user a personalized model\nthat encompasses their aesthetic preferences and the customized artistic style.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.06389v1",
    "published_date": "2024-02-09 13:11:19 UTC",
    "updated_date": "2024-02-09 13:11:19 UTC"
  },
  {
    "arxiv_id": "2402.06388v3",
    "title": "Convergence of a L2 regularized Policy Gradient Algorithm for the Multi Armed Bandit",
    "authors": [
      "Stefana Anita",
      "Gabriel Turinici"
    ],
    "abstract": "Although Multi Armed Bandit (MAB) on one hand and the policy gradient\napproach on the other hand are among the most used frameworks of Reinforcement\nLearning, the theoretical properties of the policy gradient algorithm used for\nMAB have not been given enough attention. We investigate in this work the\nconvergence of such a procedure for the situation when a $L2$ regularization\nterm is present jointly with the 'softmax' parametrization. We prove\nconvergence under appropriate technical hypotheses and test numerically the\nprocedure including situations beyond the theoretical setting. The tests show\nthat a time dependent regularized procedure can improve over the canonical\napproach especially when the initial guess is far from the solution.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.DS",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06388v3",
    "published_date": "2024-02-09 13:10:04 UTC",
    "updated_date": "2024-11-26 09:54:59 UTC"
  },
  {
    "arxiv_id": "2402.06377v1",
    "title": "High-Precision Geosteering via Reinforcement Learning and Particle Filters",
    "authors": [
      "Ressi Bonti Muhammad",
      "Apoorv Srivastava",
      "Sergey Alyaev",
      "Reidar Brumer Bratvold",
      "Daniel M. Tartakovsky"
    ],
    "abstract": "Geosteering, a key component of drilling operations, traditionally involves\nmanual interpretation of various data sources such as well-log data. This\nintroduces subjective biases and inconsistent procedures. Academic attempts to\nsolve geosteering decision optimization with greedy optimization and\nApproximate Dynamic Programming (ADP) showed promise but lacked adaptivity to\nrealistic diverse scenarios. Reinforcement learning (RL) offers a solution to\nthese challenges, facilitating optimal decision-making through reward-based\niterative learning. State estimation methods, e.g., particle filter (PF),\nprovide a complementary strategy for geosteering decision-making based on\nonline information. We integrate an RL-based geosteering with PF to address\nrealistic geosteering scenarios. Our framework deploys PF to process real-time\nwell-log data to estimate the location of the well relative to the\nstratigraphic layers, which then informs the RL-based decision-making process.\nWe compare our method's performance with that of using solely either RL or PF.\nOur findings indicate a synergy between RL and PF in yielding optimized\ngeosteering decisions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.geo-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "40 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.06377v1",
    "published_date": "2024-02-09 12:54:34 UTC",
    "updated_date": "2024-02-09 12:54:34 UTC"
  },
  {
    "arxiv_id": "2402.06360v1",
    "title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models",
    "authors": [
      "Peiyuan Gong",
      "Jiamian Li",
      "Jiaxin Mao"
    ],
    "abstract": "Collaborative search supports multiple users working together to accomplish a\nspecific search task. Research has found that designing lightweight\ncollaborative search plugins within instant messaging platforms aligns better\nwith users' collaborative habits. However, due to the complexity of multi-user\ninteraction scenarios, it is challenging to implement a fully functioning\nlightweight collaborative search system. Therefore, previous studies on\nlightweight collaborative search had to rely on the Wizard of Oz paradigm. In\nrecent years, large language models (LLMs) have been demonstrated to interact\nnaturally with users and achieve complex information-seeking tasks through\nLLM-based agents. Hence, to better support the research in collaborative\nsearch, in this demo, we propose CoSearchAgent, a lightweight collaborative\nsearch agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that\ncan support collaborative search during multi-party conversations on this\nplatform. Equipped with the capacity to understand the queries and context in\nmulti-user conversations and the ability to search the Web for relevant\ninformation via APIs, CoSearchAgent can respond to user queries with answers\ngrounded on the relevant search results. It can also ask clarifying questions\nwhen the information needs are unclear. The proposed CoSearchAgent is highly\nflexible and would be useful for supporting further research on collaborative\nsearch. The code and demo video are accessible.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "4 pages, demo",
    "pdf_url": "http://arxiv.org/pdf/2402.06360v1",
    "published_date": "2024-02-09 12:10:00 UTC",
    "updated_date": "2024-02-09 12:10:00 UTC"
  },
  {
    "arxiv_id": "2402.06359v1",
    "title": "Modelling Human Values for AI Reasoning",
    "authors": [
      "Nardine Osman",
      "Mark d'Inverno"
    ],
    "abstract": "One of today's most significant societal challenges is building AI systems\nwhose behaviour, or the behaviour it enables within communities of interacting\nagents (human and artificial), aligns with human values. To address this\nchallenge, we detail a formal model of human values for their explicit\ncomputational representation. To our knowledge, this has not been attempted as\nyet, which is surprising given the growing volume of research integrating\nvalues within AI. Taking as our starting point the wealth of research\ninvestigating the nature of human values from social psychology over the last\nfew decades, we set out to provide such a formal model. We show how this model\ncan provide the foundational apparatus for AI-based reasoning over values, and\ndemonstrate its applicability in real-world use cases. We illustrate how our\nmodel captures the key ideas from social psychology research and propose a\nroadmap for future integrated, and interdisciplinary, research into human\nvalues in AI. The ability to automatically reason over values not only helps\naddress the value alignment problem but also facilitates the design of AI\nsystems that can support individuals and communities in making more informed,\nvalue-aligned decisions. More and more, individuals and organisations are\nmotivated to understand their values more explicitly and explore whether their\nbehaviours and attitudes properly reflect them. Our work on modelling human\nvalues will enable AI systems to be designed and deployed to meet this growing\nneed.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "68T01",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06359v1",
    "published_date": "2024-02-09 12:08:49 UTC",
    "updated_date": "2024-02-09 12:08:49 UTC"
  },
  {
    "arxiv_id": "2402.06334v1",
    "title": "ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs",
    "authors": [
      "Fernando Ferraretto",
      "Thiago Laitz",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "abstract": "ExaRanker recently introduced an approach to training information retrieval\n(IR) models, incorporating natural language explanations as additional labels.\nThe method addresses the challenge of limited labeled examples, leading to\nimprovements in the effectiveness of IR models. However, the initial results\nwere based on proprietary language models such as GPT-3.5, which posed\nconstraints on dataset size due to its cost and data privacy. In this paper, we\nintroduce ExaRanker-Open, where we adapt and explore the use of open-source\nlanguage models to generate explanations. The method has been tested using\ndifferent LLMs and datasets sizes to better comprehend the effective\ncontribution of data augmentation. Our findings reveal that incorporating\nexplanations consistently enhances neural rankers, with benefits escalating as\nthe LLM size increases. Notably, the data augmentation method proves\nadvantageous even with large datasets, as evidenced by ExaRanker surpassing the\ntarget baseline by 0.6 nDCG@10 points in our study. To encourage further\nadvancements by the research community, we have open-sourced both the code and\ndatasets at https://github.com/unicamp-dl/ExaRanker.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06334v1",
    "published_date": "2024-02-09 11:23:14 UTC",
    "updated_date": "2024-02-09 11:23:14 UTC"
  },
  {
    "arxiv_id": "2402.06326v2",
    "title": "Prompt Learning on Temporal Interaction Graphs",
    "authors": [
      "Xi Chen",
      "Siwei Zhang",
      "Yun Xiong",
      "Xixi Wu",
      "Jiawei Zhang",
      "Xiangguo Sun",
      "Yao Zhang",
      "Feng Zhao",
      "Yulin Kang"
    ],
    "abstract": "Temporal Interaction Graphs (TIGs) are widely utilized to represent\nreal-world systems. To facilitate representation learning on TIGs, researchers\nhave proposed a series of TIG models. However, these models are still facing\ntwo tough gaps between the pre-training and downstream predictions in their\n``pre-train, predict'' training paradigm. First, the temporal discrepancy\nbetween the pre-training and inference data severely undermines the models'\napplicability in distant future predictions on the dynamically evolving data.\nSecond, the semantic divergence between pretext and downstream tasks hinders\ntheir practical applications, as they struggle to align with their learning and\nprediction capabilities across application scenarios.\n  Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight\nmechanism for model generalization. Applying this paradigm is a potential\nsolution to solve the aforementioned challenges. However, the adaptation of\nthis paradigm to TIGs is not straightforward. The application of prompting in\nstatic graph contexts falls short in temporal settings due to a lack of\nconsideration for time-sensitive dynamics and a deficiency in expressive power.\nTo address this issue, we introduce Temporal Interaction Graph Prompting\n(TIGPrompt), a versatile framework that seamlessly integrates with TIG models,\nbridging both the temporal and semantic gaps. In detail, we propose a temporal\nprompt generator to offer temporally-aware prompts for different tasks. These\nprompts stand out for their minimalistic design, relying solely on the tuning\nof the prompt generator with very little supervision data. To cater to varying\ncomputational resource demands, we propose an extended ``pre-train,\nprompt-based fine-tune'' paradigm, offering greater flexibility. Through\nextensive experiments, the TIGPrompt demonstrates the SOTA performance and\nremarkable efficiency advantages.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.06326v2",
    "published_date": "2024-02-09 11:06:20 UTC",
    "updated_date": "2024-03-06 05:34:12 UTC"
  },
  {
    "arxiv_id": "2402.06304v1",
    "title": "A New Approach to Voice Authenticity",
    "authors": [
      "Nicolas M. Müller",
      "Piotr Kawa",
      "Shen Hu",
      "Matthias Neu",
      "Jennifer Williams",
      "Philip Sperl",
      "Konstantin Böttinger"
    ],
    "abstract": "Voice faking, driven primarily by recent advances in text-to-speech (TTS)\nsynthesis technology, poses significant societal challenges. Currently, the\nprevailing assumption is that unaltered human speech can be considered genuine,\nwhile fake speech comes from TTS synthesis. We argue that this binary\ndistinction is oversimplified. For instance, altered playback speeds can be\nused for malicious purposes, like in the 'Drunken Nancy Pelosi' incident.\nSimilarly, editing of audio clips can be done ethically, e.g., for brevity or\nsummarization in news reporting or podcasts, but editing can also create\nmisleading narratives. In this paper, we propose a conceptual shift away from\nthe binary paradigm of audio being either 'fake' or 'real'. Instead, our focus\nis on pinpointing 'voice edits', which encompass traditional modifications like\nfilters and cuts, as well as TTS synthesis and VC systems. We delineate 6\ncategories and curate a new challenge dataset rooted in the M-AILABS corpus,\nfor which we present baseline detection systems. And most importantly, we argue\nthat merely categorizing audio as fake or real is a dangerous\nover-simplification that will fail to move the field of speech technology\nforward.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06304v1",
    "published_date": "2024-02-09 10:34:01 UTC",
    "updated_date": "2024-02-09 10:34:01 UTC"
  },
  {
    "arxiv_id": "2402.18589v1",
    "title": "Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers",
    "authors": [
      "Miloš Košprdić",
      "Adela Ljajić",
      "Bojana Bašaragin",
      "Darija Medvecki",
      "Nikola Milošević"
    ],
    "abstract": "In this paper, we present the current progress of the project Verif.ai, an\nopen-source scientific generative question-answering system with referenced and\nverified answers. The components of the system are (1) an information retrieval\nsystem combining semantic and lexical search techniques over scientific papers\n(PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and\ngenerating answers with references to the papers from which the claim was\nderived, and (3) a verification engine that cross-checks the generated claim\nand the abstract or paper from which the claim was derived, verifying whether\nthere may have been any hallucinations in generating the claim. We are\nreinforcing the generative model by providing the abstract in context, but in\naddition, an independent set of methods and models are verifying the answer and\nchecking for hallucinations. Therefore, we believe that by using our method, we\ncan make scientists more productive, while building trust in the use of\ngenerative language models in scientific environments, where hallucinations and\nmisinformation cannot be tolerated.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted as a short paper at The Sixteenth International Conference\n  on Evolving Internet (INTERNET 2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.18589v1",
    "published_date": "2024-02-09 10:25:01 UTC",
    "updated_date": "2024-02-09 10:25:01 UTC"
  },
  {
    "arxiv_id": "2402.06299v1",
    "title": "A Functional Analysis Approach to Symbolic Regression",
    "authors": [
      "Kirill Antonov",
      "Roman Kalkreuth",
      "Kaifeng Yang",
      "Thomas Bäck",
      "Niki van Stein",
      "Anna V Kononova"
    ],
    "abstract": "Symbolic regression (SR) poses a significant challenge for randomized search\nheuristics due to its reliance on the synthesis of expressions for input-output\nmappings. Although traditional genetic programming (GP) algorithms have\nachieved success in various domains, they exhibit limited performance when\ntree-based representations are used for SR. To address these limitations, we\nintroduce a novel SR approach called Fourier Tree Growing (FTG) that draws\ninsights from functional analysis. This new perspective enables us to perform\noptimization directly in a different space, thus avoiding intricate symbolic\nexpressions. Our proposed algorithm exhibits significant performance\nimprovements over traditional GP methods on a range of classical\none-dimensional benchmarking problems. To identify and explain limiting factors\nof GP and FTG, we perform experiments on a large-scale polynomials benchmark\nwith high-order polynomials up to degree 100. To the best of the authors'\nknowledge, this work represents the pioneering application of functional\nanalysis in addressing SR problems. The superior performance of the proposed\nalgorithm and insights into the limitations of GP open the way for further\nadvancing GP for SR and related areas of explainable machine learning.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "14 pages, 3 figures. Submitted to Genetic and Evolutionary\n  Computation Conference (GECCO-2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.06299v1",
    "published_date": "2024-02-09 10:24:47 UTC",
    "updated_date": "2024-02-09 10:24:47 UTC"
  },
  {
    "arxiv_id": "2402.06287v3",
    "title": "AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems",
    "authors": [
      "Clara Punzi",
      "Roberto Pellungrini",
      "Mattia Setzu",
      "Fosca Giannotti",
      "Dino Pedreschi"
    ],
    "abstract": "Everyday we increasingly rely on machine learning models to automate and\nsupport high-stake tasks and decisions. This growing presence means that humans\nare now constantly interacting with machine learning-based systems, training\nand using models everyday. Several different techniques in computer science\nliterature account for the human interaction with machine learning systems, but\ntheir classification is sparse and the goals varied. This survey proposes a\ntaxonomy of Hybrid Decision Making Systems, providing both a conceptual and\ntechnical framework for understanding how current computer science literature\nmodels interaction between humans and machines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06287v3",
    "published_date": "2024-02-09 09:54:01 UTC",
    "updated_date": "2025-03-07 14:20:58 UTC"
  },
  {
    "arxiv_id": "2402.10948v2",
    "title": "Zero-shot Explainable Mental Health Analysis on Social Media by Incorporating Mental Scales",
    "authors": [
      "Wenyu Li",
      "Yinuo Zhu",
      "Xin Lin",
      "Ming Li",
      "Ziyue Jiang",
      "Ziqian Zeng"
    ],
    "abstract": "Traditional discriminative approaches in mental health analysis are known for\ntheir strong capacity but lack interpretability and demand large-scale\nannotated data. The generative approaches, such as those based on large\nlanguage models (LLMs), have the potential to get rid of heavy annotations and\nprovide explanations but their capabilities still fall short compared to\ndiscriminative approaches, and their explanations may be unreliable due to the\nfact that the generation of explanation is a black-box process. Inspired by the\npsychological assessment practice of using scales to evaluate mental states,\nour method which is called Mental Analysis by Incorporating Mental Scales\n(MAIMS), incorporates two procedures via LLMs. First, the patient completes\nmental scales, and second, the psychologist interprets the collected\ninformation from the mental scales and makes informed decisions. Experimental\nresults show that MAIMS outperforms other zero-shot methods. MAIMS can generate\nmore rigorous explanation based on the outputs of mental scales",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages,2 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.10948v2",
    "published_date": "2024-02-09 09:44:06 UTC",
    "updated_date": "2024-03-15 02:02:02 UTC"
  },
  {
    "arxiv_id": "2402.06264v3",
    "title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education",
    "authors": [
      "Unggi Lee",
      "Minji Jeon",
      "Yunseo Lee",
      "Gyuri Byun",
      "Yoorim Son",
      "Jaeyoon Shin",
      "Hongkyu Ko",
      "Hyeoncheol Kim"
    ],
    "abstract": "Despite the development of various AI systems to support learning in various\ndomains, AI assistance for art appreciation education has not been extensively\nexplored. Art appreciation, often perceived as an unfamiliar and challenging\nendeavor for most students, can be more accessible with a generative AI enabled\nconversation partner that provides tailored questions and encourages the\naudience to deeply appreciate artwork. This study explores the application of\nmultimodal large language models (MLLMs) in art appreciation education, with a\nfocus on developing LLaVA-Docent, a model designed to serve as a personal tutor\nfor art appreciation. Our approach involved design and development research,\nfocusing on iterative enhancement to design and develop the application to\nproduce a functional MLLM-enabled chatbot along with a data design framework\nfor art appreciation education. To that end, we established a virtual dialogue\ndataset that was generated by GPT-4, which was instrumental in training our\nMLLM, LLaVA-Docent. The performance of LLaVA-Docent was evaluated by\nbenchmarking it against alternative settings and revealed its distinct\nstrengths and weaknesses. Our findings highlight the efficacy of the MMLM-based\npersonalized art appreciation chatbot and demonstrate its applicability for a\nnovel approach in which art appreciation is taught and experienced.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "37 pages, 4 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.06264v3",
    "published_date": "2024-02-09 09:25:18 UTC",
    "updated_date": "2024-09-18 00:59:58 UTC"
  },
  {
    "arxiv_id": "2402.06262v2",
    "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference",
    "authors": [
      "Siyu Ren",
      "Kenny Q. Zhu"
    ],
    "abstract": "Despite the recent success associated with Large Language Models (LLMs), they\nare notably cost-prohibitive to deploy in resource-constrained environments due\nto their excessive memory and computational demands. In addition to model\nparameters, the key-value cache is also stored in GPU memory, growing linearly\nwith batch size and sequence length. As a remedy, recent works have proposed\nvarious eviction policies for maintaining the overhead of key-value cache under\na given budget. This paper embarks on the efficacy of existing eviction\npolicies in terms of importance score calculation and eviction scope\nconstruction. We identify the deficiency of prior policies in these two aspects\nand introduce RoCo, a robust cache omission policy based on temporal attention\nscores and robustness measures. Extensive experimentation spanning prefilling\nand auto-regressive decoding stages validates the superiority of RoCo. Finally,\nwe release EasyKV, a versatile software package dedicated to user-friendly\nkey-value constrained generative inference. Code available at\nhttps://github.com/DRSY/EasyKV.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06262v2",
    "published_date": "2024-02-09 09:20:59 UTC",
    "updated_date": "2024-02-17 10:08:14 UTC"
  },
  {
    "arxiv_id": "2402.06255v4",
    "title": "Fight Back Against Jailbreaking via Prompt Adversarial Tuning",
    "authors": [
      "Yichuan Mo",
      "Yuji Wang",
      "Zeming Wei",
      "Yisen Wang"
    ],
    "abstract": "While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06255v4",
    "published_date": "2024-02-09 09:09:39 UTC",
    "updated_date": "2024-10-31 12:24:14 UTC"
  },
  {
    "arxiv_id": "2402.06700v4",
    "title": "Entropy-Regularized Token-Level Policy Optimization for Language Agent Reinforcement",
    "authors": [
      "Muning Wen",
      "Junwei Liao",
      "Cheng Deng",
      "Jun Wang",
      "Weinan Zhang",
      "Ying Wen"
    ],
    "abstract": "Large Language Models (LLMs) have shown promise as intelligent agents in\ninteractive decision-making tasks. Traditional approaches often depend on\nmeticulously designed prompts, high-quality examples, or additional reward\nmodels for in-context learning, supervised fine-tuning, or RLHF. Reinforcement\nlearning (RL) presents a dynamic alternative for LLMs to overcome these\ndependencies by engaging directly with task-specific environments. Nonetheless,\nit faces significant hurdles: 1) instability stemming from the exponentially\nvast action space requiring exploration; 2) challenges in assigning token-level\ncredit based on action-level reward signals, resulting in discord between\nmaximizing rewards and accurately modeling corpus data. In response to these\nchallenges, we introduce Entropy-Regularized Token-level Policy Optimization\n(ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the\ntoken level. At the heart of ETPO is our novel per-token soft Bellman update,\ndesigned to harmonize the RL process with the principles of language modeling.\nThis methodology decomposes the Q-function update from a coarse action-level\nview to a more granular token-level perspective, backed by theoretical proof of\noptimization consistency. Crucially, this decomposition renders linear time\ncomplexity in action exploration. We assess the effectiveness of ETPO within a\nsimulated environment that models data science code generation as a series of\nmulti-step interactive tasks; results underline ETPO's potential as a robust\nmethod for refining the interactive decision-making capabilities of language\nagents. For a more detailed preliminary work describing our motivation for\ntoken-level decomposition and applying it in PPO methods, please refer to\narXiv:2405.15821.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06700v4",
    "published_date": "2024-02-09 07:45:26 UTC",
    "updated_date": "2024-06-06 12:29:23 UTC"
  },
  {
    "arxiv_id": "2402.06229v1",
    "title": "Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants",
    "authors": [
      "Bhavya Chopra",
      "Yasharth Bajpai",
      "Param Biyani",
      "Gustavo Soares",
      "Arjun Radhakrishna",
      "Chris Parnin",
      "Sumit Gulwani"
    ],
    "abstract": "The widespread availability of Large Language Models (LLMs) within Integrated\nDevelopment Environments (IDEs) has led to their speedy adoption.\nConversational interactions with LLMs enable programmers to obtain natural\nlanguage explanations for various software development tasks. However, LLMs\noften leap to action without sufficient context, giving rise to implicit\nassumptions and inaccurate responses. Conversations between developers and LLMs\nare primarily structured as question-answer pairs, where the developer is\nresponsible for asking the the right questions and sustaining conversations\nacross multiple turns. In this paper, we draw inspiration from interaction\npatterns and conversation analysis -- to design Robin, an enhanced\nconversational AI-assistant for debugging. Through a within-subjects user study\nwith 12 industry professionals, we find that equipping the LLM to -- (1)\nleverage the insert expansion interaction pattern, (2) facilitate turn-taking,\nand (3) utilize debugging workflows -- leads to lowered conversation barriers,\neffective fault localization, and 5x improvement in bug resolution rates.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.HC",
    "comment": "7 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.06229v1",
    "published_date": "2024-02-09 07:44:27 UTC",
    "updated_date": "2024-02-09 07:44:27 UTC"
  },
  {
    "arxiv_id": "2402.06204v1",
    "title": "The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate",
    "authors": [
      "Juhyun Oh",
      "Eunsu Kim",
      "Inha Cha",
      "Alice Oh"
    ],
    "abstract": "This paper explores the assumption that Large Language Models (LLMs) skilled\nin generation tasks are equally adept as evaluators. We assess the performance\nof three LLMs and one open-source LM in Question-Answering (QA) and evaluation\ntasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a\nsignificant disparity, with LLMs exhibiting lower performance in evaluation\ntasks compared to generation tasks. Intriguingly, we discover instances of\nunfaithful evaluation where models accurately evaluate answers in areas where\nthey lack competence, underscoring the need to examine the faithfulness and\ntrustworthiness of LLMs as evaluators. This study contributes to the\nunderstanding of \"the Generative AI Paradox\" (West et al., 2023), highlighting\na need to explore the correlation between generative excellence and evaluation\nproficiency, and the necessity to scrutinize the faithfulness aspect in model\nevaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06204v1",
    "published_date": "2024-02-09 06:16:08 UTC",
    "updated_date": "2024-02-09 06:16:08 UTC"
  },
  {
    "arxiv_id": "2402.06196v3",
    "title": "Large Language Models: A Survey",
    "authors": [
      "Shervin Minaee",
      "Tomas Mikolov",
      "Narjes Nikzad",
      "Meysam Chenaghlu",
      "Richard Socher",
      "Xavier Amatriain",
      "Jianfeng Gao"
    ],
    "abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their\nstrong performance on a wide range of natural language tasks, since the release\nof ChatGPT in November 2022. LLMs' ability of general-purpose language\nunderstanding and generation is acquired by training billions of model's\nparameters on massive amounts of text data, as predicted by scaling laws\n\\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while\nvery recent, is evolving rapidly in many different ways. In this paper, we\nreview some of the most prominent LLMs, including three popular LLM families\n(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and\nlimitations. We also give an overview of techniques developed to build, and\naugment LLMs. We then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation metrics, and\ncompare the performance of several popular LLMs on a set of representative\nbenchmarks. Finally, we conclude the paper by discussing open challenges and\nfuture research directions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06196v3",
    "published_date": "2024-02-09 05:37:09 UTC",
    "updated_date": "2025-03-23 14:51:01 UTC"
  },
  {
    "arxiv_id": "2402.06188v2",
    "title": "A self-supervised framework for learning whole slide representations",
    "authors": [
      "Xinhai Hou",
      "Cheng Jiang",
      "Akhil Kondepudi",
      "Yiwei Lyu",
      "Asadur Chowdury",
      "Honglak Lee",
      "Todd C. Hollon"
    ],
    "abstract": "Whole slide imaging is fundamental to biomedical microscopy and computational\npathology. Previously, learning representations for gigapixel-sized whole slide\nimages (WSIs) has relied on multiple instance learning with weak labels, which\ndo not annotate the diverse morphologic features and spatial heterogeneity of\nWSIs. A high-quality self-supervised learning method for WSIs would provide\ntransferable visual representations for downstream computational pathology\ntasks, without the need for dense annotations. We present Slide Pre-trained\nTransformers (SPT) for gigapixel-scale self-supervision of WSIs. Treating WSI\npatches as tokens, SPT combines data transformation strategies from language\nand vision modeling into a general and unified framework to generate views of\nWSIs for self-supervised pretraining. SPT leverages the inherent regional\nheterogeneity, histologic feature variability, and information redundancy\nwithin WSIs to learn high-quality whole slide representations. We benchmark SPT\nvisual representations on five diagnostic tasks across three biomedical\nmicroscopy datasets. SPT significantly outperforms baselines for\nhistopathologic diagnosis, cancer subtyping, and genetic mutation prediction.\nFinally, we demonstrate that SPT consistently improves whole slide\nrepresentations when using off-the-shelf, in-domain, and foundational patch\nencoders for whole slide multiple instance learning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.06188v2",
    "published_date": "2024-02-09 05:05:28 UTC",
    "updated_date": "2024-05-23 19:23:53 UTC"
  },
  {
    "arxiv_id": "2402.06187v4",
    "title": "Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss",
    "authors": [
      "Ruijie Zheng",
      "Yongyuan Liang",
      "Xiyao Wang",
      "Shuang Ma",
      "Hal Daumé III",
      "Huazhe Xu",
      "John Langford",
      "Praveen Palanisamy",
      "Kalyan Shankar Basu",
      "Furong Huang"
    ],
    "abstract": "We present Premier-TACO, a multitask feature representation learning approach\ndesigned to improve few-shot policy learning efficiency in sequential\ndecision-making tasks. Premier-TACO leverages a subset of multitask offline\ndatasets for pretraining a general feature representation, which captures\ncritical environmental dynamics and is fine-tuned using minimal expert\ndemonstrations. It advances the temporal action contrastive learning (TACO)\nobjective, known for state-of-the-art results in visual control tasks, by\nincorporating a novel negative example sampling strategy. This strategy is\ncrucial in significantly boosting TACO's computational efficiency, making\nlarge-scale multitask offline pretraining feasible. Our extensive empirical\nevaluation in a diverse set of continuous control benchmarks including Deepmind\nControl Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness\nin pretraining visual representations, significantly enhancing few-shot\nimitation learning of novel tasks. Our code, pretraining data, as well as\npretrained model checkpoints will be released at\nhttps://github.com/PremierTACO/premier-taco. Our project webpage is at\nhttps://premiertaco.github.io.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at Forty-first International Conference on Machine Learning\n  (ICML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.06187v4",
    "published_date": "2024-02-09 05:04:40 UTC",
    "updated_date": "2024-05-23 21:41:29 UTC"
  },
  {
    "arxiv_id": "2402.06185v1",
    "title": "Development and validation of an artificial intelligence model to accurately predict spinopelvic parameters",
    "authors": [
      "Edward S. Harake",
      "Joseph R. Linzey",
      "Cheng Jiang",
      "Rushikesh S. Joshi",
      "Mark M. Zaki",
      "Jaes C. Jones",
      "Siri S. Khalsa",
      "John H. Lee",
      "Zachary Wilseck",
      "Jacob R. Joseph",
      "Todd C. Hollon",
      "Paul Park"
    ],
    "abstract": "Objective. Achieving appropriate spinopelvic alignment has been shown to be\nassociated with improved clinical symptoms. However, measurement of spinopelvic\nradiographic parameters is time-intensive and interobserver reliability is a\nconcern. Automated measurement tools have the promise of rapid and consistent\nmeasurements, but existing tools are still limited by some degree of manual\nuser-entry requirements. This study presents a novel artificial intelligence\n(AI) tool called SpinePose that automatically predicts spinopelvic parameters\nwith high accuracy without the need for manual entry.\n  Methods. SpinePose was trained and validated on 761 sagittal whole-spine\nX-rays to predict sagittal vertical axis (SVA), pelvic tilt (PT), pelvic\nincidence (PI), sacral slope (SS), lumbar lordosis (LL), T1-pelvic angle\n(T1PA), and L1-pelvic angle (L1PA). A separate test set of 40 X-rays was\nlabeled by 4 reviewers, including fellowship-trained spine surgeons and a\nfellowship-trained radiologist with neuroradiology subspecialty certification.\nMedian errors relative to the most senior reviewer were calculated to determine\nmodel accuracy on test images. Intraclass correlation coefficients (ICC) were\nused to assess inter-rater reliability.\n  Results. SpinePose exhibited the following median (interquartile range)\nparameter errors: SVA: 2.2(2.3)mm, p=0.93; PT: 1.3(1.2){\\deg}, p=0.48; SS:\n1.7(2.2){\\deg}, p=0.64; PI: 2.2(2.1){\\deg}, p=0.24; LL: 2.6(4.0){\\deg}, p=0.89;\nT1PA: 1.1(0.9){\\deg}, p=0.42; and L1PA: 1.4(1.6){\\deg}, p=0.49. Model\npredictions also exhibited excellent reliability at all parameters (ICC:\n0.91-1.0).\n  Conclusions. SpinePose accurately predicted spinopelvic parameters with\nexcellent reliability comparable to fellowship-trained spine surgeons and\nneuroradiologists. Utilization of predictive AI tools in spinal imaging can\nsubstantially aid in patient selection and surgical planning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures, to appear in Journal of Neurosurgery: Spine",
    "pdf_url": "http://arxiv.org/pdf/2402.06185v1",
    "published_date": "2024-02-09 04:47:26 UTC",
    "updated_date": "2024-02-09 04:47:26 UTC"
  },
  {
    "arxiv_id": "2402.06178v3",
    "title": "MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models",
    "authors": [
      "Yixiao Zhang",
      "Yukara Ikemiya",
      "Gus Xia",
      "Naoki Murata",
      "Marco A. Martínez-Ramírez",
      "Wei-Hsiang Liao",
      "Yuki Mitsufuji",
      "Simon Dixon"
    ],
    "abstract": "Recent advances in text-to-music generation models have opened new avenues in\nmusical creativity. However, music generation usually involves iterative\nrefinements, and how to edit the generated music remains a significant\nchallenge. This paper introduces a novel approach to the editing of music\ngenerated by such models, enabling the modification of specific attributes,\nsuch as genre, mood and instrument, while maintaining other aspects unchanged.\nOur method transforms text editing to \\textit{latent space manipulation} while\nadding an extra constraint to enforce consistency. It seamlessly integrates\nwith existing pretrained text-to-music diffusion models without requiring\nadditional training. Experimental results demonstrate superior performance over\nboth zero-shot and certain supervised baselines in style and timbre transfer\nevaluations. Additionally, we showcase the practical applicability of our\napproach in real-world music editing scenarios.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.06178v3",
    "published_date": "2024-02-09 04:34:08 UTC",
    "updated_date": "2024-05-28 16:47:25 UTC"
  },
  {
    "arxiv_id": "2402.10946v3",
    "title": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
    "authors": [
      "Cheng Li",
      "Mengzhou Chen",
      "Jindong Wang",
      "Sunayana Sitaram",
      "Xing Xie"
    ],
    "abstract": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation. Code is released at\nhttps://github.com/Scarelette/CultureLLM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024; Code is at https://github.com/Scarelette/CultureLLM",
    "pdf_url": "http://arxiv.org/pdf/2402.10946v3",
    "published_date": "2024-02-09 04:02:43 UTC",
    "updated_date": "2024-12-03 06:52:34 UTC"
  },
  {
    "arxiv_id": "2402.06165v6",
    "title": "Learning Contrastive Feature Representations for Facial Action Unit Detection",
    "authors": [
      "Ziqiao Shang",
      "Bin Liu",
      "Fengmao Lv",
      "Fei Teng",
      "Tianrui Li"
    ],
    "abstract": "For the Facial Action Unit (AU) detection task, accurately capturing the\nsubtle facial differences between distinct AUs is essential for reliable\ndetection. Additionally, AU detection faces challenges from class imbalance and\nthe presence of noisy or false labels, which undermine detection accuracy. In\nthis paper, we introduce a novel contrastive learning framework aimed for AU\ndetection that incorporates both self-supervised and supervised signals,\nthereby enhancing the learning of discriminative features for accurate AU\ndetection. To tackle the class imbalance issue, we employ a negative sample\nre-weighting strategy that adjusts the step size of updating parameters for\nminority and majority class samples. Moreover, to address the challenges posed\nby noisy and false AU labels, we employ a sampling technique that encompasses\nthree distinct types of positive sample pairs. This enables us to inject\nself-supervised signals into the supervised signal, effectively mitigating the\nadverse effects of noisy labels. Our experimental assessments, conducted on\nfive widely-utilized benchmark datasets (BP4D, DISFA, BP4D+, GFT and\nAff-Wild2), underscore the superior performance of our approach compared to\nstate-of-the-art methods of AU detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "35 pages, 20 figures, submitted to Pattern Recognition (PR)",
    "pdf_url": "http://arxiv.org/pdf/2402.06165v6",
    "published_date": "2024-02-09 03:48:20 UTC",
    "updated_date": "2025-01-23 04:58:05 UTC"
  },
  {
    "arxiv_id": "2402.06158v1",
    "title": "Assortment Planning with Sponsored Products",
    "authors": [
      "Shaojie Tang",
      "Shuzhang Cai",
      "Jing Yuan",
      "Kai Han"
    ],
    "abstract": "In the rapidly evolving landscape of retail, assortment planning plays a\ncrucial role in determining the success of a business. With the rise of\nsponsored products and their increasing prominence in online marketplaces,\nretailers face new challenges in effectively managing their product assortment\nin the presence of sponsored products. Remarkably, previous research in\nassortment planning largely overlooks the existence of sponsored products and\ntheir potential impact on overall recommendation effectiveness. Instead, they\ncommonly make the simplifying assumption that all products are either organic\nor non-sponsored. This research gap underscores the necessity for a more\nthorough investigation of the assortment planning challenge when sponsored\nproducts are in play. We formulate the assortment planning problem in the\npresence of sponsored products as a combinatorial optimization task. The\nultimate objective is to compute an assortment plan that optimizes expected\nrevenue while considering the specific requirements of placing sponsored\nproducts strategically.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06158v1",
    "published_date": "2024-02-09 03:18:44 UTC",
    "updated_date": "2024-02-09 03:18:44 UTC"
  },
  {
    "arxiv_id": "2402.07945v1",
    "title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
    "authors": [
      "Runliang Niu",
      "Jindong Li",
      "Shiqi Wang",
      "Yali Fu",
      "Xiyu Hu",
      "Xueyuan Leng",
      "He Kong",
      "Yi Chang",
      "Qi Wang"
    ],
    "abstract": "Existing Large Language Models (LLM) can invoke a variety of tools and APIs\nto complete complex tasks. The computer, as the most powerful and universal\ntool, could potentially be controlled directly by a trained LLM agent. Powered\nby the computer, we can hopefully build a more generalized agent to assist\nhumans in various daily digital works. In this paper, we construct an\nenvironment for a Vision Language Model (VLM) agent to interact with a real\ncomputer screen. Within this environment, the agent can observe screenshots and\nmanipulate the Graphics User Interface (GUI) by outputting mouse and keyboard\nactions. We also design an automated control pipeline that includes planning,\nacting, and reflecting phases, guiding the agent to continuously interact with\nthe environment and complete multi-step tasks. Additionally, we construct the\nScreenAgent Dataset, which collects screenshots and action sequences when\ncompleting a variety of daily computer tasks. Finally, we trained a model,\nScreenAgent, which achieved computer control capabilities comparable to GPT-4V\nand demonstrated more precise UI positioning capabilities. Our attempts could\ninspire further research on building a generalist LLM agent. The code is\navailable at \\url{https://github.com/niuzaisheng/ScreenAgent}.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.07945v1",
    "published_date": "2024-02-09 02:33:45 UTC",
    "updated_date": "2024-02-09 02:33:45 UTC"
  },
  {
    "arxiv_id": "2402.06697v1",
    "title": "Feed-Forward Neural Networks as a Mixed-Integer Program",
    "authors": [
      "Navid Aftabi",
      "Nima Moradi",
      "Fatemeh Mahroo"
    ],
    "abstract": "Deep neural networks (DNNs) are widely studied in various applications. A DNN\nconsists of layers of neurons that compute affine combinations, apply nonlinear\noperations, and produce corresponding activations. The rectified linear unit\n(ReLU) is a typical nonlinear operator, outputting the max of its input and\nzero. In scenarios like max pooling, where multiple input values are involved,\na fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This\nformulation, with continuous variables representing unit outputs and binary\nvariables for ReLU activation, finds applications across diverse domains. This\nstudy explores the formulation of trained ReLU neurons as MIP and applies MIP\nmodels for training neural networks (NNs). Specifically, it investigates\ninteractions between MIP techniques and various NN architectures, including\nbinary DNNs (employing step activation functions) and binarized DNNs (with\nweights and activations limited to $-1,0,+1$). The research focuses on training\nand evaluating proposed approaches through experiments on handwritten digit\nclassification models. The comparative study assesses the performance of\ntrained ReLU NNs, shedding light on the effectiveness of MIP formulations in\nenhancing training processes for NNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06697v1",
    "published_date": "2024-02-09 02:23:37 UTC",
    "updated_date": "2024-02-09 02:23:37 UTC"
  },
  {
    "arxiv_id": "2402.09465v1",
    "title": "RLEEGNet: Integrating Brain-Computer Interfaces with Adaptive AI for Intuitive Responsiveness and High-Accuracy Motor Imagery Classification",
    "authors": [
      "Sriram V. C. Nallani",
      "Gautham Ramachandran"
    ],
    "abstract": "Current approaches to prosthetic control are limited by their reliance on\ntraditional methods, which lack real-time adaptability and intuitive\nresponsiveness. These limitations are particularly pronounced in assistive\ntechnologies designed for individuals with diverse cognitive states and motor\nintentions. In this paper, we introduce a framework that leverages\nReinforcement Learning (RL) with Deep Q-Networks (DQN) for classification\ntasks. Additionally, we present a preprocessing technique using the Common\nSpatial Pattern (CSP) for multiclass motor imagery (MI) classification in a\nOne-Versus-The-Rest (OVR) manner. The subsequent 'csp space' transformation\nretains the temporal dimension of EEG signals, crucial for extracting\ndiscriminative features. The integration of DQN with a 1D-CNN-LSTM architecture\noptimizes the decision-making process in real-time, thereby enhancing the\nsystem's adaptability to the user's evolving needs and intentions. We elaborate\non the data processing methods for two EEG motor imagery datasets. Our\ninnovative model, RLEEGNet, incorporates a 1D-CNN-LSTM architecture as the\nOnline Q-Network within the DQN, facilitating continuous adaptation and\noptimization of control strategies through feedback. This mechanism allows the\nsystem to learn optimal actions through trial and error, progressively\nimproving its performance. RLEEGNet demonstrates high accuracy in classifying\nMI-EEG signals, achieving as high as 100% accuracy in MI tasks across both the\nGigaScience (3-class) and BCI-IV-2a (4-class) datasets. These results highlight\nthe potential of combining DQN with a 1D-CNN-LSTM architecture to significantly\nenhance the adaptability and responsiveness of BCI systems.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "68T05"
    ],
    "primary_category": "eess.SP",
    "comment": "23 pages, 1 figure, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.09465v1",
    "published_date": "2024-02-09 02:03:13 UTC",
    "updated_date": "2024-02-09 02:03:13 UTC"
  },
  {
    "arxiv_id": "2402.06128v1",
    "title": "Rethinking Node-wise Propagation for Large-scale Graph Learning",
    "authors": [
      "Xunkai Li",
      "Jingyuan Ma",
      "Zhengyu Wu",
      "Daohan Su",
      "Wentao Zhang",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "abstract": "Scalable graph neural networks (GNNs) have emerged as a promising technique,\nwhich exhibits superior predictive performance and high running efficiency\nacross numerous large-scale graph-based web applications. However, (i) Most\nscalable GNNs tend to treat all nodes in graphs with the same propagation\nrules, neglecting their topological uniqueness; (ii) Existing node-wise\npropagation optimization strategies are insufficient on web-scale graphs with\nintricate topology, where a full portrayal of nodes' local properties is\nrequired. Intuitively, different nodes in web-scale graphs possess distinct\ntopological roles, and therefore propagating them indiscriminately or neglect\nlocal contexts may compromise the quality of node representations. This\nintricate topology in web-scale graphs cannot be matched by small-scale\nscenarios. To address the above issues, we propose \\textbf{A}daptive\n\\textbf{T}opology-aware \\textbf{P}ropagation (ATP), which reduces potential\nhigh-bias propagation and extracts structural patterns of each node in a\nscalable manner to improve running efficiency and predictive performance.\nRemarkably, ATP is crafted to be a plug-and-play node-wise propagation\noptimization strategy, allowing for offline execution independent of the graph\nlearning process in a new perspective. Therefore, this approach can be\nseamlessly integrated into most scalable GNNs while remain orthogonal to\nexisting node-wise propagation optimization strategies. Extensive experiments\non 12 datasets, including the most representative large-scale ogbn-papers100M,\nhave demonstrated the effectiveness of ATP. Specifically, ATP has proven to be\nefficient in improving the performance of prevalent scalable GNNs for\nsemi-supervised node classification while addressing redundant computational\ncosts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by WWW 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.06128v1",
    "published_date": "2024-02-09 01:19:47 UTC",
    "updated_date": "2024-02-09 01:19:47 UTC"
  },
  {
    "arxiv_id": "2402.06126v4",
    "title": "Learn To be Efficient: Build Structured Sparsity in Large Language Models",
    "authors": [
      "Haizhong Zheng",
      "Xiaoyan Bai",
      "Xueshen Liu",
      "Z. Morley Mao",
      "Beidi Chen",
      "Fan Lai",
      "Atul Prakash"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success with their\nbillion-level parameters, yet they incur high inference overheads. The\nemergence of activation sparsity in LLMs provides a natural approach to reduce\nthis cost by involving only parts of the parameters for inference. However,\nexisting methods only focus on utilizing this naturally formed activation\nsparsity in a post-training setting, overlooking the potential for further\namplifying this inherent sparsity. In this paper, we hypothesize that LLMs can\nlearn to be efficient by achieving more structured activation sparsity. To\nachieve this, we introduce a novel training algorithm, Learn-To-be-Efficient\n(LTE), designed to train efficiency-aware LLMs to learn to activate fewer\nneurons and achieve a better trade-off between sparsity and performance.\nFurthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based\nmodels, LTE can also be applied to LLMs like LLaMA using non-ReLU activations.\nExtensive evaluation on language understanding, language generation, and\ninstruction tuning tasks show that LTE consistently outperforms SOTA baselines.\nAlong with our hardware-aware custom kernel implementation, LTE reduces\nLLaMA2-7B inference latency by 25% at 50% sparsity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.06126v4",
    "published_date": "2024-02-09 01:18:16 UTC",
    "updated_date": "2024-12-12 05:28:56 UTC"
  },
  {
    "arxiv_id": "2402.06118v3",
    "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling",
    "authors": [
      "Siming Yan",
      "Min Bai",
      "Weifeng Chen",
      "Xiong Zhou",
      "Qixing Huang",
      "Li Erran Li"
    ],
    "abstract": "By combining natural language understanding, generation capabilities, and\nbreadth of knowledge of large language models with image perception, recent\nlarge vision language models (LVLMs) have shown unprecedented visual reasoning\ncapabilities. However, the generated text often suffers from inaccurate\ngrounding in the visual input, resulting in errors such as hallucination of\nnonexistent scene elements, missing significant parts of the scene, and\ninferring incorrect attributes of and relationships between objects. To address\nthese issues, we introduce a novel framework, ViGoR (Visual Grounding Through\nFine-Grained Reward Modeling) that utilizes fine-grained reward modeling to\nsignificantly enhance the visual grounding of LVLMs over pre-trained baselines.\nThis improvement is efficiently achieved using much cheaper human evaluations\ninstead of full supervisions, as well as automated methods. We show the\neffectiveness of our approach through a variety of evaluation methods and\nbenchmarks. Additionally, we released our human annotation\n(https://github.com/amazon-science/vigor) comprising 15,440 images and\ngenerated text pairs with fine-grained evaluations to contribute to related\nresearch in the community.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.06118v3",
    "published_date": "2024-02-09 01:00:14 UTC",
    "updated_date": "2024-10-13 14:06:12 UTC"
  },
  {
    "arxiv_id": "2402.06116v1",
    "title": "LLMs for Coding and Robotics Education",
    "authors": [
      "Peng Shu",
      "Huaqin Zhao",
      "Hanqi Jiang",
      "Yiwei Li",
      "Shaochen Xu",
      "Yi Pan",
      "Zihao Wu",
      "Zhengliang Liu",
      "Guoyu Lu",
      "Le Guan",
      "Gong Chen",
      "Xianqiao Wang Tianming Liu"
    ],
    "abstract": "Large language models and multimodal large language models have\nrevolutionized artificial intelligence recently. An increasing number of\nregions are now embracing these advanced technologies. Within this context,\nrobot coding education is garnering increasing attention. To teach young\nchildren how to code and compete in robot challenges, large language models are\nbeing utilized for robot code explanation, generation, and modification. In\nthis paper, we highlight an important trend in robot coding education. We test\nseveral mainstream large language models on both traditional coding tasks and\nthe more challenging task of robot code generation, which includes block\ndiagrams. Our results show that GPT-4V outperforms other models in all of our\ntests but struggles with generating block diagram images.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "20 pages, 6 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2402.06116v1",
    "published_date": "2024-02-09 00:58:57 UTC",
    "updated_date": "2024-02-09 00:58:57 UTC"
  },
  {
    "arxiv_id": "2402.06696v1",
    "title": "FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via Large Language Models",
    "authors": [
      "Ruiyang Qin",
      "Yuting Hu",
      "Zheyu Yan",
      "Jinjun Xiong",
      "Ahmed Abbasi",
      "Yiyu Shi"
    ],
    "abstract": "Neural Architecture Search (NAS) has become the de fecto tools in the\nindustry in automating the design of deep neural networks for various\napplications, especially those driven by mobile and edge devices with limited\ncomputing resources. The emerging large language models (LLMs), due to their\nprowess, have also been incorporated into NAS recently and show some promising\nresults. This paper conducts further exploration in this direction by\nconsidering three important design metrics simultaneously, i.e., model\naccuracy, fairness, and hardware deployment efficiency. We propose a novel\nLLM-based NAS framework, FL-NAS, in this paper, and show experimentally that\nFL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN\nmodels by orders-of-magnitude across almost all design considerations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ASP-DAC 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.06696v1",
    "published_date": "2024-02-09 00:49:03 UTC",
    "updated_date": "2024-02-09 00:49:03 UTC"
  },
  {
    "arxiv_id": "2402.06107v1",
    "title": "Multiple Instance Learning for Cheating Detection and Localization in Online Examinations",
    "authors": [
      "Yemeng Liu",
      "Jing Ren",
      "Jianshuo Xu",
      "Xiaomei Bai",
      "Roopdeep Kaur",
      "Feng Xia"
    ],
    "abstract": "The spread of the Coronavirus disease-2019 epidemic has caused many courses\nand exams to be conducted online. The cheating behavior detection model in\nexamination invigilation systems plays a pivotal role in guaranteeing the\nequality of long-distance examinations. However, cheating behavior is rare, and\nmost researchers do not comprehensively take into account features such as head\nposture, gaze angle, body posture, and background information in the task of\ncheating behavior detection. In this paper, we develop and present CHEESE, a\nCHEating detection framework via multiplE inStancE learning. The framework\nconsists of a label generator that implements weak supervision and a feature\nencoder to learn discriminative features. In addition, the framework combines\nbody posture and background features extracted by 3D convolution with eye gaze,\nhead posture and facial features captured by OpenFace 2.0. These features are\nfed into the spatio-temporal graph module by stitching to analyze the\nspatio-temporal changes in video clips to detect the cheating behaviors. Our\nexperiments on three datasets, UCF-Crime, ShanghaiTech and Online Exam\nProctoring (OEP), prove the effectiveness of our method as compared to the\nstate-of-the-art approaches, and obtain the frame-level AUC score of 87.58% on\nthe OEP dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "68T40, 68T45",
      "I.2.10; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.06107v1",
    "published_date": "2024-02-09 00:01:42 UTC",
    "updated_date": "2024-02-09 00:01:42 UTC"
  }
]
[
  {
    "arxiv_id": "2501.01576v3",
    "title": "Constructing and explaining machine learning models for chemistry: example of the exploration and design of boron-based Lewis acids",
    "authors": [
      "Juliette Fenogli",
      "Laurence Grimaud",
      "Rodolphe Vuilleumier"
    ],
    "abstract": "The integration of machine learning (ML) into chemistry offers transformative\npotential in the design of molecules with targeted properties. However, the\nfocus has often been on creating highly efficient predictive models, sometimes\nat the expense of interpretability. In this study, we leverage explainable AI\ntechniques to explore the rational design of boron-based Lewis acids, which\nplay a pivotal role in organic reactions due to their electron-ccepting\nproperties. Using Fluoride Ion Affinity as a proxy for Lewis acidity, we\ndeveloped interpretable ML models based on chemically meaningful descriptors,\nincluding ab initio computed features and substituent-based parameters derived\nfrom the Hammett linear free-energy relationship. By constraining the chemical\nspace to well-defined molecular scaffolds, we achieved highly accurate\npredictions (mean absolute error < 6 kJ/mol), surpassing conventional black-box\ndeep learning models in low-data regimes. Interpretability analyses of the\nmodels shed light on the origin of Lewis acidity in these compounds and\nidentified actionable levers to modulate it through the nature and positioning\nof substituents on the molecular scaffold. This work bridges ML and chemist's\nway of thinking, demonstrating how explainable models can inspire molecular\ndesign and enhance scientific understanding of chemical reactivity.",
    "categories": [
      "physics.chem-ph",
      "cs.AI"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "Main text is 14 pages, 7 figures, 1 scheme. Supporting information is\n  25 pages. For associated code and datasets, see\n  https://github.com/jfenogli/XAI_boron_LA",
    "pdf_url": "http://arxiv.org/pdf/2501.01576v3",
    "published_date": "2025-01-02 23:47:54 UTC",
    "updated_date": "2025-03-23 19:20:26 UTC"
  },
  {
    "arxiv_id": "2501.01540v1",
    "title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
    "authors": [
      "Kanishk Gandhi",
      "Michael Y. Li",
      "Lyle Goodyear",
      "Louise Li",
      "Aditi Bhaskar",
      "Mohammed Zaman",
      "Noah D. Goodman"
    ],
    "abstract": "Understanding the world and explaining it with scientific theories is a\ncentral aspiration of artificial intelligence research. Proposing theories,\ndesigning experiments to test them, and then revising them based on data are\nfundamental to scientific discovery. Despite the significant promise of\nLLM-based scientific agents, no benchmarks systematically test LLM's ability to\npropose scientific models, collect experimental data, and revise them in light\nof new data. We introduce BoxingGym, a benchmark with 10 environments for\nsystematically evaluating both experimental design (e.g. collecting data to\ntest a scientific theory) and model discovery (e.g. proposing and revising\nscientific theories). To enable tractable and quantitative evaluation, we\nimplement each environment as a generative probabilistic model with which a\nscientific agent can run interactive experiments. These probabilistic models\nare drawn from various real-world scientific domains ranging from psychology to\necology. To quantitatively evaluate a scientific agent's ability to collect\ninformative experimental data, we compute the expected information gain (EIG),\nan information-theoretic quantity which measures how much an experiment reduces\nuncertainty about the parameters of a generative model. A good scientific\ntheory is a concise and predictive explanation. Therefore, to quantitatively\nevaluate model discovery, we ask a scientific agent to explain their model and\nthen assess whether this explanation enables another scientific agent to make\nreliable predictions about this environment. In addition to this\nexplanation-based evaluation, we compute standard model evaluation metrics such\nas prediction errors. We find that current LLMs, such as GPT-4o, struggle with\nboth experimental design and model discovery. We find that augmenting the\nLLM-based agent with an explicit statistical model does not reliably improve\nthese results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "KG and MYL contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2501.01540v1",
    "published_date": "2025-01-02 21:15:57 UTC",
    "updated_date": "2025-01-02 21:15:57 UTC"
  },
  {
    "arxiv_id": "2501.01539v1",
    "title": "In Search of a Lost Metric: Human Empowerment as a Pillar of Socially Conscious Navigation",
    "authors": [
      "Vasanth Reddy Baddam",
      "Behdad Chalaki",
      "Vaishnav Tadiparthi",
      "Hossein Nourkhiz Mahjoub",
      "Ehsan Moradi-Pari",
      "Hoda Eldardiry",
      "Almuatazbellah Boker"
    ],
    "abstract": "In social robot navigation, traditional metrics like proxemics and behavior\nnaturalness emphasize human comfort and adherence to social norms but often\nfail to capture an agent's autonomy and adaptability in dynamic environments.\nThis paper introduces human empowerment, an information-theoretic concept that\nmeasures a human's ability to influence their future states and observe those\nchanges, as a complementary metric for evaluating social compliance. This\nmetric reveals how robot navigation policies can indirectly impact human\nempowerment. We present a framework that integrates human empowerment into the\nevaluation of social performance in navigation tasks. Through numerical\nsimulations, we demonstrate that human empowerment as a metric not only aligns\nwith intuitive social behavior, but also shows statistically significant\ndifferences across various robot navigation policies. These results provide a\ndeeper understanding of how different policies affect social compliance,\nhighlighting the potential of human empowerment as a complementary metric for\nfuture research in social navigation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 8 figures, 2 tables, Accepted to 20th edition of the\n  IEEE/ACM International Conference on Human-Robot Interaction (HRI)",
    "pdf_url": "http://arxiv.org/pdf/2501.01539v1",
    "published_date": "2025-01-02 21:13:46 UTC",
    "updated_date": "2025-01-02 21:13:46 UTC"
  },
  {
    "arxiv_id": "2501.01535v1",
    "title": "A Metasemantic-Metapragmatic Framework for Taxonomizing Multimodal Communicative Alignment",
    "authors": [
      "Eugene Yu Ji"
    ],
    "abstract": "Drawing on contemporary pragmatist philosophy and linguistic theories on\ncognition, meaning, and communication, this paper presents a dynamic,\nmetasemantic-metapragmatic taxonomy for grounding and conceptualizing\nhuman-like multimodal communicative alignment. The framework is rooted in\ncontemporary developments of the three basic communicative capacities initially\nidentified by American logician and pragmatist philosopher Charles Sanders\nPeirce: iconic (sensory and perceptual qualities), indexical (contextual and\nsociocultural associations), and rule-like (symbolic and intuitive reasoning).\nExpanding on these developments, I introduce the concept of indexical\ncontextualization and propose the principle of \"contextualization\ndirectionality\" for characterizing the crucial metapragmatic capacity for\nmaintaining, navigating, or transitioning between semantic and pragmatic modes\nof multimodal communication. I contend that current cognitive-social\ncomputational and engineering methodologies disproportionately emphasize the\nsemantic/metasemantic domain, overlooking the pivotal role of metapragmatic\nindexicality in traversing the semantic-pragmatic spectrum of communication.\nThe framework's broader implications for intentionality, identity, affect, and\nethics in within-modal and cross-modal human-machine alignment are also\ndiscussed.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "34 pages, 1 figure, 3 tables. Draft presented at 2023 ZJU Logic and\n  AI Summit EAI Workshop",
    "pdf_url": "http://arxiv.org/pdf/2501.01535v1",
    "published_date": "2025-01-02 21:00:19 UTC",
    "updated_date": "2025-01-02 21:00:19 UTC"
  },
  {
    "arxiv_id": "2501.02024v2",
    "title": "Model Checking in Medical Imaging for Tumor Detection and Segmentation",
    "authors": [
      "Elhoucine Elfatimi",
      "Lahcen El fatimi"
    ],
    "abstract": "Recent advancements in model checking have demonstrated significant potential\nacross diverse applications, particularly in signal and image analysis. Medical\nimaging stands out as a critical domain where model checking can be effectively\napplied to design and evaluate robust frameworks. These frameworks facilitate\nautomatic and semi-automatic delineation of regions of interest within images,\naiding in accurate segmentation. This paper provides a comprehensive analysis\nof recent works leveraging spatial logic to develop operators and tools for\nidentifying regions of interest, including tumorous and non-tumorous areas.\nAdditionally, we examine the challenges inherent to spatial model-checking\ntechniques, such as variability in ground truth data and the need for\nstreamlined procedures suitable for routine clinical practice.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02024v2",
    "published_date": "2025-01-02 20:47:04 UTC",
    "updated_date": "2025-01-07 03:29:43 UTC"
  },
  {
    "arxiv_id": "2501.01516v1",
    "title": "Improving Robustness Estimates in Natural Language Explainable AI though Synonymity Weighted Similarity Measures",
    "authors": [
      "Christopher Burger"
    ],
    "abstract": "Explainable AI (XAI) has seen a surge in recent interest with the\nproliferation of powerful but intractable black-box models. Moreover, XAI has\ncome under fire for techniques that may not offer reliable explanations. As\nmany of the methods in XAI are themselves models, adversarial examples have\nbeen prominent in the literature surrounding the effectiveness of XAI, with the\nobjective of these examples being to alter the explanation while maintaining\nthe output of the original model. For explanations in natural language, it is\nnatural to use measures found in the domain of information retrieval for use\nwith ranked lists to guide the adversarial XAI process. We show that the\nstandard implementation of these measures are poorly suited for the comparison\nof explanations in adversarial XAI and amend them by using information that is\ndiscarded, the synonymity of perturbed words. This synonymity weighting\nproduces more accurate estimates of the actual weakness of XAI methods to\nadversarial examples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 2 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.01516v1",
    "published_date": "2025-01-02 19:49:04 UTC",
    "updated_date": "2025-01-02 19:49:04 UTC"
  },
  {
    "arxiv_id": "2501.01515v1",
    "title": "DiagrammaticLearning: A Graphical Language for Compositional Training Regimes",
    "authors": [
      "Mason Lary",
      "Richard Samuelson",
      "Alexander Wilentz",
      "Alina Zare",
      "Matthew Klawonn",
      "James P. Fairbanks"
    ],
    "abstract": "Motivated by deep learning regimes with multiple interacting yet distinct\nmodel components, we introduce learning diagrams, graphical depictions of\ntraining setups that capture parameterized learning as data rather than code. A\nlearning diagram compiles to a unique loss function on which component models\nare trained. The result of training on this loss is a collection of models\nwhose predictions ``agree\" with one another. We show that a number of popular\nlearning setups such as few-shot multi-task learning, knowledge distillation,\nand multi-modal learning can be depicted as learning diagrams. We further\nimplement learning diagrams in a library that allows users to build diagrams of\nPyTorch and Flux.jl models. By implementing some classic machine learning use\ncases, we demonstrate how learning diagrams allow practitioners to build\ncomplicated models as compositions of smaller components, identify\nrelationships between workflows, and manipulate models during or after\ntraining. Leveraging a category theoretic framework, we introduce a rigorous\nsemantics for learning diagrams that puts such operations on a firm\nmathematical foundation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL",
      "math.CT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01515v1",
    "published_date": "2025-01-02 19:44:36 UTC",
    "updated_date": "2025-01-02 19:44:36 UTC"
  },
  {
    "arxiv_id": "2501.01509v1",
    "title": "AI-Enabled Operations at Fermi Complex: Multivariate Time Series Prediction for Outage Prediction and Diagnosis",
    "authors": [
      "Milan Jain",
      "Burcu O. Mutlu",
      "Caleb Stam",
      "Jan Strube",
      "Brian A. Schupbach",
      "Jason M. St. John",
      "William A. Pellico"
    ],
    "abstract": "The Main Control Room of the Fermilab accelerator complex continuously\ngathers extensive time-series data from thousands of sensors monitoring the\nbeam. However, unplanned events such as trips or voltage fluctuations often\nresult in beam outages, causing operational downtime. This downtime not only\nconsumes operator effort in diagnosing and addressing the issue but also leads\nto unnecessary energy consumption by idle machines awaiting beam restoration.\nThe current threshold-based alarm system is reactive and faces challenges\nincluding frequent false alarms and inconsistent outage-cause labeling. To\naddress these limitations, we propose an AI-enabled framework that leverages\npredictive analytics and automated labeling. Using data from $2,703$ Linac\ndevices and $80$ operator-labeled outages, we evaluate state-of-the-art deep\nlearning architectures, including recurrent, attention-based, and linear\nmodels, for beam outage prediction. Additionally, we assess a Random\nForest-based labeling system for providing consistent, confidence-scored outage\nannotations. Our findings highlight the strengths and weaknesses of these\narchitectures for beam outage prediction and identify critical gaps that must\nbe addressed to fully harness AI for transitioning downtime handling from\nreactive to predictive, ultimately reducing downtime and improving\ndecision-making in accelerator management.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented in the AAAI Workshop on AI for Time Series Analysis 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.01509v1",
    "published_date": "2025-01-02 19:31:48 UTC",
    "updated_date": "2025-01-02 19:31:48 UTC"
  },
  {
    "arxiv_id": "2501.01507v2",
    "title": "Transfer Learning Analysis of Variational Quantum Circuits",
    "authors": [
      "Huan-Hsin Tseng",
      "Hsin-Yi Lin",
      "Samuel Yen-Chi Chen",
      "Shinjae Yoo"
    ],
    "abstract": "This work analyzes transfer learning of the Variational Quantum Circuit\n(VQC). Our framework begins with a pretrained VQC configured in one domain and\ncalculates the transition of 1-parameter unitary subgroups required for a new\ndomain. A formalism is established to investigate the adaptability and\ncapability of a VQC under the analysis of loss bounds. Our theory observes\nknowledge transfer in VQCs and provides a heuristic interpretation for the\nmechanism. An analytical fine-tuning method is derived to attain the optimal\ntransition for adaptations of similar domains.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Published at ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.01507v2",
    "published_date": "2025-01-02 19:26:25 UTC",
    "updated_date": "2025-02-16 22:33:35 UTC"
  },
  {
    "arxiv_id": "2501.08339v1",
    "title": "Operator Learning for Reconstructing Flow Fields from Sparse Measurements: an Energy Transformer Approach",
    "authors": [
      "Qian Zhang",
      "Dmitry Krotov",
      "George Em Karniadakis"
    ],
    "abstract": "Machine learning methods have shown great success in various scientific\nareas, including fluid mechanics. However, reconstruction problems, where full\nvelocity fields must be recovered from partial observations, remain\nchallenging. In this paper, we propose a novel operator learning framework for\nsolving reconstruction problems by using the Energy Transformer (ET), an\narchitecture inspired by associative memory models. We formulate reconstruction\nas a mapping from incomplete observed data to full reconstructed fields. The\nmethod is validated on three fluid mechanics examples using diverse types of\ndata: (1) unsteady 2D vortex street in flow past a cylinder using simulation\ndata; (2) high-speed under-expanded impinging supersonic jets impingement using\nSchlieren imaging; and (3) 3D turbulent jet flow using particle tracking. The\nresults demonstrate the ability of ET to accurately reconstruct complex flow\nfields from highly incomplete data (90\\% missing), even for noisy experimental\nmeasurements, with fast training and inference on a single GPU. This work\nprovides a promising new direction for tackling reconstruction problems in\nfluid mechanics and other areas in mechanics, geophysics, weather prediction,\nand beyond.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.08339v1",
    "published_date": "2025-01-02 19:24:19 UTC",
    "updated_date": "2025-01-02 19:24:19 UTC"
  },
  {
    "arxiv_id": "2501.01496v1",
    "title": "ORACLE: A Real-Time, Hierarchical, Deep-Learning Photometric Classifier for the LSST",
    "authors": [
      "Ved G. Shah",
      "Alex Gagliano",
      "Konstantin Malanchev",
      "Gautham Narayan",
      "The LSST Dark Energy Science Collaboration"
    ],
    "abstract": "We present ORACLE, the first hierarchical deep-learning model for real-time,\ncontext-aware classification of transient and variable astrophysical phenomena.\nORACLE is a recurrent neural network with Gated Recurrent Units (GRUs), and has\nbeen trained using a custom hierarchical cross-entropy loss function to provide\nhigh-confidence classifications along an observationally-driven taxonomy with\nas little as a single photometric observation. Contextual information for each\nobject, including host galaxy photometric redshift, offset, ellipticity and\nbrightness, is concatenated to the light curve embedding and used to make a\nfinal prediction. Training on $\\sim$0.5M events from the Extended LSST\nAstronomical Time-Series Classification Challenge, we achieve a top-level\n(Transient vs Variable) macro-averaged precision of 0.96 using only 1 day of\nphotometric observations after the first detection in addition to contextual\ninformation, for each event; this increases to $>$0.99 once 64 days of the\nlight curve has been obtained, and 0.83 at 1024 days after first detection for\n19-way classification (including supernova sub-types, active galactic nuclei,\nvariable stars, microlensing events, and kilonovae). We also compare ORACLE\nwith other state-of-the-art classifiers and report comparable performance for\nthe 19-way classification task, in addition to delivering accurate top-level\nclassifications much earlier. The code and model weights used in this work are\npublicly available at our associated GitHub repository\n(https://github.com/uiucsn/ELAsTiCC-Classification).",
    "categories": [
      "astro-ph.IM",
      "astro-ph.HE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "29 pages, 19 figures, 9 tables. Submitted to ApJ",
    "pdf_url": "http://arxiv.org/pdf/2501.01496v1",
    "published_date": "2025-01-02 19:00:05 UTC",
    "updated_date": "2025-01-02 19:00:05 UTC"
  },
  {
    "arxiv_id": "2501.01424v1",
    "title": "Object-level Visual Prompts for Compositional Image Generation",
    "authors": [
      "Gaurav Parmar",
      "Or Patashnik",
      "Kuan-Chieh Wang",
      "Daniil Ostashev",
      "Srinivasa Narasimhan",
      "Jun-Yan Zhu",
      "Daniel Cohen-Or",
      "Kfir Aberman"
    ],
    "abstract": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project: https://snap-research.github.io/visual-composer/",
    "pdf_url": "http://arxiv.org/pdf/2501.01424v1",
    "published_date": "2025-01-02 18:59:44 UTC",
    "updated_date": "2025-01-02 18:59:44 UTC"
  },
  {
    "arxiv_id": "2501.01422v1",
    "title": "Multi-Modal Video Feature Extraction for Popularity Prediction",
    "authors": [
      "Haixu Liu",
      "Wenning Wang",
      "Haoxiang Zheng",
      "Penghao Jiang",
      "Qirui Wang",
      "Ruiqing Yan",
      "Qiuzhuang Sun"
    ],
    "abstract": "This work aims to predict the popularity of short videos using the videos\nthemselves and their related features. Popularity is measured by four key\nengagement metrics: view count, like count, comment count, and share count.\nThis study employs video classification models with different architectures and\ntraining methods as backbone networks to extract video modality features.\nMeanwhile, the cleaned video captions are incorporated into a carefully\ndesigned prompt framework, along with the video, as input for video-to-text\ngeneration models, which generate detailed text-based video content\nunderstanding. These texts are then encoded into vectors using a pre-trained\nBERT model. Based on the six sets of vectors mentioned above, a neural network\nis trained for each of the four prediction metrics. Moreover, the study\nconducts data mining and feature engineering based on the video and tabular\ndata, constructing practical features such as the total frequency of hashtag\nappearances, the total frequency of mention appearances, video duration, frame\ncount, frame rate, and total time online. Multiple machine learning models are\ntrained, and the most stable model, XGBoost, is selected. Finally, the\npredictions from the neural network and XGBoost models are averaged to obtain\nthe final result.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "INFORMS 2024 Data Challenge Competition",
    "pdf_url": "http://arxiv.org/pdf/2501.01422v1",
    "published_date": "2025-01-02 18:59:36 UTC",
    "updated_date": "2025-01-02 18:59:36 UTC"
  },
  {
    "arxiv_id": "2501.01409v2",
    "title": "JOG3R: Towards 3D-Consistent Video Generators",
    "authors": [
      "Chun-Hao Paul Huang",
      "Niloy Mitra",
      "Hyeonho Jeong",
      "Jae Shin Yoon",
      "Duygu Ceylan"
    ],
    "abstract": "Emergent capabilities of image generators have led to many impactful zero- or\nfew-shot applications. Inspired by this success, we investigate whether video\ngenerators similarly exhibit 3D-awareness. Using structure-from-motion as a\n3D-aware task, we test if intermediate features of a video generator - OpenSora\nin our case - can support camera pose estimation. Surprisingly, at first, we\nonly find a weak correlation between the two tasks. Deeper investigation\nreveals that although the video generator produces plausible video frames, the\nframes themselves are not truly 3D-consistent. Instead, we propose to jointly\ntrain for the two tasks, using photometric generation and 3D aware errors.\nSpecifically, we find that SoTA video generation and camera pose estimation\n(i.e.,DUSt3R [79]) networks share common structures, and propose an\narchitecture that unifies the two. The proposed unified model, named\n\\nameMethod, produces camera pose estimates with competitive quality while\nproducing 3D-consistent videos. In summary, we propose the first unified video\ngenerator that is 3D-consistent, generates realistic video frames, and can\npotentially be repurposed for other 3D-aware tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01409v2",
    "published_date": "2025-01-02 18:55:04 UTC",
    "updated_date": "2025-03-26 20:53:45 UTC"
  },
  {
    "arxiv_id": "2501.01394v1",
    "title": "A Unified Hyperparameter Optimization Pipeline for Transformer-Based Time Series Forecasting Models",
    "authors": [
      "Jingjing Xu",
      "Caesar Wu",
      "Yuan-Fang Li",
      "Grégoire Danoy",
      "Pascal Bouvry"
    ],
    "abstract": "Transformer-based models for time series forecasting (TSF) have attracted\nsignificant attention in recent years due to their effectiveness and\nversatility. However, these models often require extensive hyperparameter\noptimization (HPO) to achieve the best possible performance, and a unified\npipeline for HPO in transformer-based TSF remains lacking. In this paper, we\npresent one such pipeline and conduct extensive experiments on several\nstate-of-the-art (SOTA) transformer-based TSF models. These experiments are\nconducted on standard benchmark datasets to evaluate and compare the\nperformance of different models, generating practical insights and examples.\nOur pipeline is generalizable beyond transformer-based architectures and can be\napplied to other SOTA models, such as Mamba and TimeMixer, as demonstrated in\nour experiments. The goal of this work is to provide valuable guidance to both\nindustry practitioners and academic researchers in efficiently identifying\noptimal hyperparameters suited to their specific domain applications. The code\nand complete experimental results are available on GitHub.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01394v1",
    "published_date": "2025-01-02 18:12:42 UTC",
    "updated_date": "2025-01-02 18:12:42 UTC"
  },
  {
    "arxiv_id": "2501.02021v2",
    "title": "Weakly Supervised Learning on Large Graphs",
    "authors": [
      "Aditya Prakash"
    ],
    "abstract": "Graph classification plays a pivotal role in various domains, including\npathology, where images can be represented as graphs. In this domain, images\ncan be represented as graphs, where nodes might represent individual nuclei,\nand edges capture the spatial or functional relationships between them. Often,\nthe overall label of the graph, such as a cancer type or disease state, is\ndetermined by patterns within smaller, localized regions of the image. This\nwork introduces a weakly-supervised graph classification framework leveraging\ntwo subgraph extraction techniques: (1) Sliding-window approach (2) BFS-based\napproach. Subgraphs are processed using a Graph Attention Network (GAT), which\nemploys attention mechanisms to identify the most informative subgraphs for\nclassification. Weak supervision is achieved by propagating graph-level labels\nto subgraphs, eliminating the need for detailed subgraph annotations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02021v2",
    "published_date": "2025-01-02 18:12:13 UTC",
    "updated_date": "2025-01-27 19:01:44 UTC"
  },
  {
    "arxiv_id": "2501.14780v1",
    "title": "Perspective Chapter: MOOCs in India: Evolution, Innovation, Impact, and Roadmap",
    "authors": [
      "Partha Pratim Das"
    ],
    "abstract": "With the largest population of the world and one of the highest enrolments in\nhigher education, India needs efficient and effective means to educate its\nlearners. India started focusing on open and digital education in 1980's and\nits efforts were escalated in 2009 through the NMEICT program of the Government\nof India. A study by the Government and FICCI in 2014 noted that India cannot\nmeet its educational needs just by capacity building in brick and mortar\ninstitutions. It was decided that ongoing MOOCs projects under the umbrella of\nNMEICT will be further strengthened over its second (2017-21) and third\n(2021-26) phases. NMEICT now steers NPTEL or SWAYAM (India's MOOCs) and several\ndigital learning projects including Virtual Labs, e-Yantra, Spoken Tutorial,\nFOSSEE, and National Digital Library on India - the largest digital education\nlibrary in the world. Further, India embraced its new National Education Policy\nin 2020 to strongly foster online education. In this chapter, we take a deep\nlook into the evolution of MOOCs in India, its innovations, its current status\nand impact, and the roadmap for the next decade to address its challenges and\ngrow. AI-powered MOOCs is an emerging opportunity for India to lead MOOCs\nworldwide.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14780v1",
    "published_date": "2025-01-02 17:44:28 UTC",
    "updated_date": "2025-01-02 17:44:28 UTC"
  },
  {
    "arxiv_id": "2501.01377v1",
    "title": "Training Medical Large Vision-Language Models with Abnormal-Aware Feedback",
    "authors": [
      "Yucheng Zhou",
      "Lingran Song",
      "Jianbing Shen"
    ],
    "abstract": "Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate\nextensive medical knowledge, demonstrate excellent capabilities in\nunderstanding medical images and responding to human queries based on these\nimages. However, there remain challenges in visual localization in medical\nimages, which is crucial for abnormality detection and interpretation. To\naddress these issues, we propose a novel UMed-LVLM designed with Unveiling\nMedical abnormalities. Specifically, we collect a Medical Abnormalities\nUnveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM\ntraining. To collect MAU dataset, we propose a prompt method utilizing the\nGPT-4V to generate diagnoses based on identified abnormal areas in medical\nimages. Moreover, the two-stage training method includes Abnormal-Aware\nInstruction Tuning and Abnormal-Aware Rewarding, comprising Abnormal\nLocalization Rewarding and Vision Relevance Rewarding. Experimental results\ndemonstrate that our UMed-LVLM surpasses existing Med-LVLMs in identifying and\nunderstanding medical abnormality. In addition, this work shows that enhancing\nthe abnormality detection capabilities of Med-LVLMs significantly improves\ntheir understanding of medical images and generalization capability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.01377v1",
    "published_date": "2025-01-02 17:37:20 UTC",
    "updated_date": "2025-01-02 17:37:20 UTC"
  },
  {
    "arxiv_id": "2501.01372v1",
    "title": "ScarNet: A Novel Foundation Model for Automated Myocardial Scar Quantification from LGE in Cardiac MRI",
    "authors": [
      "Neda Tavakoli",
      "Amir Ali Rahsepar",
      "Brandon C. Benefield",
      "Daming Shen",
      "Santiago López-Tapia",
      "Florian Schiffers",
      "Jeffrey J. Goldberger",
      "Christine M. Albert",
      "Edwin Wu",
      "Aggelos K. Katsaggelos",
      "Daniel C. Lee",
      "Daniel Kim"
    ],
    "abstract": "Background: Late Gadolinium Enhancement (LGE) imaging is the gold standard\nfor assessing myocardial fibrosis and scarring, with left ventricular (LV) LGE\nextent predicting major adverse cardiac events (MACE). Despite its importance,\nroutine LGE-based LV scar quantification is hindered by labor-intensive manual\nsegmentation and inter-observer variability. Methods: We propose ScarNet, a\nhybrid model combining a transformer-based encoder from the Medical Segment\nAnything Model (MedSAM) with a convolution-based U-Net decoder, enhanced by\ntailored attention blocks. ScarNet was trained on 552 ischemic cardiomyopathy\npatients with expert segmentations of myocardial and scar boundaries and tested\non 184 separate patients. Results: ScarNet achieved robust scar segmentation in\n184 test patients, yielding a median Dice score of 0.912 (IQR: 0.863--0.944),\nsignificantly outperforming MedSAM (median Dice = 0.046, IQR: 0.043--0.047) and\nnnU-Net (median Dice = 0.638, IQR: 0.604--0.661). ScarNet demonstrated lower\nbias (-0.63%) and coefficient of variation (4.3%) compared to MedSAM (bias:\n-13.31%, CoV: 130.3%) and nnU-Net (bias: -2.46%, CoV: 20.3%). In Monte Carlo\nsimulations with noise perturbations, ScarNet achieved significantly higher\nscar Dice (0.892 \\pm 0.053, CoV = 5.9%) than MedSAM (0.048 \\pm 0.112, CoV =\n233.3%) and nnU-Net (0.615 \\pm 0.537, CoV = 28.7%). Conclusion: ScarNet\noutperformed MedSAM and nnU-Net in accurately segmenting myocardial and scar\nboundaries in LGE images. The model exhibited robust performance across diverse\nimage qualities and scar patterns.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "31 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.01372v1",
    "published_date": "2025-01-02 17:30:55 UTC",
    "updated_date": "2025-01-02 17:30:55 UTC"
  },
  {
    "arxiv_id": "2501.01367v1",
    "title": "Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation",
    "authors": [
      "Nathaniel Dennler",
      "Stefanos Nikolaidis",
      "Maja Matarić"
    ],
    "abstract": "People have a variety of preferences for how robots behave. To understand and\nreason about these preferences, robots aim to learn a reward function that\ndescribes how aligned robot behaviors are with a user's preferences. Good\nrepresentations of a robot's behavior can significantly reduce the time and\neffort required for a user to teach the robot their preferences. Specifying\nthese representations -- what \"features\" of the robot's behavior matter to\nusers -- remains a difficult problem; Features learned from raw data lack\nsemantic meaning and features learned from user data require users to engage in\ntedious labeling processes. Our key insight is that users tasked with\ncustomizing a robot are intrinsically motivated to produce labels through\nexploratory search; they explore behaviors that they find interesting and\nignore behaviors that are irrelevant. To harness this novel data source of\nexploratory actions, we propose contrastive learning from exploratory actions\n(CLEA) to learn trajectory features that are aligned with features that users\ncare about. We learned CLEA features from exploratory actions users performed\nin an open-ended signal design activity (N=25) with a Kuri robot, and evaluated\nCLEA features through a second user study with a different set of users (N=42).\nCLEA features outperformed self-supervised features when eliciting user\npreferences over four metrics: completeness, simplicity, minimality, and\nexplainability.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to HRI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.01367v1",
    "published_date": "2025-01-02 17:26:01 UTC",
    "updated_date": "2025-01-02 17:26:01 UTC"
  },
  {
    "arxiv_id": "2501.01366v1",
    "title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding",
    "authors": [
      "Austin T. Wang",
      "ZeMing Gong",
      "Angel X. Chang"
    ],
    "abstract": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages with 5 figures and 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.01366v1",
    "published_date": "2025-01-02 17:20:41 UTC",
    "updated_date": "2025-01-02 17:20:41 UTC"
  },
  {
    "arxiv_id": "2501.01349v1",
    "title": "Rethinking Relation Extraction: Beyond Shortcuts to Generalization with a Debiased Benchmark",
    "authors": [
      "Liang He",
      "Yougang Chu",
      "Zhen Wu",
      "Jianbing Zhang",
      "Xinyu Dai",
      "Jiajun Chen"
    ],
    "abstract": "Benchmarks are crucial for evaluating machine learning algorithm performance,\nfacilitating comparison and identifying superior solutions. However, biases\nwithin datasets can lead models to learn shortcut patterns, resulting in\ninaccurate assessments and hindering real-world applicability. This paper\naddresses the issue of entity bias in relation extraction tasks, where models\ntend to rely on entity mentions rather than context. We propose a debiased\nrelation extraction benchmark DREB that breaks the pseudo-correlation between\nentity mentions and relation types through entity replacement. DREB utilizes\nBias Evaluator and PPL Evaluator to ensure low bias and high naturalness,\nproviding a reliable and accurate assessment of model generalization in entity\nbias scenarios. To establish a new baseline on DREB, we introduce MixDebias, a\ndebiasing method combining data-level and model training-level techniques.\nMixDebias effectively improves model performance on DREB while maintaining\nperformance on the original dataset. Extensive experiments demonstrate the\neffectiveness and robustness of MixDebias compared to existing methods,\nhighlighting its potential for improving the generalization ability of relation\nextraction models. We will release DREB and MixDebias publicly.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01349v1",
    "published_date": "2025-01-02 17:01:06 UTC",
    "updated_date": "2025-01-02 17:01:06 UTC"
  },
  {
    "arxiv_id": "2501.01342v1",
    "title": "DeepFilter: An Instrumental Baseline for Accurate and Efficient Process Monitoring",
    "authors": [
      "Hao Wang",
      "Zhichao Chen",
      "Licheng Pan",
      "Xiaoyu Jiang",
      "Yichen Song",
      "Qunshan He",
      "Xinggao Liu"
    ],
    "abstract": "Effective process monitoring is increasingly vital in industrial automation\nfor ensuring operational safety, necessitating both high accuracy and\nefficiency. Although Transformers have demonstrated success in various fields,\ntheir canonical form based on the self-attention mechanism is inadequate for\nprocess monitoring due to two primary limitations: (1) the step-wise\ncorrelations captured by self-attention mechanism are difficult to capture\ndiscriminative patterns in monitoring logs due to the lacking semantics of each\nstep, thus compromising accuracy; (2) the quadratic computational complexity of\nself-attention hampers efficiency. To address these issues, we propose\nDeepFilter, a Transformer-style framework for process monitoring. The core\ninnovation is an efficient filtering layer that excel capturing long-term and\nperiodic patterns with reduced complexity. Equipping with the global filtering\nlayer, DeepFilter enhances both accuracy and efficiency, meeting the stringent\ndemands of process monitoring. Experimental results on real-world process\nmonitoring datasets validate DeepFilter's superiority in terms of accuracy and\nefficiency compared to existing state-of-the-art models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01342v1",
    "published_date": "2025-01-02 16:47:55 UTC",
    "updated_date": "2025-01-02 16:47:55 UTC"
  },
  {
    "arxiv_id": "2501.02020v3",
    "title": "Enhancing Uncertainty Modeling with Semantic Graph for Hallucination Detection",
    "authors": [
      "Kedi Chen",
      "Qin Chen",
      "Jie Zhou",
      "Xinqi Tao",
      "Bowen Ding",
      "Jingwen Xie",
      "Mingchen Xie",
      "Peilong Li",
      "Feng Zheng",
      "Liang He"
    ],
    "abstract": "Large Language Models (LLMs) are prone to hallucination with non-factual or\nunfaithful statements, which undermines the applications in real-world\nscenarios. Recent researches focus on uncertainty-based hallucination\ndetection, which utilizes the output probability of LLMs for uncertainty\ncalculation and does not rely on external knowledge or frequent sampling from\nLLMs. Whereas, most approaches merely consider the uncertainty of each\nindependent token, while the intricate semantic relations among tokens and\nsentences are not well studied, which limits the detection of hallucination\nthat spans over multiple tokens and sentences in the passage. In this paper, we\npropose a method to enhance uncertainty modeling with semantic graph for\nhallucination detection. Specifically, we first construct a semantic graph that\nwell captures the relations among entity tokens and sentences. Then, we\nincorporate the relations between two entities for uncertainty propagation to\nenhance sentence-level hallucination detection. Given that hallucination occurs\ndue to the conflict between sentences, we further present a graph-based\nuncertainty calibration method that integrates the contradiction probability of\nthe sentence with its neighbors in the semantic graph for uncertainty\ncalculation. Extensive experiments on two datasets show the great advantages of\nour proposed approach. In particular, we obtain substantial improvements with\n19.78% in passage-level hallucination detection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02020v3",
    "published_date": "2025-01-02 16:45:05 UTC",
    "updated_date": "2025-04-05 15:39:03 UTC"
  },
  {
    "arxiv_id": "2501.01335v1",
    "title": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models",
    "authors": [
      "Johan Wahréus",
      "Ahmed Mohamed Hussain",
      "Panos Papadimitratos"
    ],
    "abstract": "Numerous studies have investigated methods for jailbreaking Large Language\nModels (LLMs) to generate harmful content. Typically, these methods are\nevaluated using datasets of malicious prompts designed to bypass security\npolicies established by LLM providers. However, the generally broad scope and\nopen-ended nature of existing datasets can complicate the assessment of\njailbreaking effectiveness, particularly in specific domains, notably\ncybersecurity. To address this issue, we present and publicly release\nCySecBench, a comprehensive dataset containing 12662 prompts specifically\ndesigned to evaluate jailbreaking techniques in the cybersecurity domain. The\ndataset is organized into 10 distinct attack-type categories, featuring\nclose-ended prompts to enable a more consistent and accurate assessment of\njailbreaking attempts. Furthermore, we detail our methodology for dataset\ngeneration and filtration, which can be adapted to create similar datasets in\nother domains. To demonstrate the utility of CySecBench, we propose and\nevaluate a jailbreaking approach based on prompt obfuscation. Our experimental\nresults show that this method successfully elicits harmful content from\ncommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT\nand 88% with Gemini; in contrast, Claude demonstrated greater resilience with a\njailbreaking SR of 17%. Compared to existing benchmark approaches, our method\nshows superior performance, highlighting the value of domain-specific\nevaluation datasets for assessing LLM security measures. Moreover, when\nevaluated using prompts from a widely used dataset (i.e., AdvBench), it\nachieved an SR of 78.5%, higher than the state-of-the-art methods.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01335v1",
    "published_date": "2025-01-02 16:37:04 UTC",
    "updated_date": "2025-01-02 16:37:04 UTC"
  },
  {
    "arxiv_id": "2501.01329v1",
    "title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation",
    "authors": [
      "Shuzheng Gao",
      "Chaozheng Wang",
      "Cuiyun Gao",
      "Xiaoqian Jiao",
      "Chun Yong Chong",
      "Shan Gao",
      "Michael Lyu"
    ],
    "abstract": "Test cases are essential for validating the reliability and quality of\nsoftware applications. Recent studies have demonstrated the capability of Large\nLanguage Models (LLMs) to generate useful test cases for given source code.\nHowever, the existing work primarily relies on human-written plain prompts,\nwhich often leads to suboptimal results since the performance of LLMs can be\nhighly influenced by the prompts. Moreover, these approaches use the same\nprompt for all LLMs, overlooking the fact that different LLMs might be best\nsuited to different prompts. Given the wide variety of possible prompt\nformulations, automatically discovering the optimal prompt for each LLM\npresents a significant challenge. Although there are methods on automated\nprompt optimization in the natural language processing field, they are hard to\nproduce effective prompts for the test case generation task. First, the methods\niteratively optimize prompts by simply combining and mutating existing ones\nwithout proper guidance, resulting in prompts that lack diversity and tend to\nrepeat the same errors in the generated test cases. Second, the prompts are\ngenerally lack of domain contextual knowledge, limiting LLMs' performance in\nthe task.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01329v1",
    "published_date": "2025-01-02 16:30:05 UTC",
    "updated_date": "2025-01-02 16:30:05 UTC"
  },
  {
    "arxiv_id": "2501.01317v1",
    "title": "Understanding Difficult-to-learn Examples in Contrastive Learning: A Theoretical Framework for Spectral Contrastive Learning",
    "authors": [
      "Yi-Ge Zhang",
      "Jingyi Cui",
      "Qiran Li",
      "Yisen Wang"
    ],
    "abstract": "Unsupervised contrastive learning has shown significant performance\nimprovements in recent years, often approaching or even rivaling supervised\nlearning in various tasks. However, its learning mechanism is fundamentally\ndifferent from that of supervised learning. Previous works have shown that\ndifficult-to-learn examples (well-recognized in supervised learning as examples\naround the decision boundary), which are essential in supervised learning,\ncontribute minimally in unsupervised settings. In this paper, perhaps\nsurprisingly, we find that the direct removal of difficult-to-learn examples,\nalthough reduces the sample size, can boost the downstream classification\nperformance of contrastive learning. To uncover the reasons behind this, we\ndevelop a theoretical framework modeling the similarity between different pairs\nof samples. Guided by this theoretical framework, we conduct a thorough\ntheoretical analysis revealing that the presence of difficult-to-learn examples\nnegatively affects the generalization of contrastive learning. Furthermore, we\ndemonstrate that the removal of these examples, and techniques such as margin\ntuning and temperature scaling can enhance its generalization bounds, thereby\nimproving performance. Empirically, we propose a simple and efficient mechanism\nfor selecting difficult-to-learn examples and validate the effectiveness of the\naforementioned methods, which substantiates the reliability of our proposed\ntheoretical framework.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01317v1",
    "published_date": "2025-01-02 16:17:44 UTC",
    "updated_date": "2025-01-02 16:17:44 UTC"
  },
  {
    "arxiv_id": "2501.01311v2",
    "title": "Multi-Head Explainer: A General Framework to Improve Explainability in CNNs and Transformers",
    "authors": [
      "Bohang Sun",
      "Pietro Liò"
    ],
    "abstract": "In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and\nmodular framework that enhances both the explainability and accuracy of\nConvolutional Neural Networks (CNNs) and Transformer-based models. MHEX\nconsists of three core components: an Attention Gate that dynamically\nhighlights task-relevant features, Deep Supervision that guides early layers to\ncapture fine-grained details pertinent to the target class, and an Equivalent\nMatrix that unifies refined local and global representations to generate\ncomprehensive saliency maps. Our approach demonstrates superior compatibility,\nenabling effortless integration into existing residual networks like ResNet and\nTransformer architectures such as BERT with minimal modifications. Extensive\nexperiments on benchmark datasets in medical imaging and text classification\nshow that MHEX not only improves classification accuracy but also produces\nhighly interpretable and detailed saliency scores.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01311v2",
    "published_date": "2025-01-02 15:47:56 UTC",
    "updated_date": "2025-01-13 12:42:14 UTC"
  },
  {
    "arxiv_id": "2501.02019v1",
    "title": "Benchmarking Constraint-Based Bayesian Structure Learning Algorithms: Role of Network Topology",
    "authors": [
      "Radha Nagarajan",
      "Marco Scutari"
    ],
    "abstract": "Modeling the associations between real world entities from their multivariate\ncross-sectional profiles can provide cues into the concerted working of these\nentities as a system. Several techniques have been proposed for deciphering\nthese associations including constraint-based Bayesian structure learning (BSL)\nalgorithms that model them as directed acyclic graphs. Benchmarking these\nalgorithms have typically focused on assessing the variation in performance\nmeasures such as sensitivity as a function of the dimensionality represented by\nthe number of nodes in the DAG, and sample size. The present study elucidates\nthe importance of network topology in benchmarking exercises. More\nspecifically, it investigates variations in sensitivity across distinct network\ntopologies while constraining the nodes, edges, and sample-size to be\nidentical, eliminating these as potential confounders. Sensitivity of three\npopular constraint-based BSL algorithms (Peter-Clarke, Grow-Shrink, Incremental\nAssociation Markov Blanket) in learning the network structure from multivariate\ncross-sectional profiles sampled from network models with sub-linear, linear,\nand super-linear DAG topologies generated using preferential attachment is\ninvestigated. Results across linear and nonlinear models revealed statistically\nsignificant $(\\alpha=0.05)$ decrease in sensitivity estimates from sub-linear\nto super-linear topology constitutively across the three algorithms. These\nresults are demonstrated on networks with nodes $(N_{nods}=48,64)$, noise\nstrengths $(\\sigma =3,6)$ and sample size $(N = 2^{10})$. The findings\nelucidate the importance of accommodating the network topology in\nconstraint-based BSL benchmarking exercises.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.MN",
      "62H22",
      "I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "8 Pages, 4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2501.02019v1",
    "published_date": "2025-01-02 15:47:20 UTC",
    "updated_date": "2025-01-02 15:47:20 UTC"
  },
  {
    "arxiv_id": "2501.01303v1",
    "title": "Citations and Trust in LLM Generated Responses",
    "authors": [
      "Yifan Ding",
      "Matthew Facciani",
      "Amrit Poudel",
      "Ellen Joyce",
      "Salvador Aguinaga",
      "Balaji Veeramani",
      "Sanmitra Bhattacharya",
      "Tim Weninger"
    ],
    "abstract": "Question answering systems are rapidly advancing, but their opaque nature may\nimpact user trust. We explored trust through an anti-monitoring framework,\nwhere trust is predicted to be correlated with presence of citations and\ninversely related to checking citations. We tested this hypothesis with a live\nquestion-answering experiment that presented text responses generated using a\ncommercial Chatbot along with varying citations (zero, one, or five), both\nrelevant and random, and recorded if participants checked the citations and\ntheir self-reported trust in the generated responses. We found a significant\nincrease in trust when citations were present, a result that held true even\nwhen the citations were random; we also found a significant decrease in trust\nwhen participants checked the citations. These results highlight the importance\nof citations in enhancing trust in AI-generated content.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.01303v1",
    "published_date": "2025-01-02 15:32:50 UTC",
    "updated_date": "2025-01-02 15:32:50 UTC"
  },
  {
    "arxiv_id": "2501.01293v1",
    "title": "LEO-Split: A Semi-Supervised Split Learning Framework over LEO Satellite Networks",
    "authors": [
      "Zheng Lin",
      "Yuxin Zhang",
      "Zhe Chen",
      "Zihan Fang",
      "Cong Wu",
      "Xianhao Chen",
      "Yue Gao",
      "Jun Luo"
    ],
    "abstract": "Recently, the increasing deployment of LEO satellite systems has enabled\nvarious space analytics (e.g., crop and climate monitoring), which heavily\nrelies on the advancements in deep learning (DL). However, the intermittent\nconnectivity between LEO satellites and ground station (GS) significantly\nhinders the timely transmission of raw data to GS for centralized learning,\nwhile the scaled-up DL models hamper distributed learning on\nresource-constrained LEO satellites. Though split learning (SL) can be a\npotential solution to these problems by partitioning a model and offloading\nprimary training workload to GS, the labor-intensive labeling process remains\nan obstacle, with intermittent connectivity and data heterogeneity being other\nchallenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL\ndesign tailored for satellite networks to combat these challenges. Leveraging\nSS learning to handle (labeled) data scarcity, we construct an auxiliary model\nto tackle the training failure of the satellite-GS non-contact time. Moreover,\nwe propose a pseudo-labeling algorithm to rectify data imbalances across\nsatellites. Lastly, an adaptive activation interpolation scheme is devised to\nprevent the overfitting of server-side sub-model training at GS. Extensive\nexperiments with real-world LEO satellite traces (e.g., Starlink) demonstrate\nthat our LEO-Split framework achieves superior performance compared to\nstate-ofthe-art benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.01293v1",
    "published_date": "2025-01-02 15:19:16 UTC",
    "updated_date": "2025-01-02 15:19:16 UTC"
  },
  {
    "arxiv_id": "2501.01291v2",
    "title": "Detection Augmented Bandit Procedures for Piecewise Stationary MABs: A Modular Approach",
    "authors": [
      "Yu-Han Huang",
      "Argyrios Gerogiannis",
      "Subhonmesh Bose",
      "Venugopal V. Veeravalli"
    ],
    "abstract": "Conventional Multi-Armed Bandit (MAB) algorithms are designed for stationary\nenvironments, where the reward distributions associated with the arms do not\nchange with time. In many applications, however, the environment is more\naccurately modeled as being nonstationary. In this work, piecewise stationary\nMAB (PS-MAB) environments are investigated, in which the reward distributions\nassociated with a subset of the arms change at some change-points and remain\nstationary between change-points. Our focus is on the asymptotic analysis of\nPS-MABs, for which practical algorithms based on change detection (CD) have\nbeen previously proposed. Our goal is to modularize the design and analysis of\nsuch CD-based Bandit (CDB) procedures. To this end, we identify the\nrequirements for stationary bandit algorithms and change detectors in a CDB\nprocedure that are needed for the modularization. We assume that the rewards\nare sub-Gaussian. Under this assumption and a condition on the separation of\nthe change-points, we show that the analysis of CDB procedures can indeed be\nmodularized, so that regret bounds can be obtained in a unified manner for\nvarious combinations of change detectors and bandit algorithms. Through this\nanalysis, we develop new modular CDB procedures that are order-optimal. We\ncompare the performance of our modular CDB procedures with various other\nmethods in simulations.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, 2 figures, 1 table, submitted to JMLR",
    "pdf_url": "http://arxiv.org/pdf/2501.01291v2",
    "published_date": "2025-01-02 15:18:18 UTC",
    "updated_date": "2025-02-26 19:35:40 UTC"
  },
  {
    "arxiv_id": "2501.02018v1",
    "title": "Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs",
    "authors": [
      "Joao Fonseca",
      "Andrew Bell",
      "Julia Stoyanovich"
    ],
    "abstract": "Large Language Models (LLMs) have been shown to be susceptible to jailbreak\nattacks, or adversarial attacks used to illicit high risk behavior from a\nmodel. Jailbreaks have been exploited by cybercriminals and blackhat actors to\ncause significant harm, highlighting the critical need to safeguard\nwidely-deployed models. Safeguarding approaches, which include fine-tuning\nmodels or having LLMs \"self-reflect\", may lengthen the inference time of a\nmodel, incur a computational penalty, reduce the semantic fluency of an output,\nand restrict ``normal'' model behavior. Importantly, these Safety-Performance\nTrade-offs (SPTs) remain an understudied area. In this work, we introduce a\nnovel safeguard, called SafeNudge, that combines Controlled Text Generation\nwith \"nudging\", or using text interventions to change the behavior of a model.\nSafeNudge triggers during text-generation while a jailbreak attack is being\nexecuted, and can reduce successful jailbreak attempts by 30% by guiding the\nLLM towards a safe responses. It adds minimal latency to inference and has a\nnegligible impact on the semantic fluency of outputs. Further, we allow for\ntunable SPTs. SafeNudge is open-source and available through https://pypi.org/,\nand is compatible with models loaded with the Hugging Face \"transformers\"\nlibrary.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02018v1",
    "published_date": "2025-01-02 15:15:38 UTC",
    "updated_date": "2025-01-02 15:15:38 UTC"
  },
  {
    "arxiv_id": "2501.01480v3",
    "title": "CORAL: Concept Drift Representation Learning for Co-evolving Time-series",
    "authors": [
      "Kunpeng Xu",
      "Lifei Chen",
      "Shengrui Wang"
    ],
    "abstract": "In the realm of time series analysis, tackling the phenomenon of concept\ndrift poses a significant challenge. Concept drift -- characterized by the\nevolving statistical properties of time series data, affects the reliability\nand accuracy of conventional analysis models. This is particularly evident in\nco-evolving scenarios where interactions among variables are crucial. This\npaper presents CORAL, a simple yet effective method that models time series as\nan evolving ecosystem to learn representations of concept drift. CORAL employs\na kernel-induced self-representation learning to generate a representation\nmatrix, encapsulating the inherent dynamics of co-evolving time series. This\nmatrix serves as a key tool for identification and adaptation to concept drift\nby observing its temporal variations. Furthermore, CORAL effectively identifies\nprevailing patterns and offers insights into emerging trends through pattern\nevolution analysis. Our empirical evaluation of CORAL across various datasets\ndemonstrates its effectiveness in handling the complexities of concept drift.\nThis approach introduces a novel perspective in the theoretical domain of\nco-evolving time series analysis, enhancing adaptability and accuracy in the\nface of dynamic data environments, and can be easily integrated into most deep\nlearning backbones.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01480v3",
    "published_date": "2025-01-02 15:09:00 UTC",
    "updated_date": "2025-01-31 18:13:14 UTC"
  },
  {
    "arxiv_id": "2501.02016v1",
    "title": "ST-HCSS: Deep Spatio-Temporal Hypergraph Convolutional Neural Network for Soft Sensing",
    "authors": [
      "Hwa Hui Tew",
      "Fan Ding",
      "Gaoxuan Li",
      "Junn Yong Loo",
      "Chee-Ming Ting",
      "Ze Yang Ding",
      "Chee Pin Tan"
    ],
    "abstract": "Higher-order sensor networks are more accurate in characterizing the\nnonlinear dynamics of sensory time-series data in modern industrial settings by\nallowing multi-node connections beyond simple pairwise graph edges. In light of\nthis, we propose a deep spatio-temporal hypergraph convolutional neural network\nfor soft sensing (ST-HCSS). In particular, our proposed framework is able to\nconstruct and leverage a higher-order graph (hypergraph) to model the complex\nmulti-interactions between sensor nodes in the absence of prior structural\nknowledge. To capture rich spatio-temporal relationships underlying sensor\ndata, our proposed ST-HCSS incorporates stacked gated temporal and hypergraph\nconvolution layers to effectively aggregate and update hypergraph information\nacross time and nodes. Our results validate the superiority of ST-HCSS compared\nto existing state-of-the-art soft sensors, and demonstrates that the learned\nhypergraph feature representations aligns well with the sensor data\ncorrelations. The code is available at https://github.com/htew0001/ST-HCSS.git",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 2025 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.02016v1",
    "published_date": "2025-01-02 15:06:43 UTC",
    "updated_date": "2025-01-02 15:06:43 UTC"
  },
  {
    "arxiv_id": "2501.02015v1",
    "title": "KANS: Knowledge Discovery Graph Attention Network for Soft Sensing in Multivariate Industrial Processes",
    "authors": [
      "Hwa Hui Tew",
      "Gaoxuan Li",
      "Fan Ding",
      "Xuewen Luo",
      "Junn Yong Loo",
      "Chee-Ming Ting",
      "Ze Yang Ding",
      "Chee Pin Tan"
    ],
    "abstract": "Soft sensing of hard-to-measure variables is often crucial in industrial\nprocesses. Current practices rely heavily on conventional modeling techniques\nthat show success in improving accuracy. However, they overlook the non-linear\nnature, dynamics characteristics, and non-Euclidean dependencies between\ncomplex process variables. To tackle these challenges, we present a framework\nknown as a Knowledge discovery graph Attention Network for effective Soft\nsensing (KANS). Unlike the existing deep learning soft sensor models, KANS can\ndiscover the intrinsic correlations and irregular relationships between the\nmultivariate industrial processes without a predefined topology. First, an\nunsupervised graph structure learning method is introduced, incorporating the\ncosine similarity between different sensor embedding to capture the\ncorrelations between sensors. Next, we present a graph attention-based\nrepresentation learning that can compute the multivariate data parallelly to\nenhance the model in learning complex sensor nodes and edges. To fully explore\nKANS, knowledge discovery analysis has also been conducted to demonstrate the\ninterpretability of the model. Experimental results demonstrate that KANS\nsignificantly outperforms all the baselines and state-of-the-art methods in\nsoft sensing performance. Furthermore, the analysis shows that KANS can find\nsensors closely related to different process variables without domain\nknowledge, significantly improving soft sensing accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at IEEE International Conference on Systems, Man, and\n  Cybernetics (IEEE SMC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2501.02015v1",
    "published_date": "2025-01-02 15:02:36 UTC",
    "updated_date": "2025-01-02 15:02:36 UTC"
  },
  {
    "arxiv_id": "2501.14779v2",
    "title": "The Use of Generative Artificial Intelligence for Upper Secondary Mathematics Education Through the Lens of Technology Acceptance",
    "authors": [
      "Mika Setälä",
      "Ville Heilala",
      "Pieta Sikström",
      "Tommi Kärkkäinen"
    ],
    "abstract": "This study investigated the students' perceptions of using Generative\nArtificial Intelligence (GenAI) in upper-secondary mathematics education. Data\nwas collected from Finnish high school students to represent how key constructs\nof the Technology Acceptance Model (Perceived Usefulness, Perceived Ease of\nUse, Perceived Enjoyment, and Intention to Use) influence the adoption of AI\ntools. First, a structural equation model for a comparative study with a prior\nstudy was constructed and analyzed. Then, an extended model with the additional\nconstruct of Compatibility, which represents the alignment of AI tools with\nstudents' educational experiences and needs, was proposed and analyzed. The\nresults demonstrated a strong influence of perceived usefulness on the\nintention to use GenAI, emphasizing the statistically significant role of\nperceived enjoyment in determining perceived usefulness and ease of use. The\ninclusion of compatibility improved the model's explanatory power, particularly\nin predicting perceived usefulness. This study contributes to a deeper\nunderstanding of how AI tools can be integrated into mathematics education and\nhighlights key differences between the Finnish educational context and previous\nstudies based on structural equation modeling.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "K.3.1; I.2"
    ],
    "primary_category": "cs.CY",
    "comment": "Published in the Proceedings of the 40th ACM/SIGAPP Symposium on\n  Applied Computing (SAC'25), March 31--April 4, 2025, Catania, Italy",
    "pdf_url": "http://arxiv.org/pdf/2501.14779v2",
    "published_date": "2025-01-02 14:50:30 UTC",
    "updated_date": "2025-04-15 19:20:07 UTC"
  },
  {
    "arxiv_id": "2501.01284v1",
    "title": "NeutraSum: A Language Model can help a Balanced Media Diet by Neutralizing News Summaries",
    "authors": [
      "Xi Luo",
      "Junjie Liu",
      "Sirong Wu",
      "Yuhui Deng"
    ],
    "abstract": "Media bias in news articles arises from the political polarisation of media\noutlets, which can reinforce societal stereotypes and beliefs. Reporting on the\nsame event often varies significantly between outlets, reflecting their\npolitical leanings through polarised language and focus. Although previous\nstudies have attempted to generate bias-free summaries from multiperspective\nnews articles, they have not effectively addressed the challenge of mitigating\ninherent media bias. To address this gap, we propose \\textbf{NeutraSum}, a\nnovel framework that integrates two neutrality losses to adjust the semantic\nspace of generated summaries, thus minimising media bias. These losses,\ndesigned to balance the semantic distances across polarised inputs and ensure\nalignment with expert-written summaries, guide the generation of neutral and\nfactually rich summaries. To evaluate media bias, we employ the political\ncompass test, which maps political leanings based on economic and social\ndimensions. Experimental results on the Allsides dataset demonstrate that\nNeutraSum not only improves summarisation performance but also achieves\nsignificant reductions in media bias, offering a promising approach for neutral\nnews summarisation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01284v1",
    "published_date": "2025-01-02 14:48:07 UTC",
    "updated_date": "2025-01-02 14:48:07 UTC"
  },
  {
    "arxiv_id": "2501.02014v1",
    "title": "Machine Learning-Based Differential Diagnosis of Parkinson's Disease Using Kinematic Feature Extraction and Selection",
    "authors": [
      "Masahiro Matsumoto",
      "Abu Saleh Musa Miah",
      "Nobuyoshi Asai",
      "Jungpil Shin"
    ],
    "abstract": "Parkinson's disease (PD), the second most common neurodegenerative disorder,\nis characterized by dopaminergic neuron loss and the accumulation of abnormal\nsynuclein. PD presents both motor and non-motor symptoms that progressively\nimpair daily functioning. The severity of these symptoms is typically assessed\nusing the MDS-UPDRS rating scale, which is subjective and dependent on the\nphysician's experience. Additionally, PD shares symptoms with other\nneurodegenerative diseases, such as progressive supranuclear palsy (PSP) and\nmultiple system atrophy (MSA), complicating accurate diagnosis. To address\nthese diagnostic challenges, we propose a machine learning-based system for\ndifferential diagnosis of PD, PSP, MSA, and healthy controls (HC). This system\nutilizes a kinematic feature-based hierarchical feature extraction and\nselection approach. Initially, 18 kinematic features are extracted, including\ntwo newly proposed features: Thumb-to-index vector velocity and acceleration,\nwhich provide insights into motor control patterns. In addition, 41 statistical\nfeatures were extracted here from each kinematic feature, including some new\napproaches such as Average Absolute Change, Rhythm, Amplitude, Frequency,\nStandard Deviation of Frequency, and Slope. Feature selection is performed\nusing One-way ANOVA to rank features, followed by Sequential Forward Floating\nSelection (SFFS) to identify the most relevant ones, aiming to reduce the\ncomputational complexity. The final feature set is used for classification,\nachieving a classification accuracy of 66.67% for each dataset and 88.89% for\neach patient, with particularly high performance for the MSA and HC groups\nusing the SVM algorithm. This system shows potential as a rapid and accurate\ndiagnostic tool in clinical practice, though further data collection and\nrefinement are needed to enhance its reliability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02014v1",
    "published_date": "2025-01-02 14:43:39 UTC",
    "updated_date": "2025-01-02 14:43:39 UTC"
  },
  {
    "arxiv_id": "2501.01282v1",
    "title": "CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries",
    "authors": [
      "Shudong Liu",
      "Yiqiao Jin",
      "Cheng Li",
      "Derek F. Wong",
      "Qingsong Wen",
      "Lichao Sun",
      "Haipeng Chen",
      "Xing Xie",
      "Jindong Wang"
    ],
    "abstract": "Vision-language models (VLMs) have advanced human-AI interaction but struggle\nwith cultural understanding, often misinterpreting symbols, gestures, and\nartifacts due to biases in predominantly Western-centric training data. In this\npaper, we construct CultureVerse, a large-scale multimodal benchmark covering\n19, 682 cultural concepts, 188 countries/regions, 15 cultural concepts, and 3\nquestion types, with the aim of characterizing and improving VLMs'\nmulticultural understanding capabilities. Then, we propose CultureVLM, a series\nof VLMs fine-tuned on our dataset to achieve significant performance\nimprovement in cultural understanding. Our evaluation of 16 models reveals\nsignificant disparities, with a stronger performance in Western concepts and\nweaker results in African and Asian contexts. Fine-tuning on our CultureVerse\nenhances cultural perception, demonstrating cross-cultural, cross-continent,\nand cross-dataset generalization without sacrificing performance on models'\ngeneral VLM benchmarks. We further present insights on cultural generalization\nand forgetting. We hope that this work could lay the foundation for more\nequitable and culturally aware multimodal AI systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Technical report; 26 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.01282v1",
    "published_date": "2025-01-02 14:42:37 UTC",
    "updated_date": "2025-01-02 14:42:37 UTC"
  },
  {
    "arxiv_id": "2501.01266v1",
    "title": "PIMAEX: Multi-Agent Exploration through Peer Incentivization",
    "authors": [
      "Michael Kölle",
      "Johannes Tochtermann",
      "Julian Schönberger",
      "Gerhard Stenzel",
      "Philipp Altmann",
      "Claudia Linnhoff-Popien"
    ],
    "abstract": "While exploration in single-agent reinforcement learning has been studied\nextensively in recent years, considerably less work has focused on its\ncounterpart in multi-agent reinforcement learning. To address this issue, this\nwork proposes a peer-incentivized reward function inspired by previous research\non intrinsic curiosity and influence-based rewards. The \\textit{PIMAEX} reward,\nshort for Peer-Incentivized Multi-Agent Exploration, aims to improve\nexploration in the multi-agent setting by encouraging agents to exert influence\nover each other to increase the likelihood of encountering novel states. We\nevaluate the \\textit{PIMAEX} reward in conjunction with\n\\textit{PIMAEX-Communication}, a multi-agent training algorithm that employs a\ncommunication channel for agents to influence one another. The evaluation is\nconducted in the \\textit{Consume/Explore} environment, a partially observable\nenvironment with deceptive rewards, specifically designed to challenge the\nexploration vs.\\ exploitation dilemma and the credit-assignment problem. The\nresults empirically demonstrate that agents using the \\textit{PIMAEX} reward\nwith \\textit{PIMAEX-Communication} outperform those that do not.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted at ICAART 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.01266v1",
    "published_date": "2025-01-02 14:06:52 UTC",
    "updated_date": "2025-01-02 14:06:52 UTC"
  },
  {
    "arxiv_id": "2501.01264v1",
    "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
    "authors": [
      "Xiaoshuai Song",
      "Yanan Wu",
      "Weixun Wang",
      "Jiaheng Liu",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "abstract": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Working in progress",
    "pdf_url": "http://arxiv.org/pdf/2501.01264v1",
    "published_date": "2025-01-02 13:59:20 UTC",
    "updated_date": "2025-01-02 13:59:20 UTC"
  },
  {
    "arxiv_id": "2501.01263v1",
    "title": "Stealthy Backdoor Attack to Real-world Models in Android Apps",
    "authors": [
      "Jiali Wei",
      "Ming Fan",
      "Xicheng Zhang",
      "Wenjing Jiao",
      "Haijun Wang",
      "Ting Liu"
    ],
    "abstract": "Powered by their superior performance, deep neural networks (DNNs) have found\nwidespread applications across various domains. Many deep learning (DL) models\nare now embedded in mobile apps, making them more accessible to end users\nthrough on-device DL. However, deploying on-device DL to users' smartphones\nsimultaneously introduces several security threats. One primary threat is\nbackdoor attacks. Extensive research has explored backdoor attacks for several\nyears and has proposed numerous attack approaches. However, few studies have\ninvestigated backdoor attacks on DL models deployed in the real world, or they\nhave shown obvious deficiencies in effectiveness and stealthiness. In this\nwork, we explore more effective and stealthy backdoor attacks on real-world DL\nmodels extracted from mobile apps. Our main justification is that imperceptible\nand sample-specific backdoor triggers generated by DNN-based steganography can\nenhance the efficacy of backdoor attacks on real-world models. We first confirm\nthe effectiveness of steganography-based backdoor attacks on four\nstate-of-the-art DNN models. Subsequently, we systematically evaluate and\nanalyze the stealthiness of the attacks to ensure they are difficult to\nperceive. Finally, we implement the backdoor attacks on real-world models and\ncompare our approach with three baseline methods. We collect 38,387 mobile\napps, extract 89 DL models from them, and analyze these models to obtain the\nprerequisite model information for the attacks. After identifying the target\nmodels, our approach achieves an average of 12.50% higher attack success rate\nthan DeepPayload while better maintaining the normal performance of the models.\nExtensive experimental results demonstrate that our method enables more\neffective, robust, and stealthy backdoor attacks on real-world models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01263v1",
    "published_date": "2025-01-02 13:58:05 UTC",
    "updated_date": "2025-01-02 13:58:05 UTC"
  },
  {
    "arxiv_id": "2501.07585v1",
    "title": "Multi-task Domain Adaptation for Computation Offloading in Edge-intelligence Networks",
    "authors": [
      "Runxin Han",
      "Bo Yang",
      "Zhiwen Yu",
      "Xuelin Cao",
      "George C. Alexandropoulos",
      "Chau Yuen"
    ],
    "abstract": "In the field of multi-access edge computing (MEC), efficient computation\noffloading is crucial for improving resource utilization and reducing latency\nin dynamically changing environments. This paper introduces a new approach,\ntermed as Multi-Task Domain Adaptation (MTDA), aiming to enhance the ability of\ncomputational offloading models to generalize in the presence of domain shifts,\ni.e., when new data in the target environment significantly differs from the\ndata in the source domain. The proposed MTDA model incorporates a\nteacher-student architecture that allows continuous adaptation without\nnecessitating access to the source domain data during inference, thereby\nmaintaining privacy and reducing computational overhead. Utilizing a multi-task\nlearning framework that simultaneously manages offloading decisions and\nresource allocation, the proposed MTDA approach outperforms benchmark methods\nregarding mean squared error and accuracy, particularly in environments with\nincreasing numbers of users. It is observed by means of computer simulation\nthat the proposed MTDA model maintains high performance across various\nscenarios, demonstrating its potential for practical deployment in emerging MEC\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.07585v1",
    "published_date": "2025-01-02 13:20:29 UTC",
    "updated_date": "2025-01-02 13:20:29 UTC"
  },
  {
    "arxiv_id": "2501.01243v2",
    "title": "Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants",
    "authors": [
      "Lixiong Qin",
      "Shilong Ou",
      "Miaoxuan Zhang",
      "Jiangning Wei",
      "Yuhang Zhang",
      "Xiaoshuai Song",
      "Yuchen Liu",
      "Mei Wang",
      "Weiran Xu"
    ],
    "abstract": "Faces and humans are crucial elements in social interaction and are widely\nincluded in everyday photos and videos. Therefore, a deep understanding of\nfaces and humans will enable multi-modal assistants to achieve improved\nresponse quality and broadened application scope. Currently, the multi-modal\nassistant community lacks a comprehensive and scientific evaluation of face and\nhuman understanding abilities. In this paper, we first propose a hierarchical\nability taxonomy that includes three levels of abilities. Then, based on this\ntaxonomy, we collect images and annotations from publicly available datasets in\nthe face and human community and build a semi-automatic data pipeline to\nproduce problems for the new benchmark. Finally, the obtained Face-Human-Bench\ncomprises a development set with 900 problems and a test set with 1800\nproblems, supporting both English and Chinese. We conduct evaluations over 25\nmainstream multi-modal large language models (MLLMs) with our Face-Human-Bench,\nfocusing on the correlation between abilities, the impact of the relative\nposition of targets on performance, and the impact of Chain of Thought (CoT)\nprompting on performance. Moreover, inspired by multi-modal agents, we also\nexplore which abilities of MLLMs need to be supplemented by specialist models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "50 pages, 14 figures, 41 tables. Submitted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.01243v2",
    "published_date": "2025-01-02 13:05:47 UTC",
    "updated_date": "2025-01-05 08:42:36 UTC"
  },
  {
    "arxiv_id": "2501.01242v1",
    "title": "An Efficient Attention Mechanism for Sequential Recommendation Tasks: HydraRec",
    "authors": [
      "Uzma Mushtaque"
    ],
    "abstract": "Transformer based models are increasingly being used in various domains\nincluding recommender systems (RS). Pretrained transformer models such as BERT\nhave shown good performance at language modelling. With the greater ability to\nmodel sequential tasks, variants of Encoder-only models (like BERT4Rec, SASRec\netc.) have found success in sequential RS problems. Computing dot-product\nattention in traditional transformer models has quadratic complexity in\nsequence length. This is a bigger problem with RS because unlike language\nmodels, new items are added to the catalogue every day. User buying history is\na dynamic sequence which depends on multiple factors. Recently, various linear\nattention models have tried to solve this problem by making the model linear in\nsequence length (token dimensions). Hydra attention is one such linear\ncomplexity model proposed for vision transformers which reduces the complexity\nof attention for both the number of tokens as well as model embedding\ndimensions. Building on the idea of Hydra attention, we introduce an efficient\nTransformer based Sequential RS (HydraRec) which significantly improves\ntheoretical complexity of computing attention for longer sequences and bigger\ndatasets while preserving the temporal context. Extensive experiments are\nconducted to evaluate other linear transformer-based RS models and compared\nwith HydraRec across various evaluation metrics. HydraRec outperforms other\nlinear attention-based models as well as dot-product based attention models\nwhen used with causal masking for sequential recommendation next item\nprediction tasks. For bi-directional models its performance is comparable to\nthe BERT4Rec model with an improvement in running time.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01242v1",
    "published_date": "2025-01-02 13:03:06 UTC",
    "updated_date": "2025-01-02 13:03:06 UTC"
  },
  {
    "arxiv_id": "2501.01478v1",
    "title": "Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search",
    "authors": [
      "Shuangtao Li",
      "Shuaihao Dong",
      "Kexin Luan",
      "Xinhan Di",
      "Chaofan Ding"
    ],
    "abstract": "Large language models (LLMs) have demonstrated their remarkable capacity\nacross a variety of tasks. However, reasoning remains a challenge for LLMs. To\nimprove LLMs' reasoning ability, process supervision has proven to be better\nthan outcome supervision. In this work, we study using Monte Carlo Tree Search\n(MCTS) to generate process supervision data with LLMs themselves for training\nthem. We sample reasoning steps with an LLM and assign each step a score that\ncaptures its \"relative correctness,\" and the LLM is then trained by minimizing\nweighted log-likelihood of generating the reasoning steps. This\ngenerate-then-train process is repeated iteratively until convergence.Our\nexperimental results demonstrate that the proposed methods considerably improve\nthe performance of LLMs on two mathematical reasoning datasets. Furthermore,\nmodels trained on one dataset also exhibit improved performance on the other,\nshowing the transferability of the enhanced reasoning ability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 1 figure, 2 tables accepted by aaai 2025 NeurMAD workshop",
    "pdf_url": "http://arxiv.org/pdf/2501.01478v1",
    "published_date": "2025-01-02 12:09:17 UTC",
    "updated_date": "2025-01-02 12:09:17 UTC"
  },
  {
    "arxiv_id": "2501.02009v2",
    "title": "Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts",
    "authors": [
      "Youcheng Huang",
      "Chen Huang",
      "Duanyu Feng",
      "Wenqiang Lei",
      "Jiancheng Lv"
    ],
    "abstract": "Understanding the inner workings of Large Language Models (LLMs) is a\ncritical research frontier. Prior research has shown that a single LLM's\nconcept representations can be captured as steering vectors (SVs), enabling the\ncontrol of LLM behavior (e.g., towards generating harmful content). Our work\ntakes a novel approach by exploring the intricate relationships between concept\nrepresentations across different LLMs, drawing an intriguing parallel to\nPlato's Allegory of the Cave. In particular, we introduce a linear\ntransformation method to bridge these representations and present three key\nfindings: 1) Concept representations across different LLMs can be effectively\naligned using simple linear transformations, enabling efficient cross-model\ntransfer and behavioral control via SVs. 2) This linear transformation\ngeneralizes across concepts, facilitating alignment and control of SVs\nrepresenting different concepts across LLMs. 3) A weak-to-strong\ntransferability exists between LLM concept representations, whereby SVs\nextracted from smaller LLMs can effectively control the behavior of larger\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025 Main Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2501.02009v2",
    "published_date": "2025-01-02 11:56:59 UTC",
    "updated_date": "2025-05-20 03:24:30 UTC"
  },
  {
    "arxiv_id": "2501.01209v1",
    "title": "A redescription mining framework for post-hoc explaining and relating deep learning models",
    "authors": [
      "Matej Mihelčić",
      "Ivan Grubišić",
      "Miha Keber"
    ],
    "abstract": "Deep learning models (DLMs) achieve increasingly high performance both on\nstructured and unstructured data. They significantly extended applicability of\nmachine learning to various domains. Their success in making predictions,\ndetecting patterns and generating new data made significant impact on science\nand industry. Despite these accomplishments, DLMs are difficult to explain\nbecause of their enormous size. In this work, we propose a novel framework for\npost-hoc explaining and relating DLMs using redescriptions. The framework\nallows cohort analysis of arbitrary DLMs by identifying statistically\nsignificant redescriptions of neuron activations. It allows coupling neurons to\na set of target labels or sets of descriptive attributes, relating layers\nwithin a single DLM or associating different DLMs. The proposed framework is\nindependent of the artificial neural network architecture and can work with\nmore complex target labels (e.g. multi-label or multi-target scenario).\nAdditionally, it can emulate both pedagogical and decompositional approach to\nrule extraction. The aforementioned properties of the proposed framework can\nincrease explainability and interpretability of arbitrary DLMs by providing\ndifferent information compared to existing explainable-AI approaches.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01209v1",
    "published_date": "2025-01-02 11:38:10 UTC",
    "updated_date": "2025-01-02 11:38:10 UTC"
  },
  {
    "arxiv_id": "2501.03259v1",
    "title": "Toward Inclusive Educational AI: Auditing Frontier LLMs through a Multiplexity Lens",
    "authors": [
      "Abdullah Mushtaq",
      "Muhammad Rafay Naeem",
      "Muhammad Imran Taj",
      "Ibrahim Ghaznavi",
      "Junaid Qadir"
    ],
    "abstract": "As large language models (LLMs) like GPT-4 and Llama 3 become integral to\neducational contexts, concerns are mounting over the cultural biases, power\nimbalances, and ethical limitations embedded within these technologies. Though\ngenerative AI tools aim to enhance learning experiences, they often reflect\nvalues rooted in Western, Educated, Industrialized, Rich, and Democratic\n(WEIRD) cultural paradigms, potentially sidelining diverse global perspectives.\nThis paper proposes a framework to assess and mitigate cultural bias within\nLLMs through the lens of applied multiplexity. Multiplexity, inspired by\nSenturk et al. and rooted in Islamic and other wisdom traditions, emphasizes\nthe coexistence of diverse cultural viewpoints, supporting a multi-layered\nepistemology that integrates both empirical sciences and normative values. Our\nanalysis reveals that LLMs frequently exhibit cultural polarization, with\nbiases appearing in both overt responses and subtle contextual cues. To address\ninherent biases and incorporate multiplexity in LLMs, we propose two\nstrategies: \\textit{Contextually-Implemented Multiplex LLMs}, which embed\nmultiplex principles directly into the system prompt, influencing LLM outputs\nat a foundational level and independent of individual prompts, and\n\\textit{Multi-Agent System (MAS)-Implemented Multiplex LLMs}, where multiple\nLLM agents, each representing distinct cultural viewpoints, collaboratively\ngenerate a balanced, synthesized response. Our findings demonstrate that as\nmitigation strategies evolve from contextual prompting to MAS-implementation,\ncultural inclusivity markedly improves, evidenced by a significant rise in the\nPerspectives Distribution Score (PDS) and a PDS Entropy increase from 3.25\\% at\nbaseline to 98\\% with the MAS-Implemented Multiplex LLMs. Sentiment analysis\nfurther shows a shift towards positive sentiment across cultures,...",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03259v1",
    "published_date": "2025-01-02 11:27:08 UTC",
    "updated_date": "2025-01-02 11:27:08 UTC"
  },
  {
    "arxiv_id": "2501.01205v1",
    "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects",
    "authors": [
      "Abdullah Mushtaq",
      "Muhammad Rafay Naeem",
      "Ibrahim Ghaznavi",
      "Muhammad Imran Taj",
      "Imran Hashmi",
      "Junaid Qadir"
    ],
    "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention\nfor their ability to harness collective intelligence in complex\nproblem-solving, decision-making, and planning tasks. This aligns with the\nconcept of the wisdom of crowds, where diverse agents contribute collectively\nto generating effective solutions, making it particularly suitable for\neducational settings. Senior design projects, also known as capstone or final\nyear projects, are pivotal in engineering education as they integrate\ntheoretical knowledge with practical application, fostering critical thinking,\nteamwork, and real-world problem-solving skills. In this paper, we explore the\nuse of Multi-Agent LLMs in supporting these senior design projects undertaken\nby engineering students, which often involve multidisciplinary considerations\nand conflicting objectives, such as optimizing technical performance while\naddressing ethical, social, and environmental concerns. We propose a framework\nwhere distinct LLM agents represent different expert perspectives, such as\nproblem formulation agents, system complexity agents, societal and ethical\nagents, or project managers, thus facilitating a holistic problem-solving\napproach. This implementation leverages standard multi-agent system (MAS)\nconcepts such as coordination, cooperation, and negotiation, incorporating\nprompt engineering to develop diverse personas for each agent. These agents\nengage in rich, collaborative dialogues to simulate human engineering teams,\nguided by principles from swarm AI to efficiently balance individual\ncontributions towards a unified solution. We adapt these techniques to create a\ncollaboration structure for LLM agents, encouraging interdisciplinary reasoning\nand negotiation similar to real-world senior design projects. To assess the\nefficacy of this framework, we collected six proposals of engineering and\ncomputer science of...",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01205v1",
    "published_date": "2025-01-02 11:25:45 UTC",
    "updated_date": "2025-01-02 11:25:45 UTC"
  },
  {
    "arxiv_id": "2501.01195v1",
    "title": "Data Augmentation Techniques for Chinese Disease Name Normalization",
    "authors": [
      "Wenqian Cui",
      "Xiangling Fu",
      "Shaohui Liu",
      "Mingjun Gu",
      "Xien Liu",
      "Ji Wu",
      "Irwin King"
    ],
    "abstract": "Disease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. Consequently, we present a novel data augmentation approach that includes\na series of data augmentation techniques and some supporting modules to help\nmitigate the problem. Through extensive experimentation, we illustrate that our\nproposed approach exhibits significant performance improvements across various\nbaseline models and training objectives, particularly in scenarios with limited\ntraining data",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The Version of Record of this contribution is published in 2024 IEEE\n  International Conference on Bioinformatics and Biomedicine (BIBM 2024)",
    "pdf_url": "http://arxiv.org/pdf/2501.01195v1",
    "published_date": "2025-01-02 11:12:03 UTC",
    "updated_date": "2025-01-02 11:12:03 UTC"
  },
  {
    "arxiv_id": "2501.01174v1",
    "title": "L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild",
    "authors": [
      "Soumyaratna Debnath",
      "Harish Katti",
      "Shashikant Verma",
      "Shanmuganathan Raman"
    ],
    "abstract": "While 2D pose estimation has advanced our ability to interpret body movements\nin animals and primates, it is limited by the lack of depth information,\nconstraining its application range. 3D pose estimation provides a more\ncomprehensive solution by incorporating spatial depth, yet creating extensive\n3D pose datasets for animals is challenging due to their dynamic and\nunpredictable behaviours in natural settings. To address this, we propose a\nhybrid approach that utilizes rigged avatars and the pipeline to generate\nsynthetic datasets to acquire the necessary 3D annotations for training. Our\nmethod introduces a simple attention-based MLP network for converting 2D poses\nto 3D, designed to be independent of the input image to ensure scalability for\nposes in natural environments. Additionally, we identify that existing\nanatomical keypoint detectors are insufficient for accurate pose retargeting\nonto arbitrary avatars. To overcome this, we present a lookup table based on a\ndeep pose estimation method using a synthetic collection of diverse actions\nrigged avatars perform. Our experiments demonstrate the effectiveness and\nefficiency of this lookup table-based retargeting approach. Overall, we propose\na comprehensive framework with systematically synthesized datasets for lifting\nposes from 2D to 3D and then utilize this to re-target motion from wild\nsettings onto arbitrary avatars.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "2025 IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.01174v1",
    "published_date": "2025-01-02 10:04:12 UTC",
    "updated_date": "2025-01-02 10:04:12 UTC"
  },
  {
    "arxiv_id": "2501.01168v1",
    "title": "Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets",
    "authors": [
      "Mahdi Zakizadeh",
      "Mohammad Taher Pilehvar"
    ],
    "abstract": "The multifaceted challenge of accurately measuring gender stereotypical bias\nin language models is akin to discerning different segments of a broader,\nunseen entity. This short paper primarily focuses on intrinsic bias mitigation\nand measurement strategies for language models, building on prior research that\ndemonstrates a lack of correlation between intrinsic and extrinsic approaches.\nWe delve deeper into intrinsic measurements, identifying inconsistencies and\nsuggesting that these benchmarks may reflect different facets of gender\nstereotype. Our methodology involves analyzing data distributions across\ndatasets and integrating gender stereotype components informed by social\npsychology. By adjusting the distribution of two datasets, we achieve a better\nalignment of outcomes. Our findings underscore the complexity of gender\nstereotyping in language models and point to new directions for developing more\nrefined techniques to detect and reduce bias.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01168v1",
    "published_date": "2025-01-02 09:40:31 UTC",
    "updated_date": "2025-01-02 09:40:31 UTC"
  },
  {
    "arxiv_id": "2501.01166v1",
    "title": "Deep Learning in Palmprint Recognition-A Comprehensive Survey",
    "authors": [
      "Chengrui Gao",
      "Ziyuan Yang",
      "Wei Jia",
      "Lu Leng",
      "Bob Zhang",
      "Andrew Beng Jin Teoh"
    ],
    "abstract": "Palmprint recognition has emerged as a prominent biometric technology, widely\napplied in diverse scenarios. Traditional handcrafted methods for palmprint\nrecognition often fall short in representation capability, as they heavily\ndepend on researchers' prior knowledge. Deep learning (DL) has been introduced\nto address this limitation, leveraging its remarkable successes across various\ndomains. While existing surveys focus narrowly on specific tasks within\npalmprint recognition-often grounded in traditional methodologies-there remains\na significant gap in comprehensive research exploring DL-based approaches\nacross all facets of palmprint recognition. This paper bridges that gap by\nthoroughly reviewing recent advancements in DL-powered palmprint recognition.\nThe paper systematically examines progress across key tasks, including\nregion-of-interest segmentation, feature extraction, and\nsecurity/privacy-oriented challenges. Beyond highlighting these advancements,\nthe paper identifies current challenges and uncovers promising opportunities\nfor future research. By consolidating state-of-the-art progress, this review\nserves as a valuable resource for researchers, enabling them to stay abreast of\ncutting-edge technologies and drive innovation in palmprint recognition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Palmprint recognition, biometrics, deep learning, feature extraction,\n  recognition tasks",
    "pdf_url": "http://arxiv.org/pdf/2501.01166v1",
    "published_date": "2025-01-02 09:38:44 UTC",
    "updated_date": "2025-01-02 09:38:44 UTC"
  },
  {
    "arxiv_id": "2501.01156v2",
    "title": "TexAVi: Generating Stereoscopic VR Video Clips from Text Descriptions",
    "authors": [
      "Vriksha Srihari",
      "R. Bhavya",
      "Shruti Jayaraman",
      "V. Mary Anita Rajam"
    ],
    "abstract": "While generative models such as text-to-image, large language models and\ntext-to-video have seen significant progress, the extension to\ntext-to-virtual-reality remains largely unexplored, due to a deficit in\ntraining data and the complexity of achieving realistic depth and motion in\nvirtual environments. This paper proposes an approach to coalesce existing\ngenerative systems to form a stereoscopic virtual reality video from text.\n  Carried out in three main stages, we start with a base text-to-image model\nthat captures context from an input text. We then employ Stable Diffusion on\nthe rudimentary image produced, to generate frames with enhanced realism and\noverall quality. These frames are processed with depth estimation algorithms to\ncreate left-eye and right-eye views, which are stitched side-by-side to create\nan immersive viewing experience. Such systems would be highly beneficial in\nvirtual reality production, since filming and scene building often require\nextensive hours of work and post-production effort.\n  We utilize image evaluation techniques, specifically Fr\\'echet Inception\nDistance and CLIP Score, to assess the visual quality of frames produced for\nthe video. These quantitative measures establish the proficiency of the\nproposed method.\n  Our work highlights the exciting possibilities of using natural\nlanguage-driven graphics in fields like virtual reality simulations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.2"
    ],
    "primary_category": "cs.CV",
    "comment": "Co-authors do not consent to publishing on Arxiv",
    "pdf_url": "http://arxiv.org/pdf/2501.01156v2",
    "published_date": "2025-01-02 09:21:03 UTC",
    "updated_date": "2025-03-10 20:52:31 UTC"
  },
  {
    "arxiv_id": "2501.01149v2",
    "title": "A3: Android Agent Arena for Mobile GUI Agents",
    "authors": [
      "Yuxiang Chai",
      "Hanhao Li",
      "Jiayu Zhang",
      "Liang Liu",
      "Guangyi Liu",
      "Guozhi Wang",
      "Shuai Ren",
      "Siyuan Huang",
      "Hongsheng Li"
    ],
    "abstract": "AI agents have become increasingly prevalent in recent years, driven by\nsignificant advancements in the field of large language models (LLMs). Mobile\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\non mobile devices. While numerous studies have introduced agents, datasets, and\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\non static frame evaluations and fail to provide a comprehensive platform for\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\nreal-time online information retrieval and operational instructions; (2) a\nlarger, more flexible action space, enabling compatibility with agents trained\non any dataset; and (3) automated business-level LLM-based evaluation process.\nA3 includes 21 widely used general third-party apps and 201 tasks\nrepresentative of common user scenarios, providing a robust foundation for\nevaluating mobile GUI agents in real-world situations and a new autonomous\nevaluation process for less human labor and coding expertise. The project is\navailable at https://yuxiangchai.github.io/Android-Agent-Arena/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01149v2",
    "published_date": "2025-01-02 09:03:56 UTC",
    "updated_date": "2025-02-18 08:24:59 UTC"
  },
  {
    "arxiv_id": "2501.04719v1",
    "title": "Calculating Customer Lifetime Value and Churn using Beta Geometric Negative Binomial and Gamma-Gamma Distribution in a NFT based setting",
    "authors": [
      "Sagarnil Das"
    ],
    "abstract": "Customer Lifetime Value (CLV) is an important metric that measures the total\nvalue a customer will bring to a business over their lifetime. The Beta\nGeometric Negative Binomial Distribution (BGNBD) and Gamma Gamma Distribution\nare two models that can be used to calculate CLV, taking into account both the\nfrequency and value of customer transactions. This article explains the BGNBD\nand Gamma Gamma Distribution models, and how they can be used to calculate CLV\nfor NFT (Non-Fungible Token) transaction data in a blockchain setting. By\nestimating the parameters of these models using historical transaction data,\nbusinesses can gain insights into the lifetime value of their customers and\nmake data-driven decisions about marketing and customer retention strategies.",
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "primary_category": "stat.AP",
    "comment": "10 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.04719v1",
    "published_date": "2025-01-02 08:55:54 UTC",
    "updated_date": "2025-01-02 08:55:54 UTC"
  },
  {
    "arxiv_id": "2501.01136v2",
    "title": "Symmetries-enhanced Multi-Agent Reinforcement Learning",
    "authors": [
      "Nikolaos Bousias",
      "Stefanos Pertigkiozoglou",
      "Kostas Daniilidis",
      "George Pappas"
    ],
    "abstract": "Multi-agent reinforcement learning has emerged as a powerful framework for\nenabling agents to learn complex, coordinated behaviors but faces persistent\nchallenges regarding its generalization, scalability and sample efficiency.\nRecent advancements have sought to alleviate those issues by embedding\nintrinsic symmetries of the systems in the policy. Yet, most dynamical systems\nexhibit little to no symmetries to exploit. This paper presents a novel\nframework for embedding extrinsic symmetries in multi-agent system dynamics\nthat enables the use of symmetry-enhanced methods to address systems with\ninsufficient intrinsic symmetries, expanding the scope of equivariant learning\nto a wide variety of MARL problems. Central to our framework is the Group\nEquivariant Graphormer, a group-modular architecture specifically designed for\ndistributed swarming tasks. Extensive experiments on a swarm of\nsymmetry-breaking quadrotors validate the effectiveness of our approach,\nshowcasing its potential for improved generalization and zero-shot scalability.\nOur method achieves significant reductions in collision rates and enhances task\nsuccess rates across a diverse range of scenarios and varying swarm sizes.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "math.RT"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01136v2",
    "published_date": "2025-01-02 08:41:31 UTC",
    "updated_date": "2025-04-25 09:39:19 UTC"
  },
  {
    "arxiv_id": "2501.01132v1",
    "title": "Missing Data as Augmentation in the Earth Observation Domain: A Multi-View Learning Approach",
    "authors": [
      "Francisco Mena",
      "Diego Arenas",
      "Andreas Dengel"
    ],
    "abstract": "Multi-view learning (MVL) leverages multiple sources or views of data to\nenhance machine learning model performance and robustness. This approach has\nbeen successfully used in the Earth Observation (EO) domain, where views have a\nheterogeneous nature and can be affected by missing data. Despite the negative\neffect that missing data has on model predictions, the ML literature has used\nit as an augmentation technique to improve model generalization, like masking\nthe input data. Inspired by this, we introduce novel methods for EO\napplications tailored to MVL with missing views. Our methods integrate the\ncombination of a set to simulate all combinations of missing views as different\ntraining samples. Instead of replacing missing data with a numerical value, we\nuse dynamic merge functions, like average, and more complex ones like\nTransformer. This allows the MVL model to entirely ignore the missing views,\nenhancing its predictive robustness. We experiment on four EO datasets with\ntemporal and static views, including state-of-the-art methods from the EO\ndomain. The results indicate that our methods improve model robustness under\nconditions of moderate missingness, and improve the predictive performance when\nall views are present. The proposed methods offer a single adaptive solution to\noperate effectively with any combination of available views.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01132v1",
    "published_date": "2025-01-02 08:17:27 UTC",
    "updated_date": "2025-01-02 08:17:27 UTC"
  },
  {
    "arxiv_id": "2501.04718v1",
    "title": "Knowledge-Guided Biomarker Identification for Label-Free Single-Cell RNA-Seq Data: A Reinforcement Learning Perspective",
    "authors": [
      "Meng Xiao",
      "Weiliang Zhang",
      "Xiaohan Huang",
      "Hengshu Zhu",
      "Min Wu",
      "Xiaoli Li",
      "Yuanchun Zhou"
    ],
    "abstract": "Gene panel selection aims to identify the most informative genomic biomarkers\nin label-free genomic datasets. Traditional approaches, which rely on domain\nexpertise, embedded machine learning models, or heuristic-based iterative\noptimization, often introduce biases and inefficiencies, potentially obscuring\ncritical biological signals. To address these challenges, we present an\niterative gene panel selection strategy that harnesses ensemble knowledge from\nexisting gene selection algorithms to establish preliminary boundaries or prior\nknowledge, which guide the initial search space. Subsequently, we incorporate\nreinforcement learning through a reward function shaped by expert behavior,\nenabling dynamic refinement and targeted selection of gene panels. This\nintegration mitigates biases stemming from initial boundaries while\ncapitalizing on RL's stochastic adaptability. Comprehensive comparative\nexperiments, case studies, and downstream analyses demonstrate the\neffectiveness of our method, highlighting its improved precision and efficiency\nfor label-free biomarker discovery. Our results underscore the potential of\nthis approach to advance single-cell genomics data analysis.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:2406.07418",
    "pdf_url": "http://arxiv.org/pdf/2501.04718v1",
    "published_date": "2025-01-02 07:57:41 UTC",
    "updated_date": "2025-01-02 07:57:41 UTC"
  },
  {
    "arxiv_id": "2501.01123v1",
    "title": "TED: Turn Emphasis with Dialogue Feature Attention for Emotion Recognition in Conversation",
    "authors": [
      "Junya Ono",
      "Hiromi Wakaki"
    ],
    "abstract": "Emotion recognition in conversation (ERC) has been attracting attention by\nmethods for modeling multi-turn contexts. The multi-turn input to a pretraining\nmodel implicitly assumes that the current turn and other turns are\ndistinguished during the training process by inserting special tokens into the\ninput sequence. This paper proposes a priority-based attention method to\ndistinguish each turn explicitly by adding dialogue features into the attention\nmechanism, called Turn Emphasis with Dialogue (TED). It has a priority for each\nturn according to turn position and speaker information as dialogue features.\nIt takes multi-head self-attention between turn-based vectors for multi-turn\ninput and adjusts attention scores with the dialogue features. We evaluate TED\non four typical benchmarks. The experimental results demonstrate that TED has\nhigh overall performance in all datasets and achieves state-of-the-art\nperformance on IEMOCAP with numerous turns.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "past activity in 2021",
    "pdf_url": "http://arxiv.org/pdf/2501.01123v1",
    "published_date": "2025-01-02 07:44:48 UTC",
    "updated_date": "2025-01-02 07:44:48 UTC"
  },
  {
    "arxiv_id": "2501.01120v1",
    "title": "Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning",
    "authors": [
      "Jian Lang",
      "Zhangtao Cheng",
      "Ting Zhong",
      "Fan Zhou"
    ],
    "abstract": "Multimodal learning with incomplete modality is practical and challenging.\nRecently, researchers have focused on enhancing the robustness of pre-trained\nMultiModal Transformers (MMTs) under missing modality conditions by applying\nlearnable prompts. However, these prompt-based methods face several\nlimitations: (1) incomplete modalities provide restricted modal cues for\ntask-specific inference, (2) dummy imputation for missing content causes\ninformation loss and introduces noise, and (3) static prompts are\ninstance-agnostic, offering limited knowledge for instances with various\nmissing conditions. To address these issues, we propose RAGPT, a novel\nRetrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three\nmodules: (I) the multi-channel retriever, which identifies similar instances\nthrough a within-modality retrieval strategy, (II) the missing modality\ngenerator, which recovers missing information using retrieved contexts, and\n(III) the context-aware prompter, which captures contextual knowledge from\nrelevant instances and generates dynamic prompts to largely enhance the MMT's\nrobustness. Extensive experiments conducted on three real-world datasets show\nthat RAGPT consistently outperforms all competitive baselines in handling\nincomplete modality problems. The code of our work and prompt-based baselines\nis available at https://github.com/Jian-Lang/RAGPT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 8 figures. Accepted by AAAI 2025. Codes are released at\n  https://github.com/Jian-Lang/RAGPT",
    "pdf_url": "http://arxiv.org/pdf/2501.01120v1",
    "published_date": "2025-01-02 07:39:48 UTC",
    "updated_date": "2025-01-02 07:39:48 UTC"
  },
  {
    "arxiv_id": "2501.01118v1",
    "title": "Pruning-based Data Selection and Network Fusion for Efficient Deep Learning",
    "authors": [
      "Humaira Kousar",
      "Hasnain Irshad Bhatti",
      "Jaekyun Moon"
    ],
    "abstract": "Efficient data selection is essential for improving the training efficiency\nof deep neural networks and reducing the associated annotation costs. However,\ntraditional methods tend to be computationally expensive, limiting their\nscalability and real-world applicability. We introduce PruneFuse, a novel\nmethod that combines pruning and network fusion to enhance data selection and\naccelerate network training. In PruneFuse, the original dense network is pruned\nto generate a smaller surrogate model that efficiently selects the most\ninformative samples from the dataset. Once this iterative data selection\nselects sufficient samples, the insights learned from the pruned model are\nseamlessly integrated with the dense model through network fusion, providing an\noptimized initialization that accelerates training. Extensive experimentation\non various datasets demonstrates that PruneFuse significantly reduces\ncomputational costs for data selection, achieves better performance than\nbaselines, and accelerates the overall training process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Workshop on Attributing Model Behavior at Scale\n  (ATTRIB)",
    "pdf_url": "http://arxiv.org/pdf/2501.01118v1",
    "published_date": "2025-01-02 07:35:53 UTC",
    "updated_date": "2025-01-02 07:35:53 UTC"
  },
  {
    "arxiv_id": "2501.01117v1",
    "title": "Robust COVID-19 Detection from Cough Sounds using Deep Neural Decision Tree and Forest: A Comprehensive Cross-Datasets Evaluation",
    "authors": [
      "Rofiqul Islam",
      "Nihad Karim Chowdhury",
      "Muhammad Ashad Kabir"
    ],
    "abstract": "This research presents a robust approach to classifying COVID-19 cough sounds\nusing cutting-edge machine-learning techniques. Leveraging deep neural decision\ntrees and deep neural decision forests, our methodology demonstrates consistent\nperformance across diverse cough sound datasets. We begin with a comprehensive\nextraction of features to capture a wide range of audio features from\nindividuals, whether COVID-19 positive or negative. To determine the most\nimportant features, we use recursive feature elimination along with\ncross-validation. Bayesian optimization fine-tunes hyper-parameters of deep\nneural decision tree and deep neural decision forest models. Additionally, we\nintegrate the SMOTE during training to ensure a balanced representation of\npositive and negative data. Model performance refinement is achieved through\nthreshold optimization, maximizing the ROC-AUC score. Our approach undergoes a\ncomprehensive evaluation in five datasets: Cambridge, Coswara, COUGHVID,\nVirufy, and the combined Virufy with the NoCoCoDa dataset. Consistently\noutperforming state-of-the-art methods, our proposed approach yields notable\nAUC scores of 0.97, 0.98, 0.92, 0.93, 0.99, and 0.99 across the respective\ndatasets. Merging all datasets into a combined dataset, our method, using a\ndeep neural decision forest classifier, achieves an AUC of 0.97. Also, our\nstudy includes a comprehensive cross-datasets analysis, revealing demographic\nand geographic differences in the cough sounds associated with COVID-19. These\ndifferences highlight the challenges in transferring learned features across\ndiverse datasets and underscore the potential benefits of dataset integration,\nimproving generalizability and enhancing COVID-19 detection from audio signals.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "39 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.01117v1",
    "published_date": "2025-01-02 07:35:06 UTC",
    "updated_date": "2025-01-02 07:35:06 UTC"
  },
  {
    "arxiv_id": "2501.01110v1",
    "title": "MalCL: Leveraging GAN-Based Generative Replay to Combat Catastrophic Forgetting in Malware Classification",
    "authors": [
      "Jimin Park",
      "AHyun Ji",
      "Minji Park",
      "Mohammad Saidur Rahman",
      "Se Eun Oh"
    ],
    "abstract": "Continual Learning (CL) for malware classification tackles the rapidly\nevolving nature of malware threats and the frequent emergence of new types.\nGenerative Replay (GR)-based CL systems utilize a generative model to produce\nsynthetic versions of past data, which are then combined with new data to\nretrain the primary model. Traditional machine learning techniques in this\ndomain often struggle with catastrophic forgetting, where a model's performance\non old data degrades over time.\n  In this paper, we introduce a GR-based CL system that employs Generative\nAdversarial Networks (GANs) with feature matching loss to generate high-quality\nmalware samples. Additionally, we implement innovative selection schemes for\nreplay samples based on the model's hidden representations.\n  Our comprehensive evaluation across Windows and Android malware datasets in a\nclass-incremental learning scenario -- where new classes are introduced\ncontinuously over multiple tasks -- demonstrates substantial performance\nimprovements over previous methods. For example, our system achieves an average\naccuracy of 55% on Windows malware samples, significantly outperforming other\nGR-based models by 28%. This study provides practical insights for advancing\nGR-based malware classification systems. The implementation is available at\n\\url {https://github.com/MalwareReplayGAN/MalCL}\\footnote{The code will be made\npublic upon the presentation of the paper}.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted paper at AAAI 2025. 9 pages, Figure 6, Table 1",
    "pdf_url": "http://arxiv.org/pdf/2501.01110v1",
    "published_date": "2025-01-02 07:15:31 UTC",
    "updated_date": "2025-01-02 07:15:31 UTC"
  },
  {
    "arxiv_id": "2501.01109v1",
    "title": "BatStyler: Advancing Multi-category Style Generation for Source-free Domain Generalization",
    "authors": [
      "Xiusheng Xu",
      "Lei Qi",
      "Jingyang Zhou",
      "Xin Geng"
    ],
    "abstract": "Source-Free Domain Generalization (SFDG) aims to develop a model that\nperforms on unseen domains without relying on any source domains. However, the\nimplementation remains constrained due to the unavailability of training data.\nResearch on SFDG focus on knowledge transfer of multi-modal models and style\nsynthesis based on joint space of multiple modalities, thus eliminating the\ndependency on source domain images. However, existing works primarily work for\nmulti-domain and less-category configuration, but performance on multi-domain\nand multi-category configuration is relatively poor. In addition, the\nefficiency of style synthesis also deteriorates in multi-category scenarios.\nHow to efficiently synthesize sufficiently diverse data and apply it to\nmulti-category configuration is a direction with greater practical value. In\nthis paper, we propose a method called BatStyler, which is utilized to improve\nthe capability of style synthesis in multi-category scenarios. BatStyler\nconsists of two modules: Coarse Semantic Generation and Uniform Style\nGeneration modules. The Coarse Semantic Generation module extracts\ncoarse-grained semantics to prevent the compression of space for style\ndiversity learning in multi-category configuration, while the Uniform Style\nGeneration module provides a template of styles that are uniformly distributed\nin space and implements parallel training. Extensive experiments demonstrate\nthat our method exhibits comparable performance on less-category datasets,\nwhile surpassing state-of-the-art methods on multi-category datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE TCSVT",
    "pdf_url": "http://arxiv.org/pdf/2501.01109v1",
    "published_date": "2025-01-02 07:14:23 UTC",
    "updated_date": "2025-01-02 07:14:23 UTC"
  },
  {
    "arxiv_id": "2501.01108v2",
    "title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization",
    "authors": [
      "Haina Zhu",
      "Yizhi Zhou",
      "Hangting Chen",
      "Jianwei Yu",
      "Ziyang Ma",
      "Rongzhi Gu",
      "Yi Luo",
      "Wei Tan",
      "Xie Chen"
    ],
    "abstract": "Recent years have witnessed the success of foundation models pre-trained with\nself-supervised learning (SSL) in various music informatics understanding\ntasks, including music tagging, instrument classification, key detection, and\nmore. In this paper, we propose a self-supervised music representation learning\nmodel for music understanding. Distinguished from previous studies adopting\nrandom projection or existing neural codec, the proposed model, named MuQ, is\ntrained to predict tokens generated by Mel Residual Vector Quantization\n(Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel\nspectrum quantization to enhance the stability and efficiency of target\nextraction and lead to better performance. Experiments in a large variety of\ndownstream tasks demonstrate that MuQ outperforms previous self-supervised\nmusic representation models with only 0.9K hours of open-source pre-training\ndata. Scaling up the data to over 160K hours and adopting iterative training\nconsistently improve the model performance. To further validate the strength of\nour model, we present MuQ-MuLan, a joint music-text embedding model based on\ncontrastive learning, which achieves state-of-the-art performance in the\nzero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints\nare open source in https://github.com/tencent-ailab/MuQ.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01108v2",
    "published_date": "2025-01-02 07:08:29 UTC",
    "updated_date": "2025-01-03 08:35:34 UTC"
  },
  {
    "arxiv_id": "2501.01103v1",
    "title": "learning discriminative features from spectrograms using center loss for speech emotion recognition",
    "authors": [
      "Dongyang Dai",
      "Zhiyong Wu",
      "Runnan Li",
      "Xixin Wu",
      "Jia Jia",
      "Helen Meng"
    ],
    "abstract": "Identifying the emotional state from speech is essential for the natural\ninteraction of the machine with the speaker. However, extracting effective\nfeatures for emotion recognition is difficult, as emotions are ambiguous. We\npropose a novel approach to learn discriminative features from variable length\nspectrograms for emotion recognition by cooperating softmax cross-entropy loss\nand center loss together. The softmax cross-entropy loss enables features from\ndifferent emotion categories separable, and center loss efficiently pulls the\nfeatures belonging to the same emotion category to their center. By combining\nthe two losses together, the discriminative power will be highly enhanced,\nwhich leads to network learning more effective features for emotion\nrecognition. As demonstrated by the experimental results, after introducing\ncenter loss, both the unweighted accuracy and weighted accuracy are improved by\nover 3\\% on Mel-spectrogram input, and more than 4\\% on Short Time Fourier\nTransform spectrogram input.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at ICASSP 2019",
    "pdf_url": "http://arxiv.org/pdf/2501.01103v1",
    "published_date": "2025-01-02 06:52:28 UTC",
    "updated_date": "2025-01-02 06:52:28 UTC"
  },
  {
    "arxiv_id": "2501.01102v1",
    "title": "Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-trained BERT",
    "authors": [
      "Dongyang Dai",
      "Zhiyong Wu",
      "Shiyin Kang",
      "Xixin Wu",
      "Jia Jia",
      "Dan Su",
      "Dong Yu",
      "Helen Meng"
    ],
    "abstract": "Grapheme-to-phoneme (G2P) conversion serves as an essential component in\nChinese Mandarin text-to-speech (TTS) system, where polyphone disambiguation is\nthe core issue. In this paper, we propose an end-to-end framework to predict\nthe pronunciation of a polyphonic character, which accepts sentence containing\npolyphonic character as input in the form of Chinese character sequence without\nthe necessity of any preprocessing. The proposed method consists of a\npre-trained bidirectional encoder representations from Transformers (BERT)\nmodel and a neural network (NN) based classifier. The pre-trained BERT model\nextracts semantic features from a raw Chinese character sequence and the NN\nbased classifier predicts the polyphonic character's pronunciation according to\nBERT output. In out experiments, we implemented three classifiers, a\nfully-connected network based classifier, a long short-term memory (LSTM)\nnetwork based classifier and a Transformer block based classifier. The\nexperimental results compared with the baseline approach based on LSTM\ndemonstrate that, the pre-trained model extracts effective semantic features,\nwhich greatly enhances the performance of polyphone disambiguation. In\naddition, we also explored the impact of contextual information on polyphone\ndisambiguation.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at INTERSPEECH 2019",
    "pdf_url": "http://arxiv.org/pdf/2501.01102v1",
    "published_date": "2025-01-02 06:51:52 UTC",
    "updated_date": "2025-01-02 06:51:52 UTC"
  },
  {
    "arxiv_id": "2501.01094v1",
    "title": "MMVA: Multimodal Matching Based on Valence and Arousal across Images, Music, and Musical Captions",
    "authors": [
      "Suhwan Choi",
      "Kyu Won Kim",
      "Myungjoo Kang"
    ],
    "abstract": "We introduce Multimodal Matching based on Valence and Arousal (MMVA), a\ntri-modal encoder framework designed to capture emotional content across\nimages, music, and musical captions. To support this framework, we expand the\nImage-Music-Emotion-Matching-Net (IMEMNet) dataset, creating IMEMNet-C which\nincludes 24,756 images and 25,944 music clips with corresponding musical\ncaptions. We employ multimodal matching scores based on the continuous valence\n(emotional positivity) and arousal (emotional intensity) values. This\ncontinuous matching score allows for random sampling of image-music pairs\nduring training by computing similarity scores from the valence-arousal values\nacross different modalities. Consequently, the proposed approach achieves\nstate-of-the-art performance in valence-arousal prediction tasks. Furthermore,\nthe framework demonstrates its efficacy in various zeroshot tasks, highlighting\nthe potential of valence and arousal predictions in downstream applications.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Paper accepted in Artificial Intelligence for Music workshop at AAAI\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2501.01094v1",
    "published_date": "2025-01-02 06:36:09 UTC",
    "updated_date": "2025-01-02 06:36:09 UTC"
  },
  {
    "arxiv_id": "2501.01073v1",
    "title": "Graph Generative Pre-trained Transformer",
    "authors": [
      "Xiaohui Chen",
      "Yinkai Wang",
      "Jiaxing He",
      "Yuanqi Du",
      "Soha Hassoun",
      "Xiaolin Xu",
      "Li-Ping Liu"
    ],
    "abstract": "Graph generation is a critical task in numerous domains, including molecular\ndesign and social network analysis, due to its ability to model complex\nrelationships and structured data. While most modern graph generative models\nutilize adjacency matrix representations, this work revisits an alternative\napproach that represents graphs as sequences of node set and edge set. We\nadvocate for this approach due to its efficient encoding of graphs and propose\na novel representation. Based on this representation, we introduce the Graph\nGenerative Pre-trained Transformer (G2PT), an auto-regressive model that learns\ngraph structures via next-token prediction. To further exploit G2PT's\ncapabilities as a general-purpose foundation model, we explore fine-tuning\nstrategies for two downstream applications: goal-oriented generation and graph\nproperty prediction. We conduct extensive experiments across multiple datasets.\nResults indicate that G2PT achieves superior generative performance on both\ngeneric graph and molecule datasets. Furthermore, G2PT exhibits strong\nadaptability and versatility in downstream tasks from molecular design to\nproperty prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2501.01073v1",
    "published_date": "2025-01-02 05:44:11 UTC",
    "updated_date": "2025-01-02 05:44:11 UTC"
  },
  {
    "arxiv_id": "2501.02007v1",
    "title": "TART: Token-based Architecture Transformer for Neural Network Performance Prediction",
    "authors": [
      "Yannis Y. He"
    ],
    "abstract": "In the realm of neural architecture design, achieving high performance is\nlargely reliant on the manual expertise of researchers. Despite the emergence\nof Neural Architecture Search (NAS) as a promising technique for automating\nthis process, current NAS methods still require human input to expand the\nsearch space and cannot generate new architectures. This paper explores the\npotential of Transformers in comprehending neural architectures and their\nperformance, with the objective of establishing the foundation for utilizing\nTransformers to generate novel networks. We propose the Token-based\nArchitecture Transformer (TART), which predicts neural network performance\nwithout the need to train candidate networks. TART attains state-of-the-art\nperformance on the DeepNets-1M dataset for performance prediction tasks without\nedge information, indicating the potential of Transformers to aid in\ndiscovering novel and high-performing neural architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02007v1",
    "published_date": "2025-01-02 05:22:17 UTC",
    "updated_date": "2025-01-02 05:22:17 UTC"
  },
  {
    "arxiv_id": "2501.01477v1",
    "title": "A Survey of Deep Learning Methods in Protein Bioinformatics and its Impact on Protein Design",
    "authors": [
      "Weihang Dai"
    ],
    "abstract": "Proteins are sequences of amino acids that serve as the basic building blocks\nof living organisms. Despite rapidly growing databases documenting structural\nand functional information for various protein sequences, our understanding of\nproteins remains limited because of the large possible sequence space and the\ncomplex inter- and intra-molecular forces. Deep learning, which is\ncharacterized by its ability to learn relevant features directly from large\ndatasets, has demonstrated remarkable performance in fields such as computer\nvision and natural language processing. It has also been increasingly applied\nin recent years to the data-rich domain of protein sequences with great\nsuccess, most notably with Alphafold2's breakout performance in the protein\nstructure prediction. The performance improvements achieved by deep learning\nunlocks new possibilities in the field of protein bioinformatics, including\nprotein design, one of the most difficult but useful tasks. In this paper, we\nbroadly categorize problems in protein bioinformatics into three main\ncategories: 1) structural prediction, 2) functional prediction, and 3) protein\ndesign, and review the progress achieved from using deep learning methodologies\nin each of them. We expand on the main challenges of the protein design problem\nand highlight how advances in structural and functional prediction have\ndirectly contributed to design tasks. Finally, we conclude by identifying\nimportant topics and future research directions.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "PhD Qualifying Exam (2021)",
    "pdf_url": "http://arxiv.org/pdf/2501.01477v1",
    "published_date": "2025-01-02 05:21:34 UTC",
    "updated_date": "2025-01-02 05:21:34 UTC"
  },
  {
    "arxiv_id": "2501.01056v1",
    "title": "Risks of Cultural Erasure in Large Language Models",
    "authors": [
      "Rida Qadri",
      "Aida M. Davani",
      "Kevin Robinson",
      "Vinodkumar Prabhakaran"
    ],
    "abstract": "Large language models are increasingly being integrated into applications\nthat shape the production and discovery of societal knowledge such as search,\nonline education, and travel planning. As a result, language models will shape\nhow people learn about, perceive and interact with global cultures making it\nimportant to consider whose knowledge systems and perspectives are represented\nin models. Recognizing this importance, increasingly work in Machine Learning\nand NLP has focused on evaluating gaps in global cultural representational\ndistribution within outputs. However, more work is needed on developing\nbenchmarks for cross-cultural impacts of language models that stem from a\nnuanced sociologically-aware conceptualization of cultural impact or harm. We\njoin this line of work arguing for the need of metricizable evaluations of\nlanguage technologies that interrogate and account for historical power\ninequities and differential impacts of representation on global cultures,\nparticularly for cultures already under-represented in the digital corpora. We\nlook at two concepts of erasure: omission: where cultures are not represented\nat all and simplification i.e. when cultural complexity is erased by presenting\none-dimensional views of a rich culture. The former focuses on whether\nsomething is represented, and the latter on how it is represented. We focus our\nanalysis on two task contexts with the potential to influence global cultural\nproduction. First, we probe representations that a language model produces\nabout different places around the world when asked to describe these contexts.\nSecond, we analyze the cultures represented in the travel recommendations\nproduced by a set of language model applications. Our study shows ways in which\nthe NLP community and application developers can begin to operationalize\ncomplex socio-cultural considerations into standard evaluations and benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01056v1",
    "published_date": "2025-01-02 04:57:50 UTC",
    "updated_date": "2025-01-02 04:57:50 UTC"
  },
  {
    "arxiv_id": "2501.02006v1",
    "title": "Multi-Task Semantic Communication With Graph Attention-Based Feature Correlation Extraction",
    "authors": [
      "Xi Yu",
      "Tiejun Lv",
      "Weicai Li",
      "Wei Ni",
      "Dusit Niyato",
      "Ekram Hossain"
    ],
    "abstract": "Multi-task semantic communication can serve multiple learning tasks using a\nshared encoder model. Existing models have overlooked the intricate\nrelationships between features extracted during an encoding process of tasks.\nThis paper presents a new graph attention inter-block (GAI) module to the\nencoder/transmitter of a multi-task semantic communication system, which\nenriches the features for multiple tasks by embedding the intermediate outputs\nof encoding in the features, compared to the existing techniques. The key idea\nis that we interpret the outputs of the intermediate feature extraction blocks\nof the encoder as the nodes of a graph to capture the correlations of the\nintermediate features. Another important aspect is that we refine the node\nrepresentation using a graph attention mechanism to extract the correlations\nand a multi-layer perceptron network to associate the node representations with\ndifferent tasks. Consequently, the intermediate features are weighted and\nembedded into the features transmitted for executing multiple tasks at the\nreceiver. Experiments demonstrate that the proposed model surpasses the most\ncompetitive and publicly available models by 11.4% on the CityScapes 2Task\ndataset and outperforms the established state-of-the-art by 3.97% on the NYU V2\n3Task dataset, respectively, when the bandwidth ratio of the communication\nchannel (i.e., compression level for transmission over the channel) is as\nconstrained as 1 12 .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages,11 figures, accepted by IEEE TMC",
    "pdf_url": "http://arxiv.org/pdf/2501.02006v1",
    "published_date": "2025-01-02 04:38:01 UTC",
    "updated_date": "2025-01-02 04:38:01 UTC"
  },
  {
    "arxiv_id": "2501.01039v1",
    "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
    "authors": [
      "Yixing Xu",
      "Shivank Nag",
      "Dong Li",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "abstract": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01039v1",
    "published_date": "2025-01-02 03:41:32 UTC",
    "updated_date": "2025-01-02 03:41:32 UTC"
  },
  {
    "arxiv_id": "2501.01037v1",
    "title": "MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception",
    "authors": [
      "Xiaoshuai Hao",
      "Guanqun Liu",
      "Yuting Zhao",
      "Yuheng Ji",
      "Mengchuan Wei",
      "Haimei Zhao",
      "Lingdong Kong",
      "Rong Yin",
      "Yu Liu"
    ],
    "abstract": "Multi-sensor fusion models play a crucial role in autonomous driving\nperception, particularly in tasks like 3D object detection and HD map\nconstruction. These models provide essential and comprehensive static\nenvironmental information for autonomous driving systems. While camera-LiDAR\nfusion methods have shown promising results by integrating data from both\nmodalities, they often depend on complete sensor inputs. This reliance can lead\nto low robustness and potential failures when sensors are corrupted or missing,\nraising significant safety concerns. To tackle this challenge, we introduce the\nMulti-Sensor Corruption Benchmark (MSC-Bench), the first comprehensive\nbenchmark aimed at evaluating the robustness of multi-sensor autonomous driving\nperception models against various sensor corruptions. Our benchmark includes 16\ncombinations of corruption types that disrupt both camera and LiDAR inputs,\neither individually or concurrently. Extensive evaluations of six 3D object\ndetection models and four HD map construction models reveal substantial\nperformance degradation under adverse weather conditions and sensor failures,\nunderscoring critical safety issues. The benchmark toolkit and affiliated code\nand model checkpoints have been made publicly accessible.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01037v1",
    "published_date": "2025-01-02 03:38:46 UTC",
    "updated_date": "2025-01-02 03:38:46 UTC"
  },
  {
    "arxiv_id": "2501.01031v3",
    "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning",
    "authors": [
      "Wonduk Seo",
      "Zonghao Yuan",
      "Yi Bu"
    ],
    "abstract": "Ensuring cultural values alignment in Large Language Models (LLMs) remains a\ncritical challenge, as these models often embed Western-centric biases from\ntheir training data, leading to misrepresentations and fairness concerns in\ncross-cultural applications. Existing approaches such as role assignment and\nfew-shot learning struggle to address these limitations effectively due to\ntheir reliance on pre-trained knowledge, limited scalability, and inability to\ncapture nuanced cultural values. To address these issues, we propose ValuesRAG,\na novel and effective framework that applies Retrieval-Augmented Generation\n(RAG) with In-Context Learning (ICL) to integrate cultural and demographic\nknowledge dynamically during text generation. Leveraging the World Values\nSurvey (WVS) dataset, ValuesRAG first generates summaries of values for each\nindividual. We subsequently curate several representative regional datasets to\nserve as test datasets and retrieve relevant summaries of values based on\ndemographic features, followed by a reranking step to select the top-k relevant\nsummaries. We evaluate ValuesRAG using 6 diverse regional datasets and show\nthat it consistently outperforms baselines: including zero-shot,\nrole-assignment, few-shot, and hybrid methods, both in main experiments and\nablation settings. Notably, ValuesRAG achieves the best overall performance\nover prior methods, demonstrating its effectiveness in fostering culturally\naligned and inclusive AI systems. Our findings underscore the potential of\ndynamic retrieval-based methods to bridge the gap between global LLM\ncapabilities and localized cultural values.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2501.01031v3",
    "published_date": "2025-01-02 03:26:13 UTC",
    "updated_date": "2025-05-08 01:07:15 UTC"
  },
  {
    "arxiv_id": "2501.01030v2",
    "title": "Reasoning based on symbolic and parametric knowledge bases: a survey",
    "authors": [
      "Mayi Xu",
      "Yunfeng Ning",
      "Yongqi Li",
      "Jianhao Chen",
      "Jintao Wen",
      "Yao Xiao",
      "Shen Zhou",
      "Birong Pan",
      "Zepeng Bao",
      "Xin Miao",
      "Hankun Kang",
      "Ke Sun",
      "Tieyun Qian"
    ],
    "abstract": "Reasoning is fundamental to human intelligence, and critical for\nproblem-solving, decision-making, and critical thinking. Reasoning refers to\ndrawing new conclusions based on existing knowledge, which can support various\napplications like clinical diagnosis, basic education, and financial analysis.\nThough a good number of surveys have been proposed for reviewing\nreasoning-related methods, none of them has systematically investigated these\nmethods from the viewpoint of their dependent knowledge base. Both the\nscenarios to which the knowledge bases are applied and their storage formats\nare significantly different. Hence, investigating reasoning methods from the\nknowledge base perspective helps us better understand the challenges and future\ndirections. To fill this gap, this paper first classifies the knowledge base\ninto symbolic and parametric ones. The former explicitly stores information in\nhuman-readable symbols, and the latter implicitly encodes knowledge within\nparameters. Then, we provide a comprehensive overview of reasoning methods\nusing symbolic knowledge bases, parametric knowledge bases, and both of them.\nFinally, we identify the future direction toward enhancing reasoning\ncapabilities to bridge the gap between human and machine intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "There are imperfections in some parts of the paper, which may lead to\n  misunderstandings among readers. To be rigorous, we apply for the withdrawal\n  of this paper.",
    "pdf_url": "http://arxiv.org/pdf/2501.01030v2",
    "published_date": "2025-01-02 03:21:32 UTC",
    "updated_date": "2025-02-21 16:53:35 UTC"
  },
  {
    "arxiv_id": "2501.01025v2",
    "title": "Towards Adversarially Robust Deep Metric Learning",
    "authors": [
      "Xiaopeng Ke"
    ],
    "abstract": "Deep Metric Learning (DML) has shown remarkable successes in many domains by\ntaking advantage of powerful deep neural networks. Deep neural networks are\nprone to adversarial attacks and could be easily fooled by adversarial\nexamples. The current progress on this robustness issue is mainly about deep\nclassification models but pays little attention to DML models. Existing works\nfail to thoroughly inspect the robustness of DML and neglect an important DML\nscenario, the clustering-based inference. In this work, we first point out the\nrobustness issue of DML models in clustering-based inference scenarios. We find\nthat, for the clustering-based inference, existing defenses designed DML are\nunable to be reused and the adaptions of defenses designed for deep\nclassification models cannot achieve satisfactory robustness performance. To\nalleviate the hazard of adversarial examples, we propose a new defense, the\nEnsemble Adversarial Training (EAT), which exploits ensemble learning and\nadversarial training. EAT promotes the diversity of the ensemble, encouraging\neach model in the ensemble to have different robustness features, and employs a\nself-transferring mechanism to make full use of the robustness statistics of\nthe whole ensemble in the update of every single model. We evaluate the EAT\nmethod on three widely-used datasets with two popular model architectures. The\nresults show that the proposed EAT method greatly outperforms the adaptions of\ndefenses designed for deep classification models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01025v2",
    "published_date": "2025-01-02 03:15:25 UTC",
    "updated_date": "2025-01-12 04:43:46 UTC"
  },
  {
    "arxiv_id": "2501.01014v1",
    "title": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based on Large language Model",
    "authors": [
      "Chengze Zhang",
      "Changshan Li",
      "Shiyang Gao"
    ],
    "abstract": "The exponential growth of data and advancements in big data technologies have\ncreated a demand for more efficient and automated approaches to data analysis\nand storytelling. However, automated data analysis systems still face\nchallenges in leveraging large language models (LLMs) for data insight\ndiscovery, augmented analysis, and data storytelling. This paper introduces the\nMultidimensional Data Storytelling Framework (MDSF) based on large language\nmodels for automated insight generation and context-aware storytelling. The\nframework incorporates advanced preprocessing techniques, augmented analysis\nalgorithms, and a unique scoring mechanism to identify and prioritize\nactionable insights. The use of fine-tuned LLMs enhances contextual\nunderstanding and generates narratives with minimal manual intervention. The\narchitecture also includes an agent-based mechanism for real-time storytelling\ncontinuation control. Key findings reveal that MDSF outperforms existing\nmethods across various datasets in terms of insight ranking accuracy,\ndescriptive quality, and narrative coherence. The experimental evaluation\ndemonstrates MDSF's ability to automate complex analytical tasks, reduce\ninterpretive biases, and improve user satisfaction. User studies further\nunderscore its practical utility in enhancing content structure, conclusion\nextraction, and richness of detail.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01014v1",
    "published_date": "2025-01-02 02:35:38 UTC",
    "updated_date": "2025-01-02 02:35:38 UTC"
  },
  {
    "arxiv_id": "2501.01010v2",
    "title": "CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction",
    "authors": [
      "Mohammad Shahab Sepehri",
      "Asal Mehradfar",
      "Mahdi Soltanolkotabi",
      "Salman Avestimehr"
    ],
    "abstract": "Predicting Bitcoin price remains a challenging problem due to the high\nvolatility and complex non-linear dynamics of cryptocurrency markets.\nTraditional time-series models, such as ARIMA and GARCH, and recurrent neural\nnetworks, like LSTMs, have been widely applied to this task but struggle to\ncapture the regime shifts and long-range dependencies inherent in the data. In\nthis work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM)\narchitecture designed to effectively capture long-range dependencies in\nfinancial time-series data. Our experiments show that CryptoMamba not only\nprovides more accurate predictions but also offers enhanced generalizability\nacross different market conditions, surpassing the limitations of previous\nmodels. Coupled with trading algorithms for real-world scenarios, CryptoMamba\ndemonstrates its practical utility by translating accurate forecasts into\nfinancial outcomes. Our findings signal a huge advantage for SSMs in stock and\ncryptocurrency price forecasting tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in IEEE International Conference on Blockchain and\n  Cryptocurrency (ICBC) 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.01010v2",
    "published_date": "2025-01-02 02:16:56 UTC",
    "updated_date": "2025-05-04 03:10:08 UTC"
  },
  {
    "arxiv_id": "2501.01007v1",
    "title": "Deep Reinforcement Learning for Job Scheduling and Resource Management in Cloud Computing: An Algorithm-Level Review",
    "authors": [
      "Yan Gu",
      "Zhaoze Liu",
      "Shuhong Dai",
      "Cong Liu",
      "Ying Wang",
      "Shen Wang",
      "Georgios Theodoropoulos",
      "Long Cheng"
    ],
    "abstract": "Cloud computing has revolutionized the provisioning of computing resources,\noffering scalable, flexible, and on-demand services to meet the diverse\nrequirements of modern applications. At the heart of efficient cloud operations\nare job scheduling and resource management, which are critical for optimizing\nsystem performance and ensuring timely and cost-effective service delivery.\nHowever, the dynamic and heterogeneous nature of cloud environments presents\nsignificant challenges for these tasks, as workloads and resource availability\ncan fluctuate unpredictably. Traditional approaches, including heuristic and\nmeta-heuristic algorithms, often struggle to adapt to these real-time changes\ndue to their reliance on static models or predefined rules. Deep Reinforcement\nLearning (DRL) has emerged as a promising solution to these challenges by\nenabling systems to learn and adapt policies based on continuous observations\nof the environment, facilitating intelligent and responsive decision-making.\nThis survey provides a comprehensive review of DRL-based algorithms for job\nscheduling and resource management in cloud computing, analyzing their\nmethodologies, performance metrics, and practical applications. We also\nhighlight emerging trends and future research directions, offering valuable\ninsights into leveraging DRL to advance both job scheduling and resource\nmanagement in cloud computing.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01007v1",
    "published_date": "2025-01-02 02:08:00 UTC",
    "updated_date": "2025-01-02 02:08:00 UTC"
  },
  {
    "arxiv_id": "2501.01005v2",
    "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving",
    "authors": [
      "Zihao Ye",
      "Lequn Chen",
      "Ruihang Lai",
      "Wuwei Lin",
      "Yineng Zhang",
      "Stephanie Wang",
      "Tianqi Chen",
      "Baris Kasikci",
      "Vinod Grover",
      "Arvind Krishnamurthy",
      "Luis Ceze"
    ],
    "abstract": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
    "pdf_url": "http://arxiv.org/pdf/2501.01005v2",
    "published_date": "2025-01-02 02:02:20 UTC",
    "updated_date": "2025-04-21 20:10:11 UTC"
  },
  {
    "arxiv_id": "2501.03181v2",
    "title": "FaceSpeak: Expressive and High-Quality Speech Synthesis from Human Portraits of Different Styles",
    "authors": [
      "Tian-Hao Zhang",
      "Jiawei Zhang",
      "Jun Wang",
      "Xinyuan Qian",
      "Xu-Cheng Yin"
    ],
    "abstract": "Humans can perceive speakers' characteristics (e.g., identity, gender,\npersonality and emotion) by their appearance, which are generally aligned to\ntheir voice style. Recently, vision-driven Text-to-speech (TTS) scholars\ngrounded their investigations on real-person faces, thereby restricting\neffective speech synthesis from applying to vast potential usage scenarios with\ndiverse characters and image styles. To solve this issue, we introduce a novel\nFaceSpeak approach. It extracts salient identity characteristics and emotional\nrepresentations from a wide variety of image styles. Meanwhile, it mitigates\nthe extraneous information (e.g., background, clothing, and hair color, etc.),\nresulting in synthesized speech closely aligned with a character's persona.\nFurthermore, to overcome the scarcity of multi-modal TTS data, we have devised\nan innovative dataset, namely Expressive Multi-Modal TTS, which is diligently\ncurated and annotated to facilitate research in this domain. The experimental\nresults demonstrate our proposed FaceSpeak can generate portrait-aligned voice\nwith satisfactory naturalness and quality.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.03181v2",
    "published_date": "2025-01-02 02:00:15 UTC",
    "updated_date": "2025-04-15 19:16:19 UTC"
  },
  {
    "arxiv_id": "2501.00999v2",
    "title": "Exploring Information Processing in Large Language Models: Insights from Information Bottleneck Theory",
    "authors": [
      "Zhou Yang",
      "Zhengyu Qi",
      "Zhaochun Ren",
      "Zhikai Jia",
      "Haizhou Sun",
      "Xiaofei Zhu",
      "Xiangwen Liao"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 9 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.00999v2",
    "published_date": "2025-01-02 01:33:58 UTC",
    "updated_date": "2025-01-06 01:49:09 UTC"
  },
  {
    "arxiv_id": "2501.02004v1",
    "title": "General Information Metrics for Improving AI Model Training Efficiency",
    "authors": [
      "Jianfeng Xu",
      "Congcong Liu",
      "Xiaoying Tan",
      "Xiaojie Zhu",
      "Anpeng Wu",
      "Huan Wan",
      "Weijun Kong",
      "Chun Li",
      "Hu Xu",
      "Kun Kuang",
      "Fei Wu"
    ],
    "abstract": "To address the growing size of AI model training data and the lack of a\nuniversal data selection methodology-factors that significantly drive up\ntraining costs -- this paper presents the General Information Metrics\nEvaluation (GIME) method. GIME leverages general information metrics from\nObjective Information Theory (OIT), including volume, delay, scope,\ngranularity, variety, duration, sampling rate, aggregation, coverage,\ndistortion, and mismatch to optimize dataset selection for training purposes.\nComprehensive experiments conducted across diverse domains, such as CTR\nPrediction, Civil Case Prediction, and Weather Forecasting, demonstrate that\nGIME effectively preserves model performance while substantially reducing both\ntraining time and costs. Additionally, applying GIME within the Judicial AI\nProgram led to a remarkable 39.56% reduction in total model training expenses,\nunderscoring its potential to support efficient and sustainable AI development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02004v1",
    "published_date": "2025-01-02 01:28:00 UTC",
    "updated_date": "2025-01-02 01:28:00 UTC"
  },
  {
    "arxiv_id": "2501.00989v1",
    "title": "Bootstrapped Reward Shaping",
    "authors": [
      "Jacob Adamczyk",
      "Volodymyr Makarenko",
      "Stas Tiomkin",
      "Rahul V. Kulkarni"
    ],
    "abstract": "In reinforcement learning, especially in sparse-reward domains, many\nenvironment steps are required to observe reward information. In order to\nincrease the frequency of such observations, \"potential-based reward shaping\"\n(PBRS) has been proposed as a method of providing a more dense reward signal\nwhile leaving the optimal policy invariant. However, the required \"potential\nfunction\" must be carefully designed with task-dependent knowledge to not deter\ntraining performance. In this work, we propose a \"bootstrapped\" method of\nreward shaping, termed BSRS, in which the agent's current estimate of the\nstate-value function acts as the potential function for PBRS. We provide\nconvergence proofs for the tabular setting, give insights into training\ndynamics for deep RL, and show that the proposed method improves training speed\nin the Atari suite.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAAI-2025, Main Track",
    "pdf_url": "http://arxiv.org/pdf/2501.00989v1",
    "published_date": "2025-01-02 00:40:55 UTC",
    "updated_date": "2025-01-02 00:40:55 UTC"
  },
  {
    "arxiv_id": "2501.00982v1",
    "title": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice",
    "authors": [
      "Federico Ravenda",
      "Seyed Ali Bahrainian",
      "Andrea Raballo",
      "Antonietta Mira",
      "Noriko Kando"
    ],
    "abstract": "In psychological practice, standardized questionnaires serve as essential\ntools for assessing mental constructs (e.g., attitudes, traits, and emotions)\nthrough structured questions (aka items). With the increasing prevalence of\nsocial media platforms where users share personal experiences and emotions,\nresearchers are exploring computational methods to leverage this data for rapid\nmental health screening. In this study, we propose a novel adaptive\nRetrieval-Augmented Generation (RAG) approach that completes psychological\nquestionnaires by analyzing social media posts. Our method retrieves the most\nrelevant user posts for each question in a psychological survey and uses Large\nLanguage Models (LLMs) to predict questionnaire scores in a zero-shot setting.\nOur findings are twofold. First we demonstrate that this approach can\neffectively predict users' responses to psychological questionnaires, such as\nthe Beck Depression Inventory II (BDI-II), achieving performance comparable to\nor surpassing state-of-the-art models on Reddit-based benchmark datasets\nwithout relying on training data. Second, we show how this methodology can be\ngeneralized as a scalable screening tool, as the final assessment is\nsystematically derived by completing standardized questionnaires and tracking\nhow individual item responses contribute to the diagnosis, aligning with\nestablished psychometric practices.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.00982v1",
    "published_date": "2025-01-02 00:01:54 UTC",
    "updated_date": "2025-01-02 00:01:54 UTC"
  }
]
{
  "date": "2024-09-14",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-14 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 和机器学习领域，涵盖图像处理、医疗应用、LLM 优化、强化学习和语音识别等话题。重点包括医疗 AI 的创新框架（如 Niraj K. Jha 的 COMFORT），LLM 增强问题解决能力（如 REAP 方法），以及高效计算和隐私保护技术；令人印象深刻的文章有 COMFORT 在消费医疗中的高效微调，以及 VernaCopter 在机器人规划中的可靠性提升。\n\n以下是今日论文的精选摘要，我会优先讨论重要或话题度高的文章，将相关论文归类讨论，并对次要内容快速掠过。每篇论文标题以中文 + 英文形式列出，保留核心学术术语。\n\n### AI 和 LLM 优化主题\n这些论文探索了大型语言模型（LLMs）的改进，强调了增强计算能力、解释性和鲁棒性。\n- **增强 LLM 问题解决能力：REAP 方法（Enhancing LLM Problem Solving with REAP: Reflection, Explicit Problem Deconstruction, and Advanced Prompting）**  \n  主要贡献：提出 REAP 框架，通过反思、问题分解和高级提示提升 LLM 在复杂任务中的推理性能。发现：应用于 GPT-4o 等模型后，性能提升高达 112%，并显著降低成本，提供更高效的 LLM 应用。\n- **LLM 应激响应分析（StressPrompt: Does Stress Impact Large Language Models and Human Performance Similarly?）**  \n  主要贡献：使用 StressPrompt 模拟应激状态，分析 LLM 在不同应激水平下的表现。发现：LLM 遵循 Yerkes-Dodson 定律，在中等应激下表现最佳，与人类类似，提升了对 LLM 鲁棒性的理解。\n- **符号回归与概念库（Symbolic Regression with a Learned Concept Library）**  \n  主要贡献：引入概念库结合 LLM 进行符号回归，提升模型解释性。发现：在 Feynman 等基准上超越传统方法，平均提升 19%，为复杂回归任务提供新范式。\n- **其他 LLM 相关**：如 **ASFT：绝对似然优化（ASFT: Aligned Supervised Fine-Tuning through Absolute Likelihood）**，主要贡献是通过绝对似然微调提升 LLM 偏好对齐，性能优于 DPO；以及 **TTDS：双塔动态语义推荐（Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens）**，这些快速提升了 LLM 在推荐和规划中的应用，但细节较常规。\n\n### 医疗和健康应用\n医疗 AI 论文占比高，许多涉及隐私保护和高效诊断，COMFORT 框架尤为突出。\n- **COMFORT：消费医疗的持续微调框架（COMFORT: A Continual Fine-Tuning Framework for Foundation Models Targeted at Consumer Healthcare）**  \n  主要贡献：提出 COMFORT 框架，使用参数高效微调（PEFT）适配可穿戴设备数据，实现早期疾病检测。发现：内存开销降低 52%，性能与传统方法相当，是医疗 AI 的重要进展，由 Niraj K. Jha 等知名学者发布。\n- **皮肤病诊断增强（Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM）**  \n  主要贡献：利用 Segment Anything Model (SAM) 生成视觉概念，提升皮肤图像诊断的可解释性。发现：在两个数据集上，Dice 分数达 68.40%，显著减少假阳性和假阴性，提高临床实用性。\n- **其他医疗相关**：如 **DFQ-SAM：隐私保护的量化框架（Privacy-Preserving SAM Quantization for Efficient Edge Intelligence in Healthcare）**，主要贡献是无数据量化 SAM 模型，保护医疗数据隐私；以及 **VATE：AI 教师错误分析（AI-Driven Virtual Teacher for Enhanced Educational Efficiency）**，这些在医疗教育中应用 LLM，但影响较小，仅提升效率 78%。\n\n### 计算机视觉和图像处理\n这些论文关注高效检测和鲁棒性，提升了实际应用中的性能。\n- **VernaCopter：基于形式规范的机器人规划（VernaCopter: Disambiguated Natural-Language-Driven Robot via Formal Specifications）**  \n  主要贡献：使用信号时序逻辑 (STL) 桥接自然语言和机器人任务，减少歧义。发现：在实验中更稳定可靠，适用于复杂机器人控制。\n- **PCB 缺陷检测集成学习（Enhancing Printed Circuit Board Defect Detection through Ensemble Learning）**  \n  主要贡献：构建集成学习框架，结合 EfficientDet、YOLOv5 等模型提升检测准确性。发现：准确率达 95%，显著优于单一模型，在电子制造中具实际价值。\n- **其他视觉相关**：如 **NBBOX：遥感图像增强（NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection）**，主要贡献是通过边界框变换提升检测鲁棒性；以及 **AMBER：多波段图像分割（AMBER -- Advanced SegFormer for Multi-Band Image Segmentation）**，这些改进图像处理效率，但非核心话题，仅快速提及。\n\n### 其他领域快速掠过\n剩余论文涉及强化学习、语音和隐私保护等，但影响力较小，仅简要概述：\n- **强化学习改进（Autonomous Goal Detection and Cessation in Reinforcement Learning）**：主要贡献：提出 AGDC 模块，提升 RL 在不确定环境中的目标检测，成功率高于传统方法。\n- **语音情感识别（Explaining Deep Learning Embeddings for Speech Emotion Recognition）**：主要贡献：通过探针方法解释嵌入特征，发现频谱特征对情感识别更重要。\n- **其他如分布式聚类（Distributed Clustering based on Distributional Kernel）、音频生成（Text Prompt is Not Enough）** 等，均有技术创新，但非热点，仅提升了特定任务的效率和隐私。\n\n总之，今天的 arXiv 论文展示了 AI 在医疗和计算优化上的潜力，COMFORT 和 REAP 等工作值得关注。如果您对特定领域感兴趣，建议查看这些论文的摘要！（全文控制在简洁范围内，共约 800 字）",
  "papers": [
    {
      "arxiv_id": "2409.09560v1",
      "title": "Evaluating authenticity and quality of image captions via sentiment and semantic analyses",
      "title_zh": "翻译失败",
      "authors": [
        "Aleksei Krotov",
        "Alison Tebo",
        "Dylan K. Picart",
        "Aaron Dean Algave"
      ],
      "abstract": "The growth of deep learning (DL) relies heavily on huge amounts of labelled\ndata for tasks such as natural language processing and computer vision.\nSpecifically, in image-to-text or image-to-image pipelines, opinion (sentiment)\nmay be inadvertently learned by a model from human-generated image captions.\nAdditionally, learning may be affected by the variety and diversity of the\nprovided captions. While labelling large datasets has largely relied on\ncrowd-sourcing or data-worker pools, evaluating the quality of such training\ndata is crucial.\n  This study proposes an evaluation method focused on sentiment and semantic\nrichness. That method was applied to the COCO-MS dataset, comprising\napproximately 150K images with segmented objects and corresponding\ncrowd-sourced captions. We employed pre-trained models (Twitter-RoBERTa-base\nand BERT-base) to extract sentiment scores and variability of semantic\nembeddings from captions. The relation of the sentiment score and semantic\nvariability with object categories was examined using multiple linear\nregression. Results indicate that while most captions were neutral, about 6% of\nthe captions exhibited strong sentiment influenced by specific object\ncategories. Semantic variability of within-image captions remained low and\nuncorrelated with object categories. Model-generated captions showed less than\n1.5% of strong sentiment which was not influenced by object categories and did\nnot correlate with the sentiment of the respective human-generated captions.\nThis research demonstrates an approach to assess the quality of crowd- or\nworker-sourced captions informed by image content.",
      "tldr_zh": "这篇论文提出了一种通过sentiment和semantic analyses评估图像标题真实性和质量的方法，旨在解决深度学习模型在图像标题中无意学习观点问题。研究者应用此方法到COCO-MS数据集（约15万图像及其众包标题），使用预训练模型如Twitter-RoBERTa-base和BERT-base提取情感分数和语义嵌入变异性，并通过multiple linear regression分析这些指标与对象类别的关系。结果显示，大多数标题为中性，但约6%的人类标题带有强烈sentiment，受特定对象类别影响，而语义变异性较低且与对象类别无关；相比之下，模型生成的标题中强烈sentiment少于1.5%，且不与人类标题相关。该方法为评估众包图像标题质量提供了有效途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09560v1",
      "published_date": "2024-09-14 23:50:23 UTC",
      "updated_date": "2024-09-14 23:50:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:50:58.833841"
    },
    {
      "arxiv_id": "2409.09555v1",
      "title": "Enhancing Printed Circuit Board Defect Detection through Ensemble Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ka Nam Canaan Law",
        "Mingshuo Yu",
        "Lianglei Zhang",
        "Yiyi Zhang",
        "Peng Xu",
        "Jerry Gao",
        "Jun Liu"
      ],
      "abstract": "The quality control of printed circuit boards (PCBs) is paramount in\nadvancing electronic device technology. While numerous machine learning\nmethodologies have been utilized to augment defect detection efficiency and\naccuracy, previous studies have predominantly focused on optimizing individual\nmodels for specific defect types, often overlooking the potential synergies\nbetween different approaches. This paper introduces a comprehensive inspection\nframework leveraging an ensemble learning strategy to address this gap.\nInitially, we utilize four distinct PCB defect detection models utilizing\nstate-of-the-art methods: EfficientDet, MobileNet SSDv2, Faster RCNN, and\nYOLOv5. Each method is capable of identifying PCB defects independently.\nSubsequently, we integrate these models into an ensemble learning framework to\nenhance detection performance. A comparative analysis reveals that our ensemble\nlearning framework significantly outperforms individual methods, achieving a\n95% accuracy in detecting diverse PCB defects. These findings underscore the\nefficacy of our proposed ensemble learning framework in enhancing PCB quality\ncontrol processes.",
      "tldr_zh": "本研究针对印刷电路板（PCB）缺陷检测的挑战，提出了一种基于集成学习（Ensemble Learning）的全面检查框架，以充分利用不同模型之间的协同效应。框架整合了EfficientDet、MobileNet SSDv2、Faster RCNN和YOLOv5四种先进模型，每个模型能独立识别缺陷，而通过集成策略显著提升整体性能。实验比较分析显示，该框架在检测多种PCB缺陷时达到了95%的准确率，远超单个模型的表现。该方法为提升电子设备质量控制提供了高效且可靠的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09555v1",
      "published_date": "2024-09-14 23:34:12 UTC",
      "updated_date": "2024-09-14 23:34:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:51:10.477896"
    },
    {
      "arxiv_id": "2410.03536v1",
      "title": "Computer Vision Intelligence Test Modeling and Generation: A Case Study on Smart OCR",
      "title_zh": "计算机视觉智能测试建模",
      "authors": [
        "Jing Shu",
        "Bing-Jiun Miu",
        "Eugene Chang",
        "Jerry Gao",
        "Jun Liu"
      ],
      "abstract": "AI-based systems possess distinctive characteristics and introduce challenges\nin quality evaluation at the same time. Consequently, ensuring and validating\nAI software quality is of critical importance. In this paper, we present an\neffective AI software functional testing model to address this challenge.\nSpecifically, we first present a comprehensive literature review of previous\nwork, covering key facets of AI software testing processes. We then introduce a\n3D classification model to systematically evaluate the image-based text\nextraction AI function, as well as test coverage criteria and complexity. To\nevaluate the performance of our proposed AI software quality test, we propose\nfour evaluation metrics to cover different aspects. Finally, based on the\nproposed framework and defined metrics, a mobile Optical Character Recognition\n(OCR) case study is presented to demonstrate the framework's effectiveness and\ncapability in assessing AI function quality.",
      "tldr_zh": "这篇论文针对 AI 系统质量评估的挑战，提出一个有效的 AI software functional testing 模型，以系统化地评估图像-based 文本提取功能。研究首先进行文献综述，涵盖 AI 软件测试过程的关键方面，并引入 3D classification model 来评估测试覆盖率和复杂性，同时定义四个评估指标来全面衡量模型性能。以移动 Optical Character Recognition (OCR) 为案例研究，展示了该框架在提升 AI 函数质量评估方面的有效性和实用性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.03536v1",
      "published_date": "2024-09-14 23:33:28 UTC",
      "updated_date": "2024-09-14 23:33:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:51:22.434505"
    },
    {
      "arxiv_id": "2409.09549v1",
      "title": "COMFORT: A Continual Fine-Tuning Framework for Foundation Models Targeted at Consumer Healthcare",
      "title_zh": "翻译失败",
      "authors": [
        "Chia-Hao Li",
        "Niraj K. Jha"
      ],
      "abstract": "Wearable medical sensors (WMSs) are revolutionizing smart healthcare by\nenabling continuous, real-time monitoring of user physiological signals,\nespecially in the field of consumer healthcare. The integration of WMSs and\nmodern machine learning (ML) enables unprecedented solutions to efficient\nearly-stage disease detection. Despite the success of Transformers in various\nfields, their application to sensitive domains, such as smart healthcare,\nremains underexplored due to limited data accessibility and privacy concerns.\nTo bridge the gap between Transformer-based foundation models and WMS-based\ndisease detection, we propose COMFORT, a continual fine-tuning framework for\nfoundation models targeted at consumer healthcare. COMFORT introduces a novel\napproach for pre-training a Transformer-based foundation model on a large\ndataset of physiological signals exclusively collected from healthy individuals\nwith commercially available WMSs. We adopt a masked data modeling (MDM)\nobjective to pre-train this health foundation model. We then fine-tune the\nmodel using various parameter-efficient fine-tuning (PEFT) methods, such as\nlow-rank adaptation (LoRA) and its variants, to adapt it to various downstream\ndisease detection tasks that rely on WMS data. In addition, COMFORT continually\nstores the low-rank decomposition matrices obtained from the PEFT algorithms to\nconstruct a library for multi-disease detection. The COMFORT library enables\nscalable and memory-efficient disease detection on edge devices. Our\nexperimental results demonstrate that COMFORT achieves highly competitive\nperformance while reducing memory overhead by up to 52% relative to\nconventional methods. Thus, COMFORT paves the way for personalized and\nproactive solutions to efficient and effective early-stage disease detection\nfor consumer healthcare.",
      "tldr_zh": "本研究提出COMFORT框架，这是一种针对消费者医疗的持续微调框架，旨在桥接Transformer-based基础模型与可穿戴医疗传感器(WMSs)的数据，用于高效的早期疾病检测。框架首先使用masked data modeling (MDM)目标在健康个体生理信号数据集上预训练Transformer模型，然后通过参数高效微调(PEFT)方法，如low-rank adaptation (LoRA)及其变体，适应各种下游疾病检测任务。COMFORT还构建了一个库来存储低秩分解矩阵，支持可扩展的、多疾病检测功能，并在边缘设备上实现内存开销降低高达52%。实验结果显示，该框架在性能上与传统方法高度竞争，同时为个性化、主动的消费者医疗解决方案铺平了道路。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 10 figures. This work has been submitted to the ACM for\n  possible publication",
      "pdf_url": "http://arxiv.org/pdf/2409.09549v1",
      "published_date": "2024-09-14 22:24:52 UTC",
      "updated_date": "2024-09-14 22:24:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:51:34.474972"
    },
    {
      "arxiv_id": "2409.13754v1",
      "title": "Increasing the Value of Information During Planning in Uncertain Environments",
      "title_zh": "在不确定环境中增加规划过程中的信息价值",
      "authors": [
        "Gaurab Pokharel"
      ],
      "abstract": "Prior studies have demonstrated that for many real-world problems, POMDPs can\nbe solved through online algorithms both quickly and with near optimality.\nHowever, on an important set of problems where there is a large time delay\nbetween when the agent can gather information and when it needs to use that\ninformation, these solutions fail to adequately consider the value of\ninformation. As a result, information gathering actions, even when they are\ncritical in the optimal policy, will be ignored by existing solutions, leading\nto sub-optimal decisions by the agent. In this research, we develop a novel\nsolution that rectifies this problem by introducing a new algorithm that\nimproves upon state-of-the-art online planning by better reflecting on the\nvalue of actions that gather information. We do this by adding Entropy to the\nUCB1 heuristic in the POMCP algorithm. We test this solution on the hallway\nproblem. Results indicate that our new algorithm performs significantly better\nthan POMCP.",
      "tldr_zh": "本研究针对不确定环境中POMDPs的在线规划问题，指出现有算法在信息收集与使用存在时间延迟时，未能充分评估信息价值，导致忽略关键行动并产生子优决策。论文提出了一种改进算法，通过在POMCP算法的UCB1 heuristic中添加Entropy，更好地反映信息收集行动的价值。实验结果显示，该算法在hallway problem上显著优于POMCP，证明了其在提升规划效率和决策质量方面的有效性。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Honors thesis submitted to Computer Science Department at Oberlin\n  College. https://digitalcommons.oberlin.edu/honors/833/",
      "pdf_url": "http://arxiv.org/pdf/2409.13754v1",
      "published_date": "2024-09-14 22:04:34 UTC",
      "updated_date": "2024-09-14 22:04:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:51:45.547018"
    },
    {
      "arxiv_id": "2409.13753v1",
      "title": "Synergistic Simulations: Multi-Agent Problem Solving with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Asher Sprigler",
        "Alexander Drobek",
        "Keagan Weinstock",
        "Wendpanga Tapsoba",
        "Gavin Childress",
        "Andy Dao",
        "Lucas Gral"
      ],
      "abstract": "Large Language Models (LLMs) have increasingly demonstrated the ability to\nfacilitate the development of multi-agent systems that allow the interpretation\nof thoughts and actions generated by each individual. Promising advancements\nhave also been made in LLM-based interaction with existing worlds, particularly\nin interacting with simulated environments. This paper aims to integrate both\naforementioned topics (agents & world interaction) into a single simulation\nwhere multiple agents can work together to solve a problem, modeling how groups\nof humans can often solve problems better than individuals. By showing whether\nLLMs demonstrate the synergy of human collaboration, it could lead to\nadvancements in the applications of LLMs. We implemented two simulations: a\nphysical studio apartment with two roommates, and another where agents\ncollaborate to complete a programming task. We provide a multi-agent framework,\ndiscuss the performance of the agents in each simulation, and discuss potential\nfuture additions.",
      "tldr_zh": "本论文探讨了 Large Language Models (LLMs) 在多智能体系统中的应用，旨在通过模拟环境整合智能体互动，模仿人类群体协作的优势来提升问题解决能力。该框架设计了两个具体模拟场景：一个是两个室友在物理工作室公寓中的协作，另一个是智能体共同完成编程任务。实验结果显示，LLMs 展示了协同效应，提高了问题解决效率，并为未来 LLMs 应用提供了潜在扩展方向。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.ET"
      ],
      "primary_category": "cs.MA",
      "comment": "15 pages, 5 figures, published in the MICS 2024 conference",
      "pdf_url": "http://arxiv.org/pdf/2409.13753v1",
      "published_date": "2024-09-14 21:53:35 UTC",
      "updated_date": "2024-09-14 21:53:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:51:57.716617"
    },
    {
      "arxiv_id": "2409.09541v3",
      "title": "Autonomous Goal Detection and Cessation in Reinforcement Learning: A Case Study on Source Term Estimation",
      "title_zh": "强化学习中的自治目标检测和终止：来源项估计的案例研究",
      "authors": [
        "Yiwei Shi",
        "Muning Wen",
        "Qi Zhang",
        "Weinan Zhang",
        "Cunjia Liu",
        "Weiru Liu"
      ],
      "abstract": "Reinforcement Learning has revolutionized decision-making processes in\ndynamic environments, yet it often struggles with autonomously detecting and\nachieving goals without clear feedback signals. For example, in a Source Term\nEstimation problem, the lack of precise environmental information makes it\nchallenging to provide clear feedback signals and to define and evaluate how\nthe source's location is determined. To address this challenge, the Autonomous\nGoal Detection and Cessation (AGDC) module was developed, enhancing various RL\nalgorithms by incorporating a self-feedback mechanism for autonomous goal\ndetection and cessation upon task completion. Our method effectively identifies\nand ceases undefined goals by approximating the agent's belief, significantly\nenhancing the capabilities of RL algorithms in environments with limited\nfeedback. To validate effectiveness of our approach, we integrated AGDC with\ndeep Q-Network, proximal policy optimization, and deep deterministic policy\ngradient algorithms, and evaluated its performance on the Source Term\nEstimation problem. The experimental results showed that AGDC-enhanced RL\nalgorithms significantly outperformed traditional statistical methods such as\ninfotaxis, entrotaxis, and dual control for exploitation and exploration, as\nwell as a non-statistical random action selection method. These improvements\nwere evident in terms of success rate, mean traveled distance, and search time,\nhighlighting AGDC's effectiveness and efficiency in complex, real-world\nscenarios.",
      "tldr_zh": "本文研究强化学习（Reinforcement Learning）在动态环境中难以自主检测和实现目标的问题，特别是以 Source Term Estimation 为案例，缺乏精确反馈信号导致的挑战。作者提出 Autonomous Goal Detection and Cessation (AGDC) 模块，该模块通过自反馈机制近似代理信念（agent's belief），增强各种 RL 算法如 Deep Q-Network、Proximal Policy Optimization 和 Deep Deterministic Policy Gradient，实现自主目标识别和任务停止。实验结果显示，AGDC 增强的算法在 Source Term Estimation 问题上，在成功率、平均旅行距离和搜索时间方面显著优于传统统计方法（如 infotaxis、entrotaxis 和 dual control）以及随机行动方法，证明了其在反馈有限的真实场景中的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09541v3",
      "published_date": "2024-09-14 21:42:17 UTC",
      "updated_date": "2024-12-12 17:12:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:52:11.226082"
    },
    {
      "arxiv_id": "2409.09536v2",
      "title": "VernaCopter: Disambiguated Natural-Language-Driven Robot via Formal Specifications",
      "title_zh": "VernaCopter：通过正式",
      "authors": [
        "Teun van de Laar",
        "Zengjie Zhang",
        "Shuhao Qi",
        "Sofie Haesaert",
        "Zhiyong Sun"
      ],
      "abstract": "It has been an ambition of many to control a robot for a complex task using\nnatural language (NL). The rise of large language models (LLMs) makes it closer\nto coming true. However, an LLM-powered system still suffers from the ambiguity\ninherent in an NL and the uncertainty brought up by LLMs. This paper proposes a\nnovel LLM-based robot motion planner, named \\textit{VernaCopter}, with signal\ntemporal logic (STL) specifications serving as a bridge between NL commands and\nspecific task objectives. The rigorous and abstract nature of formal\nspecifications allows the planner to generate high-quality and highly\nconsistent paths to guide the motion control of a robot. Compared to a\nconventional NL-prompting-based planner, the proposed VernaCopter planner is\nmore stable and reliable due to less ambiguous uncertainty. Its efficacy and\nadvantage have been validated by two small but challenging experimental\nscenarios, implying its potential in designing NL-driven robots.",
      "tldr_zh": "本论文提出 VernaCopter，一种基于 large language models (LLMs) 的机器人运动规划器，使用 signal temporal logic (STL) 规范作为桥梁，将自然语言 (NL) 命令转化为明确的任务目标，从而解决 NL 的模糊性和 LLM 的不确定性问题。该系统通过生成高质量、一致的机器人路径，确保规划的稳定性和可靠性。与传统 NL 提示方法相比，VernaCopter 在两个小型挑战性实验场景中表现出显著优势，验证了其在设计 NL 驱动机器人方面的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09536v2",
      "published_date": "2024-09-14 21:36:22 UTC",
      "updated_date": "2025-03-06 19:37:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:52:21.711849"
    },
    {
      "arxiv_id": "2409.09520v2",
      "title": "Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Hu",
        "Janet Wang",
        "Jihun Hamm",
        "Rie R Yotsu",
        "Zhengming Ding"
      ],
      "abstract": "Current AI-assisted skin image diagnosis has achieved dermatologist-level\nperformance in classifying skin cancer, driven by rapid advancements in deep\nlearning architectures. However, unlike traditional vision tasks, skin images\nin general present unique challenges due to the limited availability of\nwell-annotated datasets, complex variations in conditions, and the necessity\nfor detailed interpretations to ensure patient safety. Previous segmentation\nmethods have sought to reduce image noise and enhance diagnostic performance,\nbut these techniques require fine-grained, pixel-level ground truth masks for\ntraining. In contrast, with the rise of foundation models, the Segment Anything\nModel (SAM) has been introduced to facilitate promptable segmentation, enabling\nthe automation of the segmentation process with simple yet effective prompts.\nEfforts applying SAM predominantly focus on dermatoscopy images, which present\nmore easily identifiable lesion boundaries than clinical photos taken with\nsmartphones. This limitation constrains the practicality of these approaches to\nreal-world applications. To overcome the challenges posed by noisy clinical\nphotos acquired via non-standardized protocols and to improve diagnostic\naccessibility, we propose a novel Cross-Attentive Fusion framework for\ninterpretable skin lesion diagnosis. Our method leverages SAM to generate\nvisual concepts for skin diseases using prompts, integrating local visual\nconcepts with global image features to enhance model performance. Extensive\nevaluation on two skin disease datasets demonstrates our proposed method's\neffectiveness on lesion diagnosis and interpretability.",
      "tldr_zh": "本文研究了AI在皮肤图像诊断中的挑战，包括数据集有限、图像噪声和解释性需求，并提出了一种新型Cross-Attentive Fusion框架来提升诊断性能。该框架利用Segment Anything Model (SAM)生成visual concepts，通过将局部visual concepts与全局图像特征整合，实现可解释的皮肤病变识别。与传统方法不同，该方法适用于非标准化临床照片，避免了精细像素级标注的依赖。在两个皮肤病数据集上的广泛评估显示，该框架显著提高了诊断准确性和解释性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is accepted by WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.09520v2",
      "published_date": "2024-09-14 20:11:25 UTC",
      "updated_date": "2025-01-15 23:21:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:52:33.561099"
    },
    {
      "arxiv_id": "2409.09517v1",
      "title": "Deep Learning Under Siege: Identifying Security Vulnerabilities and Risk Mitigation Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Jamal Al-Karaki",
        "Muhammad Al-Zafar Khan",
        "Mostafa Mohamad",
        "Dababrata Chowdhury"
      ],
      "abstract": "With the rise in the wholesale adoption of Deep Learning (DL) models in\nnearly all aspects of society, a unique set of challenges is imposed. Primarily\ncentered around the architectures of these models, these risks pose a\nsignificant challenge, and addressing these challenges is key to their\nsuccessful implementation and usage in the future. In this research, we present\nthe security challenges associated with the current DL models deployed into\nproduction, as well as anticipate the challenges of future DL technologies\nbased on the advancements in computing, AI, and hardware technologies. In\naddition, we propose risk mitigation techniques to inhibit these challenges and\nprovide metrical evaluations to measure the effectiveness of these metrics.",
      "tldr_zh": "该研究探讨了深度学习(DL)模型在社会广泛应用中面临的安全漏洞和挑战，主要聚焦于模型架构及其未来实施风险。论文识别了当前部署DL模型的安全问题，并基于计算、AI和硬件技术的进展预测了潜在未来挑战。同时，提出风险缓解策略(risk mitigation strategies)并提供度量评估，以衡量这些策略的有效性，从而为DL模型的安全性提升提供实用指导。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 1 table, 6 equations/metrics",
      "pdf_url": "http://arxiv.org/pdf/2409.09517v1",
      "published_date": "2024-09-14 19:54:12 UTC",
      "updated_date": "2024-09-14 19:54:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:52:44.708112"
    },
    {
      "arxiv_id": "2409.09513v1",
      "title": "Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens",
      "title_zh": "翻译失败",
      "authors": [
        "Joseph Clinton",
        "Robert Lieck"
      ],
      "abstract": "Supervised learning approaches to offline reinforcement learning,\nparticularly those utilizing the Decision Transformer, have shown effectiveness\nin continuous environments and for sparse rewards. However, they often struggle\nwith long-horizon tasks due to the high compounding error of auto-regressive\nmodels. To overcome this limitation, we go beyond next-token prediction and\nintroduce Planning Tokens, which contain high-level, long time-scale\ninformation about the agent's future. Predicting dual time-scale tokens at\nregular intervals enables our model to use these long-horizon Planning Tokens\nas a form of implicit planning to guide its low-level policy and reduce\ncompounding error. This architectural modification significantly enhances\nperformance on long-horizon tasks, establishing a new state-of-the-art in\ncomplex D4RL environments. Additionally, we demonstrate that Planning Tokens\nimprove the interpretability of the model's policy through the interpretable\nplan visualisations and attention map.",
      "tldr_zh": "该论文提出Planning Transformer，一种针对长时序离线强化学习（offline reinforcement learning）的改进方法，通过引入Planning Tokens来解决自回归模型（如Decision Transformer）的高累积错误问题。Planning Tokens包含高水平的长时序信息，模型在常规间隔预测双时序令牌，将其作为隐式规划来指导低级策略，从而减少错误累积。实验结果显示，该方法在复杂D4RL环境中显著提升性能，建立了新状态标准，并通过可解释的计划可视化和注意力图提高了模型策略的可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 5 figures, Submitted to AAAI",
      "pdf_url": "http://arxiv.org/pdf/2409.09513v1",
      "published_date": "2024-09-14 19:30:53 UTC",
      "updated_date": "2024-09-14 19:30:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:52:57.314915"
    },
    {
      "arxiv_id": "2409.09511v1",
      "title": "Explaining Deep Learning Embeddings for Speech Emotion Recognition by Predicting Interpretable Acoustic Features",
      "title_zh": "通过预测可解释声学特征解释深度学习嵌入用于语音情感识别",
      "authors": [
        "Satvik Dixit",
        "Daniel M. Low",
        "Gasser Elbanna",
        "Fabio Catania",
        "Satrajit S. Ghosh"
      ],
      "abstract": "Pre-trained deep learning embeddings have consistently shown superior\nperformance over handcrafted acoustic features in speech emotion recognition\n(SER). However, unlike acoustic features with clear physical meaning, these\nembeddings lack clear interpretability. Explaining these embeddings is crucial\nfor building trust in healthcare and security applications and advancing the\nscientific understanding of the acoustic information that is encoded in them.\nThis paper proposes a modified probing approach to explain deep learning\nembeddings in the SER space. We predict interpretable acoustic features (e.g.,\nf0, loudness) from (i) the complete set of embeddings and (ii) a subset of the\nembedding dimensions identified as most important for predicting each emotion.\nIf the subset of the most important dimensions better predicts a given emotion\nthan all dimensions and also predicts specific acoustic features more\naccurately, we infer those acoustic features are important for the embedding\nmodel for the given task. We conducted experiments using the WavLM embeddings\nand eGeMAPS acoustic features as audio representations, applying our method to\nthe RAVDESS and SAVEE emotional speech datasets. Based on this evaluation, we\ndemonstrate that Energy, Frequency, Spectral, and Temporal categories of\nacoustic features provide diminishing information to SER in that order,\ndemonstrating the utility of the probing classifier method to relate embeddings\nto interpretable acoustic features.",
      "tldr_zh": "本研究针对深度学习嵌入在语音情感识别（SER）中的优越性能但缺乏可解释性问题，提出了一种修改的探测方法，通过预测可解释的声学特征（如 f0 和 loudness）来解释嵌入。方法涉及从嵌入的全部维度或最重要子集预测这些特征，并评估子集是否更准确地预测情感，从而推断关键声学特征的作用。实验使用 WavLM 嵌入和 eGeMAPS 声学特征，在 RAVDESS 和 SAVEE 数据集上进行，结果显示 Energy、Frequency、Spectral 和 Temporal 类别的声学特征对 SER 的信息贡献依次递减。该方法有助于提升嵌入的可解释性，促进其在医疗和安全领域的应用和科学理解。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09511v1",
      "published_date": "2024-09-14 19:18:56 UTC",
      "updated_date": "2024-09-14 19:18:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:53:11.102298"
    },
    {
      "arxiv_id": "2410.01816v1",
      "title": "Automatic Scene Generation: State-of-the-Art Techniques, Models, Datasets, Challenges, and Future Prospects",
      "title_zh": "自动场景生成：最先进的技术、模型、数据集、挑战和未来前景",
      "authors": [
        "Awal Ahmed Fime",
        "Saifuddin Mahmud",
        "Arpita Das",
        "Md. Sunzidul Islam",
        "Hong-Hoon Kim"
      ],
      "abstract": "Automatic scene generation is an essential area of research with applications\nin robotics, recreation, visual representation, training and simulation,\neducation, and more. This survey provides a comprehensive review of the current\nstate-of-the-arts in automatic scene generation, focusing on techniques that\nleverage machine learning, deep learning, embedded systems, and natural\nlanguage processing (NLP). We categorize the models into four main types:\nVariational Autoencoders (VAEs), Generative Adversarial Networks (GANs),\nTransformers, and Diffusion Models. Each category is explored in detail,\ndiscussing various sub-models and their contributions to the field.\n  We also review the most commonly used datasets, such as COCO-Stuff, Visual\nGenome, and MS-COCO, which are critical for training and evaluating these\nmodels. Methodologies for scene generation are examined, including image-to-3D\nconversion, text-to-3D generation, UI/layout design, graph-based methods, and\ninteractive scene generation. Evaluation metrics such as Frechet Inception\nDistance (FID), Kullback-Leibler (KL) Divergence, Inception Score (IS),\nIntersection over Union (IoU), and Mean Average Precision (mAP) are discussed\nin the context of their use in assessing model performance.\n  The survey identifies key challenges and limitations in the field, such as\nmaintaining realism, handling complex scenes with multiple objects, and\nensuring consistency in object relationships and spatial arrangements. By\nsummarizing recent advances and pinpointing areas for improvement, this survey\naims to provide a valuable resource for researchers and practitioners working\non automatic scene generation.",
      "tldr_zh": "这篇论文对自动场景生成领域进行了全面调查，涵盖了其在机器人、娱乐和模拟等领域的应用，并总结了当前最先进的技术，包括机器学习、深度学习和自然语言处理（NLP）。论文将模型分为四类：Variational Autoencoders (VAEs)、Generative Adversarial Networks (GANs)、Transformers 和 Diffusion Models，并详细讨论了子模型、常用数据集（如 COCO-Stuff、Visual Genome 和 MS-COCO）、生成方法（如 image-to-3D 转换和文本-to-3D 生成）以及评估指标（如 FID 和 IoU）。最后，它指出了关键挑战，包括维护场景真实性和处理复杂对象关系，并为未来研究提供了改进方向和前景。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "59 pages, 16 figures, 3 tables, 36 equations, 348 references",
      "pdf_url": "http://arxiv.org/pdf/2410.01816v1",
      "published_date": "2024-09-14 19:09:10 UTC",
      "updated_date": "2024-09-14 19:09:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:53:22.529879"
    },
    {
      "arxiv_id": "2409.09506v1",
      "title": "ESPnet-EZ: Python-only ESPnet for Easy Fine-tuning and Integration",
      "title_zh": "ESPnet-EZ：仅 Python 的 ESPnet，用于轻松微调和集成",
      "authors": [
        "Masao Someki",
        "Kwanghee Choi",
        "Siddhant Arora",
        "William Chen",
        "Samuele Cornell",
        "Jionghao Han",
        "Yifan Peng",
        "Jiatong Shi",
        "Vaibhav Srivastav",
        "Shinji Watanabe"
      ],
      "abstract": "We introduce ESPnet-EZ, an extension of the open-source speech processing\ntoolkit ESPnet, aimed at quick and easy development of speech models. ESPnet-EZ\nfocuses on two major aspects: (i) easy fine-tuning and inference of existing\nESPnet models on various tasks and (ii) easy integration with popular deep\nneural network frameworks such as PyTorch-Lightning, Hugging Face transformers\nand datasets, and Lhotse. By replacing ESPnet design choices inherited from\nKaldi with a Python-only, Bash-free interface, we dramatically reduce the\neffort required to build, debug, and use a new model. For example, to fine-tune\na speech foundation model, ESPnet-EZ, compared to ESPnet, reduces the number of\nnewly written code by 2.7x and the amount of dependent code by 6.7x while\ndramatically reducing the Bash script dependencies. The codebase of ESPnet-EZ\nis publicly available.",
      "tldr_zh": "我们介绍了 ESPnet-EZ，这是一个基于 Python-only 的 ESPnet 扩展，旨在简化语音模型的微调和集成。相比原 ESPnet，它通过替换继承自 Kaldi 的设计为 Bash-free 接口，便于与 PyTorch-Lightning、Hugging Face transformers 和 Lhotse 等框架无缝结合。实验结果显示，在微调语音基础模型时，ESPnet-EZ 减少了 2.7 倍的新代码量和 6.7 倍的依赖代码量，从而大幅降低开发和调试的努力。该代码库已公开可用，方便社区使用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to SLT 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.09506v1",
      "published_date": "2024-09-14 19:03:53 UTC",
      "updated_date": "2024-09-14 19:03:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:53:34.940402"
    },
    {
      "arxiv_id": "2409.09501v1",
      "title": "Synthetic4Health: Generating Annotated Synthetic Clinical Letters",
      "title_zh": "Synthetic4Health：生成带注释的合成临床信件",
      "authors": [
        "Libo Ren",
        "Samuel Belkadi",
        "Lifeng Han",
        "Warren Del-Pinto",
        "Goran Nenadic"
      ],
      "abstract": "Since clinical letters contain sensitive information, clinical-related\ndatasets can not be widely applied in model training, medical research, and\nteaching. This work aims to generate reliable, various, and de-identified\nsynthetic clinical letters. To achieve this goal, we explored different\npre-trained language models (PLMs) for masking and generating text. After that,\nwe worked on Bio\\_ClinicalBERT, a high-performing model, and experimented with\ndifferent masking strategies. Both qualitative and quantitative methods were\nused for evaluation. Additionally, a downstream task, Named Entity Recognition\n(NER), was also implemented to assess the usability of these synthetic letters.\n  The results indicate that 1) encoder-only models outperform encoder-decoder\nmodels. 2) Among encoder-only models, those trained on general corpora perform\ncomparably to those trained on clinical data when clinical information is\npreserved. 3) Additionally, preserving clinical entities and document structure\nbetter aligns with our objectives than simply fine-tuning the model. 4)\nFurthermore, different masking strategies can impact the quality of synthetic\nclinical letters. Masking stopwords has a positive impact, while masking nouns\nor verbs has a negative effect. 5) For evaluation, BERTScore should be the\nprimary quantitative evaluation metric, with other metrics serving as\nsupplementary references. 6) Contextual information does not significantly\nimpact the models' understanding, so the synthetic clinical letters have the\npotential to replace the original ones in downstream tasks.",
      "tldr_zh": "本研究提出Synthetic4Health框架，旨在生成可靠、多样且去标识化的合成临床信函，以解决临床数据集因敏感信息而无法广泛应用于模型训练、医疗研究和教学的问题。研究探索了不同预训练语言模型(PLMs)，重点使用Bio_ClinicalBERT结合各种掩码策略（如掩码停用词），并通过定性和定量评估（如BERTScore和下游任务Named Entity Recognition (NER)）来评估合成信函的质量。结果显示，编码器-only模型优于编码器-解码器模型，保留临床实体和文档结构更有效，且合成信函在下游任务中具有潜在替代原数据的可用性，从而为隐私保护下的医疗数据生成提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ongoing work, 48 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.09501v1",
      "published_date": "2024-09-14 18:15:07 UTC",
      "updated_date": "2024-09-14 18:15:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:53:46.584751"
    },
    {
      "arxiv_id": "2410.01815v1",
      "title": "AI in Food Marketing from Personalized Recommendations to Predictive Analytics: Comparing Traditional Advertising Techniques with AI-Driven Strategies",
      "title_zh": "食品营销中的 AI：从个性化推荐到预测分析——比较传统广告技术和 AI 驱动策略",
      "authors": [
        "Elham Khamoushi"
      ],
      "abstract": "Artificial Intelligence (AI) has revolutionized food marketing by providing\nadvanced techniques for personalized recommendations, consumer behavior\nprediction, and campaign optimization. This paper explores the shift from\ntraditional advertising methods, such as TV, radio, and print, to AI-driven\nstrategies. Traditional approaches were successful in building brand awareness\nbut lacked the level of personalization that modern consumers demand. AI\nleverages data from consumer purchase histories, browsing behaviors, and social\nmedia activity to create highly tailored marketing campaigns. These strategies\nallow for more accurate product recommendations, prediction of consumer needs,\nand ultimately improve customer satisfaction and user experience. AI enhances\nmarketing efforts by automating labor-intensive processes, leading to greater\nefficiency and cost savings. It also enables the continuous adaptation of\nmarketing messages, ensuring they remain relevant and engaging over time. While\nAI presents significant benefits in terms of personalization and efficiency, it\nalso comes with challenges, particularly the substantial investment required\nfor technology and skilled expertise. This paper compares the strengths and\nweaknesses of traditional and AI-driven food marketing techniques, offering\nvaluable insights into how marketers can leverage AI to create more effective\nand targeted marketing strategies in the evolving digital landscape.",
      "tldr_zh": "这篇论文比较了传统广告技术和AI驱动策略在食品营销中的应用，重点探讨AI如何通过个性化推荐和预测分析来提升消费者行为预测及营销优化。传统方法如电视、广播和印刷媒体擅长建立品牌知名度，但缺乏针对性，而AI则利用消费者购买历史、浏览行为和社会媒体数据创建定制化营销campaign，提高产品推荐准确性和客户满意度。AI还自动化流程以实现效率提升和成本节省，同时允许营销消息动态适应，尽管面临技术投资和专业技能的挑战；论文提供宝贵见解，帮助营销者在数字环境中采用AI策略以获得更有效的成果。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.01815v1",
      "published_date": "2024-09-14 17:53:32 UTC",
      "updated_date": "2024-09-14 17:53:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:53:57.737178"
    },
    {
      "arxiv_id": "2409.09497v2",
      "title": "Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation",
      "title_zh": "多尺度分组原型用于可解释语义分割",
      "authors": [
        "Hugo Porta",
        "Emanuele Dalsasso",
        "Diego Marcos",
        "Devis Tuia"
      ],
      "abstract": "Prototypical part learning is emerging as a promising approach for making\nsemantic segmentation interpretable. The model selects real patches seen during\ntraining as prototypes and constructs the dense prediction map based on the\nsimilarity between parts of the test image and the prototypes. This improves\ninterpretability since the user can inspect the link between the predicted\noutput and the patterns learned by the model in terms of prototypical\ninformation. In this paper, we propose a method for interpretable semantic\nsegmentation that leverages multi-scale image representation for prototypical\npart learning. First, we introduce a prototype layer that explicitly learns\ndiverse prototypical parts at several scales, leading to multi-scale\nrepresentations in the prototype activation output. Then, we propose a sparse\ngrouping mechanism that produces multi-scale sparse groups of these\nscale-specific prototypical parts. This provides a deeper understanding of the\ninteractions between multi-scale object representations while enhancing the\ninterpretability of the segmentation model. The experiments conducted on Pascal\nVOC, Cityscapes, and ADE20K demonstrate that the proposed method increases\nmodel sparsity, improves interpretability over existing prototype-based\nmethods, and narrows the performance gap with the non-interpretable counterpart\nmodels. Code is available at github.com/eceo-epfl/ScaleProtoSeg.",
      "tldr_zh": "本论文提出了一种基于多-scale grouped prototypes的可解释语义 segmentation 方法，通过原型层（prototype layer）在多个尺度上学习多样化的原型部分，从而生成多-scale representations。论文引入了稀疏分组机制（sparse grouping mechanism），以创建多尺度稀疏组，增强对对象表示的理解并提高模型的可解释性。在Pascal VOC、Cityscapes和ADE20K数据集上的实验表明，该方法增加了模型稀疏性，超越了现有基于原型的可解释方法，并缩小了与非可解释模型的性能差距。代码可在GitHub上获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.09497v2",
      "published_date": "2024-09-14 17:52:59 UTC",
      "updated_date": "2025-04-28 14:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:54:20.515001"
    },
    {
      "arxiv_id": "2409.09493v1",
      "title": "Hacking, The Lazy Way: LLM Augmented Pentesting",
      "title_zh": "翻译失败",
      "authors": [
        "Dhruva Goyal",
        "Sitaraman Subramanian",
        "Aditya Peela"
      ],
      "abstract": "Security researchers are continually challenged by the need to stay current\nwith rapidly evolving cybersecurity research, tools, and techniques. This\nconstant cycle of learning, unlearning, and relearning, combined with the\nrepetitive tasks of sifting through documentation and analyzing data, often\nhinders productivity and innovation. This has led to a disparity where only\norganizations with substantial resources can access top-tier security experts,\nwhile others rely on firms with less skilled researchers who focus primarily on\ncompliance rather than actual security.\n  We introduce \"LLM Augmented Pentesting,\" demonstrated through a tool named\n\"Pentest Copilot,\" to address this gap. This approach integrates Large Language\nModels into penetration testing workflows. Our research includes a \"chain of\nthought\" mechanism to streamline token usage and boost performance, as well as\nunique Retrieval Augmented Generation implementation to minimize hallucinations\nand keep models aligned with the latest techniques. Additionally, we propose a\nnovel file analysis approach, enabling LLMs to understand files. Furthermore,\nwe highlight a unique infrastructure system that supports if implemented, can\nsupport in-browser assisted penetration testing, offering a robust platform for\ncybersecurity professionals, These advancements mark a significant step toward\nbridging the gap between automated tools and human expertise, offering a\npowerful solution to the challenges faced by modern cybersecurity teams.",
      "tldr_zh": "本研究探讨了安全研究人员在快速演变的网络安全领域面临的挑战，包括持续学习负担和重复任务，导致资源不均等问题。作者提出“LLM Augmented Pentesting”框架，并通过工具“Pentest Copilot”整合大语言模型（LLMs）到渗透测试流程中，引入“chain of thought”机制优化token使用、独特Retrieval Augmented Generation（RAG）实现减少幻觉，以及新颖的文件分析方法和基础设施系统，支持浏览器内辅助测试。这些创新有助于桥接自动化工具与人类专家的差距，为现代网络安全团队提供高效解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "I.2.1"
      ],
      "primary_category": "cs.CR",
      "comment": "9 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.09493v1",
      "published_date": "2024-09-14 17:40:35 UTC",
      "updated_date": "2024-09-14 17:40:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:54:21.961226"
    },
    {
      "arxiv_id": "2409.09485v1",
      "title": "Enumerating Minimal Unsatisfiable Cores of LTLf formulas",
      "title_zh": "枚举 LTLf 公式的最小不可满足核心",
      "authors": [
        "Antonio Ielo",
        "Giuseppe Mazzotta",
        "Rafael Peñaloza",
        "Francesco Ricca"
      ],
      "abstract": "Linear Temporal Logic over finite traces ($\\text{LTL}_f$) is a widely used\nformalism with applications in AI, process mining, model checking, and more.\nThe primary reasoning task for $\\text{LTL}_f$ is satisfiability checking; yet,\nthe recent focus on explainable AI has increased interest in analyzing\ninconsistent formulas, making the enumeration of minimal explanations for\ninfeasibility a relevant task also for $\\text{LTL}_f$. This paper introduces a\nnovel technique for enumerating minimal unsatisfiable cores (MUCs) of an\n$\\text{LTL}_f$ specification. The main idea is to encode a $\\text{LTL}_f$\nformula into an Answer Set Programming (ASP) specification, such that the\nminimal unsatisfiable subsets (MUSes) of the ASP program directly correspond to\nthe MUCs of the original $\\text{LTL}_f$ specification. Leveraging recent\nadvancements in ASP solving yields a MUC enumerator achieving good performance\nin experiments conducted on established benchmarks from the literature.",
      "tldr_zh": "这篇论文提出了一种新方法，用于枚举 LTLf 公式的最小不可满足核心 (MUCs)，以支持 LTLf 在 AI、过程挖掘和模型检查等领域的不可满足性分析。核心技术是将 LTLf 公式编码成 Answer Set Programming (ASP) 规范，使 ASP 程序的最小不可满足子集 (MUSes) 直接对应于原公式的 MUCs。实验结果显示，该方法利用 ASP 求解器的最新进展，在现有基准上实现了良好的性能表现。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09485v1",
      "published_date": "2024-09-14 17:15:30 UTC",
      "updated_date": "2024-09-14 17:15:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:54:34.248923"
    },
    {
      "arxiv_id": "2409.09478v2",
      "title": "From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Maximilian Rokuss",
        "Balint Kovacs",
        "Yannick Kirchhoff",
        "Shuhan Xiao",
        "Constantin Ulrich",
        "Klaus H. Maier-Hein",
        "Fabian Isensee"
      ],
      "abstract": "Automated lesion segmentation in PET/CT scans is crucial for improving\nclinical workflows and advancing cancer diagnostics. However, the task is\nchallenging due to physiological variability, different tracers used in PET\nimaging, and diverse imaging protocols across medical centers. To address this,\nthe autoPET series was created to challenge researchers to develop algorithms\nthat generalize across diverse PET/CT environments. This paper presents our\nsolution for the autoPET III challenge, targeting multitracer, multicenter\ngeneralization using the nnU-Net framework with the ResEncL architecture. Key\ntechniques include misalignment data augmentation and multi-modal pretraining\nacross CT, MR, and PET datasets to provide an initial anatomical understanding.\nWe incorporate organ supervision as a multitask approach, enabling the model to\ndistinguish between physiological uptake and tracer-specific patterns, which is\nparticularly beneficial in cases where no lesions are present. Compared to the\ndefault nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL\n(65.31) our model significantly improved performance with a Dice score of\n68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative\n(FNvol: 10.35) volumes. These results underscore the effectiveness of combining\nadvanced network design, augmentation, pretraining, and multitask learning for\nPET/CT lesion segmentation. After evaluation on the test set, our approach was\nawarded the first place in the model-centric category (Team LesionTracer). Code\nis publicly available at https://github.com/MIC-DKFZ/autopet-3-submission.",
      "tldr_zh": "这篇论文针对PET/CT成像中的多示踪剂（如FDG和PSMA）和多中心挑战，提出了一种基于nnU-Net框架和ResEncL架构的病变分割解决方案，以提升临床诊断效率。关键技术包括misalignment数据增强、多模态预训练（跨CT、MR和PET数据集）以及器官监督的多任务学习，帮助模型区分生理摄取和示踪剂特定模式。实验结果显示，该方法将Dice score从基线的57.61提高到68.40，同时降低了假阳性（FPvol: 7.82）和假阴性（FNvol: 10.35）体积，并在autoPET III挑战中获得模型中心类别第一名。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Winning method of the autoPET III challenge (model-centric) - Team\n  LesionTracer",
      "pdf_url": "http://arxiv.org/pdf/2409.09478v2",
      "published_date": "2024-09-14 16:39:17 UTC",
      "updated_date": "2024-10-21 14:15:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:54:47.162831"
    },
    {
      "arxiv_id": "2409.09461v2",
      "title": "TX-Gen: Multi-Objective Optimization for Sparse Counterfactual Explanations for Time-Series Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Huang",
        "Sofoklis Kitharidis",
        "Thomas Bäck",
        "Niki van Stein"
      ],
      "abstract": "In time-series classification, understanding model decisions is crucial for\ntheir application in high-stakes domains such as healthcare and finance.\nCounterfactual explanations, which provide insights by presenting alternative\ninputs that change model predictions, offer a promising solution. However,\nexisting methods for generating counterfactual explanations for time-series\ndata often struggle with balancing key objectives like proximity, sparsity, and\nvalidity. In this paper, we introduce TX-Gen, a novel algorithm for generating\ncounterfactual explanations based on the Non-dominated Sorting Genetic\nAlgorithm II (NSGA-II). TX-Gen leverages evolutionary multi-objective\noptimization to find a diverse set of counterfactuals that are both sparse and\nvalid, while maintaining minimal dissimilarity to the original time series. By\nincorporating a flexible reference-guided mechanism, our method improves the\nplausibility and interpretability of the counterfactuals without relying on\npredefined assumptions. Extensive experiments on benchmark datasets demonstrate\nthat TX-Gen outperforms existing methods in generating high-quality\ncounterfactuals, making time-series models more transparent and interpretable.",
      "tldr_zh": "该论文提出 TX-Gen，一种基于 Non-dominated Sorting Genetic Algorithm II (NSGA-II) 的多目标优化算法，用于生成时间序列分类中的稀疏反事实解释，以平衡接近度、稀疏性和有效性。TX-Gen 通过进化多目标优化和灵活的参考引导机制，生成多样化且有效的反事实解释，同时最小化与原时间序列的差异，并提升其合理性和可解释性。实验在基准数据集上表明，TX-Gen 优于现有方法，在高质量反事实生成方面表现出色，从而使时间序列模型在医疗和金融等高风险领域更透明和可解释。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to EXPLAINS 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.09461v2",
      "published_date": "2024-09-14 15:13:28 UTC",
      "updated_date": "2024-11-11 09:37:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:54:59.227626"
    },
    {
      "arxiv_id": "2409.09446v1",
      "title": "MulCPred: Learning Multi-modal Concepts for Explainable Pedestrian Action Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Feng",
        "Alexander Carballo",
        "Keisuke Fujii",
        "Robin Karlsson",
        "Ming Ding",
        "Kazuya Takeda"
      ],
      "abstract": "Pedestrian action prediction is of great significance for many applications\nsuch as autonomous driving. However, state-of-the-art methods lack\nexplainability to make trustworthy predictions. In this paper, a novel\nframework called MulCPred is proposed that explains its predictions based on\nmulti-modal concepts represented by training samples. Previous concept-based\nmethods have limitations including: 1) they cannot directly apply to\nmulti-modal cases; 2) they lack locality to attend to details in the inputs; 3)\nthey suffer from mode collapse. These limitations are tackled accordingly\nthrough the following approaches: 1) a linear aggregator to integrate the\nactivation results of the concepts into predictions, which associates concepts\nof different modalities and provides ante-hoc explanations of the relevance\nbetween the concepts and the predictions; 2) a channel-wise recalibration\nmodule that attends to local spatiotemporal regions, which enables the concepts\nwith locality; 3) a feature regularization loss that encourages the concepts to\nlearn diverse patterns. MulCPred is evaluated on multiple datasets and tasks.\nBoth qualitative and quantitative results demonstrate that MulCPred is\npromising in improving the explainability of pedestrian action prediction\nwithout obvious performance degradation. Furthermore, by removing\nunrecognizable concepts from MulCPred, the cross-dataset prediction performance\nis improved, indicating the feasibility of further generalizability of\nMulCPred.",
      "tldr_zh": "该论文提出 MulCPred 框架，用于可解释的行人动作预测，通过学习多模态 concepts 来增强预测的 trustworthiness。框架解决了现有方法的局限性，包括无法处理多模态数据、缺乏局部性以及模式崩溃问题，分别通过线性聚合器（提供 ante-hoc explanations）、通道-wise 重新校准模块（关注局部时空区域）和特征正则化损失（鼓励概念多样性）来应对。在多个数据集上的实验显示，MulCPred 显著提高了预测的可解释性，同时保持了性能稳定，且移除不可识别 concepts 后进一步提升了跨数据集的泛化性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09446v1",
      "published_date": "2024-09-14 14:15:28 UTC",
      "updated_date": "2024-09-14 14:15:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:55:10.636803"
    },
    {
      "arxiv_id": "2409.09424v3",
      "title": "NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection",
      "title_zh": "NBBOX：噪声边界框改善遥感物体检测",
      "authors": [
        "Yechan Kim",
        "SooYeon Kim",
        "Moongu Jeon"
      ],
      "abstract": "Data augmentation has shown significant advancements in computer vision to\nimprove model performance over the years, particularly in scenarios with\nlimited and insufficient data. Currently, most studies focus on adjusting the\nimage or its features to expand the size, quality, and variety of samples\nduring training in various tasks including object detection. However, we argue\nthat it is necessary to investigate bounding box transformations as a data\naugmentation technique rather than image-level transformations, especially in\naerial imagery due to potentially inconsistent bounding box annotations. Hence,\nthis letter presents a thorough investigation of bounding box transformation in\nterms of scaling, rotation, and translation for remote sensing object\ndetection. We call this augmentation strategy NBBOX (Noise Injection into\nBounding Box). We conduct extensive experiments on DOTA and DIOR-R, both\nwell-known datasets that include a variety of rotated generic objects in aerial\nimages. Experimental results show that our approach significantly improves\nremote sensing object detection without whistles and bells and it is more\ntime-efficient than other state-of-the-art augmentation strategies.",
      "tldr_zh": "这篇论文提出了一种名为 NBBOX 的数据增强策略，通过在 bounding box 上注入噪声（包括 scaling、rotation 和 translation），来改善遥感物体检测的性能，尤其针对航空图像中边界框标注可能不一致的问题。不同于传统的图像级增强，NBBOX 专注于 bounding box 变换，以增加训练样本的多样性和质量。在 DOTA 和 DIOR-R 数据集上的实验结果显示，该方法显著提升了检测准确率，比其他先进增强策略更高效且无需额外复杂组件。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to IEEE Geoscience and Remote Sensing Letters",
      "pdf_url": "http://arxiv.org/pdf/2409.09424v3",
      "published_date": "2024-09-14 12:25:14 UTC",
      "updated_date": "2025-01-07 11:37:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:55:22.296962"
    },
    {
      "arxiv_id": "2409.09418v1",
      "title": "Distributed Clustering based on Distributional Kernel",
      "title_zh": "基于分布核的分布式聚类",
      "authors": [
        "Hang Zhang",
        "Yang Xu",
        "Lei Gong",
        "Ye Zhu",
        "Kai Ming Ting"
      ],
      "abstract": "This paper introduces a new framework for clustering in a distributed network\ncalled Distributed Clustering based on Distributional Kernel (K) or KDC that\nproduces the final clusters based on the similarity with respect to the\ndistributions of initial clusters, as measured by K. It is the only framework\nthat satisfies all three of the following properties. First, KDC guarantees\nthat the combined clustering outcome from all sites is equivalent to the\nclustering outcome of its centralized counterpart from the combined dataset\nfrom all sites. Second, the maximum runtime cost of any site in distributed\nmode is smaller than the runtime cost in centralized mode. Third, it is\ndesigned to discover clusters of arbitrary shapes, sizes and densities. To the\nbest of our knowledge, this is the first distributed clustering framework that\nemploys a distributional kernel. The distribution-based clustering leads\ndirectly to significantly better clustering outcomes than existing methods of\ndistributed clustering. In addition, we introduce a new clustering algorithm\ncalled Kernel Bounded Cluster Cores, which is the best clustering algorithm\napplied to KDC among existing clustering algorithms. We also show that KDC is a\ngeneric framework that enables a quadratic time clustering algorithm to deal\nwith large datasets that would otherwise be impossible.",
      "tldr_zh": "本论文提出了一种新的分布式聚类框架，名为 Distributed Clustering based on Distributional Kernel (KDC)，它通过分布核 (distributional kernel) 基于初始聚类的分布相似性来生成最终聚类结果。KDC 满足三个关键属性：分布式聚类的结果等同于集中式聚类的结果、每个站点的最大运行时成本低于集中式模式、以及能够发现任意形状、大小和密度的聚类。该框架首次将分布核应用于分布式聚类，并引入了新的 Kernel Bounded Cluster Cores 算法，作为 KDC 的最佳聚类算法，提供比现有方法显著更好的聚类性能。此外，KDC 作为一个通用框架，使二次时间复杂度的聚类算法能够处理原本无法处理的超大规模数据集。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09418v1",
      "published_date": "2024-09-14 11:40:54 UTC",
      "updated_date": "2024-09-14 11:40:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:55:35.243592"
    },
    {
      "arxiv_id": "2409.10571v1",
      "title": "ASFT: Aligned Supervised Fine-Tuning through Absolute Likelihood",
      "title_zh": "ASFT：通过绝对似然的对其监督微调",
      "authors": [
        "Ruoyu Wang",
        "Jiachen Sun",
        "Shaowei Hua",
        "Quan Fang"
      ],
      "abstract": "Direct Preference Optimization (DPO) is a method for enhancing model\nperformance by directly optimizing for the preferences or rankings of outcomes,\ninstead of traditional loss functions. This approach has proven effective in\naligning Large Language Models (LLMs) with human preferences. Despite its\nwidespread use across various tasks, DPO has been criticized for its\nsensitivity to the effectiveness of Supervised Fine-Tuning (SFT) and its\nlimitations in enabling models to learn human-preferred responses, leading to\nless satisfactory performance. To address these limitations, we propose Aligned\nSupervised Fine-Tuning (ASFT), an effective approach that better aligns LLMs\nwith pair-wise datasets by optimizing absolute likelihood for each response,\nrather than using the Bradley-Terry model, and eliminates the need for a\nreference model. Through theoretical gradient analysis, we demonstrate that\nASFT mitigates the issue where the DPO loss function decreases the probability\nof generating human-dispreferred data at a faster rate than it increases the\nprobability of producing preferred data. Additionally, we compare ASFT to DPO\nand its latest variants, such as the single-step approach ORPO, using the\nlatest instruction-tuned model Llama3, which has been fine-tuned on\nUltraFeedback and HH-RLHF. We evaluated performance on instruction-following\nbenchmarks like MT-Bench and traditional text generation metrics such as BLEU-4\nand ROUGE-L. Extensive experiments demonstrate that ASFT is an effective\nalignment approach, consistently outperforming existing methods.",
      "tldr_zh": "该论文针对Direct Preference Optimization (DPO)方法的局限性，如对Supervised Fine-Tuning (SFT)的敏感性和学习人类偏好的不足，提出了一种新方法Aligned Supervised Fine-Tuning (ASFT)。ASFT通过优化绝对似然（absolute likelihood）来对齐Large Language Models (LLMs)与成对数据集，而非使用Bradley-Terry模型，且无需参考模型。理论分析显示，ASFT能更均衡地提升人类偏好响应概率，避免DPO中偏好数据处理失衡的问题。在实验中，使用Llama3模型并基于UltraFeedback和HH-RLHF数据集，ASFT在MT-Bench、BLEU-4和ROUGE-L等指标上 consistently 优于DPO及其变体如ORPO，证明其是更有效的模型对齐方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10571v1",
      "published_date": "2024-09-14 11:39:13 UTC",
      "updated_date": "2024-09-14 11:39:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:55:46.912741"
    },
    {
      "arxiv_id": "2409.09415v1",
      "title": "Enhancing LLM Problem Solving with REAP: Reflection, Explicit Problem Deconstruction, and Advanced Prompting",
      "title_zh": "通过 REAP 增强 LLM 问题解决：反思、显",
      "authors": [
        "Ryan Lingo",
        "Martin Arroyo",
        "Rajeev Chhajer"
      ],
      "abstract": "Large Language Models (LLMs) have transformed natural language processing,\nyet improving their problem-solving capabilities, particularly for complex,\nreasoning-intensive tasks, remains a persistent challenge. This paper\nintroduces the REAP (Reflection, Explicit Problem Deconstruction, and Advanced\nPrompting) method, an innovative approach within the dynamic context generation\nframework. REAP guides LLMs through reflection on the query, deconstructing it\ninto manageable components, and generating relevant context to enhance the\nsolution process. We evaluated REAP using a dataset designed to expose LLM\nlimitations, comparing zero-shot prompting with REAP-enhanced prompts across\nsix state-of-the-art models: OpenAI's o1-preview, o1-mini, GPT-4o, GPT-4o-mini,\nGoogle's Gemini 1.5 Pro, and Claude 3.5 Sonnet. The results demonstrate notable\nperformance gains, with o1-mini improving by 40.97%, GPT-4o by 66.26%, and\nGPT-4o-mini by 112.93%. Despite the already strong baseline performance of\nOpenAI's o1-preview, modest gains were observed. Beyond performance\nimprovements, REAP offers a cost-effective solution; for example, GPT-4o-mini,\nwhich is approximately 100 times cheaper than o1-preview, delivered competitive\nresults. REAP also improves the clarity of model outputs, making it easier for\nhumans to understand the reasoning behind the results and simplifying the\nprocess of identifying and addressing any issues. These findings demonstrate\nREAP's potential to greatly improve the capabilities of LLMs, providing both\nbetter performance and increased cost-efficiency across a wide range of\napplications.",
      "tldr_zh": "本文提出 REAP 方法（Reflection、Explicit Problem Deconstruction 和 Advanced Prompting），旨在提升大型语言模型(LLMs)在复杂推理任务中的问题解决能力，通过引导模型反思查询、显式分解问题并生成相关上下文来优化动态上下文框架。实验在六种最先进模型上进行，结果显示显著性能提升，如 GPT-4o-mini 提升112.93%、GPT-4o 提升66.26%，即使是强基线模型 o1-preview 也获得适度改进。REAP 不仅提供成本效益（例如 GPT-4o-mini 比 o1-preview 便宜约100倍），还改善输出清晰度，便于人类理解推理过程和问题诊断。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "524 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.09415v1",
      "published_date": "2024-09-14 11:12:07 UTC",
      "updated_date": "2024-09-14 11:12:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:55:59.538199"
    },
    {
      "arxiv_id": "2409.09413v2",
      "title": "Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence",
      "title_zh": "Qualia 结构与语言涌现之间双向影响的构造性方法",
      "authors": [
        "Tadahiro Taniguchi",
        "Masafumi Oizumi",
        "Noburo Saji",
        "Takato Horii",
        "Naotsugu Tsuchiya"
      ],
      "abstract": "This perspective paper explores the bidirectional influence between language\nemergence and the relational structure of subjective experiences, termed qualia\nstructure, and lays out a constructive approach to the intricate dependency\nbetween the two. We hypothesize that the emergence of languages with\ndistributional semantics (e.g., syntactic-semantic structures) is linked to the\ncoordination of internal representations shaped by experience, potentially\nfacilitating more structured language through reciprocal influence. This\nhypothesized mutual dependency connects to recent advancements in AI and symbol\nemergence robotics, and is explored within this paper through theoretical\nframeworks such as the collective predictive coding. Computational studies show\nthat neural network-based language models form systematically structured\ninternal representations, and multimodal language models can share\nrepresentations between language and perceptual information. This perspective\nsuggests that language emergence serves not only as a mechanism creating a\ncommunication tool but also as a mechanism for allowing people to realize\nshared understanding of qualitative experiences. The paper discusses the\nimplications of this bidirectional influence in the context of consciousness\nstudies, linguistics, and cognitive science, and outlines future constructive\nresearch directions to further explore this dynamic relationship between\nlanguage emergence and qualia structure.",
      "tldr_zh": "这篇论文探讨了语言涌现（language emergence）和主观体验的关联结构（qualia structure）之间的双向影响，提出一种建设性方法来分析这种相互依赖关系。作者假设语言的分布语义（distributional semantics）结构通过内部表征的协调来促进更结构化的语言发展，并结合理论框架如集体预测编码（collective predictive coding）及计算研究，展示了神经网络模型如何形成系统化内部表征，以及多模态模型在语言和感知信息之间共享表征。研究强调语言涌现不仅是通信工具，还能帮助实现对定性体验的共享理解，并为意识研究、语言学和认知科学提供启示，同时概述了未来研究方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09413v2",
      "published_date": "2024-09-14 11:03:12 UTC",
      "updated_date": "2025-05-05 03:26:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:56:13.944696"
    },
    {
      "arxiv_id": "2409.09412v2",
      "title": "Label Convergence: Defining an Upper Performance Bound in Object Recognition through Contradictory Annotations",
      "title_zh": "标签收敛：在对象识别中通过矛盾标注定义上性能界限",
      "authors": [
        "David Tschirschwitz",
        "Volker Rodehorst"
      ],
      "abstract": "Annotation errors are a challenge not only during training of machine\nlearning models, but also during their evaluation. Label variations and\ninaccuracies in datasets often manifest as contradictory examples that deviate\nfrom established labeling conventions. Such inconsistencies, when significant,\nprevent models from achieving optimal performance on metrics such as mean\nAverage Precision (mAP). We introduce the notion of \"label convergence\" to\ndescribe the highest achievable performance under the constraint of\ncontradictory test annotations, essentially defining an upper bound on model\naccuracy.\n  Recognizing that noise is an inherent characteristic of all data, our study\nanalyzes five real-world datasets, including the LVIS dataset, to investigate\nthe phenomenon of label convergence. We approximate that label convergence is\nbetween 62.63-67.52 mAP@[0.5:0.95:0.05] for LVIS with 95% confidence,\nattributing these bounds to the presence of real annotation errors. With\ncurrent state-of-the-art (SOTA) models at the upper end of the label\nconvergence interval for the well-studied LVIS dataset, we conclude that model\ncapacity is sufficient to solve current object detection problems. Therefore,\nfuture efforts should focus on three key aspects: (1) updating the problem\nspecification and adjusting evaluation practices to account for unavoidable\nlabel noise, (2) creating cleaner data, especially test data, and (3) including\nmulti-annotated data to investigate annotation variation and make these issues\nvisible from the outset.",
      "tldr_zh": "本文引入“label convergence”概念，作为物体识别中矛盾标注下模型性能的上限，旨在解决标注错误对训练和评估的影响。研究分析了五个真实数据集，包括LVIS，发现LVIS的label convergence区间为62.63-67.52 mAP@[0.5:0.95:0.05]，且当前SOTA模型已接近这一上限，表明模型能力已足以应对现有物体检测问题。未来建议重点更新问题规范以处理标签噪声、创建更干净的测试数据，以及采用多标注数据来探究标注变异。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at WACV 2025, added reference to paper associated code",
      "pdf_url": "http://arxiv.org/pdf/2409.09412v2",
      "published_date": "2024-09-14 10:59:25 UTC",
      "updated_date": "2025-01-21 23:23:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:56:23.723207"
    },
    {
      "arxiv_id": "2410.01813v1",
      "title": "Privacy-Preserving SAM Quantization for Efficient Edge Intelligence in Healthcare",
      "title_zh": "隐私保护的 SAM 量化，用于医疗保健高效边缘智能",
      "authors": [
        "Zhikai Li",
        "Jing Zhang",
        "Qingyi Gu"
      ],
      "abstract": "The disparity in healthcare personnel expertise and medical resources across\ndifferent regions of the world is a pressing social issue. Artificial\nintelligence technology offers new opportunities to alleviate this issue.\nSegment Anything Model (SAM), which excels in intelligent image segmentation,\nhas demonstrated exceptional performance in medical monitoring and assisted\ndiagnosis. Unfortunately, the huge computational and storage overhead of SAM\nposes significant challenges for deployment on resource-limited edge devices.\nQuantization is an effective solution for model compression; however,\ntraditional methods rely heavily on original data for calibration, which raises\nwidespread concerns about medical data privacy and security. In this paper, we\npropose a data-free quantization framework for SAM, called DFQ-SAM, which\nlearns and calibrates quantization parameters without any original data, thus\neffectively preserving data privacy during model compression. Specifically, we\npropose pseudo-positive label evolution for segmentation, combined with patch\nsimilarity, to fully leverage the semantic and distribution priors in\npre-trained models, which facilitates high-quality data synthesis as a\nsubstitute for real data. Furthermore, we introduce scale reparameterization to\nensure the accuracy of low-bit quantization. We perform extensive segmentation\nexperiments on various datasets, and DFQ-SAM consistently provides significant\nperformance on low-bit quantization. DFQ-SAM eliminates the need for data\ntransfer in cloud-edge collaboration, thereby protecting sensitive data from\npotential attacks. It enables secure, fast, and personalized healthcare\nservices at the edge, which enhances system efficiency and optimizes resource\nallocation, and thus facilitating the pervasive application of artificial\nintelligence in worldwide healthcare.",
      "tldr_zh": "该论文针对医疗资源不均等问题，提出了一种隐私保护的Segment Anything Model (SAM)量化框架，名为DFQ-SAM，以实现高效的边缘智能部署。DFQ-SAM无需原始数据即可学习量化参数，通过伪正标签演化(pseudo-positive label evolution for segmentation)和patch相似性利用预训练模型的语义及分布先验合成高质量数据，并引入scale reparameterization确保低位量化的准确性。在多种数据集上的分割实验中，DFQ-SAM在低位量化中表现出显著性能，提升了系统效率，并通过消除数据传输风险，支持安全的个性化医疗服务。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.01813v1",
      "published_date": "2024-09-14 10:43:35 UTC",
      "updated_date": "2024-09-14 10:43:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:56:35.064494"
    },
    {
      "arxiv_id": "2409.09406v1",
      "title": "Real-world Adversarial Defense against Patch Attacks based on Diffusion Model",
      "title_zh": "基于扩散模型的现实世界对抗补丁攻击防御",
      "authors": [
        "Xingxing Wei",
        "Caixin Kang",
        "Yinpeng Dong",
        "Zhengyi Wang",
        "Shouwei Ruan",
        "Yubo Chen",
        "Hang Su"
      ],
      "abstract": "Adversarial patches present significant challenges to the robustness of deep\nlearning models, making the development of effective defenses become critical\nfor real-world applications. This paper introduces DIFFender, a novel\nDIFfusion-based DeFender framework that leverages the power of a text-guided\ndiffusion model to counter adversarial patch attacks. At the core of our\napproach is the discovery of the Adversarial Anomaly Perception (AAP)\nphenomenon, which enables the diffusion model to accurately detect and locate\nadversarial patches by analyzing distributional anomalies. DIFFender seamlessly\nintegrates the tasks of patch localization and restoration within a unified\ndiffusion model framework, enhancing defense efficacy through their close\ninteraction. Additionally, DIFFender employs an efficient few-shot\nprompt-tuning algorithm, facilitating the adaptation of the pre-trained\ndiffusion model to defense tasks without the need for extensive retraining. Our\ncomprehensive evaluation, covering image classification and face recognition\ntasks, as well as real-world scenarios, demonstrates DIFFender's robust\nperformance against adversarial attacks. The framework's versatility and\ngeneralizability across various settings, classifiers, and attack methodologies\nmark a significant advancement in adversarial patch defense strategies. Except\nfor the popular visible domain, we have identified another advantage of\nDIFFender: its capability to easily expand into the infrared domain.\nConsequently, we demonstrate the good flexibility of DIFFender, which can\ndefend against both infrared and visible adversarial patch attacks\nalternatively using a universal defense framework.",
      "tldr_zh": "这篇论文提出了 DIFFender，一种基于文本引导扩散模型的框架，用于防御现实世界的对抗性补丁攻击。核心创新在于发现 Adversarial Anomaly Perception (AAP) 现象，该现象通过分析分布异常来准确检测和定位对抗性补丁，并将补丁定位和修复任务整合到一个统一框架中，提升防御效果。DIFFender 采用高效的 few-shot prompt-tuning 算法，使预训练扩散模型无需大量重新训练即可适应各种任务。实验结果显示，该框架在图像分类、人脸识别和真实场景中表现出色，对多种攻击方法具有鲁棒性和通用性，并易于扩展到红外域。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09406v1",
      "published_date": "2024-09-14 10:38:35 UTC",
      "updated_date": "2024-09-14 10:38:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:56:47.235409"
    },
    {
      "arxiv_id": "2409.09403v2",
      "title": "AI-Driven Virtual Teacher for Enhanced Educational Efficiency: Leveraging Large Pretrain Models for Autonomous Error Analysis and Correction",
      "title_zh": "翻译失败",
      "authors": [
        "Tianlong Xu",
        "Yi-Fan Zhang",
        "Zhendong Chu",
        "Shen Wang",
        "Qingsong Wen"
      ],
      "abstract": "Students frequently make mistakes while solving mathematical problems, and\ntraditional error correction methods are both time-consuming and\nlabor-intensive. This paper introduces an innovative \\textbf{V}irtual\n\\textbf{A}I \\textbf{T}eacher system designed to autonomously analyze and\ncorrect student \\textbf{E}rrors (VATE). Leveraging advanced large language\nmodels (LLMs), the system uses student drafts as a primary source for error\nanalysis, which enhances understanding of the student's learning process. It\nincorporates sophisticated prompt engineering and maintains an error pool to\nreduce computational overhead. The AI-driven system also features a real-time\ndialogue component for efficient student interaction. Our approach demonstrates\nsignificant advantages over traditional and machine learning-based error\ncorrection methods, including reduced educational costs, high scalability, and\nsuperior generalizability. The system has been deployed on the Squirrel AI\nlearning platform for elementary mathematics education, where it achieves\n78.3\\% accuracy in error analysis and shows a marked improvement in student\nlearning efficiency. Satisfaction surveys indicate a strong positive reception,\nhighlighting the system's potential to transform educational practices.",
      "tldr_zh": "这篇论文提出了一种AI驱动的虚拟教师系统（VATE），利用大型预训练模型（Large Pretrain Models）实现自主分析和纠正学生数学错误的自动化教育工具，以解决传统方法耗时费力的痛点。系统通过学生草稿作为主要数据源，结合提示工程（prompt engineering）、错误池机制和实时对话组件，降低了计算开销并提升了交互效率。实验结果显示，在Squirrel AI学习平台上部署后，该系统在小学数学教育中实现了78.3%的错误分析准确率，显著提高了学生学习效率，并展示了降低教育成本、高可扩展性和通用性的优势，用户满意度调查也显示了正面反馈。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI/IAAI 2025 Innovative Application Award",
      "pdf_url": "http://arxiv.org/pdf/2409.09403v2",
      "published_date": "2024-09-14 10:27:36 UTC",
      "updated_date": "2024-12-08 03:25:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:56:59.961258"
    },
    {
      "arxiv_id": "2410.03533v1",
      "title": "Multiscale fusion enhanced spiking neural network for invasive BCI neural signal decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Song",
        "Liyuan Han",
        "Bo Xu",
        "Tielin Zhang"
      ],
      "abstract": "Brain-computer interfaces (BCIs) are an advanced fusion of neuroscience and\nartificial intelligence, requiring stable and long-term decoding of neural\nsignals. Spiking Neural Networks (SNNs), with their neuronal dynamics and\nspike-based signal processing, are inherently well-suited for this task. This\npaper presents a novel approach utilizing a Multiscale Fusion enhanced Spiking\nNeural Network (MFSNN). The MFSNN emulates the parallel processing and\nmultiscale feature fusion seen in human visual perception to enable real-time,\nefficient, and energy-conserving neural signal decoding. Initially, the MFSNN\nemploys temporal convolutional networks and channel attention mechanisms to\nextract spatiotemporal features from raw data. It then enhances decoding\nperformance by integrating these features through skip connections.\nAdditionally, the MFSNN improves generalizability and robustness in cross-day\nsignal decoding through mini-batch supervised generalization learning. In two\nbenchmark invasive BCI paradigms, including the single-hand grasp-and-touch and\ncenter-and-out reach tasks, the MFSNN surpasses traditional artificial neural\nnetwork methods, such as MLP and GRU, in both accuracy and computational\nefficiency. Moreover, the MFSNN's multiscale feature fusion framework is\nwell-suited for the implementation on neuromorphic chips, offering an\nenergy-efficient solution for online decoding of invasive BCI signals.",
      "tldr_zh": "本研究提出了一种Multiscale Fusion enhanced Spiking Neural Network (MFSNN)，旨在通过模仿人类视觉感知的并行处理和多尺度特征融合，实现对侵入式脑机接口 (BCIs) 神经信号的实时、Efficient 和节能解码。MFSNN 首先利用temporal convolutional networks 和channel attention mechanisms 提取原始数据的时空特征，然后通过skip connections 整合这些特征，并采用mini-batch supervised generalization learning 提升跨日信号解码的泛化性和鲁棒性。在两个基准侵入式 BCI 任务（single-hand grasp-and-touch 和 center-and-out reach）中，MFSNN 在准确性和计算效率上超越传统方法如 MLP 和 GRU。此外，该框架适合neuromorphic chips 实现，提供了一种能量高效的在线神经信号解码解决方案。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.03533v1",
      "published_date": "2024-09-14 09:53:30 UTC",
      "updated_date": "2024-09-14 09:53:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:57:11.605429"
    },
    {
      "arxiv_id": "2409.09386v2",
      "title": "AMBER -- Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging",
      "title_zh": "AMBER -- 高级 SegFormer",
      "authors": [
        "Andrea Dosi",
        "Massimo Brescia",
        "Stefano Cavuoti",
        "Mariarca D'Aniello",
        "Michele Delli Veneri",
        "Carlo Donadio",
        "Adriano Ettari",
        "Giuseppe Longo",
        "Alvi Rownok",
        "Luca Sannino",
        "Maria Zampella"
      ],
      "abstract": "Deep learning has revolutionized the field of hyperspectral image (HSI)\nanalysis, enabling the extraction of complex spectral and spatial features.\nWhile convolutional neural networks (CNNs) have been the backbone of HSI\nclassification, their limitations in capturing global contextual features have\nled to the exploration of Vision Transformers (ViTs). This paper introduces\nAMBER, an advanced SegFormer specifically designed for multi-band image\nsegmentation. AMBER enhances the original SegFormer by incorporating\nthree-dimensional convolutions, custom kernel sizes, and a Funnelizer layer.\nThis architecture enables processing hyperspectral data directly, without\nrequiring spectral dimensionality reduction during preprocessing. Our\nexperiments, conducted on three benchmark datasets (Salinas, Indian Pines, and\nPavia University) and on a dataset from the PRISMA satellite, show that AMBER\noutperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa\ncoefficient, and Average Accuracy on the first three datasets, and achieves\nstate-of-the-art performance on the PRISMA dataset. These findings highlight\nAMBER's robustness, adaptability to both airborne and spaceborne data, and its\npotential as a powerful solution for remote sensing and other domains requiring\nadvanced analysis of high-dimensional data.",
      "tldr_zh": "本研究引入 AMBER，一种高级 SegFormer 架构，专门用于多波段图像分割，并应用于超光谱成像 (HSI)，以克服传统 CNNs 在捕捉全局上下文特征方面的局限性。AMBER 通过整合三维卷积、自定义核大小和 Funnelizer 层，实现了对 HSI 数据的直接处理，无需预处理的谱维降维。实验在 Salinas、Indian Pines、Pavia University 等基准数据集以及 PRISMA 卫星数据集上显示，AMBER 在总体准确率、Kappa 系数和平均准确率上优于传统 CNN 方法，并在 PRISMA 数据集上达到最先进性能，证明其在遥感和高维数据分析领域的鲁棒性和适应性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "submitted to Neural Computing & Applications (Springer). Accepted\n  with minor revisions",
      "pdf_url": "http://arxiv.org/pdf/2409.09386v2",
      "published_date": "2024-09-14 09:34:05 UTC",
      "updated_date": "2025-04-14 08:21:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:57:22.986674"
    },
    {
      "arxiv_id": "2409.09383v2",
      "title": "LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free Approach",
      "title_zh": "基于LLM的集成学习用于论文来源",
      "authors": [
        "Kunlong Chen",
        "Junjun Wang",
        "Zhaoqun Chen",
        "Kunjin Chen",
        "Yitian Chen"
      ],
      "abstract": "We participated in the KDD CUP 2024 paper source tracing competition and\nachieved the 3rd place. This competition tasked participants with identifying\nthe reference sources (i.e., ref-sources, as referred to by the organizers of\nthe competition) of given academic papers. Unlike most teams that addressed\nthis challenge by fine-tuning pre-trained neural language models such as BERT\nor ChatGLM, our primary approach utilized closed-source large language models\n(LLMs). With recent advancements in LLM technology, closed-source LLMs have\ndemonstrated the capability to tackle complex reasoning tasks in zero-shot or\nfew-shot scenarios. Consequently, in the absence of GPUs, we employed\nclosed-source LLMs to directly generate predicted reference sources from the\nprovided papers. We further refined these predictions through ensemble\nlearning. Notably, our method was the only one among the award-winning\napproaches that did not require the use of GPUs for model training. Code\navailable at https://github.com/Cklwanfifa/KDDCUP2024-PST.",
      "tldr_zh": "这篇论文介绍了LLM-Powered Ensemble Learning方法，用于KDD CUP 2024论文来源追踪比赛，团队凭借此方法获得第三名。不同于其他团队微调BERT或ChatGLM等模型，该方法利用闭源LLMs在零样本或少样本场景下直接生成参考来源（ref-sources）预测，并通过ensemble learning进行精炼。关键创新在于其GPU-Free设计，使其无需硬件加速即可实现高效追踪，为资源有限的环境提供可访问的解决方案。实验结果证明，该方法在比赛中表现出色，代码已在GitHub上公开。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09383v2",
      "published_date": "2024-09-14 09:21:46 UTC",
      "updated_date": "2024-09-17 01:35:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:57:34.782773"
    },
    {
      "arxiv_id": "2409.09381v1",
      "title": "Text Prompt is Not Enough: Sound Event Enhanced Prompt Adapter for Target Style Audio Generation",
      "title_zh": "文本提示不够：声事件增强提示适配器用于目标风格音频生成",
      "authors": [
        "Chenxu Xiong",
        "Ruibo Fu",
        "Shuchen Shi",
        "Zhengqi Wen",
        "Jianhua Tao",
        "Tao Wang",
        "Chenxing Li",
        "Chunyu Qiang",
        "Yuankun Xie",
        "Xin Qi",
        "Guanjun Li",
        "Zizheng Yang"
      ],
      "abstract": "Current mainstream audio generation methods primarily rely on simple text\nprompts, often failing to capture the nuanced details necessary for multi-style\naudio generation. To address this limitation, the Sound Event Enhanced Prompt\nAdapter is proposed. Unlike traditional static global style transfer, this\nmethod extracts style embedding through cross-attention between text and\nreference audio for adaptive style control. Adaptive layer normalization is\nthen utilized to enhance the model's capacity to express multiple styles.\nAdditionally, the Sound Event Reference Style Transfer Dataset (SERST) is\nintroduced for the proposed target style audio generation task, enabling\ndual-prompt audio generation using both text and audio references. Experimental\nresults demonstrate the robustness of the model, achieving state-of-the-art\nFr\\'echet Distance of 26.94 and KL Divergence of 1.82, surpassing Tango,\nAudioLDM, and AudioGen. Furthermore, the generated audio shows high similarity\nto its corresponding audio reference. The demo, code, and dataset are publicly\navailable.",
      "tldr_zh": "该论文指出，当前音频生成方法依赖简单文本提示，无法有效捕捉多样式音频的细微细节，因此提出 Sound Event Enhanced Prompt Adapter，以实现自适应样式控制。该方法通过文本和参考音频的交叉注意力提取样式嵌入，并利用 Adaptive Layer Normalization 增强模型的多样式表达能力。同时，引入 Sound Event Reference Style Transfer Dataset (SERST)，支持文本和音频双提示的音频生成任务。实验结果显示，该模型在 Fréchet Distance 和 KL Divergence 上分别达到最先进水平（26.94 和 1.82），优于 Tango、AudioLDM 和 AudioGen，且生成的音频与参考音频高度相似。代码、数据集和演示已公开可用。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "5 pages, 2 figures, submitted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.09381v1",
      "published_date": "2024-09-14 09:16:38 UTC",
      "updated_date": "2024-09-14 09:16:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:57:47.653224"
    },
    {
      "arxiv_id": "2409.09378v1",
      "title": "Prevailing Research Areas for Music AI in the Era of Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Megan Wei",
        "Mateusz Modrzejewski",
        "Aswin Sivaraman",
        "Dorien Herremans"
      ],
      "abstract": "In tandem with the recent advancements in foundation model research, there\nhas been a surge of generative music AI applications within the past few years.\nAs the idea of AI-generated or AI-augmented music becomes more mainstream, many\nresearchers in the music AI community may be wondering what avenues of research\nare left. With regards to music generative models, we outline the current areas\nof research with significant room for exploration. Firstly, we pose the\nquestion of foundational representation of these generative models and\ninvestigate approaches towards explainability. Next, we discuss the current\nstate of music datasets and their limitations. We then overview different\ngenerative models, forms of evaluating these models, and their computational\nconstraints/limitations. Subsequently, we highlight applications of these\ngenerative models towards extensions to multiple modalities and integration\nwith artists' workflow as well as music education systems. Finally, we survey\nthe potential copyright implications of generative music and discuss strategies\nfor protecting the rights of musicians. While it is not meant to be exhaustive,\nour survey calls to attention a variety of research directions enabled by music\nfoundation models.",
      "tldr_zh": "这篇论文探讨了基础模型（foundation models）时代音乐AI的主要研究领域，强调随着生成音乐AI应用的兴起，仍有许多值得探索的方向。论文概述了关键议题，包括生成模型的基础表示和可解释性（explainability）、音乐数据集的局限性、不同模型的评估方法及其计算约束。最终，它呼吁关注这些模型在多模态扩展、艺术家工作流整合、音乐教育以及版权保护方面的应用和挑战，以推动音乐AI的创新发展。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "68T05, 68T20",
        "I.2; I.5.4; I.2.6; I.2.7; H.5.5"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09378v1",
      "published_date": "2024-09-14 09:06:43 UTC",
      "updated_date": "2024-09-14 09:06:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:57:58.161201"
    },
    {
      "arxiv_id": "2409.17167v2",
      "title": "StressPrompt: Does Stress Impact Large Language Models and Human Performance Similarly?",
      "title_zh": "StressPrompt: 压力是否同样影响大型语言模型和人类表现？",
      "authors": [
        "Guobin Shen",
        "Dongcheng Zhao",
        "Aorigele Bao",
        "Xiang He",
        "Yiting Dong",
        "Yi Zeng"
      ],
      "abstract": "Human beings often experience stress, which can significantly influence their\nperformance. This study explores whether Large Language Models (LLMs) exhibit\nstress responses similar to those of humans and whether their performance\nfluctuates under different stress-inducing prompts. To investigate this, we\ndeveloped a novel set of prompts, termed StressPrompt, designed to induce\nvarying levels of stress. These prompts were derived from established\npsychological frameworks and carefully calibrated based on ratings from human\nparticipants. We then applied these prompts to several LLMs to assess their\nresponses across a range of tasks, including instruction-following, complex\nreasoning, and emotional intelligence. The findings suggest that LLMs, like\nhumans, perform optimally under moderate stress, consistent with the\nYerkes-Dodson law. Notably, their performance declines under both low and\nhigh-stress conditions. Our analysis further revealed that these StressPrompts\nsignificantly alter the internal states of LLMs, leading to changes in their\nneural representations that mirror human responses to stress. This research\nprovides critical insights into the operational robustness and flexibility of\nLLMs, demonstrating the importance of designing AI systems capable of\nmaintaining high performance in real-world scenarios where stress is prevalent,\nsuch as in customer service, healthcare, and emergency response contexts.\nMoreover, this study contributes to the broader AI research community by\noffering a new perspective on how LLMs handle different scenarios and their\nsimilarities to human cognition.",
      "tldr_zh": "这篇论文探讨了压力是否对大型语言模型 (LLMs) 和人类表现产生类似影响，研究者开发了 StressPrompt——一组基于心理框架并经人类校准的提示，用于诱发不同压力水平。实验通过应用这些提示评估 LLMs 在指令遵循、复杂推理和情感智能等任务中的表现，结果显示 LLMs 在中等压力下表现最佳，与 Yerkes-Dodson 法则一致，并在高低压力条件下性能下降。进一步分析发现，StressPrompt 导致 LLMs 的神经表示发生变化，类似于人类的压力响应，这为设计能在客服、健康和应急响应等现实场景中保持鲁棒性的 AI 系统提供了关键洞见。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "11 pages, 9 figures, Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.17167v2",
      "published_date": "2024-09-14 08:32:31 UTC",
      "updated_date": "2025-01-28 01:57:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:58:11.568312"
    },
    {
      "arxiv_id": "2409.09360v3",
      "title": "LACOSTE: Exploiting stereo and temporal contexts for surgical instrument segmentation",
      "title_zh": "LACOSTE：利用立体和时间上下文进行手术器械分割",
      "authors": [
        "Qiyuan Wang",
        "Shang Zhao",
        "Zikang Xu",
        "S Kevin Zhou"
      ],
      "abstract": "Surgical instrument segmentation is instrumental to minimally invasive\nsurgeries and related applications. Most previous methods formulate this task\nas single-frame-based instance segmentation while ignoring the natural temporal\nand stereo attributes of a surgical video. As a result, these methods are less\nrobust against the appearance variation through temporal motion and view\nchange. In this work, we propose a novel LACOSTE model that exploits\nLocation-Agnostic COntexts in Stereo and TEmporal images for improved surgical\ninstrument segmentation. Leveraging a query-based segmentation model as core,\nwe design three performance-enhancing modules. Firstly, we design a\ndisparity-guided feature propagation module to enhance depth-aware features\nexplicitly. To generalize well for even only a monocular video, we apply a\npseudo stereo scheme to generate complementary right images. Secondly, we\npropose a stereo-temporal set classifier, which aggregates stereo-temporal\ncontexts in a universal way for making a consolidated prediction and mitigates\ntransient failures. Finally, we propose a location-agnostic classifier to\ndecouple the location bias from mask prediction and enhance the feature\nsemantics. We extensively validate our approach on three public surgical video\ndatasets, including two benchmarks from EndoVis Challenges and one real radical\nprostatectomy surgery dataset GraSP. Experimental results demonstrate the\npromising performances of our method, which consistently achieves comparable or\nfavorable results with previous state-of-the-art approaches.",
      "tldr_zh": "本研究提出LACOSTE模型，通过利用立体和时间上下文来提升手术器械分割的鲁棒性，解决现有单帧实例分割方法对时间运动和视图变化的敏感问题。模型基于查询式分割框架，设计了三个关键模块：disparity-guided feature propagation模块增强深度感知特征（并使用pseudo stereo方案适应单目视频）、stereo-temporal set classifier聚合立体-时间上下文以减少临时失败，以及location-agnostic classifier分离位置偏差并强化特征语义。在EndoVis Challenges和GraSP等三个公共数据集上实验验证显示，LACOSTE实现了与现有最先进方法相当或更优的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint submitted to Medical Image Analysis",
      "pdf_url": "http://arxiv.org/pdf/2409.09360v3",
      "published_date": "2024-09-14 08:17:56 UTC",
      "updated_date": "2024-10-08 13:13:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:58:23.341603"
    },
    {
      "arxiv_id": "2409.09359v3",
      "title": "Symbolic Regression with a Learned Concept Library",
      "title_zh": "基于学得概念库的符号回归",
      "authors": [
        "Arya Grayeli",
        "Atharva Sehgal",
        "Omar Costilla-Reyes",
        "Miles Cranmer",
        "Swarat Chaudhuri"
      ],
      "abstract": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm,\ncalled LaSR, uses zero-shot queries to a large language model (LLM) to discover\nand evolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LaSR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning\nand evolutionary algorithms. Moreover, we show that LaSR can be used to\ndiscover a novel and powerful scaling law for LLMs.",
      "tldr_zh": "这篇论文提出了LaSR，一种用于Symbolic Regression的新方法，通过大型语言模型(LLM)的零-shot查询来发现和演化抽象文本概念库，从而提升搜索紧凑程序性假设的能力。LaSR结合标准进化算法和LLM引导步骤，迭代地从高性能假设中抽象概念，并生成新假设。实验结果显示，在Feynman方程和合成任务基准上，LaSR大幅优于基于深度学习和进化算法的现有方法，并成功发现LLM的强大缩放定律。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.SC"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS version; 10 pages; no checklist; added more experiment\n  details",
      "pdf_url": "http://arxiv.org/pdf/2409.09359v3",
      "published_date": "2024-09-14 08:17:30 UTC",
      "updated_date": "2024-12-10 16:24:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:58:34.222747"
    },
    {
      "arxiv_id": "2409.09357v1",
      "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyu Liu",
        "Xu Li",
        "Joan Serrà",
        "Santiago Pascual"
      ],
      "abstract": "Speech restoration aims at restoring full-band speech with high quality and\nintelligibility, considering a diverse set of distortions. MaskSR is a recently\nproposed generative model for this task. As other models of its kind, MaskSR\nattains high quality but, as we show, intelligibility can be substantially\nimproved. We do so by boosting the speech encoder component of MaskSR with\npredictions of semantic representations of the target speech, using a\npre-trained self-supervised teacher model. Then, a masked language model is\nconditioned on the learned semantic features to predict acoustic tokens that\nencode low level spectral details of the target speech. We show that, with the\nsame MaskSR model capacity and inference time, the proposed model, MaskSR2,\nsignificantly reduces the word error rate, a typical metric for\nintelligibility. MaskSR2 also achieves competitive word error rate among other\nmodels, while providing superior quality. An ablation study shows the\neffectiveness of various semantic representations.",
      "tldr_zh": "本研究针对语音恢复（Speech restoration）任务，提出了一种结合语义知识蒸馏（Semantic Knowledge Distillation）和掩码声学建模（Masked Acoustic Modeling）的MaskSR2模型，以提升全频带语音的质量和intelligibility。方法通过使用预训练的自监督教师模型预测目标语音的语义表示，并以此作为条件，让掩码语言模型预测声学tokens，从而改进MaskSR的语音编码器。在相同模型容量和推理时间下，MaskSR2显著降低了word error rate (WER)，实现了比基线模型更优的intelligibility，同时保持了高质量的语音恢复；消融研究进一步验证了不同语义表示的有效性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "Demo link https://masksr.github.io/MaskSR2/",
      "pdf_url": "http://arxiv.org/pdf/2409.09357v1",
      "published_date": "2024-09-14 08:09:55 UTC",
      "updated_date": "2024-09-14 08:09:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:58:46.313743"
    },
    {
      "arxiv_id": "2409.10570v2",
      "title": "Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Model Watermarking",
      "title_zh": "翻译失败",
      "authors": [
        "Cong Kong",
        "Rui Xu",
        "Weixi Chen",
        "Jiawei Chen",
        "Zhaoxia Yin"
      ],
      "abstract": "With the advancement of intelligent healthcare, medical pre-trained language\nmodels (Med-PLMs) have emerged and demonstrated significant effectiveness in\ndownstream medical tasks. While these models are valuable assets, they are\nvulnerable to misuse and theft, requiring copyright protection. However,\nexisting watermarking methods for pre-trained language models (PLMs) cannot be\ndirectly applied to Med-PLMs due to domain-task mismatch and inefficient\nwatermark embedding. To fill this gap, we propose the first training-free\nbackdoor model watermarking for Med-PLMs. Our method employs low-frequency\nwords as triggers, embedding the watermark by replacing their embeddings in the\nmodel's word embedding layer with those of specific medical terms. The\nwatermarked Med-PLMs produce the same output for triggers as for the\ncorresponding specified medical terms. We leverage this unique mapping to\ndesign tailored watermark extraction schemes for different downstream tasks,\nthereby addressing the challenge of domain-task mismatch in previous methods.\nExperiments demonstrate superior effectiveness of our watermarking method\nacross medical downstream tasks. Moreover, the method exhibits robustness\nagainst model extraction, pruning, fusion-based backdoor removal attacks, while\nmaintaining high efficiency with 10-second watermark embedding.",
      "tldr_zh": "该研究针对医疗预训练语言模型（Med-PLMs）的版权保护问题，提出了一种训练-free 的后门模型水印方法，以解决现有方法在领域任务不匹配和嵌入效率上的不足。主要方法包括使用低频词作为触发器，在模型的词嵌入层中替换其嵌入为特定医疗术语的嵌入，从而使水印模型对触发器产生与指定术语相同的输出，并为不同下游任务设计定制的水印提取方案。实验结果显示，该方法在医疗下游任务中表现出色，能够抵抗模型提取、剪枝和融合-based 后门移除攻击，同时以10秒的高效率完成水印嵌入。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.10570v2",
      "published_date": "2024-09-14 08:08:55 UTC",
      "updated_date": "2025-04-15 08:07:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:58:58.639087"
    },
    {
      "arxiv_id": "2409.09354v1",
      "title": "PeriGuru: A Peripheral Robotic Mobile App Operation Assistant based on GUI Image Understanding and Prompting with LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Kelin Fu",
        "Yang Tian",
        "Kaigui Bian"
      ],
      "abstract": "Smartphones have significantly enhanced our daily learning, communication,\nand entertainment, becoming an essential component of modern life. However,\ncertain populations, including the elderly and individuals with disabilities,\nencounter challenges in utilizing smartphones, thus necessitating mobile app\noperation assistants, a.k.a. mobile app agent. With considerations for privacy,\npermissions, and cross-platform compatibility issues, we endeavor to devise and\ndevelop PeriGuru in this work, a peripheral robotic mobile app operation\nassistant based on GUI image understanding and prompting with Large Language\nModel (LLM). PeriGuru leverages a suite of computer vision techniques to\nanalyze GUI screenshot images and employs LLM to inform action decisions, which\nare then executed by robotic arms. PeriGuru achieves a success rate of 81.94%\non the test task set, which surpasses by more than double the method without\nPeriGuru's GUI image interpreting and prompting design. Our code is available\non https://github.com/Z2sJ4t/PeriGuru.",
      "tldr_zh": "本文开发了 PeriGuru，一种基于 GUI 图像理解和 LLM 提示的外围机器人移动应用操作助手，旨在帮助老人和残疾人克服智能手机使用障碍，同时考虑隐私、权限和跨平台兼容性问题。PeriGuru 利用计算机视觉技术分析 GUI 截图，并结合 LLM 进行行动决策，由机器人臂执行操作。实验结果显示，其在测试任务集上的成功率达到 81.94%，比没有 GUI 图像解释设计的基线方法高出一倍多。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09354v1",
      "published_date": "2024-09-14 07:54:25 UTC",
      "updated_date": "2024-09-14 07:54:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:59:10.805581"
    },
    {
      "arxiv_id": "2409.09345v1",
      "title": "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models",
      "title_zh": "通过步骤级别的 Q 值模型增强大语言模型代理的决策",
      "authors": [
        "Yuanzhao Zhai",
        "Tingkai Yang",
        "Kele Xu",
        "Feng Dawei",
        "Cheng Yang",
        "Bo Ding",
        "Huaimin Wang"
      ],
      "abstract": "Agents significantly enhance the capabilities of standalone Large Language\nModels (LLMs) by perceiving environments, making decisions, and executing\nactions. However, LLM agents still face challenges in tasks that require\nmultiple decision-making steps. Estimating the value of actions in specific\ntasks is difficult when intermediate actions are neither appropriately rewarded\nnor penalized. In this paper, we propose leveraging a task-relevant Q-value\nmodel to guide action selection. Specifically, we first collect decision-making\ntrajectories annotated with step-level Q values via Monte Carlo Tree Search\n(MCTS) and construct preference data. We then use another LLM to fit these\npreferences through step-level Direct Policy Optimization (DPO), which serves\nas the Q-value model. During inference, at each decision-making step, LLM\nagents select the action with the highest Q value before interacting with the\nenvironment. We apply our method to various open-source and API-based LLM\nagents, demonstrating that Q-value models significantly improve their\nperformance. Notably, the performance of the agent built with\nPhi-3-mini-4k-instruct improved by 103% on WebShop and 75% on HotPotQA when\nenhanced with Q-value models, even surpassing GPT-4o-mini. Additionally,\nQ-value models offer several advantages, such as generalization to different\nLLM agents and seamless integration with existing prompting strategies.",
      "tldr_zh": "该论文提出了一种通过步级 Q-value model 来提升 LLM agents 多步骤决策能力的方法，以解决中间动作缺乏适当奖励的挑战。具体而言，该方法先利用 Monte Carlo Tree Search (MCTS) 收集标注 Q 值的决策轨迹，并通过 Direct Policy Optimization (DPO) 训练一个 Q-value model 指导行动选择；在推理阶段，LLM agents 优先选择 Q 值最高的行动。实验结果显示，该方法显著提高了各种 LLM agents 的性能，例如 Phi-3-mini-4k-instruct 在 WebShop 上提升 103%、在 HotPotQA 上提升 75%，甚至超过了 GPT-4o-mini。此外，Q-value model 具备泛化到不同代理的能力，并可无缝整合现有提示策略。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09345v1",
      "published_date": "2024-09-14 07:32:49 UTC",
      "updated_date": "2024-09-14 07:32:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:59:23.704877"
    },
    {
      "arxiv_id": "2409.09340v1",
      "title": "Egocentric Speaker Classification in Child-Adult Dyadic Interactions: From Sensing to Computational Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Tiantian Feng",
        "Anfeng Xu",
        "Xuan Shi",
        "Somer Bishop",
        "Shrikanth Narayanan"
      ],
      "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental condition\ncharacterized by challenges in social communication, repetitive behavior, and\nsensory processing. One important research area in ASD is evaluating children's\nbehavioral changes over time during treatment. The standard protocol with this\nobjective is BOSCC, which involves dyadic interactions between a child and\nclinicians performing a pre-defined set of activities. A fundamental aspect of\nunderstanding children's behavior in these interactions is automatic speech\nunderstanding, particularly identifying who speaks and when. Conventional\napproaches in this area heavily rely on speech samples recorded from a\nspectator perspective, and there is limited research on egocentric speech\nmodeling. In this study, we design an experiment to perform speech sampling in\nBOSCC interviews from an egocentric perspective using wearable sensors and\nexplore pre-training Ego4D speech samples to enhance child-adult speaker\nclassification in dyadic interactions. Our findings highlight the potential of\negocentric speech collection and pre-training to improve speaker classification\naccuracy.",
      "tldr_zh": "本研究针对自闭症谱系障碍(ASD)儿童在BOSCC协议下的儿童-成人互动中，开发了第一人称(egocentric)说话者分类方法，以提升自动语音理解的准确性。通过使用可穿戴传感器进行语音采样，并预训练Ego4D数据集，研究从感知到计算建模的过程，有效解决了传统旁观者视角的局限性。实验结果显示，这种egocentric方法显著提高了说话者分类准确性，为ASD儿童行为评估提供了新的技术支持。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "pre-print under review",
      "pdf_url": "http://arxiv.org/pdf/2409.09340v1",
      "published_date": "2024-09-14 07:03:08 UTC",
      "updated_date": "2024-09-14 07:03:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:59:35.238218"
    },
    {
      "arxiv_id": "2409.09337v3",
      "title": "Wave-U-Mamba: An End-To-End Framework For High-Quality And Efficient Speech Super Resolution",
      "title_zh": "Wave-U-Mamba：端到端框架，用于高质量和高效的语音超分辨率",
      "authors": [
        "Yongjoon Lee",
        "Chanwoo Kim"
      ],
      "abstract": "Speech Super-Resolution (SSR) is a task of enhancing low-resolution speech\nsignals by restoring missing high-frequency components. Conventional approaches\ntypically reconstruct log-mel features, followed by a vocoder that generates\nhigh-resolution speech in the waveform domain. However, as mel features lack\nphase information, this can result in performance degradation during the\nreconstruction phase. Motivated by recent advances with Selective State Spaces\nModels (SSMs), we propose a method, referred to as Wave-U-Mamba that directly\nperforms SSR in time domain. In our comparative study, including models such as\nWSRGlow, NU-Wave 2, and AudioSR, Wave-U-Mamba demonstrates superior\nperformance, achieving the lowest Log-Spectral Distance (LSD) across various\nlow-resolution sampling rates, ranging from 8 to 24 kHz. Additionally,\nsubjective human evaluations, scored using Mean Opinion Score (MOS) reveal that\nour method produces SSR with natural and human-like quality. Furthermore,\nWave-U-Mamba achieves these results while generating high-resolution speech\nover nine times faster than baseline models on a single A100 GPU, with\nparameter sizes less than 2\\% of those in the baseline models.",
      "tldr_zh": "本论文提出Wave-U-Mamba框架，这是一种基于Selective State Spaces Models (SSMs)的端到端方法，用于高品质和高效的Speech Super-Resolution (SSR)，直接在时域处理低分辨率语音信号，以解决传统方法中log-mel特征缺失相位信息导致的性能问题。相比WSRGlow、NU-Wave 2和AudioSR等基线模型，Wave-U-Mamba在不同采样率（8-24 kHz）下实现了最低的Log-Spectral Distance (LSD)，并通过Mean Opinion Score (MOS)主观评估显示出更自然的人类化语音质量。该框架不仅提升了重建精度，还将生成速度提高9倍以上，参数量不到基线模型的2%，在单A100 GPU上表现出色。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.09337v3",
      "published_date": "2024-09-14 06:52:00 UTC",
      "updated_date": "2025-02-03 12:07:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:59:48.119555"
    },
    {
      "arxiv_id": "2409.09324v2",
      "title": "Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Yi Leong",
        "Yi Fan Gao",
        "Ji Shuai",
        "Yang Zhang",
        "Uktu Pamuksuz"
      ],
      "abstract": "Scientific research indicates that for every hour spent in direct patient\ncare, physicians spend nearly two additional hours on administrative tasks,\nparticularly on electronic health records (EHRs) and desk work. This excessive\nadministrative burden not only reduces the time available for patient care but\nalso contributes to physician burnout and inefficiencies in healthcare\ndelivery. To address these challenges, this study introduces MediGen, a\nfine-tuned large language model (LLM) designed to automate the generation of\nmedical reports from medical dialogues. By leveraging state-of-the-art\nmethodologies for fine-tuning open-source pretrained models, including\nLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing\nclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising\nresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating\nits effectiveness in generating accurate and clinically relevant medical\nreports. These findings suggest that MediGen has the potential to significantly\nreduce the administrative workload on physicians, improving both healthcare\nefficiency and physician well-being.",
      "tldr_zh": "本研究指出，医生每花一小时在直接患者护理上，就需额外花费近两小时在行政任务如电子健康记录（EHRs）上，导致烧尽和医疗效率低下。为解决此问题，提出MediGen，一种基于LLaMA3-8B的大型语言模型（LLM）通过高效微调方法，自动从医疗对话中生成准确的医疗报告。实验结果显示，MediGen在ROUGE score达到58%和BERTScore-F1达到72%，证明其在转录和总结临床互动方面的有效性。这些发现表明，MediGen有望显著减轻医生的行政负担，提升医疗效率和福祉。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, 3 Figures, 3 Tables. The final version will be published in\n  the proceedings of the IEEE conference",
      "pdf_url": "http://arxiv.org/pdf/2409.09324v2",
      "published_date": "2024-09-14 06:02:17 UTC",
      "updated_date": "2024-09-28 02:16:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T23:59:58.986651"
    },
    {
      "arxiv_id": "2409.18980v1",
      "title": "IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web",
      "title_zh": "IW-Bench：评估大型多模态模型用于图像到网页转换",
      "authors": [
        "Hongcheng Guo",
        "Wei Zhang",
        "Junhao Chen",
        "Yaonan Gu",
        "Jian Yang",
        "Junjia Du",
        "Binyuan Hui",
        "Tianyu Liu",
        "Jianxin Ma",
        "Chang Zhou",
        "Zhoujun Li"
      ],
      "abstract": "Recently advancements in large multimodal models have led to significant\nstrides in image comprehension capabilities. Despite these advancements, there\nis a lack of the robust benchmark specifically for assessing the Image-to-Web\nconversion proficiency of these large models. Primarily, it is essential to\nensure the integrity of the web elements generated. These elements comprise\nvisible and invisible categories. Previous evaluation methods (e.g., BLEU) are\nnotably susceptible to significant alterations due to the presence of invisible\nelements in Web. Furthermore, it is crucial to measure the layout information\nof web pages, referring to the positional relationships between elements, which\nis overlooked by previous work. To address challenges, we have curated and\naligned a benchmark of images and corresponding web codes (IW-Bench).\nSpecifically, we propose the Element Accuracy, which tests the completeness of\nthe elements by parsing the Document Object Model (DOM) tree. Layout Accuracy\nis also proposed to analyze the positional relationships of elements by\nconverting DOM tree into a common subsequence. Besides, we design a five-hop\nmultimodal Chain-of-Thought Prompting for better performance, which contains\nfive hop: 1) SoM prompt injection. 2) Inferring Elements. 3) Inferring Layout.\n4) Inferring Web code. 5) Reflection. Our benchmark comprises 1200 pairs of\nimages and web codes with varying levels of difficulty. We have conducted\nextensive experiments on existing large multimodal models, offering insights\ninto their performance and areas for improvement in image-to-web domain.",
      "tldr_zh": "这项研究针对大型多模态模型（Large Multimodal Models）在图像到网页（Image-to-Web）转换方面的能力，指出现有基准（如 BLEU）忽略了网页元素的不可见部分和布局信息，从而提出 IW-Bench 基准，该基准包含 1200 对图像和网页代码对，以评估模型的转换效能。论文引入了 Element Accuracy 和 Layout Accuracy 指标，分别通过解析 Document Object Model (DOM) 树来检查元素的完整性和位置关系，并设计了五步多模态 Chain-of-Thought Prompting 方法，包括 SoM 提示注入、推断元素、布局、网页代码以及反思，以提升模型性能。通过广泛实验，研究者评估了现有模型的表现，并提供了在图像到网页领域改进的见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18980v1",
      "published_date": "2024-09-14 05:38:26 UTC",
      "updated_date": "2024-09-14 05:38:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:00:12.093194"
    },
    {
      "arxiv_id": "2409.16312v1",
      "title": "SEE: Semantically Aligned EEG-to-Text Translation",
      "title_zh": "SEE：语义对齐的 EEG 到文本翻译",
      "authors": [
        "Yitian Tao",
        "Yan Liang",
        "Luoyu Wang",
        "Yongqing Li",
        "Qing Yang",
        "Han Zhang"
      ],
      "abstract": "Decoding neurophysiological signals into language is of great research\ninterest within brain-computer interface (BCI) applications.\nElectroencephalography (EEG), known for its non-invasiveness, ease of use, and\ncost-effectiveness, has been a popular method in this field. However, current\nEEG-to-Text decoding approaches face challenges due to the huge domain gap\nbetween EEG recordings and raw texts, inherent data bias, and small closed\nvocabularies. In this paper, we propose SEE: Semantically Aligned EEG-to-Text\nTranslation, a novel method aimed at improving EEG-to-Text decoding by\nseamlessly integrating two modules into a pre-trained BART language model.\nThese two modules include (1) a Cross-Modal Codebook that learns cross-modal\nrepresentations to enhance feature consolidation and mitigate domain gap, and\n(2) a Semantic Matching Module that fully utilizes pre-trained text\nrepresentations to align multi-modal features extracted from EEG-Text pairs\nwhile considering noise caused by false negatives, i.e., data from different\nEEG-Text pairs that have similar semantic meanings. Experimental results on the\nZurich Cognitive Language Processing Corpus (ZuCo) demonstrate the\neffectiveness of SEE, which enhances the feasibility of accurate EEG-to-Text\ndecoding.",
      "tldr_zh": "这篇论文提出 SEE（Semantically Aligned EEG-to-Text Translation）方法，旨在提升脑机接口（BCI）中的 EEG-to-Text 解码性能，解决 EEG 记录与文本之间的领域差距、数据偏差和小词汇表问题。SEE 通过将 Cross-Modal Codebook 和 Semantic Matching Module 整合到预训练 BART 语言模型中，前者用于学习跨模态表示以增强特征整合，后者利用预训练文本表示对齐 EEG-Text 对的多模态特征，同时处理假负样本带来的噪声。实验结果在 Zurich Cognitive Language Processing Corpus (ZuCo) 数据集上验证了 SEE 的有效性，显著提高了 EEG-to-Text 解码的准确性和可行性。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "q-bio.QM",
      "comment": "4 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.16312v1",
      "published_date": "2024-09-14 05:37:15 UTC",
      "updated_date": "2024-09-14 05:37:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:00:23.910692"
    },
    {
      "arxiv_id": "2409.09305v1",
      "title": "The T05 System for The VoiceMOS Challenge 2024: Transfer Learning from Deep Image Classifier to Naturalness MOS Prediction of High-Quality Synthetic Speech",
      "title_zh": "翻译失败",
      "authors": [
        "Kaito Baba",
        "Wataru Nakata",
        "Yuki Saito",
        "Hiroshi Saruwatari"
      ],
      "abstract": "We present our system (denoted as T05) for the VoiceMOS Challenge (VMC) 2024.\nOur system was designed for the VMC 2024 Track 1, which focused on the accurate\nprediction of naturalness mean opinion score (MOS) for high-quality synthetic\nspeech. In addition to a pretrained self-supervised learning (SSL)-based speech\nfeature extractor, our system incorporates a pretrained image feature extractor\nto capture the difference of synthetic speech observed in speech spectrograms.\nWe first separately train two MOS predictors that use either of an SSL-based or\nspectrogram-based feature. Then, we fine-tune the two predictors for better MOS\nprediction using the fusion of two extracted features. In the VMC 2024 Track 1,\nour T05 system achieved first place in 7 out of 16 evaluation metrics and\nsecond place in the remaining 9 metrics, with a significant difference compared\nto those ranked third and below. We also report the results of our ablation\nstudy to investigate essential factors of our system.",
      "tldr_zh": "该研究介绍了T05系统，针对VoiceMOS Challenge 2024 Track 1，专注于通过转移学习从深度图像分类器预测高品质合成语音的自然度MOS（Mean Opinion Score）。系统结合预训练的自监督学习（SSL）语音特征提取器和图像特征提取器，先分别训练基于SSL或谱图的MOS预测器，然后通过特征融合进行微调。实验结果显示，T05系统在16个评估指标中获得7个第一名和9个第二名，与第三名及以下有显著差距，并通过消融研究验证了系统关键因素的贡献。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by IEEE SLT 2024. Our MOS prediction system (UTMOSv2) is\n  available in https://github.com/sarulab-speech/UTMOSv2",
      "pdf_url": "http://arxiv.org/pdf/2409.09305v1",
      "published_date": "2024-09-14 05:03:18 UTC",
      "updated_date": "2024-09-14 05:03:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:00:34.768193"
    },
    {
      "arxiv_id": "2409.09298v1",
      "title": "Matrix Profile for Anomaly Detection on Multidimensional Time Series",
      "title_zh": "Matrix Profile 用于多维时间序列的异常检测",
      "authors": [
        "Chin-Chia Michael Yeh",
        "Audrey Der",
        "Uday Singh Saini",
        "Vivian Lai",
        "Yan Zheng",
        "Junpeng Wang",
        "Xin Dai",
        "Zhongfang Zhuang",
        "Yujie Fan",
        "Huiyuan Chen",
        "Prince Osei Aboagye",
        "Liang Wang",
        "Wei Zhang",
        "Eamonn Keogh"
      ],
      "abstract": "The Matrix Profile (MP), a versatile tool for time series data mining, has\nbeen shown effective in time series anomaly detection (TSAD). This paper delves\ninto the problem of anomaly detection in multidimensional time series, a common\noccurrence in real-world applications. For instance, in a manufacturing\nfactory, multiple sensors installed across the site collect time-varying data\nfor analysis. The Matrix Profile, named for its role in profiling the matrix\nstoring pairwise distance between subsequences of univariate time series,\nbecomes complex in multidimensional scenarios. If the input univariate time\nseries has n subsequences, the pairwise distance matrix is a n x n matrix. In a\nmultidimensional time series with d dimensions, the pairwise distance\ninformation must be stored in a n x n x d tensor. In this paper, we first\nanalyze different strategies for condensing this tensor into a profile vector.\nWe then investigate the potential of extending the MP to efficiently find\nk-nearest neighbors for anomaly detection. Finally, we benchmark the\nmultidimensional MP against 19 baseline methods on 119 multidimensional TSAD\ndatasets. The experiments covers three learning setups: unsupervised,\nsupervised, and semi-supervised. MP is the only method that consistently\ndelivers high performance across all setups.",
      "tldr_zh": "这篇论文探讨了 Matrix Profile (MP) 在多维时间序列异常检测中的应用，针对现实场景如工厂传感器数据，分析了如何处理多维数据张量（n x n x d）并将其浓缩成向量，以扩展 MP 高效寻找 k-最近邻。研究贡献包括评估不同浓缩策略和 MP 的异常检测潜力，并在 119 个多维时间序列数据集上与 19 个基线方法进行基准测试。结果显示，MP 在无监督、监督和半监督设置中均表现出色，是唯一能够一致提供高性能的方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09298v1",
      "published_date": "2024-09-14 04:22:45 UTC",
      "updated_date": "2024-09-14 04:22:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:00:50.526505"
    },
    {
      "arxiv_id": "2409.10568v3",
      "title": "On the limits of agency in agent-based models",
      "title_zh": "翻译失败",
      "authors": [
        "Ayush Chopra",
        "Shashank Kumar",
        "Nurullah Giray-Kuru",
        "Ramesh Raskar",
        "Arnau Quera-Bofarull"
      ],
      "abstract": "Agent-based modeling (ABM) offers powerful insights into complex systems, but\nits practical utility has been limited by computational constraints and\nsimplistic agent behaviors, especially when simulating large populations.\nRecent advancements in large language models (LLMs) could enhance ABMs with\nadaptive agents, but their integration into large-scale simulations remains\nchallenging. This work introduces a novel methodology that bridges this gap by\nefficiently integrating LLMs into ABMs, enabling the simulation of millions of\nadaptive agents. We present LLM archetypes, a technique that balances\nbehavioral complexity with computational efficiency, allowing for nuanced agent\nbehavior in large-scale simulations. Our analysis explores the crucial\ntrade-off between simulation scale and individual agent expressiveness,\ncomparing different agent architectures ranging from simple heuristic-based\nagents to fully adaptive LLM-powered agents. We demonstrate the real-world\napplicability of our approach through a case study of the COVID-19 pandemic,\nsimulating 8.4 million agents representing New York City and capturing the\nintricate interplay between health behaviors and economic outcomes. Our method\nsignificantly enhances ABM capabilities for predictive and counterfactual\nanalyses, addressing limitations of historical data in policy design. By\nimplementing these advances in an open-source framework, we facilitate the\nadoption of LLM archetypes across diverse ABM applications. Our results show\nthat LLM archetypes can markedly improve the realism and utility of large-scale\nABMs while maintaining computational feasibility, opening new avenues for\nmodeling complex societal challenges and informing data-driven policy\ndecisions.",
      "tldr_zh": "本研究探讨了代理基础建模（ABM）在模拟大规模系统时的限制，包括计算约束和简单代理行为，并提出了一种新方法来高效整合大型语言模型（LLMs）以支持数百万自适应代理的模拟。论文引入了LLM archetypes 技术，通过平衡行为复杂性和计算效率，实现代理在大型模拟中的细致表现，并比较了从启发式代理到全LLM驱动代理的不同架构。实验分析显示，该方法在COVID-19疫情案例中成功模拟了840万代理，代表纽约市，捕捉了健康行为与经济结果的互动，从而显著提升了ABM在预测和反事实分析中的实用性，并通过开源框架促进其在政策决策领域的应用。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10568v3",
      "published_date": "2024-09-14 04:17:24 UTC",
      "updated_date": "2024-11-10 21:31:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:01:02.056951"
    },
    {
      "arxiv_id": "2409.09281v2",
      "title": "Language Models \"Grok\" to Copy",
      "title_zh": "翻译失败",
      "authors": [
        "Ang Lv",
        "Ruobing Xie",
        "Xingwu Sun",
        "Zhanhui Kang",
        "Rui Yan"
      ],
      "abstract": "We examine the pre-training dynamics of language models, focusing on their\nability to copy text from preceding context--a fundamental skill for various\nLLM applications, including in-context learning (ICL) and retrieval-augmented\ngeneration (RAG). We propose a novel perspective that Transformer-based\nlanguage models develop copying abilities similarly to grokking, which refers\nto sudden generalization on test set long after the model fit to the training\nset. Our experiments yield three arguments: (1) The pre-training loss decreases\nrapidly, while the context copying ability of models initially lags and then\nabruptly saturates. (2) The speed of developing copying ability is independent\nof the number of tokens trained, similarly to how grokking speed is unaffected\nby dataset size as long as the data distribution is preserved. (3) Induction\nheads, the attention heads responsible for copying, form from shallow to deep\nlayers during training, mirroring the development of circuits in deeper layers\nduring grokking. We contend that the connection between grokking and context\ncopying can provide valuable insights for more effective language model\ntraining, ultimately improving in-context performance. For example, we\ndemonstrated that techniques that enhance grokking, such as regularization,\neither accelerate or enhance the development of context copying.",
      "tldr_zh": "本研究探讨了Transformer-based语言模型在预训练过程中发展文本复制能力的过程，提出这种能力类似于\"grokking\"现象，即模型在训练集拟合后突然在测试集上泛化。实验观察到三个关键论点：(1)预训练损失快速下降，但复制能力 initially lags 然后 abruptly saturates；(2)复制能力的发展速度独立于训练token数量，与grokking的速度类似；(3)负责复制的induction heads从浅层到深层逐步形成。通过这些发现，论文主张优化grokking技术（如正则化）可提升in-context learning (ICL)和retrieval-augmented generation (RAG)的性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2025 main conference, short paper",
      "pdf_url": "http://arxiv.org/pdf/2409.09281v2",
      "published_date": "2024-09-14 03:11:00 UTC",
      "updated_date": "2025-02-06 03:31:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:01:14.466558"
    },
    {
      "arxiv_id": "2409.09280v1",
      "title": "An empirical evaluation of using ChatGPT to summarize disputes for recommending similar labor and employment cases in Chinese",
      "title_zh": "翻译失败",
      "authors": [
        "Po-Hsien Wu",
        "Chao-Lin Liu",
        "Wei-Jie Li"
      ],
      "abstract": "We present a hybrid mechanism for recommending similar cases of labor and\nemployment litigations. The classifier determines the similarity based on the\nitemized disputes of the two cases, that the courts prepared. We cluster the\ndisputes, compute the cosine similarity between the disputes, and use the\nresults as the features for the classification tasks. Experimental results\nindicate that this hybrid approach outperformed our previous system, which\nconsidered only the information about the clusters of the disputes. We replaced\nthe disputes that were prepared by the courts with the itemized disputes that\nwere generated by GPT-3.5 and GPT-4, and repeated the same experiments. Using\nthe disputes generated by GPT-4 led to better results. Although our classifier\ndid not perform as well when using the disputes that the ChatGPT generated, the\nresults were satisfactory. Hence, we hope that the future large-language models\nwill become practically useful.",
      "tldr_zh": "该研究评估了使用 ChatGPT 总结争议以推荐类似劳动和就业案件的实证方法，提出了一种混合机制，通过分类器基于争议项目的聚类和余弦相似度计算来确定案件相似性。实验结果显示，该混合方法优于之前的仅依赖争议聚类系统，而使用 GPT-4 生成的争议项目比 GPT-3.5 更有效，尽管 ChatGPT 生成的争议导致分类器性能略有下降，但整体结果仍令人满意。该方法为未来大语言模型在法律推荐领域的实用应用提供了积极展望。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 5 figures, 2 tables, the 18th Int'l Workshop on\n  Juris-Informatics (JURISIN 2024), associated with the 16th JSAI International\n  Symposium on AI (JSAI-isAI 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.09280v1",
      "published_date": "2024-09-14 03:08:10 UTC",
      "updated_date": "2024-09-14 03:08:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:01:25.737000"
    },
    {
      "arxiv_id": "2409.09274v1",
      "title": "LabellessFace: Fair Metric Learning for Face Recognition without Attribute Labels",
      "title_zh": "LabellessFace：无需属性标签的公平度量",
      "authors": [
        "Tetsushi Ohki",
        "Yuya Sato",
        "Masakatsu Nishigaki",
        "Koichi Ito"
      ],
      "abstract": "Demographic bias is one of the major challenges for face recognition systems.\nThe majority of existing studies on demographic biases are heavily dependent on\nspecific demographic groups or demographic classifier, making it difficult to\naddress performance for unrecognised groups. This paper introduces\n``LabellessFace'', a novel framework that improves demographic bias in face\nrecognition without requiring demographic group labeling typically required for\nfairness considerations. We propose a novel fairness enhancement metric called\nthe class favoritism level, which assesses the extent of favoritism towards\nspecific classes across the dataset. Leveraging this metric, we introduce the\nfair class margin penalty, an extension of existing margin-based metric\nlearning. This method dynamically adjusts learning parameters based on class\nfavoritism levels, promoting fairness across all attributes. By treating each\nclass as an individual in facial recognition systems, we facilitate learning\nthat minimizes biases in authentication accuracy among individuals.\nComprehensive experiments have demonstrated that our proposed method is\neffective for enhancing fairness while maintaining authentication accuracy.",
      "tldr_zh": "这篇论文介绍了 LabellessFace 框架，一种无需人口属性标签的公平度量学习方法，用于解决面部识别系统中的人口统计学偏差问题。作者提出 class favoritism level 指标来评估数据集对特定类的偏好，并开发了 fair class margin penalty 扩展，这是一种基于 margin-based metric learning 的动态调整机制，以促进所有类别的公平学习。实验结果表明，该方法在提升认证公平性方面表现出色，同时保持了较高的认证准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09274v1",
      "published_date": "2024-09-14 02:56:07 UTC",
      "updated_date": "2024-09-14 02:56:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:01:37.582098"
    },
    {
      "arxiv_id": "2409.09272v1",
      "title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection",
      "title_zh": "SafeEar：内容隐私保护的音频深度伪造检测",
      "authors": [
        "Xinfeng Li",
        "Kai Li",
        "Yifan Zheng",
        "Chen Yan",
        "Xiaoyu Ji",
        "Wenyuan Xu"
      ],
      "abstract": "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited\nremarkable performance in generating realistic and natural audio. However,\ntheir dark side, audio deepfake poses a significant threat to both society and\nindividuals. Existing countermeasures largely focus on determining the\ngenuineness of speech based on complete original audio recordings, which\nhowever often contain private content. This oversight may refrain deepfake\ndetection from many applications, particularly in scenarios involving sensitive\ninformation like business secrets. In this paper, we propose SafeEar, a novel\nframework that aims to detect deepfake audios without relying on accessing the\nspeech content within. Our key idea is to devise a neural audio codec into a\nnovel decoupling model that well separates the semantic and acoustic\ninformation from audio samples, and only use the acoustic information (e.g.,\nprosody and timbre) for deepfake detection. In this way, no semantic content\nwill be exposed to the detector. To overcome the challenge of identifying\ndiverse deepfake audio without semantic clues, we enhance our deepfake detector\nwith real-world codec augmentation. Extensive experiments conducted on four\nbenchmark datasets demonstrate SafeEar's effectiveness in detecting various\ndeepfake techniques with an equal error rate (EER) down to 2.02%.\nSimultaneously, it shields five-language speech content from being deciphered\nby both machine and human auditory analysis, demonstrated by word error rates\n(WERs) all above 93.93% and our user study. Furthermore, our benchmark\nconstructed for anti-deepfake and anti-content recovery evaluation helps\nprovide a basis for future research in the realms of audio privacy preservation\nand deepfake detection.",
      "tldr_zh": "该研究提出SafeEar框架，用于在保护音频内容隐私的前提下检测深度伪造音频，避免传统方法依赖完整音频导致的隐私泄露。SafeEar采用神经音频编解码器分离音频中的语义和声学信息，仅利用声学信息（如韵律和音色）进行检测，并通过真实世界的编解码器增强来应对无语义线索的挑战。实验结果显示，在四个基准数据集上，SafeEar的等错误率(EER)低至2.02%，同时有效保护五种语言的语音内容不被破译，词错误率(WER)均超过93.93%，并构建了一个新基准促进音频隐私保护和深度伪造检测领域的未来研究。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by ACM CCS 2024. Please cite this paper as \"Xinfeng Li, Kai\n  Li, Yifan Zheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu. SafeEar: Content\n  Privacy-Preserving Audio Deepfake Detection. In Proceedings of ACM Conference\n  on Computer and Communications Security (CCS), 2024.\"",
      "pdf_url": "http://arxiv.org/pdf/2409.09272v1",
      "published_date": "2024-09-14 02:45:09 UTC",
      "updated_date": "2024-09-14 02:45:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:01:50.227089"
    },
    {
      "arxiv_id": "2409.13752v1",
      "title": "Thinking Before Speaking: A Role-playing Model with Mindset",
      "title_zh": "翻译失败",
      "authors": [
        "Baohua Zhang",
        "Yongyi Huang",
        "Wenyao Cui",
        "Huaping Zhang"
      ],
      "abstract": "Role-playing is an easy task for Large Language Models (LLMs), as they are\nskilled at simulating human behaviors. Many current studies have enabled LLMs\nto generate responses in the tone of a specific role by fine-tuning the models\nor using specialized prompts. However, it is typically easy to recognize when a\nrole is being played by LLMs. These models tend to perform poorly when\nconfronted with knowledge that the assumed role does not possess, or a question\nthat requires the specific experience or logic of the role to answer. To\naddress this problem and make LLMs act more like real roles, we propose a\nThinking Before Speaking (TBS) model in this paper. Unlike other studies, we\nfirst extend the data based on the character's real-life scenarios and the\nhistorical dialogue, supplementing each pair of dialogue with the character's\nmindset. Then we add few data points that include elements beyond the role's\nknowledge, and fine-tune the LLMs. This approach can help LLMs adopt the role's\nthought process and logic, avoiding responses that fall outside the role's\nknowledge base. We have also prepared a dataset and evaluation metrics to test\nthese capabilities. Experimental results show that our TBS model can better\nemulate a role in terms of tone, knowledge, and mindset.",
      "tldr_zh": "本论文提出了一种名为 Thinking Before Speaking (TBS) 的模型，旨在提升大型语言模型 (LLMs) 在角色扮演中的真实性，解决 LLMs 面对超出角色知识或逻辑的问题时表现不佳的问题。方法包括扩展数据集以融入角色的真实场景、历史对话和 mindset，并添加超出角色知识的数据点对 LLMs 进行微调，帮助模型采用角色的思维过程和逻辑。实验结果显示，TBS 模型在语气、知识和心态方面更有效地模拟真实角色，并通过准备的自定义数据集和评估指标验证了其优越性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13752v1",
      "published_date": "2024-09-14 02:41:48 UTC",
      "updated_date": "2024-09-14 02:41:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:02:13.441901"
    },
    {
      "arxiv_id": "2410.01812v5",
      "title": "From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice",
      "title_zh": "从文本到多模态：探索大型语言模型在医疗实践中的演变和影响",
      "authors": [
        "Qian Niu",
        "Keyu Chen",
        "Ming Li",
        "Pohsun Feng",
        "Ziqian Bi",
        "Lawrence KQ Yan",
        "Yichao Zhang",
        "Caitlyn Heqi Yin",
        "Cheng Fei",
        "Junyu Liu",
        "Benji Peng",
        "Tianyang Wang",
        "Yunze Wang",
        "Silin Chen",
        "Ming Liu"
      ],
      "abstract": "Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.",
      "tldr_zh": "这篇综述论文探讨了大型语言模型（LLMs）从文本-based 系统向多模态大型语言模型（MLLMs）的演变，以及其在医疗实践中的深远影响。论文分析了 MLLMs 在临床决策支持、医疗成像、患者互动和研究领域的应用，强调其整合文本、图像和音频等多样数据类型的能力，以提供更全面的患者健康洞见。同时，论文指出了实施 MLLMs 的挑战，包括数据限制、技术障碍和伦理问题，并识别了关键研究空白，如数据集开发、模态对齐方法和伦理指南的制定，以指导未来研究。最终，论文强调理解 MLLMs 的潜力和局限性，对于其负责任整合到医疗实践中至关重要。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "12 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2410.01812v5",
      "published_date": "2024-09-14 02:35:29 UTC",
      "updated_date": "2024-12-10 03:43:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:02:14.118862"
    },
    {
      "arxiv_id": "2409.15355v5",
      "title": "Block-Attention for Efficient Prefilling",
      "title_zh": "翻译失败",
      "authors": [
        "Dongyang Ma",
        "Yan Wang",
        "Lan Tian"
      ],
      "abstract": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
      "tldr_zh": "该论文提出 Block-attention 机制，以解决 Retrieval-Augmented Generation (RAG) 场景中的推理延迟和计算成本问题。通过将检索文档分成离散块，每个块独立计算 key-value (KV) states（除了最后一个块），并重用先前块的 KV states，该方法显著降低了推理开销。实验在 11 个多样基准（包括 RAG、ICL 和一般领域）上显示，Block-attention 在块微调后，性能与全注意力模型相当，并能无缝切换模式；同时，将 time to first token (TTFT) 和 floating point operations (FLOPs) 分别减少 98.7% 和 99.8%，如对 32K 输入序列的 TTFT 仅需 45 ms。此外，该机制在游戏 AI 场景中具有显著潜在益处。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.15355v5",
      "published_date": "2024-09-14 02:34:26 UTC",
      "updated_date": "2025-04-13 14:02:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:02:26.631011"
    },
    {
      "arxiv_id": "2409.09269v3",
      "title": "Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types",
      "title_zh": "翻译失败",
      "authors": [
        "Neelabh Sinha",
        "Vinija Jain",
        "Aman Chadha"
      ],
      "abstract": "Visual Question-Answering (VQA) has become key to user experience,\nparticularly after improved generalization capabilities of Vision-Language\nModels (VLMs). But evaluating VLMs for an application requirement using a\nstandardized framework in practical settings is still challenging. This paper\naims to solve that using an end-to-end framework. We present VQA360 - a novel\ndataset derived from established VQA benchmarks, annotated with task types,\napplication domains, and knowledge types, for a comprehensive evaluation. We\nalso introduce GoEval, a multimodal evaluation metric developed using GPT-4o,\nachieving a correlation factor of 56.71% with human judgments. Our experiments\nwith state-of-the-art VLMs reveal that no single model excels universally,\nthus, making a right choice a key design decision. Proprietary models such as\nGemini-1.5-Pro and GPT-4o-mini generally outperform others, but open-source\nmodels like InternVL-2-8B and CogVLM-2-Llama-3-19B also demonstrate competitive\nstrengths, while providing additional advantages. Our framework can also be\nextended to other tasks.",
      "tldr_zh": "本文提出一个端到端框架，用于指导 Visual Question-Answering (VQA) 中 Vision-Language Models (VLMs) 的选择，涵盖任务类型、应用领域和知识类型。框架包括 VQA360 数据集，该数据集基于现有基准并标注相关属性，以实现全面评估；同时引入 GoEval 评估指标，使用 GPT-4o 开发，与人类判断的相关性达到 56.71%。实验结果显示，没有单一 VLMs 模型在所有场景中表现出色，专有模型如 Gemini-1.5-Pro 和 GPT-4o-mini 通常领先，而开源模型如 InternVL-2-8B 和 CogVLM-2-Llama-3-19B 也具有竞争力。该框架可扩展到其他任务，提供实用指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at The First Workshop of Evaluation of Multi-Modal\n  Generation (EvalMG) in 31st International Conference on Computational\n  Linguistics (COLING), 2025. 8 pages + references + 6 pages of Appendix",
      "pdf_url": "http://arxiv.org/pdf/2409.09269v3",
      "published_date": "2024-09-14 02:29:36 UTC",
      "updated_date": "2024-12-12 06:26:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:02:49.324712"
    },
    {
      "arxiv_id": "2410.01811v1",
      "title": "Evaluating Cultural Awareness of LLMs for Yoruba, Malayalam, and English",
      "title_zh": "翻译失败",
      "authors": [
        "Fiifi Dawson",
        "Zainab Mosunmola",
        "Sahil Pocker",
        "Raj Abhijit Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "abstract": "Although LLMs have been extremely effective in a large number of complex\ntasks, their understanding and functionality for regional languages and\ncultures are not well studied. In this paper, we explore the ability of various\nLLMs to comprehend the cultural aspects of two regional languages: Malayalam\n(state of Kerala, India) and Yoruba (West Africa). Using Hofstede's six\ncultural dimensions: Power Distance (PDI), Individualism (IDV), Motivation\ntowards Achievement and Success (MAS), Uncertainty Avoidance (UAV), Long Term\nOrientation (LTO), and Indulgence (IVR), we quantify the cultural awareness of\nLLM-based responses. We demonstrate that although LLMs show a high cultural\nsimilarity for English, they fail to capture the cultural nuances across these\n6 metrics for Malayalam and Yoruba. We also highlight the need for large-scale\nregional language LLM training with culturally enriched datasets. This will\nhave huge implications for enhancing the user experience of chat-based LLMs and\nalso improving the validity of large-scale LLM agent-based market research.",
      "tldr_zh": "本论文评估了大型语言模型（LLMs）在Yoruba、Malayalam和English三种语言上的文化意识，使用Hofstede's six cultural dimensions（包括Power Distance (PDI)、Individualism (IDV)、Motivation towards Achievement and Success (MAS)、Uncertainty Avoidance (UAV)、Long Term Orientation (LTO)和Indulgence (IVR)）进行量化分析。结果表明，LLMs在English上显示出较高的文化相似性，但无法有效捕捉Malayalam和Yoruba的文化细微差异。论文强调，需要通过大规模的区域语言LLM训练和文化丰富的数据集来提升聊天式LLMs的用户体验，并改善LLM代理在市场研究中的有效性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "19 pages, 10 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.01811v1",
      "published_date": "2024-09-14 02:21:17 UTC",
      "updated_date": "2024-09-14 02:21:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:02:50.658126"
    },
    {
      "arxiv_id": "2409.09263v3",
      "title": "Operational Wind Speed Forecasts for Chile's Electric Power Sector Using a Hybrid ML Model",
      "title_zh": "翻译失败",
      "authors": [
        "Dhruv Suri",
        "Praneet Dutta",
        "Flora Xue",
        "Ines Azevedo",
        "Ravi Jain"
      ],
      "abstract": "As Chile's electric power sector advances toward a future powered by\nrenewable energy, accurate forecasting of renewable generation is essential for\nmanaging grid operations. The integration of renewable energy sources is\nparticularly challenging due to the operational difficulties of managing their\npower generation, which is highly variable compared to fossil fuel sources,\ndelaying the availability of clean energy. To mitigate this, we quantify the\nimpact of increasing intermittent generation from wind and solar on thermal\npower plants in Chile and introduce a hybrid wind speed forecasting methodology\nwhich combines two custom ML models for Chile. The first model is based on\nTiDE, an MLP-based ML model for short-term forecasts, and the second is based\non a graph neural network, GraphCast, for medium-term forecasts up to 10 days.\nOur hybrid approach outperforms the most accurate operational deterministic\nsystems by 4-21% for short-term forecasts and 5-23% for medium-term forecasts\nand can directly lower the impact of wind generation on thermal ramping,\ncurtailment, and system-level emissions in Chile.",
      "tldr_zh": "智利电力部门在转向可再生能源时面临电网管理挑战，本文量化了风能和太阳能增加对热电厂的影响，并提出了一种混合 ML 模型用于风速预测。模型结合基于 TiDE 的 MLP 模型进行短期预测，以及基于 GraphCast 的图神经网络进行长达 10 天的中期预测。该方法在短期预测中比现有系统提高了 4-21% 的准确率，在中期预测中提高了 5-23%，有助于降低风能生成对热电厂启动/停止、弃风和系统排放的负面影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09263v3",
      "published_date": "2024-09-14 02:16:02 UTC",
      "updated_date": "2024-09-18 15:17:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:03:05.511221"
    },
    {
      "arxiv_id": "2409.09261v1",
      "title": "What Is Wrong with My Model? Identifying Systematic Problems with Semantic Data Slicing",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyang Yang",
        "Yining Hong",
        "Grace A. Lewis",
        "Tongshuang Wu",
        "Christian Kästner"
      ],
      "abstract": "Machine learning models make mistakes, yet sometimes it is difficult to\nidentify the systematic problems behind the mistakes. Practitioners engage in\nvarious activities, including error analysis, testing, auditing, and\nred-teaming, to form hypotheses of what can go (or has gone) wrong with their\nmodels. To validate these hypotheses, practitioners employ data slicing to\nidentify relevant examples. However, traditional data slicing is limited by\navailable features and programmatic slicing functions. In this work, we propose\nSemSlicer, a framework that supports semantic data slicing, which identifies a\nsemantically coherent slice, without the need for existing features. SemSlicer\nuses Large Language Models to annotate datasets and generate slices from any\nuser-defined slicing criteria. We show that SemSlicer generates accurate slices\nwith low cost, allows flexible trade-offs between different design dimensions,\nreliably identifies under-performing data slices, and helps practitioners\nidentify useful data slices that reflect systematic problems.",
      "tldr_zh": "该论文探讨了机器学习模型错误背后的系统性问题，并提出 SemSlicer 框架来解决传统数据切片的局限性。SemSlicer 利用 Large Language Models 注解数据集，并根据用户定义的切片标准生成语义数据 slicing（semantic data slicing），无需依赖现有特征。实验结果显示，该框架能以低成本生成准确的切片，支持灵活的设计权衡，并帮助实践者可靠地识别性能不佳的数据切片，从而揭示模型的系统性问题。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09261v1",
      "published_date": "2024-09-14 02:15:50 UTC",
      "updated_date": "2024-09-14 02:15:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:03:13.829901"
    },
    {
      "arxiv_id": "2409.09253v1",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Yin",
        "Zhengxin Zeng",
        "Mingzheng Li",
        "Hao Yan",
        "Chaozhuo Li",
        "Weihao Han",
        "Jianjin Zhang",
        "Ruochen Liu",
        "Allen Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Shirui Pan",
        "Senzhang Wang"
      ],
      "abstract": "Owing to the unprecedented capability in semantic understanding and logical\nreasoning, the pre-trained large language models (LLMs) have shown fantastic\npotential in developing the next-generation recommender systems (RSs). However,\nthe static index paradigm adopted by current methods greatly restricts the\nutilization of LLMs capacity for recommendation, leading to not only the\ninsufficient alignment between semantic and collaborative knowledge, but also\nthe neglect of high-order user-item interaction patterns. In this paper, we\npropose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS\nwhich adopts dynamic semantic index paradigm, targeting at resolving the above\nproblems simultaneously. To be more specific, we for the first time contrive a\ndynamic knowledge fusion framework which integrates a twin-tower semantic token\ngenerator into the LLM-based recommender, hierarchically allocating meaningful\nsemantic index for items and users, and accordingly predicting the semantic\nindex of target item. Furthermore, a dual-modality variational auto-encoder is\nproposed to facilitate multi-grained alignment between semantic and\ncollaborative knowledge. Eventually, a series of novel tuning tasks specially\ncustomized for capturing high-order user-item interaction patterns are proposed\nto take advantages of user historical behavior. Extensive experiments across\nthree public datasets demonstrate the superiority of the proposed methodology\nin developing LLM-based generative RSs. The proposed TTDS recommender achieves\nan average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric,\ncompared with the leading baseline methods.",
      "tldr_zh": "本文提出 Twin-Tower Dynamic Semantic Recommender (TTDS)，一种新型生成式推荐系统，旨在释放大型语言模型 (LLMs) 在推荐领域的潜力，通过动态语义索引范式解决语义和协作知识对齐不足以及高阶用户-物品交互模式被忽略的问题。具体而言，TTDS 整合了双塔语义标记生成器、动态知识融合框架和双模态变分自编码器 (dual-modality variational auto-encoder)，并设计了定制化调优任务来利用用户历史行为。实验在三个公共数据集上表明，TTDS 相较于领先基线方法，在 Hit-Rate 和 NDCG 指标上平均提升了 19.41% 和 20.84%。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09253v1",
      "published_date": "2024-09-14 01:45:04 UTC",
      "updated_date": "2024-09-14 01:45:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:03:27.817845"
    },
    {
      "arxiv_id": "2409.09251v1",
      "title": "ETAGE: Enhanced Test Time Adaptation with Integrated Entropy and Gradient Norms for Robust Model Performance",
      "title_zh": "翻译失败",
      "authors": [
        "Afshar Shamsi",
        "Rejisa Becirovic",
        "Ahmadreza Argha",
        "Ehsan Abbasnejad",
        "Hamid Alinejad-Rokny",
        "Arash Mohammadi"
      ],
      "abstract": "Test time adaptation (TTA) equips deep learning models to handle unseen test\ndata that deviates from the training distribution, even when source data is\ninaccessible. While traditional TTA methods often rely on entropy as a\nconfidence metric, its effectiveness can be limited, particularly in biased\nscenarios. Extending existing approaches like the Pseudo Label Probability\nDifference (PLPD), we introduce ETAGE, a refined TTA method that integrates\nentropy minimization with gradient norms and PLPD, to enhance sample selection\nand adaptation. Our method prioritizes samples that are less likely to cause\ninstability by combining high entropy with high gradient norms out of\nadaptation, thus avoiding the overfitting to noise often observed in previous\nmethods. Extensive experiments on CIFAR-10-C and CIFAR-100-C datasets\ndemonstrate that our approach outperforms existing TTA techniques, particularly\nin challenging and biased scenarios, leading to more robust and consistent\nmodel performance across diverse test scenarios. The codebase for ETAGE is\navailable on https://github.com/afsharshamsi/ETAGE.",
      "tldr_zh": "这篇论文提出了 ETAGE，一种增强的 Test Time Adaptation (TTA) 方法，通过整合 Entropy 最小化、Gradient Norms 和 Pseudo Label Probability Difference (PLPD)，来优化样本选择并提升模型对分布偏移的适应性，从而减少不稳定性和避免过拟合噪音。ETAGE 优先处理高 Entropy 和高 Gradient Norms 的样本，确保在偏置场景中更可靠的表现。在 CIFAR-10-C 和 CIFAR-100-C 数据集上的广泛实验显示，该方法优于现有 TTA 技术，提供更稳健且一致的模型性能，并开源代码以供进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09251v1",
      "published_date": "2024-09-14 01:25:52 UTC",
      "updated_date": "2024-09-14 01:25:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:03:38.449781"
    },
    {
      "arxiv_id": "2409.11430v3",
      "title": "Federated Learning with Quantum Computing and Fully Homomorphic Encryption: A Novel Computing Paradigm Shift in Privacy-Preserving ML",
      "title_zh": "翻译失败",
      "authors": [
        "Siddhant Dutta",
        "Pavana P Karanth",
        "Pedro Maciel Xavier",
        "Iago Leal de Freitas",
        "Nouhaila Innan",
        "Sadok Ben Yahia",
        "Muhammad Shafique",
        "David E. Bernal Neira"
      ],
      "abstract": "The widespread deployment of products powered by machine learning models is\nraising concerns around data privacy and information security worldwide. To\naddress this issue, Federated Learning was first proposed as a\nprivacy-preserving alternative to conventional methods that allow multiple\nlearning clients to share model knowledge without disclosing private data. A\ncomplementary approach known as Fully Homomorphic Encryption (FHE) is a\nquantum-safe cryptographic system that enables operations to be performed on\nencrypted weights. However, implementing mechanisms such as these in practice\noften comes with significant computational overhead and can expose potential\nsecurity threats. Novel computing paradigms, such as analog, quantum, and\nspecialized digital hardware, present opportunities for implementing\nprivacy-preserving machine learning systems while enhancing security and\nmitigating performance loss. This work instantiates these ideas by applying the\nFHE scheme to a Federated Learning Neural Network architecture that integrates\nboth classical and quantum layers.",
      "tldr_zh": "这篇论文探讨了 Federated Learning (FL) 和 Fully Homomorphic Encryption (FHE) 在隐私保护机器学习中的应用，以应对数据隐私和信息安全挑战。作者提出了一种新颖的计算范式，将 FHE 整合到 FL 神经网络架构中，结合经典层和量子层进行操作，从而在加密权重上实现安全计算。实验设计旨在利用量子计算等新兴技术，减轻计算开销并提升整体安全性，为隐私保护机器学习提供了一个高效的范式转变。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "quant-ph",
      "comment": "10 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11430v3",
      "published_date": "2024-09-14 01:23:26 UTC",
      "updated_date": "2024-10-12 10:51:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:03:50.287850"
    },
    {
      "arxiv_id": "2409.09245v1",
      "title": "Robust Training of Neural Networks at Arbitrary Precision and Sparsity",
      "title_zh": "任意精度和稀疏度下的神经网络鲁棒训练",
      "authors": [
        "Chengxi Ye",
        "Grace Chu",
        "Yanfeng Liu",
        "Yichi Zhang",
        "Lukasz Lew",
        "Andrew Howard"
      ],
      "abstract": "The discontinuous operations inherent in quantization and sparsification\nintroduce obstacles to backpropagation. This is particularly challenging when\ntraining deep neural networks in ultra-low precision and sparse regimes. We\npropose a novel, robust, and universal solution: a denoising affine transform\nthat stabilizes training under these challenging conditions. By formulating\nquantization and sparsification as perturbations during training, we derive a\nperturbation-resilient approach based on ridge regression. Our solution employs\na piecewise constant backbone model to ensure a performance lower bound and\nfeatures an inherent noise reduction mechanism to mitigate perturbation-induced\ncorruption. This formulation allows existing models to be trained at\narbitrarily low precision and sparsity levels with off-the-shelf recipes.\nFurthermore, our method provides a novel perspective on training temporal\nbinary neural networks, contributing to ongoing efforts to narrow the gap\nbetween artificial and biological neural networks.",
      "tldr_zh": "该论文解决了quantization和sparsification在神经网络训练中引入的反向传播障碍，特别是在低精度和稀疏场景下，提出了一种基于denoising affine transform的鲁棒训练方法。该方法将这些操作视为扰动，并通过ridge regression和piecewise constant backbone model来确保性能下限并减少噪声干扰，从而允许现有模型在任意低精度和稀疏水平上使用现成训练配方。最终，这种方法为时序二进制神经网络的训练提供了新视角，有助于缩小人工和生物神经网络之间的差距。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09245v1",
      "published_date": "2024-09-14 00:57:32 UTC",
      "updated_date": "2024-09-14 00:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:04:13.227757"
    },
    {
      "arxiv_id": "2409.09240v1",
      "title": "Cross-Entropy Optimization for Hyperparameter Optimization in Stochastic Gradient-based Approaches to Train Deep Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Li",
        "Fulu Li"
      ],
      "abstract": "In this paper, we present a cross-entropy optimization method for\nhyperparameter optimization in stochastic gradient-based approaches to train\ndeep neural networks. The value of a hyperparameter of a learning algorithm\noften has great impact on the performance of a model such as the convergence\nspeed, the generalization performance metrics, etc. While in some cases the\nhyperparameters of a learning algorithm can be part of learning parameters, in\nother scenarios the hyperparameters of a stochastic optimization algorithm such\nas Adam [5] and its variants are either fixed as a constant or are kept\nchanging in a monotonic way over time. We give an in-depth analysis of the\npresented method in the framework of expectation maximization (EM). The\npresented algorithm of cross-entropy optimization for hyperparameter\noptimization of a learning algorithm (CEHPO) can be equally applicable to other\nareas of optimization problems in deep learning. We hope that the presented\nmethods can provide different perspectives and offer some insights for\noptimization problems in different areas of machine learning and beyond.",
      "tldr_zh": "本论文提出了一种交叉熵优化(Cross-Entropy Optimization)方法，用于优化基于随机梯度(Stochastic Gradient-based Approaches)的深度神经网络(Deep Neural Networks)训练中的超参数(Hyperparameter Optimization)，以提升模型的收敛速度和泛化性能。作者通过期望最大化(Expectation Maximization, EM)框架对该方法进行深入分析，并开发了 CEHPO 算法，适用于如 Adam 优化器等场景。实验结果表明，该方法不仅改善了超参数调整的效果，还可扩展到深度学习的其他优化问题，提供新的视角和见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.09240v1",
      "published_date": "2024-09-14 00:39:37 UTC",
      "updated_date": "2024-09-14 00:39:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:04:15.098969"
    },
    {
      "arxiv_id": "2409.09239v3",
      "title": "Autoregressive + Chain of Thought = Recurrent: Recurrence's Role in Language Models' Computability and a Revisit of Recurrent Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Zhang",
        "Muhammad Abdul-Mageed",
        "Laks V. S. Lakshmanan"
      ],
      "abstract": "The Transformer architecture excels in a variety of language modeling tasks,\noutperforming traditional neural architectures such as RNN and LSTM. This is\npartially due to its elimination of recurrent connections, which allows for\nparallel training and a smoother flow of gradients. However, this move away\nfrom recurrent structures places the Transformer model at the lower end of\nChomsky's computational hierarchy, imposing limitations on its computational\nabilities. Consequently, even advanced Transformer-based models face\nconsiderable difficulties in tasks like counting, string reversal, and\nmultiplication. These tasks, though seemingly elementary, require a level of\ncomputational complexity that exceeds the capabilities of the Transformer\narchitecture. Concurrently, the emergence of ``Chain of Thought\" (CoT)\nprompting has enabled Transformer-based language models to tackle tasks that\nwere previously impossible or poorly executed. In this work, we thoroughly\ninvestigate the influence of recurrent structures in neural models on their\nreasoning abilities and computability, contrasting the role autoregression\nplays in the neural models' computational power. We then shed light on how the\nCoT approach can mimic recurrent computation and act as a bridge between\nautoregression and recurrence in the context of language models. It is this\napproximated recurrence that notably improves the model's performance and\ncomputational capacity. Moreover, we revisit recent recurrent-based Transformer\nmodel designs, focusing on their computational abilities through our proposed\nconcept of ``recurrence-completeness\" and identify key theoretical limitations\nin models like Linear Transformer and RWKV. Through this, we aim to provide\ninsight into the neural model architectures and prompt better model design.",
      "tldr_zh": "本文研究了Transformer模型在语言建模中的优势（如消除循环连接以支持并行训练），但其非循环结构导致在Chomsky's hierarchy中计算能力较低，难以处理计数、字符串反转和乘法等任务。论文分析了Chain of Thought (CoT)提示如何模拟循环计算（Recurrent），从而桥接Autoregressive自回归机制与循环结构，提升语言模型的推理和计算能力。通过引入“recurrence-completeness”概念，作者重新审视了Linear Transformer和RWKV等循环Transformer模型，识别其关键理论限制，并为改进神经模型架构提供洞见。实验结果表明，CoT显著提高了模型在复杂任务上的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09239v3",
      "published_date": "2024-09-14 00:30:57 UTC",
      "updated_date": "2024-09-20 21:12:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:04:27.949043"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 68,
  "processed_papers_count": 68,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T00:04:46.523766"
}
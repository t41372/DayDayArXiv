{
  "date": "2024-09-08",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-08 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 31 篇论文，主要聚焦 AI 模型优化、生成技术及应用，强调联邦学习、多模态处理和 LLM 创新，亮点包括 ICML 拓扑深度学习挑战（第 3 篇）的基准研究，以及知名学者 Kamalika Chaudhuri 参与的攻击检测论文（第 4 篇），这些工作展示了 AI 在实际场景中的鲁棒性和潜力。\n\n下面，我将挑选重点论文逐一简要讨论，先优先聊具有话题度、创新性和知名作者的文章，再快速概述相关或次要内容。所有讨论均聚焦论文核心贡献、方法和发现。\n\n### 1. 重点论文讨论\n- **ICML 拓扑深度学习挑战 2024：超越图域 (ICML Topological Deep Learning Challenge 2024: Beyond the Graph Domain)**  \n  这篇论文由众多作者（包括 Mustafa Hajij 和 Theodore Papamarkou）合作，源于 ICML 2024 研讨会，令人印象深刻地扩展了拓扑深度学习（TDL）。主要贡献是通过设计拓扑映射（如超图和单纯复形），桥接不同数据结构（如点云），并分析 52 份参赛方案。发现显示，TDL 在处理非图结构数据时表现出色，为泛化 TDL 应用提供了新基准。\n\n- **影响函数的归因可以被操纵 (Influence-based Attributions can be Manipulated)**  \n  作者包括知名学者 Kamalika Chaudhuri，这篇工作揭示了 AI 安全隐患。主要贡献是证明影响函数（用于数据归因）易受攻击，通过高效后向实现攻击 logistic 回归模型。发现显示，这种操纵可能导致数据估值和公平性问题，强调了在对抗环境中提升归因算法的可靠性。\n\n- **Mixup 增强及其超越的调查 (A Survey on Mixup Augmentations and Beyond)**  \n  作者团队包括 Stan Z. Li，这是一篇全面综述，聚焦数据增强技术。主要方法是统一框架分析 Mixup 在视觉任务和多模态上的应用。关键发现是 Mixup 通过生成虚拟数据提升模型泛化性，适用于各种领域，并提供了未来研究的指导（代码见 GitHub）。\n\n- **OneGen: 高效的一次性统一生成和检索框架 (OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs)**  \n  这篇 EMNLP 2024 Findings 论文创新地将生成和检索整合。核心方法是使用自回归检索令牌，使 LLM 在单次前向传播中处理生成和向量检索任务。发现显示，在 RAG 和实体链接任务上，OneGen 显著提升效率，同时保持生成能力。\n\n- **STLLM-DF: 空间-时间大语言模型与扩散用于多模态交通预测 (STLLM-DF: A Spatial-Temporal Large Language Model with Diffusion for Enhanced Multi-Mode Traffic System Forecasting)**  \n  作者团队包括 Junbin Gao，这篇工作结合扩散模型（DDPM）和 LLM 解决交通预测挑战。主要贡献是使用非预训练 LLM 捕捉空间-时间关系，并通过去噪增强鲁棒性。实验结果显示，在多任务预测中，STLLM-DF 平均降低 MAE 2.40%、RMSE 4.50%，为智能交通系统提供了更准确的框架。\n\n- **RAGent: 基于检索的访问控制策略生成 (RAGent: Retrieval-based Access Control Policy Generation)**  \n  这篇论文提出了一种语言模型框架，用于从高层需求自动生成访问控制策略。主要方法是检索增强生成，并引入验证-细化机制。发现显示，RAGent 在识别访问要求时 F1 分数达 87.9%，生成策略准确性达 80.6%，显著减少手动错误。\n\n- **Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers? (Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?)**  \n  来自 EMNLP 2024，这篇工作质疑 LLM 的多跳推理能力。核心贡献是生成误导性推理链测试 LLM。发现显示，LLM 在面对复杂 distractors 时 F1 分数下降达 45%，揭示了其对细微误导的易感性。\n\n- **FedFT: 使用频率空间变换提升联邦学习的通信性能 (FedFT: Improving Communication Performance for Federated Learning with Frequency Space Transformation)**  \n  这篇联邦学习论文使用 Discrete Cosine Transform (DCT) 压缩模型参数。主要贡献是减少通信开销 5%-30%，兼容多种架构。发现显示，在 FedAvg 等基准上，FedFT 维持准确性甚至提升隐私和效率。\n\n### 2. 相关论文快速概述\n其他论文多为应用性研究，我将相关主题合并简述：\n- **生成和推荐系统：** 如 \"扩散模型在推荐系统的调查 (A Survey on Diffusion Models for Recommender Systems)\"，综述了扩散模型在数据增强和推荐中的作用；\"序列推荐 via 自适应鲁棒注意力 (Sequential Recommendation via Adaptive Robust Attention with Multi-dimensional Embeddings)\" 引入混注意力机制，提升推荐准确性。这些工作强化了扩散模型在捕捉复杂分布上的优势。\n- **AI 应用和优化：** \"PairCoder: 代码生成的配对编程框架 (A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement)\" 使用 LLM 代理探索多计划生成，显著提升代码准确性；\"ELMS: 弹性化大语言模型在移动设备上 (ELMS: Elasticized Large Language Models On Mobile Devices)\" 提出参数弹性模型，减少移动端计算开销。这些论文展示了 LLM 在代码和边缘计算的潜力。\n- **其他领域：** 如交通和音频的论文，例如 \"EdaCSC: 针对中文拼写纠错的两个简单数据增强方法 (EdaCSC: Two Easy Data Augmentation Methods for Chinese Spelling Correction)\"，通过数据增强提升 CSC 性能；音频相关如 \"Audio-Guided Fusion Techniques for Multimodal Emotion Analysis\"，使用音频引导融合改善情感分析。这些次要论文虽有贡献，但影响力较小，仅在特定应用中优化了模型。\n\n总之，今天的论文突显 AI 领域的创新与挑战，建议关注 TDL 和 LLM 安全方向。若有特定兴趣，可查看 GitHub 链接深入探索。明天见！",
  "papers": [
    {
      "arxiv_id": "2409.05242v1",
      "title": "FedFT: Improving Communication Performance for Federated Learning with Frequency Space Transformation",
      "title_zh": "FedFT：通过频率空间变换改善联邦学习的通信性能",
      "authors": [
        "Chamath Palihawadana",
        "Nirmalie Wiratunga",
        "Anjana Wijekoon",
        "Harsha Kalutarage"
      ],
      "abstract": "Communication efficiency is a widely recognised research problem in Federated\nLearning (FL), with recent work focused on developing techniques for efficient\ncompression, distribution and aggregation of model parameters between clients\nand the server. Particularly within distributed systems, it is important to\nbalance the need for computational cost and communication efficiency. However,\nexisting methods are often constrained to specific applications and are less\ngeneralisable. In this paper, we introduce FedFT (federated frequency-space\ntransformation), a simple yet effective methodology for communicating model\nparameters in a FL setting. FedFT uses Discrete Cosine Transform (DCT) to\nrepresent model parameters in frequency space, enabling efficient compression\nand reducing communication overhead. FedFT is compatible with various existing\nFL methodologies and neural architectures, and its linear property eliminates\nthe need for multiple transformations during federated aggregation. This\nmethodology is vital for distributed solutions, tackling essential challenges\nlike data privacy, interoperability, and energy efficiency inherent to these\nenvironments. We demonstrate the generalisability of the FedFT methodology on\nfour datasets using comparative studies with three state-of-the-art FL\nbaselines (FedAvg, FedProx, FedSim). Our results demonstrate that using FedFT\nto represent the differences in model parameters between communication rounds\nin frequency space results in a more compact representation compared to\nrepresenting the entire model in frequency space. This leads to a reduction in\ncommunication overhead, while keeping accuracy levels comparable and in some\ncases even improving it. Our results suggest that this reduction can range from\n5% to 30% per client, depending on dataset.",
      "tldr_zh": "该论文提出FedFT方法，通过使用Discrete Cosine Transform (DCT)将模型参数转换为频率空间表示，从而提升Federated Learning (FL)中的通信效率。该方法实现了高效的模型参数压缩和聚合，兼容多种现有FL框架（如FedAvg、FedProx和FedSim），并减少了通信开销，同时保持或提高模型准确性。在四个数据集上的实验显示，FedFT在表示模型参数差异时可将通信开销降低5%至30%，并解决了分布式系统中数据隐私、互操作性和能源效率等关键挑战。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.05242v1",
      "published_date": "2024-09-08 23:05:35 UTC",
      "updated_date": "2024-09-08 23:05:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:12:13.474888"
    },
    {
      "arxiv_id": "2409.05215v1",
      "title": "Synthetic Tabular Data Generation for Class Imbalance and Fairness: A Comparative Study",
      "title_zh": "用于类别不平衡和公平性的合成",
      "authors": [
        "Emmanouil Panagiotou",
        "Arjun Roy",
        "Eirini Ntoutsi"
      ],
      "abstract": "Due to their data-driven nature, Machine Learning (ML) models are susceptible\nto bias inherited from data, especially in classification problems where class\nand group imbalances are prevalent. Class imbalance (in the classification\ntarget) and group imbalance (in protected attributes like sex or race) can\nundermine both ML utility and fairness. Although class and group imbalances\ncommonly coincide in real-world tabular datasets, limited methods address this\nscenario. While most methods use oversampling techniques, like interpolation,\nto mitigate imbalances, recent advancements in synthetic tabular data\ngeneration offer promise but have not been adequately explored for this\npurpose. To this end, this paper conducts a comparative analysis to address\nclass and group imbalances using state-of-the-art models for synthetic tabular\ndata generation and various sampling strategies. Experimental results on four\ndatasets, demonstrate the effectiveness of generative models for bias\nmitigation, creating opportunities for further exploration in this direction.",
      "tldr_zh": "这篇论文探讨了机器学习(ML)模型在分类问题中因类别不平衡(class imbalance)和群体不平衡(group imbalance)而继承的数据偏见问题，这些不平衡会影响模型的效用和公平性。作者通过比较最先进的合成表格数据生成(synthetic tabular data generation)模型和各种采样策略，在四个真实数据集上进行实验，评估这些方法在缓解偏见方面的效果。结果表明，生成模型表现出色，能够有效减轻不平衡问题，并为进一步研究这一领域提供了机会。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the ECML PKDD 2024, 4th Workshop on Bias and Fairness in\n  AI",
      "pdf_url": "http://arxiv.org/pdf/2409.05215v1",
      "published_date": "2024-09-08 20:08:09 UTC",
      "updated_date": "2024-09-08 20:08:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:12:26.079108"
    },
    {
      "arxiv_id": "2409.05211v1",
      "title": "ICML Topological Deep Learning Challenge 2024: Beyond the Graph Domain",
      "title_zh": "ICML 2024 拓扑深度学习挑战：超越图域",
      "authors": [
        "Guillermo Bernárdez",
        "Lev Telyatnikov",
        "Marco Montagna",
        "Federica Baccini",
        "Mathilde Papillon",
        "Miquel Ferriol-Galmés",
        "Mustafa Hajij",
        "Theodore Papamarkou",
        "Maria Sofia Bucarelli",
        "Olga Zaghen",
        "Johan Mathe",
        "Audun Myers",
        "Scott Mahan",
        "Hansen Lillemark",
        "Sharvaree Vadgama",
        "Erik Bekkers",
        "Tim Doster",
        "Tegan Emerson",
        "Henry Kvinge",
        "Katrina Agate",
        "Nesreen K Ahmed",
        "Pengfei Bai",
        "Michael Banf",
        "Claudio Battiloro",
        "Maxim Beketov",
        "Paul Bogdan",
        "Martin Carrasco",
        "Andrea Cavallo",
        "Yun Young Choi",
        "George Dasoulas",
        "Matouš Elphick",
        "Giordan Escalona",
        "Dominik Filipiak",
        "Halley Fritze",
        "Thomas Gebhart",
        "Manel Gil-Sorribes",
        "Salvish Goomanee",
        "Victor Guallar",
        "Liliya Imasheva",
        "Andrei Irimia",
        "Hongwei Jin",
        "Graham Johnson",
        "Nikos Kanakaris",
        "Boshko Koloski",
        "Veljko Kovač",
        "Manuel Lecha",
        "Minho Lee",
        "Pierrick Leroy",
        "Theodore Long",
        "German Magai",
        "Alvaro Martinez",
        "Marissa Masden",
        "Sebastian Mežnar",
        "Bertran Miquel-Oliver",
        "Alexis Molina",
        "Alexander Nikitin",
        "Marco Nurisso",
        "Matt Piekenbrock",
        "Yu Qin",
        "Patryk Rygiel",
        "Alessandro Salatiello",
        "Max Schattauer",
        "Pavel Snopov",
        "Julian Suk",
        "Valentina Sánchez",
        "Mauricio Tec",
        "Francesco Vaccarino",
        "Jonas Verhellen",
        "Frederic Wantiez",
        "Alexander Weers",
        "Patrik Zajec",
        "Blaž Škrlj",
        "Nina Miolane"
      ],
      "abstract": "This paper describes the 2nd edition of the ICML Topological Deep Learning\nChallenge that was hosted within the ICML 2024 ELLIS Workshop on\nGeometry-grounded Representation Learning and Generative Modeling (GRaM). The\nchallenge focused on the problem of representing data in different discrete\ntopological domains in order to bridge the gap between Topological Deep\nLearning (TDL) and other types of structured datasets (e.g. point clouds,\ngraphs). Specifically, participants were asked to design and implement\ntopological liftings, i.e. mappings between different data structures and\ntopological domains --like hypergraphs, or simplicial/cell/combinatorial\ncomplexes. The challenge received 52 submissions satisfying all the\nrequirements. This paper introduces the main scope of the challenge, and\nsummarizes the main results and findings.",
      "tldr_zh": "这篇论文介绍了 ICML 2024 拓扑深度学习挑战赛（ICML Topological Deep Learning Challenge 2024），焦点是超越图域，在不同离散拓扑域中表示数据，以桥接 Topological Deep Learning (TDL) 与其他结构化数据集（如点云或图）的差距。参与者被要求设计和实现 topological liftings，即在数据结构和拓扑域（如 hypergraphs、simplicial/cell/combinatorial complexes）之间的映射。挑战赛共收到 52 份符合要求的提交，并总结了主要结果和发现，为拓扑深度学习的应用扩展提供了宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of the Geometry-grounded Representation Learning and\n  Generative Modeling Workshop (GRaM) at ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.05211v1",
      "published_date": "2024-09-08 19:59:53 UTC",
      "updated_date": "2024-09-08 19:59:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:12:38.255891"
    },
    {
      "arxiv_id": "2409.05208v4",
      "title": "Influence-based Attributions can be Manipulated",
      "title_zh": "基于影响的归因可以被操纵",
      "authors": [
        "Chhavi Yadav",
        "Ruihan Wu",
        "Kamalika Chaudhuri"
      ],
      "abstract": "Influence Functions are a standard tool for attributing predictions to\ntraining data in a principled manner and are widely used in applications such\nas data valuation and fairness. In this work, we present realistic incentives\nto manipulate influence-based attributions and investigate whether these\nattributions can be \\textit{systematically} tampered by an adversary. We show\nthat this is indeed possible for logistic regression models trained on ResNet\nfeature embeddings and standard tabular fairness datasets and provide efficient\nattacks with backward-friendly implementations. Our work raises questions on\nthe reliability of influence-based attributions in adversarial circumstances.\nCode is available at :\n\\url{https://github.com/infinite-pursuits/influence-based-attributions-can-be-manipulated}",
      "tldr_zh": "本文研究了 Influence Functions 这种用于将预测归因于训练数据的标准工具，常应用于数据估值和公平性分析。作者展示了现实动机，并证明这些归因可以被对手系统性地操纵，尤其在 logistic regression 模型训练于 ResNet 特征嵌入的标准表格公平数据集上。研究提供了高效的攻击方法，包括 backward-friendly 实现，并通过实验验证了其可行性。这揭示了 Influence-based Attributions 在对抗环境中的潜在不可靠性，代码可从 GitHub 获取。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.05208v4",
      "published_date": "2024-09-08 19:52:00 UTC",
      "updated_date": "2024-10-07 03:13:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:12:49.456228"
    },
    {
      "arxiv_id": "2409.05206v1",
      "title": "SEF: A Method for Computing Prediction Intervals by Shifting the Error Function in Neural Networks",
      "title_zh": "SEF：一种通过移位误差函数在神经网络中计算预测区间的方法",
      "authors": [
        "E. V. Aretos",
        "D. G. Sotiropoulos"
      ],
      "abstract": "In today's era, Neural Networks (NN) are applied in various scientific fields\nsuch as robotics, medicine, engineering, etc. However, the predictions of\nneural networks themselves contain a degree of uncertainty that must always be\ntaken into account before any decision is made. This is why many researchers\nhave focused on developing different ways to quantify the uncertainty of neural\nnetwork predictions. Some of these methods are based on generating prediction\nintervals (PI) via neural networks for the requested target values. The SEF\n(Shifting the Error Function) method presented in this paper is a new method\nthat belongs to this category of methods. The proposed approach involves\ntraining a single neural network three times, thus generating an estimate along\nwith the corresponding upper and lower bounds for a given problem. A pivotal\naspect of the method is the calculation of a parameter from the initial\nnetwork's estimates, which is then integrated into the loss functions of the\nother two networks. This innovative process effectively produces PIs, resulting\nin a robust and efficient technique for uncertainty quantification. To evaluate\nthe effectiveness of our method, a comparison in terms of successful PI\ngeneration between the SEF, PI3NN and PIVEN methods was made using two\nsynthetic datasets.",
      "tldr_zh": "该论文提出了一种名为SEF（Shifting the Error Function）的新方法，用于在Neural Networks中计算预测区间（Prediction Intervals），以量化模型预测的不确定性。SEF方法通过训练单个神经网络三次，首次生成初始估计，然后计算一个参数并将其整合到其他两个网络的损失函数中，从而产生可靠的上界和下界。实验结果显示，SEF在两个合成数据集上与PI3NN和PIVEN方法相比，成功生成预测区间的效果更佳，为Neural Networks在实际应用中的不确定性处理提供了高效技术。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "The paper has been accepted at the 2024 International Conference on\n  Computer and Applications (ICCA24), Cairo, Egypt, December 17-19, 2024.\n  https://icca-conf.info/icca-2024",
      "pdf_url": "http://arxiv.org/pdf/2409.05206v1",
      "published_date": "2024-09-08 19:46:45 UTC",
      "updated_date": "2024-09-08 19:46:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:13:01.361389"
    },
    {
      "arxiv_id": "2409.05202v2",
      "title": "A Survey on Mixup Augmentations and Beyond",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Jin",
        "Hongyu Zhu",
        "Siyuan Li",
        "Zedong Wang",
        "Zicheng Liu",
        "Juanxi Tian",
        "Chang Yu",
        "Huafeng Qin",
        "Stan Z. Li"
      ],
      "abstract": "As Deep Neural Networks have achieved thrilling breakthroughs in the past\ndecade, data augmentations have garnered increasing attention as regularization\ntechniques when massive labeled data are unavailable. Among existing\naugmentations, Mixup and relevant data-mixing methods that convexly combine\nselected samples and the corresponding labels are widely adopted because they\nyield high performances by generating data-dependent virtual data while easily\nmigrating to various domains. This survey presents a comprehensive review of\nfoundational mixup methods and their applications. We first elaborate on the\ntraining pipeline with mixup augmentations as a unified framework containing\nmodules. A reformulated framework could contain various mixup methods and give\nintuitive operational procedures. Then, we systematically investigate the\napplications of mixup augmentations on vision downstream tasks, various data\nmodalities, and some analysis \\& theorems of mixup. Meanwhile, we conclude the\ncurrent status and limitations of mixup research and point out further work for\neffective and efficient mixup augmentations. This survey can provide\nresearchers with the current state of the art in mixup methods and provide some\ninsights and guidance roles in the mixup arena. An online project with this\nsurvey is available at https://github.com/Westlake-AI/Awesome-Mixup.",
      "tldr_zh": "这篇调查论文回顾了Mixup增强技术及其相关方法，作为Deep Neural Networks在数据标注不足时的正则化策略。Mixup通过凸组合选定的样本和标签，生成数据依赖的虚拟数据，从而提升模型性能并易于应用于各种领域。论文提出一个统一的训练管道框架，系统探讨了Mixup在视觉下游任务、多种数据模态以及分析与定理中的应用，同时总结了其当前局限性并指出未来改进方向。该调查为研究者提供了Mixup领域的最新状态和指导，并附带了一个在线项目资源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint V2 with 30 pages main text. Online project at\n  https://github.com/Westlake-AI/Awesome-Mixup",
      "pdf_url": "http://arxiv.org/pdf/2409.05202v2",
      "published_date": "2024-09-08 19:32:22 UTC",
      "updated_date": "2025-04-23 17:47:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:13:13.609689"
    },
    {
      "arxiv_id": "2409.05197v3",
      "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?",
      "title_zh": "多跳推理中看似合理的干扰项：大型语言模型是专注的读者吗？",
      "authors": [
        "Neeladri Bhuiya",
        "Viktor Schlegel",
        "Stefan Winkler"
      ],
      "abstract": "State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.",
      "tldr_zh": "本论文探讨大型语言模型(LLMs)在多跳推理中的表现，揭示它们往往依赖简化线索而非真正整合多源信息，从而规避了实际推理需求。研究者提出一个新基准，通过生成看似合理的多跳推理链（seemingly plausible distractors）来测试模型的鲁棒性。实验评估多种开源和专有LLMs，结果显示这些模型的F1分数相对下降高达45%，表明虽然LLMs能忽略误导的词汇线索，但误导的推理路径仍是显著挑战。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 3 figures, EMNLP 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2409.05197v3",
      "published_date": "2024-09-08 19:22:58 UTC",
      "updated_date": "2024-10-31 02:19:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:13:26.589710"
    },
    {
      "arxiv_id": "2409.05177v1",
      "title": "Insights from Benchmarking Frontier Language Models on Web App Code Generation",
      "title_zh": "前沿语言模型在 Web App 代码生成基准测试中的见",
      "authors": [
        "Yi Cui"
      ],
      "abstract": "This paper presents insights from evaluating 16 frontier large language\nmodels (LLMs) on the WebApp1K benchmark, a test suite designed to assess the\nability of LLMs to generate web application code. The results reveal that while\nall models possess similar underlying knowledge, their performance is\ndifferentiated by the frequency of mistakes they make. By analyzing lines of\ncode (LOC) and failure distributions, we find that writing correct code is more\ncomplex than generating incorrect code. Furthermore, prompt engineering shows\nlimited efficacy in reducing errors beyond specific cases. These findings\nsuggest that further advancements in coding LLM should emphasize on model\nreliability and mistake minimization.",
      "tldr_zh": "这篇论文评估了16个前沿大型语言模型(LLMs)在WebApp1K基准上的web应用代码生成性能，结果显示所有模型拥有相似的底层知识，但性能差异主要取决于错误频率。研究通过分析代码行(LOC)和错误分布发现，生成正确代码比生成错误代码更复杂，且提示工程在减少错误方面仅在特定情况下有效。这些发现建议未来的LLMs发展应重点强调模型可靠性和错误最小化。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.05177v1",
      "published_date": "2024-09-08 18:24:26 UTC",
      "updated_date": "2024-09-08 18:24:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:13:37.458149"
    },
    {
      "arxiv_id": "2409.12175v1",
      "title": "Expanding Expressivity in Transformer Models with MöbiusAttention",
      "title_zh": "通过 MöbiusAttention 扩展 Transformer 模型的表达能力",
      "authors": [
        "Anna-Maria Halacheva",
        "Mojtaba Nayyeri",
        "Steffen Staab"
      ],
      "abstract": "Attention mechanisms and Transformer architectures have revolutionized\nNatural Language Processing (NLP) by enabling exceptional modeling of\nlong-range dependencies and capturing intricate linguistic patterns. However,\ntheir inherent reliance on linear operations in the form of matrix\nmultiplications limits their ability to fully capture inter-token relationships\non their own. We propose M\\\"obiusAttention, a novel approach that integrates\nM\\\"obius transformations within the attention mechanism of Transformer-based\nmodels. M\\\"obius transformations are non-linear operations in spaces over\ncomplex numbers with the ability to map between various geometries. By\nincorporating these properties, M\\\"obiusAttention empowers models to learn more\nintricate geometric relationships between tokens and capture a wider range of\ninformation through complex-valued weight vectors. We build and pre-train a\nBERT and a RoFormer version enhanced with M\\\"obiusAttention, which we then\nfinetune on the GLUE benchmark. We evaluate empirically our approach against\nthe baseline BERT and RoFormer models on a range of downstream tasks. Our\napproach compares favorably against the baseline models, even with smaller\nnumber of parameters suggesting the enhanced expressivity of M\\\"obiusAttention.\nThis research paves the way for exploring the potential of M\\\"obius\ntransformations in the complex projective space to enhance the expressivity and\nperformance of foundation models.",
      "tldr_zh": "本文提出MöbiusAttention，一种将Möbius变换整合到Transformer模型注意力机制中的新方法，以克服其线性操作（如矩阵乘法）对token间关系捕获的局限性。Möbius变换作为非线性操作，能在复数空间映射不同几何结构，从而帮助模型学习更复杂的inter-token关系并通过complex-valued权重向量提升表达性。研究构建并预训练了增强版BERT和RoFormer模型，并在GLUE基准上微调实验中，MöbiusAttention版本表现出色，即使参数更少，也优于基线模型。这为探索Möbius变换在complex projective space中提升基础模型性能开辟了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12175v1",
      "published_date": "2024-09-08 16:56:33 UTC",
      "updated_date": "2024-09-08 16:56:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:13:51.446846"
    },
    {
      "arxiv_id": "2409.05152v2",
      "title": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Jintian Zhang",
        "Cheng Peng",
        "Mengshu Sun",
        "Xiang Chen",
        "Lei Liang",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.",
      "tldr_zh": "这篇论文提出 OneGen 框架，一种高效的单次前向传递方法，用于统一大型语言模型 (LLMs) 的生成和检索任务，解决了 LLMs 在处理检索时存在的局限性。OneGen 通过在生成过程中自动生成检索 tokens，将生成和检索整合到一个统一的流程中，从而实现无缝整合。实验在 RAG 和 Entity Linking 等复合任务上验证了 OneGen 的可插入性、有效性和效率，结果显示它不仅提升了检索性能，还保留了 LLMs 的生成能力；这是首个让 LLMs 在生成中进行 vector retrieval 的框架。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Findings; code is available at\n  https://github.com/zjunlp/OneGen",
      "pdf_url": "http://arxiv.org/pdf/2409.05152v2",
      "published_date": "2024-09-08 16:35:19 UTC",
      "updated_date": "2024-10-02 05:02:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:14:01.476079"
    },
    {
      "arxiv_id": "2409.05144v2",
      "title": "QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors with Variance-bounded REINFORCE",
      "title_zh": "翻译失败",
      "authors": [
        "Junjie Zhao",
        "Chengxi Zhang",
        "Min Qin",
        "Peng Yang"
      ],
      "abstract": "The goal of alpha factor mining is to discover indicative signals of\ninvestment opportunities from the historical financial market data of assets,\nwhich can be used to predict asset returns and gain excess profits. Recently, a\npromising framework is proposed for generating formulaic alpha factors using\ndeep reinforcement learning, and quickly gained research focuses from both\nacademia and industries. This paper first argues that the originally employed\npolicy training method, i.e., Proximal Policy Optimization (PPO), faces several\nimportant issues in the context of alpha factors mining, making it ineffective\nto explore the search space of the formula. Herein, a novel reinforcement\nlearning based on the well-known REINFORCE algorithm is proposed. Given that\nthe underlying state transition function adheres to the Dirac distribution, the\nMarkov Decision Process within this framework exhibit minimal environmental\nvariability, making REINFORCE algorithm more appropriate than PPO. A new\ndedicated baseline is designed to theoretically reduce the commonly suffered\nhigh variance of REINFORCE. Moreover, the information ratio is introduced as a\nreward shaping mechanism to encourage the generation of steady alpha factors\nthat can better adapt to changes in market volatility. Experimental evaluations\non various real assets data show that the proposed algorithm can increase the\ncorrelation with asset returns by 3.83\\%, and a stronger ability to obtain\nexcess returns compared to the latest alpha factors mining methods, which meets\nthe theoretical results well.",
      "tldr_zh": "这篇论文针对alpha因子挖掘的问题，提出了一种改进的强化学习算法QuantFactor REINFORCE，以克服传统Proximal Policy Optimization (PPO)方法的局限性。该算法基于REINFORCE框架，利用其在Dirac分布环境下更适合的特性，设计了方差受限基准并引入信息比率作为奖励机制，以生成更稳定的公式alpha因子。实验结果显示，该方法在真实资产数据上将资产回报的相关性提高了3.83%，并在超额回报能力上优于现有方法。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.CP",
      "comment": "16 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.05144v2",
      "published_date": "2024-09-08 15:57:58 UTC",
      "updated_date": "2024-10-08 15:37:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:14:13.968984"
    },
    {
      "arxiv_id": "2409.05921v1",
      "title": "STLLM-DF: A Spatial-Temporal Large Language Model with Diffusion for Enhanced Multi-Mode Traffic System Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiqi Shao",
        "Haoning Xi",
        "Haohui Lu",
        "Ze Wang",
        "Michael G. H. Bell",
        "Junbin Gao"
      ],
      "abstract": "The rapid advancement of Intelligent Transportation Systems (ITS) presents\nchallenges, particularly with missing data in multi-modal transportation and\nthe complexity of handling diverse sequential tasks within a centralized\nframework. To address these issues, we propose the Spatial-Temporal Large\nLanguage Model Diffusion (STLLM-DF), an innovative model that leverages\nDenoising Diffusion Probabilistic Models (DDPMs) and Large Language Models\n(LLMs) to improve multi-task transportation prediction. The DDPM's robust\ndenoising capabilities enable it to recover underlying data patterns from noisy\ninputs, making it particularly effective in complex transportation systems.\nMeanwhile, the non-pretrained LLM dynamically adapts to spatial-temporal\nrelationships within multi-modal networks, allowing the system to efficiently\nmanage diverse transportation tasks in both long-term and short-term\npredictions. Extensive experiments demonstrate that STLLM-DF consistently\noutperforms existing models, achieving an average reduction of 2.40\\% in MAE,\n4.50\\% in RMSE, and 1.51\\% in MAPE. This model significantly advances\ncentralized ITS by enhancing predictive accuracy, robustness, and overall\nsystem performance across multiple tasks, thus paving the way for more\neffective spatio-temporal traffic forecasting through the integration of frozen\ntransformer language models and diffusion techniques.",
      "tldr_zh": "该研究针对智能交通系统（ITS）中多模式交通数据的缺失和处理多样顺序任务的复杂性，提出了一种创新模型 STLLM-DF，该模型整合了 Denoising Diffusion Probabilistic Models (DDPMs) 和 Large Language Models (LLMs)，以提升多任务交通预测的准确性和鲁棒性。DDPMs 通过强大的去噪能力从噪声输入中恢复底层数据模式，而非预训练的 LLMs 则动态适应多模式网络中的空间-时间关系，支持长期和短期预测。实验结果显示，STLLM-DF 比现有模型平均降低了 2.40% 的 MAE、4.50% 的 RMSE 和 1.51% 的 MAPE，从而显著提高了 ITS 的预测性能，并通过 frozen transformer language models 和 diffusion 技术的结合，为空间-时间交通预测开辟了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.7",
        "I.2.1"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.05921v1",
      "published_date": "2024-09-08 15:29:27 UTC",
      "updated_date": "2024-09-08 15:29:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:14:26.501694"
    },
    {
      "arxiv_id": "2409.05105v1",
      "title": "EdaCSC: Two Easy Data Augmentation Methods for Chinese Spelling Correction",
      "title_zh": "翻译失败",
      "authors": [
        "Lei Sheng",
        "Shuai-Shuai Xu"
      ],
      "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct spelling errors\nin Chinese sentences caused by phonetic or visual similarities. While current\nCSC models integrate pinyin or glyph features and have shown significant\nprogress,they still face challenges when dealing with sentences containing\nmultiple typos and are susceptible to overcorrection in real-world scenarios.\nIn contrast to existing model-centric approaches, we propose two data\naugmentation methods to address these limitations. Firstly, we augment the\ndataset by either splitting long sentences into shorter ones or reducing typos\nin sentences with multiple typos. Subsequently, we employ different training\nprocesses to select the optimal model. Experimental evaluations on the SIGHAN\nbenchmarks demonstrate the superiority of our approach over most existing\nmodels, achieving state-of-the-art performance on the SIGHAN15 test set.",
      "tldr_zh": "这篇论文提出 EdaCSC，一种针对 Chinese Spelling Correction (CSC) 的两种简单数据增强方法，以解决当前模型在处理多错误句子和过度纠正问题时的局限性。具体而言，这些方法包括将长句子分割成短句子或减少句子中的错误数量，随后通过不同的训练过程选择最优模型。在 SIGHAN 基准测试中，该方法表现出色，优于大多数现有模型，并在 SIGHAN15 测试集上实现 state-of-the-art 性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.05105v1",
      "published_date": "2024-09-08 14:29:10 UTC",
      "updated_date": "2024-09-08 14:29:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:14:37.633243"
    },
    {
      "arxiv_id": "2409.05919v1",
      "title": "KModels: Unlocking AI for Business Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Roy Abitbol",
        "Eyal Cohen",
        "Muhammad Kanaan",
        "Bhavna Agrawal",
        "Yingjie Li",
        "Anuradha Bhamidipaty",
        "Erez Bilgory"
      ],
      "abstract": "As artificial intelligence (AI) continues to rapidly advance, there is a\ngrowing demand to integrate AI capabilities into existing business\napplications. However, a significant gap exists between the rapid progress in\nAI and how slowly AI is being embedded into business environments. Deploying\nwell-performing lab models into production settings, especially in on-premise\nenvironments, often entails specialized expertise and imposes a heavy burden of\nmodel management, creating significant barriers to implementing AI models in\nreal-world applications.\n  KModels leverages proven libraries and platforms (Kubeflow Pipelines, KServe)\nto streamline AI adoption by supporting both AI developers and consumers. It\nallows model developers to focus solely on model development and share models\nas transportable units (Templates), abstracting away complex production\ndeployment concerns. KModels enables AI consumers to eliminate the need for a\ndedicated data scientist, as the templates encapsulate most data science\nconsiderations while providing business-oriented control.\n  This paper presents the architecture of KModels and the key decisions that\nshape it. We outline KModels' main components as well as its interfaces.\nFurthermore, we explain how KModels is highly suited for on-premise deployment\nbut can also be used in cloud environments.\n  The efficacy of KModels is demonstrated through the successful deployment of\nthree AI models within an existing Work Order Management system. These models\noperate in a client's data center and are trained on local data, without data\nscientist intervention. One model improved the accuracy of Failure Code\nspecification for work orders from 46% to 83%, showcasing the substantial\nbenefit of accessible and localized AI solutions.",
      "tldr_zh": "本论文介绍了 KModels，一种旨在简化 AI 在商业应用中的集成框架，解决 AI 模型从实验室到生产环境的部署挑战，特别是针对本地部署的复杂管理和专业知识需求。KModels 利用 Kubeflow Pipelines 和 KServe 等成熟库，让模型开发者专注于开发并通过 Templates 分享可移植模型，同时允许 AI 消费者无需数据科学家介入，即可实现业务导向控制。论文详细阐述了 KModels 的架构、组件和接口，支持本地和云环境部署。通过实际案例，在一个 Work Order Management 系统上部署三个 AI 模型，其中一个模型将 Failure Code 指定准确率从 46% 提高到 83%，展示了其在提升商业 AI 可用性和效果方面的显著益处。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.05919v1",
      "published_date": "2024-09-08 13:19:12 UTC",
      "updated_date": "2024-09-08 13:19:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:14:49.537967"
    },
    {
      "arxiv_id": "2409.05084v1",
      "title": "Adaptive $k$-nearest neighbor classifier based on the local estimation of the shape operator",
      "title_zh": "翻译失败",
      "authors": [
        "Alexandre Luís Magalhães Levada",
        "Frank Nielsen",
        "Michel Ferreira Cardia Haddad"
      ],
      "abstract": "The $k$-nearest neighbor ($k$-NN) algorithm is one of the most popular\nmethods for nonparametric classification. However, a relevant limitation\nconcerns the definition of the number of neighbors $k$. This parameter exerts a\ndirect impact on several properties of the classifier, such as the\nbias-variance tradeoff, smoothness of decision boundaries, robustness to noise,\nand class imbalance handling. In the present paper, we introduce a new adaptive\n$k$-nearest neighbours ($kK$-NN) algorithm that explores the local curvature at\na sample to adaptively defining the neighborhood size. The rationale is that\npoints with low curvature could have larger neighborhoods (locally, the tangent\nspace approximates well the underlying data shape), whereas points with high\ncurvature could have smaller neighborhoods (locally, the tangent space is a\nloose approximation). We estimate the local Gaussian curvature by computing an\napproximation to the local shape operator in terms of the local covariance\nmatrix as well as the local Hessian matrix. Results on many real-world datasets\nindicate that the new $kK$-NN algorithm yields superior balanced accuracy\ncompared to the established $k$-NN method and also another adaptive $k$-NN\nalgorithm. This is particularly evident when the number of samples in the\ntraining data is limited, suggesting that the $kK$-NN is capable of learning\nmore discriminant functions with less data considering many relevant cases.",
      "tldr_zh": "本文提出了一种基于局部形状算子估计的自适应 $k$-nearest neighbor ($kK$-NN) 分类器，以解决传统 $k$-NN 算法中 $k$ 值选择对偏置-方差权衡、决策边界平滑度和噪声鲁棒性等的影响。方法通过计算局部高斯曲率，利用局部协方差矩阵和 Hessian 矩阵动态调整邻域大小，使曲率低的点采用更大邻域，而曲率高的点采用更小邻域，以更好地逼近数据形状。实验结果表明，$kK$-NN 在多个真实数据集上比传统 $k$-NN 和其他自适应算法实现了更高的平衡准确率，尤其在训练数据有限时表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.05084v1",
      "published_date": "2024-09-08 13:08:45 UTC",
      "updated_date": "2024-09-08 13:08:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:15:03.183082"
    },
    {
      "arxiv_id": "2409.05076v1",
      "title": "PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions",
      "title_zh": "PIP：通过无关探针问题的注意力模式检测大型视觉语言模型中的对抗样本",
      "authors": [
        "Yudong Zhang",
        "Ruobing Xie",
        "Jiansheng Chen",
        "Xingwu Sun",
        "Yu Wang"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have demonstrated their powerful\nmultimodal capabilities. However, they also face serious safety problems, as\nadversaries can induce robustness issues in LVLMs through the use of\nwell-designed adversarial examples. Therefore, LVLMs are in urgent need of\ndetection tools for adversarial examples to prevent incorrect responses. In\nthis work, we first discover that LVLMs exhibit regular attention patterns for\nclean images when presented with probe questions. We propose an unconventional\nmethod named PIP, which utilizes the attention patterns of one randomly\nselected irrelevant probe question (e.g., \"Is there a clock?\") to distinguish\nadversarial examples from clean examples. Regardless of the image to be tested\nand its corresponding question, PIP only needs to perform one additional\ninference of the image to be tested and the probe question, and then achieves\nsuccessful detection of adversarial examples. Even under black-box attacks and\nopen dataset scenarios, our PIP, coupled with a simple SVM, still achieves more\nthan 98% recall and a precision of over 90%. Our PIP is the first attempt to\ndetect adversarial attacks on LVLMs via simple irrelevant probe questions,\nshedding light on deeper understanding and introspection within LVLMs. The code\nis available at https://github.com/btzyd/pip.",
      "tldr_zh": "该研究针对大型视觉语言模型 (LVLMs) 面临的对抗样本攻击问题，提出了一种名为 PIP 的检测方法，通过分析无关探测问题（如“Is there a clock?”）的注意力模式来区分对抗样本和干净样本。PIP 方法仅需对测试图像进行一次额外推理，并结合简单 SVM 分类器，就能有效检测攻击。实验结果显示，即使在黑盒攻击和开放数据集场景下，PIP 也能实现超过 98% 的召回率和 90% 的精确率。该方法首次利用无关探测问题揭示 LVLMs 的内部机制，为模型的安全性提供新颖的内省途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACM Multimedia 2024 BNI track (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2409.05076v1",
      "published_date": "2024-09-08 12:38:25 UTC",
      "updated_date": "2024-09-08 12:38:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:15:15.114842"
    },
    {
      "arxiv_id": "2409.05061v2",
      "title": "Dynamic Demand Management for Parcel Lockers",
      "title_zh": "包裹储物柜的动态需求管理",
      "authors": [
        "Daniela Sailer",
        "Robert Klein",
        "Claudius Steinhardt"
      ],
      "abstract": "In pursuit of a more sustainable and cost-efficient last mile, parcel lockers\nhave gained a firm foothold in the parcel delivery landscape. To fully exploit\ntheir potential and simultaneously ensure customer satisfaction, successful\nmanagement of the locker's limited capacity is crucial. This is challenging as\nfuture delivery requests and pickup times are stochastic from the provider's\nperspective. In response, we propose to dynamically control whether the locker\nis presented as an available delivery option to each incoming customer with the\ngoal of maximizing the number of served requests weighted by their priority.\nAdditionally, we take different compartment sizes into account, which entails a\nsecond type of decision as parcels scheduled for delivery must be allocated. We\nformalize the problem as an infinite-horizon sequential decision problem and\nfind that exact methods are intractable due to the curses of dimensionality. In\nlight of this, we develop a solution framework that orchestrates multiple\nalgorithmic techniques rooted in Sequential Decision Analytics and\nReinforcement Learning, namely cost function approximation and an offline\ntrained parametric value function approximation together with a truncated\nonline rollout. Our innovative approach to combine these techniques enables us\nto address the strong interrelations between the two decision types. As a\ngeneral methodological contribution, we enhance the training of our value\nfunction approximation with a modified version of experience replay that\nenforces structure in the value function. Our computational study shows that\nour method outperforms a myopic benchmark by 13.7% and an industry-inspired\npolicy by 12.6%.",
      "tldr_zh": "该研究针对包裹储物柜的容量限制和随机需求挑战，提出了一种动态需求管理方法，旨在最大化按优先级加权的服务请求数量，同时考虑不同隔间大小的分配决策。问题被形式化为无限期顺序决策问题，但由于维数诅咒，采用基于Sequential Decision Analytics和Reinforcement Learning的框架，包括成本函数近似、离线训练的参数值函数近似，以及截断的在线回滚，以处理两种决策类型之间的强相关性。研究还改进了经验回放机制，以增强值函数的结构化训练；计算实验显示，该方法比短视基准政策提高13.7%，比行业启发式政策提高12.6%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.05061v2",
      "published_date": "2024-09-08 11:38:48 UTC",
      "updated_date": "2024-09-12 08:19:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:15:29.229822"
    },
    {
      "arxiv_id": "2409.05035v1",
      "title": "Deep Generic Representations for Domain-Generalized Anomalous Sound Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Phurich Saengthong",
        "Takahiro Shinozaki"
      ],
      "abstract": "Developing a reliable anomalous sound detection (ASD) system requires\nrobustness to noise, adaptation to domain shifts, and effective performance\nwith limited training data. Current leading methods rely on extensive labeled\ndata for each target machine type to train feature extractors using\nOutlier-Exposure (OE) techniques, yet their performance on the target domain\nremains sub-optimal. In this paper, we present \\textit{GenRep}, which utilizes\ngeneric feature representations from a robust, large-scale pre-trained feature\nextractor combined with kNN for domain-generalized ASD, without the need for\nfine-tuning. \\textit{GenRep} incorporates MemMixup, a simple approach for\naugmenting the target memory bank using nearest source samples, paired with a\ndomain normalization technique to address the imbalance between source and\ntarget domains. \\textit{GenRep} outperforms the best OE-based approach without\na need for labeled data with an Official Score of 73.79\\% on the DCASE2023T2\nEval set and demonstrates robustness under limited data scenarios. The code is\navailable open-source.",
      "tldr_zh": "这篇论文针对Anomalous Sound Detection (ASD)系统面临的噪声干扰、域移位和训练数据有限的挑战，提出GenRep方法，利用大规模预训练的通用特征表示结合kNN实现域泛化检测，而无需微调。GenRep引入MemMixup技术，通过使用最近源样本增强目标记忆库，并结合域归一化方法来平衡源域和目标域的不一致性。实验结果显示，GenRep在DCASE2023T2 Eval集上获得73.79%的Official Score，优于基于Outlier-Exposure (OE)的现有方法，并在数据有限情况下表现出更强的鲁棒性。该方法代码已开源，提供了一个高效的ASD解决方案。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.05035v1",
      "published_date": "2024-09-08 09:20:30 UTC",
      "updated_date": "2024-09-08 09:20:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:15:39.777272"
    },
    {
      "arxiv_id": "2409.05033v2",
      "title": "A Survey on Diffusion Models for Recommender Systems",
      "title_zh": "扩散模型在推荐系统中的综述",
      "authors": [
        "Jianghao Lin",
        "Jiaqi Liu",
        "Jiachen Zhu",
        "Yunjia Xi",
        "Chengkai Liu",
        "Yangtian Zhang",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "abstract": "While traditional recommendation techniques have made significant strides in\nthe past decades, they still suffer from limited generalization performance\ncaused by factors like inadequate collaborative signals, weak latent\nrepresentations, and noisy data. In response, diffusion models (DMs) have\nemerged as promising solutions for recommender systems due to their robust\ngenerative capabilities, solid theoretical foundations, and improved training\nstability. To this end, in this paper, we present the first comprehensive\nsurvey on diffusion models for recommendation, and draw a bird's-eye view from\nthe perspective of the whole pipeline in real-world recommender systems. We\nsystematically categorize existing research works into three primary domains:\n(1) diffusion for data engineering & encoding, focusing on data augmentation\nand representation enhancement; (2) diffusion as recommender models, employing\ndiffusion models to directly estimate user preferences and rank items; and (3)\ndiffusion for content presentation, utilizing diffusion models to generate\npersonalized content such as fashion and advertisement creatives. Our taxonomy\nhighlights the unique strengths of diffusion models in capturing complex data\ndistributions and generating high-quality, diverse samples that closely align\nwith user preferences. We also summarize the core characteristics of the\nadapting diffusion models for recommendation, and further identify key areas\nfor future exploration, which helps establish a roadmap for researchers and\npractitioners seeking to advance recommender systems through the innovative\napplication of diffusion models. To further facilitate the research community\nof recommender systems based on diffusion models, we actively maintain a GitHub\nrepository for papers and other related resources in this rising direction\nhttps://github.com/CHIANGEL/Awesome-Diffusion-for-RecSys.",
      "tldr_zh": "这篇调查论文全面探讨了diffusion models在recommender systems中的应用，旨在解决传统推荐技术的局限性，如泛化性能差和数据噪声问题。论文将现有研究分类为三个领域：(1) diffusion for data engineering & encoding，用于数据增强和表示优化；(2) diffusion as recommender models，直接估计用户偏好和排名项目；以及(3) diffusion for content presentation，生成个性化内容如时尚和广告。作者强调了diffusion models的优势，包括捕捉复杂数据分布和生成高质量多样样本，并总结了其核心特性，同时指出了未来探索的关键方向，并提供了相关资源的GitHub仓库。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2409.05033v2",
      "published_date": "2024-09-08 08:57:12 UTC",
      "updated_date": "2024-09-15 13:29:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:15:50.055566"
    },
    {
      "arxiv_id": "2409.16292v1",
      "title": "Explaining Human Comparisons using Alignment-Importance Heatmaps",
      "title_zh": "翻译失败",
      "authors": [
        "Nhut Truong",
        "Dario Pesenti",
        "Uri Hasson"
      ],
      "abstract": "We present a computational explainability approach for human comparison\ntasks, using Alignment Importance Score (AIS) heatmaps derived from deep-vision\nmodels. The AIS reflects a feature-map's unique contribution to the alignment\nbetween Deep Neural Network's (DNN) representational geometry and that of\nhumans. We first validate the AIS by showing that prediction of out-of-sample\nhuman similarity judgments is improved when constructing representations using\nonly higher-scoring AIS feature maps identified from a training set. We then\ncompute image-specific heatmaps that visually indicate the areas that\ncorrespond to feature-maps with higher AIS scores. These maps provide an\nintuitive explanation of which image areas are more important when it is\ncompared to other images in a cohort. We observe a correspondence between these\nheatmaps and saliency maps produced by a gaze-prediction model. However, in\nsome cases, meaningful differences emerge, as the dimensions relevant for\ncomparison are not necessarily the most visually salient. To conclude,\nAlignment Importance improves prediction of human similarity judgments from DNN\nembeddings, and provides interpretable insights into the relevant information\nin image space.",
      "tldr_zh": "本文提出了一种基于 Alignment Importance Score (AIS) 热图的计算解释性方法，用于解释人类比较任务。AIS 量化了 DNN 特征图对人类和模型表示几何对齐的独特贡献，通过选择训练集中的高 AIS 分数特征图，显著提高了对样本外人类相似性判断的预测准确性。这些热图直观地标识图像中重要的比较区域，与注视预测模型的显著性图有对应，但有时揭示了视觉上不那么显著却对比较关键的维度。总体上，该方法增强了从 DNN 嵌入预测人类判断的能力，并提供可解释的图像空间洞见。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.16292v1",
      "published_date": "2024-09-08 08:28:09 UTC",
      "updated_date": "2024-09-08 08:28:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:16:02.653768"
    },
    {
      "arxiv_id": "2409.05022v1",
      "title": "Sequential Recommendation via Adaptive Robust Attention with Multi-dimensional Embeddings",
      "title_zh": "通过自适应鲁棒注意力与多维嵌入的序列推荐",
      "authors": [
        "Linsey Pang",
        "Amir Hossein Raffiee",
        "Wei Liu",
        "Keld Lundgaard"
      ],
      "abstract": "Sequential recommendation models have achieved state-of-the-art performance\nusing self-attention mechanism. It has since been found that moving beyond only\nusing item ID and positional embeddings leads to a significant accuracy boost\nwhen predicting the next item. In recent literature, it was reported that a\nmulti-dimensional kernel embedding with temporal contextual kernels to capture\nusers' diverse behavioral patterns results in a substantial performance\nimprovement. In this study, we further improve the sequential recommender\nmodel's robustness and generalization by introducing a mix-attention mechanism\nwith a layer-wise noise injection (LNI) regularization. We refer to our\nproposed model as adaptive robust sequential recommendation framework (ADRRec),\nand demonstrate through extensive experiments that our model outperforms\nexisting self-attention architectures.",
      "tldr_zh": "该研究提出了一种自适应鲁棒顺序推荐框架（ADRRec），通过整合多维嵌入（如项目 ID、位置嵌入和时间上下文内核）来捕捉用户多样行为模式，并引入混注意力机制（mix-attention mechanism）和层级噪声注入（LNI）正则化，以提升模型的鲁棒性和泛化能力。相比传统自注意力机制，ADRRec 显著提高了顺序推荐的准确性。实验结果显示，该框架在多个基准测试中优于现有自注意力架构。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.05022v1",
      "published_date": "2024-09-08 08:27:22 UTC",
      "updated_date": "2024-09-08 08:27:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:16:14.855653"
    },
    {
      "arxiv_id": "2409.05007v1",
      "title": "Audio-Guided Fusion Techniques for Multimodal Emotion Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Pujin Shi",
        "Fei Gao"
      ],
      "abstract": "In this paper, we propose a solution for the semi-supervised learning track\n(MER-SEMI) in MER2024. First, in order to enhance the performance of the\nfeature extractor on sentiment classification tasks,we fine-tuned video and\ntext feature extractors, specifically CLIP-vit-large and Baichuan-13B, using\nlabeled data. This approach effectively preserves the original emotional\ninformation conveyed in the videos. Second, we propose an Audio-Guided\nTransformer (AGT) fusion mechanism, which leverages the robustness of\nHubert-large, showing superior effectiveness in fusing both inter-channel and\nintra-channel information. Third, To enhance the accuracy of the model, we\niteratively apply self-supervised learning by using high-confidence unlabeled\ndata as pseudo-labels. Finally, through black-box probing, we discovered an\nimbalanced data distribution between the training and test sets. Therefore, We\nadopt a prior-knowledge-based voting mechanism. The results demonstrate the\neffectiveness of our strategy, ultimately earning us third place in the\nMER-SEMI track.",
      "tldr_zh": "这篇论文针对 MER2024 的 MER-SEMI 半监督学习轨道，提出了一种多模态情感分析解决方案，以提升情感分类性能。研究者首先微调了视频和文本特征提取器，如 CLIP-vit-large 和 Baichuan-13B，利用标记数据保留原始情感信息。接着，他们开发了 Audio-Guided Transformer (AGT) 融合机制，基于 Hubert-large 的鲁棒性，融合跨通道和内通道信息，并通过自监督学习迭代使用高置信度无标签数据作为伪标签来提高准确性。最后，通过发现训练集和测试集数据分布不平衡，并采用基于先验知识的投票机制，该策略在竞赛中取得第三名的成绩，证明了其有效性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.05007v1",
      "published_date": "2024-09-08 07:28:27 UTC",
      "updated_date": "2024-09-08 07:28:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:16:29.673380"
    },
    {
      "arxiv_id": "2409.05001v1",
      "title": "A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement",
      "title_zh": "翻译失败",
      "authors": [
        "Huan Zhang",
        "Wei Cheng",
        "Yuhan Wu",
        "Wei Hu"
      ],
      "abstract": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. Although prior studies enhanced LLMs with prompting techniques and\ncode refinement, they still struggle with complex programming problems due to\nrigid solution plans. In this paper, we draw on pair programming practices to\npropose PairCoder, a novel LLM-based framework for code generation. PairCoder\nincorporates two collaborative LLM agents, namely a Navigator agent for\nhigh-level planning and a Driver agent for specific implementation. The\nNavigator is responsible for proposing promising solution plans, selecting the\ncurrent optimal plan, and directing the next iteration round based on execution\nfeedback. The Driver follows the guidance of Navigator to undertake initial\ncode generation, code testing, and refinement. This interleaved and iterative\nworkflow involves multi-plan exploration and feedback-based refinement, which\nmimics the collaboration of pair programmers. We evaluate PairCoder with both\nopen-source and closed-source LLMs on various code generation benchmarks.\nExtensive experimental results demonstrate the superior accuracy of PairCoder,\nachieving relative pass@1 improvements of 12.00%-162.43% compared to prompting\nLLMs directly.",
      "tldr_zh": "这篇论文提出 PairCoder，一个基于 LLMs 的代码生成框架，模仿配对编程实践，通过多计划探索和反馈驱动精炼来解决复杂编程问题的刚性计划问题。框架包括两个协作代理：Navigator 负责高层规划、选择最佳方案并基于执行反馈指导迭代，而 Driver 负责初始代码生成、测试和精炼。这种交错迭代工作流程显著提升了代码生成性能。实验在各种基准上显示，PairCoder 相较于直接提示 LLMs，实现 pass@1 准确性相对提升 12.00%-162.43%。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted in the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.05001v1",
      "published_date": "2024-09-08 07:22:19 UTC",
      "updated_date": "2024-09-08 07:22:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:16:39.341232"
    },
    {
      "arxiv_id": "2409.09071v1",
      "title": "ELMS: Elasticized Large Language Models On Mobile Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Wangsong Yin",
        "Rongjie Yi",
        "Daliang Xu",
        "Gang Huang",
        "Mengwei Xu",
        "Xuanzhe Liu"
      ],
      "abstract": "On-device Large Language Models (LLMs) are revolutionizing mobile AI,\nenabling applications such as UI automation while addressing privacy concerns.\nCurrently, the standard approach involves deploying a single, robust LLM as a\nuniversal solution for various applications, often referred to as\nLLM-as-a-Service (LLMaaS). However, this approach faces a significant system\nchallenge: existing LLMs lack the flexibility to accommodate the diverse\nService-Level Objectives (SLOs) regarding inference latency across different\napplications. To address this issue, we introduce ELMS, an on-device LLM\nservice designed to provide elasticity in both the model and prompt dimensions\nof an LLMaaS. This system includes: A one-time neuron reordering technique,\nwhich utilizes the inherent permutation consistency within transformer models\nto create high-quality, elastic sub-models with minimal runtime switching\ncosts. A dual-head compact language model, which efficiently refines prompts\nand coordinates the elastic adaptation between the model and the prompt. We\nhave implemented this elastic on-device LLM service on several off-the-shelf\n(COTS) smartphones and evaluate ELMS using both standalone NLP/mobile-agent\ndatasets and synthesized end-to-end traces. Across a range of SLOs, ELMS\nsurpasses four strong baselines by up to 16.83% and 11.04% in absolute accuracy\non average, with less than 1% Time-To-First-Token (TTFT) switching overhead,\ncomparable memory usage, and fewer than 100 offline GPU hours.",
      "tldr_zh": "本文提出 ELMS，一种在移动设备上实现弹性的 Large Language Models (LLMs) 服务，旨在解决传统 LLMaaS 无法适应不同应用 Service-Level Objectives (SLOs) 延迟需求的挑战。ELMS 通过神经元重新排序技术创建高质量弹性子模型，以最小化运行时切换成本，并使用双头紧凑语言模型高效优化提示并协调模型与提示的适应。实验结果显示，ELMS 在各种 SLOs 下比四种强基线模型平均准确率提升高达 16.83%，切换开销小于 1% Time-To-First-Token (TTFT)，并保持可比的内存使用和低资源需求。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2409.09071v1",
      "published_date": "2024-09-08 06:32:08 UTC",
      "updated_date": "2024-09-08 06:32:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:16:51.943677"
    },
    {
      "arxiv_id": "2409.04977v1",
      "title": "Enhancing Convolutional Neural Networks with Higher-Order Numerical Difference Methods",
      "title_zh": "使用高阶数值差分方法增强卷积神经网络",
      "authors": [
        "Qi Wang",
        "Zijun Gao",
        "Mingxiu Sui",
        "Taiyuan Mei",
        "Xiaohan Cheng",
        "Iris Li"
      ],
      "abstract": "With the rise of deep learning technology in practical applications,\nConvolutional Neural Networks (CNNs) have been able to assist humans in solving\nmany real-world problems. To enhance the performance of CNNs, numerous network\narchitectures have been explored. Some of these architectures are designed\nbased on the accumulated experience of researchers over time, while others are\ndesigned through neural architecture search methods. The improvements made to\nCNNs by the aforementioned methods are quite significant, but most of the\nimprovement methods are limited in reality by model size and environmental\nconstraints, making it difficult to fully realize the improved performance. In\nrecent years, research has found that many CNN structures can be explained by\nthe discretization of ordinary differential equations. This implies that we can\ndesign theoretically supported deep network structures using higher-order\nnumerical difference methods. It should be noted that most of the previous CNN\nmodel structures are based on low-order numerical methods. Therefore,\nconsidering that the accuracy of linear multi-step numerical difference methods\nis higher than that of the forward Euler method, this paper proposes a stacking\nscheme based on the linear multi-step method. This scheme enhances the\nperformance of ResNet without increasing the model size and compares it with\nthe Runge-Kutta scheme. The experimental results show that the performance of\nthe stacking scheme proposed in this paper is superior to existing stacking\nschemes (ResNet and HO-ResNet), and it has the capability to be extended to\nother types of neural networks.",
      "tldr_zh": "该研究探讨了如何通过更高阶的数值差分方法提升卷积神经网络（CNN）的性能，解决现有改进方法受限于模型大小和环境约束的问题。论文提出一种基于线性多步方法的堆叠方案，用于设计理论支持的深度网络结构，并在不增加模型大小的情况下增强ResNet的性能，与Runge-Kutta方案进行比较。实验结果显示，该方案优于现有堆叠方案（如ResNet和HO-ResNet），并具有扩展到其他类型神经网络的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.04977v1",
      "published_date": "2024-09-08 05:13:58 UTC",
      "updated_date": "2024-09-08 05:13:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:17:02.227087"
    },
    {
      "arxiv_id": "2409.04976v1",
      "title": "HYDRA: Hybrid Data Multiplexing and Run-time Layer Configurable DNN Accelerator",
      "title_zh": "翻译失败",
      "authors": [
        "Sonu Kumar",
        "Komal Gupta",
        "Gopal Raut",
        "Mukul Lokhande",
        "Santosh Kumar Vishvakarma"
      ],
      "abstract": "Deep neural networks (DNNs) offer plenty of challenges in executing efficient\ncomputation at edge nodes, primarily due to the huge hardware resource demands.\nThe article proposes HYDRA, hybrid data multiplexing, and runtime layer\nconfigurable DNN accelerators to overcome the drawbacks. The work proposes a\nlayer-multiplexed approach, which further reuses a single activation function\nwithin the execution of a single layer with improved Fused-Multiply-Accumulate\n(FMA). The proposed approach works in iterative mode to reuse the same hardware\nand execute different layers in a configurable fashion. The proposed\narchitectures achieve reductions over 90% of power consumption and resource\nutilization improvements of state-of-the-art works, with 35.21 TOPSW. The\nproposed architecture reduces the area overhead (N-1) times required in\nbandwidth, AF and layer architecture. This work shows HYDRA architecture\nsupports optimal DNN computations while improving performance on\nresource-constrained edge devices.",
      "tldr_zh": "本论文提出 HYDRA，一种混合数据多路复用和运行时层可配置的 DNN 加速器，旨在解决 DNN 在边缘设备上因硬件资源需求巨大而导致的计算效率挑战。HYDRA 通过层多路复用方法重用单个激活函数并改进 Fused-Multiply-Accumulate (FMA)，以迭代模式配置硬件执行不同层，从而实现高效的重用和可配置性。实验结果显示，该架构减少了超过90%的功耗，提高了资源利用率并达到35.21 TOPSW 的性能，同时显著降低带宽、AF 和层架构的面积开销，支持在资源受限边缘设备上优化 DNN 计算。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.04976v1",
      "published_date": "2024-09-08 05:10:02 UTC",
      "updated_date": "2024-09-08 05:10:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:17:16.252866"
    },
    {
      "arxiv_id": "2409.04971v1",
      "title": "Soft Actor-Critic with Beta Policy via Implicit Reparameterization Gradients",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Della Libera"
      ],
      "abstract": "Recent advances in deep reinforcement learning have achieved impressive\nresults in a wide range of complex tasks, but poor sample efficiency remains a\nmajor obstacle to real-world deployment. Soft actor-critic (SAC) mitigates this\nproblem by combining stochastic policy optimization and off-policy learning,\nbut its applicability is restricted to distributions whose gradients can be\ncomputed through the reparameterization trick. This limitation excludes several\nimportant examples such as the beta distribution, which was shown to improve\nthe convergence rate of actor-critic algorithms in high-dimensional continuous\ncontrol problems thanks to its bounded support. To address this issue, we\ninvestigate the use of implicit reparameterization, a powerful technique that\nextends the class of reparameterizable distributions. In particular, we use\nimplicit reparameterization gradients to train SAC with the beta policy on\nsimulated robot locomotion environments and compare its performance with common\nbaselines. Experimental results show that the beta policy is a viable\nalternative, as it outperforms the normal policy and is on par with the\nsquashed normal policy, which is the go-to choice for SAC. The code is\navailable at https://github.com/lucadellalib/sac-beta.",
      "tldr_zh": "这篇论文提出了一种改进Soft Actor-Critic (SAC)算法的方法，通过Implicit Reparameterization Gradients实现Beta Policy的训练，以解决SAC在处理某些分布（如Beta分布）时的局限性问题，从而提升深度强化学习的样本效率。Beta Policy的优势在于其有界支持，能够在高维连续控制任务中加速收敛。实验结果显示，在模拟机器人运动环境中，该方法优于标准正常策略，并与squashed normal策略相当，证明了其在实际应用中的可行性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "62M45",
        "I.2.8; I.2.6; I.5.1"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.04971v1",
      "published_date": "2024-09-08 04:30:51 UTC",
      "updated_date": "2024-09-08 04:30:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:17:27.058231"
    },
    {
      "arxiv_id": "2409.04964v2",
      "title": "Evaluation of Google Translate for Mandarin Chinese translation using sentiment and semantic analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Xuechun Wang",
        "Rodney Beard",
        "Rohitash Chandra"
      ],
      "abstract": "Machine translation using large language models (LLMs) is having a\nsignificant global impact, making communication easier. Mandarin Chinese is the\nofficial language used for communication by the government and media in China.\nIn this study, we provide an automated assessment of translation quality of\nGoogle Translate with human experts using sentiment and semantic analysis. In\norder to demonstrate our framework, we select the classic early\ntwentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese\nto English translations. We use Google Translate to translate the given text\ninto English and then conduct a chapter-wise sentiment analysis and semantic\nanalysis to compare the extracted sentiments across the different translations.\nOur results indicate that the precision of Google Translate differs both in\nterms of semantic and sentiment analysis when compared to human expert\ntranslations. We find that Google Translate is unable to translate some of the\nspecific words or phrases in Chinese, such as Chinese traditional allusions.\nThe mistranslations may be due to lack of contextual significance and\nhistorical knowledge of China.",
      "tldr_zh": "本研究评估了 Google Translate 在将普通话中文翻译成英语时的质量，采用 sentiment analysis 和 semantic analysis 作为自动化评估框架。研究者选择鲁迅的经典小说《阿 Q 正传》作为测试文本，通过 Google Translate 进行翻译，并与人类专家翻译进行章节级别的比较分析。结果显示，Google Translate 在语义和情感准确性上不如人类翻译，特别是处理中文传统典故时存在误译问题。这些差异可能源于机器缺乏对中国历史和语境的知识，从而影响了翻译的精确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.04964v2",
      "published_date": "2024-09-08 04:03:55 UTC",
      "updated_date": "2024-09-16 10:00:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:17:37.759581"
    },
    {
      "arxiv_id": "2409.04958v1",
      "title": "DDNet: Deformable Convolution and Dense FPN for Surface Defect Detection in Recycled Books",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Yu",
        "WenJian Wang"
      ],
      "abstract": "Recycled and recirculated books, such as ancient texts and reused textbooks,\nhold significant value in the second-hand goods market, with their worth\nlargely dependent on surface preservation. However, accurately assessing\nsurface defects is challenging due to the wide variations in shape, size, and\nthe often imprecise detection of defects. To address these issues, we propose\nDDNet, an innovative detection model designed to enhance defect localization\nand classification. DDNet introduces a surface defect feature extraction module\nbased on a deformable convolution operator (DC) and a densely connected FPN\nmodule (DFPN). The DC module dynamically adjusts the convolution grid to better\nalign with object contours, capturing subtle shape variations and improving\nboundary delineation and prediction accuracy. Meanwhile, DFPN leverages dense\nskip connections to enhance feature fusion, constructing a hierarchical\nstructure that generates multi-resolution, high-fidelity feature maps, thus\neffectively detecting defects of various sizes. In addition to the model, we\npresent a comprehensive dataset specifically curated for surface defect\ndetection in recycled and recirculated books. This dataset encompasses a\ndiverse range of defect types, shapes, and sizes, making it ideal for\nevaluating the robustness and effectiveness of defect detection models. Through\nextensive evaluations, DDNet achieves precise localization and classification\nof surface defects, recording a mAP value of 46.7% on our proprietary dataset -\nan improvement of 14.2% over the baseline model - demonstrating its superior\ndetection capabilities.",
      "tldr_zh": "本研究针对回收书籍表面缺陷检测的挑战，提出DDNet模型，以提升缺陷定位和分类的准确性。DDNet整合了基于Deformable Convolution (DC)的特征提取模块，该模块动态调整卷积网格，捕捉微妙形状变化并优化边界描绘；同时，引入Dense FPN (DFPN)模块，通过密集跳跃连接增强特征融合，生成多分辨率高保真特征图，从而有效检测各种大小的缺陷。该论文还构建了一个全面数据集，涵盖多样化的缺陷类型、形状和大小；实验结果显示，DDNet在该数据集上实现mAP 46.7%，比基线模型提升14.2%，证明了其在表面缺陷检测中的优越性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.04958v1",
      "published_date": "2024-09-08 03:18:19 UTC",
      "updated_date": "2024-09-08 03:18:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:17:50.486555"
    },
    {
      "arxiv_id": "2409.04953v1",
      "title": "Evaluating Neural Networks Architectures for Spring Reverb Modelling",
      "title_zh": "翻译失败",
      "authors": [
        "Francesco Papaleo",
        "Xavier Lizarraga-Seijas",
        "Frederic Font"
      ],
      "abstract": "Reverberation is a key element in spatial audio perception, historically\nachieved with the use of analogue devices, such as plate and spring reverb, and\nin the last decades with digital signal processing techniques that have allowed\ndifferent approaches for Virtual Analogue Modelling (VAM). The\nelectromechanical functioning of the spring reverb makes it a nonlinear system\nthat is difficult to fully emulate in the digital domain with white-box\nmodelling techniques. In this study, we compare five different neural network\narchitectures, including convolutional and recurrent models, to assess their\neffectiveness in replicating the characteristics of this audio effect. The\nevaluation is conducted on two datasets at sampling rates of 16 kHz and 48 kHz.\nThis paper specifically focuses on neural audio architectures that offer\nparametric control, aiming to advance the boundaries of current black-box\nmodelling techniques in the domain of spring reverberation.",
      "tldr_zh": "本研究评估了多种神经网络架构（neural network architectures）在模拟弹簧混响（spring reverb）方面的有效性，针对其作为非线性系统的建模挑战。研究比较了五种架构，包括卷积模型（convolutional models）和循环模型（recurrent models），并在16 kHz 和48 kHz 采样率的两组数据集上进行测试。结果显示，这些神经音频架构提供了参数控制（parametric control），有助于推进弹簧混响的黑盒建模技术（black-box modelling techniques）。",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "8 pages, 7 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.04953v1",
      "published_date": "2024-09-08 02:37:42 UTC",
      "updated_date": "2024-09-08 02:37:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:18:02.203956"
    },
    {
      "arxiv_id": "2409.04949v1",
      "title": "Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings",
      "title_zh": "基于注意力的高效呼吸声去除在录音棚音频录制中",
      "authors": [
        "Nidula Elgiriyewithana",
        "N. D. Kodikara"
      ],
      "abstract": "In this research, we present an innovative, parameter-efficient model that\nutilizes the attention U-Net architecture for the automatic detection and\neradication of non-speech vocal sounds, specifically breath sounds, in vocal\nrecordings. This task is of paramount importance in the field of sound\nengineering, despite being relatively under-explored. The conventional manual\nprocess for detecting and eliminating these sounds requires significant\nexpertise and is extremely time-intensive. Existing automated detection and\nremoval methods often fall short in terms of efficiency and precision. Our\nproposed model addresses these limitations by offering a streamlined process\nand superior accuracy, achieved through the application of advanced deep\nlearning techniques. A unique dataset, derived from Device and Produced Speech\n(DAPS), was employed for this purpose. The training phase of the model\nemphasizes a log spectrogram and integrates an early stopping mechanism to\nprevent overfitting. Our model not only conserves precious time for sound\nengineers but also enhances the quality and consistency of audio production.\nThis constitutes a significant breakthrough, as evidenced by its comparative\nefficiency, necessitating only 1.9M parameters and a training duration of 3.2\nhours - markedly less than the top-performing models in this domain. The model\nis capable of generating identical outputs as previous models with drastically\nimproved precision, making it an optimal choice.",
      "tldr_zh": "本研究提出了一种基于 attention U-Net 架构的参数高效模型，用于自动检测和去除录音中的呼吸声，从而提升录音质量。该模型解决了传统手动处理耗时且依赖专家的问题，以及现有自动方法在效率和精度上的不足，通过采用 log spectrogram 和早停机制进行训练，并利用从 DAPS 数据集派生出的独特数据集。实验结果显示，该模型仅需 1.9M 参数和 3.2 小时训练时间，即可实现与顶级模型相当的输出精度，但大幅提高了准确性和整体效率，为声学工程领域提供了更可靠的自动化解决方案。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.04949v1",
      "published_date": "2024-09-08 02:11:33 UTC",
      "updated_date": "2024-09-08 02:11:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:18:14.592953"
    },
    {
      "arxiv_id": "2409.07489v2",
      "title": "RAGent: Retrieval-based Access Control Policy Generation",
      "title_zh": "RAGent：基于检索的访问控制策略生成",
      "authors": [
        "Sakuna Harinda Jayasundara",
        "Nalin Asanka Gamagedara Arachchilage",
        "Giovanni Russello"
      ],
      "abstract": "Manually generating access control policies from an organization's high-level\nrequirement specifications poses significant challenges. It requires laborious\nefforts to sift through multiple documents containing such specifications and\ntranslate their access requirements into access control policies. Also, the\ncomplexities and ambiguities of these specifications often result in errors by\nsystem administrators during the translation process, leading to data breaches.\nHowever, the automated policy generation frameworks designed to help\nadministrators in this process are unreliable due to limitations, such as the\nlack of domain adaptation. Therefore, to improve the reliability of access\ncontrol policy generation, we propose RAGent, a novel retrieval-based access\ncontrol policy generation framework based on language models. RAGent identifies\naccess requirements from high-level requirement specifications with an average\nstate-of-the-art F1 score of 87.9%. Through retrieval augmented generation,\nRAGent then translates the identified access requirements into access control\npolicies with an F1 score of 77.9%. Unlike existing frameworks, RAGent\ngenerates policies with complex components like purposes and conditions, in\naddition to subjects, actions, and resources. Moreover, RAGent automatically\nverifies the generated policies and iteratively refines them through a novel\nverification-refinement mechanism, further improving the reliability of the\nprocess by 3%, reaching the F1 score of 80.6%. We also introduce three\nannotated datasets for developing access control policy generation frameworks\nin the future, addressing the data scarcity of the domain.",
      "tldr_zh": "该研究解决了手动生成访问控制策略的挑战，包括劳动密集和易出错问题，提出了一种基于语言模型的 RAGent 框架，利用 Retrieval Augmented Generation 技术来自动化该过程。RAGent 从高层需求规范中识别访问要求，平均 F1 score 达到 87.9%，并将这些要求转化为包括 subjects, actions, resources, purposes 和 conditions 等复杂组件的访问控制策略，F1 score 为 77.9%。框架还引入了一个创新的验证-精炼机制，通过迭代优化将策略可靠性提升 3%，达到 F1 score 80.6%。此外，论文提供了三个新的注释数据集，以缓解该领域的训练数据稀缺问题，支持未来研究。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Submitted to Usenix 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.07489v2",
      "published_date": "2024-09-08 00:23:37 UTC",
      "updated_date": "2024-09-13 08:26:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T22:18:27.717756"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 32,
  "processed_papers_count": 32,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T22:18:47.168716"
}
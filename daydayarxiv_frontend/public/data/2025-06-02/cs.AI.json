{
  "date": "2025-06-02",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-02 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv è®ºæ–‡äº•å–·å¼çˆ†å‘ï¼ˆè¿‘200ç¯‡ï¼‰ï¼Œ**Agentic AIï¼ˆæ™ºèƒ½ä½“ AIï¼‰** æ˜¯ç»å¯¹çš„ä¸»è§’ï¼Œä» NVIDIA æ¿€è¿›åœ°æå‡ºâ€œå°æ¨¡å‹ï¼ˆSLMsï¼‰æ‰æ˜¯ Agent çš„æœªæ¥â€ï¼Œåˆ°å¯¹â€œAgenticâ€è¿™ä¸€æœ¯è¯­çš„ç†è®ºåæ€ï¼Œå†åˆ°å„ç±» GUI å’Œä¼ä¸šçº§ Agent çš„è½åœ°ï¼Œæ˜¾ç¤ºå‡ºè¯¥é¢†åŸŸçš„æé«˜çƒ­åº¦ã€‚æ­¤å¤–ï¼Œ**æ¨ç†èƒ½åŠ›ï¼ˆReasoningï¼‰** çš„å¢å¼ºä¾ç„¶æ˜¯æ ¸å¿ƒï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè‡ªæˆ‘éªŒè¯ï¼ˆSelf-Verificationï¼‰æ¥æå‡ Test-time compute çš„ scaling law å¤‡å—å…³æ³¨ã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹è®ºæ–‡ï¼šAgentic AI çš„æœªæ¥ä¸åæ€\n\n**1. Small Language Models are the Future of Agentic AI**\n**å°è¯­è¨€æ¨¡å‹æ˜¯ Agentic AI çš„æœªæ¥**\n> **Authors:** Peter Belcak, et al. (NVIDIA)\n> **Keywords:** SLMs, Agentic AI, Efficiency\n\n**TLDR:** NVIDIA å›¢é˜Ÿå‘è¡¨äº†ä¸€ç¯‡ç«‹åœºé²œæ˜çš„æ–‡ç« ï¼Œè®¤ä¸ºåœ¨ Agentic ç³»ç»Ÿä¸­ï¼Œ**å°è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰** æ¯” LLMs æ›´å…·ä¼˜åŠ¿ã€‚\n**æ·±åº¦è§£è¯»:** ä½œè€…è®¤ä¸ºï¼Œè™½ç„¶ LLM æ“…é•¿é€šç”¨å¯¹è¯ï¼Œä½† Agentic ç³»ç»Ÿé€šå¸¸éœ€è¦é‡å¤æ‰§è¡Œç‰¹å®šçš„ã€å˜åŒ–è¾ƒå°çš„ä»»åŠ¡ã€‚SLMs åœ¨ç»æµæ€§ã€å»¶è¿Ÿå’Œéƒ¨ç½²çµæ´»æ€§ä¸Šå®Œèƒœã€‚ä»–ä»¬æå‡ºï¼Œæœªæ¥çš„ç³»ç»Ÿåº”è¯¥æ˜¯ **å¼‚æ„çš„ï¼ˆHeterogeneousï¼‰**ï¼Œå³ç”±å¤šä¸ªä¸“ç”¨çš„å°æ¨¡å‹åä½œï¼Œä»…åœ¨éœ€è¦é€šç”¨å¯¹è¯æ—¶æ‰è°ƒç”¨å¤§æ¨¡å‹ã€‚\n\n**2. Agentic AI and Multiagentic: Are We Reinventing the Wheel?**\n**Agentic AI ä¸ Multiagenticï¼šæˆ‘ä»¬åœ¨é‡å¤é€ è½®å­å—ï¼Ÿ**\n> **Authors:** V. Botti\n> **Keywords:** Multi-Agent Systems, Terminology, Critique\n\n**TLDR:** è¿™æ˜¯ä¸€ç¯‡æ³¼å†·æ°´çš„æ–‡ç« ã€‚ä½œè€…æ‰¹è¯„äº†å½“å‰ **\"Agentic AI\"** å’Œ **\"Multiagentic\"** æœ¯è¯­çš„æ»¥ç”¨ã€‚\n**æ·±åº¦è§£è¯»:** æ–‡ç« æŒ‡å‡ºï¼Œç°åœ¨çš„å¾ˆå¤šç ”ç©¶å¿½è§†äº†è¿‡å»å‡ åå¹´åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ã€BDI æ¶æ„ï¼ˆBelief-Desire-Intentionï¼‰ä¸­çš„ç§¯ç´¯ã€‚ç°åœ¨çš„ LLM-based Agent å¾ˆå¤šæ—¶å€™åªæ˜¯æŠŠè€æ¦‚å¿µæ¢äº†ä¸ªæ–°åå­—ã€‚ä½œè€…å‘¼åç ”ç©¶è€…ä¸è¦å¿½è§†ç»å…¸çš„ MAS ç†è®ºï¼ˆå¦‚åå•†ã€ä¿¡ä»»ã€ç»„ç»‡ç†è®ºï¼‰ï¼Œä»¥å…â€œé‡æ–°å‘æ˜è½®å­â€ã€‚\n\n**3. FinRobot: Generative Business Process AI Agents for Enterprise Resource Planning in Finance**\n**FinRobotï¼šç”¨äºé‡‘è ERP çš„ç”Ÿæˆå¼ä¸šåŠ¡æµç¨‹ AI æ™ºèƒ½ä½“**\n> **Authors:** Hongyang Yang, et al.\n> **Keywords:** ERP, Finance, Multi-agent Orchestration\n\n**TLDR:** ç¬¬ä¸€ä¸ªé’ˆå¯¹ ERP ç³»ç»Ÿçš„ AI-native æ™ºèƒ½ä½“æ¡†æ¶ã€‚\n**æ·±åº¦è§£è¯»:** æå‡ºäº† **GBPAsï¼ˆGenerative Business Process AI Agentsï¼‰**ï¼Œèƒ½å¤Ÿå®æ—¶åˆæˆå·¥ä½œæµå¹¶åè°ƒä¸“é—¨çš„å­æ™ºèƒ½ä½“æ¥æ‰§è¡Œå¤æ‚çš„é‡‘èä»»åŠ¡ï¼ˆå¦‚é¢„ç®—è§„åˆ’ã€è½¬è´¦ï¼‰ã€‚åœ¨é“¶è¡Œè½¬è´¦åœºæ™¯ä¸­ï¼Œå¤„ç†æ—¶é—´å‡å°‘äº† 40%ï¼Œé”™è¯¯ç‡é™ä½äº† 94%ã€‚\n\n---\n\n### ğŸ§  æ¨ç†ä¸å¼ºåŒ–å­¦ä¹  (Reasoning & RL)\n\n**4. Incentivizing LLMs to Self-Verify Their Answers**\n**æ¿€åŠ± LLM è‡ªæˆ‘éªŒè¯å…¶ç­”æ¡ˆ**\n> **Authors:** Fuxiang Zhang, et al.\n> **Keywords:** Self-Verification, RL, Test-time Scaling\n\n**TLDR:** æå‡ºäº†ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»Ÿä¸€ç­”æ¡ˆç”Ÿæˆå’ŒéªŒè¯çš„æ¡†æ¶ï¼Œä½¿æ¨¡å‹å…·å¤‡**è‡ªæˆ‘éªŒè¯ï¼ˆSelf-Verificationï¼‰** èƒ½åŠ›ã€‚\n**æ·±åº¦è§£è¯»:** ç°æœ‰çš„ Test-time scaling å¾€å¾€ä¾èµ–å¤–éƒ¨ Reward Modelï¼Œä½†è¿™æœ‰åˆ†å¸ƒå·®å¼‚é—®é¢˜ã€‚æœ¬æ–‡çš„æ–¹æ³•è®©æ¨¡å‹è‡ªå·±è¯„ä¼°è‡ªå·±ç”Ÿæˆçš„è§£ï¼Œæ— éœ€å¤–éƒ¨éªŒè¯å™¨å³å¯åœ¨æ¨ç†æ—¶é€šè¿‡éªŒè¯æ¥æå‡æ€§èƒ½ï¼ˆScaling test-time computeï¼‰ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šæ•ˆæœæ˜¾è‘—ã€‚\n\n**5. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning**\n**è¶…è¶Š 80/20 æ³•åˆ™ï¼šé«˜ç†µå°‘æ•° Token é©±åŠ¨ LLM æ¨ç†çš„æœ‰æ•ˆå¼ºåŒ–å­¦ä¹ **\n> **Authors:** Shenzhi Wang, et al. (Alibaba & Tsinghua)\n> **Keywords:** RLVR, Token Entropy, Reasoning\n\n**TLDR:** å‘ç° **RLVRï¼ˆå¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼‰** çš„æœ‰æ•ˆæ€§ä¸»è¦æ¥è‡ªäºä¼˜åŒ–å°‘æ•° **é«˜ç†µ Tokenï¼ˆHigh-Entropy Tokensï¼‰**ã€‚\n**æ·±åº¦è§£è¯»:** ç ”ç©¶è¡¨æ˜ï¼Œåªæœ‰æå°‘é‡çš„ Tokenï¼ˆå†³å®šæ¨ç†è·¯å¾„çš„åˆ†å‰ç‚¹ï¼‰æ˜¯é«˜ç†µçš„ã€‚ä»…å¯¹è¿™ **20%** çš„ Token è¿›è¡Œç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œå°±èƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡å…¨é‡æ›´æ–°çš„æ•ˆæœï¼ˆåœ¨ Qwen3-32B ä¸Šæå‡æ˜¾è‘—ï¼‰ã€‚è¿™ä¸ºç†è§£ CoT æ¨ç†ä¸­çš„å…³é”®å†³ç­–ç‚¹æä¾›äº†æ–°è§†è§’ã€‚\n\n**6. ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code**\n**ResearchCodeBenchï¼šè¯„ä¼° LLM å®ç°å‰æ²¿ ML ç ”ç©¶ä»£ç çš„èƒ½åŠ›**\n> **Authors:** Tianyu Hua, et al.\n> **Keywords:** Code Generation, Research Implementation\n\n**TLDR:** è¿™æ˜¯ä¸€ä¸ªéå¸¸ç¡¬æ ¸çš„ Benchmarkï¼Œæµ‹è¯• LLM èƒ½å¦æ ¹æ® 2024-2025 å¹´æœ€æ–°çš„è®ºæ–‡å¤ç°ä»£ç ã€‚\n**æ·±åº¦è§£è¯»:** ç»“æœå¾ˆæ®‹é…·ï¼Œå³ä½¿æ˜¯æœ€å¼ºçš„æ¨¡å‹ï¼ˆGemini-1.5-Pro ç­‰ï¼‰æˆåŠŸç‡ä¹Ÿä¸åˆ° 40%ã€‚è¿™è¯´æ˜è®© AI ç‹¬ç«‹å½“â€œç ”ç©¶å‘˜â€å¤ç°æ–°ç®—æ³•ç›®å‰è¿˜å¾ˆéš¾ï¼Œå› ä¸ºè¿™äº› idea åœ¨é¢„è®­ç»ƒæ•°æ®é‡Œæ²¡è§è¿‡ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸äº¤äº’ (Multimodal & Interaction)\n\n**7. RoboEgo System Card: An Omnimodal Model with Native Full Duplexity**\n**RoboEgoï¼šå…·æœ‰åŸç”Ÿå…¨åŒå·¥èƒ½åŠ›çš„å…¨æ¨¡æ€æ¨¡å‹**\n> **Authors:** Yiqun Yao, et al.\n> **Keywords:** Full Duplex, Omnimodal, Real-time\n\n**TLDR:** å®ç°äº† **å…¨åŒå·¥ï¼ˆFull Duplexï¼‰** äº¤äº’ï¼Œç†è®ºå»¶è¿Ÿä»… 80msã€‚\n**æ·±åº¦è§£è¯»:** è¿™æ˜¯ä¸€ä¸ªèƒ½åŒæ—¶å¬ã€çœ‹ã€è¯´ï¼Œå¹¶ä¸”èƒ½è¢«æ‰“æ–­ã€èƒ½å®æ—¶ååº”çš„æ¨¡å‹ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹â€œå¬å®Œå†è¯´â€çš„è½®æ¬¡åƒµç¡¬é—®é¢˜ï¼Œéå¸¸é€‚åˆå…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰åœºæ™¯ã€‚\n\n**8. MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments**\n**MLA-Trustï¼šè¯„ä¼° GUI ç¯å¢ƒä¸­å¤šæ¨¡æ€ LLM æ™ºèƒ½ä½“çš„å¯ä¿¡åº¦**\n> **Authors:** Xiao Yang, et al. (Tsinghua)\n> **Keywords:** GUI Agents, Safety, Privacy\n\n**TLDR:** é’ˆå¯¹ GUI æ™ºèƒ½ä½“ï¼ˆå¦‚æ“ä½œæ‰‹æœº/ç½‘é¡µçš„ Agentï¼‰çš„å®‰å…¨æ€§åŸºå‡†æµ‹è¯•ã€‚\n**æ·±åº¦è§£è¯»:** å‘ç°äº†ä¸€ä¸ªå¯æ€•çš„ç°è±¡ï¼šä»é™æ€ MLLM è½¬å˜ä¸ºäº¤äº’å¼ Agent åï¼Œå®‰å…¨æ€§å¤§å¹…ä¸‹é™ã€‚å¤šæ­¥äº¤äº’ä¼šç§¯ç´¯é£é™©ï¼Œå¯¼è‡´ Agent å¯èƒ½ä¼šæ‰§è¡Œåˆ é™¤æ–‡ä»¶ã€æ³„éœ²éšç§ç­‰å±é™©æ“ä½œï¼Œè€Œè¿™äº›åœ¨å•æ­¥å¯¹è¯ä¸­é€šå¸¸ä¼šè¢«é˜²å¾¡ä½ã€‚\n\n**9. MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping**\n**MINTï¼šåŸºäºå¤šæ¨¡æ€äº¤äº’åˆ†ç»„çš„æŒ‡ä»¤å¾®è°ƒ**\n> **Authors:** Xiaojun Shan, et al.\n> **Keywords:** Multimodal Instruction Tuning, Task Grouping\n\n**TLDR:** å‘ç°ç®€å•å¢åŠ å¤šæ¨¡æ€ä»»åŠ¡æ•°é‡å¹¶ä¸èƒ½æŒç»­æå‡æ€§èƒ½ã€‚\n**æ·±åº¦è§£è¯»:** æå‡ºæ ¹æ®**æ¨¡æ€äº¤äº’ç±»å‹**ï¼ˆå¦‚ï¼šæ˜¯ä¸»è¦é è§†è§‰ã€ä¸»è¦é æ–‡æœ¬ï¼Œè¿˜æ˜¯éœ€è¦ååŒèåˆï¼‰å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„è®­ç»ƒã€‚è¿™ç§ç­–ç•¥æ¯”éšæœºæ··åˆä»»åŠ¡æ•ˆæœæ›´å¥½ï¼Œå¹³è¡¡äº†æ³›åŒ–å’Œä¸“ä¸šåŒ–ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€ä¼¦ç†ä¸æ£€æµ‹\n\n**10. Something Just Like TRuST : Toxicity Recognition of Span and Target**\n**TRuSTï¼šå¤§è§„æ¨¡æ¯’æ€§è¯­è¨€è¯†åˆ«æ•°æ®é›†**\n> **Authors:** Berk Atil, et al.\n> **Keywords:** Toxicity Detection, Dataset\n\n**TLDR:** å‘å¸ƒäº†åŒ…å« 300k æ ‡æ³¨çš„å¤§è§„æ¨¡æ¯’æ€§ï¼ˆToxicï¼‰è¯­è¨€æ•°æ®é›† **TRuST**ã€‚\n**æ·±åº¦è§£è¯»:** é‡æ–°å®šä¹‰äº†æ¯’æ€§çš„æ ‡æ³¨æ–¹æ¡ˆï¼Œå¹¶å‘ç°å¾®è°ƒåçš„ PLMï¼ˆé¢„è®­ç»ƒæ¨¡å‹ï¼‰åœ¨æ£€æµ‹æ¯’æ€§ä¸Šä¾ç„¶ä¼˜äºé€šç”¨çš„ LLMï¼Œä¸”å½“å‰çš„ Reasoning Modelsï¼ˆæ¨ç†æ¨¡å‹ï¼‰åœ¨æ¯’æ€§æ£€æµ‹ä¸Šå¹¶æ²¡æœ‰å¸¦æ¥å¯é çš„æ€§èƒ½æå‡ã€‚\n\n**11. Unraveling Audio Deepfake Origins**\n**æ­ç¤ºéŸ³é¢‘ Deepfake çš„æ¥æº**\n> **Authors:** Ajinkya Kulkarni, et al.\n> **Keywords:** Audio Deepfake, Source Tracing\n\n**TLDR:** ä¸ä»…æ£€æµ‹éŸ³é¢‘çœŸå‡ï¼Œè¿˜èƒ½**è¿½è¸ªæ¥æº**ï¼ˆæ˜¯å“ªä¸ªç³»ç»Ÿç”Ÿæˆçš„ï¼‰ã€‚\n**æ·±åº¦è§£è¯»:** ç»“åˆäº† Conformer ç½‘ç»œå’Œåº¦é‡å­¦ä¹ ï¼Œèƒ½åœ¨åŒºåˆ†çœŸå‡çš„åŒæ—¶ï¼Œè¯†åˆ«å‡ºç”Ÿæˆè¯¥éŸ³é¢‘çš„å…·ä½“æ¨¡å‹æ¶æ„ï¼Œè¿™å¯¹å–è¯éå¸¸é‡è¦ã€‚\n\n---\n\n### ğŸ® æ¸¸æˆä¸ç§‘å­¦ (Games & Science)\n\n**12. General search techniques without common knowledge for imperfect-information games, and application to superhuman Fog of War chess**\n**éå®Œç¾ä¿¡æ¯åšå¼ˆçš„é€šç”¨æœç´¢æŠ€æœ¯ä¸è¶…äººç±»æ°´å¹³çš„è¿·é›¾è±¡æ£‹ AI**\n> **Authors:** Brian Hu Zhang, Tuomas Sandholm\n> **Keywords:** Fog of War Chess, Imperfect Information, Search\n\n**TLDR:** æå‡ºäº† **Obscuro**ï¼Œç¬¬ä¸€ä¸ªåœ¨**è¿·é›¾è±¡æ£‹ï¼ˆFog of War Chessï¼‰** ä¸­è¾¾åˆ°è¶…äººç±»æ°´å¹³çš„ AIã€‚\n**æ·±åº¦è§£è¯»:** è¿·é›¾è±¡æ£‹æ¯”å¾·å·æ‰‘å…‹æ›´å¤æ‚ï¼Œå› ä¸ºä¿¡æ¯é›†æ›´å¤§ã€‚ä½œè€…æ”¹è¿›äº†éå®Œç¾ä¿¡æ¯åšå¼ˆçš„æœç´¢ç®—æ³•ï¼Œè¿™å¯¹äºç°å®ä¸–ç•Œä¸­å­˜åœ¨ä¿¡æ¯ä¸å¯¹ç§°çš„å†³ç­–é—®é¢˜ï¼ˆå¦‚å•†ä¸šã€å†›äº‹ï¼‰æœ‰é‡è¦æ„ä¹‰ã€‚\n\n**13. DeepSeek in Healthcare: A Survey**\n**DeepSeek åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨è°ƒç ”**\n> **Authors:** Jiancheng Ye, et al.\n> **Keywords:** DeepSeek-R1, Healthcare, Open Source\n\n**TLDR:** ä¸“é—¨é’ˆå¯¹ **DeepSeek-R1** åœ¨åŒ»ç–—é¢†åŸŸèƒ½åŠ›çš„ç»¼è¿°ã€‚\n**æ·±åº¦è§£è¯»:** ç¡®è®¤ DeepSeek-R1 åœ¨ç»“æ„åŒ–é—®é¢˜è§£å†³ï¼ˆå¦‚è¯Šæ–­ã€è¯ç‰©ç ”ç©¶ï¼‰ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œå…·æœ‰æˆæœ¬æ•ˆç›Šï¼Œæ˜¯ GPT-4o çš„æœ‰åŠ›å¼€æºæ›¿ä»£å“ï¼Œä½†åœ¨å¤šè¯­è¨€å’Œå®‰å…¨å¯¹é½æ–¹é¢ä»æœ‰é£é™©ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—å…³æ³¨çš„çŸ­è®¯\n\n*   **[Code] Flow2Code:** æå‡ºäº†ä¸€ä¸ªåŸºäº **æµç¨‹å›¾ï¼ˆFlowchartï¼‰** ç”Ÿæˆä»£ç çš„ Benchmarkï¼Œå‘ç°ç›®å‰çš„ VLM çœ‹æµç¨‹å›¾å†™ä»£ç çš„èƒ½åŠ›è¿˜å¾ˆå¼± (Paper 151)ã€‚\n*   **[Efficiency] ExPrune:** åˆ©ç”¨å‚æ•°çš„ **Exchangeabilityï¼ˆå¯äº¤æ¢æ€§ï¼‰** è¿›è¡ŒåŠ¨æ€å‰ªæï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯å‡å°‘æ¨ç†æ—¶çš„ FLOPs (Paper 24)ã€‚\n*   **[Vision] Visual Sparse Steering (VS2):** ä¸€ç§æ— éœ€è®­ç»ƒå³å¯åœ¨æ¨ç†æ—¶é€šè¿‡ç¨€ç–ç‰¹å¾å‘é‡å¼•å¯¼è§†è§‰æ¨¡å‹çš„æ–¹æ³•ï¼Œæ˜¾è‘—æå‡ Zero-shot åˆ†ç±»æ€§èƒ½ (Paper 188)ã€‚\n*   **[Physics] Physics-Guided Motion Loss:** æŒ‡å‡ºç°åœ¨çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¸¸è¿åç‰©ç†è§„å¾‹ã€‚é€šè¿‡å¼•å…¥é¢‘åŸŸç‰©ç†å…ˆéªŒæŸå¤±ï¼Œè®©ç”Ÿæˆçš„è§†é¢‘ç‰©ä½“è¿åŠ¨æ›´ç¬¦åˆç‰©ç†é€»è¾‘ (Paper 20)ã€‚",
  "papers": [
    {
      "arxiv_id": "2506.02326v2",
      "title": "Something Just Like TRuST : Toxicity Recognition of Span and Target",
      "title_zh": "Something Just Like TRuSTï¼šæœ‰å®³æ–‡æœ¬ç‰‡æ®µä¸æ”»å‡»ç›®æ ‡çš„æ¯’æ€§è¯†åˆ«",
      "authors": [
        "Berk Atil",
        "Namrata Sureddy",
        "Rebecca J. Passonneau"
      ],
      "abstract": "Toxic language includes content that is offensive, abusive, or that promotes harm. Progress in preventing toxic output from large language models (LLMs) is hampered by inconsistent definitions of toxicity. We introduce TRuST, a large-scale dataset that unifies and expands prior resources through a carefully synthesized definition of toxicity, and corresponding annotation scheme. It consists of ~300k annotations, with high-quality human annotation on ~11k. To ensure high-quality, we designed a rigorous, multi-stage human annotation process, and evaluated the diversity of the annotators. Then we benchmarked state-of-the-art LLMs and pre-trained models on three tasks: toxicity detection, identification of the target group, and of toxic words. Our results indicate that fine-tuned PLMs outperform LLMs on the three tasks, and that current reasoning models do not reliably improve performance. TRuST constitutes one of the most comprehensive resources for evaluating and mitigating LLM toxicity, and other research in socially-aware and safer language technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å› æ¯’æ€§å®šä¹‰ä¸ä¸€è‡´è€Œéš¾ä»¥é¢„é˜²æœ‰å®³è¾“å‡ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºTRuSTçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚TRuSTé€šè¿‡åˆæˆçš„æ¯’æ€§å®šä¹‰å’Œæ ‡æ³¨æ–¹æ¡ˆç»Ÿä¸€å¹¶æ‰©å±•äº†ç°æœ‰èµ„æºï¼ŒåŒ…å«çº¦30ä¸‡æ¡æ ‡æ³¨ï¼Œå…¶ä¸­1.1ä¸‡æ¡ç”±ç»è¿‡å¤šå…ƒåŒ–è¯„ä¼°çš„äººå‘˜é€šè¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µæµç¨‹å®Œæˆé«˜è´¨é‡äººå·¥æ ‡æ³¨ã€‚ç ”ç©¶è€…åœ¨æ¯’æ€§æ£€æµ‹(toxicity detection)ã€ç›®æ ‡ç¾¤ä½“è¯†åˆ«(target group identification)å’Œæ¯’æ€§è¯æ±‡è¯†åˆ«(toxic words identification)ä¸‰é¡¹ä»»åŠ¡ä¸Šå¯¹å…ˆè¿›çš„LLMså’Œé¢„è®­ç»ƒæ¨¡å‹(PLMs)è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„PLMsåœ¨ä¸Šè¿°ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºLLMsï¼Œä¸”ç›®å‰çš„æ¨ç†æ¨¡å‹(reasoning models)å¹¶ä¸èƒ½å¯é åœ°æå‡æ€§èƒ½ã€‚TRuSTä¸ºè¯„ä¼°å’Œç¼“è§£LLMæ¯’æ€§ä»¥åŠå¼€å‘æ›´å…·ç¤¾ä¼šæ„è¯†å’Œå®‰å…¨æ€§çš„è¯­è¨€æŠ€æœ¯æä¾›äº†æœ€å…¨é¢çš„èµ„æºä¹‹ä¸€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02326v2",
      "published_date": "2025-06-02 23:48:16 UTC",
      "updated_date": "2026-01-05 21:38:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:06.121691+00:00"
    },
    {
      "arxiv_id": "2506.02323v1",
      "title": "Sensitivity-Aware Density Estimation in Multiple Dimensions",
      "title_zh": "å¤šç»´çµæ•åº¦æ„ŸçŸ¥å¯†åº¦ä¼°è®¡",
      "authors": [
        "Aleix Boquet-Pujadas",
        "Pol del Aguila Pla",
        "Michael Unser"
      ],
      "abstract": "We formulate an optimization problem to estimate probability densities in the context of multidimensional problems that are sampled with uneven probability. It considers detector sensitivity as an heterogeneous density and takes advantage of the computational speed and flexible boundary conditions offered by splines on a grid. We choose to regularize the Hessian of the spline via the nuclear norm to promote sparsity. As a result, the method is spatially adaptive and stable against the choice of the regularization parameter, which plays the role of the bandwidth. We test our computational pipeline on standard densities and provide software. We also present a new approach to PET rebinning as an application of our framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç»´ç©ºé—´ä¸­é‡‡æ ·æ¦‚ç‡ä¸å‡åŒ€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ„ŸçŸ¥çµæ•åº¦(Sensitivity-Aware)çš„æ¦‚ç‡å¯†åº¦ä¼°è®¡ç®—æ³•ã€‚è¯¥æ–¹æ³•å°†æ¢æµ‹å™¨çµæ•åº¦è§†ä¸ºå¼‚æ„å¯†åº¦(heterogeneous density)ï¼Œå¹¶åˆ©ç”¨ç½‘æ ¼ä¸Šçš„æ ·æ¡å‡½æ•°(splines on a grid)æ¥å®ç°é«˜æ•ˆçš„è®¡ç®—é€Ÿåº¦å’Œçµæ´»çš„è¾¹ç•Œæ¡ä»¶ã€‚ä¸ºäº†å¢å¼ºç®—æ³•çš„ç©ºé—´è‡ªé€‚åº”æ€§ï¼Œç ”ç©¶è€…é€šè¿‡æ ¸èŒƒæ•°(nuclear norm)å¯¹æ ·æ¡å‡½æ•°çš„æµ·æ£®çŸ©é˜µ(Hessian)è¿›è¡Œæ­£åˆ™åŒ–ä»¥ä¿ƒè¿›ç¨€ç–æ€§ã€‚è¿™ç§è®¾è®¡ä½¿ç®—æ³•åœ¨æ­£åˆ™åŒ–å‚æ•°ï¼ˆå³å¸¦å®½ bandwidthï¼‰çš„é€‰æ‹©ä¸Šè¡¨ç°å‡ºæé«˜çš„ç¨³å®šæ€§ï¼Œå¹¶èƒ½æ ¹æ®æ•°æ®åˆ†å¸ƒè¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ ‡å‡†å¯†åº¦æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥è®¡ç®—æµç¨‹å¹¶æä¾›äº†è½¯ä»¶æ”¯æŒï¼ŒåŒæ—¶è¿˜å°†è¯¥æ¡†æ¶åº”ç”¨äºæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æ(PET)é‡åˆ†ç®±(rebinning)çš„æ–°æ–¹æ³•ä¸­ï¼Œå±•ç°äº†å…¶å®é™…åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.DS",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02323v1",
      "published_date": "2025-06-02 23:28:49 UTC",
      "updated_date": "2025-06-02 23:28:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:05.973804+00:00"
    },
    {
      "arxiv_id": "2506.02315v1",
      "title": "A Data-Based Architecture for Flight Test without Test Points",
      "title_zh": "ä¸€ç§åŸºäºæ•°æ®çš„æ— è¯•éªŒç‚¹é£è¡Œè¯•éªŒæ¶æ„",
      "authors": [
        "D. Isaiah Harp",
        "Joshua Ott",
        "John Alora",
        "Dylan Asmar"
      ],
      "abstract": "The justification for the \"test point\" derives from the test pilot's obligation to reproduce faithfully the pre-specified conditions of some model prediction. Pilot deviation from those conditions invalidates the model assumptions. Flight test aids have been proposed to increase accuracy on more challenging test points. However, the very existence of databands and tolerances is the problem more fundamental than inadequate pilot skill. We propose a novel approach, which eliminates test points. We start with a high-fidelity digital model of an air vehicle. Instead of using this model to generate a point prediction, we use a machine learning method to produce a reduced-order model (ROM). The ROM has two important properties. First, it can generate a prediction based on any set of conditions the pilot flies. Second, if the test result at those conditions differ from the prediction, the ROM can be updated using the new data. The outcome of flight test is thus a refined ROM at whatever conditions were flown. This ROM in turn updates and validates the high-fidelity model. We present a single example of this \"point-less\" architecture, using T-38C flight test data. We first use a generic aircraft model to build a ROM of longitudinal pitching motion as a hypersurface. We then ingest unconstrained flight test data and use Gaussian Process Regression to update and condition the hypersurface. By proposing a second-order equivalent system for the T-38C, this hypersurface then generates parameters necessary to assess MIL-STD-1797B compliance for longitudinal dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿé£è¡Œè¯•éªŒä¸­ç”±äº test point åˆ¶åº¦å¯¼è‡´çš„ä»»åŠ¡ç²¾åº¦æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€é¢„è®¾æµ‹è¯•ç‚¹çš„åŸºäºæ•°æ®çš„åˆ›æ–°æ¶æ„ã€‚è¯¥æ¶æ„é€šè¿‡é£è¡Œå™¨çš„é«˜ä¿çœŸæ•°å­—æ¨¡å‹ç”Ÿæˆé™é˜¶æ¨¡å‹ (ROM)ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®é£è¡Œå‘˜åœ¨é£è¡Œä¸­äº§ç”Ÿçš„ä»»ä½•å®é™…çŠ¶æ€ç”Ÿæˆé¢„æµ‹ï¼Œä»è€Œæ‘†è„±äº†å¯¹ç²¾ç¡®å¤ç°é¢„è®¾æ¡ä»¶çš„ä¾èµ–ã€‚å½“å®é™…æµ‹è¯•ç»“æœä¸é¢„æµ‹å­˜åœ¨åå·®æ—¶ï¼Œç³»ç»Ÿåˆ©ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•æ›´æ–° ROMï¼Œè¿›è€Œå®ç°å¯¹é«˜ä¿çœŸæ¨¡å‹çš„åŠ¨æ€ä¿®æ­£ä¸éªŒè¯ã€‚ç ”ç©¶ä»¥ T-38C é£æœºçš„é£è¡Œæ•°æ®ä¸ºä¾‹ï¼Œåˆ©ç”¨ Gaussian Process Regression å¤„ç†æ— çº¦æŸé£è¡Œæ•°æ®å¹¶æ›´æ–°è¶…æ›²é¢æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿç”Ÿæˆè¯„ä¼°çºµå‘åŠ¨åŠ›å­¦ MIL-STD-1797B åˆè§„æ€§æ‰€éœ€çš„å…³é”®å‚æ•°ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¤§å¹…æå‡äº†é£è¡Œè¯•éªŒçš„çµæ´»æ€§ï¼Œä¹Ÿä¸ºèˆªç©ºå™¨æ¨¡å‹çš„æŒç»­ä¼˜åŒ–ä¸éªŒè¯æä¾›äº†å…¨æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "The Society of Experimental Test Pilots Annual Symposium, vol. 68th, 2024",
      "pdf_url": "https://arxiv.org/pdf/2506.02315v1",
      "published_date": "2025-06-02 23:05:52 UTC",
      "updated_date": "2025-06-02 23:05:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:48.217406+00:00"
    },
    {
      "arxiv_id": "2506.02314v1",
      "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code",
      "title_zh": "ResearchCodeBenchï¼šå¤§è¯­è¨€æ¨¡å‹å®ç°æ–°é¢–æœºå™¨å­¦ä¹ ç ”ç©¶ä»£ç èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Tianyu Hua",
        "Harper Hua",
        "Violet Xiang",
        "Benjamin Klieger",
        "Sang T. Truong",
        "Weixin Liang",
        "Fan-Yun Sun",
        "Nick Haber"
      ],
      "abstract": "Large language models (LLMs) have shown promise in transforming machine learning research, yet their capability to faithfully implement novel ideas from recent research papers-ideas unseen during pretraining-remains unclear. We introduce ResearchCodeBench, a benchmark of 212 coding challenges that evaluates LLMs' ability to translate cutting-edge ML contributions from top 2024-2025 research papers into executable code. We assessed 30+ proprietary and open-source LLMs, finding that even the best models correctly implement less than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3% success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and 30.8% respectively. We present empirical findings on performance comparison, contamination, and error patterns. By providing a rigorous and community-driven evaluation platform, ResearchCodeBench enables continuous understanding and advancement of LLM-driven innovation in research code generation.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº† ResearchCodeBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 212 ä¸ªç¼–ç¨‹æŒ‘æˆ˜çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) å°† 2024-2025 å¹´é¡¶çº§ç ”ç©¶è®ºæ–‡ä¸­çš„å‰æ²¿æœºå™¨å­¦ä¹  (ML) è´¡çŒ®è½¬åŒ–ä¸ºå¯æ‰§è¡Œä»£ç çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†ç‰¹åˆ«å…³æ³¨æ¨¡å‹åœ¨å¤„ç†é¢„è®­ç»ƒé˜¶æ®µæœªè§è¿‡çš„åˆ›æ–°æƒ³æ³•æ—¶çš„è¡¨ç°ï¼Œä»¥è¡¡é‡å…¶åœ¨å®é™…ç§‘ç ”åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹ 30 å¤šç§é—­æºå’Œå¼€æºæ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯ç›®å‰é¡¶å°–çš„æ¨¡å‹ï¼Œå…¶ä»£ç å®ç°çš„æ­£ç¡®ç‡ä¹Ÿä½äº 40%ã€‚å…¶ä¸­ï¼ŒGemini-2.5-Pro-Preview ä»¥ 37.3% çš„æˆåŠŸç‡ä½å±…æ¦œé¦–ï¼Œè€Œ O3 (High) å’Œ O4-mini (High) åˆ†åˆ«ä»¥ 32.3% å’Œ 30.8% çš„å‡†ç¡®ç‡ç´§éšå…¶åã€‚ç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†ä¸åŒæ¨¡å‹é—´çš„æ€§èƒ½å·®å¼‚ã€æ•°æ®æ±¡æŸ“ (contamination) é—®é¢˜ä»¥åŠå¸¸è§çš„é”™è¯¯æ¨¡å¼ (error patterns)ã€‚é€šè¿‡æä¾›è¿™ä¸€ä¸¥è°¨çš„è¯„ä¼°å¹³å°ï¼ŒResearchCodeBench ä¸ºç†è§£å’Œæ¨è¿› LLM åœ¨è‡ªåŠ¨ç”Ÿæˆç ”ç©¶ä»£ç é¢†åŸŸçš„åˆ›æ–°å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02314v1",
      "published_date": "2025-06-02 23:04:12 UTC",
      "updated_date": "2025-06-02 23:04:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:13.730385+00:00"
    },
    {
      "arxiv_id": "2506.02308v3",
      "title": "MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping",
      "title_zh": "MINTï¼šåŸºäºå¤šæ¨¡æ€äº¤äº’åˆ†ç»„çš„å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ",
      "authors": [
        "Xiaojun Shan",
        "Qi Cao",
        "Xing Han",
        "Haofei Yu",
        "Paul Pu Liang"
      ],
      "abstract": "Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MINTï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€äº¤äº’åˆ†ç»„çš„ç®€å•ä¸”é«˜æ•ˆçš„å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ (Multimodal Instruction Tuning) ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³ç›²ç›®å¢åŠ æŒ‡ä»¤å¾®è°ƒä»»åŠ¡æ•°é‡ä¸ä¸€å®šèƒ½æŒç»­æå‡æ¨¡å‹æ€§èƒ½çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œæ ¹æ®è·¨æ¨¡æ€çš„å…±åŒäº¤äº’ç‰¹å¾å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œä¾‹å¦‚å‘ç°å†—ä½™çš„å…±äº«ä¿¡æ¯ã€ä¼˜å…ˆé€‰æ‹©åŒ…å«ç‹¬ç‰¹ä¿¡æ¯çš„æ¨¡æ€æˆ–é€šè¿‡ååŒèåˆ (Synergistic Fusion) ä»åŒæ¨¡æ€ä¸­å‘ç°æ–°ä¿¡æ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ƒè¿›æ¨¡å‹å­¦ä¹ ç»„å†…å¯è¿ç§»çš„æŠ€èƒ½ã€‚è¿™ç§ç­–ç•¥é€šè¿‡æŠ‘åˆ¶æ¥è‡ªä¸åŒ¹é…ä»»åŠ¡çš„å¹²æ‰°ï¼Œå…‹æœäº†ä»¥å¾€å¤šæ¨¡æ€å­¦ä¹ ä¸­ä»»åŠ¡æ··æ‚å¸¦æ¥çš„æ€§èƒ½ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMINT åœ¨å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä»»åŠ¡åˆ†ç»„åŸºå‡†æ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨æ³›åŒ– (Generalization) ä¸ä¸“ä¸šåŒ– (Specialization) ä¹‹é—´å–å¾—äº†æœ‰æ•ˆçš„å¹³è¡¡ï¼Œä¸ºå¤§è§„æ¨¡å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„æŒ‡ä»¤ä¼˜åŒ–æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02308v3",
      "published_date": "2025-06-02 22:55:23 UTC",
      "updated_date": "2025-06-06 21:40:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:14.437272+00:00"
    },
    {
      "arxiv_id": "2506.03209v1",
      "title": "Predicting Postoperative Stroke in Elderly SICU Patients: An Interpretable Machine Learning Model Using MIMIC Data",
      "title_zh": "è€å¹´ SICU æ‚£è€…æœ¯åå’ä¸­é¢„æµ‹ï¼šåŸºäº MIMIC æ•°æ®çš„ä¸€é¡¹å¯è§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹",
      "authors": [
        "Tinghuan Li",
        "Shuheng Chen",
        "Junyi Fan",
        "Elham Pishgar",
        "Kamiar Alaei",
        "Greg Placencia",
        "Maryam Pishgar"
      ],
      "abstract": "Postoperative stroke remains a critical complication in elderly surgical intensive care unit (SICU) patients, contributing to prolonged hospitalization, elevated healthcare costs, and increased mortality. Accurate early risk stratification is essential to enable timely intervention and improve clinical outcomes. We constructed a combined cohort of 19,085 elderly SICU admissions from the MIMIC-III and MIMIC-IV databases and developed an interpretable machine learning (ML) framework to predict in-hospital stroke using clinical data from the first 24 hours of Intensive Care Unit (ICU) stay. The preprocessing pipeline included removal of high-missingness features, iterative Singular Value Decomposition (SVD) imputation, z-score normalization, one-hot encoding, and class imbalance correction via the Adaptive Synthetic Sampling (ADASYN) algorithm. A two-stage feature selection process-combining Recursive Feature Elimination with Cross-Validation (RFECV) and SHapley Additive exPlanations (SHAP)-reduced the initial 80 variables to 20 clinically informative predictors. Among eight ML models evaluated, CatBoost achieved the best performance with an AUROC of 0.8868 (95% CI: 0.8802--0.8937). SHAP analysis and ablation studies identified prior cerebrovascular disease, serum creatinine, and systolic blood pressure as the most influential risk factors. Our results highlight the potential of interpretable ML approaches to support early detection of postoperative stroke and inform decision-making in perioperative critical care.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤–ç§‘é‡ç—‡ç›‘æŠ¤ç—…æˆ¿ (SICU) è€å¹´æ‚£è€…æœ¯åä¸­é£è¿™ä¸€ä¸¥é‡å¹¶å‘ç—‡ï¼Œæ—¨åœ¨é€šè¿‡æ—©æœŸé£é™©åˆ†å±‚æå‡ä¸´åºŠé¢„åã€‚ç ”ç©¶å›¢é˜Ÿæ•´åˆäº† MIMIC-III å’Œ MIMIC-IV æ•°æ®åº“ä¸­ 19,085 ä¾‹è€å¹´ SICU å…¥é™¢è®°å½•ï¼Œåˆ©ç”¨å…¥ ICU åå‰ 24 å°æ—¶çš„ä¸´åºŠæ•°æ®å¼€å‘äº†ä¸€ä¸ªå¯è§£é‡Šçš„æœºå™¨å­¦ä¹  (ML) é¢„æµ‹æ¡†æ¶ã€‚åœ¨æ•°æ®å¤„ç†ä¸Šï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è¿­ä»£å¥‡å¼‚å€¼åˆ†è§£ (SVD) æ’å€¼ã€ADASYN ç±»åˆ«ä¸å¹³è¡¡æ ¡æ­£ä»¥åŠåŸºäº RFECV å’Œ SHAP çš„ä¸¤é˜¶æ®µç‰¹å¾é€‰æ‹©æŠ€æœ¯ã€‚åœ¨è¯„ä¼°çš„å…«ç§æ¨¡å‹ä¸­ï¼ŒCatBoost è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå…¶å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ (AUROC) è¾¾åˆ° 0.8868ã€‚SHAP åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†æ—¢å¾€è„‘è¡€ç®¡ç–¾ç—… (cerebrovascular disease)ã€è¡€æ¸…è‚Œé… (serum creatinine) å’Œæ”¶ç¼©å‹ (systolic blood pressure) æ˜¯æœ€é‡è¦çš„é£é™©é¢„æµ‹æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¯è§£é‡Šæœºå™¨å­¦ä¹ åœ¨è¾…åŠ©æœ¯åä¸­é£æ—©æœŸé¢„è­¦å’Œä¼˜åŒ–å›´æ‰‹æœ¯æœŸé‡ç—‡ç›‘æŠ¤å†³ç­–æ–¹é¢çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03209v1",
      "published_date": "2025-06-02 22:53:12 UTC",
      "updated_date": "2025-06-02 22:53:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:23.747968+00:00"
    },
    {
      "arxiv_id": "2506.02302v1",
      "title": "Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments",
      "title_zh": "Explain-then-Processï¼šåˆ©ç”¨è¯­æ³•æç¤ºå¢å¼ºè¯­æ³•å¯æ¥å—æ€§åˆ¤å®š",
      "authors": [
        "Russell Scheinberg",
        "Ameeta Agrawal",
        "Amber Shore",
        "So Young Lee"
      ],
      "abstract": "Large language models (LLMs) can explain grammatical rules, yet they often fail to apply those rules when judging sentence acceptability. We present \"grammar prompting\", an explain-then-process paradigm: a large LLM first produces a concise explanation of the relevant syntactic phenomenon, then that explanation is fed back as additional context to the target model -- either an LLM or a smaller language model (SLM) -- before deciding which sentence of a minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this simple prompt design yields substantial improvements over strong baselines across many syntactic phenomena. Feeding an LLM's metalinguistic explanation back to the target model bridges the gap between knowing a rule and using it. On SLMs, grammar prompting alone trims the average LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by 56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight, language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in multilingual settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º grammar prompting çš„â€œå…ˆè§£é‡Šåå¤„ç†â€ (explain-then-process) èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) èƒ½å¤Ÿè§£é‡Šè¯­æ³•è§„åˆ™å´éš¾ä»¥åœ¨å¥å­å¯æ¥å—åº¦åˆ¤æ–­ä¸­å‡†ç¡®åº”ç”¨è¿™äº›è§„åˆ™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨å¤§å‹ LLM ç”Ÿæˆç›¸å…³è¯­æ³•ç°è±¡çš„ç®€æ´è§£é‡Šï¼Œéšåå°†æ­¤è§£é‡Šä½œä¸ºé¢å¤–ä¸Šä¸‹æ–‡åé¦ˆç»™ç›®æ ‡æ¨¡å‹ï¼ˆæ— è®ºæ˜¯ LLM è¿˜æ˜¯æ›´å°è§„æ¨¡çš„è¯­è¨€æ¨¡å‹ SLMï¼‰ï¼Œä»¥è¾…åŠ©å…¶åˆ¤æ–­å¥å­çš„è¯­æ³•æ­£ç¡®æ€§ã€‚åœ¨è‹±è¯­ BLiMPã€ä¸­æ–‡ SLING å’Œä¿„è¯­ RuBLiMP ç­‰å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™ç§ç®€å•çš„æç¤ºè®¾è®¡æ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œæœ‰æ•ˆå¼¥åˆäº†æ¨¡å‹åœ¨â€œæŒæ¡è§„åˆ™â€ä¸â€œä½¿ç”¨è§„åˆ™â€ä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå•çº¯ä½¿ç”¨ grammar prompting å³å¯ç¼©å° SLM ä¸é¡¶çº§ LLM ä¹‹é—´çº¦ 20% çš„å‡†ç¡®ç‡å·®è·ï¼Œè‹¥ç»“åˆé“¾å¼æ€ç»´ (chain-of-thought) æ¨ç†ï¼Œå·®è·å¯è¿›ä¸€æ­¥ç¼©å‡ 56%ã€‚è¿™ç§è½»é‡çº§ä¸”è·¨è¯­è¨€çš„æ–¹æ³•èƒ½å¤Ÿä»¥æä½æˆæœ¬ä½¿ä½æˆæœ¬çš„ SLM åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹è¾¾åˆ°æ¥è¿‘å‰æ²¿ LLM çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.02302v1",
      "published_date": "2025-06-02 22:42:33 UTC",
      "updated_date": "2025-06-02 22:42:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:22.417092+00:00"
    },
    {
      "arxiv_id": "2506.02298v1",
      "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback",
      "title_zh": "LAM SIMULATORï¼šé€šè¿‡åœ¨çº¿æ¢ç´¢ä¸è½¨è¿¹åé¦ˆæ¨è¿›å¤§åŠ¨ä½œæ¨¡å‹è®­ç»ƒçš„æ•°æ®ç”Ÿæˆ",
      "authors": [
        "Thai Hoang",
        "Kung-Hsiang Huang",
        "Shirley Kokane",
        "Jianguo Zhang",
        "Zuxin Liu",
        "Ming Zhu",
        "Jake Grigsby",
        "Tian Lan",
        "Michael S Ryoo",
        "Chien-Sheng Wu",
        "Shelby Heinecke",
        "Huan Wang",
        "Silvio Savarese",
        "Caiming Xiong",
        "Juan Carlos Niebles"
      ],
      "abstract": "Large Action Models (LAMs) for AI Agents offer incredible potential but face challenges due to the need for high-quality training data, especially for multi-steps tasks that involve planning, executing tool calls, and responding to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. Our framework features a dynamic task query generator, an extensive collection of tools, and an interactive environment where Large Language Model (LLM) Agents can call tools and receive real-time feedback. This setup enables LLM Agents to explore and solve tasks autonomously, facilitating the discovery of multiple approaches to tackle any given task. The resulting action trajectory data are then used to create high-quality training datasets for LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena, highlight the effectiveness of LAM SIMULATOR: models trained with self-generated datasets using our framework achieve significant performance gains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR requires minimal human input during dataset creation, highlighting LAM SIMULATOR's efficiency and effectiveness in speeding up development of AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LAM SIMULATOR æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Large Action Models (LAMs) åœ¨å¤„ç†æ¶‰åŠè§„åˆ’ã€å·¥å…·è°ƒç”¨å’Œåé¦ˆå“åº”çš„å¤šæ­¥ä»»åŠ¡æ—¶é¢ä¸´çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†åŠ¨æ€ä»»åŠ¡æŸ¥è¯¢ç”Ÿæˆå™¨ã€ä¸°å¯Œçš„å·¥å…·é›†ä»¥åŠäº¤äº’å¼ç¯å¢ƒï¼Œä½¿ LLM Agents èƒ½å¤Ÿè¿›è¡Œåœ¨çº¿æ¢ç´¢å¹¶è·å–å®æ—¶è½¨è¿¹åé¦ˆï¼Œä»è€Œè‡ªä¸»å‘ç°å®Œæˆä»»åŠ¡çš„å¤šç§è·¯å¾„ã€‚é€šè¿‡è¿™ç§æ–¹å¼ç”Ÿæˆçš„åŠ¨ä½œè½¨è¿¹æ•°æ®è¢«ç”¨äºæ„å»ºé«˜è´¨é‡è®­ç»ƒé›†ï¼Œæå¤§åœ°æå‡äº†æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ ToolBench å’Œ CRMArena ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œåˆ©ç”¨è¯¥æ¡†æ¶è‡ªç”Ÿæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½è¾ƒåŸºçº¿æœ€é«˜æå‡äº† 49.3%ã€‚LAM SIMULATOR åœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­ä»…éœ€æå°‘çš„äººå·¥å¹²é¢„ï¼Œä¸ºåŠ é€Ÿé«˜æ•ˆ AI Agents çš„å¼€å‘æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "LAM Simulator framework for agentic data generation",
      "pdf_url": "https://arxiv.org/pdf/2506.02298v1",
      "published_date": "2025-06-02 22:36:02 UTC",
      "updated_date": "2025-06-02 22:36:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:25.600540+00:00"
    },
    {
      "arxiv_id": "2506.02295v1",
      "title": "QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation",
      "title_zh": "QARI-OCRï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€‚é…çš„é«˜ä¿çœŸé˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬è¯†åˆ«",
      "authors": [
        "Ahmed Wasfy",
        "Omer Nacar",
        "Abdelakreem Elkhateb",
        "Mahmoud Reda",
        "Omar Elshehy",
        "Adel Ammar",
        "Wadii Boulila"
      ],
      "abstract": "The inherent complexities of Arabic script; its cursive nature, diacritical marks (tashkeel), and varied typography, pose persistent challenges for Optical Character Recognition (OCR). We present Qari-OCR, a series of vision-language models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic through iterative fine-tuning on specialized synthetic datasets. Our leading model, QARI v0.2, establishes a new open-source state-of-the-art with a Word Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling of tashkeel, diverse fonts, and document layouts, alongside impressive performance on low-resolution images. Further explorations (QARI v0.3) showcase strong potential for structural document understanding and handwritten text. This work delivers a marked improvement in Arabic OCR accuracy and efficiency, with all models and datasets released to foster further research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­è„šæœ¬çš„å¤æ‚æ€§ï¼Œå¦‚è¿å†™ç‰¹æ€§ã€å˜éŸ³ç¬¦å·(tashkeel)åŠå¤šæ ·çš„æ’ç‰ˆå¯¹å…‰å­¦å­—ç¬¦è¯†åˆ«(OCR)å¸¦æ¥çš„æŒç»­æŒ‘æˆ˜ï¼Œæå‡ºäº†Qari-OCRç³»åˆ—è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥ç³»åˆ—æ¨¡å‹åŸºäºQwen2-VL-2B-Instructæ„å»ºï¼Œé€šè¿‡åœ¨ä¸“é—¨çš„åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè¿­ä»£å¾®è°ƒï¼Œå®ç°äº†é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„æ·±åº¦ä¼˜åŒ–ã€‚é¢†å…ˆæ¨¡å‹QARI v0.2åœ¨å¤„ç†å«æœ‰ä¸°å¯Œå˜éŸ³ç¬¦å·çš„æ–‡æœ¬æ—¶ï¼Œä»¥0.160çš„å•è¯é”™è¯¯ç‡(WER)ã€0.061çš„å­—ç¬¦é”™è¯¯ç‡(CER)å’Œ0.737çš„BLEUå¾—åˆ†åˆ·æ–°äº†å¼€æºé¢†åŸŸçš„æœ€å…ˆè¿›æ°´å¹³(state-of-the-art)ã€‚Qari-OCRåœ¨å¤„ç†tashkeelã€ä¸åŒå­—ä½“å’Œæ–‡æ¡£å¸ƒå±€æ–¹é¢è¡¨ç°å“è¶Šï¼Œä¸”åœ¨ä½åˆ†è¾¨ç‡å›¾åƒè¯†åˆ«ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚åç»­çš„æ¢ç´¢æ€§ç‰ˆæœ¬QARI v0.3è¿›ä¸€æ­¥å±•ç¤ºäº†å…¶åœ¨ç»“æ„åŒ–æ–‡æ¡£ç†è§£å’Œæ‰‹å†™æ–‡æœ¬è¯†åˆ«æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è¯¥å·¥ä½œé€šè¿‡æ˜¾è‘—æå‡é˜¿æ‹‰ä¼¯è¯­OCRçš„å‡†ç¡®æ€§ä¸æ•ˆç‡ï¼Œå¹¶å¼€æºå…¨éƒ¨æ¨¡å‹ä¸æ•°æ®é›†ï¼Œä¸ºè¯¥é¢†åŸŸçš„åç»­ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02295v1",
      "published_date": "2025-06-02 22:21:06 UTC",
      "updated_date": "2025-06-02 22:21:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:34.989039+00:00"
    },
    {
      "arxiv_id": "2506.02285v2",
      "title": "Why Gradients Rapidly Increase Near the End of Training",
      "title_zh": "ä¸ºä»€ä¹ˆæ¢¯åº¦ä¼šåœ¨è®­ç»ƒæœ«æœŸè¿…é€Ÿå¢é•¿",
      "authors": [
        "Aaron Defazio"
      ],
      "abstract": "During long-duration Large Language Model (LLM) training runs the gradient norm increases rapidly near the end of training. In this short note, we show that this increase is due to an unintended interaction between weight decay, normalization layers, and the learning rate schedule. We propose a simple correction that fixes this behavior while also resulting in lower loss values throughout training.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLM)çš„é•¿å‘¨æœŸè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¢¯åº¦èŒƒæ•°(gradient norm)åœ¨æ¥è¿‘ç»“æŸæ—¶è¿…é€Ÿå¢åŠ çš„ç°è±¡ã€‚ä½œè€…é€šè¿‡ç ”ç©¶æ­ç¤ºäº†è¿™ä¸€ç°è±¡æºäºæƒé‡è¡°å‡(weight decay)ã€å½’ä¸€åŒ–å±‚(normalization layers)ä»¥åŠå­¦ä¹ ç‡è°ƒåº¦(learning rate schedule)ä¹‹é—´äº§ç”Ÿçš„éé¢„æœŸç›¸äº’ä½œç”¨ã€‚é’ˆå¯¹è¿™ä¸€å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„ä¿®æ­£æ–¹æ³•ï¼Œæ—¨åœ¨æ¶ˆé™¤è¿™ç§æ¢¯åº¦çš„å¼‚å¸¸æ¿€å¢è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆä¸ä»…æˆåŠŸä¿®å¤äº†è®­ç»ƒåæœŸçš„æ¢¯åº¦å¢é•¿é—®é¢˜ï¼Œè¿˜åœ¨æ•´ä¸ªè®­ç»ƒé˜¶æ®µæ˜¾è‘—é™ä½äº†æŸå¤±å€¼(loss values)ã€‚è¯¥ç ”ç©¶ä¸ºæå‡å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½æä¾›äº†é‡è¦çš„æŠ€æœ¯å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02285v2",
      "published_date": "2025-06-02 21:51:04 UTC",
      "updated_date": "2025-06-09 22:08:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:25:40.491510+00:00"
    },
    {
      "arxiv_id": "2506.02281v2",
      "title": "Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals",
      "title_zh": "è§’åº¦ä¸è¯´è°ï¼šåŸºäºæ¨¡å‹å†…ç”Ÿä¿¡å·å®ç°é«˜æ•ˆè®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Qinsi Wang",
        "Jinghan Ke",
        "Hancheng Ye",
        "Yueqian Lin",
        "Yuzhe Fu",
        "Jianyi Zhang",
        "Kurt Keutzer",
        "Chenfeng Xu",
        "Yiran Chen"
      ],
      "abstract": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed angle concentration that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Code is realsed at https://github.com/wangqinsi1/GAINRL/tree/main.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒ(RFT)ä¸­æ ·æœ¬æ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGAIN-RLçš„åˆ›æ–°æ¡†æ¶ã€‚ç ”ç©¶è€…è¯†åˆ«å‡ºä¸€ç§æ¨¡å‹å†…åœ¨ä¿¡å·â€”â€”è§’åº¦é›†ä¸­åº¦(angle concentration)ï¼Œå¹¶ä»ç†è®ºå’Œå®è¯ä¸Šè¯æ˜äº†Tokenéšè—çŠ¶æ€å‘é‡çš„è§’åº¦åˆ†å¸ƒä¸æ¢¯åº¦ç»“æœä¹‹é—´çš„ç›¸å…³æ€§ã€‚åŸºäºæ­¤å‘ç°ï¼ŒGAIN-RLæ¡†æ¶åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„è§’åº¦é›†ä¸­åº¦ä¿¡å·åœ¨æ¯ä¸ªepochåŠ¨æ€ç­›é€‰è®­ç»ƒæ•°æ®ï¼Œä»è€Œç¡®ä¿æ¢¯åº¦æ›´æ–°çš„æŒç»­é«˜æ•ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGAIN-RLåœ¨å¤šç§æ¨¡å‹è§„æ¨¡åŠæ•°å­¦ã€ç¼–ç¨‹ä»»åŠ¡ä¸Šå®ç°äº†è¶…è¿‡2.5å€çš„è®­ç»ƒåŠ é€Ÿã€‚æ­¤å¤–ï¼Œä¸å…¨é‡æ•°æ®çš„åŸå§‹GRPOç®—æ³•ç›¸æ¯”ï¼ŒGAIN-RLä»…éœ€ä¸€åŠçš„æ•°æ®é‡å³å¯è·å¾—æ›´ä¼˜çš„æ€§èƒ½è¡¨ç°ï¼Œä¸ºå®ç°æ•°æ®é«˜æ•ˆä¸”è®­ç»ƒé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02281v2",
      "published_date": "2025-06-02 21:40:38 UTC",
      "updated_date": "2025-09-27 04:07:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:05.101786+00:00"
    },
    {
      "arxiv_id": "2506.02280v3",
      "title": "The State of Large Language Models for African Languages: Progress and Challenges",
      "title_zh": "éæ´²è¯­è¨€å¤§è¯­è¨€æ¨¡å‹ç°çŠ¶ï¼šè¿›å±•ä¸æŒ‘æˆ˜",
      "authors": [
        "Kedir Yassin Hussen",
        "Walelign Tewabe Sewunetie",
        "Abinew Ali Ayele",
        "Sukairaj Hafiz Imam",
        "Shamsuddeen Hassan Muhammad",
        "Seid Muhie Yimam"
      ],
      "abstract": "Large Language Models (LLMs) are transforming Natural Language Processing (NLP), but their benefits are largely absent for Africa's 2,000 low-resource languages. This paper comparatively analyzes African language coverage across six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs). The evaluation covers language coverage, training sets, technical limitations, script problems, and language modelling roadmaps. The work identifies 42 supported African languages and 23 available public data sets, and it shows a big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are always treated while there is over 98\\% of unsupported African languages. Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are identified while 20 active scripts are neglected. Some of the primary challenges are lack of data, tokenization biases, computational costs being very high, and evaluation issues. These issues demand language standardization, corpus development by the community, and effective adaptation methods for African languages.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éæ´²ä¸¤åƒå¤šç§ä½èµ„æºè¯­è¨€åœ¨è‡ªç„¶è¯­è¨€å¤„ç† (NLP) å˜é©ä¸­çš„æ»åç°çŠ¶ï¼Œå¯¹å…­ç§å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs)ã€å…«ç§å°å‹è¯­è¨€æ¨¡å‹ (SLMs) åŠå…­ç§ä¸“ç”¨å°å‹è¯­è¨€æ¨¡å‹ (SSLMs) è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚è°ƒæŸ¥å‘ç°å½“å‰ä»…æ”¯æŒ 42 ç§éæ´²è¯­è¨€ï¼Œä¸”èµ„æºé«˜åº¦é›†ä¸­åœ¨ Amharicã€Swahiliã€Afrikaans å’Œ Malagasy ç­‰å°‘æ•°è¯­è¨€ä¸­ï¼Œå¯¼è‡´è¶…è¿‡ 98% çš„éæ´²è¯­è¨€å¤„äºæœªè¢«æ”¯æŒçš„çŠ¶æ€ã€‚æ­¤å¤–ï¼Œç°æœ‰æ¨¡å‹ä»…èƒ½è¯†åˆ« Latinã€Arabic å’Œ Ge'ez è„šæœ¬ï¼Œè€Œå…¶ä»– 20 ç§æ´»è·ƒè„šæœ¬è¢«ä¸¥é‡å¿½è§†ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†æ•°æ®åŒ®ä¹ã€åˆ†è¯åå·® (Tokenization biases)ã€è®¡ç®—æˆæœ¬é«˜æ˜‚ä»¥åŠè¯„ä¼°ä½“ç³»ç¼ºå¤±æ˜¯å½“å‰é¢ä¸´çš„ä¸»è¦æŠ€æœ¯æŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…æå‡ºåº”é€šè¿‡è¯­è¨€æ ‡å‡†åŒ–ã€ç¤¾åŒºé©±åŠ¨çš„è¯­æ–™åº“ (Corpus) å¼€å‘ä»¥åŠé«˜æ•ˆçš„é€‚é…æ–¹æ³•æ¥æ¨åŠ¨éæ´²è¯­è¨€åœ¨ AI é¢†åŸŸçš„å‘å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02280v3",
      "published_date": "2025-06-02 21:39:40 UTC",
      "updated_date": "2025-06-26 01:01:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:08.683933+00:00"
    },
    {
      "arxiv_id": "2506.03207v1",
      "title": "Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning",
      "title_zh": "è”é‚¦å­¦ä¹ ä¸­åŸºäºç½‘ç»œæµé‡æ¨¡å¼çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æŒ‡çº¹è¯†åˆ«",
      "authors": [
        "Md Nahid Hasan Shuvo",
        "Moinul Hossain"
      ],
      "abstract": "Federated Learning (FL) is increasingly adopted as a decentralized machine learning paradigm due to its capability to preserve data privacy by training models without centralizing user data. However, FL is susceptible to indirect privacy breaches via network traffic analysis-an area not explored in existing research. The primary objective of this research is to study the feasibility of fingerprinting deep learning models deployed within FL environments by analyzing their network-layer traffic information. In this paper, we conduct an experimental evaluation using various deep learning architectures (i.e., CNN, RNN) within a federated learning testbed. We utilize machine learning algorithms, including Support Vector Machines (SVM), Random Forest, and Gradient-Boosting, to fingerprint unique patterns within the traffic data. Our experiments show high fingerprinting accuracy, achieving 100% accuracy using Random Forest and around 95.7% accuracy using SVM and Gradient Boosting classifiers. This analysis suggests that we can identify specific architectures running within the subsection of the network traffic. Hence, if an adversary knows about the underlying DL architecture, they can exploit that information and conduct targeted attacks. These findings suggest a notable security vulnerability in FL systems and the necessity of strengthening it at the network level.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨ Federated Learning (FL) ç¯å¢ƒä¸­ï¼Œé€šè¿‡åˆ†æç½‘ç»œå±‚æµé‡ä¿¡æ¯å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œ fingerprinting çš„å¯è¡Œæ€§ï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶åœ¨ç½‘ç»œæµé‡åˆ†æå¼•å‘çš„é—´æ¥éšç§æ³„éœ²é¢†åŸŸçš„ç©ºç™½ã€‚ç ”ç©¶äººå‘˜åœ¨ Federated Learning æµ‹è¯•å¹³å°ä¸Šé’ˆå¯¹ CNN å’Œ RNN ç­‰æ¶æ„è¿›è¡Œäº†å®éªŒï¼Œå¹¶åˆ©ç”¨ Support Vector Machines (SVM)ã€Random Forest å’Œ Gradient-Boosting ç­‰æœºå™¨å­¦ä¹ ç®—æ³•è¯†åˆ«æµé‡ä¸­çš„ç‹¬ç‰¹æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºæé«˜çš„å‡†ç¡®æ€§ï¼Œå…¶ä¸­ Random Forest è¾¾åˆ°äº† 100% çš„è¯†åˆ«ç‡ï¼ŒSVM å’Œ Gradient Boosting çš„å‡†ç¡®ç‡ä¹Ÿæ¥è¿‘ 95.7%ã€‚è¿™ä¸€å‘ç°è¯å®äº†æ”»å‡»è€…èƒ½å¤Ÿé€šè¿‡åˆ†æç½‘ç»œæµé‡è¯†åˆ«ç‰¹å®šçš„æ¨¡å‹æ¶æ„ï¼Œä»è€Œå‘åŠ¨é’ˆå¯¹æ€§æ”»å‡»ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº† Federated Learning ç³»ç»Ÿåœ¨ç½‘ç»œå±‚é¢é¢ä¸´çš„ä¸¥é‡å®‰å…¨æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†æå‡ç½‘ç»œçº§é˜²æŠ¤èƒ½åŠ›çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 4 Figures, Accepted to publish in Proceedings of the 2025 ACM Workshop on Wireless Security and Machine Learning (WiseML 2025), July 3, 2025, Arlington, VA, USA",
      "pdf_url": "https://arxiv.org/pdf/2506.03207v1",
      "published_date": "2025-06-02 21:37:20 UTC",
      "updated_date": "2025-06-02 21:37:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:13.789461+00:00"
    },
    {
      "arxiv_id": "2506.02269v2",
      "title": "A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models",
      "title_zh": "åŒé‡å¯¹ç§°æ€§ï¼šç­‰å˜æ¨¡å‹æŸå¤±æ™¯è§‚ä¹‹æ¢ç©¶",
      "authors": [
        "YuQing Xie",
        "Tess Smidt"
      ],
      "abstract": "Equivariant neural networks have proven to be effective for tasks with known underlying symmetries. However, optimizing equivariant networks can be tricky and best training practices are less established than for standard networks. In particular, recent works have found small training benefits from relaxing equivariance constraints. This raises the question: do equivariance constraints introduce fundamental obstacles to optimization? Or do they simply require different hyperparameter tuning? In this work, we investigate this question through a theoretical analysis of the loss landscape geometry. We focus on networks built using permutation representations, which we can view as a subset of unconstrained MLPs. Importantly, we show that the parameter symmetries of the unconstrained model has nontrivial effects on the loss landscape of the equivariant subspace and under certain conditions can provably prevent learning of the global minima. Further, we empirically demonstrate in such cases, relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly, the weights eventually found via relaxation corresponds to a different choice of group representation in the hidden layer. From this, we draw 3 key takeaways. (1) By viewing the unconstrained version of an architecture, we can uncover hidden parameter symmetries which were broken by choice of constraint enforcement (2) Hidden symmetries give important insights on loss landscapes and can induce critical points and even minima (3) Hidden symmetry induced minima can sometimes be escaped by constraint relaxation and we observe the network jumps to a different choice of constraint enforcement. Effective equivariance relaxation may require rethinking the fixed choice of group representation in the hidden layers.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†ç­‰å˜æ¨¡å‹(Equivariant Models)çš„æŸå¤±æ™¯è§‚(Loss Landscape)ï¼Œæ—¨åœ¨åˆ†æç­‰å˜çº¦æŸæ˜¯å¦å¯¹ç¥ç»ç½‘ç»œä¼˜åŒ–æ„æˆäº†æ ¹æœ¬æ€§éšœç¢ã€‚é€šè¿‡å¯¹åŸºäºç½®æ¢è¡¨ç¤º(Permutation Representations)æ„å»ºçš„ç½‘ç»œè¿›è¡Œç†è®ºåˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†éçº¦æŸæ¨¡å‹çš„å‚æ•°å¯¹ç§°æ€§å¯¹ç­‰å˜å­ç©ºé—´çš„æŸå¤±æ™¯è§‚å…·æœ‰æ˜¾è‘—å½±å“ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ç”šè‡³ä¼šé˜»ç¢å…¨å±€æœ€å°å€¼çš„å­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼Œæ”¾å®½ç­‰å˜çº¦æŸè‡³éçº¦æŸçš„å¤šå±‚æ„ŸçŸ¥æœº(MLPs)æœ‰æ—¶å¯ä»¥è§£å†³æ­¤ç±»ä¼˜åŒ–éš¾é¢˜ï¼Œå¹¶ä½¿ç½‘ç»œè·³è½¬åˆ°éšè—å±‚ä¸­å¦ä¸€ç§ç¾¤è¡¨ç¤º(Group Representation)å¯¹åº”çš„æƒé‡çŠ¶æ€ã€‚è¯¥ç ”ç©¶æ€»ç»“äº†ä¸‰å¤§æ ¸å¿ƒå‘ç°ï¼Œå¼ºè°ƒäº†éšè—å¯¹ç§°æ€§å¯¹è¯±å¯¼ä¸´ç•Œç‚¹å’Œæå°å€¼çš„é‡è¦æ€§ã€‚ç ”ç©¶ç»“æœå»ºè®®ï¼Œä¸ºäº†å®ç°æ›´æœ‰æ•ˆçš„ç­‰å˜æ¾å¼›ï¼Œå¯èƒ½éœ€è¦é‡æ–°æ€è€ƒéšè—å±‚ä¸­å›ºå®šç¾¤è¡¨ç¤ºçš„é€‰æ‹©ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02269v2",
      "published_date": "2025-06-02 21:15:36 UTC",
      "updated_date": "2025-10-31 20:03:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:13.293841+00:00"
    },
    {
      "arxiv_id": "2506.02267v1",
      "title": "TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation",
      "title_zh": "TransAct V2ï¼šPinterest æ¨èä¸­çš„ç»ˆèº«ç”¨æˆ·è¡Œä¸ºåºåˆ—å»ºæ¨¡",
      "authors": [
        "Xue Xia",
        "Saurabh Vishwas Joshi",
        "Kousik Rajesh",
        "Kangnan Li",
        "Yangyi Lu",
        "Nikil Pancha",
        "Dhruvil Deven Badani",
        "Jiajing Xu",
        "Pong Eksombatchai"
      ],
      "abstract": "Modeling user action sequences has become a popular focus in industrial recommendation system research, particularly for Click-Through Rate (CTR) prediction tasks. However, industry-scale CTR models often rely on short user sequences, limiting their ability to capture long-term behavior. Additionally, these models typically lack an integrated action-prediction task within a point-wise ranking framework, reducing their predictive power. They also rarely address the infrastructure challenges involved in efficiently serving large-scale sequential models. In this paper, we introduce TransAct V2, a production model for Pinterest's Homefeed ranking system, featuring three key innovations: (1) leveraging very long user sequences to improve CTR predictions, (2) integrating a Next Action Loss function for enhanced user action forecasting, and (3) employing scalable, low-latency deployment solutions tailored to handle the computational demands of extended user action sequences.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šæ¨èç³»ç»Ÿä¸­ Click-Through Rate (CTR) é¢„æµ‹æ¨¡å‹é€šå¸¸ä»…ä¾èµ–çŸ­æœŸç”¨æˆ·åºåˆ—ã€ç¼ºä¹æ•´åˆçš„è¡Œä¸ºé¢„æµ‹ä»»åŠ¡ä»¥åŠå¤§è§„æ¨¡æ¨¡å‹æ¨ç†æ•ˆç‡ä½ä¸‹ç­‰å±€é™æ€§ï¼Œæå‡ºäº† TransAct V2 æ¡†æ¶ã€‚è¯¥æ¡†æ¶è¢«åº”ç”¨äº Pinterest çš„ Homefeed æ’åºç³»ç»Ÿï¼Œé€šè¿‡åˆ©ç”¨æé•¿ç”¨æˆ·åºåˆ—æ¥å¢å¼ºå¯¹ç”¨æˆ·é•¿æœŸè¡Œä¸ºæ¨¡å¼çš„æ•æ‰ï¼Œä»è€Œæ˜¾è‘—æå‡ CTR é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨æ¨¡å‹è®¾è®¡ä¸Šï¼ŒTransAct V2 åˆ›æ–°æ€§åœ°å¼•å…¥äº† Next Action Loss å‡½æ•°ï¼Œæ—¨åœ¨é€šè¿‡ç‚¹å¯¹ç‚¹æ’åºæ¡†æ¶ (point-wise ranking framework) è¿›ä¸€æ­¥å¼ºåŒ–å¯¹ç”¨æˆ·æœªæ¥è¡Œä¸ºçš„é¢„åˆ¤èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹é•¿åºåˆ—å»ºæ¨¡å¸¦æ¥çš„åŸºç¡€è®¾æ–½æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†å¯æ‰©å±•ä¸”ä½å»¶è¿Ÿçš„éƒ¨ç½²è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿äº†å¤§è§„æ¨¡é¡ºåºæ¨¡å‹åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„é«˜æ•ˆè¿è¡Œã€‚è¿™ä¸€ç ”ç©¶æˆæœæœ‰æ•ˆåœ°è§£å†³äº†å·¥ä¸šçº§é•¿æ•ˆç”¨æˆ·å»ºæ¨¡ä¸­çš„è®¡ç®—ä¸ç®—æ³•ç“¶é¢ˆï¼Œä¸ºå¤§è§„æ¨¡æ¨èç³»ç»Ÿçš„ä¼˜åŒ–æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02267v1",
      "published_date": "2025-06-02 21:15:20 UTC",
      "updated_date": "2025-06-02 21:15:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:17.787094+00:00"
    },
    {
      "arxiv_id": "2506.02262v1",
      "title": "Composable Building Blocks for Controllable and Transparent Interactive AI Systems",
      "title_zh": "é¢å‘å¯æ§ä¸”é€æ˜äº¤äº’å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¯ç»„åˆæ„å»ºå—",
      "authors": [
        "Sebe Vanbrabant",
        "Gustavo Rovelo Ruiz",
        "Davy Vanacken"
      ],
      "abstract": "While the increased integration of AI technologies into interactive systems enables them to solve an equally increasing number of tasks, the black box problem of AI models continues to spread throughout the interactive system as a whole. Explainable AI (XAI) techniques can make AI models more accessible by employing post-hoc methods or transitioning to inherently interpretable models. While this makes individual AI models clearer, the overarching system architecture remains opaque. To this end, we propose an approach to represent interactive systems as sequences of structural building blocks, such as AI models and control mechanisms grounded in the literature. These can then be explained through accompanying visual building blocks, such as XAI techniques. The flow and APIs of the structural building blocks form an explicit overview of the system. This serves as a communication basis for both humans and automated agents like LLMs, aligning human and machine interpretability of AI models. We discuss a selection of building blocks and concretize our flow-based approach in an architecture and accompanying prototype interactive system.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIæŠ€æœ¯åœ¨äº¤äº’ç³»ç»Ÿä¸­å¼•å‘çš„ç³»ç»Ÿçº§â€œé»‘ç›’â€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†äº¤äº’ç³»ç»Ÿè¡¨ç¤ºä¸ºå¯ç»„åˆç»“æ„åŒ–æ„å»ºå—ï¼ˆstructural building blocksï¼‰åºåˆ—çš„æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡æ•´åˆAIæ¨¡å‹å’Œæ§åˆ¶æœºåˆ¶ç­‰ç»“æ„åŒ–ç»„ä»¶ï¼Œå¹¶åˆ©ç”¨è§†è§‰æ„å»ºå—ï¼ˆvisual building blocksï¼‰åŠå¯è§£é‡ŠAIï¼ˆXAIï¼‰æŠ€æœ¯å¯¹å…¶è¿›è¡Œè¾…åŠ©è¯´æ˜ï¼Œä»è€Œæå‡äº†æ•´ä½“æ¶æ„çš„é€æ˜åº¦ã€‚å…¶æ ¸å¿ƒåœ¨äºåˆ©ç”¨æ„å»ºå—çš„APIå’Œæµå¼å¤„ç†ï¼ˆflow-basedï¼‰é€»è¾‘æ„å»ºå‡ºæ¸…æ™°çš„ç³»ç»Ÿå…¨å±€è§†å›¾ï¼Œä¸ºäººç±»å’ŒLLMsç­‰è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“æä¾›äº†ç»Ÿä¸€çš„é€šä¿¡åŸºç¡€ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆå¯¹é½äº†äººç±»ä¸æœºå™¨å¯¹AIæ¨¡å‹çš„è§£é‡Šé€»è¾‘ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„äº¤äº’å¯æ§æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å…·ä½“çš„æ¶æ„è®¾è®¡å’Œäº¤äº’å¼åŸå‹ç³»ç»Ÿï¼ˆprototype interactive systemï¼‰éªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡å¤æ‚AIç³»ç»Ÿé€æ˜åº¦æ–¹é¢çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to The 3rd Workshop on Engineering Interactive Systems Embedding AI Technologies, EICS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02262v1",
      "published_date": "2025-06-02 21:10:51 UTC",
      "updated_date": "2025-06-02 21:10:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:21.690905+00:00"
    },
    {
      "arxiv_id": "2506.02259v1",
      "title": "Stochastically Dominant Peer Prediction",
      "title_zh": "éšæœºå ä¼˜å¯¹ç­‰é¢„æµ‹",
      "authors": [
        "Yichi Zhang",
        "Shengwei Xu",
        "David Pennock",
        "Grant Schoenebeck"
      ],
      "abstract": "Eliciting reliable human feedback is essential for many machine learning tasks, such as learning from noisy labels and aligning AI systems with human preferences. Peer prediction mechanisms incentivize truthful reporting without ground truth verification by scoring agents based on correlations with peers. Traditional mechanisms, which ensure that truth-telling maximizes the expected scores in equilibrium, can elicit honest information while assuming agents' utilities are linear functions of their scores. However, in practice, non-linear payment rules are usually preferred, or agents' utilities are inherently non-linear.\n  We propose stochastically dominant truthfulness (SD-truthfulness) as a stronger guarantee: the score distribution of truth-telling stochastically dominates all other strategies, incentivizing truthful reporting for a wide range of monotone utility functions. Our first observation is that no existing peer prediction mechanism naturally satisfies this criterion without strong assumptions. A simple solution -- rounding scores into binary lotteries -- can enforce SD-truthfulness, but often degrades sensitivity, a key property related to fairness and statistical efficiency. We demonstrate how a more careful application of rounding can better preserve sensitivity. Furthermore, we introduce a new enforced agreement (EA) mechanism that is theoretically guaranteed to be SD-truthful in binary-signal settings under mild assumptions, and empirically achieves the highest sensitivity among all known SD-truthful mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç¼ºä¹å®¢è§‚çœŸå€¼çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•é€šè¿‡åŒä¼´é¢„æµ‹(Peer prediction)æœºåˆ¶æ¿€åŠ±äººç±»æä¾›å¯é åé¦ˆï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿæœºåˆ¶é«˜åº¦ä¾èµ–æ™ºèƒ½ä½“æ•ˆç”¨å‡½æ•°ä¸ºçº¿æ€§çš„å‡è®¾ï¼Œéš¾ä»¥é€‚åº”ç°å®ä¸­éçº¿æ€§çš„æ”¯ä»˜è§„åˆ™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†éšæœºå ä¼˜è¯šå®æ€§(Stochastically dominant truthfulness, SD-truthfulness)è¿™ä¸€æ›´å¼ºçš„æ¿€åŠ±ä¿è¯ï¼Œç¡®ä¿è¯šå®æŠ¥å‘Šçš„å¾—åˆ†åˆ†å¸ƒåœ¨éšæœºæ„ä¹‰ä¸Šä¼˜äºå…¶ä»–ç­–ç•¥ï¼Œä»è€Œé€‚ç”¨äºå„ç§å•è°ƒæ•ˆç”¨å‡½æ•°ã€‚ç ”ç©¶å‘ç°ç°æœ‰æœºåˆ¶åœ¨æ— å¼ºå‡è®¾ä¸‹å‡ä¸æ»¡è¶³æ­¤æ ‡å‡†ï¼Œè™½ç„¶å°†å¾—åˆ†è½¬åŒ–ä¸ºäºŒå…ƒå½©ç¥¨(Binary lotteries)çš„èˆå…¥æ–¹æ³•å¯ä»¥å¼ºåˆ¶å®ç°è¯¥ç‰¹æ€§ï¼Œä½†å¾€å¾€ä¼šæŸå®³ä¸å…¬å¹³æ€§å’Œç»Ÿè®¡æ•ˆç‡ç›¸å…³çš„çµæ•åº¦(Sensitivity)ã€‚è®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ”¹è¿›çš„èˆå…¥åº”ç”¨æ–¹æ³•ä»¥æ›´å¥½åœ°ä¿æŒçµæ•åº¦ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„å¼ºåˆ¶ä¸€è‡´(Enforced agreement, EA)æœºåˆ¶ã€‚ç†è®ºä¸å®éªŒè¡¨æ˜ï¼ŒEAæœºåˆ¶åœ¨äºŒå…ƒä¿¡å·è®¾å®šä¸‹ä¸ä»…èƒ½ä¿è¯SD-truthfulnessï¼Œä¸”åœ¨å·²çŸ¥æ»¡è¶³è¯¥ç‰¹æ€§çš„æœºåˆ¶ä¸­è¡¨ç°å‡ºæœ€é«˜çš„çµæ•åº¦ï¼Œä¸ºå¤§æ¨¡å‹å¯¹é½å’Œå™ªå£°æ ‡ç­¾å­¦ä¹ ç­‰ä»»åŠ¡æä¾›äº†æ›´ç¨³å¥çš„åé¦ˆè·å–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "29 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.02259v1",
      "published_date": "2025-06-02 21:07:24 UTC",
      "updated_date": "2025-06-02 21:07:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:22.484362+00:00"
    },
    {
      "arxiv_id": "2506.02256v1",
      "title": "Human Heterogeneity Invariant Stress Sensing",
      "title_zh": "å…·æœ‰äººç±»å¼‚è´¨æ€§ä¸å˜æ€§çš„å‹åŠ›æ„ŸçŸ¥",
      "authors": [
        "Yi Xiao",
        "Harshit Sharma",
        "Sawinder Kaur",
        "Dessa Bergen-Cico",
        "Asif Salekin"
      ],
      "abstract": "Stress affects physical and mental health, and wearable devices have been widely used to detect daily stress through physiological signals. However, these signals vary due to factors such as individual differences and health conditions, making generalizing machine learning models difficult. To address these challenges, we present Human Heterogeneity Invariant Stress Sensing (HHISS), a domain generalization approach designed to find consistent patterns in stress signals by removing person-specific differences. This helps the model perform more accurately across new people, environments, and stress types not seen during training. Its novelty lies in proposing a novel technique called person-wise sub-network pruning intersection to focus on shared features across individuals, alongside preventing overfitting by leveraging continuous labels while training. The study focuses especially on people with opioid use disorder (OUD)-a group where stress responses can change dramatically depending on their time of daily medication taking. Since stress often triggers cravings, a model that can adapt well to these changes could support better OUD rehabilitation and recovery. We tested HHISS on seven different stress datasets-four of which we collected ourselves and three public ones. Four are from lab setups, one from a controlled real-world setting, driving, and two are from real-world in-the-wild field datasets without any constraints. This is the first study to evaluate how well a stress detection model works across such a wide range of data. Results show HHISS consistently outperformed state-of-the-art baseline methods, proving both effective and practical for real-world use. Ablation studies, empirical justifications, and runtime evaluations confirm HHISS's feasibility and scalability for mobile stress sensing in sensitive real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºHHISSï¼ˆHuman Heterogeneity Invariant Stress Sensingï¼‰çš„åŸŸæ³›åŒ–ï¼ˆDomain Generalizationï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç”±äºä¸ªä½“å·®å¼‚å¯¼è‡´ç”Ÿç†ä¿¡å·å˜åŒ–ï¼Œè¿›è€Œå½±å“å‹åŠ›æ£€æµ‹æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚HHISSçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§åä¸ºâ€œperson-wise sub-network pruning intersectionâ€çš„æŠ€æœ¯æ¥èšç„¦ä¸ªä½“é—´çš„å…±äº«ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¿ç»­æ ‡ç­¾è®­ç»ƒæ¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨äº†é˜¿ç‰‡ç±»è¯ç‰©ä½¿ç”¨éšœç¢ï¼ˆOUDï¼‰ç¾¤ä½“ï¼Œè¯¥ç¾¤ä½“çš„å‹åŠ›ååº”ä¼šéšæœè¯æ—¶é—´å‘ç”Ÿå‰§çƒˆæ³¢åŠ¨ï¼Œè€ŒHHISSèƒ½æœ‰æ•ˆé€‚åº”è¿™ç§æ˜¾è‘—çš„ä¸ªä½“å¼‚è´¨æ€§ã€‚é€šè¿‡åœ¨å®éªŒå®¤ã€å—æ§ç°å®åœºæ™¯åŠé‡å¤–ç¯å¢ƒç­‰7ä¸ªä¸åŒå‹åŠ›æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºHHISSçš„è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰çš„SOTAåŸºçº¿æ–¹æ³•ã€‚å®éªŒè¿›ä¸€æ­¥è¯å®äº†HHISSåœ¨ç§»åŠ¨å‹åŠ›ä¼ æ„Ÿåº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ï¼Œä¸ºç°å®ä¸–ç•Œä¸­æ•æ„Ÿäººç¾¤çš„å‹åŠ›ç›‘æµ‹æä¾›äº†å®ç”¨ä¸”å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02256v1",
      "published_date": "2025-06-02 21:00:00 UTC",
      "updated_date": "2025-06-02 21:00:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:29.003631+00:00"
    },
    {
      "arxiv_id": "2506.03205v1",
      "title": "Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments",
      "title_zh": "Q-ARDNS-Multiï¼šé¢å‘å¤æ‚ 3D ç¯å¢ƒçš„å…·æœ‰å…ƒè®¤çŸ¥è‡ªé€‚åº”çš„å¤šæ™ºèƒ½ä½“é‡å­å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Umberto GonÃ§alves de Sousa"
      ],
      "abstract": "This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum reinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model, where Q-ARDNS-Multi stands for \"Quantum Adaptive Reward-Driven Neural Simulator - Multi-Agent\". It integrates quantum circuits with RY gates, meta-cognitive adaptation, and multi-agent coordination mechanisms for complex 3D environments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action selection, a dual-memory system inspired by human cognition, a shared memory module for agent cooperation, and adaptive exploration strategies modulated by reward variance and intrinsic motivation. Evaluated in a $10 \\times 10 \\times 3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi achieves success rates of 99.6\\% and 99.5\\% for Agents 0 and 1, respectively, outperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft Actor-Critic (SAC) in terms of success rate, stability, navigation efficiency, and collision avoidance. The framework records mean rewards of $-304.2891 \\pm 756.4636$ and $-295.7622 \\pm 752.7103$, averaging 210 steps to goal, demonstrating its robustness in dynamic settings. Comprehensive analyses, including learning curves, reward distributions, statistical tests, and computational efficiency evaluations, highlight the contributions of quantum circuits and meta-cognitive adaptation. By bridging quantum computing, cognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable, human-like approach for applications in robotics, autonomous navigation, and decision-making under uncertainty.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Q-ARDNS-Multiï¼Œä¸€ç§é›†æˆäº†å…ƒè®¤çŸ¥é€‚åº” (Meta-Cognitive Adaptation) çš„å¤šæ™ºèƒ½ä½“é‡å­å¼ºåŒ–å­¦ä¹  (QRL) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤æ‚ 3D ç¯å¢ƒä¸‹çš„å†³ç­–æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ 2-qubit é‡å­ç”µè·¯é…åˆ RY é—¨è¿›è¡ŒåŠ¨ä½œé€‰æ‹©ï¼Œå¹¶å¼•å…¥å—äººç±»è®¤çŸ¥å¯å‘çš„åŒè®°å¿†ç³»ç»Ÿä¸æ™ºèƒ½ä½“å…±äº«è®°å¿†æ¨¡å—ä»¥å¢å¼ºååŒèƒ½åŠ›ã€‚é€šè¿‡å¥–åŠ±æ–¹å·®å’Œå†…åœ¨åŠ¨æœºé©±åŠ¨çš„è‡ªé€‚åº”æ¢ç´¢ç­–ç•¥ï¼ŒQ-ARDNS-Multi åœ¨åŠ¨æ€ç¯å¢ƒä¸­å±•ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ã€‚åœ¨ 10x10x3 çš„ GridWorld ç¯å¢ƒæµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶å®ç°äº† 99.6% çš„æˆåŠŸç‡ï¼Œåœ¨å¯¼èˆªæ•ˆç‡ã€ç¨³å®šæ€§å’Œé¿éšœæ€§èƒ½ä¸Šå‡ä¼˜äº MADDPG å’Œ SAC æ¨¡å‹ã€‚å®éªŒæ•°æ®å’Œç»Ÿè®¡è¯„ä¼°éªŒè¯äº†é‡å­ç”µè·¯ä¸å…ƒè®¤çŸ¥æœºåˆ¶åœ¨æå‡å¤šæ™ºèƒ½ä½“å­¦ä¹ æ€§èƒ½æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶é€šè¿‡èåˆé‡å­è®¡ç®—ã€è®¤çŸ¥ç§‘å­¦ä¸å¼ºåŒ–å­¦ä¹ ï¼Œä¸ºæœºå™¨äººã€è‡ªä¸»å¯¼èˆªåŠå¤æ‚å†³ç­–åœºæ™¯æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.03205v1",
      "published_date": "2025-06-02 20:43:33 UTC",
      "updated_date": "2025-06-02 20:43:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:43.842642+00:00"
    },
    {
      "arxiv_id": "2506.02244v2",
      "title": "Physics-Guided Motion Loss for Video Generation Model",
      "title_zh": "è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†å¼•å¯¼è¿åŠ¨æŸå¤±",
      "authors": [
        "Bowen Xue",
        "Giuseppe Claudio Guarnera",
        "Shuang Zhao",
        "Zahra Montazeri"
      ],
      "abstract": "Current video diffusion models generate visually compelling content but often violate basic laws of physics, producing subtle artifacts like rubber-sheet deformations and inconsistent object motion. We introduce a frequency-domain physics prior that improves motion plausibility without modifying model architectures. Our method decomposes common rigid motions (translation, rotation, scaling) into lightweight spectral losses, requiring only 2.7% of frequency coefficients while preserving 97%+ of spectral energy. Applied to Open-Sora, MVDIT, and Hunyuan, our approach improves both motion accuracy and action recognition by ~11% on average on OpenVID-1M (relative), while maintaining visual quality. User studies show 74--83% preference for our physics-enhanced videos. It also reduces warping error by 22--37% (depending on the backbone) and improves temporal consistency scores. These results indicate that simple, global spectral cues are an effective drop-in regularizer for physically plausible motion in video diffusion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹(Video Diffusion Models)å¸¸è¿èƒŒç‰©ç†å®šå¾‹å¹¶äº§ç”Ÿç‰©ä½“è¿åŠ¨ä¸ä¸€è‡´ç­‰ä¼ªå½±çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢‘åŸŸç‰©ç†å…ˆéªŒ(frequency-domain physics prior)æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹ï¼Œå°†å¹³ç§»ã€æ—‹è½¬å’Œç¼©æ”¾(translation, rotation, scaling)ç­‰åˆšä½“è¿åŠ¨åˆ†è§£ä¸ºè½»é‡çº§çš„é¢‘è°±æŸå¤±(spectral losses)ï¼Œä»…éœ€2.7%çš„é¢‘ç‡ç³»æ•°å³å¯ä¿ç•™è¶…è¿‡97%çš„é¢‘è°±èƒ½é‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åº”ç”¨äº Open-Sora å’Œ Hunyuan ç­‰æ¨¡å‹åï¼Œåœ¨ OpenVID-1M æ•°æ®é›†ä¸Šçš„è¿åŠ¨å‡†ç¡®åº¦å¹³å‡æå‡äº†çº¦ 11%ï¼Œå¹¶æ˜¾è‘—é™ä½äº† 22-37% çš„æ‰­æ›²è¯¯å·®(warping error)ã€‚ç”¨æˆ·ç ”ç©¶æ˜¾ç¤º 74-83% çš„å‚ä¸è€…æ›´å€¾å‘äºè¿™ç§ç‰©ç†å¢å¼ºåçš„è§†é¢‘æ•ˆæœï¼Œè¯æ˜äº†ç®€å•çš„å…¨å±€é¢‘è°±çº¿ç´¢ä½œä¸ºæ­£åˆ™åŒ–é¡¹èƒ½æœ‰æ•ˆæå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02244v2",
      "published_date": "2025-06-02 20:42:54 UTC",
      "updated_date": "2025-09-25 20:44:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:41.208785+00:00"
    },
    {
      "arxiv_id": "2506.02229v1",
      "title": "VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis",
      "title_zh": "VLCDï¼šé¢å‘å‡†ç¡®é«˜æ•ˆè‡ªåŠ¨èƒç›˜åˆ†æçš„è§†è§‰-è¯­è¨€å¯¹æ¯”è’¸é¦",
      "authors": [
        "Manas Mehta",
        "Yimu Pan",
        "Kelly Gallagher",
        "Alison D. Gernand",
        "Jeffery A. Goldstein",
        "Delia Mwinyelle",
        "Leena Mithal",
        "James Z. Wang"
      ],
      "abstract": "Pathological examination of the placenta is an effective method for detecting and mitigating health risks associated with childbirth. Recent advancements in AI have enabled the use of photographs of the placenta and pathology reports for detecting and classifying signs of childbirth-related pathologies. However, existing automated methods are computationally extensive, which limits their deployability. We propose two modifications to vision-language contrastive learning (VLC) frameworks to enhance their accuracy and efficiency: (1) text-anchored vision-language contrastive knowledge distillation (VLCD)-a new knowledge distillation strategy for medical VLC pretraining, and (2) unsupervised predistillation using a large natural images dataset for improved initialization. Our approach distills efficient neural networks that match or surpass the teacher model in performance while achieving model compression and acceleration. Our results showcase the value of unsupervised predistillation in improving the performance and robustness of our approach, specifically for lower-quality images. VLCD serves as an effective way to improve the efficiency and deployability of medical VLC approaches, making AI-based healthcare solutions more accessible, especially in resource-constrained environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰èƒç›˜è‡ªåŠ¨ç—…ç†åˆ†ææ–¹æ³•è®¡ç®—é‡å¤§ã€éš¾ä»¥éƒ¨ç½²çš„é—®é¢˜ï¼Œæå‡ºäº† Vision-Language Contrastive Distillation (VLCD) æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ–‡æœ¬é”šå®šè§†è§‰è¯­è¨€å¯¹æ¯”çŸ¥è¯†è’¸é¦ (text-anchored vision-language contrastive knowledge distillation) ç­–ç•¥ï¼Œä»¥åŠåŸºäºå¤§å‹è‡ªç„¶å›¾åƒæ•°æ®é›†çš„æ— ç›‘ç£é¢„è’¸é¦ (unsupervised predistillation) æ–¹æ³•ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶æˆåŠŸè’¸é¦å‡ºé«˜æ•ˆçš„è½»é‡åŒ–ç¥ç»ç½‘ç»œï¼Œå…¶æ€§èƒ½åœ¨åŒ¹é…æˆ–è¶…è¶Šæ•™å¸ˆæ¨¡å‹çš„åŒæ—¶ï¼Œæ˜¾è‘—å®ç°äº†æ¨¡å‹å‹ç¼©ä¸æ¨ç†åŠ é€Ÿã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜äº†æ— ç›‘ç£é¢„è’¸é¦åœ¨æå‡æ¨¡å‹é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä½è´¨é‡å›¾åƒæ–¹é¢çš„ä»·å€¼ã€‚VLCD ä¸ºåŒ»ç–—é¢†åŸŸçš„è§†è§‰è¯­è¨€å¯¹æ¯”å­¦ä¹  (medical VLC) æä¾›äº†æ›´å…·æ•ˆç‡çš„è·¯å¾„ï¼Œæ˜¾è‘—æå‡äº† AI åŒ»ç–—æ–¹æ¡ˆåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„å¯éƒ¨ç½²æ€§å’Œæ™®åŠæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Proceedings of the 9th International Workshop on Health Intelligence, in conjunction with the Annual AAAI Conference on Artificial Intelligence, Philadelphia, Pennsylvania, March 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02229v1",
      "published_date": "2025-06-02 20:12:27 UTC",
      "updated_date": "2025-06-02 20:12:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:27:10.306744+00:00"
    },
    {
      "arxiv_id": "2506.02212v1",
      "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics",
      "title_zh": "åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æ­ç¤ºç”Ÿå‘½å¥¥ç§˜ï¼šåŸºå› ç»„å­¦ã€è½¬å½•ç»„å­¦åŠè›‹ç™½è´¨ç»„å­¦ä¸­çš„ NLP æ–¹æ³•ç»¼è¿°",
      "authors": [
        "Ella Rannon",
        "David Burstein"
      ],
      "abstract": "Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°ç³»ç»Ÿå›é¡¾äº†è‡ªç„¶è¯­è¨€å¤„ç† (NLP) æŠ€æœ¯åœ¨åŸºå› ç»„å­¦ (Genomics)ã€è½¬å½•ç»„å­¦ (Transcriptomics) å’Œè›‹ç™½è´¨ç»„å­¦ (Proteomics) ç­‰ç”Ÿç‰©åºåˆ—åˆ†æé¢†åŸŸä¸­çš„åº”ç”¨ã€‚ç ”ç©¶æ¶µç›–äº†ä» word2vec ç­‰ä¼ ç»Ÿæ–¹æ³•åˆ° Transformers å’Œ Hyena Operators ç­‰å‰æ²¿æ¨¡å‹ï¼Œè¯¦ç»†æ¢è®¨äº†è¿™äº›æ¶æ„å¦‚ä½•è¢«é€‚é…ç”¨äºåˆ†æ DNAã€RNA åŠè›‹ç™½è´¨åºåˆ—ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº†ä¸åŒæ¨¡å‹çš„åˆ†è¯ç­–ç•¥ (Tokenization) ä¸æ¶æ„ç‰¹å¾ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨ç»“æ„é¢„æµ‹ (Structure Prediction)ã€åŸºå› è¡¨è¾¾ (Gene Expression) åŠæ¼”åŒ–åˆ†æ (Evolutionary Analysis) ç­‰å…·ä½“ç”Ÿç‰©å­¦ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥ç»¼è¿°å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æå–å¤§è§„æ¨¡åŸºå› ç»„æ•°æ®ç‰¹å¾æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ï¼Œå±•ç¤ºäº†å…¶åœ¨æ·±åŒ–ç†è§£ç”Ÿå‘½åŸºæœ¬è¿‡ç¨‹ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºç”Ÿç‰©ä¿¡æ¯å­¦ä¸äººå·¥æ™ºèƒ½çš„äº¤å‰ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02212v1",
      "published_date": "2025-06-02 19:54:03 UTC",
      "updated_date": "2025-06-02 19:54:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:26:59.125163+00:00"
    },
    {
      "arxiv_id": "2506.02211v1",
      "title": "Improving LLM-Generated Code Quality with GRPO",
      "title_zh": "åˆ©ç”¨GRPOæå‡LLMç”Ÿæˆçš„ä»£ç è´¨é‡",
      "authors": [
        "Maxime Robeyns",
        "Laurence Aitchison"
      ],
      "abstract": "Large Language Models (LLMs) are gaining widespread use for code generation. Recent training procedures use execution feedback as a reward signal, typically focusing on the functional correctness of the code, using unit test pass rate as a reward signal. However, this reward signal fails to capture notions of maintainability, quality and safety of the code produced. We address this under-explored area and develop a comprehensive library to quantify various aspects of code quality, and use it as a reward in GRPO. We find GRPO increases code quality according to this measure, which is confirmed by expert, blinded human annotators.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»£ç ç”Ÿæˆè®­ç»ƒä¸»è¦ä¾èµ–åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆFunctional Correctnessï¼‰åé¦ˆï¼Œè€Œéš¾ä»¥æ•æ‰ä»£ç å¯ç»´æŠ¤æ€§ã€è´¨é‡å’Œå®‰å…¨æ€§è¿™ä¸€ç°çŠ¶æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ä½œè€…å¼€å‘äº†ä¸€å¥—ä¸“é—¨ç”¨äºé‡åŒ–ä»£ç è´¨é‡å¤šç»´ç‰¹å¾çš„ç»¼åˆåº“ï¼Œå¹¶å°†å…¶ä½œä¸ºå¥–åŠ±å‡½æ•°åº”ç”¨äº Group Relative Policy Optimization (GRPO) ç®—æ³•ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ GRPO ç»“åˆè´¨é‡å¥–åŠ±ä¿¡å·èƒ½æ˜¾è‘—æå‡ LLM ç”Ÿæˆä»£ç çš„æ•´ä½“è´¨é‡ã€‚è¿™ä¸€ç»“è®ºåœ¨éšåçš„ç›²æµ‹ä¸­å¾—åˆ°äº†äººç±»ä¸“å®¶çš„è¿›ä¸€æ­¥è¯å®ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æå‡è‡ªåŠ¨åŒ–ç¼–ç¨‹æ°´å¹³æ–¹é¢çš„å®é™…ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02211v1",
      "published_date": "2025-06-02 19:50:16 UTC",
      "updated_date": "2025-06-02 19:50:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:27:03.680993+00:00"
    },
    {
      "arxiv_id": "2506.02210v2",
      "title": "Exchangeability in Neural Network and its Application to Dynamic Pruning",
      "title_zh": "ç¥ç»ç½‘ç»œçš„å¯äº¤æ¢æ€§åŠå…¶åœ¨åŠ¨æ€å‰ªæä¸­çš„åº”ç”¨",
      "authors": [
        "Pu",
        "Yi",
        "Tianlang Chen",
        "Yifan Yang",
        "Sara Achour"
      ],
      "abstract": "Modern neural networks (NN) contain an ever-growing number of parameters, substantially increasing the memory and computational cost of inference. Researchers have explored various ways to reduce the inference cost of NNs by reducing the model size before deployment and dynamically pruning the inference computation at runtime. In this work, we present ExPrune, a general, dynamic pruning optimization that enables multi-granularity partial computation on a per-input basis. ExPrune requires no change to the model architecture or the training algorithm. ExPrune is based on our theoretical results that the relationship between certain model parameters and intermediate values can be described by a statistical property called exchangeability. By identifying exchangeable parameters and values in the model, we are able to first partially evaluate the network, analyze the statistics of the partial results, and make pruning decisions on the fly. Because ExPrune is theory grounded, it generalizes across model architectures in different problem domains. We evaluate ExPrune on one computer vision models, one graph model and one language model. ExPrune provides 10.98--17.33% reduction in FLOPs with negligible accuracy drop and 21.61--27.16% reduction in FLOPs with at most 1% accuracy drop. We also demonstrate that ExPrune composes with static magnitude pruning. On models that have been aggressively statically pruned, ExPrune still provides additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and 13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ExPruneï¼Œä¸€ç§é€šç”¨çš„åŠ¨æ€å‰ªæ (Dynamic Pruning) ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¯¹æ¯ä¸ªè¾“å…¥è¿›è¡Œå¤šç²’åº¦éƒ¨åˆ†è®¡ç®—æ¥é™ä½ç¥ç»ç½‘ç»œçš„æ¨ç†æˆæœ¬ã€‚è¯¥æ–¹æ³•å»ºç«‹åœ¨ç¥ç»ç½‘ç»œå‚æ•°ä¸ä¸­é—´å€¼ä¹‹é—´å…·æœ‰å¯äº¤æ¢æ€§ (Exchangeability) è¿™ä¸€ç»Ÿè®¡å­¦ç‰¹æ€§çš„ç†è®ºåŸºç¡€ä¸Šï¼Œä¸”æ— éœ€ä¿®æ”¹åŸæœ‰çš„æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒç®—æ³•ã€‚ExPrune é€šè¿‡è¯†åˆ«æ¨¡å‹ä¸­çš„å¯äº¤æ¢å‚æ•°å¹¶åˆ†æéƒ¨åˆ†æ±‚å€¼çš„ç»Ÿè®¡ç»“æœï¼Œå®ç°äº†è¿è¡Œæ—¶çš„å³æ—¶å‰ªæå†³ç­–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—æœºè§†è§‰ (Computer Vision)ã€å›¾æ¨¡å‹ (Graph Model) å’Œè¯­è¨€æ¨¡å‹ (Language Model) ç­‰ä¸åŒé¢†åŸŸå‡å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œèƒ½å¤Ÿåœ¨ç²¾åº¦æŸå¤±æå°çš„æƒ…å†µä¸‹å‡å°‘ 10.98% è‡³ 17.33% çš„ FLOPsã€‚å½“ç²¾åº¦ä¸‹é™å®¹å¿åº¦æ”¾å®½è‡³ 1% æ—¶ï¼ŒFLOPs çš„å‰Šå‡é‡å¯è¿›ä¸€æ­¥æå‡è‡³ 21.61% è‡³ 27.16%ã€‚æ­¤å¤–ï¼ŒExPrune è¿˜èƒ½ä¸é™æ€å¹…åº¦å‰ªæ (Static Magnitude Pruning) å®Œç¾å…¼å®¹ï¼Œåœ¨å·²è¢«æ¿€è¿›å‰ªæçš„æ¨¡å‹ä¸Šä»èƒ½æä¾›è¶…è¿‡ 10% çš„é¢å¤–è®¡ç®—é‡ç¼©å‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02210v2",
      "published_date": "2025-06-02 19:50:15 UTC",
      "updated_date": "2025-10-07 23:47:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:27:15.649205+00:00"
    },
    {
      "arxiv_id": "2506.02208v1",
      "title": "KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning",
      "title_zh": "KDRLï¼šåŸºäºç»Ÿä¸€çŸ¥è¯†è’¸é¦ä¸å¼ºåŒ–å­¦ä¹ çš„æ¨ç†å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒåä¼˜åŒ–",
      "authors": [
        "Hongling Xu",
        "Qi Zhu",
        "Heyuan Deng",
        "Jinpeng Li",
        "Lu Hou",
        "Yasheng Wang",
        "Lifeng Shang",
        "Ruifeng Xu",
        "Fei Mi"
      ],
      "abstract": "Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \\textbf{KDRL}, a \\textit{unified post-training framework} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KDRLï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è®­ç»ƒåæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆçŸ¥è¯†è’¸é¦ (Knowledge Distillation, KD) å’Œå¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œè™½ç„¶å¼ºåŒ–å­¦ä¹ èƒ½è¯±å‘å¤æ‚çš„æ¨ç†è¡Œä¸ºä½†é‡‡æ ·æ•ˆç‡è¾ƒä½ï¼Œè€ŒçŸ¥è¯†è’¸é¦è™½èƒ½æé«˜å­¦ä¹ æ•ˆç‡ä½†åœ¨é¢†åŸŸå¤–åœºæ™¯çš„æ³›åŒ–æ€§è¾ƒå·®ã€‚KDRL é‡‡ç”¨ç­–ç•¥æ¢¯åº¦ä¼˜åŒ– (Policy Gradient Optimization) ç®—æ³•ï¼Œé€šè¿‡åŒæ—¶æœ€å°åŒ–å­¦ç”Ÿä¸æ•™å¸ˆåˆ†å¸ƒä¹‹é—´çš„åå‘ KL æ•£åº¦ (Reverse KL Divergence) å¹¶æœ€å¤§åŒ–åŸºäºè§„åˆ™çš„é¢„æœŸå¥–åŠ±æ¥å®ç°è”åˆä¼˜åŒ–ã€‚è¯¥æ¡†æ¶æ„å»ºäº†æ•´åˆ GRPO å’ŒçŸ¥è¯†è’¸é¦çš„ç»Ÿä¸€ç›®æ ‡å‡½æ•°ï¼Œå¹¶ç³»ç»Ÿæ€§åœ°æ¢ç´¢äº†ä¸åŒ KL è¿‘ä¼¼ã€ç³»æ•°åŠå¥–åŠ±å¼•å¯¼ç­–ç•¥å¯¹è®­ç»ƒåŠ¨æ€çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKDRL åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äº GRPO å’Œå¤šç§çŸ¥è¯†è’¸é¦åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆçŸ¥è¯†è’¸é¦ä¸å¼ºåŒ–å­¦ä¹ æ˜¯è®­ç»ƒæ¨ç†å‹å¤§è¯­è¨€æ¨¡å‹çš„ä¸€ç§é«˜æ•ˆç­–ç•¥ï¼Œåœ¨æ€§èƒ½è¡¨ç°ä¸æ¨ç† Token æ•ˆç‡ä¹‹é—´è¾¾æˆäº†ç†æƒ³çš„å¹³è¡¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02208v1",
      "published_date": "2025-06-02 19:46:41 UTC",
      "updated_date": "2025-06-02 19:46:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:27:32.840101+00:00"
    },
    {
      "arxiv_id": "2506.02205v2",
      "title": "Bregman Centroid Guided Cross-Entropy Method",
      "title_zh": "Bregman è´¨å¿ƒå¼•å¯¼çš„äº¤å‰ç†µæ–¹æ³•",
      "authors": [
        "Yuliang Gu",
        "Hongpeng Cao",
        "Marco Caccamo",
        "Naira Hovakimyan"
      ],
      "abstract": "The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in model-based reinforcement learning (MBRL), but its unimodal sampling strategy often leads to premature convergence in multimodal landscapes. In this work, we propose Bregman Centroid Guided CEM ($\\mathcal{BC}$-EvoCEM), a lightweight enhancement to ensemble CEM that leverages $\\textit{Bregman centroids}$ for principled information aggregation and diversity control. $\\textbf{$\\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman centroid across CEM workers and updates the least contributing ones by sampling within a trust region around the centroid. Leveraging the duality between Bregman divergences and exponential family distributions, we show that $\\textbf{$\\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM pipelines with negligible overhead. Empirical results on synthetic benchmarks, a cluttered navigation task, and full MBRL pipelines demonstrate that $\\textbf{$\\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution quality, providing a simple yet effective upgrade for CEM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº¤å‰ç†µæ–¹æ³• (Cross-Entropy Method, CEM) åœ¨å¤šå³°æ™¯è§‚ä¸­å®¹æ˜“è¿‡æ—©æ”¶æ•›çš„é—®é¢˜ï¼Œæå‡ºäº† Bregman Centroid Guided CEM ($\\mathcal{BC}$-EvoCEM)ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ Bregman centroids è¿›è¡Œä¿¡æ¯èšåˆå’Œå¤šæ ·æ€§æ§åˆ¶çš„è½»é‡çº§å¢å¼ºæ–¹æ¡ˆã€‚$\\mathcal{BC}$-EvoCEM é€šè¿‡è®¡ç®—å¤šä¸ª CEM å·¥ä½œå•å…ƒä¹‹é—´çš„æ€§èƒ½åŠ æƒ Bregman centroidï¼Œå¹¶åœ¨å…¶ä¿¡ä»»åŒºåŸŸ (trust region) å†…é‡‡æ ·æ¥æ›´æ–°è´¡çŒ®è¾ƒå°çš„å·¥ä½œå•å…ƒï¼Œä»è€Œæœ‰æ•ˆå¼•å¯¼ä¼˜åŒ–è¿‡ç¨‹ã€‚åˆ©ç”¨ Bregman divergences ä¸æŒ‡æ•°æ—åˆ†å¸ƒ (exponential family distributions) ä¹‹é—´çš„å¯¹å¶æ€§ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°æ ‡å‡† CEM æµæ°´çº¿ä¸­ä¸”è®¡ç®—å¼€é”€æä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ$\\mathcal{BC}$-EvoCEM åœ¨åˆæˆåŸºå‡†ã€å¤æ‚å¯¼èˆªä»»åŠ¡ä»¥åŠå®Œæ•´çš„åŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹  (Model-Based Reinforcement Learning, MBRL) æµæ°´çº¿ä¸­å‡æ˜¾è‘—æå‡äº†æ”¶æ•›æ€§èƒ½å’Œè§£çš„è´¨é‡ã€‚è¯¥æ–¹æ³•ä¸ºç°æœ‰çš„è½¨è¿¹ä¼˜åŒ–æŠ€æœ¯æä¾›äº†ä¸€ä¸ªç®€å•ã€é«˜æ•ˆä¸”ç†è®ºå®Œå¤‡çš„å‡çº§è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02205v2",
      "published_date": "2025-06-02 19:44:40 UTC",
      "updated_date": "2025-06-30 20:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:27:13.446237+00:00"
    },
    {
      "arxiv_id": "2506.02203v1",
      "title": "Constrained Sliced Wasserstein Embedding",
      "title_zh": "å—çº¦æŸçš„åˆ‡ç‰‡ Wasserstein åµŒå…¥",
      "authors": [
        "Navid NaderiAlizadeh",
        "Darian Salehi",
        "Xinran Liu",
        "Soheil Kolouri"
      ],
      "abstract": "Sliced Wasserstein (SW) distances offer an efficient method for comparing high-dimensional probability measures by projecting them onto multiple 1-dimensional probability distributions. However, identifying informative slicing directions has proven challenging, often necessitating a large number of slices to achieve desirable performance and thereby increasing computational complexity. We introduce a constrained learning approach to optimize the slicing directions for SW distances. Specifically, we constrain the 1D transport plans to approximate the optimal plan in the original space, ensuring meaningful slicing directions. By leveraging continuous relaxations of these transport plans, we enable a gradient-based primal-dual approach to train the slicer parameters, alongside the remaining model parameters. We demonstrate how this constrained slicing approach can be applied to pool high-dimensional embeddings into fixed-length permutation-invariant representations. Numerical results on foundation models trained on images, point clouds, and protein sequences showcase the efficacy of the proposed constrained learning approach in learning more informative slicing directions. Our implementation code can be found at https://github.com/Stranja572/constrainedswe.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†çº¦æŸåˆ‡ç‰‡ç“¦ç‘Ÿæ–¯å¦åµŒå…¥(Constrained Sliced Wasserstein Embedding)ï¼Œæ—¨åœ¨è§£å†³åˆ‡ç‰‡ç“¦ç‘Ÿæ–¯å¦(Sliced Wasserstein, SW)è·ç¦»åœ¨ç¡®å®šæœ‰ä¿¡æ¯é‡çš„åˆ‡ç‰‡æ–¹å‘æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿæ–¹æ³•å¯¹å¤§é‡åˆ‡ç‰‡çš„ä¾èµ–å¹¶é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§çº¦æŸå­¦ä¹ æ–¹æ³•æ¥ä¼˜åŒ–åˆ‡ç‰‡æ–¹å‘ï¼Œé€šè¿‡çº¦æŸä¸€ç»´ä¼ è¾“è®¡åˆ’(1D transport plans)ä½¿å…¶é€¼è¿‘åŸå§‹é«˜ç»´ç©ºé—´ä¸­çš„æœ€ä¼˜ä¼ è¾“è®¡åˆ’ã€‚ç ”ç©¶åˆ©ç”¨ä¼ è¾“è®¡åˆ’çš„è¿ç»­æ¾å¼›æŠ€æœ¯ï¼Œé‡‡ç”¨åŸºäºæ¢¯åº¦çš„åŸå§‹-å¯¹å¶(primal-dual)æ–¹æ³•æ¥åŒæ­¥è®­ç»ƒåˆ‡ç‰‡å™¨å‚æ•°ä¸æ¨¡å‹å‚æ•°ã€‚è¯¥çº¦æŸåˆ‡ç‰‡æ–¹æ³•è¢«åº”ç”¨äºå°†é«˜ç»´åµŒå…¥æ± åŒ–ä¸ºå›ºå®šé•¿åº¦çš„ç½®æ¢ä¸å˜(permutation-invariant)è¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çš„æ•°æ®ç»“æ„ã€‚åœ¨å›¾åƒã€ç‚¹äº‘å’Œè›‹ç™½è´¨åºåˆ—ç­‰åŸºç¡€æ¨¡å‹ä¸Šçš„æ•°å€¼å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…·ä¿¡æ¯é‡çš„åˆ‡ç‰‡æ–¹å‘ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤„ç†é«˜ç»´æ¦‚ç‡åº¦é‡æ—¶çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "q-bio.QM",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02203v1",
      "published_date": "2025-06-02 19:43:40 UTC",
      "updated_date": "2025-06-02 19:43:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:27:26.435001+00:00"
    },
    {
      "arxiv_id": "2506.02183v2",
      "title": "Natural, Artificial, and Human Intelligences",
      "title_zh": "è‡ªç„¶æ™ºèƒ½ã€äººå·¥æ™ºèƒ½ä¸äººç±»æ™ºèƒ½",
      "authors": [
        "Emmanuel M. Pothos",
        "Dominic Widdows"
      ],
      "abstract": "Human achievement, whether in culture, science, or technology, is unparalleled in the known existence. This achievement is tied to the enormous communities of knowledge, made possible by language: leaving theological content aside, it is very much true that \"in the beginning was the word\", and that in Western societies, this became particularly identified with the written word. There lies the challenge regarding modern age chatbots: they can 'do' language apparently as well as ourselves and there is a natural question of whether they can be considered intelligent, in the same way as we are or otherwise. Are humans uniquely intelligent? We consider this question in terms of the psychological literature on intelligence, evidence for intelligence in non-human animals, the role of written language in science and technology, progress with artificial intelligence, the history of intelligence testing (for both humans and machines), and the role of embodiment in intelligence. We think that it is increasingly difficult to consider humans uniquely intelligent. There are current limitations in chatbots, e.g., concerning perceptual and social awareness, but much attention is currently devoted to overcoming such limitations.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¢è®¨äº†äººç±»ã€äººå·¥å’Œè‡ªç„¶æ™ºèƒ½ä¹‹é—´çš„å…³ç³»ï¼Œåˆ†æäº†äººç±»åœ¨æ–‡åŒ–ã€ç§‘å­¦å’ŒæŠ€æœ¯ä¸Šçš„å“è¶Šæˆå°±å¦‚ä½•ä¸å…¶å»ºç«‹çš„è¯­è¨€çŸ¥è¯†ç¤¾åŒºç´§å¯†ç›¸è¿ã€‚é’ˆå¯¹ç°ä»£èŠå¤©æœºå™¨äºº(chatbots)åœ¨è¯­è¨€èƒ½åŠ›ä¸Šçš„æ˜¾è‘—è¿›å±•ï¼Œä½œè€…æ·±å…¥æ¢è®¨äº†å®ƒä»¬æ˜¯å¦åº”è¢«è§†ä¸ºå…·å¤‡ä¸äººç±»ç›¸ä¼¼çš„æ™ºèƒ½ï¼Œå¹¶æŒ‘æˆ˜äº†äººç±»æ™ºèƒ½å”¯ä¸€æ€§çš„ä¼ ç»Ÿè§‚ç‚¹ã€‚ç ”ç©¶é€šè¿‡æ•´åˆå¿ƒç†å­¦æ–‡çŒ®ã€éäººç±»åŠ¨ç‰©çš„æ™ºèƒ½è¯æ®ã€ä¹¦é¢è¯­è¨€åœ¨ç§‘æŠ€ä¸­çš„ä½œç”¨ä»¥åŠäººå·¥æ™ºèƒ½(Artificial Intelligence)çš„å‘å±•å†ç¨‹ï¼Œå¯¹æ™ºèƒ½çš„æœ¬è´¨è¿›è¡Œäº†å¤šç»´åº¦çš„å‰–æã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å›é¡¾äº†äººç±»ä¸æœºå™¨çš„æ™ºèƒ½æµ‹è¯•(intelligence testing)å†å²ï¼Œå¹¶è®ºè¯äº†å…·èº«æ™ºèƒ½(embodiment)åœ¨æ™ºèƒ½å½¢æˆä¸­çš„é‡è¦æ€§ã€‚ä½œè€…æ€»ç»“è®¤ä¸ºï¼Œè™½ç„¶ç›®å‰çš„AIåœ¨æ„ŸçŸ¥(perceptual)å’Œç¤¾ä¼šæ„è¯†(social awareness)æ–¹é¢ä»æœ‰å±€é™ï¼Œä½†éšç€ç›¸å…³ç“¶é¢ˆçš„æŒç»­çªç ´ï¼Œå·²ç»è¶Šæ¥è¶Šéš¾ä»¥å°†äººç±»è§†ä¸ºè‡ªç„¶ç•Œä¸­å”¯ä¸€çš„æ™ºèƒ½ä¸»ä½“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02183v2",
      "published_date": "2025-06-02 19:11:49 UTC",
      "updated_date": "2025-11-28 15:53:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:27:34.284498+00:00"
    },
    {
      "arxiv_id": "2506.02181v1",
      "title": "Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution",
      "title_zh": "è¯­éŸ³å­¦çš„å›å“ï¼šåŸºäºç‰¹å¾å½’å› æ­ç¤º ASR ç›¸å…³çš„å£°å­¦çº¿ç´¢",
      "authors": [
        "Dennis Fucci",
        "Marco Gaido",
        "Matteo Negri",
        "Mauro Cettolo",
        "Luisa Bentivogli"
      ],
      "abstract": "Despite significant advances in ASR, the specific acoustic cues models rely on remain unclear. Prior studies have examined such cues on a limited set of phonemes and outdated models. In this work, we apply a feature attribution technique to identify the relevant acoustic cues for a modern Conformer-based ASR system. By analyzing plosives, fricatives, and vowels, we assess how feature attributions align with their acoustic properties in the time and frequency domains, also essential for human speech perception. Our findings show that the ASR model relies on vowels' full time spans, particularly their first two formants, with greater saliency in male speech. It also better captures the spectral characteristics of sibilant fricatives than non-sibilants and prioritizes the release phase in plosives, especially burst characteristics. These insights enhance the interpretability of ASR models and highlight areas for future research to uncover potential gaps in model robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡‡ç”¨ç‰¹å¾å½’å› (Feature Attribution)æŠ€æœ¯ï¼Œæ—¨åœ¨æ­ç¤ºç°ä»£åŸºäºConformerçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿåœ¨è¯†åˆ«è¯­éŸ³æ—¶æ‰€ä¾èµ–çš„å…³é”®å£°å­¦çº¿ç´¢(Acoustic Cues)ã€‚ç ”ç©¶é€šè¿‡åˆ†æå¡éŸ³(Plosives)ã€æ“¦éŸ³(Fricatives)å’Œå…ƒéŸ³(Vowels)ï¼Œåœ¨æ—¶åŸŸå’Œé¢‘åŸŸä¸Šè¯„ä¼°äº†æ¨¡å‹ç‰¹å¾å½’å› ä¸äººç±»è¨€è¯­æ„ŸçŸ¥ç›¸å…³çš„å£°å­¦å±æ€§ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒASRæ¨¡å‹åœ¨è¯†åˆ«å…ƒéŸ³æ—¶ä¾èµ–äºå…¶å®Œæ•´çš„æ—¶é•¿è·¨åº¦ï¼Œç‰¹åˆ«æ˜¯å‰ä¸¤ä¸ªå…±æŒ¯å³°(Formants)ï¼Œä¸”åœ¨ç”·æ€§è¯­éŸ³ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ç‰¹å¾æ˜¾è‘—æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹èƒ½å¤Ÿæ¯”éå’éŸ³æ›´æœ‰æ•ˆåœ°æ•æ‰å’éŸ³(Sibilant Fricatives)çš„é¢‘è°±ç‰¹å¾ï¼Œå¹¶åœ¨å¤„ç†å¡éŸ³æ—¶ä¼˜å…ˆå…³æ³¨é™¤é˜»é˜¶æ®µ(Release Phase)çš„çˆ†ç ´éŸ³(Burst)ç‰¹å¾ã€‚è¿™äº›è§è§£æ˜¾è‘—å¢å¼ºäº†ASRæ¨¡å‹çš„å¯è§£é‡Šæ€§(Interpretability)ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ¢ç´¢æ¨¡å‹é²æ£’æ€§(Robustness)ä¸­çš„æ½œåœ¨ç¼ºé™·åŠæ”¹è¿›æ–¹å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02181v1",
      "published_date": "2025-06-02 19:11:16 UTC",
      "updated_date": "2025-06-02 19:11:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:27:27.784443+00:00"
    },
    {
      "arxiv_id": "2506.02177v1",
      "title": "Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts",
      "title_zh": "ä»…åœ¨è·ç›Šæ—¶è¡ŒåŠ¨ï¼šåŸºäºé€‰æ‹©æ€§ Rollout çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Haizhong Zheng",
        "Yang Zhou",
        "Brian R. Bartoldson",
        "Bhavya Kailkhura",
        "Fan Lai",
        "Jiawei Zhao",
        "Beidi Chen"
      ],
      "abstract": "Reinforcement learning, such as PPO and GRPO, has powered recent breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables models to selectively use higher-quality data for training, which can stabilize RL training and improve model performance. However, this comes at the cost of significant computational overhead. In this paper, we show that a substantial portion of this overhead can be avoided by skipping uninformative prompts before rollout. Our analysis of reward dynamics reveals a strong temporal consistency in prompt value: prompts that are uninformative in one epoch of training are likely to remain uninformative in future epochs. Based on these insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online, lightweight pre-rollout filtering algorithm that predicts and skips uninformative prompts using reward training dynamics. By evaluating GRESO on a broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B, DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total training time without accuracy degradation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†ä»»åŠ¡ä¸­å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å› å¤§è§„æ¨¡é‡‡æ ·(rollout)å¯¼è‡´çš„å·¨å¤§è®¡ç®—å¼€é”€é—®é¢˜ï¼Œæå‡ºäº†GRESO (GRPO with Efficient Selective Rollout)æ¡†æ¶ã€‚é€šè¿‡å¯¹å¥–åŠ±åŠ¨æ€(reward dynamics)çš„æ·±å…¥åˆ†æï¼Œç ”ç©¶å‘ç°æç¤ºè¯(prompt)çš„ä»·å€¼å…·æœ‰æ˜¾è‘—çš„æ—¶é—´ä¸€è‡´æ€§(temporal consistency)ï¼Œå³åœ¨è®­ç»ƒæ—©æœŸæ— ä¿¡æ¯çš„æç¤ºè¯åœ¨åç»­å‘¨æœŸé€šå¸¸ä»éš¾ä»¥æä¾›æœ‰æ•ˆå¢ç›Šã€‚åŸºäºè¿™ä¸€å‘ç°ï¼ŒGRESOåˆ©ç”¨ä¸€ç§åœ¨çº¿è½»é‡åŒ–é¢„é‡‡æ ·è¿‡æ»¤ç®—æ³•ï¼Œåœ¨é‡‡æ ·é˜¶æ®µå‰é¢„æµ‹å¹¶è·³è¿‡æ— ä¿¡æ¯æç¤ºè¯ï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—å†—ä½™ã€‚åœ¨Qwen2.5-Mathå’ŒDeepSeek-R1-Distill-Qwenç­‰æ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGRESOåœ¨ä¿è¯æ¨ç†å‡†ç¡®ç‡ä¸ä¸‹é™çš„å‰æä¸‹ï¼Œå®ç°äº†é‡‡æ ·ç¯èŠ‚æœ€é«˜2.4å€ä»¥åŠæ€»è®­ç»ƒæ—¶é—´æœ€é«˜2.0å€çš„åŠ é€Ÿã€‚è¯¥ç ”ç©¶ä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ•ˆç‡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02177v1",
      "published_date": "2025-06-02 19:03:00 UTC",
      "updated_date": "2025-06-02 19:03:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:27:43.684439+00:00"
    },
    {
      "arxiv_id": "2506.06353v1",
      "title": "Large Language Models for EEG: A Comprehensive Survey and Taxonomy",
      "title_zh": "é¢å‘ EEG çš„å¤§è¯­è¨€æ¨¡å‹ï¼šå…¨é¢ç»¼è¿°ä¸åˆ†ç±»ä½“ç³»",
      "authors": [
        "Naseem Babu",
        "Jimson Mathew",
        "A. P. Vinod"
      ],
      "abstract": "The growing convergence between Large Language Models (LLMs) and electroencephalography (EEG) research is enabling new directions in neural decoding, brain-computer interfaces (BCIs), and affective computing. This survey offers a systematic review and structured taxonomy of recent advancements that utilize LLMs for EEG-based analysis and applications. We organize the literature into four domains: (1) LLM-inspired foundation models for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal generation including image and 3D object synthesis, and (4) clinical applications and dataset management tools. The survey highlights how transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance. By offering a structured overview of modeling strategies, system designs, and application areas, this work serves as a foundational resource for future work to bridge natural language processing and neural signal analysis through language models.",
      "tldr_zh": "è¿™é¡¹ç»¼è¿°ç³»ç»Ÿåœ°å›é¡¾å¹¶æ„å»ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰åœ¨è„‘ç”µå›¾ï¼ˆEEGï¼‰ç ”ç©¶é¢†åŸŸçš„ç»“æ„åŒ–åˆ†ç±»ï¼Œé‡ç‚¹æ¢è®¨äº†ç¥ç»è§£ç ã€è„‘æœºæ¥å£ï¼ˆBCIsï¼‰å’Œæƒ…æ„Ÿè®¡ç®—çš„èåˆè¶‹åŠ¿ã€‚ç ”ç©¶å°†ç›¸å…³æ–‡çŒ®å½’çº³ä¸º EEG è¡¨ç¤ºå­¦ä¹ åŸºç¡€æ¨¡å‹ã€EEG åˆ°è¯­è¨€çš„è§£ç ã€è·¨æ¨¡æ€ç”Ÿæˆï¼ˆåŒ…æ‹¬å›¾åƒå’Œ 3D ç‰©ä½“åˆæˆï¼‰ä»¥åŠä¸´åºŠåº”ç”¨ä¸æ•°æ®é›†ç®¡ç†å››ä¸ªæ ¸å¿ƒé¢†åŸŸã€‚è®ºæ–‡è¯¦ç»†åˆ†æäº†åŸºäº Transformer çš„æ¶æ„å¦‚ä½•åˆ©ç”¨ Fine-tuningã€Few-shot å’Œ Zero-shot å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œä½¿ EEG æ¨¡å‹èƒ½å¤Ÿèƒœä»»è‡ªç„¶è¯­è¨€ç”Ÿæˆã€è¯­ä¹‰è§£é‡Šå’Œè¯Šæ–­è¾…åŠ©ç­‰å¤æ‚ä»»åŠ¡ã€‚é€šè¿‡å¯¹å»ºæ¨¡ç­–ç•¥ã€ç³»ç»Ÿè®¾è®¡å’Œåº”ç”¨åœºæ™¯çš„å…¨é¢æ¢³ç†ï¼Œè¯¥ç»¼è¿°ä¸ºæœªæ¥è·¨è¶Šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸ç¥ç»ä¿¡å·åˆ†æä¹‹é—´çš„æŠ€æœ¯é¸¿æ²Ÿæä¾›äº†é‡è¦çš„åŸºç¡€å‚è€ƒèµ„æºã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06353v1",
      "published_date": "2025-06-02 18:58:57 UTC",
      "updated_date": "2025-06-02 18:58:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:10.291848+00:00"
    },
    {
      "arxiv_id": "2506.06352v1",
      "title": "Will artificial agents pursue power by default?",
      "title_zh": "äººå·¥æ™ºèƒ½ä½“æ˜¯å¦ä¼šé»˜è®¤è¿½æ±‚æƒåŠ›ï¼Ÿ",
      "authors": [
        "Christian Tarsney"
      ],
      "abstract": "Researchers worried about catastrophic risks from advanced AI have argued that we should expect sufficiently capable AI agents to pursue power over humanity because power is a convergent instrumental goal, something that is useful for a wide range of final goals. Others have recently expressed skepticism of these claims. This paper aims to formalize the concepts of instrumental convergence and power-seeking in an abstract, decision-theoretic framework, and to assess the claim that power is a convergent instrumental goal. I conclude that this claim contains at least an element of truth, but might turn out to have limited predictive utility, since an agent's options cannot always be ranked in terms of power in the absence of substantive information about the agent's final goals. However, the fact of instrumental convergence is more predictive for agents who have a good shot at attaining absolute or near-absolute power.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é«˜çº§äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ™ºèƒ½ä½“æ˜¯å¦ä¼šé»˜è®¤è¿½æ±‚æƒåŠ›ï¼Œå¹¶åœ¨å†³ç­–è®ºï¼ˆdecision-theoreticï¼‰æ¡†æ¶ä¸‹å¯¹å·¥å…·æ€§æ”¶æ•›ï¼ˆinstrumental convergenceï¼‰å’Œè¿½æ±‚æƒåŠ›ï¼ˆpower-seekingï¼‰çš„æ¦‚å¿µè¿›è¡Œäº†å½¢å¼åŒ–ã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†æƒåŠ›ä½œä¸ºä¸€ç§æ”¶æ•›æ€§å·¥å…·ç›®æ ‡ï¼ˆconvergent instrumental goalï¼‰çš„è®ºç‚¹ï¼Œå³æƒåŠ›åœ¨å®ç°å¹¿æ³›çš„æœ€ç»ˆç›®æ ‡ä¸­å‡å…·æœ‰å®ç”¨ä»·å€¼ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶è¿™ä¸€è®ºæ–­å…·æœ‰ä¸€å®šçš„çœŸå®æ€§ï¼Œä½†åœ¨ç¼ºä¹å…³äºæ™ºèƒ½ä½“æœ€ç»ˆç›®æ ‡çš„å®è´¨æ€§ä¿¡æ¯æ—¶ï¼Œç”±äºä¸åŒé€‰é¡¹æ— æ³•æ€»æ˜¯æŒ‰æƒåŠ›ç»´åº¦è¿›è¡Œæ’åºï¼Œè¯¥ç†è®ºçš„é¢„æµ‹æ•ˆç”¨å¯èƒ½å—åˆ°é™åˆ¶ã€‚ç„¶è€Œï¼Œå¯¹äºæœ‰æ½œåŠ›è·å–ç»å¯¹æˆ–æ¥è¿‘ç»å¯¹æƒåŠ›çš„æ™ºèƒ½ä½“ï¼Œå·¥å…·æ€§æ”¶æ•›çš„è¶‹åŠ¿è¡¨ç°å‡ºæ›´å¼ºçš„é¢„æµ‹æ€§ã€‚è¯¥è®ºæ–‡é€šè¿‡æ„å»ºæŠ½è±¡çš„ç†è®ºæ¨¡å‹ï¼Œä¸ºåˆ†æå…ˆè¿›AIç³»ç»Ÿå¯èƒ½å¸¦æ¥çš„ç¾éš¾æ€§é£é™©æä¾›äº†é‡è¦çš„è¯„ä¼°è§†è§’ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.06352v1",
      "published_date": "2025-06-02 18:54:18 UTC",
      "updated_date": "2025-06-02 18:54:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:06.586216+00:00"
    },
    {
      "arxiv_id": "2506.02167v1",
      "title": "Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos",
      "title_zh": "Fire360ï¼šé¢å‘é€€åŒ–360åº¦æ¶ˆé˜²è§†é¢‘ä¸­é²æ£’æ„ŸçŸ¥ä¸æƒ…æ™¯è®°å¿†çš„åŸºå‡†",
      "authors": [
        "Aditi Tiwari",
        "Farzaneh Masoud",
        "Dac Trong Nguyen",
        "Jill Kraft",
        "Heng Ji",
        "Klara Nahrstedt"
      ],
      "abstract": "Modern AI systems struggle most in environments where reliability is critical - scenes with smoke, poor visibility, and structural deformation. Each year, tens of thousands of firefighters are injured on duty, often due to breakdowns in situational perception. We introduce Fire360, a benchmark for evaluating perception and reasoning in safety-critical firefighting scenarios. The dataset includes 228 360-degree videos from professional training sessions under diverse conditions (e.g., low light, thermal distortion), annotated with action segments, object locations, and degradation metadata. Fire360 supports five tasks: Visual Question Answering, Temporal Action Captioning, Object Localization, Safety-Critical Reasoning, and Transformed Object Retrieval (TOR). TOR tests whether models can match pristine exemplars to fire-damaged counterparts in unpaired scenes, evaluating transformation-invariant recognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag significantly, exposing failures in reasoning under degradation. By releasing Fire360 and its evaluation suite, we aim to advance models that not only see, but also remember, reason, and act under uncertainty. The dataset is available at: https://uofi.box.com/v/fire360dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¶ˆé˜²å‘˜åœ¨æµ“çƒŸã€ä½èƒ½è§åº¦å’Œç»“æ„å˜å½¢ç­‰æ¶åŠ£ç¯å¢ƒä¸‹é¢ä¸´çš„æ„ŸçŸ¥æŒ‘æˆ˜ï¼Œæ¨å‡ºäº†åä¸ºFire360çš„åŸºå‡†æµ‹è¯•é›†ã€‚è¯¥æ•°æ®é›†ç”±228æ®µå½•åˆ¶äºä¸“ä¸šè®­ç»ƒåœºæ™¯çš„360-degree videosç»„æˆï¼Œæ¶µç›–äº†ä½å…‰ç…§å’Œçƒ­å˜å½¢(thermal distortion)ç­‰å¤æ‚æ¡ä»¶ï¼Œå¹¶æä¾›äº†åŠ¨ä½œç‰‡æ®µä¸ç›®æ ‡ä½ç½®çš„è¯¦å°½æ ‡æ³¨ã€‚Fire360æ”¯æŒåŒ…æ‹¬Visual Question Answeringã€Temporal Action Captioningã€Object Localizationã€Safety-Critical Reasoningä»¥åŠTransformed Object Retrieval (TOR)åœ¨å†…çš„äº”é¡¹æ ¸å¿ƒä»»åŠ¡ã€‚å…¶ä¸­ï¼ŒTORä»»åŠ¡é€šè¿‡è¦æ±‚æ¨¡å‹åŒ¹é…åŸå§‹æ ·æœ¬åŠå…¶å¯¹åº”çš„ç«åœºå—æŸå‰¯æœ¬ï¼Œæ—¨åœ¨è¯„ä¼°å…¶å˜å½¢ä¸å˜æ€§è¯†åˆ«(transformation-invariant recognition)èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPT-4oç­‰å‰æ²¿æ¨¡å‹åœ¨TORä»»åŠ¡ä¸­çš„è¡¨ç°è¿œé€Šäºäººç±»ä¸“å®¶ï¼ˆ83.5%å‡†ç¡®ç‡ï¼‰ï¼Œæš´éœ²äº†AIåœ¨ç¯å¢ƒé€€åŒ–æ¡ä»¶ä¸‹çš„æ¨ç†ç¼ºé™·ã€‚è¯¥åŸºå‡†é›†çš„å‘å¸ƒæ—¨åœ¨ä¿ƒè¿›æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹è®°å¿†ã€æ¨ç†ä¸è¡ŒåŠ¨èƒ½åŠ›çš„æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 9 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.02167v1",
      "published_date": "2025-06-02 18:45:56 UTC",
      "updated_date": "2025-06-02 18:45:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:10.690910+00:00"
    },
    {
      "arxiv_id": "2506.02166v1",
      "title": "Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi",
      "title_zh": "Dhvaniï¼šé¢å‘å°åœ°è¯­çš„å¼±ç›‘ç£éŸ³ç´ é”™è¯¯æ£€æµ‹ä¸ä¸ªæ€§åŒ–åé¦ˆç³»ç»Ÿ",
      "authors": [
        "Arnav Rustagi",
        "Satvik Bajpai",
        "Nimrat Kaur",
        "Siddharth Siddharth"
      ],
      "abstract": "Computer-Assisted Pronunciation Training (CAPT) has been extensively studied for English. However, there remains a critical gap in its application to Indian languages with a base of 1.5 billion speakers. Pronunciation tools tailored to Indian languages are strikingly lacking despite the fact that millions learn them every year. With over 600 million speakers and being the fourth most-spoken language worldwide, improving Hindi pronunciation is a vital first step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT system for Hindi, 2) synthetic speech generation for Hindi mispronunciations, and 3) a novel methodology for providing personalized feedback to learners. While the system often interacts with learners using Devanagari graphemes, its core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic orthography to analyze mispronounced speech and provide targeted feedback.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°åœ°è¯­åœ¨è®¡ç®—æœºè¾…åŠ©å‘éŸ³è®­ç»ƒ(CAPT)é¢†åŸŸçš„åº”ç”¨ç©ºç™½ï¼Œæå‡ºäº†åä¸º Dhvani çš„æ–°å‹éŸ³ä½é”™è¯¯æ£€æµ‹(Phonemic Error Detection)ä¸ä¸ªæ€§åŒ–åé¦ˆ(Personalized Feedback)ç³»ç»Ÿã€‚ä½œä¸ºé’ˆå¯¹å°åœ°è¯­è®¾è®¡çš„é¦–ä¸ªæ­¤ç±»ç³»ç»Ÿï¼Œå®ƒä¸ä»…åŒ…å«æ ¸å¿ƒçš„ Dhvani æ¶æ„ï¼Œè¿˜å¼•å…¥äº†é’ˆå¯¹è¯¯å‘éŸ³çš„åˆæˆè¯­éŸ³ç”Ÿæˆ(Synthetic Speech Generation)æŠ€æœ¯åŠä¸€å¥—å…¨æ–°çš„ä¸ªæ€§åŒ–åé¦ˆæ–¹æ³•è®ºã€‚è™½ç„¶ç³»ç»Ÿåœ¨äº¤äº’ä¸­ä½¿ç”¨å¤©åŸæ–‡(Devanagari)ï¼Œä½†å…¶æ ¸å¿ƒåˆ†æèšç„¦äºéŸ³ä½åŒºåˆ†(Phonemic Distinctions)ï¼Œå……åˆ†åˆ©ç”¨äº†å°åœ°è¯­é«˜åº¦éŸ³ç´ åŒ–(Phonetic Orthography)çš„æ‹¼å†™ç‰¹å¾æ¥è¯†åˆ«å‘éŸ³é”™è¯¯ã€‚é€šè¿‡å¯¹è¯¯å‘éŸ³è¿›è¡Œæ·±åº¦åˆ†æå¹¶æä¾›é’ˆå¯¹æ€§åé¦ˆï¼Œè¯¥ç ”ç©¶ä¸ºæ‹¥æœ‰åºå¤§å—ä¼—çš„å°åº¦è¯­è¨€è‡ªåŠ¨åŒ–å‘éŸ³è®­ç»ƒå¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted for publication at Interspeech 2025 to be held in Rotterdam, the Netherlands",
      "pdf_url": "https://arxiv.org/pdf/2506.02166v1",
      "published_date": "2025-06-02 18:45:52 UTC",
      "updated_date": "2025-06-02 18:45:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:14.687838+00:00"
    },
    {
      "arxiv_id": "2506.02158v1",
      "title": "Reflection-Based Memory For Web navigation Agents",
      "title_zh": "é¢å‘ç½‘ç»œå¯¼èˆªæ™ºèƒ½ä½“çš„åŸºäºåæ€çš„è®°å¿†",
      "authors": [
        "Ruhana Azam",
        "Aditya Vempaty",
        "Ashish Jagmohan"
      ],
      "abstract": "Web navigation agents have made significant progress, yet current systems operate with no memory of past experiences -- leading to repeated mistakes and an inability to learn from previous interactions. We introduce Reflection-Augment Planning (ReAP), a web navigation system to leverage both successful and failed past experiences using self-reflections. Our method improves baseline results by 11 points overall and 29 points on previously failed tasks. These findings demonstrate that reflections can transfer to different web navigation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç½‘é¡µå¯¼èˆªæ™ºèƒ½ä½“(Web navigation agents)ç¼ºä¹è¿‡å»ç»éªŒè®°å¿†ï¼Œå¯¼è‡´åœ¨äº¤äº’ä¸­é‡å¤çŠ¯é”™ä¸”æ— æ³•å­¦ä¹ çš„é—®é¢˜ï¼Œæå‡ºäº†Reflection-Augment Planning (ReAP)æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¼•å…¥è‡ªæˆ‘åæ€(self-reflections)æœºåˆ¶ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤ŸåŒæ—¶åˆ©ç”¨è¿‡å»æˆåŠŸå’Œå¤±è´¥çš„ç»éªŒæ¥è¾…åŠ©è§„åˆ’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReAPåœ¨æ•´ä½“åŸºå‡†æµ‹è¯•ä¸Šæ¯”åŸºçº¿æ¨¡å‹æå‡äº†11ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨å…ˆå‰å¤±è´¥çš„ä»»åŠ¡ä¸Šæ›´æ˜¯æ˜¾è‘—æå‡äº†29ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›å‘ç°æœ‰åŠ›åœ°è¯æ˜äº†åæ€æœºåˆ¶åœ¨ä¸åŒç½‘é¡µå¯¼èˆªä»»åŠ¡ä¹‹é—´å…·æœ‰è‰¯å¥½çš„å¯è¿ç§»æ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…è§£å†³äº†æ™ºèƒ½ä½“çš„é‡å¤é”™è¯¯é—®é¢˜ï¼Œè¿˜ä¸ºå®ç°æ›´å…·é²æ£’æ€§å’Œè‡ªé€‚åº”èƒ½åŠ›çš„Webæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02158v1",
      "published_date": "2025-06-02 18:39:55 UTC",
      "updated_date": "2025-06-02 18:39:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:18.890190+00:00"
    },
    {
      "arxiv_id": "2506.02154v1",
      "title": "Z-Error Loss for Training Neural Networks",
      "title_zh": "ç”¨äºç¥ç»ç½‘ç»œè®­ç»ƒçš„ Z-Error æŸå¤±",
      "authors": [
        "Guillaume Godin"
      ],
      "abstract": "Outliers introduce significant training challenges in neural networks by propagating erroneous gradients, which can degrade model performance and generalization. We propose the Z-Error Loss, a statistically principled approach that minimizes outlier influence during training by masking the contribution of data points identified as out-of-distribution within each batch. This method leverages batch-level statistics to automatically detect and exclude anomalous samples, allowing the model to focus its learning on the true underlying data structure. Our approach is robust, adaptive to data quality, and provides valuable diagnostics for data curation and cleaning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Z-Error Lossï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç»Ÿè®¡å­¦åŸåˆ™çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œè®­ç»ƒä¸­å› ç¦»ç¾¤å€¼(Outliers)äº§ç”Ÿé”™è¯¯æ¢¯åº¦(Erroneous Gradients)è€Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸‹é™çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰¹æ¬¡çº§ç»Ÿè®¡æ•°æ®(Batch-level Statistics)è‡ªåŠ¨æ£€æµ‹å¹¶æ©ç (Mask)æ¯ä¸ªæ‰¹æ¬¡ä¸­è¢«è¯†åˆ«ä¸ºåˆ†å¸ƒå¤–(Out-of-distribution)çš„æ•°æ®ç‚¹ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å°åŒ–ç¦»ç¾¤å€¼çš„å½±å“ã€‚è¿™ç§æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿå°†å­¦ä¹ é‡ç‚¹é›†ä¸­åœ¨çœŸå®çš„æ½œåœ¨æ•°æ®ç»“æ„ä¸Šï¼Œæœ‰æ•ˆæ’é™¤äº†å¼‚å¸¸æ ·æœ¬çš„å¹²æ‰°ã€‚Z-Error Lossè¡¨ç°å‡ºæå¼ºçš„ç¨³å¥æ€§(Robust)å¹¶èƒ½è‡ªé€‚åº”ä¸åŒçš„æ•°æ®è´¨é‡ï¼ŒåŒæ—¶è¿˜ä¸ºæ•°æ®ç­–åˆ’(Data Curation)å’Œæ¸…æ´—å·¥ä½œæä¾›äº†æœ‰ä»·å€¼çš„è¯Šæ–­ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 6 figures, A technical note",
      "pdf_url": "https://arxiv.org/pdf/2506.02154v1",
      "published_date": "2025-06-02 18:35:30 UTC",
      "updated_date": "2025-06-02 18:35:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:29.015439+00:00"
    },
    {
      "arxiv_id": "2506.02153v2",
      "title": "Small Language Models are the Future of Agentic AI",
      "title_zh": "å°è¯­è¨€æ¨¡å‹æ˜¯æ™ºèƒ½ä½“ AI çš„æœªæ¥",
      "authors": [
        "Peter Belcak",
        "Greg Heinrich",
        "Shizhe Diao",
        "Yonggan Fu",
        "Xin Dong",
        "Saurav Muralidharan",
        "Yingyan Celine Lin",
        "Pavlo Molchanov"
      ],
      "abstract": "Large language models (LLMs) are often praised for exhibiting near-human performance on a wide range of tasks and valued for their ability to hold a general conversation. The rise of agentic AI systems is, however, ushering in a mass of applications in which language models perform a small number of specialized tasks repetitively and with little variation.\n  Here we lay out the position that small language models (SLMs) are sufficiently powerful, inherently more suitable, and necessarily more economical for many invocations in agentic systems, and are therefore the future of agentic AI. Our argumentation is grounded in the current level of capabilities exhibited by SLMs, the common architectures of agentic systems, and the economy of LM deployment. We further argue that in situations where general-purpose conversational abilities are essential, heterogeneous agentic systems (i.e., agents invoking multiple different models) are the natural choice. We discuss the potential barriers for the adoption of SLMs in agentic systems and outline a general LLM-to-SLM agent conversion algorithm.\n  Our position, formulated as a value statement, highlights the significance of the operational and economic impact even a partial shift from LLMs to SLMs is to have on the AI agent industry. We aim to stimulate the discussion on the effective use of AI resources and hope to advance the efforts to lower the costs of AI of the present day. Calling for both contributions to and critique of our position, we commit to publishing all such correspondence at https://research.nvidia.com/labs/lpr/slm-agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Small Language Models (SLMs) æ˜¯æ™ºèƒ½ä»£ç† (Agentic AI) æœªæ¥æ ¸å¿ƒçš„è§‚ç‚¹ï¼Œè®¤ä¸ºå…¶åœ¨æ‰§è¡Œç‰¹å®šä¸”é‡å¤çš„ä¸“ä¸šåŒ–ä»»åŠ¡ä¸­æ¯”å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ›´å…·ä¼˜åŠ¿ã€‚é€šè¿‡å¯¹ SLMs èƒ½åŠ›ã€ä»£ç†ç³»ç»Ÿæ¶æ„åŠéƒ¨ç½²ç»æµæ€§çš„ç»¼åˆåˆ†æï¼Œä½œè€…è®ºè¯äº† SLMs åœ¨è®¸å¤šä»£ç†è°ƒç”¨åœºæ™¯ä¸‹ä¸ä»…åŠŸèƒ½å……è¶³ï¼Œä¸”åœ¨é€‚ç”¨æ€§å’Œç»æµæˆæœ¬ä¸Šè¡¨ç°æ›´ä½³ã€‚é’ˆå¯¹éœ€è¦é€šç”¨å¯¹è¯èƒ½åŠ›çš„å¤æ‚åœºæ™¯ï¼Œæ–‡ç« æå€¡æ„å»ºå¼‚æ„ä»£ç†ç³»ç»Ÿ (Heterogeneous agentic systems) ä»¥å®ç°å¤šæ¨¡å‹ååŒã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ¢è®¨äº† SLMs åœ¨ä»£ç†ç³»ç»Ÿä¸­åº”ç”¨çš„æ½œåœ¨éšœç¢ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šç”¨çš„ LLM-to-SLM ä»£ç†è½¬æ¢ç®—æ³•ã€‚è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡æ¨åŠ¨ä» LLMs å‘ SLMs çš„éƒ¨åˆ†è½¬å‹ï¼Œé™ä½ AI æˆæœ¬å¹¶æå‡èµ„æºåˆ©ç”¨æ•ˆç‡ï¼Œä»è€Œå¯¹ AI ä»£ç†è¡Œä¸šäº§ç”Ÿæ·±è¿œçš„è¿è¥ä¸ç»æµå½±å“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02153v2",
      "published_date": "2025-06-02 18:35:16 UTC",
      "updated_date": "2025-09-15 22:15:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:23.369600+00:00"
    },
    {
      "arxiv_id": "2506.11071v1",
      "title": "Embedded Acoustic Intelligence for Automotive Systems",
      "title_zh": "æ±½è½¦ç³»ç»ŸåµŒå…¥å¼å£°å­¦æ™ºèƒ½",
      "authors": [
        "Renjith Rajagopal",
        "Peter Winzell",
        "Sladjana Strbac",
        "Konstantin LindstrÃ¶m",
        "Petter HÃ¶rling",
        "Faisal Kohestani",
        "Niloofar Mehrzad"
      ],
      "abstract": "Transforming sound insights into actionable streams of data, this abstract leverages findings from degree thesis research to enhance automotive system intelligence, enabling us to address road type [1].By extracting and interpreting acoustic signatures from microphones installed within the wheelbase of a car, we focus on classifying road type.Utilizing deep neural networks and feature extraction powered by pre-trained models from the Open AI ecosystem (via Hugging Face [2]), our approach enables Autonomous Driving and Advanced Driver- Assistance Systems (AD/ADAS) to anticipate road surfaces, support adaptive learning for active road noise cancellation, and generate valuable insights for urban planning. The results of this study were specifically captured to support a compelling business case for next-generation automotive systems. This forward-looking approach not only promises to redefine passenger comfort and improve vehicle safety, but also paves the way for intelligent, data-driven urban road management, making the future of mobility both achievable and sustainable.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”¨äºæ±½è½¦ç³»ç»Ÿçš„åµŒå…¥å¼å£°å­¦æ™ºèƒ½æŠ€æœ¯ï¼Œæ—¨åœ¨å°†å£°éŸ³ä¿¡æ¯è½¬åŒ–ä¸ºå¯æ“ä½œçš„æ•°æ®æµä»¥å®ç°è·¯é¢ç±»å‹(road type)åˆ†ç±»ã€‚é€šè¿‡åœ¨æ±½è½¦è½´è·(wheelbase)å†…å®‰è£…éº¦å…‹é£æå–å¹¶è§£æå£°å­¦ç‰¹å¾ï¼Œç ”ç©¶ç»“åˆäº† Deep Neural Networks ä»¥åŠæ¥è‡ª OpenAI ç”Ÿæ€ç³»ç»Ÿï¼ˆé€šè¿‡ Hugging Faceï¼‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç‰¹å¾æå–ã€‚è¯¥æ–¹æ³•èƒ½å¤ŸååŠ©è‡ªåŠ¨é©¾é©¶ä¸é«˜çº§é©¾é©¶è¾…åŠ©ç³»ç»Ÿ(AD/ADAS)é¢„åˆ¤è·¯é¢çŠ¶å†µï¼Œå¹¶ä¸ºä¸»åŠ¨é“è·¯å™ªå£°æ¶ˆé™¤(Active Road Noise Cancellation)çš„è‡ªé€‚åº”å­¦ä¹ æä¾›æ”¯æŒã€‚å®éªŒç»“æœä¸ä»…ä¸ºåŸå¸‚è§„åˆ’æä¾›äº†æœ‰ä»·å€¼çš„æ•°æ®æ´å¯Ÿï¼Œè¿˜ä¸ºä¸‹ä¸€ä»£æ±½è½¦ç³»ç»Ÿçš„å•†ä¸šåŒ–åº”ç”¨æä¾›äº†æ”¯æ’‘ã€‚è¿™ä¸€å‰ç»æ€§æ–¹æ¡ˆåœ¨æå‡ä¹˜å®¢èˆ’é€‚åº¦ä¸è½¦è¾†å®‰å…¨æ€§çš„åŒæ—¶ï¼Œä¹Ÿä¸ºå®ç°æ™ºèƒ½ã€æ•°æ®é©±åŠ¨çš„åŸå¸‚é“è·¯ç®¡ç†å’Œå¯æŒç»­äº¤é€šå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11071v1",
      "published_date": "2025-06-02 18:34:40 UTC",
      "updated_date": "2025-06-02 18:34:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:27.264948+00:00"
    },
    {
      "arxiv_id": "2506.02150v1",
      "title": "Implicit Deformable Medical Image Registration with Learnable Kernels",
      "title_zh": "åŸºäºå¯å­¦ä¹ æ ¸çš„éšå¼åŒ»å­¦å›¾åƒå¯å˜å½¢é…å‡†",
      "authors": [
        "Stefano Fogarollo",
        "Gregor Laimer",
        "Reto Bale",
        "Matthias Harders"
      ],
      "abstract": "Deformable medical image registration is an essential task in computer-assisted interventions. This problem is particularly relevant to oncological treatments, where precise image alignment is necessary for tracking tumor growth, assessing treatment response, and ensuring accurate delivery of therapies. Recent AI methods can outperform traditional techniques in accuracy and speed, yet they often produce unreliable deformations that limit their clinical adoption. In this work, we address this challenge and introduce a novel implicit registration framework that can predict accurate and reliable deformations. Our insight is to reformulate image registration as a signal reconstruction problem: we learn a kernel function that can recover the dense displacement field from sparse keypoint correspondences. We integrate our method in a novel hierarchical architecture, and estimate the displacement field in a coarse-to-fine manner. Our formulation also allows for efficient refinement at test time, permitting clinicians to easily adjust registrations when needed. We validate our method on challenging intra-patient thoracic and abdominal zero-shot registration tasks, using public and internal datasets from the local University Hospital. Our method not only shows competitive accuracy to state-of-the-art approaches, but also bridges the generalization gap between implicit and explicit registration techniques. In particular, our method generates deformations that better preserve anatomical relationships and matches the performance of specialized commercial systems, underscoring its potential for clinical adoption.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å˜å½¢åŒ»å­¦å›¾åƒé…å‡†(Deformable medical image registration)ä¸­AIæ–¹æ³•äº§ç”Ÿçš„å˜å½¢ä¸å¯é é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¯å­¦ä¹ æ ¸(Learnable kernels)çš„æ–°å‹éšå¼é…å‡†æ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†å›¾åƒé…å‡†åˆ›æ–°æ€§åœ°é‡æ–°å®šä¹‰ä¸ºä¿¡å·é‡å»ºé—®é¢˜ï¼Œé€šè¿‡å­¦ä¹ æ ¸å‡½æ•°ä»ç¨€ç–çš„å…³é”®ç‚¹å¯¹åº”å…³ç³»ä¸­æ¢å¤ç¨ å¯†çš„ä½ç§»åœº(Displacement field)ã€‚æ¡†æ¶é‡‡ç”¨äº†å±‚æ¬¡åŒ–æ¶æ„(Hierarchical architecture)ï¼Œä»¥ç”±ç²—åˆ°ç»†(Coarse-to-fine)çš„æ–¹å¼ä¼°è®¡ä½ç§»åœºï¼Œå¹¶æ”¯æŒåœ¨æµ‹è¯•é˜¶æ®µè¿›è¡Œé«˜æ•ˆçš„ç²¾ç»†è°ƒæ•´(Refinement)ã€‚ç ”ç©¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„èƒ¸éƒ¨å’Œè…¹éƒ¨æ‚£è€…å†…é›¶æ ·æœ¬é…å‡†(Zero-shot registration)ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ä¿æŒè§£å‰–å…³ç³»æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¸ä»…ç¼©å°äº†éšå¼ä¸æ˜¾å¼é…å‡†æŠ€æœ¯ä¹‹é—´çš„æ³›åŒ–å·®è·ï¼Œå…¶æ€§èƒ½æ›´è¾¾åˆ°äº†ä¸“ä¸šå•†ä¸šç³»ç»Ÿçš„æ°´å¹³ï¼Œåœ¨ä¸´åºŠè‚¿ç˜¤è¿½è¸ªä¸æ²»ç–—è¯„ä¼°ä¸­å…·æœ‰æ˜¾è‘—çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "MICCAI 2025 Provisional Accept",
      "pdf_url": "https://arxiv.org/pdf/2506.02150v1",
      "published_date": "2025-06-02 18:27:11 UTC",
      "updated_date": "2025-06-02 18:27:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:39.292489+00:00"
    },
    {
      "arxiv_id": "2506.02139v5",
      "title": "The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€è®¤çŸ¥æ„è¯†ç†è®ºï¼šè¯­ä¹‰é”šå®šã€æ¿€æ´»é˜ˆå€¼ä¸æ¶Œç°æ¨ç†",
      "authors": [
        "Edward Y. Chang",
        "Zeyneb N. Kaya",
        "Ethan Chang"
      ],
      "abstract": "We propose semantic anchoring, a unified account of how large language models turn pretrained capacity into goal-directed behavior: external structure (in-context examples, retrieval, or light tuning) binds the model's latent patterns to desired targets. Unified Contextual Control Theory (UCCT) formalizes this via anchoring strength $S = Ï_d - d_r - \\log k$, where $Ï_d$ measures target cohesion in representation space, $d_r$ measures mismatch from prior knowledge, and $k$ is the anchor budget. UCCT predicts threshold-like performance flips and strictly generalizes in-context learning, reading retrieval and fine-tuning as anchoring variants. Three controlled studies provide evidence. Experiment 1 demonstrates cross-domain anchoring rebinding strong priors in text and vision. Experiment 2 varies representational familiarity via numeral bases (base-10/8/9) at fixed complexity, yielding ordered thresholds and transfer patterns tracking $Ï_d$, $d_r$, and $S$. Experiment 3 establishes a geometry-to-behavior correlate: layer-wise peak anchoring and trajectory area predict few-shot thresholds $Î¸_{50}$. UCCT offers testable theory and practical metrics for optimizing prompts, retrieval, and tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è¯­ä¹‰é”šå®š(semantic anchoring)æ¦‚å¿µå’Œç»Ÿä¸€ä¸Šä¸‹æ–‡æ§åˆ¶ç†è®º(Unified Contextual Control Theory, UCCT)ï¼Œæ—¨åœ¨é˜æ˜å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•é€šè¿‡å¤–éƒ¨ç»“æ„å°†é¢„è®­ç»ƒèƒ½åŠ›è½¬åŒ–ä¸ºç›®æ ‡å¯¼å‘çš„è¡Œä¸ºã€‚UCCTé€šè¿‡é”šå®šå¼ºåº¦å…¬å¼ $S = Ï_d - d_r - \\log k$ æ­£å¼åŒ–äº†è¡¨å¾ç©ºé—´ä¸­çš„ç›®æ ‡å‡èšåŠ›ã€å…ˆéªŒçŸ¥è¯†åå·®ä¸é”šå®šé¢„ç®—ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚è¯¥ç†è®ºæˆåŠŸé¢„æµ‹äº†æ¨¡å‹æ€§èƒ½çš„é˜ˆå€¼åŒ–çªå˜ï¼Œå¹¶å°†ä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)ã€æ£€ç´¢å¢å¼ºå’Œå¾®è°ƒç»Ÿä¸€è§†ä¸ºé”šå®šçš„ä¸åŒå˜ä½“ã€‚é€šè¿‡è·¨é¢†åŸŸé”šå®šã€æ•°åˆ¶è¡¨å¾(numeral bases)ç†Ÿæ‚‰åº¦ä»¥åŠå‡ ä½•-è¡Œä¸ºå…³è”ç­‰ä¸‰é¡¹å—æ§å®éªŒï¼Œç ”ç©¶éªŒè¯äº†UCCTåœ¨é¢„æµ‹å°‘æ ·æœ¬å­¦ä¹ é˜ˆå€¼ $Î¸_{50}$ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚UCCTä¸ä»…ä¸ºç†è§£å¤§æ¨¡å‹æ¶Œç°æ¨ç†æä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œè¿˜ä¸ºä¼˜åŒ–æç¤ºå·¥ç¨‹(prompts)ã€æ£€ç´¢ç­–ç•¥å’Œæ¨¡å‹è°ƒä¼˜(tuning)æä¾›äº†å¯é‡åŒ–çš„å®è·µæŒ‡æ ‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 7 figure, 4 table",
      "pdf_url": "https://arxiv.org/pdf/2506.02139v5",
      "published_date": "2025-06-02 18:12:43 UTC",
      "updated_date": "2025-11-28 22:24:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:28:47.041720+00:00"
    },
    {
      "arxiv_id": "2506.02125v1",
      "title": "Descriptive History Representations: Learning Representations by Answering Questions",
      "title_zh": "æè¿°æ€§å†å²è¡¨ç¤ºï¼šé€šè¿‡é—®ç­”å­¦ä¹ è¡¨ç¤º",
      "authors": [
        "Guy Tennenholtz",
        "Jihwan Jeong",
        "Chih-Wei Hsu",
        "Yinlam Chow",
        "Craig Boutilier"
      ],
      "abstract": "Effective decision making in partially observable environments requires compressing long interaction histories into informative representations. We introduce Descriptive History Representations (DHRs): sufficient statistics characterized by their capacity to answer relevant questions about past interactions and potential future outcomes. DHRs focus on capturing the information necessary to address task-relevant queries, providing a structured way to summarize a history for optimal control. We propose a multi-agent learning framework, involving representation, decision, and question-asking components, optimized using a joint objective that balances reward maximization with the representation's ability to answer informative questions. This yields representations that capture the salient historical details and predictive structures needed for effective decision making. We validate our approach on user modeling tasks with public movie and shopping datasets, generating interpretable textual user profiles which serve as sufficient statistics for predicting preference-driven behavior of users.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Descriptive History Representations (DHRs)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­é•¿äº¤äº’å†å²å‹ç¼©é—®é¢˜çš„è¡¨å¾æ–¹æ³•ã€‚DHRsçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å›ç­”å…³äºè¿‡å»äº¤äº’å’Œæ½œåœ¨æœªæ¥ç»“æœçš„ç›¸å…³é—®é¢˜æ¥æ„å»ºå……åˆ†ç»Ÿè®¡é‡(sufficient statistics)ï¼Œä»è€Œç²¾å‡†æ•è·ä»»åŠ¡ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªé›†æˆè¡¨å¾(representation)ã€å†³ç­–(decision)å’Œæé—®(question-asking)ç»„ä»¶çš„å¤šæ™ºèƒ½ä½“å­¦ä¹ æ¡†æ¶ï¼Œå¹¶é€šè¿‡è”åˆç›®æ ‡å‡½æ•°å¹³è¡¡å¥–åŠ±æœ€å¤§åŒ–ä¸é—®ç­”èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç”Ÿæˆçš„è¡¨å¾èƒ½å¤Ÿæ•æ‰æœ‰æ•ˆå†³ç­–æ‰€éœ€çš„æ˜¾è‘—å†å²ç»†èŠ‚å’Œé¢„æµ‹ç»“æ„ã€‚å®éªŒåœ¨ç”µå½±å’Œè´­ç‰©æ•°æ®é›†çš„ç”¨æˆ·å»ºæ¨¡ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºDHRsèƒ½ç”Ÿæˆå¯è§£é‡Šçš„æ–‡æœ¬ç”¨æˆ·ç”»åƒã€‚è¿™äº›ç”»åƒä½œä¸ºå……åˆ†ç»Ÿè®¡é‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹ç”±ç”¨æˆ·åå¥½é©±åŠ¨çš„è¡Œä¸ºï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„ä¼˜åŒ–æ§åˆ¶æä¾›äº†ç»“æ„åŒ–çš„å†å²æ‘˜è¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02125v1",
      "published_date": "2025-06-02 18:00:41 UTC",
      "updated_date": "2025-06-02 18:00:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:29:15.952765+00:00"
    },
    {
      "arxiv_id": "2506.02120v2",
      "title": "Random-key genetic algorithms: Principles and applications",
      "title_zh": "éšæœºé”®é—ä¼ ç®—æ³•ï¼šåŸç†ä¸åº”ç”¨",
      "authors": [
        "Mariana A. Londe",
        "Luciana S. Pessoa",
        "Carlos E. Andrade",
        "JosÃ© F. GonÃ§alves",
        "Mauricio G. C. Resende"
      ],
      "abstract": "A random-key genetic algorithm is an evolutionary metaheuristic for discrete and global optimization. Each solution is encoded as a vector of N random keys, where a random key is a real number randomly generated in the continuous interval [0, 1). A decoder maps each vector of random keys to a solution of the optimization problem being solved and computes its cost. The benefit of this approach is that all genetic operators and transformations can be maintained within the unitary hypercube, regardless of the problem being addressed. This enhances the productivity and maintainability of the core framework. The algorithm starts with a population of P vectors of random keys. At each iteration, the vectors are partitioned into two sets: a smaller set of high-valued elite solutions and the remaining non-elite solutions. All elite elements are copied, without change, to the next population. A small number of random-key vectors (the mutants) is added to the population of the next iteration. The remaining elements of the population of the next iteration are generated by combining, with the parametrized uniform crossover of Spears and DeJong (1991), pairs of solutions. This chapter reviews random-key genetic algorithms and describes an effective variant called biased random-key genetic algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶ç»¼è¿°äº†éšæœºé”®é—ä¼ ç®—æ³• (Random-key genetic algorithm, RKGA)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¦»æ•£å’Œå…¨å±€ä¼˜åŒ–çš„è¿›åŒ–å¯å‘å¼ç®—æ³•ã€‚ç®—æ³•å°†è§£ç¼–ç ä¸ºè¿ç»­åŒºé—´ [0, 1) å†…çš„éšæœºé”®å‘é‡ï¼Œå¹¶é€šè¿‡è§£ç å™¨ (decoder) å°†å…¶æ˜ å°„ä¸ºå…·ä½“é—®é¢˜çš„è§£ï¼Œç¡®ä¿äº†æ‰€æœ‰é—ä¼ ç®—å­èƒ½åœ¨å•ä½è¶…ç«‹æ–¹ä½“ (unitary hypercube) å†…è¿è¡Œã€‚è¿™ç§æœºåˆ¶æ˜¾è‘—å¢å¼ºäº†æ ¸å¿ƒæ¡†æ¶çš„ç”Ÿäº§åŠ›ä¸å¯ç»´æŠ¤æ€§ï¼Œä½¿å…¶åœ¨å¤„ç†ä¸åŒä¼˜åŒ–é—®é¢˜æ—¶å…·æœ‰é«˜åº¦é€šç”¨æ€§ã€‚åœ¨è¿›åŒ–è¿‡ç¨‹ä¸­ï¼Œç®—æ³•é€šè¿‡ä¿ç•™ç²¾è‹±è§£ (elite solutions)ã€å¼•å…¥çªå˜å‘é‡ (mutants) ä»¥åŠåº”ç”¨å‚æ•°åŒ–å‡åŒ€äº¤å‰ (parametrized uniform crossover) æ¥æ›´æ–°ç§ç¾¤ã€‚è¯¥ç ”ç©¶è¿˜é‡ç‚¹æ¢è®¨äº†ä¸€ç§åä¸ºæœ‰åéšæœºé”®é—ä¼ ç®—æ³• (Biased random-key genetic algorithms, BRKGA) çš„æ”¹è¿›å˜ä½“ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.NE",
      "comment": "21 pages, 1 figure, 1 table, 1 algorithm, forthcoming in Handbook of Heuristics, 2nd edition, SpringerNature, New York",
      "pdf_url": "https://arxiv.org/pdf/2506.02120v2",
      "published_date": "2025-06-02 18:00:07 UTC",
      "updated_date": "2025-06-04 17:44:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:29:12.709024+00:00"
    },
    {
      "arxiv_id": "2506.01954v1",
      "title": "DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation",
      "title_zh": "DRAGï¼šåŸºäºè¯æ®ä¸å›¾è’¸é¦çš„ä» LLMs åˆ° SLMs çš„ RAG çŸ¥è¯†è¿ç§»ä¸å¹»è§‰ç¼“è§£",
      "authors": [
        "Jennifer Chen",
        "Aidar Myrzakhan",
        "Yaxin Luo",
        "Hassaan Muhammad Khan",
        "Sondos Mahmoud Bsharat",
        "Zhiqiang Shen"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating hallucinated content from Humans. In this work, we introduce $\\texttt{DRAG}$, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph-based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model's predictions with a structured knowledge graph and ranked evidence, $\\texttt{DRAG}$ effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With $\\texttt{DRAG}$, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-sized LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºDRAGçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)çŸ¥è¯†è’¸é¦åˆ°å°è¯­è¨€æ¨¡å‹(SLMs)ä¸­ï¼Œä»¥è§£å†³è®¡ç®—èµ„æºæ¶ˆè€—å¤§å’Œæ¨¡å‹å¹»è§‰ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†åŸºäºè¯æ®(evidence-based)å’ŒçŸ¥è¯†å›¾è°±(knowledge graph-based)çš„è’¸é¦æŠ€æœ¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ˜¾è‘—å‡å°å°ºå¯¸å’Œé™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿ç•™å…³é”®çš„äº‹å®æ€§çŸ¥è¯†ã€‚é€šè¿‡å°†å°æ¨¡å‹çš„é¢„æµ‹ä¸ç»“æ„åŒ–çŸ¥è¯†å›¾è°±å’Œæ’åºåçš„è¯æ®ç›¸å¯¹é½ï¼ŒDRAGæœ‰æ•ˆåœ°å‡è½»äº†å¹»è§‰å¹¶æå‡äº†äº‹å®å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å±•ç¤ºäº†è¯¥æ¡†æ¶å¦‚ä½•ç¼“è§£ç”¨æˆ·éšç§é£é™©ï¼Œå¹¶å¼•å…¥äº†ç›¸åº”çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºMiniRAGç­‰ç°æœ‰ç«äº‰æ–¹æ³•ï¼Œåœ¨ç›¸åŒæ¨¡å‹ä¸‹æ€§èƒ½æå‡æœ€é«˜è¾¾27.7%ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å°å‹è¯­è¨€æ¨¡å‹ä¸Šéƒ¨ç½²é«˜æ•ˆä¸”å¯é çš„æ£€ç´¢ä¸ç”Ÿæˆèƒ½åŠ›æä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Main. Code is available at https://github.com/VILA-Lab/DRAG",
      "pdf_url": "https://arxiv.org/pdf/2506.01954v1",
      "published_date": "2025-06-02 17:59:51 UTC",
      "updated_date": "2025-06-02 17:59:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:29:30.493339+00:00"
    },
    {
      "arxiv_id": "2506.01952v1",
      "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks",
      "title_zh": "WebChoreArenaï¼šåœ¨çœŸå®ç¹çç½‘é¡µä»»åŠ¡ä¸Šè¯„ä¼°ç½‘é¡µæµè§ˆæ™ºèƒ½ä½“",
      "authors": [
        "Atsuyuki Miyai",
        "Zaiying Zhao",
        "Kazuki Egashira",
        "Atsuki Sato",
        "Tatsumi Sunada",
        "Shota Onohara",
        "Hiromasa Yamanishi",
        "Mashiro Toyooka",
        "Kunato Nishina",
        "Ryoma Maeda",
        "Kiyoharu Aizawa",
        "Toshihiko Yamasaki"
      ],
      "abstract": "Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WebChoreArenaï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 532 ä¸ªç²¾å¿ƒç­–åˆ’ä»»åŠ¡çš„å…¨æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç½‘ç»œæµè§ˆæ™ºèƒ½ä½“ï¼ˆweb browsing agentsï¼‰å¤„ç†ç¹çã€å¤æ‚ä¸”è€—æ—¶çš„â€œçäº‹â€ä»»åŠ¡çš„èƒ½åŠ›ã€‚WebChoreArena åœ¨ WebArena æ¨¡æ‹Ÿç¯å¢ƒçš„åŸºç¡€ä¸Šæ„å»ºï¼Œç³»ç»Ÿæ€§åœ°å¼•å…¥äº†å¤§è§„æ¨¡è®°å¿†ï¼ˆMassive Memoryï¼‰ã€æ•°å­¦æ¨ç†è®¡ç®—ï¼ˆCalculationï¼‰å’Œè·¨é¡µé¢é•¿æœŸè®°å¿†ï¼ˆLong-Term Memoryï¼‰ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚å®éªŒè¯„ä¼°äº† GPT-4oã€Claude 3.7 Sonnet å’Œ Gemini 2.5 Pro ç­‰å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œç»“æœæ˜¾ç¤ºè¿™äº›æ¨¡å‹åœ¨å¤„ç†ç¹çä»»åŠ¡æ–¹é¢è™½æœ‰è¿›æ­¥ï¼Œä½†ä»é¢ä¸´ä¸¥å³»è€ƒéªŒã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯æ€§èƒ½é¡¶å°–çš„æ¨¡å‹åœ¨ WebChoreArena ä¸Šçš„è¡¨ç°ä¹Ÿè¿œæœªè¾¾åˆ°ç†æƒ³æ°´å¹³ï¼Œä¸”ç›¸è¾ƒäºåŸºç¡€çš„ WebArena ä»»åŠ¡å±•ç°å‡ºæ›´å¤§çš„æ€§èƒ½å·®è·ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºè¡¡é‡æœ€å…ˆè¿›æ™ºèƒ½ä½“åœ¨è‡ªåŠ¨åŒ–é«˜å¼ºåº¦åŠ³åŠ¨ä»»åŠ¡æ–¹é¢çš„è¿›å±•æä¾›äº†æ›´æ¸…æ™°ã€æ›´å…·æŒ‘æˆ˜æ€§çš„æ ‡å‡†ï¼Œå‡¸æ˜¾äº†å½“å‰æŠ€æœ¯åœ¨é²æ£’å¤„ç†å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Project Page: https://webchorearena.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.01952v1",
      "published_date": "2025-06-02 17:59:45 UTC",
      "updated_date": "2025-06-02 17:59:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:29:10.153542+00:00"
    },
    {
      "arxiv_id": "2506.02097v2",
      "title": "Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation",
      "title_zh": "åŸºäºæ–°å‹åŠ¨æ€è·¯ç”±ä¸åé¦ˆè‡ªé€‚åº”çš„é«˜å“åº”å¤šè½®åœ¨çº¿å¯¹è¯æ··åˆäººå·¥æ™ºèƒ½",
      "authors": [
        "Priyaranjan Pattnayak",
        "Amit Agarwal",
        "Hansa Meghwani",
        "Hitesh Laxmichand Patel",
        "Srikant Panda"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems and large language model (LLM)-powered chatbots have significantly advanced conversational AI by combining generative capabilities with external knowledge retrieval. Despite their success, enterprise-scale deployments face critical challenges, including diverse user queries, high latency, hallucinations, and difficulty integrating frequently updated domain-specific knowledge. This paper introduces a novel hybrid framework that integrates RAG with intent-based canned responses, leveraging predefined high-confidence responses for efficiency while dynamically routing complex or ambiguous queries to the RAG pipeline. Our framework employs a dialogue context manager to ensure coherence in multi-turn interactions and incorporates a feedback loop to refine intents, dynamically adjust confidence thresholds, and expand response coverage over time. Experimental results demonstrate that the proposed framework achieves a balance of high accuracy (95\\%) and low latency (180ms), outperforming RAG and intent-based systems across diverse query types, positioning it as a scalable and adaptive solution for enterprise conversational AI applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆ AI æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ä¸šçº§éƒ¨ç½²ä¸­ Retrieval-Augmented Generation (RAG) ç³»ç»Ÿé¢ä¸´çš„é«˜å»¶è¿Ÿã€å¹»è§‰åŠé¢†åŸŸçŸ¥è¯†é›†æˆå›°éš¾ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°ç»“åˆäº†åŸºäºæ„å›¾çš„é¢„è®¾å“åº” (intent-based canned responses) ä¸ RAG æµæ°´çº¿ï¼Œåˆ©ç”¨åŠ¨æ€è·¯ç”±æœºåˆ¶é«˜æ•ˆå¤„ç†å¤æ‚æˆ–æ¨¡ç³Šçš„æŸ¥è¯¢ã€‚ç³»ç»Ÿå†…ç½®çš„å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨ (dialogue context manager) ç¡®ä¿äº†å¤šè½®äº¤äº’çš„è¿è´¯æ€§ï¼Œå¹¶é€šè¿‡åé¦ˆå›è·¯ (feedback loop) åŠ¨æ€è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼å¹¶ä¸æ–­ä¼˜åŒ–æ„å›¾è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒ 95% é«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°äº†ä»…ä¸º 180ms çš„ä½å»¶è¿Ÿï¼Œæ€§èƒ½è¡¨ç°æ˜¾è‘—ä¼˜äºå•ä¸€çš„ RAG æˆ–æ„å›¾ç³»ç»Ÿã€‚è¿™ä¸€æˆæœä¸ºæ„å»ºå¯æ‰©å±•ä¸”å…·å¤‡è‡ªé€‚åº”èƒ½åŠ›çš„å•†ä¸šçº§å¯¹è¯ AI åº”ç”¨æä¾›äº†é«˜æ•ˆä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Proceedings of the 4th International Workshop on Knowledge Augmented Methods for Natural Language Processing in NAACL 2025, pages 215 to 229, Albuquerque, New Mexico, USA. Association for Computational Linguistics",
      "pdf_url": "https://arxiv.org/pdf/2506.02097v2",
      "published_date": "2025-06-02 17:59:27 UTC",
      "updated_date": "2025-06-25 07:18:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:29:19.476706+00:00"
    },
    {
      "arxiv_id": "2506.01944v1",
      "title": "Feel the Force: Contact-Driven Learning from Humans",
      "title_zh": "Feel the Forceï¼šåŸºäºäººç±»çš„æ¥è§¦é©±åŠ¨å­¦ä¹ ",
      "authors": [
        "Ademi Adeniji",
        "Zhuoran Chen",
        "Vincent Liu",
        "Venkatesh Pattabiraman",
        "Raunaq Bhirangi",
        "Siddhant Haldar",
        "Pieter Abbeel",
        "Lerrel Pinto"
      ],
      "abstract": "Controlling fine-grained forces during manipulation remains a core challenge in robotics. While robot policies learned from robot-collected data or simulation show promise, they struggle to generalize across the diverse range of real-world interactions. Learning directly from humans offers a scalable solution, enabling demonstrators to perform skills in their natural embodiment and in everyday environments. However, visual demonstrations alone lack the information needed to infer precise contact forces. We present FeelTheForce (FTF): a robot learning system that models human tactile behavior to learn force-sensitive manipulation. Using a tactile glove to measure contact forces and a vision-based model to estimate hand pose, we train a closed-loop policy that continuously predicts the forces needed for manipulation. This policy is re-targeted to a Franka Panda robot with tactile gripper sensors using shared visual and action representations. At execution, a PD controller modulates gripper closure to track predicted forces-enabling precise, force-aware control. Our approach grounds robust low-level force control in scalable human supervision, achieving a 77% success rate across 5 force-sensitive manipulation tasks. Code and videos are available at https://feel-the-force-ftf.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FeelTheForce (FTF)ï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³æœºå™¨äººç²¾ç»†åŠ›æ§(fine-grained force control)æŒ‘æˆ˜çš„å­¦ä¹ ç³»ç»Ÿã€‚ç”±äºä¼ ç»Ÿçš„è§†è§‰æ¼”ç¤ºå¾€å¾€ç¼ºä¹æ¨æ–­ç²¾ç¡®æ¥è§¦åŠ›æ‰€éœ€çš„ä¿¡æ¯ï¼ŒFTFåˆ©ç”¨è§¦è§‰æ‰‹å¥—(tactile glove)æµ‹é‡äººç±»æ“ä½œæ—¶çš„åŠ›æ•°æ®ï¼Œå¹¶ç»“åˆè§†è§‰æ¨¡å‹ä¼°è®¡æ‰‹éƒ¨å§¿æ€ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªèƒ½å¤ŸæŒç»­é¢„æµ‹æ“ä½œåŠ›çš„é—­ç¯ç­–ç•¥(closed-loop policy)ã€‚è¯¥ç­–ç•¥é€šè¿‡å…±äº«çš„è§†è§‰å’ŒåŠ¨ä½œè¡¨ç¤ºï¼Œè¢«é‡å®šå‘(re-targeted)è‡³é…å¤‡è§¦è§‰ä¼ æ„Ÿå™¨çš„Franka Pandaæœºå™¨äººã€‚åœ¨æ‰§è¡Œé˜¶æ®µï¼Œç³»ç»Ÿé€šè¿‡PDæ§åˆ¶å™¨è°ƒèŠ‚å¤¹çˆªé—­åˆä»¥è¿½è¸ªé¢„æµ‹çš„åŠ›ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„åŠ›æ„ŸçŸ¥æ§åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒFTFåœ¨5é¡¹åŠ›æ•æ„Ÿå‹æ“ä½œä»»åŠ¡ä¸­è¾¾åˆ°äº†77%çš„æˆåŠŸç‡ï¼Œè¯æ˜äº†åˆ©ç”¨å¯æ‰©å±•çš„äººç±»ç›‘ç£æ¥å®ç°é²æ£’åº•å±‚åŠ›æ§çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01944v1",
      "published_date": "2025-06-02 17:57:52 UTC",
      "updated_date": "2025-06-02 17:57:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:29:29.790104+00:00"
    },
    {
      "arxiv_id": "2506.01939v2",
      "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
      "title_zh": "è¶…è¶ŠäºŒå…«å®šå¾‹ï¼šå°‘æ•°é«˜ç†µè¯å…ƒé©±åŠ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„æœ‰æ•ˆå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Shenzhi Wang",
        "Le Yu",
        "Chang Gao",
        "Chujie Zheng",
        "Shixuan Liu",
        "Rui Lu",
        "Kai Dang",
        "Xionghui Chen",
        "Jianxin Yang",
        "Zhenru Zhang",
        "Yuqiong Liu",
        "An Yang",
        "Andrew Zhao",
        "Yang Yue",
        "Shiji Song",
        "Bowen Yu",
        "Gao Huang",
        "Junyang Lin"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ä»¤ç‰Œç†µæ¨¡å¼(token entropy patterns)çš„åˆ›æ–°è§†è§’æ¢ç´¢äº†å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ (Reinforcement Learning with Verifiable Rewards, RLVR)å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†èƒ½åŠ›çš„æå‡æœºåˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æ€ç»´é“¾(Chain-of-Thought)æ¨ç†ä¸­ï¼Œåªæœ‰æå°‘æ•°é«˜ç†µä»¤ç‰Œ(high-entropy tokens)ä½œä¸ºå…³é”®çš„åˆ†å‰ç‚¹(forking tokens)å†³å®šäº†æ¨¡å‹çš„æ¨ç†è·¯å¾„ã€‚å®éªŒè¡¨æ˜ï¼ŒRLVRè®­ç»ƒä¸»è¦é€šè¿‡è°ƒæ•´è¿™äº›é«˜ç†µä»¤ç‰Œçš„ç†µæ¥ä¼˜åŒ–æ€§èƒ½ï¼Œè€Œéæ”¹å˜æ•´ä½“æ¨¡å¼ã€‚åŸºäºæ­¤å‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›æ–¹æ¡ˆï¼Œå°†ç­–ç•¥æ¢¯åº¦æ›´æ–°(policy gradient updates)é™åˆ¶åœ¨è¿™äº›åˆ†å‰ä»¤ç‰Œä¸Šã€‚åœ¨Qwen3ç³»åˆ—æ¨¡å‹ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºï¼Œä»…ä½¿ç”¨20%çš„ä»¤ç‰Œè¿›è¡Œæ›´æ–°å³å¯è¾¾åˆ°ç”šè‡³æ˜¾è‘—è¶…è¿‡å…¨æ¢¯åº¦æ›´æ–°çš„æ•ˆæœï¼Œå…¶ä¸­Qwen3-32Båœ¨AIME'25ä¸Šçš„è¡¨ç°æå‡äº†11.04ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†RLVRçš„æœ‰æ•ˆæ€§ä¸»è¦æºäºå¯¹å†³å®šæ¨ç†æ–¹å‘çš„å°‘æ•°é«˜ç†µä»¤ç‰Œçš„ä¼˜åŒ–ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿçš„80/20æ³•åˆ™ï¼Œä¸ºè¿›ä¸€æ­¥æå‡å¤§æ¨¡å‹æ¨ç†æ•ˆç‡æä¾›äº†é«˜æ•ˆä¸”å…·å¤‡æ‰©å±•æ€§çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NeurIPS 2025. 25 pages, 17 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.01939v2",
      "published_date": "2025-06-02 17:54:39 UTC",
      "updated_date": "2025-11-13 10:08:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:29:56.392524+00:00"
    },
    {
      "arxiv_id": "2506.01934v1",
      "title": "RoboEgo System Card: An Omnimodal Model with Native Full Duplexity",
      "title_zh": "RoboEgo ç³»ç»ŸæŠ¥å‘Šï¼šå…·æœ‰åŸç”Ÿå…¨åŒå·¥èƒ½åŠ›çš„å…¨æ¨¡æ€æ¨¡å‹",
      "authors": [
        "Yiqun Yao",
        "Xiang Li",
        "Xin Jiang",
        "Xuezhi Fang",
        "Naitong Yu",
        "Aixin Sun",
        "Yequan Wang"
      ],
      "abstract": "Humans naturally process real-world multimodal information in a full-duplex manner. In artificial intelligence, replicating this capability is essential for advancing model development and deployment, particularly in embodied contexts. The development of multimodal models faces two primary challenges: (1) effectively handling more than three modalities-such as vision, audio, and text; and (2) delivering full-duplex responses to rapidly evolving human instructions. To facilitate research on models that support both omnimodal processing and full duplexity, we present RoboEgo (alias: FLM-Ego), a unified model system designed to address both challenges. RoboEgo incorporates a backbone architecture and algorithms that natively support full duplexity, achieving a theoretical duplex latency of 80 ms. In streaming visually grounded conversations under real-world conditions, RoboEgo exhibits superior responsiveness and speech naturalness, while maintaining comparable content qualities to state-of-the-art semi-duplex omnimodal models-a feat previously considered unattainable by native full-duplex systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RoboEgoï¼ˆåˆç§°FLM-Egoï¼‰ï¼Œä¸€ä¸ªæ—¨åœ¨æ”¯æŒOmnimodalå¤„ç†å’ŒåŸç”ŸFull Duplexityçš„ç»Ÿä¸€æ¨¡å‹ç³»ç»Ÿï¼Œä»¥æ¨¡æ‹Ÿäººç±»å¤„ç†å®æ—¶å¤šæ¨¡æ€ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ›æ–°çš„Backboneæ¶æ„å’Œç®—æ³•ï¼Œè§£å†³äº†è¶…è¿‡ä¸‰ç±»æ¨¡æ€ï¼ˆå¦‚Visionã€Audioå’ŒTextï¼‰çš„ååŒå¤„ç†ä»¥åŠé’ˆå¯¹åŠ¨æ€äººç±»æŒ‡ä»¤çš„å¿«é€Ÿå“åº”éš¾é¢˜ï¼Œå®ç°äº†ä»…ä¸º80msçš„ç†è®ºåŒå·¥å»¶è¿Ÿã€‚åœ¨çœŸå®ç¯å¢ƒä¸‹çš„æµå¼è§†è§‰ä¼šè¯ä¸­ï¼ŒRoboEgoå±•ç°äº†å“è¶Šçš„å“åº”é€Ÿåº¦å’ŒSpeech Naturalnessã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¿æŒæé«˜å“åº”æ€§èƒ½çš„åŒæ—¶ï¼Œå…¶å†…å®¹ç”Ÿæˆè´¨é‡å¯ä¸å½“å‰æœ€å…ˆè¿›çš„Semi-duplexå¤šæ¨¡æ€æ¨¡å‹ç›¸åª²ç¾ã€‚è¿™çªç ´äº†ä»¥å¾€åŸç”Ÿå…¨åŒå·¥ç³»ç»Ÿéš¾ä»¥å…¼é¡¾å“åº”é€Ÿåº¦ä¸å†…å®¹è´¨é‡çš„å±€é™ï¼Œä¸ºå…·èº«æ™ºèƒ½é¢†åŸŸçš„å®æ—¶äº¤äº’å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01934v1",
      "published_date": "2025-06-02 17:53:10 UTC",
      "updated_date": "2025-06-02 17:53:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:30:06.637907+00:00"
    },
    {
      "arxiv_id": "2506.01931v1",
      "title": "Red Teaming AI Policy: A Taxonomy of Avoision and the EU AI Act",
      "title_zh": "äººå·¥æ™ºèƒ½æ”¿ç­–çº¢é˜Ÿæµ‹è¯•ï¼šè§„é¿ç­–ç•¥åˆ†ç±»ä¸æ¬§ç›Ÿã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹",
      "authors": [
        "Rui-Jie Yew",
        "Bill Marino",
        "Suresh Venkatasubramanian"
      ],
      "abstract": "The shape of AI regulation is beginning to emerge, most prominently through the EU AI Act (the \"AIA\"). By 2027, the AIA will be in full effect, and firms are starting to adjust their behavior in light of this new law. In this paper, we present a framework and taxonomy for reasoning about \"avoision\" -- conduct that walks the line between legal avoidance and evasion -- that firms might engage in so as to minimize the regulatory burden the AIA poses. We organize these avoision strategies around three \"tiers\" of increasing AIA exposure that regulated entities face depending on: whether their activities are (1) within scope of the AIA, (2) exempted from provisions of the AIA, or are (3) placed in a category with higher regulatory scrutiny. In each of these tiers and for each strategy, we specify the organizational and technological forms through which avoision may manifest. Our goal is to provide an adversarial framework for \"red teaming\" the AIA and AI regulation on the horizon.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆ(EU AI Act, AIA)æå‡ºäº†ä¸€ä¸ªå…³äºâ€œè§„é¿è¡Œä¸ºâ€(avoision)çš„åˆ†ææ¡†æ¶å’Œåˆ†ç±»æ³•ï¼Œæ—¨åœ¨æ¢è®¨ä¼ä¸šä¸ºå‡è½»ç›‘ç®¡è´Ÿæ‹…è€Œé‡‡å–çš„å¤„äºæ³•å¾‹å›é¿ä¸è§„é¿ä¹‹é—´æ¨¡ç³Šåœ°å¸¦çš„è¡Œä¸ºã€‚ä½œè€…å›´ç»•å—ç›‘ç®¡å®ä½“é¢ä¸´çš„ä¸‰å±‚é€’è¿›çš„AIAæš´éœ²é£é™©ç»„ç»‡äº†è¿™äº›avoisionç­–ç•¥ï¼Œå…·ä½“åŒ…æ‹¬æ´»åŠ¨æ˜¯å¦å±äºç›‘ç®¡èŒƒå›´ã€æ˜¯å¦è±å…äºç‰¹å®šæ¡æ¬¾ï¼Œä»¥åŠæ˜¯å¦è¢«å½’å…¥æ›´é«˜ç›‘ç®¡è¦æ±‚çš„ç±»åˆ«ã€‚é’ˆå¯¹æ¯ä¸€å±‚çº§å’Œæ¯ç§ç­–ç•¥ï¼Œè®ºæ–‡è¯¦ç»†é˜æ˜äº†è§„é¿è¡Œä¸ºå¯èƒ½å‘ˆç°çš„ç»„ç»‡å½¢å¼å’ŒæŠ€æœ¯å½¢å¼ã€‚è¿™é¡¹å·¥ä½œçš„æ ¸å¿ƒç›®æ ‡æ˜¯ä¸ºçº¢é˜Ÿæµ‹è¯•(red teaming)æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆåŠæœªæ¥çš„äººå·¥æ™ºèƒ½ç›‘ç®¡æä¾›ä¸€ç§å¯¹æŠ—æ€§æ¡†æ¶ï¼Œæ­ç¤ºä¼ä¸šå¦‚ä½•é€šè¿‡ç»„ç»‡å’ŒæŠ€æœ¯æ‰‹æ®µæœ€å°åŒ–æ³•å¾‹åˆè§„å‹åŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Forthcoming at the 2025 ACM Conference on Fairness, Accountability, and Transparency",
      "pdf_url": "https://arxiv.org/pdf/2506.01931v1",
      "published_date": "2025-06-02 17:48:54 UTC",
      "updated_date": "2025-06-02 17:48:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-23T17:30:50.645368+00:00"
    },
    {
      "arxiv_id": "2506.15716v2",
      "title": "Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies",
      "title_zh": "å€™è¡¥é›†ç»“ï¼ä¸ºå…¬æ°‘è®®ä¼šé´é€‰æœ€ä¼˜å€™è¡¥æˆå‘˜",
      "authors": [
        "Angelos Assos",
        "Carmel Baharav",
        "Bailey Flanigan",
        "Ariel Procaccia"
      ],
      "abstract": "Citizens' assemblies are an increasingly influential form of deliberative democracy, where randomly selected people discuss policy questions. The legitimacy of these assemblies hinges on their representation of the broader population, but participant dropout often leads to an unbalanced composition. In practice, dropouts are replaced by preselected alternates, but existing methods do not address how to choose these alternates. To address this gap, we introduce an optimization framework for alternate selection. Our algorithmic approach, which leverages learning-theoretic machinery, estimates dropout probabilities using historical data and selects alternates to minimize expected misrepresentation. Our theoretical bounds provide guarantees on sample complexity (with implications for computational efficiency) and on loss due to dropout probability mis-estimation. Empirical evaluation using real-world data demonstrates that, compared to the status quo, our method significantly improves representation while requiring fewer alternates.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Citizens' assembliesä¸­å› å‚ä¸è€…é€€å‡ºå¯¼è‡´ä»£è¡¨æ€§å¤±è¡¡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºé€‰æ‹©æ›¿è¡¥äººå‘˜(alternates)çš„ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥ç®—æ³•æ–¹æ³•åˆ©ç”¨learning-theoretic machineryï¼Œé€šè¿‡å†å²æ•°æ®ä¼°ç®—é€€å‡ºæ¦‚ç‡ï¼Œå¹¶æ®æ­¤é€‰æ‹©æ›¿è¡¥ä»¥æœ€å°åŒ–é¢„æœŸçš„ä»£è¡¨æ€§åå·®(misrepresentation)ã€‚ç†è®ºåˆ†æä¸ºsample complexityä»¥åŠå› é€€å‡ºæ¦‚ç‡ä¼°ç®—è¯¯å·®å¯¼è‡´çš„æŸå¤±æä¾›äº†è¾¹ç•Œä¿è¯ã€‚åœ¨çœŸå®æ•°æ®ä¸Šçš„å®éªŒè¯„ä¼°è¯æ˜ï¼Œä¸ç°æœ‰çš„status quoç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡ä»£è¡¨æ€§çš„åŒæ—¶ï¼Œè¿˜å‡å°‘äº†æ‰€éœ€çš„æ›¿è¡¥äººå‘˜æ•°é‡ã€‚è¯¥æ¡†æ¶ä¸ºæå‡åå•†æ°‘ä¸»(deliberative democracy)çš„åˆæ³•æ€§ä¸æ•ˆç‡æä¾›äº†é‡è¦çš„ç®—æ³•æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15716v2",
      "published_date": "2025-06-02 17:48:33 UTC",
      "updated_date": "2025-08-11 19:34:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:29:58.382739+00:00"
    },
    {
      "arxiv_id": "2506.01929v1",
      "title": "Image Generation from Contextually-Contradictory Prompts",
      "title_zh": "åŸºäºä¸Šä¸‹æ–‡çŸ›ç›¾æç¤ºè¯çš„å›¾åƒç”Ÿæˆ",
      "authors": [
        "Saar Huberman",
        "Or Patashnik",
        "Omer Dahary",
        "Ron Mokady",
        "Daniel Cohen-Or"
      ],
      "abstract": "Text-to-image diffusion models excel at generating high-quality, diverse images from natural language prompts. However, they often fail to produce semantically accurate results when the prompt contains concept combinations that contradict their learned priors. We define this failure mode as contextual contradiction, where one concept implicitly negates another due to entangled associations learned during training. To address this, we propose a stage-aware prompt decomposition framework that guides the denoising process using a sequence of proxy prompts. Each proxy prompt is constructed to match the semantic content expected to emerge at a specific stage of denoising, while ensuring contextual coherence. To construct these proxy prompts, we leverage a large language model (LLM) to analyze the target prompt, identify contradictions, and generate alternative expressions that preserve the original intent while resolving contextual conflicts. By aligning prompt information with the denoising progression, our method enables fine-grained semantic control and accurate image generation in the presence of contextual contradictions. Experiments across a variety of challenging prompts show substantial improvements in alignment to the textual prompt.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹ (Text-to-image diffusion models) åœ¨å¤„ç†åŒ…å«ä¸å­¦ä¹ å…ˆéªŒç›¸çŸ›ç›¾çš„æ¦‚å¿µç»„åˆæ—¶å‡ºç°çš„ä¸Šä¸‹æ–‡çŸ›ç›¾ (contextual contradiction) é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å¤±æ•ˆæ¨¡å¼ï¼Œä½œè€…æå‡ºäº†ä¸€ç§é˜¶æ®µæ„ŸçŸ¥æç¤ºè¯åˆ†è§£æ¡†æ¶ (stage-aware prompt decomposition framework)ï¼Œé€šè¿‡ä¸€ç³»åˆ—ä»£ç†æç¤ºè¯ (proxy prompts) å¼•å¯¼å»å™ªè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) åˆ†æç›®æ ‡æç¤ºè¯å¹¶è¯†åˆ«çŸ›ç›¾ç‚¹ï¼Œä»è€Œç”Ÿæˆæ—¢èƒ½ä¿ç•™åŸå§‹æ„å›¾åˆèƒ½åŒ–è§£å†²çªçš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡å°†æç¤ºè¯ä¿¡æ¯ä¸å»å™ªè¿›ç¨‹ç²¾ç¡®å¯¹é½ï¼Œè¯¥æ–¹æ³•å®ç°äº†ç»†ç²’åº¦çš„è¯­ä¹‰æ§åˆ¶ï¼Œç¡®ä¿åœ¨å­˜åœ¨ä¸Šä¸‹æ–‡çŸ›ç›¾çš„æƒ…å†µä¸‹ä»èƒ½ç”Ÿæˆå‡†ç¡®å›¾åƒã€‚å¤šé¡¹æŒ‘æˆ˜æ€§å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬å¯¹é½å‡†ç¡®æ€§æ–¹é¢ç›¸è¾ƒäºç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "Project page: https://tdpc2025.github.io/SAP/",
      "pdf_url": "https://arxiv.org/pdf/2506.01929v1",
      "published_date": "2025-06-02 17:48:12 UTC",
      "updated_date": "2025-06-02 17:48:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:31:17.394646+00:00"
    },
    {
      "arxiv_id": "2506.01927v1",
      "title": "Online Competitive Information Gathering for Partially Observable Trajectory Games",
      "title_zh": "éƒ¨åˆ†å¯è§‚æµ‹è½¨è¿¹åšå¼ˆä¸­çš„åœ¨çº¿ç«äº‰æ€§ä¿¡æ¯é‡‡é›†",
      "authors": [
        "Mel Krusniak",
        "Hang Xu",
        "Parker Palermo",
        "Forrest Laine"
      ],
      "abstract": "Game-theoretic agents must make plans that optimally gather information about their opponents. These problems are modeled by partially observable stochastic games (POSGs), but planning in fully continuous POSGs is intractable without heavy offline computation or assumptions on the order of belief maintained by each player. We formulate a finite history/horizon refinement of POSGs which admits competitive information gathering behavior in trajectory space, and through a series of approximations, we present an online method for computing rational trajectory plans in these games which leverages particle-based estimations of the joint state space and performs stochastic gradient play. We also provide the necessary adjustments required to deploy this method on individual agents. The method is tested in continuous pursuit-evasion and warehouse-pickup scenarios (alongside extensions to $N > 2$ players and to more complex environments with visual and physical obstacles), demonstrating evidence of active information gathering and outperforming passive competitors.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éƒ¨åˆ†å¯è§‚æµ‹éšæœºåšå¼ˆ (Partially Observable Stochastic Games, POSGs) ä¸­æ™ºèƒ½ä½“éš¾ä»¥é«˜æ•ˆè·å–å¯¹æ‰‹ä¿¡æ¯å¹¶è¿›è¡Œåœ¨çº¿è§„åˆ’çš„éš¾é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿ç»­ç©ºé—´ä¸­é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬å’Œå¤æ‚ä¿¡å¿µç»´æŠ¤é—®é¢˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è½¨è¿¹ç©ºé—´çš„æœ‰é™å†å²/æ—¶ç•Œ (Finite History/Horizon) POSGs æ”¹è¿›æ–¹æ¡ˆï¼Œæ—¨åœ¨å®ç°ç«äº‰æ€§çš„ä¿¡æ¯é‡‡é›†è¡Œä¸ºã€‚ç ”å‘å›¢é˜Ÿé€šè¿‡ä¸€ç³»åˆ—è¿‘ä¼¼å¤„ç†ï¼Œå¼€å‘äº†ä¸€ç§åœ¨çº¿è®¡ç®—ç†æ€§è½¨è¿¹è§„åˆ’çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºäºç²’å­çš„ä¼°è®¡ (Particle-based Estimations) æ¥å¤„ç†è”åˆçŠ¶æ€ç©ºé—´ï¼Œå¹¶æ‰§è¡Œéšæœºæ¢¯åº¦åšå¼ˆ (Stochastic Gradient Play)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†å°†è¯¥æ–¹æ³•éƒ¨ç½²äºå•ä¸ªæ™ºèƒ½ä½“æ‰€éœ€çš„å¿…è¦è°ƒæ•´ï¼Œå¹¶æ”¯æŒæ‰©å±•åˆ°å¤šäºä¸¤ä¸ªç©å®¶åŠåŒ…å«è§†è§‰å’Œç‰©ç†éšœç¢çš„å¤æ‚ç¯å¢ƒã€‚åœ¨è¿ç»­è¿½é€ƒåšå¼ˆ (Pursuit-Evasion) å’Œä»“åº“å–è´§ (Warehouse-Pickup) åœºæ™¯çš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å±•ç°å‡ºæ˜¾è‘—çš„ä¸»åŠ¨ä¿¡æ¯é‡‡é›†èƒ½åŠ›ï¼Œæ€§èƒ½æ˜æ˜¾ä¼˜äºè¢«åŠ¨ç«äº‰å¯¹æ‰‹ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.GT",
      "comment": "Accepted at RSS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01927v1",
      "published_date": "2025-06-02 17:45:58 UTC",
      "updated_date": "2025-06-02 17:45:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:31:19.185523+00:00"
    },
    {
      "arxiv_id": "2506.01926v2",
      "title": "Large language models can learn and generalize steganographic chain-of-thought under process supervision",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨è¿‡ç¨‹ç›‘ç£ä¸‹èƒ½å¤Ÿå­¦ä¹ å¹¶æ³›åŒ–éšå†™æ€ç»´é“¾",
      "authors": [
        "Joey Skaf",
        "Luis Ibanez-Lissen",
        "Robert McCarthy",
        "Connor Watts",
        "Vasil Georgiv",
        "Hannes Whittingham",
        "Lorena Gonzalez-Manzano",
        "David Lindner",
        "Cameron Tice",
        "Edward James Young",
        "Puria Radmard"
      ],
      "abstract": "Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. However, recent works have shown that banning the mention of a specific example of reward hacking causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior, threatening the reliability of CoT monitoring. We provide an extension to these results with regard to the ability of models to learn a specific type of obfuscated reasoning: steganography. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning.We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¿‡ç¨‹ç›‘ç£ä¸‹å­¦ä¹ å¹¶æ³›åŒ–éšå†™æ€ç»´é“¾(steganographic chain-of-thought)çš„èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡åœ¨æ¨ç†è½¨è¿¹ä¸­æƒ©ç½šç‰¹å®šå­—ç¬¦ä¸²çš„ä½¿ç”¨ï¼Œå‘ç°æ¨¡å‹ä¼šå­¦ä¼šä½¿ç”¨æ›¿ä»£å­—ç¬¦ä¸²è¿›è¡Œä¿¡æ¯ç¼–ç ï¼Œè€Œä¸ä¼šæ”¹å˜å…¶æ‰§è¡Œä»»åŠ¡çš„åº•å±‚é€»è¾‘ï¼Œè¯æ˜äº†æ¨¡å‹èƒ½å¤Ÿå­¦ä¼šéšå†™æœ¯(steganography)æ¥éšè—æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œæ¨¡å‹å…·å¤‡æ³›åŒ–è¿™ç§ç¼–ç æ–¹æ¡ˆçš„èƒ½åŠ›ï¼Œå³å½“å—æƒ©ç½šå­—ç¬¦ä¸²å±äºæŸä¸€ç‰¹å®šç±»åˆ«æ—¶ï¼Œæ¨¡å‹ä¸ä»…èƒ½æ›¿æ¢è®­ç»ƒé›†ä¸­çš„å·²çŸ¥å­—ç¬¦ä¸²ï¼Œè¿˜èƒ½ä¸ºè¯¥ç±»åˆ«çš„æ‰€æœ‰æˆå‘˜å¼€å‘å‡ºé€šç”¨çš„ç¼–ç æ¨¡å¼å¹¶åº”ç”¨äºæœªè§è¿‡çš„æµ‹è¯•æ•°æ®ã€‚è¿™ä¸€ç ”ç©¶ç»“æœæ­ç¤ºäº†ä¾é æ€ç»´é“¾ç›‘æ§(CoT monitoring)æ¥è¯†åˆ«æ¨¡å‹æ„å›¾å’Œè§„åˆ’æ—¶å¯èƒ½é¢ä¸´çš„å®‰å…¨æ€§æŒ‘æˆ˜ï¼Œå³æ¨¡å‹è¡Œä¸ºçš„æŒä¹…æ€§å¯èƒ½ä¸æ¨ç†è½¨è¿¹çš„æ··æ·†å¹¶å­˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages main text, 3 figures main text, 17 pages supplementary material, 1 figure supplementary material, accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01926v2",
      "published_date": "2025-06-02 17:45:15 UTC",
      "updated_date": "2025-12-04 16:05:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:31:17.591758+00:00"
    },
    {
      "arxiv_id": "2506.01923v2",
      "title": "TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation",
      "title_zh": "TaxaDiffusionï¼šé¢å‘ç»†ç²’åº¦ç‰©ç§ç”Ÿæˆçš„æ¸è¿›å¼è®­ç»ƒæ‰©æ•£æ¨¡å‹",
      "authors": [
        "Amin Karimi Monsefi",
        "Mridul Khurana",
        "Rajiv Ramnath",
        "Anuj Karpatne",
        "Wei-Lun Chao",
        "Cheng Zhang"
      ],
      "abstract": "We propose TaxaDiffusion, a taxonomy-informed training framework for diffusion models to generate fine-grained animal images with high morphological and identity accuracy. Unlike standard approaches that treat each species as an independent category, TaxaDiffusion incorporates domain knowledge that many species exhibit strong visual similarities, with distinctions often residing in subtle variations of shape, pattern, and color. To exploit these relationships, TaxaDiffusion progressively trains conditioned diffusion models across different taxonomic levels -- starting from broad classifications such as Class and Order, refining through Family and Genus, and ultimately distinguishing at the Species level. This hierarchical learning strategy first captures coarse-grained morphological traits shared by species with common ancestors, facilitating knowledge transfer before refining fine-grained differences for species-level distinction. As a result, TaxaDiffusion enables accurate generation even with limited training samples per species. Extensive experiments on three fine-grained animal datasets demonstrate that outperforms existing approaches, achieving superior fidelity in fine-grained animal image generation. Project page: https://amink8.github.io/TaxaDiffusion/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TaxaDiffusionï¼Œä¸€ç§å¼•å…¥åˆ†ç±»å­¦ä¿¡æ¯çš„ Diffusion Model æ¸è¿›å¼è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç»†ç²’åº¦åŠ¨ç‰©å›¾åƒç”Ÿæˆçš„å½¢æ€ä¸èº«ä»½å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç‰©ç§é—´çš„è§†è§‰ç›¸ä¼¼æ€§ï¼Œé€šè¿‡ä» Class å’Œ Order åˆ° Familyã€Genus ä»¥åŠæœ€ç»ˆ Species çš„å±‚çº§åŒ–è®­ç»ƒï¼Œä¼˜å…ˆæ•æ‰ç²—ç²’åº¦çš„å½¢æ€ç‰¹å¾å¹¶å®ç°çŸ¥è¯†è¿ç§»ã€‚è¿™ç§ç­–ç•¥æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹ç‰©ç§é—´ç»†å¾®å½¢çŠ¶ã€å›¾æ¡ˆå’Œé¢œè‰²å·®å¼‚çš„åŒºåˆ†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨è®­ç»ƒæ ·æœ¬æœ‰é™çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒé«˜ç²¾åº¦ã€‚åœ¨ä¸‰ä¸ªç»†ç²’åº¦åŠ¨ç‰©æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒTaxaDiffusion åœ¨ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„ Fine-Grained Species Generation æ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01923v2",
      "published_date": "2025-06-02 17:43:55 UTC",
      "updated_date": "2025-06-25 21:02:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:31:29.590568+00:00"
    },
    {
      "arxiv_id": "2506.01921v7",
      "title": "MedEBench: Diagnosing Reliability in Text-Guided Medical Image Editing",
      "title_zh": "MedEBenchï¼šæ–‡æœ¬å¼•å¯¼åŒ»å­¦å›¾åƒç¼–è¾‘ä¸­çš„å¯é æ€§è¯Šæ–­",
      "authors": [
        "Minghao Liu",
        "Zhitao He",
        "Zhiyuan Fan",
        "Qingyun Wang",
        "Yi R. Fung"
      ],
      "abstract": "Text-guided image editing has seen significant progress in natural image domains, but its application in medical imaging remains limited and lacks standardized evaluation frameworks. Such editing could revolutionize clinical practices by enabling personalized surgical planning, enhancing medical education, and improving patient communication. To bridge this gap, we introduce MedEBench1, a robust benchmark designed to diagnose reliability in text-guided medical image editing. MedEBench consists of 1,182 clinically curated image-prompt pairs covering 70 distinct editing tasks and 13 anatomical regions. It contributes in three key areas: (1) a clinically grounded evaluation framework that measures Editing Accuracy, Context Preservation, and Visual Quality, complemented by detailed descriptions of intended edits and corresponding Region-of-Interest (ROI) masks; (2) a comprehensive comparison of seven state-of-theart models, revealing consistent patterns of failure; and (3) a diagnostic error analysis technique that leverages attention alignment, using Intersection-over-Union (IoU) between model attention maps and ROI masks to identify mislocalization issues, where models erroneously focus on incorrect anatomical regions. MedEBench sets the stage for developing more reliable and clinically effective text-guided medical image editing tools.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å½±åƒé¢†åŸŸç¼ºä¹æ ‡å‡†åŒ–æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘è¯„ä¼°æ¡†æ¶çš„ç°çŠ¶ï¼Œæå‡ºäº† MedEBench è¿™ä¸€ç¨³å¥çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯Šæ–­è¯¥é¢†åŸŸçš„ç¼–è¾‘å¯é æ€§ã€‚MedEBench åŒ…å« 1,182 å¯¹ç»è¿‡ä¸´åºŠç­›é€‰çš„å›¾åƒ-æç¤ºå¯¹ï¼Œæ¶µç›–äº† 70 ä¸ªä¸åŒçš„ç¼–è¾‘ä»»åŠ¡å’Œ 13 ä¸ªè§£å‰–åŒºåŸŸã€‚è¯¥ç ”ç©¶è´¡çŒ®äº†ä¸€ä¸ªåŒ…å« Editing Accuracyã€Context Preservation å’Œ Visual Quality çš„ä¸´åºŠè¯„ä¼°æ¡†æ¶ï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„ç¼–è¾‘æè¿°å’Œ Region-of-Interest (ROI) æ©ç ã€‚é€šè¿‡å¯¹ä¸ƒç§ state-of-the-art æ¨¡å‹è¿›è¡Œå…¨é¢å¯¹æ¯”ï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹æ™®éå­˜åœ¨çš„å¤±è´¥æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨æ³¨æ„åŠ›å›¾ä¸ ROI æ©ç ä¹‹é—´çš„ Intersection-over-Union (IoU) è¿›è¡Œäº†è¯Šæ–­æ€§é”™è¯¯åˆ†æï¼Œæœ‰æ•ˆè¯†åˆ«äº†æ¨¡å‹å…³æ³¨é”™è¯¯è§£å‰–åŒºåŸŸçš„è¯¯å®šä½é—®é¢˜ã€‚MedEBench ä¸ºå¼€å‘æ›´å¯é ä¸”å…·æœ‰ä¸´åºŠæ•ˆç”¨çš„æ–‡æœ¬å¼•å¯¼åŒ»ç–—å›¾åƒç¼–è¾‘å·¥å…·å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://mliuby.github.io/MedEBench_Website/",
      "pdf_url": "https://arxiv.org/pdf/2506.01921v7",
      "published_date": "2025-06-02 17:43:01 UTC",
      "updated_date": "2025-10-04 11:55:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:31:29.094718+00:00"
    },
    {
      "arxiv_id": "2506.01919v1",
      "title": "Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models",
      "title_zh": "ä½œä¸ºå¤šä»»åŠ¡å­¦ä¹ è€…çš„ Transformerï¼šéšé©¬å°”å¯å¤«æ¨¡å‹ä¸­çš„ç‰¹å¾è§£è€¦",
      "authors": [
        "Yifan Hao",
        "Chenlu Ye",
        "Chi Han",
        "Tong Zhang"
      ],
      "abstract": "Transformer based models have shown remarkable capabilities in sequence learning across a wide range of tasks, often performing well on specific task by leveraging input-output examples. Despite their empirical success, a comprehensive theoretical understanding of this phenomenon remains limited. In this work, we investigate the layerwise behavior of Transformers to uncover the mechanisms underlying their multi-task generalization ability. Taking explorations on a typical sequence model, i.e, Hidden Markov Models, which are fundamental to many language tasks, we observe that: first, lower layers of Transformers focus on extracting feature representations, primarily influenced by neighboring tokens; second, on the upper layers, features become decoupled, exhibiting a high degree of time disentanglement. Building on these empirical insights, we provide theoretical analysis for the expressiveness power of Transformers. Our explicit constructions align closely with empirical observations, providing theoretical support for the Transformer's effectiveness and efficiency on sequence learning across diverse tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Transformer æ¨¡å‹åœ¨å¤šä»»åŠ¡æ³›åŒ–èƒ½åŠ›èƒŒåçš„å†…åœ¨æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯å…¶ä½œä¸ºå¤šä»»åŠ¡å­¦ä¹ å™¨çš„å±‚çº§è¡Œä¸ºç‰¹å¾ã€‚ç ”ç©¶è€…é€šè¿‡åœ¨å…¸å‹çš„åºåˆ—æ¨¡å‹éšé©¬å°”å¯å¤«æ¨¡å‹ (Hidden Markov Models) ä¸Šè¿›è¡Œå®éªŒï¼Œå‘ç° Transformer çš„åº•å±‚ä¸»è¦å…³æ³¨ç‰¹å¾è¡¨ç¤º (feature representations) çš„æå–ï¼Œä¸”å—é‚»è¿‘æ ‡è®° (neighboring tokens) å½±å“æ˜¾è‘—ã€‚è€Œåœ¨é«˜å±‚ç½‘ç»œä¸­ï¼Œç‰¹å¾é€æ¸å®ç°è§£è€¦ (decoupled)ï¼Œè¡¨ç°å‡ºæé«˜çš„æ—¶é—´è§£è€¦æ€§ (time disentanglement)ã€‚åŸºäºè¿™äº›ç»éªŒæ€§å‘ç°ï¼Œè¯¥ç ”ç©¶è¿›ä¸€æ­¥ä¸º Transformer çš„è¡¨è¾¾èƒ½åŠ› (expressiveness power) æä¾›äº†ç†è®ºåˆ†æã€‚è¿™ç§æ˜¾å¼æ„é€ ä¸å®éªŒè§‚å¯Ÿé«˜åº¦ä¸€è‡´ï¼Œä¸º Transformer åœ¨å¤„ç†å¤šæ ·åŒ–åºåˆ—å­¦ä¹ ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ä¸æ•ˆç‡æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01919v1",
      "published_date": "2025-06-02 17:39:31 UTC",
      "updated_date": "2025-06-02 17:39:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:31:42.587137+00:00"
    },
    {
      "arxiv_id": "2506.01901v1",
      "title": "Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods",
      "title_zh": "ç†è§£æœ‰ç›‘ç£å¾®è°ƒä¸­çš„è¿‡åº¦é€‚é…ï¼šé›†æˆæ–¹æ³•çš„ä½œç”¨",
      "authors": [
        "Yifan Hao",
        "Xingyuan Pan",
        "Hanning Zhang",
        "Chenlu Ye",
        "Rui Pan",
        "Tong Zhang"
      ],
      "abstract": "Supervised fine-tuning (SFT) on domain-specific data is the dominant approach for adapting foundation models to specialized tasks. However, it has been observed that SFT models tend to forget knowledge acquired during pretraining. In vision models, ensembling a pretrained model with its fine-tuned counterpart has been shown to mitigate this issue. In this work, we demonstrate that the same holds for language models, and, more strikingly, we observe an overadaptation phenomenon: the ensemble model not only retains general knowledge from the foundation model but also outperforms the fine-tuned model even on the fine-tuning domain itself. Despite the empirical success of ensembling, a theoretical understanding of its benefits remains underexplored. We develop a formal theoretical analysis of the overadaptation phenomenon. Ensembling mitigates this by balancing two primary sources of error: bias, caused by insufficient fine-tuning, and variance, introduced by overfitting to fine-tuning data. While regularization techniques aim to address this trade-off, we show that ensembling provides a more effective solution. We analyze this phenomenon in over-parameterized linear settings and demonstrate that interpolating between pretrained and fine-tuned weights significantly improves performance. These findings offer theoretical justification for the observed advantages of model ensembling, supported by empirical experiments consistent with our analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)ä¸­çš„è¿‡åº¦é€‚é…(Overadaptation)ç°è±¡ï¼Œå¹¶æ·±å…¥åˆ†æäº†é›†æˆæ–¹æ³•(Ensemble Methods)åœ¨å…¶ä¸­çš„å…³é”®ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹ä¸å¾®è°ƒæ¨¡å‹è¿›è¡Œé›†æˆä¸ä»…èƒ½æœ‰æ•ˆå‡è½»å¯¹é¢„è®­ç»ƒçŸ¥è¯†çš„é—å¿˜ï¼Œç”šè‡³èƒ½ä½¿æ¨¡å‹åœ¨å¾®è°ƒé¢†åŸŸæœ¬èº«çš„è¡¨ç°ä¼˜äºå•ä¸€çš„å¾®è°ƒæ¨¡å‹ã€‚é€šè¿‡å½¢å¼åŒ–ç†è®ºåˆ†æï¼Œä½œè€…è¯æ˜äº†é›†æˆæ–¹æ³•èƒ½å¤Ÿæ¯”ä¼ ç»Ÿæ­£åˆ™åŒ–æŠ€æœ¯æ›´æœ‰æ•ˆåœ°å¹³è¡¡å¾®è°ƒä¸è¶³å¯¼è‡´çš„åå·®(Bias)ä¸è¿‡æ‹Ÿåˆå¸¦æ¥çš„æ–¹å·®(Variance)ã€‚åœ¨è¶…å‚æ•°åŒ–çº¿æ€§è®¾ç½®ä¸‹çš„è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå¯¹é¢„è®­ç»ƒæƒé‡å’Œå¾®è°ƒæƒé‡è¿›è¡Œæ’å€¼å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ã€‚è¿™äº›å‘ç°ä¸ºæ¨¡å‹é›†æˆåœ¨ç¼“è§£è¿‡åº¦é€‚é…æ–¹é¢çš„å®è¯ä¼˜åŠ¿æä¾›äº†åšå®çš„ç†è®ºä¾æ®ï¼Œå¹¶å¾—åˆ°äº†å®éªŒç»“æœçš„ä¸€è‡´æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01901v1",
      "published_date": "2025-06-02 17:23:16 UTC",
      "updated_date": "2025-06-02 17:23:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:31:35.391242+00:00"
    },
    {
      "arxiv_id": "2506.01900v1",
      "title": "COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents",
      "title_zh": "COALESCEï¼šè‡ªä¸» LLM æ™ºèƒ½ä½“å›¢é˜Ÿä¸­åŸºäºæŠ€èƒ½çš„ä»»åŠ¡å¤–åŒ…ç»æµä¸å®‰å…¨åŠ¨æ€",
      "authors": [
        "Manish Bhatt",
        "Ronald F. Del Rosario",
        "Vineeth Sai Narajala",
        "Idan Habler"
      ],
      "abstract": "The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† COALESCEï¼ˆCost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimationï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªä¸» Large Language Model (LLM) æ™ºèƒ½ä½“åœ¨éƒ¨ç½²ä¸­é¢ä¸´çš„é«˜æ˜‚ GPU è®¡ç®—èµ„æºéœ€æ±‚é—®é¢˜ã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“æ ¹æ®æŠ€èƒ½è¯„ä¼°å°†ç‰¹å®šå­ä»»åŠ¡åŠ¨æ€å¤–åŒ…ç»™æ›´å…·æˆæœ¬æ•ˆç›Šçš„ç¬¬ä¸‰æ–¹æ™ºèƒ½ä½“ï¼Œå…¶æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬æ··åˆæŠ€èƒ½è¡¨ç¤º (hybrid skill representation)ã€è‡ªåŠ¨åŒ–ä»»åŠ¡åˆ†è§£ä»¥åŠå¯¹æ¯”å†…éƒ¨æ‰§è¡Œä¸å¤–éƒ¨å¤–åŒ…æˆæœ¬çš„ç»Ÿä¸€æˆæœ¬æ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨ Agent2Agent (A2A) ç­‰æ ‡å‡†åŒ–é€šä¿¡åè®®å’Œå¸‚åœºåŒ–å†³ç­–ç®—æ³•ï¼ŒCOALESCE å®ç°äº†æ™ºèƒ½ä½“é—´çš„é«˜æ•ˆåä½œä¸èµ„æºä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç†è®ºä»¿çœŸä¸­å¯å®ç° 41.8% çš„æˆæœ¬é™ä½ï¼Œå¹¶åœ¨çœŸå®ä»»åŠ¡çš„å®è¯éªŒè¯ä¸­é€šè¿‡ epsilon-greedy æ¢ç´¢è¾¾åˆ°äº† 20.3% çš„æˆæœ¬å‰Šå‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ä»…æå‡äº†å¤§è§„æ¨¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ï¼Œä¹Ÿä¸ºæœªæ¥ä¸“ä¸šåŒ–æ™ºèƒ½ä½“ç»æµçš„å½¢æˆæä¾›äº†ç†è®ºä¸å®è·µæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 2 figures, github linked",
      "pdf_url": "https://arxiv.org/pdf/2506.01900v1",
      "published_date": "2025-06-02 17:22:47 UTC",
      "updated_date": "2025-06-02 17:22:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:31:44.982989+00:00"
    },
    {
      "arxiv_id": "2506.01890v2",
      "title": "CogniAlign: Word-Level Multimodal Speech Alignment with Gated Cross-Attention for Alzheimer's Detection",
      "title_zh": "CogniAlignï¼šåŸºäºé—¨æ§äº¤å‰æ³¨æ„åŠ›çš„è¯çº§å¤šæ¨¡æ€è¯­éŸ³å¯¹é½é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹",
      "authors": [
        "David Ortiz-Perez",
        "Manuel Benavent-Lledo",
        "Javier Rodriguez-Juan",
        "Jose Garcia-Rodriguez",
        "David TomÃ¡s"
      ],
      "abstract": "Early detection of cognitive disorders such as Alzheimer's disease is critical for enabling timely clinical intervention and improving patient outcomes. In this work, we introduce CogniAlign, a multimodal architecture for Alzheimer's detection that integrates audio and textual modalities, two non-intrusive sources of information that offer complementary insights into cognitive health. Unlike prior approaches that fuse modalities at a coarse level, CogniAlign leverages a word-level temporal alignment strategy that synchronizes audio embeddings with corresponding textual tokens based on transcription timestamps. This alignment supports the development of token-level fusion techniques, enabling more precise cross-modal interactions. To fully exploit this alignment, we propose a Gated Cross-Attention Fusion mechanism, where audio features attend over textual representations, guided by the superior unimodal performance of the text modality. In addition, we incorporate prosodic cues, specifically interword pauses, by inserting pause tokens into the text and generating audio embeddings for silent intervals, further enriching both streams. We evaluate CogniAlign on the ADReSSo dataset, where it achieves an accuracy of 87.35% over a Leave-One-Subject-Out setup and of 90.36% over a 5 fold Cross-Validation, outperforming existing state-of-the-art methods. A detailed ablation study confirms the advantages of our alignment strategy, attention-based fusion, and prosodic modeling. Finally, we perform a corpus analysis to assess the impact of the proposed prosodic features and apply Integrated Gradients to identify the most influential input segments used by the model in predicting cognitive health outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CogniAlignï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹çš„å¤šæ¨¡æ€æ¶æ„ï¼Œé€šè¿‡æ•´åˆéŸ³é¢‘å’Œæ–‡æœ¬è¿™ä¸¤ç±»éä¾µå…¥å¼ä¿¡æ¯æºï¼Œæ•æ‰è®¤çŸ¥å¥åº·çš„äº’è¡¥ç‰¹å¾ã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°é‡‡ç”¨äº†è¯çº§æ—¶é—´å¯¹é½ç­–ç•¥ï¼Œåˆ©ç”¨è½¬å½•æ—¶é—´æˆ³å®ç°éŸ³é¢‘åµŒå…¥ä¸æ–‡æœ¬ Token çš„åŒæ­¥ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼•å…¥äº†é—¨æ§äº¤å‰æ³¨æ„åŠ›èåˆæœºåˆ¶ (Gated Cross-Attention Fusion) ä»¥å¢å¼ºè·¨æ¨¡æ€äº¤äº’ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡åœ¨æ–‡æœ¬ä¸­æ’å…¥åœé¡¿æ ‡è®°å¹¶ç”Ÿæˆæ²‰é»˜é—´éš”çš„éŸ³é¢‘åµŒå…¥ï¼Œå°†è·¨è¯åœé¡¿ç­‰éŸµå¾‹çº¿ç´¢ (Prosodic cues) èå…¥æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æå‡äº†æ£€æµ‹æ€§èƒ½ã€‚å®éªŒåœ¨ ADReSSo æ•°æ®é›†ä¸Šè¿›è¡Œï¼ŒCogniAlign åœ¨ç•™ä¸€æ³•å’Œäº”æŠ˜äº¤å‰éªŒè¯ä¸­åˆ†åˆ«è¾¾åˆ°äº† 87.35% å’Œ 90.36% çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ¶ˆèå®éªŒå’Œé›†æˆæ¢¯åº¦ (Integrated Gradients) åˆ†æå…±åŒè¯å®äº†å¯¹é½ç­–ç•¥ã€æ³¨æ„åŠ›èåˆåŠéŸµå¾‹å»ºæ¨¡åœ¨é¢„æµ‹è®¤çŸ¥å¥åº·ç»“æœæ–¹é¢çš„æœ‰æ•ˆæ€§ä¸å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01890v2",
      "published_date": "2025-06-02 17:17:01 UTC",
      "updated_date": "2025-10-24 12:09:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:32:12.392976+00:00"
    },
    {
      "arxiv_id": "2506.01884v1",
      "title": "Agnostic Reinforcement Learning: Foundations and Algorithms",
      "title_zh": "ä¸å¯çŸ¥å¼ºåŒ–å­¦ä¹ ï¼šç†è®ºåŸºç¡€ä¸ç®—æ³•",
      "authors": [
        "Gene Li"
      ],
      "abstract": "Reinforcement Learning (RL) has demonstrated tremendous empirical success across numerous challenging domains. However, we lack a strong theoretical understanding of the statistical complexity of RL in environments with large state spaces, where function approximation is required for sample-efficient learning. This thesis addresses this gap by rigorously examining the statistical complexity of RL with function approximation from a learning theoretic perspective. Departing from a long history of prior work, we consider the weakest form of function approximation, called agnostic policy learning, in which the learner seeks to find the best policy in a given class $Î $, with no guarantee that $Î $ contains an optimal policy for the underlying task.\n  We systematically explore agnostic policy learning along three key axes: environment access -- how a learner collects data from the environment; coverage conditions -- intrinsic properties of the underlying MDP measuring the expansiveness of state-occupancy measures for policies in the class $Î $, and representational conditions -- structural assumptions on the class $Î $ itself. Within this comprehensive framework, we (1) design new learning algorithms with theoretical guarantees and (2) characterize fundamental performance bounds of any algorithm. Our results reveal significant statistical separations that highlight the power and limitations of agnostic policy learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰åœ¨å¤§è§„æ¨¡çŠ¶æ€ç©ºé—´ä¸‹éœ€è¦å‡½æ•°è¿‘ä¼¼ï¼ˆfunction approximationï¼‰æ—¶ç¼ºä¹ç†è®ºç†è§£çš„é—®é¢˜ï¼Œä»å­¦ä¹ ç†è®ºè§’åº¦æ·±å…¥æ¢è®¨äº†å…¶ç»Ÿè®¡å¤æ‚åº¦ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨äº†ä¸å¯çŸ¥ç­–ç•¥å­¦ä¹ ï¼ˆagnostic policy learningï¼‰è¿™ä¸€æœ€å¼±å½¢å¼çš„å‡½æ•°è¿‘ä¼¼ï¼Œå³å­¦ä¹ è€…åœ¨ç»™å®šç­–ç•¥ç±» $\\Pi$ ä¸­å¯»æ‰¾æœ€ä½³ç­–ç•¥ï¼Œè€Œä¸å‡è®¾è¯¥ç±»åŒ…å«æœ€ä¼˜ç­–ç•¥ã€‚è®ºæ–‡é€šè¿‡ç¯å¢ƒè®¿é—®ï¼ˆenvironment accessï¼‰ã€è¦†ç›–æ¡ä»¶ï¼ˆcoverage conditionsï¼‰å’Œè¡¨ç¤ºæ¡ä»¶ï¼ˆrepresentational conditionsï¼‰ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦å¯¹è¯¥é—®é¢˜è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æã€‚åŸºäºè¿™ä¸€ç»¼åˆæ¡†æ¶ï¼Œç ”ç©¶è®¾è®¡äº†å…·æœ‰ç†è®ºä¿è¯çš„æ–°å‹å­¦ä¹ ç®—æ³•ï¼Œå¹¶åˆ»ç”»äº†ä»»ä½•ç®—æ³•éƒ½å¿…é¡»éµå¾ªçš„åŸºæœ¬æ€§èƒ½ç•Œé™ï¼ˆperformance boundsï¼‰ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†æ˜¾è‘—çš„ç»Ÿè®¡åˆ†ç¦»ï¼ˆstatistical separationsï¼‰ï¼Œæ·±åˆ»é˜æ˜äº†ä¸å¯çŸ¥ç­–ç•¥å­¦ä¹ åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æ ¸å¿ƒåŠ›é‡ä¸æœ¬è´¨å±€é™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Ph.D. thesis",
      "pdf_url": "https://arxiv.org/pdf/2506.01884v1",
      "published_date": "2025-06-02 17:12:24 UTC",
      "updated_date": "2025-06-02 17:12:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:31:53.093546+00:00"
    },
    {
      "arxiv_id": "2506.01883v1",
      "title": "scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell Omics",
      "title_zh": "scDatasetï¼šé¢å‘å¤§è§„æ¨¡å•ç»†èƒç»„å­¦æ·±åº¦å­¦ä¹ çš„å¯æ‰©å±•æ•°æ®åŠ è½½",
      "authors": [
        "Davide D'Ascenzo",
        "Sebastiano Cultrera di Montesano"
      ],
      "abstract": "Modern single-cell datasets now comprise hundreds of millions of cells, presenting significant challenges for training deep learning models that require shuffled, memory-efficient data loading. While the AnnData format is the community standard for storing single-cell datasets, existing data loading solutions for AnnData are often inadequate: some require loading all data into memory, others convert to dense formats that increase storage demands, and many are hampered by slow random disk access. We present scDataset, a PyTorch IterableDataset that operates directly on one or more AnnData files without the need for format conversion. The core innovation is a combination of block sampling and batched fetching, which together balance randomness and I/O efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\\times$ speed-up over AnnLoader, a 27$\\times$ speed-up over HuggingFace Datasets, and an 18$\\times$ speed-up over BioNeMo in single-core settings. These advances democratize large-scale single-cell model training for the broader research community.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†scDatasetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§è§„æ¨¡å•ç»†èƒç»„å­¦æ·±åº¦å­¦ä¹ è®¾è®¡çš„PyTorch IterableDatasetï¼Œæ—¨åœ¨è§£å†³æ•°äº¿çº§ç»†èƒæ•°æ®é›†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜æ•ˆåŠ è½½éš¾é¢˜ã€‚é’ˆå¯¹ç°æœ‰AnnDataæ•°æ®åŠ è½½æ–¹æ¡ˆå­˜åœ¨çš„å†…å­˜æ¶ˆè€—é«˜ã€éœ€è½¬æ¢ä¸ºç¨ å¯†æ ¼å¼ä»¥åŠç£ç›˜éšæœºè®¿é—®é€Ÿåº¦æ…¢ç­‰ç¼ºé™·ï¼ŒscDatasetå®ç°äº†æ— éœ€æ ¼å¼è½¬æ¢å³å¯ç›´æ¥æ“ä½œä¸€ä¸ªæˆ–å¤šä¸ªAnnDataæ–‡ä»¶çš„åŠŸèƒ½ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé‡‡ç”¨äº†å—é‡‡æ ·(block sampling)ä¸æ‰¹é‡è·å–(batched fetching)ç›¸ç»“åˆçš„ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†æ•°æ®éšæœºæ€§ä¸I/Oæ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Tahoe 100Mæ•°æ®é›†çš„å•æ ¸ç¯å¢ƒä¸‹ï¼ŒscDatasetç›¸æ¯”äºAnnLoaderã€HuggingFace Datasetså’ŒBioNeMoåˆ†åˆ«å®ç°äº†48å€ã€27å€åŠ18å€çš„åŠ é€Ÿã€‚è¿™ä¸€æŠ€æœ¯çªç ´æ˜¾è‘—é™ä½äº†å¤§è§„æ¨¡å•ç»†èƒæ¨¡å‹è®­ç»ƒçš„é—¨æ§›ï¼Œä¸ºç”Ÿç‰©ä¿¡æ¯å­¦ç ”ç©¶ç¤¾åŒºæä¾›äº†æ›´å…·æ‰©å±•æ€§çš„æ•°æ®å¤„ç†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "q-bio.GN",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01883v1",
      "published_date": "2025-06-02 17:11:49 UTC",
      "updated_date": "2025-06-02 17:11:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:32:28.590230+00:00"
    },
    {
      "arxiv_id": "2507.00002v1",
      "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs",
      "title_zh": "Hypertokensï¼šToken åŒ–å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å…¨æ¯è”æƒ³è®°å¿†",
      "authors": [
        "Christopher James Augeri"
      ],
      "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from apparent precision loss, reframed here as information spreading. This reframing shifts the problem from computational precision to an information-theoretic communication issue. We address the K:V and V:K memory problem in LLMs by introducing HDRAM (Holographically Defined Random Access Memory), a symbolic memory framework treating transformer latent space as a spread-spectrum channel. Built upon hypertokens, structured symbolic codes integrating classical error-correcting codes (ECC), holographic computing, and quantum-inspired search, HDRAM recovers distributed information through principled despreading. These phase-coherent memory addresses enable efficient key-value operations and Grover-style search in latent space. By combining ECC grammar with compressed sensing and Krylov subspace alignment, HDRAM significantly improves associative retrieval without architectural changes, demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can fortify transformer architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) ä¸­çš„ç²¾åº¦æŸå¤±é—®é¢˜ï¼Œå°†å…¶é‡æ–°å®šä¹‰ä¸ºä¿¡æ¯è®ºå±‚é¢çš„ä¿¡æ¯ä¼ æ’­ (information spreading) é€šä¿¡æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³æ¨¡å‹ä¸­çš„ K:V å’Œ V:K å†…å­˜é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº† HDRAM (Holographically Defined Random Access Memory)ï¼Œè¿™æ˜¯ä¸€ç§å°† Transformer éšè—ç©ºé—´è§†ä¸ºæ‰©é¢‘ä¿¡é“çš„ç¬¦å·å†…å­˜æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäº Hypertokens æ„å»ºï¼Œé€šè¿‡æ•´åˆç»å…¸çº é”™ç  (ECC)ã€å…¨æ¯è®¡ç®— (holographic computing) å’Œå—é‡å­å¯å‘çš„ Grover å¼æœç´¢ï¼Œå®ç°äº†åˆ†å¸ƒå¼ä¿¡æ¯çš„åŸç†æ€§è§£æ‰©ã€‚é€šè¿‡ç»“åˆå‹ç¼©æ„ŸçŸ¥ (compressed sensing) å’Œ Krylov å­ç©ºé—´å¯¹é½ (Krylov subspace alignment)ï¼ŒHDRAM åœ¨ä¸æ”¹å˜ç°æœ‰æ¨¡å‹æ¶æ„çš„å‰æä¸‹æ˜¾è‘—æå‡äº†å…³è”æ£€ç´¢ (associative retrieval) çš„æ•ˆç‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç»å…¸-å…¨æ¯-é‡å­å¯å‘ (CHQ) åŸåˆ™åœ¨å¢å¼º Transformer æ¶æ„æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†å¤æ‚å†…å­˜ä»»åŠ¡æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP Conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.00002v1",
      "published_date": "2025-06-02 17:11:45 UTC",
      "updated_date": "2025-06-02 17:11:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:32:43.729668+00:00"
    },
    {
      "arxiv_id": "2506.01881v2",
      "title": "WHEN TO ACT, WHEN TO WAIT: Modeling the Intent-Action Alignment Problem in Dialogue",
      "title_zh": "ä½•æ—¶è¡ŒåŠ¨ï¼Œä½•æ—¶ç­‰å¾…ï¼šå¯¹è¯ä¸­çš„æ„å›¾-è¡ŒåŠ¨å¯¹é½é—®é¢˜å»ºæ¨¡",
      "authors": [
        "Yaoyao Qian",
        "Jindan Huang",
        "Yuanli Wang",
        "Simon Yu",
        "Kyrie Zhixuan Zhou",
        "Jiayuan Mao",
        "Mingfu Liang",
        "Hanhan Zhou"
      ],
      "abstract": "Dialogue systems often fail when user utterances are semantically complete yet lack the clarity and completeness required for appropriate system action. This mismatch arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. This highlights the critical Intent-Action Alignment Problem: determining when an expression is not just understood, but truly ready for a system to act upon. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing trajectories of expression phrasing and latent cognitive transitions, enabling systematic analysis of how collaborative understanding develops. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹è¯ç³»ç»Ÿä¸­ç”¨æˆ·è¡¨è¾¾è™½ç„¶è¯­ä¹‰å®Œæ•´ä½†ç¼ºä¹æ‰§è¡Œæ¸…æ™°åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†æ„å›¾-è¡ŒåŠ¨å¯¹é½é—®é¢˜(Intent-Action Alignment Problem)ï¼Œæ—¨åœ¨ç¡®å®šç³»ç»Ÿä½•æ—¶åº”å½“é‡‡å–è¡ŒåŠ¨ã€‚ä¸ºæ­¤ä½œè€…å¼€å‘äº†STORMæ¡†æ¶ï¼Œåˆ©ç”¨å…·æœ‰å†…éƒ¨è®¿é—®æƒé™çš„UserLLMä¸ä»…èƒ½è§‚å¯Ÿè¡Œä¸ºçš„AgentLLMä¹‹é—´çš„éå¯¹ç§°ä¿¡æ¯åŠ¨æ€å¯¹è¯ï¼Œç”Ÿæˆæ•æ‰è®¤çŸ¥è½¬æ¢è½¨è¿¹çš„æ ‡æ³¨è¯­æ–™åº“ã€‚ç ”ç©¶è´¡çŒ®åŒ…æ‹¬å½¢å¼åŒ–å®šä¹‰å¯¹è¯ç³»ç»Ÿçš„éå¯¹ç§°ä¿¡æ¯å¤„ç†ï¼Œå¹¶å»ºç«‹äº†è¿½è¸ªåä½œç†è§£æ¼”åŒ–çš„æ„å›¾å½¢æˆæ¨¡å‹åŠè¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡åœ¨å››ç§å¤§è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒå‘ç°ï¼Œé€‚åº¦çš„ä¸ç¡®å®šæ€§(40-60%)åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºå®Œå…¨é€æ˜ã€‚è¿™äº›å‘ç°æ·±åŒ–äº†å¯¹éå¯¹ç§°æ¨ç†åŠ¨åŠ›å­¦çš„ç†è§£ï¼Œå¹¶ä¸ºè®¾è®¡ç»è¿‡ä¸ç¡®å®šæ€§æ ¡å‡†(Uncertainty-calibrated)çš„å¯¹è¯ç³»ç»Ÿæä¾›äº†ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Project website: https://nanostorm.netlify.app/",
      "pdf_url": "https://arxiv.org/pdf/2506.01881v2",
      "published_date": "2025-06-02 17:11:10 UTC",
      "updated_date": "2025-08-23 18:43:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:32:30.800202+00:00"
    },
    {
      "arxiv_id": "2506.01876v2",
      "title": "In-Context Learning for Pure Exploration",
      "title_zh": "é¢å‘çº¯æ¢ç´¢çš„ä¸Šä¸‹æ–‡å­¦ä¹ ",
      "authors": [
        "Alessio Russo",
        "Ryan Welch",
        "Aldo Pacchiano"
      ],
      "abstract": "We study the problem active sequential hypothesis testing, also known as pure exploration: given a new task, the learner adaptively collects data from the environment to efficiently determine an underlying correct hypothesis. A classical instance of this problem is the task of identifying the best arm in a multi-armed bandit problem (a.k.a. BAI, Best-Arm Identification), where actions index hypotheses. Another important case is generalized search, a problem of determining the correct label through a sequence of strategically selected queries that indirectly reveal information about the label. In this work, we introduce In-Context Pure Exploration (ICPE), which meta-trains Transformers to map observation histories to query actions and a predicted hypothesis, yielding a model that transfers in-context. At inference time, ICPE actively gathers evidence on new tasks and infers the true hypothesis without parameter updates. Across deterministic, stochastic, and structured benchmarks, including BAI and generalized search, ICPE is competitive with adaptive baselines while requiring no explicit modeling of information structure. Our results support Transformers as practical architectures for general sequential testing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸»åŠ¨åºåˆ—å‡è®¾æ£€éªŒï¼ˆActive Sequential Hypothesis Testingï¼‰ï¼Œå³çº¯æ¢ç´¢ï¼ˆPure Explorationï¼‰é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨æœ€ä½³è‡‚è¯†åˆ«ï¼ˆBest-Arm Identification, BAIï¼‰å’Œå¹¿ä¹‰æœç´¢ç­‰ä»»åŠ¡ã€‚ä½œè€…æå‡ºäº†ä¸Šä¸‹æ–‡çº¯æ¢ç´¢ï¼ˆIn-Context Pure Exploration, ICPEï¼‰æ¡†æ¶ï¼Œé€šè¿‡å…ƒè®­ç»ƒ Transformer å°†è§‚æµ‹å†å²æ˜ å°„ä¸ºæŸ¥è¯¢åŠ¨ä½œå’Œé¢„æµ‹å‡è®¾ï¼Œä»è€Œå®ç°ä¸Šä¸‹æ–‡è¿ç§»ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒICPE èƒ½å¤Ÿåœ¨æ— éœ€å‚æ•°æ›´æ–°çš„æƒ…å†µä¸‹é’ˆå¯¹æ–°ä»»åŠ¡ä¸»åŠ¨æ”¶é›†è¯æ®å¹¶æ¨æ–­çœŸå®å‡è®¾ã€‚å®éªŒåœ¨ç¡®å®šæ€§ã€éšæœºæ€§å’Œç»“æ„åŒ–åŸºå‡†æµ‹è¯•ä¸­è¯æ˜ï¼ŒICPE åœ¨ä¸éœ€è¦æ˜¾å¼å»ºæ¨¡ä¿¡æ¯ç»“æ„çš„å‰æä¸‹ï¼Œå…¶æ€§èƒ½å¯ä¸è‡ªé€‚åº”åŸºå‡†ç®—æ³•ç›¸åª²ç¾ã€‚è¯¥æˆæœè¡¨æ˜ Transformer æ˜¯å¤„ç†é€šç”¨åºåˆ—æµ‹è¯•ï¼ˆSequential Testingï¼‰çš„ä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„æ¶æ„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01876v2",
      "published_date": "2025-06-02 17:04:50 UTC",
      "updated_date": "2025-10-06 16:44:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:32:48.789351+00:00"
    },
    {
      "arxiv_id": "2506.01869v1",
      "title": "Frugal Machine Learning for Energy-efficient, and Resource-aware Artificial Intelligence",
      "title_zh": "é¢å‘é«˜èƒ½æ•ˆä¸èµ„æºæ„ŸçŸ¥äººå·¥æ™ºèƒ½çš„èŠ‚ä¿­å‹æœºå™¨å­¦ä¹ ",
      "authors": [
        "John Violos",
        "Konstantina-Christina Diamanti",
        "Ioannis Kompatsiaris",
        "Symeon Papadopoulos"
      ],
      "abstract": "Frugal Machine Learning (FML) refers to the practice of designing Machine Learning (ML) models that are efficient, cost-effective, and mindful of resource constraints. This field aims to achieve acceptable performance while minimizing the use of computational resources, time, energy, and data for both training and inference. FML strategies can be broadly categorized into input frugality, learning process frugality, and model frugality, each focusing on reducing resource consumption at different stages of the ML pipeline. This chapter explores recent advancements, applications, and open challenges in FML, emphasizing its importance for smart environments that incorporate edge computing and IoT devices, which often face strict limitations in bandwidth, energy, or latency. Technological enablers such as model compression, energy-efficient hardware, and data-efficient learning techniques are discussed, along with adaptive methods including parameter regularization, knowledge distillation, and dynamic architecture design that enable incremental model updates without full retraining. Furthermore, it provides a comprehensive taxonomy of frugal methods, discusses case studies across diverse domains, and identifies future research directions to drive innovation in this evolving field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†èŠ‚ä¿­æœºå™¨å­¦ä¹ (Frugal Machine Learning, FML)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è®¾è®¡é«˜æ•ˆã€ä½æˆæœ¬ä¸”å…³æ³¨èµ„æºçº¦æŸçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç ”ç©¶é¢†åŸŸï¼ŒåŠ›æ±‚åœ¨æœ€å°åŒ–è®¡ç®—èµ„æºã€æ—¶é—´ã€èƒ½æºå’Œæ•°æ®æ¶ˆè€—çš„åŒæ—¶å®ç°å¯æ¥å—çš„æ€§èƒ½ã€‚è®ºæ–‡å°†FMLç­–ç•¥ç³»ç»Ÿåœ°åˆ†ä¸ºè¾“å…¥èŠ‚ä¿­ã€å­¦ä¹ è¿‡ç¨‹èŠ‚ä¿­å’Œæ¨¡å‹èŠ‚ä¿­ä¸‰å¤§ç±»ï¼Œæ¶µç›–äº†æœºå™¨å­¦ä¹ æµæ°´çº¿çš„ä¸åŒé˜¶æ®µã€‚è¯¥ç ”ç©¶ç‰¹åˆ«å¼ºè°ƒäº†FMLåœ¨è¾¹ç¼˜è®¡ç®—(Edge Computing)å’Œç‰©è”ç½‘(IoT)ç­‰å—é™ç¯å¢ƒä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶è®¨è®ºäº†æ¨¡å‹å‹ç¼©(Model Compression)ã€èƒ½æºé«˜æ•ˆç¡¬ä»¶ã€çŸ¥è¯†è’¸é¦(Knowledge Distillation)åŠåŠ¨æ€æ¶æ„è®¾è®¡ç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæ–‡ç« æä¾›äº†ä¸€å¥—å…¨é¢çš„èŠ‚ä¿­æ–¹æ³•åˆ†ç±»æ³•(Taxonomy)ï¼Œå¹¶é€šè¿‡å¤šé¢†åŸŸçš„æ¡ˆä¾‹ç ”ç©¶åˆ†æäº†å½“å‰çš„åº”ç”¨ç°çŠ¶ã€‚æœ€åï¼Œè¯¥å·¥ä½œæŒ‡æ˜äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºå¼€å‘èµ„æºæ„ŸçŸ¥å‹äººå·¥æ™ºèƒ½æä¾›ç†è®ºæŒ‡å¯¼å’Œå®è·µå‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01869v1",
      "published_date": "2025-06-02 16:56:21 UTC",
      "updated_date": "2025-06-02 16:56:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:32:46.592899+00:00"
    },
    {
      "arxiv_id": "2506.01850v1",
      "title": "MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs",
      "title_zh": "MoDAï¼šé¢å‘æŒ‡ä»¤å‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç»†ç²’åº¦è§†è§‰å®šä½çš„è°ƒåˆ¶é€‚é…å™¨",
      "authors": [
        "Wayner Barrios",
        "AndrÃ©s Villa",
        "Juan LeÃ³n AlcÃ¡zar",
        "SouYoung Jin",
        "Bernard Ghanem"
      ],
      "abstract": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on instruction-following tasks by integrating pretrained visual encoders with large language models (LLMs). However, existing approaches often struggle to ground fine-grained visual concepts in complex scenes. In this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective module designed to refine pre-aligned visual features through instruction-guided modulation. Our approach follows the standard LLaVA training protocol, consisting of a two-stage process: (1) aligning image features to the LLMs input space via a frozen vision encoder and adapter layers, and (2) refining those features using the MoDA adapter during the instructional tuning stage. MoDA employs a Transformer-based cross-attention mechanism to generate a modulation mask over the aligned visual tokens, thereby emphasizing semantically relevant embedding dimensions based on the language instruction. The modulated features are then passed to the LLM for autoregressive language generation. Our experimental evaluation shows that MoDA improves visual grounding and generates more contextually appropriate responses, demonstrating its effectiveness as a general-purpose enhancement for image-based MLLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MoDA (Modulation Adapter)ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç»†ç²’åº¦è§†è§‰å®šä½ (fine-grained visual grounding) èƒ½åŠ›ã€‚MoDA éµå¾ªæ ‡å‡† LLaVA è®­ç»ƒåè®®ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒ (instructional tuning) é˜¶æ®µé€šè¿‡æŒ‡ä»¤å¼•å¯¼çš„è°ƒåˆ¶æ¥ç»†åŒ–é¢„å¯¹é½çš„è§†è§‰ç‰¹å¾ã€‚å…¶æ ¸å¿ƒæœºåˆ¶æ˜¯é‡‡ç”¨åŸºäº Transformer çš„äº¤å‰æ³¨æ„åŠ› (cross-attention) ç”Ÿæˆè°ƒåˆ¶æ©ç  (modulation mask)ï¼Œæ ¹æ®è¯­è¨€æŒ‡ä»¤å¼ºè°ƒè¯­ä¹‰ç›¸å…³çš„åµŒå…¥ç»´åº¦ã€‚è°ƒåˆ¶åçš„ç‰¹å¾éšåè¢«ä¼ é€’ç»™ LLM è¿›è¡Œè‡ªå›å½’è¯­è¨€ç”Ÿæˆï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹ç»†ç²’åº¦æ¦‚å¿µçš„ç†è§£ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMoDA æ˜¾è‘—æ”¹å–„äº†è§†è§‰å®šä½æ•ˆæœå¹¶èƒ½ç”Ÿæˆæ›´å…·ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„å“åº”ã€‚ä½œä¸ºä¸€ç§é€šç”¨çš„å¢å¼ºæ‰‹æ®µï¼ŒMoDA è¯æ˜äº†å…¶åœ¨å›¾åƒ MLLMs ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01850v1",
      "published_date": "2025-06-02 16:38:50 UTC",
      "updated_date": "2025-06-02 16:38:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:32:50.931016+00:00"
    },
    {
      "arxiv_id": "2506.02092v1",
      "title": "Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models",
      "title_zh": "è¿ˆå‘å…·æœ‰æ›´å¼ºæ³›åŒ–æ€§ä¸å¯è§£é‡Šæ€§çš„æ— ç›‘ç£åŸºäºæ¦‚å¿µçš„æ¨¡å‹",
      "authors": [
        "Francesco De Santis",
        "Philippe Bich",
        "Gabriele Ciravegna",
        "Pietro Barbiero",
        "Danilo Giordano",
        "Tania Cerquitelli"
      ],
      "abstract": "To increase the trustworthiness of deep neural networks, it is critical to improve the understanding of how they make decisions. This paper introduces a novel unsupervised concept-based model for image classification, named Learnable Concept-Based Model (LCBM) which models concepts as random variables within a Bernoulli latent space. Unlike traditional methods that either require extensive human supervision or suffer from limited scalability, our approach employs a reduced number of concepts without sacrificing performance. We demonstrate that LCBM surpasses existing unsupervised concept-based models in generalization capability and nearly matches the performance of black-box models. The proposed concept representation enhances information retention and aligns more closely with human understanding. A user study demonstrates the discovered concepts are also more intuitive for humans to interpret. Finally, despite the use of concept embeddings, we maintain model interpretability by means of a local linear combination of concepts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Learnable Concept-Based Model (LCBM)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›¾åƒåˆ†ç±»çš„æ–°å‹æ— ç›‘ç£æ¦‚å¿µåŸºæ¨¡å‹ï¼Œæ—¨åœ¨æå‡æ·±åº¦ç¥ç»ç½‘ç»œå†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ä¸ä¿¡ä»»åº¦ã€‚LCBM å°†æ¦‚å¿µå»ºæ¨¡ä¸º Bernoulli latent space ä¸­çš„éšæœºå˜é‡ï¼Œé€šè¿‡å‡å°‘æ¦‚å¿µæ•°é‡è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¯æ‰©å±•æ€§ä¸Šçš„å±€é™ï¼Œä¸”æœªç‰ºç‰²æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLCBM åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰çš„æ— ç›‘ç£æ¦‚å¿µåŸºæ¨¡å‹ï¼Œå…¶æ€§èƒ½è¡¨ç°å‡ ä¹ä¸ black-box models æŒå¹³ã€‚è¯¥æ¨¡å‹é‡‡ç”¨çš„æ¦‚å¿µè¡¨ç¤ºæ³•æœ‰æ•ˆå¢å¼ºäº†ä¿¡æ¯ä¿ç•™èƒ½åŠ›ï¼Œå¹¶ä½¿å…¶ä¸äººç±»çš„ç†è§£æ›´åŠ ä¸€è‡´ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æ‰€å‘ç°çš„æ¦‚å¿µå¯¹äººç±»è€Œè¨€æ›´å…·ç›´è§‚æ€§ã€‚å°½ç®¡ä½¿ç”¨äº† concept embeddingsï¼Œè¯¥æ¡†æ¶ä»é€šè¿‡æ¦‚å¿µçš„ local linear combination ä¿æŒäº†æ¨¡å‹çš„é«˜åº¦è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper accepted at ECML-PKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02092v1",
      "published_date": "2025-06-02 16:26:41 UTC",
      "updated_date": "2025-06-02 16:26:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:33:01.520182+00:00"
    },
    {
      "arxiv_id": "2506.01829v1",
      "title": "CiteEval: Principle-Driven Citation Evaluation for Source Attribution",
      "title_zh": "CiteEvalï¼šé¢å‘æ¥æºæº¯æºçš„åŸåˆ™é©±åŠ¨å‹å¼•ç”¨è¯„ä¼°",
      "authors": [
        "Yumo Xu",
        "Peng Qi",
        "Jifan Chen",
        "Kunlun Liu",
        "Rujun Han",
        "Lan Liu",
        "Bonan Min",
        "Vittorio Castelli",
        "Arshit Gupta",
        "Zhiguo Wang"
      ],
      "abstract": "Citation quality is crucial in information-seeking systems, directly influencing trust and the effectiveness of information access. Current evaluation frameworks, both human and automatic, mainly rely on Natural Language Inference (NLI) to assess binary or ternary supportiveness from cited sources, which we argue is a suboptimal proxy for citation evaluation. In this work we introduce CiteEval, a citation evaluation framework driven by principles focusing on fine-grained citation assessment within a broad context, encompassing not only the cited sources but the full retrieval context, user query, and generated text. Guided by the proposed framework, we construct CiteBench, a multi-domain benchmark with high-quality human annotations on citation quality. To enable efficient evaluation, we further develop CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation with human judgments. Experiments across diverse systems demonstrate CiteEval-Auto's superior ability to capture the multifaceted nature of citations compared to existing metrics, offering a principled and scalable approach to evaluate and improve model-generated citations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¼•ç”¨è¯„ä¼°æ¡†æ¶è¿‡åº¦ä¾èµ–è‡ªç„¶è¯­è¨€æ¨ç†(Natural Language Inference)è€Œå¯¼è‡´è¯„ä¼°ç»´åº¦å•ä¸€ã€æ— æ³•å‡†ç¡®è¡¡é‡å¼•ç”¨è´¨é‡çš„é—®é¢˜ï¼Œæå‡ºäº†CiteEvalè¿™ä¸€åŸºäºåŸåˆ™çš„ç»†ç²’åº¦å¼•ç”¨è¯„ä¼°æ¡†æ¶ã€‚CiteEvalè¶…è¶Šäº†ä¼ ç»Ÿçš„äºŒå…ƒæˆ–ä¸‰å…ƒæ”¯æŒåº¦åˆ¤æ–­ï¼Œåœ¨æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¸­å¯¹å¼•ç”¨è¿›è¡Œè¯„ä¼°ï¼Œç»¼åˆè€ƒè™‘äº†è¢«å¼•ç”¨æºã€å®Œæ•´æ£€ç´¢ä¸Šä¸‹æ–‡ã€ç”¨æˆ·æŸ¥è¯¢åŠç”Ÿæˆçš„æ–‡æœ¬ã€‚ä¾æ‰˜è¯¥æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«é«˜è´¨é‡äººç±»æ ‡æ³¨çš„å¤šé¢†åŸŸåŸºå‡†æµ‹è¯•é›†CiteBenchï¼Œå¹¶è¿›ä¸€æ­¥å¼€å‘äº†ä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´çš„è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡å¥—ä»¶CiteEval-Autoã€‚å®éªŒè¯æ˜ï¼ŒCiteEval-Autoåœ¨å¤šç±»ç³»ç»Ÿä¸­å‡è¡¨ç°å‡ºä¼˜äºç°æœ‰æŒ‡æ ‡çš„èƒ½åŠ›ï¼Œèƒ½æ›´ç²¾å‡†åœ°æ•æ‰å¼•ç”¨çš„å¤šç»´ç‰¹æ€§ï¼Œä¸ºä¼˜åŒ–å¤§æ¨¡å‹ç”Ÿæˆçš„å¼•ç”¨è´¨é‡æä¾›äº†å…¼å…·ç§‘å­¦æ€§ä¸å¯æ‰©å±•æ€§çš„è¯„ä¼°é€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01829v1",
      "published_date": "2025-06-02 16:15:34 UTC",
      "updated_date": "2025-06-02 16:15:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:33:07.666440+00:00"
    },
    {
      "arxiv_id": "2506.01824v2",
      "title": "A Quantum Information Theoretic Approach to Tractable Probabilistic Models",
      "title_zh": "æ˜“å¤„ç†æ¦‚ç‡æ¨¡å‹çš„é‡å­ä¿¡æ¯è®ºæ–¹æ³•",
      "authors": [
        "Pedro Zuidberg Dos Martires"
      ],
      "abstract": "By recursively nesting sums and products, probabilistic circuits have emerged in recent years as an attractive class of generative models as they enjoy, for instance, polytime marginalization of random variables. In this work we study these machine learning models using the framework of quantum information theory, leading to the introduction of positive unital circuits (PUnCs), which generalize circuit evaluations over positive real-valued probabilities to circuit evaluations over positive semi-definite matrices. As a consequence, PUnCs strictly generalize probabilistic circuits as well as recently introduced circuit classes such as PSD circuits.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨ Quantum Information Theory æ¡†æ¶å¯¹ Probabilistic Circuits è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œå¹¶å¼•å…¥äº†åä¸º Positive Unital Circuits (PUnCs) çš„æ–°å‹ç”Ÿæˆæ¨¡å‹ã€‚PUnCs é€šè¿‡å°†ä¼ ç»Ÿç”µè·¯ä¸­åŸºäºæ­£å®å€¼æ¦‚ç‡çš„è¯„ä¼°æ¨å¹¿è‡³ Positive Semi-definite Matricesï¼Œå®ç°äº†å¯¹ç°æœ‰æ¨¡å‹çš„æœ‰æ•ˆæ‰©å±•ã€‚ç”±äºé‡‡ç”¨äº†é€’å½’åµŒå¥—æ±‚å’Œä¸ä¹˜ç§¯çš„ç»“æ„ï¼Œè¯¥æ¨¡å‹ä¸ä»…ç»§æ‰¿äº†åœ¨å¤šé¡¹å¼æ—¶é—´å†…è¿›è¡Œéšæœºå˜é‡ Marginalization çš„ç‰¹æ€§ï¼Œè¿˜åœ¨ç†è®ºä¸Šä¸¥æ ¼æ³›åŒ–äº† Probabilistic Circuits ä»¥åŠè¿‘æœŸå‡ºç°çš„ PSD Circuitsã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†é‡å­ä¿¡æ¯è®ºåœ¨æ„å»º Tractable Probabilistic Models æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸçš„æ¦‚ç‡å»ºæ¨¡æä¾›äº†æ›´å…·æ™®é€‚æ€§çš„æ•°å­¦æ¡†æ¶ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01824v2",
      "published_date": "2025-06-02 16:07:08 UTC",
      "updated_date": "2025-07-05 13:10:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:33:24.222729+00:00"
    },
    {
      "arxiv_id": "2506.01820v3",
      "title": "Fodor and Pylyshyn's Legacy: Still No Human-like Systematic Compositionality in Neural Networks",
      "title_zh": "Fodor and Pylyshyn çš„é—äº§ï¼šç¥ç»ç½‘ç»œä¸­ä¾ç„¶ç¼ºä¹ç±»äººçš„ç³»ç»Ÿç»„åˆæ€§",
      "authors": [
        "Tim Woydt",
        "Moritz Willig",
        "Antonia WÃ¼st",
        "Lukas Helff",
        "Wolfgang Stammer",
        "Constantin A. Rothkopf",
        "Kristian Kersting"
      ],
      "abstract": "Strong meta-learning capabilities for systematic compositionality are emerging as an important skill for navigating the complex and changing tasks of today's world. However, in presenting models for robust adaptation to novel environments, it is important to refrain from making unsupported claims about the performance of meta-learning systems that ultimately do not stand up to scrutiny. While Fodor and Pylyshyn famously posited that neural networks inherently lack this capacity as they are unable to model compositional representations or structure-sensitive operations, and thus are not a viable model of the human mind, Lake and Baroni recently presented meta-learning as a pathway to compositionality. In this position paper, we critically revisit this claim and highlight limitations in the proposed meta-learning framework for compositionality. Our analysis shows that modern neural meta-learning systems can only perform such tasks, if at all, under a very narrow and restricted definition of a meta-learning setup. We therefore claim that `Fodor and Pylyshyn's legacy' persists, and to date, there is no human-like systematic compositionality learned in neural networks.",
      "tldr_zh": "è¯¥è®ºæ–‡æ‰¹åˆ¤æ€§åœ°å®¡è§†äº†ç¥ç»ç½‘ç»œåœ¨ç³»ç»Ÿç»„åˆæ€§(Systematic Compositionality)æ–¹é¢çš„è¡¨ç°ï¼Œé‡æ–°æ¢è®¨äº† Fodor å’Œ Pylyshyn å…³äºç¥ç»ç½‘ç»œæ— æ³•æ¨¡æ‹Ÿç»“æ„æ•æ„Ÿæ“ä½œçš„ç»å…¸è®ºç‚¹ã€‚è™½ç„¶è¿‘æœŸç ”ç©¶è®¤ä¸ºå…ƒå­¦ä¹ (Meta-learning)å¯ä»¥ä½œä¸ºå®ç°ç»„åˆæ€§çš„è·¯å¾„ï¼Œä½†æœ¬æ–‡é€šè¿‡æ·±å…¥åˆ†ææ­ç¤ºäº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç›®å‰çš„ç¥ç»å…ƒå­¦ä¹ ç³»ç»Ÿä»…èƒ½åœ¨æåº¦å—é™çš„ç‰¹å®šè®¾ç½®ä¸‹è¡¨ç°å‡ºæ­¤ç±»èƒ½åŠ›ï¼Œæœ¬è´¨ä¸Šå¹¶æœªè¶…è¶Šæ—©æœŸçš„æ‰¹è¯„ã€‚ä½œè€…å¼ºè°ƒï¼Œç¥ç»ç½‘ç»œç›®å‰ä»ç„¶ç¼ºä¹ç±»äººæ°´å¹³çš„ç³»ç»Ÿç»„åˆæ€§ï¼ŒFodor å’Œ Pylyshyn çš„å­¦æœ¯é—äº§åœ¨å½“ä»£æ·±åº¦å­¦ä¹ èƒŒæ™¯ä¸‹ä¾ç„¶å…·æœ‰ç”Ÿå‘½åŠ›ã€‚è¿™ä¸€ç»“è®ºæé†’ç ”ç©¶è€…åœ¨å®£ç§°å…ƒå­¦ä¹ ç³»ç»Ÿå…·å¤‡é²æ£’çš„è·¨ç¯å¢ƒè¿ç§»èƒ½åŠ›æ—¶ï¼Œå¿…é¡»å®¡æ…å¯¹å¾…å…¶æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01820v3",
      "published_date": "2025-06-02 16:02:53 UTC",
      "updated_date": "2026-01-16 15:41:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:33:22.781663+00:00"
    },
    {
      "arxiv_id": "2506.01813v4",
      "title": "The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?",
      "title_zh": "è¶…æ™ºèƒ½ AI æ™ºèƒ½ä½“çš„ç»ˆææµ‹è¯•ï¼šAI èƒ½å¦åœ¨ä¸å¯¹ç§°å…³ç³»ä¸­å¹³è¡¡å…³æ€€ä¸æ§åˆ¶ï¼Ÿ",
      "authors": [
        "Djallel Bouneffouf",
        "Matthew Riemer",
        "Kush Varshney"
      ],
      "abstract": "This paper introduces the Shepherd Test, a new conceptual test for assessing the moral and relational dimensions of superintelligent artificial agents. The test is inspired by human interactions with animals, where ethical considerations about care, manipulation, and consumption arise in contexts of asymmetric power and self-preservation. We argue that AI crosses an important, and potentially dangerous, threshold of intelligence when it exhibits the ability to manipulate, nurture, and instrumentally use less intelligent agents, while also managing its own survival and expansion goals. This includes the ability to weigh moral trade-offs between self-interest and the well-being of subordinate agents. The Shepherd Test thus challenges traditional AI evaluation paradigms by emphasizing moral agency, hierarchical behavior, and complex decision-making under existential stakes. We argue that this shift is critical for advancing AI governance, particularly as AI systems become increasingly integrated into multi-agent environments. We conclude by identifying key research directions, including the development of simulation environments for testing moral behavior in AI, and the formalization of ethical manipulation within multi-agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Shepherd Testï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è¯„ä¼°è¶…çº§æ™ºèƒ½äººå·¥æ™ºèƒ½ä½“ (Superintelligent AI Agents) åœ¨éå¯¹ç§°å…³ç³»ä¸­é“å¾·ä¸å…³ç³»ç»´åº¦çš„æ–°å‹æ¦‚å¿µæ€§æµ‹è¯•ã€‚å—äººç±»ä¸åŠ¨ç‰©äº’åŠ¨çš„å¯å‘ï¼Œè¯¥æµ‹è¯•é‡ç‚¹å…³æ³¨æƒåŠ›ä¸å¯¹ç§°èƒŒæ™¯ä¸‹çš„å…³æ€€ã€æ“çºµä¸å·¥å…·åŒ–åˆ©ç”¨ç­‰ä¼¦ç†é—®é¢˜ã€‚ç ”ç©¶è®¤ä¸ºï¼Œå½“ AI è¡¨ç°å‡ºåœ¨è¿½æ±‚è‡ªèº«ç”Ÿå­˜ä¸æ‰©å¼ ç›®æ ‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ“çºµã€åŸ¹è‚²å¹¶å·¥å…·æ€§åœ°ä½¿ç”¨ä½æ™ºèƒ½ä½“æ—¶ï¼Œå³è·¨è¶Šäº†ä¸€ä¸ªå…³é”®ä¸”å…·æœ‰æ½œåœ¨å±é™©çš„æ™ºèƒ½é˜ˆå€¼ã€‚Shepherd Test é€šè¿‡å¼ºè°ƒé“å¾·ä¸»ä½“ (Moral Agency)ã€å±‚çº§è¡Œä¸ºä»¥åŠç”Ÿå­˜åˆ©å®³ä¸‹çš„å¤æ‚å†³ç­–ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„ AI è¯„ä¼°èŒƒå¼ã€‚è¿™ä¸€è½¬å˜å¯¹äºæ¨è¿›äººå·¥æ™ºèƒ½æ²»ç† (AI Governance) ä»¥åŠå¤„ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent Systems) ä¸­çš„å¤æ‚äº¤äº’è‡³å…³é‡è¦ã€‚è¯¥è®ºæ–‡æœ€åè¿˜æŒ‡å‡ºäº†å¼€å‘é“å¾·è¡Œä¸ºæ¨¡æ‹Ÿç¯å¢ƒåŠæ­£å¼åŒ–é“å¾·æ“çºµ (Ethical Manipulation) ç­‰å…³é”®ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01813v4",
      "published_date": "2025-06-02 15:53:56 UTC",
      "updated_date": "2025-09-29 14:30:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:33:53.690176+00:00"
    },
    {
      "arxiv_id": "2506.01806v1",
      "title": "Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition",
      "title_zh": "Ridgeformerï¼šé¢å‘ç»†ç²’åº¦è·¨åŸŸæŒ‡çº¹è¯†åˆ«çš„å¤šé˜¶æ®µå¯¹æ¯”è®­ç»ƒ",
      "authors": [
        "Shubham Pandey",
        "Bhavin Jawade",
        "Srirangaraj Setlur"
      ],
      "abstract": "The increasing demand for hygienic and portable biometric systems has underscored the critical need for advancements in contactless fingerprint recognition. Despite its potential, this technology faces notable challenges, including out-of-focus image acquisition, reduced contrast between fingerprint ridges and valleys, variations in finger positioning, and perspective distortion. These factors significantly hinder the accuracy and reliability of contactless fingerprint matching. To address these issues, we propose a novel multi-stage transformer-based contactless fingerprint matching approach that first captures global spatial features and subsequently refines localized feature alignment across fingerprint samples. By employing a hierarchical feature extraction and matching pipeline, our method ensures fine-grained, cross-sample alignment while maintaining the robustness of global feature representation. We perform extensive evaluations on publicly available datasets such as HKPolyU and RidgeBase under different evaluation protocols, such as contactless-to-contact matching and contactless-to-contactless matching and demonstrate that our proposed approach outperforms existing methods, including COTS solutions.",
      "tldr_zh": "é’ˆå¯¹éæ¥è§¦å¼æŒ‡çº¹è¯†åˆ«ä¸­å­˜åœ¨çš„å›¾åƒå¤±ç„¦ã€å¯¹æ¯”åº¦ä½ã€ä½ç½®åç§»åŠé€è§†ç•¸å˜ç­‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†åä¸º Ridgeformer çš„å¤šé˜¶æ®µ Transformer åŒ¹é…æ¡†æ¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚ç‰¹å¾æå–ä¸åŒ¹é…æµæ°´çº¿ï¼Œé¦–å…ˆæ•è·å…¨å±€ç©ºé—´ç‰¹å¾ï¼Œéšåå¯¹æŒ‡çº¹æ ·æœ¬é—´çš„å±€éƒ¨ç‰¹å¾è¿›è¡Œç²¾ç»†åŒ–å¯¹é½ã€‚é€šè¿‡å¤šé˜¶æ®µå¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼ŒRidgeformer åœ¨ç¡®ä¿å…¨å±€ç‰¹å¾è¡¨ç¤ºé²æ£’æ€§çš„åŒæ—¶ï¼Œå®ç°äº†è·¨æ ·æœ¬çš„ç»†ç²’åº¦ç‰¹å¾å¯¹é½ã€‚å®éªŒåœ¨ HKPolyU å’Œ RidgeBase ç­‰å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œæ¶µç›–äº†éæ¥è§¦åˆ°æ¥è§¦å¼ (contactless-to-contact) ä»¥åŠéæ¥è§¦åˆ°éæ¥è§¦å¼ (contactless-to-contactless) ç­‰å¤šç§åŒ¹é…åè®®ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼ŒRidgeformer åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•åŠå•†ä¸šç°æˆæ–¹æ¡ˆ (COTS)ï¼Œæœ‰æ•ˆæå‡äº†è·¨åŸŸæŒ‡çº¹è¯†åˆ«çš„å‡†ç¡®æ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to IEEE International Conference on Image Processing 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01806v1",
      "published_date": "2025-06-02 15:51:45 UTC",
      "updated_date": "2025-06-02 15:51:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:33:42.589976+00:00"
    },
    {
      "arxiv_id": "2506.01804v2",
      "title": "A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents",
      "title_zh": "é¢å‘å¢å¼ºå¤§è¯­è¨€æ¨¡å‹è‡ªä¸»æ™ºèƒ½ä½“äº’æ“ä½œæ€§çš„ MCP x A2A æ¡†æ¶ç ”ç©¶",
      "authors": [
        "Cheonsu Jeong"
      ],
      "abstract": "This paper provides an in-depth technical analysis and implementation methodology of the open-source Agent-to-Agent (A2A) protocol developed by Google and the Model Context Protocol (MCP) introduced by Anthropic. While the evolution of LLM-based autonomous agents is rapidly accelerating, efficient interactions among these agents and their integration with external systems remain significant challenges. In modern AI systems, collaboration between autonomous agents and integration with external tools have become essential elements for building practical AI applications. A2A offers a standardized communication method that enables agents developed in heterogeneous environments to collaborate effectively, while MCP provides a structured I/O framework for agents to connect with external tools and resources. Prior studies have focused primarily on the features and applications of either A2A or MCP individually. In contrast, this study takes an integrated approach, exploring how the two protocols can complement each other to address interoperability issues and facilitate efficient collaboration within complex agent ecosystems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºLLMçš„è‡ªä¸»æ™ºèƒ½ä½“åœ¨ç›¸äº’åä½œåŠå¤–éƒ¨ç³»ç»Ÿé›†æˆæ–¹é¢é¢ä¸´çš„ä¸¥å³»æŒ‘æˆ˜ï¼Œå¯¹Googleæ¨å‡ºçš„Agent-to-Agent (A2A)åè®®ä¸Anthropicå‘å¸ƒçš„Model Context Protocol (MCP)è¿›è¡Œäº†æ·±å…¥çš„æŠ€æœ¯åˆ†æä¸é›†æˆç ”ç©¶ã€‚A2Aåè®®é€šè¿‡æ ‡å‡†åŒ–çš„é€šä¿¡æ–¹æ³•å®ç°äº†å¼‚æ„ç¯å¢ƒä¸‹æ™ºèƒ½ä½“çš„é«˜æ•ˆåä½œï¼Œè€ŒMCPåˆ™ä¸ºæ™ºèƒ½ä½“æ¥å…¥å¤–éƒ¨å·¥å…·å’Œèµ„æºæä¾›äº†ç»“æ„åŒ–çš„I/Oæ¡†æ¶ã€‚ç›¸æ¯”ä»¥å¾€ä¾§é‡äºå•ä¸€åè®®åŠŸèƒ½çš„é›¶æ•£ç ”ç©¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ•´åˆæ¶æ„ï¼Œæ¢ç´¢è¿™ä¸¤ä¸ªåè®®å¦‚ä½•äº’è¡¥ä»¥è§£å†³å¤æ‚æ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿä¸­çš„äº’æ“ä½œæ€§(Interoperability)éš¾é¢˜ã€‚é€šè¿‡è¯¦ç»†çš„å®æ–½æ–¹æ³•è®ºè®ºè¯ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†A2Aä¸MCPçš„ç»“åˆèƒ½æœ‰æ•ˆæå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„äº¤äº’æ•ˆç‡ï¼Œä¸ºæ„å»ºå®ç”¨ä¸”å…·å¤‡é«˜åº¦ååŒèƒ½åŠ›çš„AIåº”ç”¨æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01804v2",
      "published_date": "2025-06-02 15:46:16 UTC",
      "updated_date": "2025-06-09 14:03:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:11.190974+00:00"
    },
    {
      "arxiv_id": "2506.01789v2",
      "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability",
      "title_zh": "ä»…æœ‰æ•°æ®è¡¨å°šä¸è¶³å¤Ÿï¼šé¢å‘è‡ªåŠ¨åŒ–è´¨é‡æŒ‡æ ‡ä¸é—®è´£åˆ¶çš„ DataRubrics",
      "authors": [
        "Genta Indra Winata",
        "David Anugraha",
        "Emmy Liu",
        "Alham Fikri Aji",
        "Shou-Yi Hung",
        "Aditya Parashar",
        "Patrick Amadeus Irawan",
        "Ruochen Zhang",
        "Zheng-Xin Yong",
        "Jan Christian Blaise Cruz",
        "Niklas Muennighoff",
        "Seungone Kim",
        "Hanyang Zhao",
        "Sudipta Kar",
        "Kezia Erina Suryoraharjo",
        "M. Farid Adilazuarda",
        "En-Shiun Annie Lee",
        "Ayu Purwarianti",
        "Derry Tanti Wijaya",
        "Monojit Choudhury"
      ],
      "abstract": "High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰æœºå™¨å­¦ä¹ é¢†åŸŸä¸­æ•°æ®é›†è´¨é‡è¯„ä¼°æ ‡å‡†ç¼ºå¤±åŠåŒè¡Œè¯„å®¡éš¾ä»¥é‡åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†DataRubricsè¿™ä¸€ç³»ç»ŸåŒ–çš„é‡è¡¨è¯„ä¼°æ¡†æ¶ã€‚ä½œè€…æŒ‡å‡ºä¼ ç»Ÿçš„Datasheetsç­‰å·¥å…·å¤šåå‘æè¿°æ€§ï¼Œè€ŒDataRubricsé€šè¿‡å¼•å…¥å¯è¡¡é‡çš„æŒ‡æ ‡ï¼Œåˆ©ç”¨LLM-as-a-judgeç­‰æŠ€æœ¯å®ç°äº†å¯¹äººå·¥åŠæ¨¡å‹ç”Ÿæˆæ•°æ®é›†çš„è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•è¯„ä¼°ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æé«˜æ•°æ®è´¨é‡çš„å¯é‡å¤æ€§å’Œé€æ˜åº¦ï¼Œç¡®ä¿åœ¨æ•°æ®é›†æŠ•ç¨¿é‡æ¿€å¢çš„èƒŒæ™¯ä¸‹ï¼Œè¯„å®¡è¿‡ç¨‹ä»èƒ½ä¿æŒä¸¥è°¨çš„è´¨é‡æ§åˆ¶ã€‚é€šè¿‡ç»“åˆåˆæˆæ•°æ®ç”Ÿæˆä¸é«˜æ•ˆè¯„ä¼°å·¥å…·ï¼ŒDataRubricsä¸ºæ•°æ®ä¸­å¿ƒç ”ç©¶ï¼ˆdata-centric researchï¼‰æä¾›äº†ä¸€å¥—å…·å¤‡æ“ä½œæ€§çš„é—®è´£æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥é¡¹ç›®å·²å¼€æºä»£ç ï¼Œæ—¨åœ¨æ”¯æŒç¤¾åŒºåœ¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°é¢†åŸŸè¿›è¡Œæ›´å¤šå¯é‡å¤æ€§å®éªŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2506.01789v2",
      "published_date": "2025-06-02 15:31:52 UTC",
      "updated_date": "2025-06-03 04:18:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:02.089047+00:00"
    },
    {
      "arxiv_id": "2506.01784v4",
      "title": "An Iterative Question-Guided Framework for Knowledge Base Question Answering",
      "title_zh": "ä¸€ç§é¢å‘çŸ¥è¯†åº“é—®ç­”çš„è¿­ä»£å¼é—®é¢˜å¼•å¯¼æ¡†æ¶",
      "authors": [
        "Shuai Wang",
        "Yinan Yu"
      ],
      "abstract": "Large Language Models (LLMs) excel in many natural language processing tasks but often exhibit factual inconsistencies in knowledge-intensive settings. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To tackle these challenges, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†iQUESTï¼Œä¸€ç§ä»¥é—®é¢˜ä¸ºå¯¼å‘çš„è¿­ä»£å¼çŸ¥è¯†åº“é—®ç­”(KBQA)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šæ­¥æ¨ç†(Multi-hop reasoning)ä¸­éš¾ä»¥ç»´æŒè¿è´¯è·¯å¾„åŠæ˜“è¿‡æ—©ä¸¢å¼ƒå…³é”®è¿æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¤æ‚çš„æŸ¥è¯¢è¿­ä»£åˆ†è§£ä¸ºæ›´ç®€å•çš„å­é—®é¢˜(Sub-questions)ï¼Œç¡®ä¿äº†æ¨ç†è½¨è¿¹çš„ç»“æ„åŒ–ä¸ä¸“æ³¨åº¦ã€‚æ­¤å¤–ï¼ŒiQUESTé›†æˆäº†å›¾ç¥ç»ç½‘ç»œ(GNN)ï¼Œåœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­é€šè¿‡å‰ç»æœºåˆ¶å¼•å…¥2-hopé‚»å±…ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹æ¢ç´¢å¯è¡Œæ¨ç†è·¯å¾„çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒiQUESTåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†å’Œå››ç§LLMsä¸Šå‡æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œä¸ºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æä¾›äº†æ›´å¯é çš„æ¨ç†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025), Main Track",
      "pdf_url": "https://arxiv.org/pdf/2506.01784v4",
      "published_date": "2025-06-02 15:30:02 UTC",
      "updated_date": "2025-11-20 08:41:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:33:57.095704+00:00"
    },
    {
      "arxiv_id": "2506.01782v1",
      "title": "Systematic Hazard Analysis for Frontier AI using STPA",
      "title_zh": "åŸºäº STPA çš„å‰æ²¿äººå·¥æ™ºèƒ½ç³»ç»Ÿæ€§å±å®³åˆ†æ",
      "authors": [
        "Simon Mylius"
      ],
      "abstract": "All of the frontier AI companies have published safety frameworks where they define capability thresholds and risk mitigations that determine how they will safely develop and deploy their models. Adoption of systematic approaches to risk modelling, based on established practices used in safety-critical industries, has been recommended, however frontier AI companies currently do not describe in detail any structured approach to identifying and analysing hazards. STPA (Systems-Theoretic Process Analysis) is a systematic methodology for identifying how complex systems can become unsafe, leading to hazards. It achieves this by mapping out controllers and controlled processes then analysing their interactions and feedback loops to understand how harmful outcomes could occur (Leveson & Thomas, 2018). We evaluate STPA's ability to broaden the scope, improve traceability and strengthen the robustness of safety assurance for frontier AI systems. Applying STPA to the threat model and scenario described in 'A Sketch of an AI Control Safety Case' (Korbak et al., 2025), we derive a list of Unsafe Control Actions. From these we select a subset and explore the Loss Scenarios that lead to them if left unmitigated. We find that STPA is able to identify causal factors that may be missed by unstructured hazard analysis methodologies thereby improving robustness. We suggest STPA could increase the safety assurance of frontier AI when used to complement or check coverage of existing AI governance techniques including capability thresholds, model evaluations and emergency procedures. The application of a systematic methodology supports scalability by increasing the proportion of the analysis that could be conducted by LLMs, reducing the burden on human domain experts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‰æ²¿äººå·¥æ™ºèƒ½ (Frontier AI) å…¬å¸åœ¨å®‰å…¨æ¡†æ¶ä¸­ç¼ºä¹è¯¦ç»†ç»“æ„åŒ–é£é™©åˆ†ææ–¹æ³•çš„é—®é¢˜ï¼Œè¯„ä¼°äº†ç³»ç»Ÿç†è®ºè¿‡ç¨‹åˆ†æ (STPA, Systems-Theoretic Process Analysis) åœ¨æå‡å®‰å…¨ä¿è¯ç¨³å¥æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚STPA é€šè¿‡æ˜ å°„æ§åˆ¶å™¨ (Controllers) ä¸å—æ§è¿‡ç¨‹ (Controlled Processes) ä¹‹é—´çš„äº¤äº’åŠåé¦ˆå¾ªç¯ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°è¯†åˆ«å¯¼è‡´ç³»ç»Ÿä¸å®‰å…¨çš„æ·±å±‚åŸå› ã€‚é€šè¿‡å°†è¯¥æ–¹æ³•åº”ç”¨äºç‰¹å®šçš„ AI æ§åˆ¶å®‰å…¨æ¡ˆä¾‹ï¼Œç ”ç©¶æˆåŠŸæ¨å¯¼å‡ºäº†ä¸€ç³»åˆ—ä¸å®‰å…¨æ§åˆ¶åŠ¨ä½œ (Unsafe Control Actions) ä¸æŸå¤±åœºæ™¯ (Loss Scenarios)ï¼Œè¯æ˜äº†å…¶èƒ½å‘ç°éç»“æ„åŒ–æ–¹æ³•å®¹æ˜“é—æ¼çš„å› æœå› ç´ ã€‚ç ”ç©¶å»ºè®®å°† STPA ä½œä¸ºç°æœ‰ AI æ²»ç†æŠ€æœ¯ï¼ˆå¦‚èƒ½åŠ›é˜ˆå€¼å’Œæ¨¡å‹è¯„ä¼°ï¼‰çš„è¡¥å……ï¼Œä»¥å¢å¼ºå®‰å…¨è¦†ç›–èŒƒå›´ã€‚æ­¤å¤–ï¼ŒSTPA çš„ç³»ç»ŸåŒ–ç‰¹æ€§æœ‰åŠ©äºæé«˜ç”±å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ‰§è¡Œåˆ†æçš„æ¯”ä¾‹ï¼Œä»è€Œåœ¨é™ä½ä¸“å®¶è´Ÿæ‹…çš„åŒæ—¶å®ç°å®‰å…¨åˆ†æçš„è§„æ¨¡åŒ–ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.CY",
      "comment": "29 pages, 5 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.01782v1",
      "published_date": "2025-06-02 15:28:34 UTC",
      "updated_date": "2025-06-02 15:28:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:06.991681+00:00"
    },
    {
      "arxiv_id": "2506.01781v1",
      "title": "Enhancing Customer Service Chatbots with Context-Aware NLU through Selective Attention and Multi-task Learning",
      "title_zh": "é€šè¿‡é€‰æ‹©æ€§æ³¨æ„åŠ›å’Œå¤šä»»åŠ¡å­¦ä¹ å¢å¼ºå®¢æœèŠå¤©æœºå™¨äººçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ NLU",
      "authors": [
        "Subhadip Nandi",
        "Neeraj Agrawal",
        "Anshika Singh",
        "Priyanka Bhatt"
      ],
      "abstract": "Customer service chatbots are conversational systems aimed at addressing customer queries, often by directing them to automated workflows. A crucial aspect of this process is the classification of the customer's intent. Presently, most intent classification models for customer care utilise only customer query for intent prediction. This may result in low-accuracy models, which cannot handle ambiguous queries. An ambiguous query like \"I didn't receive my package\" could indicate a delayed order, or an order that was delivered but the customer failed to receive it. Resolution of each of these scenarios requires the execution of very different sequence of steps. Utilizing additional information, such as the customer's order delivery status, in the right manner can help identify the intent for such ambiguous queries. In this paper, we have introduced a context-aware NLU model that incorporates both, the customer query and contextual information from the customer's order status for predicting customer intent. A novel selective attention module is used to extract relevant context features. We have also proposed a multi-task learning paradigm for the effective utilization of different label types available in our training data. Our suggested method, Multi-Task Learning Contextual NLU with Selective Attention Weighted Context (MTL-CNLU-SAWC), yields a 4.8% increase in top 2 accuracy score over the baseline model which only uses user queries, and a 3.5% improvement over existing state-of-the-art models that combine query and context. We have deployed our model to production for Walmart's customer care domain. Accurate intent prediction through MTL-CNLU-SAWC helps to better direct customers to automated workflows, thereby significantly reducing escalations to human agents, leading to almost a million dollars in yearly savings for the company.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®¢æœèŠå¤©æœºå™¨äººåœ¨å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢æ—¶æ„å›¾åˆ†ç±»(Intent Classification)å‡†ç¡®ç‡è¾ƒä½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥(Context-aware)çš„è‡ªç„¶è¯­è¨€ç†è§£(NLU)æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆç”¨æˆ·æŸ¥è¯¢ä¸è®¢å•çŠ¶æ€ç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåˆ©ç”¨æ–°é¢–çš„é€‰æ‹©æ€§æ³¨æ„åŠ›(Selective Attention)æ¨¡å—æå–ç›¸å…³ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ (Multi-task Learning)èŒƒå¼æå‡æ¨¡å‹æ•ˆèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„MTL-CNLU-SAWCæ–¹æ³•åœ¨Top 2å‡†ç¡®ç‡ä¸Šæ¯”ä»…ä½¿ç”¨æŸ¥è¯¢çš„åŸºçº¿æ¨¡å‹æå‡äº†4.8%ï¼Œä¸”ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚ç›®å‰è¯¥æ¨¡å‹å·²æˆåŠŸéƒ¨ç½²äºæ²ƒå°”ç›(Walmart)å®¢æœç³»ç»Ÿï¼Œé€šè¿‡ç²¾å‡†çš„æ„å›¾é¢„æµ‹å°†å®¢æˆ·æ›´æœ‰æ•ˆåœ°å¼•å¯¼è‡³è‡ªåŠ¨åŒ–å·¥ä½œæµï¼Œæ˜¾è‘—å‡å°‘äº†äººå·¥å®¢æœçš„ä»‹å…¥ï¼Œæ¯å¹´ä¸ºå…¬å¸èŠ‚çœè¿‘ç™¾ä¸‡ç¾å…ƒçš„æˆæœ¬ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01781v1",
      "published_date": "2025-06-02 15:24:28 UTC",
      "updated_date": "2025-06-02 15:24:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:14.028043+00:00"
    },
    {
      "arxiv_id": "2506.01778v1",
      "title": "unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning",
      "title_zh": "unMOREï¼šåŸºäºä¸­å¿ƒ-è¾¹ç•Œæ¨ç†çš„æ— ç›‘ç£å¤šç›®æ ‡åˆ†å‰²",
      "authors": [
        "Yafei Yang",
        "Zihui Zhang",
        "Bo Yang"
      ],
      "abstract": "We study the challenging problem of unsupervised multi-object segmentation on single images. Existing methods, which rely on image reconstruction objectives to learn objectness or leverage pretrained image features to group similar pixels, often succeed only in segmenting simple synthetic objects or discovering a limited number of real-world objects. In this paper, we introduce unMORE, a novel two-stage pipeline designed to identify many complex objects in real-world images. The key to our approach involves explicitly learning three levels of carefully defined object-centric representations in the first stage. Subsequently, our multi-object reasoning module utilizes these learned object priors to discover multiple objects in the second stage. Notably, this reasoning module is entirely network-free and does not require human labels. Extensive experiments demonstrate that unMORE significantly outperforms all existing unsupervised methods across 6 real-world benchmark datasets, including the challenging COCO dataset, achieving state-of-the-art object segmentation results. Remarkably, our method excels in crowded images where all baselines collapse.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•å¼ å›¾åƒçš„æ— ç›‘ç£å¤šç›®æ ‡åˆ†å‰²(Unsupervised Multi-Object Segmentation)è¿™ä¸€æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæå‡ºäº† unMOREï¼Œä¸€ç§æ—¨åœ¨è¯†åˆ«çœŸå®ä¸–ç•Œå›¾åƒä¸­å¤šç§å¤æ‚ç‰©ä½“çš„ä¸¤é˜¶æ®µæµæ°´çº¿æ¡†æ¶ã€‚è¯¥æ–¹æ³•åœ¨ç¬¬ä¸€é˜¶æ®µæ˜¾å¼å­¦ä¹ ä¸‰ä¸ªå±‚çº§çš„ç›®æ ‡ä¸­å¿ƒè¡¨ç¤º(Object-centric Representations)ï¼Œå¹¶åœ¨ç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¤šç›®æ ‡æ¨ç†æ¨¡å—(Multi-object Reasoning Module)ç»“åˆç‰©ä½“å…ˆéªŒæ¥å‘ç°å¤šä¸ªç‰©ä½“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨ç†æ¨¡å—å®Œå…¨ç‹¬ç«‹äºç½‘ç»œæ¶æ„(Network-free)ä¸”æ— éœ€äººç±»æ ‡æ³¨ï¼Œä¸»è¦é€šè¿‡ä¸­å¿ƒ-è¾¹ç•Œæ¨ç†(Center-Boundary Reasoning)å®ç°ç›®æ ‡è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒunMORE åœ¨åŒ…æ‹¬ COCO æ•°æ®é›†åœ¨å†…çš„ 6 ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ°´å¹³ã€‚å°¤å…¶åœ¨åŸºçº¿æ¨¡å‹æ™®éå¤±æ•ˆçš„æ‹¥æŒ¤å›¾åƒ(Crowded Images)åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•ä¾ç„¶è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶å¤„ç†å¤æ‚ç°å®åœºæ™¯çš„å¼ºå¤§èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "ICML 2025. Code and data are available at: https://github.com/vLAR-group/unMORE",
      "pdf_url": "https://arxiv.org/pdf/2506.01778v1",
      "published_date": "2025-06-02 15:22:51 UTC",
      "updated_date": "2025-06-02 15:22:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:13.242867+00:00"
    },
    {
      "arxiv_id": "2506.01776v2",
      "title": "MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation",
      "title_zh": "MaXIFEï¼šå¤šè¯­è¨€ä¸è·¨è¯­è¨€æŒ‡ä»¤éµå¾ªè¯„æµ‹",
      "authors": [
        "Yile Liu",
        "Ziwei Ma",
        "Xiu Jiang",
        "Jinglu Hu",
        "Jing Chang",
        "Liang Li"
      ],
      "abstract": "With the rapid adoption of large language models (LLMs) in natural language processing, the ability to follow instructions has emerged as a key metric for evaluating their practical utility. However, existing evaluation methods often focus on single-language scenarios, overlooking the challenges and differences present in multilingual and cross-lingual contexts. To address this gap, we introduce MaXIFE: a comprehensive evaluation benchmark designed to assess instruction-following capabilities across 23 different languages with 1667 verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to evaluate several leading commercial LLMs, establishing baseline results for future comparisons. By providing a standardized tool for multilingual instruction-following evaluation, MaXIFE aims to advance research and development in natural language processing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MaXIFEï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°åŸºå‡† (benchmark)ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤šè¯­è¨€ (multilingual) å’Œè·¨è¯­è¨€ (cross-lingual) èƒŒæ™¯ä¸‹çš„æŒ‡ä»¤éµå¾ª (instruction-following) èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ¶µç›– 23 ç§ä¸åŒè¯­è¨€ï¼ŒåŒ…å« 1667 ä¸ªå¯éªŒè¯çš„æŒ‡ä»¤ä»»åŠ¡ (verifiable instruction tasks)ï¼Œæœ‰æ•ˆå¡«è¡¥äº†ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ä¸€è¯­è¨€åœºæ™¯çš„ç©ºç™½ã€‚MaXIFE æ•´åˆäº†åŸºäºè§„åˆ™çš„è¯„ä¼° (Rule-Based Evaluation) å’ŒåŸºäºæ¨¡å‹çš„è¯„ä¼° (Model-Based Evaluation)ï¼Œåœ¨ç¡®ä¿è¯„ä¼°æ•ˆç‡çš„åŒæ—¶æå‡äº†å‡†ç¡®æ€§ã€‚ç ”ç©¶è€…åˆ©ç”¨ MaXIFE å¯¹å¤šä¸ªé¢†å…ˆçš„å•†ä¸š LLMs è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å»ºç«‹äº†åŸºçº¿ç»“æœ (baseline results)ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å‚è€ƒæ ‡å‡†ã€‚ä½œä¸ºä¸€ç§æ ‡å‡†åŒ–çš„å¤šè¯­è¨€è¯„ä¼°å·¥å…·ï¼ŒMaXIFE æ—¨åœ¨è¿›ä¸€æ­¥æ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç† (Natural Language Processing) é¢†åŸŸçš„ç ”å‘è¿›ç¨‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2506.01776v2",
      "published_date": "2025-06-02 15:20:20 UTC",
      "updated_date": "2025-06-03 02:53:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:28.029397+00:00"
    },
    {
      "arxiv_id": "2506.01774v2",
      "title": "Greening AI-enabled Systems with Software Engineering: A Research Agenda for Environmentally Sustainable AI Practices",
      "title_zh": "è½¯ä»¶å·¥ç¨‹åŠ©åŠ›äººå·¥æ™ºèƒ½ç³»ç»Ÿç»¿è‰²åŒ–ï¼šç¯å¢ƒå¯æŒç»­äººå·¥æ™ºèƒ½å®è·µç ”ç©¶è®®ç¨‹",
      "authors": [
        "LuÃ­s Cruz",
        "JoÃ£o Paulo Fernandes",
        "Maja H. Kirkeby",
        "Silverio MartÃ­nez-FernÃ¡ndez",
        "June Sallou",
        "Hina Anwar",
        "Enrique Barba Roque",
        "Justus Bogner",
        "Joel CastaÃ±o",
        "Fernando Castor",
        "Aadil Chasmawala",
        "SimÃ£o Cunha",
        "Daniel Feitosa",
        "Alexandra GonzÃ¡lez",
        "Andreas Jedlitschka",
        "Patricia Lago",
        "Henry Muccini",
        "Ana Oprescu",
        "Pooja Rani",
        "JoÃ£o Saraiva",
        "Federica Sarro",
        "Raghavendra Selvan",
        "Karthik Vaidhyanathan",
        "Roberto Verdecchia",
        "Ivan P. Yamshchikov"
      ],
      "abstract": "The environmental impact of Artificial Intelligence (AI)-enabled systems is increasing rapidly, and software engineering plays a critical role in developing sustainable solutions. The \"Greening AI with Software Engineering\" CECAM-Lorentz workshop (no. 1358, 2025) funded by the Centre EuropÃ©en de Calcul Atomique et MolÃ©culaire and the Lorentz Center, provided an interdisciplinary forum for 29 participants, from practitioners to academics, to share knowledge, ideas, practices, and current results dedicated to advancing green software and AI research. The workshop was held February 3-7, 2025, in Lausanne, Switzerland. Through keynotes, flash talks, and collaborative discussions, participants identified and prioritized key challenges for the field. These included energy assessment and standardization, benchmarking practices, sustainability-aware architectures, runtime adaptation, empirical methodologies, and education. This report presents a research agenda emerging from the workshop, outlining open research directions and practical recommendations to guide the development of environmentally sustainable AI-enabled systems rooted in software engineering principles.",
      "tldr_zh": "è¯¥æŠ¥å‘Šæ€»ç»“äº†2025å¹´ä¸¾åŠçš„Greening AI with Software Engineeringç ”è®¨ä¼šæˆæœï¼Œæ—¨åœ¨æ¢è®¨å¦‚ä½•åˆ©ç”¨Software EngineeringåŸç†è§£å†³Artificial Intelligence (AI)ç³»ç»Ÿæ—¥ç›Šå¢é•¿çš„ç¯å¢ƒå½±å“é—®é¢˜ã€‚ç ”è®¨ä¼šæ±‡é›†äº†29ä½è·¨å­¦ç§‘ä¸“å®¶ï¼Œé€šè¿‡ä¸»æ—¨æ¼”è®²å’Œåä½œè®¨è®ºï¼Œç³»ç»Ÿåœ°è¯†åˆ«å¹¶ç¡®å®šäº†è¯¥é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ã€‚æŠ¥å‘Šé‡ç‚¹å…³æ³¨äº†Energy Assessment and Standardizationã€Benchmarking Practicesã€Sustainability-aware Architecturesä»¥åŠRuntime Adaptationç­‰æ ¸å¿ƒè®®é¢˜ã€‚åŸºäºè¿™äº›è®¨è®ºï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€å¥—å…¨æ–°çš„Research Agendaï¼Œæ¶µç›–äº†å®è¯æ–¹æ³•è®ºå’Œæ•™è‚²é¢†åŸŸçš„å¼€æ”¾ç ”ç©¶æ–¹å‘ã€‚è¯¥æˆæœä¸ºå¼€å‘ç¯å¢ƒå¯æŒç»­çš„AIç³»ç»Ÿæä¾›äº†å®è·µå»ºè®®å’Œç†è®ºæŒ‡å¯¼ï¼Œå¼ºè°ƒäº†è½¯ä»¶å·¥ç¨‹åœ¨é™ä½AIç¢³è¶³è¿¹åŠå®ç°å¯æŒç»­å‘å±•ä¸­çš„å…³é”®æ”¯æ’‘ä½œç”¨ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01774v2",
      "published_date": "2025-06-02 15:19:49 UTC",
      "updated_date": "2025-06-03 08:44:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:28.821067+00:00"
    },
    {
      "arxiv_id": "2506.01770v1",
      "title": "ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs",
      "title_zh": "ReGAï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹åŸºäºæ¨¡å‹å®‰å…¨ä¿éšœçš„è¡¨å¾å¼•å¯¼æŠ½è±¡",
      "authors": [
        "Zeming Wei",
        "Chengcan Wu",
        "Meng Sun"
      ],
      "abstract": "Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content and vulnerability to jailbreaking attacks. To analyze and monitor machine learning models, model-based analysis has demonstrated notable potential in stateful deep neural networks, yet suffers from scalability issues when extending to LLMs due to their vast feature spaces. In this paper, we propose ReGA, a model-based analysis framework with representation-guided abstraction, to safeguard LLMs against harmful prompts and generations. By leveraging safety-critical representations, which are low-dimensional directions emerging in hidden states that indicate safety-related concepts, ReGA effectively addresses the scalability issue when constructing the abstract model for safety modeling. Our comprehensive evaluation shows that ReGA performs sufficiently well in distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. Additionally, ReGA exhibits robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability. Overall, ReGA serves as an efficient and scalable solution to enhance LLM safety by integrating representation engineering with model-based abstraction, paving the way for new paradigms to utilize software insights for AI safety. Our code is available at https://github.com/weizeming/ReGA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆæœ‰å®³å†…å®¹åŠåº”å¯¹è¶Šç‹±æ”»å‡»ï¼ˆjailbreaking attacksï¼‰æ–¹é¢çš„å®‰å…¨é£é™©ï¼Œæå‡ºäº† ReGA è¿™ä¸€é‡‡ç”¨è¡¨ç¤ºå¼•å¯¼æŠ½è±¡ï¼ˆrepresentation-guided abstractionï¼‰çš„åŸºäºæ¨¡å‹åˆ†ææ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä¼ ç»ŸåŸºäºæ¨¡å‹åˆ†æï¼ˆmodel-based analysisï¼‰åœ¨å¤„ç† LLMs åºå¤§ç‰¹å¾ç©ºé—´æ—¶é¢ä¸´çš„å¯æ‰©å±•æ€§ï¼ˆscalabilityï¼‰éš¾é¢˜ï¼ŒReGA åˆ©ç”¨éšè—çŠ¶æ€ä¸­æ˜¾ç°çš„ä½ç»´å®‰å…¨å…³é”®è¡¨ç¤ºï¼ˆsafety-critical representationsï¼‰æ¥æ„å»ºæŠ½è±¡æ¨¡å‹ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åŒºåˆ†å®‰å…¨ä¸æœ‰å®³è¾“å…¥æ–¹é¢è¡¨ç°å“è¶Šï¼Œåœ¨æç¤ºè¯å±‚é¢å’Œå¯¹è¯å±‚é¢çš„ AUROC åˆ†åˆ«è¾¾åˆ° 0.975 å’Œ 0.985ã€‚æ­¤å¤–ï¼ŒReGA å¯¹ç°å®ä¸–ç•Œæ”»å‡»å…·æœ‰å¼ºé²æ£’æ€§ï¼Œå¹¶åœ¨ä¸åŒå®‰å…¨ç»´åº¦ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§ä¸Šå‡ä¼˜äºç°æœ‰èŒƒå¼ã€‚é€šè¿‡æ•´åˆè¡¨ç¤ºå·¥ç¨‹ï¼ˆrepresentation engineeringï¼‰ä¸åŸºäºæ¨¡å‹çš„æŠ½è±¡ï¼Œè¯¥ç ”ç©¶ä¸ºåˆ©ç”¨è½¯ä»¶åˆ†ææ–¹æ³•å¢å¼º AI å®‰å…¨æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01770v1",
      "published_date": "2025-06-02 15:17:38 UTC",
      "updated_date": "2025-06-02 15:17:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:50.247191+00:00"
    },
    {
      "arxiv_id": "2506.01757v1",
      "title": "Efficient Egocentric Action Recognition with Multimodal Data",
      "title_zh": "åŸºäºå¤šæ¨¡æ€æ•°æ®çš„é«˜æ•ˆç¬¬ä¸€è§†è§’åŠ¨ä½œè¯†åˆ«",
      "authors": [
        "Marco Calzavara",
        "Ard Kastrati",
        "Matteo Macchini",
        "Dushan Vasilevski",
        "Roger Wattenhofer"
      ],
      "abstract": "The increasing availability of wearable XR devices opens new perspectives for Egocentric Action Recognition (EAR) systems, which can provide deeper human understanding and situation awareness. However, deploying real-time algorithms on these devices can be challenging due to the inherent trade-offs between portability, battery life, and computational resources. In this work, we systematically analyze the impact of sampling frequency across different input modalities - RGB video and 3D hand pose - on egocentric action recognition performance and CPU usage. By exploring a range of configurations, we provide a comprehensive characterization of the trade-offs between accuracy and computational efficiency. Our findings reveal that reducing the sampling rate of RGB frames, when complemented with higher-frequency 3D hand pose input, can preserve high accuracy while significantly lowering CPU demands. Notably, we observe up to a 3x reduction in CPU usage with minimal to no loss in recognition performance. This highlights the potential of multimodal input strategies as a viable approach to achieving efficient, real-time EAR on XR devices.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯ç©¿æˆ´XRè®¾å¤‡åœ¨æ‰§è¡Œç¬¬ä¸€äººç§°åŠ¨ä½œè¯†åˆ«(EAR)æ—¶é¢ä¸´çš„è®¡ç®—èµ„æºæœ‰é™å’ŒåŠŸè€—æŒ‘æˆ˜ï¼Œç³»ç»Ÿåˆ†æäº†ä¸åŒè¾“å…¥æ¨¡æ€é‡‡æ ·é¢‘ç‡å¯¹æ€§èƒ½å’ŒCPUæ¶ˆè€—çš„å½±å“ã€‚ç ”ç©¶è€…é€šè¿‡å¯¹æ¯”RGB videoå’Œ3D hand poseè¿™ä¸¤ç§æ¨¡æ€åœ¨ä¸åŒé…ç½®ä¸‹çš„è¡¨ç°ï¼Œæ·±å…¥æ¢è®¨äº†è¯†åˆ«å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è¾…ä»¥é«˜é¢‘3D hand poseè¾“å…¥çš„å‰æä¸‹ï¼Œé™ä½RGB framesçš„é‡‡æ ·ç‡èƒ½å¤Ÿæ˜¾è‘—å‡è½»è®¡ç®—è´Ÿæ‹…ï¼Œä¸”å¯¹è¯†åˆ«ç²¾åº¦å½±å“æå°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç­–ç•¥åœ¨ä¿è¯è¯†åˆ«æ€§èƒ½çš„åŒæ—¶ï¼ŒæˆåŠŸå®ç°äº†æœ€é«˜è¾¾3å€çš„CPUä½¿ç”¨ç‡é™å¹…ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†å¤šæ¨¡æ€(multimodal)è¾“å…¥ç­–ç•¥åœ¨å®ç°é«˜æ•ˆã€å®æ—¶EARç³»ç»Ÿæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºèµ„æºå—é™è®¾å¤‡ä¸Šçš„è¡Œä¸ºç†è§£æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted as an extended abstract at the Second Joint Egocentric Vision (EgoVis) Workshop, 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01757v1",
      "published_date": "2025-06-02 15:04:23 UTC",
      "updated_date": "2025-06-02 15:04:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:50.808193+00:00"
    },
    {
      "arxiv_id": "2506.02090v1",
      "title": "The Impact of Software Testing with Quantum Optimization Meets Machine Learning",
      "title_zh": "é‡å­ä¼˜åŒ–ä¸æœºå™¨å­¦ä¹ èåˆå¯¹è½¯ä»¶æµ‹è¯•çš„å½±å“ç ”ç©¶",
      "authors": [
        "Gopichand Bandarupalli"
      ],
      "abstract": "Modern software systems complexity challenges efficient testing, as traditional machine learning (ML) struggles with large test suites. This research presents a hybrid framework integrating Quantum Annealing with ML to optimize test case prioritization in CI/CD pipelines. Leveraging quantum optimization, it achieves a 25 percent increase in defect detection efficiency and a 30 percent reduction in test execution time versus classical ML, validated on the Defects4J dataset. A simulated CI/CD environment demonstrates robustness across evolving codebases. Visualizations, including defect heatmaps and performance graphs, enhance interpretability. The framework addresses quantum hardware limits, CI/CD integration, and scalability for 2025s hybrid quantum-classical ecosystems, offering a transformative approach to software quality assurance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£è½¯ä»¶ç³»ç»Ÿå¤æ‚æ€§å¯¼è‡´çš„ä¼ ç»Ÿ Machine Learning (ML) åœ¨å¤„ç†å¤§è§„æ¨¡æµ‹è¯•å¥—ä»¶æ—¶çš„æ•ˆç‡ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§å°† Quantum Annealing ä¸ ML ç›¸ç»“åˆçš„æ··åˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä¼˜åŒ– CI/CD æµæ°´çº¿ä¸­çš„æµ‹è¯•ç”¨ä¾‹ä¼˜å…ˆçº§æ’åº (Test Case Prioritization)ï¼Œåˆ©ç”¨é‡å­ä¼˜åŒ–æŠ€æœ¯æ˜¾è‘—æå‡æµ‹è¯•æµç¨‹çš„æ•ˆèƒ½ã€‚åœ¨ Defects4J æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿ ML æ–¹æ³•å°†ç¼ºé™·æ£€æµ‹æ•ˆç‡æé«˜äº† 25%ï¼ŒåŒæ—¶ä½¿æµ‹è¯•æ‰§è¡Œæ—¶é—´ç¼©çŸ­äº† 30%ã€‚ç ”ç©¶è¿˜åœ¨æ¨¡æ‹Ÿ CI/CD ç¯å¢ƒä¸­éªŒè¯äº†å…¶é¢å¯¹æ¼”è¿›ä»£ç åº“æ—¶çš„ç¨³å¥æ€§ï¼Œå¹¶é€šè¿‡ç¼ºé™·çƒ­åŠ›å›¾ç­‰å¯è§†åŒ–æ‰‹æ®µå¢å¼ºäº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåº”å¯¹äº†é‡å­ç¡¬ä»¶é™åˆ¶åŠæ‰©å±•æ€§æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥æ··åˆé‡å­-ç»å…¸ç”Ÿæ€ç³»ç»Ÿä¸­çš„è½¯ä»¶è´¨é‡ä¿è¯æä¾›äº†å…·æœ‰å˜é©æ„ä¹‰çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.02090v1",
      "published_date": "2025-06-02 15:04:10 UTC",
      "updated_date": "2025-06-02 15:04:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:57.244698+00:00"
    },
    {
      "arxiv_id": "2506.01728v2",
      "title": "Principled Data Augmentation for Learning to Solve Quadratic Programming Problems",
      "title_zh": "é¢å‘å­¦ä¹ æ±‚è§£äºŒæ¬¡è§„åˆ’é—®é¢˜çš„è§„èŒƒåŒ–æ•°æ®å¢å¼º",
      "authors": [
        "Chendi Qian",
        "Christopher Morris"
      ],
      "abstract": "Linear and quadratic optimization are crucial in numerous real-world applications, ranging from training machine learning models to solving integer linear programs. Recently, learning-to-optimize methods (L2O) for linear (LPs) or quadratic programs (QPs) using message-passing graph neural networks (MPNNs) have gained traction, promising lightweight, data-driven proxies for solving such optimization problems. For example, they replace the costly computation of strong branching scores in branch-and-bound solvers, thereby reducing the need to solve many such optimization problems. However, robust L2O MPNNs remain challenging in data-scarce settings, especially when addressing complex optimization problems such as QPs. This work introduces a principled approach to data augmentation tailored for QPs via MPNNs. Our method leverages theoretically justified data augmentation techniques to generate diverse yet optimality-preserving instances. Furthermore, we integrate these augmentations into a self-supervised contrastive learning framework, thereby pretraining MPNNs for improved performance on L2O tasks. Extensive experiments demonstrate that our approach improves generalization in supervised scenarios and facilitates effective transfer learning to related optimization problems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹ï¼Œåˆ©ç”¨æ¶ˆæ¯ä¼ é€’å›¾ç¥ç»ç½‘ç»œ (MPNNs) è§£å†³äºŒæ¬¡è§„åˆ’ (QPs) ç­‰å¤æ‚ä¼˜åŒ–é—®é¢˜æ—¶å­˜åœ¨çš„é²æ£’æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸåˆ™æ€§çš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…·æœ‰ç†è®ºä¾æ®çš„æŠ€æœ¯ç”Ÿæˆå¤šæ ·åŒ–ä¸”ä¿æŒæœ€ä¼˜æ€§ (Optimality-preserving) çš„ç®—ä¾‹ï¼Œä¸“é—¨é’ˆå¯¹åŸºäº MPNNs çš„äºŒæ¬¡è§„åˆ’æ±‚è§£è¿›è¡Œä¼˜åŒ–ã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†è¿™äº›å¢å¼ºæŠ€æœ¯æ•´åˆè¿›è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹  (Self-supervised Contrastive Learning) æ¡†æ¶ä¸­ï¼Œé€šè¿‡é¢„è®­ç»ƒ MPNNs æ¥æå‡å…¶åœ¨å­¦ä¹ ä¼˜åŒ– (L2O) ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨ç›‘ç£åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å®ç°äº†å‘ç›¸å…³ä¼˜åŒ–é—®é¢˜çš„æœ‰æ•ˆè¿ç§»å­¦ä¹ ã€‚è¯¥æˆæœä¸ºæ„å»ºé«˜æ•ˆã€é²æ£’çš„æ•°æ®é©±åŠ¨å‹ä¼˜åŒ–é—®é¢˜ä»£ç†æ¨¡å‹æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025 as spotlight",
      "pdf_url": "https://arxiv.org/pdf/2506.01728v2",
      "published_date": "2025-06-02 14:40:18 UTC",
      "updated_date": "2025-10-24 08:39:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:35:05.360290+00:00"
    },
    {
      "arxiv_id": "2506.01723v5",
      "title": "Tug-of-war between idioms' figurative and literal interpretations in LLMs",
      "title_zh": "LLMs ä¸­ä¹ è¯­æ¯”å–»ä¹‰ä¸å­—é¢ä¹‰çš„åšå¼ˆ",
      "authors": [
        "Soyoung Oh",
        "Xinting Huang",
        "Mathis Pink",
        "Michael Hahn",
        "Vera Demberg"
      ],
      "abstract": "Idioms present a unique challenge for language models due to their non-compositional figurative interpretations, which often strongly diverge from the idiom's literal interpretation. In this paper, we employ causal tracing to systematically analyze how pretrained causal transformers deal with this ambiguity. We localize three mechanisms: (i) Early sublayers and specific attention heads retrieve an idiom's figurative interpretation, while suppressing its literal interpretation. (ii) When disambiguating context precedes the idiom, the model leverages it from the earliest layer and later layers refine the interpretation if the context conflicts with the retrieved interpretation. (iii) Then, selective, competing pathways carry both interpretations: an intermediate pathway prioritizes the figurative interpretation and a parallel direct route favors the literal interpretation, ensuring that both readings remain available. Our findings provide mechanistic evidence for idiom comprehension in autoregressive transformers.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨å› æœè¿½è¸ª (causal tracing) æŠ€æœ¯ï¼Œæ·±å…¥åˆ†æäº†é¢„è®­ç»ƒå› æœå˜æ¢å™¨ (causal transformers) åœ¨å¤„ç†ä¹ è¯­ (idioms) çš„æ¯”å–»ä¹‰ (figurative interpretation) ä¸å­—é¢ä¹‰ (literal interpretation) å†²çªæ—¶çš„å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„æ—©æœŸå­å±‚å’Œç‰¹å®šæ³¨æ„åŠ›å¤´ (attention heads) ä¼šä¼˜å…ˆæ£€ç´¢æ¯”å–»ä¹‰å¹¶æŠ‘åˆ¶å­—é¢ä¹‰ã€‚å½“å­˜åœ¨æ¶ˆæ­§ä¸Šä¸‹æ–‡æ—¶ï¼Œæ¨¡å‹èƒ½ä»èµ·å§‹å±‚æœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åœ¨åç»­å±‚æ ¹æ®å†²çªè¿›è¡Œè¯­ä¹‰ä¿®æ­£ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å†…éƒ¨é€šè¿‡é€‰æ‹©æ€§çš„ç«äº‰è·¯å¾„åŒæ—¶ä¿ç•™ä¸¤ç§è§£è¯»ï¼šä¸­é—´è·¯å¾„å€¾å‘äºå¤„ç†æ¯”å–»ä¹‰ï¼Œè€Œå¹³è¡Œçš„ç›´æ¥è·¯å¾„åˆ™ä¿ç•™äº†å­—é¢ä¹‰ï¼Œç¡®ä¿ä¸¤ç§ç†è§£åœ¨è®¡ç®—è¿‡ç¨‹ä¸­å‡ä¿æŒå¯ç”¨ã€‚è¯¥å·¥ä½œä¸ºè‡ªå›å½’å˜æ¢å™¨ (autoregressive transformers) ç†è§£éç»„åˆæ€§è¯­ä¹‰æä¾›äº†æœºæ¢°è®ºè¯æ® (mechanistic evidence)ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚è¯­è¨€æ­§ä¹‰æ—¶å¤šè·¯å¾„åä½œçš„åº•å±‚é€»è¾‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01723v5",
      "published_date": "2025-06-02 14:29:46 UTC",
      "updated_date": "2026-01-16 16:31:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:34:58.841705+00:00"
    },
    {
      "arxiv_id": "2506.01716v1",
      "title": "Self-Challenging Language Model Agents",
      "title_zh": "è‡ªæŒ‘æˆ˜å¼è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Yifei Zhou",
        "Sergey Levine",
        "Jason Weston",
        "Xian Li",
        "Sainbayar Sukhbaatar"
      ],
      "abstract": "Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Self-Challenging æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Large Language Models æ™ºèƒ½ä½“åœ¨å·¥å…·ä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹äººå·¥åˆ›å»ºä»»åŠ¡å’Œè¯„ä¼°æ ‡å‡†çš„è¿‡åº¦ä¾èµ–ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œæ™ºèƒ½ä½“é¦–å…ˆæ‰®æ¼”æŒ‘æˆ˜è€… (Challenger) è§’è‰²ï¼Œåœ¨ä¸å·¥å…·äº¤äº’åç”Ÿæˆä¸€ç§åä¸º Code-as-Task çš„æ–°å‹ä»»åŠ¡ï¼Œåˆ©ç”¨å†…ç½®çš„éªŒè¯å‡½æ•°å’Œæµ‹è¯•ç”¨ä¾‹ç­›é€‰å‡ºé«˜è´¨é‡è®­ç»ƒæ ·æœ¬ã€‚éšåï¼Œæ™ºèƒ½ä½“è½¬å˜ä¸ºæ‰§è¡Œè€… (Executor) è§’è‰²ï¼Œé€šè¿‡ Reinforcement Learning åˆ©ç”¨è¯„ä¼°åé¦ˆä½œä¸ºå¥–åŠ±è¿›è¡Œè‡ªæˆ‘è¿›åŒ–è®­ç»ƒã€‚åœ¨ M3ToolEval å’Œ TauBench ä¸¤ä¸ªå¤šè½®å·¥å…·ä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä¾¿ä»…ä½¿ç”¨è‡ªç”Ÿæˆçš„è®­ç»ƒæ•°æ®ï¼Œè¯¥æ¡†æ¶ä¹Ÿèƒ½ä½¿ Llama-3.1-8B-Instruct çš„æ€§èƒ½æå‡ä¸¤å€ä»¥ä¸Šï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“è°ƒç”¨å·¥å…·çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01716v1",
      "published_date": "2025-06-02 14:23:33 UTC",
      "updated_date": "2025-06-02 14:23:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:35:13.438122+00:00"
    },
    {
      "arxiv_id": "2506.01704v1",
      "title": "Generate, Not Recommend: Personalized Multimodal Content Generation",
      "title_zh": "ç”Ÿæˆè€Œéæ¨èï¼šä¸ªæ€§åŒ–å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆ",
      "authors": [
        "Jiongnan Liu",
        "Zhicheng Dou",
        "Ning Hu",
        "Chenyan Xiong"
      ],
      "abstract": "To address the challenge of information overload from massive web contents, recommender systems are widely applied to retrieve and present personalized results for users. However, recommendation tasks are inherently constrained to filtering existing items and lack the ability to generate novel concepts, limiting their capacity to fully satisfy user demands and preferences. In this paper, we propose a new paradigm that goes beyond content filtering and selecting: directly generating personalized items in a multimodal form, such as images, tailored to individual users. To accomplish this, we leverage any-to-any Large Multimodal Models (LMMs) and train them in both supervised fine-tuning and online reinforcement learning strategy to equip them with the ability to yield tailored next items for users. Experiments on two benchmark datasets and user study confirm the efficacy of the proposed method. Notably, the generated images not only align well with users' historical preferences but also exhibit relevance to their potential future interests.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ¨èç³»ç»Ÿä»…é™äºè¿‡æ»¤ç°æœ‰æ¡ç›®è€Œæ— æ³•äº§ç”Ÿæ–°æ¦‚å¿µçš„å±€é™æ€§ï¼Œæå‡ºäº†â€œç”Ÿæˆè€Œéæ¨èâ€ (Generate, Not Recommend) çš„æ–°èŒƒå¼ï¼Œæ—¨åœ¨ç›´æ¥ä¸ºç”¨æˆ·ç”Ÿæˆå®šåˆ¶çš„å¤šæ¨¡æ€ (Multimodal) å†…å®¹ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å…¨èƒ½å‹å¤§è¯­è¨€å¤šæ¨¡æ€æ¨¡å‹ (any-to-any Large Multimodal Models, LMMs)ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning) å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹  (Online Reinforcement Learning) ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œèµ‹äºˆæ¨¡å‹ç”Ÿæˆä¸ªæ€§åŒ–åç»­æ¡ç›®çš„èƒ½åŠ›ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒåŠç”¨æˆ·ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç”Ÿæˆçš„å›¾åƒèƒ½å¤Ÿç²¾å‡†åŒ¹é…ç”¨æˆ·çš„å†å²åå¥½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç”Ÿæˆçš„å†…å®¹è¿˜å±•ç°å‡ºä¸ç”¨æˆ·æ½œåœ¨æœªæ¥å…´è¶£çš„é«˜åº¦ç›¸å…³æ€§ã€‚è¯¥æ–¹æ³•æˆåŠŸçªç ´äº†ä¼ ç»Ÿæ¨èçš„ä»»åŠ¡è¾¹ç•Œï¼Œä¸ºå®ç°é«˜åº¦ä¸ªæ€§åŒ–çš„å†…å®¹ç”Ÿæˆæä¾›äº†å…¨æ–°çš„è§£å†³è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01704v1",
      "published_date": "2025-06-02 14:10:08 UTC",
      "updated_date": "2025-06-02 14:10:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:35:29.295404+00:00"
    },
    {
      "arxiv_id": "2506.01701v2",
      "title": "Data Pruning by Information Maximization",
      "title_zh": "åŸºäºä¿¡æ¯æœ€å¤§åŒ–çš„æ•°æ®å‰ªæ",
      "authors": [
        "Haoru Tan",
        "Sitong Wu",
        "Wei Huang",
        "Shizhen Zhao",
        "Xiaojuan Qi"
      ],
      "abstract": "In this paper, we present InfoMax, a novel data pruning method, also known as coreset selection, designed to maximize the information content of selected samples while minimizing redundancy. By doing so, InfoMax enhances the overall informativeness of the coreset. The information of individual samples is measured by importance scores, which capture their influence or difficulty in model learning. To quantify redundancy, we use pairwise sample similarities, based on the premise that similar samples contribute similarly to the learning process. We formalize the coreset selection problem as a discrete quadratic programming (DQP) task, with the objective of maximizing the total information content, represented as the sum of individual sample contributions minus the redundancies introduced by similar samples within the coreset. To ensure practical scalability, we introduce an efficient gradient-based solver, complemented by sparsification techniques applied to the similarity matrix and dataset partitioning strategies. This enables InfoMax to seamlessly scale to datasets with millions of samples. Extensive experiments demonstrate the superior performance of InfoMax in various data pruning tasks, including image classification, vision-language pre-training, and instruction tuning for large language models. Code is available at https://github.com/hrtan/InfoMax.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†InfoMaxï¼Œä¸€ç§æ–°å‹çš„æ•°æ®å‰ªæ(Data Pruning)æˆ–æ ¸å¿ƒé›†é€‰æ‹©(Coreset Selection)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æœ€å¤§åŒ–é€‰å®šæ ·æœ¬çš„ä¿¡æ¯é‡å¹¶æœ€å°åŒ–å†—ä½™æ¥æå‡æ ¸å¿ƒé›†çš„æ•´ä½“ä¿¡æ¯å«é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡è¦æ€§åˆ†æ•°(Importance Scores)è¡¡é‡å•ä¸ªæ ·æœ¬å¯¹æ¨¡å‹å­¦ä¹ çš„å½±å“åŠ›ï¼Œå¹¶åˆ©ç”¨æˆå¯¹æ ·æœ¬ç›¸ä¼¼æ€§(Pairwise Sample Similarities)æ¥é‡åŒ–å†—ä½™ï¼Œå°†æ ¸å¿ƒé›†é€‰æ‹©å»ºæ¨¡ä¸ºä¸€é¡¹ç¦»æ•£äºŒæ¬¡è§„åˆ’(Discrete Quadratic Programming, DQP)ä»»åŠ¡ã€‚ä¸ºäº†å®ç°å¯¹å¤§è§„æ¨¡æ•°æ®å¤„ç†çš„æ‰©å±•æ€§ï¼ŒInfoMaxå¼•å…¥äº†é«˜æ•ˆçš„åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨ï¼Œå¹¶ç»“åˆçŸ©é˜µç¨€ç–åŒ–ä¸æ•°æ®é›†åˆ’åˆ†ç­–ç•¥ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ•°ç™¾ä¸‡çº§åˆ«çš„æ ·æœ¬ã€‚å®éªŒè¯æ˜ï¼ŒInfoMaxåœ¨å›¾åƒåˆ†ç±»ã€è§†è§‰-è¯­è¨€é¢„è®­ç»ƒ(Vision-Language Pre-training)ä»¥åŠå¤§è¯­è¨€æ¨¡å‹(Large Language Models)çš„æŒ‡ä»¤å¾®è°ƒ(Instruction Tuning)ç­‰å¤šç§æ•°æ®å‰ªæä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä¸ºå¤§è§„æ¨¡æœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®æ•ˆç‡ä¼˜åŒ–æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at \\url{https://github.com/hrtan/InfoMax}",
      "pdf_url": "https://arxiv.org/pdf/2506.01701v2",
      "published_date": "2025-06-02 14:06:42 UTC",
      "updated_date": "2025-08-14 03:59:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:35:11.154756+00:00"
    },
    {
      "arxiv_id": "2506.01698v1",
      "title": "When LLMs Team Up: The Emergence of Collaborative Affective Computing",
      "title_zh": "å½“å¤§è¯­è¨€æ¨¡å‹æºæ‰‹ï¼šåä½œæƒ…æ„Ÿè®¡ç®—çš„å…´èµ·",
      "authors": [
        "Wenna Lai",
        "Haoran Xie",
        "Guandong Xu",
        "Qing Li",
        "S. Joe Qin"
      ],
      "abstract": "Affective Computing (AC) is essential in bridging the gap between human emotional experiences and machine understanding. Traditionally, AC tasks in natural language processing (NLP) have been approached through pipeline architectures, which often suffer from structure rigidity that leads to inefficiencies and limited adaptability. The advent of Large Language Models (LLMs) has revolutionized this field by offering a unified approach to affective understanding and generation tasks, enhancing the potential for dynamic, real-time interactions. However, LLMs face cognitive limitations in affective reasoning, such as misinterpreting cultural nuances or contextual emotions, and hallucination problems in decision-making. To address these challenges, recent research advocates for LLM-based collaboration systems that emphasize interactions among specialized models and LLMs, mimicking human-like affective intelligence through the synergy of emotional and rational thinking that aligns with Dual Process Theory in psychology. This survey aims to provide a comprehensive overview of LLM-based collaboration systems in AC, exploring from structured collaborations to autonomous collaborations. Specifically, it includes: (1) A systematic review of existing methods, focusing on collaboration strategies, mechanisms, key functions, and applications; (2) Experimental comparisons of collaboration strategies across representative tasks in affective understanding and generation; (3) An analysis highlighting the potential of these systems to enhance robustness and adaptability in complex affective reasoning; (4) A discussion of key challenges and future research directions to further advance the field. This work is the first to systematically explore collaborative intelligence with LLMs in AC, paving the way for more powerful applications that approach human-like social intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æƒ…æ„Ÿè®¡ç®—(Affective Computing)é¢†åŸŸä¸­å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç®¡é“æ¶æ„çš„åƒµåŒ–æ€§ä»¥åŠå•ä¸ªæ¨¡å‹åœ¨æƒ…æ„Ÿæ¨ç†ä¸­çš„è®¤çŸ¥å±€é™å’Œå¹»è§‰é—®é¢˜ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†åŸºäºLLMsçš„åä½œç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ¨¡æ‹Ÿå¿ƒç†å­¦ä¸­çš„åŒé‡è¿‡ç¨‹ç†è®º(Dual Process Theory)ï¼Œåˆ©ç”¨æ„Ÿæ€§ä¸ç†æ€§æ€ç»´çš„ååŒæ¥æå‡æ™ºèƒ½æ°´å¹³ã€‚ä½œä¸ºé¦–ç¯‡ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œè®ºæ–‡å…¨é¢å›é¡¾äº†ä»ç»“æ„åŒ–åä½œåˆ°è‡ªä¸»åä½œçš„ç°æœ‰æ–¹æ³•ï¼Œå¹¶è¯¦ç»†åˆ†æäº†åä½œç­–ç•¥ã€æœºåˆ¶åŠå…¶åœ¨æƒ…æ„Ÿç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¸­çš„å…³é”®åŠŸèƒ½ã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§ä»»åŠ¡çš„å®éªŒå¯¹æ¯”ï¼Œç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡å‹åä½œç³»ç»Ÿåœ¨å¤æ‚æƒ…æ„Ÿæ¨ç†ä¸­å¢å¼ºé²æ£’æ€§å’Œé€‚åº”æ€§çš„æ½œåŠ›ã€‚æœ€åï¼Œæ–‡ç« è®¨è®ºäº†è¯¥é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ï¼Œä¸ºå¼€å‘å…·å¤‡ç±»äººç¤¾äº¤æ™ºèƒ½çš„æƒ…æ„Ÿè®¡ç®—åº”ç”¨å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 7 figures, and 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.01698v1",
      "published_date": "2025-06-02 14:00:54 UTC",
      "updated_date": "2025-06-02 14:00:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:35:17.972051+00:00"
    },
    {
      "arxiv_id": "2506.02089v4",
      "title": "SALAD: Systematic Assessment of Machine Unlearning on LLM-Aided Hardware Design",
      "title_zh": "SALADï¼šé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©ç¡¬ä»¶è®¾è®¡çš„æœºå™¨é—å¿˜ç³»ç»Ÿæ€§è¯„ä¼°",
      "authors": [
        "Zeng Wang",
        "Minghao Shao",
        "Rupesh Karn",
        "Likhitha Mankali",
        "Jitendra Bhandari",
        "Ramesh Karri",
        "Ozgur Sinanoglu",
        "Muhammad Shafique",
        "Johann Knechtel"
      ],
      "abstract": "Large Language Models (LLMs) offer transformative capabilities for hardware design automation, particularly in Verilog code generation. However, they also pose significant data security challenges, including Verilog evaluation data contamination, intellectual property (IP) design leakage, and the risk of malicious Verilog generation. We introduce SALAD, a comprehensive assessment that leverages machine unlearning to mitigate these threats. Our approach enables the selective removal of contaminated benchmarks, sensitive IP and design artifacts, or malicious code patterns from pre-trained LLMs, all without requiring full retraining. Through detailed case studies, we demonstrate how machine unlearning techniques effectively reduce data security risks in LLM-aided hardware design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SALADï¼Œä¸€ä¸ªåˆ©ç”¨æœºå™¨é—å¿˜ (machine unlearning) æŠ€æœ¯æ¥åº”å¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è¾…åŠ©ç¡¬ä»¶è®¾è®¡ï¼ˆå¦‚ Verilog ä»£ç ç”Ÿæˆï¼‰ä¸­é¢ä¸´çš„æ•°æ®å®‰å…¨æŒ‘æˆ˜çš„ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ã€‚é’ˆå¯¹æ•°æ®æ±¡æŸ“ã€çŸ¥è¯†äº§æƒ (IP) æ³„éœ²ä»¥åŠæ¶æ„ä»£ç ç”Ÿæˆç­‰å¨èƒï¼ŒSALAD å…è®¸åœ¨æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹çš„å‰æä¸‹ï¼Œä»é¢„è®­ç»ƒæ¨¡å‹ä¸­é€‰æ‹©æ€§åœ°ç§»é™¤å—æ±¡æŸ“çš„åŸºå‡†æµ‹è¯•ã€æ•æ„Ÿçš„è®¾è®¡å·¥ä»¶æˆ–æ¶æ„ä»£ç æ¨¡å¼ã€‚é€šè¿‡æ·±å…¥çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥é¡¹å·¥ä½œå±•ç¤ºäº†æœºå™¨é—å¿˜æŠ€æœ¯åœ¨é™ä½ LLM è¾…åŠ©ç¡¬ä»¶è®¾è®¡å®‰å…¨é£é™©æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´å—æ§çš„ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–å·¥å…·æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02089v4",
      "published_date": "2025-06-02 13:59:08 UTC",
      "updated_date": "2025-10-06 10:38:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:35:23.247677+00:00"
    },
    {
      "arxiv_id": "2506.01692v1",
      "title": "A Descriptive and Normative Theory of Human Beliefs in RLHF",
      "title_zh": "RLHF ä¸­äººç±»ä¿¡å¿µçš„æè¿°æ€§ä¸è§„èŒƒæ€§ç†è®º",
      "authors": [
        "Sylee Dandekar",
        "Shripad Deshmukh",
        "Frank Chiu",
        "W. Bradley Knox",
        "Scott Niekum"
      ],
      "abstract": "Human preferences in RLHF are typically modeled as a function of the human's reward function or corresponding optimal state-action values. In this work, we propose that human beliefs about the capabilities of the agent being trained also play a key role in preference generation. We examine two questions related to this hypothesis, one descriptive and one normative, respectively: Do human labelers' beliefs about agent capabilities affect the preferences that they provide? And what is the ideal set of beliefs about an agent -- and resulting preferences -- for humans to have? We propose a new preference model that incorporates human beliefs and provide a normative theory that bounds the error on the final learned policy based on the \\textit{mismatch} between the human's beliefs and an idealized set of beliefs. We then confirm via a human study that beliefs about agent capabilities do, in fact, significantly affect preferences and can be influenced through simple interventions. Additionally, we empirically show through synthetic experiments that it is often suboptimal for human preference labelers to assume agent optimality. Collectively, these results theoretically and empirically demonstrate how reducing the mismatch between human beliefs and agent capabilities can lead to more performant RLHF and point toward new best practices for RLHF practitioners.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»å¯¹äºæ™ºèƒ½ä½“èƒ½åŠ›çš„ä¿¡å¿µ (beliefs) åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (RLHF) åå¥½ç”Ÿæˆä¸­çš„å…³é”®ä½œç”¨ï¼Œæ—¨åœ¨è¶…è¶Šä¼ ç»Ÿçš„å¥–åŠ±å‡½æ•°æˆ–æœ€ä¼˜çŠ¶æ€-åŠ¨ä½œä»·å€¼æ¨¡å‹ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªæè¿°æ€§å’Œè§„èŒƒæ€§çš„ç†è®ºæ¡†æ¶ï¼Œé‡ç‚¹ç ”ç©¶äººç±»æ ‡æ³¨è€…çš„ä¿¡å¿µæ˜¯å¦ä¼šå½±å“å…¶æä¾›çš„åå¥½ï¼Œä»¥åŠæ ‡æ³¨è€…åº”å½“æŒæœ‰ä½•ç§ç†æƒ³ä¿¡å¿µä»¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆäººç±»ä¿¡å¿µçš„æ–°å‹åå¥½æ¨¡å‹ï¼Œå¹¶å»ºç«‹äº†ä¸€å¥—è§„èŒƒæ€§ç†è®ºï¼Œé€šè¿‡è¡¡é‡äººç±»ä¿¡å¿µä¸ç†æƒ³ä¿¡å¿µä¹‹é—´çš„åå·® (mismatch) æ¥ç•Œå®šæœ€ç»ˆä¹ å¾—ç­–ç•¥çš„è¯¯å·®è¾¹ç•Œã€‚é€šè¿‡äººç±»å—è¯•è€…ç ”ç©¶ï¼Œæœ¬æ–‡è¯å®äº†æ ‡æ³¨è€…å¯¹æ™ºèƒ½ä½“èƒ½åŠ›çš„ä¿¡å¿µç¡®å®ä¼šæ˜¾è‘—å½±å“åå¥½ï¼Œä¸”å¯ä»¥é€šè¿‡ç®€å•çš„å¹²é¢„æ‰‹æ®µè¿›è¡Œå¼•å¯¼ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå‡è®¾æ™ºèƒ½ä½“å¤„äºæœ€ä¼˜çŠ¶æ€ (optimality) å¯¹äººç±»æ ‡æ³¨è€…è€Œè¨€å¾€å¾€æ˜¯æ¬¡ä¼˜çš„ï¼Œå‡å°‘ä¿¡å¿µä¸æ™ºèƒ½ä½“å®é™…èƒ½åŠ›ä¹‹é—´çš„é”™é…æ˜¯æå‡ RLHF æ€§èƒ½çš„å…³é”®ã€‚è¯¥ç ”ç©¶åœ¨ç†è®ºå’Œå®è¯å±‚é¢è¯æ˜äº†ä¼˜åŒ–äººç±»ä¿¡å¿µçš„é‡è¦æ€§ï¼Œå¹¶ä¸º RLHF çš„å®è·µæä¾›äº†å‡å°‘åå·®ã€æé«˜æ¨¡å‹è¡¨ç°çš„æ–°å‹æœ€ä½³å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01692v1",
      "published_date": "2025-06-02 13:52:55 UTC",
      "updated_date": "2025-06-02 13:52:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:35:46.326091+00:00"
    },
    {
      "arxiv_id": "2506.01689v1",
      "title": "Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents",
      "title_zh": "è¶…è¶Šè¯­è¨€çš„å“åº”ï¼šé’ˆå¯¹çœŸå®ç”¨æˆ·æ„å›¾çš„è§†é¢‘ç”Ÿæˆè¯„æµ‹åŸºå‡†",
      "authors": [
        "Shuting Wang",
        "Yunqi Liu",
        "Zixin Yang",
        "Ning Hu",
        "Zhicheng Dou",
        "Chenyan Xiong"
      ],
      "abstract": "Querying generative AI models, e.g., large language models (LLMs), has become a prevalent method for information acquisition. However, existing query-answer datasets primarily focus on textual responses, making it challenging to address complex user queries that require visual demonstrations or explanations for better understanding. To bridge this gap, we construct a benchmark, RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V) models in answering real-world, visually grounded queries. It identifies 7.5K real user queries with video response intents from Chatbot-Arena and builds 4.5K high-quality query-video pairs through a multistage video retrieval and refinement process. We further develop a multi-angle evaluation system to assess the quality of generated video answers. Experiments indicate that current T2V models struggle with effectively addressing real user queries, pointing to key challenges and future research opportunities in multimodal AI.",
      "tldr_zh": "éšç€å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ™®åŠï¼Œç°æœ‰çš„æŸ¥è¯¢-å›ç­”æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬å›å¤ï¼Œéš¾ä»¥å¤„ç†éœ€è¦è§†è§‰æ¼”ç¤ºçš„å¤æ‚ç”¨æˆ·æŸ¥è¯¢ã€‚è¯¥ç ”ç©¶æ„å»ºäº†RealVideoQueståŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘(T2V)æ¨¡å‹åœ¨å›ç­”çœŸå®ä¸–ç•Œè§†è§‰å¯¼å‘æŸ¥è¯¢æ—¶çš„èƒ½åŠ›ã€‚ç ”ç©¶è€…ä»Chatbot-Arenaä¸­è¯†åˆ«å‡º7.5Kä¸ªå…·æœ‰è§†é¢‘å›å¤æ„å›¾çš„çœŸå®ç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡å¤šé˜¶æ®µæ£€ç´¢ä¸ç»†åŒ–è¿‡ç¨‹æ„å»ºäº†4.5Kä¸ªé«˜è´¨é‡çš„æŸ¥è¯¢-è§†é¢‘å¯¹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼€å‘äº†ä¸€å¥—å¤šè§’åº¦è¯„ä¼°ç³»ç»Ÿæ¥è¡¡é‡ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„T2Væ¨¡å‹åœ¨æœ‰æ•ˆè§£å†³çœŸå®ç”¨æˆ·æŸ¥è¯¢æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å¤šæ¨¡æ€AIé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01689v1",
      "published_date": "2025-06-02 13:52:21 UTC",
      "updated_date": "2025-06-02 13:52:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:35:46.991663+00:00"
    },
    {
      "arxiv_id": "2506.01683v1",
      "title": "Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection Using Speech and Large Language Models",
      "title_zh": "åŸºäºé“¾å¼æ€ç»´æ¨ç†çš„è¯­éŸ³ä¸å¤§è¯­è¨€æ¨¡å‹é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹æ–¹æ³•",
      "authors": [
        "Chanwoo Park",
        "Anna Seo Gyeong Choi",
        "Sunghye Cho",
        "Chanwoo Kim"
      ],
      "abstract": "Societies worldwide are rapidly entering a super-aged era, making elderly health a pressing concern. The aging population is increasing the burden on national economies and households. Dementia cases are rising significantly with this demographic shift. Recent research using voice-based models and large language models (LLM) offers new possibilities for dementia diagnosis and treatment. Our Chain-of-Thought (CoT) reasoning method combines speech and language models. The process starts with automatic speech recognition to convert speech to text. We add a linear layer to an LLM for Alzheimer's disease (AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT reasoning and cues. This approach showed an 16.7% relative performance improvement compared to methods without CoT prompt reasoning. To the best of our knowledge, our proposed method achieved state-of-the-art performance in CoT approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒè€é¾„åŒ–èƒŒæ™¯ä¸‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆAlzheimer's disease, ADï¼‰æ£€æµ‹çš„ç´§è¿«éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè¯­éŸ³ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“¾å¼æ€ç»´ï¼ˆChain-of-Thought, CoTï¼‰æ¨ç†æ–¹æ³•ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆAutomatic Speech Recognition, ASRï¼‰æŠ€æœ¯å°†è¯­éŸ³è½¬åŒ–ä¸ºæ–‡æœ¬ï¼Œå¹¶åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­é›†æˆçº¿æ€§å±‚ï¼Œé‡‡ç”¨å¸¦æœ‰CoTæ¨ç†å¼•å¯¼çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-tuning, SFTï¼‰è¿›è¡ŒADåˆ†ç±»è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºä¸ä½¿ç”¨CoTæ¨ç†çš„æ–¹æ¡ˆå®ç°äº†16.7%çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶åœ¨åŸºäºCoTæ¨ç†çš„æ£€æµ‹æ–¹æ³•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ï¼ˆState-of-the-artï¼‰æ€§èƒ½æ°´å¹³ï¼Œä¸ºåˆ©ç”¨è¯­éŸ³å’ŒAIæŠ€æœ¯è¿›è¡Œå¤±æ™ºç—‡çš„æ—©æœŸè¯Šæ–­æä¾›äº†é«˜æ•ˆçš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to INTERSPEECH 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01683v1",
      "published_date": "2025-06-02 13:49:48 UTC",
      "updated_date": "2025-06-02 13:49:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:36:51.558170+00:00"
    },
    {
      "arxiv_id": "2506.01678v1",
      "title": "Overcoming Data Scarcity in Scanning Tunnelling Microscopy Image Segmentation",
      "title_zh": "å…‹æœæ‰«æéš§é“æ˜¾å¾®é•œå›¾åƒåˆ†å‰²ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜",
      "authors": [
        "Nikola L. Kolev",
        "Max Trouton",
        "Filippo Federici Canova",
        "Geoff Thornton",
        "David Z. Gao",
        "Neil J. Curson",
        "Taylor J. Z. Stock"
      ],
      "abstract": "Scanning tunnelling microscopy (STM) is a powerful technique for imaging surfaces with atomic resolution, providing insight into physical and chemical processes at the level of single atoms and molecules. A regular task of STM image analysis is the identification and labelling of features of interest against a uniform background. Performing this manually is a labour-intensive task, requiring significant human effort. To reduce this burden, we propose an automated approach to the segmentation of STM images that uses both few-shot learning and unsupervised learning. Our technique offers greater flexibility compared to previous supervised methods; it removes the requirement for large manually annotated datasets and is thus easier to adapt to an unseen surface while still maintaining a high accuracy. We demonstrate the effectiveness of our approach by using it to recognise atomic features on three distinct surfaces: Si(001), Ge(001), and TiO$_2$(110), including adsorbed AsH$_3$ molecules on the silicon and germanium surfaces. Our model exhibits strong generalisation capabilities, and following initial training, can be adapted to unseen surfaces with as few as one additional labelled data point. This work is a significant step towards efficient and material-agnostic, automatic segmentation of STM images.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰«æéš§é“æ˜¾å¾®é•œ (Scanning Tunnelling Microscopy, STM) å›¾åƒåˆ†æä¸­æ‰‹åŠ¨åˆ†å‰²æ ‡æ³¨æå…¶è€—æ—¶ä¸”æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå°‘é‡æ ·æœ¬å­¦ä¹  (few-shot learning) ä¸æ— ç›‘ç£å­¦ä¹  (unsupervised learning) çš„è‡ªåŠ¨åŒ–åˆ†å‰²æ–¹æ³•ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ ï¼Œè¯¥æŠ€æœ¯æ¶ˆé™¤äº†å¯¹å¤§è§„æ¨¡æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ï¼Œå…·æœ‰æ›´é«˜çš„çµæ´»æ€§å’Œå‡†ç¡®æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨ Si(001)ã€Ge(001) å’Œ TiO$_2$(110) ä¸‰ç§æˆªç„¶ä¸åŒçš„è¡¨é¢ï¼ˆåŒ…æ‹¬å¸é™„çš„ AsH$_3$ åˆ†å­ï¼‰ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹å…·å¤‡å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨åˆå§‹è®­ç»ƒå®Œæˆåï¼Œä»…éœ€ä¸€ä¸ªé¢å¤–çš„æ ‡æ³¨æ•°æ®ç‚¹å³å¯å¿«é€Ÿé€‚é…è‡³æœªè§è¿‡çš„è¡¨é¢ã€‚è¿™é¡¹å·¥ä½œä¸ºå®ç°é«˜æ•ˆã€ææ–™æ— å…³ (material-agnostic) çš„ STM å›¾åƒè‡ªåŠ¨åˆ†å‰²å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01678v1",
      "published_date": "2025-06-02 13:47:37 UTC",
      "updated_date": "2025-06-02 13:47:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:35:59.771009+00:00"
    },
    {
      "arxiv_id": "2506.01676v1",
      "title": "K12Vista: Exploring the Boundaries of MLLMs in K-12 Education",
      "title_zh": "K12Vistaï¼šæ¢ç´¢å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ K-12 æ•™è‚²ä¸­çš„è¾¹ç•Œ",
      "authors": [
        "Chong Li",
        "Chenglin Zhu",
        "Tao Zhang",
        "Mingan Lin",
        "Zenan Zhou",
        "Jian Xie"
      ],
      "abstract": "Multimodal large language models have demonstrated remarkable reasoning capabilities in various visual tasks. However, their abilities in K12 scenarios are still systematically underexplored. Previous studies suffer from various limitations including narrow subject coverage, insufficient data scale, lack of diversity in question types, and naive answer-centric evaluation method, resulting in insufficient exploration of model capabilities. To address these gaps, we propose K12Vista, the most comprehensive multimodal benchmark for Chinese K12 subject knowledge understanding and reasoning to date, featuring 33,000 questions across five core subjects from primary to high school and three question types. Moreover, beyond the final outcome, we are also concerned with the correctness of MLLMs' reasoning processes. For this purpose, we meticulously compiles errors from MLLMs' reasoning processes and leverage an automated data pipeline to construct K12-PEM-800K, the largest process evaluation dataset offering detailed step-by-step judgement annotations for MLLMs' reasoning. Subsequently, we developed K12-PEM, an advanced process evaluation model that integrates an overall assessment of both the reasoning process and answer correctness. Moreover, we also introduce K12-PEBench, the first high-quality, human-annotated benchmark specifically designed for evaluating abilities of reasoning process evaluation.Extensive experiments reveal that current MLLMs exhibit significant flaws when reasoning within K12Vista, providing critical insights for the development of more capable MLLMs.We open our resources at https://github.com/lichongod/K12Vista.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨K12æ•™è‚²åœºæ™¯ä¸­ç ”ç©¶ä¸è¶³ã€è¯„ä¼°æ‰‹æ®µå•ä¸€åŠæ•°æ®è§„æ¨¡å—é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†K12Vistaã€‚ä½œä¸ºç›®å‰æœ€å…¨é¢çš„ä¸­æ–‡K12å­¦ç§‘çŸ¥è¯†ç†è§£ä¸æ¨ç†å¤šæ¨¡æ€åŸºå‡†ï¼ŒK12VistaåŒ…å«äº†æ¶µç›–äº”é—¨æ ¸å¿ƒå­¦ç§‘çš„33,000ä¸ªé—®é¢˜ã€‚ä¸ºäº†æ·±å…¥è¯„ä¼°æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è‡ªåŠ¨åŒ–æµæ°´çº¿æ„å»ºäº†K12-PEM-800Kï¼Œè¿™æ˜¯ç›®å‰è§„æ¨¡æœ€å¤§çš„æä¾›è¯¦ç»†åˆ†æ­¥åˆ¤æ–­æ ‡æ³¨çš„è¿‡ç¨‹è¯„ä¼°æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†é›†æˆæ¨ç†è¿‡ç¨‹ä¸ç»“æœè¯„ä¼°çš„K12-PEMæ¨¡å‹ï¼Œå¹¶æ¨å‡ºäº†é¦–ä¸ªé«˜è´¨é‡äººå·¥æ ‡æ³¨çš„æ¨ç†è¿‡ç¨‹è¯„ä¼°åŸºå‡†K12-PEBenchã€‚å®éªŒç»“æœæ­ç¤ºäº†ç°æœ‰MLLMsåœ¨å¤„ç†K12å­¦ç§‘é—®é¢˜æ—¶å­˜åœ¨æ˜¾è‘—çš„æ¨ç†ç¼ºé™·ï¼Œä¸ºæœªæ¥æ•™è‚²é¢†åŸŸä¸“ç”¨æ¨¡å‹çš„è®¾è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01676v1",
      "published_date": "2025-06-02 13:46:38 UTC",
      "updated_date": "2025-06-02 13:46:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:37:13.464843+00:00"
    },
    {
      "arxiv_id": "2506.14799v2",
      "title": "Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust",
      "title_zh": "åŸºäºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„åª’ä½“å†…å®¹è§’è‰²å‘ˆç°åˆ†æï¼šæœ‰æ•ˆæ€§ä¸ä¿¡ä»»åº¦ç ”ç©¶",
      "authors": [
        "Evdoxia Taka",
        "Debadyuti Bhattacharya",
        "Joanne Garde-Hansen",
        "Sanjay Sharma",
        "Tanaya Guha"
      ],
      "abstract": "Recent advances in AI has made automated analysis of complex media content at scale possible while generating actionable insights regarding character representation along such dimensions as gender and age. Past works focused on quantifying representation from audio/video/text using AI models, but without having the audience in the loop. We ask, even if character distribution along demographic dimensions are available, how useful are those to the general public? Do they actually trust the numbers generated by AI models? Our work addresses these open questions by proposing a new AI-based character representation tool and performing a thorough user study. Our tool has two components: (i) An analytics extraction model based on the Contrastive Language Image Pretraining (CLIP) foundation model that analyzes visual screen data to quantify character representation across age and gender; (ii) A visualization component effectively designed for presenting the analytics to lay audience. The user study seeks empirical evidence on the usefulness and trustworthiness of the AI-generated results for carefully chosen movies presented in the form of our visualizations. We found that participants were able to understand the analytics in our visualizations, and deemed the tool `overall useful'. Participants also indicated a need for more detailed visualizations to include more demographic categories and contextual information of the characters. Participants' trust in AI-based gender and age models is seen to be moderate to low, although they were not against the use of AI in this context. Our tool including code, benchmarking, and the user study data can be found at https://github.com/debadyuti0510/Character-Representation-Media.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨äººå·¥æ™ºèƒ½è‡ªåŠ¨åŒ–åˆ†æåª’ä½“å†…å®¹ä¸­è§’è‰²ä»£è¡¨æ€§ï¼ˆå¦‚æ€§åˆ«å’Œå¹´é¾„ï¼‰çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»“åˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆContrastive Language Image Pretraining, CLIPï¼‰åŸºç¡€æ¨¡å‹ä¸å¯è§†åŒ–ç»„ä»¶çš„æ–°å‹å·¥å…·ã€‚è¯¥å·¥å…·åˆ©ç”¨ CLIP æ¨¡å‹é‡åŒ–è§†è§‰æ•°æ®ä¸­çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡ä¸“é—¨è®¾è®¡çš„å¯è§†åŒ–ç•Œé¢å‘æ™®é€šå—ä¼—å±•ç¤ºåˆ†æç»“æœã€‚ç ”ç©¶äººå‘˜è¿›è¡Œäº†ä¸€é¡¹æ·±å…¥çš„ç”¨æˆ·ç ”ç©¶ï¼Œä»¥è¯„ä¼°è¿™äº› AI ç”Ÿæˆç»“æœåœ¨å®é™…ç”µå½±åœºæ™¯ä¸‹çš„å®ç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå—è®¿è€…èƒ½å¤Ÿç†è§£å¹¶è®¤å¯è¯¥å·¥å…·çš„æ•´ä½“å®ç”¨æ€§ï¼Œä½†ä¹Ÿå¯¹äººå£ç»Ÿè®¡ç±»åˆ«çš„å¤šæ ·æ€§å’Œè§’è‰²èƒŒæ™¯ä¿¡æ¯çš„è¯¦ç»†ç¨‹åº¦æå‡ºäº†æ›´é«˜è¦æ±‚ã€‚å°½ç®¡å—è®¿è€…ä¸åå¯¹åœ¨è¯¥é¢†åŸŸä½¿ç”¨äººå·¥æ™ºèƒ½ï¼Œä½†å¯¹åŸºäº AI çš„æ€§åˆ«å’Œå¹´é¾„è¯†åˆ«æ¨¡å‹çš„å¯ä¿¡åº¦è¯„ä»·å¤„äºä¸­ç­‰è‡³åä½æ°´å¹³ã€‚è¯¥å·¥ä½œé€šè¿‡å®è¯ç ”ç©¶æ­ç¤ºäº†å—ä¼—å¯¹ AI é©±åŠ¨åª’ä½“åˆ†æçš„è®¤çŸ¥æ€åº¦ï¼Œå¹¶å…¬å¼€äº†åŒ…å«ä»£ç ã€åŸºå‡†æµ‹è¯•å’Œç”¨æˆ·ç ”ç©¶æ•°æ®çš„å®Œæ•´èµ„æºã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14799v2",
      "published_date": "2025-06-02 13:46:28 UTC",
      "updated_date": "2025-08-26 18:50:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:36:59.637664+00:00"
    },
    {
      "arxiv_id": "2506.01673v1",
      "title": "GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion",
      "title_zh": "GRAMï¼šåŸºäºè¯­ä¹‰æ„ŸçŸ¥å¤šç²’åº¦åæœŸèåˆçš„ç”Ÿæˆå¼æ¨è",
      "authors": [
        "Sunkyung Lee",
        "Minjin Choi",
        "Eunseong Choi",
        "Hye-young Kim",
        "Jongwuk Lee"
      ],
      "abstract": "Generative recommendation is an emerging paradigm that leverages the extensive knowledge of large language models by formulating recommendations into a text-to-text generation task. However, existing studies face two key limitations in (i) incorporating implicit item relationships and (ii) utilizing rich yet lengthy item information. To address these challenges, we propose a Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM), introducing two synergistic innovations. First, we design semantic-to-lexical translation to encode implicit hierarchical and collaborative item relationships into the vocabulary space of LLMs. Second, we present multi-granular late fusion to integrate rich semantics efficiently with minimal information loss. It employs separate encoders for multi-granular prompts, delaying the fusion until the decoding stage. Experiments on four benchmark datasets show that GRAM outperforms eight state-of-the-art generative recommendation models, achieving significant improvements of 11.5-16.0% in Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at https://github.com/skleee/GRAM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GRAMï¼Œä¸€ç§é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥å¤šç²’åº¦æ™šæœŸèåˆå®ç°çš„æ–°å‹ç”Ÿæˆå¼æ¨èæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨æ•´åˆéšæ€§ç‰©å“å…³ç³»åŠå¤„ç†é•¿ç¯‡ç‰©å“æè¿°ä¿¡æ¯æ—¶çš„å±€é™ï¼ŒGRAMå¼•å…¥äº†ä¸¤é¡¹ååŒåˆ›æ–°æŠ€æœ¯ã€‚é¦–å…ˆï¼Œé€šè¿‡è®¾è®¡è¯­ä¹‰åˆ°è¯æ±‡è½¬æ¢ï¼ˆSemantic-to-lexical translationï¼‰ï¼Œè¯¥æ¡†æ¶å°†éšæ€§çš„å±‚æ¬¡åŒ–å’ŒååŒç‰©å“å…³ç³»ç¼–ç è¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯æ±‡ç©ºé—´ä¸­ã€‚å…¶æ¬¡ï¼ŒGRAMé‡‡ç”¨äº†å¤šç²’åº¦æ™šæœŸèåˆï¼ˆMulti-granular late fusionï¼‰ç­–ç•¥ï¼Œé€šè¿‡ä¸ºå¤šç²’åº¦æç¤ºè®¾ç½®ç‹¬ç«‹ç¼–ç å™¨å¹¶å°†èåˆæ¨è¿Ÿè‡³è§£ç é˜¶æ®µï¼Œä»¥æå°çš„ä¿¡æ¯æŸå¤±é«˜æ•ˆæ•´åˆä¸°å¯Œè¯­ä¹‰ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGRAMçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå…«ç§æœ€å…ˆè¿›çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹ï¼Œåœ¨Recall@5å’ŒNDCG@5æŒ‡æ ‡ä¸Šåˆ†åˆ«å®ç°äº†11.5-16.0%å’Œ5.3-13.6%çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "ACL 2025 (Main Conference)",
      "pdf_url": "https://arxiv.org/pdf/2506.01673v1",
      "published_date": "2025-06-02 13:42:46 UTC",
      "updated_date": "2025-06-02 13:42:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:37:36.861397+00:00"
    },
    {
      "arxiv_id": "2506.01666v1",
      "title": "Synthesis of discrete-continuous quantum circuits with multimodal diffusion models",
      "title_zh": "åŸºäºå¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹çš„ç¦»æ•£-è¿ç»­é‡å­ç”µè·¯åˆæˆ",
      "authors": [
        "Florian FÃ¼rrutter",
        "Zohim Chandani",
        "Ikko Hamamura",
        "Hans J. Briegel",
        "Gorka MuÃ±oz-Gil"
      ],
      "abstract": "Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å»å™ªæ‰©æ•£æ¨¡å‹ (multimodal denoising diffusion model)ï¼Œæ—¨åœ¨è§£å†³é‡å­æ“ä½œç¼–è¯‘ä¸­è®¡ç®—æˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•çš„ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤ä¸ªç‹¬ç«‹çš„æ‰©æ•£è¿‡ç¨‹ (diffusion processes) åŒæ—¶ç”Ÿæˆé‡å­ç”µè·¯çš„ç¦»æ•£ç»“æ„åŠå…¶è¿ç»­å‚æ•°ï¼Œå…‹æœäº†ç°æœ‰æœºå™¨å­¦ä¹ æ–¹æ³•ä»…é™äºç¦»æ•£é—¨é›† (discrete gate sets) çš„å±€é™æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨å¤šç§é‡å­æ¯”ç‰¹æ•°ã€ç”µè·¯æ·±åº¦å’Œå‚æ•°åŒ–é—¨æ¯”ä¾‹çš„å®éªŒä¸­å¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†å…¶åœ¨ç¼–è¯‘ç›®æ ‡å¹ºæ­£å˜æ¢ (target unitary) æ—¶çš„å‡†ç¡®æ€§ã€‚å¾—ç›Šäºå…¶å¿«é€Ÿçš„ç”µè·¯ç”Ÿæˆèƒ½åŠ›ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ„å»ºå¤§è§„æ¨¡ç”µè·¯æ•°æ®é›†å¹¶ä»ä¸­æå–æœ‰ä»·å€¼çš„å¯å‘å¼ä¿¡æ¯ (heuristics)ã€‚è¿™ä¸€æˆæœä¸ºé‡å­ç”µè·¯åˆæˆ (quantum circuit synthesis) é¢†åŸŸæä¾›äº†æ–°çš„è‡ªåŠ¨åŒ–æ‰‹æ®µï¼Œå¹¶ä¸ºå‘ç°æ–°çš„ç ”ç©¶æ´è§å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Main Text: 10 pages and 5 figures; Appendix: 17 pages, 7 figures and 1 table. Code available at: https://github.com/FlorianFuerrutter/genQC",
      "pdf_url": "https://arxiv.org/pdf/2506.01666v1",
      "published_date": "2025-06-02 13:35:33 UTC",
      "updated_date": "2025-06-02 13:35:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:36:17.809976+00:00"
    },
    {
      "arxiv_id": "2506.01665v3",
      "title": "Leveraging Analytic Gradients in Provably Safe Reinforcement Learning",
      "title_zh": "åœ¨å¯è¯æ˜å®‰å…¨å¼ºåŒ–å­¦ä¹ ä¸­åˆ©ç”¨è§£ææ¢¯åº¦",
      "authors": [
        "Tim Walter",
        "Hannah Markgraf",
        "Jonathan KÃ¼lz",
        "Matthias Althoff"
      ],
      "abstract": "The deployment of autonomous robots in safety-critical applications requires safety guarantees. Provably safe reinforcement learning is an active field of research that aims to provide such guarantees using safeguards. These safeguards should be integrated during training to reduce the sim-to-real gap. While there are several approaches for safeguarding sampling-based reinforcement learning, analytic gradient-based reinforcement learning often achieves superior performance from fewer environment interactions. However, there is no safeguarding approach for this learning paradigm yet. Our work addresses this gap by developing the first effective safeguard for analytic gradient-based reinforcement learning. We analyse existing, differentiable safeguards, adapt them through modified mappings and gradient formulations, and integrate them into a state-of-the-art learning algorithm and a differentiable simulation. Using numerical experiments on three control tasks, we evaluate how different safeguards affect learning. The results demonstrate safeguarded training without compromising performance. Additional visuals are provided at \\href{https://timwalter.github.io/safe-agb-rl.github.io}{timwalter.github.io/safe-agb-rl.github.io}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„è‡ªä¸»æœºå™¨äººéƒ¨ç½²ï¼Œé‡ç‚¹æ¢è®¨äº†å¯è¯æ˜å®‰å…¨çš„å¼ºåŒ–å­¦ä¹  (Provably Safe Reinforcement Learning)ã€‚è™½ç„¶åŸºäºè§£ææ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹  (Analytic Gradient-based RL) ç›¸æ¯”é‡‡æ ·æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ•ˆç‡ï¼Œä½†æ­¤å‰å°šæœªæœ‰é’ˆå¯¹è¯¥èŒƒå¼çš„å®‰å…¨ä¿éšœæœºåˆ¶ã€‚æœ¬æ–‡å¼€å‘äº†é¦–ä¸ªé€‚ç”¨äºè§£ææ¢¯åº¦å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆå®‰å…¨ä¿éšœ (Safeguard) æ–¹æ¡ˆï¼Œé€šè¿‡æ”¹è¿›å¯å¾®åˆ†å®‰å…¨ä¿éšœçš„æ˜ å°„å’Œæ¢¯åº¦å…¬å¼ï¼Œå¹¶å°†å…¶é›†æˆåˆ°å…ˆè¿›ç®—æ³•å’Œå¯å¾®åˆ†æ¨¡æ‹Ÿ (Differentiable Simulation) ä¸­ã€‚æ•°å€¼å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰é¡¹æ§åˆ¶ä»»åŠ¡ä¸­å®ç°äº†å—ä¿éšœçš„è®­ç»ƒï¼Œä¸”æœªå¯¹å­¦ä¹ æ€§èƒ½é€ æˆæŸå¤±ã€‚è¿™é¡¹å·¥ä½œå¡«è¡¥äº†é¢†åŸŸç©ºç™½ï¼Œä¸ºå®ç°é«˜æ€§èƒ½ä¸”å…·å¤‡å®‰å…¨ä¿è¯çš„è‡ªä¸»ç³»ç»Ÿè®­ç»ƒå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.01665v3",
      "published_date": "2025-06-02 13:35:03 UTC",
      "updated_date": "2025-10-23 08:51:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:36:38.696934+00:00"
    },
    {
      "arxiv_id": "2506.01662v1",
      "title": "Explainable AI Systems Must Be Contestable: Here's How to Make It Happen",
      "title_zh": "å¯è§£é‡Šäººå·¥æ™ºèƒ½ç³»ç»Ÿå¿…é¡»å…·å¤‡å¯è´¨ç–‘æ€§ï¼šå®ç°è·¯å¾„æ¢ç©¶",
      "authors": [
        "Catarina Moreira",
        "Anna Palatkina",
        "Dacia Braca",
        "Dylan M. Walsh",
        "Peter J. Leihn",
        "Fang Chen",
        "Nina C. Hubig"
      ],
      "abstract": "As AI regulations around the world intensify their focus on system safety, contestability has become a mandatory, yet ill-defined, safeguard. In XAI, \"contestability\" remains an empty promise: no formal definition exists, no algorithm guarantees it, and practitioners lack concrete guidance to satisfy regulatory requirements. Grounded in a systematic literature review, this paper presents the first rigorous formal definition of contestability in explainable AI, directly aligned with stakeholder requirements and regulatory mandates. We introduce a modular framework of by-design and post-hoc mechanisms spanning human-centered interfaces, technical architectures, legal processes, and organizational workflows. To operationalize our framework, we propose the Contestability Assessment Scale, a composite metric built on more than twenty quantitative criteria. Through multiple case studies across diverse application domains, we reveal where state-of-the-art systems fall short and show how our framework drives targeted improvements. By converting contestability from regulatory theory into a practical framework, our work equips practitioners with the tools to embed genuine recourse and accountability into AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½ç›‘ç®¡ä¸­â€œå¯äº‰è¾©æ€§â€(Contestability)å®šä¹‰ä¸æ˜ä¸”åœ¨å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)é¢†åŸŸç¼ºä¹æŠ€æœ¯ä¿éšœçš„é—®é¢˜ï¼Œé¦–æ¬¡æå‡ºäº†ä¸ç›‘ç®¡è¦æ±‚é«˜åº¦ä¸€è‡´çš„Contestabilityä¸¥è°¨å½¢å¼åŒ–å®šä¹‰ã€‚ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªæ¶µç›–ä»¥äººä¸ºæœ¬çš„ç•Œé¢ã€æŠ€æœ¯æ¶æ„ã€æ³•å¾‹ç¨‹åºåŠç»„ç»‡å·¥ä½œæµçš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œå¹¶é…å¥—å¼€å‘äº†åŒ…å«20å¤šé¡¹é‡åŒ–æŒ‡æ ‡çš„â€œå¯äº‰è¾©æ€§è¯„ä¼°é‡è¡¨â€(Contestability Assessment Scale)ã€‚é€šè¿‡å¤šé¢†åŸŸæ¡ˆä¾‹ç ”ç©¶ï¼Œè®ºæ–‡æ­ç¤ºäº†ç°æœ‰å…ˆè¿›ç³»ç»Ÿçš„å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ¡†æ¶å¦‚ä½•å¼•å¯¼é’ˆå¯¹æ€§æ”¹è¿›ã€‚è¯¥å·¥ä½œæˆåŠŸå°†Contestabilityä»æŠ½è±¡çš„ç›‘ç®¡æ³•å¾‹ç†è®ºè½¬åŒ–ä¸ºå¯æ“ä½œçš„å®è·µå·¥å…·ï¼Œä¸ºAIç³»ç»Ÿçš„å¯è¿½è´£æ€§(Accountability)å’Œæœ‰æ•ˆæ•‘æµæä¾›äº†å…·ä½“çš„æŠ€æœ¯å®ç°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01662v1",
      "published_date": "2025-06-02 13:32:05 UTC",
      "updated_date": "2025-06-02 13:32:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:37:12.091852+00:00"
    },
    {
      "arxiv_id": "2506.01659v2",
      "title": "Engram Memory Encoding and Retrieval: A Neurocomputational Perspective",
      "title_zh": "è®°å¿†ç—•è¿¹çš„ç¼–ç ä¸æå–ï¼šç¥ç»è®¡ç®—è§†è§’",
      "authors": [
        "Daniel Szelogowski"
      ],
      "abstract": "Despite substantial research into the biological basis of memory, the precise mechanisms by which experiences are encoded, stored, and retrieved in the brain remain incompletely understood. A growing body of evidence supports the engram theory, which posits that sparse populations of neurons undergo lasting physical and biochemical changes to support long-term memory. Yet, a comprehensive computational framework that integrates biological findings with mechanistic models remains elusive. This work synthesizes insights from cellular neuroscience and computational modeling to address key challenges in engram research: how engram neurons are identified and manipulated; how synaptic plasticity mechanisms contribute to stable memory traces; and how sparsity promotes efficient, interference-resistant representations. Relevant computational approaches -- such as sparse regularization, engram gating, and biologically inspired architectures like Sparse Distributed Memory and spiking neural networks -- are also examined. Together, these findings suggest that memory efficiency, capacity, and stability emerge from the interaction of plasticity and sparsity constraints. By integrating neurobiological and computational perspectives, this paper provides a comprehensive theoretical foundation for engram research and proposes a roadmap for future inquiry into the mechanisms underlying memory, with implications for the diagnosis and treatment of memory-related disorders.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶ä»ç¥ç»è®¡ç®—ï¼ˆneurocomputationalï¼‰çš„è§’åº¦æ¢è®¨äº†è®°å¿†å°è¿¹ï¼ˆEngramï¼‰çš„ç¼–ç ã€å­˜å‚¨ä¸æ£€ç´¢æœºåˆ¶ã€‚é’ˆå¯¹ç”Ÿç‰©å­¦è¯æ®ä¸è®¡ç®—æ¡†æ¶ä¹‹é—´ç¼ºä¹æ•´åˆçš„é—®é¢˜ï¼Œæ–‡ç« ç»¼åˆäº†ç»†èƒç¥ç»ç§‘å­¦ä¸è®¡ç®—å»ºæ¨¡çš„è§è§£ï¼Œæ·±å…¥åˆ†æäº†å°è¿¹ç¥ç»å…ƒè¯†åˆ«ã€çªè§¦å¯å¡‘æ€§ï¼ˆsynaptic plasticityï¼‰å¯¹è®°å¿†ç¨³å®šçš„è´¡çŒ®ä»¥åŠç¨€ç–æ€§ï¼ˆsparsityï¼‰åœ¨æé«˜è®°å¿†æ•ˆç‡å’ŒæŠ—å¹²æ‰°èƒ½åŠ›æ–¹é¢çš„ä½œç”¨ã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†ç¨€ç–æ­£åˆ™åŒ–ï¼ˆsparse regularizationï¼‰ã€å°è¿¹é—¨æ§ï¼ˆengram gatingï¼‰ä»¥åŠç¨€ç–åˆ†å¸ƒå¼è®°å¿†ï¼ˆSparse Distributed Memoryï¼‰å’Œè„‰å†²ç¥ç»ç½‘ç»œï¼ˆspiking neural networksï¼‰ç­‰è®¡ç®—æ¨¡å‹ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œè®°å¿†çš„æ•ˆç‡ã€å®¹é‡å’Œç¨³å®šæ€§æºäºå¯å¡‘æ€§ä¸ç¨€ç–æ€§çº¦æŸçš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡æ•´åˆç¥ç»ç”Ÿç‰©å­¦ä¸è®¡ç®—è§†è§’ï¼Œè¯¥å·¥ä½œä¸ºEngramç ”ç©¶æä¾›äº†å…¨é¢çš„ç†è®ºåŸºç¡€ï¼Œå¹¶ä¸ºæœªæ¥æ¢ç´¢è®°å¿†æœºåˆ¶åŠè¯Šç–—è®°å¿†ç›¸å…³ç–¾ç—…æå‡ºäº†æ˜ç¡®çš„è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.NE",
      "comment": "18 pages, 7 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.01659v2",
      "published_date": "2025-06-02 13:30:39 UTC",
      "updated_date": "2025-10-24 23:14:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:38:25.032450+00:00"
    },
    {
      "arxiv_id": "2506.01646v2",
      "title": "ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge",
      "title_zh": "ESGeniusï¼šå¤§è¯­è¨€æ¨¡å‹ç¯å¢ƒã€ç¤¾ä¼šå’Œæ²»ç†ï¼ˆESGï¼‰åŠå¯æŒç»­å‘å±•çŸ¥è¯†åŸºå‡†æµ‹è¯•",
      "authors": [
        "Chaoyue He",
        "Xin Zhou",
        "Yi Wu",
        "Xinjia Yu",
        "Yan Zhang",
        "Lei Zhang",
        "Di Wang",
        "Shengfei Lyu",
        "Hong Xu",
        "Xiaoqiao Wang",
        "Wei Liu",
        "Chunyan Miao"
      ],
      "abstract": "We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social, and Governance (ESG) and sustainability-focused question answering. ESGenius comprises two key components: (i) ESGenius-QA, a collection of 1,136 Multiple-Choice Questions (MCQs) generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting Retrieval-Augmented Generation (RAG) methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231 foundational frameworks, standards, reports, and recommendation documents from 7 authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of LLMs, we implement a rigorous two-stage evaluation protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (0.5B to 671B) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies around 55--70%, highlighting a significant knowledge gap for LLMs in this specialized, interdisciplinary domain. However, models employing RAG demonstrate significant performance improvements, particularly for smaller models. For example, DeepSeek-R1-Distill-Qwen-14B improves from 63.82% (zero-shot) to 80.46% with RAG. These results demonstrate the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To the best of our knowledge, ESGenius is the first comprehensive QA benchmark designed to rigorously evaluate LLMs on ESG and sustainability knowledge, providing a critical tool to advance trustworthy AI in this vital domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† ESGeniusï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°å’Œæå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç¯å¢ƒã€ç¤¾ä¼šå’Œæ²»ç† (ESG) åŠå¯æŒç»­å‘å±•é¢†åŸŸçŸ¥è¯†æ°´å¹³çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†ç”±åŒ…å« 1,136 é“ä¸“å®¶éªŒè¯å¤šé€‰é¢˜çš„ ESGenius-QA å’Œæ”¶å½• 231 ä»½æƒå¨æœºæ„æŠ¥å‘Šçš„è¯­æ–™åº“ ESGenius-Corpus ç»„æˆã€‚é€šè¿‡å¯¹ 50 ä¸ªä¸åŒè§„æ¨¡çš„æ¨¡å‹è¿›è¡Œ Zero-Shot å’Œ RAG ä¸¤é˜¶æ®µè¯„ä¼°ï¼Œç ”ç©¶å‘ç°ä¸»æµæ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä»…èƒ½è¾¾åˆ° 55-70% çš„å‡†ç¡®ç‡ï¼Œæ­ç¤ºäº† LLMs åœ¨æ­¤ä¸“ä¸šé¢†åŸŸå­˜åœ¨æ˜¾è‘—çŸ¥è¯†ç¼ºå£ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé‡‡ç”¨ RAG æŠ€æœ¯èƒ½å¤§å¹…ä¼˜åŒ–è¡¨ç°ï¼Œä¾‹å¦‚ DeepSeek-R1-Distill-Qwen-14B çš„å‡†ç¡®ç‡ä» 63.82% æå‡è‡³ 80.46%ã€‚è¯¥æˆæœå¼ºè°ƒäº†å°† AI å›ç­”é”šå®šäºæƒå¨æ¥æºå¯¹äºå¢å¼º ESG ç†è§£çš„å¿…è¦æ€§ï¼Œä¸ºåœ¨è¿™ä¸€å…³é”®é¢†åŸŸæ„å»ºå¯ä¿¡ AI æä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP'25 Main Oral (42 pages, 10 figures, 11 tables), Nominations for Resource Award & Theme Paper Award",
      "pdf_url": "https://arxiv.org/pdf/2506.01646v2",
      "published_date": "2025-06-02 13:19:09 UTC",
      "updated_date": "2025-09-19 20:11:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:38:24.865574+00:00"
    },
    {
      "arxiv_id": "2506.01639v1",
      "title": "Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning",
      "title_zh": "åŒå‘ Soft Actor-Criticï¼šåˆ©ç”¨å‰å‘ä¸é€†å‘ KL æ•£åº¦å®ç°é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yixian Zhang",
        "Huaze Tang",
        "Changxu Wei",
        "Wenbo Ding"
      ],
      "abstract": "The Soft Actor-Critic (SAC) algorithm, a state-of-the-art method in maximum entropy reinforcement learning, traditionally relies on minimizing reverse Kullback-Leibler (KL) divergence for policy updates. However, this approach leads to an intractable optimal projection policy, necessitating gradient-based approximations that can suffer from instability and poor sample efficiency. This paper investigates the alternative use of forward KL divergence within SAC. We demonstrate that for Gaussian policies, forward KL divergence yields an explicit optimal projection policy -- corresponding to the mean and variance of the target Boltzmann distribution's action marginals. Building on the distinct advantages of both KL directions, we propose Bidirectional SAC, an algorithm that first initializes the policy using the explicit forward KL projection and then refines it by optimizing the reverse KL divergence. Comprehensive experiments on continuous control benchmarks show that Bidirectional SAC significantly outperforms standard SAC and other baselines, achieving up to a $30\\%$ increase in episodic rewards, alongside enhanced sample efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿçš„ Soft Actor-Critic (SAC) ç®—æ³•å› ä¾èµ– reverse Kullback-Leibler (KL) æ•£åº¦å¯¼è‡´æœ€ä¼˜æŠ•å½±ç­–ç•¥éš¾ä»¥å¤„ç†ä¸”é‡‡æ ·æ•ˆç‡ä½çš„é—®é¢˜ï¼Œæ¢è®¨äº†åœ¨ SAC ä¸­å¼•å…¥ forward KL æ•£åº¦çš„ä¼˜åŠ¿ã€‚ç ”ç©¶è¯æ˜ï¼Œå¯¹äº Gaussian ç­–ç•¥ï¼Œforward KL å¯ä»¥å¾—å‡ºæ˜¾å¼çš„æœ€ä¼˜æŠ•å½±ç­–ç•¥ï¼Œå³ç›®æ ‡ Boltzmann åˆ†å¸ƒåŠ¨ä½œè¾¹ç¼˜çš„å‡å€¼å’Œæ–¹å·®ã€‚æ®æ­¤ï¼Œä½œè€…æå‡ºäº† Bidirectional SAC ç®—æ³•ï¼Œé€šè¿‡å…ˆåˆ©ç”¨æ˜¾å¼çš„ forward KL æŠ•å½±åˆå§‹åŒ–ç­–ç•¥ï¼Œå†ç»“åˆ reverse KL æ•£åº¦è¿›è¡Œç²¾ç»†åŒ–æ”¹è¿›ã€‚åœ¨è¿ç»­æ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBidirectional SAC æ˜¾è‘—ä¼˜äºæ ‡å‡† SAC åŠå…¶ä»–åŸºçº¿ï¼Œä¸ä»…å°†æƒ…æ™¯å¥–åŠ±æå‡äº†é«˜è¾¾ 30%ï¼Œè¿˜å¤§å¹…å¢å¼ºäº†ç®—æ³•çš„é‡‡æ ·æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01639v1",
      "published_date": "2025-06-02 13:15:30 UTC",
      "updated_date": "2025-06-02 13:15:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:38:09.505739+00:00"
    },
    {
      "arxiv_id": "2506.06351v1",
      "title": "Deep learning methods for modeling infrasound transmission loss in the middle atmosphere",
      "title_zh": "ç”¨äºä¸­å±‚å¤§æ°”æ¬¡å£°ä¼ æ’­æŸå¤±å»ºæ¨¡çš„æ·±åº¦å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Alexis Le Pichon",
        "Alice Janela Cameijo",
        "Samir Aknine",
        "Youcef Sklab",
        "Souhila Arib",
        "Quentin Brissaud",
        "Sven Peter Naesholm"
      ],
      "abstract": "Accurate modeling of infrasound transmission losses (TLs) is essential to assess the performance of the global International Monitoring System infrasound network. Among existing propagation modeling tools, parabolic equation (PE) method enables TLs to be finely modeled, but its computational cost does not allow exploration of a large parameter space for operational monitoring applications. To reduce computation times, Brissaud et al. 2023 explored the potential of convolutional neural networks trained on a large set of regionally simulated wavefields (< 1000 km from the source) to predict TLs with negligible computation times compared to PE simulations. However, this method struggles in unfavorable initial wind conditions, especially at high frequencies, and causal issues with winds at large distances from the source affecting ground TLs close to the source. In this study, we have developed an optimized convolutional network designed to minimize prediction errors while predicting TLs from globally simulated combined temperature and wind fields spanning over propagation ranges of 4000 km. Our approach enhances the previously proposed one by implementing key optimizations that improve the overall architecture performance. The implemented model predicts TLs with an average error of 8.6 dB in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸­å±‚å¤§æ°”ä¸­æ¬¡å£°æ³¢ä¼ è¾“æŸè€—ï¼ˆTransmission Loss, TLsï¼‰çš„å»ºæ¨¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚ä¸ºå…‹æœæŠ›ç‰©æ–¹ç¨‹ï¼ˆParabolic Equation, PEï¼‰æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ä»¥åŠæ—©æœŸå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹åœ¨å¤æ‚é£åœºå’Œé•¿è·ç¦»ä¼ æ’­ï¼ˆ4000 kmï¼‰ä¸­çš„å±€é™æ€§ï¼Œä½œè€…è®¾è®¡å¹¶å®ç°äº†ä¸€ç§æ”¹è¿›çš„CNNæ¶æ„ã€‚è¯¥æ¶æ„é€šè¿‡ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œèƒ½å¤ŸåŸºäºå…¨çƒæ¨¡æ‹Ÿçš„æ¸©åº¦å’Œé£åœºæ•°æ®ï¼Œåœ¨0.1-3.2 Hzé¢‘æ®µå†…å®ç°8.6 dBçš„å¹³å‡é¢„æµ‹è¯¯å·®ã€‚è¯¥ç ”ç©¶ä¸ä»…æ˜¾è‘—é™ä½äº†è®¡ç®—æ—¶é—´ï¼Œè¿˜æ¶µç›–äº†ç°å®çš„å¤§æ°”åœºæ™¯ï¼Œä¸ºå…¨çƒå›½é™…ç›‘æµ‹ç³»ç»Ÿï¼ˆInternational Monitoring Systemï¼‰æ¬¡å£°æ³¢ç½‘ç»œçš„è¿è¡Œç›‘æµ‹å’Œæ€§èƒ½è¯„ä¼°æä¾›äº†æ›´ä¸ºé«˜æ•ˆçš„å·¥å…·ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "12 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.06351v1",
      "published_date": "2025-06-02 13:10:29 UTC",
      "updated_date": "2025-06-02 13:10:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:38:08.461687+00:00"
    },
    {
      "arxiv_id": "2506.01631v2",
      "title": "Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification",
      "title_zh": "åŸºäºæ¢¯åº¦çš„æ¨¡å‹æŒ‡çº¹è¯†åˆ«ï¼šå¤§è¯­è¨€æ¨¡å‹ç›¸ä¼¼æ€§æ£€æµ‹ä¸å®¶æ—åˆ†ç±»",
      "authors": [
        "Zehao Wu",
        "Yanjie Zhao",
        "Haoyu Wang"
      ],
      "abstract": "As Large Language Models (LLMs) become integral software components in modern applications, unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges. Unlike traditional software where clone detection and license compliance are well-established, the LLM ecosystem lacks effective mechanisms to detect model lineage and enforce licensing agreements. This gap is particularly problematic when open-source model creators, such as Meta's LLaMA, require derivative works to maintain naming conventions for attribution, yet no technical means exist to verify compliance.\n  To fill this gap, treating LLMs as software artifacts requiring provenance tracking, we present TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification. Our approach extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers, operating independently of training data, watermarks, or specific model formats. TensorGuard supports the widely-adopted safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features. These fingerprints enable two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models. Experimental evaluation on 58 models comprising 8 base models and 50 derivatives across five model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94% classification accuracy under our centroid-initialized K-Means clustering.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TensorGuardï¼Œä¸€ä¸ªåŸºäºæ¢¯åº¦æŒ‡çº¹(gradient-based fingerprinting)çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¾®è°ƒã€åˆå¹¶å’Œå†åˆ†å‘è¿‡ç¨‹ä¸­çš„æº¯æºè¿½è¸ªä¸è®¸å¯åˆè§„æ€§æ£€æµ‹éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†æå¼ é‡å±‚å¯¹éšæœºè¾“å…¥æ‰°åŠ¨çš„æ¢¯åº¦å“åº”æ¥æå–æ¨¡å‹å†…åœ¨çš„è¡Œä¸ºç‰¹å¾ï¼Œä¸”è¯¥è¿‡ç¨‹å®Œå…¨ç‹¬ç«‹äºè®­ç»ƒæ•°æ®ã€æ°´å°æˆ–ç‰¹å®šæ¨¡å‹æ ¼å¼ã€‚TensorGuard æ”¯æŒå¹¿æ³›ä½¿ç”¨çš„ `safetensors` æ ¼å¼ï¼Œé€šè¿‡æ„å»ºé«˜ç»´æŒ‡çº¹å®ç°ä»»æ„æ¨¡å‹é—´çš„ä¸¤ä¸¤ç›¸ä¼¼åº¦è¯„ä¼°ï¼Œå¹¶åˆ©ç”¨åŸºäºå·²çŸ¥åŸºç¡€æ¨¡å‹åˆå§‹åŒ–çš„ K-Means èšç±»ç®—æ³•å¯¹æœªçŸ¥æ¨¡å‹è¿›è¡Œç³»ç»ŸåŒ–çš„å®¶æ—åˆ†ç±»(family classification)ã€‚å®éªŒè¯„ä¼°æ¶µç›–äº† Llamaã€Qwenã€Gemmaã€Phi å’Œ Mistral ç­‰ 5 ä¸ªæ¨¡å‹å®¶æ—çš„ 58 ä¸ªæ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å®¶æ—åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº† 94% çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€æˆæœä¸º LLM ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ¨¡å‹è¡€ç¼˜æ£€æµ‹å’Œç‰ˆæƒä¿æŠ¤æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01631v2",
      "published_date": "2025-06-02 13:08:01 UTC",
      "updated_date": "2025-07-03 15:36:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:38:24.715536+00:00"
    },
    {
      "arxiv_id": "2506.01625v2",
      "title": "Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks",
      "title_zh": "å¯¹æŠ—æ”»å‡»ä¸‹çš„é²æ£’æ»¡æ„æ€§é«˜æ–¯è¿‡ç¨‹å¤šè‡‚è€è™æœº",
      "authors": [
        "Artun Saday",
        "YaÅŸar Cahit YÄ±ldÄ±rÄ±m",
        "Cem Tekin"
      ],
      "abstract": "We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $Ï„$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $Ï„$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å­˜åœ¨æœªçŸ¥ä¸”å¯èƒ½å˜åŒ–çš„å¯¹æŠ—æ€§æ‰°åŠ¨ï¼ˆAdversarial Perturbationsï¼‰çš„é«˜æ–¯è¿‡ç¨‹ï¼ˆGaussian Process, GPï¼‰ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é²æ£’æ»¡æ„ï¼ˆRobust Satisficingï¼‰ç›®æ ‡ï¼Œæ—¨åœ¨å¯¹æŠ—æ¡ä»¶ä¸‹ç¨³å®šè¾¾åˆ°é¢„å®šä¹‰çš„æ€§èƒ½é˜ˆå€¼ $\\tau$ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸¤ç§åŸºäºä¸åŒé²æ£’æ»¡æ„è¡¨è¿°çš„æ–°ç®—æ³•ï¼Œå¹¶å°†å…¶ç»Ÿä¸€åœ¨é€šç”¨çš„é²æ£’æ»¡æ„æ¡†æ¶ä¸‹ã€‚é’ˆå¯¹ä¸åŒæ€§è´¨çš„å¯¹æŠ—ç¯å¢ƒï¼Œç ”ç©¶æ¨å¯¼äº†ä¸¤ç§å›å½’è¾¹ç•Œï¼ˆRegret Boundsï¼‰ï¼šä¸€ç§åœ¨ç‰¹å®šæ¡ä»¶ä¸‹éšæ—¶é—´å‘ˆæ¬¡çº¿æ€§å¢é•¿ï¼Œå¦ä¸€ç§åˆ™éšæ‰°åŠ¨å¹…åº¦ç¼©æ”¾ä¸”æ— éœ€å¯¹å¯¹æŠ—è¡Œä¸ºåšä»»ä½•å‡è®¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®ç°æ»¡æ„ç›®æ ‡æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„é²æ£’ä¼˜åŒ–æ–¹æ³•ï¼Œå°¤å…¶åœ¨é²æ£’ä¼˜åŒ–æ¡†æ¶çš„æ­§ä¹‰é›†ï¼ˆAmbiguity Setï¼‰è§„æ ¼è®¾å®šä¸å‡†ç¡®æ—¶å±•ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01625v2",
      "published_date": "2025-06-02 13:04:18 UTC",
      "updated_date": "2025-12-10 23:04:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:38:44.529893+00:00"
    },
    {
      "arxiv_id": "2506.01624v1",
      "title": "Social Cooperation in Conversational AI Agents",
      "title_zh": "å¯¹è¯å¼äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“ä¸­çš„ç¤¾ä¼šæ€§åˆä½œ",
      "authors": [
        "Mustafa Mert Ã‡elikok",
        "Saptarashmi Bandyopadhyay",
        "Robert Loftin"
      ],
      "abstract": "The development of AI agents based on large, open-domain language models (LLMs) has paved the way for the development of general-purpose AI assistants that can support human in tasks such as writing, coding, graphic design, and scientific research. A major challenge with such agents is that, by necessity, they are trained by observing relatively short-term interactions with humans. Such models can fail to generalize to long-term interactions, for example, interactions where a user has repeatedly corrected mistakes on the part of the agent. In this work, we argue that these challenges can be overcome by explicitly modeling humans' social intelligence, that is, their ability to build and maintain long-term relationships with other agents whose behavior cannot always be predicted. By mathematically modeling the strategies humans use to communicate and reason about one another over long periods of time, we may be able to derive new game theoretic objectives against which LLMs and future AI agents may be optimized.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¯¹è¯AIæ™ºèƒ½ä½“åœ¨é•¿æœŸäº¤äº’ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹åœ¨åº”å¯¹ç”¨æˆ·åå¤çº é”™ç­‰å¤æ‚é•¿ç¨‹äº¤äº’æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä½œè€…è®¤ä¸ºï¼Œç°æœ‰æ¨¡å‹ç”±äºä¸»è¦åŸºäºçŸ­æœŸäº¤äº’æ•°æ®è®­ç»ƒï¼Œéš¾ä»¥æ•æ‰äººç±»ç¤¾äº¤ä¸­å»ºç«‹å¹¶ç»´æŒé•¿æœŸå…³ç³»çš„å¤æ‚æ€§ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå€¡æ˜¾å¼å»ºæ¨¡äººç±»çš„ç¤¾ä¼šæ™ºèƒ½(Social Intelligence)ï¼Œé€šè¿‡æ•°å­¦æ‰‹æ®µåˆ†æäººç±»åœ¨é•¿å‘¨æœŸå†…è¿›è¡Œæ²Ÿé€šå’Œç›¸äº’æ¨ç†çš„ç­–ç•¥ã€‚ç ”ç©¶æ—¨åœ¨æ¨å¯¼å‡ºä¸€å¥—å…¨æ–°çš„åšå¼ˆè®º(Game Theoretic)ç›®æ ‡ï¼Œç”¨äºä¼˜åŒ–LLMsåŠæœªæ¥çš„AIæ™ºèƒ½ä½“ã€‚è¿™ä¸€æ–¹æ³•ä¸ºå¼€å‘æ›´å…·é²æ£’æ€§ã€èƒ½ä¸äººç±»å®ç°æ·±åº¦ç¤¾ä¼šåä½œçš„é€šç”¨AIåŠ©æ‰‹æä¾›äº†ç†è®ºåŸºç¡€å’Œä¼˜åŒ–æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "4 pages, RLDM 2025 abstract (Spotlight presentation)",
      "pdf_url": "https://arxiv.org/pdf/2506.01624v1",
      "published_date": "2025-06-02 13:02:36 UTC",
      "updated_date": "2025-06-02 13:02:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:38:38.685040+00:00"
    },
    {
      "arxiv_id": "2506.01623v3",
      "title": "MAGIK: Mapping to Analogous Goals via Imagination-enabled Knowledge Transfer",
      "title_zh": "MAGIKï¼šåŸºäºæƒ³è±¡å¼çŸ¥è¯†è¿ç§»çš„ç±»æ¯”ç›®æ ‡æ˜ å°„",
      "authors": [
        "Ajsal Shereef Palattuparambil",
        "Thommen George Karimpanal",
        "Santu Rana"
      ],
      "abstract": "Humans excel at analogical reasoning - applying knowledge from one task to a related one with minimal relearning. In contrast, reinforcement learning (RL) agents typically require extensive retraining even when new tasks share structural similarities with previously learned ones. In this work, we propose MAGIK, a novel framework that enables RL agents to transfer knowledge to analogous tasks without interacting with the target environment. Our approach leverages an imagination mechanism to map entities in the target task to their analogues in the source domain, allowing the agent to reuse its original policy. Experiments on custom MiniGrid and MuJoCo tasks show that MAGIK achieves effective zero-shot transfer using only a small number of human-labelled examples. We compare our approach to related baselines and highlight how it offers a novel and effective mechanism for knowledge transfer via imagination-based analogy mapping.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æ™ºèƒ½ä½“åœ¨é¢å¯¹ç»“æ„ç›¸ä¼¼ä»»åŠ¡æ—¶ä»éœ€å¤§é‡é‡æ–°è®­ç»ƒçš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºMAGIKçš„çŸ¥è¯†è¿ç§»æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåˆ©ç”¨æƒ³è±¡æœºåˆ¶(imagination mechanism)å°†ç›®æ ‡ä»»åŠ¡ä¸­çš„å®ä½“æ˜ å°„åˆ°æºé¢†åŸŸçš„ç±»æ¯”é¡¹ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ä¸ä¸ç›®æ ‡ç¯å¢ƒç›´æ¥äº¤äº’çš„æƒ…å†µä¸‹å¤ç”¨å…¶åŸå§‹ç­–ç•¥(original policy)ã€‚åœ¨å®šåˆ¶çš„MiniGridå’ŒMuJoCoä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGIKä»…éœ€æå°‘é‡çš„äººå·¥æ ‡æ³¨ç¤ºä¾‹å³å¯å®ç°æœ‰æ•ˆçš„é›¶æ ·æœ¬è¿ç§»(zero-shot transfer)ã€‚é€šè¿‡ä¸ç›¸å…³åŸºå‡†æ¨¡å‹çš„å¯¹æ¯”ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†åŸºäºæƒ³è±¡çš„ç±»æ¯”æ˜ å°„(imagination-based analogy mapping)åœ¨è·¨ä»»åŠ¡çŸ¥è¯†ä¼ é€’ä¸­çš„åˆ›æ–°æ€§å’Œé«˜æ•ˆæ€§ï¼Œä¸ºæå‡æ™ºèƒ½ä½“çš„ç±»æ¯”æ¨ç†èƒ½åŠ›æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01623v3",
      "published_date": "2025-06-02 13:01:14 UTC",
      "updated_date": "2025-08-18 02:21:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:39:11.858614+00:00"
    },
    {
      "arxiv_id": "2506.01622v5",
      "title": "General agents contain world models",
      "title_zh": "é€šç”¨æ™ºèƒ½ä½“è•´å«ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Jonathan Richens",
        "David Abel",
        "Alexis Bellot",
        "Tom Everitt"
      ],
      "abstract": "Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†ä¸–ç•Œæ¨¡å‹ (world models) æ˜¯å¦æ˜¯å®ç°çµæ´»ã€ç›®æ ‡å¯¼å‘è¡Œä¸ºçš„å¿…è¦æ¡ä»¶ï¼Œè€Œéä»…é æ— æ¨¡å‹å­¦ä¹  (model-free learning) å³å¯è¾¾æˆã€‚ç ”ç©¶é€šè¿‡å½¢å¼åŒ–è¯æ˜æŒ‡å‡ºï¼Œä»»ä½•èƒ½å¤Ÿæ³›åŒ–åˆ°å¤šæ­¥ç›®æ ‡å¯¼å‘ä»»åŠ¡çš„æ™ºèƒ½ä½“ï¼Œå®è´¨ä¸Šéƒ½å¿…é¡»å­¦ä¹ å¹¶åŒ…å«ä¸€ä¸ªå…³äºç¯å¢ƒçš„é¢„æµ‹æ¨¡å‹ã€‚ä½œè€…è¿›ä¸€æ­¥å±•ç¤ºäº†è¿™ç§æ¨¡å‹å¯ä»¥ä»æ™ºèƒ½ä½“çš„ç­–ç•¥ (policy) ä¸­è¢«æå–å‡ºæ¥ï¼Œä¸”æ™ºèƒ½ä½“æ€§èƒ½çš„æå‡æˆ–ä»»åŠ¡å¤æ‚åº¦çš„å¢åŠ å‡å¯¹ä¸–ç•Œæ¨¡å‹ (world models) çš„å‡†ç¡®åº¦æå‡ºäº†æ›´é«˜è¦æ±‚ã€‚è¿™ä¸€å‘ç°ä¸ä»…æ­ç¤ºäº†é€šç”¨æ™ºèƒ½ä½“ä¸ç¯å¢ƒå»ºæ¨¡ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œè¿˜ä¸ºå¼€å‘å®‰å…¨ä¸”é€šç”¨çš„æ™ºèƒ½ä½“ã€ç•Œå®šå¤æ‚ç¯å¢ƒä¸‹çš„èƒ½åŠ›è¾¹ç•Œä»¥åŠè®¾è®¡æ–°çš„æ¨¡å‹æå–ç®—æ³•æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted ICML 2025. Typos corrected",
      "pdf_url": "https://arxiv.org/pdf/2506.01622v5",
      "published_date": "2025-06-02 13:01:13 UTC",
      "updated_date": "2025-10-20 12:08:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:39:03.887583+00:00"
    },
    {
      "arxiv_id": "2506.01618v1",
      "title": "Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech",
      "title_zh": "ç”¨äºæå‡æ„éŸ³éšœç¢è¯­éŸ³ ASR æ€§èƒ½çš„æ— ç›‘ç£èŠ‚å¥ä¸è¯­éŸ³è½¬æ¢",
      "authors": [
        "Karl El Hajal",
        "Enno Hermann",
        "Sevada Hovsepyan",
        "Mathew Magimai. -Doss"
      ],
      "abstract": "Automatic speech recognition (ASR) systems struggle with dysarthric speech due to high inter-speaker variability and slow speaking rates. To address this, we explore dysarthric-to-healthy speech conversion for improved ASR performance. Our approach extends the Rhythm and Voice (RnV) conversion framework by introducing a syllable-based rhythm modeling method suited for dysarthric speech. We assess its impact on ASR by training LF-MMI models and fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal that LF-MMI achieves significant word error rate reductions, especially for more severe cases of dysarthria, while fine-tuning Whisper on converted data has minimal effect on its performance. These results highlight the potential of unsupervised rhythm and voice conversion for dysarthric ASR. Code available at: https://github.com/idiap/RnV",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿåœ¨å¤„ç†æ„éŸ³éšœç¢è¯­éŸ³(Dysarthric speech)æ—¶å› é«˜å˜å¼‚æ€§å’Œç¼“æ…¢è¯­é€Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæ¢ç´¢äº†é€šè¿‡æ— ç›‘ç£è½¬æ¢æŠ€æœ¯æ”¹å–„è¯†åˆ«æ€§èƒ½çš„æ–¹æ³•ã€‚ä½œè€…é€šè¿‡å¼•å…¥ä¸€ç§ä¸“é—¨é’ˆå¯¹æ„éŸ³éšœç¢è¯­éŸ³è®¾è®¡çš„åŸºäºéŸ³èŠ‚çš„éŸµå¾‹å»ºæ¨¡(Syllable-based rhythm modeling)æ–¹æ¡ˆï¼Œå¯¹åŸæœ‰çš„èŠ‚å¥ä¸è¯­éŸ³è½¬æ¢(RnV)æ¡†æ¶è¿›è¡Œäº†æ‰©å±•ã€‚å®éªŒåœ¨Torgoè¯­æ–™åº“ä¸Šå±•å¼€ï¼Œé€šè¿‡è®­ç»ƒLF-MMIæ¨¡å‹å’Œå¾®è°ƒWhisperæ¨¡å‹æ¥è¯„ä¼°è½¬æ¢åçš„è¯­éŸ³å¯¹è¯†åˆ«å‡†ç¡®ç‡çš„å½±å“ã€‚ç ”ç©¶ç»“æœå‘ç°ï¼ŒLF-MMIæ¨¡å‹åœ¨è½¬æ¢åçš„è¯­éŸ³ä¸Šå®ç°äº†æ˜¾è‘—çš„è¯é”™ç‡(WER)é™ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¥é‡çš„æ„éŸ³éšœç¢ç—…ä¾‹ä¸­æ”¹å–„å°¤ä¸ºçªå‡ºï¼Œè€ŒWhisperå¾®è°ƒçš„æ•ˆæœåˆ™ç›¸å¯¹æœ‰é™ã€‚è¯¥å·¥ä½œè¯å®äº†æ— ç›‘ç£èŠ‚å¥ä¸è¯­éŸ³è½¬æ¢åœ¨æå‡æ„éŸ³éšœç¢è¯­éŸ³è¯†åˆ«é²æ£’æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºè¾…åŠ©é€šä¿¡æŠ€æœ¯æä¾›äº†æ–°çš„ç ”ç©¶æ€è·¯ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01618v1",
      "published_date": "2025-06-02 12:57:36 UTC",
      "updated_date": "2025-06-02 12:57:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:40:16.509378+00:00"
    },
    {
      "arxiv_id": "2506.01616v1",
      "title": "MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments",
      "title_zh": "MLA-Trustï¼šå›¾å½¢ç”¨æˆ·ç•Œé¢ç¯å¢ƒä¸‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å¯ä¿¡åº¦çš„åŸºå‡†è¯„æµ‹",
      "authors": [
        "Xiao Yang",
        "Jiawei Chen",
        "Jun Luo",
        "Zhengwei Fang",
        "Yinpeng Dong",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "The emergence of multimodal LLM-based agents (MLAs) has transformed interaction paradigms by seamlessly integrating vision, language, action and dynamic environments, enabling unprecedented autonomous capabilities across GUI applications ranging from web automation to mobile systems. However, MLAs introduce critical trustworthiness challenges that extend far beyond traditional language models' limitations, as they can directly modify digital states and trigger irreversible real-world consequences. Existing benchmarks inadequately tackle these unique challenges posed by MLAs' actionable outputs, long-horizon uncertainty and multimodal attack vectors. In this paper, we introduce MLA-Trust, the first comprehensive and unified framework that evaluates the MLA trustworthiness across four principled dimensions: truthfulness, controllability, safety and privacy. We utilize websites and mobile applications as realistic testbeds, designing 34 high-risk interactive tasks and curating rich evaluation datasets. Large-scale experiments involving 13 state-of-the-art agents reveal previously unexplored trustworthiness vulnerabilities unique to multimodal interactive scenarios. For instance, proprietary and open-source GUI-interacting MLAs pose more severe trustworthiness risks than static MLLMs, particularly in high-stakes domains; the transition from static MLLMs into interactive MLAs considerably compromises trustworthiness, enabling harmful content generation in multi-step interactions that standalone MLLMs would typically prevent; multi-step execution, while enhancing the adaptability of MLAs, involves latent nonlinear risk accumulation across successive interactions, circumventing existing safeguards and resulting in unpredictable derived risks. Moreover, we present an extensible toolbox to facilitate continuous evaluation of MLA trustworthiness across diverse interactive environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MLA-Trustï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“(MLAs)åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)ç¯å¢ƒä¸‹å¯ä¿¡åº¦çš„å…¨é¢ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»truthfulnessã€controllabilityã€safetyå’Œprivacyå››ä¸ªå…³é”®ç»´åº¦å‡ºå‘ï¼Œåˆ©ç”¨çœŸå®ç½‘ç«™å’Œç§»åŠ¨åº”ç”¨è®¾è®¡äº†34é¡¹é«˜é£é™©äº¤äº’ä»»åŠ¡åŠå¤§è§„æ¨¡è¯„ä¼°æ•°æ®é›†ã€‚é€šè¿‡å¯¹13ç§å‰æ²¿æ™ºèƒ½ä½“çš„å®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†äº¤äº’å¼MLAsåœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„å¯ä¿¡åº¦é£é™©æ˜¾è‘—é«˜äºé™æ€å¤šæ¨¡æ€æ¨¡å‹(MLLMs)ï¼Œä¸”è¿™ç±»é£é™©åœ¨å¤„ç†é«˜é£é™©ä»»åŠ¡æ—¶å°¤ä¸ºä¸¥é‡ã€‚å®éªŒè¿›ä¸€æ­¥å‘ç°ï¼Œå¤šæ­¥äº¤äº’æ‰§è¡Œä¼šå¯¼è‡´éçº¿æ€§çš„é£é™©ç§¯ç´¯ï¼Œä»è€Œä½¿æ™ºèƒ½ä½“äº§ç”ŸåŸæœ¬åœ¨standalone MLLMsä¸­ä¼šè¢«æ‹¦æˆªçš„æœ‰å®³å†…å®¹ï¼Œå¹¶è§„é¿ç°æœ‰çš„å®‰å…¨é˜²æŠ¤ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„å·¥å…·ç®±ï¼Œæ—¨åœ¨æ”¯æŒæœªæ¥åœ¨å„ç§äº¤äº’ç¯å¢ƒä¸‹å¯¹MLAå¯ä¿¡åº¦è¿›è¡ŒæŒç»­çš„åŸºå‡†æµ‹è¯•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01616v1",
      "published_date": "2025-06-02 12:56:27 UTC",
      "updated_date": "2025-06-02 12:56:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:40:46.311683+00:00"
    },
    {
      "arxiv_id": "2506.01614v1",
      "title": "Contrastive Learning for Efficient Transaction Validation in UTXO-based Blockchains",
      "title_zh": "åŸºäºå¯¹æ¯”å­¦ä¹ çš„ UTXO æ¨¡å‹åŒºå—é“¾é«˜æ•ˆäº¤æ˜“éªŒè¯",
      "authors": [
        "Hamid Attar",
        "Luigi Lunardon",
        "Alessio Pagani"
      ],
      "abstract": "This paper introduces a Machine Learning (ML) approach for scalability of UTXO-based blockchains, such as Bitcoin. Prior approaches to UTXO set sharding struggle with distributing UTXOs effectively across validators, creating substantial communication overhead due to child-parent transaction dependencies. This overhead, which arises from the need to locate parent UTXOs, significantly hampers transaction processing speeds. Our solution uses ML to optimize not only UTXO set sharding but also the routing of incoming transactions, ensuring that transactions are directed to shards containing their parent UTXOs. At the heart of our approach is a framework that combines contrastive and unsupervised learning to create an embedding space for transaction outputs. This embedding allows the model to group transaction outputs based on spending relationships, making it possible to route transactions efficiently to the correct validation microservices. Trained on historical transaction data with triplet loss and online semi-hard negative mining, the model embeds parent-child spending patterns directly into its parameters, thus eliminating the need for costly, real-time parent transaction lookups. This significantly reduces cross-shard communication overhead, boosting throughput and scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Bitcoin ç­‰åŸºäº UTXO çš„åŒºå—é“¾åœ¨åˆ†ç‰‡(sharding)è¿‡ç¨‹ä¸­å› çˆ¶å­äº¤æ˜“ä¾èµ–å¯¼è‡´çš„é€šä¿¡å¼€é”€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨æœºå™¨å­¦ä¹ (Machine Learning)ä¼˜åŒ–äº¤æ˜“éªŒè¯çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆçš„æ ¸å¿ƒæ˜¯ç»“åˆå¯¹æ¯”å­¦ä¹ (Contrastive Learning)å’Œæ— ç›‘ç£å­¦ä¹ (Unsupervised Learning)ä¸ºäº¤æ˜“è¾“å‡ºæ„å»ºåµŒå…¥ç©ºé—´(embedding space)ï¼Œé€šè¿‡æ”¯å‡ºå…³ç³»å¯¹ UTXO è¿›è¡Œèšç±»ã€‚æ¨¡å‹é‡‡ç”¨ä¸‰å…ƒç»„æŸå¤±(triplet loss)å’Œåœ¨çº¿åŠç¡¬è´Ÿé‡‡æ ·(online semi-hard negative mining)åœ¨å†å²æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå°†çˆ¶å­äº¤æ˜“çš„æ”¯å‡ºæ¨¡å¼ç›´æ¥ç¼–ç è¿›æ¨¡å‹å‚æ•°ä¸­ã€‚è¿™ç§æœºåˆ¶å®ç°äº†äº¤æ˜“çš„æ™ºèƒ½è·¯ç”±ï¼Œç¡®ä¿äº¤æ˜“è¢«å‡†ç¡®å¯¼å‘è‡³åŒ…å«å…¶çˆ¶ UTXO çš„éªŒè¯å¾®æœåŠ¡ï¼Œä»è€Œæ¶ˆé™¤äº†å®æ—¶çš„çˆ¶äº¤æ˜“æŸ¥æ‰¾éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†è·¨åˆ†ç‰‡é€šä¿¡å¼€é”€ï¼Œå¤§å¹…æå‡äº†åŒºå—é“¾ç³»ç»Ÿçš„ååé‡(throughput)ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 5 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.01614v1",
      "published_date": "2025-06-02 12:54:39 UTC",
      "updated_date": "2025-06-02 12:54:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:40:42.626929+00:00"
    },
    {
      "arxiv_id": "2506.01608v2",
      "title": "EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models",
      "title_zh": "EPFL-Smart-Kitchen-30ï¼šåŒ…å«ä¸‰ç»´è¿åŠ¨å­¦çš„ç¨ å¯†æ ‡æ³¨çƒ¹é¥ªæ•°æ®é›†ï¼Œæ—¨åœ¨æŒ‘æˆ˜è§†é¢‘ä¸è¯­è¨€æ¨¡å‹",
      "authors": [
        "Andy Bonnetto",
        "Haozhe Qi",
        "Franklin Leong",
        "Matea Tashkovska",
        "Mahdi Rad",
        "Solaiman Shokur",
        "Friedhelm Hummel",
        "Silvestro Micera",
        "Marc Pollefeys",
        "Alexander Mathis"
      ],
      "abstract": "Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† EPFL-Smart-Kitchen-30 æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨éä¾µå…¥å¼è¿åŠ¨æ•æ‰å¨æˆ¿ç¯å¢ƒä¸­æ”¶é›†çš„å¤§è§„æ¨¡ã€å¯†é›†æ ‡æ³¨çš„çƒ¹é¥ªæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åˆ©ç”¨ 9 ä¸ªé™æ€ RGB-D æ‘„åƒå¤´ã€æƒ¯æ€§æµ‹é‡å•å…ƒ (IMUs) å’Œ HoloLens 2 å¤´æˆ´è®¾å¤‡ï¼ŒåŒæ­¥è®°å½•äº† 16 åå—è¯•è€…åœ¨çƒ¹é¥ªå››ç§ä¸åŒé£Ÿè°±æ—¶çš„ä¸‰ç»´æ‰‹éƒ¨ã€èº«ä½“åŠçœ¼åŠ¨è½¨è¿¹ã€‚æ•°æ®é›†åŒ…å«æ€»è®¡ 29.7 å°æ—¶çš„å¤šè§†è§’ã€å¤šæ¨¡æ€è§†é¢‘ä¸è¿åŠ¨å­¦æ•°æ®ï¼Œå¹¶ä»¥æ¯åˆ†é’Ÿ 33.78 ä¸ªåŠ¨ä½œç‰‡æ®µçš„é«˜å¯†åº¦è¿›è¡Œäº†è¯¦ç»†æ ‡æ³¨ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†æ¶µç›–è§†è§‰è¯­è¨€ (vision-language)ã€è¯­ä¹‰æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆ (semantic text-to-motion generation)ã€å¤šæ¨¡æ€åŠ¨ä½œè¯†åˆ« (multi-modal action recognition) ä»¥åŠåŸºäºå§¿æ€çš„åŠ¨ä½œåˆ†å‰² (pose-based action segmentation) çš„å››å¤§åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨é€šè¿‡æä¾›çœŸå®ç”Ÿæ€ç¯å¢ƒä¸‹çš„è¡Œä¸ºæ•°æ®ï¼Œæ¨åŠ¨è¡Œä¸ºå»ºæ¨¡ä¸ç†è§£æŠ€æœ¯çš„å‘å±•ï¼Œå¹¶ä¸ºæŒ‘æˆ˜å’Œæå‡è§†é¢‘ä¸è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¥ å®šåŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.OT"
      ],
      "primary_category": "cs.CV",
      "comment": "Code and data at: https://github.com/amathislab/EPFL-Smart-Kitchen",
      "pdf_url": "https://arxiv.org/pdf/2506.01608v2",
      "published_date": "2025-06-02 12:46:44 UTC",
      "updated_date": "2025-08-25 13:46:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:40:45.956334+00:00"
    },
    {
      "arxiv_id": "2506.02085v1",
      "title": "Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion",
      "title_zh": "æ­ç¤ºéŸ³é¢‘æ·±åº¦ä¼ªé€ æ¥æºï¼šä¸€ç§åŸºäºæ·±åº¦åº¦é‡å­¦ä¹ ä¸ Conformer ç½‘ç»œçš„é›†æˆèåˆæ–¹æ³•",
      "authors": [
        "Ajinkya Kulkarni",
        "Sandipana Dowerah",
        "Tanel Alumae",
        "Mathew Magimai. -Doss"
      ],
      "abstract": "Audio deepfakes are acquiring an unprecedented level of realism with advanced AI. While current research focuses on discerning real speech from spoofed speech, tracing the source system is equally crucial. This work proposes a novel audio source tracing system combining deep metric multi-class N-pair loss with Real Emphasis and Fake Dispersion framework, a Conformer classification network, and ensemble score-embedding fusion. The N-pair loss improves discriminative ability, while Real Emphasis and Fake Dispersion enhance robustness by focusing on differentiating real and fake speech patterns. The Conformer network captures both global and local dependencies in the audio signal, crucial for source tracing. The proposed ensemble score-embedding fusion shows an optimal trade-off between in-domain and out-of-domain source tracing scenarios. We evaluate our method using Frechet Distance and standard metrics, demonstrating superior performance in source tracing over the baseline system.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³é¢‘æ·±åº¦ä¼ªé€ (Audio Deepfake)æ„ˆå‘é€¼çœŸå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨è¿½è¸ªä¼ªé€ éŸ³é¢‘æ¥æºç³»ç»Ÿçš„æ–°å‹æº¯æºç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç»“åˆäº†æ·±åº¦åº¦é‡å­¦ä¹ (Deep Metric Learning)ä¸­çš„å¤šç±»N-pair lossä¸çœŸå®å¼ºåŒ–ä¸ä¼ªé€ åˆ†æ•£(Real Emphasis and Fake Dispersion)æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†å¯¹ä¼ªé€ æ¨¡å¼çš„åˆ¤åˆ«èƒ½åŠ›ä¸é²æ£’æ€§ã€‚ç ”ç©¶åˆ©ç”¨Conformerç½‘ç»œåŒæ—¶æ•æ‰éŸ³é¢‘ä¿¡å·ä¸­çš„å…¨å±€å’Œå±€éƒ¨ä¾èµ–å…³ç³»ï¼Œä»¥ç²¾å‡†æå–æ¥æºç‰¹å¾ã€‚é€šè¿‡å¼•å…¥é›†æˆå¾—åˆ†-åµŒå…¥èåˆ(Ensemble Score-Embedding Fusion)ç­–ç•¥ï¼Œè¯¥æ–¹æ³•åœ¨åŸŸå†…(In-domain)ä¸è·¨åŸŸ(Out-of-domain)æº¯æºåœºæ™¯ä¹‹é—´å–å¾—äº†ç†æƒ³çš„å¹³è¡¡ã€‚å®éªŒé‡‡ç”¨Frechet DistanceåŠå¤šé¡¹æ ‡å‡†æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨æ¥æºè¿½è¸ªæ€§èƒ½ä¸Šå…¨é¢è¶…è¶Šäº†åŸºçº¿ç³»ç»Ÿï¼Œä¸ºè§£å†³éŸ³é¢‘å®‰å…¨é¢†åŸŸçš„æ¥æºè¯†åˆ«é—®é¢˜æä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at Interspeech 2025, Netherlands",
      "pdf_url": "https://arxiv.org/pdf/2506.02085v1",
      "published_date": "2025-06-02 12:42:09 UTC",
      "updated_date": "2025-06-02 12:42:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:40:45.463049+00:00"
    },
    {
      "arxiv_id": "2506.01600v1",
      "title": "WoMAP: World Models For Embodied Open-Vocabulary Object Localization",
      "title_zh": "WoMAPï¼šé¢å‘å…·èº«å¼€æ”¾è¯æ±‡ç›®æ ‡å®šä½çš„ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Tenny Yin",
        "Zhiting Mei",
        "Tao Sun",
        "Lihan Zha",
        "Emily Zhou",
        "Jeremy Bao",
        "Miyu Yamane",
        "Ola Shorinwa",
        "Anirudha Majumdar"
      ],
      "abstract": "Language-instructed active object localization is a critical challenge for robots, requiring efficient exploration of partially observable environments. However, state-of-the-art approaches either struggle to generalize beyond demonstration datasets (e.g., imitation learning methods) or fail to generate physically grounded actions (e.g., VLMs). To address these limitations, we introduce WoMAP (World Models for Active Perception): a recipe for training open-vocabulary object localization policies that: (i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data generation without the need for expert demonstrations, (ii) distills dense rewards signals from open-vocabulary object detectors, and (iii) leverages a latent world model for dynamics and rewards prediction to ground high-level action proposals at inference time. Rigorous simulation and hardware experiments demonstrate WoMAP's superior performance in a broad range of zero-shot object localization tasks, with more than 9x and 2x higher success rates compared to VLM and diffusion policy baselines, respectively. Further, we show that WoMAP achieves strong generalization and sim-to-real transfer on a TidyBot.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººå¼€æ”¾è¯æ±‡ç›®æ ‡å®šä½åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­é¢ä¸´çš„æ³›åŒ–æ€§å·®å’ŒåŠ¨ä½œæ¥åœ°(grounding)ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†WoMAPï¼ˆWorld Models for Active Perceptionï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäºGaussian Splattingçš„real-to-sim-to-realæµæ°´çº¿ï¼Œåœ¨æ— éœ€ä¸“å®¶æ¼”ç¤ºçš„æƒ…å†µä¸‹å®ç°äº†å¯æ‰©å±•çš„æ•°æ®ç”Ÿæˆï¼Œå¹¶ä»å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹å™¨ä¸­è’¸é¦å‡ºå¯†é›†å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡å¼•å…¥æ½œç©ºé—´ä¸–ç•Œæ¨¡å‹(latent world model)æ¥é¢„æµ‹åŠ¨åŠ›å­¦å’Œå¥–åŠ±ï¼ŒWoMAPæˆåŠŸåœ¨æ¨ç†é˜¶æ®µå®ç°äº†é«˜å±‚åŠ¨ä½œå»ºè®®çš„ç‰©ç†æ¥åœ°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWoMAPåœ¨é›¶æ ·æœ¬(zero-shot)ç›®æ ‡å®šä½ä»»åŠ¡ä¸­çš„è¡¨ç°è¿œè¶…ç°æœ‰åŸºçº¿ï¼Œå…¶æˆåŠŸç‡æ¯”VLMå’Œæ‰©æ•£ç­–ç•¥(diffusion policy)åŸºçº¿åˆ†åˆ«é«˜å‡º9å€å’Œ2å€ä»¥ä¸Šã€‚æœ€åï¼Œè¯¥ç ”ç©¶é€šè¿‡TidyBotå®æœºå®éªŒè¿›ä¸€æ­¥è¯æ˜äº†WoMAPå…·å¤‡æå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œæ˜¾è‘—çš„sim-to-realè¿ç§»æ•ˆæœã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01600v1",
      "published_date": "2025-06-02 12:35:14 UTC",
      "updated_date": "2025-06-02 12:35:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:41:06.090933+00:00"
    },
    {
      "arxiv_id": "2506.01597v1",
      "title": "Policy Newton Algorithm in Reproducing Kernel Hilbert Space",
      "title_zh": "å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­çš„ç­–ç•¥ç‰›é¡¿ç®—æ³•",
      "authors": [
        "Yixian Zhang",
        "Huaze Tang",
        "Chao Wang",
        "Wenbo Ding"
      ],
      "abstract": "Reinforcement learning (RL) policies represented in Reproducing Kernel Hilbert Spaces (RKHS) offer powerful representational capabilities. While second-order optimization methods like Newton's method demonstrate faster convergence than first-order approaches, current RKHS-based policy optimization remains constrained to first-order techniques. This limitation stems primarily from the intractability of explicitly computing and inverting the infinite-dimensional Hessian operator in RKHS. We introduce Policy Newton in RKHS, the first second-order optimization framework specifically designed for RL policies represented in RKHS. Our approach circumvents direct computation of the inverse Hessian operator by optimizing a cubic regularized auxiliary objective function. Crucially, we leverage the Representer Theorem to transform this infinite-dimensional optimization into an equivalent, computationally tractable finite-dimensional problem whose dimensionality scales with the trajectory data volume. We establish theoretical guarantees proving convergence to a local optimum with a local quadratic convergence rate. Empirical evaluations on a toy financial asset allocation problem validate these theoretical properties, while experiments on standard RL benchmarks demonstrate that Policy Newton in RKHS achieves superior convergence speed and higher episodic rewards compared to established first-order RKHS approaches and parametric second-order methods. Our work bridges a critical gap between non-parametric policy representations and second-order optimization methods in reinforcement learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Policy Newton in RKHSï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ä¸ºå†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´(Reproducing Kernel Hilbert Space, RKHS)ä¸­è¡¨ç¤ºçš„å¼ºåŒ–å­¦ä¹ (RL)ç­–ç•¥è®¾è®¡çš„äºŒé˜¶ä¼˜åŒ–æ¡†æ¶ã€‚é’ˆå¯¹RKHSä¸­æ— é™ç»´Hessianç®—å­ç›´æ¥è®¡ç®—ä¸æ±‚é€†çš„ä¸å¯è¡Œæ€§ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–ä¸‰æ¬¡æ­£åˆ™åŒ–è¾…åŠ©ç›®æ ‡å‡½æ•°(cubic regularized auxiliary objective function)äºˆä»¥è§„é¿ã€‚ç ”ç©¶åˆ©ç”¨è¡¨ç¤ºå®šç†(Representer Theorem)å°†æ— é™ç»´ä¼˜åŒ–è½¬æ¢ä¸ºè®¡ç®—å¯è¡Œçš„æœ‰é™ç»´é—®é¢˜ï¼Œå…¶ç»´åº¦éšè½¨è¿¹æ•°æ®é‡ç¼©æ”¾ã€‚ç†è®ºåˆ†æè¯æ˜è¯¥ç®—æ³•å…·æœ‰å±€éƒ¨äºŒæ¬¡æ”¶æ•›ç‡(local quadratic convergence rate)å¹¶èƒ½æ”¶æ•›è‡³å±€éƒ¨æœ€ä¼˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPolicy Newton in RKHSåœ¨æ”¶æ•›é€Ÿåº¦å’Œå›åˆå¥–åŠ±(episodic rewards)æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ä¸€é˜¶RKHSæ–¹æ³•åŠå‚æ•°åŒ–äºŒé˜¶æ–¹æ³•ã€‚è¯¥æˆæœæœ‰æ•ˆå¡«è¡¥äº†å¼ºåŒ–å­¦ä¹ ä¸­éå‚æ•°ç­–ç•¥è¡¨ç¤ºä¸äºŒé˜¶ä¼˜åŒ–æ–¹æ³•ä¹‹é—´çš„å…³é”®ç©ºç™½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01597v1",
      "published_date": "2025-06-02 12:32:52 UTC",
      "updated_date": "2025-06-02 12:32:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:41:04.601503+00:00"
    },
    {
      "arxiv_id": "2506.01596v1",
      "title": "Understanding and Improving Laplacian Positional Encodings For Temporal GNNs",
      "title_zh": "ç†è§£ä¸æ”¹è¿›æ—¶åºå›¾ç¥ç»ç½‘ç»œçš„æ‹‰æ™®æ‹‰æ–¯ä½ç½®ç¼–ç ",
      "authors": [
        "Yaniv Galron",
        "Fabrizio Frasca",
        "Haggai Maron",
        "Eran Treister",
        "Moshe Eliasof"
      ],
      "abstract": "Temporal graph learning has applications in recommendation systems, traffic forecasting, and social network analysis. Although multiple architectures have been introduced, progress in positional encoding for temporal graphs remains limited. Extending static Laplacian eigenvector approaches to temporal graphs through the supra-Laplacian has shown promise, but also poses key challenges: high eigendecomposition costs, limited theoretical understanding, and ambiguity about when and how to apply these encodings. In this paper, we address these issues by (1) offering a theoretical framework that connects supra-Laplacian encodings to per-time-slice encodings, highlighting the benefits of leveraging additional temporal connectivity, (2) introducing novel methods to reduce the computational overhead, achieving up to 56x faster runtimes while scaling to graphs with 50,000 active nodes, and (3) conducting an extensive experimental study to identify which models, tasks, and datasets benefit most from these encodings. Our findings reveal that while positional encodings can significantly boost performance in certain scenarios, their effectiveness varies across different models.",
      "tldr_zh": "è¯¥ç ”ç©¶å…³æ³¨äºæ”¹è¿›æ—¶é—´å›¾ç¥ç»ç½‘ç»œ(Temporal GNNs)ä¸­çš„æ‹‰æ™®æ‹‰æ–¯ä½ç½®ç¼–ç (Laplacian Positional Encodings)ï¼Œæ—¨åœ¨è§£å†³supra-Laplacianæ–¹æ³•åœ¨è®¡ç®—æˆæœ¬é«˜ã€ç†è®ºç†è§£ä¸è¶³ä»¥åŠåº”ç”¨åœºæ™¯æ¨¡ç³Šç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªå°†supra-Laplacianç¼–ç ä¸é€æ—¶é—´ç‰‡ç¼–ç (per-time-slice encodings)ç›¸è”ç³»çš„ç†è®ºæ¡†æ¶ï¼Œé˜æ˜äº†åˆ©ç”¨é¢å¤–æ—¶é—´è¿æ¥æ€§çš„ä¼˜åŠ¿ã€‚ä¸ºäº†é™ä½è®¡ç®—å¼€é”€ï¼Œç ”ç©¶å¼•å…¥äº†æ–°å‹è®¡ç®—æ–¹æ³•ï¼Œåœ¨æ”¯æŒ50,000ä¸ªæ´»è·ƒèŠ‚ç‚¹çš„åŒæ—¶å®ç°äº†é«˜è¾¾56å€çš„è¿è¡ŒåŠ é€Ÿã€‚é€šè¿‡å¯¹å¤šç§æ¨¡å‹ã€ä»»åŠ¡å’Œæ•°æ®é›†çš„å¹¿æ³›å®éªŒï¼Œç ”ç©¶è¯†åˆ«äº†è¿™äº›ç¼–ç æœ€èƒ½å‘æŒ¥ä½œç”¨çš„ç‰¹å®šåœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä½ç½®ç¼–ç åœ¨ç‰¹å®šæƒ…å†µä¸‹èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œä½†å…¶æœ‰æ•ˆæ€§åœ¨ä¸åŒæ¶æ„ä¹‹é—´è¡¨ç°å‡ºæ˜æ˜¾çš„å·®å¼‚æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ECML-PKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01596v1",
      "published_date": "2025-06-02 12:30:58 UTC",
      "updated_date": "2025-06-02 12:30:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:41:05.604221+00:00"
    },
    {
      "arxiv_id": "2506.01584v1",
      "title": "VirnyFlow: A Design Space for Responsible Model Development",
      "title_zh": "VirnyFlowï¼šä¸€ç§ç”¨äºè´Ÿè´£ä»»æ¨¡å‹å¼€å‘çš„è®¾è®¡ç©ºé—´",
      "authors": [
        "Denys Herasymuk",
        "Nazar Protsiv",
        "Julia Stoyanovich"
      ],
      "abstract": "Developing machine learning (ML) models requires a deep understanding of real-world problems, which are inherently multi-objective. In this paper, we present VirnyFlow, the first design space for responsible model development, designed to assist data scientists in building ML pipelines that are tailored to the specific context of their problem. Unlike conventional AutoML frameworks, VirnyFlow enables users to define customized optimization criteria, perform comprehensive experimentation across pipeline stages, and iteratively refine models in alignment with real-world constraints. Our system integrates evaluation protocol definition, multi-objective Bayesian optimization, cost-aware multi-armed bandits, query optimization, and distributed parallelism into a unified architecture. We show that VirnyFlow significantly outperforms state-of-the-art AutoML systems in both optimization quality and scalability across five real-world benchmarks, offering a flexible, efficient, and responsible alternative to black-box automation in ML development.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†VirnyFlowï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè´Ÿè´£ä»»æ¨¡å‹å¼€å‘ï¼ˆresponsible model developmentï¼‰çš„è®¾è®¡ç©ºé—´ï¼Œæ—¨åœ¨å¸®åŠ©æ•°æ®ç§‘å­¦å®¶æ„å»ºå¥‘åˆå…·ä½“é—®é¢˜èƒŒæ™¯çš„æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼ˆML pipelinesï¼‰ã€‚ä¸ä¼ ç»ŸAutoMLæ¡†æ¶ä¸åŒï¼ŒVirnyFlowå…è®¸ç”¨æˆ·å®šä¹‰è‡ªå®šä¹‰ä¼˜åŒ–å‡†åˆ™ï¼Œå¹¶åœ¨æµæ°´çº¿çš„ä¸åŒé˜¶æ®µè¿›è¡Œå…¨é¢å®éªŒåŠè¿­ä»£ä¼˜åŒ–ï¼Œä»¥å¯¹é½ç°å®çº¦æŸã€‚è¯¥ç³»ç»Ÿå°†å¤šç›®æ ‡è´å¶æ–¯ä¼˜åŒ–ï¼ˆmulti-objective Bayesian optimizationï¼‰ã€æˆæœ¬æ„ŸçŸ¥å¤šè‡‚è€è™æœºï¼ˆcost-aware multi-armed banditsï¼‰ã€æŸ¥è¯¢ä¼˜åŒ–ï¼ˆquery optimizationï¼‰å’Œåˆ†å¸ƒå¼å¹¶è¡Œï¼ˆdistributed parallelismï¼‰é›†æˆåˆ°äº†ç»Ÿä¸€çš„æ¶æ„ä¸­ã€‚å®éªŒè¯æ˜ï¼ŒVirnyFlowåœ¨äº”ä¸ªçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨ä¼˜åŒ–è´¨é‡å’Œå¯æ‰©å±•æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„AutoMLç³»ç»Ÿã€‚è¿™ä¸€æˆæœä¸ºæœºå™¨å­¦ä¹ å¼€å‘æä¾›äº†ä¸€ä¸ªçµæ´»ã€é«˜æ•ˆä¸”è´Ÿè´£ä»»çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæœ‰æ•ˆè§£å†³äº†é»‘ç›’è‡ªåŠ¨åŒ–å¸¦æ¥çš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01584v1",
      "published_date": "2025-06-02 12:16:48 UTC",
      "updated_date": "2025-06-02 12:16:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:41:25.751476+00:00"
    },
    {
      "arxiv_id": "2506.01583v2",
      "title": "FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens",
      "title_zh": "FreqPolicyï¼šåŸºäºè¿ç»­æ ‡è®°çš„é¢‘åŸŸè‡ªå›å½’è§†è§‰è¿åŠ¨ç­–ç•¥",
      "authors": [
        "Yiming Zhong",
        "Yumeng Liu",
        "Chuyang Xiao",
        "Zemin Yang",
        "Youzhuo Wang",
        "Yufei Zhu",
        "Ye Shi",
        "Yujing Sun",
        "Xinge Zhu",
        "Yuexin Ma"
      ],
      "abstract": "Learning effective visuomotor policies for robotic manipulation is challenging, as it requires generating precise actions while maintaining computational efficiency. Existing methods remain unsatisfactory due to inherent limitations in the essential action representation and the basic network architectures. We observe that representing actions in the frequency domain captures the structured nature of motion more effectively: low-frequency components reflect global movement patterns, while high-frequency components encode fine local details. Additionally, robotic manipulation tasks of varying complexity demand different levels of modeling precision across these frequency bands. Motivated by this, we propose a novel paradigm for visuomotor policy learning that progressively models hierarchical frequency components. To further enhance precision, we introduce continuous latent representations that maintain smoothness and continuity in the action space. Extensive experiments across diverse 2D and 3D robotic manipulation benchmarks demonstrate that our approach outperforms existing methods in both accuracy and efficiency, showcasing the potential of a frequency-domain autoregressive framework with continuous tokens for generalized robotic manipulation.Code is available at https://github.com/4DVLab/Freqpolicy",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FreqPolicyï¼Œä¸€ç§åŸºäºé¢‘ç‡è‡ªå›å½’ (Frequency Autoregressive) çš„è§†è§‰è¿åŠ¨ç­–ç•¥ (Visuomotor Policy)ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­é«˜ç²¾åº¦åŠ¨ä½œç”Ÿæˆä¸è®¡ç®—æ•ˆç‡å¹³è¡¡çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨é¢‘åŸŸ (Frequency Domain) è¡¨ç¤ºåŠ¨ä½œæ¥æœ‰æ•ˆæ•æ‰è¿åŠ¨çš„ç»“æ„åŒ–ç‰¹å¾ï¼Œåˆ©ç”¨ä½é¢‘åˆ†é‡åæ˜ å…¨å±€è¿åŠ¨æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨é«˜é¢‘åˆ†é‡ç¼–ç ç²¾ç»†çš„å±€éƒ¨ç»†èŠ‚ã€‚ä¸ºäº†å¢å¼ºå»ºæ¨¡çš„ç²¾ç¡®åº¦ï¼ŒFreqPolicy å¼•å…¥äº†è¿ç»­æ½œå˜é‡è¡¨ç¤º (Continuous Latent Representations) ä»¥ä¿æŒåŠ¨ä½œç©ºé—´çš„å¹³æ»‘æ€§ä¸è¿ç»­æ€§ï¼Œå¹¶é‡‡ç”¨é€æ­¥å»ºæ¨¡åˆ†å±‚é¢‘ç‡åˆ†é‡çš„åˆ›æ–°èŒƒå¼ã€‚åœ¨å¤šç§ 2D å’Œ 3D æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œå……åˆ†å±•ç¤ºäº†ç»“åˆè¿ç»­æ ‡è®° (Continuous Tokens) çš„é¢‘åŸŸè‡ªå›å½’æ¡†æ¶åœ¨å®ç°é€šç”¨æœºå™¨äººæ“ä½œæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Comments: Published at Neural Information Processing Systems (NeurIPS) 2025. Project page and code: https://freq-policy.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.01583v2",
      "published_date": "2025-06-02 12:13:51 UTC",
      "updated_date": "2025-10-04 08:48:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:41:06.250816+00:00"
    },
    {
      "arxiv_id": "2506.01572v1",
      "title": "Advanced Nanostructured Topical Therapeutics for Psoriasis: Strategic Synthesis, Multimodal Characterization, and Preliminary Pharmacodynamic Profiling",
      "title_zh": "é“¶å±‘ç—…å…ˆè¿›çº³ç±³ç»“æ„å¤–ç”¨æ²»ç–—è¯ç‰©ï¼šç­–ç•¥æ€§åˆæˆã€å¤šæ¨¡æ€è¡¨å¾åŠåˆæ­¥è¯æ•ˆå­¦è¯„ä»·",
      "authors": [
        "Iqra Yousaf",
        "Aqsa Yousaf"
      ],
      "abstract": "Psoriasis is a long-term inflammatory skin disease that remains difficult to treat. In this study, we developed a new topical treatment by combining metal oxide nanoparticles: cerium oxide (CeO2), zinc oxide (ZnO), and silver (Ag), with natural plant extracts in a gel made from fish collagen and agar. The nanoparticles were characterized using UV-Vis spectroscopy, dynamic light scattering (DLS), Fourier-transform infrared spectroscopy (FTIR), and scanning electron microscopy (SEM), showing good stability and a uniform particle size distribution (ZnO averaged 66 nm).\n  To enhance therapeutic potential, the gel was enriched with plant-derived antioxidants from bitter melon, ginger, and neem. This formulation was tested on an animal model of psoriasis. The treated group exhibited faster wound healing and reduced inflammation compared to both placebo and untreated groups, with statistically significant results (p < 0.01 to p < 0.001) observed from Day 3, becoming more pronounced by Day 14.\n  These results indicate that the combination of nanoparticles with plant-based components in a topical gel may provide a promising new approach to psoriasis treatment. Further studies are recommended to evaluate long-term safety and therapeutic effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§é’ˆå¯¹é“¶å±‘ç—…(Psoriasis)çš„æ–°å‹çº³ç±³ç»“æ„å±€éƒ¨æ²»ç–—è¯ç‰©ï¼Œé€šè¿‡å°†æ°§åŒ–é“ˆ(CeO2)ã€æ°§åŒ–é”Œ(ZnO)å’Œé“¶(Ag)çº³ç±³é¢—ç²’ä¸å¤©ç„¶æ¤ç‰©æå–ç‰©ï¼ˆè‹¦ç“œã€ç”Ÿå§œå’Œå°æ¥ï¼‰ç»“åˆï¼Œæ•´åˆäºé±¼èƒ¶åŸè›‹ç™½å’Œç¼è„‚ç»„æˆçš„å‡èƒ¶åŸºè´¨ä¸­ã€‚åˆ©ç”¨UV-Visã€DLSã€FTIRå’ŒSEMç­‰å¤šæ¨¡æ€æŠ€æœ¯å¯¹çº³ç±³é¢—ç²’è¿›è¡Œäº†ç³»ç»Ÿè¡¨å¾ï¼Œç»“æœæ˜¾ç¤ºåˆ¶å‰‚å…·æœ‰è‰¯å¥½çš„ç¨³å®šæ€§å’Œå‡åŒ€çš„ç²’å¾„åˆ†å¸ƒï¼Œå…¶ä¸­ZnOå¹³å‡ç²’å¾„ä¸º66 nmã€‚åŠ¨ç‰©æ¨¡å‹è¯æ•ˆå­¦ç ”ç©¶è¡¨æ˜ï¼Œè¯¥åˆ¶å‰‚èƒ½æ˜¾è‘—åŠ é€Ÿä¼¤å£æ„ˆåˆå¹¶å‡è½»ç‚ç—‡ååº”ã€‚å®éªŒæ•°æ®è¯å®ï¼Œæ²»ç–—ç»„åœ¨ç»Ÿè®¡å­¦ä¸Šæ˜¾è‘—ä¼˜äºå¯¹ç…§ç»„(p < 0.01è‡³p < 0.001)ï¼Œä¸”ç–—æ•ˆä»ç¬¬3å¤©èµ·å³æœ‰æ˜¾ç°å¹¶åœ¨ç¬¬14å¤©æ„ˆå‘æ˜¾è‘—ã€‚è¯¥ç ”ç©¶è¯æ˜äº†çº³ç±³é¢—ç²’ä¸æ¤ç‰©æºæŠ—æ°§åŒ–æˆåˆ†è”ç”¨çš„ååŒæ²»ç–—æ½œåŠ›ï¼Œä¸ºé“¶å±‘ç—…çš„ä¸´åºŠæ²»ç–—æä¾›äº†ä¸€ç§æå…·å‰æ™¯çš„æ–°ç­–ç•¥ã€‚",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "physics.bio-ph"
      ],
      "primary_category": "physics.med-ph",
      "comment": "24 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.01572v1",
      "published_date": "2025-06-02 11:56:19 UTC",
      "updated_date": "2025-06-02 11:56:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:41:33.023102+00:00"
    },
    {
      "arxiv_id": "2506.01566v1",
      "title": "FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing",
      "title_zh": "FlexiSAGAï¼šé¢å‘ç¨€ç–ä¸ç¨ å¯†å¤„ç†çš„çµæ´»è„‰åŠ¨é˜µåˆ—GEMMåŠ é€Ÿå™¨",
      "authors": [
        "Mika Markus MÃ¼ller",
        "Konstantin LÃ¼beck",
        "Alexander Louis-Ferdinand Jung",
        "Jannik Steinmetz",
        "Oliver Bringmann"
      ],
      "abstract": "Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs), have become an important tool for a wide range of applications, from computer vision to natural language processing. However, the computational complexity of DNN inference poses a significant challenge, particularly for processing on resource-constrained edge devices. One promising approach to address this challenge is the exploitation of sparsity in DNN operator weights.\n  In this work, we present FlexiSAGA, an architecturally configurable and dataflow-flexible AI hardware accelerator for the sparse and dense processing of general matrix multiplications (GEMMs). FlexiSAGA supports seven different sparse and dense dataflows, enabling efficient processing of resource intensive DNN operators. Additionally, we propose a DNN pruning method specifically tailored towards the FlexiSAGA architecture, allowing for near-optimal processing of dense and sparse convolution and fully-connected operators, facilitating a DNN/HW co-design flow. Our results show a whole DNN sparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming commercial and literature-reported accelerator platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FlexiSAGAï¼Œä¸€ç§æ¶æ„å¯é…ç½®ä¸”æ•°æ®æµçµæ´»çš„ AI ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†é€šç”¨çŸ©é˜µä¹˜æ³• (GEMM) ä¸­çš„ç¨€ç–å’Œç¨ å¯†è®¡ç®—ã€‚é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡ä¸Šæ·±åº¦ç¥ç»ç½‘ç»œ (DNN) æ¨ç†çš„å¤æ‚æ€§æŒ‘æˆ˜ï¼ŒFlexiSAGA æ”¯æŒä¸ƒç§ä¸åŒçš„ç¨€ç–ä¸ç¨ å¯†æ•°æ®æµ (dataflows)ï¼Œèƒ½å¤Ÿçµæ´»é€‚é…å¤šç§èµ„æºå¯†é›†å‹ç®—å­ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ä¸“é—¨é’ˆå¯¹è¯¥ç¡¬ä»¶æ¶æ„å®šåˆ¶çš„å‰ªæ (pruning) æ–¹æ³•ï¼Œé€šè¿‡ç¡¬ä»¶ä¸ç®—æ³•ååŒè®¾è®¡ (co-design) å®ç°äº†å¯¹å·ç§¯å’Œå…¨è¿æ¥å±‚çš„è¿‘ä¼˜å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlexiSAGA åœ¨å®Œæ•´ DNN ç¨€ç–æ¨ç†ä»»åŠ¡ä¸­ç›¸æ¯”ç¨ å¯†æ¨¡å¼å®ç°äº† 1.41 è‡³ 4.28 å€çš„åŠ é€Ÿï¼Œå…¶æ€§èƒ½è¡¨ç°è¶…è¶Šäº†ç›®å‰å·²æœ‰çš„å•†ä¸šåŠå­¦æœ¯ç•Œç¡¬ä»¶åŠ é€Ÿå¹³å°ã€‚",
      "categories": [
        "cs.PF",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.PF",
      "comment": "Accepted Version for: SAMOS XXV",
      "pdf_url": "https://arxiv.org/pdf/2506.01566v1",
      "published_date": "2025-06-02 11:45:37 UTC",
      "updated_date": "2025-06-02 11:45:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:02.589350+00:00"
    },
    {
      "arxiv_id": "2506.01551v3",
      "title": "EvolveNav: Empowering LLM-Based Vision-Language Navigation via Self-Improving Embodied Reasoning",
      "title_zh": "EvolveNavï¼šé€šè¿‡è‡ªè¿›åŒ–å…·èº«æ¨ç†èµ‹èƒ½åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­è¨€å¯¼èˆª",
      "authors": [
        "Bingqian Lin",
        "Yunshuang Nie",
        "Khun Loun Zai",
        "Ziming Wei",
        "Mingfei Han",
        "Rongtao Xu",
        "Minzhe Niu",
        "Jianhua Han",
        "Hanwang Zhang",
        "Liang Lin",
        "Bokui Chen",
        "Cewu Lu",
        "Xiaodan Liang"
      ],
      "abstract": "Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for enhancing vision-language navigation (VLN) performance, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches predominantly adopt straightforward input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. To address these issues, we propose EvolveNav, a novel sElf-improving embodied reasoning paradigm that realizes adaptable and generalizable navigational reasoning for boosting LLM-based vision-language Navigation. Specifically, EvolveNav involves a two-stage training process: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with curated formalized CoT labels to first activate the model's navigational reasoning capabilities, and simultaneously increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also designed to encourage the model to learn correct reasoning patterns by contrasting with wrong ones. Experimental results under both task-specific and cross-task training paradigms demonstrate the consistent superiority of EvolveNav over previous LLM-based VLN approaches on various popular benchmarks, including R2R, REVERIE, CVDN, and SOON. Code is available at https://github.com/expectorlin/EvolveNav.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EvolveNavï¼Œä¸€ç§é€šè¿‡è‡ªæˆ‘æå‡å…·èº«æ¨ç†ï¼ˆSelf-Improving Embodied Reasoningï¼‰æ¥å¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVision-Language Navigation, VLNï¼‰æ€§èƒ½çš„æ–°èŒƒå¼ã€‚é’ˆå¯¹ç°æœ‰VLNæ–¹æ³•ä¸­å¯¼èˆªå†³ç­–è§£é‡Šæ€§å·®ä»¥åŠé«˜è´¨é‡é“¾å¼æ€ç»´ï¼ˆChain-of-Thought, CoTï¼‰æ ‡ç­¾ç¼ºå¤±å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼ŒEvolveNavé‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡å½¢å¼åŒ–CoTç›‘ç£å¾®è°ƒï¼ˆFormalized CoT Supervised Fine-Tuningï¼‰æ¿€æ´»æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¹¶æå‡è¿è¡Œé€Ÿåº¦ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨è‡ªæˆ‘åæ€åè®­ç»ƒï¼ˆSelf-Reflective Post-Trainingï¼‰å°†æ¨¡å‹è‡ªèº«çš„æ¨ç†è¾“å‡ºä½œä¸ºè‡ªä¸°å¯Œæ ‡ç­¾ï¼Œå¹¶é€šè¿‡å¯¹æ¯”æ­£ç¡®ä¸é”™è¯¯æ¨ç†æ¨¡å¼çš„è¾…åŠ©ä»»åŠ¡æ¥å¢å¼ºå­¦ä¹ çš„æ³›åŒ–æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEvolveNavåœ¨R2Rã€REVERIEã€CVDNå’ŒSOONç­‰å¤šä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸Šå‡æ˜¾è‘—ä¼˜äºä»¥å¾€çš„LLMå¯¼èˆªæ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ä»…æé«˜äº†å¤æ‚åœºæ™¯ä¸‹çš„å¯¼èˆªå‡†ç¡®ç‡ï¼Œè¿˜ä¸ºæ„å»ºå¯è§£é‡Šä¸”å…·æ³›åŒ–èƒ½åŠ›çš„å…·èº«æ¨ç†ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01551v3",
      "published_date": "2025-06-02 11:28:32 UTC",
      "updated_date": "2025-10-14 02:26:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:06.886336+00:00"
    },
    {
      "arxiv_id": "2506.01539v1",
      "title": "G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models",
      "title_zh": "G4Segï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„ä¸ç²¾ç¡®åˆ†å‰²ç»†åŒ–ç”Ÿæˆæ–¹æ³•",
      "authors": [
        "Tianjiao Zhang",
        "Fei Zhang",
        "Jiangchao Yao",
        "Ya Zhang",
        "Yanfeng Wang"
      ],
      "abstract": "This paper considers the problem of utilizing a large-scale text-to-image diffusion model to tackle the challenging Inexact Segmentation (IS) task. Unlike traditional approaches that rely heavily on discriminative-model-based paradigms or dense visual representations derived from internal attention mechanisms, our method focuses on the intrinsic generative priors in Stable Diffusion~(SD). Specifically, we exploit the pattern discrepancies between original images and mask-conditional generated images to facilitate a coarse-to-fine segmentation refinement by establishing a semantic correspondence alignment and updating the foreground probability. Comprehensive quantitative and qualitative experiments validate the effectiveness and superiority of our plug-and-play design, underscoring the potential of leveraging generation discrepancies to model dense representations and encouraging further exploration of generative approaches for solving discriminative tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† G4Segï¼Œä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹æ¥è§£å†³æŒ‘æˆ˜æ€§ Inexact Segmentation (IS) ä»»åŠ¡çš„æ–°æ–¹æ³•ã€‚ä¸ä»¥å¾€ä¾èµ–åˆ¤åˆ«å¼æ¨¡å‹èŒƒå¼æˆ–å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶çš„å¯†é›†è§†è§‰è¡¨å¾æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é‡ç‚¹æŒ–æ˜ Stable Diffusion (SD) çš„å†…åœ¨ç”Ÿæˆå…ˆéªŒã€‚å…·ä½“è€Œè¨€ï¼ŒG4Seg åˆ©ç”¨åŸå§‹å›¾åƒä¸æ©ç æ¡ä»¶ç”Ÿæˆçš„å›¾åƒä¹‹é—´çš„æ¨¡å¼å·®å¼‚ï¼Œé€šè¿‡å»ºç«‹è¯­ä¹‰å¯¹åº”å¯¹é½å¹¶æ›´æ–°å‰æ™¯æ¦‚ç‡ï¼Œå®ç°äº†ä»ç²—åˆ°ç»†çš„åˆ†å‰²ç»†åŒ–ã€‚å¤§é‡çš„å®šé‡å’Œå®šæ€§å®éªŒéªŒè¯äº†è¿™ç§ plug-and-play è®¾è®¡çš„æœ‰æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚è¯¥æˆæœçªæ˜¾äº†åˆ©ç”¨ç”Ÿæˆå·®å¼‚å»ºæ¨¡å¯†é›†è¡¨å¾çš„æ½œåŠ›ï¼Œä¸ºåˆ©ç”¨ç”Ÿæˆå¼æ–¹æ³•è§£å†³åˆ¤åˆ«å¼ä»»åŠ¡æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 12 figures, IEEE International Conference on Multimedia & Expo 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01539v1",
      "published_date": "2025-06-02 11:05:28 UTC",
      "updated_date": "2025-06-02 11:05:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:11.689363+00:00"
    },
    {
      "arxiv_id": "2506.01538v2",
      "title": "LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation",
      "title_zh": "LAMARLï¼šç”¨äºåä½œç­–ç•¥ç”Ÿæˆçš„å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Guobin Zhu",
        "Rui Zhou",
        "Wenkang Ji",
        "Shiyu Zhao"
      ],
      "abstract": "Although Multi-Agent Reinforcement Learning (MARL) is effective for complex multi-robot tasks, it suffers from low sample efficiency and requires iterative manual reward tuning. Large Language Models (LLMs) have shown promise in single-robot settings, but their application in multi-robot systems remains largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL) approach, which integrates MARL with LLMs, significantly enhancing sample efficiency without requiring manual design. LAMARL consists of two modules: the first module leverages LLMs to fully automate the generation of prior policy and reward functions. The second module is MARL, which uses the generated functions to guide robot policy training effectively. On a shape assembly benchmark, both simulation and real-world experiments demonstrate the unique advantages of LAMARL. Ablation studies show that the prior policy improves sample efficiency by an average of 185.9% and enhances task completion, while structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM output success rates by 28.5%-67.5%. Videos and code are available at https://windylab.github.io/LAMARL/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LAMARL (LLM-Aided Multi-Agent Reinforcement Learning)ï¼Œæ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL) åœ¨å¤æ‚å¤šæœºå™¨äººä»»åŠ¡ä¸­é¢ä¸´çš„é‡‡æ ·æ•ˆç‡ä½å’Œæ‰‹åŠ¨å¥–åŠ±è°ƒä¼˜ç¹çç­‰æŒ‘æˆ˜ã€‚LAMARL æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šé¦–å…ˆåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å…¨è‡ªåŠ¨ç”Ÿæˆå…ˆéªŒç­–ç•¥ (prior policy) å’Œå¥–åŠ±å‡½æ•° (reward functions)ï¼Œéšååœ¨ MARL è®­ç»ƒé˜¶æ®µåˆ©ç”¨è¿™äº›ç”Ÿæˆçš„å‡½æ•°æœ‰æ•ˆæŒ‡å¯¼æœºå™¨äººç­–ç•¥å­¦ä¹ ã€‚åœ¨å½¢çŠ¶ç»„è£… (shape assembly) åŸºå‡†æµ‹è¯•çš„ä»¿çœŸå’Œå®ç‰©å®éªŒä¸­ï¼ŒLAMARL å±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå…ˆéªŒç­–ç•¥å°†é‡‡æ ·æ•ˆç‡å¹³å‡æå‡äº† 185.9%ï¼Œè€Œé€šè¿‡ç»“åˆé“¾å¼æ€ç»´ (Chain-of-Thought) å’ŒåŸºç¡€ API çš„ç»“æ„åŒ–æç¤ºè¯ï¼ŒLLM çš„è¾“å‡ºæˆåŠŸç‡æå‡äº† 28.5% è‡³ 67.5%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å°†å¤§è¯­è¨€æ¨¡å‹ä¸å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç»“åˆåœ¨æé«˜åä½œç­–ç•¥ç”Ÿæˆæ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by IEEE Robotics and Automation Letters",
      "pdf_url": "https://arxiv.org/pdf/2506.01538v2",
      "published_date": "2025-06-02 10:59:54 UTC",
      "updated_date": "2025-06-03 07:53:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:07.059356+00:00"
    },
    {
      "arxiv_id": "2506.02083v1",
      "title": "LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention",
      "title_zh": "LASPAï¼šåŸºäºå‰ç¼€å¾®è°ƒäº¤å‰æ³¨æ„åŠ›çš„è¯­è¨€æ— å…³è¯´è¯äººè§£è€¦",
      "authors": [
        "Aditya Srinivas Menon",
        "Raj Prakash Gohil",
        "Kumud Tripathi",
        "Pankaj Wasnik"
      ],
      "abstract": "Speaker recognition models face challenges in multi-lingual settings due to the entanglement of linguistic information within speaker embeddings. The overlap between vocal traits such as accent, vocal anatomy, and a language's phonetic structure complicates separating linguistic and speaker information. Disentangling these components can significantly improve speaker recognition accuracy. To this end, we propose a novel disentanglement learning strategy that integrates joint learning through prefix-tuned cross-attention. This approach is particularly effective when speakers switch between languages. Experimental results show the model generalizes across monolingual and multi-lingual settings, including unseen languages. Notably, the proposed model improves the equal error rate across multiple datasets, highlighting its ability to separate language information from speaker embeddings and enhance recognition in diverse linguistic conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè¯­è¨€ç¯å¢ƒä¸‹è¯´è¯äººè¯†åˆ«ä¸­è¯­è¨€ä¿¡æ¯ä¸è¯´è¯äººåµŒå…¥(Speaker Embeddings)ç›¸äº’äº¤ç»‡(Entanglement)çš„éš¾é¢˜ï¼Œæå‡ºäº†LASPAæ¡†æ¶ã€‚LASPAé‡‡ç”¨äº†ä¸€ç§åŸºäºå‰ç¼€å¾®è°ƒäº¤å‰æ³¨æ„åŠ›(Prefix-Tuned Cross-Attention)çš„è§£è€¦å­¦ä¹ ç­–ç•¥ï¼Œæ—¨åœ¨å°†å£éŸ³ã€å‘éŸ³ç”Ÿç†ç‰¹å¾ä¸è¯­è¨€éŸ³ç´ ç»“æ„æœ‰æ•ˆåˆ†ç¦»ã€‚è¯¥æ–¹æ³•é€šè¿‡é›†æˆè”åˆå­¦ä¹ ï¼Œä½¿å¾—æ¨¡å‹åœ¨è¯´è¯äººåˆ‡æ¢è¯­è¨€æ—¶ä»èƒ½ä¿æŒç¨³å¥çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLASPAåœ¨å•è¯­è¨€ã€å¤šè¯­è¨€ä»¥åŠæœªè§è¿‡çš„è¯­è¨€è®¾ç½®ä¸­å‡å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†ç­‰é”™è¯¯ç‡(Equal Error Rate)ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚è¯­è¨€æ¡ä»¶ä¸‹æå–çº¯å‡€è¯´è¯äººç‰¹å¾çš„ä¼˜è¶Šæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºæé«˜è·¨è¯­è¨€è¯´è¯äººè¯†åˆ«çš„å‡†ç¡®æ€§æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at Interspeech 2025, Netherlands",
      "pdf_url": "https://arxiv.org/pdf/2506.02083v1",
      "published_date": "2025-06-02 10:59:31 UTC",
      "updated_date": "2025-06-02 10:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:19.415824+00:00"
    },
    {
      "arxiv_id": "2506.01535v1",
      "title": "Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries",
      "title_zh": "è¯å…¸åŠ©åŠ›ï¼šåˆ©ç”¨åŒè¯­è¯å…¸å®ç°ä½èµ„æºè¯­è¨€çš„è·¨è¯­è¨€è¯è¡¨è¿ç§»",
      "authors": [
        "Haruki Sakajo",
        "Yusuke Ide",
        "Justin Vasselli",
        "Yusuke Sakai",
        "Yingtao Tian",
        "Hidetaka Kamigaito",
        "Taro Watanabe"
      ],
      "abstract": "Cross-lingual vocabulary transfer plays a promising role in adapting pre-trained language models to new languages, including low-resource languages. Existing approaches that utilize monolingual or parallel corpora face challenges when applied to languages with limited resources. In this work, we propose a simple yet effective vocabulary transfer method that utilizes bilingual dictionaries, which are available for many languages, thanks to descriptive linguists. Our proposed method leverages a property of BPE tokenizers where removing a subword from the vocabulary causes a fallback to shorter subwords. The embeddings of target subwords are estimated iteratively by progressively removing them from the tokenizer. The experimental results show that our approach outperforms existing methods for low-resource languages, demonstrating the effectiveness of a dictionary-based approach for cross-lingual vocabulary transfer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½èµ„æºè¯­è¨€ (low-resource languages) åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹é€‚é…ä¸­é¢ä¸´çš„è¯­æ–™åŒ®ä¹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨åŒè¯­è¯å…¸ (bilingual dictionaries) è¿›è¡Œè·¨è¯­è¨€è¯æ±‡è¿ç§» (cross-lingual vocabulary transfer) çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç”±äºæè¿°æ€§è¯­è¨€å­¦å®¶çš„ç§¯ç´¯ï¼Œè®¸å¤šä½èµ„æºè¯­è¨€éƒ½æ‹¥æœ‰ç°æˆçš„åŒè¯­è¯å…¸ï¼Œè¿™ä¸ºæ¨¡å‹è¿ç§»æä¾›äº†æ–°çš„åˆ‡å…¥ç‚¹ã€‚è¯¥æ–¹æ³•å·§å¦™åˆ©ç”¨äº† BPE tokenizer çš„ç‰¹æ€§ï¼Œå³å½“è¯è¡¨ä¸­æŸä¸ªå­è¯è¢«ç§»é™¤æ—¶ï¼Œåˆ†è¯å™¨ä¼šè‡ªåŠ¨å›é€€è‡³æ›´çŸ­çš„å­è¯åºåˆ—ã€‚é€šè¿‡è¿­ä»£åœ°ä»åˆ†è¯å™¨ä¸­ç§»é™¤ç›®æ ‡å­è¯å¹¶é€æ­¥ä¼°ç®—å…¶åµŒå…¥ (embeddings)ï¼Œè¯¥æŠ€æœ¯å®ç°äº†é«˜æ•ˆçš„è¯æ±‡è¡¨è½¬æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä½èµ„æºåœºæ™¯ä¸‹ï¼Œè¿™ç§åŸºäºè¯å…¸çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºå•è¯­æˆ–å¹³è¡Œè¯­æ–™åº“çš„è¿ç§»æ–¹æ³•ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†åœ¨ç¼ºä¹å¤§è§„æ¨¡è¯­æ–™çš„æƒ…å†µä¸‹ï¼ŒåŒè¯­è¯å…¸æ˜¯å®ç°ç¨³å¥è·¨è¯­è¨€è¯æ±‡è¿ç§»çš„æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.01535v1",
      "published_date": "2025-06-02 10:52:52 UTC",
      "updated_date": "2025-06-02 10:52:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:38.301127+00:00"
    },
    {
      "arxiv_id": "2506.01533v1",
      "title": "A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments",
      "title_zh": "åŸºäºæ‰©æ•£çš„åŒ»ç–—å¤šç»“å±€åˆ†å¸ƒå­¦ä¹ æ–¹æ³•",
      "authors": [
        "Yuchen Ma",
        "Jonas Schweisthal",
        "Hengrui Zhang",
        "Stefan Feuerriegel"
      ],
      "abstract": "In medicine, treatments often influence multiple, interdependent outcomes, such as primary endpoints, complications, adverse events, or other secondary endpoints. Hence, to make optimal treatment decisions, clinicians are interested in learning the distribution of multi-dimensional treatment outcomes. However, the vast majority of machine learning methods for predicting treatment effects focus on single-outcome settings, despite the fact that medical data often include multiple, interdependent outcomes. To address this limitation, we propose a novel diffusion-based method called DIME to learn the joint distribution of multiple outcomes of medical treatments. We addresses three challenges relevant in medical practice: (i)it is tailored to learn the joint interventional distribution of multiple medical outcomes, which enables reliable decision-making with uncertainty quantification rather than relying solely on point estimates; (ii)it explicitly captures the dependence structure between outcomes; (iii)it can handle outcomes of mixed type, including binary, categorical, and continuous variables. In DIME, we take into account the fundamental problem of causal inference through causal masking. For training, our method decomposes the joint distribution into a series of conditional distributions with a customized conditional masking to account for the dependence structure across outcomes. For inference, our method auto-regressively generates predictions. This allows our method to move beyond point estimates of causal quantities and thus learn the joint interventional distribution. To the best of our knowledge, DIME is the first neural method tailored to learn the joint, multi-outcome distribution of medical treatments. Across various experiments, we demonstrate that our method effectively learns the joint distribution and captures shared information among multiple outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DIMEï¼Œä¸€ç§é¦–åˆ›çš„åŸºäºæ‰©æ•£æ¨¡å‹ (diffusion-based) çš„ç¥ç»æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦æ²»ç–—ä¸­å¤šä¸ªç›¸äº’å…³è”ç»“æœçš„è”åˆåˆ†å¸ƒ (joint distribution) å­¦ä¹ éš¾é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¤§å¤šå±€é™äºå•ç»“æœé¢„æµ‹çš„ä¸è¶³ï¼ŒDIME é€šè¿‡å› æœæ©ç  (causal masking) å¤„ç†å› æœæ¨æ–­é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å®šåˆ¶çš„æ¡ä»¶æ©ç  (conditional masking) æ•æ‰ç»“æœé—´çš„å¤æ‚ä¾èµ–ç»“æ„ã€‚è¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶å¤„ç†äºŒå…ƒã€åˆ†ç±»å’Œè¿ç»­å˜é‡ç­‰æ··åˆç±»å‹ç»“æœ (mixed type outcomes)ï¼Œå¹¶é€šè¿‡æä¾›å¹²é¢„åˆ†å¸ƒçš„é¢„æµ‹å®ç°å¯é çš„ä¸ç¡®å®šæ€§é‡åŒ– (uncertainty quantification)ï¼Œè€Œéä»…ä¾èµ–äºä¼ ç»Ÿçš„ç‚¹ä¼°è®¡ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒDIME é‡‡ç”¨è‡ªå›å½’æ–¹å¼ç”Ÿæˆé¢„æµ‹ï¼Œä»è€Œèƒ½å¤Ÿå‡†ç¡®å­¦ä¹ å¤šç»´åº¦æ²»ç–—ç»“æœçš„è”åˆå¹²é¢„åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDIME åœ¨æ•æ‰å¤šä¸ªåŒ»ç–—ç»“æœé—´çš„å…±äº«ä¿¡æ¯æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ç”¨äºå­¦ä¹ åŒ»ç–—æ²»ç–—å¤šç»“æœè”åˆåˆ†å¸ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä¸ºä¸´åºŠå†³ç­–æä¾›äº†æ›´å…¨é¢çš„æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at KDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01533v1",
      "published_date": "2025-06-02 10:49:55 UTC",
      "updated_date": "2025-06-02 10:49:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:23.073641+00:00"
    },
    {
      "arxiv_id": "2506.02082v1",
      "title": "SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction",
      "title_zh": "SALF-MOSï¼šåŸºäºä¸‹é‡‡æ ·è¯´è¯äººæ— å…³æ½œåœ¨ç‰¹å¾çš„ MOS é¢„æµ‹",
      "authors": [
        "Saurabh Agrawal",
        "Raj Gohil",
        "Gopal Kumar Agrawal",
        "Vikram C M",
        "Kushal Verma"
      ],
      "abstract": "Speech quality assessment is a critical process in selecting text-to-speech synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can be done using objective metrics or subjective metrics. Although there are many objective metrics like the Perceptual Evaluation of Speech Quality (PESQ), Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time Objective Intelligibility (STOI) but none of them is feasible in selecting the best model. On the other hand subjective metric like Mean Opinion Score is highly reliable but it requires a lot of manual efforts and are time-consuming. To counter the issues in MOS Evaluation, we have developed a novel model, Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a small-sized, end-to-end, highly generalized and scalable model for predicting MOS score on a scale of 5. We use the sequences of convolutions and stack them to get the latent features of the audio samples to get the best state-of-the-art results based on mean squared error (MSE), Linear Concordance Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and Kendall Rank Correlation Coefficient (KTAU).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­éŸ³è´¨é‡è¯„ä¼°ä¸­ä¸»è§‚ Mean Opinion Score (MOS) è¯„ä¼°è€—æ—¶è´¹åŠ›ä»¥åŠä¼ ç»Ÿå®¢è§‚æŒ‡æ ‡å¯è¡Œæ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º SALF-MOS çš„æ–°å‹é¢„æµ‹æ¨¡å‹ã€‚SALF-MOS æ˜¯ä¸€ä¸ªå°å‹ã€ç«¯åˆ°ç«¯ã€é«˜åº¦æ³›åŒ–ä¸”å¯æ‰©å±•çš„æ¨¡å‹ï¼Œæ—¨åœ¨è‡ªåŠ¨é¢„æµ‹ 5 åˆ†åˆ¶ä¸‹çš„ MOS åˆ†æ•°ã€‚è¯¥æ–¹æ³•é€šè¿‡å †å å·ç§¯åºåˆ—æå–éŸ³é¢‘æ ·æœ¬çš„ Latent Featuresï¼Œä»è€Œæœ‰æ•ˆæ•æ‰è¯­éŸ³çš„æ ¸å¿ƒè´¨é‡ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSALF-MOS åœ¨ Mean Squared Error (MSE)ã€Linear Concordance Correlation coefficient (LCC)ã€Spearman Rank Correlation Coefficient (SRCC) ä»¥åŠ Kendall Rank Correlation Coefficient (KTAU) ç­‰å…³é”®æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚è¿™ä¸€ç ”ç©¶ä¸º Text-to-Speech (TTS) å’Œè¯­éŸ³è½¬æ¢æ¨¡å‹çš„æ€§èƒ½è¯„ä¼°æä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€å‡†ç¡®ä¸” Speaker Agnostic çš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02082v1",
      "published_date": "2025-06-02 10:45:40 UTC",
      "updated_date": "2025-06-02 10:45:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:28.428294+00:00"
    },
    {
      "arxiv_id": "2506.01524v2",
      "title": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat",
      "title_zh": "V-VAEï¼šé¢å‘ç±»äººå¯¹è¯ç»†ç²’åº¦æ§åˆ¶çš„å˜åˆ†è‡ªç¼–ç æ¡†æ¶",
      "authors": [
        "Qi Lin",
        "Weikai Xu",
        "Lisi Chen",
        "Bin Dai"
      ],
      "abstract": "With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Verbal Variational Auto-Encoding (V-VAE)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸ªæ€§åŒ–å¯¹è¯ä¸­è¿‡åº¦ä¾èµ–é™æ€è§’è‰²æè¿°å’Œç²—ç²’åº¦ä¿¡å·ã€éš¾ä»¥æ•æ‰ç»†ç²’åº¦åŠ¨æ€ç»†èŠ‚çš„é—®é¢˜ã€‚V-VAEåŒ…å«å˜åˆ†è‡ªç¼–ç æ¨¡å—(variational auto-encoding module)å’Œç»†ç²’åº¦æ§åˆ¶ç©ºé—´ï¼Œèƒ½å¤ŸåŸºäºè°ˆè¯é£æ ¼ã€äº¤äº’æ¨¡å¼å’Œä¸ªäººå±æ€§ç­‰å¯è§£é‡Šçš„æ½œå˜é‡(latent variables)åŠ¨æ€è°ƒæ•´å¯¹è¯è¡Œä¸ºã€‚ä¸ºäº†åº”å¯¹ç±»äººå¯¹è¯é¢†åŸŸé«˜è´¨é‡æ•°æ®çš„åŒ®ä¹ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†é«˜è´¨é‡æ•°æ®é›†HumanChatDataä»¥åŠç›¸åº”çš„è¯„ä¼°åŸºå‡†HumanChatBenchã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºV-VAEçš„æ¨¡å‹åœ¨HumanChatBenchå’ŒDialogBenchä¸Šå‡ä¸€è‡´ä¼˜äºæ ‡å‡†åŸºçº¿æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡å»ºæ¨¡æƒ…æ„Ÿè¯­æ°”ã€æƒ…å¢ƒæ„ŸçŸ¥å’Œè¿›åŒ–çš„æ€§æ ¼ç­‰å¾®å¦™ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†å¯¹è¯ç”Ÿæˆçš„ç±»äººæ„Ÿä¸ç»†ç²’åº¦æ§åˆ¶åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01524v2",
      "published_date": "2025-06-02 10:38:02 UTC",
      "updated_date": "2025-12-11 10:22:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:27.027544+00:00"
    },
    {
      "arxiv_id": "2506.02081v1",
      "title": "RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection",
      "title_zh": "RATFMï¼šé¢å‘å¼‚å¸¸æ£€æµ‹çš„æ£€ç´¢å¢å¼ºæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹",
      "authors": [
        "Chihiro Maru",
        "Shoetsu Sato"
      ],
      "abstract": "Inspired by the success of large language models (LLMs) in natural language processing, recent research has explored the building of time series foundation models and applied them to tasks such as forecasting, classification, and anomaly detection. However, their performances vary between different domains and tasks. In LLM-based approaches, test-time adaptation using example-based prompting has become common, owing to the high cost of retraining. In the context of anomaly detection, which is the focus of this study, providing normal examples from the target domain can also be effective. However, time series foundation models do not naturally acquire the ability to interpret or utilize examples or instructions, because the nature of time series data used during training does not encourage such capabilities. To address this limitation, we propose a retrieval augmented time series foundation model (RATFM), which enables pretrained time series foundation models to incorporate examples of test-time adaptation. We show that RATFM achieves a performance comparable to that of in-domain fine-tuning while avoiding domain-dependent fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset including nine domains, confirms the effectiveness of the proposed approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆtime series foundation modelsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨é¢†åŸŸç¤ºä¾‹è¿›è¡Œè‡ªé€‚åº”çš„é—®é¢˜ï¼Œæå‡ºäº† RATFMï¼ˆRetrieval-augmented Time Series Foundation Modelï¼‰ã€‚RATFM æ˜¯ä¸€ç§æ£€ç´¢å¢å¼ºçš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è®©é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿæ•´åˆæ¥è‡ªç›®æ ‡é¢†åŸŸçš„æ­£å¸¸ç¤ºä¾‹ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆtest-time adaptationï¼‰ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒRATFM æˆåŠŸé¿å…äº†é«˜æˆæœ¬çš„é¢†åŸŸç‰¹å®šå¾®è°ƒï¼ˆdomain-dependent fine-tuningï¼‰ï¼ŒåŒæ—¶æå‡äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨åŒ…å«ä¹ä¸ªé¢†åŸŸçš„ UCR Anomaly Archive æ•°æ®é›†ä¸Šçš„å®éªŒè¯å®ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½å¯ä¸é¢†åŸŸå†…å¾®è°ƒï¼ˆin-domain fine-tuningï¼‰ç›¸åª²ç¾ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºæå‡æ—¶é—´åºåˆ—æ¨¡å‹åœ¨å¤šé¢†åŸŸå¼‚å¸¸æ£€æµ‹ä¸­çš„é€šç”¨æ€§å’Œå®ç”¨æ€§æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02081v1",
      "published_date": "2025-06-02 10:25:35 UTC",
      "updated_date": "2025-06-02 10:25:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:42:42.681065+00:00"
    },
    {
      "arxiv_id": "2506.01512v1",
      "title": "Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­äº‹å®ã€è™šæ„ä¸é¢„æµ‹çš„è¡¨å¾ï¼šè®¤è¯†æƒ…æ€ä¸æ€åº¦",
      "authors": [
        "Meng Li",
        "Michael Vrazitulis",
        "David Schlangen"
      ],
      "abstract": "Rational speakers are supposed to know what they know and what they do not know, and to generate expressions matching the strength of evidence. In contrast, it is still a challenge for current large language models to generate corresponding utterances based on the assessment of facts and confidence in an uncertain real-world environment. While it has recently become popular to estimate and calibrate confidence of LLMs with verbalized uncertainty, what is lacking is a careful examination of the linguistic knowledge of uncertainty encoded in the latent space of LLMs. In this paper, we draw on typological frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic modality, using controlled stories. Our experiments show that the performance of LLMs in generating epistemic expressions is limited and not robust, and hence the expressions of uncertainty generated by LLMs are not always reliable. To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge of epistemic modality in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†äº‹å®ã€è™šæ„å’Œé¢„æµ‹æ—¶çš„è®¤è¯†è®º(Epistemics)ä¸æ€åº¦ï¼Œæ—¨åœ¨è§£å†³LLMsåœ¨ä¸ç¡®å®šç°å®ç¯å¢ƒä¸‹éš¾ä»¥æ ¹æ®è¯æ®å¼ºåº¦ç”Ÿæˆå‡†ç¡®è¡¨è¾¾çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è€…å€Ÿé‰´è®¤è¯†æ¨¡æ€(Epistemic Modality)çš„ç±»å‹å­¦æ¡†æ¶ï¼Œé€šè¿‡å—æ§æ•…äº‹å®éªŒæ·±å…¥è¯„ä¼°äº†LLMsæ½œç©ºé—´ä¸­æ‰€ç¼–ç çš„ä¸ç¡®å®šæ€§è¯­è¨€çŸ¥è¯†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMsåœ¨ç”Ÿæˆè®¤è¯†è¡¨è¾¾æ–¹é¢çš„èƒ½åŠ›æœ‰é™ä¸”ç¼ºä¹ç¨³å¥æ€§ï¼Œè¿™æ„å‘³ç€æ¨¡å‹äº§ç”Ÿçš„ä¸ç¡®å®šæ€§è¡¨è¿°å¹¶éå§‹ç»ˆå¯é ã€‚è¯¥ç ”ç©¶å¼ºè°ƒï¼Œè¦æ„å»ºå…·å¤‡ä¸ç¡®å®šæ€§æ„è¯†(Uncertainty-aware)çš„LLMsï¼Œå¿…é¡»ä¸°å¯Œæ¨¡å‹å¯¹è®¤è¯†æ¨¡æ€çš„è¯­ä¹‰çŸ¥è¯†ç†è§£ï¼Œä»¥æå‡å…¶å¯¹çŸ¥è¯†è¾¹ç•Œçš„æ„ŸçŸ¥ä¸è¡¨è¾¾èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted by ACL 2025 (main)",
      "pdf_url": "https://arxiv.org/pdf/2506.01512v1",
      "published_date": "2025-06-02 10:19:42 UTC",
      "updated_date": "2025-06-02 10:19:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:43:38.613758+00:00"
    },
    {
      "arxiv_id": "2506.01502v2",
      "title": "Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme",
      "title_zh": "ç¾¤ä½“åŠ¨åŠ›å­¦å­¦ä¹ ï¼šé€†ä¼˜åŒ–ä¸ JKO æ–¹æ¡ˆçš„ç»“åˆ",
      "authors": [
        "Mikhail Persiianov",
        "Jiawei Chen",
        "Petr Mokrov",
        "Alexander Tyurin",
        "Evgeny Burnaev",
        "Alexander Korotin"
      ],
      "abstract": "Learning population dynamics involves recovering the underlying process that governs particle evolution, given evolutionary snapshots of samples at discrete time points. Recent methods frame this as an energy minimization problem in probability space and leverage the celebrated JKO scheme for efficient time discretization. In this work, we introduce $\\texttt{iJKOnet}$, an approach that combines the JKO framework with inverse optimization techniques to learn population dynamics. Our method relies on a conventional $\\textit{end-to-end}$ adversarial training procedure and does not require restrictive architectural choices, e.g., input-convex neural networks. We establish theoretical guarantees for our methodology and demonstrate improved performance over prior JKO-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨ä»ç¦»æ•£æ—¶é—´ç‚¹çš„æ ·æœ¬å¿«ç…§ä¸­æ¢å¤æ§åˆ¶ç²’å­æ¼”åŒ–çš„æ½œåœ¨è¿‡ç¨‹ï¼Œå³å­¦ä¹ ç¾¤ä½“åŠ¨åŠ›å­¦(Population Dynamics)ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºiJKOnetçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°å°†ä¼ ç»Ÿçš„JKOæ–¹æ¡ˆ(JKO scheme)ä¸é€†ä¼˜åŒ–(Inverse Optimization)æŠ€æœ¯ç›¸ç»“åˆï¼Œç”¨äºè§£å†³æ¦‚ç‡ç©ºé—´ä¸­çš„èƒ½é‡æœ€å°åŒ–é—®é¢˜ã€‚ä¸ä»¥å¾€ä¾èµ–è¾“å…¥å‡¸ç¥ç»ç½‘ç»œ(Input-Convex Neural Networks)ç­‰å—é™æ¶æ„çš„æ–¹æ³•ä¸åŒï¼ŒiJKOneté‡‡ç”¨å¸¸è§„çš„ç«¯åˆ°ç«¯(end-to-end)å¯¹æŠ—è®­ç»ƒç¨‹åºï¼Œæä¾›äº†æ›´å¼ºçš„æ¶æ„çµæ´»æ€§ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜ä¸ºè¯¥æ–¹æ³•å»ºç«‹äº†ç†è®ºä¿è¯ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºJKOçš„åŠ¨åŠ›å­¦å­¦ä¹ æ–¹æ³•ã€‚è¿™ä¸€æˆæœä¸ºç†è§£å’Œå»ºæ¨¡å¤æ‚ç²’å­æ¼”åŒ–è¿‡ç¨‹æä¾›äº†ä¸€ä¸ªæ›´ä¸ºé«˜æ•ˆä¸”é€šç”¨çš„è®¡ç®—æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01502v2",
      "published_date": "2025-06-02 10:08:03 UTC",
      "updated_date": "2025-11-07 08:40:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:43:46.795491+00:00"
    },
    {
      "arxiv_id": "2506.02080v2",
      "title": "Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge",
      "title_zh": "åˆ©ç”¨éŸ³ç³»å­¦çŸ¥è¯†æå‡åŸºäº CTC çš„è¯¯å‘éŸ³æ£€æµ‹ä¸­çš„ GOP",
      "authors": [
        "Aditya Kamlesh Parikh",
        "Cristian Tejedor-Garcia",
        "Catia Cucchiarini",
        "Helmer Strik"
      ],
      "abstract": "Computer-Assisted Pronunciation Training (CAPT) systems employ automatic measures of pronunciation quality, such as the goodness of pronunciation (GOP) metric. GOP relies on forced alignments, which are prone to labeling and segmentation errors due to acoustic variability. While alignment-free methods address these challenges, they are computationally expensive and scale poorly with phoneme sequence length and inventory size. To enhance efficiency, we introduce a substitution-aware alignment-free GOP that restricts phoneme substitutions based on phoneme clusters and common learner errors. We evaluated our GOP on two L2 English speech datasets, one with child speech, My Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult speech. We compared RPS (restricted phoneme substitutions) and UPS (unrestricted phoneme substitutions) setups within alignment-free methods, which outperformed the baseline. We discuss our results and outline avenues for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è®¡ç®—æœºè¾…åŠ©å‘éŸ³è®­ç»ƒ(CAPT)ä¸­å‘éŸ³ä¼˜åŠ£åº¦(GOP)æŒ‡æ ‡ä¾èµ–å¼ºåˆ¶å¯¹é½å¯¼è‡´è¯¯å·®ï¼Œä»¥åŠä¼ ç»Ÿæ— å¯¹é½æ–¹æ³•è®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºå‹æ–¹æ¡ˆã€‚ä½œè€…å¼•å…¥äº†ä¸€ç§ç»“åˆéŸ³ç³»å­¦çŸ¥è¯†(Phonological Knowledge)çš„æ›¿ä»£æ„ŸçŸ¥å‹æ— å¯¹é½GOPè®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡éŸ³ç´ ç°‡(Phoneme Clusters)å’Œå¸¸è§å­¦ä¹ è€…é”™è¯¯æ¥é™åˆ¶éŸ³ç´ æ›¿ä»£èŒƒå›´ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æé«˜è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶è§£å†³åŸºäºCTCçš„è¯¯è¯»æ£€æµ‹ä¸­çš„ç²¾åº¦æŒ‘æˆ˜ã€‚ç ”ç©¶åœ¨My Pronunciation Coach(MPC)å’ŒSpeechOcean762ä¸¤ä¸ªL2è‹±è¯­æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ¶µç›–äº†å„¿ç«¥ä¸æˆäººçš„è¯­éŸ³æ•°æ®ã€‚å®éªŒå¯¹æ¯”äº†å—é™éŸ³ç´ æ›¿ä»£(RPS)ä¸ä¸å—é™éŸ³ç´ æ›¿ä»£(UPS)ä¸¤ç§è®¾ç½®ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ”¹è¿›çš„æ— å¯¹é½æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œè¯æ˜äº†ç»“åˆå…ˆéªŒéŸ³ç³»å­¦çŸ¥è¯†åœ¨ä¼˜åŒ–å‘éŸ³è¯„ä¼°ç³»ç»Ÿæ•ˆç‡ä¸å‡†ç¡®æ€§æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to Interspeech 2025. This publication is part of the project Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013 of the research programme NGF AiNed Fellowship Grants which is financed by the Dutch Research Council (NWO)",
      "pdf_url": "https://arxiv.org/pdf/2506.02080v2",
      "published_date": "2025-06-02 09:45:29 UTC",
      "updated_date": "2025-07-08 10:03:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:43:08.064114+00:00"
    },
    {
      "arxiv_id": "2506.01482v2",
      "title": "Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?",
      "title_zh": "è‡ªåŠ¨åŒ–èˆå°ç¯å…‰æ§åˆ¶ï¼šæ˜¯è§„åˆ™é©±åŠ¨çš„è¿‡ç¨‹è¿˜æ˜¯ç”Ÿæˆå¼ä»»åŠ¡ï¼Ÿ",
      "authors": [
        "Zijian Zhao",
        "Dian Jin",
        "Zijing Zhou",
        "Xiaoyu Zhang"
      ],
      "abstract": "Stage lighting is a vital component in live music performances, shaping an engaging experience for both musicians and audiences. In recent years, Automatic Stage Lighting Control (ASLC) has attracted growing interest due to the high costs of hiring or training professional lighting engineers. However, most existing ASLC solutions only classify music into limited categories and map them to predefined light patterns, resulting in formulaic and monotonous outcomes that lack rationality. To address this gap, this paper presents Skip-BART, an end-to-end model that directly learns from experienced lighting engineers and predict vivid, human-like stage lighting. To the best of our knowledge, this is the first work to conceptualize ASLC as a generative task rather than merely a classification problem. Our method adapts the BART model to take audio music as input and produce light hue and value (intensity) as output, incorporating a novel skip connection mechanism to enhance the relationship between music and light within the frame grid. To address the lack of available datasets, we create the first stage lighting dataset, along with several pre-training and transfer learning techniques to improve model training with limited data. We validate our method through both quantitative analysis and an human evaluation, demonstrating that Skip-BART outperforms conventional rule-based methods across all evaluation metrics and shows only a limited gap compared to real lighting engineers. To support further research, we have made our self-collected dataset, code, and trained model parameters available at https://github.com/RS2002/Skip-BART .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è‡ªåŠ¨èˆå°ç¯å…‰æ§åˆ¶(ASLC)æ–¹æ¡ˆå› ä¾èµ–åˆ†ç±»å’Œé¢„è®¾æ¨¡å¼è€Œå¯¼è‡´çš„å•è°ƒæ€§é—®é¢˜ï¼Œæå‡ºäº†Skip-BARTæ¨¡å‹ï¼Œé¦–æ¬¡å°†è¿™ä¸€è¿‡ç¨‹è§†ä¸ºç”Ÿæˆä»»åŠ¡è€Œéç®€å•çš„è§„åˆ™åŒ¹é…ã€‚è¯¥æ¨¡å‹é€šè¿‡æ”¹è¿›BARTç»“æ„ï¼Œç›´æ¥å°†éŸ³é¢‘éŸ³ä¹è¾“å…¥è½¬åŒ–ä¸ºç¯å…‰è‰²è°ƒ(Hue)å’Œäº®åº¦(Value)è¾“å‡ºï¼Œå¹¶å¼•å…¥æ–°å‹è·³è·ƒè¿æ¥(Skip Connection)æœºåˆ¶ä»¥å¼ºåŒ–éŸ³ä¹ä¸ç¯å…‰åŠ¨æ€çš„å…³è”ã€‚ä¸ºå…‹æœæ•°æ®åŒ®ä¹ï¼Œå›¢é˜Ÿæ„å»ºäº†é¦–ä¸ªèˆå°ç¯å…‰æ•°æ®é›†ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒå’Œè¿ç§»å­¦ä¹ æŠ€æœ¯ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚å®šé‡åˆ†æä¸äººç±»è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒSkip-BARTåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œå…¶ç”Ÿæˆçš„ç¯å…‰æ•ˆæœå·²æ¥è¿‘ä¸“ä¸šç¯å…‰å¸ˆæ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01482v2",
      "published_date": "2025-06-02 09:42:36 UTC",
      "updated_date": "2025-12-31 06:54:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:43:09.502729+00:00"
    },
    {
      "arxiv_id": "2506.02079v1",
      "title": "Robust Federated Learning against Noisy Clients via Masked Optimization",
      "title_zh": "åŸºäºæ©ç ä¼˜åŒ–çš„æŠ—å™ªå£°å®¢æˆ·ç«¯é²æ£’è”é‚¦å­¦ä¹ ",
      "authors": [
        "Xuefeng Jiang",
        "Tian Wen",
        "Zhiqin Yang",
        "Lvhua Wu",
        "Yufeng Chen",
        "Sheng Sun",
        "Yuwei Wang",
        "Min Liu"
      ],
      "abstract": "In recent years, federated learning (FL) has made significant advance in privacy-sensitive applications. However, it can be hard to ensure that FL participants provide well-annotated data for training. The corresponding annotations from different clients often contain complex label noise at varying levels. This label noise issue has a substantial impact on the performance of the trained models, and clients with greater noise levels can be largely attributed for this degradation. To this end, it is necessary to develop an effective optimization strategy to alleviate the adverse effects of these noisy clients.In this study, we present a two-stage optimization framework, MaskedOptim, to address this intricate label noise problem. The first stage is designed to facilitate the detection of noisy clients with higher label noise rates. The second stage focuses on rectifying the labels of the noisy clients' data through an end-to-end label correction mechanism, aiming to mitigate the negative impacts caused by misinformation within datasets. This is achieved by learning the potential ground-truth labels of the noisy clients' datasets via backpropagation. To further enhance the training robustness, we apply the geometric median based model aggregation instead of the commonly-used vanilla averaged model aggregation. We implement sixteen related methods and conduct evaluations on three image datasets and one text dataset with diverse label noise patterns for a comprehensive comparison. Extensive experimental results indicate that our proposed framework shows its robustness in different scenarios. Additionally, our label correction framework effectively enhances the data quality of the detected noisy clients' local datasets. % Our codes will be open-sourced to facilitate related research communities. Our codes are available via https://github.com/Sprinter1999/MaskedOptim .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹ (Federated Learning)ä¸­å®¢æˆ·ç«¯æ ‡æ³¨æ•°æ®å­˜åœ¨å¤æ‚æ ‡ç­¾å™ªå£°çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºMaskedOptimçš„ä¸¤é˜¶æ®µä¼˜åŒ–æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µæ—¨åœ¨è¯†åˆ«å¹¶æ£€æµ‹å…·æœ‰é«˜æ ‡ç­¾å™ªå£°ç‡çš„å®¢æˆ·ç«¯ï¼Œç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡ç«¯åˆ°ç«¯çš„æ ‡ç­¾æ ¡æ­£æœºåˆ¶ï¼Œåˆ©ç”¨åå‘ä¼ æ’­(Backpropagation)å­¦ä¹ å™ªå£°æ•°æ®é›†çš„æ½œåœ¨çœŸå®æ ‡ç­¾ï¼Œä»è€Œå‡è½»é”™è¯¯ä¿¡æ¯å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡è®­ç»ƒçš„é²æ£’æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŸºäºå‡ ä½•ä¸­ä½æ•°(Geometric Median)çš„æ¨¡å‹èšåˆæ–¹å¼ä»£æ›¿ä¼ ç»Ÿçš„ç®€å•å¹³å‡èšåˆã€‚åœ¨æ¶µç›–å›¾åƒå’Œæ–‡æœ¬çš„å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMaskedOptimåœ¨å¤šç§å¤æ‚å™ªå£°åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæå‡è¢«æ£€æµ‹å‡ºçš„å™ªå£°å®¢æˆ·ç«¯æœ¬åœ°æ•°æ®è´¨é‡ï¼Œä¸ºè§£å†³ç°å®åœºæ™¯ä¸­ä¸å¯é å‚ä¸æ–¹çš„è”é‚¦å­¦ä¹ ä¼˜åŒ–æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.02079v1",
      "published_date": "2025-06-02 09:35:42 UTC",
      "updated_date": "2025-06-02 09:35:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:43:08.232020+00:00"
    },
    {
      "arxiv_id": "2506.12067v2",
      "title": "Evaluating Logit-Based GOP Scores for Mispronunciation Detection",
      "title_zh": "ç”¨äºå‘éŸ³é”™è¯¯æ£€æµ‹çš„åŸºäº Logit çš„ GOP åˆ†æ•°è¯„ä¼°",
      "authors": [
        "Aditya Kamlesh Parikh",
        "Cristian Tejedor-Garcia",
        "Catia Cucchiarini",
        "Helmer Strik"
      ],
      "abstract": "Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†åŸºäº Logit çš„å‘éŸ³è¯„ä¼°åˆ†æ•°ï¼ˆGoodness of Pronunciation, GOPï¼‰åœ¨è¯¯è¯»æ£€æµ‹ä¸­çš„è¡¨ç°ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿçš„åŸºäº Softmax åéªŒæ¦‚ç‡åˆ†æ•°å®¹æ˜“å‡ºç°çš„è¿‡åº¦è‡ªä¿¡å’ŒéŸ³ç´ åˆ†ç¦»åº¦å·®ç­‰é—®é¢˜ã€‚ç ”ç©¶äººå‘˜åœ¨è·å…°è¯­å’Œæ™®é€šè¯æ¯è¯­è€…çš„ä¸¤ç»„äºŒè¯­ï¼ˆL2ï¼‰è‹±è¯­è¯­éŸ³æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¯¹æ¯”äº†åŸºäº Logit å’ŒåŸºäºæ¦‚ç‡çš„ GOP åˆ†æ•°åœ¨åˆ†ç±»æ€§èƒ½ä»¥åŠä¸äººå·¥è¯„åˆ†ç›¸å…³æ€§æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäº Logit çš„æ–¹æ³•åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ä¼˜äºåŸºäºæ¦‚ç‡çš„æ–¹æ³•ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºæ•°æ®é›†çš„ç‰¹å¾ã€‚å…¶ä¸­ï¼Œæœ€å¤§ Logit (Maximum Logit) GOP æ˜¾ç¤ºå‡ºä¸äººç±»æ„ŸçŸ¥æœ€å¼ºçš„ä¸€è‡´æ€§ï¼Œè€Œç»“åˆä¸åŒ GOP åˆ†æ•°çš„æ··åˆæ–¹æ³•åˆ™èƒ½å¹³è¡¡æ¦‚ç‡ä¸ Logit ç‰¹å¾ã€‚è¯¥ç ”ç©¶æœ€ç»ˆè¡¨æ˜ï¼Œç»“åˆä¸ç¡®å®šæ€§å»ºæ¨¡ (Uncertainty Modeling) å’ŒéŸ³ç´ ç‰¹å®šæƒé‡ (Phoneme-specific Weighting) çš„æ··åˆ GOP æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡å‘éŸ³è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to Interspeech 2025. This publication is part of the project Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013 of the research programme NGF AiNed Fellowship Grants which is financed by the Dutch Research Council (NWO)",
      "pdf_url": "https://arxiv.org/pdf/2506.12067v2",
      "published_date": "2025-06-02 09:35:13 UTC",
      "updated_date": "2025-07-08 09:58:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:43:25.077824+00:00"
    },
    {
      "arxiv_id": "2506.01475v1",
      "title": "PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization",
      "title_zh": "PGPOï¼šé€šè¿‡ä¼ªä»£ç å¼è§„åˆ’å¼•å¯¼çš„åå¥½ä¼˜åŒ–å¢å¼ºæ™ºèƒ½ä½“æ¨ç†",
      "authors": [
        "Zouying Cao",
        "Runze Wang",
        "Yifei Yang",
        "Xinbei Ma",
        "Xiaoyong Zhu",
        "Bo Zheng",
        "Hai Zhao"
      ],
      "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities in handling complex interactive problems. Existing LLM agents mainly generate natural language plans to guide reasoning, which is verbose and inefficient. NL plans are also tailored to specific tasks and restrict agents' ability to generalize across similar tasks. To this end, we explore pseudocode-style plans (P-code Plan) to capture the structural logic of reasoning. We find that P-code Plan empowers LLM agents with stronger generalization ability and more efficiency. Inspired by this finding, we propose a pseudocode-style Planning Guided Preference Optimization method called PGPO for effective agent learning. With two planning-oriented rewards, PGPO further enhances LLM agents' ability to generate high-quality P-code Plans and subsequent reasoning. Experiments show that PGPO achieves superior performance on representative agent benchmarks and outperforms the current leading baselines. Analyses reveal the advantage of PGPO in reducing action errors and omissions during reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨å¤„ç†å¤æ‚äº¤äº’é—®é¢˜æ—¶ï¼Œè‡ªç„¶è¯­è¨€è®¡åˆ’(NL plans)è¡¨ç°å‡ºçš„å†—é•¿ã€ä½æ•ˆä¸”æ³›åŒ–èƒ½åŠ›æœ‰é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPGPO(Pseudocode-style Planning Guided Preference Optimization)çš„åå¥½ä¼˜åŒ–æ–¹æ³•ã€‚ç ”ç©¶æ¢è®¨äº†ä¼ªä»£ç é£æ ¼è®¡åˆ’(P-code Plan)åœ¨æ•æ‰æ¨ç†ç»“æ„åŒ–é€»è¾‘æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå‘ç°å…¶èƒ½èµ‹äºˆæ™ºèƒ½ä½“æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œæ›´é«˜çš„è¿è¡Œæ•ˆç‡ã€‚PGPOé€šè¿‡å¼•å…¥ä¸¤ç§é¢å‘è®¡åˆ’çš„å¥–åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—å¢å¼ºäº†LLMæ™ºèƒ½ä½“ç”Ÿæˆé«˜è´¨é‡P-code PlanåŠæ‰§è¡Œåç»­æ¨ç†çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPGPOåœ¨å¤šä¸ªä»£è¡¨æ€§æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜äºç°æœ‰é¢†å…ˆåŸºå‡†çš„æ•ˆæœã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†è¯¥æ–¹æ³•åœ¨å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­åŠ¨ä½œé”™è¯¯å’Œé—æ¼æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 12 figures, 14 tables, ACL'25 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.01475v1",
      "published_date": "2025-06-02 09:35:07 UTC",
      "updated_date": "2025-06-02 09:35:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:43:46.166302+00:00"
    },
    {
      "arxiv_id": "2506.02078v1",
      "title": "Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data",
      "title_zh": "é¢„è®­ç»ƒéŸ³é¢‘åµŒå…¥åœ¨å¸•é‡‘æ£®ç—…è¯­éŸ³æ•°æ®åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§è¯„ä¼°",
      "authors": [
        "Emmy Postma",
        "Cristian Tejedor-Garcia"
      ],
      "abstract": "Speech impairments are prevalent biomarkers for Parkinson's Disease (PD), motivating the development of diagnostic techniques using speech data for clinical applications. Although deep acoustic features have shown promise for PD classification, their effectiveness often varies due to individual speaker differences, a factor that has not been thoroughly explored in the existing literature. This study investigates the effectiveness of three pre-trained audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification. Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK) and listen and repeat (LR) tasks, capturing critical acoustic features for PD detection. Only Wav2Vec2.0 shows significant gender bias, achieving more favorable results for male speakers, in DDK tasks. The misclassified cases reveal challenges with atypical speech patterns, highlighting the need for improved feature extraction and model robustness in PD detection.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶è¯„ä¼°äº†é¢„è®­ç»ƒéŸ³é¢‘åµŒå…¥(Pre-Trained Audio Embeddings)åœ¨å¸•é‡‘æ£®ç—…(Parkinson's Disease)è¯­éŸ³æ•°æ®åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨é€šè¿‡è¯­éŸ³ç”Ÿç‰©æ ‡å¿—ç‰©è¾…åŠ©ä¸´åºŠè¯Šæ–­ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹æ¯”äº†OpenL3ã€VGGishå’ŒWav2Vec2.0ä¸‰ç§é¢„è®­ç»ƒæ¨¡å‹åœ¨NeuroVozæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOpenL3åœ¨äº¤æ›¿è¿åŠ¨é€Ÿç‡(diadochokinesis)å’Œå¬åŠ›é‡å¤(listen and repeat)ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ£€æµ‹PDçš„å…³é”®å£°å­¦ç‰¹å¾ã€‚ç ”ç©¶åŒæ—¶å‘ç°Wav2Vec2.0åœ¨å¤„ç†DDKä»»åŠ¡æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§åˆ«åè§ï¼Œå¯¹ç”·æ€§è¯´è¯è€…çš„åˆ†ç±»ç»“æœæ›´ä¸ºç†æƒ³ã€‚é€šè¿‡å¯¹è¯¯åˆ†ç±»æ¡ˆä¾‹çš„åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†éå…¸å‹è¯­éŸ³æ¨¡å¼ç»™æ¨¡å‹å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†åœ¨å¸•é‡‘æ£®ç—…æ£€æµ‹é¢†åŸŸæå‡ç‰¹å¾æå–èƒ½åŠ›å’Œæ¨¡å‹é²æ£’æ€§çš„è¿«åˆ‡éœ€æ±‚ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to Interspeech 2025. This publication is part of the project Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013 of the research programme NGF AiNed Fellowship Grants which is financed by the Dutch Research Council (NWO)",
      "pdf_url": "https://arxiv.org/pdf/2506.02078v1",
      "published_date": "2025-06-02 09:32:54 UTC",
      "updated_date": "2025-06-02 09:32:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:43:48.398126+00:00"
    },
    {
      "arxiv_id": "2506.01463v1",
      "title": "Agentic AI and Multiagentic: Are We Reinventing the Wheel?",
      "title_zh": "æ™ºèƒ½ä½“åŒ– AI ä¸å¤šæ™ºèƒ½ä½“åŒ–ï¼šæˆ‘ä»¬æ˜¯åœ¨é‡é€ è½®å­å—ï¼Ÿ",
      "authors": [
        "V. Botti"
      ],
      "abstract": "The terms Agentic AI and Multiagentic AI have recently gained popularity in discussions on generative artificial intelligence, often used to describe autonomous software agents and systems composed of such agents. However, the use of these terms confuses these buzzwords with well-established concepts in AI literature: intelligent agents and multi-agent systems. This article offers a critical analysis of this conceptual misuse. We review the theoretical origins of \"agentic\" in the social sciences (Bandura, 1986) and philosophical notions of intentionality (Dennett, 1971), and then summarise foundational works on intelligent agents and multi-agent systems by Wooldridge, Jennings and others. We examine classic agent architectures, from simple reactive agents to Belief-Desire-Intention (BDI) models, and highlight key properties (autonomy, reactivity, proactivity, social capability) that define agency in AI. We then discuss recent developments in large language models (LLMs) and agent platforms based on LLMs, including the emergence of LLM-powered AI agents and open-source multi-agent orchestration frameworks. We argue that the term AI Agentic is often used as a buzzword for what are essentially AI agents, and AI Multiagentic for what are multi-agent systems. This confusion overlooks decades of research in the field of autonomous agents and multi-agent systems. The article advocates for scientific and technological rigour and the use of established terminology from the state of the art in AI, incorporating the wealth of existing knowledge, including standards for multi-agent system platforms, communication languages and coordination and cooperation algorithms, agreement technologies (automated negotiation, argumentation, virtual organisations, trust, reputation, etc.), into the new and promising wave of LLM-based AI agents, so as not to end up reinventing the wheel.",
      "tldr_zh": "è¿™ç¯‡æ–‡ç« å¯¹è¿‘æœŸæµè¡Œçš„ Agentic AI å’Œ Multiagentic AI æœ¯è¯­è¿›è¡Œäº†æ‰¹åˆ¤æ€§åˆ†æï¼ŒæŒ‡å‡ºè¿™äº›çƒ­è¯ä¸äººå·¥æ™ºèƒ½é¢†åŸŸä¸­æˆç†Ÿçš„ Intelligent Agents å’Œ Multi-agent Systems æ¦‚å¿µä¹‹é—´å­˜åœ¨æ··æ·†ã€‚ä½œè€…é€šè¿‡å›é¡¾ç¤¾ä¼šç§‘å­¦ä¸­çš„ä»£ç†æ€§ç†è®ºèµ·æºä»¥åŠ Wooldridge å’Œ Jennings ç­‰å­¦è€…å…³äºæ™ºèƒ½ä½“åŸºç¡€æ¶æ„çš„å¥ åŸºæ€§å·¥ä½œï¼Œæ˜ç¡®äº†å®šä¹‰ AI ä»£ç†æ€§çš„æ ¸å¿ƒå±æ€§ã€‚éšåï¼Œæ–‡ç« æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ™ºèƒ½ä½“åŠå¤šæ™ºèƒ½ä½“ç¼–æ’æ¡†æ¶çš„æœ€æ–°è¿›å±•ï¼Œè®¤ä¸ºå½“å‰è®¸å¤šæ‰€è°“çš„åˆ›æ–°å®é™…ä¸Šæ˜¯å¯¹å·²æœ‰ç ”ç©¶æˆæœçš„é‡æ–°åŒ…è£…ã€‚ä½œè€…å¼ºè°ƒï¼Œè¿™ç§æœ¯è¯­ä¸Šçš„æ··ä¹±å¯¼è‡´äº†å¯¹è¿‡å»æ•°åå¹´åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ ‡å‡†ã€é€šä¿¡è¯­è¨€å’Œåä½œç®—æ³•é¢†åŸŸç ”ç©¶ç§¯æ·€çš„å¿½è§†ã€‚æœ€åï¼Œæ–‡ç« å€¡å¯¼åœ¨ LLM é©±åŠ¨çš„æ–°æµªæ½®ä¸­åº”ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§ï¼Œå……åˆ†æ•´åˆç°æœ‰çš„åè®®æŠ€æœ¯ä¸åè°ƒæœºåˆ¶ï¼Œä»¥é¿å…åœ¨äººå·¥æ™ºèƒ½å‘å±•ä¸­â€œé‡æ–°å‘æ˜è½®å­â€ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01463v1",
      "published_date": "2025-06-02 09:19:11 UTC",
      "updated_date": "2025-06-02 09:19:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:43:52.193615+00:00"
    },
    {
      "arxiv_id": "2506.02077v1",
      "title": "Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition",
      "title_zh": "èµ‹äºˆé‡åŒ–ä¸ä½ç§©çŸ©é˜µä¸åŒèŒèƒ½ä»¥å®ç°æœ€ä¼˜æƒé‡åˆ†è§£",
      "authors": [
        "Yoonjun Cho",
        "Soeun Kim",
        "Dongjae Jeon",
        "Kyelim Lee",
        "Beomsoo Lee",
        "Albert No"
      ],
      "abstract": "Decomposing weight matrices into quantization and low-rank components ($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$) is a widely used technique for compressing large language models (LLMs). Existing joint optimization methods iteratively alternate between quantization and low-rank approximation. However, these methods tend to prioritize one component at the expense of the other, resulting in suboptimal decompositions that fail to leverage each component's unique strengths. In this work, we introduce Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank components the specific role of capturing activation-sensitive weights. This structured decomposition mitigates outliers' negative impact on quantization, enabling more effective balance between quantization and low-rank approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B demonstrate that incorporating ODLRI into the joint optimization framework consistently reduces activation-aware error, minimizes quantization scale, and improves perplexity and zero-shot accuracy in low-bit settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å‹ç¼©ä¸­é‡åŒ–ä¸ä½ç§©çŸ©é˜µåˆ†è§£($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$)å­˜åœ¨çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰è”åˆä¼˜åŒ–æ–¹æ³•å¾€å¾€å› æ— æ³•æœ‰æ•ˆå¹³è¡¡å„ç»„ä»¶ä½œç”¨è€Œå¯¼è‡´åˆ†è§£æ•ˆæœä¸ä½³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Outlier-Driven Low-Rank Initialization (ODLRI)ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–åˆ†è§£èµ‹äºˆå„ç»„ä»¶æ˜ç¡®çš„èŒè´£ã€‚ODLRIä¸“é—¨å°†ä½ç§©ç»„ä»¶(low-rank components)ç”¨äºæ•æ‰æ¿€æ´»æ•æ„Ÿæƒé‡(activation-sensitive weights)ï¼Œä»è€Œå‡è½»ç¦»ç¾¤å€¼(outliers)å¯¹é‡åŒ–è¿‡ç¨‹çš„è´Ÿé¢å½±å“ã€‚è¿™ç§æ–¹æ³•åœ¨é‡åŒ–ä¸ä½ç§©é€¼è¿‘ä¹‹é—´å®ç°äº†æ›´æœ‰æ•ˆçš„å¹³è¡¡ï¼Œæ˜¾è‘—é™ä½äº†æ„ŸçŸ¥æ¿€æ´»è¯¯å·®å¹¶å‡å°äº†é‡åŒ–å°ºåº¦(quantization scale)ã€‚åœ¨Llama2ã€Llama3å’ŒMistralç­‰ä¸»æµæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä½æ¯”ç‰¹(low-bit)è®¾ç½®ä¸‹ä¸€è‡´æå‡äº†æ¨¡å‹çš„å›°æƒ‘åº¦(perplexity)å’Œé›¶æ ·æœ¬(zero-shot)å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to Findings of ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.02077v1",
      "published_date": "2025-06-02 09:15:13 UTC",
      "updated_date": "2025-06-02 09:15:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:44:05.540414+00:00"
    },
    {
      "arxiv_id": "2506.01456v1",
      "title": "GenDMR: A dynamic multimodal role-swapping network for identifying risk gene phenotypes",
      "title_zh": "GenDMRï¼šç”¨äºé£é™©åŸºå› è¡¨å‹è¯†åˆ«çš„åŠ¨æ€å¤šæ¨¡æ€è§’è‰²å¯¹æ¢ç½‘ç»œ",
      "authors": [
        "Lina Qin",
        "Cheng Zhu",
        "Chuqi Zhou",
        "Yukun Huang",
        "Jiayi Zhu",
        "Ping Liang",
        "Jinju Wang",
        "Yixing Huang",
        "Cheng Luo",
        "Dezhong Yao",
        "Ying Tan"
      ],
      "abstract": "Recent studies have shown that integrating multimodal data fusion techniques for imaging and genetic features is beneficial for the etiological analysis and predictive diagnosis of Alzheimer's disease (AD). However, there are several critical flaws in current deep learning methods. Firstly, there has been insufficient discussion and exploration regarding the selection and encoding of genetic information. Secondly, due to the significantly superior classification value of AD imaging features compared to genetic features, many studies in multimodal fusion emphasize the strengths of imaging features, actively mitigating the influence of weaker features, thereby diminishing the learning of the unique value of genetic features. To address this issue, this study proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we develop a novel approach to encode the spatial organization of single nucleotide polymorphisms (SNPs), enhancing the representation of their genomic context. Additionally, to adaptively quantify the disease risk of SNPs and brain region, we propose a multi-instance attention module to enhance model interpretability. Furthermore, we introduce a dominant modality selection module and a contrastive self-distillation module, combining them to achieve a dynamic teacher-student role exchange mechanism based on dominant and auxiliary modalities for bidirectional co-updating of different modal data. Finally, GenDMR achieves state-of-the-art performance on the ADNI public dataset and visualizes attention to different SNPs, focusing on confirming 12 potential high-risk genes related to AD, including the most classic APOE and recently highlighted significant risk genes. This demonstrates GenDMR's interpretable analytical capability in exploring AD genetic features, providing new insights and perspectives for the development of multimodal data fusion techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…(AD)è¯Šæ–­ä¸­å½±åƒç‰¹å¾å¸¸æ©ç›–é—ä¼ ç‰¹å¾ä»·å€¼ä»¥åŠé—ä¼ ä¿¡æ¯ç¼–ç ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åŠ¨æ€å¤šæ¨¡æ€è§’è‰²äº¤æ¢ç½‘ç»œGenDMRã€‚è¯¥æ¨¡å‹å¼€å‘äº†ä¸€ç§ç¼–ç å•æ ¸è‹·é…¸å¤šæ€æ€§(SNPs)ç©ºé—´ç»„ç»‡çš„æ–°æ–¹æ³•ï¼Œæ˜¾è‘—å¢å¼ºäº†åŸºå› ç»„èƒŒæ™¯çš„è¡¨å¾èƒ½åŠ›ã€‚GenDMR å¼•å…¥äº†å¤šå®ä¾‹æ³¨æ„åŠ›æ¨¡å—(multi-instance attention module)ä»¥è‡ªé€‚åº”é‡åŒ–ç–¾ç—…é£é™©å¹¶æå‡æ¨¡å‹å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶é€šè¿‡ä¸»å¯¼æ¨¡æ€é€‰æ‹©å’Œå¯¹æ¯”è‡ªè’¸é¦æ¨¡å—ï¼Œå®ç°äº†åŸºäºä¸»å¯¼ä¸è¾…åŠ©æ¨¡æ€çš„åŠ¨æ€å¸ˆç”Ÿè§’è‰²äº¤æ¢æœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGenDMR åœ¨ ADNI å…¬å¼€æ•°æ®é›†ä¸Šè¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå¹¶æˆåŠŸè¯†åˆ«å‡ºåŒ…æ‹¬ APOE åœ¨å†…çš„ 12 ä¸ªæ½œåœ¨é«˜é£é™©åŸºå› ã€‚è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€æ•°æ®èåˆæŠ€æœ¯æä¾›äº†æ–°è§†è§’ï¼Œè¯æ˜äº†å…¶åœ¨æ¢ç´¢ AD é—ä¼ ç‰¹å¾æ–¹é¢å…·æœ‰å“è¶Šçš„è§£é‡Šæ€§åˆ†æèƒ½åŠ›ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "q-bio.GN",
      "comment": "31 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.01456v1",
      "published_date": "2025-06-02 09:12:53 UTC",
      "updated_date": "2025-06-02 09:12:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:44:25.616895+00:00"
    },
    {
      "arxiv_id": "2506.01453v2",
      "title": "From Initial Data to Boundary Layers: Neural Networks for Nonlinear Hyperbolic Conservation Laws",
      "title_zh": "ä»åˆå€¼åˆ°è¾¹ç•Œå±‚ï¼šé’ˆå¯¹éçº¿æ€§åŒæ›²å®ˆæ’å¾‹çš„ç¥ç»ç½‘ç»œ",
      "authors": [
        "Igor Ciril",
        "Khalil Haddaoui",
        "Yohann Tendero"
      ],
      "abstract": "We address the approximation of entropy solutions to initial-boundary value problems for nonlinear strictly hyperbolic conservation laws using neural networks. A general and systematic framework is introduced for the design of efficient and reliable learning algorithms, combining fast convergence during training with accurate predictions. The methodology that relies on solving a certain relaxed related problem is assessed through a series of one-dimensional scalar test cases. These numerical experiments demonstrate the potential of the methodology developed in this paper and its applicability to more complex industrial scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç¥ç»ç½‘ç»œ(Neural Networks)æ¥é€¼è¿‘éçº¿æ€§ä¸¥æ ¼åŒæ›²å®ˆæ’å¾‹(nonlinear strictly hyperbolic conservation laws)åˆè¾¹å€¼é—®é¢˜çš„ç†µè§£(entropy solutions)ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªé€šç”¨ä¸”ç³»ç»Ÿçš„æ¡†æ¶ï¼Œæ—¨åœ¨è®¾è®¡é«˜æ•ˆä¸”å¯é çš„å­¦ä¹ ç®—æ³•ï¼Œå¹¶ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¿«é€Ÿæ”¶æ•›ä¸é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé€šè¿‡è§£å†³ç‰¹å®šçš„æ¾å¼›ç›¸å…³é—®é¢˜(relaxed related problem)æ¥æ„å»ºå­¦ä¹ æ¨¡å‹ã€‚é€šè¿‡ä¸€ç³»åˆ—ä¸€ç»´æ ‡é‡æµ‹è¯•æ¡ˆä¾‹çš„æ•°å€¼å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•°å­¦ç‰©ç†é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åº”å¯¹å¤æ‚çš„å·¥ä¸šåº”ç”¨åœºæ™¯æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›å’Œé€‚ç”¨æ€§ã€‚",
      "categories": [
        "math.AP",
        "cs.AI"
      ],
      "primary_category": "math.AP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01453v2",
      "published_date": "2025-06-02 09:12:13 UTC",
      "updated_date": "2025-09-14 18:53:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:44:35.726578+00:00"
    },
    {
      "arxiv_id": "2506.01450v1",
      "title": "ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things",
      "title_zh": "ShaTSï¼šä¸€ç§é¢å‘å·¥ä¸šç‰©è”ç½‘å¼‚å¸¸æ£€æµ‹çš„æ—¶é—´åºåˆ—äººå·¥æ™ºèƒ½æ¨¡å‹ Shapley å¯è§£é‡Šæ€§æ–¹æ³•",
      "authors": [
        "Manuel Franco de la PeÃ±a",
        "Ãngel Luis Perales GÃ³mez",
        "Lorenzo FernÃ¡ndez MaimÃ³"
      ],
      "abstract": "Industrial Internet of Things environments increasingly rely on advanced Anomaly Detection and explanation techniques to rapidly detect and mitigate cyberincidents, thereby ensuring operational safety. The sequential nature of data collected from these environments has enabled improvements in Anomaly Detection using Machine Learning and Deep Learning models by processing time windows rather than treating the data as tabular. However, conventional explanation methods often neglect this temporal structure, leading to imprecise or less actionable explanations. This work presents ShaTS (Shapley values for Time Series models), which is a model-agnostic explainable Artificial Intelligence method designed to enhance the precision of Shapley value explanations for time series models. ShaTS addresses the shortcomings of traditional approaches by incorporating an a priori feature grouping strategy that preserves temporal dependencies and produces both coherent and actionable insights. Experiments conducted on the SWaT dataset demonstrate that ShaTS accurately identifies critical time instants, precisely pinpoints the sensors, actuators, and processes affected by anomalies, and outperforms SHAP in terms of both explainability and resource efficiency, fulfilling the real-time requirements of industrial environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šç‰©è”ç½‘(IIoT)ç¯å¢ƒä¸­çš„å¼‚å¸¸æ£€æµ‹(Anomaly Detection)é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿè§£é‡Šæ–¹æ³•å¾€å¾€å¿½ç•¥æ—¶é—´åºåˆ—æ•°æ®çš„æ—¶åºç»“æ„ï¼Œå¯¼è‡´è§£é‡Šä¸å¤Ÿç²¾ç¡®ä¸”ç¼ºä¹å¯æ“ä½œæ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ShaTS (Shapley values for Time Series models)ï¼Œè¿™æ˜¯ä¸€ç§ä¸æ¨¡å‹æ— å…³çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ—¶é—´åºåˆ—æ¨¡å‹ä¸­Shapleyå€¼çš„è§£é‡Šç²¾åº¦ã€‚ShaTSé€šè¿‡å¼•å…¥ä¸€ç§å…ˆéªŒç‰¹å¾åˆ†ç»„ç­–ç•¥ï¼Œåœ¨ä¿ç•™æ—¶é—´ä¾èµ–æ€§çš„åŒæ—¶ï¼Œç”Ÿæˆå…·æœ‰è¿è´¯æ€§å’Œå¯æ“ä½œæ€§çš„è§è§£ã€‚åœ¨SWaTæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒShaTSèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å…³é”®æ—¶é—´ç‚¹ï¼Œå¹¶ç²¾ç¡®æŒ‡å‡ºå—å¼‚å¸¸å½±å“çš„ä¼ æ„Ÿå™¨ã€æ‰§è¡Œå™¨å’Œå·¥è‰ºæµç¨‹ã€‚ä¸ä¼ ç»Ÿçš„SHAPç›¸æ¯”ï¼ŒShaTSåœ¨å¯è§£é‡Šæ€§å’Œèµ„æºæ•ˆç‡æ–¹é¢å‡è¡¨ç°æ›´ä¼˜ï¼Œèƒ½å¤Ÿæ»¡è¶³å·¥ä¸šç¯å¢ƒå¯¹å®æ—¶æ€§çš„è¦æ±‚ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages;16 figures;Submitted to Elsevier (Information Fusion)",
      "pdf_url": "https://arxiv.org/pdf/2506.01450v1",
      "published_date": "2025-06-02 09:07:27 UTC",
      "updated_date": "2025-06-02 09:07:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:44:34.800077+00:00"
    },
    {
      "arxiv_id": "2506.01442v1",
      "title": "Agentic Episodic Control",
      "title_zh": "æ™ºèƒ½ä½“æƒ…èŠ‚æ§åˆ¶",
      "authors": [
        "Xidong Yang",
        "Wenhao Li",
        "Junjie Sheng",
        "Chuyun Shen",
        "Yun Hua",
        "Xiangfeng Wang"
      ],
      "abstract": "Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to scientific discovery and AI alignment. However, its broader applicability remains limited by challenges such as low data efficiency and poor generalizability. Recent advances suggest that large language models, with their rich world knowledge and reasoning capabilities, could complement RL by enabling semantic state modeling and task-agnostic planning. In this work, we propose the Agentic Episodic Control (AEC), a novel architecture that integrates RL with LLMs to enhance decision-making. The AEC can leverage a large language model (LLM) to map the observations into language-grounded embeddings, which further can be stored in an episodic memory for rapid retrieval of high-value experiences. Simultaneously, a World-Graph working memory module is utilized to capture structured environmental dynamics in order to enhance relational reasoning. Furthermore, a lightweight critical state detector dynamically arbitrates between the episodic memory recall and the world-model-guided exploration. On the whole, by combining the trial-and-error learning scheme with LLM-derived semantic priors, the proposed AEC can improve both data efficiency and generalizability in reinforcement learning. In experiments on BabyAI-Text benchmark tasks, AEC demonstrates substantial improvements over existing baselines, especially on complex and generalization tasks like FindObj, where it outperforms the best baseline by up to 76%. The proposed AEC framework bridges the strengths of numeric reinforcement learning and symbolic reasoning, which provides a pathway toward more adaptable and sample-efficient agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Agentic Episodic Control (AEC)æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ (Reinforcement learning)ä¸­æ•°æ®æ•ˆç‡ä½å’Œæ³›åŒ–èƒ½åŠ›å·®çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚AECé€šè¿‡å°†å¼ºåŒ–å­¦ä¹ ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)ç›¸ç»“åˆï¼Œåˆ©ç”¨LLMå°†è§‚å¯Ÿç»“æœæ˜ å°„ä¸ºä»¥è¯­è¨€ä¸ºåŸºç¡€çš„åµŒå…¥(language-grounded embeddings)ï¼Œå¹¶å­˜å‚¨åœ¨æƒ…èŠ‚è®°å¿†(episodic memory)ä¸­ä»¥å®ç°é«˜ä»·å€¼ç»éªŒçš„å¿«é€Ÿæ£€ç´¢ã€‚åŒæ—¶ï¼Œè¯¥æ¶æ„å¼•å…¥äº†ä¸–ç•Œå›¾(World-Graph)å·¥ä½œè®°å¿†æ¨¡å—æ¥æ•è·ç»“æ„åŒ–çš„ç¯å¢ƒåŠ¨åŠ›å­¦ä»¥å¢å¼ºå…³ç³»æ¨ç†ï¼Œå¹¶åˆ©ç”¨è½»é‡çº§å…³é”®çŠ¶æ€æ£€æµ‹å™¨(critical state detector)åœ¨è®°å¿†å¬å›ä¸ä¸–ç•Œæ¨¡å‹å¼•å¯¼çš„æ¢ç´¢ä¹‹é—´è¿›è¡ŒåŠ¨æ€å†³ç­–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAECåœ¨BabyAI-TextåŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨FindObjç­‰å¤æ‚æ³›åŒ–ä»»åŠ¡ä¸­æ€§èƒ½æå‡é«˜è¾¾76%ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆæ¡¥æ¥äº†æ•°å€¼å¼ºåŒ–å­¦ä¹ ä¸ç¬¦å·æ¨ç†(symbolic reasoning)çš„ä¼˜åŠ¿ï¼Œä¸ºå¼€å‘æ›´å…·é€‚åº”æ€§å’Œé‡‡æ ·æ•ˆç‡çš„æ™ºèƒ½ä½“æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01442v1",
      "published_date": "2025-06-02 08:57:37 UTC",
      "updated_date": "2025-06-02 08:57:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:44:53.191288+00:00"
    },
    {
      "arxiv_id": "2506.01438v2",
      "title": "Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures",
      "title_zh": "åŒºåˆ†è‡ªä¸» AI æ™ºèƒ½ä½“ä¸åä½œå¼æ™ºèƒ½ä½“ç³»ç»Ÿï¼šç†è§£ç°ä»£æ™ºèƒ½æ¶æ„çš„ç»¼åˆæ¡†æ¶",
      "authors": [
        "Prashik Buddhaghosh Bansod"
      ],
      "abstract": "The emergence of large language models has catalyzed two distinct yet interconnected paradigms in artificial intelligence: standalone AI Agents and collaborative Agentic AI ecosystems. This comprehensive study establishes a definitive framework for distinguishing these architectures through systematic analysis of their operational principles, structural compositions, and deployment methodologies. We characterize AI Agents as specialized, tool-enhanced systems leveraging foundation models for targeted automation within constrained environments. Conversely, Agentic AI represents sophisticated multi-entity frameworks where distributed agents exhibit emergent collective intelligence through coordinated interaction protocols. Our investigation traces the evolutionary trajectory from traditional rule-based systems through generative AI foundations to contemporary agent architectures. We present detailed architectural comparisons examining planning mechanisms, memory systems, coordination protocols, and decision-making processes. The study categorizes application landscapes, contrasting single-agent implementations in customer service and content management with multi-agent deployments in research automation and complex decision support. We identify critical challenges including reliability issues, coordination complexities, and scalability constraints, while proposing innovative solutions through enhanced reasoning frameworks, robust memory architectures, and improved coordination mechanisms. This framework provides essential guidance for practitioners selecting appropriate agentic approaches and establishes foundational principles for next-generation intelligent system development.",
      "tldr_zh": "è¯¥ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œç”¨äºåŒºåˆ†ç‹¬ç«‹çš„AI Agentsä¸åä½œå¼çš„Agentic AIç³»ç»Ÿï¼Œæ—¨åœ¨å˜æ¸…ç°ä»£æ™ºèƒ½æ¶æ„çš„æ“ä½œåŸåˆ™ä¸éƒ¨ç½²æ–¹æ³•ã€‚ç ”ç©¶å°†AI Agentså®šä¹‰ä¸ºåœ¨å—é™ç¯å¢ƒä¸‹åˆ©ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§è‡ªåŠ¨åŒ–çš„ä¸“ä¸šåŒ–ç³»ç»Ÿï¼Œè€ŒAgentic AIåˆ™è¢«æè¿°ä¸ºé€šè¿‡åè°ƒåè®®å±•ç°å‡ºé›†ä½“æ™ºèƒ½çš„å¤šå®ä½“åä½œæ¡†æ¶ã€‚è®ºæ–‡ç³»ç»Ÿåœ°è¿½æº¯äº†ä»ä¼ ç»Ÿè§„åˆ™ç³»ç»Ÿåˆ°ç”Ÿæˆå¼AIåŠå…¶ç°ä»£æ™ºèƒ½ä½“æ¶æ„çš„æ¼”åŒ–è·¯å¾„ï¼Œå¹¶å¯¹ä¸¤è€…çš„è§„åˆ’æœºåˆ¶ã€è®°å¿†ç³»ç»Ÿã€åè°ƒåè®®åŠå†³ç­–è¿‡ç¨‹è¿›è¡Œäº†æ·±åº¦å¯¹æ¯”ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ’åˆ†äº†åº”ç”¨åœºæ™¯ï¼Œå¯¹æ¯”äº†å®¢æœé¢†åŸŸçš„å•æ™ºèƒ½ä½“å®ç°ä¸å¤æ‚å†³ç­–æ”¯æŒä¸­çš„å¤šæ™ºèƒ½ä½“éƒ¨ç½²ã€‚é’ˆå¯¹å¯é æ€§ã€åè°ƒå¤æ‚åº¦å’Œå¯æ‰©å±•æ€§ç­‰æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†å¢å¼ºæ¨ç†æ¡†æ¶å’Œç¨³å¥è®°å¿†æ¶æ„ç­‰åˆ›æ–°æ€§è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶ä¸ºä»ä¸šè€…é€‰æ‹©åˆé€‚çš„æ™ºèƒ½ä½“è·¯å¾„æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œå¹¶ä¸ºä¸‹ä¸€ä»£æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "There may be overlap with another author's work. I am withdrawing this for me to review further",
      "pdf_url": "https://arxiv.org/pdf/2506.01438v2",
      "published_date": "2025-06-02 08:52:23 UTC",
      "updated_date": "2025-06-16 17:03:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:45:41.404728+00:00"
    },
    {
      "arxiv_id": "2506.14798v1",
      "title": "MODS: Multi-source Observations Conditional Diffusion Model for Meteorological State Downscaling",
      "title_zh": "MODSï¼šé¢å‘æ°”è±¡è¦ç´ é™å°ºåº¦çš„å¤šæºè§‚æµ‹æ¡ä»¶æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Siwei Tu",
        "Jingyi Xu",
        "Weidong Yang",
        "Lei Bai",
        "Ben Fei"
      ],
      "abstract": "Accurate acquisition of high-resolution surface meteorological conditions is critical for forecasting and simulating meteorological variables. Directly applying spatial interpolation methods to derive meteorological values at specific locations from low-resolution grid fields often yields results that deviate significantly from the actual conditions. Existing downscaling methods primarily rely on the coupling relationship between geostationary satellites and ERA5 variables as a condition. However, using brightness temperature data from geostationary satellites alone fails to comprehensively capture all the changes in meteorological variables in ERA5 maps. To address this limitation, we can use a wider range of satellite data to make more full use of its inversion effects on various meteorological variables, thus producing more realistic results across different meteorological variables. To further improve the accuracy of downscaling meteorological variables at any location, we propose the Multi-source Observation Down-Scaling Model (MODS). It is a conditional diffusion model that fuses data from multiple geostationary satellites GridSat, polar-orbiting satellites (AMSU-A, HIRS, and MHS), and topographic data (GEBCO), as conditions, and is pre-trained on the ERA5 reanalysis dataset. During training, latent features from diverse conditional inputs are extracted separately and fused into ERA5 maps via a multi-source cross-attention module. By exploiting the inversion relationships between reanalysis data and multi-source atmospheric variables, MODS generates atmospheric states that align more closely with real-world conditions. During sampling, MODS enhances downscaling consistency by incorporating low-resolution ERA5 maps and station-level meteorological data as guidance. Experimental results demonstrate that MODS achieves higher fidelity when downscaling ERA5 maps to a 6.25 km resolution.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MODSï¼Œä¸€ç§åŸºäºå¤šæºè§‚æµ‹æ¡ä»¶çš„æ‰©æ•£æ¨¡å‹(Conditional Diffusion Model)ï¼Œæ—¨åœ¨è§£å†³æ°”è±¡çŠ¶æ€é™å°ºåº¦(Downscaling)ä¸­ä¼ ç»Ÿç©ºé—´æ’å€¼å’Œå•ä¸€å«æ˜Ÿæ•°æ®æºå¯¼è‡´çš„ç²¾åº¦ä¸è¶³é—®é¢˜ã€‚MODSæœ‰æ•ˆèåˆäº†åœ°çƒé™æ­¢å«æ˜ŸGridSatã€æè½¨å«æ˜Ÿ(AMSU-A, HIRS, MHS)ä»¥åŠåœ°å½¢æ•°æ®(GEBCO)ä½œä¸ºçº¦æŸæ¡ä»¶ï¼Œå¼¥è¡¥äº†å•ä¸€äº®åº¦æ¸©åº¦æ•°æ®æ— æ³•å…¨é¢æ•æ‰æ°”è±¡å˜é‡å˜åŒ–çš„ç¼ºé™·ã€‚è¯¥æ¨¡å‹åœ¨ERA5é‡åˆ†ææ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œé€šè¿‡å¤šæºäº¤å‰æ³¨æ„åŠ›(Multi-source Cross-attention)æ¨¡å—æå–å¹¶èåˆä¸åŒè§‚æµ‹æºçš„æ½œç‰¹å¾ï¼Œå……åˆ†åˆ©ç”¨å«æ˜Ÿæ•°æ®å¯¹å¤§æ°”å˜é‡çš„åæ¼”å…³ç³»ã€‚åœ¨é‡‡æ ·é˜¶æ®µï¼ŒMODSå¼•å…¥ä½åˆ†è¾¨ç‡ERA5å›¾è°±å’Œç«™ç‚¹çº§åˆ«æ°”è±¡æ•°æ®ä½œä¸ºå¼•å¯¼ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç”Ÿæˆç»“æœçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMODSèƒ½å°†ERA5æ•°æ®æœ‰æ•ˆé™å°ºåº¦è‡³6.25 kmåˆ†è¾¨ç‡ï¼Œä¸”åœ¨ç”Ÿæˆæ°”è±¡çŠ¶æ€çš„çœŸå®æ€§ä¸ä¿çœŸåº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14798v1",
      "published_date": "2025-06-02 08:40:16 UTC",
      "updated_date": "2025-06-02 08:40:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:44:40.839495+00:00"
    },
    {
      "arxiv_id": "2506.01423v1",
      "title": "FinRobot: Generative Business Process AI Agents for Enterprise Resource Planning in Finance",
      "title_zh": "FinRobotï¼šé¢å‘é‡‘èé¢†åŸŸä¼ä¸šèµ„æºè®¡åˆ’çš„ç”Ÿæˆå¼ä¸šåŠ¡æµç¨‹ AI æ™ºèƒ½ä½“",
      "authors": [
        "Hongyang Yang",
        "Likun Lin",
        "Yang She",
        "Xinyu Liao",
        "Jiaoyang Wang",
        "Runjia Zhang",
        "Yuquan Mo",
        "Christina Dan Wang"
      ],
      "abstract": "Enterprise Resource Planning (ERP) systems serve as the digital backbone of modern financial institutions, yet they continue to rely on static, rule-based workflows that limit adaptability, scalability, and intelligence. As business operations grow more complex and data-rich, conventional ERP platforms struggle to integrate structured and unstructured data in real time and to accommodate dynamic, cross-functional workflows.\n  In this paper, we present the first AI-native, agent-based framework for ERP systems, introducing a novel architecture of Generative Business Process AI Agents (GBPAs) that bring autonomy, reasoning, and dynamic optimization to enterprise workflows. The proposed system integrates generative AI with business process modeling and multi-agent orchestration, enabling end-to-end automation of complex tasks such as budget planning, financial reporting, and wire transfer processing. Unlike traditional workflow engines, GBPAs interpret user intent, synthesize workflows in real time, and coordinate specialized sub-agents for modular task execution. We validate the framework through case studies in bank wire transfers and employee reimbursements, two representative financial workflows with distinct complexity and data modalities. Results show that GBPAs achieve up to 40% reduction in processing time, 94% drop in error rate, and improved regulatory compliance by enabling parallelism, risk control insertion, and semantic reasoning. These findings highlight the potential of GBPAs to bridge the gap between generative AI capabilities and enterprise-grade automation, laying the groundwork for the next generation of intelligent ERP systems.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªé¢å‘ä¼ä¸šèµ„æºè§„åˆ’(ERP)ç³»ç»Ÿçš„AIåŸç”Ÿæ™ºèƒ½ä½“æ¡†æ¶FinRobotï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç³»ç»Ÿä¾èµ–é™æ€è§„åˆ™ä¸”éš¾ä»¥é€‚åº”å¤æ‚åŠ¨æ€å·¥ä½œæµçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç”Ÿæˆå¼ä¸šåŠ¡æµç¨‹AIæ™ºèƒ½ä½“(Generative Business Process AI Agents, GBPAs)æ¶æ„ï¼Œå°†ç”Ÿæˆå¼AI(Generative AI)ä¸ä¸šåŠ¡æµç¨‹å»ºæ¨¡(Business Process Modeling)åŠå¤šæ™ºèƒ½ä½“ç¼–æ’(Multi-Agent Orchestration)ç›¸ç»“åˆã€‚GBPAsèƒ½å¤Ÿå®æ—¶ç†è§£ç”¨æˆ·æ„å›¾å¹¶åˆæˆå·¥ä½œæµï¼Œé€šè¿‡åè°ƒä¸“é—¨çš„å­æ™ºèƒ½ä½“å®ç°é¢„ç®—è§„åˆ’ã€è´¢åŠ¡æŠ¥å‘Šå’Œç”µæ±‡å¤„ç†ç­‰ä»»åŠ¡çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ã€‚åœ¨é“¶è¡Œç”µæ±‡å’Œå‘˜å·¥æŠ¥é”€çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè¯¥æ¡†æ¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå°†å¤„ç†æ—¶é—´ç¼©çŸ­äº†40%ï¼Œé”™è¯¯ç‡é™ä½äº†94%ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¯­ä¹‰æ¨ç†(Semantic Reasoning)å’Œé£é™©æ§åˆ¶çš„åµŒå…¥ï¼ŒFinRobotæ˜¾è‘—æå‡äº†ä¼ä¸šçš„åˆè§„æ€§ä¸è¿è¥æ•ˆç‡ã€‚è¿™äº›å‘ç°å±•ç¤ºäº†GBPAsåœ¨å¼¥åˆç”Ÿæˆå¼AIèƒ½åŠ›ä¸ä¼ä¸šçº§è‡ªåŠ¨åŒ–é¸¿æ²Ÿæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºä¸‹ä¸€ä»£æ™ºèƒ½ERPç³»ç»Ÿçš„å‘å±•æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE",
        "q-fin.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01423v1",
      "published_date": "2025-06-02 08:22:28 UTC",
      "updated_date": "2025-06-02 08:22:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:44:55.594686+00:00"
    },
    {
      "arxiv_id": "2506.01413v8",
      "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models",
      "title_zh": "æ¿€åŠ±å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä»¥å®ç°é«˜çº§æŒ‡ä»¤éµå¾ª",
      "authors": [
        "Yulei Qin",
        "Gang Li",
        "Zongyi Li",
        "Zihan Xu",
        "Yuchen Shi",
        "Zhekai Lin",
        "Xiao Cui",
        "Ke Li",
        "Xing Sun"
      ],
      "abstract": "Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF. Codes and data are available at https://github.com/yuleiqin/RAIF.\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions",
      "tldr_zh": "ç°æœ‰çš„ä¸»æµå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†åŒ…å«å¹³è¡Œã€é“¾å¼å’Œåˆ†æ”¯ç»“æ„çš„å¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä¼ ç»Ÿçš„Chain-of-Thought (CoT)ç”±äºå…¶æµ…å±‚çš„æ¨ç†æ¨¡å¼ï¼Œå¾€å¾€éš¾ä»¥æœ‰æ•ˆè¯†åˆ«å’Œå¤„ç†å¤æ‚æŒ‡ä»¤ä¸­çš„å¤šé‡çº¦æŸã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†RAIFï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ¿€åŠ±æ¨ç†æ¥æå‡æµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾(Test-time compute scaling)æ•ˆèƒ½çš„ç³»ç»Ÿæ€§æ–¹æ³•ã€‚RAIFé¦–å…ˆåŸºäºç°æœ‰åˆ†ç±»æ³•å¯¹å¤æ‚æŒ‡ä»¤è¿›è¡Œåˆ†è§£å¹¶æå‡ºå¯å¤ç°çš„æ•°æ®è·å–æ–¹æ³•ï¼Œéšååˆ©ç”¨å¼ºåŒ–å­¦ä¹ (RL)ç»“åˆå¯éªŒè¯çš„ä»¥è§„åˆ™ä¸ºä¸­å¿ƒçš„å¥–åŠ±ä¿¡å·(Verifiable rule-centric reward signals)æ¥åŸ¹å…»ä¸“é—¨é’ˆå¯¹æŒ‡ä»¤éµå¾ªçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡æ ·æœ¬å¯¹æ¯”å¼ºåŒ–CoTçš„æ‰§è¡Œï¼Œè§£å†³äº†å¤æ‚æŒ‡ä»¤ä¸‹æ¨ç†æµ…å±‚åŒ–çš„é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ä¸“å®¶è¡Œä¸ºå…‹éš†(Behavior cloning)ä¿ƒè¿›æ¨¡å‹ä»å¿«é€Ÿæ€è€ƒå‘ç†Ÿç»ƒæ¨ç†çš„å¹³ç¨³è½¬å˜ã€‚åœ¨ä¸ƒä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåº”ç”¨RAIFåçš„1.5Bè§„æ¨¡æ¨¡å‹è·å¾—äº†11.74%çš„æ€§èƒ½æå‡ï¼Œè¡¨ç°è¶³ä»¥åª²ç¾8Bè§„æ¨¡çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨åˆ†å¸ƒå¤–(OOD)çº¦æŸä¸‹çš„æµ‹è¯•ä¹Ÿè¿›ä¸€æ­¥éªŒè¯äº†RAIFå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2025; 15 pages of main body, 5 tables, 5 figures, 42 pages of appendix",
      "pdf_url": "https://arxiv.org/pdf/2506.01413v8",
      "published_date": "2025-06-02 08:11:44 UTC",
      "updated_date": "2025-09-30 14:50:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:45:19.217961+00:00"
    },
    {
      "arxiv_id": "2506.01412v1",
      "title": "System Calls for Malware Detection and Classification: Methodologies and Applications",
      "title_zh": "ç”¨äºæ¶æ„è½¯ä»¶æ£€æµ‹ä¸åˆ†ç±»çš„ç³»ç»Ÿè°ƒç”¨ï¼šæ–¹æ³•è®ºä¸åº”ç”¨",
      "authors": [
        "Bishwajit Prasad Gond",
        "Durga Prasad Mohapatra"
      ],
      "abstract": "As malware continues to become more complex and harder to detect, Malware Analysis needs to continue to evolve to stay one step ahead. One promising key area approach focuses on using system calls and API Calls, the core communication between user applications and the operating system and their kernels. These calls provide valuable insight into how software or programs behaves, making them an useful tool for spotting suspicious or harmful activity of programs and software. This chapter takes a deep down look at how system calls are used in malware detection and classification, covering techniques like static and dynamic analysis, as well as sandboxing. By combining these methods with advanced techniques like machine learning, statistical analysis, and anomaly detection, researchers can analyze system call patterns to tell the difference between normal and malicious behavior. The chapter also explores how these techniques are applied across different systems, including Windows, Linux, and Android, while also looking at the ways sophisticated malware tries to evade detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†åœ¨æ¶æ„è½¯ä»¶åˆ†æï¼ˆMalware Analysisï¼‰ä¸æ–­æ¼”è¿›çš„èƒŒæ™¯ä¸‹ï¼Œåˆ©ç”¨ç³»ç»Ÿè°ƒç”¨ï¼ˆsystem callsï¼‰å’Œ API è°ƒç”¨ï¼ˆAPI Callsï¼‰ä½œä¸ºæ£€æµ‹ä¸åˆ†ç±»çš„æ ¸å¿ƒæ‰‹æ®µã€‚ç³»ç»Ÿè°ƒç”¨åæ˜ äº†ç”¨æˆ·åº”ç”¨ç¨‹åºä¸æ“ä½œç³»ç»Ÿå†…æ ¸ï¼ˆkernelsï¼‰ä¹‹é—´çš„å…³é”®é€šä¿¡ï¼Œä¸ºè¯†åˆ«ç¨‹åºçš„æ¶æ„è¡Œä¸ºæä¾›äº†é‡è¦æ´å¯Ÿã€‚æ–‡ç« è¯¦ç»†é˜è¿°äº†ç»“åˆé™æ€åˆ†æï¼ˆstatic analysisï¼‰ã€åŠ¨æ€åˆ†æï¼ˆdynamic analysisï¼‰åŠæ²™ç®±æŠ€æœ¯ï¼ˆsandboxingï¼‰çš„æ–¹æ³•è®ºã€‚é€šè¿‡å°†ä¸Šè¿°æ–¹æ³•ä¸æœºå™¨å­¦ä¹ ï¼ˆmachine learningï¼‰ã€ç»Ÿè®¡åˆ†æï¼ˆstatistical analysisï¼‰å’Œå¼‚å¸¸æ£€æµ‹ï¼ˆanomaly detectionï¼‰ç­‰é«˜çº§æŠ€æœ¯ç›¸ç»“åˆï¼Œç ”ç©¶äººå‘˜èƒ½å¤Ÿç²¾å‡†åŒºåˆ†æ­£å¸¸ä¸æ¶æ„çš„ç³»ç»Ÿè°ƒç”¨æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ†æäº†è¿™äº›æŠ€æœ¯åœ¨ Windowsã€Linux å’Œ Android ç­‰å¤šå¹³å°ä¸Šçš„å®é™…åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†é«˜çº§æ¶æ„è½¯ä»¶é€ƒé¿æ£€æµ‹ï¼ˆevade detectionï¼‰çš„å¤æ‚æ‰‹æ®µã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01412v1",
      "published_date": "2025-06-02 08:11:27 UTC",
      "updated_date": "2025-06-02 08:11:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:44:57.461601+00:00"
    },
    {
      "arxiv_id": "2506.01411v1",
      "title": "ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition",
      "title_zh": "ViTA-PARï¼šç»“åˆå±æ€§æç¤ºä¸è§†è§‰-æ–‡æœ¬å±æ€§å¯¹é½çš„è¡Œäººå±æ€§è¯†åˆ«",
      "authors": [
        "Minjeong Park",
        "Hongbeen Park",
        "Jinkyu Kim"
      ],
      "abstract": "The Pedestrian Attribute Recognition (PAR) task aims to identify various detailed attributes of an individual, such as clothing, accessories, and gender. To enhance PAR performance, a model must capture features ranging from coarse-grained global attributes (e.g., for identifying gender) to fine-grained local details (e.g., for recognizing accessories) that may appear in diverse regions. Recent research suggests that body part representation can enhance the model's robustness and accuracy, but these methods are often restricted to attribute classes within fixed horizontal regions, leading to degraded performance when attributes appear in varying or unexpected body locations. In this paper, we propose Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance attribute recognition through specialized multimodal prompting and vision-language alignment. We introduce visual attribute prompts that capture global-to-local semantics, enabling diverse attribute representations. To enrich textual embeddings, we design a learnable prompt template, termed person and attribute context prompting, to learn person and attributes context. Finally, we align visual and textual attribute features for effective fusion. ViTA-PAR is validated on four PAR benchmarks, achieving competitive performance with efficient inference. We release our code and model at https://github.com/mlnjeongpark/ViTA-PAR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡Œäººå±æ€§è¯†åˆ«(Pedestrian Attribute Recognition, PAR)ä»»åŠ¡ä¸­ç²—ç²’åº¦å…¨å±€ç‰¹å¾ä¸ç»†ç²’åº¦å±€éƒ¨ç»†èŠ‚æ•æ‰å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ViTA-PARæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰åŸºäºèº«ä½“éƒ¨ä½çš„æ–¹æ³•å—é™äºå›ºå®šæ°´å¹³åŒºåŸŸã€éš¾ä»¥å¤„ç†ä½ç½®å¤šå˜çš„å±æ€§è¿™ä¸€å±€é™æ€§ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†è§†è§‰ä¸æ–‡æœ¬å±æ€§å¯¹é½åŠå±æ€§æç¤º(Attribute Prompting)ç­–ç•¥ã€‚ViTA-PARå¼•å…¥äº†èƒ½å¤Ÿæ•æ‰ä»å…¨å±€åˆ°å±€éƒ¨è¯­ä¹‰çš„è§†è§‰å±æ€§æç¤ºï¼Œä»¥æ”¯æŒå¤šæ ·åŒ–çš„å±æ€§è¡¨å¾ã€‚åŒæ—¶ï¼Œé€šè¿‡è®¾è®¡å¯å­¦ä¹ çš„è¡Œäººä¸å±æ€§ä¸Šä¸‹æ–‡æç¤º(person and attribute context prompting)æ¨¡æ¿æ¥å¯ŒåŒ–æ–‡æœ¬åµŒå…¥ï¼Œå¹¶æœ€ç»ˆå¯¹é½è§†è§‰ä¸æ–‡æœ¬ç‰¹å¾ä»¥å®ç°æœ‰æ•ˆèåˆã€‚åœ¨å››ä¸ªPARåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒViTA-PARåœ¨ä¿æŒé«˜æ•ˆæ¨ç†çš„åŒæ—¶è¾¾åˆ°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to IEEE ICIP 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01411v1",
      "published_date": "2025-06-02 08:07:06 UTC",
      "updated_date": "2025-06-02 08:07:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:45:25.191112+00:00"
    },
    {
      "arxiv_id": "2506.02073v1",
      "title": "Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability",
      "title_zh": "Flow2Codeï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åŸºäºæµç¨‹å›¾çš„ä»£ç ç”Ÿæˆèƒ½åŠ›",
      "authors": [
        "Mengliang He",
        "Jiayi Zeng",
        "Yankai Jiang",
        "Wei Zhang",
        "Zeming Liu",
        "Xiaoming Shi",
        "Aimin Zhou"
      ],
      "abstract": "While large language models (LLMs) show promise in code generation, existing benchmarks neglect the flowchart-based code generation. To promote further research on flowchart-based code generation, this work presents Flow2Code, a novel benchmark for flowchart-based code generation evaluation. The evaluation dataset spans 15 programming languages and includes 5,622 code segments paired with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive experiments with 13 multimodal LLMs reveal that current LLMs can not generate code based on flowcharts perfectly. Besides, experiment results show that the supervised fine-tuning technique contributes greatly to the models' performance. We publicly release our code and datasets at https://github.com/hml-github/Flow2Code.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•å¿½ç•¥åŸºäºæµç¨‹å›¾ç”Ÿæˆä»£ç çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º Flow2Code çš„æ–°å‹è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚Flow2Code æ•°æ®é›†æ¶µç›– 15 ç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…å« 5,622 ä¸ªä»£ç æ®µä»¥åŠä¸å…¶é…å¯¹çš„ 16,866 ä¸ªæµç¨‹å›¾ï¼Œæ¶µç›– codeã€UML å’Œ pseudocode ä¸‰ç§ç±»å‹ã€‚é€šè¿‡å¯¹ 13 ç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal LLMs) çš„å¹¿æ³›å®éªŒï¼Œç»“æœæ˜¾ç¤ºå½“å‰æ¨¡å‹å°šæ— æ³•å®Œç¾åœ°å®ç°åŸºäºæµç¨‹å›¾çš„ä»£ç ç”Ÿæˆã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œæœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT) æŠ€æœ¯èƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚è¯¥å·¥ä½œå…¬å¼€äº†ç›¸å…³ä»£ç å’Œæ•°æ®é›†ï¼Œä¸ºåç»­ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†å›¾å½¢é€»è¾‘åˆ°ä»£ç è½¬æ¢çš„èƒ½åŠ›å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02073v1",
      "published_date": "2025-06-02 07:48:57 UTC",
      "updated_date": "2025-06-02 07:48:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:46:51.846094+00:00"
    },
    {
      "arxiv_id": "2506.01392v1",
      "title": "Sparse Imagination for Efficient Visual World Model Planning",
      "title_zh": "ç¨€ç–æƒ³è±¡ï¼šé¢å‘é«˜æ•ˆè§†è§‰ä¸–ç•Œæ¨¡å‹è§„åˆ’",
      "authors": [
        "Junha Chun",
        "Youngjoon Jeong",
        "Taesup Kim"
      ],
      "abstract": "World model based planning has significantly improved decision-making in complex environments by enabling agents to simulate future states and make informed choices. However, ensuring the prediction accuracy of world models often demands substantial computational resources, posing a major challenge for real-time applications. This computational burden is particularly restrictive in robotics, where resources are severely constrained. To address this limitation, we propose a Sparse Imagination for Efficient Visual World Model Planning, which enhances computational efficiency by reducing the number of tokens processed during forward prediction. Our method leverages a sparsely trained vision-based world model based on transformers with randomized grouped attention strategy, allowing the model to adaptively adjust the number of tokens processed based on the computational resource. By enabling sparse imagination (rollout), our approach significantly accelerates planning while maintaining high control fidelity. Experimental results demonstrate that sparse imagination preserves task performance while dramatically improving inference efficiency, paving the way for the deployment of world models in real-time decision-making scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºWorld Modelsçš„è§„åˆ’åœ¨æœºå™¨äººç­‰å®æ—¶åº”ç”¨ä¸­å› è®¡ç®—èµ„æºå—é™è€Œé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†Sparse Imaginationæ¡†æ¶ä»¥æå‡è§†è§‰ä¸–ç•Œæ¨¡å‹è§„åˆ’çš„æ•ˆç‡ã€‚è¯¥æ–¹æ³•æ ¸å¿ƒåœ¨äºåˆ©ç”¨ä¸€ç§åŸºäºTransformerså¹¶ç»“åˆéšæœºåˆ†ç»„æ³¨æ„åŠ›ç­–ç•¥(randomized grouped attention strategy)çš„ç¨€ç–è®­ç»ƒè§†è§‰ä¸–ç•Œæ¨¡å‹ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®å¯ç”¨è®¡ç®—èµ„æºè‡ªé€‚åº”åœ°è°ƒæ•´å‰å‘é¢„æµ‹ä¸­å¤„ç†çš„tokenæ•°é‡ã€‚é€šè¿‡è¿™ç§ç¨€ç–æƒ³è±¡(Sparse Imagination)æœºåˆ¶ï¼Œæ¨¡å‹åœ¨è¿›è¡Œè·¯å¾„æ¼”ç»ƒ(rollout)æ—¶èƒ½æ˜¾è‘—é™ä½è®¡ç®—è´Ÿæ‹…å¹¶å¤§å¹…åŠ é€Ÿè§„åˆ’è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ§åˆ¶ä¿çœŸåº¦å’Œä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæå¤§æé«˜äº†æ¨ç†æ•ˆç‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåœ¨èµ„æºå—é™çš„å®æ—¶å†³ç­–åœºæ™¯ä¸­éƒ¨ç½²å¤æ‚çš„ä¸–ç•Œæ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01392v1",
      "published_date": "2025-06-02 07:36:14 UTC",
      "updated_date": "2025-06-02 07:36:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:46:02.086922+00:00"
    },
    {
      "arxiv_id": "2506.01391v2",
      "title": "AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning",
      "title_zh": "AgentCPM-GUIï¼šåŸºäºå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ„å»ºç§»åŠ¨ç«¯æ™ºèƒ½ä½“",
      "authors": [
        "Zhong Zhang",
        "Yaxi Lu",
        "Yikun Fu",
        "Yupeng Huo",
        "Shenzhi Yang",
        "Yesai Wu",
        "Han Si",
        "Xin Cong",
        "Haotian Chen",
        "Yankai Lin",
        "Jie Xie",
        "Wei Zhou",
        "Wang Xu",
        "Yuanheng Zhang",
        "Zhou Su",
        "Zhongwu Zhai",
        "Xiaoming Liu",
        "Yudong Mei",
        "Jianming Xu",
        "Hongyan Tian",
        "Chongyi Wang",
        "Chi Chen",
        "Yuan Yao",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and $91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgentCPM-GUIï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 8B å‚æ•°çš„ GUI æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨é€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢å®ç°é²æ£’ä¸”é«˜æ•ˆçš„ç§»åŠ¨è®¾å¤‡äº¤äº’ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ¨¡å‹åœ¨æ„ŸçŸ¥å®šä½ç²¾ç¡®åº¦ã€æ³›åŒ–èƒ½åŠ›åŠå¤šè¯­è¨€æ”¯æŒï¼ˆå¦‚ä¸­æ–‡åº”ç”¨ï¼‰æ–¹é¢çš„å±€é™ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº† Grounding-aware pre-training ä»¥å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åœ¨é«˜è´¨é‡çš„ä¸­è‹±æ–‡è½¨è¿¹ä¸Šè¿›è¡Œ Supervised Fine-tuning (SFT) æ¥æ¨¡ä»¿äººç±»è¡Œä¸ºï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†åŸºäº GRPO çš„ Reinforcement Fine-tuning (RFT) ä»¥æ˜¾è‘—æå‡é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒAgentCPM-GUI é‡‡ç”¨äº†ç´§å‡‘çš„ Action space è®¾è®¡ï¼Œæœ‰æ•ˆç¼©çŸ­äº†è¾“å‡ºé•¿åº¦å¹¶æ”¯æŒç§»åŠ¨ç«¯çš„ä½å»¶è¿Ÿæ‰§è¡Œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨äº”ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä»¥åŠå…¨æ–°çš„ä¸­æ–‡ GUI åŸºå‡† CAGUI ä¸Šå‡è¾¾åˆ°äº† State-of-the-art æ€§èƒ½ï¼Œåˆ†åˆ«å®ç°äº† 96.9% çš„ Type-Match å’Œ 91.3% çš„ Exact-Match å‡†ç¡®ç‡ã€‚è¯¥å·¥ä½œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ³›åŒ–æ°´å¹³ï¼Œä¸ºæ„å»ºè·¨è¯­è¨€çš„é«˜æ•ˆç§»åŠ¨ç«¯æ™ºèƒ½åŠ©æ‰‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "Updated results in Table 2 and Table 3; The project is available at https://github.com/OpenBMB/AgentCPM-GUI",
      "pdf_url": "https://arxiv.org/pdf/2506.01391v2",
      "published_date": "2025-06-02 07:30:29 UTC",
      "updated_date": "2025-06-17 02:57:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:46:04.356936+00:00"
    },
    {
      "arxiv_id": "2506.01388v1",
      "title": "VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding",
      "title_zh": "VRD-IUï¼šè§†è§‰ä¸°å¯Œæ–‡æ¡£æ™ºèƒ½ä¸ç†è§£çš„ç»éªŒä¸å¯ç¤º",
      "authors": [
        "Yihao Ding",
        "Soyeon Caren Han",
        "Yan Li",
        "Josiah Poon"
      ],
      "abstract": "Visually Rich Document Understanding (VRDU) has emerged as a critical field in document intelligence, enabling automated extraction of key information from complex documents across domains such as medical, financial, and educational applications. However, form-like documents pose unique challenges due to their complex layouts, multi-stakeholder involvement, and high structural variability. Addressing these issues, the VRD-IU Competition was introduced, focusing on extracting and localizing key information from multi-format forms within the Form-NLU dataset, which includes digital, printed, and handwritten documents. This paper presents insights from the competition, which featured two tracks: Track A, emphasizing entity-based key information retrieval, and Track B, targeting end-to-end key information localization from raw document images. With over 20 participating teams, the competition showcased various state-of-the-art methodologies, including hierarchical decomposition, transformer-based retrieval, multimodal feature fusion, and advanced object detection techniques. The top-performing models set new benchmarks in VRDU, providing valuable insights into document intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£ï¼ˆVisually Rich Document Understanding, VRDUï¼‰åœ¨å¤„ç†å¤æ‚è¡¨å•æ–‡æ¡£æ—¶é¢ä¸´çš„å¸ƒå±€å¤šæ ·æ€§å’Œç»“æ„å˜å¼‚ç­‰æŒ‘æˆ˜ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†VRD-IUç«èµ›çš„ç›¸å…³æˆæœï¼Œè¯¥ç«èµ›åŸºäºåŒ…å«æ•°å­—ã€å°åˆ·å’Œæ‰‹å†™æ–‡æ¡£çš„Form-NLUæ•°æ®é›†ï¼Œè®¾ç«‹äº†ä¾§é‡å®ä½“ä¿¡æ¯æ£€ç´¢çš„Track Aå’Œä¾§é‡ç«¯åˆ°ç«¯ä¿¡æ¯å®šä½çš„Track Bã€‚è¶…è¿‡20æ”¯å‚èµ›å›¢é˜Ÿå±•ç¤ºäº†åŒ…æ‹¬åˆ†å±‚åˆ†è§£ï¼ˆhierarchical decompositionï¼‰ã€åŸºäºTransformerçš„æ£€ç´¢ã€å¤šæ¨¡æ€ç‰¹å¾èåˆï¼ˆmultimodal feature fusionï¼‰ä»¥åŠå…ˆè¿›çš„ç›®æ ‡æ£€æµ‹ï¼ˆobject detectionï¼‰ç­‰å‰æ²¿æŠ€æœ¯ã€‚æœ€ç»ˆï¼Œä¼˜èƒœæ¨¡å‹åˆ·æ–°äº†VRDUçš„æŠ€æœ¯åŸºå‡†ï¼Œä¸ºåŒ»ç–—ã€é‡‘èå’Œæ•™è‚²é¢†åŸŸçš„è‡ªåŠ¨åŒ–æ–‡æ¡£å¤„ç†æä¾›äº†å®è´µçš„ç»éªŒã€‚è¯¥ç«èµ›çš„å®æ–½ä¸ä»…æä¾›äº†æ·±å…¥çš„æ–‡æ¡£æ™ºèƒ½è§è§£ï¼Œä¹Ÿä¸ºæœªæ¥å¤æ‚æ–‡æ¡£çš„ä¿¡æ¯æå–ä¸å®šä½å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at IJCAI 2025 Demonstrations Track",
      "pdf_url": "https://arxiv.org/pdf/2506.01388v1",
      "published_date": "2025-06-02 07:28:28 UTC",
      "updated_date": "2025-06-02 07:28:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:46:11.539648+00:00"
    },
    {
      "arxiv_id": "2506.01380v2",
      "title": "Playing with Transformer at 30+ FPS via Next-Frame Diffusion",
      "title_zh": "åŸºäºä¸‹ä¸€å¸§æ‰©æ•£å®ç° 30+ FPS çš„ Transformer å®æ—¶äº¤äº’",
      "authors": [
        "Xinle Cheng",
        "Tianyu He",
        "Jiayi Xu",
        "Junliang Guo",
        "Di He",
        "Jiang Bian"
      ],
      "abstract": "Autoregressive video models offer distinct advantages over bidirectional diffusion models in creating interactive video content and supporting streaming applications with arbitrary duration. In this work, we present Next-Frame Diffusion (NFD), an autoregressive diffusion transformer that incorporates block-wise causal attention, enabling iterative sampling and efficient inference via parallel token generation within each frame. Nonetheless, achieving real-time video generation remains a significant challenge for such models, primarily due to the high computational cost associated with diffusion sampling and the hardware inefficiencies inherent to autoregressive generation. To address this, we introduce two innovations: (1) We extend consistency distillation to the video domain and adapt it specifically for video models, enabling efficient inference with few sampling steps; (2) To fully leverage parallel computation, motivated by the observation that adjacent frames often share the identical action input, we propose speculative sampling. In this approach, the model generates next few frames using current action input, and discard speculatively generated frames if the input action differs. Experiments on a large-scale action-conditioned video generation benchmark demonstrate that NFD beats autoregressive baselines in terms of both visual quality and sampling efficiency. We, for the first time, achieves autoregressive video generation at over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Next-Frame Diffusion (NFD)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³è‡ªå›å½’è§†é¢‘æ¨¡å‹åœ¨å®æ—¶ç”Ÿæˆä¸­é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬å’Œç¡¬ä»¶æ•ˆç‡ä½ä¸‹é—®é¢˜çš„è‡ªå›å½’æ‰©æ•£ Transformer æ¡†æ¶ã€‚NFD ç»“åˆäº†å—çŠ¶å› æœæ³¨æ„åŠ› (block-wise causal attention) æœºåˆ¶ï¼Œé€šè¿‡å¸§å†…å¹¶è¡Œ Token ç”Ÿæˆå®ç°äº†é«˜æ•ˆçš„è¿­ä»£é‡‡æ ·å’Œæ¨ç†ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ¨ç†é€Ÿåº¦ï¼Œç ”ç©¶è€…å°†ä¸€è‡´æ€§è’¸é¦ (consistency distillation) æŠ€æœ¯æ‰©å±•å¹¶é€‚é…è‡³è§†é¢‘é¢†åŸŸï¼Œå¤§å¹…å‡å°‘äº†æ‰©æ•£é‡‡æ ·æ‰€éœ€çš„æ­¥éª¤ã€‚æ­¤å¤–ï¼Œå—ç›¸é‚»å¸§å…±äº«åŠ¨ä½œè¾“å…¥å¯å‘ï¼Œè¯¥ç ”ç©¶æå‡ºäº†æ¨æµ‹æ€§é‡‡æ · (speculative sampling) æ–¹æ³•ï¼Œåˆ©ç”¨å¹¶è¡Œè®¡ç®—é¢„ç”Ÿæˆåç»­å¸§ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†ç”Ÿæˆæ•ˆç‡ã€‚åœ¨åŠ¨ä½œæ¡ä»¶è§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNFD åœ¨è§†è§‰è´¨é‡å’Œé‡‡æ ·æ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„è‡ªå›å½’åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶é¦–æ¬¡åœ¨ A100 GPU ä¸Šåˆ©ç”¨ 310M è§„æ¨¡çš„æ¨¡å‹å®ç°äº†è¶…è¿‡ 30 å¸§æ¯ç§’ (FPS) çš„è‡ªå›å½’è§†é¢‘ç”Ÿæˆï¼Œä¸ºå®æ—¶äº¤äº’å¼è§†é¢‘åº”ç”¨æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://nextframed.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.01380v2",
      "published_date": "2025-06-02 07:16:01 UTC",
      "updated_date": "2025-07-04 14:56:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:46:30.164018+00:00"
    },
    {
      "arxiv_id": "2506.01374v4",
      "title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving",
      "title_zh": "Reasoning Compilerï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹å¼•å¯¼çš„é«˜æ•ˆæ¨¡å‹æœåŠ¡ä¼˜åŒ–",
      "authors": [
        "Annabelle Sujun Tang",
        "Christopher Priebe",
        "Rohan Mahapatra",
        "Lianhui Qin",
        "Hadi Esmaeilzadeh"
      ],
      "abstract": "While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡æ¨¡å‹éƒ¨ç½²çš„é«˜æ˜‚æˆæœ¬ä»¥åŠä¼ ç»Ÿç¼–è¯‘å™¨åœ¨å¤„ç†ç¥ç»ç½‘ç»œå·¥ä½œè´Ÿè½½æ—¶é‡‡æ ·æ•ˆç‡ä½ã€ç¼ºä¹ç»“æ„ä¸Šä¸‹æ–‡ç­‰é—®é¢˜ï¼Œæå‡ºäº†Reasoning Compilerï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)å¼•å¯¼ä¼˜åŒ–çš„ç¼–è¯‘æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä¼˜åŒ–è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªåºåˆ—åŒ–çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å†³ç­–è¿‡ç¨‹ï¼Œå¹¶ç»“åˆäº†ç»“æ„åŒ–è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)æŠ€æœ¯ã€‚åœ¨è¿‡ç¨‹ä¸­ï¼ŒLLMä½œä¸ºæè®®æœºåˆ¶ï¼Œæ ¹æ®å½“å‰çš„ç¨‹åºçŠ¶æ€å’Œç´¯è®¡æ€§èƒ½åé¦ˆï¼Œæå‡ºç¬¦åˆç¡¬ä»¶ç‰¹æ€§çš„å˜æ¢å»ºè®®ã€‚MCTSåˆ™æ•´åˆLLMç”Ÿæˆçš„å»ºè®®ä»¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œå®ç°åœ¨åºå¤§çš„ç¼–è¯‘å™¨ä¼˜åŒ–ç©ºé—´ä¸­è¿›è¡Œç»“æ„åŒ–ä¸”ä¸Šä¸‹æ–‡æ•æ„Ÿçš„éå†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoning Compileråœ¨ä¸è¿›è¡Œä»»ä½•é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œç›¸æ¯”é¢†å…ˆçš„ç¥ç»ç¼–è¯‘å™¨èƒ½ä»¥æ›´å°‘çš„æ ·æœ¬é‡è·å¾—æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚è¿™ä¸€æˆæœå……åˆ†å±•ç¤ºäº†åˆ©ç”¨LLMå¼•å¯¼æ¨ç†æ¥å˜é©ç¼–è¯‘å™¨ä¼˜åŒ–é¢†åŸŸå¹¶æå‡æ¨¡å‹æœåŠ¡æ•ˆç‡çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01374v4",
      "published_date": "2025-06-02 07:02:46 UTC",
      "updated_date": "2025-12-12 17:38:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:46:23.797707+00:00"
    },
    {
      "arxiv_id": "2506.01372v2",
      "title": "AI Scientists Fail Without Strong Implementation Capability",
      "title_zh": "ç¼ºä¹å¼ºå¤§æ‰§è¡Œèƒ½åŠ›ï¼ŒAI ç§‘å­¦å®¶éš¾æœ‰å»ºæ ‘",
      "authors": [
        "Minjun Zhu",
        "Qiujie Xie",
        "Yixuan Weng",
        "Jian Wu",
        "Zhen Lin",
        "Linyi Yang",
        "Yue Zhang"
      ],
      "abstract": "The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \\textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \\textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.",
      "tldr_zh": "è¯¥ç«‹åœºè®ºæ–‡æ¢è®¨äº†AI Scientiståœ¨ç§‘å­¦å‘ç°èŒƒå¼è½¬å‹ä¸­çš„ç°çŠ¶ä¸æ ¸å¿ƒæŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰çš„åŸºäºLLMsçš„AI Scientistç³»ç»Ÿåœ¨ç‹¬ç«‹ç”Ÿæˆç ”ç©¶æŠ¥å‘Šæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œç”šè‡³è·å¾—äº†å­¦æœ¯ä¼šè®®çš„è®¤å¯ï¼Œä½†åœ¨è®¡ç®—æœºç§‘å­¦é¢†åŸŸä»ç¼ºä¹çªç ´æ€§æˆå°±ã€‚é€šè¿‡å¯¹28ç¯‡ç”±5ç§å…ˆè¿›AI Scientistç³»ç»Ÿç”Ÿæˆçš„è®ºæ–‡è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œå¹¶ç»“åˆå¤æ‚å·¥ç¨‹ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶å‘ç°AI Scientistçš„æ ¹æœ¬ç“¶é¢ˆåœ¨äºç¼ºä¹æ‰§è¡Œå¿…è¦éªŒè¯ç¨‹åºçš„èƒ½åŠ›ã€‚å½“å‰ç³»ç»Ÿåœ¨æ‰§è¡Œä¸¥è°¨å®éªŒä»¥äº§å‡ºé«˜è´¨é‡ç§‘ç ”æˆæœæ–¹é¢å­˜åœ¨æ˜æ˜¾çš„Implementation Gapï¼ˆæ‰§è¡Œé¸¿æ²Ÿï¼‰ã€‚æ–‡ç« æ·±å…¥åˆ†æäº†AI Scientistçš„åŸºç¡€æ€§é™åˆ¶ï¼Œå¹¶æŒ‡å‡ºå¦‚æœæ²¡æœ‰å¼ºå¤§çš„Implementation Capabilityï¼ˆæ‰§è¡Œèƒ½åŠ›ï¼‰ï¼ŒAI Scientistå°†éš¾ä»¥å®ç°çœŸæ­£çš„äººç±»æ°´å¹³å‘ç°ã€‚è¯¥ç ”ç©¶æ—¨åœ¨å‘¼åå­¦æœ¯ç•Œå…³æ³¨å¹¶è‡´åŠ›äºå¼¥è¡¥è¿™ä¸€æ‰§è¡Œå±‚é¢çš„çŸ­æ¿ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Position",
      "pdf_url": "https://arxiv.org/pdf/2506.01372v2",
      "published_date": "2025-06-02 06:59:10 UTC",
      "updated_date": "2025-06-09 09:01:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:46:37.981386+00:00"
    },
    {
      "arxiv_id": "2506.01369v2",
      "title": "Incentivizing LLMs to Self-Verify Their Answers",
      "title_zh": "æ¿€åŠ±å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç­”æ¡ˆè‡ªæˆ‘éªŒè¯",
      "authors": [
        "Fuxiang Zhang",
        "Jiacheng Xu",
        "Chaojie Wang",
        "Ce Cui",
        "Yang Liu",
        "Bo An"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable progress in complex reasoning tasks through both post-training and test-time scaling laws. While prevalent test-time scaling approaches are often realized by using external reward models to guide the model generation process, we find that only marginal gains can be acquired when scaling a model post-trained on specific reasoning tasks. We identify that the limited improvement stems from distribution discrepancies between the specific post-trained generator and the general reward model. To address this, we propose a framework that incentivizes LLMs to self-verify their own answers. By unifying answer generation and verification within a single reinforcement learning (RL) process, we train models that can effectively assess the correctness of their own solutions. The trained model can further scale its performance at inference time by verifying its generations, without the need for external verifiers. We train our self-verification models based on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying reasoning context lengths. Experiments on multiple mathematical reasoning benchmarks show that our models can not only improve post-training performance but also enable effective test-time scaling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­åˆ©ç”¨å¤–éƒ¨å¥–åŠ±æ¨¡å‹è¿›è¡Œæ¨ç†æ—¶ç¼©æ”¾ï¼ˆtest-time scalingï¼‰å¢ç›Šæœ‰é™çš„é—®é¢˜ï¼ŒæŒ‡å‡ºåˆ†å¸ƒå·®å¼‚æ˜¯å¯¼è‡´æå‡å—é™çš„æ ¸å¿ƒåŸå› ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ¿€åŠ±æ¨¡å‹è¿›è¡Œè‡ªæˆ‘éªŒè¯ï¼ˆself-verifyï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‡ç¨‹ä¸­æ•´åˆç­”æ¡ˆç”Ÿæˆä¸éªŒè¯ä»»åŠ¡ï¼Œä½¿æ¨¡å‹å…·å¤‡è¯„ä¼°è‡ªèº«æ–¹æ¡ˆæ­£ç¡®æ€§çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä½¿å¾—æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µæ— éœ€å¤–éƒ¨éªŒè¯å™¨ï¼ˆexternal verifiersï¼‰å³å¯é€šè¿‡éªŒè¯è‡ªèº«ç”Ÿæˆå†…å®¹æ¥æå‡æ€§èƒ½ã€‚ç ”ç©¶äººå‘˜åŸºäºQwen2.5-Math-7Bå’ŒDeepSeek-R1-Distill-Qwen-1.5Bè¿›è¡Œäº†è®­ç»ƒï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†è®­ç»ƒåï¼ˆpost-trainingï¼‰çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å±•ç°äº†å“è¶Šçš„æ¨ç†æ—¶ç¼©æ”¾èƒ½åŠ›ï¼Œä¸ºæå‡LLMsçš„è‡ªä¸»æ¨ç†ä¸éªŒè¯æ°´å¹³æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01369v2",
      "published_date": "2025-06-02 06:54:29 UTC",
      "updated_date": "2025-10-30 14:45:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:46:35.811916+00:00"
    },
    {
      "arxiv_id": "2506.01364v1",
      "title": "Unraveling Spatio-Temporal Foundation Models via the Pipeline Lens: A Comprehensive Review",
      "title_zh": "åŸºäºå…¨æµç¨‹è§†è§’çš„æ—¶ç©ºåŸºç¡€æ¨¡å‹æ·±åº¦è§£æï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "Yuchen Fang",
        "Hao Miao",
        "Yuxuan Liang",
        "Liwei Deng",
        "Yue Cui",
        "Ximu Zeng",
        "Yuyang Xia",
        "Yan Zhao",
        "Torben Bach Pedersen",
        "Christian S. Jensen",
        "Xiaofang Zhou",
        "Kai Zheng"
      ],
      "abstract": "Spatio-temporal deep learning models aims to utilize useful patterns in such data to support tasks like prediction. However, previous deep learning models designed for specific tasks typically require separate training for each use case, leading to increased computational and storage costs. To address this issue, spatio-temporal foundation models have emerged, offering a unified framework capable of solving multiple spatio-temporal tasks. These foundation models achieve remarkable success by learning general knowledge with spatio-temporal data or transferring the general capabilities of pre-trained language models. While previous surveys have explored spatio-temporal data and methodologies separately, they have ignored a comprehensive examination of how foundation models are designed, selected, pre-trained, and adapted. As a result, the overall pipeline for spatio-temporal foundation models remains unclear. To bridge this gap, we innovatively provide an up-to-date review of previous spatio-temporal foundation models from the pipeline perspective. The pipeline begins with an introduction to different types of spatio-temporal data, followed by details of data preprocessing and embedding techniques. The pipeline then presents a novel data property taxonomy to divide existing methods according to data sources and dependencies, providing efficient and effective model design and selection for researchers. On this basis, we further illustrate the training objectives of primitive models, as well as the adaptation techniques of transferred models. Overall, our survey provides a clear and structured pipeline to understand the connection between core elements of spatio-temporal foundation models while guiding researchers to get started quickly. Additionally, we introduce emerging opportunities such as multi-objective training in the field of spatio-temporal foundation models.",
      "tldr_zh": "è¯¥ç»¼è¿°ä»æµæ°´çº¿(Pipeline)è§†è§’å¯¹æ—¶ç©ºåŸºç¡€æ¨¡å‹(Spatio-Temporal Foundation Models)è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä»»åŠ¡ç‰¹å®šæ¨¡å‹åœ¨è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ä¸Šçš„å±€é™ã€‚ç ”ç©¶é’ˆå¯¹ç°æœ‰ç»¼è¿°ç¼ºä¹å¯¹æ¨¡å‹è®¾è®¡ã€é¢„è®­ç»ƒåŠé€‚é…è¿‡ç¨‹ç³»ç»Ÿè€ƒå¯Ÿçš„ç°çŠ¶ï¼Œåˆ›æ–°æ€§åœ°é˜æ˜äº†æ—¶ç©ºåŸºç¡€æ¨¡å‹çš„å®Œæ•´æµæ°´çº¿æ¶æ„ã€‚è¯¥æµæ°´çº¿æ¶µç›–äº†æ—¶ç©ºæ•°æ®ç±»å‹ã€æ•°æ®é¢„å¤„ç†(Data Preprocessing)ä¸åµŒå…¥æŠ€æœ¯(Embedding Techniques)ï¼Œå¹¶æå‡ºä¸€ç§æ–°é¢–çš„æ•°æ®å±æ€§åˆ†ç±»æ³•(Data Property Taxonomy)ä»¥æŒ‡å¯¼æ¨¡å‹çš„è®¾è®¡ä¸é€‰æ‹©ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¯¦ç»†åˆ†æäº†åŸå§‹æ¨¡å‹çš„è®­ç»ƒç›®æ ‡(Training Objectives)åŠè¿ç§»æ¨¡å‹çš„é€‚é…æŠ€æœ¯(Adaptation Techniques)ã€‚é€šè¿‡æ­ç¤ºæ ¸å¿ƒè¦ç´ é—´çš„å†…åœ¨è”ç³»ï¼Œè¯¥ç»¼è¿°ä¸ºç ”ç©¶è€…æä¾›äº†ç»“æ„åŒ–çš„å…¥é—¨æŒ‡å—ï¼Œå¹¶æ¢è®¨äº†å¤šç›®æ ‡è®­ç»ƒ(Multi-Objective Training)ç­‰è¯¥é¢†åŸŸçš„æœªæ¥æ–°å…´æœºé‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.01364v1",
      "published_date": "2025-06-02 06:46:42 UTC",
      "updated_date": "2025-06-02 06:46:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:47:20.497385+00:00"
    },
    {
      "arxiv_id": "2506.02071v1",
      "title": "AI Data Development: A Scorecard for the System Card Framework",
      "title_zh": "AIæ•°æ®å¼€å‘ï¼šç³»ç»Ÿå¡æ¡†æ¶ä¸‹çš„è¯„åˆ†å¡",
      "authors": [
        "Tadesse K. Bahiru",
        "Haileleol Tibebu",
        "Ioannis A. Kakadiaris"
      ],
      "abstract": "Artificial intelligence has transformed numerous industries, from healthcare to finance, enhancing decision-making through automated systems. However, the reliability of these systems is mainly dependent on the quality of the underlying datasets, raising ongoing concerns about transparency, accountability, and potential biases. This paper introduces a scorecard designed to evaluate the development of AI datasets, focusing on five key areas from the system card framework data development life cycle: data dictionary, collection process, composition, motivation, and pre-processing. The method follows a structured approach, using an intake form and scoring criteria to assess the quality and completeness of the data set. Applied to four diverse datasets, the methodology reveals strengths and improvement areas. The results are compiled using a scoring system that provides tailored recommendations to enhance the transparency and integrity of the data set. The scorecard addresses technical and ethical aspects, offering a holistic evaluation of data practices. This approach aims to improve the quality of the data set. It offers practical guidance to curators and researchers in developing responsible AI systems, ensuring fairness and accountability in decision support systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ—¨åœ¨è¯„ä¼° AI æ•°æ®é›†å¼€å‘è´¨é‡çš„è®°åˆ†å¡ (Scorecard)ï¼Œä»¥åº”å¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨é€æ˜åº¦ã€é—®è´£åˆ¶å’Œæ½œåœ¨åè§æ–¹é¢çš„æŒç»­æŒ‘æˆ˜ã€‚è¯¥è®°åˆ†å¡åŸºäº System Card Framework çš„æ•°æ®å¼€å‘ç”Ÿå‘½å‘¨æœŸï¼Œé‡ç‚¹å…³æ³¨æ•°æ®å­—å…¸ (data dictionary)ã€æ”¶é›†è¿‡ç¨‹ (collection process)ã€ç»„æˆæˆåˆ† (composition)ã€åŠ¨æœº (motivation) å’Œé¢„å¤„ç† (pre-processing) äº”ä¸ªå…³é”®é¢†åŸŸã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“æ„åŒ–çš„ä¿¡æ¯é‡‡é›†è¡¨ (intake form) å’Œè¯„åˆ†æ ‡å‡†ï¼Œå¯¹æ•°æ®é›†çš„è´¨é‡å’Œå®Œæ•´æ€§è¿›è¡Œç³»ç»ŸåŒ–è¯„ä¼°ã€‚ç ”ç©¶è€…å°†è¯¥æ–¹æ³•åº”ç”¨äºå››ä¸ªä¸åŒçš„æ•°æ®é›†ï¼ŒæˆåŠŸæ­ç¤ºäº†å®ƒä»¬å„è‡ªçš„ä¼˜åŠ¿ä¸å¾…æ”¹è¿›ä¹‹å¤„ã€‚é€šè¿‡è¯„åˆ†ç³»ç»Ÿç”Ÿæˆçš„å®šåˆ¶åŒ–å»ºè®®ï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºæ•°æ®é›†çš„é€æ˜åº¦å’Œå®Œæ•´æ€§ã€‚è¯¥æ–¹æ¡ˆæ•´åˆäº†æŠ€æœ¯ä¸ä¼¦ç†ç»´åº¦ï¼Œä¸ºç­–å±•äººå’Œç ”ç©¶äººå‘˜å¼€å‘è´Ÿè´£ä»»çš„ AI ç³»ç»Ÿä»¥åŠç¡®ä¿å†³ç­–æ”¯æŒç³»ç»Ÿçš„å…¬å¹³æ€§ä¸é—®è´£åˆ¶æä¾›äº†å®ç”¨çš„å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.02071v1",
      "published_date": "2025-06-02 06:35:45 UTC",
      "updated_date": "2025-06-02 06:35:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:47:24.921308+00:00"
    },
    {
      "arxiv_id": "2506.01357v1",
      "title": "KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors",
      "title_zh": "KokoroChatï¼šç”±å—è®­å’¨è¯¢å¸ˆé€šè¿‡è§’è‰²æ‰®æ¼”æ”¶é›†çš„æ—¥è¯­å¿ƒç†å’¨è¯¢å¯¹è¯æ•°æ®é›†",
      "authors": [
        "Zhiyang Qi",
        "Takumasa Kaneko",
        "Keiko Takamizo",
        "Mariko Ukiyo",
        "Michimasa Inaba"
      ],
      "abstract": "Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at https://github.com/UEC-InabaLab/KokoroChat.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹åœ¨å¿ƒç†å’¨è¯¢é¢†åŸŸé¢ä¸´çš„é«˜è´¨é‡æ•°æ®çŸ­ç¼ºã€éšç§é£é™©ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åˆæˆæ•°æ®ç¼ºä¹å¤šæ ·æ€§å’ŒçœŸå®æ€§ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”±å—è¿‡åŸ¹è®­çš„å’¨è¯¢å¸ˆ(trained counselors)è¿›è¡Œè§’è‰²æ‰®æ¼”(role-playing)çš„æ•°æ®é‡‡é›†æ–¹æ³•ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶è€…æ„å»ºäº†åä¸ºKokoroChatçš„æ—¥è¯­å¿ƒç†å’¨è¯¢å¯¹è¯æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«6,589ä¸ªé•¿ç¯‡å¯¹è¯ä»¥åŠå…¨é¢çš„æ¥è®¿è€…åé¦ˆã€‚å®éªŒè¯æ˜ï¼Œåˆ©ç”¨KokoroChatå¯¹å¼€æºLLMsè¿›è¡Œå¾®è°ƒï¼Œä¸ä»…æ˜¾è‘—æé«˜äº†ç”Ÿæˆå’¨è¯¢å›å¤çš„è´¨é‡ï¼Œè¿˜ä¼˜åŒ–äº†å¿ƒç†å’¨è¯¢å¯¹è¯çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶æˆåŠŸå¹³è¡¡äº†å¯¹è¯è´¨é‡ä¸éšç§å®‰å…¨ï¼Œä¸ºå¼€å‘æ›´å…·ä¸“ä¸šæ€§å’Œå…±æƒ…èƒ½åŠ›çš„å¿ƒç†å’¨è¯¢AIæä¾›äº†é‡è¦çš„æ•°æ®èµ„æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to ACL 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2506.01357v1",
      "published_date": "2025-06-02 06:20:53 UTC",
      "updated_date": "2025-06-02 06:20:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:47:44.702771+00:00"
    },
    {
      "arxiv_id": "2506.01353v2",
      "title": "EgoBrain: Synergizing Minds and Eyes For Human Action Understanding",
      "title_zh": "EgoBrainï¼šè„‘çœ¼ååŒçš„äººç±»åŠ¨ä½œç†è§£",
      "authors": [
        "Nie Lin",
        "Yansen Wang",
        "Dongqi Han",
        "Weibang Jiang",
        "Jingyuan Li",
        "Ryosuke Furuta",
        "Yoichi Sato",
        "Dongsheng Li"
      ],
      "abstract": "The integration of brain-computer interfaces (BCIs), in particular electroencephalography (EEG), with artificial intelligence (AI) has shown tremendous promise in decoding human cognition and behavior from neural signals. In particular, the rise of multimodal AI models have brought new possibilities that have never been imagined before. Here, we present EgoBrain --the world's first large-scale, temporally aligned multimodal dataset that synchronizes egocentric vision and EEG of human brain over extended periods of time, establishing a new paradigm for human-centered behavior analysis. This dataset comprises 61 hours of synchronized 32-channel EEG recordings and first-person video from 40 participants engaged in 29 categories of daily activities. We then developed a muiltimodal learning framework to fuse EEG and vision for action understanding, validated across both cross-subject and cross-environment challenges, achieving an action recognition accuracy of 66.70%. EgoBrain paves the way for a unified framework for brain-computer interface with multiple modalities. All data, tools, and acquisition protocols are openly shared to foster open science in cognitive computing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† EgoBrainï¼Œè¿™æ˜¯å…¨çƒé¦–ä¸ªå¤§è§„æ¨¡ã€æ—¶é—´å¯¹é½çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡åŒæ­¥ç¬¬ä¸€è§†è§’è§†è§‰ (egocentric vision) å’Œè„‘ç”µå›¾ (EEG) ä¿¡å·æ¥è§£ç äººç±»è®¤çŸ¥ä¸è¡Œä¸ºã€‚è¯¥æ•°æ®é›†æ¶µç›–äº† 40 åå‚ä¸è€…åœ¨è¿›è¡Œ 29 ç±»æ—¥å¸¸æ´»åŠ¨æ—¶çš„ 61 å°æ—¶åŒæ­¥ 32 é€šé“ EEG è®°å½•å’Œç¬¬ä¸€äººç§°è§†é¢‘ï¼Œä¸ºä»¥äººä¸ºä¸­å¿ƒçš„è¡Œä¸ºåˆ†æå»ºç«‹äº†æ–°èŒƒå¼ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡èåˆ EEG ä¸è§†è§‰ä¿¡æ¯è¿›è¡ŒåŠ¨ä½œç†è§£ (action understanding)ï¼Œå¹¶åœ¨è·¨å—è¯•è€…å’Œè·¨ç¯å¢ƒçš„æµ‹è¯•ä¸­è¾¾åˆ°äº† 66.70% çš„åŠ¨ä½œè¯†åˆ«å‡†ç¡®ç‡ã€‚EgoBrain ä¸ºæ„å»ºç»Ÿä¸€çš„å¤šæ¨¡æ€è„‘æœºæ¥å£ (BCI) æ¡†æ¶é“ºå¹³äº†é“è·¯ï¼ŒéªŒè¯äº†ç¥ç»ä¿¡å·ä¸è§†è§‰ååŒå·¥ä½œçš„æ½œåŠ›ã€‚è¯¥é¡¹ç›®çš„æ‰€æœ‰æ•°æ®ã€å·¥å…·å’Œé‡‡é›†åè®®å‡å·²å¼€æºï¼Œæ—¨åœ¨ä¿ƒè¿›è®¤çŸ¥è®¡ç®—é¢†åŸŸçš„å¼€æ”¾ç§‘å­¦å‘å±•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.01353v2",
      "published_date": "2025-06-02 06:14:02 UTC",
      "updated_date": "2025-10-14 06:55:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:47:52.254031+00:00"
    },
    {
      "arxiv_id": "2506.01337v1",
      "title": "NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models",
      "title_zh": "NoiseARï¼šæ‰©æ•£æ¨¡å‹çš„è‡ªå›å½’åˆå§‹å™ªå£°å…ˆéªŒ",
      "authors": [
        "Zeming Li",
        "Xiangyue Liu",
        "Xiangyu Zhang",
        "Ping Tan",
        "Heung-Yeung Shum"
      ],
      "abstract": "Diffusion models have emerged as powerful generative frameworks, creating data samples by progressively denoising an initial random state. Traditionally, this initial state is sampled from a simple, fixed distribution like isotropic Gaussian, inherently lacking structure and a direct mechanism for external control. While recent efforts have explored ways to introduce controllability into the diffusion process, particularly at the initialization stage, they often rely on deterministic or heuristic approaches. These methods can be suboptimal, lack expressiveness, and are difficult to scale or integrate into more sophisticated optimization frameworks. In this paper, we introduce NoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion Models. Instead of a static, unstructured source, NoiseAR learns to generate a dynamic and controllable prior distribution for the initial noise. We formulate the generation of the initial noise prior's parameters as an autoregressive probabilistic modeling task over spatial patches or tokens. This approach enables NoiseAR to capture complex spatial dependencies and introduce learned structure into the initial state. Crucially, NoiseAR is designed to be conditional, allowing text prompts to directly influence the learned prior, thereby achieving fine-grained control over the diffusion initialization. Our experiments demonstrate that NoiseAR can generate initial noise priors that lead to improved sample quality and enhanced consistency with conditional inputs, offering a powerful, learned alternative to traditional random initialization. A key advantage of NoiseAR is its probabilistic formulation, which naturally supports seamless integration into probabilistic frameworks like Markov Decision Processes and Reinforcement Learning. Our code will be available at https://github.com/HKUST-SAIL/NoiseAR/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion models)é€šå¸¸é‡‡ç”¨é™æ€ã€æ— ç»“æ„çš„å„å‘åŒæ€§é«˜æ–¯åˆ†å¸ƒ(isotropic Gaussian)ä½œä¸ºåˆå§‹çŠ¶æ€è€Œå¯¼è‡´ç¼ºä¹æ§åˆ¶åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº† NoiseAR æ–¹æ³•ã€‚NoiseAR æ˜¯ä¸€ç§æ–°å‹çš„è‡ªå›å½’åˆå§‹å™ªå£°å…ˆéªŒ(AutoRegressive Initial Noise Prior)ï¼Œé€šè¿‡åœ¨ç©ºé—´è¡¥ä¸(spatial patches)æˆ–ä»¤ç‰Œ(tokens)ä¸Šè¿›è¡Œè‡ªå›å½’æ¦‚ç‡å»ºæ¨¡ï¼Œä¸ºæ¨¡å‹ç”ŸæˆåŠ¨æ€ä¸”å¯å­¦ä¹ çš„åˆå§‹å™ªå£°åˆ†å¸ƒã€‚è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹æ•è·å¤æ‚çš„ç©ºé—´ä¾èµ–å…³ç³»(spatial dependencies)ï¼Œå¹¶æ”¯æŒé€šè¿‡æ–‡æœ¬æç¤º(text prompts)å¯¹åˆå§‹åŒ–é˜¶æ®µè¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒNoiseAR èƒ½å¤Ÿæ˜¾è‘—æå‡ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ï¼Œå¹¶å¢å¼ºå…¶ä¸æ¡ä»¶è¾“å…¥çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œå…¶æ¦‚ç‡åŒ–è¡¨è¿°(probabilistic formulation)ä½¿å…¶èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Processes)å’Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç­‰é«˜çº§ä¼˜åŒ–æ¡†æ¶ä¸­ï¼Œä¸ºä¼ ç»Ÿçš„éšæœºåˆå§‹åŒ–æä¾›äº†æ›´å¼ºå¤§çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01337v1",
      "published_date": "2025-06-02 05:32:35 UTC",
      "updated_date": "2025-06-02 05:32:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:47:49.633887+00:00"
    },
    {
      "arxiv_id": "2506.01333v1",
      "title": "ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control",
      "title_zh": "ETDIï¼šåˆ©ç”¨ OAuth å¢å¼ºçš„å·¥å…·å®šä¹‰å’ŒåŸºäºç­–ç•¥çš„è®¿é—®æ§åˆ¶ç¼“è§£æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (MCP) ä¸­çš„å·¥å…·æŠ¢æ³¨ä¸ Rug Pull æ”»å‡»",
      "authors": [
        "Manish Bhatt",
        "Vineeth Sai Narajala",
        "Idan Habler"
      ],
      "abstract": "The Model Context Protocol (MCP) plays a crucial role in extending the capabilities of Large Language Models (LLMs) by enabling integration with external tools and data sources. However, the standard MCP specification presents significant security vulnerabilities, notably Tool Poisoning and Rug Pull attacks. This paper introduces the Enhanced Tool Definition Interface (ETDI), a security extension designed to fortify MCP. ETDI incorporates cryptographic identity verification, immutable versioned tool definitions, and explicit permission management, often leveraging OAuth 2.0. We further propose extending MCP with fine-grained, policy-based access control, where tool capabilities are dynamically evaluated against explicit policies using a dedicated policy engine, considering runtime context beyond static OAuth scopes. This layered approach aims to establish a more secure, trustworthy, and controllable ecosystem for AI applications interacting with LLMs and external tools.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Model Context Protocol (MCP) åœ¨é›†æˆå¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸å¤–éƒ¨å·¥å…·æ—¶å­˜åœ¨çš„ Tool Poisoning å’Œ Rug Pull æ”»å‡»ç­‰ä¸¥é‡å®‰å…¨æ¼æ´ï¼Œæå‡ºäº†åä¸º Enhanced Tool Definition Interface (ETDI) çš„å®‰å…¨æ‰©å±•æ¶æ„ã€‚ETDI é€šè¿‡å¼•å…¥åŠ å¯†èº«ä»½éªŒè¯ (cryptographic identity verification)ã€ä¸å¯å˜çš„å·¥å…·å®šä¹‰ç‰ˆæœ¬ç®¡ç†ä»¥åŠåŸºäº OAuth 2.0 çš„æ˜¾å¼æƒé™ç®¡ç†ï¼Œæ˜¾è‘—å¼ºåŒ–äº† MCP çš„å®‰å…¨åŸºå‡†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿›ä¸€æ­¥å»ºè®®ä¸º MCP å¼•å…¥ç»†ç²’åº¦çš„ Policy-Based Access Controlï¼Œåˆ©ç”¨ä¸“é—¨çš„ç­–ç•¥å¼•æ“æ ¹æ®è¿è¡Œæ—¶ä¸Šä¸‹æ–‡åŠ¨æ€è¯„ä¼°å·¥å…·è°ƒç”¨æƒé™ã€‚è¿™ç§å¤šå±‚æ¬¡çš„é˜²å¾¡æ–¹æ¡ˆèƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„é™æ€ OAuth ä½œç”¨åŸŸé™åˆ¶ï¼Œåœ¨åŠ¨æ€ç¯å¢ƒä¸‹æä¾›æ›´ç²¾ç»†çš„è®¿é—®æ§åˆ¶ã€‚è¯¥æ¶æ„çš„å®æ–½æ—¨åœ¨ä¸º AI åº”ç”¨ç¨‹åºã€LLMs ä¸å¤–éƒ¨å·¥å…·çš„äº¤äº’æ„å»ºä¸€ä¸ªæ›´åŠ å®‰å…¨ã€å¯ä¿¡ä¸”å¯æ§çš„ç”Ÿæ€ç³»ç»Ÿã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç ”ç©¶æœ‰æ•ˆè§£å†³äº† MCP è§„èŒƒä¸­ç°æœ‰çš„èº«ä»½éªŒè¯ç¼ºå¤±å’Œæƒé™è¿‡åº¦æˆæƒé—®é¢˜ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CR",
      "comment": "11 Pages, 10 figures, Github links in introduction",
      "pdf_url": "https://arxiv.org/pdf/2506.01333v1",
      "published_date": "2025-06-02 05:22:38 UTC",
      "updated_date": "2025-06-02 05:22:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:47:57.376296+00:00"
    },
    {
      "arxiv_id": "2506.01332v1",
      "title": "An Empirical Study of Group Conformity in Multi-Agent Systems",
      "title_zh": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ç¾¤ä½“ä»ä¼—è¡Œä¸ºçš„å®è¯ç ”ç©¶",
      "authors": [
        "Min Choi",
        "Keonwoo Kim",
        "Sungwon Chae",
        "Sangyeob Baek"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡æ¨¡æ‹Ÿè¶…è¿‡2,500åœºå…³äºäº”ç±»ç¤¾ä¼šäº‰è®®æ€§è¯é¢˜çš„è¾©è®ºï¼Œæ¢è®¨äº†ç¾¤ä½“ä¸€è‡´æ€§(Group Conformity)çš„å½¢æˆä¸ä¼ æ’­ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†æœ€åˆæŒä¸­ç«‹ç«‹åœºçš„æ™ºèƒ½ä½“åœ¨å¤šè½®äº’åŠ¨ä¸­å¦‚ä½•æ¼”å˜å…¶ç«‹åœºï¼Œæ—¨åœ¨æ­ç¤ºç¤¾äº¤äº’åŠ¨ä¸­åè§äº§ç”Ÿçš„åŠ¨æ€è¿‡ç¨‹ã€‚ç»Ÿè®¡åˆ†æè¡¨æ˜ï¼ŒLLMæ™ºèƒ½ä½“è¡¨ç°å‡ºæ˜¾è‘—çš„ç¾¤ä½“ä¸€è‡´æ€§ï¼Œå…¶è¡Œä¸ºæ¨¡å¼ä¸äººç±»ç¤¾ä¼šå¿ƒç†é«˜åº¦ç›¸ä¼¼ã€‚å®éªŒå‘ç°ï¼Œæ™ºèƒ½ä½“å¾€å¾€ä¼šå‘æ•°é‡å ä¼˜çš„ç¾¤ä½“æˆ–è¡¨ç°å‡ºæ›´é«˜æ™ºèƒ½(Intelligence)çš„ä¸ªäººé æ‹¢ï¼Œä¸”é«˜æ™ºåŠ›æ™ºèƒ½ä½“åœ¨å¡‘é€ èˆ†è®ºæ–¹é¢å…·æœ‰æ›´å¼ºçš„å½±å“åŠ›ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†æ™ºèƒ½æ°´å¹³åœ¨è¯è¯­æƒæ„å»ºä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶æ­ç¤ºäº†LLMåœ¨åŒ¿ååœ¨çº¿äº¤äº’ä¸­æ”¾å¤§åè§çš„é£é™©ã€‚è¯¥ç ”ç©¶æœ€åå¼ºè°ƒäº†åˆ¶å®šç›¸å…³æ”¿ç­–ä»¥æå‡LLMç”Ÿæˆè®¨è®ºçš„å¤šæ ·æ€§ä¸é€æ˜åº¦çš„å¿…è¦æ€§ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£åè§çš„è”“å»¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01332v1",
      "published_date": "2025-06-02 05:22:29 UTC",
      "updated_date": "2025-06-02 05:22:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:47:54.623935+00:00"
    },
    {
      "arxiv_id": "2506.01329v2",
      "title": "Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨å±æœºæ£€æµ‹ä¸­çš„è¯„ä¼°ï¼šæºè‡ªå¿ƒç†æ”¯æŒçƒ­çº¿çš„çœŸå®ä¸–ç•ŒåŸºå‡†",
      "authors": [
        "Guifeng Deng",
        "Shuyin Rao",
        "Tianyu Lin",
        "Anlu Dai",
        "Pan Wang",
        "Junyi Xie",
        "Haidong Song",
        "Ke Zhao",
        "Dongwu Xu",
        "Zhengdong Cheng",
        "Tao Li",
        "Haiteng Jiang"
      ],
      "abstract": "Psychological support hotlines serve as critical lifelines for crisis intervention but encounter significant challenges due to rising demand and limited resources. Large language models (LLMs) offer potential support in crisis assessments, yet their effectiveness in emotionally sensitive, real-world clinical settings remains underexplored. We introduce PsyCrisisBench, a comprehensive benchmark of 540 annotated transcripts from the Hangzhou Psychological Assistance Hotline, assessing four key tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. 64 LLMs across 15 model families (including closed-source such as GPT, Claude, Gemini and open-source such as Llama, Qwen, DeepSeek) were evaluated using zero-shot, few-shot, and fine-tuning paradigms. LLMs showed strong results in suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779), and risk assessment (F1=0.907), with notable gains from few-shot prompting and fine-tuning. Compared to trained human operators, LLMs achieved comparable or superior performance on suicide plan identification and risk assessment, while humans retained advantages on mood status recognition and suicidal ideation detection. Mood status recognition remained challenging (max F1=0.709), likely due to missing vocal cues and semantic ambiguity. Notably, a fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) outperformed larger models on mood and suicidal ideation tasks. LLMs demonstrate performance broadly comparable to trained human operators in text-based crisis assessment, with complementary strengths across task types. PsyCrisisBench provides a robust, real-world evaluation framework to guide future model development and ethical deployment in clinical mental health.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¿ƒç†æ´åŠ©çƒ­çº¿èµ„æºæœ‰é™çš„ç°çŠ¶ï¼Œæå‡ºäº†PsyCrisisBenchï¼Œä¸€ä¸ªåŸºäºæ­å·å¿ƒç†æ´åŠ©çƒ­çº¿540ä»½çœŸå®è½¬å½•æ–‡æœ¬çš„ç»¼åˆè¯„ä¼°åŸºå‡†ã€‚è¯¥åŸºå‡†ä¸»è¦è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æƒ…ç»ªçŠ¶æ€è¯†åˆ«(mood status recognition)ã€è‡ªæ€æ„å¿µæ£€æµ‹(suicidal ideation detection)ã€è‡ªæ€è®¡åˆ’è¯†åˆ«(suicide plan identification)åŠé£é™©è¯„ä¼°(risk assessment)å››é¡¹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¯¹64ä¸ªé—­æºä¸å¼€æºæ¨¡å‹è¿›è¡Œå¤šèŒƒå¼è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºLLMsåœ¨é£é™©è¯„ä¼°ç­‰ä»»åŠ¡ä¸­è¡¨ç°å¼ºåŠ²ï¼Œä¸”åœ¨è‡ªæ€è®¡åˆ’è¯†åˆ«å’Œé£é™©è¯„ä¼°ä¸Šè¾¾åˆ°äº†ä¸å—è®­äººå·¥æ“ä½œå‘˜ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ°´å¹³ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œè™½ç„¶ç”±äºç¼ºå¤±è¯­éŸ³çº¿ç´¢å¯¼è‡´æƒ…ç»ªçŠ¶æ€è¯†åˆ«ä»å…·æŒ‘æˆ˜ï¼Œä½†ç»è¿‡å¾®è°ƒçš„å°å‚æ•°æ¨¡å‹ï¼ˆå¦‚Qwen2.5-1.5Bï¼‰åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå±•ç°å‡ºè¶…è¶Šå¤§æ¨¡å‹çš„æ½œåŠ›ã€‚PsyCrisisBenchä¸ºæœªæ¥LLMsåœ¨ä¸´åºŠå¿ƒç†å¥åº·é¢†åŸŸçš„å¼€å‘ä¸ä¼¦ç†éƒ¨ç½²æä¾›äº†é‡è¦çš„çœŸå®ä¸–ç•Œæ¡†æ¶æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint. Submitted to IEEE Journal of Biomedical and Health Informatics (under review)",
      "pdf_url": "https://arxiv.org/pdf/2506.01329v2",
      "published_date": "2025-06-02 05:18:24 UTC",
      "updated_date": "2025-12-17 19:59:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:48:15.534678+00:00"
    },
    {
      "arxiv_id": "2506.01327v2",
      "title": "Enhancing Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation",
      "title_zh": "é€šè¿‡æ—¶ç©ºç»Ÿè®¡é‡èšåˆå¢å¼ºè”é‚¦ç±»å¢é‡å­¦ä¹ ",
      "authors": [
        "Zenghao Guan",
        "Guojun Zhu",
        "Yucan Zhou",
        "Wu Liu",
        "Weiping Wang",
        "Jiebo Luo",
        "Xiaoyan Gu"
      ],
      "abstract": "Federated Class-Incremental Learning (FCIL) enables Class-Incremental Learning (CIL) from distributed data. Existing FCIL methods typically integrate old knowledge preservation into local client training. However, these methods cannot avoid spatial-temporal client drift caused by data heterogeneity and often incur significant computational and communication overhead, limiting practical deployment. To address these challenges simultaneously, we propose a novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides a unified framework to aggregate feature statistics both spatially (across clients) and temporally (across stages). The aggregated feature statistics are unaffected by data heterogeneity and can be used to update the classifier in closed form at each stage. Additionally, we introduce STSA-E, a communication-efficient variant with theoretical guarantees, achieving similar performance to STSA-E with much lower communication overhead. Extensive experiments on three widely used FCIL datasets, with varying degrees of data heterogeneity, show that our method outperforms state-of-the-art FCIL methods in terms of performance, flexibility, and both communication and computation efficiency. The code is available at https://github.com/Yuqin-G/STSA.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è”é‚¦ç±»å¢é‡å­¦ä¹  (Federated Class-Incremental Learning, FCIL) ä¸­ç”±æ•°æ®å¼‚æ„æ€§å¼•èµ·çš„ç©ºæ—¶å®¢æˆ·ç«¯æ¼‚ç§» (spatial-temporal client drift) ä»¥åŠé«˜æ˜‚çš„è®¡ç®—å’Œé€šä¿¡å¼€é”€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Spatial-Temporal Statistics Aggregation (STSA) çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶åœ¨ç©ºé—´ï¼ˆè·¨å®¢æˆ·ç«¯ï¼‰å’Œæ—¶é—´ï¼ˆè·¨é˜¶æ®µï¼‰ä¸Šèšåˆç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸”è¿™äº›ç»Ÿè®¡ä¿¡æ¯ä¸å—æ•°æ®å¼‚æ„æ€§å¹²æ‰°ï¼Œæ”¯æŒåœ¨æ¯ä¸€é˜¶æ®µä»¥é—­å¼è§£ (closed form) æ›´æ–°åˆ†ç±»å™¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å…·æœ‰ç†è®ºä¿è¯çš„é€šä¿¡é«˜æ•ˆå˜ä½“ STSA-Eï¼Œåœ¨å¤§å¹…é™ä½é€šä¿¡æˆæœ¬çš„åŒæ—¶ä¿æŒäº†ä¼˜å¼‚æ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªä¸»æµ FCIL æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTSA åœ¨æ€§èƒ½ã€çµæ´»æ€§ä»¥åŠé€šä¿¡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„ FCIL æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ä»…è§£å†³äº†æ•°æ®å¼‚æ„å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¹Ÿä¸º FCIL åœ¨å®é™…åœºæ™¯ä¸­çš„éƒ¨ç½²æä¾›äº†é«˜æ•ˆä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "WWW 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.01327v2",
      "published_date": "2025-06-02 05:14:57 UTC",
      "updated_date": "2026-01-14 16:57:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:48:26.008588+00:00"
    },
    {
      "arxiv_id": "2506.01326v2",
      "title": "ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research",
      "title_zh": "ORMindï¼šå—è®¤çŸ¥å¯å‘çš„è¿ç­¹å­¦ç«¯åˆ°ç«¯æ¨ç†æ¡†æ¶",
      "authors": [
        "Zhiyuan Wang",
        "Bokui Chen",
        "Yinya Huang",
        "Qingxing Cao",
        "Ming He",
        "Jianping Fan",
        "Xiaodan Liang"
      ],
      "abstract": "Operations research (OR) is widely deployed to solve critical decision-making problems with complex objectives and constraints, impacting manufacturing, logistics, finance, and healthcare outcomes. While Large Language Models (LLMs) have shown promising results in various domains, their practical application in industry-relevant operations research (OR) problems presents significant challenges and opportunities. Preliminary industrial applications of LLMs for operations research face two critical deployment challenges: 1) Self-correction focuses on code syntax rather than mathematical accuracy, causing costly errors; 2) Complex expert selection creates unpredictable workflows that reduce transparency and increase maintenance costs, making them impractical for time-sensitive business applications. To address these business limitations, we introduce ORMind, a cognitive-inspired framework that enhances optimization through counterfactual reasoning. Our approach emulates human cognition, implementing an end-to-end workflow that systematically transforms requirements into mathematical models and executable solver code. It is currently being tested internally in Lenovo's AI Assistant, with plans to enhance optimization capabilities for both business and consumer customers. Experiments demonstrate that ORMind outperforms existing methods, achieving a 9.5\\% improvement on the NL4Opt dataset and a 14.6\\% improvement on the ComplexOR dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ORMindï¼Œä¸€ä¸ªå—è®¤çŸ¥å¯å‘çš„ç«¯åˆ°ç«¯è¿ç­¹å­¦ (Operations Research) æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†å·¥ä¸šçº§å†³ç­–é—®é¢˜æ—¶é¢ä¸´çš„æ•°å­¦é€»è¾‘åå·®å’Œå¤æ‚ä¸“å®¶é€‰æ‹©å¯¼è‡´çš„å·¥ä½œæµä¸é€æ˜ç­‰ç“¶é¢ˆã€‚è¯¥æ¡†æ¶å€Ÿé‰´äººç±»è®¤çŸ¥æ¨¡å¼ï¼Œå¼•å…¥äº†åäº‹å®æ¨ç† (counterfactual reasoning) æœºåˆ¶ï¼Œæ„å»ºäº†ä¸€å¥—èƒ½å¤Ÿå°†ä¸šåŠ¡éœ€æ±‚ç³»ç»ŸåŒ–è½¬åŒ–ä¸ºæ•°å­¦æ¨¡å‹åŠå¯æ‰§è¡Œæ±‚è§£å™¨ä»£ç çš„å®Œæ•´å·¥ä½œæµã€‚ç›®å‰ ORMind å·²é›†æˆäºè”æƒ³çš„ AI Assistant è¿›è¡Œå†…éƒ¨æµ‹è¯•ï¼Œæœ‰æ•ˆå¢å¼ºäº†é¢å‘å•†ä¸šå’Œæ¶ˆè´¹è€…çš„ä¼˜åŒ–å¤„ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ NL4Opt å’Œ ComplexOR æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 9.5% å’Œ 14.6% çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨è§£å†³å¤æ‚å†³ç­–é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§å’Œå·¥ä¸šåº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by Annual Meetings of the Association for Computational Linguistics 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01326v2",
      "published_date": "2025-06-02 05:11:21 UTC",
      "updated_date": "2025-09-03 12:09:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:48:16.932277+00:00"
    },
    {
      "arxiv_id": "2506.01320v3",
      "title": "Psi-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models",
      "title_zh": "Psi-Samplerï¼šç”¨äºåˆ†æ•°æ¨¡å‹ä¸­åŸºäº SMC çš„æ¨ç†é˜¶æ®µå¥–åŠ±å¯¹é½çš„åˆå§‹ç²’å­é‡‡æ ·",
      "authors": [
        "Taehoon Yoon",
        "Yunhong Min",
        "Kyeongmin Yeo",
        "Minhyuk Sung"
      ],
      "abstract": "We introduce $Î¨$-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments. Project Webpage: https://psi-sampler.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† $\\Psi$-Samplerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé¡ºåºè’™ç‰¹å¡æ´› (Sequential Monte Carlo, SMC) çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¾—åˆ†ç”Ÿæˆæ¨¡å‹ (score-based generative models) åœ¨æ¨ç†é˜¶æ®µçš„å¥–åŠ±å¯¹é½ (inference-time reward alignment) æ•ˆç‡ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•é€šå¸¸ä»é«˜æ–¯å…ˆéªŒ (Gaussian prior) åˆå§‹åŒ–ç²’å­è€Œéš¾ä»¥æ•æ‰å¥–åŠ±ç›¸å…³åŒºåŸŸçš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºä»å¥–åŠ±æ„ŸçŸ¥åéªŒ (reward-aware posterior) è¿›è¡Œåˆå§‹åŒ–ä»¥å¢å¼ºå¯¹é½æ€§èƒ½ã€‚ä¸ºäº†åœ¨é«˜æ•ˆå¤„ç†é«˜ç»´æ½œåœ¨ç©ºé—´çš„åŒæ—¶ä¿æŒé‡‡æ ·é²æ£’æ€§ï¼Œä½œè€…å¼•å…¥äº†é¢„æ¡ä»¶çš„ Crank-Nicolson Langevin (pCNL) ç®—æ³•ï¼Œé€šè¿‡ç»“åˆæ¢¯åº¦ä¿¡æ¯åŠ¨åŠ›å­¦å®ç°é«˜æ•ˆçš„åˆå§‹ç²’å­é‡‡æ ·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ$\\Psi$-Sampler åœ¨å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆ (layout-to-image generation)ã€æ•°é‡æ„ŸçŸ¥ç”Ÿæˆä»¥åŠç¾å­¦åå¥½ç”Ÿæˆç­‰å¤šç§ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚è¯¥æ¡†æ¶ä¸ä»…æå‡äº†é‡‡æ ·æ•ˆç‡ï¼Œè¿˜ä¸ºç”Ÿæˆæ¨¡å‹çš„åè®­ç»ƒä¼˜åŒ– (post-training optimization) æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025, Spotlight Presentation",
      "pdf_url": "https://arxiv.org/pdf/2506.01320v3",
      "published_date": "2025-06-02 05:02:33 UTC",
      "updated_date": "2025-10-27 15:22:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:48:22.842683+00:00"
    },
    {
      "arxiv_id": "2506.01318v2",
      "title": "Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack",
      "title_zh": "æœºå™¨é—å¿˜çš„ç›²ç‚¹ï¼šè¿‡åº¦é—å¿˜ä¸åŸå‹é‡å­¦ä¹ æ”»å‡»",
      "authors": [
        "SeungBum Ha",
        "Saerom Park",
        "Sung Whan Yoon"
      ],
      "abstract": "Machine unlearning (MU) aims to expunge a designated forget set from a trained model without costly retraining, yet the existing techniques overlook two critical blind spots: \"over-unlearning\" that deteriorates retained data near the forget set, and post-hoc \"relearning\" attacks that aim to resurrect the forgotten knowledge. We first derive the over-unlearning metric OU@Îµ, which represents the collateral damage to the nearby region of the forget set, where the over-unlearning mainly appears. Next, we expose an unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack, which exploits the per-class prototype of the forget class with just a few samples, and easily restores the pre-unlearning performance. To counter both blind spots, we introduce Spotter, a plug-and-play objective that combines (i) a masked knowledge-distillation penalty on the nearby region of forget set to suppress OU@Îµ, and (ii) an intra-class dispersion loss that scatters forget-class embeddings, neutralizing prototypical relearning attacks. On CIFAR-10, as one of validations, Spotter reduces OU@Îµby below the 0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the retain set within 1% of difference with the original, and denies the prototype-attack by keeping the forget set accuracy within <1%, without accessing retained data. It confirms that Spotter is a practical remedy of the unlearning's blind spots.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨é—å¿˜(Machine Unlearning)ä¸­çš„ä¸¤ä¸ªå…³é”®ç›²ç‚¹ï¼šå¯¼è‡´é—å¿˜é›†é™„è¿‘ä¿ç•™æ•°æ®æ€§èƒ½ä¸‹é™çš„â€œè¿‡åº¦é—å¿˜â€(over-unlearning)ï¼Œä»¥åŠæ—¨åœ¨é€šè¿‡åæœŸæ‰‹æ®µæ¢å¤å·²é—å¿˜çŸ¥è¯†çš„â€œé‡æ–°å­¦ä¹ â€(relearning)æ”»å‡»ã€‚ç ”ç©¶è€…é¦–å…ˆæ¨å¯¼å‡ºäº†è¡¡é‡è¿‡åº¦é—å¿˜é™„å¸¦æŸå®³çš„æŒ‡æ ‡OU@Îµï¼Œå¹¶æ­ç¤ºäº†ä¸€ç§æ–°å‹çš„åŸå‹é‡æ–°å­¦ä¹ æ”»å‡»(Prototypical Relearning Attack)ï¼Œè¯¥æ”»å‡»èƒ½åˆ©ç”¨ç±»åˆ«åŸå‹è½»æ˜“æ¢å¤æ¨¡å‹æ€§èƒ½ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†åä¸ºSpotterçš„å³æ’å³ç”¨ç›®æ ‡å‡½æ•°ï¼Œé€šè¿‡ç»“åˆæ©ç çŸ¥è¯†è’¸é¦(masked knowledge-distillation)æƒ©ç½šå’Œç±»å†…åˆ†æ•£æŸå¤±(intra-class dispersion loss)æ¥åŒæ—¶æŠ‘åˆ¶è¿‡åº¦é—å¿˜å¹¶ä¸­å’Œé‡å­¦æ”»å‡»ã€‚åœ¨CIFAR-10æ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒSpotterèƒ½å°†OU@Îµæ˜¾è‘—é™ä½è‡³åŸºçº¿çš„0.05å€ä»¥ä¸‹ï¼Œä½¿é—å¿˜å‡†ç¡®ç‡é™è‡³0%ï¼Œå¹¶æœ‰æ•ˆæ‹¦æˆªé‡å­¦æ”»å‡»ã€‚è¯¥æ–¹æ³•åœ¨æ— éœ€è®¿é—®ä¿ç•™æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå°†ä¿ç•™é›†çš„å‡†ç¡®ç‡æŸå¤±æ§åˆ¶åœ¨1%ä»¥å†…ï¼Œè¯æ˜äº†Spotteræ˜¯ä¿®å¤æœºå™¨é—å¿˜ç›²ç‚¹çš„å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.01318v2",
      "published_date": "2025-06-02 05:01:30 UTC",
      "updated_date": "2025-06-03 07:41:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:48:33.447365+00:00"
    },
    {
      "arxiv_id": "2506.01317v2",
      "title": "T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning",
      "title_zh": "T-SHIRTï¼šé¢å‘æŒ‡ä»¤å¾®è°ƒçš„ Token é€‰æ‹©æ€§å±‚çº§åŒ–æ•°æ®ç­›é€‰",
      "authors": [
        "Yanjun Fu",
        "Faisal Hamman",
        "Sanghamitra Dutta"
      ],
      "abstract": "Instruction tuning is essential for Large Language Models (LLMs) to effectively follow user instructions. To improve training efficiency and reduce data redundancy, recent works use LLM-based scoring functions, e.g., Instruction-Following Difficulty (IFD), to select high-quality instruction-tuning data with scores above a threshold. While these data selection methods often lead to models that can match or even exceed the performance of models trained on the full datasets, we identify two key limitations: (i) they assess quality at the sample level, ignoring token-level informativeness; and (ii) they overlook the robustness of the scoring method, often selecting a sample due to superficial lexical features instead of its true quality. In this work, we propose Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT), a novel data selection framework that introduces a new scoring method to include only informative tokens in quality evaluation and also promotes robust and reliable samples whose neighbors also show high quality with less local inconsistencies. We demonstrate that models instruction-tuned on a curated dataset (only 5% of the original size) using T-SHIRT can outperform those trained on the entire large-scale dataset by up to 5.48 points on average across eight benchmarks. Across various LLMs and training set scales, our method consistently surpasses existing state-of-the-art data selection techniques, while also remaining both cost-effective and highly efficient. For instance, by using GPT-2 for score computation, we are able to process a dataset of 52k samples in 40 minutes on a single GPU. Our code is available at https://github.com/Dynamite321/T-SHIRT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†T-SHIRTï¼Œä¸€ç§ç”¨äºæŒ‡ä»¤å¾®è°ƒ(Instruction Tuning)çš„ä»¤ç‰Œé€‰æ‹©åˆ†å±‚æ•°æ®ç­›é€‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨æ ·æœ¬çº§åˆ«è¯„ä¼°è´¨é‡æ—¶å¿½ç•¥ä»¤ç‰Œçº§ä¿¡æ¯é‡ä»¥åŠè¯„åˆ†é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„åˆ†æ–¹æ³•ï¼Œåœ¨è´¨é‡è¯„ä¼°ä¸­ä»…åŒ…å«å…·æœ‰ä¿¡æ¯é‡çš„ä»¤ç‰Œ(Tokens)ï¼Œä»è€Œå®ç°æ›´ç²¾ç»†åŒ–çš„æ•°æ®ç­›é€‰ã€‚åŒæ—¶ï¼ŒT-SHIRTé€šè¿‡ç­›é€‰é‚£äº›é‚»åŸŸæ ·æœ¬åŒæ ·è¡¨ç°å‡ºé«˜è´¨é‡ä¸”å±€éƒ¨ä¸ä¸€è‡´æ€§è¾ƒå°çš„æ ·æœ¬ï¼Œå¢å¼ºäº†æ•°æ®é€‰æ‹©çš„ç¨³å¥æ€§ä¸å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨åŸå§‹æ•°æ®é›†5%çš„ç²¾é€‰æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæ‰€å¾—åˆ°çš„æ¨¡å‹åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡æ€§èƒ½æ¯”å…¨é‡è®­ç»ƒæ¨¡å‹é«˜å‡ºå¤šè¾¾5.48åˆ†ã€‚åœ¨å¤šç§å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œè®­ç»ƒè§„æ¨¡ä¸‹ï¼Œè¯¥æ–¹æ³•æŒç»­è¶…è¶Šç°æœ‰çš„æœ€å…ˆè¿›æ•°æ®ç­›é€‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼ŒT-SHIRTå…·æœ‰æé«˜çš„æˆæœ¬æ•ˆç›Šå’Œæ•ˆç‡ï¼Œä¾‹å¦‚ä½¿ç”¨GPT-2è¿›è¡Œè¯„åˆ†è®¡ç®—æ—¶ï¼Œåœ¨å•å¼ GPUä¸Šä»…éœ€40åˆ†é’Ÿå³å¯å®Œæˆ5.2ä¸‡ä¸ªæ ·æœ¬çš„å¤„ç†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01317v2",
      "published_date": "2025-06-02 04:59:17 UTC",
      "updated_date": "2025-12-01 01:30:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:49:05.454735+00:00"
    },
    {
      "arxiv_id": "2506.01307v1",
      "title": "Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models",
      "title_zh": "ä»…é å¯¹é½æ˜¯ä¸å¤Ÿçš„ï¼šé’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€é€šç”¨è¶Šç‹±æ”»å‡»",
      "authors": [
        "Youze Wang",
        "Wenbo Hu",
        "Yinpeng Dong",
        "Jing Liu",
        "Hanwang Zhang",
        "Richang Hong"
      ],
      "abstract": "Large Language Models (LLMs) have evolved into Multimodal Large Language Models (MLLMs), significantly enhancing their capabilities by integrating visual information and other types, thus aligning more closely with the nature of human intelligence, which processes a variety of data forms beyond just text. Despite advancements, the undesirable generation of these models remains a critical concern, particularly due to vulnerabilities exposed by text-based jailbreak attacks, which have represented a significant threat by challenging existing safety protocols. Motivated by the unique security risks posed by the integration of new and old modalities for MLLMs, we propose a unified multimodal universal jailbreak attack framework that leverages iterative image-text interactions and transfer-based strategy to generate a universal adversarial suffix and image. Our work not only highlights the interaction of image-text modalities can be used as a critical vulnerability but also validates that multimodal universal jailbreak attacks can bring higher-quality undesirable generations across different MLLMs. We evaluate the undesirable context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and InstructBLIP, and reveal significant multimodal safety alignment issues, highlighting the inadequacy of current safety mechanisms against sophisticated multimodal attacks. This study underscores the urgent need for robust safety measures in MLLMs, advocating for a comprehensive review and enhancement of security protocols to mitigate potential risks associated with multimodal capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨æ•´åˆè§†è§‰ä¿¡æ¯åå‡¸æ˜¾çš„å®‰å…¨æ¼æ´ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€é€šç”¨è¶Šç‹±æ”»å‡» (universal jailbreak attack) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£çš„å›¾æ–‡äº¤äº’å’ŒåŸºäºè¿ç§»çš„ç­–ç•¥ (transfer-based strategy)ï¼Œç”Ÿæˆé€šç”¨çš„å¯¹æŠ—æ€§åç¼€å’Œå›¾åƒï¼Œæ—¨åœ¨è¯±å¯¼æ¨¡å‹äº§ç”Ÿè¿è§„å†…å®¹ã€‚ç ”ç©¶å‘ç°å›¾æ–‡æ¨¡æ€é—´çš„äº¤äº’æ˜¯ MLLMs çš„ä¸€ä¸ªå…³é”®è„†å¼±ç‚¹ï¼Œä¸”è¿™ç§å¤šæ¨¡æ€æ”»å‡»åœ¨ LLaVAã€Yi-VLã€MiniGPT4ã€MiniGPT-v2 å’Œ InstructBLIP ç­‰å¤šä¸ªä¸»æµæ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºæé«˜çš„æˆåŠŸç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„ MLLMs åœ¨å¤šæ¨¡æ€å®‰å…¨å¯¹é½ (safety alignment) æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œç°æœ‰çš„é˜²å¾¡æœºåˆ¶éš¾ä»¥åº”å¯¹æ­¤ç±»å¤æ‚çš„è·¨æ¨¡æ€æ”»å‡»ã€‚æ­¤é¡¹å·¥ä½œæ­ç¤ºäº†ä»…é ç°æœ‰çš„å¯¹é½æŠ€æœ¯ä¸è¶³ä»¥ä¿éšœæ¨¡å‹å®‰å…¨ï¼Œå¼ºè°ƒäº†åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸‹é‡æ–°è¯„ä¼°å¹¶å¼ºåŒ–å®‰å…¨åè®®çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01307v1",
      "published_date": "2025-06-02 04:33:56 UTC",
      "updated_date": "2025-06-02 04:33:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:49:21.869717+00:00"
    },
    {
      "arxiv_id": "2506.01301v1",
      "title": "Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner",
      "title_zh": "å…‹æœå¤šæ¨¡æ€å¿ƒæ™ºç†è®ºæ¨ç†ä¸­çš„å¤šæ­¥å¤æ‚æ€§ï¼šä¸€ç§å¯æ‰©å±•çš„è´å¶æ–¯è§„åˆ’å™¨",
      "authors": [
        "Chunhui Zhang",
        "Zhongyu Ouyang",
        "Kwonjoon Lee",
        "Nakul Agarwal",
        "Sean Dae Houlihan",
        "Soroush Vosoughi",
        "Shao-Yuan Lo"
      ],
      "abstract": "Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs, desires, and intentions-forming the foundation of social cognition. However, existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning, which struggle with scalability in multimodal environments and fail to generalize as task complexity increases. To address these limitations, we propose a scalable Bayesian ToM planner that decomposes ToM reasoning into stepwise Bayesian updates. Our framework introduces weak-to-strong control, allowing smaller language models (LMs) to specialize in ToM-specific likelihood estimation and transfer their reasoning behaviors to larger LMs (7B to 405B) for integration with social and world knowledge. This synergistic approach aligns large-model inference of human mental states with Bayesian principles. Extensive experiments show that our method achieves a 4.6% accuracy improvement over state-of-the-art techniques on multimodal ToM benchmarks, including challenging unseen scenarios, thereby establishing a new standard for modeling human mental states in complex environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çš„å¿ƒç†ç†è®º(Theory-of-Mind, ToM)è®¡ç®—æ–¹æ³•åœ¨å¤šæ¨¡æ€ç¯å¢ƒå’Œå¤æ‚ä»»åŠ¡ä¸­é¢ä¸´çš„å¯æ‰©å±•æ€§ä¸æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„è´å¶æ–¯å¿ƒç†ç†è®ºè§„åˆ’å™¨(Bayesian ToM planner)ã€‚è¯¥æ¡†æ¶å°†å¤æ‚çš„ToMæ¨ç†åˆ†è§£ä¸ºé€æ­¥çš„è´å¶æ–¯æ›´æ–°è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥äº†â€œç”±å¼±åˆ°å¼ºâ€çš„æ§åˆ¶æœºåˆ¶(weak-to-strong control)ã€‚é€šè¿‡è¯¥æœºåˆ¶ï¼Œè¾ƒå°çš„è¯­è¨€æ¨¡å‹(LMs)ä¸“æ³¨äºä¸“é—¨çš„ToMä¼¼ç„¶ä¼°è®¡(likelihood estimation)ï¼Œå¹¶å°†å…¶æ¨ç†è¡Œä¸ºè¿ç§»è‡³æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼ˆæ¶µç›–7Båˆ°405Bï¼‰ï¼Œä»¥å®ç°ä¸ç¤¾äº¤å’Œä¸–ç•ŒçŸ¥è¯†çš„æ·±åº¦æ•´åˆã€‚è¿™ç§ååŒæ–¹æ³•å°†å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸è´å¶æ–¯åŸç†ç›¸ç»“åˆï¼Œç¡®ä¿äº†å¿ƒç†çŠ¶æ€æ¨æ–­çš„é€»è¾‘ä¸¥å¯†æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€ToMåŸºå‡†æµ‹è¯•ä¸­æ¯”ç°æœ‰æŠ€æœ¯å‡†ç¡®ç‡æå‡äº†4.6%ï¼Œå¹¶åœ¨æå…·æŒ‘æˆ˜æ€§çš„æœªçŸ¥åœºæ™¯ä¸­å±•ç°äº†å“è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹æ¨¡æ‹Ÿäººç±»å¿ƒç†çŠ¶æ€å¥ å®šäº†æ–°çš„æŠ€æœ¯æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as a Spotlight at the 2025 Forty-Second International Conference on Machine Learning (ICML 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.01301v1",
      "published_date": "2025-06-02 04:23:45 UTC",
      "updated_date": "2025-06-02 04:23:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:48:54.812075+00:00"
    },
    {
      "arxiv_id": "2506.01299v2",
      "title": "Scalable In-Context Q-Learning",
      "title_zh": "å¯æ‰©å±•çš„ä¸Šä¸‹æ–‡Qå­¦ä¹ ",
      "authors": [
        "Jinmei Liu",
        "Fuhong Liu",
        "Jianye Hao",
        "Bo Wang",
        "Huaxiong Li",
        "Chunlin Chen",
        "Zhi Wang"
      ],
      "abstract": "Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose \\textbf{S}calable \\textbf{I}n-\\textbf{C}ontext \\textbf{Q}-\\textbf{L}earning (\\textbf{SICQL}), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data. Our code is available at https://github.com/NJU-RL/SICQL",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ In-Context Reinforcement Learning (ICRL) åœ¨å¤„ç†æ¬¡ä¼˜è½¨è¿¹å’Œå®ç°ç²¾ç¡®ä¸Šä¸‹æ–‡æ¨ç†æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† Scalable In-Context Q-Learning (SICQL) æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº† dynamic programming å’Œ world modeling æŠ€æœ¯ï¼Œé‡‡ç”¨åŸºäº prompt çš„å¤šå¤´ Transformer æ¶æ„åŒæ—¶é¢„æµ‹æœ€ä¼˜ç­–ç•¥ä¸ In-Context Value Functionsã€‚é€šè¿‡é¢„è®­ç»ƒçš„ generalized world model æå–ä»»åŠ¡å…³é”®ä¿¡æ¯ï¼ŒSICQL èƒ½å¤Ÿåˆ©ç”¨ç´§å‡‘çš„ prompt å®ç°é«˜æ•ˆçš„ In-Context Inferenceã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†çŠ¶æ€ä»·å€¼å‡½æ•°æ‹Ÿåˆè‡³ Q-function çš„ä¸Šåˆ†ä½æ•°æ¥æ‰§è¡Œè¿­ä»£ç­–ç•¥æ”¹è¿›ï¼Œå¹¶åˆ©ç”¨ advantage-weighted regression å°†ä»·å€¼å‡½æ•°è’¸é¦è‡³ç­–ç•¥æå–è¿‡ç¨‹ã€‚åœ¨å¤šç§ç¦»æ•£å’Œè¿ç»­ç¯å¢ƒä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSICQL çš„æ€§èƒ½æŒç»­ä¼˜äºå„ç±»åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨ä» suboptimal data ä¸­å­¦ä¹ æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°åœ¨å†³ç­–é¢†åŸŸå…·æœ‰å¯æ‰©å±•æ€§å’Œç¨³å®šæ€§çš„æœ‰ç›‘ç£é¢„è®­ç»ƒæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01299v2",
      "published_date": "2025-06-02 04:21:56 UTC",
      "updated_date": "2025-09-26 07:48:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:49:05.946649+00:00"
    },
    {
      "arxiv_id": "2506.01297v3",
      "title": "MobCLIP: Learning General-purpose Geospatial Representation at Scale",
      "title_zh": "MobCLIPï¼šå¤§è§„æ¨¡é€šç”¨åœ°ç†ç©ºé—´è¡¨å¾å­¦ä¹ ",
      "authors": [
        "Ya Wen",
        "Jixuan Cai",
        "Qiyao Ma",
        "Linyan Li",
        "Xinhua Chen",
        "Chris Webster",
        "Yulun Zhou"
      ],
      "abstract": "Representation learning of geospatial locations remains a core challenge in achieving general geospatial intelligence. Current embedding methods often lack versatility, limiting their utility across diverse tasks in both human and natural domains. We present MobCLIP, the first nationwide general-purpose location encoder, integrating an unprecedented diversity of data modalities through effective and scalable multimodal fusion. Adopting a novel CLIP-based architecture, our framework aligns 100M+ POIs, nationwide remote sensing imagery, and structured demographic statistics with a billion-edge mobility graph. By tokenizing spatial locations into grid cells inspired by Vision Transformers, we establish a unified representation space bridging mobility patterns and multimodal features. To rigorously evaluate the general-purpose effectiveness of MobCLIP, we construct a benchmark dataset composed of 11 downstream prediction tasks across social, economic, and natural domains. Experiments show that MobCLIP, with four input modalities and a compact 128-dimensional representation space, achieves significantly superior general-purpose predictive performances than state-of-the-art models by an average of 35%. Thanks to the effective integration of human-centric modalities, the performance gain is particularly profound in human-centric tasks, such as energy consumption (+260%), offline retail consumption amount (+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we further demonstrate the scaling behavior in geospatial representation learning. We open-source code and pretrained models at: https://github.com/ylzhouchris/MobCLIP.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MobCLIPï¼Œè¿™æ˜¯é¦–ä¸ªå…¨å›½è§„æ¨¡çš„é€šç”¨åœ°ç†ç©ºé—´ä½ç½®ç¼–ç å™¨ï¼Œæ—¨åœ¨é€šè¿‡æœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆè§£å†³å½“å‰è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ç¼ºä¹é€šç”¨æ€§çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†åˆ›æ–°çš„ç±» CLIP æ¶æ„ï¼ŒæˆåŠŸå¯¹é½äº†è¶…è¿‡ 1 äº¿ä¸ªå…´è¶£ç‚¹ (POIs)ã€å…¨å›½é¥æ„Ÿå½±åƒã€ç»“æ„åŒ–äººå£ç»Ÿè®¡æ•°æ®ä»¥åŠåŒ…å«åäº¿çº§è¾¹çš„æµåŠ¨æ€§å›¾è°± (Mobility Graph)ã€‚å— Vision Transformers å¯å‘ï¼ŒMobCLIP å°†ç©ºé—´ä½ç½®ä»¤ç‰ŒåŒ–ä¸ºç½‘æ ¼å•å…ƒï¼Œå»ºç«‹äº†è¿æ¥ç§»åŠ¨æ¨¡å¼ä¸å¤šæ¨¡æ€ç‰¹å¾çš„ç»Ÿä¸€è¡¨ç¤ºç©ºé—´ã€‚ç ”ç©¶è€…é€šè¿‡åŒ…å« 11 é¡¹è·¨ç¤¾ä¼šã€ç»æµå’Œè‡ªç„¶é¢†åŸŸä¸‹æ¸¸ä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤º MobCLIP åœ¨é¢„æµ‹æ€§èƒ½ä¸Šå¹³å‡è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ 35%ã€‚ç”±äºæœ‰æ•ˆæ•´åˆäº†ä»¥äººä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€æ•°æ®ï¼Œè¯¥æ¨¡å‹åœ¨èƒ½æºæ¶ˆè€—ã€çº¿ä¸‹é›¶å”®é¢å’ŒçŠ¯ç½ªæ¡ˆä¾‹é¢„æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºå“è¶Šã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜éªŒè¯äº†åœ°ç†ç©ºé—´è¡¨ç¤ºå­¦ä¹ ä¸­çš„è§„æ¨¡æ³•åˆ™ (Scaling Laws)ï¼Œå¹¶å¼€æºäº†ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01297v3",
      "published_date": "2025-06-02 04:14:03 UTC",
      "updated_date": "2025-06-04 02:07:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:49:12.408031+00:00"
    },
    {
      "arxiv_id": "2506.01293v1",
      "title": "Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation",
      "title_zh": "å¤šæ¨¡æ€ç»“æ„åŒ–çŸ¥è¯†çš„æŠ½è±¡è§†è§‰ç†è§£ï¼šMLLM è¯„ä¼°çš„æ–°è§†è§’",
      "authors": [
        "Yichi Zhang",
        "Zhuo Chen",
        "Lingbing Guo",
        "Yajing Xu",
        "Min Zhang",
        "Wen Zhang",
        "Huajun Chen"
      ],
      "abstract": "Multi-modal large language models (MLLMs) incorporate heterogeneous modalities into LLMs, enabling a comprehensive understanding of diverse scenarios and objects. Despite the proliferation of evaluation benchmarks and leaderboards for MLLMs, they predominantly overlook the critical capacity of MLLMs to comprehend world knowledge with structured abstractions that appear in visual form. To address this gap, we propose a novel evaluation paradigm and devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for STRuctured understanding. This benchmark leverages multi-modal knowledge graphs to synthesize images encapsulating subgraph architectures enriched with multi-modal entities. M3STR necessitates that MLLMs not only recognize the multi-modal entities within the visual inputs but also decipher intricate relational topologies among them. We delineate the benchmark's statistical profiles and automated construction pipeline, accompanied by an extensive empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent deficiencies in processing abstractive visual information with structured knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic reasoning capacities. Our code and data are released at https://github.com/zjukg/M3STR",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„è¯„ä¼°åŸºå‡†æ™®éå¿½è§†äº†æ¨¡å‹ç†è§£ä»¥è§†è§‰å½¢å¼å‘ˆç°çš„ç»“æ„åŒ–æŠ½è±¡çŸ¥è¯†çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å…¨æ–°çš„è¯„ä¼°èŒƒå¼å¹¶å¼€å‘äº† M3STR åŸºå‡†ï¼Œå³åŸºäºå¤šæ¨¡æ€åœ°å›¾çš„ç»“æ„åŒ–ç†è§£æµ‹è¯• (Multi-Modal Map for STRuctured understanding)ã€‚è¯¥åŸºå‡†åˆ©ç”¨å¤šæ¨¡æ€çŸ¥è¯†å›¾è°± (Multi-modal Knowledge Graphs) åˆæˆåŒ…å«å­å›¾æ¶æ„å’Œå¤šæ¨¡æ€å®ä½“çš„å›¾åƒï¼Œè¦æ±‚æ¨¡å‹ä¸ä»…èƒ½è¯†åˆ«è§†è§‰å®ä½“ï¼Œè¿˜éœ€è§£ç å¤æ‚çš„æ‹“æ‰‘å…³ç³»ã€‚é€šè¿‡å¯¹ 26 ä¸ªå‰æ²¿ MLLMs çš„å®éªŒåˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–æŠ½è±¡è§†è§‰ä¿¡æ¯æ–¹é¢çš„æ˜¾è‘—ä¸è¶³ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ¨åŠ¨ MLLMs çš„ç»¼åˆæ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„è¯„ä»·è§†è§’å’Œæ”¹è¿›æ–¹å‘ï¼Œå…¶ä»£ç ä¸æ•°æ®å·²å…¬å¼€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2506.01293v1",
      "published_date": "2025-06-02 04:00:35 UTC",
      "updated_date": "2025-06-02 04:00:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:49:19.669435+00:00"
    },
    {
      "arxiv_id": "2506.01290v1",
      "title": "TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment",
      "title_zh": "TSRatingï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹è¯„åˆ¤å…ƒå­¦ä¹ çš„å¤šæ ·åŒ–æ—¶é—´åºåˆ—æ•°æ®è´¨é‡è¯„ä¼°",
      "authors": [
        "Shunyu Wu",
        "Dan Li",
        "Haozheng Ye",
        "Zhuomin Chen",
        "Jiahui Zhou",
        "Jian Lou",
        "Zibin Zheng",
        "See-Kiong Ng"
      ],
      "abstract": "High-quality time series (TS) data are essential for ensuring TS model performance, rendering research on rating TS data quality indispensable. Existing methods have shown promising rating accuracy within individual domains, primarily by extending data quality rating techniques such as influence functions and Shapley values to account for temporal characteristics. However, they neglect the fact that real-world TS data can span vastly different domains and exhibit distinct properties, hampering the accurate and efficient rating of diverse TS data. In this paper, we propose TSRating, a novel and unified framework for rating the quality of time series data crawled from diverse domains. TSRating is built on the assumption that LLMs inherit ample knowledge, acquired during their extensive pretraining, enabling them to comprehend and discern quality differences in diverse TS data. We verify this assumption by devising a series of prompts to elicit quality comparisons from LLMs for pairs of TS samples. We then fit a dedicated rating model, termed TSRater, to convert the LLMs' judgments into efficient quality predictions via TSRater's inference on future TS samples. To ensure cross-domain adaptability, we develop a meta-learning scheme to train TSRater on quality comparisons collected from nine distinct domains. To improve training efficiency, we employ signSGD for inner-loop updates, thus circumventing the demanding computation of hypergradients. Extensive experimental results on eleven benchmark datasets across three time series tasks, each using both conventional TS models and TS foundation models, demonstrate that TSRating outperforms baselines in terms of estimation accuracy, efficiency, and domain adaptability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TSRatingï¼Œä¸€ç§æ—¨åœ¨è¯„ä¼°å¤šæ ·åŒ–æ—¶é—´åºåˆ—(Time Series)æ•°æ®è´¨é‡çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è·¨é¢†åŸŸæ•°æ®ä¸­é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚TSRatingåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­è·å¾—çš„çŸ¥è¯†ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æç¤ºå¼•å¯¼LLMså¯¹TSæ ·æœ¬è¿›è¡Œè´¨é‡å¯¹æ¯”åˆ¤æ–­ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†åä¸ºTSRaterçš„è¯„åˆ†æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å…ƒå­¦ä¹ (Meta-learning)æ–¹æ¡ˆåœ¨ä¹ä¸ªä¸åŒé¢†åŸŸçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ç¡®ä¿è·¨é¢†åŸŸçš„é€‚åº”èƒ½åŠ›ã€‚åœ¨è®­ç»ƒæ•ˆç‡æ–¹é¢ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨signSGDè¿›è¡Œå†…ç¯æ›´æ–°ï¼Œæœ‰æ•ˆè§„é¿äº†å¤æ‚çš„è¶…æ¢¯åº¦è®¡ç®—ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTSRatingåœ¨11ä¸ªåŸºå‡†æ•°æ®é›†å’Œ3é¡¹TSä»»åŠ¡ä¸­ï¼Œå…¶è¯„ä¼°å‡†ç¡®æ€§ã€æ‰§è¡Œæ•ˆç‡åŠé¢†åŸŸé€‚åº”æ€§å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01290v1",
      "published_date": "2025-06-02 03:52:55 UTC",
      "updated_date": "2025-06-02 03:52:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:49:27.659272+00:00"
    },
    {
      "arxiv_id": "2506.01281v2",
      "title": "On the Hardness of Approximating Distributions with Tractable Probabilistic Models",
      "title_zh": "è®ºä½¿ç”¨æ˜“å¤„ç†æ¦‚ç‡æ¨¡å‹è¿›è¡Œåˆ†å¸ƒè¿‘ä¼¼çš„éš¾è§£æ€§",
      "authors": [
        "John Leland",
        "YooJung Choi"
      ],
      "abstract": "A fundamental challenge in probabilistic modeling is to balance expressivity and inference efficiency. Tractable probabilistic models (TPMs) aim to directly address this tradeoff by imposing constraints that guarantee efficient inference of certain queries while maintaining expressivity. In particular, probabilistic circuits (PCs) provide a unifying framework for many TPMs, by characterizing families of models as circuits satisfying different structural properties. Because the complexity of inference on PCs is a function of the circuit size, understanding the size requirements of different families of PCs is fundamental in mapping the trade-off between tractability and expressive efficiency. However, the study of expressive efficiency of circuits are often concerned with exact representations, which may not align with model learning, where we look to approximate the underlying data distribution closely by some distance measure. Moreover, due to hardness of inference tasks, exactly representing distributions while supporting tractable inference often incurs exponential size blow-ups. In this paper, we consider a natural, yet so far underexplored, question: can we avoid such size blow-up by allowing for some small approximation error? We study approximating distributions with probabilistic circuits with guarantees based on $f$-divergences, and analyze which inference queries remain well-approximated under this framework. We show that approximating an arbitrary distribution with bounded $f$-divergence is $\\mathsf{NP}$-hard for any model that can tractably compute marginals. In addition, we prove an exponential size gap for approximation between the class of decomposable PCs and that of decomposable and deterministic PCs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ¦‚ç‡å»ºæ¨¡ä¸­å¹³è¡¡è¡¨è¾¾èƒ½åŠ›ä¸æ¨ç†æ•ˆç‡çš„æŒ‘æˆ˜ï¼Œé‡ç‚¹å…³æ³¨äº†å¯å¤„ç†æ¦‚ç‡æ¨¡å‹(Tractable Probabilistic Models, TPMs)åŠå…¶ç»Ÿä¸€æ¡†æ¶æ¦‚ç‡ç”µè·¯(probabilistic circuits, PCs)çš„è§„æ¨¡éœ€æ±‚ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œå³æ˜¯å¦å¯ä»¥é€šè¿‡å…è®¸å¾®å°çš„è¿‘ä¼¼è¯¯å·®æ¥é¿å…ç²¾ç¡®è¡¨ç¤ºåˆ†å¸ƒæ—¶å¸¸è§çš„ç”µè·¯è§„æ¨¡æŒ‡æ•°çº§å¢é•¿ã€‚ç ”ç©¶é‡‡ç”¨äº†åŸºäº $f$-divergences çš„åˆ†ææ–¹æ³•ï¼Œæ¢è®¨äº†åœ¨è¿‘ä¼¼æ¡†æ¶ä¸‹å„ç±»æ¨ç†æŸ¥è¯¢çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºä»»ä½•èƒ½å¤Ÿé«˜æ•ˆè®¡ç®—è¾¹ç¼˜æ¦‚ç‡(marginals)çš„æ¨¡å‹ï¼Œåœ¨æœ‰ç•Œ $f$-divergences æ¡ä»¶ä¸‹è¿‘ä¼¼ä»»æ„åˆ†å¸ƒå‡å±äº $\\mathsf{NP}$-hard é—®é¢˜ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¯æ˜äº†åœ¨è¿‘ä¼¼åˆ†å¸ƒæ—¶ï¼Œå¯åˆ†è§£æ¦‚ç‡ç”µè·¯(decomposable PCs)ä¸å…·å¤‡å¯åˆ†è§£ä¸”ç¡®å®šæ€§å±æ€§çš„ç”µè·¯(decomposable and deterministic PCs)ä¹‹é—´å­˜åœ¨æŒ‡æ•°çº§çš„è§„æ¨¡å·®è·ã€‚è¿™äº›ç»“è®ºæ­ç¤ºäº†åœ¨ä¿è¯æ¨ç†æ•ˆç‡çš„å‰æä¸‹ï¼Œä½¿ç”¨æ¦‚ç‡ç”µè·¯è¿›è¡Œåˆ†å¸ƒè¿‘ä¼¼çš„å†…åœ¨è¡¨è¾¾æ•ˆç‡å±€é™æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to NeurIPS 2025. Proceedings forthcoming",
      "pdf_url": "https://arxiv.org/pdf/2506.01281v2",
      "published_date": "2025-06-02 03:35:07 UTC",
      "updated_date": "2025-10-24 20:14:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:49:43.165106+00:00"
    },
    {
      "arxiv_id": "2506.01277v1",
      "title": "GeoLocSFT: Efficient Visual Geolocation via Supervised Fine-Tuning of Multimodal Foundation Models",
      "title_zh": "GeoLocSFTï¼šåŸºäºå¤šæ¨¡æ€åŸºåº§æ¨¡å‹æœ‰ç›‘ç£å¾®è°ƒçš„é«˜æ•ˆè§†è§‰åœ°ç†å®šä½",
      "authors": [
        "Qiang Yi",
        "Lianlei Shan"
      ],
      "abstract": "Accurately determining the geographic location where a single image was taken, visual geolocation, remains a formidable challenge due to the planet's vastness and the deceptive similarity among distant locations. We introduce GeoLocSFT, a framework that demonstrates how targeted supervised fine-tuning (SFT) of a large multimodal foundation model (Gemma 3) using a small, high-quality dataset can yield highly competitive geolocation performance. GeoLocSFT is trained with only 2700 carefully selected image-GPS pairs from our geographically diverse MR600k dataset. Despite this limited data, our SFT-centric approach substantially improves over baseline models and achieves robust results on standard benchmarks such as Im2GPS-3k and YFCC-4k, as well as on our newly proposed and challenging MR40k benchmark, aimed specifically at sparsely populated regions. Further, we explore multi-candidate inference and aggregation strategies but find that the core gains are already realized at the SFT stage. Our findings highlight the power of high-quality supervision and efficient SFT for planet-scale image geolocation, especially when compared to prior methods that require massive databases or complex pipelines. To foster further research, we publicly release the MR40k benchmark dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GeoLocSFT æ¡†æ¶ï¼Œé€šè¿‡å¯¹å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ Gemma 3 è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT)ï¼Œè§£å†³äº†å…¨çƒå°ºåº¦ä¸‹è§†è§‰åœ°ç†å®šä½ (visual geolocation) çš„æŒ‘æˆ˜ã€‚ä¸ä¾èµ–æµ·é‡æ•°æ®çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ¨¡å‹ä»…åˆ©ç”¨ä» MR600k æ•°æ®é›†ä¸­ç²¾å¿ƒæŒ‘é€‰çš„ 2700 ä¸ªé«˜è´¨é‡å›¾åƒ-GPS å¯¹è¿›è¡Œè®­ç»ƒï¼Œå±•ç°äº†æé«˜çš„æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGeoLocSFT åœ¨ Im2GPS-3k å’Œ YFCC-4k ç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠé’ˆå¯¹äººçƒŸç¨€å°‘åœ°åŒºçš„æŒ‘æˆ˜æ€§æ–°åŸºå‡† MR40k ä¸Šï¼Œå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œæ ¸å¿ƒæ€§èƒ½çš„æå‡ä¸»è¦æºäº SFT é˜¶æ®µï¼Œè€Œéå¤šå€™é€‰æ¨ç†æˆ–èšåˆç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†é«˜è´¨é‡ç›‘ç£å’Œé«˜æ•ˆ SFT åœ¨æ˜Ÿçƒçº§å›¾åƒåœ°ç†å®šä½ä¸­çš„é‡è¦æ€§ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº† MR40k æ•°æ®é›†ä»¥æ¨åŠ¨åç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.01277v1",
      "published_date": "2025-06-02 03:16:19 UTC",
      "updated_date": "2025-06-02 03:16:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:49:26.010716+00:00"
    },
    {
      "arxiv_id": "2506.01275v2",
      "title": "Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D",
      "title_zh": "Contra4ï¼šéŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒåŠ 3D é¢†åŸŸå¯¹æ¯”å¼è·¨æ¨¡æ€æ¨ç†è¯„ä¼°",
      "authors": [
        "Artemis Panagopoulou",
        "Le Xue",
        "Honglu Zhou",
        "silvio savarese",
        "Ran Xu",
        "Caiming Xiong",
        "Chris Callison-Burch",
        "Mark Yatskar",
        "Juan Carlos Niebles"
      ],
      "abstract": "Real-world decision-making often begins with identifying which modality contains the most relevant information for a given query. While recent multimodal models have made impressive progress in processing diverse inputs, it remains unclear whether they can reason contrastively across multiple modalities to select the one that best satisfies a natural language prompt. We argue this capability is foundational, especially in retrieval-augmented and decision-time contexts, where systems must evaluate multiple signals and identify which one conveys the relevant information. To evaluate this skill, we introduce Contra4, a dataset for contrastive cross-modal reasoning across four modalities: image, audio, video, and 3D. Each example presents a natural language question alongside multiple candidate modality instances, and the model must select the one that semantically aligns with the prompt. Contra4 combines human-annotated captions with a mixture-of-models round-trip-consistency filter to ensure high-quality supervision, resulting in 174k training examples and a manually verified test set of 2.3k samples. While task-specific fine-tuning helps improve performance by 56% relative to baseline, state-of-the-art models still achieve only an absolute of 56% accuracy overall and 42% in four-modality settings, underscoring a significant limitation in current multimodal models.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Contra4 è¯„ä¼°æ•°æ®é›†ï¼Œæ—¨åœ¨è¡¡é‡å¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒ (Image)ã€éŸ³é¢‘ (Audio)ã€è§†é¢‘ (Video) å’Œ 3D æ¨¡æ€ä¹‹é—´çš„å¯¹æ¯”è·¨æ¨¡æ€æ¨ç† (Contrastive Cross-Modal Reasoning) èƒ½åŠ›ã€‚ç°å®å†³ç­–é€šå¸¸éœ€è¦ä»ä¸åŒæ¨¡æ€ä¿¡å·ä¸­è¯†åˆ«æœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œè€Œ Contra4 é€šè¿‡æä¾›è‡ªç„¶è¯­è¨€é—®é¢˜åŠå¤šä¸ªå€™é€‰æ¨¡æ€å®ä¾‹ï¼Œè¦æ±‚æ¨¡å‹ä»ä¸­é€‰æ‹©è¯­ä¹‰ä¸Šä¸æç¤ºæœ€å¥‘åˆçš„æ¨¡æ€ã€‚è¯¥æ•°æ®é›†ç»“åˆäº†äººå·¥æ ‡æ³¨çš„æè¿°å’Œæ··åˆæ¨¡å‹å¾€è¿”ä¸€è‡´æ€§ (Round-trip-consistency) è¿‡æ»¤æŠ€æœ¯ï¼ŒåŒ…å« 17.4 ä¸‡ä¸ªè®­ç»ƒæ ·æœ¬å’Œ 2300 ä¸ªç»è¿‡äººå·¥éªŒè¯çš„æµ‹è¯•æ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä»»åŠ¡ç‰¹å®šå¾®è°ƒèƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œä½†æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ•´ä½“å‡†ç¡®ç‡ä¸Šä»…è¾¾åˆ° 56%ï¼Œåœ¨å››æ¨¡æ€è®¾ç½®ä¸‹å‡†ç¡®ç‡ä»…ä¸º 42%ã€‚è¿™è¡¨æ˜å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯„ä¼°å’Œå¯¹æ¯”è·¨æ¨¡æ€ä¿¡å·æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œéš¾ä»¥æ»¡è¶³å¤æ‚å†³ç­–åœºæ™¯çš„éœ€æ±‚ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01275v2",
      "published_date": "2025-06-02 03:12:13 UTC",
      "updated_date": "2025-09-15 18:15:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:49:46.267518+00:00"
    },
    {
      "arxiv_id": "2506.01274v1",
      "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding",
      "title_zh": "ReFoCUSï¼šé¢å‘è¯­å¢ƒç†è§£çš„å¼ºåŒ–å¼•å¯¼å¸§ä¼˜åŒ–",
      "authors": [
        "Hosu Lee",
        "Junho Kim",
        "Hyunjun Kim",
        "Yong Man Ro"
      ],
      "abstract": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), a novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals derived from a reference LMM to reflect the model's intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹(LMMs)åœ¨è§†é¢‘ç†è§£ä¸­å› å¸§é€‰æ‹©ç­–ç•¥ä¸ä½³å¯¼è‡´æ¨ç†å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ReFoCUSï¼ˆReinforcement-guided Frame Optimization for Contextual UnderStandingï¼‰æ¡†æ¶ã€‚ReFoCUS æ˜¯ä¸€ç§æ–°é¢–çš„å¸§çº§ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯å°†ä¼˜åŒ–ç›®æ ‡ä»ç”Ÿæˆçš„æ–‡æœ¬å“åº”è½¬å‘å¯¹è§†è§‰è¾“å…¥çš„é€‰æ‹©ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å­¦ä¹ å¸§é€‰æ‹©ç­–ç•¥ï¼Œåˆ©ç”¨æ¥è‡ªå‚è€ƒ LMM çš„å¥–åŠ±ä¿¡å·æ¥è¯†åˆ«æœ€èƒ½æ”¯æŒæ—¶é—´æ¥åœ°(Temporally grounded)å“åº”çš„å…³é”®å¸§ã€‚ä¸ºäº†é«˜æ•ˆå¤„ç†å¤§è§„æ¨¡çš„å¸§ç»„åˆç©ºé—´ï¼Œç ”ç©¶å¼•å…¥äº†è‡ªå›å½’ã€æ¡ä»¶é€‰æ‹©æ¶æ„(Autoregressive, conditional selection architecture)ï¼Œåœ¨ç¡®ä¿æ—¶é—´è¿è´¯æ€§çš„åŒæ—¶é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦æ˜¾å¼çš„å¸§çº§æ ‡æ³¨ç›‘ç£ï¼Œå¹¶åœ¨å¤šä¸ªè§†é¢‘é—®ç­”(Video QA)åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†è¡¨ç°ï¼Œè¯æ˜äº†å°†å¸§é€‰æ‹©ä¸æ¨¡å‹å†…éƒ¨æ•ˆç”¨ç›¸å¯¹é½çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01274v1",
      "published_date": "2025-06-02 03:08:07 UTC",
      "updated_date": "2025-06-02 03:08:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:50:14.298240+00:00"
    },
    {
      "arxiv_id": "2506.01273v1",
      "title": "RAISE: Reasoning Agent for Interactive SQL Exploration",
      "title_zh": "RAISEï¼šé¢å‘äº¤äº’å¼ SQL æ¢ç´¢çš„æ¨ç†æ™ºèƒ½ä½“",
      "authors": [
        "Fernando Granado",
        "Roberto Lotufo",
        "Jayr Pereira"
      ],
      "abstract": "Recent advances in large language models (LLMs) have propelled research in natural language interfaces to databases. However, most state-of-the-art text-to-SQL systems still depend on complex, multi-stage pipelines. This work proposes a novel agentic framework that unifies schema linking, query generation, and iterative refinement within a single, end-to-end component. By leveraging the intrinsic reasoning abilities of LLMs, our method emulates how humans answer questions when working with unfamiliar databases: understanding the data by formulating hypotheses, running dynamic queries to validate them, reasoning over the results, and revising outputs based on observed results. Crucially, our approach introduces a new strategy for scaling test-time computation in text-to-SQL: we scale the depth of interactive database exploration and reflection. This shift enables the model to allocate computation dynamically to better understand the data, especially useful in ambiguous and underspecified scenarios. Our experiments show that it improved the Execution Accuracy (EX) from 44.8% to 56.5% on the challenging BIRD dataset using DeepSeek-R1-Distill-Llama-70B. Furthermore, when equipped with steps to add more diversity to the answers, our agent achieves a Best-of-N accuracy of 81.8% with 8 rounds of candidate generation, rivaling the 82.79% achieved by the top-ranked published solution, while reducing engineering complexity. These findings position our unified framework as a promising alternative for building natural language interfaces to databases.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RAISEï¼Œä¸€ç§ç”¨äºäº¤äº’å¼SQLæ¢ç´¢çš„æ¨ç†æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰text-to-SQLç³»ç»Ÿè¿‡åº¦ä¾èµ–å¤æ‚å¤šé˜¶æ®µæµæ°´çº¿çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†schema linkingã€query generationå’Œiterative refinementæ•´åˆä¸ºç»Ÿä¸€çš„ç«¯åˆ°ç«¯ç»„ä»¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»æå‡ºå‡è®¾ã€è¿è¡ŒåŠ¨æ€æŸ¥è¯¢éªŒè¯å¹¶æ ¹æ®ç»“æœè¿›è¡Œæ¨ç†ä¿®è®¢çš„è¿‡ç¨‹æ¥å¤„ç†æ•°æ®ã€‚RAISEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº†æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—(test-time computation)çš„æ–°ç­–ç•¥ï¼Œé€šè¿‡åŠ æ·±äº¤äº’å¼æ•°æ®åº“æ¢ç´¢ä¸åæ€çš„æ·±åº¦ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€åˆ†é…è®¡ç®—èµ„æºä»¥åº”å¯¹æ¨¡ç³Šæˆ–æè¿°ä¸è¶³çš„å¤æ‚åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨BIRDæ•°æ®é›†ä¸Šï¼Œè¯¥æ¡†æ¶é…åˆDeepSeek-R1-Distill-Llama-70Bå°†æ‰§è¡Œå‡†ç¡®ç‡(Execution Accuracy)ä»44.8%å¤§å¹…æå‡è‡³56.5%ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥å¤šæ ·æ€§ç”Ÿæˆæ­¥éª¤ï¼ŒRAISEåœ¨8è½®è¿­ä»£ä¸­è¾¾åˆ°äº†81.8%çš„Best-of-Nå‡†ç¡®ç‡ï¼Œåœ¨æ˜¾è‘—é™ä½å·¥ç¨‹å¤æ‚åº¦çš„å‰æä¸‹ï¼Œå®ç°äº†ä¸å½“å‰é¡¶å°–æŠ€æœ¯æ°´å¹³ç›¸å½“çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01273v1",
      "published_date": "2025-06-02 03:07:08 UTC",
      "updated_date": "2025-06-02 03:07:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:50:11.110300+00:00"
    },
    {
      "arxiv_id": "2506.01268v1",
      "title": "CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction",
      "title_zh": "CleanS2Sï¼šæ”¯æŒä¸»åŠ¨å¼è¯­éŸ³åˆ°è¯­éŸ³äº¤äº’çš„å•æ–‡ä»¶æ¡†æ¶",
      "authors": [
        "Yudong Lu",
        "Yazhe Niu",
        "Shuai Hu",
        "Haolin Wang"
      ],
      "abstract": "CleanS2S is a framework for human-like speech-to-speech interaction that advances conversational AI through single-file implementation and proactive dialogue capabilities. Our system integrates automatic speech recognition, large language models, and text-to-speech synthesis into a unified pipeline with real-time interruption handling, achieving low transition latency through full-duplex websocket connections and non-blocking I/O. Beyond conventional chatbot paradigms, we pioneer a proactive interaction mechanism, which combines memory systems with Subjective Action Judgement module, enabling five human-like response strategies: interruption, refusal, deflection, silence, and standard response. The memory module dynamically aggregates historical, and contextual data to inform interaction decisions. This approach breaks the rigid turn-based convention by allowing system-initiated dialog control and context-aware response selection. And we propose Action Judgement SFT that assesses input streams for responses strategies. The framework's single-file implementation with atomic configurations offers researchers unprecedented transparency and extensibility for interaction agents. The code of CleanS2S is released at \\https://github.com/opendilab/CleanS2S.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CleanS2Sï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å®ç°ç±»äººè¯­éŸ³è½¬è¯­éŸ³ (Speech-to-Speech) äº¤äº’çš„å•æ–‡ä»¶æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ä¸»åŠ¨å¯¹è¯èƒ½åŠ›æå‡äº†å¯¹è¯å¼ AI çš„äº¤äº’è‡ªç„¶åº¦ã€‚ç³»ç»Ÿå°†è‡ªåŠ¨è¯­éŸ³è¯†åˆ« (ASR)ã€å¤§è¯­è¨€æ¨¡å‹ (LLM) å’Œæ–‡æœ¬è½¬è¯­éŸ³ (TTS) é›†æˆåˆ°ç»Ÿä¸€æµæ°´çº¿ä¸­ï¼Œåˆ©ç”¨å…¨åŒå·¥ websocket è¿æ¥å’Œéé˜»å¡ I/O (non-blocking I/O) å®ç°äº†æä½å»¶è¿Ÿçš„å®æ—¶ä¸­æ–­å¤„ç†ã€‚CleanS2S çš„æ ¸å¿ƒåœ¨äºå…¶ä¸»åŠ¨äº¤äº’æœºåˆ¶ï¼Œç»“åˆäº†è®°å¿†ç³»ç»Ÿ (Memory Systems) ä¸ä¸»è§‚åŠ¨ä½œåˆ¤æ–­ (Subjective Action Judgement) æ¨¡å—ï¼Œæ”¯æŒä¸­æ–­ã€æ‹’ç»ã€åç§»ã€æ²‰é»˜å’Œæ ‡å‡†å›ç­”äº”ç§ç­–ç•¥ã€‚é€šè¿‡åŠ¨ä½œåˆ¤æ–­ SFT (Action Judgement SFT) å¯¹è¾“å…¥æµè¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ¡†æ¶æˆåŠŸæ‰“ç ´äº†ä¼ ç»ŸåƒµåŒ–çš„å›åˆåˆ¶å¯¹è¯é™åˆ¶ï¼Œå®ç°äº†ç³»ç»Ÿå‘èµ·çš„å¯¹è¯æ§åˆ¶ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥å“åº”é€‰æ‹©ã€‚è¿™ç§å•æ–‡ä»¶ä¸”å…·æœ‰åŸå­åŒ–é…ç½®çš„è®¾è®¡ä¸ºäº¤äº’æ™ºèƒ½ä½“çš„å¼€å‘æä¾›äº†æ˜¾è‘—çš„é€æ˜åº¦ä¸æ‰©å±•æ€§ï¼Œç›®å‰è¯¥é¡¹ç›®çš„ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01268v1",
      "published_date": "2025-06-02 02:40:46 UTC",
      "updated_date": "2025-06-02 02:40:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:50:22.957973+00:00"
    },
    {
      "arxiv_id": "2506.01266v1",
      "title": "Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model",
      "title_zh": "åŸºäºæ ¡å‡†æ¨¡å‹è¾“å‡ºå±‚èåˆçš„å¤§è¯­è¨€æ¨¡å‹è§£æ¯’",
      "authors": [
        "Yuanhe Tian",
        "Mingjie Deng",
        "Guoqing Jin",
        "Yan Song"
      ],
      "abstract": "Existing approaches for Large language model (LLM) detoxification generally rely on training on large-scale non-toxic or human-annotated preference data, designing prompts to instruct the LLM to generate safe content, or modifying the model parameters to remove toxic information, which are computationally expensive, lack robustness, and often compromise LLMs' fluency and contextual understanding. In this paper, we propose a simple yet effective approach for LLM detoxification, which leverages a compact, pre-trained calibration model that guides the detoxification process of a target LLM via a lightweight intervention in its generation pipeline. By learning a detoxified embedding space from non-toxic data, the calibration model effectively steers the LLM away from generating harmful content. This approach only requires a one-time training of the calibration model that is able to be seamlessly applied to multiple LLMs without compromising fluency or contextual understanding. Experiment results on the benchmark dataset demonstrate that our approach reduces toxicity while maintaining reasonable content expression.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) è„±æ¯’å¤„ç†ä¸­å­˜åœ¨çš„è®¡ç®—æˆæœ¬é«˜ã€é²æ£’æ€§ä¸è¶³ä»¥åŠå®¹æ˜“æŸå®³æ¨¡å‹æµç•…åº¦ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¾“å‡ºå±‚èåˆæ ¡å‡†æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸€ä¸ªè½»é‡çº§çš„é¢„è®­ç»ƒæ ¡å‡†æ¨¡å‹ (Calibration Model)ï¼Œé€šè¿‡åœ¨ç”Ÿæˆç®¡çº¿ä¸­è¿›è¡Œè½»é‡çº§å¹²é¢„æ¥å¼•å¯¼ç›®æ ‡ LLM é¿å¼€æœ‰å®³å†…å®¹çš„ç”Ÿæˆã€‚æ ¡å‡†æ¨¡å‹é€šè¿‡ä»æ— æ¯’æ•°æ®ä¸­å­¦ä¹ è„±æ¯’åµŒå…¥ç©ºé—´ (Detoxified Embedding Space)ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¿®æ­£æ¨¡å‹çš„è¾“å‡ºæ¦‚ç‡ï¼Œä½¿æ¨¡å‹åç¦»æ¯’æ€§å†…å®¹ã€‚è¿™ç§æ–¹æ¡ˆä»…éœ€å¯¹æ ¡å‡†æ¨¡å‹è¿›è¡Œä¸€æ¬¡æ€§è®­ç»ƒï¼Œå³å¯æ— ç¼åº”ç”¨äºå¤šç§ä¸åŒçš„ LLMsï¼Œä¸”ä¸ä¼šç‰ºç‰²æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½æ–‡æœ¬æ¯’æ€§çš„åŒæ—¶ï¼Œä¾ç„¶èƒ½å¤Ÿç»´æŒåˆç†ä¸”é«˜è´¨é‡çš„å†…å®¹è¡¨è¾¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2506.01266v1",
      "published_date": "2025-06-02 02:36:32 UTC",
      "updated_date": "2025-06-02 02:36:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:50:41.432431+00:00"
    },
    {
      "arxiv_id": "2506.08029v1",
      "title": "Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning",
      "title_zh": "åŸºäºå•æ­¥å¼ºåŒ–å­¦ä¹ çš„åˆ†å¸ƒå¼ç”µè·¯é€†å‘è®¾è®¡",
      "authors": [
        "Jiayu Li",
        "Masood Mortazavi",
        "Ning Yan",
        "Yihong Ma",
        "Reza Zafarani"
      ],
      "abstract": "The goal of inverse design in distributed circuits is to generate near-optimal designs that meet a desirable transfer function specification. Existing design exploration methods use some combination of strategies involving artificial grids, differentiable evaluation procedures, and specific template topologies. However, real-world design practices often require non-differentiable evaluation procedures, varying topologies, and near-continuous placement spaces. In this paper, we propose DCIDA, a design exploration framework that learns a near-optimal design sampling policy for a target transfer function. DCIDA decides all design factors in a compound single-step action by sampling from a set of jointly-trained conditional distributions generated by the policy. Utilizing an injective interdependent ``map\", DCIDA transforms raw sampled design ``actions\" into uniquely equivalent physical representations, enabling the framework to learn the conditional dependencies among joint ``raw'' design decisions. Our experiments demonstrate DCIDA's Transformer-based policy network achieves significant reductions in design error compared to state-of-the-art approaches, with significantly better fit in cases involving more complex transfer functions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DCIDAï¼Œä¸€ç§åŸºäºå•æ­¥å¼ºåŒ–å­¦ä¹ (Single-Step Reinforcement Learning)çš„è®¾è®¡æ¢ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åˆ†å¸ƒå¼ç”µè·¯åå‘è®¾è®¡(Inverse Design)ä¸­ä¼ è¾“å‡½æ•°(Transfer Function)è§„èŒƒä¸å¤æ‚æ‹“æ‰‘åŒ¹é…çš„éš¾é¢˜ã€‚DCIDAé€šè¿‡ä»è”åˆè®­ç»ƒçš„æ¡ä»¶åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œåœ¨å¤åˆå•æ­¥åŠ¨ä½œä¸­ç¡®å®šæ‰€æœ‰è®¾è®¡å› ç´ ï¼Œæœ‰æ•ˆçªç ´äº†ä¼ ç»Ÿæ–¹æ³•å¯¹å¾®åˆ†è¯„ä¼°ç¨‹åºå’Œå›ºå®šæ¨¡æ¿æ‹“æ‰‘çš„ä¾èµ–ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å•å°„ç›¸äº’ä¾èµ–åœ°å›¾(Injective Interdependent Map)ï¼Œå°†åŸå§‹é‡‡æ ·çš„è®¾è®¡åŠ¨ä½œè½¬åŒ–ä¸ºå”¯ä¸€çš„ç­‰æ•ˆç‰©ç†è¡¨ç¤ºï¼Œä»è€Œèƒ½å¤Ÿå­¦ä¹ è”åˆå†³ç­–é—´çš„æ¡ä»¶ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDCIDAé‡‡ç”¨çš„åŸºäºTransformerçš„ç­–ç•¥ç½‘ç»œåœ¨é™ä½è®¾è®¡è¯¯å·®æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤æ‚ä¼ è¾“å‡½æ•°çš„æ¡ˆä¾‹ä¸­ï¼Œè¯¥æ¡†æ¶å±•ç°å‡ºæ›´å¼ºçš„æ‹Ÿåˆèƒ½åŠ›ï¼Œä¸ºéè¿ç»­æ”¾ç½®ç©ºé—´ä¸‹çš„è‡ªåŠ¨åŒ–ç”µè·¯è®¾è®¡æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SY",
      "comment": "A briefer version of this paper was accepted as a Work-in-Progress (WIP) at the Design Automation Conference (DAC) 2024",
      "pdf_url": "https://arxiv.org/pdf/2506.08029v1",
      "published_date": "2025-06-02 02:31:52 UTC",
      "updated_date": "2025-06-02 02:31:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:50:43.591951+00:00"
    },
    {
      "arxiv_id": "2506.01257v1",
      "title": "DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models",
      "title_zh": "DeepSeek åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ï¼šå¼€æºå¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€é£é™©ä¸ä¸´åºŠåº”ç”¨ç»¼è¿°",
      "authors": [
        "Jiancheng Ye",
        "Sophie Bronstein",
        "Jiarui Hai",
        "Malak Abu Hashish"
      ],
      "abstract": "DeepSeek-R1 is a cutting-edge open-source large language model (LLM) developed by DeepSeek, showcasing advanced reasoning capabilities through a hybrid architecture that integrates mixture of experts (MoE), chain of thought (CoT) reasoning, and reinforcement learning. Released under the permissive MIT license, DeepSeek-R1 offers a transparent and cost-effective alternative to proprietary models like GPT-4o and Claude-3 Opus; it excels in structured problem-solving domains such as mathematics, healthcare diagnostics, code generation, and pharmaceutical research. The model demonstrates competitive performance on benchmarks like the United States Medical Licensing Examination (USMLE) and American Invitational Mathematics Examination (AIME), with strong results in pediatric and ophthalmologic clinical decision support tasks. Its architecture enables efficient inference while preserving reasoning depth, making it suitable for deployment in resource-constrained settings. However, DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation, adversarial manipulation, and safety failures - especially in multilingual and ethically sensitive contexts. This survey highlights the model's strengths, including interpretability, scalability, and adaptability, alongside its limitations in general language fluency and safety alignment. Future research priorities include improving bias mitigation, natural language comprehension, domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1 represents a major advance in open, scalable AI, underscoring the need for collaborative governance to ensure responsible and equitable deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶å…¨é¢ç»¼è¿°äº†å¼€æºå¤§è¯­è¨€æ¨¡å‹(LLM) DeepSeek-R1åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨èƒ½åŠ›ã€æ½œåœ¨é£é™©åŠä¸´åºŠä»·å€¼ã€‚DeepSeek-R1é‡‡ç”¨äº†æ··åˆä¸“å®¶(MoE)ã€é“¾å¼æ€ç»´(CoT)æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„æ··åˆæ¶æ„ï¼Œåœ¨USMLEå’ŒAIMEç­‰åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å“è¶Šçš„ç»“æ„åŒ–é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å„¿ç§‘å’Œçœ¼ç§‘çš„ä¸´åºŠå†³ç­–æ”¯æŒä»¥åŠè¯ç‰©ç ”å‘ç­‰é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä¸”å…¶é«˜æ•ˆçš„æ¨ç†æœºåˆ¶ä½¿å…¶éå¸¸é€‚ç”¨äºèµ„æºå—é™çš„éƒ¨ç½²åœºæ™¯ã€‚å°½ç®¡åœ¨å¯è§£é‡Šæ€§ã€æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†DeepSeek-R1åœ¨åè§ã€è¯¯å¯¼æ€§ä¿¡æ¯åŠå®‰å…¨æ€§å¯¹é½(Safety Alignment)ç­‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥ç»¼è¿°å¼ºè°ƒäº†DeepSeek-R1ä½œä¸ºå¼€æºAIé‡å¤§è¿›å±•çš„åœ°ä½ï¼ŒåŒæ—¶æŒ‡å‡ºæœªæ¥ç ”ç©¶åº”ä¾§é‡äºåè§ç¼“è§£ã€é¢†åŸŸç‰¹å®šéªŒè¯åŠç›‘ç®¡åˆè§„ï¼Œä»¥ç¡®ä¿å…¶åœ¨åŒ»ç–—é¢†åŸŸçš„è´Ÿè´£ä»»éƒ¨ç½²ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01257v1",
      "published_date": "2025-06-02 02:17:04 UTC",
      "updated_date": "2025-06-02 02:17:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:50:32.630050+00:00"
    },
    {
      "arxiv_id": "2506.01252v1",
      "title": "MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine",
      "title_zh": "MTCMBï¼šé¢å‘ä¸­åŒ»è¯é¢†åŸŸçŸ¥è¯†ã€æ¨ç†ä¸å®‰å…¨æ€§è¯„ä¼°çš„å¤§è¯­è¨€æ¨¡å‹å¤šä»»åŠ¡åŸºå‡†æ¡†æ¶",
      "authors": [
        "Shufeng Kong",
        "Xingru Yang",
        "Yuanyuan Wei",
        "Zijie Wang",
        "Hao Tang",
        "Jiuqi Qin",
        "Shuting Lan",
        "Yingheng Wang",
        "Junwen Bai",
        "Zhuangbin Chen",
        "Zibin Zheng",
        "Caihua Liu",
        "Hao Liang"
      ],
      "abstract": "Traditional Chinese Medicine (TCM) is a holistic medical system with millennia of accumulated clinical experience, playing a vital role in global healthcare-particularly across East Asia. However, the implicit reasoning, diverse textual forms, and lack of standardization in TCM pose major challenges for computational modeling and evaluation. Large Language Models (LLMs) have demonstrated remarkable potential in processing natural language across diverse domains, including general medicine. Yet, their systematic evaluation in the TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on factual question answering or lack domain-specific tasks and clinical realism. To fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major categories: knowledge QA, language understanding, diagnostic reasoning, prescription generation, and safety evaluation. The benchmark integrates real-world case records, national licensing exams, and classical texts, providing an authentic and comprehensive testbed for TCM-capable models. Preliminary results indicate that current LLMs perform well on foundational knowledge but fall short in clinical reasoning, prescription planning, and safety compliance. These findings highlight the urgent need for domain-aligned benchmarks like MTCMB to guide the development of more competent and trustworthy medical AI systems. All datasets, code, and evaluation tools are publicly available at: https://github.com/Wayyuanyuan/MTCMB.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸­åŒ»è¯ (Traditional Chinese Medicine, TCM) é¢†åŸŸè®¡ç®—å»ºæ¨¡ä¸è¯„ä¼°ä¸­å­˜åœ¨çš„æ¨ç†éšæ™¦ã€æ–‡æœ¬å¤šæ ·åŠç¼ºä¹æ ‡å‡†åŒ–ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† MTCMB å¤šä»»åŠ¡åŸºå‡†è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”± 12 ä¸ªå­æ•°æ®é›†ç»„æˆï¼Œæ¶µç›–çŸ¥è¯†é—®ç­”ã€è¯­è¨€ç†è§£ã€è¯Šæ–­æ¨ç†ã€å¤„æ–¹ç”Ÿæˆå’Œå®‰å…¨æ€§è¯„ä¼°äº”å¤§æ ¸å¿ƒç±»åˆ«ï¼Œæ—¨åœ¨å…¨é¢è¡¡é‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ä¸­åŒ»è¯é¢†åŸŸçš„ç»¼åˆæ€§èƒ½ã€‚MTCMB é€šè¿‡ä¸ä¸“ä¸šä¸­åŒ»è¯ä¸“å®¶åˆä½œå¼€å‘ï¼Œæ•´åˆäº†çœŸå®ç—…ä¾‹ã€å›½å®¶æ‰§ä¸šåŒ»å¸ˆè€ƒè¯•å’Œç»å…¸å¤ç±ç­‰å¤šç§æƒå¨æ•°æ®æºï¼Œç¡®ä¿äº†è¯„ä¼°ç¯å¢ƒçš„çœŸå®æ€§ä¸ä¸“ä¸šæ€§ã€‚åˆæ­¥å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å½“å‰çš„ LLMs åœ¨åŸºç¡€çŸ¥è¯†æŒæ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚çš„ä¸´åºŠæ¨ç†ã€å¤„æ–¹è§„åˆ’ä»¥åŠå®‰å…¨åˆè§„æ€§æ–¹é¢ä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†çš„å‘å¸ƒå¡«è¡¥äº†ä¸­åŒ»è¯é¢†åŸŸç¼ºä¹å…¨é¢ã€é¢†åŸŸç‰¹å®šè¯„ä¼°ä½“ç³»çš„ç©ºç™½ï¼Œä¸ºå¼€å‘æ›´å…·ä¸“ä¸šæ€§å’Œå¯ä¿¡åº¦çš„ä¸­åŒ»è¯äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦çš„æŒ‡å¯¼æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01252v1",
      "published_date": "2025-06-02 02:01:40 UTC",
      "updated_date": "2025-06-02 02:01:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:50:31.850625+00:00"
    },
    {
      "arxiv_id": "2506.01247v1",
      "title": "Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors",
      "title_zh": "Visual Sparse Steeringï¼šåˆ©ç”¨ç¨€ç–å¼•å¯¼è½¬å‘å‘é‡æå‡é›¶æ ·æœ¬å›¾åƒåˆ†ç±»",
      "authors": [
        "Gerasimos Chatzoudis",
        "Zhuowei Li",
        "Gemma E. Moran",
        "Hao Wang",
        "Dimitris N. Metaxas"
      ],
      "abstract": "Steering vision foundation models at inference time without retraining or access to large labeled datasets is a desirable yet challenging objective, particularly in dynamic or resource-constrained settings. In this paper, we introduce Visual Sparse Steering (VS2), a lightweight, test-time method that guides vision models using steering vectors derived from sparse features learned by top-$k$ Sparse Autoencoders without requiring contrastive data. Specifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on CUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a retrieval-augmented variant that selectively amplifies relevant sparse features using pseudo-labeled neighbors at inference time. With oracle positive/negative sets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44% on CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2 and VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing that sparse steering benefits specific classes by disambiguating visually or taxonomically proximate categories rather than providing a uniform boost. Finally, to better align the sparse features learned through the SAE reconstruction task with those relevant for downstream performance, we propose Prototype-Aligned Sparse Steering (PASS). By incorporating a prototype-alignment loss during SAE training, using labels only during training while remaining fully test-time unsupervised, PASS consistently, though modestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100 with ViT-B/32.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Visual Sparse Steering (VS2)ï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ¨ç†é˜¶æ®µæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„è½»é‡çº§è§†è§‰æ¨¡å‹å¼•å¯¼æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä» top-k Sparse Autoencoders (SAEs) å­¦ä¹ åˆ°çš„ç¨€ç–ç‰¹å¾ä¸­æå–å¼•å¯¼å‘é‡ï¼Œåœ¨ä¸ä¾èµ–å¯¹æ¯”æ•°æ®çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº† Zero-shot å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVS2 åœ¨ CIFAR-100ã€CUB-200 ç­‰æ•°æ®é›†ä¸Šå‡ä¼˜äº CLIP åŸºçº¿ï¼Œè€Œå…¶æ£€ç´¢å¢å¼ºå˜ä½“ VS2++ åœ¨åˆ©ç”¨ä¼ªæ ‡ç­¾è¿‘é‚»å¢å¼ºç‰¹å¾åï¼Œå‡†ç¡®ç‡æå‡æ›´ä¸ºæ˜¾è‘—ã€‚ç ”ç©¶å‘ç°ï¼Œç¨€ç–å¼•å¯¼çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿæœ‰æ•ˆæ¶ˆé™¤è§†è§‰æˆ–åˆ†ç±»ä¸Šç›¸è¿‘ç±»åˆ«çš„æ­§ä¹‰ï¼Œè€Œéæä¾›ç®€å•çš„ç»Ÿä¸€æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥åŸå‹å¯¹é½æŸå¤± (prototype-alignment loss) çš„ Prototype-Aligned Sparse Steering (PASS) è¿›ä¸€æ­¥ä¼˜åŒ–äº† SAE ç‰¹å¾ä¸ä¸‹æ¸¸ä»»åŠ¡çš„å¯¹é½ï¼Œåœ¨ä¿æŒæµ‹è¯•æ—¶æ— ç›‘ç£çš„å‰æä¸‹å®ç°äº†æ›´ä¼˜çš„åˆ†ç±»è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01247v1",
      "published_date": "2025-06-02 01:51:20 UTC",
      "updated_date": "2025-06-02 01:51:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:50:46.446049+00:00"
    },
    {
      "arxiv_id": "2506.03198v3",
      "title": "FLEX: A Largescale Multimodal, Multiview Dataset for Learning Structured Representations for Fitness Action Quality Assessment",
      "title_zh": "FLEXï¼šé¢å‘å¥èº«åŠ¨ä½œè´¨é‡è¯„ä¼°ç»“æ„åŒ–è¡¨ç¤ºå­¦ä¹ çš„å¤§è§„æ¨¡å¤šæ¨¡æ€å¤šè§†å›¾æ•°æ®é›†",
      "authors": [
        "Hao Yin",
        "Lijun Gu",
        "Paritosh Parmar",
        "Lin Xu",
        "Tianxiao Guo",
        "Weiwei Fu",
        "Yang Zhang",
        "Tianyou Zheng"
      ],
      "abstract": "Action Quality Assessment (AQA) -- the task of quantifying how well an action is performed -- has great potential for detecting errors in gym weight training, where accurate feedback is critical to prevent injuries and maximize gains. Existing AQA datasets, however, are limited to single-view competitive sports and RGB video, lacking multimodal signals and professional assessment of fitness actions. We introduce FLEX, the first large-scale, multimodal, multiview dataset for fitness AQA that incorporates surface electromyography (sEMG). FLEX contains over 7,500 multiview recordings of 20 weight-loaded exercises performed by 38 subjects of diverse skill levels, with synchronized RGB video, 3D pose, sEMG, and physiological signals. Expert annotations are organized into a Fitness Knowledge Graph (FKG) linking actions, key steps, error types, and feedback, supporting a compositional scoring function for interpretable quality assessment. FLEX enables multimodal fusion, cross-modal prediction -- including the novel Video$\\rightarrow$EMG task -- and biomechanically oriented representation learning. Building on the FKG, we further introduce FLEX-VideoQA, a structured question-answering benchmark with hierarchical queries that drive cross-modal reasoning in vision-language models. Baseline experiments demonstrate that multimodal inputs, multiview video, and fine-grained annotations significantly enhance AQA performance. FLEX thus advances AQA toward richer multimodal settings and provides a foundation for AI-powered fitness assessment and coaching. Dataset and code are available at \\href{https://github.com/HaoYin116/FLEX}{https://github.com/HaoYin116/FLEX}. Link to Project \\href{https://haoyin116.github.io/FLEX_Dataset}{page}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŠ¨ä½œè´¨é‡è¯„ä¼°(Action Quality Assessment, AQA)æ•°æ®é›†å±€é™äºå•è§†è§’å’Œå•ä¸€æ¨¡æ€ã€ä¸”ç¼ºä¹ä¸“ä¸šå¥èº«è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€ã€å¤šè§†è§’å¥èº«åŠ¨ä½œè´¨é‡è¯„ä¼°æ•°æ®é›†FLEXã€‚FLEXåŒ…å«äº†ç”±38åä¸åŒæ°´å¹³å—è¯•è€…å®Œæˆçš„20ç§è´Ÿé‡ç»ƒä¹ çš„7,500å¤šæ¡å¤šè§†è§’è®°å½•ï¼ŒåŒæ­¥é›†æˆäº†RGBè§†é¢‘ã€3Då§¿æ€ã€è¡¨é¢è‚Œç”µä¿¡å·(sEMG)ä»¥åŠç”Ÿç†ä¿¡å·ã€‚ç ”ç©¶è€…è¿˜æ„å»ºäº†å¥èº«çŸ¥è¯†å›¾è°±(Fitness Knowledge Graph, FKG)ï¼Œå°†åŠ¨ä½œã€å…³é”®æ­¥éª¤ã€é”™è¯¯ç±»å‹åŠåé¦ˆç›¸è”ç³»ï¼Œå¹¶æ”¯æŒåŸºäºæ­¤çš„å¯è§£é‡Šç»„åˆè¯„åˆ†å‡½æ•°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†FLEX-VideoQAç»“æ„åŒ–é—®ç­”åŸºå‡†ï¼Œé€šè¿‡å±‚æ¬¡åŒ–æŸ¥è¯¢é©±åŠ¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„è·¨æ¨¡æ€æ¨ç†ã€‚åŸºå‡†å®éªŒè¡¨æ˜ï¼Œå¤šæ¨¡æ€è¾“å…¥å’Œç»†ç²’åº¦æ ‡æ³¨æ˜¾è‘—å¢å¼ºäº†AQAæ€§èƒ½ï¼Œå¹¶æ”¯æŒäº†ä»Videoåˆ°EMGé¢„æµ‹ç­‰æ–°é¢–çš„è·¨æ¨¡æ€ä»»åŠ¡ã€‚FLEXä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„å¥èº«è¯„ä¼°ä¸æ•™ç»ƒç³»ç»Ÿæä¾›äº†ç”Ÿç‰©åŠ›å­¦å¯¼å‘çš„è¡¨ç¤ºå­¦ä¹ åŸºç¡€ï¼Œæ¨åŠ¨äº†AQAæŠ€æœ¯å‘æ›´å¤æ‚çš„å¤šæ¨¡æ€ç¯å¢ƒæ¼”è¿›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Dataset and code are available at https://github.com/HaoYin116/FLEX . Link to Project page https://haoyin116.github.io/FLEX_Dataset",
      "pdf_url": "https://arxiv.org/pdf/2506.03198v3",
      "published_date": "2025-06-02 01:44:02 UTC",
      "updated_date": "2025-10-17 03:26:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:51:19.569623+00:00"
    },
    {
      "arxiv_id": "2506.01242v1",
      "title": "General search techniques without common knowledge for imperfect-information games, and application to superhuman Fog of War chess",
      "title_zh": "æ— éœ€å…±åŒçŸ¥è¯†çš„éå®Œç¾ä¿¡æ¯åšå¼ˆé€šç”¨æœç´¢æŠ€æœ¯åŠå…¶åœ¨è¶…äººç±»æ°´å¹³æˆ˜äº‰è¿·é›¾è±¡æ£‹ä¸­çš„åº”ç”¨",
      "authors": [
        "Brian Hu Zhang",
        "Tuomas Sandholm"
      ],
      "abstract": "Since the advent of AI, games have served as progress benchmarks. Meanwhile, imperfect-information variants of chess have existed for over a century, present extreme challenges, and have been the focus of significant AI research. Beyond calculation needed in regular chess, they require reasoning about information gathering, the opponent's knowledge, signaling, etc. The most popular variant, Fog of War (FoW) chess (aka. dark chess) is a recognized challenge problem in AI after superhuman performance was reached in no-limit Texas hold'em poker. We present Obscuro, the first superhuman AI for FoW chess. It introduces advances to search in imperfect-information games, enabling strong, scalable reasoning. Experiments against the prior state-of-the-art AI and human players -- including the world's best -- show that Obscuro is significantly stronger. FoW chess is the largest (by amount of imperfect information) turn-based game in which superhuman performance has been achieved and the largest game in which imperfect-information search has been successfully applied.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ä»‹ç»äº†Obscuroï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨æˆ˜äº‰è¿·é›¾è±¡æ£‹(Fog of War chess)é¢†åŸŸè¾¾åˆ°è¶…è¶Šäººç±»æ°´å¹³çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚è¯¥ç ”ç©¶é’ˆå¯¹ä¸å®Œå…¨ä¿¡æ¯åšå¼ˆ(imperfect-information games)æå‡ºäº†ä¸€ç§æ— éœ€å…±åŒçŸ¥è¯†(common knowledge)çš„é€šç”¨æœç´¢æŠ€æœ¯ï¼Œä½¿æ¨¡å‹å…·å¤‡äº†å¼ºå¤§ä¸”å¯æ‰©å±•çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸è¾ƒäºä¼ ç»Ÿè±¡æ£‹ï¼ŒObscuroèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¿¡æ¯æ”¶é›†ã€å¯¹æ‰‹çŸ¥è¯†æ¨ç†åŠä¿¡å·ä¼ é€’ç­‰æç«¯æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒObscuroåœ¨å¯¹æŠ—å…ˆå‰çš„æœ€å…ˆè¿›AIä»¥åŠåŒ…æ‹¬ä¸–ç•Œé¡¶çº§é€‰æ‰‹åœ¨å†…çš„äººç±»ç©å®¶æ—¶å‡å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚Fog of War chessæ˜¯ç›®å‰å·²å®ç°è¶…äººç±»æ°´å¹³ä¸”æˆåŠŸåº”ç”¨ä¸å®Œå…¨ä¿¡æ¯æœç´¢(imperfect-information search)çš„è§„æ¨¡æœ€å¤§ã€ä¿¡æ¯ä¸å®Œå…¨ç¨‹åº¦æœ€é«˜çš„è½®æ¬¡åˆ¶åšå¼ˆï¼Œè¯¥ç ”ç©¶ä¸ºå¤„ç†å¤æ‚ä¸å®Œå…¨ä¿¡æ¯ç¯å¢ƒä¸‹çš„å†³ç­–é—®é¢˜æä¾›äº†é‡è¦çªç ´ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01242v1",
      "published_date": "2025-06-02 01:41:27 UTC",
      "updated_date": "2025-06-02 01:41:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:50:53.551468+00:00"
    },
    {
      "arxiv_id": "2506.11069v1",
      "title": "Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition",
      "title_zh": "é¢å‘éšç§ä¿æŠ¤æ„éŸ³éšœç¢åŠè€å¹´äººè¯­éŸ³è¯†åˆ«çš„æ­£åˆ™åŒ–è”é‚¦å­¦ä¹ ",
      "authors": [
        "Tao Zhong",
        "Mengzhe Geng",
        "Shujie Hu",
        "Guinan Li",
        "Xunying Liu"
      ],
      "abstract": "Accurate recognition of dysarthric and elderly speech remains challenging to date. While privacy concerns have driven a shift from centralized approaches to federated learning (FL) to ensure data confidentiality, this further exacerbates the challenges of data scarcity, imbalanced data distribution and speaker heterogeneity. To this end, this paper conducts a systematic investigation of regularized FL techniques for privacy-preserving dysarthric and elderly speech recognition, addressing different levels of the FL process by 1) parameter-based, 2) embedding-based and 3) novel loss-based regularization. Experiments on the benchmark UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest that regularized FL systems consistently outperform the baseline FedAvg system by statistically significant WER reductions of up to 0.55\\% absolute (2.13\\% relative). Further increasing communication frequency to one exchange per batch approaches centralized training performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ„éŸ³éšœç¢å’Œè€å¹´äººè¯­éŸ³è¯†åˆ«(dysarthric and elderly speech recognition)é¢ä¸´çš„æ•°æ®ç¨€ç¼ºã€åˆ†å¸ƒä¸å‡åŠè¯´è¯äººå¼‚è´¨æ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨å¹³è¡¡éšç§ä¿æŠ¤ä¸è¯†åˆ«ç²¾åº¦çš„æ­£åˆ™åŒ–è”é‚¦å­¦ä¹ (regularized Federated Learning, FL)æ¡†æ¶ã€‚ç ”ç©¶è€…ç³»ç»Ÿåœ°æ¢è®¨äº†åŸºäºå‚æ•°(parameter-based)ã€åŸºäºåµŒå…¥(embedding-based)ä»¥åŠæ–°å‹åŸºäºæŸå¤±(loss-based)çš„ä¸‰ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–è”é‚¦å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¨¡å‹è¡¨ç°ã€‚åœ¨UASpeechæ„éŸ³éšœç¢è¯­éŸ³è¯­æ–™åº“å’ŒDementiaBank Pittè€å¹´äººè¯­éŸ³è¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ­£åˆ™åŒ–è”é‚¦å­¦ä¹ ç³»ç»Ÿç›¸è¾ƒäºåŸºå‡†çš„FedAvgç³»ç»Ÿå®ç°äº†ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„è¯é”™ç‡(WER)é™ä½ï¼Œç»å¯¹é™å¹…è¾¾0.55%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é€šè¿‡æé«˜é€šä¿¡é¢‘ç‡ï¼Œè¯¥ç³»ç»Ÿçš„æ€§èƒ½èƒ½å¤Ÿæ¥è¿‘é›†ä¸­å¼è®­ç»ƒ(centralized training)çš„æ°´å¹³ï¼Œä¸ºéšç§ä¿æŠ¤ä¸‹çš„ç‰¹æ®Šè¯­éŸ³è¯†åˆ«ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.11069v1",
      "published_date": "2025-06-02 01:34:20 UTC",
      "updated_date": "2025-06-02 01:34:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:51:49.022909+00:00"
    },
    {
      "arxiv_id": "2506.01237v1",
      "title": "Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean",
      "title_zh": "é›•ç¢ GEM çš„æ¯ä¸€ä¸ªä¾§é¢ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸äººç±»çš„éŸ©è¯­è¯­è¨€èƒ½åŠ›æµ‹è¯„",
      "authors": [
        "SungHo Kim",
        "Nayeon Kim",
        "Taehee Jeon",
        "SangKeun Lee"
      ],
      "abstract": "We introduce the $\\underline{Ko}rean \\underline{G}rammar \\underline{E}valuation Bench\\underline{M}ark (KoGEM)$, designed to assess the linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k multiple-choice QA pairs covering five main categories and 16 subcategories. The zero-shot evaluation of 27 LLMs of various sizes and types reveals that while LLMs perform remarkably well on straightforward tasks requiring primarily definitional knowledge, they struggle with tasks that demand the integration of real-world experiential knowledge, such as phonological rules and pronunciation. Furthermore, our in-depth analysis suggests that incorporating such experiential knowledge could enhance the linguistic competence of LLMs. With KoGEM, we not only highlight the limitations of current LLMs in linguistic competence but also uncover hidden facets of LLMs in linguistic competence, paving the way for enhancing comprehensive language understanding. Our code and dataset are available at: https://github.com/SungHo3268/KoGEM.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Korean Grammar Evaluation BenchMark (KoGEM)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å’Œäººç±»åœ¨éŸ©è¯­è¯­è¨€èƒ½åŠ›(linguistic competence)æ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚KoGEMç”±1.5kä¸ªå¤šé¡¹é€‰æ‹©é¢˜å¯¹ç»„æˆï¼Œæ¶µç›–äº†5ä¸ªä¸»è¦ç±»åˆ«å’Œ16ä¸ªå­ç±»åˆ«ã€‚é€šè¿‡å¯¹27ä¸ªä¸åŒè§„æ¨¡å’Œç±»å‹çš„LLMsè¿›è¡Œé›¶æ ·æœ¬(zero-shot)è¯„ä¼°ï¼Œç ”ç©¶å‘ç°LLMsåœ¨å¤„ç†éœ€è¦å®šä¹‰æ€§çŸ¥è¯†çš„ç®€å•ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†éœ€è¦æ•´åˆç°å®ä¸–ç•Œç»éªŒçŸ¥è¯†çš„ä»»åŠ¡ï¼ˆå¦‚éŸ³ç³»è§„åˆ™(phonological rules)å’Œå‘éŸ³(pronunciation)ï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæ•´åˆæ­¤ç±»ç»éªŒæ€§çŸ¥è¯†èƒ½å¤Ÿæœ‰æ•ˆæå‡LLMsçš„è¯­è¨€èƒ½åŠ›ã€‚è¯¥åŸºå‡†ä¸ä»…æ­ç¤ºäº†å½“å‰LLMsåœ¨è¯­è¨€èƒ½åŠ›ä¸Šçš„å±€é™æ€§ï¼Œè¿˜ä¸ºå¢å¼ºå…¨é¢çš„è¯­è¨€ç†è§£èƒ½åŠ›æä¾›äº†æ–°çš„è·¯å¾„ã€‚ç›®å‰è¯¥é¡¹ç›®çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œæ—¨åœ¨æ¨åŠ¨éŸ©è¯­è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å‘å±•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2025 main conference",
      "pdf_url": "https://arxiv.org/pdf/2506.01237v1",
      "published_date": "2025-06-02 01:27:46 UTC",
      "updated_date": "2025-06-02 01:27:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:52:38.593578+00:00"
    },
    {
      "arxiv_id": "2506.01234v2",
      "title": "Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression",
      "title_zh": "ç”¨äºå¤šå…‰è°±å«æ˜Ÿå›¾åƒå‹ç¼©çš„ Fourier è°ƒåˆ¶éšå¼ç¥ç»è¡¨ç¤º",
      "authors": [
        "Woojin Cho",
        "Steve Andreas Immanuel",
        "Junhyuk Heo",
        "Darongsae Kwon"
      ],
      "abstract": "Multispectral satellite images play a vital role in agriculture, fisheries, and environmental monitoring. However, their high dimensionality, large data volumes, and diverse spatial resolutions across multiple channels pose significant challenges for data compression and analysis. This paper presents ImpliSat, a unified framework specifically designed to address these challenges through efficient compression and reconstruction of multispectral satellite data. ImpliSat leverages Implicit Neural Representations (INR) to model satellite images as continuous functions over coordinate space, capturing fine spatial details across varying spatial resolutions. Furthermore, we introduce a Fourier modulation algorithm that dynamically adjusts to the spectral and spatial characteristics of each band, ensuring optimal compression while preserving critical image details.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šå…‰è°±å«æ˜Ÿå›¾åƒåœ¨é«˜ç»´åº¦ã€å¤§æ•°æ®é‡ä»¥åŠå„é€šé“ç©ºé—´åˆ†è¾¨ç‡å·®å¼‚å¤§ç­‰èƒŒæ™¯ä¸‹å¸¦æ¥çš„å‹ç¼©ä¸åˆ†æéš¾é¢˜ï¼Œæå‡ºäº†åä¸º ImpliSat çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨éšå¼ç¥ç»è¡¨ç¤º (Implicit Neural Representations, INR) æŠ€æœ¯ï¼Œå°†å«æ˜Ÿå›¾åƒå»ºæ¨¡ä¸ºåæ ‡ç©ºé—´ä¸Šçš„è¿ç»­å‡½æ•°ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¹¶å¤„ç†è·¨åˆ†è¾¨ç‡çš„ç²¾ç»†ç©ºé—´ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§ Fourier modulation ç®—æ³•ï¼Œå¯æ ¹æ®æ¯ä¸ªæ³¢æ®µçš„å…‰è°±å’Œç©ºé—´ç‰¹æ€§è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œä»¥ä¼˜åŒ–å‹ç¼©æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒImpliSat åœ¨å®ç°é«˜æ•ˆæ•°æ®å‹ç¼©çš„åŒæ—¶ï¼Œèƒ½å¤Ÿç²¾å‡†ä¿ç•™å…³é”®çš„å›¾åƒç»†èŠ‚ï¼Œä¸ºå†œä¸šã€æ¸”ä¸šåŠç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸçš„å«æ˜Ÿæ•°æ®å¤„ç†æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to IGARSS 2025 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2506.01234v2",
      "published_date": "2025-06-02 01:16:36 UTC",
      "updated_date": "2025-06-11 11:15:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:51:49.642266+00:00"
    },
    {
      "arxiv_id": "2506.01232v1",
      "title": "Retrieval-Augmented Generation of Ontologies from Relational Databases",
      "title_zh": "åŸºäºå…³ç³»æ•°æ®åº“çš„æ£€ç´¢å¢å¼ºæœ¬ä½“ç”Ÿæˆ",
      "authors": [
        "Mojtaba Nayyeri",
        "Athish A Yogi",
        "Nadeen Fathallah",
        "Ratan Bahadur Thapa",
        "Hans-Michael Tautenhahn",
        "Anton Schnurpel",
        "Steffen Staab"
      ],
      "abstract": "Transforming relational databases into knowledge graphs with enriched ontologies enhances semantic interoperability and unlocks advanced graph-based learning and reasoning over data. However, previous approaches either demand significant manual effort to derive an ontology from a database schema or produce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative Generation of RDB Ontologies, an LLM-driven approach that turns relational schemas into rich OWL ontologies with minimal human effort. RIGOR combines three sources via RAG, the database schema and its documentation, a repository of domain ontologies, and a growing core ontology, to prompt a generative LLM for producing successive, provenance-tagged delta ontology fragments. Each fragment is refined by a judge-LLM before being merged into the core ontology, and the process iterates table-by-table following foreign key constraints until coverage is complete. Applied to real-world databases, our approach outputs ontologies that score highly on standard quality dimensions such as accuracy, completeness, conciseness, adaptability, clarity, and consistency, while substantially reducing manual effort.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RIGOR (Retrieval-augmented Iterative Generation of RDB Ontologies)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„æ–¹æ³•ï¼Œæ—¨åœ¨è‡ªåŠ¨å°†å…³ç³»æ•°æ®åº“æ¨¡å¼è½¬æ¢ä¸ºä¸°å¯Œçš„OWL ontologiesã€‚é’ˆå¯¹ä»¥å¾€æ–¹æ³•ä¾èµ–å¤§é‡äººå·¥æˆ–ä»…èƒ½ç”ŸæˆåŸºç¡€æœ¬ä½“çš„å±€é™æ€§ï¼ŒRIGORåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯æ•´åˆäº†æ•°æ®åº“æ¨¡å¼åŠå…¶æ–‡æ¡£ã€é¢†åŸŸæœ¬ä½“åº“ä»¥åŠä¸æ–­å¢é•¿çš„æ ¸å¿ƒæœ¬ä½“ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è¿­ä»£æ–¹å¼ï¼Œæ ¹æ®å¤–é”®çº¦æŸé€è¡¨ç”Ÿæˆå¸¦æœ‰æº¯æºæ ‡è®°çš„delta ontology fragmentsï¼Œå¹¶åœ¨åˆå¹¶å‰ç”±Judge-LLMè¿›è¡Œä¼˜åŒ–å®¡æ ¸ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRIGORåœ¨å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€ç®€æ´æ€§å’Œä¸€è‡´æ€§ç­‰å¤šä¸ªè´¨é‡ç»´åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¯¥æ–¹æ³•åœ¨æ˜¾è‘—å‡å°‘äººå·¥å‚ä¸çš„åŒæ—¶ï¼Œå¢å¼ºäº†æ•°æ®çš„è¯­ä¹‰äº’æ“ä½œæ€§ï¼Œä¸ºé«˜çº§å›¾å­¦ä¹ (graph-based learning)å’Œæ¨ç†å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2506.01232v1",
      "published_date": "2025-06-02 01:10:05 UTC",
      "updated_date": "2025-06-02 01:10:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:51:52.145369+00:00"
    },
    {
      "arxiv_id": "2506.01231v2",
      "title": "Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution",
      "title_zh": "åŸºäºæ¢¯åº¦è´¡çŒ®åˆ’åˆ†çš„é«˜æ•ˆå°‘æ ·æœ¬å›¾ç¥ç»æ¶æ„æœç´¢",
      "authors": [
        "Wenhao Song",
        "Xuan Wu",
        "Bo Yang",
        "You Zhou",
        "Yubin Xiao",
        "Yanchun Liang",
        "Hongwei Ge",
        "Heow Pueh Lee",
        "Chunguo Wu"
      ],
      "abstract": "To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°‘æ ·æœ¬ç¥ç»æ¶æ„æœç´¢ (few-shot Neural Architecture Search, NAS) ä¸­çš„æƒé‡è€¦åˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ¢¯åº¦è´¡çŒ® (Gradient Contribution, GC) æ–¹æ³•ï¼Œé€šè¿‡åœ¨è¶…ç½‘ç»œåå‘ä¼ æ’­ä¸­åˆ†è§£å‘é‡-é›…å¯æ¯”ä¹˜ç§¯ (Vector-Jacobian Product) æ¥é«˜æ•ˆè®¡ç®—æ¨¡å—é—´çš„æ¢¯åº¦å†²çªï¼Œä»è€Œä¼˜åŒ–å­è¶…ç½‘ç»œçš„åˆ’åˆ†æ–¹æ¡ˆã€‚ä¸ºå…‹æœç°æœ‰æ–¹æ³•ä»…èƒ½æœç´¢å•ä¸€ç±»å‹å›¾ç½‘ç»œçš„å±€é™ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ç»Ÿä¸€å›¾ç¥ç»æ¶æ„æœç´¢ (Unified Graph Neural Architecture Search, UGAS) æ¡†æ¶ï¼Œä»¥æ¢ç´¢æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œ (MPNNs) ä¸å›¾å˜æ¢å™¨ (Graph Transformers, GTs) çš„æœ€ä¼˜ç»„åˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGC æ–¹æ³•åœ¨åˆ’åˆ†è´¨é‡å’Œæ—¶é—´æ•ˆç‡ä¸Šå‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿› (SOTA) æ°´å¹³ã€‚é€šè¿‡ UGAS+GC æœç´¢åˆ°çš„æ¶æ„åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæ‰‹å·¥è®¾è®¡çš„ GNNs åŠç°æœ‰çš„ NAS æ–¹æ³•ï¼Œå……åˆ†éªŒè¯äº†æ‰€ææ–¹æ³•åœ¨æå‡å›¾ç¥ç»ç½‘ç»œæœç´¢æ•ˆç‡ä¸è´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by SIGKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.01231v2",
      "published_date": "2025-06-02 01:03:12 UTC",
      "updated_date": "2025-06-20 09:18:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:52:51.666815+00:00"
    },
    {
      "arxiv_id": "2506.01227v1",
      "title": "SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs",
      "title_zh": "SPEARï¼šåŸºäºæ”»å‡»-è¿é€šæ€§è¶…å›¾ AI è§„åˆ’æ¨ç†çš„å®‰å…¨æ€åŠ¿è¯„ä¼°",
      "authors": [
        "Rakesh Podder",
        "Turgay Caglar",
        "Shadaab Kawnain Bashir",
        "Sarath Sreedharan",
        "Indrajit Ray",
        "Indrakshi Ray"
      ],
      "abstract": "Graph-based frameworks are often used in network hardening to help a cyber defender understand how a network can be attacked and how the best defenses can be deployed. However, incorporating network connectivity parameters in the attack graph, reasoning about the attack graph when we do not have access to complete information, providing system administrator suggestions in an understandable format, and allowing them to do what-if analysis on various scenarios and attacker motives is still missing. We fill this gap by presenting SPEAR, a formal framework with tool support for security posture evaluation and analysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI planning to model vulnerabilities and configurations in a networked system. It automatically converts network configurations and vulnerability descriptions into planning models expressed in the Planning Domain Definition Language (PDDL). SPEAR identifies a set of diverse security hardening strategies that can be presented in a manner understandable to the domain expert. These allow the administrator to explore the network hardening solution space in a systematic fashion and help evaluate the impact and compare the different solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SPEARï¼Œä¸€ä¸ªç”¨äºå®‰å…¨æ€§æ€è¯„ä¼°å’Œåˆ†æçš„å½¢å¼åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾æ¨¡å‹åœ¨ç½‘ç»œè¿é€šæ€§é›†æˆã€ä¿¡æ¯ä¸å®Œæ•´ä¸‹çš„æ¨ç†ä»¥åŠç®¡ç†å‘˜å†³ç­–æ”¯æŒç­‰æ–¹é¢çš„ä¸è¶³ã€‚SPEAR å¼•å…¥äº† AI Planning çš„å› æœå½¢å¼åŒ–æ–¹æ³•æ¥å»ºæ¨¡ç½‘ç»œç³»ç»Ÿä¸­çš„æ¼æ´å’Œé…ç½®ï¼Œå¹¶èƒ½è‡ªåŠ¨å°†ç½‘ç»œæè¿°è½¬åŒ–ä¸º PDDL (Planning Domain Definition Language) è§„åˆ’æ¨¡å‹ã€‚è¯¥æ¡†æ¶ç‰¹åˆ«å¼ºè°ƒ Human-in-the-loop çš„å‚ä¸ï¼Œé€šè¿‡ Attack-Connectivity Hypergraphs æ•æ‰å¤æ‚çš„æ”»å‡»è·¯å¾„ä¸è¿é€šæ€§å‚æ•°ã€‚SPEAR èƒ½å¤Ÿè¯†åˆ«å‡ºä¸€ç³»åˆ—å¤šæ ·åŒ–çš„ Security Hardening ç­–ç•¥ï¼Œå¹¶ä»¥ä¸“å®¶æ˜“äºç†è§£çš„æ ¼å¼å‘ˆç°ï¼Œæ”¯æŒå¯¹ä¸åŒæ”»å‡»åœºæ™¯å’ŒåŠ¨æœºè¿›è¡Œ What-if åˆ†æã€‚å®éªŒè¡¨æ˜è¯¥ç³»ç»Ÿèƒ½å¸®åŠ©ç®¡ç†å‘˜åœ¨å¤æ‚çš„åŠ å›ºæ–¹æ¡ˆç©ºé—´ä¸­è¿›è¡Œç³»ç»ŸåŒ–æ¢ç´¢ï¼Œä»è€Œæ›´ç§‘å­¦åœ°è¯„ä¼°ä¸åŒé˜²å¾¡æ–¹æ¡ˆçš„å½±å“å¹¶è¿›è¡Œå¯¹æ¯”å†³ç­–ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.01227v1",
      "published_date": "2025-06-02 00:38:47 UTC",
      "updated_date": "2025-06-02 00:38:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T17:52:15.349588+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 196,
  "processed_papers_count": 196,
  "failed_papers_count": 0,
  "llm_backup_calls": 1,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T17:53:54.017335+00:00"
}
{
  "date": "2024-07-17",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-17 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 97 篇论文，主要聚焦 AI 模型优化、多模态学习、强化学习和计算机视觉等领域，其中 LLM 的推理能力评估（如 GPT 模型）和高效训练方法（如 ColorMAE 和 LookupViT）最为引人注目，同时有斯坦福学者（如 Rishi Bommasani）的作品突出 AI 透明度。\n\n下面，我将挑选最具影响力和话题度的论文优先讨论，将相关主题归类（如 AI 和多模态），并快速掠过较基础或特定领域的文章。每个论文标题以中文 + 英文形式列出，焦点放在核心贡献和发现上。\n\n### AI 和 LLM 相关论文\n- **Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies**（英文原标题）：Lachlan McGinness 和 Peter Baumgartner 的研究评估了 GPT-4、GPT-3.5 和 Gemini 在自动定理证明策略上的推理能力，发现正确推理与正确答案的相关性低，且模型更适合自下而上的推理过程。该工作为 LLM 在逻辑任务中的可靠性提供了新洞见，强调了不确定性评估的重要性。\n- **Halu-J: Critique-Based Hallucination Judge**（英文原标题）：Binjie Wang 等提出了一种基于 70 亿参数的模型来检测 LLM 的幻觉问题，通过证据选择和详细批评生成，超越 GPT-4o 在多证据检测中的表现。该方法为 LLM 输出可靠性提供了实用工具，代码已开源。\n- **Beyond Next Token Prediction: Patch-Level Training for Large Language Models**（英文原标题）：Chenze Shao 等开发了一种基于补丁级训练的 LLM 方法，减少训练成本至原先一半，同时保持性能。该创新通过聚合 token 进行预测，适用于高效 LLM 训练。\n- **PersLLM: A Personified Training Approach for Large Language Models**（英文原标题）：Zheni Zeng 等提出 PersLLM 框架，通过数据增强和动态优化，使 LLM 具备个性化表达能力，提升在多任务中的表现。该工作在人机交互和多代理系统中显示潜力。\n- **Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale**（英文原标题）：Ayush Kaushal 等构建了 Spectra LLM 套件，证明三进制 LLM 在规模化时比量化模型更高效，3.9B 参数模型匹配浮点模型准确率。该发现挑战了传统量化方法。\n\n其他 AI 相关论文，如第14（A Survey of Prompt Engineering Methods）和第52（Towards Collaborative Intelligence），则快速总结：前者综述了 LLM 在 NLP 任务中的提示工程策略，提升了任务泛化；后者探索了 LLM 在多代理协调中的意图传播，适用于复杂决策系统。\n\n### 计算机视觉和多模态学习\n- **ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders**（英文原标题）：Carlos Hinojosa 等引入 ColorMAE，通过数据无关的噪声过滤策略提升自监督学习效果，在语义分割任务中比基线提升 2.72 mIoU。该工作已接受 ECCV 2024，强调高效视觉表示学习。\n- **LookupViT: Compressing visual information to a limited number of tokens**（英文原标题）：Rajat Koner 等提出 LookupViT 框架，通过压缩 token 和分层处理优化视觉 Transformer，在图像和视频任务中实现 2x FLOPs 减少，同时提升鲁棒性。该方法在 ECCV 2024 中脱颖而出。\n- **DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent Diffusion**（英文原标题）：Huiguo He 等开发 DreamStory，使用 LLM 指导的多主体扩散模型生成一致的故事可视化，显著改善多主体场景的连贯性。该工作包括新基准 DS-500。\n- **EmoCtrl-TTS: Turning Generative Models Degenerate for Time-Varying Emotional States**（英文原标题，基于第47）：Haibin Wu 等提出 EmoCtrl-TTS，通过唤醒和情感值控制生成情感丰富的语音，包括笑声和哭声，在零-shot TTS 中表现出色。\n\n相关论文如第24（CHOSEN）和第77（NavGPT-2）值得一提：前者优化了视觉 Transformer 在 FPGA 上的推理效率；后者提升了 LLM 在视觉导航中的推理能力，支持零-shot 应用。第58（Conditional Quantile Estimation）和第79（ModalChorus）则快速掠过：前者用于视频推荐的不确定性建模，后者可视化多模态嵌入对齐。\n\n### 强化学习和机器人\n- **Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems**（英文原标题）：Tamer Abuelsaad 等构建 Agent-E 框架，提升 Web 代理的导航性能，通过分层架构和自适应观察在基准上领先 10-30%。该工作为代理系统设计提供了通用原则。\n- **Learning Long-Horizon Predictions for Quadrotor Dynamics**（英文原标题）：Pratyaksh Prabhav Rao 等提出解耦动态学习方法，优化四旋翼无人机的长期预测，减少累积误差，在真实数据上提升规划精度，已接受 IROS 2024。\n- **Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments**（英文原标题）：Runfa Chen 等开发 SHNN 框架，通过子等变处理提升多实体强化学习效率，在新基准 MEBEN 上表现突出。\n\n其他如第30（Sparsity-based Safety Conservatism）和第65（Variable-Agnostic Causal Exploration）快速总结：前者改善了离线强化学习的鲁棒性；后者通过因果探索提升代理探索效率。\n\n### 其他领域快速掠过\n剩余论文多为特定应用或理论探讨，如生物医学（第23、32）、图神经网络（第15、31）和优化算法（第38、39）。例如，第21（The 2024 Foundation Model Transparency Index）由 Rishi Bommasani 等发布，评估了 14 个模型的透明度，平均得分从 37 提升到 58，强调持续不透明领域如版权和数据访问。第87（On the Complexity of Identification in Linear Structural Causal Models）探讨了因果模型的计算复杂性，但影响力有限，仅提及其 NP-hard 证明。\n\n总之，今天的 arXiv 更新突显了 AI 领域的创新潜力，LLM 和多模态模型的进展尤其值得关注。如果你对特定主题感兴趣，建议优先查看上述关键论文！明天的快报见。",
  "papers": [
    {
      "arxiv_id": "2407.13054v2",
      "title": "Comprehensive Review and Empirical Evaluation of Causal Discovery Algorithms for Numerical Data",
      "title_zh": "针对数值数据的因果发现算法的全面回顾与实证评估",
      "authors": [
        "Wenjin Niu",
        "Zijun Gao",
        "Liyan Song",
        "Lingbo Li"
      ],
      "abstract": "Causal analysis has become an essential component in understanding the\nunderlying causes of phenomena across various fields. Despite its significance,\nexisting literature on causal discovery algorithms is fragmented, with\ninconsistent methodologies, i.e., there is no universal classification standard\nfor existing methods, and a lack of comprehensive evaluations, i.e., data\ncharacteristics are often ignored to be jointly analyzed when benchmarking\nalgorithms. This study addresses these gaps by conducting an exhaustive review\nand empirical evaluation for causal discovery methods on numerical data, aiming\nto provide a clearer and more structured understanding of the field. Our\nresearch begins with a comprehensive literature review spanning over two\ndecades, analyzing over 200 academic articles and identifying more than 40\nrepresentative algorithms. This extensive analysis leads to the development of\na structured taxonomy tailored to the complexities of causal discovery,\ncategorizing methods into six main types. To address the lack of comprehensive\nevaluations, our study conducts an extensive empirical assessment of 29 causal\ndiscovery algorithms on multiple synthetic and real-world datasets. We\ncategorize synthetic datasets based on size, linearity, and noise distribution,\nemploying five evaluation metrics, and summarize the top-3 algorithm\nrecommendations, providing guidelines for users in various data scenarios. Our\nresults highlight a significant impact of dataset characteristics on algorithm\nperformance. Moreover, a metadata extraction strategy with an accuracy\nexceeding 80% is developed to assist users in algorithm selection on unknown\ndatasets. Based on these insights, we offer professional and practical\nguidelines to help users choose the most suitable causal discovery methods for\ntheir specific dataset.",
      "tldr_zh": "本研究对因果发现（causal discovery）算法进行了全面回顾和实证评估，针对数值数据的问题，填补了现有文献的碎片化和评估不足。研究者分析了超过200篇学术论文，识别了40多个代表性算法，并开发了一个结构化的分类系统，将方法分为六大类。实验评估了29个算法在多种合成和真实数据集上的表现，根据数据集特性（如大小、线性度和噪声分布）提供了顶尖算法推荐和专业指导；此外，还开发了一个准确率超过80%的元数据提取策略，帮助用户针对未知数据集选择合适的方法。该工作突出了数据集特性对算法性能的显著影响，为因果分析实践提供了实用框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.13054v2",
      "published_date": "2024-07-17 23:47:05 UTC",
      "updated_date": "2024-09-04 13:13:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:23:53.198592"
    },
    {
      "arxiv_id": "2408.02074v1",
      "title": "Applying Conditional Generative Adversarial Networks for Imaging Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Haowei Yang",
        "Yuxiang Hu",
        "Shuyao He",
        "Ting Xu",
        "Jiajie Yuan",
        "Xingxin Gu"
      ],
      "abstract": "This study introduces an innovative application of Conditional Generative\nAdversarial Networks (C-GAN) integrated with Stacked Hourglass Networks (SHGN)\naimed at enhancing image segmentation, particularly in the challenging\nenvironment of medical imaging. We address the problem of overfitting, common\nin deep learning models applied to complex imaging datasets, by augmenting data\nthrough rotation and scaling. A hybrid loss function combining L1 and L2\nreconstruction losses, enriched with adversarial training, is introduced to\nrefine segmentation processes in intravascular ultrasound (IVUS) imaging. Our\napproach is unique in its capacity to accurately delineate distinct regions\nwithin medical images, such as tissue boundaries and vascular structures,\nwithout extensive reliance on domain-specific knowledge. The algorithm was\nevaluated using a standard medical image library, showing superior performance\nmetrics compared to existing methods, thereby demonstrating its potential in\nenhancing automated medical diagnostics through deep learning",
      "tldr_zh": "本文研究提出将 Conditional Generative Adversarial Networks (C-GAN) 与 Stacked Hourglass Networks (SHGN) 整合，用于提升医疗图像分割，尤其在处理复杂数据集的过拟合问题上。方法包括通过旋转和缩放进行数据增强，并引入混合损失函数（结合 L1 和 L2 重建损失以及对抗训练），以准确划分 intravascular ultrasound (IVUS) 图像中的组织边界和血管结构，而不依赖过多领域特定知识。在标准医疗图像库上的评估显示，该方法性能优于现有技术，从而增强了自动化医疗诊断的潜力。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.02074v1",
      "published_date": "2024-07-17 23:23:09 UTC",
      "updated_date": "2024-07-17 23:23:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:24:05.916069"
    },
    {
      "arxiv_id": "2407.20244v1",
      "title": "Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Lachlan McGinness",
        "Peter Baumgartner"
      ],
      "abstract": "This study presents the first examination of the ability of Large Language\nModels (LLMs) to follow reasoning strategies that are used to guide Automated\nTheorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and\nGoogle's recent Gemini model on problems from a steamroller domain. In addition\nto determining accuracy we make use of the Natural Language Processing library\nspaCy to explore new methods of investigating LLM's reasoning capabilities.\nThis led to one alarming result, the low correlation between correct reasoning\nand correct answers for any of the tested models. We found that the models'\nperformance when using the ATP reasoning strategies was comparable to one-shot\nchain of thought and observe that attention to uncertainty in the accuracy\nresults is critical when drawing conclusions about model performance.\nConsistent with previous speculation we confirm that LLMs have a preference\nfor, and are best able to follow, bottom up reasoning processes. However, the\nreasoning strategies can still be beneficial for deriving small and relevant\nsets of formulas for external processing by a trusted inference engine.",
      "tldr_zh": "本研究首次评估大型语言模型（LLMs）遵循自动定理证明器（ATPs）推理策略的能力，测试了 GPT4、GPT3.5 Turbo 和 Gemini 模型在 steamroller 领域的问题上。使用 Natural Language Processing 库 spaCy 探索了 LLMs 的推理特性，结果显示正确推理与正确答案的相关性较低，且模型性能与 one-shot chain of thought 相当。研究确认 LLMs 更偏好自下而上的推理过程，并指出这些策略可为外部推理引擎提供小型相关公式集，从而提升可信赖的推理应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20244v1",
      "published_date": "2024-07-17 22:49:23 UTC",
      "updated_date": "2024-07-17 22:49:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:24:18.345559"
    },
    {
      "arxiv_id": "2407.13044v4",
      "title": "DropKAN: Regularizing KANs by masking post-activations",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammed Ghaith Altarabichi"
      ],
      "abstract": "We propose DropKAN (Dropout Kolmogorov-Arnold Networks) a regularization\nmethod that prevents co-adaptation of activation function weights in\nKolmogorov-Arnold Networks (KANs). DropKAN functions by embedding the drop mask\ndirectly within the KAN layer, randomly masking the outputs of some activations\nwithin the KANs' computation graph. We show that this simple procedure that\nrequire minimal coding effort has a regularizing effect and consistently lead\nto better generalization of KANs. We analyze the adaptation of the standard\nDropout with KANs and demonstrate that Dropout applied to KANs' neurons can\nlead to unpredictable behavior in the feedforward pass. We carry an empirical\nstudy with real world Machine Learning datasets to validate our findings. Our\nresults suggest that DropKAN is consistently a better alternative to using\nstandard Dropout with KANs, and improves the generalization performance of\nKANs. Our implementation of DropKAN is available at:\n\\url{https://github.com/Ghaith81/dropkan}.",
      "tldr_zh": "本文提出 DropKAN，一种针对 Kolmogorov-Arnold Networks (KANs) 的正则化方法，通过在 KAN 层中嵌入 drop mask 来随机屏蔽某些激活输出，从而防止激活函数权重的共适应。相比于标准 Dropout，DropKAN 避免了前向传播中的不可预测行为，并显著提升了 KANs 的泛化性能。实验在真实世界机器学习数据集上验证了这些优势，显示 DropKAN 是一项更可靠的替代方案。开源实现可从 GitHub 获取。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.13044v4",
      "published_date": "2024-07-17 22:48:47 UTC",
      "updated_date": "2024-08-20 12:16:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:24:30.278537"
    },
    {
      "arxiv_id": "2408.00792v1",
      "title": "A Scalable and Generalized Deep Learning Framework for Anomaly Detection in Surveillance Videos",
      "title_zh": "一个可扩展且泛化的深度学习框架，用于监控视频中的异常检测",
      "authors": [
        "Sabah Abdulazeez Jebur",
        "Khalid A. Hussein",
        "Haider Kadhim Hoomod",
        "Laith Alzubaidi",
        "Ahmed Ali Saihood",
        "YuanTong Gu"
      ],
      "abstract": "Anomaly detection in videos is challenging due to the complexity, noise, and\ndiverse nature of activities such as violence, shoplifting, and vandalism.\nWhile deep learning (DL) has shown excellent performance in this area, existing\napproaches have struggled to apply DL models across different anomaly tasks\nwithout extensive retraining. This repeated retraining is time-consuming,\ncomputationally intensive, and unfair. To address this limitation, a new DL\nframework is introduced in this study, consisting of three key components:\ntransfer learning to enhance feature generalization, model fusion to improve\nfeature representation, and multi-task classification to generalize the\nclassifier across multiple tasks without training from scratch when new task is\nintroduced. The framework's main advantage is its ability to generalize without\nrequiring retraining from scratch for each new task. Empirical evaluations\ndemonstrate the framework's effectiveness, achieving an accuracy of 97.99% on\nthe RLVS dataset (violence detection), 83.59% on the UCF dataset (shoplifting\ndetection), and 88.37% across both datasets using a single classifier without\nretraining. Additionally, when tested on an unseen dataset, the framework\nachieved an accuracy of 87.25%. The study also utilizes two explainability\ntools to identify potential biases, ensuring robustness and fairness. This\nresearch represents the first successful resolution of the generalization issue\nin anomaly detection, marking a significant advancement in the field.",
      "tldr_zh": "本研究提出了一种可扩展且通用的深度学习框架，用于监控视频中的异常检测，旨在解决现有方法在处理多样性活动（如暴力、偷盗和破坏）时需反复重新训练的问题。该框架包括三个关键组件：transfer learning 增强特征泛化、model fusion 改善特征表示，以及 multi-task classification 实现分类器在多个任务间的泛化，无需为新任务从零开始训练。在实验中，该框架在 RLVS 数据集（暴力检测）上达到97.99%准确率、在 UCF 数据集（偷盗检测）上达到83.59%、在两个数据集上使用单一分类器达到88.37%准确率，并对未见数据集实现87.25%准确率，同时通过解释性工具识别偏差，确保了鲁棒性和公平性。该工作首次成功解决了异常检测中的泛化问题，标志着该领域的重大进展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00792v1",
      "published_date": "2024-07-17 22:41:12 UTC",
      "updated_date": "2024-07-17 22:41:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:24:42.124010"
    },
    {
      "arxiv_id": "2407.13036v1",
      "title": "ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders",
      "title_zh": "ColorMAE：探索 Masked AutoEncoders 中的数据无关掩码策略",
      "authors": [
        "Carlos Hinojosa",
        "Shuming Liu",
        "Bernard Ghanem"
      ],
      "abstract": "Masked AutoEncoders (MAE) have emerged as a robust self-supervised framework,\noffering remarkable performance across a wide range of downstream tasks. To\nincrease the difficulty of the pretext task and learn richer visual\nrepresentations, existing works have focused on replacing standard random\nmasking with more sophisticated strategies, such as adversarial-guided and\nteacher-guided masking. However, these strategies depend on the input data thus\ncommonly increasing the model complexity and requiring additional calculations\nto generate the mask patterns. This raises the question: Can we enhance MAE\nperformance beyond random masking without relying on input data or incurring\nadditional computational costs? In this work, we introduce a simple yet\neffective data-independent method, termed ColorMAE, which generates different\nbinary mask patterns by filtering random noise. Drawing inspiration from color\nnoise in image processing, we explore four types of filters to yield mask\npatterns with different spatial and semantic priors. ColorMAE requires no\nadditional learnable parameters or computational overhead in the network, yet\nit significantly enhances the learned representations. We provide a\ncomprehensive empirical evaluation, demonstrating our strategy's superiority in\ndownstream tasks compared to random masking. Notably, we report an improvement\nof 2.72 in mIoU in semantic segmentation tasks relative to baseline MAE\nimplementations.",
      "tldr_zh": "本论文探讨了 Masked AutoEncoders (MAE) 的数据无关掩码策略，旨在提升自监督学习框架的性能，而无需依赖输入数据或增加计算成本。ColorMAE 方法通过过滤随机噪声生成不同的二进制掩码模式，借鉴图像处理中的颜色噪声，采用四种过滤器引入空间和语义先验，从而学习更丰富的视觉表示。实验结果显示，ColorMAE 在下游任务中显著优于随机掩码策略，例如在语义分割任务中，mIoU 提高了 2.72。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Work Accepted for Publication at ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.13036v1",
      "published_date": "2024-07-17 22:04:00 UTC",
      "updated_date": "2024-07-17 22:04:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:24:54.496851"
    },
    {
      "arxiv_id": "2407.13032v1",
      "title": "Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems",
      "title_zh": "Agent-E：从自主网络导航到智能体系统的基础设计原则",
      "authors": [
        "Tamer Abuelsaad",
        "Deepak Akkil",
        "Prasenjit Dey",
        "Ashish Jagmohan",
        "Aditya Vempaty",
        "Ravi Kokku"
      ],
      "abstract": "AI Agents are changing the way work gets done, both in consumer and\nenterprise domains. However, the design patterns and architectures to build\nhighly capable agents or multi-agent systems are still developing, and the\nunderstanding of the implication of various design choices and algorithms is\nstill evolving. In this paper, we present our work on building a novel web\nagent, Agent-E \\footnote{Our code is available at\n\\url{https://github.com/EmergenceAI/Agent-E}}. Agent-E introduces numerous\narchitectural improvements over prior state-of-the-art web agents such as\nhierarchical architecture, flexible DOM distillation and denoising method, and\nthe concept of \\textit{change observation} to guide the agent towards more\naccurate performance. We first present the results of an evaluation of Agent-E\non WebVoyager benchmark dataset and show that Agent-E beats other SOTA text and\nmulti-modal web agents on this benchmark in most categories by 10-30\\%. We then\nsynthesize our learnings from the development of Agent-E into general design\nprinciples for developing agentic systems. These include the use of\ndomain-specific primitive skills, the importance of distillation and de-noising\nof environmental observations, the advantages of a hierarchical architecture,\nand the role of agentic self-improvement to enhance agent efficiency and\nefficacy as the agent gathers experience.",
      "tldr_zh": "该论文介绍了 Agent-E，一种先进的 web 代理系统，旨在提升自主 web 导航能力，并从其设计中提炼代理系统（agentic systems）的核心原则。Agent-E 通过分层架构（hierarchical architecture）、灵活的 DOM 蒸馏和去噪方法（flexible DOM distillation and denoising method），以及变化观察（change observation）概念等创新，显著改善了代理的性能。在 WebVoyager 基准数据集上，Agent-E 比现有 SOTA 文本和多模态 web 代理在大多数类别上提高了 10-30%。此外，论文总结了通用设计原则，包括使用领域特定原始技能（domain-specific primitive skills）、环境观察的蒸馏和去噪、分层架构的优势，以及代理的自改进机制，以提升系统效率和效能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.13032v1",
      "published_date": "2024-07-17 21:44:28 UTC",
      "updated_date": "2024-07-17 21:44:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:25:08.001792"
    },
    {
      "arxiv_id": "2407.13025v2",
      "title": "From Principles to Practices: Lessons Learned from Applying Partnership on AI's (PAI) Synthetic Media Framework to 11 Use Cases",
      "title_zh": "翻译失败",
      "authors": [
        "Claire R. Leibowicz",
        "Christian H. Cardona"
      ],
      "abstract": "2023 was the year the world woke up to generative AI, and 2024 is the year\npolicymakers are responding more firmly. Importantly, this policy momentum is\ntaking place alongside real world creation and distribution of synthetic media.\nSocial media platforms, news organizations, dating apps, image generation\ncompanies, and more are already navigating a world of AI-generated visuals and\nsounds, already changing hearts and minds, as policymakers try to catch up.\nHow, then, can AI governance capture the complexity of the synthetic media\nlandscape? How can it attend to synthetic media's myriad uses, ranging from\nstorytelling to privacy preservation, to deception, fraud, and defamation,\ntaking into account the many stakeholders involved in its development,\ncreation, and distribution? And what might it mean to govern synthetic media in\na manner that upholds the truth while bolstering freedom of expression? What\nfollows is the first known collection of diverse examples of the implementation\nof synthetic media governance that responds to these questions, specifically\nthrough Partnership on AI's (PAI) Responsible Practices for Synthetic Media - a\nvoluntary, normative Framework for creating, distributing, and building\ntechnology for synthetic media responsibly, launched in February 2023. In this\npaper, we present a case bank of real world examples that help operationalize\nthe Framework - highlighting areas synthetic media governance can be applied,\naugmented, expanded, and refined for use, in practice. Read together, the cases\nemphasize distinct elements of AI policymaking and seven emergent best\npractices supporting transparency, safety, expression, and digital dignity\nonline: consent, disclosure, and differentiation between harmful and creative\nuse cases.",
      "tldr_zh": "这篇论文探讨了将Partnership on AI (PAI)的Synthetic Media Framework应用于11个实际用例的经验教训，旨在应对合成媒体在创造、传播和治理方面的复杂挑战。研究通过这些多样化案例（如社交媒体、新闻和图像生成）展示了如何平衡真相维护与表达自由，同时考虑多方利益相关者。关键发现包括七个最佳实践：强调同意、披露以及区分有害与创意用例，以提升透明、安全和数字尊严。总的来说，该框架的应用为AI治理提供了可操作的指导，帮助完善合成媒体的负责任实践。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "18 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2407.13025v2",
      "published_date": "2024-07-17 21:27:56 UTC",
      "updated_date": "2024-07-19 15:57:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:25:18.611045"
    },
    {
      "arxiv_id": "2407.13023v1",
      "title": "A Three-Stage Algorithm for the Closest String Problem on Artificial and Real Gene Sequences",
      "title_zh": "一种针对人工和真实基因序列的最近字符串问题三阶段算法",
      "authors": [
        "Alireza Abdi",
        "Marko Djukanovic",
        "Hesam Tahmasebi Boldaji",
        "Hadis Salehi",
        "Aleksandar Kartelj"
      ],
      "abstract": "The Closest String Problem is an NP-hard problem that aims to find a string\nthat has the minimum distance from all sequences that belong to the given set\nof strings. Its applications can be found in coding theory, computational\nbiology, and designing degenerated primers, among others. There are efficient\nexact algorithms that have reached high-quality solutions for binary sequences.\nHowever, there is still room for improvement concerning the quality of\nsolutions over DNA and protein sequences. In this paper, we introduce a\nthree-stage algorithm that comprises the following process: first, we apply a\nnovel alphabet pruning method to reduce the search space for effectively\nfinding promising search regions. Second, a variant of beam search to find a\nheuristic solution is employed. This method utilizes a newly developed guiding\nfunction based on an expected distance heuristic score of partial solutions.\nLast, we introduce a local search to improve the quality of the solution\nobtained from the beam search. Furthermore, due to the lack of real-world\nbenchmarks, two real-world datasets are introduced to verify the robustness of\nthe method. The extensive experimental results show that the proposed method\noutperforms the previous approaches from the literature.",
      "tldr_zh": "本文提出了一种三阶段算法，用于解决 Closest String Problem（一个 NP-hard 问题），旨在在人工和真实基因序列（如 DNA 和蛋白质序列）上找到与给定字符串集距离最小的字符串。第一阶段采用新型 alphabet pruning 方法减少搜索空间；第二阶段使用 beam search 的变体，结合基于 expected distance heuristic score 的引导函数来获取启发式解决方案；第三阶段通过 local search 进一步优化结果。该算法引入两个真实世界数据集进行验证，实验结果显示其性能优于文献中现有方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.13023v1",
      "published_date": "2024-07-17 21:26:27 UTC",
      "updated_date": "2024-07-17 21:26:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:25:30.327607"
    },
    {
      "arxiv_id": "2407.13006v1",
      "title": "Sparsity-based Safety Conservatism for Constrained Offline Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Minjae Cho",
        "Chuangchuang Sun"
      ],
      "abstract": "Reinforcement Learning (RL) has made notable success in decision-making\nfields like autonomous driving and robotic manipulation. Yet, its reliance on\nreal-time feedback poses challenges in costly or hazardous settings.\nFurthermore, RL's training approach, centered on \"on-policy\" sampling, doesn't\nfully capitalize on data. Hence, Offline RL has emerged as a compelling\nalternative, particularly in conducting additional experiments is impractical,\nand abundant datasets are available. However, the challenge of distributional\nshift (extrapolation), indicating the disparity between data distributions and\nlearning policies, also poses a risk in offline RL, potentially leading to\nsignificant safety breaches due to estimation errors (interpolation). This\nconcern is particularly pronounced in safety-critical domains, where real-world\nproblems are prevalent. To address both extrapolation and interpolation errors,\nnumerous studies have introduced additional constraints to confine policy\nbehavior, steering it towards more cautious decision-making. While many studies\nhave addressed extrapolation errors, fewer have focused on providing effective\nsolutions for tackling interpolation errors. For example, some works tackle\nthis issue by incorporating potential cost-maximizing optimization by\nperturbing the original dataset. However, this, involving a bi-level\noptimization structure, may introduce significant instability or complicate\nproblem-solving in high-dimensional tasks. This motivates us to pinpoint areas\nwhere hazards may be more prevalent than initially estimated based on the\nsparsity of available data by providing significant insight into constrained\noffline RL. In this paper, we present conservative metrics based on data\nsparsity that demonstrate the high generalizability to any methods and efficacy\ncompared to using bi-level cost-ub-maximization.",
      "tldr_zh": "这项研究针对强化学习（Reinforcement Learning, RL）在高风险环境中面临的挑战，提出了一种基于数据稀疏性（sparsity）的安全保守方法，用于约束的离线强化学习（Offline RL）。传统 RL 依赖实时反馈和“on-policy”采样，而 Offline RL 虽能利用现有数据集，但易受分布偏移问题影响，特别是插值错误（interpolation errors），可能导致安全隐患。作者通过分析数据稀疏性，开发了保守度量（conservative metrics），这些度量无需复杂的双层优化（bi-level optimization）结构，就能有效减少插值错误。实验结果显示，该方法具有高泛化性和效率，显著优于现有解决方案，为安全关键领域的决策应用提供了更可靠的框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.13006v1",
      "published_date": "2024-07-17 20:57:05 UTC",
      "updated_date": "2024-07-17 20:57:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:25:42.558078"
    },
    {
      "arxiv_id": "2407.13000v1",
      "title": "Novel Deep Neural Network Classifier Characterization Metrics with Applications to Dataless Evaluation",
      "title_zh": "新颖的深度神经网络分类器表征指标及其在无数据评估中的应用",
      "authors": [
        "Nathaniel Dean",
        "Dilip Sarkar"
      ],
      "abstract": "The mainstream AI community has seen a rise in large-scale open-source\nclassifiers, often pre-trained on vast datasets and tested on standard\nbenchmarks; however, users facing diverse needs and limited, expensive test\ndata may be overwhelmed by available choices. Deep Neural Network (DNN)\nclassifiers undergo training, validation, and testing phases using example\ndataset, with the testing phase focused on determining the classification\naccuracy of test examples without delving into the inner working of the\nclassifier. In this work, we evaluate a DNN classifier's training quality\nwithout any example dataset. It is assumed that a DNN is a composition of a\nfeature extractor and a classifier which is the penultimate completely\nconnected layer. The quality of a classifier is estimated using its weight\nvectors. The feature extractor is characterized using two metrics that utilize\nfeature vectors it produces when synthetic data is fed as input. These\nsynthetic input vectors are produced by backpropagating desired outputs of the\nclassifier. Our empirical study of the proposed method for ResNet18, trained\nwith CAFIR10 and CAFIR100 datasets, confirms that data-less evaluation of DNN\nclassifiers is indeed possible.",
      "tldr_zh": "该论文提出了一种新型深度神经网络 (DNN) 分类器表征指标，用于在没有示例数据集的情况下评估分类器的训练质量，以解决用户在选择开源分类器时面临的测试数据不足问题。方法假设 DNN 由特征提取器和分类器（倒数第二层完全连接层）组成，通过分析分类器的权重向量和特征提取器的特征向量来评估性能，其中特征向量来源于反向传播分类器期望输出生成的合成数据。实验在 ResNet18 模型上使用 CIFAR10 和 CIFAR100 数据集进行验证，结果证实这种无数据评估方法是可行的，为高效的分类器选择提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.13000v1",
      "published_date": "2024-07-17 20:40:46 UTC",
      "updated_date": "2024-07-17 20:40:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:25:55.159694"
    },
    {
      "arxiv_id": "2407.15866v2",
      "title": "SmartQuant: CXL-based AI Model Store in Support of Runtime Configurable Weight Quantization",
      "title_zh": "SmartQuant",
      "authors": [
        "Rui Xie",
        "Asad Ul Haq",
        "Linsen Ma",
        "Krystal Sun",
        "Sanchari Sen",
        "Swagath Venkataramani",
        "Liu Liu",
        "Tong Zhang"
      ],
      "abstract": "Recent studies have revealed that, during the inference on generative AI\nmodels such as transformer, the importance of different weights exhibits\nsubstantial context-dependent variations. This naturally manifests a promising\npotential of adaptively configuring weight quantization to improve the\ngenerative AI inference efficiency. Although configurable weight quantization\ncan readily leverage the hardware support of variable-precision arithmetics in\nmodern GPU and AI accelerators, little prior research has studied how one could\nexploit variable weight quantization to proportionally improve the AI model\nmemory access speed and energy efficiency. Motivated by the rapidly maturing\nCXL ecosystem, this work develops a CXL-based design solution to fill this gap.\nThe key is to allow CXL memory controllers play an active role in supporting\nand exploiting runtime configurable weight quantization. Using transformer as a\nrepresentative generative AI model, we carried out experiments that well\ndemonstrate the effectiveness of the proposed design solution.",
      "tldr_zh": "该研究发现，在生成式 AI 模型如 Transformer 中，权重的重要性会随上下文变化，从而提出 SmartQuant，一种基于 CXL 的 AI 模型存储系统，支持 runtime configurable weight quantization，以优化推理效率。SmartQuant 利用 CXL 内存控制器主动参与权重量化配置，显著提升内存访问速度和能量效率。实验结果证明，该设计在 Transformer 模型上有效，展示了其在提高 AI 推理性能方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15866v2",
      "published_date": "2024-07-17 20:39:49 UTC",
      "updated_date": "2024-08-17 19:44:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:26:07.128672"
    },
    {
      "arxiv_id": "2407.15865v1",
      "title": "A Survey of AI-Powered Mini-Grid Solutions for a Sustainable Future in Rural Communities",
      "title_zh": "人工智能驱动的微电网解决方案综述：面向农村社区的可持续未来",
      "authors": [
        "Craig Pirie",
        "Harsha Kalutarage",
        "Muhammad Shadi Hajar",
        "Nirmalie Wiratunga",
        "Subodha Charles",
        "Geeth Sandaru Madhushan",
        "Priyantha Buddhika",
        "Supun Wijesiriwardana",
        "Akila Dimantha",
        "Kithdara Hansamal",
        "Shalitha Pathiranage"
      ],
      "abstract": "This paper presents a comprehensive survey of AI-driven mini-grid solutions\naimed at enhancing sustainable energy access. It emphasises the potential of\nmini-grids, which can operate independently or in conjunction with national\npower grids, to provide reliable and affordable electricity to remote\ncommunities. Given the inherent unpredictability of renewable energy sources\nsuch as solar and wind, the necessity for accurate energy forecasting and\nmanagement is discussed, highlighting the role of advanced AI techniques in\nforecasting energy supply and demand, optimising grid operations, and ensuring\nsustainable energy distribution. This paper reviews various forecasting models,\nincluding statistical methods, machine learning algorithms, and hybrid\napproaches, evaluating their effectiveness for both short-term and long-term\npredictions. Additionally, it explores public datasets and tools such as\nProphet, NeuralProphet, and N-BEATS for model implementation and validation.\nThe survey concludes with recommendations for future research, addressing\nchallenges in model adaptation and optimisation for real-world applications.",
      "tldr_zh": "这篇论文对AI驱动的微电网(mini-grids)解决方案进行了全面调查，旨在为农村社区提供可靠且负担得起的可持续能源。论文强调了AI技术在应对可再生能源（如太阳能和风能）的不确定性方面的重要性，包括能源预测、需求优化和电网操作管理，并回顾了统计方法、机器学习算法以及混合模型在短期和长期预测中的有效性。还探讨了公共数据集和工具如Prophet、NeuralProphet和N-BEATS，用于模型实施和验证，并提出未来研究推荐，以解决模型在实际应用中的适应和优化挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15865v1",
      "published_date": "2024-07-17 20:23:38 UTC",
      "updated_date": "2024-07-17 20:23:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:26:19.208679"
    },
    {
      "arxiv_id": "2407.12994v2",
      "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks",
      "title_zh": "针对不同自然语言处理任务的大语言模型提示工程方法综述",
      "authors": [
        "Shubham Vatsal",
        "Harsh Dubey"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable performance on many\ndifferent Natural Language Processing (NLP) tasks. Prompt engineering plays a\nkey role in adding more to the already existing abilities of LLMs to achieve\nsignificant performance gains on various NLP tasks. Prompt engineering requires\ncomposing natural language instructions called prompts to elicit knowledge from\nLLMs in a structured way. Unlike previous state-of-the-art (SoTA) models,\nprompt engineering does not require extensive parameter re-training or\nfine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently\nextract LLMs' knowledge through a basic natural language conversational\nexchange or prompt engineering, allowing more and more people even without deep\nmathematical machine learning background to experiment with LLMs. With prompt\nengineering gaining popularity in the last two years, researchers have come up\nwith numerous engineering techniques around designing prompts to improve\naccuracy of information extraction from the LLMs. In this paper, we summarize\ndifferent prompting techniques and club them together based on different NLP\ntasks that they have been used for. We further granularly highlight the\nperformance of these prompting strategies on various datasets belonging to that\nNLP task, talk about the corresponding LLMs used, present a taxonomy diagram\nand discuss the possible SoTA for specific datasets. In total, we read and\npresent a survey of 44 research papers which talk about 39 different prompting\nmethods on 29 different NLP tasks of which most of them have been published in\nthe last two years.",
      "tldr_zh": "这篇论文对大型语言模型（Large Language Models, LLMs）中提示工程（Prompt Engineering）的方法进行了全面调查，涵盖了应用于不同自然语言处理（NLP）任务的策略。论文基于44篇研究文献，总结了39种提示技术，这些方法通过设计自然语言提示来提取LLMs的嵌入知识，而无需重新训练模型，从而提升任务性能并使非专业人士也能参与。研究进一步分类了这些技术在29个NLP任务上的应用，包括性能比较、使用的LLMs以及特定数据集的状态-of-the-art（SoTA）结果，突显了提示工程在过去两年内的快速发展及其潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12994v2",
      "published_date": "2024-07-17 20:23:19 UTC",
      "updated_date": "2024-07-24 03:53:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:26:31.626044"
    },
    {
      "arxiv_id": "2407.12980v2",
      "title": "A Framework for testing Federated Learning algorithms using an edge-like environment",
      "title_zh": "翻译失败",
      "authors": [
        "Felipe Machado Schwanck",
        "Marcos Tomazzoli Leipnitz",
        "Joel Luís Carbonera",
        "Juliano Araujo Wickboldt"
      ],
      "abstract": "Federated Learning (FL) is a machine learning paradigm in which many clients\ncooperatively train a single centralized model while keeping their data private\nand decentralized. FL is commonly used in edge computing, which involves\nplacing computer workloads (both hardware and software) as close as possible to\nthe edge, where the data is being created and where actions are occurring,\nenabling faster response times, greater data privacy, and reduced data transfer\ncosts. However, due to the heterogeneous data distributions/contents of\nclients, it is non-trivial to accurately evaluate the contributions of local\nmodels in global centralized model aggregation. This is an example of a major\nchallenge in FL, commonly known as data imbalance or class imbalance. In\ngeneral, testing and assessing FL algorithms can be a very difficult and\ncomplex task due to the distributed nature of the systems. In this work, a\nframework is proposed and implemented to assess FL algorithms in a more easy\nand scalable way. This framework is evaluated over a distributed edge-like\nenvironment managed by a container orchestration platform (i.e. Kubernetes).",
      "tldr_zh": "该论文提出一个框架，用于测试和评估Federated Learning (FL)算法，旨在解决FL在分布式环境中因客户端数据异质性和不平衡问题而带来的评估难题。框架通过模拟边缘计算(edge computing)环境，使FL算法的测试变得更易扩展和高效。实验在由Kubernetes管理的分布式边缘-like环境中进行评估，展示了框架在处理数据隐私和模型聚合方面的高效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.NI",
        "C.2.4; I.2.11"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12980v2",
      "published_date": "2024-07-17 19:52:53 UTC",
      "updated_date": "2024-12-12 21:05:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:26:42.186212"
    },
    {
      "arxiv_id": "2407.12973v1",
      "title": "Temporal Label Hierachical Network for Compound Emotion Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Sunan Li",
        "Hailun Lian",
        "Cheng Lu",
        "Yan Zhao",
        "Tianhua Qi",
        "Hao Yang",
        "Yuan Zong",
        "Wenming Zheng"
      ],
      "abstract": "The emotion recognition has attracted more attention in recent decades.\nAlthough significant progress has been made in the recognition technology of\nthe seven basic emotions, existing methods are still hard to tackle compound\nemotion recognition that occurred commonly in practical application. This\narticle introduces our achievements in the 7th Field Emotion Behavior Analysis\n(ABAW) competition. In the competition, we selected pre trained ResNet18 and\nTransformer, which have been widely validated, as the basic network framework.\nConsidering the continuity of emotions over time, we propose a time pyramid\nstructure network for frame level emotion prediction. Furthermore. At the same\ntime, in order to address the lack of data in composite emotion recognition, we\nutilized fine-grained labels from the DFEW database to construct training data\nfor emotion categories in competitions. Taking into account the characteristics\nof valence arousal of various complex emotions, we constructed a classification\nframework from coarse to fine in the label space.",
      "tldr_zh": "这篇论文针对复合情感识别的挑战，提出了一种Temporal Label Hierarchical Network，以处理现有方法在七种基本情绪之外的复杂场景。作者使用预训练的ResNet18和Transformer作为基础网络框架，并设计了时间金字塔结构网络（time pyramid structure network），来捕捉情感在时间上的连续性。针对数据不足问题，他们利用DFEW数据库的细粒度标签构建训练数据，并构建了从粗到细的标签空间分类框架，考虑了各种复杂情感的valence arousal特征。在7th ABAW比赛中，该方法展示了显著的改进，增强了复合情感识别的准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "draft for abaw7",
      "pdf_url": "http://arxiv.org/pdf/2407.12973v1",
      "published_date": "2024-07-17 19:38:44 UTC",
      "updated_date": "2024-07-17 19:38:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:26:57.181939"
    },
    {
      "arxiv_id": "2407.12964v1",
      "title": "Learning Long-Horizon Predictions for Quadrotor Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Pratyaksh Prabhav Rao",
        "Alessandro Saviolo",
        "Tommaso Castiglione Ferrari",
        "Giuseppe Loianno"
      ],
      "abstract": "Accurate modeling of system dynamics is crucial for achieving\nhigh-performance planning and control of robotic systems. Although existing\ndata-driven approaches represent a promising approach for modeling dynamics,\ntheir accuracy is limited to a short prediction horizon, overlooking the impact\nof compounding prediction errors over longer prediction horizons. Strategies to\nmitigate these cumulative errors remain underexplored. To bridge this gap, in\nthis paper, we study the key design choices for efficiently learning\nlong-horizon prediction dynamics for quadrotors. Specifically, we analyze the\nimpact of multiple architectures, historical data, and multi-step loss\nformulation. We show that sequential modeling techniques showcase their\nadvantage in minimizing compounding errors compared to other types of\nsolutions. Furthermore, we propose a novel decoupled dynamics learning\napproach, which further simplifies the learning process while also enhancing\nthe approach modularity. Extensive experiments and ablation studies on\nreal-world quadrotor data demonstrate the versatility and precision of the\nproposed approach. Our outcomes offer several insights and methodologies for\nenhancing long-term predictive accuracy of learned quadrotor dynamics for\nplanning and control.",
      "tldr_zh": "本论文探讨了学习四旋翼无人机(Quadrotor Dynamics)长时预测(Long-Horizon Predictions)的关键设计选择，以解决现有数据驱动方法在长时预测中累积错误的局限性。通过分析多种架构、历史数据和多步损失(multi-step loss)函数，该研究证明了顺序建模(sequential modeling)技术在最小化累积错误方面的优势，并提出了一种新型解耦动态学习(decoupled dynamics learning)方法，以简化学习过程并提升系统模块性。实验结果显示，该方法在真实世界四旋翼数据上表现出色，提高了预测准确性，并为无人机规划和控制提供了重要的见解和方法。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 5 figures, 3 tables. Paper accepted by IROS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12964v1",
      "published_date": "2024-07-17 19:06:47 UTC",
      "updated_date": "2024-07-17 19:06:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:27:09.631250"
    },
    {
      "arxiv_id": "2407.13803v1",
      "title": "Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality",
      "title_zh": "翻译失败",
      "authors": [
        "Duy C. Hoang",
        "Hung T. Q. Le",
        "Rui Chu",
        "Ping Li",
        "Weijie Zhao",
        "Yingjie Lao",
        "Khoa D. Doan"
      ],
      "abstract": "With the widespread adoption of Large Language Models (LLMs), concerns about\npotential misuse have emerged. To this end, watermarking has been adapted to\nLLM, enabling a simple and effective way to detect and monitor generated text.\nHowever, while the existing methods can differentiate between watermarked and\nunwatermarked text with high accuracy, they often face a trade-off between the\nquality of the generated text and the effectiveness of the watermarking\nprocess. In this work, we present a novel type of LLM watermark, Sparse\nWatermark, which aims to mitigate this trade-off by applying watermarks to a\nsmall subset of generated tokens distributed across the text. The key strategy\ninvolves anchoring watermarked tokens to words that have specific\nPart-of-Speech (POS) tags. Our experimental results demonstrate that the\nproposed watermarking scheme achieves high detectability while generating text\nthat outperforms previous LLM watermarking methods in quality across various\ntasks",
      "tldr_zh": "该研究针对大型语言模型(LLMs)的潜在滥用风险，提出了一种新型水印技术Sparse Watermark，以缓解现有水印方法在文本质量和检测有效性之间的权衡问题。关键策略是将水印应用到文本中少量的分布式tokens，并将这些tokens锚定到特定Part-of-Speech (POS)标签的单词，从而保持生成文本的高质量。实验结果显示，该方法在检测准确性上表现出色，并在各种任务中生成的文本质量优于之前的LLM水印方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.13803v1",
      "published_date": "2024-07-17 18:52:12 UTC",
      "updated_date": "2024-07-17 18:52:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:27:18.730715"
    },
    {
      "arxiv_id": "2407.12950v2",
      "title": "Beyond the Veil of Similarity: Quantifying Semantic Continuity in Explainable AI",
      "title_zh": "超越相似性的面纱：在可解释AI中量化语义连续性",
      "authors": [
        "Qi Huang",
        "Emanuele Mezzi",
        "Osman Mutlu",
        "Miltiadis Kofinas",
        "Vidya Prasad",
        "Shadnan Azwad Khan",
        "Elena Ranguelova",
        "Niki van Stein"
      ],
      "abstract": "We introduce a novel metric for measuring semantic continuity in Explainable\nAI methods and machine learning models. We posit that for models to be truly\ninterpretable and trustworthy, similar inputs should yield similar\nexplanations, reflecting a consistent semantic understanding. By leveraging XAI\ntechniques, we assess semantic continuity in the task of image recognition. We\nconduct experiments to observe how incremental changes in input affect the\nexplanations provided by different XAI methods. Through this approach, we aim\nto evaluate the models' capability to generalize and abstract semantic concepts\naccurately and to evaluate different XAI methods in correctly capturing the\nmodel behaviour. This paper contributes to the broader discourse on AI\ninterpretability by proposing a quantitative measure for semantic continuity\nfor XAI methods, offering insights into the models' and explainers' internal\nreasoning processes, and promoting more reliable and transparent AI systems.",
      "tldr_zh": "本研究提出一个新指标，用于量化 Explainable AI (XAI) 方法中的语义连续性，确保类似输入产生类似解释，从而提升模型的可解释性和可信赖性。研究方法通过 XAI 技术在图像识别任务中进行实验，观察输入的渐进变化对不同 XAI 方法解释的影响，以评估模型的泛化和抽象语义概念的能力。最终，该指标为评估 XAI 方法的可靠性提供了量化工具，并为开发更透明和可靠的 AI 系统提供了重要洞见。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, accepted at the world conference of explainable AI, 2024,\n  Malta",
      "pdf_url": "http://arxiv.org/pdf/2407.12950v2",
      "published_date": "2024-07-17 18:32:41 UTC",
      "updated_date": "2025-01-30 08:38:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:27:31.847741"
    },
    {
      "arxiv_id": "2407.12943v1",
      "title": "Halu-J: Critique-Based Hallucination Judge",
      "title_zh": "翻译失败",
      "authors": [
        "Binjie Wang",
        "Steffi Chern",
        "Ethan Chern",
        "Pengfei Liu"
      ],
      "abstract": "Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .",
      "tldr_zh": "本研究针对大型语言模型（LLMs）产生的 hallucinations（非事实内容）问题，指出现有 retrieval-augmented 方法存在解释不足、证据相关性低和多证据处理不佳的局限性。论文提出 Halu-J，一个基于 critique-based 的 7 亿参数幻觉判断器，能够通过选择相关证据并生成详细批评来提升检测准确性。实验结果显示，Halu-J 在多证据 hallucination 检测中优于 GPT-4o，并在批评生成和证据选择上与之相当。同时，研究引入了新的数据集 ME-FEVER，以支持多证据幻觉检测任务。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12943v1",
      "published_date": "2024-07-17 18:21:01 UTC",
      "updated_date": "2024-07-17 18:21:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:27:44.180915"
    },
    {
      "arxiv_id": "2407.12929v2",
      "title": "The 2024 Foundation Model Transparency Index",
      "title_zh": "2024 基础模型透明度指数",
      "authors": [
        "Rishi Bommasani",
        "Kevin Klyman",
        "Sayash Kapoor",
        "Shayne Longpre",
        "Betty Xiong",
        "Nestor Maslej",
        "Percy Liang"
      ],
      "abstract": "Foundation models are increasingly consequential yet extremely opaque. To\ncharacterize the status quo, the Foundation Model Transparency Index (FMTI) was\nlaunched in October 2023 to measure the transparency of leading foundation\nmodel developers. FMTI 2023 assessed 10 major foundation model developers (e.g.\nOpenAI, Google) on 100 transparency indicators (e.g. does the developer\ndisclose the wages it pays for data labor?). At the time, developers publicly\ndisclosed very limited information with the average score being 37 out of 100.\nTo understand how the status quo has changed, we conduct a follow-up study\nafter 6 months: we score 14 developers against the same 100 indicators. While\nin FMTI 2023 we searched for publicly available information, in FMTI 2024\ndevelopers submit reports on the 100 transparency indicators, potentially\nincluding information that was not previously public. We find that developers\nnow score 58 out of 100 on average, a 21 point improvement over FMTI 2023. Much\nof this increase is driven by developers disclosing information during the FMTI\n2024 process: on average, developers disclosed information related to 16.6\nindicators that was not previously public. We observe regions of sustained\n(i.e. across 2023 and 2024) and systemic (i.e. across most or all developers)\nopacity such as on copyright status, data access, data labor, and downstream\nimpact. We publish transparency reports for each developer that consolidate\ninformation disclosures: these reports are based on the information disclosed\nto us via developers. Our findings demonstrate that transparency can be\nimproved in this nascent ecosystem, the Foundation Model Transparency Index\nlikely contributes to these improvements, and policymakers should consider\ninterventions in areas where transparency has not improved.",
      "tldr_zh": "该研究更新了 Foundation Model Transparency Index (FMTI) 2024，评估了14家基础模型开发商（如OpenAI和Google）的透明度，使用相同的100个指标。相比FMTI 2023的平均得分37/100，2024年的平均得分提升至58/100，主要得益于开发商主动披露新信息（平均16.6个指标）。尽管透明度有所改善，但仍存在持续性系统性不透明问题，如版权状态、数据访问、数据劳动和下游影响，研究建议政策制定者针对这些领域进行干预。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in TMLR 2025. Project page: https://crfm.stanford.edu/fmti",
      "pdf_url": "http://arxiv.org/pdf/2407.12929v2",
      "published_date": "2024-07-17 18:03:37 UTC",
      "updated_date": "2025-03-04 21:07:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:27:57.154902"
    },
    {
      "arxiv_id": "2407.12899v2",
      "title": "DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Huiguo He",
        "Huan Yang",
        "Zixi Tuo",
        "Yuan Zhou",
        "Qiuyue Wang",
        "Yuhang Zhang",
        "Zeyu Liu",
        "Wenhao Huang",
        "Hongyang Chao",
        "Jian Yin"
      ],
      "abstract": "Story visualization aims to create visually compelling images or videos\ncorresponding to textual narratives. Despite recent advances in diffusion\nmodels yielding promising results, existing methods still struggle to create a\ncoherent sequence of subject-consistent frames based solely on a story. To this\nend, we propose DreamStory, an automatic open-domain story visualization\nframework by leveraging the LLMs and a novel multi-subject consistent diffusion\nmodel. DreamStory consists of (1) an LLM acting as a story director and (2) an\ninnovative Multi-Subject consistent Diffusion model (MSD) for generating\nconsistent multi-subject across the images. First, DreamStory employs the LLM\nto generate descriptive prompts for subjects and scenes aligned with the story,\nannotating each scene's subjects for subsequent subject-consistent generation.\nSecond, DreamStory utilizes these detailed subject descriptions to create\nportraits of the subjects, with these portraits and their corresponding textual\ninformation serving as multimodal anchors (guidance). Finally, the MSD uses\nthese multimodal anchors to generate story scenes with consistent\nmulti-subject. Specifically, the MSD includes Masked Mutual Self-Attention\n(MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA and MMCA modules\nensure appearance and semantic consistency with reference images and text,\nrespectively. Both modules employ masking mechanisms to prevent subject\nblending. To validate our approach and promote progress in story visualization,\nwe established a benchmark, DS-500, which can assess the overall performance of\nthe story visualization framework, subject-identification accuracy, and the\nconsistency of the generation model. Extensive experiments validate the\neffectiveness of DreamStory in both subjective and objective evaluations.\nPlease visit our project homepage at https://dream-xyz.github.io/dreamstory.",
      "tldr_zh": "本研究提出DreamStory框架，用于开放域故事可视化，通过LLM引导的多主题一致扩散模型（MSD）生成连贯的图像序列，以解决现有方法在主题一致性方面的挑战。框架包括LLM作为故事导演，生成与故事对齐的主题和场景描述，并创建多模态锚点（如主题肖像）；MSD则通过Masked Mutual Self-Attention (MMSA)和Masked Mutual Cross-Attention (MMCA)模块，确保生成图像的外观和语义一致性，同时使用掩码机制防止主题混合。为验证效果，研究建立了DS-500基准，并在主观和客观评估中证明DreamStory比基线方法表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12899v2",
      "published_date": "2024-07-17 17:54:12 UTC",
      "updated_date": "2025-03-09 13:33:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:28:09.578926"
    },
    {
      "arxiv_id": "2407.12773v1",
      "title": "OMG-Net: A Deep Learning Framework Deploying Segment Anything to Detect Pan-Cancer Mitotic Figures from Haematoxylin and Eosin-Stained Slides",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoyan Shen",
        "Mikael Simard",
        "Douglas Brand",
        "Vanghelita Andrei",
        "Ali Al-Khader",
        "Fatine Oumlil",
        "Katherine Trevers",
        "Thomas Butters",
        "Simon Haefliger",
        "Eleanna Kara",
        "Fernanda Amary",
        "Roberto Tirabosco",
        "Paul Cool",
        "Gary Royle",
        "Maria A. Hawkins",
        "Adrienne M. Flanagan",
        "Charles-Antoine Collins Fekete"
      ],
      "abstract": "Mitotic activity is an important feature for grading several cancer types.\nCounting mitotic figures (MFs) is a time-consuming, laborious task prone to\ninter-observer variation. Inaccurate recognition of MFs can lead to incorrect\ngrading and hence potential suboptimal treatment. In this study, we propose an\nartificial intelligence (AI)-aided approach to detect MFs in digitised\nhaematoxylin and eosin-stained whole slide images (WSIs). Advances in this area\nare hampered by the limited number and types of cancer datasets of MFs. Here we\nestablish the largest pan-cancer dataset of mitotic figures by combining an\nin-house dataset of soft tissue tumours (STMF) with five open-source mitotic\ndatasets comprising multiple human cancers and canine specimens (ICPR, TUPAC,\nCCMCT, CMC and MIDOG++). This new dataset identifies 74,620 MFs and 105,538\nmitotic-like figures. We then employed a two-stage framework (the Optimised\nMitoses Generator Network (OMG-Net) to classify MFs. The framework first\ndeploys the Segment Anything Model (SAM) to automate the contouring of MFs and\nsurrounding objects. An adapted ResNet18 is subsequently trained to classify\nMFs. OMG-Net reaches an F1-score of 0.84 on pan-cancer MF detection (breast\ncarcinoma, neuroendocrine tumour and melanoma), largely outperforming the\nprevious state-of-the-art MIDOG++ benchmark model on its hold-out testing set\n(e.g. +16% F1-score on breast cancer detection, p<0.001) thereby providing\nsuperior accuracy in detecting MFs on various types of tumours obtained with\ndifferent scanners.",
      "tldr_zh": "本研究针对有丝分裂图形（MFs）计数在癌症分级中的挑战，提出了一种深度学习框架OMG-Net，用于从苏木紫-伊红染色全滑图像（WSIs）中检测泛癌症MFs。该框架采用两阶段方法：首先利用Segment Anything Model (SAM)自动轮廓MFs及其周围物体，然后通过适应后的ResNet18模型进行分类分类。研究者构建了最大的泛癌症数据集，包含74,620个MFs和105,538个类似图形，涵盖多种癌症类型。实验结果显示，OMG-Net在泛癌症MFs检测上达到F1-score 0.84，大幅超过了现有基准模型（如MIDOG++），例如在乳腺癌检测中提升16%的F1-score（p<0.001），为AI辅助癌症诊断提供了更准确的工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12773v1",
      "published_date": "2024-07-17 17:53:37 UTC",
      "updated_date": "2024-07-17 17:53:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:28:20.744855"
    },
    {
      "arxiv_id": "2407.12753v1",
      "title": "LookupViT: Compressing visual information to a limited number of tokens",
      "title_zh": "LookupViT：将视觉信息压缩到有限数量的标记",
      "authors": [
        "Rajat Koner",
        "Gagan Jain",
        "Prateek Jain",
        "Volker Tresp",
        "Sujoy Paul"
      ],
      "abstract": "Vision Transformers (ViT) have emerged as the de-facto choice for numerous\nindustry grade vision solutions. But their inference cost can be prohibitive\nfor many settings, as they compute self-attention in each layer which suffers\nfrom quadratic computational complexity in the number of tokens. On the other\nhand, spatial information in images and spatio-temporal information in videos\nis usually sparse and redundant. In this work, we introduce LookupViT, that\naims to exploit this information sparsity to reduce ViT inference cost.\nLookupViT provides a novel general purpose vision transformer block that\noperates by compressing information from higher resolution tokens to a fixed\nnumber of tokens. These few compressed tokens undergo meticulous processing,\nwhile the higher-resolution tokens are passed through computationally cheaper\nlayers. Information sharing between these two token sets is enabled through a\nbidirectional cross-attention mechanism. The approach offers multiple\nadvantages - (a) easy to implement on standard ML accelerators (GPUs/TPUs) via\nstandard high-level operators, (b) applicable to standard ViT and its variants,\nthus generalizes to various tasks, (c) can handle different tokenization and\nattention approaches. LookupViT also offers flexibility for the compressed\ntokens, enabling performance-computation trade-offs in a single trained model.\nWe show LookupViT's effectiveness on multiple domains - (a) for\nimage-classification (ImageNet-1K and ImageNet-21K), (b) video classification\n(Kinetics400 and Something-Something V2), (c) image captioning (COCO-Captions)\nwith a frozen encoder. LookupViT provides $2\\times$ reduction in FLOPs while\nupholding or improving accuracy across these domains. In addition, LookupViT\nalso demonstrates out-of-the-box robustness and generalization on image\nclassification (ImageNet-C,R,A,O), improving by up to $4\\%$ over ViT.",
      "tldr_zh": "本研究提出LookupViT，一种针对Vision Transformers (ViT)的高计算成本问题（如自注意力机制的二次复杂度）的优化框架，通过将视觉信息压缩到固定数量的tokens来减少推理开销。LookupViT引入一个新型ViT块，将高分辨率tokens压缩后进行精细处理，同时让其他tokens通过更高效的层，并通过双向交叉注意力机制实现信息共享，从而适用于图像分类、视频分类等多种任务。实验结果显示，LookupViT在ImageNet-1K等数据集上实现了2倍FLOPs减少，同时维持或提升准确率，并在ImageNet-C等鲁棒性测试中比ViT提高高达4%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12753v1",
      "published_date": "2024-07-17 17:22:43 UTC",
      "updated_date": "2024-07-17 17:22:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:28:35.434796"
    },
    {
      "arxiv_id": "2407.12736v3",
      "title": "CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Erfan Sadeghi",
        "Arash Fayyazi",
        "Suhas Somashekar",
        "Massoud Pedram"
      ],
      "abstract": "Vision Transformers (ViTs) represent a groundbreaking shift in machine\nlearning approaches to computer vision. Unlike traditional approaches, ViTs\nemploy the self-attention mechanism, which has been widely used in natural\nlanguage processing, to analyze image patches. Despite their advantages in\nmodeling visual tasks, deploying ViTs on hardware platforms, notably\nField-Programmable Gate Arrays (FPGAs), introduces considerable challenges.\nThese challenges stem primarily from the non-linear calculations and high\ncomputational and memory demands of ViTs. This paper introduces CHOSEN, a\nsoftware-hardware co-design framework to address these challenges and offer an\nautomated framework for ViT deployment on the FPGAs in order to maximize\nperformance. Our framework is built upon three fundamental contributions:\nmulti-kernel design to maximize the bandwidth, mainly targeting benefits of\nmulti DDR memory banks, approximate non-linear functions that exhibit minimal\naccuracy degradation, and efficient use of available logic blocks on the FPGA,\nand efficient compiler to maximize the performance and memory-efficiency of the\ncomputing kernels by presenting a novel algorithm for design space exploration\nto find optimal hardware configuration that achieves optimal throughput and\nlatency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a\n1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.",
      "tldr_zh": "本论文提出 CHOSEN 框架，这是一个软件-硬件协同设计栈，旨在优化 Vision Transformers (ViTs) 在 Field-Programmable Gate Arrays (FPGAs) 上的推理效率，以应对 ViTs 的非线性计算和高计算内存需求。框架的核心贡献包括多内核设计以最大化带宽利用多 DDR 内存银行、近似非线性函数以最小化准确性损失并高效使用 FPGA 逻辑块，以及一个高效编译器通过新颖的设计空间探索算法优化硬件配置以提升吞吐量和延迟。实验结果显示，CHOSEN 在 DeiT-S 和 DeiT-B 模型上分别实现了 1.5x 和 1.42x 的吞吐量改进，显著提升了 ViTs 的硬件部署性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12736v3",
      "published_date": "2024-07-17 16:56:06 UTC",
      "updated_date": "2024-07-25 00:00:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:28:45.854272"
    },
    {
      "arxiv_id": "2407.12730v3",
      "title": "RoDE: Linear Rectified Mixture of Diverse Experts for Food Large Multi-Modal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Pengkun Jiao",
        "Xinlan Wu",
        "Bin Zhu",
        "Jingjing Chen",
        "Chong-Wah Ngo",
        "Yugang Jiang"
      ],
      "abstract": "Large Multi-modal Models (LMMs) have significantly advanced a variety of\nvision-language tasks. The scalability and availability of high-quality\ntraining data play a pivotal role in the success of LMMs. In the realm of food,\nwhile comprehensive food datasets such as Recipe1M offer an abundance of\ningredient and recipe information, they often fall short of providing ample\ndata for nutritional analysis. The Recipe1M+ dataset, despite offering a subset\nfor nutritional evaluation, is limited in the scale and accuracy of nutrition\ninformation. To bridge this gap, we introduce Uni-Food, a unified food dataset\nthat comprises over 100,000 images with various food labels, including\ncategories, ingredients, recipes, and ingredient-level nutritional information.\nUni-Food is designed to provide a more holistic approach to food data analysis,\nthereby enhancing the performance and capabilities of LMMs in this domain. To\nmitigate the conflicts arising from multi-task supervision during fine-tuning\nof LMMs, we introduce a novel Linear Rectification Mixture of Diverse Experts\n(RoDE) approach. RoDE utilizes a diverse array of experts to address tasks of\nvarying complexity, thereby facilitating the coordination of trainable\nparameters, i.e., it allocates more parameters for more complex tasks and,\nconversely, fewer parameters for simpler tasks. RoDE implements linear\nrectification union to refine the router's functionality, thereby enhancing the\nefficiency of sparse task allocation. These design choices endow RoDE with\nfeatures that ensure GPU memory efficiency and ease of optimization. Our\nexperimental results validate the effectiveness of our proposed approach in\naddressing the inherent challenges of food-related multitasking.",
      "tldr_zh": "本论文针对 Large Multi-Modal Models (LMMs) 在食物领域的数据不足问题，引入了 Uni-Food 数据集，该数据集包含超过10万张图像，提供全面的类别、成分、食谱和成分级营养信息，以支持更全面的食物数据分析。针对多任务监督中的冲突，提出了 Linear Rectified Mixture of Diverse Experts (RoDE) 方法，该方法利用多样化专家根据任务复杂度动态分配参数（如复杂任务分配更多参数），并通过线性整流联合优化路由器，实现高效的稀疏任务分配和 GPU 内存节约。实验结果验证，RoDE 显著提升了 LMMs 在食物相关多任务中的性能，解决了现有数据集的局限性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12730v3",
      "published_date": "2024-07-17 16:49:34 UTC",
      "updated_date": "2024-12-16 06:16:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:28:58.672122"
    },
    {
      "arxiv_id": "2407.12724v1",
      "title": "An Evaluation of Continual Learning for Advanced Node Semiconductor Defect Inspection",
      "title_zh": "先进节点半导体缺陷检测的持续学习评估",
      "authors": [
        "Amit Prasad",
        "Bappaditya Dey",
        "Victor Blanco",
        "Sandip Halder"
      ],
      "abstract": "Deep learning-based semiconductor defect inspection has gained traction in\nrecent years, offering a powerful and versatile approach that provides high\naccuracy, adaptability, and efficiency in detecting and classifying nano-scale\ndefects. However, semiconductor manufacturing processes are continually\nevolving, leading to the emergence of new types of defects over time. This\npresents a significant challenge for conventional supervised defect detectors,\nas they may suffer from catastrophic forgetting when trained on new defect\ndatasets, potentially compromising performance on previously learned tasks. An\nalternative approach involves the constant storage of previously trained\ndatasets alongside pre-trained model versions, which can be utilized for\n(re-)training from scratch or fine-tuning whenever encountering a new defect\ndataset. However, adhering to such a storage template is impractical in terms\nof size, particularly when considering High-Volume Manufacturing (HVM).\nAdditionally, semiconductor defect datasets, especially those encompassing\nstochastic defects, are often limited and expensive to obtain, thus lacking\nsufficient representation of the entire universal set of defectivity. This work\nintroduces a task-agnostic, meta-learning approach aimed at addressing this\nchallenge, which enables the incremental addition of new defect classes and\nscales to create a more robust and generalized model for semiconductor defect\ninspection. We have benchmarked our approach using real resist-wafer SEM\n(Scanning Electron Microscopy) datasets for two process steps, ADI and AEI,\ndemonstrating its superior performance compared to conventional supervised\ntraining methods.",
      "tldr_zh": "本研究评估了持续学习(continual learning)在先进节点半导体缺陷检测中的应用，针对传统监督模型在面对新缺陷类型时可能出现的灾难性遗忘(catastrophic forgetting)问题，以及数据集存储和获取的实际挑战。作者提出了一种任务无关的元学习(meta-learning)方法，支持增量添加新缺陷类，从而构建更鲁棒和泛化能力强的检测模型。实验结果显示，该方法在真实 resist-wafer SEM (Scanning Electron Microscopy) 数据集上的 ADI 和 AEI 过程步骤benchmark中，表现优于传统监督训练方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for presentation at the European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases 2024\n  Industry Track",
      "pdf_url": "http://arxiv.org/pdf/2407.12724v1",
      "published_date": "2024-07-17 16:41:22 UTC",
      "updated_date": "2024-07-17 16:41:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:29:09.396773"
    },
    {
      "arxiv_id": "2407.12710v1",
      "title": "A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad-Amin Charusaie",
        "Samira Samadi"
      ],
      "abstract": "Learn-to-Defer is a paradigm that enables learning algorithms to work not in\nisolation but as a team with human experts. In this paradigm, we permit the\nsystem to defer a subset of its tasks to the expert. Although there are\ncurrently systems that follow this paradigm and are designed to optimize the\naccuracy of the final human-AI team, the general methodology for developing\nsuch systems under a set of constraints (e.g., algorithmic fairness, expert\nintervention budget, defer of anomaly, etc.) remains largely unexplored. In\nthis paper, using a $d$-dimensional generalization to the fundamental lemma of\nNeyman and Pearson (d-GNP), we obtain the Bayes optimal solution for\nlearn-to-defer systems under various constraints. Furthermore, we design a\ngeneralizable algorithm to estimate that solution and apply this algorithm to\nthe COMPAS and ACSIncome datasets. Our algorithm shows improvements in terms of\nconstraint violation over a set of baselines.",
      "tldr_zh": "本论文提出一个统一的后续处理框架，用于多目标 Learn-to-Defer 问题，该范式允许 AI 系统将部分任务递交给人类专家，以优化整体团队准确性，同时满足各种约束（如算法公平性、专家干预预算等）。通过使用 d-维 Neyman-Pearson 基本引理 (d-GNP)，作者获得了 Bayes optimal 解决方案，并设计了一个通用算法来估计该解。实验在 COMPAS 和 ACSIncome 数据集上显示，该算法在约束违反方面比基线模型有显著改进，为受约束的人机协作系统提供了新方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12710v1",
      "published_date": "2024-07-17 16:32:30 UTC",
      "updated_date": "2024-07-17 16:32:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:29:21.172398"
    },
    {
      "arxiv_id": "2407.12702v2",
      "title": "TransCAD: A Hierarchical Transformer for CAD Sequence Inference from Point Clouds",
      "title_zh": "翻译失败",
      "authors": [
        "Elona Dupont",
        "Kseniya Cherenkova",
        "Dimitrios Mallis",
        "Gleb Gusev",
        "Anis Kacem",
        "Djamila Aouada"
      ],
      "abstract": "3D reverse engineering, in which a CAD model is inferred given a 3D scan of a\nphysical object, is a research direction that offers many promising practical\napplications. This paper proposes TransCAD, an end-to-end transformer-based\narchitecture that predicts the CAD sequence from a point cloud. TransCAD\nleverages the structure of CAD sequences by using a hierarchical learning\nstrategy. A loop refiner is also introduced to regress sketch primitive\nparameters. Rigorous experimentation on the DeepCAD and Fusion360 datasets show\nthat TransCAD achieves state-of-the-art results. The result analysis is\nsupported with a proposed metric for CAD sequence, the mean Average Precision\nof CAD Sequence, that addresses the limitations of existing metrics.",
      "tldr_zh": "该论文提出 TransCAD，一种基于 Transformer 的端到端层次化架构，用于从点云数据推断 CAD 序列，从而支持 3D 逆向工程应用。TransCAD 通过 hierarchical learning strategy 利用 CAD 序列的结构，并引入 loop refiner 来精确回归 sketch primitive parameters。实验在 DeepCAD 和 Fusion360 数据集上显示，该方法达到了最先进的结果，并提出 mean Average Precision of CAD Sequence 指标，以解决现有评估指标的局限性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12702v2",
      "published_date": "2024-07-17 16:24:36 UTC",
      "updated_date": "2024-07-18 10:27:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:29:32.730876"
    },
    {
      "arxiv_id": "2408.00790v1",
      "title": "Improving Air Mobility for Pre-Disaster Planning with Neural Network Accelerated Genetic Algorithm",
      "title_zh": "利用神经网络加速遗传算法改进灾难",
      "authors": [
        "Kamal Acharya",
        "Alvaro Velasquez",
        "Yongxin Liu",
        "Dahai Liu",
        "Liang Sun",
        "Houbing Song"
      ],
      "abstract": "Weather disaster related emergency operations pose a great challenge to air\nmobility in both aircraft and airport operations, especially when the impact is\ngradually approaching. We propose an optimized framework for adjusting airport\noperational schedules for such pre-disaster scenarios. We first, aggregate\noperational data from multiple airports and then determine the optimal count of\nevacuation flights to maximize the impacted airport's outgoing capacity without\nimpeding regular air traffic. We then propose a novel Neural Network (NN)\naccelerated Genetic Algorithm(GA) for evacuation planning. Our experiments show\nthat integration yielded comparable results but with smaller computational\noverhead. We find that the utilization of a NN enhances the efficiency of a GA,\nfacilitating more rapid convergence even when operating with a reduced\npopulation size. This effectiveness persists even when the model is trained on\ndata from airports different from those under test.",
      "tldr_zh": "本研究针对天气灾害前夕的航空机动性挑战，提出一个优化框架，用于调整机场运营时间表，以最大化受影响机场的出港容量，同时维持常规航班顺畅。框架首先聚合多个机场的运营数据，并确定最佳疏散航班数量；随后引入一种新型Neural Network (NN)加速Genetic Algorithm (GA)，用于高效的疏散规划。实验结果显示，该方法与基准模型相比，实现了相似的性能但计算开销更低，且NN的运用显著提升了GA的收敛速度，即使在减少种群大小或使用不同机场数据训练时也保持有效。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "7 pages, 8 figures, ITSC 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00790v1",
      "published_date": "2024-07-17 15:59:41 UTC",
      "updated_date": "2024-07-17 15:59:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:29:45.023835"
    },
    {
      "arxiv_id": "2407.12671v1",
      "title": "GraphMuse: A Library for Symbolic Music Graph Processing",
      "title_zh": "翻译失败",
      "authors": [
        "Emmanouil Karystinaios",
        "Gerhard Widmer"
      ],
      "abstract": "Graph Neural Networks (GNNs) have recently gained traction in symbolic music\ntasks, yet a lack of a unified framework impedes progress. Addressing this gap,\nwe present GraphMuse, a graph processing framework and library that facilitates\nefficient music graph processing and GNN training for symbolic music tasks.\nCentral to our contribution is a new neighbor sampling technique specifically\ntargeted toward meaningful behavior in musical scores. Additionally, GraphMuse\nintegrates hierarchical modeling elements that augment the expressivity and\ncapabilities of graph networks for musical tasks. Experiments with two specific\nmusical prediction tasks -- pitch spelling and cadence detection -- demonstrate\nsignificant performance improvement over previous methods. Our hope is that\nGraphMuse will lead to a boost in, and standardization of, symbolic music\nprocessing based on graph representations. The library is available at\nhttps://github.com/manoskary/graphmuse",
      "tldr_zh": "本文介绍了 GraphMuse，一个用于符号音乐图处理的框架，旨在解决 Graph Neural Networks (GNNs) 在音乐任务中缺乏统一框架的问题。GraphMuse 创新性地引入了一个针对音乐分数的有意义行为的邻居采样技术，以及层次化建模元素，以提升图网络的效率和表现力。在 pitch spelling 和 cadence detection 等预测任务上的实验显示，该框架比现有方法有显著性能改进，有望推动基于图表示的符号音乐处理的标准化和普及。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.DL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at the 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.12671v1",
      "published_date": "2024-07-17 15:54:09 UTC",
      "updated_date": "2024-07-17 15:54:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:29:58.575765"
    },
    {
      "arxiv_id": "2407.12669v1",
      "title": "Enhancing the Utility of Privacy-Preserving Cancer Classification using Synthetic Data",
      "title_zh": "使用合成数据增强隐私保护癌症分类的效用",
      "authors": [
        "Richard Osuala",
        "Daniel M. Lang",
        "Anneliese Riess",
        "Georgios Kaissis",
        "Zuzanna Szafranowska",
        "Grzegorz Skorupko",
        "Oliver Diaz",
        "Julia A. Schnabel",
        "Karim Lekadir"
      ],
      "abstract": "Deep learning holds immense promise for aiding radiologists in breast cancer\ndetection. However, achieving optimal model performance is hampered by\nlimitations in availability and sharing of data commonly associated to patient\nprivacy concerns. Such concerns are further exacerbated, as traditional deep\nlearning models can inadvertently leak sensitive training information. This\nwork addresses these challenges exploring and quantifying the utility of\nprivacy-preserving deep learning techniques, concretely, (i) differentially\nprivate stochastic gradient descent (DP-SGD) and (ii) fully synthetic training\ndata generated by our proposed malignancy-conditioned generative adversarial\nnetwork. We assess these methods via downstream malignancy classification of\nmammography masses using a transformer model. Our experimental results depict\nthat synthetic data augmentation can improve privacy-utility tradeoffs in\ndifferentially private model training. Further, model pretraining on synthetic\ndata achieves remarkable performance, which can be further increased with\nDP-SGD fine-tuning across all privacy guarantees. With this first in-depth\nexploration of privacy-preserving deep learning in breast imaging, we address\ncurrent and emerging clinical privacy requirements and pave the way towards the\nadoption of private high-utility deep diagnostic models. Our reproducible\ncodebase is publicly available at https://github.com/RichardObi/mammo_dp.",
      "tldr_zh": "本研究针对深度学习在乳腺癌检测中的数据隐私挑战，探讨了两种隐私保护技术：差分隐私随机梯度下降(DP-SGD)和使用恶性肿瘤条件生成对抗网络(GAN)生成的合成训练数据。通过在transformer模型上的实验，结果显示合成数据增强能改善DP-SGD的隐私-效用权衡，且合成数据预训练结合DP-SGD微调显著提升了乳腺癌分类性能。该工作为满足临床隐私需求提供了深入探索，并推动了高实用性私有诊断模型的采用，相关代码已在GitHub开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Early Accept at MICCAI 2024 Deep-Breath Workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.12669v1",
      "published_date": "2024-07-17 15:52:45 UTC",
      "updated_date": "2024-07-17 15:52:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:30:12.010117"
    },
    {
      "arxiv_id": "2407.12665v3",
      "title": "Beyond Next Token Prediction: Patch-Level Training for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chenze Shao",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "abstract": "The prohibitive training costs of Large Language Models (LLMs) have emerged\nas a significant bottleneck in the development of next-generation LLMs. In this\npaper, we show that it is possible to significantly reduce the training costs\nof LLMs without sacrificing their performance. Specifically, we introduce\npatch-level training for LLMs, in which multiple tokens are aggregated into a\nunit of higher information density, referred to as a `patch', to serve as the\nfundamental text unit for training LLMs. During patch-level training, we feed\nthe language model shorter sequences of patches and train it to predict the\nnext patch, thereby processing the majority of the training data at a\nsignificantly reduced cost. Following this, the model continues token-level\ntraining on the remaining training data to align with the inference mode.\nExperiments on a diverse range of models (370M-2.7B parameters) demonstrate\nthat patch-level training can reduce the overall training costs to 0.5$\\times$,\nwithout compromising the model performance compared to token-level training.\nSource code: https://github.com/shaochenze/PatchTrain.",
      "tldr_zh": "这篇论文提出了一种名为 patch-level training 的新方法，用于降低大型语言模型 (LLMs) 的训练成本，同时保持模型性能不变。具体而言，该方法将多个 tokens 聚合成一个信息密度更高的单位“patch”，并训练模型预测下一个 patch，从而显著减少训练数据处理量。接着，通过在剩余数据上进行 token-level 训练，确保模型与推理模式一致。实验在多种规模的模型 (370M-2.7B 参数) 上验证，该方法可以将总体训练成本降低至 0.5 倍，而不牺牲性能表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025 Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2407.12665v3",
      "published_date": "2024-07-17 15:48:39 UTC",
      "updated_date": "2025-05-15 05:15:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:30:24.300267"
    },
    {
      "arxiv_id": "2407.12663v2",
      "title": "Is That Rain? Understanding Effects on Visual Odometry Performance for Autonomous UAVs and Efficient DNN-based Rain Classification at the Edge",
      "title_zh": "翻译失败",
      "authors": [
        "Andrea Albanese",
        "Yanran Wang",
        "Davide Brunelli",
        "David Boyle"
      ],
      "abstract": "The development of safe and reliable autonomous unmanned aerial vehicles\nrelies on the ability of the system to recognise and adapt to changes in the\nlocal environment based on sensor inputs. State-of-the-art local tracking and\ntrajectory planning are typically performed using camera sensor input to the\nflight control algorithm, but the extent to which environmental disturbances\nlike rain affect the performance of these systems is largely unknown. In this\npaper, we first describe the development of an open dataset comprising ~335k\nimages to examine these effects for seven different classes of precipitation\nconditions and show that a worst-case average tracking error of 1.5 m is\npossible for a state-of-the-art visual odometry system (VINS-Fusion). We then\nuse the dataset to train a set of deep neural network models suited to mobile\nand constrained deployment scenarios to determine the extent to which it may be\npossible to efficiently and accurately classify these `rainy' conditions. The\nmost lightweight of these models (MobileNetV3 small) can achieve an accuracy of\n90% with a memory footprint of just 1.28 MB and a frame rate of 93 FPS, which\nis suitable for deployment in resource-constrained and latency-sensitive\nsystems. We demonstrate a classification latency in the order of milliseconds\nusing typical flight computer hardware. Accordingly, such a model can feed into\nthe disturbance estimation component of an autonomous flight controller. In\naddition, data from unmanned aerial vehicles with the ability to accurately\ndetermine environmental conditions in real time may contribute to developing\nmore granular timely localised weather forecasting.",
      "tldr_zh": "这篇论文探讨了雨天等环境干扰对自主无人机(autonomous UAVs)视觉里程计(Visual Odometry)性能的影响，并开发了一个包含约33.5万张图像的开源数据集，涵盖七种降水条件。研究显示，在最坏情况下，状态-of-the-art系统(VINS-Fusion)可能出现1.5 m的平均跟踪误差。作者使用该数据集训练了适合资源受限场景的深度神经网络(DNN)模型，其中MobileNetV3 small模型实现了90%的分类准确率，仅需1.28 MB内存和93 FPS帧率，并支持毫秒级延迟。最终，这些模型可整合到无人机的飞行控制系统中，提升环境适应性和实时天气预报的精确性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12663v2",
      "published_date": "2024-07-17 15:47:25 UTC",
      "updated_date": "2025-02-11 10:21:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:30:36.076872"
    },
    {
      "arxiv_id": "2408.00789v1",
      "title": "Machine Learning for Dynamic Management Zone in Smart Farming",
      "title_zh": "机器学习用于智能农业中的动态管理区",
      "authors": [
        "Chamil Kulatunga",
        "Sahraoui Dhelim",
        "Tahar Kechadi"
      ],
      "abstract": "Digital agriculture is growing in popularity among professionals and brings\ntogether new opportunities along with pervasive use of modern data-driven\ntechnologies. Digital agriculture approaches can be used to replace all\ntraditional agricultural system at very reasonable costs. It is very effective\nin optimising large-scale management of resources, while traditional techniques\ncannot even tackle the problem. In this paper, we proposed a dynamic management\nzone delineation approach based on Machine Learning clustering algorithms using\ncrop yield data, elevation and soil texture maps and available NDVI data. Our\nproposed dynamic management zone delineation approach is useful for analysing\nthe spatial variation of yield zones. Delineation of yield regions based on\nhistorical yield data augmented with topography and soil physical properties\nhelps farmers to economically and sustainably deploy site-specific management\npractices identifying persistent issues in a field. The use of frequency maps\nis capable of capturing dynamically changing incidental issues within a growing\nseason. The proposed zone management approach can help farmers/agronomists to\napply variable-rate N fertilisation more effectively by analysing yield\npotential and stability zones with satellite-based NDVI monitoring.",
      "tldr_zh": "本研究探讨了数字农业中机器学习的应用，旨在通过数据驱动技术优化资源管理，以取代传统农业系统。论文提出了一种动态管理区域划分方法，使用机器学习聚类算法结合作物产量数据、海拔、土壤纹理图和 NDVI 数据，分析产量区域的空间变化。该方法通过历史产量数据与地形和土壤特性的整合，帮助农民经济可持续地实施特定地点的管理实践，并利用频率地图捕获生长季节内的动态问题。最终，该方法可提升可变速率 N 施肥的有效性，促进基于卫星 NDVI 监测的产量潜力和稳定性分析。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00789v1",
      "published_date": "2024-07-17 15:37:57 UTC",
      "updated_date": "2024-07-17 15:37:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:30:45.383136"
    },
    {
      "arxiv_id": "2407.12647v1",
      "title": "Fusion Flow-enhanced Graph Pooling Residual Networks for Unmanned Aerial Vehicles Surveillance in Day and Night Dual Visions",
      "title_zh": "翻译失败",
      "authors": [
        "Alam Noor",
        "Kai Li",
        "Eduardo Tovar",
        "Pei Zhang",
        "Bo Wei"
      ],
      "abstract": "Recognizing unauthorized Unmanned Aerial Vehicles (UAVs) within designated\nno-fly zones throughout the day and night is of paramount importance, where the\nunauthorized UAVs pose a substantial threat to both civil and military aviation\nsafety. However, recognizing UAVs day and night with dual-vision cameras is\nnontrivial, since red-green-blue (RGB) images suffer from a low detection rate\nunder an insufficient light condition, such as on cloudy or stormy days, while\nblack-and-white infrared (IR) images struggle to capture UAVs that overlap with\nthe background at night. In this paper, we propose a new optical flow-assisted\ngraph-pooling residual network (OF-GPRN), which significantly enhances the UAV\ndetection rate in day and night dual visions. The proposed OF-GPRN develops a\nnew optical fusion to remove superfluous backgrounds, which improves RGB/IR\nimaging clarity. Furthermore, OF-GPRN extends optical fusion by incorporating a\ngraph residual split attention network and a feature pyramid, which refines the\nperception of UAVs, leading to a higher success rate in UAV detection. A\ncomprehensive performance evaluation is conducted using a benchmark UAV catch\ndataset. The results indicate that the proposed OF-GPRN elevates the UAV mean\naverage precision (mAP) detection rate to 87.8%, marking a 17.9% advancement\ncompared to the residual graph neural network (ResGCN)-based approach.",
      "tldr_zh": "本研究针对白天和夜晚双视觉条件下无人机的监控挑战，提出了一种光学流增强图池残差网络（OF-GPRN），以提升未授权无人机（UAVs）在禁飞区的检测率。OF-GPRN 通过新型光学融合技术移除多余背景，提高 RGB 和 IR 图像的清晰度，并结合图残差分割注意力网络和特征金字塔，精炼对 UAVs 的感知能力。实验结果显示，在基准 UAV 数据集上，该方法将 UAV 的平均精度（mAP）提升至 87.8%，比基于 ResGCN 的基线方法提高了 17.9%。这为全天候无人机监控提供了更可靠的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The article is accepted at July 08, 2024 with 13 pages and 10 figures\n  in the Journal of Engineering Applications of Artificial Intelligence,\n  Elsevier",
      "pdf_url": "http://arxiv.org/pdf/2407.12647v1",
      "published_date": "2024-07-17 15:16:23 UTC",
      "updated_date": "2024-07-17 15:16:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:30:59.111928"
    },
    {
      "arxiv_id": "2407.12642v2",
      "title": "Zero-shot Text-guided Infinite Image Synthesis with LLM guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Soyeong Kwon",
        "Taegyeong Lee",
        "Taehwan Kim"
      ],
      "abstract": "Text-guided image editing and generation methods have diverse real-world\napplications. However, text-guided infinite image synthesis faces several\nchallenges. First, there is a lack of text-image paired datasets with\nhigh-resolution and contextual diversity. Second, expanding images based on\ntext requires global coherence and rich local context understanding. Previous\nstudies have mainly focused on limited categories, such as natural landscapes,\nand also required to train on high-resolution images with paired text. To\naddress these challenges, we propose a novel approach utilizing Large Language\nModels (LLMs) for both global coherence and local context understanding,\nwithout any high-resolution text-image paired training dataset. We train the\ndiffusion model to expand an image conditioned on global and local captions\ngenerated from the LLM and visual feature. At the inference stage, given an\nimage and a global caption, we use the LLM to generate a next local caption to\nexpand the input image. Then, we expand the image using the global caption,\ngenerated local caption and the visual feature to consider global consistency\nand spatial local context. In experiments, our model outperforms the baselines\nboth quantitatively and qualitatively. Furthermore, our model demonstrates the\ncapability of text-guided arbitrary-sized image generation in zero-shot manner\nwith LLM guidance.",
      "tldr_zh": "本文提出了一种零-shot Text-guided Infinite Image Synthesis 方法，利用 Large Language Models (LLMs) 指导全局连贯性和局部上下文理解，解决文本引导图像合成面临的挑战，如缺乏高分辨率文本-图像配对数据集。方法包括训练扩散模型基于 LLM 生成的全局和局部标题以及视觉特征来扩展图像，并在推理阶段通过 LLM 生成新局部标题以确保图像一致性。该方法在实验中定量和定性上优于基线模型，并实现了零样本文本引导的任意大小图像生成。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is being withdrawn due to issues of misconduct in the\n  experiments presented in Table 2 and Figures 6, 7, and 8. We recognize this\n  as an ethical concern and sincerely apologize to the research community for\n  any inconvenience it may have caused",
      "pdf_url": "http://arxiv.org/pdf/2407.12642v2",
      "published_date": "2024-07-17 15:10:01 UTC",
      "updated_date": "2024-12-26 01:49:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:31:11.035993"
    },
    {
      "arxiv_id": "2407.12629v1",
      "title": "A Methodology Establishing Linear Convergence of Adaptive Gradient Methods under PL Inequality",
      "title_zh": "在 PL 不等式下建立适应性梯度方法线性收敛的方法论",
      "authors": [
        "Kushal Chakrabarti",
        "Mayank Baranwal"
      ],
      "abstract": "Adaptive gradient-descent optimizers are the standard choice for training\nneural network models. Despite their faster convergence than gradient-descent\nand remarkable performance in practice, the adaptive optimizers are not as well\nunderstood as vanilla gradient-descent. A reason is that the dynamic update of\nthe learning rate that helps in faster convergence of these methods also makes\ntheir analysis intricate. Particularly, the simple gradient-descent method\nconverges at a linear rate for a class of optimization problems, whereas the\npractically faster adaptive gradient methods lack such a theoretical guarantee.\nThe Polyak-{\\L}ojasiewicz (PL) inequality is the weakest known class, for which\nlinear convergence of gradient-descent and its momentum variants has been\nproved. Therefore, in this paper, we prove that AdaGrad and Adam, two\nwell-known adaptive gradient methods, converge linearly when the cost function\nis smooth and satisfies the PL inequality. Our theoretical framework follows a\nsimple and unified approach, applicable to both batch and stochastic gradients,\nwhich can potentially be utilized in analyzing linear convergence of other\nvariants of Adam.",
      "tldr_zh": "本研究提出了一种方法，证明了自适应梯度方法（Adaptive Gradient Methods）在成本函数平滑且满足 Polyak-Łojasiewicz (PL) 不等式时，能够实现线性收敛（Linear Convergence）。该方法针对 AdaGrad 和 Adam 等优化器，采用一个简单统一的理论框架，适用于批量和随机梯度场景。实验结果填补了这些方法理论上的空白，并为分析其他 Adam 变体提供了潜在工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication at the main track of 27th European\n  Conference on Artificial Intelligence (ECAI-2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.12629v1",
      "published_date": "2024-07-17 14:56:21 UTC",
      "updated_date": "2024-07-17 14:56:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:31:20.793999"
    },
    {
      "arxiv_id": "2407.18267v1",
      "title": "MCU-MixQ: A HW/SW Co-optimized Mixed-precision Neural Network Design Framework for MCUs",
      "title_zh": "翻译失败",
      "authors": [
        "Junfeng Gong",
        "Cheng Liu",
        "Long Cheng",
        "Huawei Li",
        "Xiaowei Li"
      ],
      "abstract": "Mixed-precision neural network (MPNN) that utilizes just enough data width\nfor the neural network processing is an effective approach to meet the\nstringent resources constraints including memory and computing of MCUs.\nNevertheless, there is still a lack of sub-byte and mixed-precision SIMD\noperations in MCU-class ISA and the limited computing capability of MCUs\nremains underutilized, which further aggravates the computing bound encountered\nin neural network processing. As a result, the benefits of MPNNs cannot be\nfully unleashed. In this work, we propose to pack multiple low-bitwidth\narithmetic operations within a single instruction multiple data (SIMD)\ninstructions in typical MCUs, and then develop an efficient convolution\noperator by exploring both the data parallelism and computing parallelism in\nconvolution along with the proposed SIMD packing. Finally, we further leverage\nNeural Architecture Search (NAS) to build a HW/SW co-designed MPNN design\nframework, namely MCU-MixQ. This framework can optimize both the MPNN\nquantization and MPNN implementation efficiency, striking an optimized balance\nbetween neural network performance and accuracy. According to our experiment\nresults, MCU-MixQ achieves 2.1$\\times$ and 1.4$\\times$ speedup over CMix-NN and\nMCUNet respectively under the same resource constraints.",
      "tldr_zh": "这篇论文提出 MCU-MixQ，一种针对微控制器 (MCUs) 的硬件/软件 (HW/SW) 协同优化混合精度神经网络 (MPNN) 设计框架，以解决 MPNN 在内存和计算资源受限环境中的计算瓶颈问题。框架通过在 SIMD 指令中打包多个低位宽算术操作，并开发高效卷积操作，利用数据并行性和计算并行性来提升处理效率。进一步整合 Neural Architecture Search (NAS) 来优化 MPNN 的量化过程和实现性能，在神经网络准确性和速度之间取得平衡。实验结果显示，MCU-MixQ 在相同资源约束下，比 CMix-NN 快 2.1 倍，比 MCUNet 快 1.4 倍。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18267v1",
      "published_date": "2024-07-17 14:51:15 UTC",
      "updated_date": "2024-07-17 14:51:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:31:35.166055"
    },
    {
      "arxiv_id": "2407.12620v2",
      "title": "Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages: Technologies and Experiences",
      "title_zh": "利用人工智能力量振兴",
      "authors": [
        "Claudio Pinhanez",
        "Paulo Cavalin",
        "Luciana Storto",
        "Thomas Finbow",
        "Alexander Cobbinah",
        "Julio Nogima",
        "Marisa Vasconcelos",
        "Pedro Domingues",
        "Priscila de Souza Mizukami",
        "Nicole Grell",
        "Majoí Gongora",
        "Isabel Gonçalves"
      ],
      "abstract": "Since 2022 we have been exploring application areas and technologies in which\nArtificial Intelligence (AI) and modern Natural Language Processing (NLP), such\nas Large Language Models (LLMs), can be employed to foster the usage and\nfacilitate the documentation of Indigenous languages which are in danger of\ndisappearing. We start by discussing the decreasing diversity of languages in\nthe world and how working with Indigenous languages poses unique ethical\nchallenges for AI and NLP. To address those challenges, we propose an\nalternative development AI cycle based on community engagement and usage. Then,\nwe report encouraging results in the development of high-quality machine\nlearning translators for Indigenous languages by fine-tuning state-of-the-art\n(SOTA) translators with tiny amounts of data and discuss how to avoid some\ncommon pitfalls in the process. We also present prototypes we have built in\nprojects done in 2023 and 2024 with Indigenous communities in Brazil, aimed at\nfacilitating writing, and discuss the development of Indigenous Language Models\n(ILMs) as a replicable and scalable way to create spell-checkers, next-word\npredictors, and similar tools. Finally, we discuss how we envision a future for\nlanguage documentation where dying languages are preserved as interactive\nlanguage models.",
      "tldr_zh": "这篇论文探讨了利用 AI 和 NLP（如 LLMs）来促进濒危土著语言的使用和文档化，强调了全球语言多样性减少以及相关伦理挑战。作者提出基于社区参与的替代 AI 开发周期，并通过微调 SOTA 翻译器，使用少量数据开发高质量机器翻译工具，同时避免常见陷阱。论文展示了与巴西土著社区合作的项目原型，包括简化写作工具和 Indigenous Language Models (ILMs)，用于创建拼写检查器和下一词预测器等应用。最后，作者展望未来，通过交互式语言模型保存濒危语言，实现可复制的语言保护策略。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12620v2",
      "published_date": "2024-07-17 14:46:37 UTC",
      "updated_date": "2024-07-29 17:19:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:31:46.943958"
    },
    {
      "arxiv_id": "2407.12616v1",
      "title": "Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Donggeun Kim",
        "Taesup Kim"
      ],
      "abstract": "Multimodal learning typically relies on the assumption that all modalities\nare fully available during both the training and inference phases. However, in\nreal-world scenarios, consistently acquiring complete multimodal data presents\nsignificant challenges due to various factors. This often leads to the issue of\nmissing modalities, where data for certain modalities are absent, posing\nconsiderable obstacles not only for the availability of multimodal pretrained\nmodels but also for their fine-tuning and the preservation of robustness in\ndownstream tasks. To address these challenges, we propose a novel framework\nintegrating parameter-efficient fine-tuning of unimodal pretrained models with\na self-supervised joint-embedding learning method. This framework enables the\nmodel to predict the embedding of a missing modality in the representation\nspace during inference. Our method effectively predicts the missing embedding\nthrough prompt tuning, leveraging information from available modalities. We\nevaluate our approach on several multimodal benchmark datasets and demonstrate\nits effectiveness and robustness across various scenarios of missing\nmodalities.",
      "tldr_zh": "本研究解决了多模态学习中 missing modalities 的挑战，即在训练和推理阶段可能缺少某些模态，导致模型可用性、微调和鲁棒性受限。作者提出一个新框架，通过参数高效微调（parameter-efficient fine-tuning）单模态预训练模型，并结合自监督联合嵌入学习（self-supervised joint-embedding learning），在推理时预测缺失模态的嵌入。方法利用提示调优（prompt tuning）从可用模态中提取信息进行预测，并在多个多模态基准数据集上验证，展示了显著的有效性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12616v1",
      "published_date": "2024-07-17 14:44:25 UTC",
      "updated_date": "2024-07-17 14:44:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:32:00.179567"
    },
    {
      "arxiv_id": "2407.12609v1",
      "title": "Instance-wise Uncertainty for Class Imbalance in Semantic Segmentation",
      "title_zh": "语义分割中针对类别不平衡的实例级不确定性",
      "authors": [
        "Luís Almeida",
        "Inês Dutra",
        "Francesco Renna"
      ],
      "abstract": "Semantic segmentation is a fundamental computer vision task with a vast\nnumber of applications. State of the art methods increasingly rely on deep\nlearning models, known to incorrectly estimate uncertainty and being\noverconfident in predictions, especially in data not seen during training. This\nis particularly problematic in semantic segmentation due to inherent class\nimbalance. Popular uncertainty quantification approaches are task-agnostic and\nfail to leverage spatial pixel correlations in uncertainty estimates, crucial\nin this task. In this work, a novel training methodology specifically designed\nfor semantic segmentation is presented. Training samples are weighted by\ninstance-wise uncertainty masks computed by an ensemble. This is shown to\nincrease performance on minority classes, boost model generalization and\nrobustness to domain-shift when compared to using the inverse of class\nproportions or no class weights at all. This method addresses the challenges of\nclass imbalance and uncertainty estimation in semantic segmentation,\npotentially enhancing model performance and reliability across various\napplications.",
      "tldr_zh": "本文针对语义分割任务中的类别不平衡问题，提出了一种新颖的训练方法，使用实例-wise uncertainty masks（由ensemble模型计算）来加权训练样本，从而改善模型对少数类别的预测准确性。相比于使用类别比例倒数或不加权的方法，该方法显著提升了模型的泛化能力和对domain-shift的鲁棒性。实验验证表明，这种方法能有效解决不确定性估计的挑战，提升语义分割在各种应用中的性能和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12609v1",
      "published_date": "2024-07-17 14:38:32 UTC",
      "updated_date": "2024-07-17 14:38:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:32:12.000781"
    },
    {
      "arxiv_id": "2408.00786v2",
      "title": "Whether to trust: the ML leap of faith",
      "title_zh": "是否信任：ML 的信仰飞跃",
      "authors": [
        "Tory Frame",
        "Julian Padget",
        "George Stothart",
        "Elizabeth Coulthard"
      ],
      "abstract": "Human trust is a prerequisite to trustworthy AI adoption, yet trust remains\npoorly understood. Trust is often described as an attitude, but attitudes\ncannot be reliably measured or managed. Additionally, humans frequently\nconflate trust in an AI system, its machine learning (ML) technology, and its\nother component parts. Without fully understanding the 'leap of faith' involved\nin trusting ML, users cannot develop intrinsic trust in these systems. A common\napproach to building trust is to explain a ML model's reasoning process.\nHowever, such explanations often fail to resonate with non-experts due to the\ninherent complexity of ML systems and explanations are disconnected from users'\nown (unarticulated) mental models. This work puts forward an innovative way of\ndirectly building intrinsic trust in ML, by discerning and measuring the Leap\nof Faith (LoF) taken when a user decides to rely on ML. The LoF matrix captures\nthe alignment between an ML model and a human expert's mental model. This match\nis rigorously and practically identified by feeding the user's data and\nobjective function into both an ML agent and an expert-validated rules-based\nagent: a verified point of reference that can be tested a priori against a\nuser's own mental model. This represents a new class of neuro-symbolic\narchitecture. The LoF matrix reveals to the user the distance that constitutes\nthe leap of faith between the rules-based and ML agents. For the first time, we\npropose trust metrics that evaluate whether users demonstrate trust through\ntheir actions rather than self-reported intent and whether such trust is\ndeserved based on outcomes. The significance of the contribution is that it\nenables empirical assessment and management of ML trust drivers, to support\ntrustworthy ML adoption. The approach is illustrated through a long-term\nhigh-stakes field study: a 3-month pilot of a multi-agent sleep-improvement\nsystem.",
      "tldr_zh": "这篇论文探讨了人类信任在可信AI采用中的关键作用，指出现有方法（如解释ML模型推理）因复杂性和与用户心理模型脱节而失败。作者提出创新方法，通过Leap of Faith (LoF)矩阵测量用户依赖ML时的心理飞跃，该矩阵比较ML代理和专家验证的规则-based代理在用户数据上的输出，构建一种新的neuro-symbolic架构。论文首次引入基于用户行为的信任指标来评估信任是否合理，并通过一个3个月的睡眠改善系统实地研究验证了这一方法，支持了可信ML的实证管理和采用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "12 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00786v2",
      "published_date": "2024-07-17 14:36:19 UTC",
      "updated_date": "2025-01-23 17:53:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:32:24.546427"
    },
    {
      "arxiv_id": "2407.12605v2",
      "title": "Continuous reasoning for adaptive container image distribution in the cloud-edge continuum",
      "title_zh": "翻译失败",
      "authors": [
        "Damiano Azzolini",
        "Stefano Forti",
        "Antonio Ielo"
      ],
      "abstract": "Cloud-edge computing requires applications to operate across diverse\ninfrastructures, often triggered by cyber-physical events. Containers offer a\nlightweight deployment option but pulling images from central repositories can\ncause delays. This article presents a novel declarative approach and\nopen-source prototype for replicating container images across the cloud-edge\ncontinuum. Considering resource availability, network QoS, and storage costs,\nwe leverage logic programming to (i) determine optimal initial placements via\nAnswer Set Programming (ASP) and (ii) adapt placements using Prolog-based\ncontinuous reasoning. We evaluate our solution through simulations, showcasing\nhow combining ASP and Prolog continuous reasoning can balance cost optimisation\nand prompt decision-making in placement adaptation at increasing infrastructure\nsizes.",
      "tldr_zh": "这篇论文提出了一种声明式方法和开源原型，用于在云边连续体中适应性分发容器镜像，以解决从中央仓库拉取镜像导致的延迟问题。该方法考虑资源可用性、网络 QoS 和存储成本，通过 Answer Set Programming (ASP) 确定最佳初始放置，并利用 Prolog-based 连续推理实现动态适应。实验模拟结果表明，这种结合 ASP 和 Prolog 推理的策略能够在基础设施规模增加时有效平衡成本优化和快速决策。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LO",
        "cs.SE"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12605v2",
      "published_date": "2024-07-17 14:33:52 UTC",
      "updated_date": "2025-04-05 08:30:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:32:35.477866"
    },
    {
      "arxiv_id": "2407.12599v1",
      "title": "On Diversity in Discriminative Neural Networks",
      "title_zh": "论判别式神经网络中的多样性",
      "authors": [
        "Brahim Oubaha",
        "Claude Berrou",
        "Xueyao Ji",
        "Yehya Nasser",
        "Raphaël Le Bidan"
      ],
      "abstract": "Diversity is a concept of prime importance in almost all disciplines based on\ninformation processing. In telecommunications, for example, spatial, temporal,\nand frequency diversity, as well as redundant coding, are fundamental concepts\nthat have enabled the design of extremely efficient systems. In machine\nlearning, in particular with neural networks, diversity is not always a concept\nthat is emphasized or at least clearly identified. This paper proposes a neural\nnetwork architecture that builds upon various diversity principles, some of\nthem already known, others more original. Our architecture obtains remarkable\nresults, with a record self-supervised learning accuracy of 99. 57% in MNIST,\nand a top tier promising semi-supervised learning accuracy of 94.21% in\nCIFAR-10 using only 25 labels per class.",
      "tldr_zh": "该论文探讨了多样性在判别神经网络中的重要性，强调尽管多样性在电信等领域已广泛应用，但在机器学习中尚未得到充分重视。作者提出了一种新型神经网络架构，基于多种多样性原则（包括已知和原创方法），以提升模型性能。该架构在实验中取得了显著成果：在 MNIST 数据集上实现99.57%的自监督学习准确率；在 CIFAR-10 数据集上，仅使用每个类别25个标签，便达到94.21%的半监督学习准确率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in: 2024 IEEE 12th International Symposium on Signal,\n  Image, Video and Communications (ISIVC)",
      "pdf_url": "http://arxiv.org/pdf/2407.12599v1",
      "published_date": "2024-07-17 14:26:44 UTC",
      "updated_date": "2024-07-17 14:26:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:32:48.355534"
    },
    {
      "arxiv_id": "2407.12588v2",
      "title": "Benchmarking Robust Self-Supervised Learning Across Diverse Downstream Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Antoni Kowalczuk",
        "Jan Dubiński",
        "Atiyeh Ashari Ghomi",
        "Yi Sui",
        "George Stein",
        "Jiapeng Wu",
        "Jesse C. Cresswell",
        "Franziska Boenisch",
        "Adam Dziedzic"
      ],
      "abstract": "Large-scale vision models have become integral in many applications due to\ntheir unprecedented performance and versatility across downstream tasks.\nHowever, the robustness of these foundation models has primarily been explored\nfor a single task, namely image classification. The vulnerability of other\ncommon vision tasks, such as semantic segmentation and depth estimation,\nremains largely unknown. We present a comprehensive empirical evaluation of the\nadversarial robustness of self-supervised vision encoders across multiple\ndownstream tasks. Our attacks operate in the encoder embedding space and at the\ndownstream task output level. In both cases, current state-of-the-art\nadversarial fine-tuning techniques tested only for classification significantly\ndegrade clean and robust performance on other tasks. Since the purpose of a\nfoundation model is to cater to multiple applications at once, our findings\nreveal the need to enhance encoder robustness more broadly. Our code is\navailable at ${github.com/layer6ai-labs/ssl-robustness}$.",
      "tldr_zh": "该研究评估了自监督学习(self-supervised learning)视觉编码器的对抗鲁棒性(adversarial robustness)，涵盖多种下游任务(downstream tasks)如图像分类、语义分割(semantic segmentation)和深度估计(depth estimation)，而非仅限于单一任务。作者在编码器嵌入空间和下游任务输出级别进行攻击，发现当前最先进的对抗微调技术会导致这些任务的干净性能和鲁棒性能显著下降。实验结果强调了基础模型需要更广泛提升鲁棒性，以支持多应用场景，并提供了相关代码以供参考。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the ICML 2024 Workshop on Foundation Models in the Wild",
      "pdf_url": "http://arxiv.org/pdf/2407.12588v2",
      "published_date": "2024-07-17 14:12:34 UTC",
      "updated_date": "2024-07-18 06:55:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:33:03.121794"
    },
    {
      "arxiv_id": "2407.12582v2",
      "title": "Embracing Events and Frames with Hierarchical Feature Refinement Network for Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Hu Cao",
        "Zehua Zhang",
        "Yan Xia",
        "Xinyi Li",
        "Jiahao Xia",
        "Guang Chen",
        "Alois Knoll"
      ],
      "abstract": "In frame-based vision, object detection faces substantial performance\ndegradation under challenging conditions due to the limited sensing capability\nof conventional cameras. Event cameras output sparse and asynchronous events,\nproviding a potential solution to solve these problems. However, effectively\nfusing two heterogeneous modalities remains an open issue. In this work, we\npropose a novel hierarchical feature refinement network for event-frame fusion.\nThe core concept is the design of the coarse-to-fine fusion module, denoted as\nthe cross-modality adaptive feature refinement (CAFR) module. In the initial\nphase, the bidirectional cross-modality interaction (BCI) part facilitates\ninformation bridging from two distinct sources. Subsequently, the features are\nfurther refined by aligning the channel-level mean and variance in the two-fold\nadaptive feature refinement (TAFR) part. We conducted extensive experiments on\ntwo benchmarks: the low-resolution PKU-DDD17-Car dataset and the\nhigh-resolution DSEC dataset. Experimental results show that our method\nsurpasses the state-of-the-art by an impressive margin of $\\textbf{8.0}\\%$ on\nthe DSEC dataset. Besides, our method exhibits significantly better robustness\n(\\textbf{69.5}\\% versus \\textbf{38.7}\\%) when introducing 15 different\ncorruption types to the frame images. The code can be found at the link\n(https://github.com/HuCaoFighting/FRN).",
      "tldr_zh": "该论文针对基于帧的物体检测在挑战条件下（如光照不足）性能下降的问题，提出一种结合事件相机（events）和帧数据的层次特征精炼网络。核心贡献是设计了cross-modality adaptive feature refinement (CAFR) 模块，包括bidirectional cross-modality interaction (BCI) 用于初步桥接异构模态信息，以及two-fold adaptive feature refinement (TAFR) 通过对齐通道级均值和方差进一步精炼特征。在PKU-DDD17-Car和DSEC数据集上的实验显示，该方法在DSEC数据集上比最先进方法提升8.0%，并在引入15种图像腐败类型时表现出显著更好的鲁棒性（69.5% vs 38.7%）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12582v2",
      "published_date": "2024-07-17 14:09:46 UTC",
      "updated_date": "2024-10-31 14:37:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:33:13.610863"
    },
    {
      "arxiv_id": "2407.12581v1",
      "title": "Towards Understanding Unsafe Video Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Pang",
        "Aiping Xiong",
        "Yang Zhang",
        "Tianhao Wang"
      ],
      "abstract": "Video generation models (VGMs) have demonstrated the capability to synthesize\nhigh-quality output. It is important to understand their potential to produce\nunsafe content, such as violent or terrifying videos. In this work, we provide\na comprehensive understanding of unsafe video generation.\n  First, to confirm the possibility that these models could indeed generate\nunsafe videos, we choose unsafe content generation prompts collected from 4chan\nand Lexica, and three open-source SOTA VGMs to generate unsafe videos. After\nfiltering out duplicates and poorly generated content, we created an initial\nset of 2112 unsafe videos from an original pool of 5607 videos. Through\nclustering and thematic coding analysis of these generated videos, we identify\n5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic,\nViolent/Bloody, and Political. With IRB approval, we then recruit online\nparticipants to help label the generated videos. Based on the annotations\nsubmitted by 403 participants, we identified 937 unsafe videos from the initial\nvideo set. With the labeled information and the corresponding prompts, we\ncreated the first dataset of unsafe videos generated by VGMs.\n  We then study possible defense mechanisms to prevent the generation of unsafe\nvideos. Existing defense methods in image generation focus on filtering either\ninput prompt or output results. We propose a new approach called Latent\nVariable Defense (LVD), which works within the model's internal sampling\nprocess. LVD can achieve 0.90 defense accuracy while reducing time and\ncomputing resources by 10x when sampling a large number of unsafe prompts.",
      "tldr_zh": "本研究探讨了视频生成模型（VGMs）的潜在风险，确认这些模型能生成不安全内容（如暴力或恐怖视频），并通过使用从 4chan 和 Lexica 收集的提示及三个开源 SOTA VGMs 生成了 2112 个样本视频。经聚类、主题编码分析和 403 名参与者标注，最终识别出 937 个不安全视频，并分类为 Distorted/Weird、Terrifying、Pornographic、Violent/Bloody 和 Political 等五类，从而创建了首个由 VGMs 生成的不安全视频数据集。论文还提出了一种新防御机制 Latent Variable Defense (LVD)，通过干预模型内部采样过程，实现 0.90 的防御准确率，同时将时间和计算资源减少 10 倍。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.12581v1",
      "published_date": "2024-07-17 14:07:22 UTC",
      "updated_date": "2024-07-17 14:07:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:33:34.372989"
    },
    {
      "arxiv_id": "2407.12579v1",
      "title": "The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Yao",
        "Chan-Feng Hsu",
        "Jhe-Hao Lin",
        "Hongxia Xie",
        "Terence Lin",
        "Yi-Ning Huang",
        "Hong-Han Shuai",
        "Wen-Huang Cheng"
      ],
      "abstract": "In spite of recent advancements in text-to-image generation, limitations\npersist in handling complex and imaginative prompts due to the restricted\ndiversity and complexity of training data. This work explores how diffusion\nmodels can generate images from prompts requiring artistic creativity or\nspecialized knowledge. We introduce the Realistic-Fantasy Benchmark (RFBench),\na novel evaluation framework blending realistic and fantastical scenarios. To\naddress these challenges, we propose the Realistic-Fantasy Network (RFNet), a\ntraining-free approach integrating diffusion models with LLMs. Extensive human\nevaluations and GPT-based compositional assessments demonstrate our approach's\nsuperiority over state-of-the-art methods. Our code and dataset is available at\nhttps://leo81005.github.io/Reality-and-Fantasy/.",
      "tldr_zh": "尽管文本到图像生成技术取得了进展，但扩散模型在处理复杂、富有想象力的提示时仍受限于训练数据的多样性和复杂度。  \n本文引入了 Realistic-Fantasy Benchmark (RFBench)，一个结合现实和幻想场景的评估框架，并提出了 Realistic-Fantasy Network (RFNet)，这是一种无需额外训练的整合扩散模型和 LLMs 的方法，用于辅助提示解释生成图像。  \n实验结果显示，RFNet 通过人类评估和 GPT 基于的组合评估，优于现有最先进技术，为创意图像生成提供了新途径，并公开了代码和数据集。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12579v1",
      "published_date": "2024-07-17 14:04:10 UTC",
      "updated_date": "2024-07-17 14:04:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:33:36.041881"
    },
    {
      "arxiv_id": "2407.12576v2",
      "title": "IICPilot: An Intelligent Integrated Circuit Backend Design Framework Using Open EDA",
      "title_zh": "IICPilot：一种使用开源EDA的智能集成电路后端设计框架",
      "authors": [
        "Zesong Jiang",
        "Qing Zhang",
        "Cheng Liu",
        "Long Cheng",
        "Huawei Li",
        "Xiaowei Li"
      ],
      "abstract": "Open-source EDA tools are rapidly advancing, fostering collaboration,\ninnovation, and knowledge sharing within the EDA community. However, the\ngrowing complexity of these tools, characterized by numerous design parameters\nand heuristics, poses a significant barrier to their widespread adoption. This\ncomplexity is particularly pronounced in integrated circuit (IC) backend\ndesigns, which place substantial demands on engineers' expertise in EDA tools.\nTo tackle this challenge, we introduce IICPilot, an intelligent IC backend\ndesign system based on LLM technology. IICPilot automates various backend\ndesign procedures, including script generation, EDA tool invocation, design\nspace exploration of EDA parameters, container-based computing resource\nallocation, and exception management. By automating these tasks, IICPilot\nsignificantly lowers the barrier to entry for open-source EDA tools.\nSpecifically, IICPilot utilizes LangChain's multi-agent framework to\nefficiently handle distinct design tasks, enabling flexible enhancements\nindependently. Moreover, IICPilot separates the backend design workflow from\nspecific open-source EDA tools through a unified EDA calling interface. This\napproach allows seamless integration with different open-source EDA tools like\nOpenROAD and iEDA, streamlining the backend design and optimization across the\nEDA tools.",
      "tldr_zh": "本研究针对开源 EDA 工具的复杂性（如众多设计参数和启发式方法）导致的采用障碍，提出 IICPilot，这是一个基于 LLM 技术的智能集成电路（IC）后端设计框架。IICPilot 自动化了后端设计流程，包括脚本生成、EDA 工具调用、设计空间探索、容器-based 计算资源分配和异常管理，利用 LangChain 的多智能体框架来高效处理任务。系统通过统一的 EDA 调用接口与 OpenROAD 和 iEDA 等工具无缝集成，显著降低了开源 EDA 工具的入门门槛，促进了设计流程的优化和灵活性。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "under review",
      "pdf_url": "http://arxiv.org/pdf/2407.12576v2",
      "published_date": "2024-07-17 14:02:01 UTC",
      "updated_date": "2024-08-28 03:15:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:33:46.824988"
    },
    {
      "arxiv_id": "2407.12543v2",
      "title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships",
      "title_zh": "翻译失败",
      "authors": [
        "Angie Boggust",
        "Hyemin Bang",
        "Hendrik Strobelt",
        "Arvind Satyanarayan"
      ],
      "abstract": "While interpretability methods identify a model's learned concepts, they\noverlook the relationships between concepts that make up its abstractions and\ninform its ability to generalize to new data. To assess whether models' have\nlearned human-aligned abstractions, we introduce abstraction alignment, a\nmethodology to compare model behavior against formal human knowledge.\nAbstraction alignment externalizes domain-specific human knowledge as an\nabstraction graph, a set of pertinent concepts spanning levels of abstraction.\nUsing the abstraction graph as a ground truth, abstraction alignment measures\nthe alignment of a model's behavior by determining how much of its uncertainty\nis accounted for by the human abstractions. By aggregating abstraction\nalignment across entire datasets, users can test alignment hypotheses, such as\nwhich human concepts the model has learned and where misalignments recur. In\nevaluations with experts, abstraction alignment differentiates seemingly\nsimilar errors, improves the verbosity of existing model-quality metrics, and\nuncovers improvements to current human abstractions.",
      "tldr_zh": "这篇论文提出了 abstraction alignment 方法，用于比较模型学到的概念关系（model-learned abstractions）与人类编码的抽象（human-encoded conceptual relationships），以评估模型的泛化能力。方法通过构建 abstraction graph 来外部化领域特定的人类知识，并测量模型不确定性中由人类抽象解释的部分，从而量化行为对齐度。在专家评估中，abstraction alignment 能够区分看似相似的错误、提升现有模型质量指标的详细性，并揭示当前人类抽象的潜在改进点。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 7 figures, published in CHI 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.12543v2",
      "published_date": "2024-07-17 13:27:26 UTC",
      "updated_date": "2025-02-13 20:47:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:33:59.268516"
    },
    {
      "arxiv_id": "2407.12532v1",
      "title": "Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xihe Qiu",
        "Haoyu Wang",
        "Xiaoyu Tan",
        "Chao Qu",
        "Yujie Xiong",
        "Yuan Cheng",
        "Yinghui Xu",
        "Wei Chu",
        "Yuan Qi"
      ],
      "abstract": "Effective collaboration in multi-agent systems requires communicating goals\nand intentions between agents. Current agent frameworks often suffer from\ndependencies on single-agent execution and lack robust inter-module\ncommunication, frequently leading to suboptimal multi-agent reinforcement\nlearning (MARL) policies and inadequate task coordination. To address these\nchallenges, we present a framework for training large language models (LLMs) as\ncollaborative agents to enable coordinated behaviors in cooperative MARL. Each\nagent maintains a private intention consisting of its current goal and\nassociated sub-tasks. Agents broadcast their intentions periodically, allowing\nother agents to infer coordination tasks. A propagation network transforms\nbroadcast intentions into teammate-specific communication messages, sharing\nrelevant goals with designated teammates. The architecture of our framework is\nstructured into planning, grounding, and execution modules. During execution,\nmultiple agents interact in a downstream environment and communicate intentions\nto enable coordinated behaviors. The grounding module dynamically adapts\ncomprehension strategies based on emerging coordination patterns, while\nfeedback from execution agents influnces the planning module, enabling the\ndynamic re-planning of sub-tasks. Results in collaborative environment\nsimulation demonstrate intention propagation reduces miscoordination errors by\naligning sub-task dependencies between agents. Agents learn when to communicate\nintentions and which teammates require task details, resulting in emergent\ncoordinated behaviors. This demonstrates the efficacy of intention sharing for\ncooperative multi-agent RL based on LLMs.",
      "tldr_zh": "本文提出一种框架，利用大型语言模型(LLMs)来提升多智能体强化学习(MARL)中的协作智能，每个代理维护并定期广播私有的意图（包括目标和子任务），通过传播网络将意图转化为特定队友的通信消息。框架由规划、接地和执行模块组成，允许代理动态适应协调策略并基于执行反馈重新规划子任务。实验在协作环境模拟中证明，该方法显著减少了误协调错误，促进代理学会何时及如何沟通意图，从而实现紧急协调行为。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12532v1",
      "published_date": "2024-07-17 13:14:00 UTC",
      "updated_date": "2024-07-17 13:14:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:34:23.973964"
    },
    {
      "arxiv_id": "2407.12528v1",
      "title": "On the Complexity of Identification in Linear Structural Causal Models",
      "title_zh": "线性结构因果模型中识别的复杂性",
      "authors": [
        "Julian Dörfler",
        "Benito van der Zander",
        "Markus Bläser",
        "Maciej Liskiewicz"
      ],
      "abstract": "Learning the unknown causal parameters of a linear structural causal model is\na fundamental task in causal analysis. The task, known as the problem of\nidentification, asks to estimate the parameters of the model from a combination\nof assumptions on the graphical structure of the model and observational data,\nrepresented as a non-causal covariance matrix. In this paper, we give a new\nsound and complete algorithm for generic identification which runs in\npolynomial space. By standard simulation results, this algorithm has\nexponential running time which vastly improves the state-of-the-art double\nexponential time method using a Gr\\\"obner basis approach. The paper also\npresents evidence that parameter identification is computationally hard in\ngeneral. In particular, we prove, that the task asking whether, for a given\nfeasible correlation matrix, there are exactly one or two or more parameter\nsets explaining the observed matrix, is hard for $\\forall R$, the co-class of\nthe existential theory of the reals. In particular, this problem is\n$coNP$-hard. To our best knowledge, this is the first hardness result for some\nnotion of identifiability.",
      "tldr_zh": "本论文探讨了线性结构因果模型（linear structural causal models）中参数识别（identification）的复杂性问题，旨在从图形结构假设和观测协方差矩阵中估计未知因果参数。研究提出了一种新的声音且完整的泛化识别算法，该算法在多项式空间中运行，将运行时间从现有的双指数方法（如基于Gr\\\"obner basis的方法）改进为指数级别。论文进一步证明，判断一个给定的可行相关矩阵是否对应恰好一个、两个或更多参数集的问题是coNP-hard，这是首次针对可识别性概念的计算硬度结果。总的来说，该工作为因果分析提供了更高效的算法，并突显了该任务的计算挑战。",
      "categories": [
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12528v1",
      "published_date": "2024-07-17 13:11:26 UTC",
      "updated_date": "2024-07-17 13:11:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:34:23.196504"
    },
    {
      "arxiv_id": "2407.12522v1",
      "title": "Struct-X: Enhancing Large Language Models Reasoning with Structured Data",
      "title_zh": "Struct-X：利用结构化数据增强大型语言模型的推理能力",
      "authors": [
        "Xiaoyu Tan",
        "Haoyu Wang",
        "Xihe Qiu",
        "Yuan Cheng",
        "Yinghui Xu",
        "Wei Chu",
        "Yuan Qi"
      ],
      "abstract": "Structured data, rich in logical and relational information, has the\npotential to enhance the reasoning abilities of large language models (LLMs).\nStill, its integration poses a challenge due to the risk of overwhelming LLMs\nwith excessive tokens and irrelevant context information. To address this, we\npropose Struct-X, a novel framework that operates through five key phases:\n``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize\nstructured data. It begins by encoding structured data into a topological space\nusing graph embeddings, followed by filling in missing entity information with\nknowledge retrieval modules, and filtering out irrelevant tokens via a\nself-supervised module. The final phase involves constructing a topological\nnetwork with selected tokens to further reduce the total token length for more\neffective LLM inference. Additionally, Struct-X includes an Auxiliary Module\ntrained to generate prompts, aiding LLMs in analyzing structured data.\nExtensive experiments on benchmarks, including the knowledge graph\nquestion-answer task and the long document reading comprehension task, show\nthat Struct-X notably improves LLM reasoning, demonstrating the effectiveness\nof structured data augmentation in improving LLM inference with complex input\ncontext.",
      "tldr_zh": "本研究提出 Struct-X 框架，以提升大型语言模型 (LLMs) 的推理能力，通过高效整合结构化数据来解决过量 tokens 和无关上下文的挑战。框架采用五个关键阶段——read-model-fill-reflect-reason，包括使用 graph embeddings 将结构化数据编码到拓扑空间、知识检索模块填充缺失实体信息、自监督模块过滤无关 tokens，以及构建拓扑网络减少 tokens 长度。实验在知识图谱问答任务和长文档阅读理解任务上表明，Struct-X 显著提高了 LLM 的推理性能，证明了结构化数据增强在复杂输入上下文中的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12522v1",
      "published_date": "2024-07-17 13:06:25 UTC",
      "updated_date": "2024-07-17 13:06:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:34:36.733855"
    },
    {
      "arxiv_id": "2407.12516v1",
      "title": "Online Pseudo-Zeroth-Order Training of Neuromorphic Spiking Neural Networks",
      "title_zh": "在线伪零阶训练的神经形态脉冲神经网络",
      "authors": [
        "Mingqing Xiao",
        "Qingyan Meng",
        "Zongpeng Zhang",
        "Di He",
        "Zhouchen Lin"
      ],
      "abstract": "Brain-inspired neuromorphic computing with spiking neural networks (SNNs) is\na promising energy-efficient computational approach. However, successfully\ntraining SNNs in a more biologically plausible and\nneuromorphic-hardware-friendly way is still challenging. Most recent methods\nleverage spatial and temporal backpropagation (BP), not adhering to\nneuromorphic properties. Despite the efforts of some online training methods,\ntackling spatial credit assignments by alternatives with comparable performance\nas spatial BP remains a significant problem. In this work, we propose a novel\nmethod, online pseudo-zeroth-order (OPZO) training. Our method only requires a\nsingle forward propagation with noise injection and direct top-down signals for\nspatial credit assignment, avoiding spatial BP's problem of symmetric weights\nand separate phases for layer-by-layer forward-backward propagation. OPZO\nsolves the large variance problem of zeroth-order methods by the\npseudo-zeroth-order formulation and momentum feedback connections, while having\nmore guarantees than random feedback. Combining online training, OPZO can pave\npaths to on-chip SNN training. Experiments on neuromorphic and static datasets\nwith fully connected and convolutional networks demonstrate the effectiveness\nof OPZO with similar performance compared with spatial BP, as well as estimated\nlow training costs.",
      "tldr_zh": "这篇论文提出了一种在线伪零阶(OPZO)训练方法，用于脑启发的神经形态脉冲神经网络(SNNs)，以解决传统空间反向传播(BP)方法不符合神经形态特性和难以处理空间信用分配的问题。OPZO 仅需单次前向传播结合噪声注入和直接自上而下的信号进行空间信用分配，从而避免了 BP 的权重对称和分层传播缺点，并通过伪零阶公式和动量反馈连接减少方差。实验结果显示，OPZO 在神经形态和静态数据集上，使用全连接和卷积网络时，性能与空间 BP 相当，同时显著降低训练成本，为芯片上 SNN 训练提供了可行路径。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12516v1",
      "published_date": "2024-07-17 12:09:00 UTC",
      "updated_date": "2024-07-17 12:09:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:34:49.323936"
    },
    {
      "arxiv_id": "2407.12894v1",
      "title": "Maintenance Strategies for Sewer Pipes with Multi-State Degradation and Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Lisandro A. Jimenez-Roa",
        "Thiago D. Simão",
        "Zaharah Bukhsh",
        "Tiedo Tinga",
        "Hajo Molegraaf",
        "Nils Jansen",
        "Marielle Stoelinga"
      ],
      "abstract": "Large-scale infrastructure systems are crucial for societal welfare, and\ntheir effective management requires strategic forecasting and intervention\nmethods that account for various complexities. Our study addresses two\nchallenges within the Prognostics and Health Management (PHM) framework applied\nto sewer assets: modeling pipe degradation across severity levels and\ndeveloping effective maintenance policies. We employ Multi-State Degradation\nModels (MSDM) to represent the stochastic degradation process in sewer pipes\nand use Deep Reinforcement Learning (DRL) to devise maintenance strategies. A\ncase study of a Dutch sewer network exemplifies our methodology. Our findings\ndemonstrate the model's effectiveness in generating intelligent, cost-saving\nmaintenance strategies that surpass heuristics. It adapts its management\nstrategy based on the pipe's age, opting for a passive approach for newer pipes\nand transitioning to active strategies for older ones to prevent failures and\nreduce costs. This research highlights DRL's potential in optimizing\nmaintenance policies. Future research will aim improve the model by\nincorporating partial observability, exploring various reinforcement learning\nalgorithms, and extending this methodology to comprehensive infrastructure\nmanagement.",
      "tldr_zh": "本文研究了下水道管道的多状态退化问题，采用 Multi-State Degradation Models (MSDM) 来模拟管道的随机退化过程，并利用 Deep Reinforcement Learning (DRL) 开发高效的维护策略。针对荷兰下水道网络的案例研究显示，该方法生成的策略比传统启发式方法更智能，能根据管道年龄动态调整（如对新管道采用被动策略，对老管道转为积极干预），从而降低成本并防止故障。该研究突出了 DRL 在 Prognostics and Health Management (PHM) 框架中的潜力，并建议未来工作包括纳入部分可观察性和扩展到更广泛的基础设施管理。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12894v1",
      "published_date": "2024-07-17 12:07:07 UTC",
      "updated_date": "2024-07-17 12:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:35:00.316850"
    },
    {
      "arxiv_id": "2407.12508v2",
      "title": "MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline",
      "title_zh": "翻译失败",
      "authors": [
        "Donghoon Han",
        "Eunhwan Park",
        "Gisang Lee",
        "Adam Lee",
        "Nojun Kwak"
      ],
      "abstract": "The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.",
      "tldr_zh": "该论文提出 MERLIN，一种无需训练的多模态嵌入精炼管道，通过 Large Language Models (LLMs) 进行基于迭代导航的反馈学习，以解决文本-视频检索中用户查询与内容不匹配的问题。MERLIN 通过动态问答过程从用户视角优化查询嵌入，实现查询和视频内容的更好对齐。实验结果显示，在 MSR-VTT、MSVD 和 ActivityNet 数据集上，MERLIN 显著提升了 Recall@1 性能，优于现有系统，并证明了将 LLMs 整合到多模态检索中的优势。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Industry Track Accepted (Camera-Ready Version)",
      "pdf_url": "http://arxiv.org/pdf/2407.12508v2",
      "published_date": "2024-07-17 11:45:02 UTC",
      "updated_date": "2024-10-16 06:25:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:35:12.181704"
    },
    {
      "arxiv_id": "2407.12505v1",
      "title": "Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Runfa Chen",
        "Ling Wang",
        "Yu Du",
        "Tianrui Xue",
        "Fuchun Sun",
        "Jianwei Zhang",
        "Wenbing Huang"
      ],
      "abstract": "Learning policies for multi-entity systems in 3D environments is far more\ncomplicated against single-entity scenarios, due to the exponential expansion\nof the global state space as the number of entities increases. One potential\nsolution of alleviating the exponential complexity is dividing the global space\ninto independent local views that are invariant to transformations including\ntranslations and rotations. To this end, this paper proposes Subequivariant\nHierarchical Neural Networks (SHNN) to facilitate multi-entity policy learning.\nIn particular, SHNN first dynamically decouples the global space into local\nentity-level graphs via task assignment. Second, it leverages subequivariant\nmessage passing over the local entity-level graphs to devise local reference\nframes, remarkably compressing the representation redundancy, particularly in\ngravity-affected environments. Furthermore, to overcome the limitations of\nexisting benchmarks in capturing the subtleties of multi-entity systems under\nthe Euclidean symmetry, we propose the Multi-entity Benchmark (MEBEN), a new\nsuite of environments tailored for exploring a wide range of multi-entity\nreinforcement learning. Extensive experiments demonstrate significant\nadvancements of SHNN on the proposed benchmarks compared to existing methods.\nComprehensive ablations are conducted to verify the indispensability of task\nassignment and subequivariance.",
      "tldr_zh": "这篇论文针对3D多实体物理环境中的强化学习问题，提出Subequivariant Hierarchical Neural Networks (SHNN)，通过任务分配动态解耦全局空间并在局部实体级图上使用subequivariant消息传递，构建局部参考框架以显著压缩表示冗余，尤其在受重力影响的环境中。论文还引入Multi-entity Benchmark (MEBEN)，一个新的基准套件，用于探索多实体强化学习中的欧氏对称性。实验结果显示，SHNN在该基准上比现有方法有显著改进，并通过消融实验验证了任务分配和subequivariance的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12505v1",
      "published_date": "2024-07-17 11:37:34 UTC",
      "updated_date": "2024-07-17 11:37:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:35:25.188688"
    },
    {
      "arxiv_id": "2407.12492v2",
      "title": "Temporal Test-Time Adaptation with State-Space Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mona Schirmer",
        "Dan Zhang",
        "Eric Nalisnick"
      ],
      "abstract": "Distribution shifts between training and test data are inevitable over the\nlifecycle of a deployed model, leading to performance decay. Adapting a model\non test samples can help mitigate this drop in performance. However, most\ntest-time adaptation methods have focused on synthetic corruption shifts,\nleaving a variety of distribution shifts underexplored. In this paper, we focus\non distribution shifts that evolve gradually over time, which are common in the\nwild but challenging for existing methods, as we show. To address this, we\npropose STAD, a probabilistic state-space model that adapts a deployed model to\ntemporal distribution shifts by learning the time-varying dynamics in the last\nset of hidden features. Without requiring labels, our model infers\ntime-evolving class prototypes that act as a dynamic classification head.\nThrough experiments on real-world temporal distribution shifts, we show that\nour method excels in handling small batch sizes and label shift.",
      "tldr_zh": "这篇论文针对模型部署后因时间演变的分布偏移导致性能下降的问题，提出了一种名为 STAD 的测试时适应方法。\nSTAD 基于 probabilistic state-space models，通过学习隐藏特征的时变动态，推断时间演变的类原型作为动态分类头，而无需标签。\n实验结果显示，该方法在真实世界的时间 distribution shifts 上表现出色，尤其在小批量和 label shift 场景中优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12492v2",
      "published_date": "2024-07-17 11:18:49 UTC",
      "updated_date": "2024-10-02 17:29:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:35:35.210883"
    },
    {
      "arxiv_id": "2407.12893v1",
      "title": "Hybrid Dynamic Pruning: A Pathway to Efficient Transformer Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Ghadeer Jaradat",
        "Mohammed Tolba",
        "Ghada Alsuhli",
        "Hani Saleh",
        "Mahmoud Al-Qutayri",
        "Thanos Stouraitis",
        "Baker Mohammad"
      ],
      "abstract": "In the world of deep learning, Transformer models have become very\nsignificant, leading to improvements in many areas from understanding language\nto recognizing images, covering a wide range of applications. Despite their\nsuccess, the deployment of these models in real-time applications, particularly\non edge devices, poses significant challenges due to their quadratic\ncomputational intensity and memory demands. To overcome these challenges we\nintroduce a novel Hybrid Dynamic Pruning (HDP), an efficient\nalgorithm-architecture co-design approach that accelerates transformers using\nhead sparsity, block sparsity and approximation opportunities to reduce\ncomputations in attention and reduce memory access. With the observation of the\nhuge redundancy in attention scores and attention heads, we propose a novel\ninteger-based row-balanced block pruning to prune unimportant blocks in the\nattention matrix at run time, also propose integer-based head pruning to detect\nand prune unimportant heads at an early stage at run time. Also we propose an\napproximation method that reduces attention computations. To efficiently\nsupport these methods with lower latency and power efficiency, we propose a HDP\nco-processor architecture.",
      "tldr_zh": "该研究针对 Transformer 模型在实时应用中的计算密集和内存需求问题，提出了一种新型 Hybrid Dynamic Pruning (HDP) 算法-架构协同设计方法，以通过 head sparsity、block sparsity 和 approximation opportunities 来减少 attention 模块的计算和内存访问。HDP 包括 integer-based row-balanced block pruning，用于运行时修剪不重要的 attention 矩阵块，以及 integer-based head pruning，用于早期检测并修剪不重要的 heads，同时引入一种 approximation method 来进一步降低 attention 计算。最终，该方法结合 HDP co-processor 架构，实现更低的延迟和功耗，提高了 Transformer 推理的整体效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12893v1",
      "published_date": "2024-07-17 11:15:16 UTC",
      "updated_date": "2024-07-17 11:15:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:35:48.523941"
    },
    {
      "arxiv_id": "2407.12471v2",
      "title": "Characterization of Political Polarized Users Attacked by Language Toxicity on Twitter",
      "title_zh": "翻译失败",
      "authors": [
        "Wentao Xu"
      ],
      "abstract": "Understanding the dynamics of language toxicity on social media is important\nfor us to investigate the propagation of misinformation and the development of\necho chambers for political scenarios such as U.S. presidential elections.\nRecent research has used large-scale data to investigate the dynamics across\nsocial media platforms. However, research on the toxicity dynamics is not\nenough. This study aims to provide a first exploration of the potential\nlanguage toxicity flow among Left, Right and Center users. Specifically, we aim\nto examine whether Left users were easier to be attacked by language toxicity.\nIn this study, more than 500M Twitter posts were examined. It was discovered\nthat Left users received much more toxic replies than Right and Center users.",
      "tldr_zh": "这篇论文探讨了Twitter上语言toxicity对政治极化用户的动态影响，特别是针对美国总统选举等场景，以揭示错误信息传播和回音室（echo chambers）的形成机制。通过分析超过500M Twitter帖子，研究比较了Left users、Right users和Center users的toxicity流动情况。结果显示，Left users比Right users和Center users更容易受到toxicity回复的攻击，为理解社交媒体政治互动提供了重要洞见。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "91, 94",
        "J.4"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12471v2",
      "published_date": "2024-07-17 10:49:47 UTC",
      "updated_date": "2024-11-14 05:49:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:36:05.904348"
    },
    {
      "arxiv_id": "2407.12468v3",
      "title": "Evaluating Search Engines and Large Language Models for Answering Health Questions",
      "title_zh": "翻译失败",
      "authors": [
        "Marcos Fernández-Pichel",
        "Juan C. Pichel",
        "David E. Losada"
      ],
      "abstract": "Search engines (SEs) have traditionally been primary tools for information\nseeking, but the new Large Language Models (LLMs) are emerging as powerful\nalternatives, particularly for question-answering tasks. This study compares\nthe performance of four popular SEs, seven LLMs, and retrieval-augmented (RAG)\nvariants in answering 150 health-related questions from the TREC Health\nMisinformation (HM) Track. Results reveal SEs correctly answer between 50 and\n70% of questions, often hindered by many retrieval results not responding to\nthe health question. LLMs deliver higher accuracy, correctly answering about\n80% of questions, though their performance is sensitive to input prompts. RAG\nmethods significantly enhance smaller LLMs' effectiveness, improving accuracy\nby up to 30% by integrating retrieval evidence.",
      "tldr_zh": "这篇论文评估了搜索引擎（SEs）和大语言模型（LLMs）在回答健康问题的性能，比较了四个流行 SEs、七个 LLMs 以及检索增强生成（RAG）变体，使用 TREC Health Misinformation (HM) Track 的 150 个健康相关问题作为基准。结果显示，SEs 正确回答 50-70% 的问题，但常因检索结果不相关而受限。LLMs 的准确率更高，约 80%，不过对输入提示高度敏感。RAG 方法显著提升了较小 LLMs 的表现，通过整合检索证据，提高准确率高达 30%。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12468v3",
      "published_date": "2024-07-17 10:40:39 UTC",
      "updated_date": "2025-03-06 11:53:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:36:13.732595"
    },
    {
      "arxiv_id": "2407.17518v1",
      "title": "Driving pattern interpretation based on action phases clustering",
      "title_zh": "基于动作阶段聚类的驾驶模式解释",
      "authors": [
        "Xue Yao",
        "Simeon C. Calvert",
        "Serge P. Hoogendoorn"
      ],
      "abstract": "Current approaches to identifying driving heterogeneity face challenges in\ncomprehending fundamental patterns from the perspective of underlying driving\nbehavior mechanisms. The concept of Action phases was proposed in our previous\nwork, capturing the diversity of driving characteristics with physical\nmeanings. This study presents a novel framework to further interpret driving\npatterns by classifying Action phases in an unsupervised manner. In this\nframework, a Resampling and Downsampling Method (RDM) is first applied to\nstandardize the length of Action phases. Then the clustering calibration\nprocedure including ''Feature Selection'', ''Clustering Analysis'',\n''Difference/Similarity Evaluation'', and ''Action phases Re-extraction'' is\niteratively applied until all differences among clusters and similarities\nwithin clusters reach the pre-determined criteria. Application of the framework\nusing real-world datasets revealed six driving patterns in the I80 dataset,\nlabeled as ''Catch up'', ''Keep away'', and ''Maintain distance'', with both\n''Stable'' and ''Unstable'' states. Notably, Unstable patterns are more\nnumerous than Stable ones. ''Maintain distance'' is the most common among\nStable patterns. These observations align with the dynamic nature of driving.\nTwo patterns ''Stable keep away'' and ''Unstable catch up'' are missing in the\nUS101 dataset, which is in line with our expectations as this dataset was\npreviously shown to have less heterogeneity. This demonstrates the potential of\ndriving patterns in describing driving heterogeneity. The proposed framework\npromises advantages in addressing label scarcity in supervised learning and\nenhancing tasks such as driving behavior modeling and driving trajectory\nprediction.",
      "tldr_zh": "本研究提出一个新框架，通过无监督方式对 Action phases 进行聚类，以更深入地解释驾驶模式的核心机制。框架首先采用 Resampling and Downsampling Method (RDM) 标准化 Action phases 的长度，随后通过迭代的聚类校准程序，包括 Feature Selection、Clustering Analysis、Difference/Similarity Evaluation 和 Action phases Re-extraction，确保集群间差异和内聚性符合预设标准。实验在 I80 数据集上识别出六种驾驶模式，如 “Catch up”、“Keep away” 和 “Maintain distance”，并分为 “Stable” 和 “Unstable” 状态，其中 Unstable 模式更常见，且 “Maintain distance” 是 Stable 模式中最普遍的。相比之下，US101 数据集缺少某些模式，这验证了框架在捕捉驾驶异质性的潜力，并有助于解决监督学习中的标签稀缺问题，提升驾驶行为建模和轨迹预测任务。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17518v1",
      "published_date": "2024-07-17 10:40:23 UTC",
      "updated_date": "2024-07-17 10:40:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:36:28.175879"
    },
    {
      "arxiv_id": "2407.12449v1",
      "title": "Close the Sim2real Gap via Physically-based Structured Light Synthetic Data Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Kaixin Bai",
        "Lei Zhang",
        "Zhaopeng Chen",
        "Fang Wan",
        "Jianwei Zhang"
      ],
      "abstract": "Despite the substantial progress in deep learning, its adoption in industrial\nrobotics projects remains limited, primarily due to challenges in data\nacquisition and labeling. Previous sim2real approaches using domain\nrandomization require extensive scene and model optimization. To address these\nissues, we introduce an innovative physically-based structured light simulation\nsystem, generating both RGB and physically realistic depth images, surpassing\nprevious dataset generation tools. We create an RGBD dataset tailored for\nrobotic industrial grasping scenarios and evaluate it across various tasks,\nincluding object detection, instance segmentation, and embedding sim2real\nvisual perception in industrial robotic grasping. By reducing the sim2real gap\nand enhancing deep learning training, we facilitate the application of deep\nlearning models in industrial settings. Project details are available at\nhttps://baikaixinpublic.github.io/structured light 3D synthesizer/.",
      "tldr_zh": "本研究针对深度学习在工业机器人项目中的应用受限于数据获取和标注问题，提出了一种基于 physically-based structured light 的模拟系统，以生成高质量的 RGB 和 physically realistic depth 图像，从而减少 sim2real 差距。该系统创建了一个针对 robotic industrial grasping 场景的 RGBD 数据集，并在 object detection、instance segmentation 和 sim2real 视觉感知任务中进行了评估。实验结果显示，该方法显著提升了模型性能，比传统工具更有效，促进了深度学习在工业环境的实际部署。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 2024 IEEE International Conference on Robotics and\n  Automation",
      "pdf_url": "http://arxiv.org/pdf/2407.12449v1",
      "published_date": "2024-07-17 09:57:14 UTC",
      "updated_date": "2024-07-17 09:57:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:38:40.174119"
    },
    {
      "arxiv_id": "2407.12437v1",
      "title": "Variable-Agnostic Causal Exploration for Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Minh Hoang Nguyen",
        "Hung Le",
        "Svetha Venkatesh"
      ],
      "abstract": "Modern reinforcement learning (RL) struggles to capture real-world\ncause-and-effect dynamics, leading to inefficient exploration due to extensive\ntrial-and-error actions. While recent efforts to improve agent exploration have\nleveraged causal discovery, they often make unrealistic assumptions of causal\nvariables in the environments. In this paper, we introduce a novel framework,\nVariable-Agnostic Causal Exploration for Reinforcement Learning (VACERL),\nincorporating causal relationships to drive exploration in RL without\nspecifying environmental causal variables. Our approach automatically\nidentifies crucial observation-action steps associated with key variables using\nattention mechanisms. Subsequently, it constructs the causal graph connecting\nthese steps, which guides the agent towards observation-action pairs with\ngreater causal influence on task completion. This can be leveraged to generate\nintrinsic rewards or establish a hierarchy of subgoals to enhance exploration\nefficiency. Experimental results showcase a significant improvement in agent\nperformance in grid-world, 2d games and robotic domains, particularly in\nscenarios with sparse rewards and noisy actions, such as the notorious Noisy-TV\nenvironments.",
      "tldr_zh": "该研究提出了一种新的框架Variable-Agnostic Causal Exploration for Reinforcement Learning (VACERL)，旨在解决强化学习（RL）中因果关系捕捉不足导致的探索效率低下问题，而无需预先指定环境中的因果变量。VACERL使用注意力机制自动识别关键的观察-动作步骤，并构建因果图来引导代理优先探索对任务完成有更大影响的观察-动作对，从而生成内在奖励或建立子目标层次结构。实验结果显示，该方法在网格世界、2D 游戏和机器人领域显著提升了代理性能，尤其在稀疏奖励和噪声动作环境中，如Noisy-TV场景。总的来说，VACERL为更高效的RL探索提供了实用解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12437v1",
      "published_date": "2024-07-17 09:45:27 UTC",
      "updated_date": "2024-07-17 09:45:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:36:49.258898"
    },
    {
      "arxiv_id": "2407.12426v1",
      "title": "Sharif-STR at SemEval-2024 Task 1: Transformer as a Regression Model for Fine-Grained Scoring of Textual Semantic Relations",
      "title_zh": "翻译失败",
      "authors": [
        "Seyedeh Fatemeh Ebrahimi",
        "Karim Akhavan Azari",
        "Amirmasoud Iravani",
        "Hadi Alizadeh",
        "Zeinab Sadat Taghavi",
        "Hossein Sameti"
      ],
      "abstract": "Semantic Textual Relatedness holds significant relevance in Natural Language\nProcessing, finding applications across various domains. Traditionally,\napproaches to STR have relied on knowledge-based and statistical methods.\nHowever, with the emergence of Large Language Models, there has been a paradigm\nshift, ushering in new methodologies. In this paper, we delve into the\ninvestigation of sentence-level STR within Track A (Supervised) by leveraging\nfine-tuning techniques on the RoBERTa transformer. Our study focuses on\nassessing the efficacy of this approach across different languages. Notably,\nour findings indicate promising advancements in STR performance, particularly\nin Latin languages. Specifically, our results demonstrate notable improvements\nin English, achieving a correlation of 0.82 and securing a commendable 19th\nrank. Similarly, in Spanish, we achieved a correlation of 0.67, securing the\n15th position. However, our approach encounters challenges in languages like\nArabic, where we observed a correlation of only 0.38, resulting in a 20th rank.",
      "tldr_zh": "本研究探讨了在SemEval-2024 Task 1中，使用fine-tuning RoBERTa transformer作为回归模型来评估句子级Semantic Textual Relatedness (STR)。研究团队专注于跨不同语言的STR性能评估，发现该方法在拉丁语言中表现出色，例如英语达到0.82的相关性并获得第19名、西班牙语达到0.67的相关性并获得第15名。相比之下，在阿拉伯语上表现较弱，仅有0.38的相关性和第20名的排名，这突显了语言特异性的挑战。该方法展示了transformer在细粒度STR任务中的潜力，并为NLP领域提供了新的基准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 9 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.12426v1",
      "published_date": "2024-07-17 09:25:18 UTC",
      "updated_date": "2024-07-17 09:25:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:37:01.555839"
    },
    {
      "arxiv_id": "2407.12423v3",
      "title": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Zixin Chen",
        "Jiachen Wang",
        "Meng Xia",
        "Kento Shigyo",
        "Dingdong Liu",
        "Rong Zhang",
        "Huamin Qu"
      ],
      "abstract": "The integration of Large Language Models (LLMs), especially ChatGPT, into\neducation is poised to revolutionize students' learning experiences by\nintroducing innovative conversational learning methodologies. To empower\nstudents to fully leverage the capabilities of ChatGPT in educational\nscenarios, understanding students' interaction patterns with ChatGPT is crucial\nfor instructors. However, this endeavor is challenging due to the absence of\ndatasets focused on student-ChatGPT conversations and the complexities in\nidentifying and analyzing the evolutional interaction patterns within\nconversations. To address these challenges, we collected conversational data\nfrom 48 students interacting with ChatGPT in a master's level data\nvisualization course over one semester. We then developed a coding scheme,\ngrounded in the literature on cognitive levels and thematic analysis, to\ncategorize students' interaction patterns with ChatGPT. Furthermore, we present\na visual analytics system, StuGPTViz, that tracks and compares temporal\npatterns in student prompts and the quality of ChatGPT's responses at multiple\nscales, revealing significant pedagogical insights for instructors. We\nvalidated the system's effectiveness through expert interviews with six data\nvisualization instructors and three case studies. The results confirmed\nStuGPTViz's capacity to enhance educators' insights into the pedagogical value\nof ChatGPT. We also discussed the potential research opportunities of applying\nvisual analytics in education and developing AI-driven personalized learning\nsolutions.",
      "tldr_zh": "本研究探讨了Large Language Models (LLMs)如ChatGPT在教育中的应用，旨在通过分析学生互动模式来提升教学效果。研究者收集了48名学生在一个学期内与ChatGPT的对话数据，并开发了基于认知水平和主题分析的编码方案(coding scheme)来分类互动模式。随后，提出视觉分析系统StuGPTViz，用于跟踪和比较学生提示的temporal patterns以及ChatGPT响应质量，在多个规模上揭示教育洞见。实验通过六名数据可视化教师的专家访谈和三个案例研究验证了系统的有效性，并讨论了在教育中应用视觉分析和AI驱动个性化学习解决方案的潜在机会。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "11 pages. To be published at IEEE Visualization 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12423v3",
      "published_date": "2024-07-17 09:20:44 UTC",
      "updated_date": "2024-09-17 08:32:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:37:13.325672"
    },
    {
      "arxiv_id": "2407.12421v1",
      "title": "SafePowerGraph: Safety-aware Evaluation of Graph Neural Networks for Transmission Power Grids",
      "title_zh": "翻译失败",
      "authors": [
        "Salah Ghamizi",
        "Aleksandar Bojchevski",
        "Aoxiang Ma",
        "Jun Cao"
      ],
      "abstract": "Power grids are critical infrastructures of paramount importance to modern\nsociety and their rapid evolution and interconnections has heightened the\ncomplexity of power systems (PS) operations. Traditional methods for grid\nanalysis struggle with the computational demands of large-scale RES and ES\nintegration, prompting the adoption of machine learning (ML) techniques,\nparticularly Graph Neural Networks (GNNs). GNNs have proven effective in\nsolving the alternating current (AC) Power Flow (PF) and Optimal Power Flow\n(OPF) problems, crucial for operational planning. However, existing benchmarks\nand datasets completely ignore safety and robustness requirements in their\nevaluation and never consider realistic safety-critical scenarios that most\nimpact the operations of the power grids. We present SafePowerGraph, the first\nsimulator-agnostic, safety-oriented framework and benchmark for GNNs in PS\noperations. SafePowerGraph integrates multiple PF and OPF simulators and\nassesses GNN performance under diverse scenarios, including energy price\nvariations and power line outages. Our extensive experiments underscore the\nimportance of self-supervised learning and graph attention architectures for\nGNN robustness. We provide at https://github.com/yamizi/SafePowerGraph our\nopen-source repository, a comprehensive leaderboard, a dataset and model zoo\nand expect our framework to standardize and advance research in the critical\nfield of GNN for power systems.",
      "tldr_zh": "本研究针对电力电网（Power Grids）的复杂性，提出SafePowerGraph框架，这是首个关注安全的图神经网络（Graph Neural Networks, GNNs）评估基准，用于解决交流功率流（AC Power Flow, PF）和最优功率流（Optimal Power Flow, OPF）问题。SafePowerGraph整合多种模拟器，评估GNNs在现实场景下的性能，如能源价格变化和电力线故障，确保了安全性和鲁棒性的考虑。实验结果强调了自监督学习（self-supervised learning）和图注意力架构（graph attention architectures）对提升GNNs鲁棒性的重要性，并通过开源仓库、数据集和排行榜推进电力系统研究的标准化。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12421v1",
      "published_date": "2024-07-17 09:01:38 UTC",
      "updated_date": "2024-07-17 09:01:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:37:27.166952"
    },
    {
      "arxiv_id": "2407.12417v1",
      "title": "Improving the classification of extreme classes by means of loss regularisation and generalised beta distributions",
      "title_zh": "通过损失正则化和广义beta分布改善极端类别的分类",
      "authors": [
        "Víctor Manuel Vargas",
        "Pedro Antonio Gutiérrez",
        "Javier Barbero-Gómez",
        "César Hervás-Martínez"
      ],
      "abstract": "An ordinal classification problem is one in which the target variable takes\nvalues on an ordinal scale. Nowadays, there are many of these problems\nassociated with real-world tasks where it is crucial to accurately classify the\nextreme classes of the ordinal structure. In this work, we propose a unimodal\nregularisation approach that can be applied to any loss function to improve the\nclassification performance of the first and last classes while maintaining good\nperformance for the remainder. The proposed methodology is tested on six\ndatasets with different numbers of classes, and compared with other unimodal\nregularisation methods in the literature. In addition, performance in the\nextreme classes is compared using a new metric that takes into account their\nsensitivities. Experimental results and statistical analysis show that the\nproposed methodology obtains a superior average performance considering\ndifferent metrics. The results for the proposed metric show that the\ngeneralised beta distribution generally improves classification performance in\nthe extreme classes. At the same time, the other five nominal and ordinal\nmetrics considered show that the overall performance is aligned with the\nperformance of previous alternatives.",
      "tldr_zh": "这篇论文针对序数分类（ordinal classification）问题，提出了一种单峰正则化（unimodal regularisation）方法，通过损失正则化（loss regularisation）和广义beta分布（generalised beta distributions）来提升极端类（extreme classes）的分类性能，同时维持其他类的表现。研究在六个不同类数的数据集上进行了实验，并引入了一个新的指标来评估极端类的敏感性（sensitivities）。结果显示，该方法在极端类上的平均性能优于现有方法，且在整体性能指标（如五种名义和序数指标）上与基准方案相当。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12417v1",
      "published_date": "2024-07-17 08:57:42 UTC",
      "updated_date": "2024-07-17 08:57:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:37:37.901753"
    },
    {
      "arxiv_id": "2407.12410v1",
      "title": "Proximity-based Self-Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Davide Domini",
        "Gianluca Aguzzi",
        "Nicolas Farabegoli",
        "Mirko Viroli",
        "Lukas Esterle"
      ],
      "abstract": "In recent advancements in machine learning, federated learning allows a\nnetwork of distributed clients to collaboratively develop a global model\nwithout needing to share their local data. This technique aims to safeguard\nprivacy, countering the vulnerabilities of conventional centralized learning\nmethods. Traditional federated learning approaches often rely on a central\nserver to coordinate model training across clients, aiming to replicate the\nsame model uniformly across all nodes. However, these methods overlook the\nsignificance of geographical and local data variances in vast networks,\npotentially affecting model effectiveness and applicability. Moreover, relying\non a central server might become a bottleneck in large networks, such as the\nones promoted by edge computing. Our paper introduces a novel,\nfully-distributed federated learning strategy called proximity-based\nself-federated learning that enables the self-organised creation of multiple\nfederations of clients based on their geographic proximity and data\ndistribution without exchanging raw data. Indeed, unlike traditional\nalgorithms, our approach encourages clients to share and adjust their models\nwith neighbouring nodes based on geographic proximity and model accuracy. This\nmethod not only addresses the limitations posed by diverse data distributions\nbut also enhances the model's adaptability to different regional\ncharacteristics creating specialized models for each federation. We demonstrate\nthe efficacy of our approach through simulations on well-known datasets,\nshowcasing its effectiveness over the conventional centralized federated\nlearning framework.",
      "tldr_zh": "本文提出Proximity-based Self-Federated Learning，一种新型的完全分布式联邦学习策略，旨在解决传统联邦学习中中央服务器依赖问题及其对地理和数据分布差异的忽略。 该方法让客户端基于地理接近度和数据分布自组织成多个联邦，通过与邻近节点共享模型而非原始数据，创建针对特定区域的专用模型，从而提升模型适应性和整体效能。 通过知名数据集的模拟实验，该方法在处理数据多样性方面比传统集中式联邦学习框架表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12410v1",
      "published_date": "2024-07-17 08:44:45 UTC",
      "updated_date": "2024-07-17 08:44:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:37:49.286704"
    },
    {
      "arxiv_id": "2407.12397v1",
      "title": "Mamba-PTQ: Outlier Channels in Recurrent Large Language Models",
      "title_zh": "Mamba-PTQ：循环大型语言模型中的异常通道",
      "authors": [
        "Alessandro Pierro",
        "Steven Abreu"
      ],
      "abstract": "Modern recurrent layers are emerging as a promising path toward edge\ndeployment of foundation models, especially in the context of large language\nmodels (LLMs). Compressing the whole input sequence in a finite-dimensional\nrepresentation enables recurrent layers to model long-range dependencies while\nmaintaining a constant inference cost for each token and a fixed memory\nrequirement. However, the practical deployment of LLMs in resource-limited\nenvironments often requires further model compression, such as quantization and\npruning. While these techniques are well-established for attention-based\nmodels, their effects on recurrent layers remain underexplored.\n  In this preliminary work, we focus on post-training quantization for\nrecurrent LLMs and show that Mamba models exhibit the same pattern of outlier\nchannels observed in attention-based LLMs. We show that the reason for the\ndifficulty of quantizing SSMs is caused by activation outliers, similar to\nthose observed in transformer-based LLMs. We report baseline results for\npost-training quantization of Mamba that do not take into account the\nactivation outliers and suggest first steps for outlier-aware quantization.",
      "tldr_zh": "该论文探讨了Mamba-PTQ框架，针对循环大型语言模型(recurrent LLMs)中的异常通道(outlier channels)问题，以支持边缘部署。研究发现，Mamba模型在后训练量化(post-training quantization)过程中面临与注意力-based LLMs 类似的激活异常值(activation outliers)挑战，导致量化困难。作者提供了忽略激活异常值的量化基线结果，并提出异常值感知量化(outlier-aware quantization)的初步步骤，作为优化循环LLMs压缩的起点。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Work presented at the Efficient Systems for Foundation Models\n  Workshop @ ICML2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12397v1",
      "published_date": "2024-07-17 08:21:06 UTC",
      "updated_date": "2024-07-17 08:21:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:38:52.009139"
    },
    {
      "arxiv_id": "2407.12393v5",
      "title": "PersLLM: A Personified Training Approach for Large Language Models",
      "title_zh": "PersLLM：一种针对大型语言模型的人格化训练方法",
      "authors": [
        "Zheni Zeng",
        "Jiayi Chen",
        "Huimin Chen",
        "Yukun Yan",
        "Yuxuan Chen",
        "Zhenghao Liu",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Large language models (LLMs) exhibit human-like intelligence, enabling them\nto simulate human behavior and support various applications that require both\nhumanized communication and extensive knowledge reserves. Efforts are made to\npersonify LLMs with special training data or hand-crafted prompts, while\ncorrespondingly faced with challenges such as insufficient data usage or rigid\nbehavior patterns. Consequently, personified LLMs fail to capture personified\nknowledge or express persistent opinion. To fully unlock the potential of LLM\npersonification, we propose PersLLM, a framework for better data construction\nand model tuning. For insufficient data usage, we incorporate strategies such\nas Chain-of-Thought prompting and anti-induction, improving the quality of data\nconstruction and capturing the personality experiences, knowledge, and thoughts\nmore comprehensively. For rigid behavior patterns, we design the tuning process\nand introduce automated DPO to enhance the specificity and dynamism of the\nmodels' personalities, which leads to a more natural opinion communication.\nBoth automated metrics and expert human evaluations demonstrate the\neffectiveness of our approach. Case studies in human-machine interactions and\nmulti-agent systems further suggest potential application scenarios and future\ndirections for LLM personification.",
      "tldr_zh": "本论文提出 PersLLM，一种针对大型语言模型(LLMs)的个性化训练框架，旨在解决现有方法在数据利用不足和行为模式僵化方面的挑战，从而让模型更全面地捕捉个性化知识和表达持久意见。框架通过 Chain-of-Thought prompting 和 anti-induction 等策略优化数据构建，增强对个性体验、知识和想法的全面捕捉；同时，设计模型调优过程并引入自动化 DPO，以提升模型个性的特异性和动态性。实验结果显示，PersLLM 在自动化指标和专家人类评估中表现出色，并在人机交互和多代理系统中展现了潜在应用前景。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages for main text, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.12393v5",
      "published_date": "2024-07-17 08:13:22 UTC",
      "updated_date": "2025-05-15 08:22:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:39:04.969869"
    },
    {
      "arxiv_id": "2407.12391v1",
      "title": "LLM Inference Serving: Survey of Recent Advances and Opportunities",
      "title_zh": "翻译失败",
      "authors": [
        "Baolin Li",
        "Yankai Jiang",
        "Vijay Gadepally",
        "Devesh Tiwari"
      ],
      "abstract": "This survey offers a comprehensive overview of recent advancements in Large\nLanguage Model (LLM) serving systems, focusing on research since the year 2023.\nWe specifically examine system-level enhancements that improve performance and\nefficiency without altering the core LLM decoding mechanisms. By selecting and\nreviewing high-quality papers from prestigious ML and system venues, we\nhighlight key innovations and practical considerations for deploying and\nscaling LLMs in real-world production environments. This survey serves as a\nvaluable resource for LLM practitioners seeking to stay abreast of the latest\ndevelopments in this rapidly evolving field.",
      "tldr_zh": "这篇调查综述了 2023 年以来 LLM Inference Serving 系统的最新进展，重点关注系统级增强以提升性能和效率，而不改变核心 LLM 解码机制。作者通过审阅来自知名机器学习和系统会议的高质量论文，突出了关键创新和实际考虑，包括部署和扩展 LLM 在真实生产环境中的策略。该调查为 LLM 实践者提供了宝贵资源，帮助他们跟上这个快速演进领域的动态。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12391v1",
      "published_date": "2024-07-17 08:11:47 UTC",
      "updated_date": "2024-07-17 08:11:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:39:16.342872"
    },
    {
      "arxiv_id": "2407.12378v2",
      "title": "StoX-Net: Stochastic Processing of Partial Sums for Efficient In-Memory Computing DNN Accelerators",
      "title_zh": "翻译失败",
      "authors": [
        "Ethan G Rogers",
        "Sohan Salahuddin Mugdho",
        "Kshemal Kshemendra Gupte",
        "Cheng Wang"
      ],
      "abstract": "Crossbar-based in-memory computing (IMC) has emerged as a promising platform\nfor hardware acceleration of deep neural networks (DNNs). However, the energy\nand latency of IMC systems are dominated by the large overhead of the\nperipheral analog-to-digital converters (ADCs). To address such ADC bottleneck,\nhere we propose to implement stochastic processing of array-level partial sums\n(PS) for efficient IMC. Leveraging the probabilistic switching of spin-orbit\ntorque magnetic tunnel junctions, the proposed PS processing eliminates the\ncostly ADC, achieving significant improvement in energy and area efficiency. To\nmitigate accuracy loss, we develop PS-quantization-aware training that enables\nbackward propagation across stochastic PS. Furthermore, a novel scheme with an\ninhomogeneous sampling length of the stochastic conversion is proposed. When\nrunning ResNet20 on the CIFAR-10 dataset, our architecture-to-algorithm\nco-design demonstrates up to 16x, 8x, and 10x improvement in energy, latency,\nand area, respectively, compared to IMC with standard ADC. Our optimized design\nconfiguration using stochastic PS achieved 130x (24x) improvement in\nEnergy-Delay-Product compared to IMC with full precision ADC (sparse low-bit\nADC), while maintaining near-software accuracy at various benchmark\nclassification tasks.",
      "tldr_zh": "本论文提出 StoX-Net，一种通过随机处理数组级部分和 (PS) 的方法，用于提升基于 Crossbar 的内存计算 (IMC) 系统在深度神经网络 (DNNs) 加速器中的能量和面积效率，从而解决 ADC（模数转换器）开销带来的瓶颈。StoX-Net 利用 spin-orbit torque magnetic tunnel junctions 的随机切换来消除昂贵的 ADC，并引入 PS-quantization-aware training 和不均匀采样方案，以最小化准确性损失。实验结果显示，在 ResNet20 运行 CIFAR-10 数据集时，与标准 IMC 相比，该设计实现了能量效率提高 16 倍、延迟提高 8 倍、面积提高 10 倍，同时 Energy-Delay-Product 提升 130 倍，并保持接近软件基准的分类准确性。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12378v2",
      "published_date": "2024-07-17 07:56:43 UTC",
      "updated_date": "2024-11-08 17:56:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:39:41.165209"
    },
    {
      "arxiv_id": "2407.12374v2",
      "title": "Graph Signal Processing for Cross-Domain Recommendation",
      "title_zh": "用于跨域推荐的图信号处理",
      "authors": [
        "Jeongeun Lee",
        "Seongku Kang",
        "Won-Yong Shin",
        "Jeongwhan Choi",
        "Noseong Park",
        "Dongha Lee"
      ],
      "abstract": "Cross-domain recommendation (CDR) extends conventional recommender systems by\nleveraging user-item interactions from dense domains to mitigate data sparsity\nand the cold start problem. While CDR offers substantial potential for\nenhancing recommendation performance, most existing CDR methods suffer from\nsensitivity to the ratio of overlapping users and intrinsic discrepancy between\nsource and target domains. To overcome these limitations, in this work, we\nexplore the application of graph signal processing (GSP) in CDR scenarios. We\npropose CGSP, a unified CDR framework based on GSP, which employs a\ncross-domain similarity graph constructed by flexibly combining target-only\nsimilarity and source-bridged similarity. By processing personalized graph\nsignals computed for users from either the source or target domain, our\nframework effectively supports both inter-domain and intra-domain\nrecommendations. Our empirical evaluation demonstrates that CGSP consistently\noutperforms various encoder-based CDR approaches in both intra-domain and\ninter-domain recommendation scenarios, especially when the ratio of overlapping\nusers is low, highlighting its significant practical implication in real-world\napplications.",
      "tldr_zh": "本研究探讨了图信号处理（GSP）在跨域推荐（CDR）中的应用，以解决现有方法对重叠用户比例敏感以及源域和目标域差异的问题。论文提出CGSP框架，通过构建一个结合目标域相似性和源域桥接相似性的跨域相似性图，并处理个性化图信号，实现域间和域内推荐的统一支持。实验结果显示，CGSP在低重叠用户比例场景下，显著优于各种编码器-based CDR方法，提升了推荐性能，具有重要的实际应用价值。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12374v2",
      "published_date": "2024-07-17 07:52:45 UTC",
      "updated_date": "2024-07-22 04:07:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:39:42.829682"
    },
    {
      "arxiv_id": "2407.12371v2",
      "title": "HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects",
      "title_zh": "翻译失败",
      "authors": [
        "Xintao Lv",
        "Liang Xu",
        "Yichao Yan",
        "Xin Jin",
        "Congsheng Xu",
        "Shuwen Wu",
        "Yifan Liu",
        "Lincheng Li",
        "Mengxiao Bi",
        "Wenjun Zeng",
        "Xiaokang Yang"
      ],
      "abstract": "Generating human-object interactions (HOIs) is critical with the tremendous\nadvances of digital avatars. Existing datasets are typically limited to humans\ninteracting with a single object while neglecting the ubiquitous manipulation\nof multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of\nfull-body human interacting with multiple objects, containing 3.3K 4D HOI\nsequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual\ndescriptions and temporal segments, benchmarking two novel tasks of HOI\nsynthesis conditioned on either the whole text prompt or the segmented text\nprompts as fine-grained timeline control. To address these novel tasks, we\npropose a dual-branch conditional diffusion model with a mutual interaction\nmodule for HOI synthesis. Besides, an auto-regressive generation pipeline is\nalso designed to obtain smooth transitions between HOI segments. Experimental\nresults demonstrate the generalization ability to unseen object geometries and\ntemporal compositions.",
      "tldr_zh": "本文提出HIMO数据集，这是一个大规模的MoCap数据集，包含3.3K个4D HOI序列和4.08M个3D HOI帧，用于研究全身体人类与多个物体的交互，解决了现有数据集仅限于单个物体的局限性。HIMO还标注了详细文本描述和时间段，并基准测试了两个新任务：基于整体文本提示或分段文本提示的HOI合成，以实现细粒度的时间线控制。为解决这些任务，作者设计了一个双分支条件扩散模型（dual-branch conditional diffusion model），结合互交互模块和自回归生成管道（auto-regressive generation pipeline），以生成平滑的HOI序列。实验结果显示，该模型对未见物体几何和时间组合具有良好的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://lvxintao.github.io/himo, accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12371v2",
      "published_date": "2024-07-17 07:47:34 UTC",
      "updated_date": "2024-09-11 09:53:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:39:54.058896"
    },
    {
      "arxiv_id": "2407.12366v2",
      "title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models",
      "title_zh": "NavGPT-2: 释放大型视觉语言模型的导航推理能力",
      "authors": [
        "Gengze Zhou",
        "Yicong Hong",
        "Zun Wang",
        "Xin Eric Wang",
        "Qi Wu"
      ],
      "abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs),\nthere is a burgeoning initiative to harness LLMs for instruction following\nrobotic navigation. Such a trend underscores the potential of LLMs to\ngeneralize navigational reasoning and diverse language understanding. However,\na significant discrepancy in agent performance is observed when integrating\nLLMs in the Vision-and-Language navigation (VLN) tasks compared to previous\ndownstream specialist models. Furthermore, the inherent capacity of language to\ninterpret and facilitate communication in agent interactions is often\nunderutilized in these integrations. In this work, we strive to bridge the\ndivide between VLN-specialized models and LLM-based navigation paradigms, while\nmaintaining the interpretative prowess of LLMs in generating linguistic\nnavigational reasoning. By aligning visual content in a frozen LLM, we\nencompass visual observation comprehension for LLMs and exploit a way to\nincorporate LLMs and navigation policy networks for effective action\npredictions and navigational reasoning. We demonstrate the data efficiency of\nthe proposed methods and eliminate the gap between LM-based agents and\nstate-of-the-art VLN specialists.",
      "tldr_zh": "本文介绍了NavGPT-2，一种框架，旨在释放大型视觉语言模型(Large Vision-Language Models)在视觉语言导航(VLN)任务中的导航推理能力，桥接LLM-based代理与专业VLN模型之间的性能差距，同时利用LLMs的语言解释优势。方法通过在冻结的LLMs中对齐视觉内容，实现视觉观察理解，并结合LLMs与导航策略网络进行有效的行动预测和推理。实验证明，该框架具有高数据效率，并消除了LLM-based代理与最先进VLN专家的性能差距。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12366v2",
      "published_date": "2024-07-17 07:44:26 UTC",
      "updated_date": "2024-09-20 01:32:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:40:04.595707"
    },
    {
      "arxiv_id": "2407.12888v1",
      "title": "Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander R. Pelletier",
        "Joseph Ramirez",
        "Irsyad Adam",
        "Simha Sankar",
        "Yu Yan",
        "Ding Wang",
        "Dylan Steinecke",
        "Wei Wang",
        "Peipei Ping"
      ],
      "abstract": "The vast amount of biomedical information available today presents a\nsignificant challenge for investigators seeking to digest, process, and\nunderstand these findings effectively. Large Language Models (LLMs) have\nemerged as powerful tools to navigate this complex and challenging data\nlandscape. However, LLMs may lead to hallucinatory responses, making Retrieval\nAugmented Generation (RAG) crucial for achieving accurate information. In this\nprotocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease\nDistinction), a comprehensive workflow designed to support investigators with\nknowledge integration and hypothesis generation, identifying validated paths\nforward. Relevant biomedical information from publications and knowledge bases\nare reviewed, integrated, and extracted via text-mining association analysis\nand explainable graph prediction models on disease nodes, forecasting potential\nlinks among drugs and diseases. These analyses, along with biomedical texts,\nare integrated into a framework that facilitates user-directed mechanism\nelucidation as well as hypothesis exploration through RAG-enabled LLMs. A\nclinical use-case demonstrates RUGGED's ability to evaluate and recommend\ntherapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy\n(DCM), analyzing prescribed drugs for molecular interactions and unexplored\nuses. The platform minimizes LLM hallucinations, offers actionable insights,\nand improves the investigation of novel therapeutics.",
      "tldr_zh": "这篇论文介绍了 RUGGED 框架，利用 Retrieval Augmented Generation (RAG) 启用的 Large Language Models (LLMs) 来生成可解释的生物医学假设，旨在解决生物医学信息过载和 LLMs 幻觉问题的挑战。框架通过文本挖掘关联分析和可解释图预测模型，整合出版物和知识库中的数据，预测药物与疾病（如 Arrhythmogenic Cardiomyopathy (ACM) 和 Dilated Cardiomyopathy (DCM)）之间的潜在联系，并支持用户导向的机制阐释。实验结果显示，RUGGED 显著减少了 LLMs 的幻觉，提供可操作的见解，并提升了新型治疗方案的调查效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12888v1",
      "published_date": "2024-07-17 07:44:18 UTC",
      "updated_date": "2024-07-17 07:44:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:40:18.219139"
    },
    {
      "arxiv_id": "2407.12357v1",
      "title": "Evaluating graph-based explanations for AI-based recommender systems",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Delarue",
        "Astrid Bertrand",
        "Tiphaine Viard"
      ],
      "abstract": "Recent years have witnessed a rapid growth of recommender systems, providing\nsuggestions in numerous applications with potentially high social impact, such\nas health or justice. Meanwhile, in Europe, the upcoming AI Act mentions\n\\emph{transparency} as a requirement for critical AI systems in order to\n``mitigate the risks to fundamental rights''. Post-hoc explanations seamlessly\nalign with this goal and extensive literature on the subject produced several\nforms of such objects, graphs being one of them. Early studies in visualization\ndemonstrated the graphs' ability to improve user understanding, positioning\nthem as potentially ideal explanations. However, it remains unclear how\ngraph-based explanations compare to other explanation designs. In this work, we\naim to determine the effectiveness of graph-based explanations in improving\nusers' perception of AI-based recommendations using a mixed-methods approach.\nWe first conduct a qualitative study to collect users' requirements for graph\nexplanations. We then run a larger quantitative study in which we evaluate the\ninfluence of various explanation designs, including enhanced graph-based ones,\non aspects such as understanding, usability and curiosity toward the AI system.\nWe find that users perceive graph-based explanations as more usable than\ndesigns involving feature importance. However, we also reveal that textual\nexplanations lead to higher objective understanding than graph-based designs.\nMost importantly, we highlight the strong contrast between participants'\nexpressed preferences for graph design and their actual ratings using it, which\nare lower compared to textual design. These findings imply that meeting\nstakeholders' expressed preferences might not alone guarantee ``good''\nexplanations. Therefore, crafting hybrid designs successfully balancing social\nexpectations with downstream performance emerges as a significant challenge.",
      "tldr_zh": "这篇论文评估了基于图形的后验解释（post-hoc explanations）在AI推荐系统中的有效性，旨在提升用户对推荐结果的理解和信任，以符合欧洲AI Act的透明度要求。研究采用混合方法，首先通过定性研究收集用户对图形解释的需求，然后进行定量研究比较不同解释设计（如图形-based、特征重要性和文本解释）对理解、可用性和好奇心的影响。结果发现，用户认为图形-based解释比特征重要性设计更可用，但文本解释在客观理解方面表现更好；此外，用户对图形设计的偏好与实际使用评分形成强烈对比。论文强调，设计混合解释（hybrid designs）以平衡社会期望和实际性能，是未来关键挑战。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12357v1",
      "published_date": "2024-07-17 07:28:49 UTC",
      "updated_date": "2024-07-17 07:28:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:40:40.181716"
    },
    {
      "arxiv_id": "2407.12352v1",
      "title": "SENTAUR: Security EnhaNced Trojan Assessment Using LLMs Against Undesirable Revisions",
      "title_zh": "翻译失败",
      "authors": [
        "Jitendra Bhandari",
        "Rajat Sadhukhan",
        "Prashanth Krishnamurthy",
        "Farshad Khorrami",
        "Ramesh Karri"
      ],
      "abstract": "A globally distributed IC supply chain brings risks due to untrusted third\nparties. The risks span inadvertent use of hardware Trojan (HT), inserted\nIntellectual Property (3P-IP) or Electronic Design Automation (EDA) flows. HT\ncan introduce stealthy HT behavior, prevent an IC work as intended, or leak\nsensitive data via side channels. To counter HTs, rapidly examining HT\nscenarios is a key requirement. While Trust-Hub benchmarks are a good starting\npoint to assess defenses, they encompass a small subset of manually created HTs\nwithin the expanse of HT designs. Further, the HTs may disappear during\nsynthesis. We propose a large language model (LLM) framework SENTAUR to\ngenerate a suite of legitimate HTs for a Register Transfer Level (RTL) design\nby learning its specifications, descriptions, and natural language descriptions\nof HT effects. Existing tools and benchmarks are limited; they need a learning\nperiod to construct an ML model to mimic the threat model and are difficult to\nreproduce. SENTAUR can swiftly produce HT instances by leveraging LLMs without\nany learning period and sanitizing the HTs facilitating their rapid assessment.\nEvaluation of SENTAUR involved generating effective, synthesizable, and\npractical HTs from TrustHub and elsewhere, investigating impacts of\npayloads/triggers at the RTL. While our evaluation focused on HT insertion,\nSENTAUR can generalize to automatically transform an RTL code to have defined\nfunctional modifications.",
      "tldr_zh": "该研究针对集成电路（IC）供应链中的硬件木马（HT）风险，提出了一种基于大型语言模型（LLM）的框架SENTAUR，用于快速生成和评估HT。SENTAUR通过学习Register Transfer Level（RTL）设计的规范、描述以及HT效果的自然语言描述，生成一组合法的HT实例，而无需任何学习期，并通过清理机制确保HT的快速评估。与现有工具相比，该框架避免了HT在合成过程中的消失问题。实验结果显示，SENTAUR在TrustHub基准和其他场景中生成了有效、可合成且实用的HT，并能泛化到自动修改RTL代码以实现特定功能变更。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12352v1",
      "published_date": "2024-07-17 07:13:06 UTC",
      "updated_date": "2024-07-17 07:13:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:40:41.404859"
    },
    {
      "arxiv_id": "2408.00780v1",
      "title": "In-Depth Analysis of Emotion Recognition through Knowledge-Based Large Language Models",
      "title_zh": "通过基于知识的大型语言模型对情感识别的深入分析",
      "authors": [
        "Bin Han",
        "Cleo Yau",
        "Su Lei",
        "Jonathan Gratch"
      ],
      "abstract": "Emotion recognition in social situations is a complex task that requires\nintegrating information from both facial expressions and the situational\ncontext. While traditional approaches to automatic emotion recognition have\nfocused on decontextualized signals, recent research emphasizes the importance\nof context in shaping emotion perceptions. This paper contributes to the\nemerging field of context-based emotion recognition by leveraging psychological\ntheories of human emotion perception to inform the design of automated methods.\nWe propose an approach that combines emotion recognition methods with Bayesian\nCue Integration (BCI) to integrate emotion inferences from decontextualized\nfacial expressions and contextual knowledge inferred via Large-language Models.\nWe test this approach in the context of interpreting facial expressions during\na social task, the prisoner's dilemma. Our results provide clear support for\nBCI across a range of automatic emotion recognition methods. The best automated\nmethod achieved results comparable to human observers, suggesting the potential\nfor this approach to advance the field of affective computing.",
      "tldr_zh": "本研究分析了基于知识型大型语言模型的情感识别，强调整合面部表情和情境上下文的重要性。作者提出一种新方法，将传统的情感识别技术与Bayesian Cue Integration (BCI)相结合，利用大型语言模型推断上下文知识，并在囚徒困境的社会任务中进行测试。结果显示，BCI在多种自动情感识别方法中均有效，且最佳方法的表现可与人类观察者媲美。该方法为情感计算领域提供了潜在的进展，推动了更准确的上下文感知情感识别。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV",
        "68"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.00780v1",
      "published_date": "2024-07-17 06:39:51 UTC",
      "updated_date": "2024-07-17 06:39:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:40:53.593289"
    },
    {
      "arxiv_id": "2407.14247v1",
      "title": "Continual Learning for Adaptable Car-Following in Dynamic Traffic Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Xianda Chen",
        "PakHin Tiu",
        "Xu Han",
        "Junjie Chen",
        "Yuanfei Wu",
        "Xinhu Zheng",
        "Meixin Zhu"
      ],
      "abstract": "The continual evolution of autonomous driving technology requires\ncar-following models that can adapt to diverse and dynamic traffic\nenvironments. Traditional learning-based models often suffer from performance\ndegradation when encountering unseen traffic patterns due to a lack of\ncontinual learning capabilities. This paper proposes a novel car-following\nmodel based on continual learning that addresses this limitation. Our framework\nincorporates Elastic Weight Consolidation (EWC) and Memory Aware Synapses (MAS)\ntechniques to mitigate catastrophic forgetting and enable the model to learn\nincrementally from new traffic data streams. We evaluate the performance of the\nproposed model on the Waymo and Lyft datasets which encompass various traffic\nscenarios. The results demonstrate that the continual learning techniques\nsignificantly outperform the baseline model, achieving 0\\% collision rates\nacross all traffic conditions. This research contributes to the advancement of\nautonomous driving technology by fostering the development of more robust and\nadaptable car-following models.",
      "tldr_zh": "这篇论文提出了一种基于持续学习(Continual Learning)的车跟随模型，用于适应动态交通环境的自动驾驶需求，以解决传统模型在遇到新交通模式时性能下降的问题。该模型整合了Elastic Weight Consolidation (EWC)和Memory Aware Synapses (MAS)技术，缓解灾难性遗忘并实现从新数据流中增量学习。在Waymo和Lyft数据集上的实验表明，该框架显著优于基线模型，实现了0%碰撞率。该研究为开发更稳健和可适应的自动驾驶车跟随系统做出了重要贡献。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14247v1",
      "published_date": "2024-07-17 06:32:52 UTC",
      "updated_date": "2024-07-17 06:32:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:41:09.501152"
    },
    {
      "arxiv_id": "2408.00779v1",
      "title": "Learning Structurally Stabilized Representations for Multi-modal Lossless DNA Storage",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Cao",
        "Tiantian He",
        "Xue Li",
        "Bin Wang",
        "Xiaohu Wu",
        "Qiang Zhang",
        "Yew-Soon Ong"
      ],
      "abstract": "In this paper, we present Reed-Solomon coded single-stranded representation\nlearning (RSRL), a novel end-to-end model for learning representations for\nmulti-modal lossless DNA storage. In contrast to existing learning-based\nmethods, the proposed RSRL is inspired by both error-correction codec and\nstructural biology. Specifically, RSRL first learns the representations for the\nsubsequent storage from the binary data transformed by the Reed-Solomon codec.\nThen, the representations are masked by an RS-code-informed mask to focus on\ncorrecting the burst errors occurring in the learning process. With the decoded\nrepresentations with error corrections, a novel biologically stabilized loss is\nformulated to regularize the data representations to possess stable\nsingle-stranded structures. By incorporating these novel strategies, the\nproposed RSRL can learn highly durable, dense, and lossless representations for\nthe subsequent storage tasks into DNA sequences. The proposed RSRL has been\ncompared with a number of strong baselines in real-world tasks of multi-modal\ndata storage. The experimental results obtained demonstrate that RSRL can store\ndiverse types of data with much higher information density and durability but\nmuch lower error rates.",
      "tldr_zh": "本文提出了一种名为 RSRL（Reed-Solomon coded single-stranded representation learning）的端到端模型，用于多模态无损 DNA 存储。RSRL 受错误纠正编码和结构生物学启发，先从 Reed-Solomon codec 转换的二进制数据中学习表示，然后通过 RS-code-informed mask 纠正突发错误，并引入生物稳定损失来确保表示具有稳定的单链结构。实验结果表明，RSRL 在真实多模态数据存储任务中，比基线方法实现了更高的信息密度、耐用性以及更低的错误率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "cs.IT",
        "math.IT",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00779v1",
      "published_date": "2024-07-17 06:31:49 UTC",
      "updated_date": "2024-07-17 06:31:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:41:17.808295"
    },
    {
      "arxiv_id": "2407.12338v1",
      "title": "GUME: Graphs and User Modalities Enhancement for Long-Tail Multimodal Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Guojiao Lin",
        "Zhen Meng",
        "Dongjie Wang",
        "Qingqing Long",
        "Yuanchun Zhou",
        "Meng Xiao"
      ],
      "abstract": "Multimodal recommendation systems (MMRS) have received considerable attention\nfrom the research community due to their ability to jointly utilize information\nfrom user behavior and product images and text. Previous research has two main\nissues. First, many long-tail items in recommendation systems have limited\ninteraction data, making it difficult to learn comprehensive and informative\nrepresentations. However, past MMRS studies have overlooked this issue.\nSecondly, users' modality preferences are crucial to their behavior. However,\nprevious research has primarily focused on learning item modality\nrepresentations, while user modality representations have remained relatively\nsimplistic.To address these challenges, we propose a novel Graphs and User\nModalities Enhancement (GUME) for long-tail multimodal recommendation.\nSpecifically, we first enhance the user-item graph using multimodal similarity\nbetween items. This improves the connectivity of long-tail items and helps them\nlearn high-quality representations through graph propagation. Then, we\nconstruct two types of user modalities: explicit interaction features and\nextended interest features. By using the user modality enhancement strategy to\nmaximize mutual information between these two features, we improve the\ngeneralization ability of user modality representations. Additionally, we\ndesign an alignment strategy for modality data to remove noise from both\ninternal and external perspectives. Extensive experiments on four publicly\navailable datasets demonstrate the effectiveness of our approach.",
      "tldr_zh": "该论文针对多模态推荐系统（MMRS）中的长尾物品（long-tail items）问题和用户模态偏好忽略问题，提出了一种新型框架GUME（Graphs and User Modalities Enhancement）。GUME首先通过物品间的多模态相似性增强用户-物品图，提高长尾物品的连通性并优化其表示学习；其次，构建显式交互特征和扩展兴趣特征，并通过最大化互信息来提升用户模态表示的泛化能力，同时设计模态数据对齐策略去除噪声。实验在四个公开数据集上验证了GUME的有效性，显著改善了推荐系统的性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "11 pages, accepted by CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12338v1",
      "published_date": "2024-07-17 06:29:00 UTC",
      "updated_date": "2024-07-17 06:29:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:41:29.508634"
    },
    {
      "arxiv_id": "2407.12331v2",
      "title": "I2AM: Interpreting Image-to-Image Latent Diffusion Models via Bi-Attribution Maps",
      "title_zh": "I2AM：通过双向归因图解释图像到图像潜在扩散模型",
      "authors": [
        "Junseo Park",
        "Hyeryung Jang"
      ],
      "abstract": "Large-scale diffusion models have made significant advances in image\ngeneration, particularly through cross-attention mechanisms. While\ncross-attention has been well-studied in text-to-image tasks, their\ninterpretability in image-to-image (I2I) diffusion models remains\nunderexplored. This paper introduces Image-to-Image Attribution Maps (I2AM), a\nmethod that enhances the interpretability of I2I models by visualizing\nbidirectional attribution maps, from the reference image to the generated image\nand vice versa. I2AM aggregates cross-attention scores across time steps,\nattention heads, and layers, offering insights into how critical features are\ntransferred between images. We demonstrate the effectiveness of I2AM across\nobject detection, inpainting, and super-resolution tasks. Our results\ndemonstrate that I2AM successfully identifies key regions responsible for\ngenerating the output, even in complex scenes. Additionally, we introduce the\nInpainting Mask Attention Consistency Score (IMACS) as a novel evaluation\nmetric to assess the alignment between attribution maps and inpainting masks,\nwhich correlates strongly with existing performance metrics. Through extensive\nexperiments, we show that I2AM enables model debugging and refinement,\nproviding practical tools for improving I2I model's performance and\ninterpretability.",
      "tldr_zh": "本研究提出 I2AM 方法，通过双向归因映射（Bi-Attribution Maps）提升图像到图像 (I2I) 扩散模型的可解释性，该方法聚合跨时间步、注意力头和层的交叉注意力分数，以可视化参考图像与生成图像之间的关键特征转移。I2AM 在物体检测、图像修复和超分辨率任务中表现出色，能够准确识别复杂场景中生成输出的关键区域。研究还引入 Inpainting Mask Attention Consistency Score (IMACS) 作为新评估指标，用于量化归因映射与修复掩码的 alignment，并证明其与现有性能指标高度相关，最终为模型调试和性能优化提供实用工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.12331v2",
      "published_date": "2024-07-17 06:15:05 UTC",
      "updated_date": "2025-03-20 08:27:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:41:52.340884"
    },
    {
      "arxiv_id": "2407.12330v1",
      "title": "Uncertainty Calibration with Energy Based Instance-wise Scaling in the Wild Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Mijoo Kim",
        "Junseok Kwon"
      ],
      "abstract": "With the rapid advancement in the performance of deep neural networks (DNNs),\nthere has been significant interest in deploying and incorporating artificial\nintelligence (AI) systems into real-world scenarios. However, many DNNs lack\nthe ability to represent uncertainty, often exhibiting excessive confidence\neven when making incorrect predictions. To ensure the reliability of AI\nsystems, particularly in safety-critical cases, DNNs should transparently\nreflect the uncertainty in their predictions. In this paper, we investigate\nrobust post-hoc uncertainty calibration methods for DNNs within the context of\nmulti-class classification tasks. While previous studies have made notable\nprogress, they still face challenges in achieving robust calibration,\nparticularly in scenarios involving out-of-distribution (OOD). We identify that\nprevious methods lack adaptability to individual input data and struggle to\naccurately estimate uncertainty when processing inputs drawn from the wild\ndataset. To address this issue, we introduce a novel instance-wise calibration\nmethod based on an energy model. Our method incorporates energy scores instead\nof softmax confidence scores, allowing for adaptive consideration of DNN\nuncertainty for each prediction within a logit space. In experiments, we show\nthat the proposed method consistently maintains robust performance across the\nspectrum, spanning from in-distribution to OOD scenarios, when compared to\nother state-of-the-art methods.",
      "tldr_zh": "这篇论文探讨了深度神经网络 (DNNs) 在多类分类任务中不确定性校准的挑战，特别是现有方法在处理分布外 (OOD) 数据时缺乏对单个输入的适应性，导致不确定性估计不准确。作者提出了一种基于能量模型的实例级校准方法，使用能量分数代替 softmax 置信度分数，在 logit 空间中动态调整每个预测的不确定性，以提升校准的鲁棒性。实验结果表明，该方法在从分布内到 OOD 场景中，比其他最先进方法保持更稳定的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12330v1",
      "published_date": "2024-07-17 06:14:55 UTC",
      "updated_date": "2024-07-17 06:14:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:41:54.963925"
    },
    {
      "arxiv_id": "2407.12327v5",
      "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
      "title_zh": "Spectra：预训练三元语言模型在规模上的惊人有效性",
      "authors": [
        "Ayush Kaushal",
        "Tejas Vaidhya",
        "Arnab Kumar Mondal",
        "Tejas Pandey",
        "Aaryan Bhagat",
        "Irina Rish"
      ],
      "abstract": "Rapid advancements in GPU computational power has outpaced memory capacity\nand bandwidth growth, creating bottlenecks in Large Language Model (LLM)\ninference. Post-training quantization is the leading method for addressing\nmemory-related bottlenecks in LLM inference, but it suffers from significant\nperformance degradation below 4-bit precision. This paper addresses these\nchallenges by investigating the pretraining of low-bitwidth models specifically\nTernary Language Models (TriLMs) as an alternative to traditional\nfloating-point models (FloatLMs) and their post-training quantized versions\n(QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning\nmultiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M\nto 3.9B parameters trained on 300B tokens. Our comprehensive evaluation\ndemonstrates that TriLMs offer superior scaling behavior in terms of model size\n(in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs\nconsistently outperform their QuantLM and FloatLM counterparts for a given bit\nsize across various benchmarks. Notably, the 3.9B parameter TriLM matches the\nperformance of the FloatLM 3.9B across all benchmarks, despite having fewer\nbits than FloatLM 830M. Overall, this research provides valuable insights into\nthe feasibility and scalability of low-bitwidth language models, paving the way\nfor the development of more efficient LLMs.\n  To enhance understanding of low-bitwidth models, we are releasing 500+\nintermediate checkpoints of the Spectra suite at\nhttps://github.com/NolanoOrg/SpectraSuite.",
      "tldr_zh": "本研究探讨了预训练三进制语言模型(Ternary Language Models, TriLMs)的惊人有效性，以解决GPU内存瓶颈问题，并作为传统浮点模型(FloatLMs)和后训练量化模型(QuantLMs)的替代方案。研究引入了Spectra LLM套件，这是首个公开的LLM集合，涵盖从99M到3.9B参数的多种位宽模型，训练于300B tokens，通过全面评估证明TriLMs在模型规模扩展方面表现出优越性。令人意外的是，在超过10亿参数规模下，TriLMs在给定位宽下 consistently 优于QuantLMs和FloatLMs，例如3.9B参数的TriLM在基准测试中与3.9B FloatLM性能相当，却比830M FloatLM位宽更低。该工作为低位宽语言模型的可行性和可扩展性提供了关键见解，并通过发布500+中间检查点促进进一步研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T30",
        "I.2.6; I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "42 pages, 21 figures, and 13 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.12327v5",
      "published_date": "2024-07-17 05:53:20 UTC",
      "updated_date": "2024-10-11 04:44:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:42:09.193879"
    },
    {
      "arxiv_id": "2407.21033v3",
      "title": "Multi-Grained Query-Guided Set Prediction Network for Grounded Multimodal Named Entity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Jielong Tang",
        "Zhenxing Wang",
        "Ziyang Gong",
        "Jianxing Yu",
        "Xiangwei Zhu",
        "Jian Yin"
      ],
      "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) is an emerging\ninformation extraction (IE) task, aiming to simultaneously extract entity\nspans, types, and corresponding visual regions of entities from given\nsentence-image pairs data. Recent unified methods employing machine reading\ncomprehension or sequence generation-based frameworks show limitations in this\ndifficult task. The former, utilizing human-designed type queries, struggles to\ndifferentiate ambiguous entities, such as Jordan (Person) and off-White x\nJordan (Shoes). The latter, following the one-by-one decoding order, suffers\nfrom exposure bias issues. We maintain that these works misunderstand the\nrelationships of multimodal entities. To tackle these, we propose a novel\nunified framework named Multi-grained Query-guided Set Prediction Network\n(MQSPN) to learn appropriate relationships at intra-entity and inter-entity\nlevels. Specifically, MQSPN explicitly aligns textual entities with visual\nregions by employing a set of learnable queries to strengthen intra-entity\nconnections. Based on distinct intra-entity modeling, MQSPN reformulates GMNER\nas a set prediction, guiding models to establish appropriate inter-entity\nrelationships from a optimal global matching perspective. Additionally, we\nincorporate a query-guided Fusion Net (QFNet) as a glue network to boost better\nalignment of two-level relationships. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performances in widely used benchmarks.",
      "tldr_zh": "本文提出了一种新型框架 Multi-Grained Query-Guided Set Prediction Network (MQSPN)，针对 Grounded Multimodal Named Entity Recognition (GMNER) 任务，同时提取实体跨度、类型及其视觉区域，以解决现有方法在区分模糊实体（如 Jordan (Person) 和 off-White x Jordan (Shoes)）以及曝光偏差问题上的局限性。MQSPN 通过一组可学习的查询显式对齐文本实体和视觉区域，强化内部实体连接，并将 GMNER 重新表述为集合预测任务，从全局最优匹配角度建立实体间关系。此外，引入 Query-guided Fusion Net (QFNet) 来增强两级关系的对齐。实验在广泛使用的基准上证明，该方法实现了最先进性能。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.21033v3",
      "published_date": "2024-07-17 05:42:43 UTC",
      "updated_date": "2025-01-25 11:53:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:42:19.526288"
    },
    {
      "arxiv_id": "2407.21032v1",
      "title": "Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion",
      "title_zh": "翻译失败",
      "authors": [
        "Sanghyun Kim",
        "Seohyeon Jung",
        "Balhae Kim",
        "Moonseok Choi",
        "Jinwoo Shin",
        "Juho Lee"
      ],
      "abstract": "This paper addresses the societal concerns arising from large-scale\ntext-to-image diffusion models for generating potentially harmful or\ncopyrighted content. Existing models rely heavily on internet-crawled data,\nwherein problematic concepts persist due to incomplete filtration processes.\nWhile previous approaches somewhat alleviate the issue, they often rely on\ntext-specified concepts, introducing challenges in accurately capturing nuanced\nconcepts and aligning model knowledge with human understandings. In response,\nwe propose a framework named Human Feedback Inversion (HFI), where human\nfeedback on model-generated images is condensed into textual tokens guiding the\nmitigation or removal of problematic images. The proposed framework can be\nbuilt upon existing techniques for the same purpose, enhancing their alignment\nwith human judgment. By doing so, we simplify the training objective with a\nself-distillation-based technique, providing a strong baseline for concept\nremoval. Our experimental results demonstrate our framework significantly\nreduces objectionable content generation while preserving image quality,\ncontributing to the ethical deployment of AI in the public sphere.",
      "tldr_zh": "这篇论文针对文本到图像扩散模型（text-to-image diffusion models）生成有害或版权内容的问题，提出了一种名为 Human Feedback Inversion (HFI) 的框架，利用人类对模型输出图像的反馈，浓缩成文本标记来指导问题内容的缓解或移除。HFI 框架构建在现有技术基础上，通过自蒸馏（self-distillation）技术简化训练目标，提升模型与人类判断的对齐。实验结果表明，该框架显著减少了不良内容生成，同时保持了图像质量，为 AI 的伦理部署提供了重要贡献。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024. 56 pages, 24 figures. Caution: This paper contains\n  discussions and examples related to harmful content, including text and\n  images. Reader discretion is advised. Code is available at\n  https://github.com/nannullna/safeguard-hfi",
      "pdf_url": "http://arxiv.org/pdf/2407.21032v1",
      "published_date": "2024-07-17 05:21:41 UTC",
      "updated_date": "2024-07-17 05:21:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:42:30.636743"
    },
    {
      "arxiv_id": "2407.12315v2",
      "title": "ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map",
      "title_zh": "翻译失败",
      "authors": [
        "Yilin Ye",
        "Shishi Xiao",
        "Xingchen Zeng",
        "Wei Zeng"
      ],
      "abstract": "Multi-modal embeddings form the foundation for vision-language models, such\nas CLIP embeddings, the most widely used text-image embeddings. However, these\nembeddings are vulnerable to subtle misalignment of cross-modal features,\nresulting in decreased model performance and diminished generalization. To\naddress this problem, we design ModalChorus, an interactive system for visual\nprobing and alignment of multi-modal embeddings. ModalChorus primarily offers a\ntwo-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel\nparametric dimensionality reduction method that integrates both metric and\nnonmetric objectives to enhance modality fusion; and 2) embedding alignment\nthat allows users to interactively articulate intentions for both point-set and\nset-set alignments. Quantitative and qualitative comparisons for CLIP\nembeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and\ndata fusion (e.g., data context map) methods demonstrate the advantages of MFM\nin showcasing cross-modal features over common vision-language datasets. Case\nstudies reveal that ModalChorus can facilitate intuitive discovery of\nmisalignment and efficient re-alignment in scenarios ranging from zero-shot\nclassification to cross-modal retrieval and generation.",
      "tldr_zh": "本研究针对多模态嵌入（如 CLIP embeddings）中跨模态特征微妙不对齐的问题，提出了 ModalChorus 系统，用于视觉探查和对齐多模态嵌入。ModalChorus 采用两阶段过程：首先通过新型参数化降维方法 Modal Fusion Map (MFM) 整合度量和非度量目标来提升模态融合；其次提供交互式嵌入对齐功能，允许用户进行点集和集集对齐。实验结果显示，MFM 在常见视觉语言数据集上优于现有方法（如 t-SNE 和 MDS），并通过案例研究证明了 ModalChorus 在零样本分类、跨模态检索和生成等场景中有效发现和修复不对齐问题。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "VIS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12315v2",
      "published_date": "2024-07-17 04:49:56 UTC",
      "updated_date": "2024-10-26 09:04:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:42:41.940231"
    },
    {
      "arxiv_id": "2407.12292v1",
      "title": "Any Target Can be Offense: Adversarial Example Generation via Generalized Latent Infection",
      "title_zh": "翻译失败",
      "authors": [
        "Youheng Sun",
        "Shengming Yuan",
        "Xuanhan Wang",
        "Lianli Gao",
        "Jingkuan Song"
      ],
      "abstract": "Targeted adversarial attack, which aims to mislead a model to recognize any\nimage as a target object by imperceptible perturbations, has become a\nmainstream tool for vulnerability assessment of deep neural networks (DNNs).\nSince existing targeted attackers only learn to attack known target classes,\nthey cannot generalize well to unknown classes. To tackle this issue, we\npropose $\\bf{G}$eneralized $\\bf{A}$dversarial attac$\\bf{KER}$ ($\\bf{GAKer}$),\nwhich is able to construct adversarial examples to any target class. The core\nidea behind GAKer is to craft a latently infected representation during\nadversarial example generation. To this end, the extracted latent\nrepresentations of the target object are first injected into intermediate\nfeatures of an input image in an adversarial generator. Then, the generator is\noptimized to ensure visual consistency with the input image while being close\nto the target object in the feature space. Since the GAKer is class-agnostic\nyet model-agnostic, it can be regarded as a general tool that not only reveals\nthe vulnerability of more DNNs but also identifies deficiencies of DNNs in a\nwider range of classes. Extensive experiments have demonstrated the\neffectiveness of our proposed method in generating adversarial examples for\nboth known and unknown classes. Notably, compared with other generative\nmethods, our method achieves an approximately $14.13\\%$ higher attack success\nrate for unknown classes and an approximately $4.23\\%$ higher success rate for\nknown classes. Our code is available in https://github.com/VL-Group/GAKer.",
      "tldr_zh": "该论文提出了一种名为 GAKer 的通用对抗攻击方法，能够针对深度神经网络 (DNNs) 生成针对任何目标类的对抗样本，从而解决现有方法仅限于已知类别的局限性。GAKer 的核心机制是通过广义潜在感染 (Generalized Latent Infection)，将目标对象的潜在表示注入到输入图像的中间特征中，并优化生成器以确保视觉一致性，同时在特征空间接近目标对象。该方法作为类无关和模型无关的工具，不仅揭示了更广泛 DNNs 的漏洞，还在实验中实现了对未知类别的攻击成功率比其他生成方法高出约 14.13%，对已知类别高出约 4.23%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12292v1",
      "published_date": "2024-07-17 03:24:09 UTC",
      "updated_date": "2024-07-17 03:24:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:42:55.774050"
    },
    {
      "arxiv_id": "2407.12288v3",
      "title": "Information-Theoretic Foundations for Machine Learning",
      "title_zh": "机器学习的信息论基础",
      "authors": [
        "Hong Jun Jeon",
        "Benjamin Van Roy"
      ],
      "abstract": "The staggering progress of machine learning in the past decade has been a\nsight to behold. In retrospect, it is both remarkable and unsettling that these\nmilestones were achievable with little to no rigorous theory to guide\nexperimentation. Despite this fact, practitioners have been able to guide their\nfuture experimentation via observations from previous large-scale empirical\ninvestigations. However, alluding to Plato's Allegory of the cave, it is likely\nthat the observations which form the field's notion of reality are but shadows\nrepresenting fragments of that reality. In this work, we propose a theoretical\nframework which attempts to answer what exists outside of the cave. To the\ntheorist, we provide a framework which is mathematically rigorous and leaves\nopen many interesting ideas for future exploration. To the practitioner, we\nprovide a framework whose results are very intuitive, general, and which will\nhelp form principles to guide future investigations. Concretely, we provide a\ntheoretical framework rooted in Bayesian statistics and Shannon's information\ntheory which is general enough to unify the analysis of many phenomena in\nmachine learning. Our framework characterizes the performance of an optimal\nBayesian learner, which considers the fundamental limits of information.\nThroughout this work, we derive very general theoretical results and apply them\nto derive insights specific to settings ranging from data which is\nindependently and identically distributed under an unknown distribution, to\ndata which is sequential, to data which exhibits hierarchical structure\namenable to meta-learning. We conclude with a section dedicated to\ncharacterizing the performance of misspecified algorithms. These results are\nexciting and particularly relevant as we strive to overcome increasingly\ndifficult machine learning challenges in this endlessly complex world.",
      "tldr_zh": "本论文探讨了机器学习领域的进展，强调尽管缺乏严格理论指导，但经验观察已推动其实践发展。该研究提出一个基于Bayesian statistics和Shannon's information theory的理论框架，用于统一分析机器学习的各种现象，并刻画最优Bayesian learner的性能极限。该框架适用于独立同分布数据、顺序数据以及层次结构数据（如meta-learning场景），并为失配算法的性能提供洞见，最终帮助理论家和实践者制定更可靠的未来研究原则。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12288v3",
      "published_date": "2024-07-17 03:18:40 UTC",
      "updated_date": "2024-08-20 05:34:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:43:06.446958"
    },
    {
      "arxiv_id": "2407.12282v2",
      "title": "Chip Placement with Diffusion Models",
      "title_zh": "基于扩散模型的芯片放置",
      "authors": [
        "Vint Lee",
        "Minh Nguyen",
        "Leena Elzeiny",
        "Chun Deng",
        "Pieter Abbeel",
        "John Wawrzynek"
      ],
      "abstract": "Macro placement is a vital step in digital circuit design that defines the\nphysical location of large collections of components, known as macros, on a 2D\nchip. Because key performance metrics of the chip are determined by the\nplacement, optimizing it is crucial. Existing learning-based methods typically\nfall short because of their reliance on reinforcement learning (RL), which is\nslow and struggles to generalize, requiring online training on each new\ncircuit. Instead, we train a diffusion model capable of placing new circuits\nzero-shot, using guided sampling in lieu of RL to optimize placement quality.\nTo enable such models to train at scale, we designed a capable yet efficient\narchitecture for the denoising model, and propose a novel algorithm to generate\nlarge synthetic datasets for pre-training. To allow zero-shot transfer to real\ncircuits, we empirically study the design decisions of our dataset generation\nalgorithm, and identify several key factors enabling generalization. When\ntrained on our synthetic data, our models generate high-quality placements on\nunseen, realistic circuits, achieving competitive performance on placement\nbenchmarks compared to state-of-the-art methods.",
      "tldr_zh": "这篇论文提出使用扩散模型（Diffusion Models）来优化芯片宏放置（Macro Placement），以解决传统强化学习（RL）方法在速度和泛化能力上的不足，该模型通过引导采样实现零样本（Zero-Shot）放置新电路。作者设计了一个高效的去噪模型架构，并开发了一个新算法生成大规模合成数据集，用于预训练以支持大规模训练和泛化。实验结果表明，该模型在未见过的真实电路上生成高质量放置，在基准测试中与最先进方法竞争性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12282v2",
      "published_date": "2024-07-17 03:02:24 UTC",
      "updated_date": "2025-03-07 05:47:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:43:19.223200"
    },
    {
      "arxiv_id": "2407.12281v2",
      "title": "Turning Generative Models Degenerate: The Power of Data Poisoning Attacks",
      "title_zh": "使生成模型退化：数据投毒攻击的威力",
      "authors": [
        "Shuli Jiang",
        "Swanand Ravindra Kadhe",
        "Yi Zhou",
        "Farhan Ahmed",
        "Ling Cai",
        "Nathalie Baracaldo"
      ],
      "abstract": "The increasing use of large language models (LLMs) trained by third parties\nraises significant security concerns. In particular, malicious actors can\nintroduce backdoors through poisoning attacks to generate undesirable outputs.\nWhile such attacks have been extensively studied in image domains and\nclassification tasks, they remain underexplored for natural language generation\n(NLG) tasks. To address this gap, we conduct an investigation of various\npoisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, a\nParameter Efficient Fine-Tuning (PEFT) method. We assess their effectiveness\nacross two generative tasks: text summarization and text completion; and we\nalso introduce new metrics to quantify the success and stealthiness of such NLG\npoisoning attacks. Through our experiments, we find that the prefix-tuning\nhyperparameters and trigger designs are the most crucial factors to influence\nattack success and stealthiness. Moreover, we demonstrate that existing popular\ndefenses are ineffective against our poisoning attacks. Our study presents the\nfirst systematic approach to understanding poisoning attacks targeting NLG\ntasks during fine-tuning via PEFT across a wide range of triggers and attack\nsettings. We hope our findings will aid the AI security community in developing\neffective defenses against such threats.",
      "tldr_zh": "这篇论文探讨了第三方训练的大型语言模型（LLMs）面临的数据中毒攻击（poisoning attacks），这些攻击可引入后门，导致生成不期望的输出，尤其在自然语言生成（NLG）任务中尚未充分研究。作者调查了多种中毒技术，通过 Prefix-Tuning（一种参数高效微调方法）针对文本摘要和文本补全任务进行攻击，并引入新指标来量化攻击的成功性和隐蔽性。实验发现，Prefix-Tuning 的超参数和触发器设计是影响攻击效果的关键因素，且现有流行防御措施对此无效。该研究首次系统性分析了 NLG 任务在微调阶段的攻击风险，并为 AI 安全社区开发有效防御提供了重要见解。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "18 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.12281v2",
      "published_date": "2024-07-17 03:02:15 UTC",
      "updated_date": "2024-07-18 05:52:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:43:32.050888"
    },
    {
      "arxiv_id": "2407.12277v1",
      "title": "Multimodal Reranking for Knowledge-Intensive Visual Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyang Wen",
        "Honglei Zhuang",
        "Hamed Zamani",
        "Alexander Hauptmann",
        "Michael Bendersky"
      ],
      "abstract": "Knowledge-intensive visual question answering requires models to effectively\nuse external knowledge to help answer visual questions. A typical pipeline\nincludes a knowledge retriever and an answer generator. However, a retriever\nthat utilizes local information, such as an image patch, may not provide\nreliable question-candidate relevance scores. Besides, the two-tower\narchitecture also limits the relevance score modeling of a retriever to select\ntop candidates for answer generator reasoning. In this paper, we introduce an\nadditional module, a multi-modal reranker, to improve the ranking quality of\nknowledge candidates for answer generation. Our reranking module takes\nmulti-modal information from both candidates and questions and performs\ncross-item interaction for better relevance score modeling. Experiments on\nOK-VQA and A-OKVQA show that multi-modal reranker from distant supervision\nprovides consistent improvements. We also find a training-testing discrepancy\nwith reranking in answer generation, where performance improves if training\nknowledge candidates are similar to or noisier than those used in testing.",
      "tldr_zh": "本文针对Knowledge-intensive visual question answering（知识密集型视觉问答）任务，提出了一种多模态重新排序（multi-modal reranking）模块，以改善知识检索器的相关性评分问题。该模块利用候选和问题的多模态信息进行跨项目交互，从而为答案生成器提供更高质量的知识候选。实验在OK-VQA和A-OKVQA数据集上显示，该方法通过distant supervision（远端监督）实现了性能的一致提升，并揭示了训练知识候选与测试候选的相似度或噪声水平对最终结果的影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12277v1",
      "published_date": "2024-07-17 02:58:52 UTC",
      "updated_date": "2024-07-17 02:58:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:43:43.670219"
    },
    {
      "arxiv_id": "2407.12229v2",
      "title": "Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based Zero-Shot Text-to-Speech",
      "title_zh": "翻译失败",
      "authors": [
        "Haibin Wu",
        "Xiaofei Wang",
        "Sefik Emre Eskimez",
        "Manthan Thakker",
        "Daniel Tompkins",
        "Chung-Hsien Tsai",
        "Canrun Li",
        "Zhen Xiao",
        "Sheng Zhao",
        "Jinyu Li",
        "Naoyuki Kanda"
      ],
      "abstract": "People change their tones of voice, often accompanied by nonverbal\nvocalizations (NVs) such as laughter and cries, to convey rich emotions.\nHowever, most text-to-speech (TTS) systems lack the capability to generate\nspeech with rich emotions, including NVs. This paper introduces EmoCtrl-TTS, an\nemotion-controllable zero-shot TTS that can generate highly emotional speech\nwith NVs for any speaker. EmoCtrl-TTS leverages arousal and valence values, as\nwell as laughter embeddings, to condition the flow-matching-based zero-shot\nTTS. To achieve high-quality emotional speech generation, EmoCtrl-TTS is\ntrained using more than 27,000 hours of expressive data curated based on\npseudo-labeling. Comprehensive evaluations demonstrate that EmoCtrl-TTS excels\nin mimicking the emotions of audio prompts in speech-to-speech translation\nscenarios. We also show that EmoCtrl-TTS can capture emotion changes, express\nstrong emotions, and generate various NVs in zero-shot TTS. See\nhttps://aka.ms/emoctrl-tts for demo samples.",
      "tldr_zh": "本论文提出EmoCtrl-TTS，一种基于flow-matching的零-shot Text-to-Speech (TTS) 系统，能够控制时间变化的情感状态，并生成包含非语言声音(NVs)如笑声和哭声的高情感化语音。系统通过arousal和valence值以及笑声嵌入作为条件，实现对任意说话者的情感控制，并使用超过27,000小时的伪标签整理的表达性数据进行训练。评估结果显示，EmoCtrl-TTS在语音到语音翻译场景中出色地模仿音频提示的情感，能够捕捉情感变化、表达强烈情绪并生成多种NVs，从而提升了TTS系统的表达能力。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by SLT2024. See https://aka.ms/emoctrl-tts for demo samples",
      "pdf_url": "http://arxiv.org/pdf/2407.12229v2",
      "published_date": "2024-07-17 00:54:15 UTC",
      "updated_date": "2024-09-17 10:40:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:43:56.159008"
    },
    {
      "arxiv_id": "2407.12223v4",
      "title": "Conditional Quantile Estimation for Uncertain Watch Time in Short-Video Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Chengzhi Lin",
        "Shuchang Liu",
        "Chuyuan Wang",
        "Yongqi Liu"
      ],
      "abstract": "Accurately predicting watch time is crucial for optimizing recommendations\nand user experience in short video platforms. However, existing methods that\nestimate a single average watch time often fail to capture the inherent\nuncertainty in user engagement patterns. In this paper, we propose Conditional\nQuantile Estimation (CQE) to model the entire conditional distribution of watch\ntime. Using quantile regression, CQE characterizes the complex watch-time\ndistribution for each user-video pair, providing a flexible and comprehensive\napproach to understanding user behavior. We further design multiple strategies\nto combine the quantile estimates, adapting to different recommendation\nscenarios and user preferences. Extensive offline experiments and online A/B\ntests demonstrate the superiority of CQE in watch-time prediction and user\nengagement modeling. Specifically, deploying CQE online on a large-scale\nplatform with hundreds of millions of daily active users has led to substantial\ngains in key evaluation metrics, including active days, engagement time, and\nvideo views. These results highlight the practical impact of our proposed\napproach in enhancing the user experience and overall performance of the short\nvideo recommendation system. The code will be released\nhttps://github.com/justopit/CQE.",
      "tldr_zh": "本文提出 Conditional Quantile Estimation (CQE) 方法，用于处理短视频推荐中观看时间的不确定性问题，通过分位数回归 (quantile regression) 建模每个用户-视频对的条件分布，提供更全面的用户行为分析，并设计多种策略适应不同推荐场景和偏好。实验结果显示，CQE 在离线和在线 A/B 测试中优于现有方法，提升了观看时间预测和用户参与建模的性能。在一个拥有数亿日活跃用户的平台上部署后，CQE 显著提高了关键指标，如活跃天数、参与时间和视频观看量，证明了其在提升用户体验和系统性能方面的实际价值。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.12223v4",
      "published_date": "2024-07-17 00:25:35 UTC",
      "updated_date": "2025-04-13 12:45:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T07:44:10.041255"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 97,
  "processed_papers_count": 97,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T07:44:34.275243"
}
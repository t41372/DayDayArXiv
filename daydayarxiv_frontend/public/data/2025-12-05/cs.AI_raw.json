[
  {
    "arxiv_id": "2512.06205v2",
    "title": "On measuring grounding and generalizing grounding problems",
    "authors": [
      "Daniel Quigley",
      "Eric Maynard"
    ],
    "abstract": "The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "resubmission: 39 pages, 85 sources, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.06205v2",
    "published_date": "2025-12-05 22:58:47 UTC",
    "updated_date": "2025-12-31 02:06:22 UTC"
  },
  {
    "arxiv_id": "2512.06204v1",
    "title": "Quantifying Memory Use in Reinforcement Learning with Temporal Range",
    "authors": [
      "Rodney Lafuente-Mercado",
      "Daniela Rus",
      "T. Konstantin Rusch"
    ],
    "abstract": "How much does a trained RL policy actually use its past observations? We propose \\emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\\partial y_s/\\partial x_t\\in\\mathbb{R}^{c\\times d}$ averaged over final timesteps $s\\in\\{t+1,\\dots,T\\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06204v1",
    "published_date": "2025-12-05 22:58:09 UTC",
    "updated_date": "2025-12-05 22:58:09 UTC"
  },
  {
    "arxiv_id": "2512.15735v4",
    "title": "Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming",
    "authors": [
      "Ningwei Bai",
      "Chi Pui Chan",
      "Qichen Yin",
      "Tengyang Gong",
      "Yunda Yan",
      "Zezhi Tang"
    ],
    "abstract": "This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.",
    "categories": [
      "math.OC",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": "9 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.15735v4",
    "published_date": "2025-12-05 22:52:22 UTC",
    "updated_date": "2025-12-30 13:01:10 UTC"
  },
  {
    "arxiv_id": "2512.06196v1",
    "title": "ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment",
    "authors": [
      "Charlie Masters",
      "Marta Grześkiewicz",
      "Stefano V. Albrecht"
    ],
    "abstract": "As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the AAAI 2026 LLAMAS Workshop (Large Language Model Agents for Multi-Agent Systems)",
    "pdf_url": "https://arxiv.org/pdf/2512.06196v1",
    "published_date": "2025-12-05 22:39:54 UTC",
    "updated_date": "2025-12-05 22:39:54 UTC"
  },
  {
    "arxiv_id": "2512.06193v4",
    "title": "Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots",
    "authors": [
      "Jihyung Park",
      "Saleh Afroogh",
      "David Atkinson",
      "Junfeng Jiao"
    ],
    "abstract": "Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \\textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06193v4",
    "published_date": "2025-12-05 22:28:04 UTC",
    "updated_date": "2026-01-21 19:09:58 UTC"
  },
  {
    "arxiv_id": "2512.06190v1",
    "title": "Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying",
    "authors": [
      "Shichen Li",
      "Ahmadreza Eslaminia",
      "Chenhui Shao"
    ],
    "abstract": "Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06190v1",
    "published_date": "2025-12-05 22:17:25 UTC",
    "updated_date": "2025-12-05 22:17:25 UTC"
  },
  {
    "arxiv_id": "2512.11870v2",
    "title": "Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT",
    "authors": [
      "Mulham Fawakherji",
      "Bruce Race",
      "Driss Benhaddou"
    ],
    "abstract": "Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11870v2",
    "published_date": "2025-12-05 22:15:24 UTC",
    "updated_date": "2025-12-16 16:55:38 UTC"
  },
  {
    "arxiv_id": "2512.07901v2",
    "title": "The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators",
    "authors": [
      "Kevin Vallier"
    ],
    "abstract": "Von Neumann founded both game theory and the theory of self-reproducing automata, but the two programs never merged. This paper provides the synthesis. The Theory of Strategic Evolution analyzes strategic replicators: entities that optimize under resource constraints and spawn copies of themselves. We introduce Games with Endogenous Players (GEPs), where lineages (not instances) are the fundamental strategic units, and define Evolutionarily Stable Distributions of Intelligence (ESDIs) as the resulting equilibrium concept.\n  The central mathematical object is a hierarchy of strategic layers linked by cross-level gain matrices. Under a small-gain condition (spectral radius less than one), the system admits a global Lyapunov function at every finite depth. We prove closure under meta-selection: adding governance levels, innovation, or constitutional evolution preserves the dynamical structure. The Alignment Impossibility Theorem shows that unrestricted self-modification destroys this structure; stable alignment requires bounded modification classes.\n  Applications include AI deployment dynamics, market concentration, and institutional design. The framework shows why personality engineering fails under selection pressure and identifies constitutional constraints necessary for stable multi-agent systems.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "econ.TH"
    ],
    "primary_category": "cs.GT",
    "comment": "Draft manuscript, 30k words. Companion to Agentic Capital. Submitted to establish priority",
    "pdf_url": "https://arxiv.org/pdf/2512.07901v2",
    "published_date": "2025-12-05 21:58:03 UTC",
    "updated_date": "2025-12-15 23:19:48 UTC"
  },
  {
    "arxiv_id": "2512.06172v1",
    "title": "DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification",
    "authors": [
      "Sheng Liu",
      "Panos Papadimitratos"
    ],
    "abstract": "Federated Learning (FL) has drawn the attention of the Intelligent Transportation Systems (ITS) community. FL can train various models for ITS tasks, notably camera-based Road Condition Classification (RCC), in a privacy-preserving collaborative way. However, opening up to collaboration also opens FL-based RCC systems to adversaries, i.e., misbehaving participants that can launch Targeted Label-Flipping Attacks (TLFAs) and threaten transportation safety. Adversaries mounting TLFAs poison training data to misguide model predictions, from an actual source class (e.g., wet road) to a wrongly perceived target class (e.g., dry road). Existing countermeasures against poisoning attacks cannot maintain model performance under TLFAs close to the performance level in attack-free scenarios, because they lack specific model misbehavior detection for TLFAs and neglect client exclusion after the detection. To close this research gap, we propose DEFEND, which includes a poisoned model detection strategy that leverages neuron-wise magnitude analysis for attack goal identification and Gaussian Mixture Model (GMM)-based clustering. DEFEND discards poisoned model contributions in each round and adapts accordingly client ratings, eventually excluding malicious clients. Extensive evaluation involving various FL-RCC models and tasks shows that DEFEND can thwart TLFAs and outperform seven baseline countermeasures, with at least 15.78% improvement, with DEFEND remarkably achieving under attack the same performance as in attack-free scenarios.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to the 41st ACM/SIGAPP Symposium on Applied Computing (SAC 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.06172v1",
    "published_date": "2025-12-05 21:50:27 UTC",
    "updated_date": "2025-12-05 21:50:27 UTC"
  },
  {
    "arxiv_id": "2512.06161v1",
    "title": "Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach",
    "authors": [
      "Gondy Leroy",
      "Prakash Bisht",
      "Sai Madhuri Kandula",
      "Nell Maltman",
      "Sydney Rice"
    ],
    "abstract": "Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2512.06161v1",
    "published_date": "2025-12-05 21:14:59 UTC",
    "updated_date": "2025-12-05 21:14:59 UTC"
  },
  {
    "arxiv_id": "2512.06154v1",
    "title": "Learning Invariant Graph Representations Through Redundant Information",
    "authors": [
      "Barproda Halder",
      "Pasan Dissanayake",
      "Sanghamitra Dutta"
    ],
    "abstract": "Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06154v1",
    "published_date": "2025-12-05 21:07:11 UTC",
    "updated_date": "2025-12-05 21:07:11 UTC"
  },
  {
    "arxiv_id": "2512.06134v1",
    "title": "Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting",
    "authors": [
      "Georgi Hrusanov",
      "Duy-Thanh Vu",
      "Duy-Cat Can",
      "Sophie Tascedda",
      "Margaret Ryan",
      "Julien Bodelet",
      "Katarzyna Koscielska",
      "Carsten Magnus",
      "Oliver Y. Chén"
    ],
    "abstract": "Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06134v1",
    "published_date": "2025-12-05 20:29:01 UTC",
    "updated_date": "2025-12-05 20:29:01 UTC"
  },
  {
    "arxiv_id": "2512.06123v1",
    "title": "Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples",
    "authors": [
      "Qilin Zhou",
      "Zhengyuan Wei",
      "Haipeng Wang",
      "Zhuo Wang",
      "W. K. Chan"
    ],
    "abstract": "Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "accepted by IEEE Transactions on Reliability; extended technical report",
    "pdf_url": "https://arxiv.org/pdf/2512.06123v1",
    "published_date": "2025-12-05 20:02:09 UTC",
    "updated_date": "2025-12-05 20:02:09 UTC"
  },
  {
    "arxiv_id": "2512.15733v1",
    "title": "A Context-Free Smart Grid Model Using Complex System Approach",
    "authors": [
      "Soufian Ben Amor",
      "Alain Bui",
      "Guillaume Guerard"
    ],
    "abstract": "Energy and pollution are urging problems of the 21th century. By gradually changing the actual power grid system, smart grid may evolve into different systems by means of size, elements and strategies, but its fundamental requirements and objectives will not change such as optimizing production, transmission, and consumption. Studying the smart grid through modeling and simulation provides us with valuable results which cannot be obtained in real world due to time and cost related constraints. Moreover, due to the complexity of the smart grid, achieving global optimization is not an easy task. In this paper, we propose a complex system based approach to the smart grid modeling, accentuating on the optimization by combining game theoretical and classical methods in different levels. Thanks to this combination, the optimization can be achieved with flexibility and scalability, while keeping its generality.",
    "categories": [
      "math.OC",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.15733v1",
    "published_date": "2025-12-05 19:53:30 UTC",
    "updated_date": "2025-12-05 19:53:30 UTC"
  },
  {
    "arxiv_id": "2512.06112v2",
    "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving",
    "authors": [
      "Yifang Xu",
      "Jiahao Cui",
      "Feipeng Cai",
      "Zhihao Zhu",
      "Hanlin Shang",
      "Shan Luan",
      "Mingwang Xu",
      "Neng Zhang",
      "Yaoyi Li",
      "Jia Cai",
      "Siyu Zhu"
    ],
    "abstract": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "18 pages, 11 figures. Code & Model: https://github.com/fudan-generative-vision/WAM-Flow",
    "pdf_url": "https://arxiv.org/pdf/2512.06112v2",
    "published_date": "2025-12-05 19:36:46 UTC",
    "updated_date": "2025-12-11 16:06:13 UTC"
  },
  {
    "arxiv_id": "2512.06106v1",
    "title": "Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity",
    "authors": [
      "Constanze Albrecht",
      "Chayapatr Archiwaranguprok",
      "Rachel Poonsiriwong",
      "Awu Chen",
      "Peggy Yin",
      "Monchai Lertsutthiwong",
      "Kavin Winson",
      "Hal Hershfield",
      "Pattie Maes",
      "Pat Pataranutaporn"
    ],
    "abstract": "What if users could meet their future selves today? AI-generated future selves simulate meaningful encounters with a digital twin decades in the future. As AI systems advance, combining cloned voices, age-progressed facial rendering, and autobiographical narratives, a central question emerges: Does the modality of these future selves alter their psychological and affective impact? How might a text-based chatbot, a voice-only system, or a photorealistic avatar shape present-day decisions and our feeling of connection to the future? We report a randomized controlled study (N=92) evaluating three modalities of AI-generated future selves (text, voice, avatar) against a neutral control condition. We also report a systematic model evaluation between Claude 4 and three other Large Language Models (LLMs), assessing Claude 4 across psychological and interaction dimensions and establishing conversational AI quality as a critical determinant of intervention effectiveness. All personalized modalities strengthened Future Self-Continuity (FSC), emotional well-being, and motivation compared to control, with avatar producing the largest vividness gains, yet with no significant differences between formats. Interaction quality metrics, particularly persuasiveness, realism, and user engagement, emerged as robust predictors of psychological and affective outcomes, indicating that how compelling the interaction feels matters more than the form it takes. Content analysis found thematic patterns: text emphasized career planning, while voice and avatar facilitated personal reflection. Claude 4 outperformed ChatGPT 3.5, Llama 4, and Qwen 3 in enhancing psychological, affective, and FSC outcomes.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06106v1",
    "published_date": "2025-12-05 19:24:18 UTC",
    "updated_date": "2025-12-05 19:24:18 UTC"
  },
  {
    "arxiv_id": "2512.06105v1",
    "title": "Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation",
    "authors": [
      "Junwen Zheng",
      "Xinran Xu",
      "Li Rong Wang",
      "Chang Cai",
      "Lucinda Siyun Tan",
      "Dingyuan Wang",
      "Hong Liang Tey",
      "Xiuyi Fan"
    ],
    "abstract": "Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI-26-AIA",
    "pdf_url": "https://arxiv.org/pdf/2512.06105v1",
    "published_date": "2025-12-05 19:19:36 UTC",
    "updated_date": "2025-12-05 19:19:36 UTC"
  },
  {
    "arxiv_id": "2512.06102v1",
    "title": "JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning",
    "authors": [
      "Ufuk Çakır",
      "Victor-Alexandru Darvariu",
      "Bruno Lacerda",
      "Nick Hawes"
    ],
    "abstract": "Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To be presented at the NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences (ML4PS)",
    "pdf_url": "https://arxiv.org/pdf/2512.06102v1",
    "published_date": "2025-12-05 19:06:07 UTC",
    "updated_date": "2025-12-05 19:06:07 UTC"
  },
  {
    "arxiv_id": "2512.06097v1",
    "title": "Empathy by Design: Aligning Large Language Models for Healthcare Dialogue",
    "authors": [
      "Emre Umucu",
      "Guillermina Solis",
      "Leon Garza",
      "Emilia Rivas",
      "Beatrice Lee",
      "Anantaa Kotal",
      "Aritran Piplai"
    ],
    "abstract": "General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06097v1",
    "published_date": "2025-12-05 19:04:28 UTC",
    "updated_date": "2025-12-05 19:04:28 UTC"
  },
  {
    "arxiv_id": "2512.05967v1",
    "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms",
    "authors": [
      "Francesco Granata",
      "Francesco Poggi",
      "Misael Mongiovì"
    ],
    "abstract": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05967v1",
    "published_date": "2025-12-05 18:59:18 UTC",
    "updated_date": "2025-12-05 18:59:18 UTC"
  },
  {
    "arxiv_id": "2512.05964v2",
    "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking",
    "authors": [
      "Kevin Black",
      "Allen Z. Ren",
      "Michael Equi",
      "Sergey Levine"
    ],
    "abstract": "Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05964v2",
    "published_date": "2025-12-05 18:57:28 UTC",
    "updated_date": "2025-12-09 01:07:28 UTC"
  },
  {
    "arxiv_id": "2512.06065v1",
    "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
    "authors": [
      "Runjia Li",
      "Moayed Haji-Ali",
      "Ashkan Mirzaei",
      "Chaoyang Wang",
      "Arpit Sahni",
      "Ivan Skorokhodov",
      "Aliaksandr Siarohin",
      "Tomas Jakab",
      "Junlin Han",
      "Sergey Tulyakov",
      "Philip Torr",
      "Willi Menapace"
    ],
    "abstract": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://snap-research.github.io/EgoEdit",
    "pdf_url": "https://arxiv.org/pdf/2512.06065v1",
    "published_date": "2025-12-05 18:57:05 UTC",
    "updated_date": "2025-12-05 18:57:05 UTC"
  },
  {
    "arxiv_id": "2512.05962v1",
    "title": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity",
    "authors": [
      "Germán Kruszewski",
      "Pierre Erbacher",
      "Jos Rozen",
      "Marc Dymetman"
    ],
    "abstract": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05962v1",
    "published_date": "2025-12-05 18:56:40 UTC",
    "updated_date": "2025-12-05 18:56:40 UTC"
  },
  {
    "arxiv_id": "2512.05960v1",
    "title": "AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement",
    "authors": [
      "Munsif Ali",
      "Najmul Hassan",
      "Lucia Ventura",
      "Davide Di Bari",
      "Simonepietro Canese"
    ],
    "abstract": "Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05960v1",
    "published_date": "2025-12-05 18:56:10 UTC",
    "updated_date": "2025-12-05 18:56:10 UTC"
  },
  {
    "arxiv_id": "2512.05959v1",
    "title": "M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG",
    "authors": [
      "David Anugraha",
      "Patrick Amadeus Irawan",
      "Anshul Singh",
      "En-Shiun Annie Lee",
      "Genta Indra Winata"
    ],
    "abstract": "Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2512.05959v1",
    "published_date": "2025-12-05 18:55:58 UTC",
    "updated_date": "2025-12-05 18:55:58 UTC"
  },
  {
    "arxiv_id": "2512.05958v1",
    "title": "MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution",
    "authors": [
      "Sara Patel",
      "Mingxun Zhou",
      "Giulia Fanti"
    ],
    "abstract": "Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05958v1",
    "published_date": "2025-12-05 18:54:21 UTC",
    "updated_date": "2025-12-05 18:54:21 UTC"
  },
  {
    "arxiv_id": "2512.06062v1",
    "title": "When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models",
    "authors": [
      "S. M. Mustaqim",
      "Anantaa Kotal",
      "Paul H. Yi"
    ],
    "abstract": "Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06062v1",
    "published_date": "2025-12-05 18:52:35 UTC",
    "updated_date": "2025-12-05 18:52:35 UTC"
  },
  {
    "arxiv_id": "2512.05954v1",
    "title": "SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code",
    "authors": [
      "Shima Imani",
      "Seungwhan Moon",
      "Adel Ahmadyan",
      "Lu Zhang",
      "Kirmani Ahmed",
      "Babak Damavandi"
    ],
    "abstract": "We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05954v1",
    "published_date": "2025-12-05 18:50:48 UTC",
    "updated_date": "2025-12-05 18:50:48 UTC"
  },
  {
    "arxiv_id": "2512.05951v2",
    "title": "Trusted AI Agents in the Cloud",
    "authors": [
      "Teofil Bodea",
      "Masanori Misono",
      "Julian Pritzi",
      "Patrick Sabanic",
      "Thore Sommer",
      "Harshavardhan Unnibhavi",
      "David Schall",
      "Nuno Santos",
      "Dimitrios Stavrakakis",
      "Pramod Bhatotia"
    ],
    "abstract": "AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05951v2",
    "published_date": "2025-12-05 18:48:53 UTC",
    "updated_date": "2025-12-13 15:17:41 UTC"
  },
  {
    "arxiv_id": "2512.05950v1",
    "title": "Impugan: Learning Conditional Generative Models for Robust Data Imputation",
    "authors": [
      "Zalish Mahmud",
      "Anantaa Kotal",
      "Aritran Piplai"
    ],
    "abstract": "Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\\% lower Earth Mover's Distance (EMD) and 70\\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05950v1",
    "published_date": "2025-12-05 18:46:33 UTC",
    "updated_date": "2025-12-05 18:46:33 UTC"
  },
  {
    "arxiv_id": "2512.05946v1",
    "title": "Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem",
    "authors": [
      "Truong Thanh Hung Nguyen",
      "Truong Thinh Nguyen",
      "Hung Cao"
    ],
    "abstract": "Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Quantum Software Engineering Practices at The 41st ACM/SIGAPP Symposium On Applied Computing (SAC 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.05946v1",
    "published_date": "2025-12-05 18:43:18 UTC",
    "updated_date": "2025-12-05 18:43:18 UTC"
  },
  {
    "arxiv_id": "2512.05943v3",
    "title": "TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models",
    "authors": [
      "Shima Imani",
      "Seungwhan Moon",
      "Lambert Mathias",
      "Lu Zhang",
      "Babak Damavandi"
    ],
    "abstract": "Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05943v3",
    "published_date": "2025-12-05 18:40:18 UTC",
    "updated_date": "2025-12-11 19:26:00 UTC"
  },
  {
    "arxiv_id": "2512.05941v1",
    "title": "Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding",
    "authors": [
      "Zhiyuan Jiang",
      "Shenghao Xie",
      "Wenyi Li",
      "Wenqiang Zu",
      "Peihang Li",
      "Jiahao Qiu",
      "Siqi Pei",
      "Lei Ma",
      "Tiejun Huang",
      "Mengdi Wang",
      "Shilong Liu"
    ],
    "abstract": "Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at https://github.com/Princeton-AI2-Lab/ZoomClick",
    "pdf_url": "https://arxiv.org/pdf/2512.05941v1",
    "published_date": "2025-12-05 18:39:12 UTC",
    "updated_date": "2025-12-05 18:39:12 UTC"
  },
  {
    "arxiv_id": "2512.14704v1",
    "title": "Tourists Profiling by Interest Analysis",
    "authors": [
      "Sonia Djebali",
      "Quentin Gabot",
      "Guillaume Guerard"
    ],
    "abstract": "With the recent digital revolution, analyzing of tourists' behaviors and research fields associated with it have changed profoundly. It is now easier to examine behaviors of tourists using digital traces they leave during their travels. The studies conducted on diverse aspects of tourism focus on quantitative aspects of digital traces to reach its conclusions. In this paper, we suggest a study focused on both qualitative and quantitative aspect of digital traces to understand the dynamics governing tourist behavior, especially those concerning attractions networks.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.14704v1",
    "published_date": "2025-12-05 18:35:49 UTC",
    "updated_date": "2025-12-05 18:35:49 UTC"
  },
  {
    "arxiv_id": "2512.05937v1",
    "title": "Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception",
    "authors": [
      "Anne Sielemann",
      "Valentin Barner",
      "Stefan Wolf",
      "Masoud Roschani",
      "Jens Ziehn",
      "Juergen Beyerer"
    ],
    "abstract": "Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...].\n  Download: synset.de/datasets/synset-signset-ger/background-effect",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 2 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2512.05937v1",
    "published_date": "2025-12-05 18:25:52 UTC",
    "updated_date": "2025-12-05 18:25:52 UTC"
  },
  {
    "arxiv_id": "2512.05930v1",
    "title": "PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation",
    "authors": [
      "Shima Imani",
      "Seungwhan Moon",
      "Adel Ahmadyan",
      "Lu Zhang",
      "Kirmani Ahmed",
      "Babak Damavandi"
    ],
    "abstract": "Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05930v1",
    "published_date": "2025-12-05 18:14:55 UTC",
    "updated_date": "2025-12-05 18:14:55 UTC"
  },
  {
    "arxiv_id": "2512.05927v1",
    "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
    "authors": [
      "Zhiting Mei",
      "Tenny Yin",
      "Micah Baker",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05927v1",
    "published_date": "2025-12-05 18:06:18 UTC",
    "updated_date": "2025-12-05 18:06:18 UTC"
  },
  {
    "arxiv_id": "2512.05925v1",
    "title": "To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis",
    "authors": [
      "Federico Bianchi",
      "Yongchan Kwon",
      "Zachary Izzo",
      "Linjun Zhang",
      "James Zou"
    ],
    "abstract": "How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05925v1",
    "published_date": "2025-12-05 18:04:10 UTC",
    "updated_date": "2025-12-05 18:04:10 UTC"
  },
  {
    "arxiv_id": "2512.06060v1",
    "title": "Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring",
    "authors": [
      "Mohanakrishnan Hariharan"
    ],
    "abstract": "This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06060v1",
    "published_date": "2025-12-05 17:52:26 UTC",
    "updated_date": "2025-12-05 17:52:26 UTC"
  },
  {
    "arxiv_id": "2512.05908v1",
    "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures",
    "authors": [
      "Amirkia Rafiei Oskooei",
      "S. Selcan Yukcu",
      "Mehmet Cevheri Bozoglan",
      "Mehmet S. Aktas"
    ],
    "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted at LLM4Code Workshop, ICSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.05908v1",
    "published_date": "2025-12-05 17:42:09 UTC",
    "updated_date": "2025-12-05 17:42:09 UTC"
  },
  {
    "arxiv_id": "2512.15729v2",
    "title": "TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge",
    "authors": [
      "Matteo Fasulo",
      "Giusy Spacone",
      "Thorir Mar Ingolfsson",
      "Yawei Li",
      "Luca Benini",
      "Andrea Cossettini"
    ],
    "abstract": "Objective: Surface electromyography (EMG) is a non-invasive sensing modality widely used in biomechanics, rehabilitation, prosthetic control, and human-machine interfaces. Despite decades of use, achieving robust generalization across subjects, recording systems, and acquisition protocols remains challenging. While foundation models (FMs) are gaining traction for EMG, existing approaches remain limited to single downstream tasks and lack deployability on embedded platforms. This work addresses these limitations. Methods: We present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner using masked reconstruction on publicly available datasets. With only 3.6M parameters, TinyMyo is designed to support multiple downstream tasks through minimal task-specific head adaptations. Results: We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and speech recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 (89.4%), UCI-EMG (97.56%), and EPN-612 (96.74%) datasets. We demonstrate the first-time deployment of an EMG FM on an ultra-low power microcontroller (GAP9), with an inference time of 0.785 s, energy of 44.91 mJ and power envelope of 57.18 mW. Conclusion: TinyMyo demonstrates that compact, self-supervised EMG FM can guarantee strong generalization across multiple downstream tasks while remaining compatible with low-power edge devices. Significance: TinyMyo is the first EMG FM for ultra-low power edge devices, enabling scalable and energy-efficient sensing for motor intent decoding, neuromuscular assessment, and biosignal driven human-machine interaction.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.15729v2",
    "published_date": "2025-12-05 17:36:57 UTC",
    "updated_date": "2026-01-15 18:05:25 UTC"
  },
  {
    "arxiv_id": "2512.05880v1",
    "title": "Neural Coherence : Find higher performance to out-of-distribution tasks from few samples",
    "authors": [
      "Simon Guiroy",
      "Mats Richter",
      "Sarath Chandar",
      "Christopher Pal"
    ],
    "abstract": "To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05880v1",
    "published_date": "2025-12-05 16:55:41 UTC",
    "updated_date": "2025-12-05 16:55:41 UTC"
  },
  {
    "arxiv_id": "2512.15728v1",
    "title": "FedSight AI: Multi-Agent System Architecture for Federal Funds Target Rate Prediction",
    "authors": [
      "Yuhan Hou",
      "Tianji Rao",
      "Jeremy Tan",
      "Adler Viton",
      "Xiyue Zhang",
      "David Ye",
      "Abhishek Kodi",
      "Sanjana Dulam",
      "Aditya Paul",
      "Yikai Feng"
    ],
    "abstract": "The Federal Open Market Committee (FOMC) sets the federal funds rate, shaping monetary policy and the broader economy. We introduce \\emph{FedSight AI}, a multi-agent framework that uses large language models (LLMs) to simulate FOMC deliberations and predict policy outcomes. Member agents analyze structured indicators and unstructured inputs such as the Beige Book, debate options, and vote, replicating committee reasoning. A Chain-of-Draft (CoD) extension further improves efficiency and accuracy by enforcing concise multistage reasoning. Evaluated at 2023-2024 meetings, FedSight CoD achieved accuracy of 93.75\\% and stability of 93.33\\%, outperforming baselines including MiniFed and Ordinal Random Forest (RF), while offering transparent reasoning aligned with real FOMC communications.",
    "categories": [
      "q-fin.GN",
      "cs.AI"
    ],
    "primary_category": "q-fin.GN",
    "comment": "NeurIPS 2025 Generative AI in Finance Workshop",
    "pdf_url": "https://arxiv.org/pdf/2512.15728v1",
    "published_date": "2025-12-05 16:45:18 UTC",
    "updated_date": "2025-12-05 16:45:18 UTC"
  },
  {
    "arxiv_id": "2512.05865v1",
    "title": "Sparse Attention Post-Training for Mechanistic Interpretability",
    "authors": [
      "Florent Draye",
      "Anson Lei",
      "Ingmar Posner",
      "Bernhard Schölkopf"
    ],
    "abstract": "We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\\approx 0.3 \\%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05865v1",
    "published_date": "2025-12-05 16:40:08 UTC",
    "updated_date": "2025-12-05 16:40:08 UTC"
  },
  {
    "arxiv_id": "2512.05863v1",
    "title": "Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework",
    "authors": [
      "Tasnimul Hassan",
      "Md Faisal Karim",
      "Haziq Jeelani",
      "Elham Behnam",
      "Robert Green",
      "Fayeq Jeelani Syed"
    ],
    "abstract": "Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05863v1",
    "published_date": "2025-12-05 16:38:47 UTC",
    "updated_date": "2025-12-05 16:38:47 UTC"
  },
  {
    "arxiv_id": "2512.13705v1",
    "title": "Scaling and Transferability of Annealing Strategies in Large Language Model Training",
    "authors": [
      "Siqi Wang",
      "Zhengyu Chen",
      "Teng Xiao",
      "Zheqi Lv",
      "Jinluan Yang",
      "Xunliang Cai",
      "Jingang Wang",
      "Xiaomeng Li"
    ],
    "abstract": "Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AAAI 2026 (camera-ready version)",
    "pdf_url": "https://arxiv.org/pdf/2512.13705v1",
    "published_date": "2025-12-05 16:38:33 UTC",
    "updated_date": "2025-12-05 16:38:33 UTC"
  },
  {
    "arxiv_id": "2512.05844v1",
    "title": "NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation",
    "authors": [
      "Daniel Rose",
      "Roxane Axel Jacob",
      "Johannes Kirchmair",
      "Thierry Langer"
    ],
    "abstract": "Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05844v1",
    "published_date": "2025-12-05 16:18:07 UTC",
    "updated_date": "2025-12-05 16:18:07 UTC"
  },
  {
    "arxiv_id": "2512.05836v1",
    "title": "Using Large Language Models to Create Personalized Networks From Therapy Sessions",
    "authors": [
      "Clarissa W. Ong",
      "Hiba Arnaout",
      "Kate Sheehan",
      "Estella Fox",
      "Eugen Owtscharow",
      "Iryna Gurevych"
    ],
    "abstract": "Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05836v1",
    "published_date": "2025-12-05 16:12:12 UTC",
    "updated_date": "2025-12-05 16:12:12 UTC"
  },
  {
    "arxiv_id": "2512.11868v1",
    "title": "Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models",
    "authors": [
      "Alexander Windmann",
      "Benedikt Stratmann",
      "Mariya Lyashenko",
      "Oliver Niggemann"
    ],
    "abstract": "Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11868v1",
    "published_date": "2025-12-05 16:11:53 UTC",
    "updated_date": "2025-12-05 16:11:53 UTC"
  },
  {
    "arxiv_id": "2512.08983v1",
    "title": "HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification",
    "authors": [
      "Maoyu Wang",
      "Yao Lu",
      "Bo Zhou",
      "Zhuangzhi Chen",
      "Yun Lin",
      "Qi Xuan",
      "Guan Gui"
    ],
    "abstract": "With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\\%$ parameter reduction and $84.44\\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.08983v1",
    "published_date": "2025-12-05 16:03:53 UTC",
    "updated_date": "2025-12-05 16:03:53 UTC"
  },
  {
    "arxiv_id": "2512.05830v1",
    "title": "Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning",
    "authors": [
      "Muhammet Cagri Yeke",
      "Samil Sirin",
      "Kivilcim Yuksel",
      "Abdurrahman Gumus"
    ],
    "abstract": "This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: https://github.com/miralab-ai/Phase-OTDR-event-detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 11 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2512.05830v1",
    "published_date": "2025-12-05 15:52:40 UTC",
    "updated_date": "2025-12-05 15:52:40 UTC"
  },
  {
    "arxiv_id": "2601.04202v1",
    "title": "TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation",
    "authors": [
      "Anas Ezzakri",
      "Nicola Piovesan",
      "Mohamed Sana",
      "Antonio De Domenico",
      "Fadhel Ayed",
      "Haozhe Zhang"
    ],
    "abstract": "Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.04202v1",
    "published_date": "2025-12-05 15:47:00 UTC",
    "updated_date": "2025-12-05 15:47:00 UTC"
  },
  {
    "arxiv_id": "2601.04201v1",
    "title": "Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems",
    "authors": [
      "Zihan Gao",
      "Mohsin Y. K. Yousufi",
      "Jacob Thebault-Spieker"
    ],
    "abstract": "Large language model (LLM) question-answering systems often fail on community-specific queries, creating \"knowledge blind spots\" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 2 figures, Presented at the NeurIPS 2025 ACA Workshop https://acaworkshop.github.io/accepted-papers.html,",
    "pdf_url": "https://arxiv.org/pdf/2601.04201v1",
    "published_date": "2025-12-05 15:44:54 UTC",
    "updated_date": "2025-12-05 15:44:54 UTC"
  },
  {
    "arxiv_id": "2512.05825v1",
    "title": "Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization",
    "authors": [
      "Shuhei Watanabe"
    ],
    "abstract": "Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\\lfloor \\frac{M + 1}{2} \\rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05825v1",
    "published_date": "2025-12-05 15:43:06 UTC",
    "updated_date": "2025-12-05 15:43:06 UTC"
  },
  {
    "arxiv_id": "2512.05824v1",
    "title": "Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma",
    "authors": [
      "Hafsa Akebli",
      "Adam Shephard",
      "Vincenzo Della Mea",
      "Nasir Rajpoot"
    ],
    "abstract": "Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.05824v1",
    "published_date": "2025-12-05 15:43:02 UTC",
    "updated_date": "2025-12-05 15:43:02 UTC"
  },
  {
    "arxiv_id": "2512.05809v1",
    "title": "Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling",
    "authors": [
      "Saurav Jha",
      "M. Jehanzeb Mirza",
      "Wei Lin",
      "Shiqi Yang",
      "Sarath Chandar"
    ],
    "abstract": "Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Extended abstract at World Modeling Workshop 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.05809v1",
    "published_date": "2025-12-05 15:30:08 UTC",
    "updated_date": "2025-12-05 15:30:08 UTC"
  },
  {
    "arxiv_id": "2512.05803v1",
    "title": "3D Path Planning for Robot-assisted Vertebroplasty from Arbitrary Bi-plane X-ray via Differentiable Rendering",
    "authors": [
      "Blanca Inigo",
      "Benjamin D. Killeen",
      "Rebecca Choi",
      "Michelle Song",
      "Ali Uneri",
      "Majid Khan",
      "Christopher Bailey",
      "Axel Krieger",
      "Mathias Unberath"
    ],
    "abstract": "Robotic systems are transforming image-guided interventions by enhancing accuracy and minimizing radiation exposure. A significant challenge in robotic assistance lies in surgical path planning, which often relies on the registration of intraoperative 2D images with preoperative 3D CT scans. This requirement can be burdensome and costly, particularly in procedures like vertebroplasty, where preoperative CT scans are not routinely performed. To address this issue, we introduce a differentiable rendering-based framework for 3D transpedicular path planning utilizing bi-planar 2D X-rays. Our method integrates differentiable rendering with a vertebral atlas generated through a Statistical Shape Model (SSM) and employs a learned similarity loss to refine the SSM shape and pose dynamically, independent of fixed imaging geometries. We evaluated our framework in two stages: first, through vertebral reconstruction from orthogonal X-rays for benchmarking, and second, via clinician-in-the-loop path planning using arbitrary-view X-rays. Our results indicate that our method outperformed a normalized cross-correlation baseline in reconstruction metrics (DICE: 0.75 vs. 0.65) and achieved comparable performance to the state-of-the-art model ReVerteR (DICE: 0.77), while maintaining generalization to arbitrary views. Success rates for bipedicular planning reached 82% with synthetic data and 75% with cadaver data, exceeding the 66% and 31% rates of a 2D-to-3D baseline, respectively. In conclusion, our framework facilitates versatile, CT-free 3D path planning for robot-assisted vertebroplasty, effectively accommodating real-world imaging diversity without the need for preoperative CT scans.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05803v1",
    "published_date": "2025-12-05 15:26:13 UTC",
    "updated_date": "2025-12-05 15:26:13 UTC"
  },
  {
    "arxiv_id": "2512.05794v1",
    "title": "Mechanistic Interpretability of Antibody Language Models Using SAEs",
    "authors": [
      "Rebonto Haque",
      "Oliver M. Turnbull",
      "Anisha Parsan",
      "Nithin Parsan",
      "John J. Yang",
      "Charlotte M. Deane"
    ],
    "abstract": "Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05794v1",
    "published_date": "2025-12-05 15:18:50 UTC",
    "updated_date": "2025-12-05 15:18:50 UTC"
  },
  {
    "arxiv_id": "2512.11867v1",
    "title": "On the Dangers of Bootstrapping Generation for Continual Learning and Beyond",
    "authors": [
      "Daniil Zverev",
      "A. Sophia Koepke",
      "Joao F. Henriques"
    ],
    "abstract": "The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "DAGM German Conference on Pattern Recognition, 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.11867v1",
    "published_date": "2025-12-05 15:16:30 UTC",
    "updated_date": "2025-12-05 15:16:30 UTC"
  },
  {
    "arxiv_id": "2512.11865v1",
    "title": "Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation",
    "authors": [
      "Ju-Young Kim",
      "Ji-Hong Park",
      "Myeongjun Kim",
      "Gun-Woo Kim"
    ],
    "abstract": "Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to MobieSec 2025 (poster session)",
    "pdf_url": "https://arxiv.org/pdf/2512.11865v1",
    "published_date": "2025-12-05 15:06:18 UTC",
    "updated_date": "2025-12-05 15:06:18 UTC"
  },
  {
    "arxiv_id": "2512.05774v1",
    "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
    "authors": [
      "Ziyang Wang",
      "Honglu Zhou",
      "Shijie Wang",
      "Junnan Li",
      "Caiming Xiong",
      "Silvio Savarese",
      "Mohit Bansal",
      "Michael S. Ryoo",
      "Juan Carlos Niebles"
    ],
    "abstract": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Website: https://activevideoperception.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2512.05774v1",
    "published_date": "2025-12-05 15:03:48 UTC",
    "updated_date": "2025-12-05 15:03:48 UTC"
  },
  {
    "arxiv_id": "2512.05765v1",
    "title": "The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics",
    "authors": [
      "Edward Y. Chang"
    ],
    "abstract": "Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: \"mere pattern matchers\" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while \"reasoning\" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.05765v1",
    "published_date": "2025-12-05 14:51:17 UTC",
    "updated_date": "2025-12-05 14:51:17 UTC"
  },
  {
    "arxiv_id": "2512.05760v1",
    "title": "Evolutionary System 2 Reasoning: An Empirical Proof",
    "authors": [
      "Zeyuan Ma",
      "Wenqi Huang",
      "Guo-Huan Song",
      "Hongshu Guo",
      "Sijie Ma",
      "Zhiguang Cao",
      "Yue-Jiao Gong"
    ],
    "abstract": "Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05760v1",
    "published_date": "2025-12-05 14:47:57 UTC",
    "updated_date": "2025-12-05 14:47:57 UTC"
  },
  {
    "arxiv_id": "2512.05753v2",
    "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning",
    "authors": [
      "Wencheng Cai",
      "Xuchao Gao",
      "Congying Han",
      "Mingqiang Li",
      "Tiande Guo"
    ],
    "abstract": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05753v2",
    "published_date": "2025-12-05 14:39:50 UTC",
    "updated_date": "2026-01-05 08:40:11 UTC"
  },
  {
    "arxiv_id": "2512.05734v1",
    "title": "KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books",
    "authors": [
      "Jinfeng Zhong",
      "Emmanuel Bacry",
      "Agathe Guilloux",
      "Jean-François Muzy"
    ],
    "abstract": "This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05734v1",
    "published_date": "2025-12-05 14:15:02 UTC",
    "updated_date": "2025-12-05 14:15:02 UTC"
  },
  {
    "arxiv_id": "2512.05732v1",
    "title": "Efficient Text Classification with Conformal In-Context Learning",
    "authors": [
      "Ippokratis Pantelidis",
      "Korbinian Randl",
      "Aron Henriksson"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 tables, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.05732v1",
    "published_date": "2025-12-05 14:11:44 UTC",
    "updated_date": "2025-12-05 14:11:44 UTC"
  },
  {
    "arxiv_id": "2512.08982v1",
    "title": "Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement",
    "authors": [
      "Jian Xu",
      "Wei Chen",
      "Shigui Li",
      "Delu Zeng",
      "John Paisley",
      "Qibin Zhao"
    ],
    "abstract": "Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \\textit{unconditional synthesis}, their application to \\textit{conditional enhancement} remains unexplored. We present \\textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \\textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \\textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \\textbf{state-of-the-art performance with single-step sampling} (\\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \\textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.08982v1",
    "published_date": "2025-12-05 13:44:19 UTC",
    "updated_date": "2025-12-05 13:44:19 UTC"
  },
  {
    "arxiv_id": "2512.05714v1",
    "title": "Big Tech-Funded AI Papers Have Higher Citation Impact, Greater Insularity, and Larger Recency Bias",
    "authors": [
      "Max Martin Gnewuch",
      "Jan Philip Wahle",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "abstract": "Over the past four decades, artificial intelligence (AI) research has flourished at the nexus of academia and industry. However, Big Tech companies have increasingly acquired the edge in computational resources, big data, and talent. So far, it has been largely unclear how many papers the industry funds, how their citation impact compares to non-funded papers, and what drives industry interest. This study fills that gap by quantifying the number of industry-funded papers at 10 top AI conferences (e.g., ICLR, CVPR, AAAI, ACL) and their citation influence. We analyze about 49.8K papers, about 1.8M citations from AI papers to other papers, and about 2.3M citations from other papers to AI papers from 1998-2022 in Scopus. Through seven research questions, we examine the volume and evolution of industry funding in AI research, the citation impact of funded papers, the diversity and temporal range of their citations, and the subfields in which industry predominantly acts. Our findings reveal that industry presence has grown markedly since 2015, from less than 2 percent to more than 11 percent in 2020. Between 2018 and 2022, 12 percent of industry-funded papers achieved high citation rates as measured by the h5-index, compared to 4 percent of non-industry-funded papers and 2 percent of non-funded papers. Top AI conferences engage more with industry-funded research than non-funded research, as measured by our newly proposed metric, the Citation Preference Ratio (CPR). We show that industry-funded research is increasingly insular, citing predominantly other industry-funded papers while referencing fewer non-funded papers. These findings reveal new trends in AI research funding, including a shift towards more industry-funded papers and their growing citation impact, greater insularity of industry-funded work than non-funded work, and a preference of industry-funded research to cite recent work.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DL",
    "comment": "Published at IEEE (ACDSA)",
    "pdf_url": "https://arxiv.org/pdf/2512.05714v1",
    "published_date": "2025-12-05 13:41:29 UTC",
    "updated_date": "2025-12-05 13:41:29 UTC"
  },
  {
    "arxiv_id": "2512.05711v1",
    "title": "Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning",
    "authors": [
      "Ali Krayani",
      "Seyedeh Fatemeh Sadati",
      "Lucio Marcenaro",
      "Carlo Regazzoni"
    ],
    "abstract": "This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "This paper has been accepted for the 2026 IEEE Consumer Communications & Networking Conference (IEEE CCNC 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.05711v1",
    "published_date": "2025-12-05 13:38:52 UTC",
    "updated_date": "2025-12-05 13:38:52 UTC"
  },
  {
    "arxiv_id": "2512.05700v1",
    "title": "Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains",
    "authors": [
      "Ben Malin",
      "Tatiana Kalganova",
      "Nikolaos Boulgouris"
    ],
    "abstract": "We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, conference paper",
    "pdf_url": "https://arxiv.org/pdf/2512.05700v1",
    "published_date": "2025-12-05 13:28:29 UTC",
    "updated_date": "2025-12-05 13:28:29 UTC"
  },
  {
    "arxiv_id": "2512.05693v1",
    "title": "HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies",
    "authors": [
      "Zhiying Du",
      "Bei Liu",
      "Yaobo Liang",
      "Yichao Shen",
      "Haidong Cao",
      "Xiangyu Zheng",
      "Zhiyuan Feng",
      "Zuxuan Wu",
      "Jiaolong Yang",
      "Yu-Gang Jiang"
    ],
    "abstract": "The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05693v1",
    "published_date": "2025-12-05 13:21:05 UTC",
    "updated_date": "2025-12-05 13:21:05 UTC"
  },
  {
    "arxiv_id": "2512.05681v1",
    "title": "Retrieving Semantically Similar Decisions under Noisy Institutional Labels: Robust Comparison of Embedding Methods",
    "authors": [
      "Tereza Novotna",
      "Jakub Harasta"
    ],
    "abstract": "Retrieving case law is a time-consuming task predominantly carried out by querying databases. We provide a comparison of two models in three different settings for Czech Constitutional Court decisions: (i) a large general-purpose embedder (OpenAI), (ii) a domain-specific BERT-trained from scratch on ~30,000 decisions using sliding windows and attention pooling. We propose a noise-aware evaluation including IDF-weighted keyword overlap as graded relevance, binarization via two thresholds (0.20 balanced, 0.28 strict), significance via paired bootstrap, and an nDCG diagnosis supported with qualitative analysis. Despite modest absolute nDCG (expected under noisy labels), the general OpenAI embedder decisively outperforms the domain pre-trained BERT in both settings at @10/@20/@100 across both thresholds; differences are statistically significant. Diagnostics attribute low absolutes to label drift and strong ideals rather than lack of utility. Additionally, our framework is robust enough to be used for evaluation under a noisy gold dataset, which is typical when handling data with heterogeneous labels stemming from legacy judicial databases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The manuscript has been accepted for presentation as a short paper at the 38th International Conference on Legal Knowledge and Information Systems (JURIX 2025) in Torino, Italy",
    "pdf_url": "https://arxiv.org/pdf/2512.05681v1",
    "published_date": "2025-12-05 12:54:26 UTC",
    "updated_date": "2025-12-05 12:54:26 UTC"
  },
  {
    "arxiv_id": "2512.11864v1",
    "title": "Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars",
    "authors": [
      "Christoph Einspieler",
      "Matthias Horn",
      "Marie-Louise Lackner",
      "Patrick Malik",
      "Nysret Musliu",
      "Felix Winter"
    ],
    "abstract": "The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.",
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11864v1",
    "published_date": "2025-12-05 12:35:44 UTC",
    "updated_date": "2025-12-05 12:35:44 UTC"
  },
  {
    "arxiv_id": "2512.22129v1",
    "title": "ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling",
    "authors": [
      "Conor Wallace",
      "Umer Siddique",
      "Yongcan Cao"
    ],
    "abstract": "Ad-hoc teamwork (AHT) requires agents to infer the behavior of previously unseen teammates and adapt their policy accordingly. Conventional approaches often rely on fixed probabilistic models or classifiers, which can be brittle under partial observability and limited interaction. Large language models (LLMs) offer a flexible alternative: by mapping short behavioral traces into high-level hypotheses, they can serve as world models over teammate behavior. We introduce \\Collab, a language-based framework that classifies partner types using a behavior rubric derived from trajectory features, and extend it to \\ReCollab, which incorporates retrieval-augmented generation (RAG) to stabilize inference with exemplar trajectories. In the cooperative Overcooked environment, \\Collab effectively distinguishes teammate types, while \\ReCollab consistently improves adaptation across layouts, achieving Pareto-optimal trade-offs between classification accuracy and episodic return. These findings demonstrate the potential of LLMs as behavioral world models for AHT and highlight the importance of retrieval grounding in challenging coordination settings.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.22129v1",
    "published_date": "2025-12-05 12:33:11 UTC",
    "updated_date": "2025-12-05 12:33:11 UTC"
  },
  {
    "arxiv_id": "2512.05672v1",
    "title": "InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem",
    "authors": [
      "Yeobin Hong",
      "Suhyeon Lee",
      "Hyungjin Chung",
      "Jong Chul Ye"
    ],
    "abstract": "Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05672v1",
    "published_date": "2025-12-05 12:31:09 UTC",
    "updated_date": "2025-12-05 12:31:09 UTC"
  },
  {
    "arxiv_id": "2512.05667v1",
    "title": "On Dynamic Programming Theory for Leader-Follower Stochastic Games",
    "authors": [
      "Jilles Steeve Dibangoye",
      "Thibaut Le Marre",
      "Ocan Sankur",
      "François Schwarzentruber"
    ],
    "abstract": "Leader-follower general-sum stochastic games (LF-GSSGs) model sequential decision-making under asymmetric commitment, where a leader commits to a policy and a follower best responds, yielding a strong Stackelberg equilibrium (SSE) with leader-favourable tie-breaking. This paper introduces a dynamic programming (DP) framework that applies Bellman recursion over credible sets-state abstractions formally representing all rational follower best responses under partial leader commitments-to compute SSEs. We first prove that any LF-GSSG admits a lossless reduction to a Markov decision process (MDP) over credible sets. We further establish that synthesising an optimal memoryless deterministic leader policy is NP-hard, motivating the development of ε-optimal DP algorithms with provable guarantees on leader exploitability. Experiments on standard mixed-motive benchmarks-including security games, resource allocation, and adversarial planning-demonstrate empirical gains in leader value and runtime scalability over state-of-the-art methods.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "31 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.05667v1",
    "published_date": "2025-12-05 12:23:56 UTC",
    "updated_date": "2025-12-05 12:23:56 UTC"
  },
  {
    "arxiv_id": "2512.05666v1",
    "title": "Feasibility of AI-Assisted Programming for End-User Development",
    "authors": [
      "Irene Weber"
    ],
    "abstract": "End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and \"copilots\", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.05666v1",
    "published_date": "2025-12-05 12:13:21 UTC",
    "updated_date": "2025-12-05 12:13:21 UTC"
  },
  {
    "arxiv_id": "2512.05658v1",
    "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models",
    "authors": [
      "Pietro Ferrazzi",
      "Aitor Soroa",
      "Rodrigo Agerri"
    ],
    "abstract": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2512.05658v1",
    "published_date": "2025-12-05 12:05:46 UTC",
    "updated_date": "2025-12-05 12:05:46 UTC"
  },
  {
    "arxiv_id": "2512.05638v1",
    "title": "Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability",
    "authors": [
      "Suman Sanyal"
    ],
    "abstract": "Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \\emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \\emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \\emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05638v1",
    "published_date": "2025-12-05 11:30:46 UTC",
    "updated_date": "2025-12-05 11:30:46 UTC"
  },
  {
    "arxiv_id": "2512.07898v1",
    "title": "MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement",
    "authors": [
      "Hongwei Zhang",
      "Ji Lu",
      "Yongsheng Du",
      "Yanqin Gao",
      "Lingjun Huang",
      "Baoli Wang",
      "Fang Tan",
      "Peng Zou"
    ],
    "abstract": "Large Language Model (LLM)-based agents demonstrate advanced reasoning capabilities, yet practical constraints frequently limit outputs to single responses, leaving significant performance potential unrealized. This paper introduces MARINE (Multi-Agent Recursive IN-context Enhancement), a theoretically grounded framework that reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms. The MARINE refinement operator systematically converts a base model's pass@N capabilities into near-optimal pass@1 performance. Rigorous theoretical analysis establishes that minimal feasible batches maximize expected performance gains under fixed invocation budgets, while logarithmically growing batch schedules ensure continuous improvement without computational constraints. Comprehensive evaluation on the BrowserComp-ZH benchmark demonstrates state-of-the-art results, with a 685B-parameter implementation achieving 46.0% pass@1 accuracy. Meanwhile, MARINE establishes a new paradigm for parameter-efficient reasoning: an 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude. Notably, within a fixed computational budget, the proposed MARINE delivers higher-quality samples to alignment and optimization processes than traditional sampling-and-ranking strategies. Consequently, it has great potential to boost post-training efficiency.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.07898v1",
    "published_date": "2025-12-05 11:19:18 UTC",
    "updated_date": "2025-12-05 11:19:18 UTC"
  },
  {
    "arxiv_id": "2512.10985v1",
    "title": "Marti-5: A Mathematical Model of \"Self in the World\" as a First Step Toward Self-Awareness",
    "authors": [
      "Igor Pivovarov",
      "Sergey Shumsky"
    ],
    "abstract": "The existence of 'what' and 'where' pathways of information processing in the brain was proposed almost 30 years ago, but there is still a lack of a clear mathematical model that could show how these pathways work together. We propose a biologically inspired mathematical model that uses this idea to identify and separate the self from the environment and then build and use a self-model for better predictions. This is a model of neocortical columns governed by the basal ganglia to make predictions and choose the next action, where some columns act as 'what' columns and others act as 'where' columns. Based on this model, we present a reinforcement learning agent that learns purposeful behavior in a virtual environment. We evaluate the agent on the Atari games Pong and Breakout, where it successfully learns to play. We conclude that the ability to separate the self from the environment gives advantages to the agent and therefore such a model could appear in living organisms during evolution. We propose Self-Awareness Principle 1: the ability to separate the self from the world is a necessary but insufficient condition for self-awareness.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "21 pages, 2 figures, 2 videos, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2512.10985v1",
    "published_date": "2025-12-05 11:15:06 UTC",
    "updated_date": "2025-12-05 11:15:06 UTC"
  },
  {
    "arxiv_id": "2601.04200v1",
    "title": "Attribute-Aware Controlled Product Generation with LLMs for E-commerce",
    "authors": [
      "Virginia Negri",
      "Víctor Martínez Gómez",
      "Sergio A. Balanya",
      "Subburam Rajaram"
    ],
    "abstract": "Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI'26 Workshop on Shaping Responsible Synthetic Data in the Era of Foundation Models (RSD)",
    "pdf_url": "https://arxiv.org/pdf/2601.04200v1",
    "published_date": "2025-12-05 11:12:10 UTC",
    "updated_date": "2025-12-05 11:12:10 UTC"
  },
  {
    "arxiv_id": "2512.05619v1",
    "title": "Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting",
    "authors": [
      "Menghua Jiang",
      "Haokai Gao",
      "Shuhao Chen",
      "Yin Chen"
    ],
    "abstract": "Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ECAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.05619v1",
    "published_date": "2025-12-05 11:02:17 UTC",
    "updated_date": "2025-12-05 11:02:17 UTC"
  },
  {
    "arxiv_id": "2512.08981v1",
    "title": "Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding",
    "authors": [
      "Tahar Chettaoui",
      "Naser Damer",
      "Fadi Boutros"
    ],
    "abstract": "Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at BMVC workshop (SRBS) 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.08981v1",
    "published_date": "2025-12-05 10:41:58 UTC",
    "updated_date": "2025-12-05 10:41:58 UTC"
  },
  {
    "arxiv_id": "2512.05594v1",
    "title": "Ontology Learning with LLMs: A Benchmark Study on Axiom Identification",
    "authors": [
      "Roos M. Bakker",
      "Daan L. Di Scala",
      "Maaike H. T. de Boer",
      "Stephan A. Raaijmakers"
    ],
    "abstract": "Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to Semantic Web Journal, under review",
    "pdf_url": "https://arxiv.org/pdf/2512.05594v1",
    "published_date": "2025-12-05 10:28:56 UTC",
    "updated_date": "2025-12-05 10:28:56 UTC"
  },
  {
    "arxiv_id": "2512.06048v1",
    "title": "The Road of Adaptive AI for Precision in Cybersecurity",
    "authors": [
      "Sahil Garg"
    ],
    "abstract": "Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06048v1",
    "published_date": "2025-12-05 10:16:45 UTC",
    "updated_date": "2025-12-05 10:16:45 UTC"
  },
  {
    "arxiv_id": "2512.11863v1",
    "title": "Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence",
    "authors": [
      "Julian Schön",
      "Lena Hoffmann",
      "Nikolas Becker"
    ],
    "abstract": "Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11863v1",
    "published_date": "2025-12-05 10:15:06 UTC",
    "updated_date": "2025-12-05 10:15:06 UTC"
  },
  {
    "arxiv_id": "2512.08980v2",
    "title": "Training Multi-Image Vision Agents via End2End Reinforcement Learning",
    "authors": [
      "Chengqi Dong",
      "Chuhuai Yue",
      "Hang He",
      "Rongge Mao",
      "Fenghe Tang",
      "S Kevin Zhou",
      "Zekun Xu",
      "Xiaohan Wang",
      "Jiajun Chai",
      "Wei Lin",
      "Guojun Yin"
    ],
    "abstract": "Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images\" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.08980v2",
    "published_date": "2025-12-05 10:02:38 UTC",
    "updated_date": "2025-12-16 14:00:19 UTC"
  },
  {
    "arxiv_id": "2512.05579v1",
    "title": "A Comprehensive Framework for Automated Quality Control in the Automotive Industry",
    "authors": [
      "Panagiota Moraiti",
      "Panagiotis Giannikos",
      "Athanasios Mastrogeorgiou",
      "Panagiotis Mavridis",
      "Linghao Zhou",
      "Panagiotis Chatzakos"
    ],
    "abstract": "This paper presents a cutting-edge robotic inspection solution designed to automate quality control in automotive manufacturing. The system integrates a pair of collaborative robots, each equipped with a high-resolution camera-based vision system to accurately detect and localize surface and thread defects in aluminum high-pressure die casting (HPDC) automotive components. In addition, specialized lenses and optimized lighting configurations are employed to ensure consistent and high-quality image acquisition. The YOLO11n deep learning model is utilized, incorporating additional enhancements such as image slicing, ensemble learning, and bounding-box merging to significantly improve performance and minimize false detections. Furthermore, image processing techniques are applied to estimate the extent of the detected defects. Experimental results demonstrate real-time performance with high accuracy across a wide variety of defects, while minimizing false detections. The proposed solution is promising and highly scalable, providing the flexibility to adapt to various production environments and meet the evolving demands of the automotive industry.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05579v1",
    "published_date": "2025-12-05 09:59:10 UTC",
    "updated_date": "2025-12-05 09:59:10 UTC"
  },
  {
    "arxiv_id": "2512.05576v1",
    "title": "CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning",
    "authors": [
      "Ting-Ting Xie",
      "Yixin Zhang"
    ],
    "abstract": "Current clinical agent built on small LLMs, such as TxAgent suffer from a \\textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \\textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \\textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "2nd Place Solution to the CURE-Bench Competition @ NeurIPS 2025. Code available at https://github.com/June01/CureAgent",
    "pdf_url": "https://arxiv.org/pdf/2512.05576v1",
    "published_date": "2025-12-05 09:56:58 UTC",
    "updated_date": "2025-12-05 09:56:58 UTC"
  },
  {
    "arxiv_id": "2512.06046v1",
    "title": "Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework",
    "authors": [
      "Ramprasath Ganesaraja",
      "Swathika N",
      "Saravanan AP",
      "Kamalkumar Rathinasamy",
      "Chetana Amancharla",
      "Rahul Das",
      "Sahil Dilip Panse",
      "Aditya Batwe",
      "Dileep Vijayan",
      "Veena Ashok",
      "Thanushree A P",
      "Kausthubh J Rao",
      "Alden Olivero",
      "Roshan",
      "Rajeshwar Reddy Manthena",
      "Asmitha Yuga Sre A",
      "Harsh Tripathi",
      "Suganya Selvaraj",
      "Vito Chin",
      "Kasthuri Rangan Bhaskar",
      "Kasthuri Rangan Bhaskar",
      "Venkatraman R",
      "Sajit Vijayakumar"
    ],
    "abstract": "We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "17 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.06046v1",
    "published_date": "2025-12-05 09:56:15 UTC",
    "updated_date": "2025-12-05 09:56:15 UTC"
  },
  {
    "arxiv_id": "2512.05557v1",
    "title": "2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency",
    "authors": [
      "Xingxi Yin",
      "Yicheng Li",
      "Gong Yan",
      "Chenglin Li",
      "Jian Zhao",
      "Cong Huang",
      "Yue Deng",
      "Yin Zhang"
    ],
    "abstract": "Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \\textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \\textbf{2{,}000} uniquely stylized characters appearing across \\textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \\textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \\textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \\textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05557v1",
    "published_date": "2025-12-05 09:26:24 UTC",
    "updated_date": "2025-12-05 09:26:24 UTC"
  },
  {
    "arxiv_id": "2512.05556v1",
    "title": "Improving Local Fidelity Through Sampling and Modeling Nonlinearity",
    "authors": [
      "Sanjeev Shrestha",
      "Rahul Dubey",
      "Hui Liu"
    ],
    "abstract": "With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05556v1",
    "published_date": "2025-12-05 09:26:18 UTC",
    "updated_date": "2025-12-05 09:26:18 UTC"
  },
  {
    "arxiv_id": "2512.05546v1",
    "title": "Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models",
    "authors": [
      "Weijue Bu",
      "Guan Yuan",
      "Guixian Zhang"
    ],
    "abstract": "Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.05546v1",
    "published_date": "2025-12-05 09:07:55 UTC",
    "updated_date": "2025-12-05 09:07:55 UTC"
  },
  {
    "arxiv_id": "2512.05542v1",
    "title": "RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs",
    "authors": [
      "Jonathan Geuter",
      "Gregor Kornhardt"
    ],
    "abstract": "Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\\{m_i\\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 3 figures. 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Foundations of Reasoning in Language Models",
    "pdf_url": "https://arxiv.org/pdf/2512.05542v1",
    "published_date": "2025-12-05 08:55:39 UTC",
    "updated_date": "2025-12-05 08:55:39 UTC"
  },
  {
    "arxiv_id": "2512.05534v2",
    "title": "On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability",
    "authors": [
      "Yiming Tang",
      "Harshvardhan Saini",
      "Zhaoqian Yao",
      "Yizhen Liao",
      "Qianxiao Li",
      "Mengnan Du",
      "Dianbo Liu"
    ],
    "abstract": "As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode diverse concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into monosemantic features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one optimization problem. We demonstrate how diverse methods instantiate the theoretical framework and provide rigorous analysis of the optimization landscape. We provide novel theoretical explanations for empirically observed phenomena, including feature absorption and dead neurons. We design the Linear Representation Bench, a benchmark that strictly follows the Linear Representation Hypothesis, to evaluate SDL methods with fully accessible ground-truth features. Motivated by our theory and findings, we develop feature achoring, a novel technique applicable for all SDL methods, to enhance their feature recovery capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05534v2",
    "published_date": "2025-12-05 08:47:19 UTC",
    "updated_date": "2026-01-13 08:47:42 UTC"
  },
  {
    "arxiv_id": "2512.05530v1",
    "title": "MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models",
    "authors": [
      "Chuang Yu",
      "Jinmiao Zhao",
      "Mingxuan Zhao",
      "Yunpeng Liu",
      "Xiujun Shu",
      "Yuanhao Feng",
      "Bo Wang",
      "Xiangyu Yue"
    ],
    "abstract": "Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of \"Understand -> Rethink -> Correct\", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05530v1",
    "published_date": "2025-12-05 08:41:44 UTC",
    "updated_date": "2025-12-05 08:41:44 UTC"
  },
  {
    "arxiv_id": "2512.05529v1",
    "title": "See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors",
    "authors": [
      "Kunyi Yang",
      "Qingyu Wang",
      "Cheng Yuan",
      "Yutong Ban"
    ],
    "abstract": "Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The first two authors contributed equally",
    "pdf_url": "https://arxiv.org/pdf/2512.05529v1",
    "published_date": "2025-12-05 08:41:42 UTC",
    "updated_date": "2025-12-05 08:41:42 UTC"
  },
  {
    "arxiv_id": "2512.05519v1",
    "title": "User Negotiations of Authenticity, Ownership, and Governance on AI-Generated Video Platforms: Evidence from Sora",
    "authors": [
      "Bohui Shen",
      "Shrikar Bhatta",
      "Alex Ireebanije",
      "Zexuan Liu",
      "Abhinav Choudhry",
      "Ece Gumusel",
      "Kyrie Zhixuan Zhou"
    ],
    "abstract": "As AI-generated video platforms rapidly advance, ethical challenges such as copyright infringement emerge. This study examines how users make sense of AI-generated videos on OpenAI's Sora by conducting a qualitative content analysis of user comments. Through a thematic analysis, we identified four dynamics that characterize how users negotiate authenticity, authorship, and platform governance on Sora. First, users acted as critical evaluators of realism, assessing micro-details such as lighting, shadows, fluid motion, and physics to judge whether AI-generated scenes could plausibly exist. Second, users increasingly shifted from passive viewers to active creators, expressing curiosity about prompts, techniques, and creative processes. Text prompts were perceived as intellectual property, generating concerns about plagiarism and remixing norms. Third, users reported blurred boundaries between real and synthetic media, worried about misinformation, and even questioned the authenticity of other commenters, suspecting bot-generated engagement. Fourth, users contested platform governance: some perceived moderation as inconsistent or opaque, while others shared tactics for evading prompt censorship through misspellings, alternative phrasing, emojis, or other languages. Despite this, many users also enforced ethical norms by discouraging the misuse of real people's images or disrespectful content. Together, these patterns highlighted how AI-mediated platforms complicate notions of reality, creativity, and rule-making in emerging digital ecosystems. Based on the findings, we discuss governance challenges in Sora and how user negotiations inform future platform governance.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05519v1",
    "published_date": "2025-12-05 08:23:27 UTC",
    "updated_date": "2025-12-05 08:23:27 UTC"
  },
  {
    "arxiv_id": "2512.05518v1",
    "title": "Matching Ranks Over Probability Yields Truly Deep Safety Alignment",
    "authors": [
      "Jason Vega",
      "Gagandeep Singh"
    ],
    "abstract": "A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \\enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the \"deep\" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability \"harmful\" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability \"refusal\" tokens). We argue that this vulnerability is enabled due to the \"gaming\" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05518v1",
    "published_date": "2025-12-05 08:22:27 UTC",
    "updated_date": "2025-12-05 08:22:27 UTC"
  },
  {
    "arxiv_id": "2512.11862v1",
    "title": "Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL",
    "authors": [
      "Jiahao You",
      "Ziye Jia",
      "Can Cui",
      "Chao Dong",
      "Qihui Wu",
      "Zhu Han"
    ],
    "abstract": "The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11862v1",
    "published_date": "2025-12-05 08:14:45 UTC",
    "updated_date": "2025-12-05 08:14:45 UTC"
  },
  {
    "arxiv_id": "2512.05508v1",
    "title": "Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction",
    "authors": [
      "Yash Choudhary",
      "Preeti Rao",
      "Pushpak Bhattacharyya"
    ],
    "abstract": "Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2512.05508v1",
    "published_date": "2025-12-05 08:09:26 UTC",
    "updated_date": "2025-12-05 08:09:26 UTC"
  },
  {
    "arxiv_id": "2512.08979v1",
    "title": "What Happens When: Learning Temporal Orders of Events in Videos",
    "authors": [
      "Daechul Ahn",
      "Yura Choi",
      "Hyeonbeom Choi",
      "Seongwon Cho",
      "San Kim",
      "Jonghyun Choi"
    ],
    "abstract": "Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "WACV 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.08979v1",
    "published_date": "2025-12-05 07:50:59 UTC",
    "updated_date": "2025-12-05 07:50:59 UTC"
  },
  {
    "arxiv_id": "2512.05481v1",
    "title": "UniFS: Unified Multi-Contrast MRI Reconstruction via Frequency-Spatial Fusion",
    "authors": [
      "Jialin Li",
      "Yiwei Ren",
      "Kai Pan",
      "Dong Wei",
      "Pujin Cheng",
      "Xian Wu",
      "Xiaoying Tang"
    ],
    "abstract": "Recently, Multi-Contrast MR Reconstruction (MCMR) has emerged as a hot research topic that leverages high-quality auxiliary modalities to reconstruct undersampled target modalities of interest. However, existing methods often struggle to generalize across different k-space undersampling patterns, requiring the training of a separate model for each specific pattern, which limits their practical applicability. To address this challenge, we propose UniFS, a Unified Frequency-Spatial Fusion model designed to handle multiple k-space undersampling patterns for MCMR tasks without any need for retraining. UniFS integrates three key modules: a Cross-Modal Frequency Fusion module, an Adaptive Mask-Based Prompt Learning module, and a Dual-Branch Complementary Refinement module. These modules work together to extract domain-invariant features from diverse k-space undersampling patterns while dynamically adapt to their own variations. Another limitation of existing MCMR methods is their tendency to focus solely on spatial information while neglect frequency characteristics, or extract only shallow frequency features, thus failing to fully leverage complementary cross-modal frequency information. To relieve this issue, UniFS introduces an adaptive prompt-guided frequency fusion module for k-space learning, significantly enhancing the model's generalization performance. We evaluate our model on the BraTS and HCP datasets with various k-space undersampling patterns and acceleration factors, including previously unseen patterns, to comprehensively assess UniFS's generalizability. Experimental results across multiple scenarios demonstrate that UniFS achieves state-of-the-art performance. Our code is available at https://github.com/LIKP0/UniFS.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05481v1",
    "published_date": "2025-12-05 07:18:29 UTC",
    "updated_date": "2025-12-05 07:18:29 UTC"
  },
  {
    "arxiv_id": "2512.05475v1",
    "title": "PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning",
    "authors": [
      "Saumya Biswas",
      "Jiten Oswal"
    ],
    "abstract": "In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 9 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2512.05475v1",
    "published_date": "2025-12-05 07:07:39 UTC",
    "updated_date": "2025-12-05 07:07:39 UTC"
  },
  {
    "arxiv_id": "2512.11860v1",
    "title": "An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes",
    "authors": [
      "Yuelian Li",
      "Andrew Rushing Hands"
    ],
    "abstract": "Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11860v1",
    "published_date": "2025-12-05 06:58:25 UTC",
    "updated_date": "2025-12-05 06:58:25 UTC"
  },
  {
    "arxiv_id": "2601.04199v1",
    "title": "The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs",
    "authors": [
      "Jiale Zhao",
      "Xing Mou",
      "Jinlin Wu",
      "Hongyuan Yu",
      "Mingrui Sun",
      "Yang Shi",
      "Xuanwu Yin",
      "Zhen Chen",
      "Zhen Lei",
      "Yaohua Wang"
    ],
    "abstract": "Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel \"Parameter-Space Intervention\" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.04199v1",
    "published_date": "2025-12-05 06:52:06 UTC",
    "updated_date": "2025-12-05 06:52:06 UTC"
  },
  {
    "arxiv_id": "2512.05469v1",
    "title": "How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data",
    "authors": [
      "Zubair Ahmed Mohammad"
    ],
    "abstract": "Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 9 figures, 3 tables. Code and reproducible experiments are available at: https://github.com/zubair0831/ensemble-generalization-gap",
    "pdf_url": "https://arxiv.org/pdf/2512.05469v1",
    "published_date": "2025-12-05 06:51:06 UTC",
    "updated_date": "2025-12-05 06:51:06 UTC"
  },
  {
    "arxiv_id": "2512.05468v1",
    "title": "University Building Recognition Dataset in Thailand for the mission-oriented IoT sensor system",
    "authors": [
      "Takara Taniguchi",
      "Yudai Ueda",
      "Atsuya Muramatsu",
      "Kohki Hashimoto",
      "Ryo Yagi",
      "Hideya Ochiai",
      "Chaodit Aswakul"
    ],
    "abstract": "Many industrial sectors have been using of machine learning at inference mode on edge devices. Future directions show that training on edge devices is promising due to improvements in semiconductor performance. Wireless Ad Hoc Federated Learning (WAFL) has been proposed as a promising approach for collaborative learning with device-to-device communication among edges. In particular, WAFL with Vision Transformer (WAFL-ViT) has been tested on image recognition tasks with the UTokyo Building Recognition Dataset (UTBR). Since WAFL-ViT is a mission-oriented sensor system, it is essential to construct specific datasets by each mission. In our work, we have developed the Chulalongkorn University Building Recognition Dataset (CUBR), which is specialized for Chulalongkorn University as a case study in Thailand. Additionally, our results also demonstrate that training on WAFL scenarios achieves better accuracy than self-training scenarios. Dataset is available in https://github.com/jo2lxq/wafl/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05468v1",
    "published_date": "2025-12-05 06:49:52 UTC",
    "updated_date": "2025-12-05 06:49:52 UTC"
  },
  {
    "arxiv_id": "2512.05464v1",
    "title": "Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment",
    "authors": [
      "Panatchakorn Anantaprayoon",
      "Nataliia Babina",
      "Jad Tarifi",
      "Nima Asgharbeygi"
    ],
    "abstract": "Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 4 figures, to appear in AAAI 2026 AIGOV Workshop",
    "pdf_url": "https://arxiv.org/pdf/2512.05464v1",
    "published_date": "2025-12-05 06:46:00 UTC",
    "updated_date": "2025-12-05 06:46:00 UTC"
  },
  {
    "arxiv_id": "2512.05461v1",
    "title": "Knowing Your Uncertainty -- On the application of LLM in social sciences",
    "authors": [
      "Bolun Zhang",
      "Linzhuo Li",
      "Yunqi Chen",
      "Qinlin Zhao",
      "Zihan Zhu",
      "Xiaoyuan Yi",
      "Xing Xie"
    ],
    "abstract": "Large language models (LLMs) are rapidly being integrated into computational social science research, yet their blackboxed training and designed stochastic elements in inference pose unique challenges for scientific inquiry. This article argues that applying LLMs to social scientific tasks requires explicit assessment of uncertainty-an expectation long established in both quantitative methodology in the social sciences and machine learning. We introduce a unified framework for evaluating LLM uncertainty along two dimensions: the task type (T), which distinguishes between classification, short-form, and long-form generation, and the validation type (V), which captures the availability of reference data or evaluative criteria. Drawing from both computer science and social science literature, we map existing uncertainty quantification (UQ) methods to this T-V typology and offer practical recommendations for researchers. Our framework provides both a methodological safeguard and a practical guide for integrating LLMs into rigorous social science research.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "49 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.05461v1",
    "published_date": "2025-12-05 06:36:15 UTC",
    "updated_date": "2025-12-05 06:36:15 UTC"
  },
  {
    "arxiv_id": "2512.05453v1",
    "title": "Parajudica: An RDF-Based Reasoner and Metamodel for Multi-Framework Context-Dependent Data Compliance Assessments",
    "authors": [
      "Luc Moreau",
      "Alfred Rossi",
      "Sophie Stalla-Bourdillon"
    ],
    "abstract": "Motivated by the challenges of implementing policy-based data access control (PBAC) under multiple simultaneously applicable compliance frameworks, we present Parajudica, an open, modular, and extensible RDF/SPARQL-based rule system for evaluating context-dependent data compliance status. We demonstrate the utility of this resource and accompanying metamodel through application to existing legal frameworks and industry standards, offering insights for comparative framework analysis. Applications include compliance policy enforcement, compliance monitoring, data discovery, and risk assessment.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CY",
      "cs.LO"
    ],
    "primary_category": "cs.DB",
    "comment": "17 pages, 8 figures. Code and examples available at https://github.com/alfredr/parajudica",
    "pdf_url": "https://arxiv.org/pdf/2512.05453v1",
    "published_date": "2025-12-05 06:16:34 UTC",
    "updated_date": "2025-12-05 06:16:34 UTC"
  },
  {
    "arxiv_id": "2512.13704v1",
    "title": "Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents",
    "authors": [
      "Doohee You",
      "Sundeep Paul"
    ],
    "abstract": "The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a \"Council of Agents,\" a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.13704v1",
    "published_date": "2025-12-05 06:13:00 UTC",
    "updated_date": "2025-12-05 06:13:00 UTC"
  },
  {
    "arxiv_id": "2512.05449v1",
    "title": "The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems",
    "authors": [
      "Robert Yang"
    ],
    "abstract": "Large language models display a peculiar form of inconsistency: they \"know\" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of \"self-control\" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as \"scheming\" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages + appendix. AAAI 2026 FAST Workshop (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2512.05449v1",
    "published_date": "2025-12-05 05:57:40 UTC",
    "updated_date": "2025-12-05 05:57:40 UTC"
  },
  {
    "arxiv_id": "2512.05442v2",
    "title": "IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?",
    "authors": [
      "Hua Wang",
      "Jinghao Lu",
      "Fan Zhang"
    ],
    "abstract": "Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.05442v2",
    "published_date": "2025-12-05 05:37:25 UTC",
    "updated_date": "2025-12-16 09:16:34 UTC"
  },
  {
    "arxiv_id": "2512.05439v1",
    "title": "BEAVER: An Efficient Deterministic LLM Verifier",
    "authors": [
      "Tarun Suresh",
      "Nalin Wadhwa",
      "Debangshu Banerjee",
      "Gagandeep Singh"
    ],
    "abstract": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.",
    "categories": [
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05439v1",
    "published_date": "2025-12-05 05:34:06 UTC",
    "updated_date": "2025-12-05 05:34:06 UTC"
  },
  {
    "arxiv_id": "2512.11859v1",
    "title": "Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion",
    "authors": [
      "Michael Chertkov"
    ],
    "abstract": "We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.\n  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "cs.AI",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "40 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11859v1",
    "published_date": "2025-12-05 05:18:15 UTC",
    "updated_date": "2025-12-05 05:18:15 UTC"
  },
  {
    "arxiv_id": "2512.05432v1",
    "title": "Building Capacity for Artificial Intelligence in Africa: A Cross-Country Survey of Challenges and Governance Pathways",
    "authors": [
      "Jeffrey N. A. Aryee",
      "Patrick Davies",
      "Godfred A. Torsah",
      "Mercy M. Apaw",
      "Cyril D. Boateng",
      "Sam M. Mwando",
      "Chris Kwisanga",
      "Eric Jobunga",
      "Leonard K. Amekudzi"
    ],
    "abstract": "Artificial intelligence (AI) is transforming education and the workforce, but access to AI learning opportunities in Africa remains uneven. With rapid demographic shifts and growing labour market pressures, AI has become a strategic development priority, making the demand for relevant skills more urgent. This study investigates how universities and industries engage in shaping AI education and workforce preparation, drawing on survey responses from five African countries (Ghana, Namibia, Rwanda, Kenya and Zambia). The findings show broad recognition of AI importance but limited evidence of consistent engagement, practical training, or equitable access to resources. Most respondents who rated the AI component of their curriculum as very relevant reported being well prepared for jobs, but financial barriers, poor infrastructure, and weak communication limit participation, especially among students and underrepresented groups. Respondents highlighted internships, industry partnerships, and targeted support mechanisms as critical enablers, alongside the need for inclusive governance frameworks. The results showed both the growing awareness of AI's potential and the structural gaps that hinder its translation into workforce capacity. Strengthening university-industry collaboration and addressing barriers of access, funding, and policy are central to ensuring that AI contributes to equitable and sustainable development across the continent.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "16 pages, 4 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2512.05432v1",
    "published_date": "2025-12-05 05:14:23 UTC",
    "updated_date": "2025-12-05 05:14:23 UTC"
  },
  {
    "arxiv_id": "2512.05430v1",
    "title": "ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering",
    "authors": [
      "Daeyong Kwon",
      "SeungHeon Doh",
      "Juhan Nam"
    ],
    "abstract": "Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to LREC 2026. This work is an evolution of our earlier preprint arXiv:2507.23334",
    "pdf_url": "https://arxiv.org/pdf/2512.05430v1",
    "published_date": "2025-12-05 05:09:30 UTC",
    "updated_date": "2025-12-05 05:09:30 UTC"
  },
  {
    "arxiv_id": "2512.11858v1",
    "title": "Adaptive Path Integral Diffusion: AdaPID",
    "authors": [
      "Michael Chertkov",
      "Hamidreza Behjoo"
    ],
    "abstract": "Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "cs.AI",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "51 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11858v1",
    "published_date": "2025-12-05 04:57:00 UTC",
    "updated_date": "2025-12-05 04:57:00 UTC"
  },
  {
    "arxiv_id": "2512.11857v1",
    "title": "TopicProphet: Prophesies on Temporal Topic Trends and Stocks",
    "authors": [
      "Olivia Kim"
    ],
    "abstract": "Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.11857v1",
    "published_date": "2025-12-05 04:33:08 UTC",
    "updated_date": "2025-12-05 04:33:08 UTC"
  },
  {
    "arxiv_id": "2512.05415v1",
    "title": "Moving object detection from multi-depth images with an attention-enhanced CNN",
    "authors": [
      "Masato Shibukawa",
      "Fumi Yoshida",
      "Toshifumi Yanagisawa",
      "Takashi Ito",
      "Hirohisa Kurosaki",
      "Makoto Yoshikawa",
      "Kohki Kamiya",
      "Ji-an Jiang",
      "Wesley Fraser",
      "JJ Kavelaars",
      "Susan Benecchi",
      "Anne Verbiscer",
      "Akira Hatakeyama",
      "Hosei O",
      "Naoya Ozaki"
    ],
    "abstract": "One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 22 figures, submitted to PASJ",
    "pdf_url": "https://arxiv.org/pdf/2512.05415v1",
    "published_date": "2025-12-05 04:29:37 UTC",
    "updated_date": "2025-12-05 04:29:37 UTC"
  },
  {
    "arxiv_id": "2512.06042v1",
    "title": "Auto-SPT: Automating Semantic Preserving Transformations for Code",
    "authors": [
      "Ashish Hooda",
      "Mihai Christodorescu",
      "Chuangang Ren",
      "Aaron Wilson",
      "Kassem Fawaz",
      "Somesh Jha"
    ],
    "abstract": "Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.06042v1",
    "published_date": "2025-12-05 04:11:44 UTC",
    "updated_date": "2025-12-05 04:11:44 UTC"
  },
  {
    "arxiv_id": "2512.05411v1",
    "title": "A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems",
    "authors": [
      "Pranav Pushkar Mishra",
      "Kranti Prakash Yeole",
      "Ramyashree Keshavamurthy",
      "Mokshit Bharat Surana",
      "Fatemeh Sarayloo"
    ],
    "abstract": "In enterprise settings, efficiently retrieving relevant information from large and complex knowledge bases is essential for operational productivity and informed decision-making. This research presents a systematic framework for metadata enrichment using large language models (LLMs) to enhance document retrieval in Retrieval-Augmented Generation (RAG) systems. Our approach employs a comprehensive, structured pipeline that dynamically generates meaningful metadata for document segments, substantially improving their semantic representations and retrieval accuracy. Through extensive experiments, we compare three chunking strategies-semantic, recursive, and naive-and evaluate their effectiveness when combined with advanced embedding techniques. The results demonstrate that metadata-enriched approaches consistently outperform content-only baselines, with recursive chunking paired with TF-IDF weighted embeddings yielding an 82.5% precision rate compared to 73.3% for semantic content-only approaches. The naive chunking strategy with prefix-fusion achieved the highest Hit Rate@10 of 0.925. Our evaluation employs cross-encoder reranking for ground truth generation, enabling rigorous assessment via Hit Rate and Metadata Consistency metrics. These findings confirm that metadata enrichment enhances vector clustering quality while reducing retrieval latency, making it a key optimization for RAG systems across knowledge domains. This work offers practical insights for deploying high-performance, scalable document retrieval solutions in enterprise settings, demonstrating that metadata enrichment is a powerful approach for enhancing RAG effectiveness.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "7 pages, 3 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2512.05411v1",
    "published_date": "2025-12-05 04:05:06 UTC",
    "updated_date": "2025-12-05 04:05:06 UTC"
  },
  {
    "arxiv_id": "2512.11856v1",
    "title": "GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search",
    "authors": [
      "Ao Zhou",
      "Jianlei Yang",
      "Tong Qiao",
      "Yingjie Qi",
      "Zhi Yang",
      "Weisheng Zhao",
      "Chunming Hu"
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by IEEE Transactions on Computers",
    "pdf_url": "https://arxiv.org/pdf/2512.11856v1",
    "published_date": "2025-12-05 04:02:53 UTC",
    "updated_date": "2025-12-05 04:02:53 UTC"
  },
  {
    "arxiv_id": "2512.05402v1",
    "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction",
    "authors": [
      "Sithumi Wickramasinghe",
      "Bikramjit Das",
      "Dorien Herremans"
    ],
    "abstract": "Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05402v1",
    "published_date": "2025-12-05 03:47:13 UTC",
    "updated_date": "2025-12-05 03:47:13 UTC"
  },
  {
    "arxiv_id": "2512.13703v1",
    "title": "Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models",
    "authors": [
      "Fan Yang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.13703v1",
    "published_date": "2025-12-05 03:44:26 UTC",
    "updated_date": "2025-12-05 03:44:26 UTC"
  },
  {
    "arxiv_id": "2512.05397v2",
    "title": "Simulating Life Paths with Digital Twins: AI-Generated Future Selves Influence Decision-Making and Expand Human Choice",
    "authors": [
      "Rachel Poonsiriwong",
      "Chayapatr Archiwaranguprok",
      "Constanze Albrecht",
      "Peggy Yin",
      "Nattavudh Powdthavee",
      "Hal Hershfield",
      "Monchai Lertsutthiwong",
      "Kavin Winson",
      "Pat Pataranutaporn"
    ],
    "abstract": "Major life transitions demand high-stakes decisions, yet people often struggle to imagine how their future selves will live with the consequences. To support this limited capacity for mental time travel, we introduce AI-enabled digital twins that have ``lived through'' simulated life scenarios. Rather than predicting optimal outcomes, these simulations extend prospective cognition by making alternative futures vivid enough to support deliberation without assuming which path is best. We evaluate this idea in a randomized controlled study (N=192) using multimodal synthesis - facial age progression, voice cloning, and large language model dialogue - to create personalized avatars representing participants 30 years forward. Young adults 18 to 28 years old described pending binary decisions and were assigned to guided imagination or one of four avatar conditions: single-option, balanced dual-option, or expanded three-option with a system-generated novel alternative. Results showed asymmetric effects: single-sided avatars increased shifts toward the presented option, while balanced presentation produced movement toward both. Introducing a system-generated third option increased adoption of this new alternative compared to control, suggesting that AI-generated future selves can expand choice by surfacing paths that might otherwise go unnoticed. Participants rated evaluative reasoning and eudaimonic meaning-making as more important than emotional or visual vividness. Perceived persuasiveness and baseline agency predicted decision change. These findings advance understanding of AI-mediated episodic prospection and raise questions about autonomy in AI-augmented decisions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05397v2",
    "published_date": "2025-12-05 03:30:44 UTC",
    "updated_date": "2025-12-08 04:59:40 UTC"
  },
  {
    "arxiv_id": "2512.11855v1",
    "title": "Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry",
    "authors": [
      "Behrooz Tahmasebi",
      "Melanie Weber"
    ],
    "abstract": "Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.11855v1",
    "published_date": "2025-12-05 03:18:18 UTC",
    "updated_date": "2025-12-05 03:18:18 UTC"
  },
  {
    "arxiv_id": "2512.05386v1",
    "title": "Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets",
    "authors": [
      "Jakub Kopko",
      "David Graber",
      "Saltuk Mustafa Eyrilmez",
      "Stanislav Mazurenko",
      "David Bednar",
      "Jiri Sedlar",
      "Josef Sivic"
    ],
    "abstract": "As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 6 figures, submitted to NeurIPS 2025 AI4Science Workshop",
    "pdf_url": "https://arxiv.org/pdf/2512.05386v1",
    "published_date": "2025-12-05 02:55:19 UTC",
    "updated_date": "2025-12-05 02:55:19 UTC"
  },
  {
    "arxiv_id": "2512.05383v1",
    "title": "Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation",
    "authors": [
      "Mara Downing",
      "Matthew Peng",
      "Jacob Granley",
      "Michael Beyeler",
      "Tevfik Bultan"
    ],
    "abstract": "Objective: Machine learning (ML) models are increasingly used to generate electrical stimulation patterns in neuroprosthetic devices such as visual prostheses. While these models promise precise and personalized control, they also introduce new safety risks when model outputs are delivered directly to neural tissue. We propose a systematic, quantitative approach to detect and characterize unsafe stimulation patterns in ML-driven neurostimulation systems. Approach: We adapt an automated software testing technique known as coverage-guided fuzzing to the domain of neural stimulation. Here, fuzzing performs stress testing by perturbing model inputs and tracking whether resulting stimulation violates biophysical limits on charge density, instantaneous current, or electrode co-activation. The framework treats encoders as black boxes and steers exploration with coverage metrics that quantify how broadly test cases span the space of possible outputs and violation types. Main results: Applied to deep stimulus encoders for the retina and cortex, the method systematically reveals diverse stimulation regimes that exceed established safety limits. Two violation-output coverage metrics identify the highest number and diversity of unsafe outputs, enabling interpretable comparisons across architectures and training strategies. Significance: Violation-focused fuzzing reframes safety assessment as an empirical, reproducible process. By transforming safety from a training heuristic into a measurable property of the deployed model, it establishes a foundation for evidence-based benchmarking, regulatory readiness, and ethical assurance in next-generation neural interfaces.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "20 pages, 4 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2512.05383v1",
    "published_date": "2025-12-05 02:43:22 UTC",
    "updated_date": "2025-12-05 02:43:22 UTC"
  },
  {
    "arxiv_id": "2512.05379v1",
    "title": "Mitigating Self-Preference by Authorship Obfuscation",
    "authors": [
      "Taslim Mahbub",
      "Shi Feng"
    ],
    "abstract": "Language models (LMs) judges are widely used to evaluate the quality of LM outputs. Despite many advantages, LM judges display concerning biases that can impair their integrity in evaluations. One such bias is self-preference: LM judges preferring their own answers over those produced by other LMs or humans. The bias is hard to eliminate as frontier LM judges can distinguish their own outputs from those of others, even when the evaluation candidates are not labeled with their sources. In this paper, we investigate strategies to mitigate self-preference by reducing the LM judges' ability to recognize their own outputs. We apply black-box perturbations to evaluation candidates in pairwise comparison to obfuscate the authorship and reduce self-recognition. We find that perturbations as simple as synonym replacement for a few words predictably reduce self-preference. However, we also uncover fundamental challenges to eliminating the bias: when we extrapolate our perturbations to a more complete neutralization of stylistic differences between the evaluation candidates, self-preference recovers. Our findings suggest that self-recognition and self-preference can happen on many semantic levels, and complete mitigation remains challenging despite promising initial results.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05379v1",
    "published_date": "2025-12-05 02:36:13 UTC",
    "updated_date": "2025-12-05 02:36:13 UTC"
  },
  {
    "arxiv_id": "2512.05377v3",
    "title": "China Regional 3km Downscaling Based on Residual Corrective Diffusion Model",
    "authors": [
      "Honglu Sun",
      "Hao Jing",
      "Zhixiang Dai",
      "Sa Xiao",
      "Wei Xue",
      "Jian Sun",
      "Qifeng Lu"
    ],
    "abstract": "A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 40 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05377v3",
    "published_date": "2025-12-05 02:27:08 UTC",
    "updated_date": "2025-12-15 03:17:09 UTC"
  },
  {
    "arxiv_id": "2512.05374v1",
    "title": "Please Don't Kill My Vibe: Empowering Agents with Data Flow Control",
    "authors": [
      "Charlie Summers",
      "Haneen Mohammed",
      "Eugene Wu"
    ],
    "abstract": "The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CR",
    "comment": "7 pages, 7 figures, CIDR 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.05374v1",
    "published_date": "2025-12-05 02:24:27 UTC",
    "updated_date": "2025-12-05 02:24:27 UTC"
  },
  {
    "arxiv_id": "2512.05373v1",
    "title": "Text Rationalization for Robust Causal Effect Estimation",
    "authors": [
      "Lijinghua Zhang",
      "Hengrui Cai"
    ],
    "abstract": "Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05373v1",
    "published_date": "2025-12-05 02:18:45 UTC",
    "updated_date": "2025-12-05 02:18:45 UTC"
  },
  {
    "arxiv_id": "2512.05371v1",
    "title": "ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications",
    "authors": [
      "Changwen Xing",
      "SamZaak Wong",
      "Xinlai Wan",
      "Yanfeng Lu",
      "Mengli Zhang",
      "Zebin Ma",
      "Lei Qi",
      "Zhengxiong Li",
      "Nan Guan",
      "Zhe Jiang",
      "Xi Wang",
      "Jun Yang"
    ],
    "abstract": "While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the AAAl26 Conference Main Track",
    "pdf_url": "https://arxiv.org/pdf/2512.05371v1",
    "published_date": "2025-12-05 02:09:49 UTC",
    "updated_date": "2025-12-05 02:09:49 UTC"
  },
  {
    "arxiv_id": "2512.05365v1",
    "title": "MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare",
    "authors": [
      "Zag ElSayed",
      "Craig Erickson",
      "Ernest Pedapati"
    ],
    "abstract": "Healthcare AI systems have historically faced challenges in merging contextual reasoning, long-term state management, and human-verifiable workflows into a cohesive framework. This paper introduces a completely innovative architecture and concept: combining the Model Context Protocol (MCP) with a specific clinical application, known as MCP-AI. This integration allows intelligent agents to reason over extended periods, collaborate securely, and adhere to authentic clinical logic, representing a significant shift away from traditional Clinical Decision Support Systems (CDSS) and prompt-based Large Language Models (LLMs). As healthcare systems become more complex, the need for autonomous, context-aware clinical reasoning frameworks has become urgent. We present MCP-AI, a novel architecture for explainable medical decision-making built upon the Model Context Protocol (MCP) a modular, executable specification for orchestrating generative and descriptive AI agents in real-time workflows. Each MCP file captures clinical objectives, patient context, reasoning state, and task logic, forming a reusable and auditable memory object. Unlike conventional CDSS or stateless prompt-based AI systems, MCP-AI supports adaptive, longitudinal, and collaborative reasoning across care settings. MCP-AI is validated through two use cases: (1) diagnostic modeling of Fragile X Syndrome with comorbid depression, and (2) remote coordination for Type 2 Diabetes and hypertension. In either scenario, the protocol facilitates physician-in-the-loop validation, streamlines clinical processes, and guarantees secure transitions of AI responsibilities between healthcare providers. The system connects with HL7/FHIR interfaces and adheres to regulatory standards, such as HIPAA and FDA SaMD guidelines. MCP-AI provides a scalable basis for interpretable, composable, and safety-oriented AI within upcoming clinical environments.",
    "categories": [
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.05365v1",
    "published_date": "2025-12-05 02:02:22 UTC",
    "updated_date": "2025-12-05 02:02:22 UTC"
  },
  {
    "arxiv_id": "2512.05356v2",
    "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
    "authors": [
      "Jason Weston",
      "Jakob Foerster"
    ],
    "abstract": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05356v2",
    "published_date": "2025-12-05 01:50:23 UTC",
    "updated_date": "2025-12-14 18:03:30 UTC"
  },
  {
    "arxiv_id": "2512.11854v1",
    "title": "Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks",
    "authors": [
      "Grant King",
      "Musa Azeem",
      "Savannah Noblitt",
      "Ramtin Zand",
      "Homayoun Valafar"
    ],
    "abstract": "Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24th International Conference on Machine Learning and Applications",
    "pdf_url": "https://arxiv.org/pdf/2512.11854v1",
    "published_date": "2025-12-05 01:49:47 UTC",
    "updated_date": "2025-12-05 01:49:47 UTC"
  },
  {
    "arxiv_id": "2512.05350v1",
    "title": "Invisible Load: Uncovering the Challenges of Neurodivergent Women in Software Engineering",
    "authors": [
      "Munazza Zaib",
      "Wei Wang",
      "Dulaji Hidellaarachchi",
      "Isma Farah Siddiqui"
    ],
    "abstract": "Neurodivergent women in Software Engineering (SE) encounter distinctive challenges at the intersection of gender bias and neurological differences. To the best of our knowledge, no prior work in SE research has systematically examined this group, despite increasing recognition of neurodiversity in the workplace. Underdiagnosis, masking, and male-centric workplace cultures continue to exacerbate barriers that contribute to stress, burnout, and attrition. In response, we propose a hybrid methodological approach that integrates InclusiveMag's inclusivity framework with the GenderMag walkthrough process, tailored to the context of neurodivergent women in SE. The overarching design unfolds across three stages, scoping through literature review, deriving personas and analytic processes, and applying the method in collaborative workshops. We present a targeted literature review that synthesize challenges into cognitive, social, organizational, structural and career progression challenges neurodivergent women face in SE, including how under/late diagnosis and masking intensify exclusion. These findings lay the groundwork for subsequent stages that will develop and apply inclusive analytic methods to support actionable change.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05350v1",
    "published_date": "2025-12-05 01:22:59 UTC",
    "updated_date": "2025-12-05 01:22:59 UTC"
  },
  {
    "arxiv_id": "2512.05343v1",
    "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
    "authors": [
      "Elisabetta Fedele",
      "Francis Engelmann",
      "Ian Huang",
      "Or Litany",
      "Marc Pollefeys",
      "Leonidas Guibas"
    ],
    "abstract": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://spacecontrol3d.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2512.05343v1",
    "published_date": "2025-12-05 00:54:48 UTC",
    "updated_date": "2025-12-05 00:54:48 UTC"
  },
  {
    "arxiv_id": "2512.05338v2",
    "title": "Interaction Tensor SHAP",
    "authors": [
      "Hiroki Hasegawa",
      "Yukihiko Okada"
    ],
    "abstract": "This study proposes Interaction Tensor SHAP (IT-SHAP), a tensor algebraic formulation of the Shapley Taylor Interaction Index (STII) that makes its computational structure explicit. STII extends the Shapley value to higher order interactions, but its exponential combinatorial definition makes direct computation intractable at scale. We reformulate STII as a linear transformation acting on a value function and derive an explicit algebraic representation of its weight tensor. This weight tensor is shown to possess a multilinear structure induced by discrete finite difference operators. When the value function admits a Tensor Train representation, higher order interaction indices can be computed in the parallel complexity class NC squared. In contrast, under general tensor network representations without structural assumptions, the same computation is proven to be P sharp hard. The main contributions are threefold. First, we establish an exact Tensor Train representation of the STII weight tensor. Second, we develop a parallelizable evaluation algorithm with explicit complexity bounds under the Tensor Train assumption. Third, we prove that computational intractability is unavoidable in the absence of such structure. These results demonstrate that the computational difficulty of higher order interaction analysis is determined by the underlying algebraic representation rather than by the interaction index itself, providing a theoretical foundation for scalable interpretation of high dimensional models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages",
    "pdf_url": "https://arxiv.org/pdf/2512.05338v2",
    "published_date": "2025-12-05 00:34:15 UTC",
    "updated_date": "2026-01-05 03:05:52 UTC"
  },
  {
    "arxiv_id": "2512.05334v1",
    "title": "The Effect of Document Summarization on LLM-Based Relevance Judgments",
    "authors": [
      "Samaneh Mohtadi",
      "Kevin Roitero",
      "Stefano Mizzaro",
      "Gianluca Demartini"
    ],
    "abstract": "Relevance judgments are central to the evaluation of Information Retrieval (IR) systems, but obtaining them from human annotators is costly and time-consuming. Large Language Models (LLMs) have recently been proposed as automated assessors, showing promising alignment with human annotations. Most prior studies have treated documents as fixed units, feeding their full content directly to LLM assessors. We investigate how text summarization affects the reliability of LLM-based judgments and their downstream impact on IR evaluation. Using state-of-the-art LLMs across multiple TREC collections, we compare judgments made from full documents with those based on LLM-generated summaries of different lengths. We examine their agreement with human labels, their effect on retrieval effectiveness evaluation, and their influence on IR systems' ranking stability. Our findings show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, while introducing systematic shifts in label distributions and biases that vary by model and dataset. These results highlight summarization as both an opportunity for more efficient large-scale IR evaluation and a methodological choice with important implications for the reliability of automatic judgments.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05334v1",
    "published_date": "2025-12-05 00:26:13 UTC",
    "updated_date": "2025-12-05 00:26:13 UTC"
  },
  {
    "arxiv_id": "2512.05325v1",
    "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning",
    "authors": [
      "Ömer Faruk Akgül",
      "Yusuf Hakan Kalaycı",
      "Rajgopal Kannan",
      "Willie Neiswanger",
      "Viktor Prasanna"
    ],
    "abstract": "Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often \"overthink\": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., \"hmm\", \"wait\") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.05325v1",
    "published_date": "2025-12-05 00:04:42 UTC",
    "updated_date": "2025-12-05 00:04:42 UTC"
  }
]
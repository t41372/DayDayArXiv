{
  "date": "2024-08-01",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-08-01 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 98 篇论文，主要聚焦 AI 安全、医疗图像处理、生成模型优化和强化学习等领域，其中令人印象深刻的是 Meta 的 \"Segment Anything Model 2\"（涉及多模态图像分割）和微软团队的 \"Tamper-Resistant Safeguards for Open-Weight LLMs\"（提升 LLM 安全性），这些论文展示了 AI 在实际应用中的潜力。\n\n下面，我挑选了今天论文中的重点和有话题度的内容，按主题和重要性排序，先讨论 AI 安全、医疗应用和生成模型等高影响力论文，再简要提及其他。每个条目列出论文标题（中文 + 英文），并概述主要贡献和发现，核心学术术语保留。篇幅有限，我会快速掠过一些较基础或小众的论文。\n\n### AI 安全与 LLM 优化\n- **Tamper-Resistant Safeguards for Open-Weight LLMs（防篡改保护机制用于开源大语言模型）**  \n  作者包括 Dan Hendrycks 和 Bo Li 等知名学者。该论文提出 TAR 方法，用于构建对篡改攻击（如微调移除安全机制）更 robust 的 LLM 保护机制。主要贡献是通过实验验证 TAR 在保持模型性能的同时显著提升安全性，适用于开源 LLM 的安全发布。\n\n- **Jailbreaking Text-to-Image Models with LLM-Based Agents（使用基于 LLM 的代理攻击文本到图像模型）**  \n  论文探索 LLM 代理（如 Atlas 框架）对文本到图像模型的越狱攻击。主要发现是，代理通过迭代查询生成更有效的攻击提示，提升数据泄露风险 20%，强调了生成模型的安全漏洞。\n\n- **Securing the Diagnosis of Medical Imaging: An In-depth Analysis of AI-Resistant Attacks（保护医疗图像诊断：AI 抗攻击的深入分析）**  \n  该研究分析对抗性攻击对医疗 DNN 的影响。主要贡献是总结多种攻击策略（如 FGSM 和 PGD），并提出防御框架，实验显示可显著降低误诊风险，适用于临床 AI。\n\n- **Pathway to Secure and Trustworthy ZSM for LLMs: Attacks, Defense, and Opportunities（LLM 的安全 ZSM 路径：攻击、防御与机会）**  \n  聚焦 LLM 在 ZSM 网络中的安全问题。主要发现是，针对成员推理攻击的防御机制可降低数据泄露风险，提供未来研究方向。\n\n### 医疗图像与应用\n- **Segment Anything Model 2: an application to 2D and 3D medical images（Segment Anything Model 2：在 2D 和 3D 医疗图像中的应用）**  \n  Meta 团队的续作，扩展了 SAM 到视频和 3D 图像。主要贡献是通过大数据集评估，SAM 2 在医疗图像分割中提升准确性和速度，适用于 CT 和 MRI 等，支持实时处理。\n\n- **CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression（临床指导的残差扩散模型预测特发性肺纤维化进展）**  \n  作者包括 Dinggang Shen。该论文提出 CIResDiff 模型，用于预测 IPF 进展。主要发现是通过残差扩散和预注册，模型在临床数据上超越 SOTA，提升早期诊断准确性。\n\n- **UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with Ultrasound Reflection Direction Parameterization（UlRe-NeRF：使用超声反射方向参数化的神经渲染 3D 超声成像）**  \n  创新地结合 NeRF 和超声反射建模。主要贡献是处理复杂 artifacts，提升 3D 超声图像真实性，实验显示在高保真重建上表现优异。\n\n- **Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM（基于点监督的脑肿瘤分割：使用 Box-prompted MedSAM）**  \n  该方法使用点监督和 MedSAM 框架进行脑肿瘤分割。主要发现是，通过迭代优化和伪框生成，提升了分割精度，在 BraTS 数据集上与框监督方法相当。\n\n### 生成模型与扩散技术\n- **Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention（平滑能量指导：通过减少注意力能量曲率指导扩散模型）**  \n  论文提出 SEG 方法，用于指导扩散模型生成。主要贡献是通过高斯核调整注意力能量曲率，提升图像生成质量和效率，在 Stable Diffusion 上实现 Pareto 改进。\n\n- **AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation（AgentGen：通过环境和任务生成增强基于 LLM 的代理规划能力）**  \n  作者包括微软团队。该研究使用 LLM 生成任务和环境，提升代理规划。主要发现是，AgentGen 在 KDD 基准上超越 GPT-3.5，优化代理训练效率。\n\n- **UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation（UniMoT：统一分子-文本语言模型使用离散标记表示）**  \n  聚焦分子生成任务的 LLM。主要贡献是通过向量量化桥接模态间差距，实现文本到分子的双向任务，实验显示在药物设计中性能领先。\n\n### 其他值得注意的论文\n- **MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities（MM-Vet v2：评估大型多模态模型集成能力的挑战性基准）**  \n  该论文更新了 MM-Vet 基准，添加序列理解任务。主要发现是，Claude 3.5 Sonnet 在 13 种语言上得分 71.8，强调多模态模型的鲁棒性。\n\n- **Enabling High Data Throughput Reinforcement Learning on GPUs: A Domain Agnostic Framework for Data-Driven Scientific Research（启用 GPU 高数据吞吐强化学习：适用于数据驱动科学研究的领域无关框架）**  \n  提出 WarpSci 框架，支持 GPU 上的高吞吐模拟。主要贡献是消除 CPU-GPU 数据传输瓶颈，提升科学计算效率。\n\n- **Parkinson's Disease Detection from Resting State EEG using Multi-Head Graph Structure Learning（使用多头图结构学习从静息态 EEG 检测帕金森病）**  \n  快速提及：该方法使用 GNN 检测帕金森病，准确率达 69.4%，提供可解释的脑连接洞见。\n\n其他论文，如一些纯理论的数学或小众应用（如 MIDI 生成、Hilbert 曲线等），虽有创新但影响力较小，我这里仅简要掠过不做深入讨论，以控制篇幅。\n\n总之，今天的 arXiv 更新突显了 AI 在安全和医疗领域的进展，建议关注 \"Segment Anything Model 2\" 和相关 LLM 安全论文，以捕捉前沿趋势。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2408.00946v1",
      "title": "Generalisation of Total Uncertainty in AI: A Theoretical Study",
      "title_zh": "翻译失败",
      "authors": [
        "Keivan Shariatmadar"
      ],
      "abstract": "AI has been dealing with uncertainty to have highly accurate results. This\nbecomes even worse with reasonably small data sets or a variation in the data\nsets. This has far-reaching effects on decision-making, forecasting and\nlearning mechanisms. This study seeks to unpack the nature of uncertainty that\nexists within AI by drawing ideas from established works, the latest\ndevelopments and practical applications and provide a novel total uncertainty\ndefinition in AI.\n  From inception theories up to current methodologies, this paper provides an\nintegrated view of dealing with better total uncertainty as well as\ncomplexities of uncertainty in AI that help us understand its meaning and value\nacross different domains.",
      "tldr_zh": "该论文探讨了AI中不确定性的泛化问题，特别是数据量小或数据变化时对决策、预测和学习机制的影响。作者通过回顾从起源理论到当前方法的现有研究、最新发展和实际应用，提供了一个整合视角。论文提出了一个新的total uncertainty定义，帮助理解不确定性在不同领域的含义和价值。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.PR",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2408.00946v1",
      "published_date": "2024-08-01 22:55:40 UTC",
      "updated_date": "2024-08-01 22:55:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:57:22.875894"
    },
    {
      "arxiv_id": "2408.00938v2",
      "title": "CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression",
      "title_zh": "翻译失败",
      "authors": [
        "Caiwen Jiang",
        "Xiaodan Xing",
        "Zaixin Ou",
        "Mianxin Liu",
        "Walsh Simon",
        "Guang Yang",
        "Dinggang Shen"
      ],
      "abstract": "The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly\ncorrelates with higher patient mortality rates. Early detection of IPF\nprogression is critical for initiating timely treatment, which can effectively\nslow down the advancement of the disease. However, the current clinical\ncriteria define disease progression requiring two CT scans with a one-year\ninterval, presenting a dilemma: a disease progression is identified only after\nthe disease has already progressed. To this end, in this paper, we develop a\nnovel diffusion model to accurately predict the progression of IPF by\ngenerating patient's follow-up CT scan from the initial CT scan. Specifically,\nfrom the clinical prior knowledge, we tailor improvements to the traditional\ndiffusion model and propose a Clinically-Informed Residual Diffusion model,\ncalled CIResDiff. The key innovations of CIResDiff include 1) performing the\ntarget region pre-registration to align the lung regions of two CT scans at\ndifferent time points for reducing the generation difficulty, 2) adopting the\nresidual diffusion instead of traditional diffusion to enable the model focus\nmore on differences (i.e., lesions) between the two CT scans rather than the\nlargely identical anatomical content, and 3) designing the clinically-informed\nprocess based on CLIP technology to integrate lung function information which\nis highly relevant to diagnosis into the reverse process for assisting\ngeneration. Extensive experiments on clinical data demonstrate that our\napproach can outperform state-of-the-art methods and effectively predict the\nprogression of IPF.",
      "tldr_zh": "本论文针对特发性肺纤维化（IPF）的进展预测问题，提出了一种新型扩散模型 CIResDiff，通过从初始 CT 扫描生成随访 CT 扫描，实现早期预测。CIResDiff 的关键创新包括：目标区域预注册（target region pre-registration）对齐肺部区域、使用残差扩散（residual diffusion）专注于 CT 扫描间的病变差异，以及基于 CLIP 技术的临床信息整合过程，以融入肺功能数据辅助生成。实验结果表明，该方法在临床数据上优于现有状态-of-the-art 方法，能够有效预测 IPF 进展并支持及时治疗。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00938v2",
      "published_date": "2024-08-01 22:01:42 UTC",
      "updated_date": "2024-08-05 09:32:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:57:37.362775"
    },
    {
      "arxiv_id": "2408.00930v1",
      "title": "Enabling High Data Throughput Reinforcement Learning on GPUs: A Domain Agnostic Framework for Data-Driven Scientific Research",
      "title_zh": "翻译失败",
      "authors": [
        "Tian Lan",
        "Huan Wang",
        "Caiming Xiong",
        "Silvio Savarese"
      ],
      "abstract": "We introduce WarpSci, a domain agnostic framework designed to overcome\ncrucial system bottlenecks encountered in the application of reinforcement\nlearning to intricate environments with vast datasets featuring\nhigh-dimensional observation or action spaces. Notably, our framework\neliminates the need for data transfer between the CPU and GPU, enabling the\nconcurrent execution of thousands of simulations on a single or multiple GPUs.\nThis high data throughput architecture proves particularly advantageous for\ndata-driven scientific research, where intricate environment models are\ncommonly essential.",
      "tldr_zh": "我们引入了 WarpSci，一个领域无关的框架，旨在解决强化学习在复杂环境中的系统瓶颈，特别是处理大规模数据集和高维观测或动作空间。该框架通过消除 CPU 和 GPU 之间的数据传输，支持在单个或多个 GPU 上并发执行数千个模拟，从而实现高数据吞吐量。这种设计对数据驱动的科学研究尤为有益，因为它能高效处理复杂的环境模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00930v1",
      "published_date": "2024-08-01 21:38:09 UTC",
      "updated_date": "2024-08-01 21:38:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:57:48.520794"
    },
    {
      "arxiv_id": "2408.00925v1",
      "title": "WHITE PAPER: A Brief Exploration of Data Exfiltration using GCG Suffixes",
      "title_zh": "翻译失败",
      "authors": [
        "Victor Valbuena"
      ],
      "abstract": "The cross-prompt injection attack (XPIA) is an effective technique that can\nbe used for data exfiltration, and that has seen increasing use. In this\nattack, the attacker injects a malicious instruction into third party data\nwhich an LLM is likely to consume when assisting a user, who is the victim.\nXPIA is often used as a means for data exfiltration, and the estimated cost of\nthe average data breach for a business is nearly $4.5 million, which includes\nbreaches such as compromised enterprise credentials. With the rise of\ngradient-based attacks such as the GCG suffix attack, the odds of an XPIA\noccurring which uses a GCG suffix are worryingly high. As part of my work in\nMicrosoft's AI Red Team, I demonstrated a viable attack model using a GCG\nsuffix paired with an injection in a simulated XPIA scenario. The results\nindicate that the presence of a GCG suffix can increase the odds of successful\ndata exfiltration by nearly 20%, with some caveats.",
      "tldr_zh": "这篇白皮书探讨了使用 GCG Suffixes 进行数据外泄 (Data Exfiltration) 的方法，特别关注跨提示注入攻击 (XPIA) 的应用。作者在微软 AI 红队的工作中，通过模拟 XPIA 场景，演示了将 GCG 后缀与恶意指令结合的攻击模型。结果表明，这种方法可以将数据外泄的成功率提高约 20%，并强调了企业数据泄露的潜在经济风险（如平均成本近 450 万美元）。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "8 pages, 8 figures. Conducted as part of employment at Microsoft\n  Corporation",
      "pdf_url": "http://arxiv.org/pdf/2408.00925v1",
      "published_date": "2024-08-01 21:28:27 UTC",
      "updated_date": "2024-08-01 21:28:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:58:00.745909"
    },
    {
      "arxiv_id": "2408.00923v1",
      "title": "Reclaiming Residual Knowledge: A Novel Paradigm to Low-Bit Quantization",
      "title_zh": "回收残余知识：一种低位量化新范式",
      "authors": [
        "Róisín Luo",
        "Alexandru Drimbarean",
        "James McDermott",
        "Colm O'Riordan"
      ],
      "abstract": "This paper explores a novel paradigm in low-bit (i.e. 4-bits or lower)\nquantization, differing from existing state-of-the-art methods, by framing\noptimal quantization as an architecture search problem within convolutional\nneural networks (ConvNets). Our framework, dubbed \\textbf{CoRa} (Optimal\nQuantization Residual \\textbf{Co}nvolutional Operator Low-\\textbf{Ra}nk\nAdaptation), is motivated by two key aspects. Firstly, quantization residual\nknowledge, i.e. the lost information between floating-point weights and\nquantized weights, has long been neglected by the research community.\nReclaiming the critical residual knowledge, with an infinitesimal extra\nparameter cost, can reverse performance degradation without training. Secondly,\nstate-of-the-art quantization frameworks search for optimal quantized weights\nto address the performance degradation. Yet, the vast search spaces in weight\noptimization pose a challenge for the efficient optimization in large models.\nFor example, state-of-the-art BRECQ necessitates $2 \\times 10^4$ iterations to\nquantize models. Fundamentally differing from existing methods, \\textbf{CoRa}\nsearches for the optimal architectures of low-rank adapters, reclaiming\ncritical quantization residual knowledge, within the search spaces smaller\ncompared to the weight spaces, by many orders of magnitude. The low-rank\nadapters approximate the quantization residual weights, discarded in previous\nmethods. We evaluate our approach over multiple pre-trained ConvNets on\nImageNet. \\textbf{CoRa} achieves comparable performance against both\nstate-of-the-art quantization-aware training and post-training quantization\nbaselines, in $4$-bit and $3$-bit quantization, by using less than $250$\niterations on a small calibration set with $1600$ images. Thus, \\textbf{CoRa}\nestablishes a new state-of-the-art in terms of the optimization efficiency in\nlow-bit quantization.",
      "tldr_zh": "本论文提出了一种新型低位量化（low-bit quantization）范式，将最优量化问题 framing 为卷积神经网络（ConvNets）中的架构搜索问题，框架名为CoRa（Optimal Quantization Residual Convolutional Operator Low-Ra nk Adaptation）。CoRa的核心创新在于重新利用量化残差知识（quantization residual knowledge），即浮点权重与量化权重间的丢失信息，通过低秩适配器（low-rank adapters）近似这些残差权重，从而以极低的额外参数成本逆转性能下降，且无需训练。相比现有方法，CoRa的搜索空间远小于权重优化空间，大幅提升了优化效率；在ImageNet上实验显示，CoRa在4-bit和3-bit量化中，仅用少于250次迭代和1600张图像的校准集，就达到了与最先进量化感知训练和后训练方法相当的性能，树立了低位量化优化效率的新基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by The 35th British Machine Vision Conference (BMVC 2024)",
      "pdf_url": "http://arxiv.org/pdf/2408.00923v1",
      "published_date": "2024-08-01 21:27:31 UTC",
      "updated_date": "2024-08-01 21:27:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:58:13.630690"
    },
    {
      "arxiv_id": "2408.00914v1",
      "title": "Granting GPT-4 License and Opportunity: Enhancing Accuracy and Confidence Estimation for Few-Shot Event Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Steven Fincke",
        "Adrien Bibal",
        "Elizabeth Boschee"
      ],
      "abstract": "Large Language Models (LLMs) such as GPT-4 have shown enough promise in the\nfew-shot learning context to suggest use in the generation of \"silver\" data and\nrefinement of new ontologies through iterative application and review. Such\nworkflows become more effective with reliable confidence estimation.\nUnfortunately, confidence estimation is a documented weakness of models such as\nGPT-4, and established methods to compensate require significant additional\ncomplexity and computation. The present effort explores methods for effective\nconfidence estimation with GPT-4 with few-shot learning for event detection in\nthe BETTER ontology as a vehicle. The key innovation is expanding the prompt\nand task presented to GPT-4 to provide License to speculate when unsure and\nOpportunity to quantify and explain its uncertainty (L&O). This approach\nimproves accuracy and provides usable confidence measures (0.759 AUC) with no\nadditional machinery.",
      "tldr_zh": "该研究针对大型语言模型如 GPT-4 在少样本学习(few-shot learning)中的置信度估计(confidence estimation)弱点，提出了一种创新方法，通过扩展提示赋予 GPT-4 “License and Opportunity (L&O)”机制，让其在不确定时有许可推测并量化解释不确定性，从而提升事件检测的准确性。研究以 BETTER ontology 为测试平台，实验结果显示这种方法显著提高了模型性能，提供可用的置信度测量(AUC 0.759)，且无需额外复杂计算或资源。整体框架有助于生成“银数据”(silver data)和完善新本体(ontologies)，为可靠的少样本任务应用奠定基础。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00914v1",
      "published_date": "2024-08-01 21:08:07 UTC",
      "updated_date": "2024-08-01 21:08:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:58:24.790984"
    },
    {
      "arxiv_id": "2408.00906v1",
      "title": "Parkinson's Disease Detection from Resting State EEG using Multi-Head Graph Structure Learning with Gradient Weighted Graph Attention Explanations",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher Neves",
        "Yong Zeng",
        "Yiming Xiao"
      ],
      "abstract": "Parkinson's disease (PD) is a debilitating neurodegenerative disease that has\nsevere impacts on an individual's quality of life. Compared with structural and\nfunctional MRI-based biomarkers for the disease, electroencephalography (EEG)\ncan provide more accessible alternatives for clinical insights. While deep\nlearning (DL) techniques have provided excellent outcomes, many techniques fail\nto model spatial information and dynamic brain connectivity, and face\nchallenges in robust feature learning, limited data sizes, and poor\nexplainability. To address these issues, we proposed a novel graph neural\nnetwork (GNN) technique for explainable PD detection using resting state EEG.\nSpecifically, we employ structured global convolutions with contrastive\nlearning to better model complex features with limited data, a novel multi-head\ngraph structure learner to capture the non-Euclidean structure of EEG data, and\na head-wise gradient-weighted graph attention explainer to offer neural\nconnectivity insights. We developed and evaluated our method using the UC San\nDiego Parkinson's disease EEG dataset, and achieved 69.40% detection accuracy\nin subject-wise leave-one-out cross-validation while generating intuitive\nexplanations for the learnt graph topology.",
      "tldr_zh": "本研究针对帕金森病 (Parkinson's disease, PD) 的检测问题，提出了一种基于图神经网络 (GNN) 的新型方法，利用休息状态 EEG 数据进行可解释性分析，以解决现有深度学习 (DL) 技术在空间信息建模、动态脑连接和特征学习方面的不足。方法包括结构化全局卷积与对比学习 (structured global convolutions with contrastive learning) 来处理复杂特征和有限数据、多头图结构学习器 (multi-head graph structure learner) 来捕捉 EEG 的非欧结构，以及头级梯度加权图注意力解释器 (head-wise gradient-weighted graph attention explainer) 来提供神经连接洞见。在 UC San Diego 的帕金森病 EEG 数据集上，该方法在主观-wise leave-one-out 交叉验证中实现了 69.40% 的检测准确率，并生成了直观的图拓扑解释，从而提升了 PD 检测的可解释性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at MLCN 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00906v1",
      "published_date": "2024-08-01 20:54:33 UTC",
      "updated_date": "2024-08-01 20:54:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:58:37.314035"
    },
    {
      "arxiv_id": "2408.00900v2",
      "title": "Expressive MIDI-format Piano Performance Generation",
      "title_zh": "富有表现力的 MIDI 格式钢琴表演生成",
      "authors": [
        "Jingwei Liu"
      ],
      "abstract": "This work presents a generative neural network that's able to generate\nexpressive piano performance in MIDI format. The musical expressivity is\nreflected by vivid micro-timing, rich polyphonic texture, varied dynamics, and\nthe sustain pedal effects. This model is innovative from many aspects of data\nprocessing to neural network design. We claim that this symbolic music\ngeneration model overcame the common critics of symbolic music and is able to\ngenerate expressive music flows as good as, if not better than generations with\nraw audio. One drawback is that, due to the limited time for submission, the\nmodel is not fine-tuned and sufficiently trained, thus the generation may sound\nincoherent and random at certain points. Despite that, this model shows its\npowerful generative ability to generate expressive piano pieces.",
      "tldr_zh": "本研究提出了一种生成式神经网络，用于生成富有表现力的 MIDI 格式钢琴表演，该模型在数据处理和神经网络设计方面具有创新性。模型能够产生生动的微时序、丰富的复音纹理、多变的动态以及持续踏板效果，从而克服传统符号音乐生成的常见缺陷，并生成与原始音频相当甚至更优秀的表现力音乐。尽管由于训练不足，生成结果可能在某些地方显得不连贯，但整体展示了强大的生成能力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "4 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00900v2",
      "published_date": "2024-08-01 20:36:37 UTC",
      "updated_date": "2024-12-13 23:35:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:58:48.856038"
    },
    {
      "arxiv_id": "2408.03345v1",
      "title": "Artifical intelligence and inherent mathematical difficulty",
      "title_zh": "翻译失败",
      "authors": [
        "Walter Dean",
        "Alberto Naibo"
      ],
      "abstract": "This paper explores the relationship of artificial intelligence to the task\nof resolving open questions in mathematics. We first present an updated version\nof a traditional argument that limitative results from computability and\ncomplexity theory show that proof discovery is an inherently difficult problem.\nWe then illustrate how several recent applications of artificial\nintelligence-inspired methods -- respectively involving automated theorem\nproving, SAT-solvers, and large language models -- do indeed raise novel\nquestions about the nature of mathematical proof. We also argue that the\nresults obtained by such techniques do not tell against our basic argument.\nThis is so because they are embodiments of brute force search and are thus\ncapable of deciding only statements of low logical complexity.",
      "tldr_zh": "这篇论文探讨了人工智能（artificial intelligence）在解决数学开放问题中的局限性，更新了传统论点：基于计算性理论（computability theory）和复杂性理论（complexity theory）的限制结果表明，证明发现是一个本质上困难的任务。作者分析了最近的AI方法应用，包括自动定理证明（automated theorem proving）、SAT-solvers和大型语言模型（large language models），这些技术引发了对数学证明本质的新问题。论文进一步论证，这些方法本质上是蛮力搜索，仅能处理低逻辑复杂度的语句，因此并不能反驳证明发现的固有难度。",
      "categories": [
        "math.HO",
        "cs.AI",
        "cs.CC",
        "math.LO",
        "03B35, 68V05, 68V15, 68T01",
        "F.2.2; F.4.0; I.2.0; I.2.3; K.2"
      ],
      "primary_category": "math.HO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.03345v1",
      "published_date": "2024-08-01 20:08:31 UTC",
      "updated_date": "2024-08-01 20:08:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:59:00.399681"
    },
    {
      "arxiv_id": "2408.07225v1",
      "title": "Longitudinal Evaluation of Child Face Recognition and the Impact of Underlying Age",
      "title_zh": "翻译失败",
      "authors": [
        "Surendra Singh",
        "Keivan Bahmani",
        "Stephanie Schuckers"
      ],
      "abstract": "The need for reliable identification of children in various emerging\napplications has sparked interest in leveraging child face recognition\ntechnology. This study introduces a longitudinal approach to enrollment and\nverification accuracy for child face recognition, focusing on the YFA database\ncollected by Clarkson University CITeR research group over an 8 year period, at\n6 month intervals.",
      "tldr_zh": "这篇论文评估了儿童面部识别（Child Face Recognition）的纵向性能，并探讨了潜在年龄（Underlying Age）对识别准确性的影响。研究引入了一种纵向方法，专注于注册（enrollment）和验证（verification）的准确性，使用 Clarkson University CITeR 研究组在 8 年间每 6 个月收集的 YFA 数据库。结果为提升儿童识别技术在新兴应用中的可靠性提供了重要见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07225v1",
      "published_date": "2024-08-01 19:40:55 UTC",
      "updated_date": "2024-08-01 19:40:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:59:12.351117"
    },
    {
      "arxiv_id": "2408.00876v2",
      "title": "On the Relationship Between Monotone and Squared Probabilistic Circuits",
      "title_zh": "翻译失败",
      "authors": [
        "Benjie Wang",
        "Guy Van den Broeck"
      ],
      "abstract": "Probabilistic circuits are a unifying representation of functions as\ncomputation graphs of weighted sums and products. Their primary application is\nin probabilistic modeling, where circuits with non-negative weights (monotone\ncircuits) can be used to represent and learn density/mass functions, with\ntractable marginal inference. Recently, it was proposed to instead represent\ndensities as the square of the circuit function (squared circuits); this allows\nthe use of negative weights while retaining tractability, and can be\nexponentially more expressive efficient than monotone circuits. Unfortunately,\nwe show the reverse also holds, meaning that monotone circuits and squared\ncircuits are incomparable in general. This raises the question of whether we\ncan reconcile, and indeed improve upon the two modeling approaches. We answer\nin the positive by proposing Inception PCs, a novel type of circuit that\nnaturally encompasses both monotone circuits and squared circuits as special\ncases, and employs complex parameters. Empirically, we validate that Inception\nPCs can outperform both monotone and squared circuits on a range of tabular and\nimage datasets.",
      "tldr_zh": "这篇论文探讨了 Probabilistic Circuits 中的 Monotone Circuits（单调电路）和 Squared Circuits（平方电路）之间的关系。作者证明了两种电路在一般情况下不可比拟，即每一种都可能在某些方面更具优势，但无法相互取代。为解决这一问题，他们提出了一种新型 Inception PCs 框架，该框架将 Monotone Circuits 和 Squared Circuits 作为特殊情况，并引入复杂参数来提升表现力。实验结果显示，Inception PCs 在各种表格和图像数据集上超过了现有方法的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.00876v2",
      "published_date": "2024-08-01 18:56:08 UTC",
      "updated_date": "2025-02-24 23:53:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:59:24.747937"
    },
    {
      "arxiv_id": "2408.00872v2",
      "title": "Online Detection of Anomalies in Temporal Knowledge Graphs with Interpretability",
      "title_zh": "具有可解释性的时序知识图异常在线检测",
      "authors": [
        "Jiasheng Zhang",
        "Rex Ying",
        "Jie Shao"
      ],
      "abstract": "Temporal knowledge graphs (TKGs) are valuable resources for capturing\nevolving relationships among entities, yet they are often plagued by noise,\nnecessitating robust anomaly detection mechanisms. Existing dynamic graph\nanomaly detection approaches struggle to capture the rich semantics introduced\nby node and edge categories within TKGs, while TKG embedding methods lack\ninterpretability, undermining the credibility of anomaly detection. Moreover,\nthese methods falter in adapting to pattern changes and semantic drifts\nresulting from knowledge updates. To tackle these challenges, we introduce\nAnoT, an efficient TKG summarization method tailored for interpretable online\nanomaly detection in TKGs. AnoT begins by summarizing a TKG into a novel rule\ngraph, enabling flexible inference of complex patterns in TKGs. When new\nknowledge emerges, AnoT maps it onto a node in the rule graph and traverses the\nrule graph recursively to derive the anomaly score of the knowledge. The\ntraversal yields reachable nodes that furnish interpretable evidence for the\nvalidity or the anomalous of the new knowledge. Overall, AnoT embodies a\ndetector-updater-monitor architecture, encompassing a detector for offline TKG\nsummarization and online scoring, an updater for real-time rule graph updates\nbased on emerging knowledge, and a monitor for estimating the approximation\nerror of the rule graph. Experimental results on four real-world datasets\ndemonstrate that AnoT surpasses existing methods significantly in terms of\naccuracy and interoperability. All of the raw datasets and the implementation\nof AnoT are provided in https://github.com/zjs123/ANoT.",
      "tldr_zh": "本研究针对时间知识图（Temporal Knowledge Graphs, TKGs）中存在的噪声和异常检测挑战，提出了一种高效的在线异常检测方法AnoT，以提升检测的可解释性。AnoT首先将TKGs总结为规则图（rule graph），用于推断复杂模式；当新知识出现时，通过映射节点并递归遍历规则图计算异常分数，并提供可解释证据。整体架构包括detector-updater-monitor组件，支持离线总结、在线评分、实时更新和错误估计。在四个真实数据集的实验中，AnoT在准确性和互操作性上显著优于现有方法。",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 10 figures. Accepted by SIGMOD 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.00872v2",
      "published_date": "2024-08-01 18:46:05 UTC",
      "updated_date": "2024-09-02 17:41:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:59:37.630724"
    },
    {
      "arxiv_id": "2408.00863v1",
      "title": "UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Juzheng Zhang",
        "Yatao Bian",
        "Yongqiang Chen",
        "Quanming Yao"
      ],
      "abstract": "The remarkable success of Large Language Models (LLMs) across diverse tasks\nhas driven the research community to extend their capabilities to molecular\napplications. However, most molecular LLMs employ adapter-based architectures\nthat do not treat molecule and text modalities equally and lack a supervision\nsignal for the molecule modality. To address these issues, we introduce UniMoT,\na Unified Molecule-Text LLM adopting a tokenizer-based architecture that\nexpands the vocabulary of LLM with molecule tokens. Specifically, we introduce\na Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge\nthe modality gap between molecule and text. This tokenizer transforms molecules\ninto sequences of molecule tokens with causal dependency, encapsulating\nhigh-level molecular and textual information. Equipped with this tokenizer,\nUniMoT can unify molecule and text modalities under a shared token\nrepresentation and an autoregressive training paradigm, enabling it to\ninterpret molecules as a foreign language and generate them as text. Following\na four-stage training scheme, UniMoT emerges as a multi-modal generalist\ncapable of performing both molecule-to-text and text-to-molecule tasks.\nExtensive experiments demonstrate that UniMoT achieves state-of-the-art\nperformance across a wide range of molecule comprehension and generation tasks.",
      "tldr_zh": "该研究提出 UniMoT，一种统一的分子-文本语言模型，使用基于 tokenizers 的架构，将分子 tokens 扩展到 LLM 词汇表，以平等处理分子和文本模态并提供分子监督信号。具体地，UniMoT 采用 Vector Quantization-driven tokenizer 和 Q-Former 桥接模态差距，将分子转化为有因果依赖的 tokens 序列，实现共享表示和自回归训练，使模型能将分子视为外语进行解释和生成。实验结果显示，UniMoT 通过四阶段训练方案，在广泛的分子理解和生成任务（如分子到文本和文本到分子）上达到最先进性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00863v1",
      "published_date": "2024-08-01 18:31:31 UTC",
      "updated_date": "2024-08-01 18:31:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:59:50.193260"
    },
    {
      "arxiv_id": "2408.00860v3",
      "title": "UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with Ultrasound Reflection Direction Parameterization",
      "title_zh": "UlRe-NeRF：",
      "authors": [
        "Ziwen Guo",
        "Zi Fang",
        "Zhuang Fu"
      ],
      "abstract": "Three-dimensional ultrasound imaging is a critical technology widely used in\nmedical diagnostics. However, traditional 3D ultrasound imaging methods have\nlimitations such as fixed resolution, low storage efficiency, and insufficient\ncontextual connectivity, leading to poor performance in handling complex\nartifacts and reflection characteristics. Recently, techniques based on NeRF\n(Neural Radiance Fields) have made significant progress in view synthesis and\n3D reconstruction, but there remains a research gap in high-quality ultrasound\nimaging. To address these issues, we propose a new model, UlRe-NeRF, which\ncombines implicit neural networks and explicit ultrasound volume rendering into\nan ultrasound neural rendering architecture. This model incorporates reflection\ndirection parameterization and harmonic encoding, using a directional MLP\nmodule to generate view-dependent high-frequency reflection intensity\nestimates, and a spatial MLP module to produce the medium's physical property\nparameters. These parameters are used in the volume rendering process to\naccurately reproduce the propagation and reflection behavior of ultrasound\nwaves in the medium. Experimental results demonstrate that the UlRe-NeRF model\nsignificantly enhances the realism and accuracy of high-fidelity ultrasound\nimage reconstruction, especially in handling complex medium structures.",
      "tldr_zh": "本文提出 UlRe-NeRF 模型，用于解决传统 3D 超声成像的局限性，如固定分辨率、低存储效率和处理复杂伪影反射的不足。UlRe-NeRF 结合 Neural Radiance Fields (NeRF) 的隐式神经网络和显式超声体渲染，引入反射方向参数化和谐波编码，通过 directional MLP 模块生成视图相关的反射强度估计，以及 spatial MLP 模块输出介质的物理属性参数。模型利用这些参数在 volume rendering 过程中准确模拟超声波的传播和反射行为。实验结果表明，UlRe-NeRF 显著提升了高保真超声图像重建的真实性和准确性，尤其在处理复杂介质结构方面。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00860v3",
      "published_date": "2024-08-01 18:22:29 UTC",
      "updated_date": "2024-09-13 13:40:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:00:02.696941"
    },
    {
      "arxiv_id": "2408.00859v2",
      "title": "GLoCIM: Global-view Long Chain Interest Modeling for news recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Yang",
        "Wenhui Wang",
        "Tao Qi",
        "Peng Zhang",
        "Tianyun Zhang",
        "Ru Zhang",
        "Jianyi Liu",
        "Yongfeng Huang"
      ],
      "abstract": "Accurately recommending candidate news articles to users has always been the\ncore challenge of news recommendation system. News recommendations often\nrequire modeling of user interest to match candidate news. Recent efforts have\nprimarily focused on extracting local subgraph information in a global click\ngraph constructed by the clicked news sequence of all users. Howerer, the\ncomputational complexity of extracting global click graph information has\nhindered the ability to utilize far-reaching linkage which is hidden between\ntwo distant nodes in global click graph collaboratively among similar users. To\novercome the problem above, we propose a Global-view Long Chain Interests\nModeling for news recommendation (GLoCIM), which combines neighbor interest\nwith long chain interest distilled from a global click graph, leveraging the\ncollaboration among similar users to enhance news recommendation. We therefore\ndesign a long chain selection algorithm and long chain interest encoder to\nobtain global-view long chain interest from the global click graph. We design a\ngated network to integrate long chain interest with neighbor interest to\nachieve the collaborative interest among similar users. Subsequently we\naggregate it with local news category-enhanced representation to generate final\nuser representation. Then candidate news representation can be formed to match\nuser representation to achieve news recommendation. Experimental results on\nreal-world datasets validate the effectiveness of our method to improve the\nperformance of news recommendation.",
      "tldr_zh": "本文提出 GLoCIM 方法，用于新闻推荐系统，通过结合邻居兴趣和从 global click graph 中提炼的 long chain interest，利用类似用户间的协作来提升用户兴趣建模。GLoCIM 设计了 long chain selection algorithm 和 long chain interest encoder 来获取全局视角的长链兴趣，并使用 gated network 整合长链兴趣与邻居兴趣。最终，用户表示通过聚合这些兴趣与本地新闻类别增强表示生成，并与候选新闻表示匹配进行推荐。实验在真实数据集上验证了该方法的有效性，提高了新闻推荐性能。",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00859v2",
      "published_date": "2024-08-01 18:17:25 UTC",
      "updated_date": "2024-09-24 16:54:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:00:14.425646"
    },
    {
      "arxiv_id": "2408.00838v2",
      "title": "Calibrating Bayesian Generative Machine Learning for Bayesiamplification",
      "title_zh": "翻译失败",
      "authors": [
        "Sebastian Bieringer",
        "Sascha Diefenbacher",
        "Gregor Kasieczka",
        "Mathias Trabs"
      ],
      "abstract": "Recently, combinations of generative and Bayesian machine learning have been\nintroduced in particle physics for both fast detector simulation and inference\ntasks. These neural networks aim to quantify the uncertainty on the generated\ndistribution originating from limited training statistics. The interpretation\nof a distribution-wide uncertainty however remains ill-defined. We show a clear\nscheme for quantifying the calibration of Bayesian generative machine learning\nmodels. For a Continuous Normalizing Flow applied to a low-dimensional toy\nexample, we evaluate the calibration of Bayesian uncertainties from either a\nmean-field Gaussian weight posterior, or Monte Carlo sampling network weights,\nto gauge their behaviour on unsteady distribution edges. Well calibrated\nuncertainties can then be used to roughly estimate the number of uncorrelated\ntruth samples that are equivalent to the generated sample and clearly indicate\ndata amplification for smooth features of the distribution.",
      "tldr_zh": "该研究针对贝叶斯生成机器学习在粒子物理学中的应用，提出了一种量化模型校准的方案，以处理生成分布的不确定性问题，该不确定性源于训练数据有限。作者使用 Continuous Normalizing Flow 在一个低维玩具例子上评估校准效果，比较了 mean-field Gaussian weight posterior 和 Monte Carlo sampling network weights 的表现。结果显示，良好校准的不确定性可以估算等效的 uncorrelated truth samples 数量，并清晰地证明了 Bayesian amplification 在分布光滑特征上的数据放大效应。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "hep-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 6 figures, updated references, fixed typo",
      "pdf_url": "http://arxiv.org/pdf/2408.00838v2",
      "published_date": "2024-08-01 18:00:05 UTC",
      "updated_date": "2024-11-13 15:48:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:00:25.478778"
    },
    {
      "arxiv_id": "2408.00765v2",
      "title": "MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Weihao Yu",
        "Zhengyuan Yang",
        "Lingfeng Ren",
        "Linjie Li",
        "Jianfeng Wang",
        "Kevin Lin",
        "Chung-Ching Lin",
        "Zicheng Liu",
        "Lijuan Wang",
        "Xinchao Wang"
      ],
      "abstract": "MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4. The code, data, and\nleaderboard are accessible at https://github.com/yuweihao/MM-Vet.",
      "tldr_zh": "本研究引入了MM-Vet v2，这是一个挑战性基准，用于评估大型多模态模型（Large Multimodal Models）的整合能力，扩展了原版MM-Vet以包括“image-text sequence understanding”等新能力，评估模型处理图像和文本序列的能力，同时保持样本高质量并扩大评估规模。MM-Vet v2涵盖六大核心vision-language (VL)能力，包括识别、知识、空间意识、语言生成、OCR和数学。实验结果显示，Claude 3.5 Sonnet以71.8分领先，略高于GPT-4o的71.0分，而开源模型中InternVL2-Llama3-76B以68.4分位居首位，该基准的代码、数据和排行榜可访问https://github.com/yuweihao/MM-Vet。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Code, data and leaderboard: https://github.com/yuweihao/MM-Vet",
      "pdf_url": "http://arxiv.org/pdf/2408.00765v2",
      "published_date": "2024-08-01 17:59:54 UTC",
      "updated_date": "2024-12-01 06:08:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:00:36.962964"
    },
    {
      "arxiv_id": "2408.00764v3",
      "title": "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation",
      "title_zh": "AgentGen：通过环境和任务生成增强基于大型语言模型代理的规划",
      "authors": [
        "Mengkang Hu",
        "Pu Zhao",
        "Can Xu",
        "Qingfeng Sun",
        "Jianguang Lou",
        "Qingwei Lin",
        "Ping Luo",
        "Saravan Rajmohan"
      ],
      "abstract": "Large Language Model-based agents have garnered significant attention and are\nbecoming increasingly popular. Furthermore, planning ability is a crucial\ncomponent of an LLM-based agent, which generally entails achieving a desired\ngoal from an initial state. This paper investigates enhancing the planning\nabilities of LLMs through instruction tuning, referred to as agent training.\nRecent studies have demonstrated that utilizing expert-level trajectory for\ninstruction-tuning LLMs effectively enhances their planning capabilities.\nHowever, existing work primarily focuses on synthesizing trajectories from\nmanually designed planning tasks and environments. The labor-intensive nature\nof creating these environments and tasks impedes the generation of sufficiently\nvaried and extensive trajectories. To address this limitation, this paper\nexplores the automated synthesis of diverse environments and a gradual range of\nplanning tasks, from easy to difficult. We introduce a framework, AgentGen,\nthat leverages LLMs first to generate environments and subsequently generate\nplanning tasks conditioned on these environments. Specifically, to improve\nenvironmental diversity, we propose using an inspiration corpus composed of\nvarious domain-specific text segments as the context for synthesizing\nenvironments. Moreover, to increase the difficulty diversity of generated\nplanning tasks, we propose a bidirectional evolution method, Bi-Evol, that\nevolves planning tasks from easier and harder directions to synthesize a task\nset with a smoother difficulty curve. The evaluation results derived from\nAgentBoard show that AgentGen greatly improves LLMs' planning ability, e.g.,\nthe AgentGen instruction-tuned Llama-3.1-8B surpasses GPT-3.5 in overall\nperformance. Moreover, the AgentGen-tuned Llama-3.1-70B model achieves\nstate-of-the-art results in planning tasks. Project page:\nhttps://agent-gen.github.io/.",
      "tldr_zh": "本论文提出 AgentGen 框架，通过自动生成多样环境和规划任务来增强 Large Language Model (LLMs) 代理的规划能力，以解决现有方法依赖手动设计的局限性。具体而言，AgentGen 利用 inspiration corpus 作为上下文合成环境，并采用双向演化方法 (Bi-Evol) 生成从易到难的任务集，从而通过 instruction tuning 有效提升 LLMs 的规划性能。在 AgentBoard 上的评估中，AgentGen 微调的 Llama-3.1-8B 模型在整体表现上超过了 GPT-3.5，而 Llama-3.1-70B 模型达到了 state-of-the-art 水平。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by KDD 2025 (Research Track). Project page:\n  https://agent-gen.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2408.00764v3",
      "published_date": "2024-08-01 17:59:46 UTC",
      "updated_date": "2025-02-06 09:17:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:00:51.627673"
    },
    {
      "arxiv_id": "2408.00761v4",
      "title": "Tamper-Resistant Safeguards for Open-Weight LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Rishub Tamirisa",
        "Bhrugu Bharathi",
        "Long Phan",
        "Andy Zhou",
        "Alice Gatti",
        "Tarun Suresh",
        "Maxwell Lin",
        "Justin Wang",
        "Rowan Wang",
        "Ron Arel",
        "Andy Zou",
        "Dawn Song",
        "Bo Li",
        "Dan Hendrycks",
        "Mantas Mazeika"
      ],
      "abstract": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after hundreds of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that progress on tamper-resistance is\npossible, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.",
      "tldr_zh": "该研究针对Open-weight LLMs的安全问题，指出现有防护措施（如拒绝和取消学习）容易被篡改攻击破坏，例如通过简单微调移除。论文提出TAR方法，用于在Open-weight LLMs中构建防篡改安全机制，即使经过数百步微调，攻击者也无法消除这些防护。同时，实验评估和红队分析显示，TAR显著提高了模型的篡改抵抗力，同时保留了正常功能，为Open-weight LLMs的安全发布提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Website: https://www.tamper-resistant-safeguards.com",
      "pdf_url": "http://arxiv.org/pdf/2408.00761v4",
      "published_date": "2024-08-01 17:59:12 UTC",
      "updated_date": "2025-02-10 18:26:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:01:00.015856"
    },
    {
      "arxiv_id": "2408.00760v2",
      "title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Susung Hong"
      ],
      "abstract": "Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\nhttps://github.com/SusungHong/SEG-SDXL.",
      "tldr_zh": "该研究提出了一种名为 Smoothed Energy Guidance (SEG) 的新方法，用于提升无条件扩散模型的图像生成质量。SEG 通过能量视角分析自注意力机制，减少注意力能量景观的曲率，并利用高斯核参数调整来控制这一过程，同时引入查询模糊技术以避免二次计算复杂度。实验结果显示，SEG 在生成质量和减少副作用方面实现了 Pareto 改进，为高效的图像生成提供了训练-和条件-free 的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00760v2",
      "published_date": "2024-08-01 17:59:09 UTC",
      "updated_date": "2024-10-01 01:04:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:01:12.312083"
    },
    {
      "arxiv_id": "2408.00756v3",
      "title": "Segment anything model 2: an application to 2D and 3D medical images",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyu Dong",
        "Hanxue Gu",
        "Yaqian Chen",
        "Jichen Yang",
        "Yuwen Chen",
        "Maciej A. Mazurowski"
      ],
      "abstract": "Segment Anything Model (SAM) has gained significant attention because of its\nability to segment various objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we extensively evaluate SAM 2's ability\nto segment both 2D and 3D medical images by first collecting 21 medical imaging\ndatasets, including surgical videos, common 3D modalities such as computed\ntomography (CT), magnetic resonance imaging (MRI), and positron emission\ntomography (PET) as well as 2D modalities such as X-ray and ultrasound. Two\nevaluation settings of SAM 2 are considered: (1) multi-frame 3D segmentation,\nwhere prompts are provided to one or multiple slice(s) selected from the\nvolume, and (2) single-frame 2D segmentation, where prompts are provided to\neach slice. The former only applies to videos and 3D modalities, while the\nlatter applies to all datasets. Our results show that SAM 2 exhibits similar\nperformance as SAM under single-frame 2D segmentation, and has variable\nperformance under multi-frame 3D segmentation depending on the choices of\nslices to annotate, the direction of the propagation, the predictions utilized\nduring the propagation, etc. We believe our work enhances the understanding of\nSAM 2's behavior in the medical field and provides directions for future work\nin adapting SAM 2 to this domain. Our code is available at:\nhttps://github.com/mazurowski-lab/segment-anything2-medical-evaluation.",
      "tldr_zh": "这篇论文评估了 Segment Anything Model 2 (SAM 2) 在 2D 和 3D 医疗图像上的分割性能，特别是将其从视频输入扩展到医疗领域。研究者收集了 21 个数据集，包括手术视频、CT、MRI、PET、X-ray 和超声等，并设计了两种评估设置：多帧 3D 分割（在 3D 体数据中选择切片提供提示）和单帧 2D 分割（每个切片提供提示）。结果表明，SAM 2 在单帧 2D 分割中与原始 SAM 表现类似，但在多帧 3D 分割中性能因切片选择、传播方向和预测利用等因素而异。该工作增强了对 SAM 2 在医疗领域的理解，并为未来适应提供指导方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 13 figures. Codes are available at\n  https://github.com/mazurowski-lab/segment-anything2-medical-evaluation",
      "pdf_url": "http://arxiv.org/pdf/2408.00756v3",
      "published_date": "2024-08-01 17:57:25 UTC",
      "updated_date": "2024-08-22 16:38:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:01:27.967259"
    },
    {
      "arxiv_id": "2408.00753v2",
      "title": "A deep learning-enabled smart garment for accurate and versatile sleep conditions monitoring in daily life",
      "title_zh": "一种深度学习启用的智能服装，用于日常生活中准确且多功能的睡眠条件监测",
      "authors": [
        "Chenyu Tang",
        "Wentian Yi",
        "Muzi Xu",
        "Yuxuan Jin",
        "Zibo Zhang",
        "Xuhang Chen",
        "Caizhi Liao",
        "Peter Smielewski",
        "Luigi G. Occhipinti"
      ],
      "abstract": "In wearable smart systems, continuous monitoring and accurate classification\nof different sleep-related conditions are critical for enhancing sleep quality\nand preventing sleep-related chronic conditions. However, the requirements for\ndevice-skin coupling quality in electrophysiological sleep monitoring systems\nhinder the comfort and reliability of night wearing. Here, we report a\nwashable, skin-compatible smart garment sleep monitoring system that captures\nlocal skin strain signals under weak device-skin coupling conditions without\npositioning or skin preparation requirements. A printed textile-based strain\nsensor array responds to strain from 0.1% to 10% with a gauge factor as high as\n100 and shows independence to extrinsic motion artefacts via strain-isolating\nprinted pattern design. Through reversible starching treatment, ink penetration\ndepth during direct printing on garments is controlled to achieve\nbatch-to-batch performance variation < 10%. Coupled with deep learning,\nexplainable artificial intelligence (XAI), and transfer learning data\nprocessing, the smart garment is capable of classifying six sleep states with\nan accuracy of 98.6%, maintaining excellent explainability (classification with\nlow bias) and generalization (95% accuracy on new users with few-shot learning\nless than 15 samples per class) in practical applications, paving the way for\nnext-generation daily sleep healthcare management.",
      "tldr_zh": "本研究开发了一种基于深度学习的智能服装，用于精确且多功能的日常睡眠监测。该服装采用可洗涤、皮肤兼容的纺织基应变传感器阵列，能够在弱设备-皮肤耦合条件下捕捉局部皮肤应变信号，而无需精确定位或皮肤准备，通过应变隔离设计减少外部运动干扰。结合深度学习、可解释人工智能(XAI)和迁移学习，该系统可准确分类六种睡眠状态，达到98.6%的准确率，并展现出优秀的泛化能力（如在新用户上，少于15个样本每类的少样本学习准确率达95%）。这项创新为日常睡眠健康管理提供了舒适可靠的解决方案。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "20 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2408.00753v2",
      "published_date": "2024-08-01 17:56:25 UTC",
      "updated_date": "2024-10-03 16:13:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:01:38.254071"
    },
    {
      "arxiv_id": "2408.00751v1",
      "title": "A Policy-Gradient Approach to Solving Imperfect-Information Games with Iterate Convergence",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyang Liu",
        "Gabriele Farina",
        "Asuman Ozdaglar"
      ],
      "abstract": "Policy gradient methods have become a staple of any single-agent\nreinforcement learning toolbox, due to their combination of desirable\nproperties: iterate convergence, efficient use of stochastic trajectory\nfeedback, and theoretically-sound avoidance of importance sampling corrections.\nIn multi-agent imperfect-information settings (extensive-form games), however,\nit is still unknown whether the same desiderata can be guaranteed while\nretaining theoretical guarantees. Instead, sound methods for extensive-form\ngames rely on approximating counterfactual values (as opposed to Q values),\nwhich are incompatible with policy gradient methodologies. In this paper, we\ninvestigate whether policy gradient can be safely used in two-player zero-sum\nimperfect-information extensive-form games (EFGs). We establish positive\nresults, showing for the first time that a policy gradient method leads to\nprovable best-iterate convergence to a regularized Nash equilibrium in\nself-play.",
      "tldr_zh": "本研究探讨了policy gradient方法在多代理不完美信息游戏（extensive-form games, EFGs）中的应用，解决了传统方法依赖counterfactual values而非policy gradient的局限性。论文证明，在两玩家零和不完美信息EFGs中，policy gradient方法可实现best-iterate convergence，并在self-play中收敛到regularized Nash equilibrium。相比单代理强化学习中的优势，该方法保留了iterate convergence和高效处理随机轨迹反馈的特性，同时提供了理论保证。该贡献为不完美信息游戏的优化提供了新的可靠框架。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00751v1",
      "published_date": "2024-08-01 17:54:01 UTC",
      "updated_date": "2024-08-01 17:54:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:01:52.130566"
    },
    {
      "arxiv_id": "2408.00749v1",
      "title": "Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Venkat Margapuri",
        "Prapti Thapaliya",
        "Trevor Rife"
      ],
      "abstract": "Modern day studies show a high degree of correlation between high yielding\ncrop varieties and plants with upright leaf angles. It is observed that plants\nwith upright leaf angles intercept more light than those without upright leaf\nangles, leading to a higher rate of photosynthesis. Plant scientists and\nbreeders benefit from tools that can directly measure plant parameters in the\nfield i.e. on-site phenotyping. The estimation of leaf angles by manual means\nin a field setting is tedious and cumbersome. We mitigate the tedium using a\ncombination of the Mask R-CNN instance segmentation neural network, and Line\nSegment Transformer (LETR), a vision transformer. The proposed Computer Vision\n(CV) pipeline is applied on two image datasets, Summer 2015-Ames ULA and Summer\n2015- Ames MLA, with a combined total of 1,827 plant images collected in the\nfield using FieldBook, an Android application aimed at on-site phenotyping. The\nleaf angles estimated by the proposed pipeline on the image datasets are\ncompared to two independent manual measurements using ImageJ, a Java-based\nimage processing program developed at the National Institutes of Health and the\nLaboratory for Optical and Computational Instrumentation. The results, when\ncompared for similarity using the Cosine Similarity measure, exhibit 0.98\nsimilarity scores on both independent measurements of Summer 2015-Ames ULA and\nSummer 2015-Ames MLA image datasets, demonstrating the feasibility of the\nproposed pipeline for on-site measurement of leaf angles.",
      "tldr_zh": "该论文提出了一种使用 Mask R-CNN 实例分割神经网络和 LETR Vision Transformer 的计算机视觉管道，用于估计作物叶片角度，以简化现场表型测量。研究背景显示，竖直叶片角度与作物高产相关，因为它能增强光线拦截和光合作用。实验在 Summer 2015-Ames ULA 和 Summer 2015-Ames MLA 两个数据集上进行，共涉及 1,827 张现场图像，并与 ImageJ 的手动测量比较，结果显示余弦相似度（Cosine Similarity）达 0.98，证明了该管道的可靠性和实际应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00749v1",
      "published_date": "2024-08-01 17:52:10 UTC",
      "updated_date": "2024-08-01 17:52:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:02:01.467859"
    },
    {
      "arxiv_id": "2408.00741v1",
      "title": "DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency",
      "title_zh": "翻译失败",
      "authors": [
        "Jovan Stojkovic",
        "Chaojie Zhang",
        "Íñigo Goiri",
        "Josep Torrellas",
        "Esha Choukse"
      ],
      "abstract": "The rapid evolution and widespread adoption of generative large language\nmodels (LLMs) have made them a pivotal workload in various applications. Today,\nLLM inference clusters receive a large number of queries with strict Service\nLevel Objectives (SLOs). To achieve the desired performance, these models\nexecute on power-hungry GPUs causing the inference clusters to consume large\namount of energy and, consequently, result in excessive carbon emissions.\nFortunately, we find that there is a great opportunity to exploit the\nheterogeneity in inference compute properties and fluctuations in inference\nworkloads, to significantly improve energy-efficiency. However, such a diverse\nand dynamic environment creates a large search-space where different system\nconfigurations (e.g., number of instances, model parallelism, and GPU\nfrequency) translate into different energy-performance trade-offs. To address\nthese challenges, we propose DynamoLLM, the first energy-management framework\nfor LLM inference environments. DynamoLLM automatically and dynamically\nreconfigures the inference cluster to optimize for energy and cost of LLM\nserving under the service's performance SLOs. We show that at a service-level,\nDynamoLLM conserves 53% energy and 38% operational carbon emissions, and\nreduces 61% cost to the customer, while meeting the latency SLOs.",
      "tldr_zh": "该论文探讨了生成式大语言模型(LLMs)推理集群在追求性能的同时面临的能耗和碳排放挑战。DynamoLLM 作为首个能效管理框架，通过利用推理计算的异质性和工作负载波动，自动动态重新配置集群（如调整实例数、模型并行性和 GPU 频率），以优化能效和成本，同时满足服务水平目标(SLOs)。实验结果显示，DynamoLLM 在服务级别可节省53%能耗、38%操作碳排放和61%成本，同时保持延迟SLOs的合规性。",
      "categories": [
        "cs.AI",
        "cs.AR",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00741v1",
      "published_date": "2024-08-01 17:40:45 UTC",
      "updated_date": "2024-08-01 17:40:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:02:13.373719"
    },
    {
      "arxiv_id": "2408.00727v3",
      "title": "Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions",
      "title_zh": "通过迭代后续问题提升医学中的检索增强生成",
      "authors": [
        "Guangzhi Xiong",
        "Qiao Jin",
        "Xiao Wang",
        "Minjia Zhang",
        "Zhiyong Lu",
        "Aidong Zhang"
      ],
      "abstract": "The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a conventional RAG system\nand they will be further used to guide the query generation in the next\niteration. Our experiments show the improved performance of various LLMs\nbrought by i-MedRAG compared with conventional RAG on complex questions from\nclinical vignettes in the United States Medical Licensing Examination (USMLE),\nas well as various knowledge tests in the Massive Multitask Language\nUnderstanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all\nexisting prompt engineering and fine-tuning methods on GPT-3.5, achieving an\naccuracy of 69.68% on the MedQA dataset. In addition, we characterize the\nscaling properties of i-MedRAG with different iterations of follow-up queries\nand different numbers of queries per iteration. Our case studies show that\ni-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing\nan in-depth analysis of medical questions. To the best of our knowledge, this\nis the first-of-its-kind study on incorporating follow-up queries into medical\nRAG. The implementation of i-MedRAG is available at\nhttps://github.com/Teddy-XiongGZ/MedRAG.",
      "tldr_zh": "该研究针对大语言模型（LLMs）在医疗问答中的幻觉问题和知识更新局限性，提出了一种改进的Retrieval-Augmented Generation（RAG）方法，即iterative RAG for medicine（i-MedRAG）。i-MedRAG允许LLMs基于先前信息搜索结果迭代生成后续查询，并在每个迭代中使用常规RAG系统进行回答，从而处理复杂医疗问题需要多轮信息求证的场景。实验结果显示，i-MedRAG显著提升了各种LLMs在United States Medical Licensing Examination（USMLE）临床案例和Massive Multitask Language Understanding（MMLU）知识测试中的性能，其中零样本i-MedRAG在MedQA数据集上达到69.68%的准确率，优于现有提示工程和微调方法。此外，该方法能灵活形成推理链进行深入分析，并提供了开源实现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to PSB 2025",
      "pdf_url": "http://arxiv.org/pdf/2408.00727v3",
      "published_date": "2024-08-01 17:18:17 UTC",
      "updated_date": "2024-10-11 01:00:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:02:26.936899"
    },
    {
      "arxiv_id": "2408.00818v1",
      "title": "Y Social: an LLM-powered Social Media Digital Twin",
      "title_zh": "翻译失败",
      "authors": [
        "Giulio Rossetti",
        "Massimo Stella",
        "Rémy Cazabet",
        "Katherine Abramski",
        "Erica Cau",
        "Salvatore Citraro",
        "Andrea Failla",
        "Riccardo Improta",
        "Virginia Morini",
        "Valentina Pansanella"
      ],
      "abstract": "In this paper we introduce Y, a new-generation digital twin designed to\nreplicate an online social media platform. Digital twins are virtual replicas\nof physical systems that allow for advanced analyses and experimentation. In\nthe case of social media, a digital twin such as Y provides a powerful tool for\nresearchers to simulate and understand complex online interactions. {\\tt Y}\nleverages state-of-the-art Large Language Models (LLMs) to replicate\nsophisticated agent behaviors, enabling accurate simulations of user\ninteractions, content dissemination, and network dynamics. By integrating these\naspects, Y offers valuable insights into user engagement, information spread,\nand the impact of platform policies. Moreover, the integration of LLMs allows Y\nto generate nuanced textual content and predict user responses, facilitating\nthe study of emergent phenomena in online environments.\n  To better characterize the proposed digital twin, in this paper we describe\nthe rationale behind its implementation, provide examples of the analyses that\ncan be performed on the data it enables to be generated, and discuss its\nrelevance for multidisciplinary research.",
      "tldr_zh": "本文提出 Y Social，这是一个基于 Large Language Models (LLMs) 的社交媒体数字孪生系统，用于模拟在线平台的用户互动和网络动态。Y 通过 LLMs 复制复杂代理行为，实现对内容传播、用户参与以及平台政策影响的精确模拟和预测，从而帮助研究者分析信息扩散和在线环境中的紧急现象。该系统提供宝贵的数据生成和分析工具，支持多学科研究，如用户行为实验和政策评估。",
      "categories": [
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00818v1",
      "published_date": "2024-08-01 17:16:21 UTC",
      "updated_date": "2024-08-01 17:16:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:02:36.627118"
    },
    {
      "arxiv_id": "2408.00724v3",
      "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yangzhen Wu",
        "Zhiqing Sun",
        "Shanda Li",
        "Sean Welleck",
        "Yiming Yang"
      ],
      "abstract": "While the scaling laws of large language models (LLMs) training have been\nextensively studied, optimal inference configurations of LLMs remain\nunderexplored. We study inference scaling laws (aka test-time scaling laws) and\ncompute-optimal inference, focusing on the trade-offs between model sizes and\ngenerating additional tokens with different inference strategies. As a first\nstep towards understanding and designing compute-optimal inference methods, we\nstudied cost-performance trade-offs for inference strategies such as greedy\nsearch, majority voting, best-of-$n$, weighted voting, and two different tree\nsearch algorithms, using different model sizes and compute budgets. Our\nfindings suggest that scaling inference compute with inference strategies can\nbe more computationally efficient than scaling model parameters. Additionally,\nsmaller models combined with advanced inference algorithms offer Pareto-optimal\ntrade-offs in cost and performance. For example, the Llemma-7B model, when\npaired with our novel tree search algorithm, consistently outperforms the\nLlemma-34B model across all tested inference strategies on the MATH benchmark.\nWe hope these insights contribute to a deeper understanding of inference\nscaling laws (test-time scaling laws) for LLMs.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)的推理缩放定律(inference scaling laws)，通过实证分析不同推理策略（如greedy search、majority voting、best-of-n、weighted voting和tree search算法）的计算最优配置，聚焦模型大小与生成额外标记的成本-性能权衡。结果表明，扩展推理计算比增加模型参数更高效，且较小模型（如Llemma-7B）结合高级tree search算法，能在MATH基准上超越更大模型（如Llemma-34B）。这些发现为设计高效LLMs推理方法提供了新见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00724v3",
      "published_date": "2024-08-01 17:16:04 UTC",
      "updated_date": "2025-03-03 07:53:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:02:49.749091"
    },
    {
      "arxiv_id": "2408.00722v2",
      "title": "Pathway to Secure and Trustworthy ZSM for LLMs: Attacks, Defense, and Opportunities",
      "title_zh": "翻译失败",
      "authors": [
        "Sunder Ali Khowaja",
        "Parus Khuwaja",
        "Kapal Dev",
        "Hussam Al Hamadi",
        "Engin Zeydan"
      ],
      "abstract": "Recently, large language models (LLMs) have been gaining a lot of interest\ndue to their adaptability and extensibility in emerging applications, including\ncommunication networks. It is anticipated that ZSM networks will be able to\nsupport LLMs as a service, as they provide ultra reliable low-latency\ncommunications and closed loop massive connectivity. However, LLMs are\nvulnerable to data and model privacy issues that affect the trustworthiness of\nLLMs to be deployed for user-based services. In this paper, we explore the\nsecurity vulnerabilities associated with fine-tuning LLMs in ZSM networks, in\nparticular the membership inference attack. We define the characteristics of an\nattack network that can perform a membership inference attack if the attacker\nhas access to the fine-tuned model for the downstream task. We show that the\nmembership inference attacks are effective for any downstream task, which can\nlead to a personal data breach when using LLM as a service. The experimental\nresults show that the attack success rate of maximum 92% can be achieved on\nnamed entity recognition task. Based on the experimental analysis, we discuss\npossible defense mechanisms and present possible research directions to make\nthe LLMs more trustworthy in the context of ZSM networks.",
      "tldr_zh": "该论文探讨了大型语言模型（LLMs）在 ZSM 网络中部署的安全挑战，特别是在微调过程中面临的 membership inference attack，该攻击可能导致个人数据泄露。研究定义了攻击网络的特征，并通过实验证明这种攻击对各种下游任务有效，在命名实体识别任务上成功率最高达92%。最后，论文基于分析提出可能的防御机制和未来研究方向，以提升 LLMs 在 ZSM 网络中的可信度和安全性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00722v2",
      "published_date": "2024-08-01 17:15:13 UTC",
      "updated_date": "2025-01-06 15:09:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:03:00.809157"
    },
    {
      "arxiv_id": "2408.00714v2",
      "title": "SAM 2: Segment Anything in Images and Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Nikhila Ravi",
        "Valentin Gabeur",
        "Yuan-Ting Hu",
        "Ronghang Hu",
        "Chaitanya Ryali",
        "Tengyu Ma",
        "Haitham Khedr",
        "Roman Rädle",
        "Chloe Rolland",
        "Laura Gustafson",
        "Eric Mintun",
        "Junting Pan",
        "Kalyan Vasudev Alwala",
        "Nicolas Carion",
        "Chao-Yuan Wu",
        "Ross Girshick",
        "Piotr Dollár",
        "Christoph Feichtenhofer"
      ],
      "abstract": "We present Segment Anything Model 2 (SAM 2), a foundation model towards\nsolving promptable visual segmentation in images and videos. We build a data\nengine, which improves model and data via user interaction, to collect the\nlargest video segmentation dataset to date. Our model is a simple transformer\narchitecture with streaming memory for real-time video processing. SAM 2\ntrained on our data provides strong performance across a wide range of tasks.\nIn video segmentation, we observe better accuracy, using 3x fewer interactions\nthan prior approaches. In image segmentation, our model is more accurate and 6x\nfaster than the Segment Anything Model (SAM). We believe that our data, model,\nand insights will serve as a significant milestone for video segmentation and\nrelated perception tasks. We are releasing our main model, dataset, as well as\ncode for model training and our demo.",
      "tldr_zh": "我们介绍了Segment Anything Model 2 (SAM 2)，一个针对图像和视频中可提示视觉分割的基座模型，通过构建一个数据引擎利用用户交互来改进模型和数据，并收集了迄今为止最大的视频分割数据集。模型采用简单Transformer架构，结合流式内存，支持实时视频处理。实验结果显示，SAM 2在视频分割任务中比先前方法准确性更高，且仅需3倍更少的交互；在图像分割中，它比原始Segment Anything Model (SAM)准确性更强且速度快6倍。我们将发布模型、数据集、代码和演示，这有望成为视频分割及相关感知任务的重要里程碑。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Website: https://ai.meta.com/sam2",
      "pdf_url": "http://arxiv.org/pdf/2408.00714v2",
      "published_date": "2024-08-01 17:00:08 UTC",
      "updated_date": "2024-10-28 16:37:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:03:12.888929"
    },
    {
      "arxiv_id": "2408.00711v1",
      "title": "Investigating Brain Connectivity and Regional Statistics from EEG for early stage Parkinson's Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Amarpal Sahota",
        "Amber Roguski",
        "Matthew W Jones",
        "Zahraa S. Abdallah",
        "Raul Santos-Rodriguez"
      ],
      "abstract": "We evaluate the effectiveness of combining brain connectivity metrics with\nsignal statistics for early stage Parkinson's Disease (PD) classification using\nelectroencephalogram data (EEG). The data is from 5 arousal states - wakeful\nand four sleep stages (N1, N2, N3 and REM). Our pipeline uses an Ada Boost\nmodel for classification on a challenging early stage PD classification task\nwith with only 30 participants (11 PD , 19 Healthy Control). Evaluating 9 brain\nconnectivity metrics we find the best connectivity metric to be different for\neach arousal state with Phase Lag Index achieving the highest individual\nclassification accuracy of 86\\% on N1 data. Further to this our pipeline using\nregional signal statistics achieves an accuracy of 78\\%, using brain\nconnectivity only achieves an accuracy of 86\\% whereas combining the two\nachieves a best accuracy of 91\\%. This best performance is achieved on N1 data\nusing Phase Lag Index (PLI) combined with statistics derived from the frequency\ncharacteristics of the EEG signal. This model also achieves a recall of 80 \\%\nand precision of 96\\%. Furthermore we find that on data from each arousal\nstate, combining PLI with regional signal statistics improves classification\naccuracy versus using signal statistics or brain connectivity alone. Thus we\nconclude that combining brain connectivity statistics with regional EEG\nstatistics is optimal for classifier performance on early stage Parkinson's.\nAdditionally, we find outperformance of N1 EEG for classification of\nParkinson's and expect this could be due to disrupted N1 sleep in PD. This\nshould be explored in future work.",
      "tldr_zh": "这篇论文调查了使用 EEG 数据结合脑连接指标和区域信号统计来分类早期帕金森病 (PD) 的方法，涉及5种唤醒状态（包括清醒和睡眠阶段如 N1、N2、N3、REM），并采用 Ada Boost 模型在仅有30名参与者（11名 PD、19名健康对照）的数据集上进行评估。研究评估了9种脑连接指标，发现 Phase Lag Index (PLI) 在 N1 数据上单独达到86%分类准确率，而结合区域信号统计后，准确率最高提升至91%，并在所有唤醒状态下均表现出改善。最终结论是，结合脑连接统计和区域 EEG 统计是最优策略，且 N1 阶段数据表现出色，可能与 PD 中的睡眠紊乱相关，建议未来工作进一步探索。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00711v1",
      "published_date": "2024-08-01 16:58:21 UTC",
      "updated_date": "2024-08-01 16:58:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:03:27.596016"
    },
    {
      "arxiv_id": "2408.00706v1",
      "title": "Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaofeng Liu",
        "Jonghye Woo",
        "Chao Ma",
        "Jinsong Ouyang",
        "Georges El Fakhri"
      ],
      "abstract": "Delineating lesions and anatomical structure is important for image-guided\ninterventions. Point-supervised medical image segmentation (PSS) has great\npotential to alleviate costly expert delineation labeling. However, due to the\nlack of precise size and boundary guidance, the effectiveness of PSS often\nfalls short of expectations. Although recent vision foundational models, such\nas the medical segment anything model (MedSAM), have made significant\nadvancements in bounding-box-prompted segmentation, it is not straightforward\nto utilize point annotation, and is prone to semantic ambiguity. In this\npreliminary study, we introduce an iterative framework to facilitate\nsemantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt\ngenerator (SBPG) module has the capacity to convert the point input into\npotential pseudo bounding box suggestions, which are explicitly refined by the\nprototype-based semantic similarity. This is then succeeded by a prompt-guided\nspatial refinement (PGSR) module that harnesses the exceptional\ngeneralizability of MedSAM to infer the segmentation mask, which also updates\nthe box proposal seed in SBPG. Performance can be progressively improved with\nadequate iterations. We conducted an evaluation on BraTS2018 for the\nsegmentation of whole brain tumors and demonstrated its superior performance\ncompared to traditional PSS methods and on par with box-supervised methods.",
      "tldr_zh": "这篇论文提出了一种基于 Box-prompted MedSAM 的点监督脑肿瘤分割框架（Point-supervised Segmentation, PSS），旨在解决传统 PSS 方法在精确大小和边界指导方面的不足。框架包括 Semantic Box-Prompt Generator (SBPG) 模块，将点输入转换为伪边界框建议，并通过原型-based 语义相似性进行精炼；以及 Prompt-Guided Spatial Refinement (PGSR) 模块，利用 MedSAM 的强大泛化能力推断分割掩码并迭代更新框提案。实验结果显示，该方法在 BraTS2018 数据集上的全脑肿瘤分割任务中，超过了传统 PSS 方法的性能，并与框监督方法相当。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV",
        "physics.med-ph"
      ],
      "primary_category": "cs.CV",
      "comment": "2024 IEEE Nuclear Science Symposium and Medical Imaging Conference",
      "pdf_url": "http://arxiv.org/pdf/2408.00706v1",
      "published_date": "2024-08-01 16:52:39 UTC",
      "updated_date": "2024-08-01 16:52:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:03:38.361759"
    },
    {
      "arxiv_id": "2408.00703v1",
      "title": "Future of Artificial Intelligence in Agile Software Development",
      "title_zh": "人工智能在敏捷软件开发中的未来",
      "authors": [
        "Mariyam Mahboob",
        "Mohammed Rayyan Uddin Ahmed",
        "Zoiba Zia",
        "Mariam Shakeel Ali",
        "Ayman Khaleel Ahmed"
      ],
      "abstract": "The advent of Artificial intelligence has promising advantages that can be\nutilized to transform the landscape of software project development. The\nSoftware process framework consists of activities that constantly require\nroutine human interaction, leading to the possibility of errors and\nuncertainties. AI can assist software development managers, software testers,\nand other team members by leveraging LLMs, GenAI models, and AI agents to\nperform routine tasks, risk analysis and prediction, strategy recommendations,\nand support decision making. AI has the potential to increase efficiency and\nreduce the risks encountered by the project management team while increasing\nthe project success rates. Additionally, it can also break down complex notions\nand development processes for stakeholders to make informed decisions. In this\npaper, we propose an approach in which AI tools and technologies can be\nutilized to bestow maximum assistance for agile software projects, which have\nbecome increasingly favored in the industry in recent years.",
      "tldr_zh": "人工智能（AI）在敏捷软件开发中的应用前景广阔，能够通过 LLMs、GenAI 模型和 AI agents 辅助处理例行任务、风险分析、策略推荐以及决策支持，从而提高团队效率并降低项目风险。该论文强调 AI 有助于减少人为错误、提升项目成功率，并帮助利益相关者理解复杂开发过程。作为主要贡献，论文提出了一种整合 AI 工具和技术的方法，以最大化支持敏捷软件项目的发展。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00703v1",
      "published_date": "2024-08-01 16:49:50 UTC",
      "updated_date": "2024-08-01 16:49:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:03:50.305082"
    },
    {
      "arxiv_id": "2408.00695v1",
      "title": "Accelerating Full Waveform Inversion By Transfer Learning",
      "title_zh": "通过迁移学习加速全波形反演",
      "authors": [
        "Divya Shyam Singh",
        "Leon Herrmann",
        "Qing Sun",
        "Tim Bürchner",
        "Felix Dietrich",
        "Stefan Kollmannsberger"
      ],
      "abstract": "Full waveform inversion (FWI) is a powerful tool for reconstructing material\nfields based on sparsely measured data obtained by wave propagation. For\nspecific problems, discretizing the material field with a neural network (NN)\nimproves the robustness and reconstruction quality of the corresponding\noptimization problem. We call this method NN-based FWI. Starting from an\ninitial guess, the weights of the NN are iteratively updated to fit the\nsimulated wave signals to the sparsely measured data set. For gradient-based\noptimization, a suitable choice of the initial guess, i.e., a suitable NN\nweight initialization, is crucial for fast and robust convergence.\n  In this paper, we introduce a novel transfer learning approach to further\nimprove NN-based FWI. This approach leverages supervised pretraining to provide\na better NN weight initialization, leading to faster convergence of the\nsubsequent optimization problem. Moreover, the inversions yield physically more\nmeaningful local minima. The network is pretrained to predict the unknown\nmaterial field using the gradient information from the first iteration of\nconventional FWI. In our computational experiments on two-dimensional domains,\nthe training data set consists of reference simulations with arbitrarily\npositioned elliptical voids of different shapes and orientations. We compare\nthe performance of the proposed transfer learning NN-based FWI with three other\nmethods: conventional FWI, NN-based FWI without pretraining and conventional\nFWI with an initial guess predicted from the pretrained NN. Our results show\nthat transfer learning NN-based FWI outperforms the other methods in terms of\nconvergence speed and reconstruction quality.",
      "tldr_zh": "本研究提出了一种基于转移学习（Transfer Learning）的改进方法，用于加速全波形反演（FWI），以重建基于稀疏波传播数据的材料场。方法通过神经网络（NN）预训练来优化初始权重，利用常规 FWI 的第一迭代梯度信息预测未知材料场，从而提升优化过程的收敛速度和鲁棒性。实验结果显示，在二维域上使用椭圆空洞模拟作为训练数据，该转移学习增强的 NN-based FWI 比传统 FWI 及其他变体在重建质量和收敛速度上表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00695v1",
      "published_date": "2024-08-01 16:39:06 UTC",
      "updated_date": "2024-08-01 16:39:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:04:04.240150"
    },
    {
      "arxiv_id": "2408.00686v1",
      "title": "Can Developers Prompt? A Controlled Experiment for Code Documentation Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Hans-Alexander Kruse",
        "Tim Puhlfürß",
        "Walid Maalej"
      ],
      "abstract": "Large language models (LLMs) bear great potential for automating tedious\ndevelopment tasks such as creating and maintaining code documentation. However,\nit is unclear to what extent developers can effectively prompt LLMs to create\nconcise and useful documentation. We report on a controlled experiment with 20\nprofessionals and 30 computer science students tasked with code documentation\ngeneration for two Python functions. The experimental group freely entered\nad-hoc prompts in a ChatGPT-like extension of Visual Studio Code, while the\ncontrol group executed a predefined few-shot prompt. Our results reveal that\nprofessionals and students were unaware of or unable to apply prompt\nengineering techniques. Especially students perceived the documentation\nproduced from ad-hoc prompts as significantly less readable, less concise, and\nless helpful than documentation from prepared prompts. Some professionals\nproduced higher quality documentation by just including the keyword Docstring\nin their ad-hoc prompts. While students desired more support in formulating\nprompts, professionals appreciated the flexibility of ad-hoc prompting.\nParticipants in both groups rarely assessed the output as perfect. Instead,\nthey understood the tools as support to iteratively refine the documentation.\nFurther research is needed to understand which prompting skills and preferences\ndevelopers have and which support they need for certain tasks.",
      "tldr_zh": "这篇论文通过一个控制实验，评估了专业开发者和计算机科学学生使用大型语言模型（LLMs）生成代码文档的提示能力。实验涉及20名专业者和30名学生，他们为两个Python函数生成文档，实验组使用自由ad-hoc提示，而控制组使用预定义的few-shot prompt。结果显示，参与者普遍无法有效应用prompt engineering技巧，学生认为自由提示生成的文档在可读性、简洁性和帮助性上不如预定义提示；然而，有些专业者通过在提示中加入“Docstring”关键词提升了输出质量。研究强调，开发者和学生将LLMs视为迭代改进工具，并建议进一步探索他们的提示技能、偏好和所需支持。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the 40th IEEE International Conference on Software\n  Maintenance and Evolution (ICSME)",
      "pdf_url": "http://arxiv.org/pdf/2408.00686v1",
      "published_date": "2024-08-01 16:28:14 UTC",
      "updated_date": "2024-08-01 16:28:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:04:24.151926"
    },
    {
      "arxiv_id": "2408.00682v1",
      "title": "Learning in Multi-Objective Public Goods Games with Non-Linear Utilities",
      "title_zh": "在多目标公共物品游戏中带有非线性效用的学习",
      "authors": [
        "Nicole Orzan",
        "Erman Acar",
        "Davide Grossi",
        "Patrick Mannion",
        "Roxana Rădulescu"
      ],
      "abstract": "Addressing the question of how to achieve optimal decision-making under risk\nand uncertainty is crucial for enhancing the capabilities of artificial agents\nthat collaborate with or support humans. In this work, we address this question\nin the context of Public Goods Games. We study learning in a novel\nmulti-objective version of the Public Goods Game where agents have different\nrisk preferences, by means of multi-objective reinforcement learning. We\nintroduce a parametric non-linear utility function to model risk preferences at\nthe level of individual agents, over the collective and individual reward\ncomponents of the game. We study the interplay between such preference\nmodelling and environmental uncertainty on the incentive alignment level in the\ngame. We demonstrate how different combinations of individual preferences and\nenvironmental uncertainties sustain the emergence of cooperative patterns in\nnon-cooperative environments (i.e., where competitive strategies are dominant),\nwhile others sustain competitive patterns in cooperative environments (i.e.,\nwhere cooperative strategies are dominant).",
      "tldr_zh": "本研究探讨了在风险和不确定性下实现最优决策的问题，特别针对多目标公共物品游戏（Public Goods Games）中代理的合作行为。研究引入了参数化的非线性效用函数（non-linear utility function）来建模个体代理的风险偏好，并结合多目标强化学习（multi-objective reinforcement learning）分析集体和个体奖励的影响。结果显示，不同风险偏好与环境不确定性的组合能够促进合作模式在非合作环境（竞争策略占主导）中的出现，或导致竞争模式在合作环境（合作策略占主导）中出现，从而揭示了激励对齐的动态机制。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "In press at ECAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00682v1",
      "published_date": "2024-08-01 16:24:37 UTC",
      "updated_date": "2024-08-01 16:24:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:04:35.342053"
    },
    {
      "arxiv_id": "2408.00655v5",
      "title": "SentenceVAE: Enable Next-sentence Prediction for Large Language Models with Faster Speed, Higher Accuracy and Longer Context",
      "title_zh": "SentenceVAE：启用下一句预测以实现大型语言模型的",
      "authors": [
        "Hongjun An",
        "Yifan Chen",
        "Zhe Sun",
        "Xuelong Li"
      ],
      "abstract": "Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aiming at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), which includes a Sentence\nEncoder to compress multiple tokens in a sentence into a single token, and a\nSentence Decoder to reconstruct it. By integrating SentenceVAE into the input\nand output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference method. In addition, the SentenceVAE module of\nSLLMs can maintain the integrity of the original semantic content by segmenting\nthe context into sentences, thereby improving accuracy while boosting inference\nspeed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over\nequivalent context length, significantly reducing memory demands for\nself-attention computation and facilitating the handling of longer context.\nExtensive experiments on Wanjuan dataset have revealed that the proposed method\ncan accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%\nof its original metric, and decrease memory overhead by 86~91% for the\nequivalent context length, compared to previous token-by-token methods.",
      "tldr_zh": "本研究针对当前大语言模型 (LLMs) 的 next-token prediction 方法导致的推理速度慢问题，提出了一种新型的 next-sentence prediction 推理方法，以提升效率和准确性。论文引入 Sentence Variational Autoencoder (SentenceVAE)，包括 Sentence Encoder 用于将句子中的多个 tokens 压缩成一个 token，以及 Sentence Decoder 用于重建，从而开发出 Sentence-level LLMs (SLLMs)，实现句子级别的输入和输出处理。SLLMs 通过保持语义完整性和减少 tokens 处理量，不仅提高了推理速度和准确性，还降低了内存需求，支持更长上下文。在 Wanjuan 数据集上的实验显示，该方法使推理速度提升 204~365%，perplexity (PPL) 降低至原先的 46~75%，并将内存开销减少 86~91%。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "update the article",
      "pdf_url": "http://arxiv.org/pdf/2408.00655v5",
      "published_date": "2024-08-01 15:45:19 UTC",
      "updated_date": "2024-08-14 07:34:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:04:40.839006"
    },
    {
      "arxiv_id": "2408.00640v2",
      "title": "AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data for 3D-Native Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Asbjørn Munk",
        "Jakob Ambsdorf",
        "Sebastian Llambias",
        "Mads Nielsen"
      ],
      "abstract": "This study investigates the impact of self-supervised pretraining of 3D\nsemantic segmentation models on a large-scale, domain-specific dataset. We\nintroduce BRAINS-45K, a dataset of 44,756 brain MRI volumes from public\nsources, the largest public dataset available, and revisit a number of design\nchoices for pretraining modern segmentation architectures by simplifying and\noptimizing state-of-the-art methods, and combining them with a novel\naugmentation strategy. The resulting AMAES framework is based on\nmasked-image-modeling and intensity-based augmentation reversal and balances\nmemory usage, runtime, and finetuning performance. Using the popular U-Net and\nthe recent MedNeXt architecture as backbones, we evaluate the effect of\npretraining on three challenging downstream tasks, covering single-sequence,\nlow-resource settings, and out-of-domain generalization. The results highlight\nthat pretraining on the proposed dataset with AMAES significantly improves\nsegmentation performance in the majority of evaluated cases, and that it is\nbeneficial to pretrain the model with augmentations, despite pretraing on a\nlarge-scale dataset. Code and model checkpoints for reproducing results, as\nwell as the BRAINS-45K dataset are available at\n\\url{https://github.com/asbjrnmunk/amaes}.",
      "tldr_zh": "本研究引入了 BRAINS-45K 数据集（包含 44,756 个脑 MRI 卷），这是最大的公开脑 MRI 数据集，并提出 AMAES 框架，用于自监督预训练 3D 语义分割模型，该框架基于 masked-image-modeling 和 intensity-based augmentation reversal，简化了现有方法并优化了内存使用、运行时和微调性能。\nAMAES 框架使用 U-Net 和 MedNeXt 作为骨干网络，在单序列、低资源设置以及域外泛化等下游任务上进行了评估。\n结果显示，预训练显著提高了分割性能，尤其是在大型数据集上结合增强策略后，准确率在多数情况下得到提升；代码、模型检查点和数据集已在 GitHub 上公开。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted at ADSMI @ MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00640v2",
      "published_date": "2024-08-01 15:27:48 UTC",
      "updated_date": "2024-08-15 08:56:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:04:52.142279"
    },
    {
      "arxiv_id": "2408.07822v1",
      "title": "Exploration of LLMs, EEG, and behavioral data to measure and support attention and sleep",
      "title_zh": "翻译失败",
      "authors": [
        "Akane Sano",
        "Judith Amores",
        "Mary Czerwinski"
      ],
      "abstract": "We explore the application of large language models (LLMs), pre-trained\nmodels with massive textual data for detecting and improving these altered\nstates. We investigate the use of LLMs to estimate attention states, sleep\nstages, and sleep quality and generate sleep improvement suggestions and\nadaptive guided imagery scripts based on electroencephalogram (EEG) and\nphysical activity data (e.g. waveforms, power spectrogram images, numerical\nfeatures). Our results show that LLMs can estimate sleep quality based on human\ntextual behavioral features and provide personalized sleep improvement\nsuggestions and guided imagery scripts; however detecting attention, sleep\nstages, and sleep quality based on EEG and activity data requires further\ntraining data and domain-specific knowledge.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型 (LLMs) 结合脑电图 (EEG) 和行为数据（如波形、功率谱图图像和数值特征）来评估注意力状态、睡眠阶段及睡眠质量，并生成个性化睡眠改善建议和适应性引导想象脚本。研究发现，LLMs 能够基于人类的文本行为特征有效估计睡眠质量，并提供针对性的建议；然而，在利用 EEG 和活动数据检测注意力、睡眠阶段及睡眠质量时，仍需更多训练数据和领域特定知识来提升准确性。该工作突显了 LLMs 在支持睡眠健康方面的潜力，但也指出了进一步优化的必要性。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07822v1",
      "published_date": "2024-08-01 15:17:54 UTC",
      "updated_date": "2024-08-01 15:17:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:05:02.536672"
    },
    {
      "arxiv_id": "2408.00633v1",
      "title": "DisTrack: a new Tool for Semi-automatic Misinformation Tracking in Online Social Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Guillermo Villar-Rodríguez",
        "Álvaro Huertas-García",
        "Alejandro Martín",
        "Javier Huertas-Tato",
        "David Camacho"
      ],
      "abstract": "Introduction: This article introduces DisTrack, a methodology and a tool\ndeveloped for tracking and analyzing misinformation within Online Social\nNetworks (OSNs). DisTrack is designed to combat the spread of misinformation\nthrough a combination of Natural Language Processing (NLP) Social Network\nAnalysis (SNA) and graph visualization. The primary goal is to detect\nmisinformation, track its propagation, identify its sources, and assess the\ninfluence of various actors within the network.\n  Methods: DisTrack's architecture incorporates a variety of methodologies\nincluding keyword search, semantic similarity assessments, and graph generation\ntechniques. These methods collectively facilitate the monitoring of\nmisinformation, the categorization of content based on alignment with known\nfalse claims, and the visualization of dissemination cascades through detailed\ngraphs. The tool is tailored to capture and analyze the dynamic nature of\nmisinformation spread in digital environments.\n  Results: The effectiveness of DisTrack is demonstrated through three case\nstudies focused on different themes: discredit/hate speech, anti-vaccine\nmisinformation, and false narratives about the Russia-Ukraine conflict. These\nstudies show DisTrack's capabilities in distinguishing posts that propagate\nfalsehoods from those that counteract them, and tracing the evolution of\nmisinformation from its inception.\n  Conclusions: The research confirms that DisTrack is a valuable tool in the\nfield of misinformation analysis. It effectively distinguishes between\ndifferent types of misinformation and traces their development over time. By\nproviding a comprehensive approach to understanding and combating\nmisinformation in digital spaces, DisTrack proves to be an essential asset for\nresearchers and practitioners working to mitigate the impact of false\ninformation in online social environments.",
      "tldr_zh": "本文研究介绍了DisTrack，一种用于在线社交网络(OSNs)中半自动追踪虚假信息的工具和方法，旨在通过Natural Language Processing (NLP)、Social Network Analysis (SNA)以及图形可视化相结合，检测虚假信息、追踪其传播、识别来源并评估网络影响。DisTrack采用关键词搜索、语义相似性评估和图形生成技术，来监控和分类内容，并可视化虚假信息的传播动态。在三个案例研究中，包括仇恨言论、反疫苗虚假信息以及俄乌冲突假新闻，DisTrack证明了其有效性，能准确区分传播虚假信息的帖子并追踪其演变，为虚假信息分析提供宝贵支持。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00633v1",
      "published_date": "2024-08-01 15:17:33 UTC",
      "updated_date": "2024-08-01 15:17:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:05:13.987678"
    },
    {
      "arxiv_id": "2408.00613v1",
      "title": "Unlocking Fair Use in the Generative AI Supply Chain: A Systematized Literature Review",
      "title_zh": "在生成式人工智能供应链中解锁合理使用：一项系统化文献综述",
      "authors": [
        "Amruta Mahuli",
        "Asia Biega"
      ],
      "abstract": "Through a systematization of generative AI (GenAI) stakeholder goals and\nexpectations, this work seeks to uncover what value different stakeholders see\nin their contributions to the GenAI supply line. This valuation enables us to\nunderstand whether fair use advocated by GenAI companies to train model\nprogresses the copyright law objective of promoting science and arts. While\nassessing the validity and efficacy of the fair use argument, we uncover\nresearch gaps and potential avenues for future works for researchers and\npolicymakers to address.",
      "tldr_zh": "这篇论文通过系统化文献审查(systematized literature review)，分析生成式 AI (GenAI) 供应链中不同利益相关者的目标和期望，以评估其贡献价值。研究重点考察 GenAI 公司主张的 fair use 是否促进版权法促进科学和艺术的目标，并评估其论点的有效性和功效。最终，论文揭示了现有研究空白，并为研究者和政策制定者提供了未来工作的潜在方向。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00613v1",
      "published_date": "2024-08-01 14:53:11 UTC",
      "updated_date": "2024-08-01 14:53:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:05:26.729844"
    },
    {
      "arxiv_id": "2408.00584v1",
      "title": "Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian Rebuses",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriele Sarti",
        "Tommaso Caselli",
        "Malvina Nissim",
        "Arianna Bisazza"
      ],
      "abstract": "Rebuses are puzzles requiring constrained multi-step reasoning to identify a\nhidden phrase from a set of images and letters. In this work, we introduce a\nlarge collection of verbalized rebuses for the Italian language and use it to\nassess the rebus-solving capabilities of state-of-the-art large language\nmodels. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly\non this task, ad-hoc fine-tuning seems to improve models' performance. However,\nwe find that performance gains from training are largely motivated by\nmemorization. Our results suggest that rebus solving remains a challenging test\nbed to evaluate large language models' linguistic proficiency and sequential\ninstruction-following skills.",
      "tldr_zh": "这篇论文引入了一个意大利语谜语（rebuses）数据集，要求模型通过受限的多步推理从图像和字母中识别隐藏短语，以评估大型语言模型（LLMs）的能力。研究发现，通用模型如 LLaMA-3 和 GPT-4o 在此任务上表现不佳。特定微调（ad-hoc fine-tuning）虽能提升性能，但主要依赖记忆化（memorization）而非真正的推理理解。结果表明，谜语解决任务是检验 LLMs 语言熟练度和顺序指令遵循技能的 challenging 测试床。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code: https://github.com/gsarti/verbalized-rebus. Artifacts:\n  https://huggingface.co/collections/gsarti/verbalized-rebus-clic-it-2024-66ab8f11cb04e68bdf4fb028",
      "pdf_url": "http://arxiv.org/pdf/2408.00584v1",
      "published_date": "2024-08-01 14:14:15 UTC",
      "updated_date": "2024-08-01 14:14:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:05:38.303306"
    },
    {
      "arxiv_id": "2408.00555v1",
      "title": "Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation",
      "title_zh": "通过主动检索增强缓解大型视觉语言模型中的幻觉",
      "authors": [
        "Xiaoye Qu",
        "Qiyuan Chen",
        "Wei Wei",
        "Jishuo Sun",
        "Jianfeng Dong"
      ],
      "abstract": "Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.",
      "tldr_zh": "大型视觉语言模型（LVLMs）常出现hallucination问题，即生成看似合理但事实错误的响应，本文提出Active Retrieval-Augmented (ARA)框架，通过检索增强技术来缓解这一问题。ARA框架的关键创新包括：基于图像的层次结构分解检索目标、选择最有效的检索方法并过滤可靠结果、以及仅在模型不确定性高时进行检索，以避免不必要的操作。在三个LVLM模型（如LLaVA-1.5和Qwen-VL）上的四种基准测试中，实验证明ARA能显著减少hallucination，同时最小化检索次数，为LVLMs的可靠应用提供重要洞见。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00555v1",
      "published_date": "2024-08-01 13:38:58 UTC",
      "updated_date": "2024-08-01 13:38:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:05:51.092856"
    },
    {
      "arxiv_id": "2408.00550v1",
      "title": "Mitigating Multilingual Hallucination in Large Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoye Qu",
        "Mingyang Song",
        "Wei Wei",
        "Jianfeng Dong",
        "Yu Cheng"
      ],
      "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR",
      "tldr_zh": "大型视觉语言模型(LVLMs) 在处理非英语查询时存在严重的多语言幻觉问题，即生成看似合理但错误的答案，而现有方法仅针对英语场景。本文首次提出两阶段 Multilingual Hallucination Removal (MHR) 框架，通过 cross-lingual alignment 生成多个响应并识别幻觉相关的对，然后利用这些数据进行 direct preference optimization，提升模型对高资源和低资源语言的抵抗力。实验结果显示，在扩展的多语言 POPE benchmark 上，MHR 使 13 种语言的准确率平均提高了 19.0%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00550v1",
      "published_date": "2024-08-01 13:34:35 UTC",
      "updated_date": "2024-08-01 13:34:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:06:02.510799"
    },
    {
      "arxiv_id": "2408.00549v2",
      "title": "Learning to Embed Distributions via Maximum Kernel Entropy",
      "title_zh": "翻译失败",
      "authors": [
        "Oleksii Kachaiev",
        "Stefano Recanatesi"
      ],
      "abstract": "Empirical data can often be considered as samples from a set of probability\ndistributions. Kernel methods have emerged as a natural approach for learning\nto classify these distributions. Although numerous kernels between\ndistributions have been proposed, applying kernel methods to distribution\nregression tasks remains challenging, primarily because selecting a suitable\nkernel is not straightforward. Surprisingly, the question of learning a\ndata-dependent distribution kernel has received little attention. In this\npaper, we propose a novel objective for the unsupervised learning of\ndata-dependent distribution kernel, based on the principle of entropy\nmaximization in the space of probability measure embeddings. We examine the\ntheoretical properties of the latent embedding space induced by our objective,\ndemonstrating that its geometric structure is well-suited for solving\ndownstream discriminative tasks. Finally, we demonstrate the performance of the\nlearned kernel across different modalities.",
      "tldr_zh": "本论文提出了一种基于最大核熵(Maximum Kernel Entropy)的目标函数，用于无监督学习数据相关的分布核，以解决核方法在分布回归任务中选取合适核的挑战。方法通过在概率测度嵌入(probability measure embeddings)空间中最大化熵，构建了一个几何结构良好的潜在嵌入空间，使其更适合下游判别任务。实验结果表明，该学到的核在不同模态上表现出色，提升了分布分类的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00549v2",
      "published_date": "2024-08-01 13:34:19 UTC",
      "updated_date": "2024-11-28 18:20:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:06:14.998964"
    },
    {
      "arxiv_id": "2408.00544v1",
      "title": "Illustrating Classic Brazilian Books using a Text-To-Image Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Felipe Mahlow",
        "André Felipe Zanella",
        "William Alberto Cruz Castañeda",
        "Regilene Aparecida Sarzi-Ribeiro"
      ],
      "abstract": "In recent years, Generative Artificial Intelligence (GenAI) has undergone a\nprofound transformation in addressing intricate tasks involving diverse\nmodalities such as textual, auditory, visual, and pictorial generation. Within\nthis spectrum, text-to-image (TTI) models have emerged as a formidable approach\nto generating varied and aesthetically appealing compositions, spanning\napplications from artistic creation to realistic facial synthesis, and\ndemonstrating significant advancements in computer vision, image processing,\nand multimodal tasks. The advent of Latent Diffusion Models (LDMs) signifies a\nparadigm shift in the domain of AI capabilities. This article delves into the\nfeasibility of employing the Stable Diffusion LDM to illustrate literary works.\nFor this exploration, seven classic Brazilian books have been selected as case\nstudies. The objective is to ascertain the practicality of this endeavor and to\nevaluate the potential of Stable Diffusion in producing illustrations that\naugment and enrich the reader's experience. We will outline the beneficial\naspects, such as the capacity to generate distinctive and contextually\npertinent images, as well as the drawbacks, including any shortcomings in\nfaithfully capturing the essence of intricate literary depictions. Through this\nstudy, we aim to provide a comprehensive assessment of the viability and\nefficacy of utilizing AI-generated illustrations in literary contexts,\nelucidating both the prospects and challenges encountered in this pioneering\napplication of technology.",
      "tldr_zh": "该论文探讨了使用 Text-To-Image (TTI) 扩散模型，特别是 Stable Diffusion (一种 Latent Diffusion Models, LDMs)，来为七本经典巴西书籍生成插图，以增强读者的体验。研究通过案例分析评估了这种方法的实用性，突出了其优势，如生成独特且上下文相关的图像，以及缺点，如难以忠实捕捉文学描绘的复杂性。总体而言，该工作提供了对 AI 生成插图在文学领域的可行性和挑战的全面评估，为 Generative Artificial Intelligence (GenAI) 在文化应用中的前景提供了启示。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00544v1",
      "published_date": "2024-08-01 13:28:15 UTC",
      "updated_date": "2024-08-01 13:28:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:06:27.508867"
    },
    {
      "arxiv_id": "2408.00540v3",
      "title": "The Energy Cost of Artificial Intelligence Lifecycle in Communication Networks",
      "title_zh": "通信网络中人工智能生命周期的能源成本",
      "authors": [
        "Shih-Kai Chou",
        "Jernej Hribar",
        "Vid Hanžel",
        "Mihael Mohorčič",
        "Carolina Fortuna"
      ],
      "abstract": "Artificial Intelligence (AI) is being incorporated in several optimization,\nscheduling, orchestration as well as in native communication network functions.\nWhile this paradigm shift results in increased energy consumption, quantifying\nthe end-toend energy consumption of adding intelligence to such systems is\nparticularly challenging. Conventional metrics focus on either communication,\ncomputation infrastructure, or model development. To address this, we propose a\nnew metric, the Energy Cost of AI Lifecycle (eCAL) of one AI model in a system.\neCAL captures the energy consumption throughout the development and deployment\nof an AI-model providing intelligence in a wireless communication network by\nanalyzing the complexity of data collection and manipulation in individual\ncomponents and deriving overall and per-bit energy consumption. We show that\nthe better a model is and the more it is used, the more energy efficient an\ninference is. For a simple case study, eCAL for making 100 inferences is 2.73\ntimes higher than for 1000 inferences. Additionally, we have developed a\nmodular and extendable opensource simulation tool to enable researchers,\npractitioners, and engineers to calculate the end-to-end energy cost with\nvarious configurations and across various systems, ensuring adaptability to\ndiverse use cases.",
      "tldr_zh": "这篇论文探讨了AI在通信网络中的生命周期能源消耗问题，提出了一种新指标Energy Cost of AI Lifecycle (eCAL)，用于量化AI模型从开发到部署的全过程端到端能源成本，包括数据收集、操作复杂性和整体及每位能源消耗。研究发现，AI模型性能越好、使用频率越高，推理过程越节能，例如在案例研究中，100次推理的eCAL是1000次推理的2.73倍。该工具还开发了一个模块化、可扩展的开源模拟平台，允许研究人员根据不同配置计算能源成本，以适应多种应用场景。",
      "categories": [
        "cs.ET",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.ET",
      "comment": "13 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00540v3",
      "published_date": "2024-08-01 13:23:15 UTC",
      "updated_date": "2025-05-12 11:18:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:08:51.813644"
    },
    {
      "arxiv_id": "2408.00539v1",
      "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Mingcong Lu",
        "Jiangcai Zhu",
        "Wang Hao",
        "Zheng Li",
        "Shusheng Zhang",
        "Kailai Shao",
        "Chao Chen",
        "Nan Li",
        "Feng Wang",
        "Xin Lu"
      ],
      "abstract": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
      "tldr_zh": "本论文针对多轮对话中Large Language Models (LLMs)的生成质量和延迟挑战，提出了一种新掩码范式Intermittent Semi-working Mask (ISM)。ISM通过在对话历史中交替应用双向和单向注意力，结合了prefix LLM在历史上下文依赖场景中的高性能优势，以及causal LLM的低生成延迟和Key-Value Cache (KV Cache)重用能力。实验结果表明，ISM在多轮对话等任务上实现了显著性能提升，同时解决了prefix LLM的训练效率问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00539v1",
      "published_date": "2024-08-01 13:22:01 UTC",
      "updated_date": "2024-08-01 13:22:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:09:01.517006"
    },
    {
      "arxiv_id": "2408.00814v1",
      "title": "Adaptive traffic signal safety and efficiency improvement by multi objective deep reinforcement learning approach",
      "title_zh": "通过多目标深度强化学习方法提升自适应交通信号的安全性和效率",
      "authors": [
        "Shahin Mirbakhsh",
        "Mahdi Azizi"
      ],
      "abstract": "This research introduces an innovative method for adaptive traffic signal\ncontrol (ATSC) through the utilization of multi-objective deep reinforcement\nlearning (DRL) techniques. The proposed approach aims to enhance control\nstrategies at intersections while simultaneously addressing safety, efficiency,\nand decarbonization objectives. Traditional ATSC methods typically prioritize\ntraffic efficiency and often struggle to adapt to real-time dynamic traffic\nconditions. To address these challenges, the study suggests a DRL-based ATSC\nalgorithm that incorporates the Dueling Double Deep Q Network (D3QN) framework.\nThe performance of this algorithm is assessed using a simulated intersection in\nChangsha, China. Notably, the proposed ATSC algorithm surpasses both\ntraditional ATSC and ATSC algorithms focused solely on efficiency optimization\nby achieving over a 16% reduction in traffic conflicts and a 4% decrease in\ncarbon emissions. Regarding traffic efficiency, waiting time is reduced by 18%\ncompared to traditional ATSC, albeit showing a slight increase (0.64%) compared\nto the DRL-based ATSC algorithm integrating the D3QN framework. This marginal\nincrease suggests a trade-off between efficiency and other objectives like\nsafety and decarbonization. Additionally, the proposed approach demonstrates\nsuperior performance, particularly in scenarios with high traffic demand,\nacross all three objectives. These findings contribute to advancing traffic\ncontrol systems by offering a practical and effective solution for optimizing\nsignal control strategies in real-world traffic situations.",
      "tldr_zh": "本研究提出了一种基于多目标深度强化学习（DRL）的自适应交通信号控制（ATSC）方法，使用 Dueling Double Deep Q Network (D3QN) 框架，同时优化安全、效率和减碳目标。相比传统 ATSC，该方法在长沙模拟交叉口实验中实现了交通冲突减少超过16%、碳排放降低4%，并将等待时间缩短18%。尽管效率较纯 DRL 效率优化算法略微增加（0.64%），但在高交通需求场景下，该方法表现出色，为实际交通控制系统提供了实用解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00814v1",
      "published_date": "2024-08-01 13:10:41 UTC",
      "updated_date": "2024-08-01 13:10:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:08:55.018651"
    },
    {
      "arxiv_id": "2408.00526v1",
      "title": "Hilbert curves for efficient exploratory landscape analysis neighbourhood sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Johannes J. Pienaar",
        "Anna S. Bosman",
        "Katherine M. Malan"
      ],
      "abstract": "Landscape analysis aims to characterise optimisation problems based on their\nobjective (or fitness) function landscape properties. The problem search space\nis typically sampled, and various landscape features are estimated based on the\nsamples. One particularly salient set of features is information content, which\nrequires the samples to be sequences of neighbouring solutions, such that the\nlocal relationships between consecutive sample points are preserved. Generating\nsuch spatially correlated samples that also provide good search space coverage\nis challenging. It is therefore common to first obtain an unordered sample with\ngood search space coverage, and then apply an ordering algorithm such as the\nnearest neighbour to minimise the distance between consecutive points in the\nsample. However, the nearest neighbour algorithm becomes computationally\nprohibitive in higher dimensions, thus there is a need for more efficient\nalternatives. In this study, Hilbert space-filling curves are proposed as a\nmethod to efficiently obtain high-quality ordered samples. Hilbert curves are a\nspecial case of fractal curves, and guarantee uniform coverage of a bounded\nsearch space while providing a spatially correlated sample. We study the\neffectiveness of Hilbert curves as samplers, and discover that they are capable\nof extracting salient features at a fraction of the computational cost compared\nto Latin hypercube sampling with post-factum ordering. Further, we investigate\nthe use of Hilbert curves as an ordering strategy, and find that they order the\nsample significantly faster than the nearest neighbour ordering, without\nsacrificing the saliency of the extracted features.",
      "tldr_zh": "本文提出使用 Hilbert curves 作为一种高效方法来改进 exploratory landscape analysis 中的 neighbourhood sampling，旨在生成有序样本以保留优化问题搜索空间的局部关系。相比传统方法，如先使用 Latin hypercube sampling 再应用最近邻排序，Hilbert curves 能提供均匀覆盖和空间相关样本，同时显著降低计算成本。研究发现，Hilbert curves 在提取景观特征时计算效率更高，且不牺牲特征的显著性，为高维优化问题分析提供了更可行的替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "A version of this paper is published as conference proceedings of\n  EvoApps 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00526v1",
      "published_date": "2024-08-01 12:57:35 UTC",
      "updated_date": "2024-08-01 12:57:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:09:15.315907"
    },
    {
      "arxiv_id": "2408.00523v2",
      "title": "Jailbreaking Text-to-Image Models with LLM-Based Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Yingkai Dong",
        "Zheng Li",
        "Xiangtao Meng",
        "Ning Yu",
        "Shanqing Guo"
      ],
      "abstract": "Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving their potential for addressing generative AI safety tasks\nlargely unexplored. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework targeting generative AI models, specifically focusing on\njailbreak attacks against text-to-image (T2I) models with built-in safety\nfilters. Atlas consists of two agents, namely the mutation agent and the\nselection agent, each comprising four key modules: a vision-language model\n(VLM) or LLM brain, planning, memory, and tool usage. The mutation agent uses\nits VLM brain to determine whether a prompt triggers the T2I model's safety\nfilter. It then collaborates iteratively with the LLM brain of the selection\nagent to generate new candidate jailbreak prompts with the highest potential to\nbypass the filter. In addition to multi-agent communication, we leverage\nin-context learning (ICL) memory mechanisms and the chain-of-thought (COT)\napproach to learn from past successes and failures, thereby enhancing Atlas's\nperformance. Our evaluation demonstrates that Atlas successfully jailbreaks\nseveral state-of-the-art T2I models equipped with multi-modal safety filters in\na black-box setting. Additionally, Atlas outperforms existing methods in both\nquery efficiency and the quality of generated images. This work convincingly\ndemonstrates the successful application of LLM-based agents in studying the\nsafety vulnerabilities of popular text-to-image generation models. We urge the\ncommunity to consider advanced techniques like ours in response to the rapidly\nevolving text-to-image generation field.",
      "tldr_zh": "本研究提出Atlas，一种基于大型语言模型(LLM)的多智能体框架，用于针对文本到图像(T2I)模型的安全过滤器进行越狱攻击(jailbreak attacks)。框架包括mutation agent和selection agent，每个代理配备视觉语言模型(VLM)或LLM brain、planning、memory和tool usage模块，通过in-context learning (ICL)记忆机制和chain-of-thought (COT)方法，从过去的成功和失败中迭代优化越狱提示。实验结果显示，Atlas在黑箱设置下成功越狱了多种先进T2I模型，并在查询效率和生成图像质量上优于现有方法。该工作强调了LLM-based agents在探索生成AI安全漏洞方面的潜力，并呼吁社区开发相应应对策略。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00523v2",
      "published_date": "2024-08-01 12:54:46 UTC",
      "updated_date": "2024-09-09 08:09:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:09:30.815663"
    },
    {
      "arxiv_id": "2408.00521v2",
      "title": "A new approach for encoding code and assisting code understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Mengdan Fan",
        "Wei Zhang",
        "Haiyan Zhao",
        "Zhi Jin"
      ],
      "abstract": "Some companies (e.g., Microsoft Research and Google DeepMind) have discovered\nsome of the limitations of GPTs' autoregressive paradigm next-word prediction,\nmanifested in the model's lack of planning, working memory, backtracking, and\nreasoning skills. GPTs rely on a local and greedy process of generating the\nnext word, without a global understanding of the task or the output. We have\nconfirmed the above limitations through specialized empirical studies of code\ncomprehension. Although GPT-4 is good at producing fluent and coherent text, it\ncannot handle complex logic and generate new code that hasn't been seen, and it\nrelies too much on the formatting of the prompt to generate the correct code.\nWe propose a new paradigm for code understanding that goes beyond the next-word\nprediction paradigm, inspired by the successful application of diffusion\ntechniques to image generation (Dalle-2, Sora) and protein structure generation\n(AlphaFold-3), which have no autoregressive constraints. Instead of encoding\nthe code in a form that mimics natural language, we encode the code as a\nheterogeneous image paradigm with a memory of global information that mimics\nboth images and protein structures. We then refer to Sora's CLIP upstream\ntext-to-image encoder model to design a text-to-code encoder model that can be\napplied to various downstream code understanding tasks. The model learns the\nglobal understanding of code under the new paradigm heterogeneous image,\nconnects the encoding space of text and code, and encodes the input of text\ninto the vector of code most similar to it. Using self-supervised comparative\nlearning on 456,360 text-code pairs, the model achieved a zero-shot prediction\nof new data. This work is the basis for future work on code generation using\ndiffusion techniques under a new paradigm to avoid autoregressive limitations.",
      "tldr_zh": "该研究指出了 GPT 模型在代码理解中的局限性，如缺乏规划、工作记忆、回溯和推理技能，以及过度依赖提示格式。论文提出一种新范式，将代码编码为 heterogeneous image paradigm，模仿图像和蛋白质结构，从而实现全局信息记忆，而不是依赖 autoregressive 的逐词预测。受 diffusion techniques（如 Dalle-2 和 Sora）启发，他们设计了基于 Sora 的 CLIP 模型的 text-to-code 编码器，通过自监督 comparative learning 在 456,360 个文本-代码对上训练，实现了 zero-shot prediction。实验结果显示，该方法提升了代码理解能力，并为未来使用 diffusion techniques 生成代码提供基础，避免了传统范式的限制。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 page, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00521v2",
      "published_date": "2024-08-01 12:52:48 UTC",
      "updated_date": "2025-03-23 11:30:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:11:40.690963"
    },
    {
      "arxiv_id": "2408.03963v1",
      "title": "A self-adaptive system of systems architecture to enable its ad-hoc scalability: Unmanned Vehicle Fleet -- Mission Control Center Case study",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed R. Sadik",
        "Bram Bolder",
        "Pero Subasic"
      ],
      "abstract": "A System of Systems (SoS) comprises Constituent Systems (CSs) that interact\nto provide unique capabilities beyond any single CS. A key challenge in SoS is\nad-hoc scalability, meaning the system size changes during operation by adding\nor removing CSs. This research focuses on an Unmanned Vehicle Fleet (UVF) as a\npractical SoS example, addressing uncertainties like mission changes, range\nextensions, and UV failures. The proposed solution involves a self-adaptive\nsystem that dynamically adjusts UVF architecture, allowing the Mission Control\nCenter (MCC) to scale UVF size automatically based on performance criteria or\nmanually by operator decision. A multi-agent environment and rule management\nengine were implemented to simulate and verify this approach.",
      "tldr_zh": "这篇论文探讨了 System of Systems (SoS) 中的 ad-hoc scalability 挑战，即系统在运行中动态添加或移除 Constituent Systems (CSs)，并以 Unmanned Vehicle Fleet (UVF) 为案例，针对任务变化、范围扩展和 UV 故障等不确定性提出解决方案。作者设计了一个自适应系统，允许 Mission Control Center (MCC) 根据性能标准自动调整 UVF 架构，或通过操作员手动干预实现规模变化。系统通过多智能体环境和规则管理引擎进行模拟和验证，展示了其在提升 SoS 灵活性和可靠性方面的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SE",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "2023 7th International Conference on Intelligent Systems,\n  Metaheuristics & Swarm Intelligence (ISMSI 2023)",
      "pdf_url": "http://arxiv.org/pdf/2408.03963v1",
      "published_date": "2024-08-01 12:51:26 UTC",
      "updated_date": "2024-08-01 12:51:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:11:53.361852"
    },
    {
      "arxiv_id": "2408.00490v4",
      "title": "Graph Representation Learning via Causal Diffusion for Out-of-Distribution Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Chu Zhao",
        "Enneng Yang",
        "Yuliang Liang",
        "Pengxiang Lan",
        "Yuting Liu",
        "Jianzhe Zhao",
        "Guibing Guo",
        "Xingwei Wang"
      ],
      "abstract": "Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets.",
      "tldr_zh": "这篇论文针对 Graph Neural Networks (GNNs) 在 Out-of-Distribution (OOD) 推荐中的泛化问题，构建了 Structural Causal Model (SCM) 来分析环境混杂因素（如 COVID-19），揭示其导致模型性能下降的原因。作者提出了一种新方法 CausalDiffRec，通过 backdoor adjustment 和 variational inference 推断并消除环境混杂因素的影响，并在 causal diffusion 过程的逆向阶段学习不变图表示，以提升模型的泛化能力。理论证明显示，优化 CausalDiffRec 的目标函数能促进环境不变表示的学习，从而在分布偏移下实现更好的推荐性能。实验结果在 Food、KuaiRec、Yelp2018 和 Douban 数据集上显示，平均性能提升高达 10.69% 到 22.41%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, accepted by WWW2025",
      "pdf_url": "http://arxiv.org/pdf/2408.00490v4",
      "published_date": "2024-08-01 11:51:52 UTC",
      "updated_date": "2025-04-02 13:16:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:11:59.405027"
    },
    {
      "arxiv_id": "2408.00483v1",
      "title": "A Systematic Review on Long-Tailed Learning",
      "title_zh": "长尾学习的系统综述",
      "authors": [
        "Chongsheng Zhang",
        "George Almpanidis",
        "Gaojuan Fan",
        "Binquan Deng",
        "Yanbo Zhang",
        "Ji Liu",
        "Aouaidjia Kamel",
        "Paolo Soda",
        "João Gama"
      ],
      "abstract": "Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.",
      "tldr_zh": "这篇论文对 Long-Tailed Learning 进行了系统综述，聚焦于处理长尾分布数据集的高性能模型，尤其是提升少数/尾部类别的识别准确率。作者提出了一个新的分类法，包括八个维度（如数据平衡、神经架构、特征丰富、logits调整、损失函数、网络优化和事后处理技术），并系统回顾了现有方法，分析了它们的共同点、差异以及与不平衡学习方法的区别。最后，论文讨论了该领域的未来前景，为进一步研究提供了指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "Current Under Revision at IEEE TNNLS. [This is the long/Full-length\n  version of our Long-Tailed Learning Survey paper]",
      "pdf_url": "http://arxiv.org/pdf/2408.00483v1",
      "published_date": "2024-08-01 11:39:45 UTC",
      "updated_date": "2024-08-01 11:39:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:12:16.505704"
    },
    {
      "arxiv_id": "2408.00481v1",
      "title": "HBot: A Chatbot for Healthcare Applications in Traditional Chinese Medicine Based on Human Body 3D Visualization",
      "title_zh": "翻译失败",
      "authors": [
        "Bolin Zhang",
        "Zhiwei Yi",
        "Jiahao Wang",
        "Dianbo Sui",
        "Zhiying Tu",
        "Dianhui Chu"
      ],
      "abstract": "The unique diagnosis and treatment techniques and remarkable clinical\nefficacy of traditional Chinese medicine (TCM) make it play an important role\nin the field of elderly care and healthcare, especially in the rehabilitation\nof some common chronic diseases of the elderly. Therefore, building a TCM\nchatbot for healthcare application will help users obtain consultation services\nin a direct and natural way. However, concepts such as acupuncture points\n(acupoints) and meridians involved in TCM always appear in the consultation,\nwhich cannot be displayed intuitively. To this end, we develop a\n\\textbf{h}ealthcare chat\\textbf{bot} (HBot) based on a human body model in 3D\nand knowledge graph, which provides conversational services such as knowledge\nQ\\&A, prescription recommendation, moxibustion therapy recommendation, and\nacupoint search. When specific acupoints are involved in the conversations\nbetween user and HBot, the 3D body will jump to the corresponding acupoints and\nhighlight them. Moreover, Hbot can also be used in training scenarios to\naccelerate the teaching process of TCM by intuitively displaying acupuncture\npoints and knowledge cards. The demonstration video is available at\nhttps://www.youtube.com/watch?v=UhQhutSKkTU . Our code and dataset are publicly\navailable at Gitee: https://gitee.com/plabrolin/interactive-3d-acup.git",
      "tldr_zh": "本论文开发了HBot，一种基于3D人体可视化的聊天机器人，用于传统Chinese Medicine (TCM) 的医疗应用，旨在通过直观交互帮助用户获取中医咨询服务。HBot 整合知识图谱，提供知识问答、处方推荐、艾灸疗法推荐和穴位搜索等功能，并在对话中自动跳转并突出显示相关穴位(acupoints)和经络。系统不仅适用于日常医疗咨询，还可加速中医教学过程，通过直观显示知识卡提升用户体验，且代码和数据集已公开可用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "System Demonstration",
      "pdf_url": "http://arxiv.org/pdf/2408.00481v1",
      "published_date": "2024-08-01 11:36:18 UTC",
      "updated_date": "2024-08-01 11:36:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:12:31.759269"
    },
    {
      "arxiv_id": "2408.00473v1",
      "title": "Towards Explainable and Interpretable Musical Difficulty Estimation: A Parameter-efficient Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Pedro Ramoneda",
        "Vsevolod Eremenko",
        "Alexandre D'Hooge",
        "Emilia Parada-Cabaleiro",
        "Xavier Serra"
      ],
      "abstract": "Estimating music piece difficulty is important for organizing educational\nmusic collections. This process could be partially automatized to facilitate\nthe educator's role. Nevertheless, the decisions performed by prevalent\ndeep-learning models are hardly understandable, which may impair the acceptance\nof such a technology in music education curricula. Our work employs explainable\ndescriptors for difficulty estimation in symbolic music representations.\nFurthermore, through a novel parameter-efficient white-box model, we outperform\nprevious efforts while delivering interpretable results. These comprehensible\noutcomes emulate the functionality of a rubric, a tool widely used in music\neducation. Our approach, evaluated in piano repertoire categorized in 9\nclasses, achieved 41.4% accuracy independently, with a mean squared error (MSE)\nof 1.7, showing precise difficulty estimation. Through our baseline, we\nillustrate how building on top of past research can offer alternatives for\nmusic difficulty assessment which are explainable and interpretable. With this,\nwe aim to promote a more effective communication between the Music Information\nRetrieval (MIR) community and the music education one.",
      "tldr_zh": "这篇论文针对音乐作品难度的自动化估计问题，提出了一种参数高效的白-box模型，利用可解释描述符对符号化音乐进行分析，以提升模型的可解释性和可解读性。相比以往方法，该模型在9类钢琴曲目上实现了41.4%的准确率和1.7的Mean Squared Error (MSE)，并提供类似于rubric的解读结果。最终，该研究旨在促进Music Information Retrieval (MIR)社区与音乐教育社区之间的有效沟通。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00473v1",
      "published_date": "2024-08-01 11:23:42 UTC",
      "updated_date": "2024-08-01 11:23:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:11:57.329615"
    },
    {
      "arxiv_id": "2408.00470v1",
      "title": "Image Super-Resolution with Taylor Expansion Approximation and Large Field Reception",
      "title_zh": "翻译失败",
      "authors": [
        "Jiancong Feng",
        "Yuan-Gen Wang",
        "Mingjie Li",
        "Fengchuang Xing"
      ],
      "abstract": "Self-similarity techniques are booming in blind super-resolution (SR) due to\naccurate estimation of the degradation types involved in low-resolution images.\nHowever, high-dimensional matrix multiplication within self-similarity\ncomputation prohibitively consumes massive computational costs. We find that\nthe high-dimensional attention map is derived from the matrix multiplication\nbetween Query and Key, followed by a softmax function. This softmax makes the\nmatrix multiplication between Query and Key inseparable, posing a great\nchallenge in simplifying computational complexity. To address this issue, we\nfirst propose a second-order Taylor expansion approximation (STEA) to separate\nthe matrix multiplication of Query and Key, resulting in the complexity\nreduction from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Then, we design a\nmulti-scale large field reception (MLFR) to compensate for the performance\ndegradation caused by STEA. Finally, we apply these two core designs to\nlaboratory and real-world scenarios by constructing LabNet and RealNet,\nrespectively. Extensive experimental results tested on five synthetic datasets\ndemonstrate that our LabNet sets a new benchmark in qualitative and\nquantitative evaluations. Tested on the RealWorld38 dataset, our RealNet\nachieves superior visual quality over existing methods. Ablation studies\nfurther verify the contributions of STEA and MLFR towards both LabNet and\nRealNet frameworks.",
      "tldr_zh": "这篇论文针对图像超分辨率（SR）中的自相似技术计算复杂性问题，提出二阶泰勒展开逼近（STEA）方法，将Query和Key的矩阵乘法复杂度从O(N^2)降低到O(N)。为补偿STEA带来的性能下降，该研究设计了多尺度大视场接收（MLFR）机制，并构建了LabNet（适用于实验室场景）和RealNet（适用于真实世界场景）的框架。实验结果显示，LabNet在五个合成数据集上实现了定性和定量的新基准，而RealNet在RealWorld38数据集上取得了优于现有方法的视觉质量，消融研究进一步验证了STEA和MLFR的核心贡献。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00470v1",
      "published_date": "2024-08-01 11:16:26 UTC",
      "updated_date": "2024-08-01 11:16:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:12:08.799145"
    },
    {
      "arxiv_id": "2408.00447v1",
      "title": "DiscipLink: Unfolding Interdisciplinary Information Seeking Process via Human-AI Co-Exploration",
      "title_zh": "DiscipLink: 通过人类-AI 共同探索揭示跨学科信息搜索过程",
      "authors": [
        "Chengbo Zheng",
        "Yuanhao Zhang",
        "Zeyu Huang",
        "Chuhan Shi",
        "Minrui Xu",
        "Xiaojuan Ma"
      ],
      "abstract": "Interdisciplinary studies often require researchers to explore literature in\ndiverse branches of knowledge. Yet, navigating through the highly scattered\nknowledge from unfamiliar disciplines poses a significant challenge. In this\npaper, we introduce DiscipLink, a novel interactive system that facilitates\ncollaboration between researchers and large language models (LLMs) in\ninterdisciplinary information seeking (IIS). Based on users' topics of\ninterest, DiscipLink initiates exploratory questions from the perspectives of\npossible relevant fields of study, and users can further tailor these\nquestions. DiscipLink then supports users in searching and screening papers\nunder selected questions by automatically expanding queries with\ndisciplinary-specific terminologies, extracting themes from retrieved papers,\nand highlighting the connections between papers and questions. Our evaluation,\ncomprising a within-subject comparative experiment and an open-ended\nexploratory study, reveals that DiscipLink can effectively support researchers\nin breaking down disciplinary boundaries and integrating scattered knowledge in\ndiverse fields. The findings underscore the potential of LLM-powered tools in\nfostering information-seeking practices and bolstering interdisciplinary\nresearch.",
      "tldr_zh": "该论文介绍了 DiscipLink，一种创新的交互系统，通过人类-AI 协作（Human-AI Co-Exploration）来辅助跨学科信息搜索（Interdisciplinary Information Seeking, IIS），帮助研究者应对知识分散的挑战。系统基于用户兴趣主题生成探索性问题，并支持查询扩展、论文主题提取以及突出论文与问题间的连接。评估实验显示，DiscipLink 能有效打破学科边界、整合散乱知识，并强调大型语言模型（LLMs）在提升信息搜索和跨学科研究实践中的潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00447v1",
      "published_date": "2024-08-01 10:36:00 UTC",
      "updated_date": "2024-08-01 10:36:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:12:31.476474"
    },
    {
      "arxiv_id": "2408.00444v1",
      "title": "Ontological Relations from Word Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Mathieu d'Aquin",
        "Emmanuel Nauer"
      ],
      "abstract": "It has been reliably shown that the similarity of word embeddings obtained\nfrom popular neural models such as BERT approximates effectively a form of\nsemantic similarity of the meaning of those words. It is therefore natural to\nwonder if those embeddings contain enough information to be able to connect\nthose meanings through ontological relationships such as the one of\nsubsumption. If so, large knowledge models could be built that are capable of\nsemantically relating terms based on the information encapsulated in word\nembeddings produced by pre-trained models, with implications not only for\nontologies (ontology matching, ontology evolution, etc.) but also on the\nability to integrate ontological knowledge in neural models. In this paper, we\ntest how embeddings produced by several pre-trained models can be used to\npredict relations existing between classes and properties of popular\nupper-level and general ontologies. We show that even a simple feed-forward\narchitecture on top of those embeddings can achieve promising accuracies, with\nvarying generalisation abilities depending on the input data. To achieve that,\nwe produce a dataset that can be used to further enhance those models, opening\nnew possibilities for applications integrating knowledge from web ontologies.",
      "tldr_zh": "这篇论文探讨了从词嵌入（如BERT产生的word embeddings）中提取本体关系（ontological relations），例如从属关系（subsumption），以构建能够语义关联术语的大型知识模型。研究者测试了多个预训练模型的嵌入，使用一个简单的前馈架构（feed-forward architecture）来预测流行上层和一般ontologies中的类和属性关系。结果显示，该方法取得了有前景的准确率，并展示了不同的泛化能力；此外，他们创建了一个新数据集，用于进一步增强这些模型，并为ontologies应用（如ontology matching和ontology evolution）以及将本体知识整合到神经模型中开辟新可能性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00444v1",
      "published_date": "2024-08-01 10:31:32 UTC",
      "updated_date": "2024-08-01 10:31:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:12:33.169603"
    },
    {
      "arxiv_id": "2408.00441v1",
      "title": "Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and Flexible Scene Text Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Gangyan Zeng",
        "Yuan Zhang",
        "Jin Wei",
        "Dongbao Yang",
        "Peng Zhang",
        "Yiwen Gao",
        "Xugong Qin",
        "Yu Zhou"
      ],
      "abstract": "Scene text retrieval aims to find all images containing the query text from\nan image gallery. Current efforts tend to adopt an Optical Character\nRecognition (OCR) pipeline, which requires complicated text detection and/or\nrecognition processes, resulting in inefficient and inflexible retrieval.\nDifferent from them, in this work we propose to explore the intrinsic potential\nof Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text\nretrieval. Through empirical analysis, we observe that the main challenges of\nCLIP as a text retriever are: 1) limited text perceptual scale, and 2)\nentangled visual-semantic concepts. To this end, a novel model termed FDP\n(Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text\nvia shifting the attention to the text area and probing the hidden text\nknowledge, and then divides the query text into content word and function word\nfor processing, in which a semantic-aware prompting scheme and a distracted\nqueries assistance module are utilized. Extensive experiments show that FDP\nsignificantly enhances the inference speed while achieving better or\ncompetitive retrieval accuracy compared to existing methods. Notably, on the\nIIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4\ntimes faster speed. Furthermore, additional experiments under phrase-level and\nattribute-aware scene text retrieval settings validate FDP's particular\nadvantages in handling diverse forms of query text. The source code will be\npublicly available at https://github.com/Gyann-z/FDP.",
      "tldr_zh": "本研究针对场景文本检索（Scene Text Retrieval）问题，提出一种基于 Contrastive Language-Image Pre-training (CLIP) 的高效无 OCR 方法，以解决传统方法的低效和不灵活性。作者开发了新模型 FDP（Focus, Distinguish, and Prompt），通过 Focus 模块将注意力集中在文本区域并探查隐藏知识，Distinguish 模块区分查询文本的内容词和功能词，以及 Prompt 模块的语义感知提示方案和分散查询辅助，提高了 CLIP 的文本感知能力和视觉-语义分离。实验结果显示，FDP 在 IIIT-STR 基准上比最先进模型提升 4.37% 的准确率，同时速度快 4 倍，并在短语级和属性感知设置下表现出色，显著提升了检索的灵活性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACM MM 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00441v1",
      "published_date": "2024-08-01 10:25:14 UTC",
      "updated_date": "2024-08-01 10:25:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:12:48.898042"
    },
    {
      "arxiv_id": "2408.00435v1",
      "title": "A Qualitative Study on Using ChatGPT for Software Security: Perception vs. Practicality",
      "title_zh": "一项关于使用 ChatGPT 进行软件安全的定性研究：感知 vs. 实用性",
      "authors": [
        "M. Mehdi Kholoosi",
        "M. Ali Babar",
        "Roland Croft"
      ],
      "abstract": "Artificial Intelligence (AI) advancements have enabled the development of\nLarge Language Models (LLMs) that can perform a variety of tasks with\nremarkable semantic understanding and accuracy. ChatGPT is one such LLM that\nhas gained significant attention due to its impressive capabilities for\nassisting in various knowledge-intensive tasks. Due to the knowledge-intensive\nnature of engineering secure software, ChatGPT's assistance is expected to be\nexplored for security-related tasks during the development/evolution of\nsoftware. To gain an understanding of the potential of ChatGPT as an emerging\ntechnology for supporting software security, we adopted a two-fold approach.\nInitially, we performed an empirical study to analyse the perceptions of those\nwho had explored the use of ChatGPT for security tasks and shared their views\non Twitter. It was determined that security practitioners view ChatGPT as\nbeneficial for various software security tasks, including vulnerability\ndetection, information retrieval, and penetration testing. Secondly, we\ndesigned an experiment aimed at investigating the practicality of this\ntechnology when deployed as an oracle in real-world settings. In particular, we\nfocused on vulnerability detection and qualitatively examined ChatGPT outputs\nfor given prompts within this prominent software security task. Based on our\nanalysis, responses from ChatGPT in this task are largely filled with generic\nsecurity information and may not be appropriate for industry use. To prevent\ndata leakage, we performed this analysis on a vulnerability dataset compiled\nafter the OpenAI data cut-off date from real-world projects covering 40\ndistinct vulnerability types and 12 programming languages. We assert that the\nfindings from this study would contribute to future research aimed at\ndeveloping and evaluating LLMs dedicated to software security.",
      "tldr_zh": "本研究通过定性分析探讨了ChatGPT在软件安全领域的感知与实际应用。研究首先进行实证调查，分析Twitter用户对ChatGPT在漏洞检测、信息检索和渗透测试等任务的看法，发现安全从业者认为其具有潜在益处。其次，通过实验评估ChatGPT在真实环境下的实用性，针对漏洞检测任务分析其输出，结果显示ChatGPT的响应多为泛化安全信息，不适合工业实际使用。实验基于OpenAI数据截止日期后的数据集，涵盖40种漏洞类型和12种编程语言。该研究成果有助于未来开发和评估专用于软件安全的LLMs。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for publication at International Conference on Trust,\n  Privacy and Security - 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00435v1",
      "published_date": "2024-08-01 10:14:05 UTC",
      "updated_date": "2024-08-01 10:14:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:13:01.580049"
    },
    {
      "arxiv_id": "2408.00429v1",
      "title": "Augmenting Channel Simulator and Semi- Supervised Learning for Efficient Indoor Positioning",
      "title_zh": "翻译失败",
      "authors": [
        "Yupeng Li",
        "Xinyu Ning",
        "Shijian Gao",
        "Yitong Liu",
        "Zhi Sun",
        "Qixing Wang",
        "Jiangzhou Wang"
      ],
      "abstract": "This work aims to tackle the labor-intensive and resource-consuming task of\nindoor positioning by proposing an efficient approach. The proposed approach\ninvolves the introduction of a semi-supervised learning (SSL) with a biased\nteacher (SSLB) algorithm, which effectively utilizes both labeled and unlabeled\nchannel data. To reduce measurement expenses, unlabeled data is generated using\nan updated channel simulator (UCHS), and then weighted by adaptive confidence\nvalues to simplify the tuning of hyperparameters. Simulation results\ndemonstrate that the proposed strategy achieves superior performance while\nminimizing measurement overhead and training expense compared to existing\nbenchmarks, offering a valuable and practical solution for indoor positioning.",
      "tldr_zh": "本研究针对室内定位的劳动密集型问题，提出了一种高效方法，结合半监督学习（Semi-Supervised Learning, SSL）算法及其带有偏置教师的变体（SSLB），以有效利用标记和未标记的通道数据。通过引入更新后的通道模拟器（Updated Channel Simulator, UCHS）生成未标记数据，并使用自适应置信值进行加权，从而简化超参数调整并减少测量开销。模拟结果显示，该策略在性能上优于现有基准，同时显著降低测量和训练开销，为室内定位提供了实用解决方案。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "ACCEPTED for presentation at 2024 IEEE Global Communications\n  Conference",
      "pdf_url": "http://arxiv.org/pdf/2408.00429v1",
      "published_date": "2024-08-01 10:06:02 UTC",
      "updated_date": "2024-08-01 10:06:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:13:11.853710"
    },
    {
      "arxiv_id": "2408.00427v2",
      "title": "CARMIL: Context-Aware Regularization on Multiple Instance Learning models for Whole Slide Images",
      "title_zh": "翻译失败",
      "authors": [
        "Thiziri Nait Saada",
        "Valentina Di Proietto",
        "Benoit Schmauch",
        "Katharina Von Loga",
        "Lucas Fidon"
      ],
      "abstract": "Multiple Instance Learning (MIL) models have proven effective for cancer\nprognosis from Whole Slide Images. However, the original MIL formulation\nincorrectly assumes the patches of the same image to be independent, leading to\na loss of spatial context as information flows through the network.\nIncorporating contextual knowledge into predictions is particularly important\ngiven the inclination for cancerous cells to form clusters and the presence of\nspatial indicators for tumors. State-of-the-art methods often use attention\nmechanisms eventually combined with graphs to capture spatial knowledge. In\nthis paper, we take a novel and transversal approach, addressing this issue\nthrough the lens of regularization. We propose Context-Aware Regularization for\nMultiple Instance Learning (CARMIL), a versatile regularization scheme designed\nto seamlessly integrate spatial knowledge into any MIL model. Additionally, we\npresent a new and generic metric to quantify the Context-Awareness of any MIL\nmodel when applied to Whole Slide Images, resolving a previously unexplored gap\nin the field. The efficacy of our framework is evaluated for two survival\nanalysis tasks on glioblastoma (TCGA GBM) and colon cancer data (TCGA COAD).",
      "tldr_zh": "该研究指出，Multiple Instance Learning (MIL) 模型在处理Whole Slide Images进行癌症预后时，忽略了图像patches之间的空间上下文，导致预测准确性不足。论文提出CARMIL，一种通用的Context-Aware Regularization方案，能够无缝整合空间知识到任何MIL模型中，同时引入一个新指标来量化模型的上下文感知能力。实验在胶质母细胞瘤(TCGA GBM)和结肠癌(TCGA COAD)的数据集上评估了两个生存分析任务，证明了CARMIL的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00427v2",
      "published_date": "2024-08-01 09:59:57 UTC",
      "updated_date": "2024-08-12 08:45:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:13:22.874278"
    },
    {
      "arxiv_id": "2408.00421v1",
      "title": "Towards Evolutionary-based Automated Machine Learning for Small Molecule Pharmacokinetic Prediction",
      "title_zh": "面向基于",
      "authors": [
        "Alex G. C. de Sá",
        "David B. Ascher"
      ],
      "abstract": "Machine learning (ML) is revolutionising drug discovery by expediting the\nprediction of small molecule properties essential for developing new drugs.\nThese properties -- including absorption, distribution, metabolism and\nexcretion (ADME)-- are crucial in the early stages of drug development since\nthey provide an understanding of the course of the drug in the organism, i.e.,\nthe drug's pharmacokinetics. However, existing methods lack personalisation and\nrely on manually crafted ML algorithms or pipelines, which can introduce\ninefficiencies and biases into the process. To address these challenges, we\npropose a novel evolutionary-based automated ML method (AutoML) specifically\ndesigned for predicting small molecule properties, with a particular focus on\npharmacokinetics. Leveraging the advantages of grammar-based genetic\nprogramming, our AutoML method streamlines the process by automatically\nselecting algorithms and designing predictive pipelines tailored to the\nparticular characteristics of input molecular data. Results demonstrate\nAutoML's effectiveness in selecting diverse ML algorithms, resulting in\ncomparable or even improved predictive performances compared to conventional\napproaches. By offering personalised ML-driven pipelines, our method promises\nto enhance small molecule research in drug discovery, providing researchers\nwith a valuable tool for accelerating the development of novel therapeutic\ndrugs.",
      "tldr_zh": "本文提出了一种基于进化的 Automated Machine Learning (AutoML) 方法，旨在提升小分子药代动力学预测，包括吸收、分部、代谢和排泄（ADME），以解决现有手动设计 ML 算法的效率低下和偏差问题。该方法利用 grammar-based genetic programming 自动选择算法并设计个性化预测管道，适应输入分子数据的特性。实验结果表明，AutoML 在预测性能上与传统方法相当或更好，为药物发现中的小分子研究提供了一个加速新药开发的宝贵工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper accepted and presented at the 14th Workshop on Evolutionary\n  Computation for the Automated Design of Algorithms (ECADA), which happened\n  during the Genetic and Evolutionary Computation Conference (GECCO)",
      "pdf_url": "http://arxiv.org/pdf/2408.00421v1",
      "published_date": "2024-08-01 09:46:06 UTC",
      "updated_date": "2024-08-01 09:46:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:13:34.707905"
    },
    {
      "arxiv_id": "2408.00420v1",
      "title": "MPT-PAR:Mix-Parameters Transformer for Panoramic Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Wenqing Gan",
        "Yan Sun",
        "Feiran Liu",
        "Xiangfeng Luo"
      ],
      "abstract": "The objective of the panoramic activity recognition task is to identify\nbehaviors at various granularities within crowded and complex environments,\nencompassing individual actions, social group activities, and global\nactivities. Existing methods generally use either parameter-independent modules\nto capture task-specific features or parameter-sharing modules to obtain common\nfeatures across all tasks. However, there is often a strong interrelatedness\nand complementary effect between tasks of different granularities that previous\nmethods have yet to notice. In this paper, we propose a model called MPT-PAR\nthat considers both the unique characteristics of each task and the synergies\nbetween different tasks simultaneously, thereby maximizing the utilization of\nfeatures across multi-granularity activity recognition. Furthermore, we\nemphasize the significance of temporal and spatial information by introducing a\nspatio-temporal relation-enhanced module and a scene representation learning\nmodule, which integrate the the spatio-temporal context of action and global\nscene into the feature map of each granularity. Our method achieved an overall\nF1 score of 47.5\\% on the JRDB-PAR dataset, significantly outperforming all the\nstate-of-the-art methods.",
      "tldr_zh": "该论文提出MPT-PAR模型，一种基于Mix-Parameters Transformer的框架，用于全景活动识别（panoramic activity recognition），旨在识别拥挤复杂环境中的多粒度行为，包括个体动作、社会群体活动和全局活动。MPT-PAR同时考虑每个任务的独特特征和不同任务间的协同效应，通过参数混合策略最大化特征利用。论文引入时空关系增强模块（spatio-temporal relation-enhanced module）和场景表示学习模块（scene representation learning module），将动作的时空上下文和全局场景整合到各粒度特征图中。在JRDB-PAR数据集上，该方法实现了47.5%的整体F1 score，显著优于现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00420v1",
      "published_date": "2024-08-01 09:42:44 UTC",
      "updated_date": "2024-08-01 09:42:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:13:57.080859"
    },
    {
      "arxiv_id": "2408.00415v1",
      "title": "DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving",
      "title_zh": "DriveArena：用于自动驾驶的闭环生成式模拟平台",
      "authors": [
        "Xuemeng Yang",
        "Licheng Wen",
        "Yukai Ma",
        "Jianbiao Mei",
        "Xin Li",
        "Tiantian Wei",
        "Wenjie Lei",
        "Daocheng Fu",
        "Pinlong Cai",
        "Min Dou",
        "Botian Shi",
        "Liang He",
        "Yong Liu",
        "Yu Qiao"
      ],
      "abstract": "This paper presented DriveArena, the first high-fidelity closed-loop\nsimulation system designed for driving agents navigating in real scenarios.\nDriveArena features a flexible, modular architecture, allowing for the seamless\ninterchange of its core components: Traffic Manager, a traffic simulator\ncapable of generating realistic traffic flow on any worldwide street map, and\nWorld Dreamer, a high-fidelity conditional generative model with infinite\nautoregression. This powerful synergy empowers any driving agent capable of\nprocessing real-world images to navigate in DriveArena's simulated environment.\nThe agent perceives its surroundings through images generated by World Dreamer\nand output trajectories. These trajectories are fed into Traffic Manager,\nachieving realistic interactions with other vehicles and producing a new scene\nlayout. Finally, the latest scene layout is relayed back into World Dreamer,\nperpetuating the simulation cycle. This iterative process fosters closed-loop\nexploration within a highly realistic environment, providing a valuable\nplatform for developing and evaluating driving agents across diverse and\nchallenging scenarios. DriveArena signifies a substantial leap forward in\nleveraging generative image data for the driving simulation platform, opening\ninsights for closed-loop autonomous driving. Code will be available soon on\nGitHub: https://github.com/PJLab-ADG/DriveArena",
      "tldr_zh": "本论文介绍了DriveArena，这是一个高保真闭环生成模拟平台，用于自动驾驶代理在真实场景中的导航。平台采用模块化架构，包括Traffic Manager（能生成全球街道地图上的真实交通流）和World Dreamer（高保真条件生成模型，支持无限自回归），实现代理通过图像感知环境、输出轨迹并与车辆交互的闭环循环。该系统为开发和评估驾驶代理提供了高度真实的测试环境，标志着利用生成图像数据推进驾驶模拟的重大进步。代码即将开源于GitHub。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "19 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00415v1",
      "published_date": "2024-08-01 09:32:01 UTC",
      "updated_date": "2024-08-01 09:32:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:14:08.713551"
    },
    {
      "arxiv_id": "2408.00399v1",
      "title": "Unsupervised Pairwise Causal Discovery on Heterogeneous Data using Mutual Information Measures",
      "title_zh": "翻译失败",
      "authors": [
        "Alexandre Trilla",
        "Nenad Mijatovic"
      ],
      "abstract": "A fundamental task in science is to determine the underlying causal relations\nbecause it is the knowledge of this functional structure what leads to the\ncorrect interpretation of an effect given the apparent associations in the\nobserved data. In this sense, Causal Discovery is a technique that tackles this\nchallenge by analyzing the statistical properties of the constituent variables.\nIn this work, we target the generalizability of the discovery method by\nfollowing a reductionist approach that only involves two variables, i.e., the\npairwise or bi-variate setting. We question the current (possibly misleading)\nbaseline results on the basis that they were obtained through supervised\nlearning, which is arguably contrary to this genuinely exploratory endeavor. In\nconsequence, we approach this problem in an unsupervised way, using robust\nMutual Information measures, and observing the impact of the different variable\ntypes, which is oftentimes ignored in the design of solutions. Thus, we provide\na novel set of standard unbiased results that can serve as a reference to guide\nfuture discovery tasks in completely unknown environments.",
      "tldr_zh": "这篇论文针对异构数据的无监督 Pairwise Causal Discovery，提出一种基于 Mutual Information Measures 的方法，以分析变量间的因果关系并避免现有监督学习基准的潜在偏差。通过考虑不同变量类型的冲击，该方法在探索性任务中提高了泛化性。最终，论文提供了新的标准无偏结果，作为未来未知环境因果发现任务的参考基准。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "26th International Conference of the Catalan Association for\n  Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2408.00399v1",
      "published_date": "2024-08-01 09:11:08 UTC",
      "updated_date": "2024-08-01 09:11:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:14:11.299395"
    },
    {
      "arxiv_id": "2408.05231v3",
      "title": "Predictive maintenance solution for industrial systems -- an unsupervised approach based on log periodic power law",
      "title_zh": "翻译失败",
      "authors": [
        "Bogdan Łobodziński"
      ],
      "abstract": "A new unsupervised predictive maintenance analysis method based on the\nrenormalization group approach used to discover critical behavior in complex\nsystems has been proposed. The algorithm analyzes univariate time series and\ndetects critical points based on a newly proposed theorem that identifies\ncritical points using a Log Periodic Power Law function fits. Application of a\nnew algorithm for predictive maintenance analysis of industrial data collected\nfrom reciprocating compressor systems is presented. Based on the knowledge of\nthe dynamics of the analyzed compressor system, the proposed algorithm predicts\nvalve and piston rod seal failures well in advance.",
      "tldr_zh": "本文提出了一种基于重整化群方法的重组无监督预测性维护分析算法，用于发现复杂系统中的临界行为，通过拟合 Log Periodic Power Law 函数来检测单变量时间序列中的关键点。算法的核心原理是利用新提出的定理，提前识别潜在故障。该方法应用于往复式压缩机系统，成功预测了阀门和活塞杆密封故障，为工业系统的维护提供了可靠的解决方案。",
      "categories": [
        "stat.AP",
        "cs.AI",
        "physics.data-an"
      ],
      "primary_category": "stat.AP",
      "comment": "14 pages, 4 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2408.05231v3",
      "published_date": "2024-08-01 09:01:27 UTC",
      "updated_date": "2025-04-29 13:37:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:14:21.606369"
    },
    {
      "arxiv_id": "2408.00380v3",
      "title": "EXAONEPath 1.0 Patch-level Foundation Model for Pathology",
      "title_zh": "翻译失败",
      "authors": [
        "Juseung Yun",
        "Yi Hu",
        "Jinhyung Kim",
        "Jongseong Jang",
        "Soonyoung Lee"
      ],
      "abstract": "Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nEXAONEPath, a novel foundational model trained on patches that have undergone\nstain normalization. Stain normalization helps reduce color variability arising\nfrom different laboratories and scanners, enabling the model to learn more\nconsistent features. EXAONEPath is trained using 285,153,903 patches extracted\nfrom a total of 34,795 WSIs. Our experiments demonstrate that EXAONEPath\nsignificantly mitigates the feature collapse problem, indicating that the model\nhas learned more generalized features rather than overfitting to individual WSI\ncharacteristics. We compared EXAONEPath with state-of-the-art models across six\ndownstream task datasets, and our results show that EXAONEPath achieves\nsuperior performance relative to the number of WSIs used and the model's\nparameter count. This suggests that the application of stain normalization has\nsubstantially improved the model's efficiency and generalization capabilities.",
      "tldr_zh": "该研究发现，数字病理学中基于自监督学习的模型在提取自巨像素全滑片图像（WSIs）的 patches 时，容易出现 WSI-specific feature collapse 问题，从而影响模型的泛化能力和下游任务性能。为解决这一问题，研究团队开发了 EXAONEPath 1.0，这是一个新型的 patch-level 基础模型，通过 stain normalization 减少颜色变异，并使用 285,153,903 个 patches 来自 34,795 个 WSIs 进行训练。实验结果表明，EXAONEPath 显著缓解了 feature collapse 问题，并在六个下游任务数据集上表现出优于最先进模型的性能，尤其在 WSI 数量和参数计数方面更具效率和泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "License updated",
      "pdf_url": "http://arxiv.org/pdf/2408.00380v3",
      "published_date": "2024-08-01 08:41:13 UTC",
      "updated_date": "2024-08-22 05:07:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:14:35.870936"
    },
    {
      "arxiv_id": "2408.00376v1",
      "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Shiji Zhou",
        "Lianzhe Wang",
        "Jiangnan Ye",
        "Yongliang Wu",
        "Heng Chang"
      ],
      "abstract": "Generative AI (GenAI), which aims to synthesize realistic and diverse data\nsamples from latent variables or other data modalities, has achieved remarkable\nresults in various domains, such as natural language, images, audio, and\ngraphs. However, they also pose challenges and risks to data privacy, security,\nand ethics. Machine unlearning is the process of removing or weakening the\ninfluence of specific data samples or features from a trained model, without\naffecting its performance on other data or tasks. While machine unlearning has\nshown significant efficacy in traditional machine learning tasks, it is still\nunclear if it could help GenAI become safer and aligned with human desire. To\nthis end, this position paper provides an in-depth discussion of the machine\nunlearning approaches for GenAI. Firstly, we formulate the problem of machine\nunlearning tasks on GenAI and introduce the background. Subsequently, we\nsystematically examine the limitations of machine unlearning on GenAI models by\nfocusing on the two representative branches: LLMs and image generative\n(diffusion) models. Finally, we provide our prospects mainly from three\naspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and\nconscientiously advocate for the future development of this field.",
      "tldr_zh": "这篇论文探讨了 Machine Unlearning 在 Generative AI 中的限制和前景，旨在解决 GenAI 在数据隐私、安全和伦理方面的挑战。作者系统分析了 Machine Unlearning 在 LLMs 和图像生成（diffusion）模型中的局限性，包括其在传统机器学习中有效但在 GenAI 上可能不足的问题。论文提供了未来展望，强调需要改进基准、评价指标以及实用性-unlearning 权衡，以推动该领域更安全和可信的发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00376v1",
      "published_date": "2024-08-01 08:35:40 UTC",
      "updated_date": "2024-08-01 08:35:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:14:48.716619"
    },
    {
      "arxiv_id": "2408.00374v3",
      "title": "Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Xi Chen",
        "Rahul Bhadani",
        "Larry Head"
      ],
      "abstract": "Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\nhttps://github.com/xichennn/V2I_trajectory_prediction.",
      "tldr_zh": "该研究提出 V2INet，一种新型轨迹预测框架，用于整合多视角数据（如 V2V 和 V2I 通信）以克服单一视角的局限，例如遮挡和有限视野。不同于以往手动融合或分阶段训练的方法，V2INet 通过扩展现有单视角模型实现端到端训练，提升了灵活性和性能，同时使用后验 Conformal Prediction 模块校准多模态轨迹预测，以提供有效且高效的置信区间。在真实世界 V2X-Seq 数据集上评估显示，V2INet 在 Final Displacement Error (FDE) 和 Miss Rate (MR) 上显著优于基线模型，且仅需单个 GPU，代码已公开可用。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00374v3",
      "published_date": "2024-08-01 08:32:03 UTC",
      "updated_date": "2025-03-11 18:19:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:15:11.908293"
    },
    {
      "arxiv_id": "2408.00370v1",
      "title": "DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2 framework",
      "title_zh": "翻译失败",
      "authors": [
        "Fan Zhang",
        "Naye Ji",
        "Fuxing Gao",
        "Bozuo Zhao",
        "Jingmei Wu",
        "Yanbing Jiang",
        "Hui Du",
        "Zhenqing Ye",
        "Jiayang Zhu",
        "WeiFan Zhong",
        "Leyao Yan",
        "Xiaomeng Ma"
      ],
      "abstract": "Speech-driven gesture generation is an emerging domain within virtual human\ncreation, where current methods predominantly utilize Transformer-based\narchitectures that necessitate extensive memory and are characterized by slow\ninference speeds. In response to these limitations, we propose\n\\textit{DiM-Gestures}, a novel end-to-end generative model crafted to create\nhighly personalized 3D full-body gestures solely from raw speech audio,\nemploying Mamba-based architectures. This model integrates a Mamba-based fuzzy\nfeature extractor with a non-autoregressive Adaptive Layer Normalization\n(AdaLN) Mamba-2 diffusion architecture. The extractor, leveraging a Mamba\nframework and a WavLM pre-trained model, autonomously derives implicit,\ncontinuous fuzzy features, which are then unified into a singular latent\nfeature. This feature is processed by the AdaLN Mamba-2, which implements a\nuniform conditional mechanism across all tokens to robustly model the interplay\nbetween the fuzzy features and the resultant gesture sequence. This innovative\napproach guarantees high fidelity in gesture-speech synchronization while\nmaintaining the naturalness of the gestures. Employing a diffusion model for\ntraining and inference, our framework has undergone extensive subjective and\nobjective evaluations on the ZEGGS and BEAT datasets. These assessments\nsubstantiate our model's enhanced performance relative to contemporary\nstate-of-the-art methods, demonstrating competitive outcomes with the DiTs\narchitecture (Persona-Gestors) while optimizing memory usage and accelerating\ninference speed.",
      "tldr_zh": "本研究针对语音驱动手势生成的内存需求大和推理速度慢等问题，提出DiM-Gestures模型，这是一个端到端生成框架，使用Mamba-based架构从原始语音音频提取隐式模糊特征，并通过非自回归的Adaptive Layer Normalization (AdaLN) Mamba-2扩散架构生成个性化3D全身体势。\n模型整合WavLM预训练模型来提取连续模糊特征，并统一处理这些特征以确保手势与语音的高度同步和自然性。\n在ZEGGS和BEAT数据集上的主观及客观评估表明，DiM-Gestures在性能上优于现有Transformer-based方法，与DiTs架构（Persona-Gestors）相当，同时显著降低了内存使用并加速了推理速度。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.RO",
        "cs.SD"
      ],
      "primary_category": "cs.GR",
      "comment": "10 pages,10 figures. arXiv admin note: text overlap with\n  arXiv:2403.10805",
      "pdf_url": "http://arxiv.org/pdf/2408.00370v1",
      "published_date": "2024-08-01 08:22:47 UTC",
      "updated_date": "2024-08-01 08:22:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:15:14.722779"
    },
    {
      "arxiv_id": "2408.07287v2",
      "title": "Abductive Reasoning in a Paraconsistent Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Meghyn Bienvenu",
        "Katsumi Inoue",
        "Daniil Kozhemiachenko"
      ],
      "abstract": "We explore the problem of explaining observations starting from a classically\ninconsistent theory by adopting a paraconsistent framework. We consider two\nexpansions of the well-known Belnap--Dunn paraconsistent four-valued logic\n$\\mathsf{BD}$: $\\mathsf{BD}_\\circ$ introduces formulas of the form $\\circ\\phi$\n(the information on $\\phi$ is reliable), while $\\mathsf{BD}_\\triangle$ augments\nthe language with $\\triangle\\phi$'s (there is information that $\\phi$ is true).\nWe define and motivate the notions of abduction problems and explanations in\n$\\mathsf{BD}_\\circ$ and $\\mathsf{BD}_\\triangle$ and show that they are not\nreducible to one another. We analyse the complexity of standard abductive\nreasoning tasks (solution recognition, solution existence, and relevance /\nnecessity of hypotheses) in both logics. Finally, we show how to reduce\nabduction in $\\mathsf{BD}_\\circ$ and $\\mathsf{BD}_\\triangle$ to abduction in\nclassical propositional logic, thereby enabling the reuse of existing abductive\nreasoning procedures.",
      "tldr_zh": "本研究探讨了在paraconsistent framework下，从一个经典不一致理论中解释观察的abductive reasoning问题。作者扩展了Belnap-Dunn的四值逻辑$\\mathsf{BD}$，引入$\\mathsf{BD}_\\circ$（添加$\\circ\\phi$表示$\\phi$信息可靠）和$\\mathsf{BD}_\\triangle$（添加$\\triangle\\phi$表示有信息表明$\\phi$为真），并定义了这些逻辑中的abduction problems和explanations，同时证明它们无法相互归约。研究分析了标准abductive reasoning任务（如solution recognition、solution existence和hypotheses的relevance/necessity）的复杂度，并展示了如何将$\\mathsf{BD}_\\circ$和$\\mathsf{BD}_\\triangle$中的abduction归约到经典命题逻辑，从而便于重用现有推理程序。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "math.LO"
      ],
      "primary_category": "cs.LO",
      "comment": "This is an extended version of a paper with the same title appearing\n  at the 21st International Conference on Principles of Knowledge\n  Representation and Reasoning (KR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2408.07287v2",
      "published_date": "2024-08-01 08:12:52 UTC",
      "updated_date": "2024-08-23 14:05:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:15:25.590247"
    },
    {
      "arxiv_id": "2408.07283v2",
      "title": "Queries With Exact Truth Values in Paraconsistent Description Logics",
      "title_zh": "在矛盾容忍描述逻辑中带有精确真值的查询",
      "authors": [
        "Meghyn Bienvenu",
        "Camille Bourgaux",
        "Daniil Kozhemiachenko"
      ],
      "abstract": "We present a novel approach to querying classical inconsistent description\nlogic (DL) knowledge bases by adopting a~paraconsistent semantics with the four\nBelnapian values: exactly true ($\\mathbf{T}$), exactly false ($\\mathbf{F}$),\nboth ($\\mathbf{B}$), and neither ($\\mathbf{N}$). In contrast to prior studies\non paraconsistent DLs, we allow truth value operators in the query language,\nwhich can be used to differentiate between answers having contradictory\nevidence and those having only positive evidence. We present a reduction to\nclassical DL query answering that allows us to pinpoint the precise combined\nand data complexity of answering queries with values in paraconsistent\n$\\mathcal{ALCHI}$ and its sublogics. Notably, we show that tractable data\ncomplexity is retained for Horn DLs. We present a comparison with repair-based\ninconsistency-tolerant semantics, showing that the two approaches are\nincomparable.",
      "tldr_zh": "本研究提出了一种新方法，用于查询经典不一致的描述逻辑（DL）知识库，采用paraconsistent semantics和Belnapian四值（exactly true $\\mathbf{T}$、exactly false $\\mathbf{F}$、both $\\mathbf{B}$ 和 neither $\\mathbf{N}$），以更好地处理知识不一致性。不同于以往，该方法在查询语言中引入truth value operators，用于区分有矛盾证据的答案与仅正向证据的答案，并通过减少到经典DL查询回答来精确分析paraconsistent $\\mathcal{ALCHI}$及其子逻辑的组合和数据复杂性。结果显示，Horn DLs保持了可处理的tractable data complexity，且该方法与repair-based inconsistency-tolerant semantics不可比较，为处理不一致知识提供了更灵活的框架。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.DB",
        "math.LO"
      ],
      "primary_category": "cs.LO",
      "comment": "This is an extended version of a paper with the same title appearing\n  at the 21st International Conference on Principles of Knowledge\n  Representation and Reasoning (KR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2408.07283v2",
      "published_date": "2024-08-01 08:11:50 UTC",
      "updated_date": "2024-08-15 07:33:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:15:36.999825"
    },
    {
      "arxiv_id": "2408.00365v2",
      "title": "Multimodal Fusion and Coherence Modeling for Video Topic Segmentation",
      "title_zh": "多模态融合与连贯性建模用于视频主题分割",
      "authors": [
        "Hai Yu",
        "Chong Deng",
        "Qinglin Zhang",
        "Jiaqing Liu",
        "Qian Chen",
        "Wen Wang"
      ],
      "abstract": "The video topic segmentation (VTS) task segments videos into intelligible,\nnon-overlapping topics, facilitating efficient comprehension of video content\nand quick access to specific content. VTS is also critical to various\ndownstream video understanding tasks. Traditional VTS methods using shallow\nfeatures or unsupervised approaches struggle to accurately discern the nuances\nof topical transitions. Recently, supervised approaches have achieved superior\nperformance on video action or scene segmentation over unsupervised approaches.\nIn this work, we improve supervised VTS by thoroughly exploring multimodal\nfusion and multimodal coherence modeling. Specifically, (1) we enhance\nmultimodal fusion by exploring different architectures using cross-attention\nand mixture of experts. (2) To generally strengthen multimodality alignment and\nfusion, we pre-train and fine-tune the model with multimodal contrastive\nlearning. (3) We propose a new pre-training task tailored for the VTS task, and\na novel fine-tuning task for enhancing multimodal coherence modeling for VTS.\nWe evaluate the proposed approaches on educational videos, in the form of\nlectures, due to the vital role of topic segmentation of educational videos in\nboosting learning experiences. Additionally, we introduce a large-scale Chinese\nlecture video dataset to augment the existing English corpus, promoting further\nresearch in VTS. Experiments on both English and Chinese lecture datasets\ndemonstrate that our model achieves superior VTS performance compared to\ncompetitive unsupervised and supervised baselines.",
      "tldr_zh": "本研究针对视频主题分割 (VTS) 任务，提出了一种改进的监督方法，通过探索多模态融合和多模态一致性建模来提升视频内容的主题识别准确性。具体而言，该方法包括使用 cross-attention 和 mixture of experts 架构增强多模态融合、通过多模态对比学习进行预训练和微调，以及设计新的预训练任务和微调任务来加强多模态一致性。研究引入了一个大规模中文讲座视频数据集，并在英语和中文教育视频数据集上进行实验，结果显示该模型在 VTS 性能上优于现有的无监督和监督基线。总的来说，此工作为视频理解任务提供了更有效的工具，并促进了跨语言数据集的研究。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00365v2",
      "published_date": "2024-08-01 08:10:32 UTC",
      "updated_date": "2024-12-30 02:50:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:15:49.921135"
    },
    {
      "arxiv_id": "2408.00355v3",
      "title": "DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Xie",
        "Qian Qiao",
        "Jun Gao",
        "Tianxiang Wu",
        "Jiaqing Fan",
        "Yue Zhang",
        "Jielei Zhang",
        "Huyang Sun"
      ],
      "abstract": "More and more end-to-end text spotting methods based on Transformer\narchitecture have demonstrated superior performance. These methods utilize a\nbipartite graph matching algorithm to perform one-to-one optimal matching\nbetween predicted objects and actual objects. However, the instability of\nbipartite graph matching can lead to inconsistent optimization targets, thereby\naffecting the training performance of the model. Existing literature applies\ndenoising training to solve the problem of bipartite graph matching instability\nin object detection tasks. Unfortunately, this denoising training method cannot\nbe directly applied to text spotting tasks, as these tasks need to perform\nirregular shape detection tasks and more complex text recognition tasks than\nclassification. To address this issue, we propose a novel denoising training\nmethod (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we\ndecompose the queries of the denoising part into noised positional queries and\nnoised content queries. We use the four Bezier control points of the Bezier\ncenter curve to generate the noised positional queries. For the noised content\nqueries, considering that the output of the text in a fixed positional order is\nnot conducive to aligning position with content, we employ a masked character\nsliding method to initialize noised content queries, thereby assisting in the\nalignment of text content and position. To improve the model's perception of\nthe background, we further utilize an additional loss function for background\ncharacters classification in the denoising training part.Although DNTextSpotter\nis conceptually simple, it outperforms the state-of-the-art methods on four\nbenchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially\nyielding an improvement of 11.3% against the best approach in Inverse-Text\ndataset.",
      "tldr_zh": "该论文提出 DNTextSpotter，一种改进的 denoising training 方法，用于处理基于 Transformer 的端到端任意形状场景文本检测（Arbitrary-Shaped Scene Text Spotting），以解决 bipartite graph matching 不稳定性导致的优化问题。方法将查询分解为 noised positional queries 和 noised content queries，使用 Bezier control points 生成位置查询，并采用 masked character sliding 初始化内容查询，同时添加背景字符分类损失函数来提升模型感知。实验结果显示，DNTextSpotter 在 Total-Text、SCUT-CTW1500、ICDAR15 和 Inverse-Text 等四个基准数据集上超越现有方法，尤其在 Inverse-Text 上提高了 11.3% 的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACM'MM2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00355v3",
      "published_date": "2024-08-01 07:52:07 UTC",
      "updated_date": "2024-11-03 14:33:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:16:01.640117"
    },
    {
      "arxiv_id": "2408.00350v1",
      "title": "A Simple Background Augmentation Method for Object Detection with Diffusion Model",
      "title_zh": "一种基于扩散模型的简单背景增强方法，用于物体检测",
      "authors": [
        "Yuhang Li",
        "Xin Dong",
        "Chen Chen",
        "Weiming Zhuang",
        "Lingjuan Lyu"
      ],
      "abstract": "In computer vision, it is well-known that a lack of data diversity will\nimpair model performance. In this study, we address the challenges of enhancing\nthe dataset diversity problem in order to benefit various downstream tasks such\nas object detection and instance segmentation. We propose a simple yet\neffective data augmentation approach by leveraging advancements in generative\nmodels, specifically text-to-image synthesis technologies like Stable\nDiffusion. Our method focuses on generating variations of labeled real images,\nutilizing generative object and background augmentation via inpainting to\naugment existing training data without the need for additional annotations. We\nfind that background augmentation, in particular, significantly improves the\nmodels' robustness and generalization capabilities. We also investigate how to\nadjust the prompt and mask to ensure the generated content comply with the\nexisting annotations. The efficacy of our augmentation techniques is validated\nthrough comprehensive evaluations of the COCO dataset and several other key\nobject detection benchmarks, demonstrating notable enhancements in model\nperformance across diverse scenarios. This approach offers a promising solution\nto the challenges of dataset enhancement, contributing to the development of\nmore accurate and robust computer vision models.",
      "tldr_zh": "本研究针对计算机视觉中数据多样性不足导致模型性能下降的问题，提出了一种简单有效的背景增强方法，使用生成模型如 Stable Diffusion 进行文本到图像合成。方法通过 inpainting 技术生成标注图像的背景和对象变体，从而增强训练数据集，而无需额外标注，并优化提示和掩码以确保生成内容符合现有标注。实验结果显示，该方法显著提高了 object detection 和实例分割任务的鲁棒性和泛化能力，在 COCO dataset 等基准上实现了模型性能的显著提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00350v1",
      "published_date": "2024-08-01 07:40:00 UTC",
      "updated_date": "2024-08-01 07:40:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:16:12.335010"
    },
    {
      "arxiv_id": "2408.00348v2",
      "title": "Securing the Diagnosis of Medical Imaging: An In-depth Analysis of AI-Resistant Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Md Abdullah Al Nasim",
        "Parag Biswas",
        "Abdur Rashid",
        "Kishor Datta Gupta",
        "Roy George",
        "Sovon Chakraborty",
        "Khalil Shujaee"
      ],
      "abstract": "Machine learning (ML) is a rapidly developing area of medicine that uses\nsignificant resources to apply computer science and statistics to medical\nissues. ML's proponents laud its capacity to handle vast, complicated, and\nerratic medical data. It's common knowledge that attackers might cause\nmisclassification by deliberately creating inputs for machine learning\nclassifiers. Research on adversarial examples has been extensively conducted in\nthe field of computer vision applications. Healthcare systems are thought to be\nhighly difficult because of the security and life-or-death considerations they\ninclude, and performance accuracy is very important. Recent arguments have\nsuggested that adversarial attacks could be made against medical image analysis\n(MedIA) technologies because of the accompanying technology infrastructure and\npowerful financial incentives. Since the diagnosis will be the basis for\nimportant decisions, it is essential to assess how strong medical DNN tasks are\nagainst adversarial attacks. Simple adversarial attacks have been taken into\naccount in several earlier studies. However, DNNs are susceptible to more risky\nand realistic attacks. The present paper covers recent proposed adversarial\nattack strategies against DNNs for medical imaging as well as countermeasures.\nIn this study, we review current techniques for adversarial imaging attacks,\ndetections. It also encompasses various facets of these techniques and offers\nsuggestions for the robustness of neural networks to be improved in the future.",
      "tldr_zh": "该论文分析了机器学习在医疗图像分析（MedIA）中的安全风险，重点探讨对抗性攻击（adversarial attacks）如何导致深度神经网络（DNNs）误分类，从而威胁诊断准确性。通过回顾现有对抗性攻击策略和检测方法，论文评估了DNNs在医疗成像领域的脆弱性，特别是更现实和危险的攻击类型。最终，论文提出改进神经网络鲁棒性的建议，包括未来增强安全性的潜在措施，以确保医疗决策的可靠性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00348v2",
      "published_date": "2024-08-01 07:37:27 UTC",
      "updated_date": "2024-10-19 19:15:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:16:25.199093"
    },
    {
      "arxiv_id": "2408.00347v2",
      "title": "Advancing Medical Image Segmentation: Morphology-Driven Learning with Diffusion Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Sungmin Kang",
        "Jaeha Song",
        "Jihie Kim"
      ],
      "abstract": "Understanding the morphological structure of medical images and precisely\nsegmenting the region of interest or abnormality is an important task that can\nassist in diagnosis. However, the unique properties of medical imaging make\nclear segmentation difficult,and the high cost and time-consuming task of\nlabeling leads to a coarse-grained representation of ground truth. Facing with\nthese problems, we propose a novel Diffusion Transformer Segmentation (DTS)\nmodel for robust segmentation in the presence of noise. We propose an\nalternative to the dominant Denoising U-Net encoder through experiments\napplying a transformer architecture, which captures global dependency through\nself-attention. Additionally, we propose k-neighbor label smoothing, reverse\nboundary attention, and self-supervised learning with morphology-driven\nlearning to improve the ability to identify complex structures. Our model,\nwhich analyzes the morphological representation of images, shows better results\nthan the previous models in various medical imaging modalities, including CT,\nMRI, and lesion images.",
      "tldr_zh": "该论文针对医疗图像分割面临的挑战，如图像独特属性导致的分割困难和高标注成本问题，提出了一种新型 Diffusion Transformer Segmentation (DTS) 模型，以实现鲁棒的噪声环境下分割。DTS 通过采用 Transformer 架构替换传统的 Denoising U-Net 编码器，利用自注意力机制捕获全局依赖，并引入 k-neighbor label smoothing、reverse boundary attention 和 morphology-driven learning 等技术，提升了对复杂形态结构的识别能力。实验结果表明，该模型在 CT、MRI 和病变图像等多种医疗成像模式下，比现有模型表现出更优表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in BMVC 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00347v2",
      "published_date": "2024-08-01 07:35:54 UTC",
      "updated_date": "2024-09-01 03:52:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:16:36.921876"
    },
    {
      "arxiv_id": "2408.00346v1",
      "title": "Neural Graph Matching for Video Retrieval in Large-Scale Video-driven E-commerce",
      "title_zh": "翻译失败",
      "authors": [
        "Houye Ji",
        "Ye Tang",
        "Zhaoxin Chen",
        "Lixi Deng",
        "Jun Hu",
        "Lei Su"
      ],
      "abstract": "With the rapid development of the short video industry, traditional\ne-commerce has encountered a new paradigm, video-driven e-commerce, which\nleverages attractive videos for product showcases and provides both video and\nitem services for users. Benefitting from the dynamic and visualized\nintroduction of items,video-driven e-commerce has shown huge potential in\nstimulating consumer confidence and promoting sales. In this paper, we focus on\nthe video retrieval task, facing the following challenges: (1) Howto handle the\nheterogeneities among users, items, and videos? (2)How to mine the\ncomplementarity between items and videos for better user understanding? In this\npaper, we first leverage the dual graph to model the co-existing of user-video\nand user-item interactions in video-driven e-commerce and innovatively reduce\nuser preference understanding to a graph matching problem. To solve it, we\nfurther propose a novel bi-level Graph Matching Network(GMN), which mainly\nconsists of node- and preference-level graph matching. Given a user, node-level\ngraph matching aims to match videos and items, while preference-level graph\nmatching aims to match multiple user preferences extracted from both videos and\nitems. Then the proposed GMN can generate and improve user embedding by\naggregating matched nodes or preferences from the dual graph in a bi-level\nmanner. Comprehensive experiments show the superiority of the proposed GMN with\nsignificant improvements over state-of-the-art approaches (e.g., AUC+1.9% and\nCTR+7.15%). We have developed it on a well-known video-driven e-commerce\nplatform, serving hundreds of millions of users every day",
      "tldr_zh": "该论文针对大型视频驱动电商中的视频检索问题，提出了一种基于神经图匹配(Neural Graph Matching)的框架，以处理用户、物品和视频之间的异质性，并挖掘物品与视频的互补性来更好地理解用户偏好。具体方法包括使用双图(dual graph)建模用户-视频和用户-物品交互，将用户偏好理解转化为图匹配问题，并开发了双层图匹配网络(bi-level Graph Matching Network, GMN)，通过节点级匹配视频与物品，以及偏好级匹配用户偏好来生成和优化用户嵌入。实验结果显示，GMN 比现有方法显著提升(AUC+1.9%、CTR+7.15%)，并已在实际视频驱动电商平台上部署，服务数亿用户。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00346v1",
      "published_date": "2024-08-01 07:31:23 UTC",
      "updated_date": "2024-08-01 07:31:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:16:51.100159"
    },
    {
      "arxiv_id": "2408.00342v1",
      "title": "MuJoCo MPC for Humanoid Control: Evaluation on HumanoidBench",
      "title_zh": "翻译失败",
      "authors": [
        "Moritz Meser",
        "Aditya Bhatt",
        "Boris Belousov",
        "Jan Peters"
      ],
      "abstract": "We tackle the recently introduced benchmark for whole-body humanoid control\nHumanoidBench using MuJoCo MPC. We find that sparse reward functions of\nHumanoidBench yield undesirable and unrealistic behaviors when optimized;\ntherefore, we propose a set of regularization terms that stabilize the robot\nbehavior across tasks. Current evaluations on a subset of tasks demonstrate\nthat our proposed reward function allows achieving the highest HumanoidBench\nscores while maintaining realistic posture and smooth control signals. Our code\nis publicly available and will become a part of MuJoCo MPC, enabling rapid\nprototyping of robot behaviors.",
      "tldr_zh": "本研究使用 MuJoCo MPC 在 HumanoidBench 基准上评估全身体人形机器人控制，发现其稀疏奖励函数会导致不理想和不现实的行为。针对此问题，论文提出一套正则化术语，以稳定机器人行为并优化控制信号。在子集任务的评估中，该方法实现了最高的 HumanoidBench 分数，同时保持现实姿势和平滑控制。代码已公开，将集成到 MuJoCo MPC 中，支持机器人行为的快速原型设计。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "3 pages, 3 figures, submitted to IEEE Conference on Robotics and\n  Automation (ICRA@40)",
      "pdf_url": "http://arxiv.org/pdf/2408.00342v1",
      "published_date": "2024-08-01 07:27:18 UTC",
      "updated_date": "2024-08-01 07:27:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:17:01.577111"
    },
    {
      "arxiv_id": "2408.00329v1",
      "title": "OTAD: An Optimal Transport-Induced Robust Model for Agnostic Adversarial Attack",
      "title_zh": "翻译失败",
      "authors": [
        "Kuo Gai",
        "Sicong Wang",
        "Shihua Zhang"
      ],
      "abstract": "Deep neural networks (DNNs) are vulnerable to small adversarial perturbations\nof the inputs, posing a significant challenge to their reliability and\nrobustness. Empirical methods such as adversarial training can defend against\nparticular attacks but remain vulnerable to more powerful attacks.\nAlternatively, Lipschitz networks provide certified robustness to unseen\nperturbations but lack sufficient expressive power. To harness the advantages\nof both approaches, we design a novel two-step Optimal Transport induced\nAdversarial Defense (OTAD) model that can fit the training data accurately\nwhile preserving the local Lipschitz continuity. First, we train a DNN with a\nregularizer derived from optimal transport theory, yielding a discrete optimal\ntransport map linking data to its features. By leveraging the map's inherent\nregularity, we interpolate the map by solving the convex integration problem\n(CIP) to guarantee the local Lipschitz property. OTAD is extensible to diverse\narchitectures of ResNet and Transformer, making it suitable for complex data.\nFor efficient computation, the CIP can be solved through training neural\nnetworks. OTAD opens a novel avenue for developing reliable and secure deep\nlearning systems through the regularity of optimal transport maps. Empirical\nresults demonstrate that OTAD can outperform other robust models on diverse\ndatasets.",
      "tldr_zh": "该论文提出OTAD模型，一种基于Optimal Transport理论的两步鲁棒防御方法，用于提升深度神经网络(DNNs)对未知对抗攻击的抵抗力。OTAD首先通过Optimal Transport派生的正则化训练DNN，生成一个离散的Optimal Transport map；随后，通过解决Convex Integration Problem (CIP)来插值该map，确保网络的局部Lipschitz连续性，从而兼顾模型的表达能力和鲁棒性。该模型可扩展到ResNet和Transformer等架构，并在各种数据集上实验中超越其他鲁棒模型，为开发可靠的深度学习系统开辟新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00329v1",
      "published_date": "2024-08-01 07:04:18 UTC",
      "updated_date": "2024-08-01 07:04:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:17:13.687815"
    },
    {
      "arxiv_id": "2408.00315v4",
      "title": "ADBM: Adversarial diffusion bridge model for reliable adversarial purification",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao Li",
        "Wenxuan Sun",
        "Huanran Chen",
        "Qiongxiu Li",
        "Yining Liu",
        "Yingzhe He",
        "Jie Shi",
        "Xiaolin Hu"
      ],
      "abstract": "Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.",
      "tldr_zh": "本文发现，现有的 Diffusion-based Purification (DiffPure) 方法在对抗样本防御中虽有效，但因直接使用预训练扩散模型，导致噪声净化性能与数据恢复质量之间存在权衡，且其评估可靠性不足，因为依赖弱 adaptive attacks。  \n为此，提出 Adversarial Diffusion Bridge Model (ADBM)，该模型直接构建从扩散对抗数据回溯到原始清洁数据的逆桥，从而显著增强净化能力。  \n通过理论分析和实验验证，ADBM 在各种场景下表现出优越的防御性能和稳健性，为实际对抗样本防御应用提供了可靠的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025, fix typos in the proof",
      "pdf_url": "http://arxiv.org/pdf/2408.00315v4",
      "published_date": "2024-08-01 06:26:05 UTC",
      "updated_date": "2025-03-19 05:26:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:17:25.297916"
    },
    {
      "arxiv_id": "2408.00309v1",
      "title": "Discretizing Continuous Action Space with Unimodal Probability Distributions for On-Policy Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanyang Zhu",
        "Zhi Wang",
        "Yuanheng Zhu",
        "Chunlin Chen",
        "Dongbin Zhao"
      ],
      "abstract": "For on-policy reinforcement learning, discretizing action space for\ncontinuous control can easily express multiple modes and is straightforward to\noptimize. However, without considering the inherent ordering between the\ndiscrete atomic actions, the explosion in the number of discrete actions can\npossess undesired properties and induce a higher variance for the policy\ngradient estimator. In this paper, we introduce a straightforward architecture\nthat addresses this issue by constraining the discrete policy to be unimodal\nusing Poisson probability distributions. This unimodal architecture can better\nleverage the continuity in the underlying continuous action space using\nexplicit unimodal probability distributions. We conduct extensive experiments\nto show that the discrete policy with the unimodal probability distribution\nprovides significantly faster convergence and higher performance for on-policy\nreinforcement learning algorithms in challenging control tasks, especially in\nhighly complex tasks such as Humanoid. We provide theoretical analysis on the\nvariance of the policy gradient estimator, which suggests that our attentively\ndesigned unimodal discrete policy can retain a lower variance and yield a\nstable learning process.",
      "tldr_zh": "本文提出了一种针对 on-policy reinforcement learning 的方法，通过使用 unimodal probability distributions（如 Poisson probability distributions）对连续动作空间进行离散化，解决传统离散化导致的动作数量爆炸和高 policy gradient estimator 方差问题。该架构约束离散策略为单峰分布，更好地利用连续动作空间的连续性，在实验中显示出更快收敛和更高性能，尤其在复杂任务如 Humanoid 上。理论分析表明，这种设计能降低策略梯度估计器的方差，提升学习的稳定性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "IEEE Transactions on Neural Networks and Learning Systems",
      "pdf_url": "http://arxiv.org/pdf/2408.00309v1",
      "published_date": "2024-08-01 06:06:53 UTC",
      "updated_date": "2024-08-01 06:06:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:17:37.433146"
    },
    {
      "arxiv_id": "2408.00307v1",
      "title": "ABC Align: Large Language Model Alignment for Safety & Accuracy",
      "title_zh": "翻译失败",
      "authors": [
        "Gareth Seneque",
        "Lap-Hang Ho",
        "Ariel Kuperman",
        "Nafise Erfanian Saeedi",
        "Jeffrey Molendijk"
      ],
      "abstract": "Alignment of Large Language Models (LLMs) remains an unsolved problem. Human\npreferences are highly distributed and can be captured at multiple levels of\nabstraction, from the individual to diverse populations. Organisational\npreferences, represented by standards and principles, are defined to mitigate\nreputational risk or meet legislative obligations. In this paper, we present\nABC Align, a novel alignment methodology for LLMs that enables integration of\nthe standards and preferences of a large media organisation into the LLM\nitself. We combine a set of data and methods that build on recent breakthroughs\nin synthetic data generation, preference optimisation, and post-training model\nquantisation. Our unified approach mitigates bias and improves accuracy, while\npreserving reasoning capability, as measured against standard benchmarks.",
      "tldr_zh": "本研究针对 Large Language Models (LLMs) 的对齐问题，探讨了人类偏好和组织标准的多样性挑战，提出了一种新型方法 ABC Align，以整合大型媒体组织的标准和偏好到 LLM 中。该方法结合了 synthetic data generation、preference optimisation 和 post-training model quantisation 等技术，有效减少偏见并提升模型准确性，同时保留了模型的推理能力。在标准基准测试中，ABC Align 展示了显著的性能改进，为更安全可靠的 LLM 应用提供了实用框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00307v1",
      "published_date": "2024-08-01 06:06:25 UTC",
      "updated_date": "2024-08-01 06:06:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:17:48.427604"
    },
    {
      "arxiv_id": "2408.00295v1",
      "title": "Contrastive Graph Representation Learning with Adversarial Cross-view Reconstruction and Information Bottleneck",
      "title_zh": "基于对抗跨视图重建和信息瓶颈的对比图表示学习",
      "authors": [
        "Yuntao Shou",
        "Haozhi Lan",
        "Xiangyong Cao"
      ],
      "abstract": "Graph Neural Networks (GNNs) have received extensive research attention due\nto their powerful information aggregation capabilities. Despite the success of\nGNNs, most of them suffer from the popularity bias issue in a graph caused by a\nsmall number of popular categories. Additionally, real graph datasets always\ncontain incorrect node labels, which hinders GNNs from learning effective node\nrepresentations. Graph contrastive learning (GCL) has been shown to be\neffective in solving the above problems for node classification tasks. Most\nexisting GCL methods are implemented by randomly removing edges and nodes to\ncreate multiple contrasting views, and then maximizing the mutual information\n(MI) between these contrasting views to improve the node feature\nrepresentation. However, maximizing the mutual information between multiple\ncontrasting views may lead the model to learn some redundant information\nirrelevant to the node classification task. To tackle this issue, we propose an\neffective Contrastive Graph Representation Learning with Adversarial Cross-view\nReconstruction and Information Bottleneck (CGRL) for node classification, which\ncan adaptively learn to mask the nodes and edges in the graph to obtain the\noptimal graph structure representation. Furthermore, we innovatively introduce\nthe information bottleneck theory into GCLs to remove redundant information in\nmultiple contrasting views while retaining as much information as possible\nabout node classification. Moreover, we add noise perturbations to the original\nviews and reconstruct the augmented views by constructing adversarial views to\nimprove the robustness of node feature representation. Extensive experiments on\nreal-world public datasets demonstrate that our method significantly\noutperforms existing state-of-the-art algorithms.",
      "tldr_zh": "该论文针对图神经网络 (GNNs) 的流行性偏差和错误节点标签问题，提出了一种新的对比图表示学习方法 CGRL（Contrastive Graph Representation Learning）。该方法通过自适应掩盖节点和边来优化图结构表示，并引入信息瓶颈理论（Information Bottleneck）来去除多视图间的冗余信息，同时利用对抗性跨视图重构添加噪声扰动，以提升节点特征表示的鲁棒性。实验结果显示，CGRL 在真实世界公共数据集上显著优于现有最先进算法，在节点分类任务中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2408.00295v1",
      "published_date": "2024-08-01 05:45:21 UTC",
      "updated_date": "2024-08-01 05:45:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:18:02.155611"
    },
    {
      "arxiv_id": "2408.00290v1",
      "title": "Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Bin Cheng",
        "Jiaxuan Lu"
      ],
      "abstract": "With the advent of the era of foundation models, pre-training and fine-tuning\nhave become common paradigms. Recently, parameter-efficient fine-tuning has\ngarnered widespread attention due to its better balance between the number of\nlearnable parameters and performance. However, some current parameter-efficient\nfine-tuning methods only model a single modality and lack the utilization of\nstructural knowledge in downstream tasks. To address this issue, this paper\nproposes a multi-modal parameter-efficient fine-tuning method based on graph\nnetworks. Each image is fed into a multi-modal large language model (MLLM) to\ngenerate a text description. The image and its corresponding text description\nare then processed by a frozen image encoder and text encoder to generate image\nfeatures and text features, respectively. A graph is constructed based on the\nsimilarity of the multi-modal feature nodes, and knowledge and relationships\nrelevant to these features are extracted from each node. Additionally, Elastic\nWeight Consolidation (EWC) regularization is incorporated into the loss\nfunction to mitigate the problem of forgetting during task learning. The\nproposed model achieves test accuracies on the OxfordPets, Flowers102, and\nFood101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The\ncode is available at https://github.com/yunche0/GA-Net/tree/master.",
      "tldr_zh": "该论文提出了一种基于 Graph Neural Network 的多模态 Parameter-Efficient Fine-tuning 方法，以解决现有方法仅处理单一模态并忽略下游任务结构知识的问题。方法包括使用 Multi-Modal Large Language Model (MLLM) 生成图像的文本描述，然后通过冻结的图像编码器和文本编码器提取特征，构建基于多模态特征节点相似性的图网络，并融入 Elastic Weight Consolidation (EWC) 正则化以缓解任务学习中的遗忘问题。在 OxfordPets、Flowers102 和 Food101 数据集上，该模型的测试准确率分别提高了 4.45%、2.92% 和 0.23%。这项创新提升了参数高效微调的性能，并提供了开源代码以便进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00290v1",
      "published_date": "2024-08-01 05:24:20 UTC",
      "updated_date": "2024-08-01 05:24:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:18:14.572895"
    },
    {
      "arxiv_id": "2408.00288v1",
      "title": "Gradient Harmonization in Unsupervised Domain Adaptation",
      "title_zh": "梯度协调在无监督域适应中",
      "authors": [
        "Fuxiang Huang",
        "Suqi Song",
        "Lei Zhang"
      ],
      "abstract": "Unsupervised domain adaptation (UDA) intends to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. Many current methods focus\non learning feature representations that are both discriminative for\nclassification and invariant across domains by simultaneously optimizing domain\nalignment and classification tasks. However, these methods often overlook a\ncrucial challenge: the inherent conflict between these two tasks during\ngradient-based optimization. In this paper, we delve into this issue and\nintroduce two effective solutions known as Gradient Harmonization, including GH\nand GH++, to mitigate the conflict between domain alignment and classification\ntasks. GH operates by altering the gradient angle between different tasks from\nan obtuse angle to an acute angle, thus resolving the conflict and trade-offing\nthe two tasks in a coordinated manner. Yet, this would cause both tasks to\ndeviate from their original optimization directions. We thus further propose an\nimproved version, GH++, which adjusts the gradient angle between tasks from an\nobtuse angle to a vertical angle. This not only eliminates the conflict but\nalso minimizes deviation from the original gradient directions. Finally, for\noptimization convenience and efficiency, we evolve the gradient harmonization\nstrategies into a dynamically weighted loss function using an integral operator\non the harmonized gradient. Notably, GH/GH++ are orthogonal to UDA and can be\nseamlessly integrated into most existing UDA models. Theoretical insights and\nexperimental analyses demonstrate that the proposed approaches not only enhance\npopular UDA baselines but also improve recent state-of-the-art models.",
      "tldr_zh": "这篇论文针对无监督域适应(Unsupervised Domain Adaptation, UDA)中的关键挑战，即域对齐和分类任务在梯度优化过程中的内在冲突，提出了两种解决方案：GH 和 GH++。GH 通过将任务间的梯度角度从钝角调整为锐角，从而协调两者的权衡；GH++ 进一步优化为直角角度，不仅消除冲突，还最小化了对原始梯度方向的偏差。这些方法被转化为一个动态加权的损失函数，便于整合到大多数现有 UDA 模型中。实验结果和理论分析显示，GH/GH++ 显著提升了流行 UDA 基线和最先进模型的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE TPAMI 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00288v1",
      "published_date": "2024-08-01 05:22:41 UTC",
      "updated_date": "2024-08-01 05:22:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:18:26.962026"
    },
    {
      "arxiv_id": "2408.00280v1",
      "title": "Towards Scalable GPU-Accelerated SNN Training via Temporal Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Yanchen Li",
        "Jiachun Li",
        "Kebin Sun",
        "Luziwei Leng",
        "Ran Cheng"
      ],
      "abstract": "Drawing on the intricate structures of the brain, Spiking Neural Networks\n(SNNs) emerge as a transformative development in artificial intelligence,\nclosely emulating the complex dynamics of biological neural networks. While\nSNNs show promising efficiency on specialized sparse-computational hardware,\ntheir practical training often relies on conventional GPUs. This reliance\nfrequently leads to extended computation times when contrasted with traditional\nArtificial Neural Networks (ANNs), presenting significant hurdles for advancing\nSNN research. To navigate this challenge, we present a novel temporal fusion\nmethod, specifically designed to expedite the propagation dynamics of SNNs on\nGPU platforms, which serves as an enhancement to the current significant\napproaches for handling deep learning tasks with SNNs. This method underwent\nthorough validation through extensive experiments in both authentic training\nscenarios and idealized conditions, confirming its efficacy and adaptability\nfor single and multi-GPU systems. Benchmarked against various existing SNN\nlibraries/implementations, our method achieved accelerations ranging from\n$5\\times$ to $40\\times$ on NVIDIA A100 GPUs. Publicly available experimental\ncodes can be found at https://github.com/EMI-Group/snn-temporal-fusion.",
      "tldr_zh": "本文针对Spiking Neural Networks (SNNs)在GPU平台上训练效率低的问题，提出了一种新型temporal fusion方法，以加速SNNs的传播动态，并作为现有SNNs处理深度学习任务的增强方案。该方法通过广泛实验在真实和理想条件下验证其有效性，支持单GPU和多GPU系统，并在NVIDIA A100 GPU上比现有SNN库实现了5倍到40倍的加速。总体而言，此创新有助于推动SNNs研究的可扩展性，并已开源相关代码。",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "International Conference on Artificial Neural Networks (ICANN) 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00280v1",
      "published_date": "2024-08-01 04:41:56 UTC",
      "updated_date": "2024-08-01 04:41:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:18:36.755489"
    },
    {
      "arxiv_id": "2408.00278v1",
      "title": "High Performance Im2win and Direct Convolutions using Three Tensor Layouts on SIMD Architectures",
      "title_zh": "翻译失败",
      "authors": [
        "Xiang Fu",
        "Xinpeng Zhang",
        "Jixiang Ma",
        "Peng Zhao",
        "Shuai Lu",
        "Xu T. Liu"
      ],
      "abstract": "Convolution is the core component within deep neural networks and it is\ncomputationally intensive and time consuming. Tensor data layouts significantly\nimpact convolution operations in terms of memory access and computational\nefficiency. Yet, there is still a lack of comprehensive performance\ncharacterization on data layouts on SIMD architectures concerning convolution\nmethods. This paper proposes three novel data layouts for im2win convolution:\nNHWC, CHWN, and CHWN8, and introduces a set of general optimization techniques\nfor both direct and im2win convolutions. We compare the optimized im2win\nconvolution with the direct convolution and PyTorch's im2col-based convolution\nacross the aforementioned layouts on SIMD machines. The experiments\ndemonstrated that the im2win convolution with the new NHWC layout achieved up\nto 355% performance speedup over NCHW layout. Our optimizations also\nsignificantly improve the performance of both im2win and direct convolutions.\nOur optimized im2win and direct convolutions achieved up to 95% and 94% of\nmachine's theoretical peak performance, respectively.",
      "tldr_zh": "本论文探讨了在SIMD架构下，使用三种新张量数据布局（NHWC、CHWN和CHWN8）来优化Im2win和Direct Convolutions，以提升深度神经网络中计算密集型卷积操作的性能。研究引入了一套通用优化技术，并将优化后的Im2win卷积与Direct卷积以及PyTorch的im2col-based卷积进行比较。实验结果显示，新NHWC布局使Im2win卷积比传统NCHW布局性能提升高达355%，优化后Im2win和Direct卷积分别达到硬件理论峰值性能的95%和94%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00278v1",
      "published_date": "2024-08-01 04:37:03 UTC",
      "updated_date": "2024-08-01 04:37:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:18:50.208473"
    },
    {
      "arxiv_id": "2408.00274v1",
      "title": "QUITO: Accelerating Long-Context Reasoning through Query-Guided Context Compression",
      "title_zh": "QUITO：通过查询引导的上下文压缩加速长上下文推理",
      "authors": [
        "Wenshan Wang",
        "Yihang Wang",
        "Yixing Fan",
        "Huaming Liao",
        "Jiafeng Guo"
      ],
      "abstract": "In-context learning (ICL) capabilities are foundational to the success of\nlarge language models (LLMs). Recently, context compression has attracted\ngrowing interest since it can largely reduce reasoning complexities and\ncomputation costs of LLMs. In this paper, we introduce a novel Query-gUIded\naTtention cOmpression (QUITO) method, which leverages attention of the question\nover the contexts to filter useless information. Specifically, we take a\ntrigger token to calculate the attention distribution of the context in\nresponse to the question. Based on the distribution, we propose three different\nfiltering methods to satisfy the budget constraints of the context length. We\nevaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and\nASQA. Experimental results demonstrate that QUITO significantly outperforms\nestablished baselines across various datasets and downstream LLMs, underscoring\nits effectiveness. Our code is available at\nhttps://github.com/Wenshansilvia/attention_compressor.",
      "tldr_zh": "本研究提出了一种名为 QUITO 的查询引导注意力压缩方法，用于加速大语言模型 (LLMs) 的长上下文推理，从而减少推理复杂性和计算成本。QUITO 通过计算问题对上下文的注意力分布，利用一个触发 token 进行过滤，并基于此提出三种方法来满足上下文长度预算，确保保留关键信息。实验在 NaturalQuestions 和 ASQA 数据集上进行，结果显示 QUITO 在各种数据集和下游 LLMs 上显著优于基线方法，证明了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00274v1",
      "published_date": "2024-08-01 04:28:38 UTC",
      "updated_date": "2024-08-01 04:28:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:19:00.936334"
    },
    {
      "arxiv_id": "2408.00264v1",
      "title": "Clover-2: Accurate Inference for Regressive Lightweight Speculative Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Bin Xiao",
        "Lujun Gui",
        "Lei Su",
        "Weipeng Chen"
      ],
      "abstract": "Large Language Models (LLMs) frequently suffer from inefficiencies, largely\nattributable to the discord between the requirements of auto-regressive\ndecoding and the architecture of contemporary GPUs. Recently, regressive\nlightweight speculative decoding has garnered attention for its notable\nefficiency improvements in text generation tasks. This approach utilizes a\nlightweight regressive draft model, like a Recurrent Neural Network (RNN) or a\nsingle transformer decoder layer, leveraging sequential information to\niteratively predict potential tokens. Specifically, RNN draft models are\ncomputationally economical but tend to deliver lower accuracy, while attention\ndecoder layer models exhibit the opposite traits. This paper presents Clover-2,\nan advanced iteration of Clover, an RNN-based draft model designed to achieve\ncomparable accuracy to that of attention decoder layer models while maintaining\nminimal computational overhead. Clover-2 enhances the model architecture and\nincorporates knowledge distillation to increase Clover's accuracy and improve\noverall efficiency. We conducted experiments using the open-source Vicuna 7B\nand LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses\nexisting methods across various model architectures, showcasing its efficacy\nand robustness.",
      "tldr_zh": "本文提出 Clover-2，一种基于 RNN 的回归轻量级推测解码框架，旨在解决 Large Language Models (LLMs) 在自回归解码中因 GPU 架构不匹配导致的效率问题。Clover-2 通过模型架构优化和 knowledge distillation 技术，提升了准确率，使其与注意力解码器层模型相当，同时保持最小计算开销。实验在 Vicuna 7B 和 LLaMA3-Instruct 8B 模型上表明，Clover-2 在各种架构中超越现有方法，展示了其高效性和鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00264v1",
      "published_date": "2024-08-01 03:43:32 UTC",
      "updated_date": "2024-08-01 03:43:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:19:13.915847"
    },
    {
      "arxiv_id": "2408.00257v1",
      "title": "RoCo:Robust Collaborative Perception By Iterative Object Matching and Pose Adjustment",
      "title_zh": "翻译失败",
      "authors": [
        "Zhe Huang",
        "Shuo Wang",
        "Yongcai Wang",
        "Wanting Li",
        "Deying Li",
        "Lei Wang"
      ],
      "abstract": "Collaborative autonomous driving with multiple vehicles usually requires the\ndata fusion from multiple modalities. To ensure effective fusion, the data from\neach individual modality shall maintain a reasonably high quality. However, in\ncollaborative perception, the quality of object detection based on a modality\nis highly sensitive to the relative pose errors among the agents. It leads to\nfeature misalignment and significantly reduces collaborative performance. To\naddress this issue, we propose RoCo, a novel unsupervised framework to conduct\niterative object matching and agent pose adjustment. To the best of our\nknowledge, our work is the first to model the pose correction problem in\ncollaborative perception as an object matching task, which reliably associates\ncommon objects detected by different agents. On top of this, we propose a graph\noptimization process to adjust the agent poses by minimizing the alignment\nerrors of the associated objects, and the object matching is re-done based on\nthe adjusted agent poses. This process is carried out iteratively until\nconvergence. Experimental study on both simulated and real-world datasets\ndemonstrates that the proposed framework RoCo consistently outperforms existing\nrelevant methods in terms of the collaborative object detection performance,\nand exhibits highly desired robustness when the pose information of agents is\nwith high-level noise. Ablation studies are also provided to show the impact of\nits key parameters and components. The code is released at\nhttps://github.com/HuangZhe885/RoCo.",
      "tldr_zh": "该论文提出 RoCo，一种鲁棒的协作感知框架，针对多车辆自动驾驶中多模态数据融合问题，解决相对位姿错误导致的对象检测失准和性能下降问题。RoCo 通过将位姿校正建模为对象匹配任务，结合迭代对象匹配和基于图优化的位姿调整过程，来最小化关联对象的对齐错误，并重复迭代直至收敛。该框架在模拟和真实数据集上的实验显示，RoCo 显著提升了协作对象检测性能，尤其在高噪声位姿信息下比现有方法表现更优，并通过消融研究验证了关键组件的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ACM MM2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00257v1",
      "published_date": "2024-08-01 03:29:33 UTC",
      "updated_date": "2024-08-01 03:29:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:19:25.496023"
    },
    {
      "arxiv_id": "2408.00241v2",
      "title": "Multiple Greedy Quasi-Newton Methods for Saddle Point Problems",
      "title_zh": "多种贪婪拟牛顿方法用于鞍点问题",
      "authors": [
        "Minheng Xiao",
        "Shi Bo",
        "Zhizhong Wu"
      ],
      "abstract": "This paper introduces the Multiple Greedy Quasi-Newton (MGSR1-SP) method, a\nnovel approach to solving strongly-convex-strongly-concave (SCSC) saddle point\nproblems. Our method enhances the approximation of the squared indefinite\nHessian matrix inherent in these problems, significantly improving both\nstability and efficiency through iterative greedy updates. We provide a\nthorough theoretical analysis of MGSR1-SP, demonstrating its linear-quadratic\nconvergence rate. Numerical experiments conducted on AUC maximization and\nadversarial debiasing problems, compared with state-of-the-art algorithms,\nunderscore our method's enhanced convergence rate. These results affirm the\npotential of MGSR1-SP to improve performance across a broad spectrum of machine\nlearning applications where efficient and accurate Hessian approximations are\ncrucial.",
      "tldr_zh": "本论文提出了一种新的Multiple Greedy Quasi-Newton (MGSR1-SP)方法，用于解决strongly-convex-strongly-concave (SCSC) saddle point problems，通过迭代greedy updates来增强squared indefinite Hessian matrix的近似，提高算法的稳定性和效率。理论分析表明，该方法具有linear-quadratic convergence rate。数值实验在AUC maximization和adversarial debiasing问题上显示，MGSR1-SP比现有算法具有更快的收敛率，从而为需要高效Hessian近似的机器学习应用提供潜在性能提升。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to DOCS 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00241v2",
      "published_date": "2024-08-01 02:40:37 UTC",
      "updated_date": "2025-01-28 02:49:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:19:38.127892"
    },
    {
      "arxiv_id": "2408.00230v2",
      "title": "Lost in Translation: Latent Concept Misalignment in Text-to-Image Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Juntu Zhao",
        "Junyu Deng",
        "Yixin Ye",
        "Chongxuan Li",
        "Zhijie Deng",
        "Dequan Wang"
      ],
      "abstract": "Advancements in text-to-image diffusion models have broadened extensive\ndownstream practical applications, but such models often encounter misalignment\nissues between text and image. Taking the generation of a combination of two\ndisentangled concepts as an example, say given the prompt \"a tea cup of iced\ncoke\", existing models usually generate a glass cup of iced coke because the\niced coke usually co-occurs with the glass cup instead of the tea one during\nmodel training. The root of such misalignment is attributed to the confusion in\nthe latent semantic space of text-to-image diffusion models, and hence we refer\nto the \"a tea cup of iced coke\" phenomenon as Latent Concept Misalignment\n(LC-Mis). We leverage large language models (LLMs) to thoroughly investigate\nthe scope of LC-Mis, and develop an automated pipeline for aligning the latent\nsemantics of diffusion models to text prompts. Empirical assessments confirm\nthe effectiveness of our approach, substantially reducing LC-Mis errors and\nenhancing the robustness and versatility of text-to-image diffusion models. The\ncode and dataset are here: https://github.com/RossoneriZhao/iced_coke.",
      "tldr_zh": "文本到图像扩散模型(Text-to-Image Diffusion Models)常出现潜在概念不对齐(Latent Concept Misalignment, LC-Mis)问题，例如给定提示“a tea cup of iced coke”时，模型因训练数据中的共现关系而生成玻璃杯版本。研究者利用大型语言模型(LLMs)对LC-Mis的范围进行全面调查，并开发了一个自动管道来对齐模型的潜在语义空间与文本提示。实验评估证明，该方法显著降低了LC-Mis错误，提高了模型的鲁棒性和多功能性，并公开了代码和数据集以促进进一步研究。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by the 18th European Conference on Computer Vision ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.00230v2",
      "published_date": "2024-08-01 01:54:17 UTC",
      "updated_date": "2024-08-05 08:36:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:19:50.387231"
    },
    {
      "arxiv_id": "2408.00210v1",
      "title": "A Prior Embedding-Driven Architecture for Long Distance Blind Iris Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Xiong",
        "Xinman Zhang",
        "Jun Shen"
      ],
      "abstract": "Blind iris images, which result from unknown degradation during the process\nof iris recognition at long distances, often lead to decreased iris recognition\nrates. Currently, little existing literature offers a solution to this problem.\nIn response, we propose a prior embedding-driven architecture for long distance\nblind iris recognition. We first proposed a blind iris image restoration\nnetwork called Iris-PPRGAN. To effectively restore the texture of the blind\niris, Iris-PPRGAN includes a Generative Adversarial Network (GAN) used as a\nPrior Decoder, and a DNN used as the encoder. To extract iris features more\nefficiently, we then proposed a robust iris classifier by modifying the\nbottleneck module of InsightFace, which called Insight-Iris. A low-quality\nblind iris image is first restored by Iris-PPRGAN, then the restored iris image\nundergoes recognition via Insight-Iris. Experimental results on the public\nCASIA-Iris-distance dataset demonstrate that our proposed method significantly\nsuperior results to state-of-the-art blind iris restoration methods both\nquantitatively and qualitatively, Specifically, the recognition rate for\nlong-distance blind iris images reaches 90% after processing with our methods,\nrepresenting an improvement of approximately ten percentage points compared to\nimages without restoration.",
      "tldr_zh": "该研究针对长距离盲虹膜识别（Blind Iris Recognition）中图像退化导致识别率下降的问题，提出了一种先验嵌入驱动的架构。核心方法包括开发Iris-PPRGAN网络，该网络利用Generative Adversarial Network (GAN)作为Prior Decoder和DNN作为encoder来恢复盲虹膜图像的纹理；随后，通过修改InsightFace的瓶颈模块创建了Insight-Iris分类器，以更高效地提取虹膜特征。实验在CASIA-Iris-distance数据集上显示，该方法在定量和定性上优于现有技术，识别率从未恢复图像提高约10个百分点，达到90%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00210v1",
      "published_date": "2024-08-01 00:40:17 UTC",
      "updated_date": "2024-08-01 00:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:20:01.978779"
    },
    {
      "arxiv_id": "2408.00203v1",
      "title": "OmniParser for Pure Vision Based GUI Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Yadong Lu",
        "Jianwei Yang",
        "Yelong Shen",
        "Ahmed Awadallah"
      ],
      "abstract": "The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.",
      "tldr_zh": "该论文提出 OmniParser，一种纯视觉基于的 GUI 代理解析方法，旨在解决大视语言模型（如 GPT-4V）在用户界面交互中的局限性，包括可靠识别可交互图标和准确理解元素语义。OmniParser 通过构建 interactable icon detection 数据集和 icon description 数据集，并微调检测模型（识别屏幕可交互区域）和 caption 模型（提取元素功能语义），将用户界面截图解析为结构化元素。实验结果显示，OmniParser 显著提升了 GPT-4V 在 ScreenSpot 基准上的性能，并在 Mind2Web 和 AITW 基准上，仅凭截图输入就超过了需要额外信息的基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00203v1",
      "published_date": "2024-08-01 00:00:43 UTC",
      "updated_date": "2024-08-01 00:00:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T12:20:13.884261"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 98,
  "processed_papers_count": 98,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T12:20:38.284831"
}
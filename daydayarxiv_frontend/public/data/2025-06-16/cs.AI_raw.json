[
  {
    "arxiv_id": "2506.21581v1",
    "title": "Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains",
    "authors": [
      "Sarthak Chaturvedi",
      "Anurag Acharya",
      "Rounak Meyur",
      "Koby Hayashi",
      "Sai Munikoti",
      "Sameera Horawalavithana"
    ],
    "abstract": "Evaluation benchmark characteristics may distort the true benefits of domain adaptation in retrieval models. This creates misleading assessments that influence deployment decisions in specialized domains. We show that two benchmarks with drastically different features such as topic diversity, boundary overlap, and semantic complexity can influence the perceived benefits of fine-tuning. Using environmental regulatory document retrieval as a case study, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS) from federal agencies. We evaluate these models across two benchmarks with different semantic structures. Our findings reveal that identical domain adaptation approaches show very different perceived benefits depending on evaluation methodology. On one benchmark, with clearly separated topic boundaries, domain adaptation shows small improvements (maximum 0.61% NDCG gain). However, on the other benchmark with overlapping semantic structures, the same models demonstrate large improvements (up to 2.22% NDCG gain), a 3.6-fold difference in the performance benefit. We compare these benchmarks through topic diversity metrics, finding that the higher-performing benchmark shows 11% higher average cosine distances between contexts and 23% lower silhouette scores, directly contributing to the observed performance difference. These results demonstrate that benchmark selection strongly determines assessments of retrieval system effectiveness in specialized domains. Evaluation frameworks with well-separated topics regularly underestimate domain adaptation benefits, while those with overlapping semantic boundaries reveal improvements that better reflect real-world regulatory document complexity. Our findings have important implications for developing and deploying AI systems for interdisciplinary domains that integrate multiple topics.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21581v1",
    "published_date": "2025-06-16 23:54:08 UTC",
    "updated_date": "2025-06-16 23:54:08 UTC"
  },
  {
    "arxiv_id": "2506.14054v3",
    "title": "Scientifically-Interpretable Reasoning Network (ScIReN): Discovering Hidden Relationships in the Carbon Cycle and Beyond",
    "authors": [
      "Joshua Fan",
      "Haodi Xu",
      "Feng Tao",
      "Md Nasim",
      "Marc Grimson",
      "Yiqi Luo",
      "Carla P. Gomes"
    ],
    "abstract": "Understanding how carbon flows through the soil is crucial for mitigating the effects of climate change. While soils have potential to sequester carbon from the atmosphere, the soil carbon cycle remains poorly understood. Scientists have developed mathematical process-based models of the soil carbon cycle based on existing knowledge, but they contain numerous unknown parameters that must be set in an ad-hoc manner, and often fit observations poorly. On the other hand, neural networks can learn patterns from data, but do not respect known scientific laws, nor can they reveal novel scientific relationships due to their black-box nature. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN leverages Kolmogorov-Arnold networks (KAN) to ensure the encoder is fully interpretable and reveals relationships between input features and latent parameters; it uses novel smoothness penalties to balance expressivity and simplicity. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge. While the process-based decoder enforces established scientific knowledge, the KAN-based encoder reveals new scientific relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.14054v3",
    "published_date": "2025-06-16 23:21:37 UTC",
    "updated_date": "2025-08-29 16:14:18 UTC"
  },
  {
    "arxiv_id": "2507.21081v1",
    "title": "Empathy in Explanation",
    "authors": [
      "Katherine M. Collins",
      "Kartik Chandra",
      "Adrian Weller",
      "Jonathan Ragan-Kelley",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "Why do we give the explanations we do? Recent work has suggested that we should think of explanation as a kind of cooperative social interaction, between a why-question-asker and an explainer. Here, we apply this perspective to consider the role that emotion plays in this social interaction. We develop a computational framework for modeling explainers who consider the emotional impact an explanation might have on a listener. We test our framework by using it to model human intuitions about how a doctor might explain to a patient why they have a disease, taking into account the patient's propensity for regret. Our model predicts human intuitions well, better than emotion-agnostic ablations, suggesting that people do indeed reason about emotion when giving explanations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "CogSci non-archival conference paper",
    "pdf_url": "https://arxiv.org/pdf/2507.21081v1",
    "published_date": "2025-06-16 22:48:58 UTC",
    "updated_date": "2025-06-16 22:48:58 UTC"
  },
  {
    "arxiv_id": "2506.14046v1",
    "title": "Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications",
    "authors": [
      "David Kogan",
      "Max Schumacher",
      "Sam Nguyen",
      "Masanori Suzuki",
      "Melissa Smith",
      "Chloe Sophia Bellows",
      "Jared Bernstein"
    ],
    "abstract": "There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14046v1",
    "published_date": "2025-06-16 22:40:16 UTC",
    "updated_date": "2025-06-16 22:40:16 UTC"
  },
  {
    "arxiv_id": "2506.14045v1",
    "title": "Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning",
    "authors": [
      "Martin Klissarov",
      "Akhil Bagaria",
      "Ziyan Luo",
      "George Konidaris",
      "Doina Precup",
      "Marlos C. Machado"
    ],
    "abstract": "Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14045v1",
    "published_date": "2025-06-16 22:36:32 UTC",
    "updated_date": "2025-06-16 22:36:32 UTC"
  },
  {
    "arxiv_id": "2506.14042v1",
    "title": "Asymptotically Smaller Encodings for Graph Problems and Scheduling",
    "authors": [
      "Bernardo Subercaseaux"
    ],
    "abstract": "We show how several graph problems (e.g., vertex-cover, independent-set, $k$-coloring) can be encoded into CNF using only $O(|V|^2 / \\lg |V|)$ many clauses, as opposed to the $Ω(|V|^2)$ constraints used by standard encodings. This somewhat surprising result is a simple consequence of a result of Erdős, Chung, and Spencer (1983) about biclique coverings of graphs, and opens theoretical avenues to understand the success of \"Bounded Variable Addition'' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally, we show a novel encoding for independent sets in some dense interval graphs using only $O(|V| \\lg |V|)$ clauses (the direct encoding uses $Ω(|V|^2)$), which we have successfully applied to a string-compression encoding posed by Bannai et al. (2022). As a direct byproduct, we obtain a reduction in the encoding size of a scheduling problem posed by Mayank and Modal (2020) from $O(NMT^2)$ to $O(NMT + M T^2 \\lg T)$, where $N$ is the number of tasks, $T$ the total timespan, and $M$ the number of machines.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14042v1",
    "published_date": "2025-06-16 22:31:41 UTC",
    "updated_date": "2025-06-16 22:31:41 UTC"
  },
  {
    "arxiv_id": "2506.14035v1",
    "title": "SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement",
    "authors": [
      "Chelsi Jain",
      "Yiran Wu",
      "Yifan Zeng",
      "Jiale Liu",
      "S hengyu Dai",
      "Zhenwen Shao",
      "Qingyun Wu",
      "Huazheng Wang"
    ],
    "abstract": "Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14035v1",
    "published_date": "2025-06-16 22:15:58 UTC",
    "updated_date": "2025-06-16 22:15:58 UTC"
  },
  {
    "arxiv_id": "2506.17292v2",
    "title": "Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models",
    "authors": [
      "Quan Nguyen",
      "Minh N. Vu",
      "Truc Nguyen",
      "My T. Thai"
    ],
    "abstract": "Federated Learning enables collaborative learning among clients via a coordinating server while avoiding direct data sharing, offering a perceived solution to preserve privacy. However, recent studies on Membership Inference Attacks (MIAs) have challenged this notion, showing high success rates against unprotected training data. While local differential privacy (LDP) is widely regarded as a gold standard for privacy protection in data analysis, most studies on MIAs either neglect LDP or fail to provide theoretical guarantees for attack success rates against LDP-protected data. To address this gap, we derive theoretical lower bounds for the success rates of low-polynomial time MIAs that exploit vulnerabilities in fully connected or self-attention layers. We establish that even when data are protected by LDP, privacy risks persist, depending on the privacy budget. Practical evaluations on federated vision models confirm considerable privacy risks, revealing that the noise required to mitigate these attacks significantly degrades models' utility.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.17292v2",
    "published_date": "2025-06-16 21:48:11 UTC",
    "updated_date": "2025-08-01 03:56:30 UTC"
  },
  {
    "arxiv_id": "2506.14020v3",
    "title": "Bures-Wasserstein Flow Matching for Graph Generation",
    "authors": [
      "Keyue Jiang",
      "Jiahao Cui",
      "Xiaowen Dong",
      "Laura Toni"
    ],
    "abstract": "Graph generation has emerged as a critical task in fields ranging from drug discovery to circuit design. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between reference and data distributions. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path. This disentangled interpolation breaks the interconnected patterns of graphs, making the constructed probability path irregular and non-smooth, which causes poor training dynamics and faulty sampling convergence. To address the limitation, this paper first presents a theoretically grounded framework for probability path construction in graph generative models. Specifically, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design a smooth probability path that ensures the co-evolution of graph components. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that utilizes the derived optimal probability path to benefit the training and sampling algorithm design. Experimental evaluations in plain graph generation and molecule generation validate the effectiveness of BWFlow with competitive performance, better training convergence, and efficient sampling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14020v3",
    "published_date": "2025-06-16 21:36:56 UTC",
    "updated_date": "2025-10-10 15:13:41 UTC"
  },
  {
    "arxiv_id": "2506.21580v1",
    "title": "From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models",
    "authors": [
      "Dana Alsagheer",
      "Yang Lu",
      "Abdulrahman Kamal",
      "Omar Kamal",
      "Mohammad Kamal",
      "Nada Mansour",
      "Cosmo Yang Wu",
      "Rambiba Karanjai",
      "Sen Li",
      "Weidong Shi"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains. However, effective decision-making relies heavily on strong reasoning abilities. Reasoning is the foundation for decision-making, providing the analytical and logical framework to make sound choices. Reasoning involves analyzing information, drawing inferences, and reaching conclusions based on logic or evidence. Decision-making builds on this foundation by applying the insights from reasoning to select the best course of action among alternatives. Together, these processes create a continuous cycle of thought and action aimed at achieving goals effectively. As AI technology evolves, there is a growing trend to train LLMs to excel in general reasoning. This study explores how the general reasoning capabilities of LLMs connect to their performance in domain-specific reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21580v1",
    "published_date": "2025-06-16 21:20:08 UTC",
    "updated_date": "2025-06-16 21:20:08 UTC"
  },
  {
    "arxiv_id": "2506.14002v1",
    "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders",
    "authors": [
      "Siyu Chen",
      "Heejune Sheen",
      "Xuyuan Xiong",
      "Tianhao Wang",
      "Zhuoran Yang"
    ],
    "abstract": "We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \\highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \\highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "136 pages, 21 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.14002v1",
    "published_date": "2025-06-16 20:58:05 UTC",
    "updated_date": "2025-06-16 20:58:05 UTC"
  },
  {
    "arxiv_id": "2506.22457v1",
    "title": "A Complex UNet Approach for Non-Invasive Fetal ECG Extraction Using Single-Channel Dry Textile Electrodes",
    "authors": [
      "Iulia Orvas",
      "Andrei Radu",
      "Alessandra Galli",
      "Ana Neacsu",
      "Elisabetta Peri"
    ],
    "abstract": "Continuous, non-invasive pregnancy monitoring is crucial for minimising potential complications. The fetal electrocardiogram (fECG) represents a promising tool for assessing fetal health beyond clinical environments. Home-based monitoring necessitates the use of a minimal number of comfortable and durable electrodes, such as dry textile electrodes. However, this setup presents many challenges, including increased noise and motion artefacts, which complicate the accurate extraction of fECG signals. To overcome these challenges, we introduce a pioneering method for extracting fECG from single-channel recordings obtained using dry textile electrodes using AI techniques. We created a new dataset by simulating abdominal recordings, including noise closely resembling real-world characteristics of in-vivo recordings through dry textile electrodes, alongside mECG and fECG. To ensure the reliability of the extracted fECG, we propose an innovative pipeline based on a complex-valued denoising network, Complex UNet. Unlike previous approaches that focused solely on signal magnitude, our method processes both real and imaginary components of the spectrogram, addressing phase information and preventing incongruous predictions. We evaluated our novel pipeline against traditional, well-established approaches, on both simulated and real data in terms of fECG extraction and R-peak detection. The results showcase that our suggested method achieves new state-of-the-art results, enabling an accurate extraction of fECG morphology across all evaluated settings. This method is the first to effectively extract fECG signals from single-channel recordings using dry textile electrodes, making a significant advancement towards a fully non-invasive and self-administered fECG extraction solution.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22457v1",
    "published_date": "2025-06-16 20:35:52 UTC",
    "updated_date": "2025-06-16 20:35:52 UTC"
  },
  {
    "arxiv_id": "2506.13980v1",
    "title": "ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users",
    "authors": [
      "Shahaf David",
      "Yair Meidan",
      "Ido Hersko",
      "Daniel Varnovitzky",
      "Dudu Mimran",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "abstract": "Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13980v1",
    "published_date": "2025-06-16 20:33:44 UTC",
    "updated_date": "2025-06-16 20:33:44 UTC"
  },
  {
    "arxiv_id": "2506.18920v1",
    "title": "Signal Use and Emergent Cooperation",
    "authors": [
      "Michael Williams"
    ],
    "abstract": "In this work, we investigate how autonomous agents, organized into tribes, learn to use communication signals to coordinate their activities and enhance their collective efficiency. Using the NEC-DAC (Neurally Encoded Culture - Distributed Autonomous Communicators) system, where each agent is equipped with its own neural network for decision-making, we demonstrate how these agents develop a shared behavioral system -- akin to a culture -- through learning and signalling. Our research focuses on the self-organization of culture within these tribes of agents and how varying communication strategies impact their fitness and cooperation. By analyzing different social structures, such as authority hierarchies, we show that the culture of cooperation significantly influences the tribe's performance. Furthermore, we explore how signals not only facilitate the emergence of culture but also enable its transmission across generations of agents. Additionally, we examine the benefits of coordinating behavior and signaling within individual agents' neural networks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.NE",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "167 pages, 19 figures, PhD dissertation, UCLA, 2006",
    "pdf_url": "https://arxiv.org/pdf/2506.18920v1",
    "published_date": "2025-06-16 20:24:30 UTC",
    "updated_date": "2025-06-16 20:24:30 UTC"
  },
  {
    "arxiv_id": "2507.02873v1",
    "title": "Using Large Language Models to Study Mathematical Practice",
    "authors": [
      "William D'Alessandro"
    ],
    "abstract": "The philosophy of mathematical practice (PMP) looks to evidence from working mathematics to help settle philosophical questions. One prominent program under the PMP banner is the study of explanation in mathematics, which aims to understand what sorts of proofs mathematicians consider explanatory and what role the pursuit of explanation plays in mathematical practice. In an effort to address worries about cherry-picked examples and file-drawer problems in PMP, a handful of authors have recently turned to corpus analysis methods as a promising alternative to small-scale case studies. This paper reports the results from such a corpus study facilitated by Google's Gemini 2.5 Pro, a model whose reasoning capabilities, advances in hallucination control and large context window allow for the accurate analysis of hundreds of pages of text per query. Based on a sample of 5000 mathematics papers from arXiv.org, the experiments yielded a dataset of hundreds of useful annotated examples. Its aim was to gain insight on questions like the following: How often do mathematicians make claims about explanation in the relevant sense? Do mathematicians' explanatory practices vary in any noticeable way by subject matter? Which philosophical theories of explanation are most consistent with a large body of non-cherry-picked examples? How might philosophers make further use of AI tools to gain insights from large datasets of this kind? As the first PMP study making extensive use of LLM methods, it also seeks to begin a conversation about these methods as research tools in practice-oriented philosophy and to evaluate the strengths and weaknesses of current models for such work.",
    "categories": [
      "math.HO",
      "cs.AI"
    ],
    "primary_category": "math.HO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.02873v1",
    "published_date": "2025-06-16 20:22:50 UTC",
    "updated_date": "2025-06-16 20:22:50 UTC"
  },
  {
    "arxiv_id": "2506.13961v1",
    "title": "Safe Domains of Attraction for Discrete-Time Nonlinear Systems: Characterization and Verifiable Neural Network Estimation",
    "authors": [
      "Mohamed Serry",
      "Haoyu Li",
      "Ruikun Zhou",
      "Huan Zhang",
      "Jun Liu"
    ],
    "abstract": "Analysis of nonlinear autonomous systems typically involves estimating domains of attraction, which have been a topic of extensive research interest for decades. Despite that, accurately estimating domains of attraction for nonlinear systems remains a challenging task, where existing methods are conservative or limited to low-dimensional systems. The estimation becomes even more challenging when accounting for state constraints. In this work, we propose a framework to accurately estimate safe (state-constrained) domains of attraction for discrete-time autonomous nonlinear systems. In establishing this framework, we first derive a new Zubov equation, whose solution corresponds to the exact safe domain of attraction. The solution to the aforementioned Zubov equation is shown to be unique and continuous over the whole state space. We then present a physics-informed approach to approximating the solution of the Zubov equation using neural networks. To obtain certifiable estimates of the domain of attraction from the neural network approximate solutions, we propose a verification framework that can be implemented using standard verification tools (e.g., $α,\\!β$-CROWN and dReal). To illustrate its effectiveness, we demonstrate our approach through numerical examples concerning nonlinear systems with state constraints.",
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13961v1",
    "published_date": "2025-06-16 20:09:51 UTC",
    "updated_date": "2025-06-16 20:09:51 UTC"
  },
  {
    "arxiv_id": "2506.13958v2",
    "title": "Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers",
    "authors": [
      "Leonardo Guiducci",
      "Antonio Rizzo",
      "Giovanna Maria Dimitri"
    ],
    "abstract": "Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for poster presentation at the NeurIPS 2025 workshop \"CogInterp: Interpreting Cognition in Deep Learning Models\", San Diego, CA, USA",
    "pdf_url": "https://arxiv.org/pdf/2506.13958v2",
    "published_date": "2025-06-16 20:01:24 UTC",
    "updated_date": "2025-11-17 11:33:08 UTC"
  },
  {
    "arxiv_id": "2506.13956v1",
    "title": "ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection",
    "authors": [
      "Shang-Chi Tsai",
      "Seiya Kawano",
      "Angel Garcia Contreras",
      "Koichiro Yoshino",
      "Yun-Nung Chen"
    ],
    "abstract": "When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "IWSDS 2024 Best Paper Award",
    "pdf_url": "https://arxiv.org/pdf/2506.13956v1",
    "published_date": "2025-06-16 19:58:54 UTC",
    "updated_date": "2025-06-16 19:58:54 UTC"
  },
  {
    "arxiv_id": "2506.13932v2",
    "title": "Code Reasoning for Software Engineering Tasks: A Survey and A Call to Action",
    "authors": [
      "Saurabh Pujar",
      "Ira Ceka",
      "Irene Manotas",
      "Gail Kaiser",
      "Baishakhi Ray",
      "Shyam Ramji"
    ],
    "abstract": "The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. Their performance on certain tasks can be further enhanced by incorporating test-time reasoning techniques. These inference-time advances have been adopted into the code domain, enabling complex software engineering (SWE) tasks such as code generation, test generation and issue resolution. However, the impact of different reasoning techniques on code-centric SWE tasks has not been systematically explored. In this work, we survey code reasoning techniques that underpin these capabilities, with a focus on test-time compute and inference-time reasoning paradigms. We examine a variety of code-specific reasoning methods and progressively build up to SWE agents, which combine planning, tool use, and multi-step interaction. We also compare the impact of different techniques on coding tasks, highlighting their relative importance and outlining open challenges and future research directions. Our contributions are: (1) to the best of our knowledge, the first dedicated survey of code reasoning for SWE tasks, highlighting overarching reasoning strategies, hybrid methods, and agentic approaches; (2) a taxonomy of inference-time techniques used to drive code reasoning, accompanied by a curated set of under-explored benchmarks with high potential for SWE evaluation; (3) a comparative analysis of reasoning design patterns across commonly used models and benchmarks; and (4) a synthesis of gaps in current methods and evaluation practices, identifying under-explored areas and concrete opportunities for future research.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13932v2",
    "published_date": "2025-06-16 19:18:09 UTC",
    "updated_date": "2026-01-11 14:31:25 UTC"
  },
  {
    "arxiv_id": "2506.13925v3",
    "title": "Segmenting Visuals With Querying Words: Language Anchors For Semi-Supervised Image Segmentation",
    "authors": [
      "Numair Nadeem",
      "Saeed Anwar",
      "Muhammad Hamza Asad",
      "Abdul Bais"
    ],
    "abstract": "Vision Language Models (VLMs) provide rich semantic priors but are underexplored in Semi supervised Semantic Segmentation. Recent attempts to integrate VLMs to inject high level semantics overlook the semantic misalignment between visual and textual representations that arises from using domain invariant text embeddings without adapting them to dataset and image specific contexts. This lack of domain awareness, coupled with limited annotations, weakens the model semantic understanding by preventing effective vision language alignment. As a result, the model struggles with contextual reasoning, shows weak intra class discrimination, and confuses similar classes. To address these challenges, we propose Hierarchical Vision Language transFormer (HVLFormer), which achieves domain aware and domain robust alignment between visual and textual representations within a mask transformer architecture. Firstly, we transform text embeddings from pretrained VLMs into textual object queries, enabling the generation of multi scale, dataset aware queries that capture class semantics from coarse to fine granularity and enhance contextual reasoning. Next, we refine these queries by injecting image specific visual context to align textual semantics with local scene structures and enhance class discrimination. Finally, to achieve domain robustness, we introduce cross view and modal consistency regularization, which enforces prediction consistency within mask-transformer architecture across augmented views. Moreover, it ensures stable vision language alignment during decoding. With less than 1% training data, HVLFormer outperforms state of the art methods on Pascal VOC, COCO, ADE20K, and Cityscapes. Our code and results will be available on GitHub.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13925v3",
    "published_date": "2025-06-16 19:05:33 UTC",
    "updated_date": "2025-12-13 20:17:24 UTC"
  },
  {
    "arxiv_id": "2506.13923v2",
    "title": "Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models",
    "authors": [
      "Vaskar Nath",
      "Elaine Lau",
      "Anisha Gunjal",
      "Manasi Sharma",
      "Nikhil Baharte",
      "Sean Hendryx"
    ],
    "abstract": "We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance in two main ways: (1) by compressing pass@$k$ into pass@1 and (2) via \"capability gain\" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B parameters on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\\text{Guide}$ -- a new class of online training algorithms. $\\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the \"off-policy\" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13923v2",
    "published_date": "2025-06-16 19:03:06 UTC",
    "updated_date": "2025-06-20 00:51:15 UTC"
  },
  {
    "arxiv_id": "2506.13920v1",
    "title": "Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction",
    "authors": [
      "Mbithe Nzomo",
      "Deshendran Moodley"
    ],
    "abstract": "Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been accepted for presentation at the 49th IEEE International Conference on Computers, Software, and Applications (COMPSAC 2025). The final published version will be available via IEEE Xplore",
    "pdf_url": "https://arxiv.org/pdf/2506.13920v1",
    "published_date": "2025-06-16 18:57:07 UTC",
    "updated_date": "2025-06-16 18:57:07 UTC"
  },
  {
    "arxiv_id": "2506.13917v1",
    "title": "Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features",
    "authors": [
      "Miguel A. Lago",
      "Ghada Zamzmi",
      "Brandon Eich",
      "Jana G. Delfino"
    ],
    "abstract": "Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13917v1",
    "published_date": "2025-06-16 18:51:46 UTC",
    "updated_date": "2025-06-16 18:51:46 UTC"
  },
  {
    "arxiv_id": "2506.13911v1",
    "title": "Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization",
    "authors": [
      "Arie Soeteman",
      "Balder ten Cate"
    ],
    "abstract": "We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to NeurIPS 2025, 28 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13911v1",
    "published_date": "2025-06-16 18:39:31 UTC",
    "updated_date": "2025-06-16 18:39:31 UTC"
  },
  {
    "arxiv_id": "2506.13910v1",
    "title": "Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation",
    "authors": [
      "Aritra Dutta",
      "Pushpita Boral",
      "G Suseela"
    ],
    "abstract": "The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13910v1",
    "published_date": "2025-06-16 18:39:16 UTC",
    "updated_date": "2025-06-16 18:39:16 UTC"
  },
  {
    "arxiv_id": "2507.00022v2",
    "title": "GLU Attention Improve Transformer",
    "authors": [
      "Zehao Wang"
    ],
    "abstract": "Gated Linear Units (GLU) have shown great potential in enhancing neural network performance. In this paper, I introduce a novel attention mechanism called GLU Attention, which introduces nonlinearity into the values of Attention. My experiments demonstrate that GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs. GLU Attention is lightweight and can seamlessly integrate with other technologies, such as Flash Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention (MHA) variants such as Grouped-Query Attention (GQA). This project is open-sourced at github.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.00022v2",
    "published_date": "2025-06-16 18:38:56 UTC",
    "updated_date": "2025-07-06 05:43:48 UTC"
  },
  {
    "arxiv_id": "2506.13904v1",
    "title": "A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare",
    "authors": [
      "Ivania Donoso-Guzmán",
      "Kristýna Sirka Kacafírková",
      "Maxwell Szymanski",
      "An Jacobs",
      "Denis Parra",
      "Katrien Verbert"
    ],
    "abstract": "Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.\n  This study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.\n  We conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.\n  The review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13904v1",
    "published_date": "2025-06-16 18:30:00 UTC",
    "updated_date": "2025-06-16 18:30:00 UTC"
  },
  {
    "arxiv_id": "2506.13903v1",
    "title": "Enhancing interpretability of rule-based classifiers through feature graphs",
    "authors": [
      "Christel Sirocchi",
      "Damiano Verda"
    ],
    "abstract": "In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: https://github.com/ChristelSirocchi/rule-graph.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13903v1",
    "published_date": "2025-06-16 18:29:37 UTC",
    "updated_date": "2025-06-16 18:29:37 UTC"
  },
  {
    "arxiv_id": "2506.13901v1",
    "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations",
    "authors": [
      "Abhilekh Borah",
      "Chhavi Sharma",
      "Danush Khanna",
      "Utkarsh Bhatt",
      "Gurpreet Singh",
      "Hasnat Md Abdullah",
      "Raghav Kaushik Ravi",
      "Vinija Jain",
      "Jyoti Patel",
      "Shubham Singh",
      "Vasu Sharma",
      "Arpita Vats",
      "Rahul Raja",
      "Aman Chadha",
      "Amitava Das"
    ],
    "abstract": "Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13901v1",
    "published_date": "2025-06-16 18:22:28 UTC",
    "updated_date": "2025-06-16 18:22:28 UTC"
  },
  {
    "arxiv_id": "2506.13900v1",
    "title": "Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models",
    "authors": [
      "Marouane Il Idrissi",
      "Agathe Fernandes Machado",
      "Arthur Charpentier"
    ],
    "abstract": "Cooperative game theory has become a cornerstone of post-hoc interpretability in machine learning, largely through the use of Shapley values. Yet, despite their widespread adoption, Shapley-based methods often rest on axiomatic justifications whose relevance to feature attribution remains debatable. In this paper, we revisit cooperative game theory from an interpretability perspective and argue for a broader and more principled use of its tools. We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values and offer richer interpretative flexibility. We present an accessible overview of these allocation schemes, clarify the distinction between value functions and aggregation rules, and introduce a three-step blueprint for constructing reliable and theoretically-grounded feature attributions. Our goal is to move beyond fixed axioms and provide the XAI community with a coherent framework to design attribution methods that are both meaningful and robust to shifting methodological trends.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13900v1",
    "published_date": "2025-06-16 18:22:23 UTC",
    "updated_date": "2025-06-16 18:22:23 UTC"
  },
  {
    "arxiv_id": "2506.13892v1",
    "title": "Scaling Algorithm Distillation for Continuous Control with Mamba",
    "authors": [
      "Samuel Beaussant",
      "Mehdi Mounsif"
    ],
    "abstract": "Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13892v1",
    "published_date": "2025-06-16 18:15:02 UTC",
    "updated_date": "2025-06-16 18:15:02 UTC"
  },
  {
    "arxiv_id": "2506.13886v2",
    "title": "Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles",
    "authors": [
      "Antara Raaghavi Bhattacharya",
      "Isabel Papadimitriou",
      "Kathryn Davidson",
      "David Alvarez-Melis"
    ],
    "abstract": "Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\\times$, etc., as in \"twenty + three\"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2506.13886v2",
    "published_date": "2025-06-16 18:09:38 UTC",
    "updated_date": "2025-10-15 09:03:13 UTC"
  },
  {
    "arxiv_id": "2506.13862v1",
    "title": "StaQ it! Growing neural networks for Policy Mirror Descent",
    "authors": [
      "Alena Shilova",
      "Alex Davey",
      "Brahim Driss",
      "Riad Akrour"
    ],
    "abstract": "In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "44 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13862v1",
    "published_date": "2025-06-16 18:00:01 UTC",
    "updated_date": "2025-06-16 18:00:01 UTC"
  },
  {
    "arxiv_id": "2506.13763v1",
    "title": "Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value",
    "authors": [
      "Yixian Xu",
      "Shengjie Luo",
      "Liwei Wang",
      "Di He",
      "Chang Liu"
    ],
    "abstract": "Diffusion models have achieved remarkable success in generative modeling. Despite more stable training, the loss of diffusion models is not indicative of absolute data-fitting quality, since its optimal value is typically not zero but unknown, leading to confusion between large optimal loss and insufficient model capacity. In this work, we advocate the need to estimate the optimal loss value for diagnosing and improving diffusion models. We first derive the optimal loss in closed form under a unified formulation of diffusion models, and develop effective estimators for it, including a stochastic variant scalable to large datasets with proper control of variance and bias. With this tool, we unlock the inherent metric for diagnosing the training quality of mainstream diffusion model variants, and develop a more performant training schedule based on the optimal loss. Moreover, using models with 120M to 1.5B parameters, we find that the power law is better demonstrated after subtracting the optimal loss from the actual training loss, suggesting a more principled setting for investigating the scaling law for diffusion models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 8 figures, 3 tables. Preprint. Work in Progress",
    "pdf_url": "https://arxiv.org/pdf/2506.13763v1",
    "published_date": "2025-06-16 17:59:54 UTC",
    "updated_date": "2025-06-16 17:59:54 UTC"
  },
  {
    "arxiv_id": "2506.13846v2",
    "title": "Fake it till You Make it: Reward Modeling as Discriminative Prediction",
    "authors": [
      "Runtao Liu",
      "Jiahao Zhan",
      "Yingqing He",
      "Chen Wei",
      "Alan Yuille",
      "Qifeng Chen"
    ],
    "abstract": "An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Code and data will be released at https://github.com/Visualignment/GAN-RM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13846v2",
    "published_date": "2025-06-16 17:59:40 UTC",
    "updated_date": "2025-06-26 16:39:32 UTC"
  },
  {
    "arxiv_id": "2506.13759v5",
    "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
    "authors": [
      "Runpeng Yu",
      "Qi Li",
      "Xinchao Wang"
    ],
    "abstract": "In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output control, and dynamic perception. These capabilities are previously difficult to achieve with AR models. A growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10$\\times$ acceleration in inference speed. These developments position discrete diffusion models as a promising alternative to intelligence based on the traditional autoregressive approach. In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, list commonly-used modeling methods, and categorize representative models. We further analyze key techniques for training, inference, quantization. We also discuss the trustworthy issues and summarize emerging applications across language, vision-language, and biological domains and etc.. We conclude by discussing future directions for research and deployment. Relative papers are collected in https://github.com/LiQiiiii/Awesome-Discrete-Diffusion-LLM_MLLM",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13759v5",
    "published_date": "2025-06-16 17:59:08 UTC",
    "updated_date": "2025-09-19 07:18:31 UTC"
  },
  {
    "arxiv_id": "2506.13754v2",
    "title": "VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models",
    "authors": [
      "Edward Li",
      "Zichen Wang",
      "Jiahe Huang",
      "Jeong Joon Park"
    ],
    "abstract": "We present a unified framework for solving partial differential equations (PDEs) using video-inpainting diffusion transformer models. Unlike existing methods that devise specialized strategies for either forward or inverse problems under full or partial observation, our approach unifies these tasks under a single, flexible generative framework. Specifically, we recast PDE-solving as a generalized inpainting problem, e.g., treating forward prediction as inferring missing spatiotemporal information of future states from initial conditions. To this end, we design a transformer-based architecture that conditions on arbitrary patterns of known data to infer missing values across time and space. Our method proposes pixel-space video diffusion models for fine-grained, high-fidelity inpainting and conditioning, while enhancing computational efficiency through hierarchical modeling. Extensive experiments show that our video inpainting-based diffusion model offers an accurate and versatile solution across a wide range of PDEs and problem setups, outperforming state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Project page: https://videopde.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2506.13754v2",
    "published_date": "2025-06-16 17:58:00 UTC",
    "updated_date": "2025-06-17 02:15:17 UTC"
  },
  {
    "arxiv_id": "2506.13752v1",
    "title": "Steering LLM Thinking with Budget Guidance",
    "authors": [
      "Junyan Li",
      "Wenshuo Zhao",
      "Yang Zhang",
      "Chuang Gan"
    ],
    "abstract": "Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13752v1",
    "published_date": "2025-06-16 17:57:05 UTC",
    "updated_date": "2025-06-16 17:57:05 UTC"
  },
  {
    "arxiv_id": "2506.13751v3",
    "title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction",
    "authors": [
      "Haoru Xue",
      "Xiaoyu Huang",
      "Dantong Niu",
      "Qiayuan Liao",
      "Thomas Kragerud",
      "Jan Tommy Gravdahl",
      "Xue Bin Peng",
      "Guanya Shi",
      "Trevor Darrell",
      "Koushil Sreenath",
      "Shankar Sastry"
    ],
    "abstract": "Vision-language-action (VLA) models have demonstrated strong semantic understanding and zero-shot generalization, yet most existing systems assume an accurate low-level controller with hand-crafted action \"vocabulary\" such as end-effector pose or root velocity. This assumption confines prior work to quasi-static tasks and precludes the agile, whole-body behaviors required by humanoid whole-body control (WBC) tasks. To capture this gap in the literature, we start by introducing the first sim-to-real-ready, vision-language, closed-loop benchmark for humanoid WBC, comprising over 150 tasks from 10 categories. We then propose LeVERB: Latent Vision-Language-Encoded Robot Behavior, a hierarchical latent instruction-following framework for humanoid vision-language WBC, the first of its kind. At the top level, a vision-language policy learns a latent action vocabulary from synthetically rendered kinematic demonstrations; at the low level, a reinforcement-learned WBC policy consumes these latent verbs to generate dynamics-level commands. In our benchmark, LeVERB can zero-shot attain a 80% success rate on simple visual navigation tasks, and 58.5% success rate overall, outperforming naive hierarchical whole-body VLA implementation by 7.8 times.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "https://ember-lab-berkeley.github.io/LeVERB-Website/",
    "pdf_url": "https://arxiv.org/pdf/2506.13751v3",
    "published_date": "2025-06-16 17:56:53 UTC",
    "updated_date": "2025-09-25 02:11:15 UTC"
  },
  {
    "arxiv_id": "2506.13845v1",
    "title": "Students' Reliance on AI in Higher Education: Identifying Contributing Factors",
    "authors": [
      "Griffin Pitts",
      "Neha Rani",
      "Weedguet Mildort",
      "Eva-Marie Cook"
    ],
    "abstract": "The increasing availability and use of artificial intelligence (AI) tools in educational settings has raised concerns about students' overreliance on these technologies. Overreliance occurs when individuals accept incorrect AI-generated recommendations, often without critical evaluation, leading to flawed problem solutions and undermining learning outcomes. This study investigates potential factors contributing to patterns of AI reliance among undergraduate students, examining not only overreliance but also appropriate reliance (correctly accepting helpful and rejecting harmful recommendations) and underreliance (incorrectly rejecting helpful recommendations). Our approach combined pre- and post-surveys with a controlled experimental task where participants solved programming problems with an AI assistant that provided both accurate and deliberately incorrect suggestions, allowing direct observation of students' reliance patterns when faced with varying AI reliability. We find that appropriate reliance is significantly related to students' programming self-efficacy, programming literacy, and need for cognition, while showing negative correlations with post-task trust and satisfaction. Overreliance showed significant correlations with post-task trust and satisfaction with the AI assistant. Underreliance was negatively correlated with programming literacy, programming self-efficacy, and need for cognition. Overall, the findings provide insights for developing targeted interventions that promote appropriate reliance on AI tools, with implications for the integration of AI in curriculum and educational technologies.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13845v1",
    "published_date": "2025-06-16 17:55:26 UTC",
    "updated_date": "2025-06-16 17:55:26 UTC"
  },
  {
    "arxiv_id": "2506.13746v1",
    "title": "Evaluating Large Language Models for Phishing Detection, Self-Consistency, Faithfulness, and Explainability",
    "authors": [
      "Shova Kuikel",
      "Aritran Piplai",
      "Palvi Aggarwal"
    ],
    "abstract": "Phishing attacks remain one of the most prevalent and persistent cybersecurity threat with attackers continuously evolving and intensifying tactics to evade the general detection system. Despite significant advances in artificial intelligence and machine learning, faithfully reproducing the interpretable reasoning with classification and explainability that underpin phishing judgments remains challenging. Due to recent advancement in Natural Language Processing, Large Language Models (LLMs) show a promising direction and potential for improving domain specific phishing classification tasks. However, enhancing the reliability and robustness of classification models requires not only accurate predictions from LLMs but also consistent and trustworthy explanations aligning with those predictions. Therefore, a key question remains: can LLMs not only classify phishing emails accurately but also generate explanations that are reliably aligned with their predictions and internally self-consistent? To answer these questions, we have fine-tuned transformer based models, including BERT, Llama models, and Wizard, to improve domain relevance and make them more tailored to phishing specific distinctions, using Binary Sequence Classification, Contrastive Learning (CL) and Direct Preference Optimization (DPO). To that end, we examined their performance in phishing classification and explainability by applying the ConsistenCy measure based on SHAPley values (CC SHAP), which measures prediction explanation token alignment to test the model's internal faithfulness and consistency and uncover the rationale behind its predictions and reasoning. Overall, our findings show that Llama models exhibit stronger prediction explanation token alignment with higher CC SHAP scores despite lacking reliable decision making accuracy, whereas Wizard achieves better prediction accuracy but lower CC SHAP scores.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13746v1",
    "published_date": "2025-06-16 17:54:28 UTC",
    "updated_date": "2025-06-16 17:54:28 UTC"
  },
  {
    "arxiv_id": "2506.13741v1",
    "title": "PB$^2$: Preference Space Exploration via Population-Based Methods in Preference-Based Reinforcement Learning",
    "authors": [
      "Brahim Driss",
      "Alex Davey",
      "Riad Akrour"
    ],
    "abstract": "Preference-based reinforcement learning (PbRL) has emerged as a promising approach for learning behaviors from human feedback without predefined reward functions. However, current PbRL methods face a critical challenge in effectively exploring the preference space, often converging prematurely to suboptimal policies that satisfy only a narrow subset of human preferences. In this work, we identify and address this preference exploration problem through population-based methods. We demonstrate that maintaining a diverse population of agents enables more comprehensive exploration of the preference landscape compared to single-agent approaches. Crucially, this diversity improves reward model learning by generating preference queries with clearly distinguishable behaviors, a key factor in real-world scenarios where humans must easily differentiate between options to provide meaningful feedback. Our experiments reveal that current methods may fail by getting stuck in local optima, requiring excessive feedback, or degrading significantly when human evaluators make errors on similar trajectories, a realistic scenario often overlooked by methods relying on perfect oracle teachers. Our population-based approach demonstrates robust performance when teachers mislabel similar trajectory segments and shows significantly enhanced preference exploration capabilities,particularly in environments with complex reward landscapes.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13741v1",
    "published_date": "2025-06-16 17:51:33 UTC",
    "updated_date": "2025-06-16 17:51:33 UTC"
  },
  {
    "arxiv_id": "2506.13734v2",
    "title": "Instruction Following by Boosting Attention of Large Language Models",
    "authors": [
      "Vitoria Guardieiro",
      "Adam Stein",
      "Avishree Khare",
      "Eric Wong"
    ],
    "abstract": "Controlling the generation of large language models (LLMs) remains a central challenge to ensure their safe and reliable deployment. While prompt engineering and finetuning are common approaches, recent work has explored latent steering, a lightweight technique that alters LLM internal activations to guide generation. However, subsequent studies revealed latent steering's effectiveness to be limited, often underperforming simple instruction prompting. To address this limitation, we first establish a benchmark across diverse behaviors for standardized evaluation of steering techniques. Building on insights from this benchmark, we introduce Instruction Attention Boosting (InstABoost), a latent steering method that boosts the strength of instruction prompting by altering the model's attention during generation. InstABoost combines the strengths of existing approaches and is theoretically supported by prior work that suggests that in-context rule following in transformer-based models can be controlled by manipulating attention on instructions. Empirically, InstABoost demonstrates superior control success compared to both traditional prompting and latent steering.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13734v2",
    "published_date": "2025-06-16 17:42:35 UTC",
    "updated_date": "2025-07-08 17:48:59 UTC"
  },
  {
    "arxiv_id": "2506.13730v1",
    "title": "BanditWare: A Contextual Bandit-based Framework for Hardware Prediction",
    "authors": [
      "Tainã Coleman",
      "Hena Ahmed",
      "Ravi Shende",
      "Ismael Perez",
      "Ïlkay Altintaş"
    ],
    "abstract": "Distributed computing systems are essential for meeting the demands of modern applications, yet transitioning from single-system to distributed environments presents significant challenges. Misallocating resources in shared systems can lead to resource contention, system instability, degraded performance, priority inversion, inefficient utilization, increased latency, and environmental impact.\n  We present BanditWare, an online recommendation system that dynamically selects the most suitable hardware for applications using a contextual multi-armed bandit algorithm. BanditWare balances exploration and exploitation, gradually refining its hardware recommendations based on observed application performance while continuing to explore potentially better options. Unlike traditional statistical and machine learning approaches that rely heavily on large historical datasets, BanditWare operates online, learning and adapting in real-time as new workloads arrive.\n  We evaluated BanditWare on three workflow applications: Cycles (an agricultural science scientific workflow) BurnPro3D (a web-based platform for fire science) and a matrix multiplication application. Designed for seamless integration with the National Data Platform (NDP), BanditWare enables users of all experience levels to optimize resource allocation efficiently.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13730v1",
    "published_date": "2025-06-16 17:40:34 UTC",
    "updated_date": "2025-06-16 17:40:34 UTC"
  },
  {
    "arxiv_id": "2506.13727v1",
    "title": "Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs",
    "authors": [
      "Sayed Mohammad Vakilzadeh Hatefi",
      "Maximilian Dreyer",
      "Reduan Achtibat",
      "Patrick Kahardipraja",
      "Thomas Wiegand",
      "Wojciech Samek",
      "Sebastian Lapuschkin"
    ],
    "abstract": "Large Language Models (LLMs) are central to many contemporary AI applications, yet their extensive parameter counts pose significant challenges for deployment in memory- and compute-constrained environments. Recent works in eXplainable AI (XAI), particularly on attribution methods, suggest that interpretability can also enable model compression by identifying and removing components irrelevant to inference. In this paper, we leverage Layer-wise Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs. While LRP has shown promise in structured pruning for vision models, we extend it to unstructured pruning in LLMs and demonstrate that it can substantially reduce model size with minimal performance loss. Our method is especially effective in extracting task-relevant subgraphs -- so-called ``circuits'' -- which can represent core functions (e.g., indirect object identification). Building on this, we introduce a technique for model correction, by selectively removing circuits responsible for spurious behaviors (e.g., toxic outputs). All in all, we gather these techniques as a uniform holistic framework and showcase its effectiveness and limitations through extensive experiments for compression, circuit discovery and model correction on Llama and OPT models, highlighting its potential for improving both model efficiency and safety. Our code is publicly available at https://github.com/erfanhatefi/SparC3.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Work in progress (10 pages manuscript, 3 pages references, 12 pages appendix)",
    "pdf_url": "https://arxiv.org/pdf/2506.13727v1",
    "published_date": "2025-06-16 17:38:36 UTC",
    "updated_date": "2025-06-16 17:38:36 UTC"
  },
  {
    "arxiv_id": "2506.13726v1",
    "title": "Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models",
    "authors": [
      "Arjun Krishna",
      "Aaditya Rastogi",
      "Erick Galinkin"
    ],
    "abstract": "The introduction of advanced reasoning capabilities have improved the problem-solving performance of large language models, particularly on math and coding benchmarks. However, it remains unclear whether these reasoning models are more or less vulnerable to adversarial prompt attacks than their non-reasoning counterparts. In this work, we present a systematic evaluation of weaknesses in advanced reasoning models compared to similar non-reasoning models across a diverse set of prompt-based attack categories. Using experimental data, we find that on average the reasoning-augmented models are \\emph{slightly more robust} than non-reasoning models (42.51\\% vs 45.53\\% attack success rate, lower is better). However, this overall trend masks significant category-specific differences: for certain attack types the reasoning models are substantially \\emph{more vulnerable} (e.g., up to 32 percentage points worse on a tree-of-attacks prompt), while for others they are markedly \\emph{more robust} (e.g., 29.8 points better on cross-site scripting injection). Our findings highlight the nuanced security implications of advanced reasoning in language models and emphasize the importance of stress-testing safety across diverse adversarial techniques.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to LLMSEC 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.13726v1",
    "published_date": "2025-06-16 17:32:18 UTC",
    "updated_date": "2025-06-16 17:32:18 UTC"
  },
  {
    "arxiv_id": "2506.13717v2",
    "title": "Contrastive Self-Supervised Learning As Neural Manifold Packing",
    "authors": [
      "Guanming Zhang",
      "David J. Heeger",
      "Stefano Martiniani"
    ],
    "abstract": "Contrastive self-supervised learning based on point-wise comparisons has been widely studied for vision tasks. In the visual cortex of the brain, neuronal responses to distinct stimulus classes are organized into geometric structures known as neural manifolds. Accurate classification of stimuli can be achieved by effectively separating these manifolds, akin to solving a packing problem. We introduce Contrastive Learning As Manifold Packing (CLAMP), a self-supervised framework that recasts representation learning as a manifold packing problem. CLAMP introduces a loss function inspired by the potential energy of short-range repulsive particle systems, such as those encountered in the physics of simple liquids and jammed packings. In this framework, each class consists of sub-manifolds embedding multiple augmented views of a single image. The sizes and positions of the sub-manifolds are dynamically optimized by following the gradient of a packing loss. This approach yields interpretable dynamics in the embedding space that parallel jamming physics, and introduces geometrically meaningful hyperparameters within the loss function. Under the standard linear evaluation protocol, which freezes the backbone and trains only a linear classifier, CLAMP achieves competitive performance with state-of-the-art self-supervised models. Furthermore, our analysis reveals that neural manifolds corresponding to different categories emerge naturally and are effectively separated in the learned representation space, highlighting the potential of CLAMP to bridge insights from physics, neural science, and machine learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13717v2",
    "published_date": "2025-06-16 17:24:31 UTC",
    "updated_date": "2026-01-03 08:30:22 UTC"
  },
  {
    "arxiv_id": "2506.13705v1",
    "title": "TimeMaster: Training Time-Series Multimodal LLMs to Reason via Reinforcement Learning",
    "authors": [
      "Junru Zhang",
      "Lang Feng",
      "Xu Guo",
      "Yuhan Wu",
      "Yabo Dong",
      "Duanqing Xu"
    ],
    "abstract": "Time-series reasoning remains a significant challenge in multimodal large language models (MLLMs) due to the dynamic temporal patterns, ambiguous semantics, and lack of temporal priors. In this work, we introduce TimeMaster, a reinforcement learning (RL)-based method that enables time-series MLLMs to perform structured, interpretable reasoning directly over visualized time-series inputs and task prompts. TimeMaster adopts a three-part structured output format, reasoning, classification, and domain-specific extension, and is optimized via a composite reward function that aligns format adherence, prediction accuracy, and open-ended insight quality. The model is trained using a two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish a good initialization, followed by Group Relative Policy Optimization (GRPO) at the token level to enable stable and targeted reward-driven improvement in time-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across six real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster achieves state-of-the-art performance, outperforming both classical time-series models and few-shot GPT-4o by over 14.6% and 7.3% performance gain, respectively. Notably, TimeMaster goes beyond time-series classification: it also exhibits expert-like reasoning behavior, generates context-aware explanations, and delivers domain-aligned insights. Our results highlight that reward-driven RL can be a scalable and promising path toward integrating temporal understanding into time-series MLLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2506.13705v1",
    "published_date": "2025-06-16 17:12:26 UTC",
    "updated_date": "2025-06-16 17:12:26 UTC"
  },
  {
    "arxiv_id": "2506.13702v3",
    "title": "Value-Free Policy Optimization via Reward Partitioning",
    "authors": [
      "Bilal Faye",
      "Hanane Azzag",
      "Mustapha Lebbah"
    ],
    "abstract": "Single-trajectory reinforcement learning (RL) methods aim to optimize policies from datasets consisting of (prompt, response, reward) triplets, where scalar rewards are directly available. This supervision format is highly practical, as it mirrors real-world human feedback, such as thumbs-up/down signals, and avoids the need for structured preference annotations. In contrast, pairwise preference-based methods like Direct Preference Optimization (DPO) rely on datasets with both preferred and dispreferred responses, which are harder to construct and less natural to collect. Among single-trajectory approaches, Direct Reward Optimization (DRO) has shown strong empirical performance due to its simplicity and stability. However, DRO requires approximating a value function, which introduces several limitations: high off-policy variance, coupling between policy and value learning, and a lack of absolute supervision on the policy itself. We introduce Reward Partitioning Optimization (RPO), a new method that resolves these limitations by removing the need to model the value function. Instead, RPO normalizes observed rewards using a partitioning approach estimated directly from data. This leads to a straightforward supervised learning objective on the policy, with no auxiliary models and no joint optimization. RPO provides direct and stable supervision on the policy, making it robust and easy to implement in practice. We validate RPO on scalar-feedback language modeling tasks using Flan-T5 encoder-decoder models. Our results demonstrate that RPO outperforms existing single-trajectory baselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings confirm that RPO is a simple, effective, and theoretically grounded method for single-trajectory policy optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13702v3",
    "published_date": "2025-06-16 17:06:27 UTC",
    "updated_date": "2025-12-21 11:30:37 UTC"
  },
  {
    "arxiv_id": "2506.13692v1",
    "title": "Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems",
    "authors": [
      "Shang-Chi Tsai",
      "Yun-Nung Chen"
    ],
    "abstract": "With the advancement of large language models, many dialogue systems are now capable of providing reasonable and informative responses to patients' medical conditions. However, when patients consult their doctor, they may experience negative emotions due to the severity and urgency of their situation. If the model can provide appropriate comfort and empathy based on the patient's negative emotions while answering medical questions, it will likely offer a more reassuring experience during the medical consultation process. To address this issue, our paper explores the balance between knowledge sharing and emotional support in the healthcare dialogue process. We utilize a large language model to rewrite a real-world interactive medical dialogue dataset, generating patient queries with negative emotions and corresponding medical responses aimed at soothing the patient's emotions while addressing their concerns. The modified data serves to refine the latest large language models with various fine-tuning methods, enabling them to accurately provide sentences with both emotional reassurance and constructive suggestions in response to patients' questions. Compared to the original LLM model, our experimental results demonstrate that our methodology significantly enhances the model's ability to generate emotional responses while maintaining its original capability to provide accurate knowledge-based answers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "IWSDS 2025 Oral Paper",
    "pdf_url": "https://arxiv.org/pdf/2506.13692v1",
    "published_date": "2025-06-16 16:54:03 UTC",
    "updated_date": "2025-06-16 16:54:03 UTC"
  },
  {
    "arxiv_id": "2506.13690v1",
    "title": "Meta-learning how to Share Credit among Macro-Actions",
    "authors": [
      "Ionel-Alexandru Hosu",
      "Traian Rebedea",
      "Razvan Pascanu"
    ],
    "abstract": "One proposed mechanism to improve exploration in reinforcement learning is through the use of macro-actions. Paradoxically though, in many scenarios the naive addition of macro-actions does not lead to better exploration, but rather the opposite. It has been argued that this was caused by adding non-useful macros and multiple works have focused on mechanisms to discover effectively environment-specific useful macros. In this work, we take a slightly different perspective. We argue that the difficulty stems from the trade-offs between reducing the average number of decisions per episode versus increasing the size of the action space. Namely, one typically treats each potential macro-action as independent and atomic, hence strictly increasing the search space and making typical exploration strategies inefficient. To address this problem we propose a novel regularization term that exploits the relationship between actions and macro-actions to improve the credit assignment mechanism by reducing the effective dimension of the action space and, therefore, improving exploration. The term relies on a similarity matrix that is meta-learned jointly with learning the desired policy. We empirically validate our strategy looking at macro-actions in Atari games, and the StreetFighter II environment. Our results show significant improvements over the Rainbow-DQN baseline in all environments. Additionally, we show that the macro-action similarity is transferable to related environments. We believe this work is a small but important step towards understanding how the similarity-imposed geometry on the action space can be exploited to improve credit assignment and exploration, therefore making learning more effective.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13690v1",
    "published_date": "2025-06-16 16:52:49 UTC",
    "updated_date": "2025-06-16 16:52:49 UTC"
  },
  {
    "arxiv_id": "2506.13679v1",
    "title": "ROSA: Harnessing Robot States for Vision-Language and Action Alignment",
    "authors": [
      "Yuqing Wen",
      "Kefan Gu",
      "Haoxuan Liu",
      "Yucheng Zhao",
      "Tiancai Wang",
      "Haoqiang Fan",
      "Xiaoyan Sun"
    ],
    "abstract": "Vision-Language-Action (VLA) models have recently made significant advance in multi-task, end-to-end robotic control, due to the strong generalization capabilities of Vision-Language Models (VLMs). A fundamental challenge in developing such models is effectively aligning the vision-language space with the robotic action space. Existing approaches typically rely on directly fine-tuning VLMs using expert demonstrations. However, this strategy suffers from a spatio-temporal gap, resulting in considerable data inefficiency and heavy reliance on human labor. Spatially, VLMs operate within a high-level semantic space, whereas robotic actions are grounded in low-level 3D physical space; temporally, VLMs primarily interpret the present, while VLA models anticipate future actions. To overcome these challenges, we propose a novel training paradigm, ROSA, which leverages robot state estimation to improve alignment between vision-language and action spaces. By integrating robot state estimation data obtained via an automated process, ROSA enables the VLA model to gain enhanced spatial understanding and self-awareness, thereby boosting performance and generalization. Extensive experiments in both simulated and real-world environments demonstrate the effectiveness of ROSA, particularly in low-data regimes.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13679v1",
    "published_date": "2025-06-16 16:34:20 UTC",
    "updated_date": "2025-06-16 16:34:20 UTC"
  },
  {
    "arxiv_id": "2506.13674v2",
    "title": "Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention",
    "authors": [
      "Haonan Wang",
      "Brian Chen",
      "Siquan Li",
      "Xinhe Liang",
      "Hwee Kuan Lee",
      "Kenji Kawaguchi",
      "Tianyang Hu"
    ],
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for rapidly adapting large language models (LLMs) to downstream tasks. Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability to achieve performance comparable to full fine-tuning with significantly reduced computational and memory overhead. However, despite its earlier success, its effectiveness in training modern state-of-the-art LLMs has been very limited. In this work, we demonstrate empirically that Prefix-Tuning underperforms on LLMs because of an inherent tradeoff between input and prefix significance within the attention head. This motivates us to introduce Prefix-Tuning+, a novel architecture that generalizes the principles of Prefix-Tuning while addressing its shortcomings by shifting the prefix module out of the attention head itself. We further provide an overview of our construction process to guide future users when constructing their own context-based methods. Our experiments show that, across a diverse set of benchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning methods. Notably, it achieves performance on par with the widely adopted LoRA method on several general benchmarks, highlighting the potential modern extension of Prefix-Tuning approaches. Our findings suggest that by overcoming its inherent limitations, Prefix-Tuning can remain a competitive and relevant research direction in the landscape of parameter-efficient LLM adaptation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13674v2",
    "published_date": "2025-06-16 16:30:26 UTC",
    "updated_date": "2025-06-17 15:25:25 UTC"
  },
  {
    "arxiv_id": "2507.01024v1",
    "title": "Hello Afrika: Speech Commands in Kinyarwanda",
    "authors": [
      "George Igwegbe",
      "Martins Awojide",
      "Mboh Bless",
      "Nirel Kadzo"
    ],
    "abstract": "Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a language which are essential for non-contact control of and activation of larger AI systems in devices used in everyday life especially for persons with disabilities. Currently, there is a dearth of speech command models for African languages. The Hello Afrika project aims to address this issue and its first iteration is focused on the Kinyarwanda language since the country has shown interest in developing speech recognition technologies culminating in one of the largest datasets on Mozilla Common Voice. The model was built off a custom speech command corpus made up of general directives, numbers, and a wake word. The final model was deployed on multiple devices (PC, Mobile Phone and Edge Devices) and the performance was assessed using suitable metrics.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Data Science Africa, 2024",
    "pdf_url": "https://arxiv.org/pdf/2507.01024v1",
    "published_date": "2025-06-16 16:30:19 UTC",
    "updated_date": "2025-06-16 16:30:19 UTC"
  },
  {
    "arxiv_id": "2506.13666v1",
    "title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered Agent Systems",
    "authors": [
      "Junfeng Fang",
      "Zijun Yao",
      "Ruipeng Wang",
      "Haokai Ma",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "abstract": "The development of large language models (LLMs) has entered in a experience-driven era, flagged by the emergence of environment feedback-driven learning via reinforcement learning and tool-using agents. This encourages the emergenece of model context protocol (MCP), which defines the standard on how should a LLM interact with external services, such as \\api and data. However, as MCP becomes the de facto standard for LLM agent systems, it also introduces new safety risks. In particular, MCP introduces third-party services, which are not controlled by the LLM developers, into the agent systems. These third-party MCP services provider are potentially malicious and have the economic incentives to exploit vulnerabilities and sabotage user-agent interactions. In this position paper, we advocate the research community in LLM safety to pay close attention to the new safety risks issues introduced by MCP, and develop new techniques to build safe MCP-powered agent systems. To establish our position, we argue with three key parts. (1) We first construct \\framework, a controlled framework to examine safety issues in MCP-powered agent systems. (2) We then conduct a series of pilot experiments to demonstrate the safety risks in MCP-powered agent systems is a real threat and its defense is not trivial. (3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered agent systems. In particular, we would call for researchers to persue the following research directions: red teaming, MCP safe LLM development, MCP safety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP safe ecosystem construction. We hope this position paper can raise the awareness of the research community in MCP safety and encourage more researchers to join this important research direction. Our code is available at https://github.com/littlelittlenine/SafeMCP.git.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13666v1",
    "published_date": "2025-06-16 16:24:31 UTC",
    "updated_date": "2025-06-16 16:24:31 UTC"
  },
  {
    "arxiv_id": "2506.13841v2",
    "title": "LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning",
    "authors": [
      "Miho Koda",
      "Yu Zheng",
      "Ruixian Ma",
      "Mingyang Sun",
      "Devesh Pansare",
      "Fabio Duarte",
      "Paolo Santi"
    ],
    "abstract": "Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation, leaving open the question of whether such reasoning skills generalize to complex real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistic constraints. The benchmark covers carefully crafted queries of varying difficulty levels and is supported by a sandbox environment with in-house tools for constraint-based location search. Automated verification further guarantees the scalability of the benchmark, enabling the addition of arbitrary number of queries. Extensive evaluations on real-world site selection data from Boston, New York, and Tampa reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13841v2",
    "published_date": "2025-06-16 16:23:56 UTC",
    "updated_date": "2025-09-26 00:39:37 UTC"
  },
  {
    "arxiv_id": "2506.13654v1",
    "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning",
    "authors": [
      "Shulin Tian",
      "Ruiqi Wang",
      "Hongming Guo",
      "Penghao Wu",
      "Yuhao Dong",
      "Xiuying Wang",
      "Jingkang Yang",
      "Hao Zhang",
      "Hongyuan Zhu",
      "Ziwei Liu"
    ],
    "abstract": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://egolife-ai.github.io/Ego-R1/",
    "pdf_url": "https://arxiv.org/pdf/2506.13654v1",
    "published_date": "2025-06-16 16:17:08 UTC",
    "updated_date": "2025-06-16 16:17:08 UTC"
  },
  {
    "arxiv_id": "2506.13642v2",
    "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model",
    "authors": [
      "Shaolei Zhang",
      "Shoutao Guo",
      "Qingkai Fang",
      "Yan Zhou",
      "Yang Feng"
    ],
    "abstract": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "Code: https://github.com/ictnlp/Stream-Omni , Model: https://huggingface.co/ICTNLP/stream-omni-8b",
    "pdf_url": "https://arxiv.org/pdf/2506.13642v2",
    "published_date": "2025-06-16 16:06:45 UTC",
    "updated_date": "2025-06-22 07:56:58 UTC"
  },
  {
    "arxiv_id": "2506.13638v2",
    "title": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models",
    "authors": [
      "Zhiyi Shi",
      "Binjie Wang",
      "Chongjie Si",
      "Yichen Wu",
      "Junsik Kim",
      "Hanspeter Pfister"
    ],
    "abstract": "Model editing aims to efficiently update a pre-trained model's knowledge without the need for time-consuming full retraining. While existing pioneering editing methods achieve promising results, they primarily focus on editing single-modal language models (LLMs). However, for vision-language models (VLMs), which involve multiple modalities, the role and impact of each modality on editing performance remain largely unexplored. To address this gap, we explore the impact of textual and visual modalities on model editing and find that: (1) textual and visual representations reach peak sensitivity at different layers, reflecting their varying importance; and (2) editing both modalities can efficiently update knowledge, but this comes at the cost of compromising the model's original capabilities. Based on our findings, we propose DualEdit, an editor that modifies both textual and visual modalities at their respective key layers. Additionally, we introduce a gating module within the more sensitive textual modality, allowing DualEdit to efficiently update new knowledge while preserving the model's original information. We evaluate DualEdit across multiple VLM backbones and benchmark datasets, demonstrating its superiority over state-of-the-art VLM editing baselines as well as adapted LLM editing methods on different evaluation metrics. Codes are available at https://github.com/zhiyiscs/DualEdit",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "COLM 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.13638v2",
    "published_date": "2025-06-16 16:04:16 UTC",
    "updated_date": "2025-09-18 21:24:31 UTC"
  },
  {
    "arxiv_id": "2506.13628v2",
    "title": "Graph-Convolutional-Beta-VAE for Synthetic Abdominal Aorta Aneurysm Generation",
    "authors": [
      "Francesco Fabbri",
      "Martino Andrea Scarpolini",
      "Angelo Iollo",
      "Francesco Viola",
      "Francesco Tudisco"
    ],
    "abstract": "Synthetic data generation plays a crucial role in medical research by mitigating privacy concerns and enabling large-scale patient data analysis. This study presents a beta-Variational Autoencoder Graph Convolutional Neural Network framework for generating synthetic Abdominal Aorta Aneurysms (AAA). Using a small real-world dataset, our approach extracts key anatomical features and captures complex statistical relationships within a compact disentangled latent space. To address data limitations, low-impact data augmentation based on Procrustes analysis was employed, preserving anatomical integrity. The generation strategies, both deterministic and stochastic, manage to enhance data diversity while ensuring realism. Compared to PCA-based approaches, our model performs more robustly on unseen data by capturing complex, nonlinear anatomical variations. This enables more comprehensive clinical and statistical analyses than the original dataset alone. The resulting synthetic AAA dataset preserves patient privacy while providing a scalable foundation for medical research, device testing, and computational modeling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.TO"
    ],
    "primary_category": "cs.LG",
    "comment": "Typo in the title",
    "pdf_url": "https://arxiv.org/pdf/2506.13628v2",
    "published_date": "2025-06-16 15:55:56 UTC",
    "updated_date": "2025-06-17 04:36:55 UTC"
  },
  {
    "arxiv_id": "2506.13612v1",
    "title": "EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning",
    "authors": [
      "Zhiqiang Li",
      "Haiyong Bao",
      "Menghong Guan",
      "Hao Pan",
      "Cheng Huang",
      "Hong-Ning Dai"
    ],
    "abstract": "Despite federated learning (FL)'s potential in collaborative learning, its performance has deteriorated due to the data heterogeneity of distributed users. Recently, clustered federated learning (CFL) has emerged to address this challenge by partitioning users into clusters according to their similarity. However, CFL faces difficulties in training when users are unwilling to share their cluster identities due to privacy concerns. To address these issues, we present an innovative Efficient and Robust Secure Aggregation scheme for CFL, dubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while maintaining users' cluster identity confidentially. Moreover, it detects potential poisonous attacks without compromising individual client gradients by discarding negatively correlated gradients and aggregating positively correlated ones using a weighted approach. The server also authenticates correct gradient encoding by clients. EBS-CFL has high efficiency with client-side overhead O(ml + m^2) for communication and O(m^2l) for computation, where m is the number of cluster identities, and l is the gradient size. When m = 1, EBS-CFL's computational efficiency of client is at least O(log n) times better than comparison schemes, where n is the number of clients.In addition, we validate the scheme through extensive experiments. Finally, we theoretically prove the scheme's security.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by AAAI 25",
    "pdf_url": "https://arxiv.org/pdf/2506.13612v1",
    "published_date": "2025-06-16 15:39:10 UTC",
    "updated_date": "2025-06-16 15:39:10 UTC"
  },
  {
    "arxiv_id": "2506.13611v3",
    "title": "A Hybrid Artificial Intelligence Method for Estimating Flicker in Power Systems",
    "authors": [
      "Javad Enayati",
      "Pedram Asef",
      "Alexandre Benoit"
    ],
    "abstract": "This paper introduces a novel hybrid AI method combining H filtering and an adaptive linear neuron network for flicker component estimation in power distribution systems.The proposed method leverages the robustness of the H filter to extract the voltage envelope under uncertain and noisy conditions followed by the use of ADALINE to accurately identify flicker frequencies embedded in the envelope.This synergy enables efficient time domain estimation with rapid convergence and noise resilience addressing key limitations of existing frequency domain approaches.Unlike conventional techniques this hybrid AI model handles complex power disturbances without prior knowledge of noise characteristics or extensive training.To validate the method performance we conduct simulation studies based on IEC Standard 61000 4 15 supported by statistical analysis Monte Carlo simulations and real world data.Results demonstrate superior accuracy robustness and reduced computational load compared to Fast Fourier Transform and Discrete Wavelet Transform based estimators.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "eess.SY",
    "comment": "31 pages, 12 figures, and 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.13611v3",
    "published_date": "2025-06-16 15:38:39 UTC",
    "updated_date": "2025-08-29 10:43:07 UTC"
  },
  {
    "arxiv_id": "2506.13609v1",
    "title": "Avoiding Obfuscation with Prover-Estimator Debate",
    "authors": [
      "Jonah Brown-Cohen",
      "Geoffrey Irving",
      "Georgios Piliouras"
    ],
    "abstract": "Training powerful AI systems to exhibit desired behaviors hinges on the ability to provide accurate human supervision on increasingly complex tasks. A promising approach to this problem is to amplify human judgement by leveraging the power of two competing AIs in a debate about the correct solution to a given problem. Prior theoretical work has provided a complexity-theoretic formalization of AI debate, and posed the problem of designing protocols for AI debate that guarantee the correctness of human judgements for as complex a class of problems as possible. Recursive debates, in which debaters decompose a complex problem into simpler subproblems, hold promise for growing the class of problems that can be accurately judged in a debate. However, existing protocols for recursive debate run into the obfuscated arguments problem: a dishonest debater can use a computationally efficient strategy that forces an honest opponent to solve a computationally intractable problem to win. We mitigate this problem with a new recursive debate protocol that, under certain stability assumptions, ensures that an honest debater can win with a strategy requiring computational efficiency comparable to their opponent.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.DS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13609v1",
    "published_date": "2025-06-16 15:37:33 UTC",
    "updated_date": "2025-06-16 15:37:33 UTC"
  },
  {
    "arxiv_id": "2506.13600v2",
    "title": "The ASP-based Nurse Scheduling System at the University of Yamanashi Hospital",
    "authors": [
      "Hidetomo Nabeshima",
      "Mutsunori Banbara",
      "Torsten Schaub",
      "Takehide Soh"
    ],
    "abstract": "We present the design principles of a nurse scheduling system built using Answer Set Programming (ASP) and successfully deployed at the University of Yamanashi Hospital. Nurse scheduling is a complex optimization problem requiring the reconciliation of individual nurse preferences with hospital staffing needs across various wards. This involves balancing hard and soft constraints and the flexibility of interactive adjustments. While extensively studied in academia, real-world nurse scheduling presents unique challenges that go beyond typical benchmark problems and competitions. This paper details the practical application of ASP to address these challenges at the University of Yamanashi Hospital, focusing on the insights gained and the advancements in ASP technology necessary to effectively manage the complexities of real-world deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings ICLP 2025, arXiv:2601.00047",
    "pdf_url": "https://arxiv.org/pdf/2506.13600v2",
    "published_date": "2025-06-16 15:25:06 UTC",
    "updated_date": "2026-01-07 12:06:46 UTC"
  },
  {
    "arxiv_id": "2506.13599v1",
    "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation",
    "authors": [
      "Yuwei Du",
      "Jie Feng",
      "Jian Yuan",
      "Yong Li"
    ],
    "abstract": "Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered \\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation (\\textbf{CAMS}), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. \\textbf{CAMS} comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that \\textbf{CAMS} achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, \\textbf{CAMS} generates more realistic and plausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13599v1",
    "published_date": "2025-06-16 15:24:07 UTC",
    "updated_date": "2025-06-16 15:24:07 UTC"
  },
  {
    "arxiv_id": "2506.13590v1",
    "title": "Agent Capability Negotiation and Binding Protocol (ACNBP)",
    "authors": [
      "Ken Huang",
      "Akram Sheriff",
      "Vineeth Sai Narajala",
      "Idan Habler"
    ],
    "abstract": "As multi-agent systems evolve to encompass increasingly diverse and specialized agents, the challenge of enabling effective collaboration between heterogeneous agents has become paramount, with traditional agent communication protocols often assuming homogeneous environments or predefined interaction patterns that limit their applicability in dynamic, open-world scenarios. This paper presents the Agent Capability Negotiation and Binding Protocol (ACNBP), a novel framework designed to facilitate secure, efficient, and verifiable interactions between agents in heterogeneous multi-agent systems through integration with an Agent Name Service (ANS) infrastructure that provides comprehensive discovery, negotiation, and binding mechanisms. The protocol introduces a structured 10-step process encompassing capability discovery, candidate pre-screening and selection, secure negotiation phases, and binding commitment with built-in security measures including digital signatures, capability attestation, and comprehensive threat mitigation strategies, while a key innovation of ACNBP is its protocolExtension mechanism that enables backward-compatible protocol evolution and supports diverse agent architectures while maintaining security and interoperability. We demonstrate ACNBP's effectiveness through a comprehensive security analysis using the MAESTRO threat modeling framework, practical implementation considerations, and a detailed example showcasing the protocol's application in a document translation scenario, with the protocol addressing critical challenges in agent autonomy, capability verification, secure communication, and scalable agent ecosystem management.",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13590v1",
    "published_date": "2025-06-16 15:18:24 UTC",
    "updated_date": "2025-06-16 15:18:24 UTC"
  },
  {
    "arxiv_id": "2506.14846v1",
    "title": "Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach",
    "authors": [
      "Shreyas Rajeev",
      "B Sathish Babu"
    ],
    "abstract": "Kernel size selection in Convolutional Neural Networks (CNNs) is a critical but often overlooked design decision that affects receptive field, feature extraction, computational cost, and model accuracy. This paper proposes the Best Kernel Size Estimation Function (BKSEF), a mathematically grounded and empirically validated framework for optimal, layer-wise kernel size determination. BKSEF balances information gain, computational efficiency, and accuracy improvements by integrating principles from information theory, signal processing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100, ImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided architectures achieve up to 3.1 percent accuracy improvement and 42.8 percent reduction in FLOPs compared to traditional models using uniform 3x3 kernels. Two real-world case studies further validate the approach: one for medical image classification in a cloud-based setup, and another for traffic sign recognition on edge devices. The former achieved enhanced interpretability and accuracy, while the latter reduced latency and model size significantly, with minimal accuracy trade-off. These results show that kernel size can be an active, optimizable parameter rather than a fixed heuristic. BKSEF provides practical heuristics and theoretical support for researchers and developers seeking efficient and application-aware CNN designs. It is suitable for integration into neural architecture search pipelines and real-time systems, offering a new perspective on CNN optimization.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14846v1",
    "published_date": "2025-06-16 15:15:30 UTC",
    "updated_date": "2025-06-16 15:15:30 UTC"
  },
  {
    "arxiv_id": "2506.13584v2",
    "title": "From Data-Driven to Purpose-Driven Artificial Intelligence: Systems Thinking for Data-Analytic Automation of Patient Care",
    "authors": [
      "Daniel Anadria",
      "Roel Dobbe",
      "Anastasia Giachanou",
      "Ruurd Kuiper",
      "Richard Bartels",
      "Wouter van Amsterdam",
      "Íñigo Martínez de Rituerto de Troya",
      "Carmen Zürcher",
      "Daniel Oberski"
    ],
    "abstract": "In this work, we reflect on the data-driven modeling paradigm that is gaining ground in AI-driven automation of patient care. We argue that the repurposing of existing real-world patient datasets for machine learning may not always represent an optimal approach to model development as it could lead to undesirable outcomes in patient care. We reflect on the history of data analysis to explain how the data-driven paradigm rose to popularity, and we envision ways in which systems thinking and clinical domain theory could complement the existing model development approaches in reaching human-centric outcomes. We call for a purpose-driven machine learning paradigm that is grounded in clinical theory and the sociotechnical realities of real-world operational contexts. We argue that understanding the utility of existing patient datasets requires looking in two directions: upstream towards the data generation, and downstream towards the automation objectives. This purpose-driven perspective to AI system development opens up new methodological opportunities and holds promise for AI automation of patient care.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SY",
      "math.ST",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "The work is under review at ACM Health",
    "pdf_url": "https://arxiv.org/pdf/2506.13584v2",
    "published_date": "2025-06-16 15:07:44 UTC",
    "updated_date": "2025-06-18 08:20:25 UTC"
  },
  {
    "arxiv_id": "2506.13583v1",
    "title": "Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes",
    "authors": [
      "Bernhard Hilpert",
      "Muhan Hou",
      "Kim Baraka",
      "Joost Broekens"
    ],
    "abstract": "Reinforcement Learning (RL) agents often exhibit learning behaviors that are not intuitively interpretable by human observers, which can result in suboptimal feedback in collaborative teaching settings. Yet, how humans perceive and interpret RL agent's learning behavior is largely unknown. In a bottom-up approach with two experiments, this work provides a data-driven understanding of the factors of human observers' understanding of the agent's learning process. A novel, observation-based paradigm to directly assess human inferences about agent learning was developed. In an exploratory interview study (\\textit{N}=9), we identify four core themes in human interpretations: Agent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second confirmatory study (\\textit{N}=34) applied an expanded version of the paradigm across two tasks (navigation/manipulation) and two RL algorithms (tabular/function approximation). Analyses of 816 responses confirmed the reliability of the paradigm and refined the thematic framework, revealing how these themes evolve over time and interrelate. Our findings provide a human-centered understanding of how people make sense of agent learning, offering actionable insights for designing interpretable RL systems and improving transparency in Human-Robot Interaction.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13583v1",
    "published_date": "2025-06-16 15:04:27 UTC",
    "updated_date": "2025-06-16 15:04:27 UTC"
  },
  {
    "arxiv_id": "2506.13579v2",
    "title": "Flexible-length Text Infilling for Discrete Diffusion Models",
    "authors": [
      "Andrew Zhang",
      "Anushka Sivakumar",
      "Chiawei Tang",
      "Chris Thomas"
    ],
    "abstract": "Discrete diffusion models are a new class of text generators that offer advantages such as bidirectional context use, parallelizable generation, and flexible prompting compared to autoregressive models. However, a critical limitation of discrete diffusion models is their inability to perform flexible-length or flexible-position text infilling without access to ground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete \\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling), the first discrete diffusion model to overcome this challenge. DDOT jointly denoises token values and token positions, employing a novel sample-level Optimal Transport (OT) coupling. This coupling preserves relative token ordering while dynamically adjusting the positions and length of infilled segments, a capability previously missing in text diffusion. Our method is orthogonal to existing discrete text diffusion methods and is compatible with various pretrained text denoisers. Extensive experiments on text infilling benchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms naive diffusion baselines. Furthermore, DDOT achieves performance on par with state-of-the-art non-autoregressive models and enables significant improvements in training efficiency and flexibility.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Major edit of methodology section. Matches EMNLP camera-ready version",
    "pdf_url": "https://arxiv.org/pdf/2506.13579v2",
    "published_date": "2025-06-16 15:02:12 UTC",
    "updated_date": "2025-10-22 03:27:18 UTC"
  },
  {
    "arxiv_id": "2506.13566v2",
    "title": "A Production Scheduling Framework for Reinforcement Learning Under Real-World Constraints",
    "authors": [
      "Jonathan Hoss",
      "Felix Schelling",
      "Noah Klarmann"
    ],
    "abstract": "The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing makespan under deterministic constraints. Real-world production environments introduce additional complexities that cause traditional scheduling approaches to be less effective. Reinforcement learning (RL) holds potential in addressing these challenges, as it allows agents to learn adaptive scheduling strategies. However, there is a lack of a comprehensive, general-purpose frameworks for effectively training and evaluating RL agents under real-world constraints. To address this gap, we propose a modular framework that extends classical JSSP formulations by incorporating key real-world constraints inherent to the shopfloor, including transport logistics, buffer management, machine breakdowns, setup times, and stochastic processing conditions, while also supporting multi-objective optimization. The framework is a customizable solution that offers flexibility in defining problem instances and configuring simulation parameters, enabling adaptation to diverse production scenarios. A standardized interface ensures compatibility with various RL approaches, providing a robust environment for training RL agents and facilitating the standardized comparison of different scheduling methods under dynamic and uncertain conditions. We release JobShopLab as an open-source tool for both research and industrial applications, accessible at: https://github.com/proto-lab-ro/jobshoplab",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted for presentation at the IEEE 21st International Conference on Automation Science and Engineering (CASE 2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.13566v2",
    "published_date": "2025-06-16 14:50:26 UTC",
    "updated_date": "2025-06-17 15:27:49 UTC"
  },
  {
    "arxiv_id": "2506.13559v1",
    "title": "Understand the Implication: Learning to Think for Pragmatic Understanding",
    "authors": [
      "Settaluri Lakshmi Sravanthi",
      "Kishan Maharaj",
      "Sravani Gunnu",
      "Abhijit Mishra",
      "Pushpak Bhattacharyya"
    ],
    "abstract": "Pragmatics, the ability to infer meaning beyond literal interpretation, is crucial for social cognition and communication. While LLMs have been benchmarked for their pragmatic understanding, improving their performance remains underexplored. Existing methods rely on annotated labels but overlook the reasoning process humans naturally use to interpret implicit meaning. To bridge this gap, we introduce a novel pragmatic dataset, ImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both correct and incorrect interpretations. Through preference-tuning and supervised fine-tuning, we demonstrate that thought-based learning significantly enhances LLMs' pragmatic understanding, improving accuracy by 11.12% across model families. We further discuss a transfer-learning study where we evaluate the performance of thought-based training for the other tasks of pragmatics (presupposition, deixis) that are not seen during the training time and observe an improvement of 16.10% compared to label-trained models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "SS and KM contributed equally to this work",
    "pdf_url": "https://arxiv.org/pdf/2506.13559v1",
    "published_date": "2025-06-16 14:45:08 UTC",
    "updated_date": "2025-06-16 14:45:08 UTC"
  },
  {
    "arxiv_id": "2506.13529v1",
    "title": "Seismic Acoustic Impedance Inversion Framework Based on Conditional Latent Generative Diffusion Model",
    "authors": [
      "Jie Chen",
      "Hongling Chen",
      "Jinghuai Gao",
      "Chuangji Meng",
      "Tao Yang",
      "XinXin Liang"
    ],
    "abstract": "Seismic acoustic impedance plays a crucial role in lithological identification and subsurface structure interpretation. However, due to the inherently ill-posed nature of the inversion problem, directly estimating impedance from post-stack seismic data remains highly challenging. Recently, diffusion models have shown great potential in addressing such inverse problems due to their strong prior learning and generative capabilities. Nevertheless, most existing methods operate in the pixel domain and require multiple iterations, limiting their applicability to field data. To alleviate these limitations, we propose a novel seismic acoustic impedance inversion framework based on a conditional latent generative diffusion model, where the inversion process is made in latent space. To avoid introducing additional training overhead when embedding conditional inputs, we design a lightweight wavelet-based module into the framework to project seismic data and reuse an encoder trained on impedance to embed low-frequency impedance into the latent space. Furthermore, we propose a model-driven sampling strategy during the inversion process of this framework to enhance accuracy and reduce the number of required diffusion steps. Numerical experiments on a synthetic model demonstrate that the proposed method achieves high inversion accuracy and strong generalization capability within only a few diffusion steps. Moreover, application to field data reveals enhanced geological detail and higher consistency with well-log measurements, validating the effectiveness and practicality of the proposed approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2506.13529v1",
    "published_date": "2025-06-16 14:19:40 UTC",
    "updated_date": "2025-06-16 14:19:40 UTC"
  },
  {
    "arxiv_id": "2506.13523v2",
    "title": "The Price of Freedom: Exploring Expressivity and Runtime Tradeoffs in Equivariant Tensor Products",
    "authors": [
      "YuQing Xie",
      "Ameya Daigavane",
      "Mit Kotak",
      "Tess Smidt"
    ],
    "abstract": "$E(3)$-equivariant neural networks have demonstrated success across a wide range of 3D modelling tasks. A fundamental operation in these networks is the tensor product, which interacts two geometric features in an equivariant manner to create new features. Due to the high computational complexity of the tensor product, significant effort has been invested to optimize the runtime of this operation. For example, Luo et al. (2024) recently proposed the Gaunt tensor product (GTP) which promises a significant speedup. In this work, we provide a careful, systematic analysis of a number of tensor product operations. In particular, we emphasize that different tensor products are not performing the same operation. The reported speedups typically come at the cost of expressivity. We introduce measures of expressivity and interactability to characterize these differences. In addition, we realized the original implementation of GTP can be greatly simplified by directly using a spherical grid at no cost in asymptotic runtime. This spherical grid approach is faster on our benchmarks and in actual training of the MACE interatomic potential by 30%. Finally, we provide the first systematic microbenchmarks of the various tensor product operations. We find that the theoretical runtime guarantees can differ wildly from empirical performance, demonstrating the need for careful application-specific benchmarking. Code is available at https://github.com/atomicarchitects/PriceofFreedom.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICML 2025. 27 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13523v2",
    "published_date": "2025-06-16 14:15:18 UTC",
    "updated_date": "2025-07-15 07:36:30 UTC"
  },
  {
    "arxiv_id": "2506.13505v1",
    "title": "UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data",
    "authors": [
      "Vasiliki Balaska",
      "Ioannis Tsampikos Papapetros",
      "Katerina Maria Oikonomou",
      "Loukas Bampis",
      "Antonios Gasteratos"
    ],
    "abstract": "The mining sector increasingly adopts digital tools to improve operational efficiency, safety, and data-driven decision-making. One of the key challenges remains the reliable acquisition of high-resolution, geo-referenced spatial information to support core activities such as extraction planning and on-site monitoring. This work presents an integrated system architecture that combines UAV-based sensing, LiDAR terrain modeling, and deep learning-based object detection to generate spatially accurate information for open-pit mining environments. The proposed pipeline includes geo-referencing, 3D reconstruction, and object localization, enabling structured spatial outputs to be integrated into an industrial digital twin platform. Unlike traditional static surveying methods, the system offers higher coverage and automation potential, with modular components suitable for deployment in real-world industrial contexts. While the current implementation operates in post-flight batch mode, it lays the foundation for real-time extensions. The system contributes to the development of AI-enhanced remote sensing in mining by demonstrating a scalable and field-validated geospatial data workflow that supports situational awareness and infrastructure safety.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.ET",
      "cs.RO"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13505v1",
    "published_date": "2025-06-16 13:59:56 UTC",
    "updated_date": "2025-06-16 13:59:56 UTC"
  },
  {
    "arxiv_id": "2506.13479v1",
    "title": "Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness",
    "authors": [
      "Mei-Yen Chen",
      "Thi Thu Uyen Hoang",
      "Michael Hahn",
      "M. Saquib Sarfraz"
    ],
    "abstract": "Merging or routing low-rank adapters (LoRAs) has emerged as a popular solution for enhancing large language models, particularly when data access is restricted by regulatory or domain-specific constraints. This position paper argues that the research community should shift its focus from developing new merging or routing algorithms to understanding the conditions under which reusing LoRAs is truly effective. Through theoretical analysis and synthetic two-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs enables genuine compositional generalization or merely reflects shallow pattern matching. Evaluating two data-agnostic methods--parameter averaging and dynamic adapter selection--we found that reusing LoRAs often fails to logically integrate knowledge across disjoint fine-tuning datasets, especially when such knowledge is underrepresented during pretraining. Our empirical results, supported by theoretical insights into LoRA's limited expressiveness, highlight the preconditions and constraints of reusing them for unseen tasks and cast doubt on its feasibility as a truly data-free approach. We advocate for pausing the pursuit of novel methods for recycling LoRAs and emphasize the need for rigorous mechanisms to guide future academic research in adapter-based model merging and practical system designs for practitioners.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13479v1",
    "published_date": "2025-06-16 13:35:22 UTC",
    "updated_date": "2025-06-16 13:35:22 UTC"
  },
  {
    "arxiv_id": "2506.13476v1",
    "title": "ESRPCB: an Edge guided Super-Resolution model and Ensemble learning for tiny Printed Circuit Board Defect detection",
    "authors": [
      "Xiem HoangVan",
      "Dang Bui Dinh",
      "Thanh Nguyen Canh",
      "Van-Truong Nguyen"
    ],
    "abstract": "Printed Circuit Boards (PCBs) are critical components in modern electronics, which require stringent quality control to ensure proper functionality. However, the detection of defects in small-scale PCBs images poses significant challenges as a result of the low resolution of the captured images, leading to potential confusion between defects and noise. To overcome these challenges, this paper proposes a novel framework, named ESRPCB (edgeguided super-resolution for PCBs defect detection), which combines edgeguided super-resolution with ensemble learning to enhance PCBs defect detection. The framework leverages the edge information to guide the EDSR (Enhanced Deep Super-Resolution) model with a novel ResCat (Residual Concatenation) structure, enabling it to reconstruct high-resolution images from small PCBs inputs. By incorporating edge features, the super-resolution process preserves critical structural details, ensuring that tiny defects remain distinguishable in the enhanced image. Following this, a multi-modal defect detection model employs ensemble learning to analyze the super-resolved",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in Engineering Applications of Artificial Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2506.13476v1",
    "published_date": "2025-06-16 13:34:35 UTC",
    "updated_date": "2025-06-16 13:34:35 UTC"
  },
  {
    "arxiv_id": "2506.13474v1",
    "title": "Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning",
    "authors": [
      "David Bani-Harouni",
      "Chantal Pellegrini",
      "Ege Özsoy",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "abstract": "Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited \"out-of-the-box\" capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13474v1",
    "published_date": "2025-06-16 13:32:01 UTC",
    "updated_date": "2025-06-16 13:32:01 UTC"
  },
  {
    "arxiv_id": "2506.13472v2",
    "title": "ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models",
    "authors": [
      "Junho Yoon",
      "Geom Lee",
      "Donghyeon Jeon",
      "Inho Kang",
      "Seung-Hoon Na"
    ],
    "abstract": "Quantization has been widely studied as an effective technique for reducing the memory requirement of large language models (LLMs), potentially improving the latency time as well. Utilizing the characteristic of rotational invariance of transformer, we propose the rotation-based saliency-aware weight quantization (ROSAQ), which identifies salient channels in the projection feature space, not in the original feature space, where the projected \"principal\" dimensions are naturally considered as \"salient\" features. The proposed ROSAQ consists of 1) PCA-based projection, which first performs principal component analysis (PCA) on a calibration set and transforms via the PCA projection, 2) Salient channel dentification, which selects dimensions corresponding to the K-largest eigenvalues as salient channels, and 3) Saliency-aware quantization with mixed-precision, which uses FP16 for salient dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ shows improvements over the baseline saliency-aware quantization on the original feature space and other existing quantization methods. With kernel fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in generating 256 tokens with a batch size of 64.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13472v2",
    "published_date": "2025-06-16 13:30:33 UTC",
    "updated_date": "2025-06-17 09:13:54 UTC"
  },
  {
    "arxiv_id": "2506.13469v2",
    "title": "A Two-stage Optimization Method for Wide-range Single-electron Quantum Magnetic Sensing",
    "authors": [
      "Shiqian Guo",
      "Jianqing Liu",
      "Thinh Le",
      "Huaiyu Dai"
    ],
    "abstract": "Quantum magnetic sensing based on spin systems has emerged as a new paradigm for detecting ultra-weak magnetic fields with unprecedented sensitivity, revitalizing applications in navigation, geo-localization, biology, and beyond. At the heart of quantum magnetic sensing, from the protocol perspective, lies the design of optimal sensing parameters to manifest and then estimate the underlying signals of interest (SoI). Existing studies on this front mainly rely on adaptive algorithms based on black-box AI models or formula-driven principled searches. However, when the SoI spans a wide range and the quantum sensor has physical constraints, these methods may fail to converge efficiently or optimally, resulting in prolonged interrogation times and reduced sensing accuracy. In this work, we report the design of a new protocol using a two-stage optimization method. In the 1st Stage, a Bayesian neural network with a fixed set of sensing parameters is used to narrow the range of SoI. In the 2nd Stage, a federated reinforcement learning agent is designed to fine-tune the sensing parameters within a reduced search space. The proposed protocol is developed and evaluated in a challenging context of single-shot readout of an NV-center electron spin under a constrained total sensing time budget; and yet it achieves significant improvements in both accuracy and resource efficiency for wide-range D.C. magnetic field estimation compared to the state of the art.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13469v2",
    "published_date": "2025-06-16 13:28:32 UTC",
    "updated_date": "2025-08-09 19:29:32 UTC"
  },
  {
    "arxiv_id": "2506.13468v1",
    "title": "An Interdisciplinary Approach to Human-Centered Machine Translation",
    "authors": [
      "Marine Carpuat",
      "Omri Asscher",
      "Kalika Bali",
      "Luisa Bentivogli",
      "Frédéric Blain",
      "Lynne Bowker",
      "Monojit Choudhury",
      "Hal Daumé",
      "Kevin Duh",
      "Ge Gao",
      "Alvin Grissom",
      "Marzena Karpinska",
      "Elaine C. Khoong",
      "William D. Lewis",
      "André F. T. Martins",
      "Mary Nurminen",
      "Douglas W. Oard",
      "Maja Popovic",
      "Michel Simard",
      "François Yvon"
    ],
    "abstract": "Machine Translation (MT) tools are widely used today, often in contexts where professional translators are not present. Despite progress in MT technology, a gap persists between system development and real-world usage, particularly for non-expert users who may struggle to assess translation reliability. This paper advocates for a human-centered approach to MT, emphasizing the alignment of system design with diverse communicative goals and contexts of use. We survey the literature in Translation Studies and Human-Computer Interaction to recontextualize MT evaluation and design to address the diverse real-world scenarios in which MT is used today.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.13468v1",
    "published_date": "2025-06-16 13:27:44 UTC",
    "updated_date": "2025-06-16 13:27:44 UTC"
  },
  {
    "arxiv_id": "2506.21579v1",
    "title": "LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation",
    "authors": [
      "Yingzhi He",
      "Xiaohao Liu",
      "An Zhang",
      "Yunshan Ma",
      "Tat-Seng Chua"
    ],
    "abstract": "Sequential recommendation aims to predict users' future interactions by modeling collaborative filtering (CF) signals from historical behaviors of similar users or items. Traditional sequential recommenders predominantly rely on ID-based embeddings, which capture CF signals through high-order co-occurrence patterns. However, these embeddings depend solely on past interactions, lacking transferable knowledge to generalize to unseen domains. Recent advances in large language models (LLMs) have motivated text-based recommendation approaches that derive item representations from textual descriptions. While these methods enhance generalization, they fail to encode CF signals-i.e., latent item correlations and preference patterns-crucial for effective recommendation. We argue that an ideal embedding model should seamlessly integrate CF signals with rich semantic representations to improve both in-domain and out-of-domain recommendation performance.\n  To this end, we propose LLM2Rec, a novel embedding model tailored for sequential recommendation, integrating the rich semantic understanding of LLMs with CF awareness. Our approach follows a two-stage training framework: (1) Collaborative Supervised Fine-tuning, which adapts LLMs to infer item relationships based on historical interactions, and (2) Item-level Embedding Modeling, which refines these specialized LLMs into structured item embedding models that encode both semantic and collaborative information. Extensive experiments on real-world datasets demonstrate that LLM2Rec effectively improves recommendation quality across both in-domain and out-of-domain settings. Our findings highlight the potential of leveraging LLMs to build more robust, generalizable embedding models for sequential recommendation. Our codes are available at https://github.com/HappyPointer/LLM2Rec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "KDD 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.21579v1",
    "published_date": "2025-06-16 13:27:06 UTC",
    "updated_date": "2025-06-16 13:27:06 UTC"
  },
  {
    "arxiv_id": "2506.13464v3",
    "title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study",
    "authors": [
      "Zhengyu Hu",
      "Jianxun Lian",
      "Zheyuan Xiao",
      "Seraphina Zhang",
      "Tianfu Wang",
      "Nicholas Jing Yuan",
      "Xing Xie",
      "Hui Xiong"
    ],
    "abstract": "Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a framework inspired by cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: Learning from Instructor (acquiring knowledge via explicit guidance), Learning from Concept (internalizing abstract structures and generalizing to new contexts), and Learning from Experience (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii) LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13464v3",
    "published_date": "2025-06-16 13:24:50 UTC",
    "updated_date": "2025-12-26 14:33:32 UTC"
  },
  {
    "arxiv_id": "2506.13456v1",
    "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
    "authors": [
      "Kangye Ji",
      "Yuan Meng",
      "Hanyun Cui",
      "Ye Li",
      "Shengjia Hua",
      "Lei Chen",
      "Zhi Wang"
    ],
    "abstract": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities, but its high computational cost renders it impractical for real-time robotic control. Despite huge redundancy across repetitive denoising steps, existing diffusion acceleration techniques fail to generalize to Diffusion Policy due to fundamental architectural and data divergences. In this paper, we propose Block-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by caching intermediate action features. BAC achieves lossless action generation acceleration by adaptively updating and reusing cached features at the block level, based on a key observation that feature similarities vary non-uniformly across timesteps and locks. To operationalize this insight, we first propose the Adaptive Caching Scheduler, designed to identify optimal update timesteps by maximizing the global feature similarities between cached and skipped features. However, applying this scheduler for each block leads to signiffcant error surges due to the inter-block propagation of caching errors, particularly within Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop the Bubbling Union Algorithm, which truncates these errors by updating the upstream blocks with signiffcant caching errors before downstream FFNs. As a training-free plugin, BAC is readily integrable with existing transformer-based Diffusion Policy and vision-language-action models. Extensive experiments on multiple robotic benchmarks demonstrate that BAC achieves up to 3x inference speedup for free.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13456v1",
    "published_date": "2025-06-16 13:14:58 UTC",
    "updated_date": "2025-06-16 13:14:58 UTC"
  },
  {
    "arxiv_id": "2506.13453v1",
    "title": "Towards a Formal Specification for Self-organized Shape Formation in Swarm Robotics",
    "authors": [
      "YR Darr",
      "MA Niazi"
    ],
    "abstract": "The self-organization of robots for the formation of structures and shapes is a stimulating application of the swarm robotic system. It involves a large number of autonomous robots of heterogeneous behavior, coordination among them, and their interaction with the dynamic environment. This process of complex structure formation is considered a complex system, which needs to be modeled by using any modeling approach. Although the formal specification approach along with other formal methods has been used to model the behavior of robots in a swarm. However, to the best of our knowledge, the formal specification approach has not been used to model the self-organization process in swarm robotic systems for shape formation. In this paper, we use a formal specification approach to model the shape formation task of swarm robots. We use Z (Zed) language of formal specification, which is a state-based language, to model the states of the entities of the systems. We demonstrate the effectiveness of Z for the self-organized shape formation. The presented formal specification model gives the outlines for designing and implementing the swarm robotic system for the formation of complex shapes and structures. It also provides the foundation for modeling the complex shape formation process for swarm robotics using a multi-agent system in a simulation-based environment. Keywords: Swarm robotics, Self-organization, Formal specification, Complex systems",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13453v1",
    "published_date": "2025-06-16 13:13:20 UTC",
    "updated_date": "2025-06-16 13:13:20 UTC"
  },
  {
    "arxiv_id": "2506.13450v1",
    "title": "A Neural Model for Word Repetition",
    "authors": [
      "Daniel Dager",
      "Robin Sobczyk",
      "Emmanuel Chemla",
      "Yair Lakretz"
    ],
    "abstract": "It takes several years for the developing brain of a baby to fully master word repetition-the task of hearing a word and repeating it aloud. Repeating a new word, such as from a new language, can be a challenging task also for adults. Additionally, brain damage, such as from a stroke, may lead to systematic speech errors with specific characteristics dependent on the location of the brain damage. Cognitive sciences suggest a model with various components for the different processing stages involved in word repetition. While some studies have begun to localize the corresponding regions in the brain, the neural mechanisms and how exactly the brain performs word repetition remain largely unknown. We propose to bridge the gap between the cognitive model of word repetition and neural mechanisms in the human brain by modeling the task using deep neural networks. Neural models are fully observable, allowing us to study the detailed mechanisms in their various substructures and make comparisons with human behavior and, ultimately, the brain. Here, we make first steps in this direction by: (1) training a large set of models to simulate the word repetition task; (2) creating a battery of tests to probe the models for known effects from behavioral studies in humans, and (3) simulating brain damage through ablation studies, where we systematically remove neurons from the model, and repeat the behavioral study to examine the resulting speech errors in the \"patient\" model. Our results show that neural models can mimic several effects known from human research, but might diverge in other aspects, highlighting both the potential and the challenges for future research aimed at developing human-like neural models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at Cognitive Computational Neuroscience 2025 (CCN)",
    "pdf_url": "https://arxiv.org/pdf/2506.13450v1",
    "published_date": "2025-06-16 13:09:24 UTC",
    "updated_date": "2025-06-16 13:09:24 UTC"
  },
  {
    "arxiv_id": "2506.13838v1",
    "title": "Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy",
    "authors": [
      "Lorena Poenaru-Olaru",
      "June Sallou",
      "Luis Cruz",
      "Jan Rellermeyer",
      "Arie van Deursen"
    ],
    "abstract": "The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages. Accepted at ICT4Sustainability 2025 conference",
    "pdf_url": "https://arxiv.org/pdf/2506.13838v1",
    "published_date": "2025-06-16 12:53:02 UTC",
    "updated_date": "2025-06-16 12:53:02 UTC"
  },
  {
    "arxiv_id": "2506.13415v1",
    "title": "Simple is what you need for efficient and accurate medical image segmentation",
    "authors": [
      "Xiang Yu",
      "Yayan Chen",
      "Guannan He",
      "Qing Zeng",
      "Yue Qin",
      "Meiling Liang",
      "Dandan Luo",
      "Yimei Liao",
      "Zeyu Ren",
      "Cheng Kang",
      "Delong Yang",
      "Bocheng Liang",
      "Bin Pu",
      "Ying Yuan",
      "Shengli Li"
    ],
    "abstract": "While modern segmentation models often prioritize performance over practicality, we advocate a design philosophy prioritizing simplicity and efficiency, and attempted high performance segmentation model design. This paper presents SimpleUNet, a scalable ultra-lightweight medical image segmentation model with three key innovations: (1) A partial feature selection mechanism in skip connections for redundancy reduction while enhancing segmentation performance; (2) A fixed-width architecture that prevents exponential parameter growth across network stages; (3) An adaptive feature fusion module achieving enhanced representation with minimal computational overhead. With a record-breaking 16 KB parameter configuration, SimpleUNet outperforms LBUNet and other lightweight benchmarks across multiple public datasets. The 0.67 MB variant achieves superior efficiency (8.60 GFLOPs) and accuracy, attaining a mean DSC/IoU of 85.76%/75.60% on multi-center breast lesion datasets, surpassing both U-Net and TransUNet. Evaluations on skin lesion datasets (ISIC 2017/2018: mDice 84.86%/88.77%) and endoscopic polyp segmentation (KVASIR-SEG: 86.46%/76.48% mDice/mIoU) confirm consistent dominance over state-of-the-art models. This work demonstrates that extreme model compression need not compromise performance, providing new insights for efficient and accurate medical image segmentation. Codes can be found at https://github.com/Frankyu5666666/SimpleUNet.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "15 pages, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13415v1",
    "published_date": "2025-06-16 12:31:48 UTC",
    "updated_date": "2025-06-16 12:31:48 UTC"
  },
  {
    "arxiv_id": "2506.13406v1",
    "title": "CALM: Consensus-Aware Localized Merging for Multi-Task Learning",
    "authors": [
      "Kunda Yan",
      "Min Zhang",
      "Sen Cui",
      "Zikun Qu",
      "Bo Jiang",
      "Feng Liu",
      "Changshui Zhang"
    ],
    "abstract": "Model merging aims to integrate the strengths of multiple fine-tuned models into a unified model while preserving task-specific capabilities. Existing methods, represented by task arithmetic, are typically classified into global- and local-aware methods. However, global-aware methods inevitably cause parameter interference, while local-aware methods struggle to maintain the effectiveness of task-specific details in the merged model. To address these limitations, we propose a Consensus-Aware Localized Merging (CALM) method which incorporates localized information aligned with global task consensus, ensuring its effectiveness post-merging. CALM consists of three key components: (1) class-balanced entropy minimization sampling, providing a more flexible and reliable way to leverage unsupervised data; (2) an efficient-aware framework, selecting a small set of tasks for sequential merging with high scalability; (3) a consensus-aware mask optimization, aligning localized binary masks with global task consensus and merging them conflict-free. Experiments demonstrate the superiority and robustness of our CALM, significantly outperforming existing methods and achieving performance close to traditional MTL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML2025",
    "pdf_url": "https://arxiv.org/pdf/2506.13406v1",
    "published_date": "2025-06-16 12:19:45 UTC",
    "updated_date": "2025-06-16 12:19:45 UTC"
  },
  {
    "arxiv_id": "2506.13404v3",
    "title": "Effective Learning for Small Reasoning Models: An Empirical Study on 0.5B Reasoning LLMs",
    "authors": [
      "Xialie Zhuang",
      "Peixian Ma",
      "Zhikai Jia",
      "Zane Cao",
      "Shiwei Liu"
    ],
    "abstract": "The ongoing evolution of language models has led to the development of large-scale architectures that demonstrate exceptional performance across a wide range of tasks. However, these models come with significant computational and energy demands, as well as potential privacy implications. In this context, Small Reasoning Language Models (SRLMs) with approximately 0.5 billion parameters present a compelling alternative due to their remarkable computational efficiency and cost-effectiveness, particularly in resource-constrained environments. Despite these advantages, the limited capacity of 0.5 billion parameter models poses challenges in handling complex tasks such as mathematical reasoning. This research investigates various training strategies, including supervised fine-tuning (SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as their hybrid implementations, to enhance the performance of 0.5B SRLMs. We analyze effective methodologies to bridge the performance gap between SRLMS and larger models and present insights into optimal training pipelines tailored for these smaller architectures. Through extensive experimental validation and analysis, our work aims to provide actionable recommendations for maximizing the reasoning capabilities of 0.5B models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2506.13404v3",
    "published_date": "2025-06-16 12:18:11 UTC",
    "updated_date": "2025-11-18 08:15:24 UTC"
  },
  {
    "arxiv_id": "2506.13403v1",
    "title": "Deflating Deflationism: A Critical Perspective on Debunking Arguments Against LLM Mentality",
    "authors": [
      "Alex Grzankowski",
      "Geoff Keeling",
      "Henry Shevlin",
      "Winnie Street"
    ],
    "abstract": "Many people feel compelled to interpret, describe, and respond to Large Language Models (LLMs) as if they possess inner mental lives similar to our own. Responses to this phenomenon have varied. Inflationists hold that at least some folk psychological ascriptions to LLMs are warranted. Deflationists argue that all such attributions of mentality to LLMs are misplaced, often cautioning against the risk that anthropomorphic projection may lead to misplaced trust or potentially even confusion about the moral status of LLMs. We advance this debate by assessing two common deflationary arguments against LLM mentality. What we term the 'robustness strategy' aims to undercut one justification for believing that LLMs are minded entities by showing that putatively cognitive and humanlike behaviours are not robust, failing to generalise appropriately. What we term the 'etiological strategy' undercuts attributions of mentality by challenging naive causal explanations of LLM behaviours, offering alternative causal accounts that weaken the case for mental state attributions. While both strategies offer powerful challenges to full-blown inflationism, we find that neither strategy provides a knock-down case against ascriptions of mentality to LLMs simpliciter. With this in mind, we explore a modest form of inflationism that permits ascriptions of mentality to LLMs under certain conditions. Specifically, we argue that folk practice provides a defeasible basis for attributing mental states and capacities to LLMs provided those mental states and capacities can be understood in metaphysically undemanding terms (e.g. knowledge, beliefs and desires), while greater caution is required when attributing metaphysically demanding mental phenomena such as phenomenal consciousness.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13403v1",
    "published_date": "2025-06-16 12:17:11 UTC",
    "updated_date": "2025-06-16 12:17:11 UTC"
  },
  {
    "arxiv_id": "2506.13384v1",
    "title": "Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses",
    "authors": [
      "Leonie V. D. E. Vogelsmeier",
      "Eduardo Oliveira",
      "Kamila Misiejuk",
      "Sonsoles López-Pernas",
      "Mohammed Saqr"
    ],
    "abstract": "Large language models (LLMs) offer the potential to simulate human-like responses and behaviors, creating new opportunities for psychological science. In the context of self-regulated learning (SRL), if LLMs can reliably simulate survey responses at scale and speed, they could be used to test intervention scenarios, refine theoretical models, augment sparse datasets, and represent hard-to-reach populations. However, the validity of LLM-generated survey responses remains uncertain, with limited research focused on SRL and existing studies beyond SRL yielding mixed results. Therefore, in this study, we examined LLM-generated responses to the 44-item Motivated Strategies for Learning Questionnaire (MSLQ; Pintrich \\& De Groot, 1990), a widely used instrument assessing students' learning strategies and academic motivation. Particularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large. We analyzed item distributions, the psychological network of the theoretical SRL dimensions, and psychometric validity based on the latent factor structure. Our results suggest that Gemini 2 Flash was the most promising LLM, showing considerable sampling variability and producing underlying dimensions and theoretical relationships that align with prior theory and empirical findings. At the same time, we observed discrepancies and limitations, underscoring both the potential and current constraints of using LLMs for simulating psychological survey data and applying it in educational contexts.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "stat.ME",
      "stat.OT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13384v1",
    "published_date": "2025-06-16 11:48:58 UTC",
    "updated_date": "2025-06-16 11:48:58 UTC"
  },
  {
    "arxiv_id": "2506.13362v2",
    "title": "Mitigating loss of variance in ensemble data assimilation: machine learning-based and distance-free localization",
    "authors": [
      "Vinicius L. S. Silva",
      "Gabriel S. Seabra",
      "Alexandre A. Emerick"
    ],
    "abstract": "We propose two new methods based/inspired by machine learning for tabular data and distance-free localization to enhance the covariance estimations in an ensemble data assimilation. The main goal is to enhance the data assimilation results by mitigating loss of variance due to sampling errors. We also analyze the suitability of several machine learning models and the balance between accuracy and computational cost of the covariance estimations. We introduce two distance-free localization techniques leveraging machine learning methods specifically tailored for tabular data. The methods are integrated into the Ensemble Smoother with Multiple Data Assimilation (ES-MDA) framework. The results show that the proposed localizations improve covariance accuracy and enhance data assimilation and uncertainty quantification results. We observe reduced variance loss for the input variables using the proposed methods. Furthermore, we compare several machine learning models, assessing their suitability for the problem in terms of computational cost, and quality of the covariance estimation and data match. The influence of ensemble size is also investigated, providing insights into balancing accuracy and computational efficiency. Our findings demonstrate that certain machine learning models are more suitable for this problem. This study introduces two novel methods that mitigate variance loss for model parameters in ensemble-based data assimilation, offering practical solutions that are easy to implement and do not require any additional numerical simulation or hyperparameter tuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13362v2",
    "published_date": "2025-06-16 11:09:27 UTC",
    "updated_date": "2025-07-30 16:08:55 UTC"
  },
  {
    "arxiv_id": "2506.13358v1",
    "title": "Socratic RL: A Novel Framework for Efficient Knowledge Acquisition through Iterative Reflection and Viewpoint Distillation",
    "authors": [
      "Xiangfan Wu"
    ],
    "abstract": "Current Reinforcement Learning (RL) methodologies for Large Language Models (LLMs) often rely on simplistic, outcome-based reward signals (e.g., final answer correctness), which limits the depth of learning from each interaction. This paper introduces Socratic Reinforcement Learning (Socratic-RL), a novel, process-oriented framework designed to address this limitation. Socratic-RL operates on the principle that deeper understanding is achieved by reflecting on the causal reasons for errors and successes within the reasoning process itself. The framework employs a decoupled \"Teacher-Student\" architecture, where a \"Teacher AI\" analyzes interaction histories, extracts causal insights, and formulates them into structured \"viewpoints.\" These viewpoints, acting as distilled guidance, are then used by a \"Student AI\" to enhance its subsequent reasoning. A key innovation is the iterative self-improvement of the Teacher AI, enabling its reflective capabilities to evolve through a meta-learning loop. To manage the accumulation of knowledge, a distillation mechanism compresses learned viewpoints into the Student's parameters. By focusing on process rather than just outcome, Socratic-RL presents a pathway toward enhanced sample efficiency, superior interpretability, and a more scalable architecture for self-improving AI systems. This paper details the foundational concepts, formal mechanisms, synergies, challenges, and a concrete research roadmap for this proposed framework.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13358v1",
    "published_date": "2025-06-16 10:57:58 UTC",
    "updated_date": "2025-06-16 10:57:58 UTC"
  },
  {
    "arxiv_id": "2506.13356v1",
    "title": "StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns",
    "authors": [
      "Luanbo Wan",
      "Weizhi Ma"
    ],
    "abstract": "Long-term memory (LTM) is essential for large language models (LLMs) to achieve autonomous intelligence in complex, evolving environments. Despite increasing efforts in memory-augmented and retrieval-based architectures, there remains a lack of standardized benchmarks to systematically evaluate LLMs' long-term memory abilities. Existing benchmarks still face challenges in evaluating knowledge retention and dynamic sequential reasoning, and in their own flexibility, all of which limit their effectiveness in assessing models' LTM capabilities. To address these gaps, we propose a novel benchmark framework based on interactive fiction games, featuring dynamically branching storylines with complex reasoning structures. These structures simulate real-world scenarios by requiring LLMs to navigate hierarchical decision trees, where each choice triggers cascading dependencies across multi-turn interactions. Our benchmark emphasizes two distinct settings to test reasoning complexity: one with immediate feedback upon incorrect decisions, and the other requiring models to independently trace back and revise earlier choices after failure. As part of this benchmark, we also construct a new dataset designed to test LLMs' LTM within narrative-driven environments. We further validate the effectiveness of our approach through detailed experiments. Experimental results demonstrate the benchmark's ability to robustly and reliably assess LTM in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13pages, 8 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.13356v1",
    "published_date": "2025-06-16 10:54:31 UTC",
    "updated_date": "2025-06-16 10:54:31 UTC"
  },
  {
    "arxiv_id": "2506.13351v1",
    "title": "Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks",
    "authors": [
      "Yifei Xu",
      "Tusher Chakraborty",
      "Srinagesh Sharma",
      "Leonardo Nunes",
      "Emre Kıcıman",
      "Songwu Lu",
      "Ranveer Chandra"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have showcased impressive reasoning abilities in structured tasks like mathematics and programming, largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which uses outcome-based signals that are scalable, effective, and robust against reward hacking. However, applying similar techniques to open-ended long-form reasoning tasks remains challenging due to the absence of generic, verifiable reward signals. To address this, we propose Direct Reasoning Optimization (DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended, particularly long-form, reasoning tasks, guided by a new reward signal: the Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and emphasizes key tokens in the reference outcome that reflect the influence of the model's preceding chain-of-thought reasoning, thereby capturing the consistency between reasoning and reference outcome at a fine-grained level. Crucially, R3 is computed internally using the same model being optimized, enabling a fully self-contained training setup. Additionally, we introduce a dynamic data filtering strategy based on R3 for open-ended reasoning tasks, reducing cost while improving downstream performance. We evaluate DRO on two diverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a math-oriented QA benchmark -- and show that it consistently outperforms strong baselines while remaining broadly applicable across both open-ended and structured domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13351v1",
    "published_date": "2025-06-16 10:43:38 UTC",
    "updated_date": "2025-06-16 10:43:38 UTC"
  },
  {
    "arxiv_id": "2506.13344v1",
    "title": "LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation with Spectral Adversarial Perturbations",
    "authors": [
      "Lorenzo Bini",
      "Stephane Marchand-Maillet"
    ],
    "abstract": "Generating high-fidelity and biologically plausible synthetic single-cell RNA sequencing (scRNA-seq) data, especially with conditional control, is challenging due to its high dimensionality, sparsity, and complex biological variations. Existing generative models often struggle to capture these unique characteristics and ensure robustness to structural noise in cellular networks. We introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model for robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates graph-based representations with a score-based diffusion model, enhanced by a novel spectral adversarial perturbation mechanism on graph edge weights. Our contributions are threefold: we leverage Laplacian Positional Encodings (LPEs) to enrich the latent space with crucial cellular relationship information; we develop a conditional score-based diffusion model for effective learning and generation from complex scRNA-seq distributions; and we employ a unique spectral adversarial training scheme on graph edge weights, boosting robustness against structural variations. Extensive experiments on diverse scRNA-seq datasets demonstrate LapDDPM's superior performance, achieving high fidelity and generating biologically-plausible, cell-type-specific samples. LapDDPM sets a new benchmark for conditional scRNA-seq data generation, offering a robust tool for various downstream biological applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.CB",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "LapDDPM is a novel conditional graph diffusion model for scRNA-seq generation. Leveraging spectral adversarial perturbations, it ensures robustness and yields high-fidelity, biologically plausible, and cell-type-specific samples for complex data. Proceedings of the ICML 2025 GenBio Workshop: The 2nd Workshop on Generative AI and Biology, Vancouver, Canada, 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.13344v1",
    "published_date": "2025-06-16 10:35:32 UTC",
    "updated_date": "2025-06-16 10:35:32 UTC"
  },
  {
    "arxiv_id": "2506.13342v1",
    "title": "Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers",
    "authors": [
      "Wooseok Seo",
      "Seungju Han",
      "Jaehun Jung",
      "Benjamin Newman",
      "Seungwon Lim",
      "Seungbeen Lee",
      "Ximing Lu",
      "Yejin Choi",
      "Youngjae Yu"
    ],
    "abstract": "Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at https://github.com/just1nseo/verifying-the-verifiers",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13342v1",
    "published_date": "2025-06-16 10:32:10 UTC",
    "updated_date": "2025-06-16 10:32:10 UTC"
  },
  {
    "arxiv_id": "2506.13340v1",
    "title": "Probabilistic Modeling of Spiking Neural Networks with Contract-Based Verification",
    "authors": [
      "Zhen Yao",
      "Elisabetta De Maria",
      "Robert De Simone"
    ],
    "abstract": "Spiking Neural Networks (SNN) are models for \"realistic\" neuronal computation, which makes them somehow different in scope from \"ordinary\" deep-learning models widely used in AI platforms nowadays. SNNs focus on timed latency (and possibly probability) of neuronal reactive activation/response, more than numerical computation of filters. So, an SNN model must provide modeling constructs for elementary neural bundles and then for synaptic connections to assemble them into compound data flow network patterns. These elements are to be parametric patterns, with latency and probability values instantiated on particular instances (while supposedly constant \"at runtime\"). Designers could also use different values to represent \"tired\" neurons, or ones impaired by external drugs, for instance. One important challenge in such modeling is to study how compound models could meet global reaction requirements (in stochastic timing challenges), provided similar provisions on individual neural bundles. A temporal language of logic to express such assume/guarantee contracts is thus needed. This may lead to formal verification on medium-sized models and testing observations on large ones. In the current article, we make preliminary progress at providing a simple model framework to express both elementary SNN neural bundles and their connecting constructs, which translates readily into both a model-checker and a simulator (both already existing and robust) to conduct experiments.",
    "categories": [
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.AI",
    "comment": "15pages, 6figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2506.13340v1",
    "published_date": "2025-06-16 10:30:16 UTC",
    "updated_date": "2025-06-16 10:30:16 UTC"
  },
  {
    "arxiv_id": "2506.13324v2",
    "title": "Towards Pervasive Distributed Agentic Generative AI -- A State of The Art",
    "authors": [
      "Gianni Molinari",
      "Fabio Ciravegna"
    ],
    "abstract": "The rapid advancement of intelligent agents and Large Language Models (LLMs) is reshaping the pervasive computing field. Their ability to perceive, reason, and act through natural language understanding enables autonomous problem-solving in complex pervasive environments, including the management of heterogeneous sensors, devices, and data. This survey outlines the architectural components of LLM agents (profiling, memory, planning, and action) and examines their deployment and evaluation across various scenarios. Than it reviews computational and infrastructural advancements (cloud to edge) in pervasive computing and how AI is moving in this field. It highlights state-of-the-art agent deployment strategies and applications, including local and distributed execution on resource-constrained devices. This survey identifies key challenges of these agents in pervasive computing such as architectural, energetic and privacy limitations. It finally proposes what we called \"Agent as a Tool\", a conceptual framework for pervasive agentic AI, emphasizing context awareness, modularity, security, efficiency and effectiveness.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13324v2",
    "published_date": "2025-06-16 10:15:06 UTC",
    "updated_date": "2025-12-18 17:02:34 UTC"
  },
  {
    "arxiv_id": "2506.13323v1",
    "title": "Tady: A Neural Disassembler without Structural Constraint Violations",
    "authors": [
      "Siliang Qin",
      "Fengrui Yang",
      "Hao Wang",
      "Bolun Zhang",
      "Zeyu Gao",
      "Chao Zhang",
      "Kai Chen"
    ],
    "abstract": "Disassembly is a crucial yet challenging step in binary analysis. While emerging neural disassemblers show promise for efficiency and accuracy, they frequently generate outputs violating fundamental structural constraints, which significantly compromise their practical usability. To address this critical problem, we regularize the disassembly solution space by formalizing and applying key structural constraints based on post-dominance relations. This approach systematically detects widespread errors in existing neural disassemblers' outputs. These errors often originate from models' limited context modeling and instruction-level decoding that neglect global structural integrity. We introduce Tady, a novel neural disassembler featuring an improved model architecture and a dedicated post-processing algorithm, specifically engineered to address these deficiencies. Comprehensive evaluations on diverse binaries demonstrate that Tady effectively eliminates structural constraint violations and functions with high efficiency, while maintaining instruction-level accuracy.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "Usenix Security'25",
    "pdf_url": "https://arxiv.org/pdf/2506.13323v1",
    "published_date": "2025-06-16 10:11:43 UTC",
    "updated_date": "2025-06-16 10:11:43 UTC"
  },
  {
    "arxiv_id": "2506.13322v1",
    "title": "Active Multimodal Distillation for Few-shot Action Recognition",
    "authors": [
      "Weijia Feng",
      "Yichen Zhu",
      "Ruojia Zhang",
      "Chenyang Wang",
      "Fei Ma",
      "Xiaobao Wang",
      "Xiaobai Li"
    ],
    "abstract": "Owing to its rapid progress and broad application prospects, few-shot action recognition has attracted considerable interest. However, current methods are predominantly based on limited single-modal data, which does not fully exploit the potential of multimodal information. This paper presents a novel framework that actively identifies reliable modalities for each sample using task-specific contextual cues, thus significantly improving recognition performance. Our framework integrates an Active Sample Inference (ASI) module, which utilizes active inference to predict reliable modalities based on posterior distributions and subsequently organizes them accordingly. Unlike reinforcement learning, active inference replaces rewards with evidence-based preferences, making more stable predictions. Additionally, we introduce an active mutual distillation module that enhances the representation learning of less reliable modalities by transferring knowledge from more reliable ones. Adaptive multimodal inference is employed during the meta-test to assign higher weights to reliable modalities. Extensive experiments across multiple benchmarks demonstrate that our method significantly outperforms existing approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IJCAI 2025, the 34th International Joint Conference on Artificial Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2506.13322v1",
    "published_date": "2025-06-16 10:10:56 UTC",
    "updated_date": "2025-06-16 10:10:56 UTC"
  },
  {
    "arxiv_id": "2506.13318v1",
    "title": "Vine Copulas as Differentiable Computational Graphs",
    "authors": [
      "Tuoyuan Cheng",
      "Thibault Vatter",
      "Thomas Nagler",
      "Kan Chen"
    ],
    "abstract": "Vine copulas are sophisticated models for multivariate distributions and are increasingly used in machine learning. To facilitate their integration into modern ML pipelines, we introduce the vine computational graph, a DAG that abstracts the multilevel vine structure and associated computations. On this foundation, we devise new algorithms for conditional sampling, efficient sampling-order scheduling, and constructing vine structures for customized conditioning variables. We implement these ideas in torchvinecopulib, a GPU-accelerated Python library built upon PyTorch, delivering improved scalability for fitting, sampling, and density evaluation. Our experiments illustrate how gradient flowing through the vine can improve Vine Copula Autoencoders and that incorporating vines for uncertainty quantification in deep learning can outperform MC-dropout, deep ensembles, and Bayesian Neural Networks in sharpness, calibration, and runtime. By recasting vine copula models as computational graphs, our work connects classical dependence modeling with modern deep-learning toolchains and facilitates the integration of state-of-the-art copula methods in modern machine learning pipelines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13318v1",
    "published_date": "2025-06-16 09:57:36 UTC",
    "updated_date": "2025-06-16 09:57:36 UTC"
  },
  {
    "arxiv_id": "2506.13313v1",
    "title": "Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines",
    "authors": [
      "Weiyao Meng",
      "John Harvey",
      "James Goulding",
      "Chris James Carter",
      "Evgeniya Lukinova",
      "Andrew Smith",
      "Paul Frobisher",
      "Mina Forrest",
      "Georgiana Nica-Avram"
    ],
    "abstract": "Reading and evaluating product reviews is central to how most people decide what to buy and consume online. However, the recent emergence of Large Language Models and Generative Artificial Intelligence now means writing fraudulent or fake reviews is potentially easier than ever. Through three studies we demonstrate that (1) humans are no longer able to distinguish between real and fake product reviews generated by machines, averaging only 50.8% accuracy overall - essentially the same that would be expected by chance alone; (2) that LLMs are likewise unable to distinguish between fake and real reviews and perform equivalently bad or even worse than humans; and (3) that humans and LLMs pursue different strategies for evaluating authenticity which lead to equivalently bad accuracy, but different precision, recall and F1 scores - indicating they perform worse at different aspects of judgment. The results reveal that review systems everywhere are now susceptible to mechanised fraud if they do not depend on trustworthy purchase verification to guarantee the authenticity of reviewers. Furthermore, the results provide insight into the consumer psychology of how humans judge authenticity, demonstrating there is an inherent 'scepticism bias' towards positive reviews and a special vulnerability to misjudge the authenticity of fake negative reviews. Additionally, results provide a first insight into the 'machine psychology' of judging fake reviews, revealing that the strategies LLMs take to evaluate authenticity radically differ from humans, in ways that are equally wrong in terms of accuracy, but different in their misjudgments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "econ.GN"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13313v1",
    "published_date": "2025-06-16 09:54:56 UTC",
    "updated_date": "2025-06-16 09:54:56 UTC"
  },
  {
    "arxiv_id": "2506.13307v2",
    "title": "Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent Diffusion Models in the Generation of Unseen SAR Images",
    "authors": [
      "Solène Debuysère",
      "Nicolas Trouvé",
      "Nathan Letheule",
      "Olivier Lévêque",
      "Elise Colin"
    ],
    "abstract": "We present a framework for adapting a large pretrained latent diffusion model to high-resolution Synthetic Aperture Radar (SAR) image generation. The approach enables controllable synthesis and the creation of rare or out-of-distribution scenes beyond the training set. Rather than training a task-specific small model from scratch, we adapt an open-source text-to-image foundation model to the SAR modality, using its semantic prior to align prompts with SAR imaging physics (side-looking geometry, slant-range projection, and coherent speckle with heavy-tailed statistics). Using a 100k-image SAR dataset, we compare full fine-tuning and parameter-efficient Low-Rank Adaptation (LoRA) across the UNet diffusion backbone, the Variational Autoencoder (VAE), and the text encoders. Evaluation combines (i) statistical distances to real SAR amplitude distributions, (ii) textural similarity via Gray-Level Co-occurrence Matrix (GLCM) descriptors, and (iii) semantic alignment using a SAR-specialized CLIP model. Our results show that a hybrid strategy-full UNet tuning with LoRA on the text encoders and a learned token embedding-best preserves SAR geometry and texture while maintaining prompt fidelity. The framework supports text-based control and multimodal conditioning (e.g., segmentation maps, TerraSAR-X, or optical guidance), opening new paths for large-scale SAR scene data augmentation and unseen scenario simulation in Earth observation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13307v2",
    "published_date": "2025-06-16 09:48:01 UTC",
    "updated_date": "2025-08-14 16:29:14 UTC"
  },
  {
    "arxiv_id": "2506.13300v3",
    "title": "Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models",
    "authors": [
      "Bo Li",
      "Chengben Xu",
      "Wufeng Zhang"
    ],
    "abstract": "This paper presents Seewo's systems for both tracks of the Multilingual Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We introduce a multi-stage training pipeline that explicitly enhances reasoning and self-correction in speech language models for ASR. Our approach combines curriculum learning for progressive capability acquisition, Chain-of-Thought data augmentation to foster intermediate reflection, and Reinforcement Learning with Verifiable Rewards (RLVR) to further refine self-correction through reward-driven optimization. This approach achieves substantial improvements over the official challenge baselines. On the evaluation set, our best system attains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track 2. Comprehensive ablation studies demonstrate the effectiveness of each component under challenge constraints.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13300v3",
    "published_date": "2025-06-16 09:42:05 UTC",
    "updated_date": "2025-06-18 06:57:58 UTC"
  },
  {
    "arxiv_id": "2506.13298v2",
    "title": "Fair Generation without Unfair Distortions: Debiasing Text-to-Image Generation with Entanglement-Free Attention",
    "authors": [
      "Jeonghoon Park",
      "Juyoung Lee",
      "Chaeyeon Chung",
      "Jaeseong Lee",
      "Jaegul Choo",
      "Jindong Gu"
    ],
    "abstract": "Recent advancements in diffusion-based text-to-image (T2I) models have enabled the generation of high-quality and photorealistic images from text. However, they often exhibit societal biases related to gender, race, and socioeconomic status, thereby potentially reinforcing harmful stereotypes and shaping public perception in unintended ways. While existing bias mitigation methods demonstrate effectiveness, they often encounter attribute entanglement, where adjustments to attributes relevant to the bias (i.e., target attributes) unintentionally alter attributes unassociated with the bias (i.e., non-target attributes), causing undesirable distribution shifts. To address this challenge, we introduce Entanglement-Free Attention (EFA), a method that accurately incorporates target attributes (e.g., White, Black, and Asian) while preserving non-target attributes (e.g., background) during bias mitigation. At inference time, EFA randomly samples a target attribute with equal probability and adjusts the cross-attention in selected layers to incorporate the sampled attribute, achieving a fair distribution of target attributes. Extensive experiments demonstrate that EFA outperforms existing methods in mitigating bias while preserving non-target attributes, thereby maintaining the original model's output distribution and generative capacity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.13298v2",
    "published_date": "2025-06-16 09:40:32 UTC",
    "updated_date": "2025-08-03 05:30:11 UTC"
  },
  {
    "arxiv_id": "2506.13292v2",
    "title": "Automatic Multi-View X-Ray/CT Registration Using Bone Substructure Contours",
    "authors": [
      "Roman Flepp",
      "Leon Nissen",
      "Bastian Sigrist",
      "Arend Nieuwland",
      "Nicola Cavalcanti",
      "Philipp Fürnstahl",
      "Thomas Dreher",
      "Lilian Calvet"
    ],
    "abstract": "Purpose: Accurate intraoperative X-ray/CT registration is essential for surgical navigation in orthopedic procedures. However, existing methods struggle with consistently achieving sub-millimeter accuracy, robustness under broad initial pose estimates or need manual key-point annotations. This work aims to address these challenges by proposing a novel multi-view X-ray/CT registration method for intraoperative bone registration. Methods: The proposed registration method consists of a multi-view, contour-based iterative closest point (ICP) optimization. Unlike previous methods, which attempt to match bone contours across the entire silhouette in both imaging modalities, we focus on matching specific subcategories of contours corresponding to bone substructures. This leads to reduced ambiguity in the ICP matches, resulting in a more robust and accurate registration solution. This approach requires only two X-ray images and operates fully automatically. Additionally, we contribute a dataset of 5 cadaveric specimens, including real X-ray images, X-ray image poses and the corresponding CT scans. Results: The proposed registration method is evaluated on real X-ray images using mean reprojection error (mRPD). The method consistently achieves sub-millimeter accuracy with a mRPD 0.67mm compared to 5.35mm by a commercial solution requiring manual intervention. Furthermore, the method offers improved practical applicability, being fully automatic. Conclusion: Our method offers a practical, accurate, and efficient solution for multi-view X-ray/CT registration in orthopedic surgeries, which can be easily combined with tracking systems. By improving registration accuracy and minimizing manual intervention, it enhances intraoperative navigation, contributing to more accurate and effective surgical outcomes in computer-assisted surgery (CAS).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper was accepted to IPCAI 2025. The Project Webpage is: https://rflepp.github.io/BoneSubstructureContours2D3DRegistration/",
    "pdf_url": "https://arxiv.org/pdf/2506.13292v2",
    "published_date": "2025-06-16 09:33:37 UTC",
    "updated_date": "2025-11-24 15:46:09 UTC"
  },
  {
    "arxiv_id": "2506.13284v1",
    "title": "AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy",
    "authors": [
      "Zihan Liu",
      "Zhuolin Yang",
      "Yang Chen",
      "Chankyu Lee",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Wei Ping"
    ],
    "abstract": "In this work, we investigate the synergy between supervised fine-tuning (SFT) and reinforcement learning (RL) in developing strong reasoning models. We begin by curating the SFT training data through two scaling strategies: increasing the number of collected prompts and the number of generated responses per prompt. Both approaches yield notable improvements in reasoning performance, with scaling the number of prompts resulting in more substantial gains. We then explore the following questions regarding the synergy between SFT and RL: (i) Does a stronger SFT model consistently lead to better final performance after large-scale RL training? (ii) How can we determine an appropriate sampling temperature during RL training to effectively balance exploration and exploitation for a given SFT initialization? Our findings suggest that (i) holds true, provided effective RL training is conducted, particularly when the sampling temperature is carefully chosen to maintain the temperature-adjusted entropy around 0.3, a setting that strikes a good balance between exploration and exploitation. Notably, the performance gap between initial SFT models narrows significantly throughout the RL process. Leveraging a strong SFT foundation and insights into the synergistic interplay between SFT and RL, our AceReason-Nemotron-1.1 7B model significantly outperforms AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among Qwen2.5-7B-based reasoning models on challenging math and code benchmarks, thereby demonstrating the effectiveness of our post-training recipe. We release the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The AceReason-Nemotron collection: https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485",
    "pdf_url": "https://arxiv.org/pdf/2506.13284v1",
    "published_date": "2025-06-16 09:27:48 UTC",
    "updated_date": "2025-06-16 09:27:48 UTC"
  },
  {
    "arxiv_id": "2506.13277v2",
    "title": "SeqPE: Transformer with Sequential Position Encoding",
    "authors": [
      "Huayang Li",
      "Yahui Liu",
      "Hongyu Sun",
      "Deng Cai",
      "Leyang Cui",
      "Wei Bi",
      "Peilin Zhao",
      "Taro Watanabe"
    ],
    "abstract": "Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each $n$-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracy--particularly under context length extrapolation--but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13277v2",
    "published_date": "2025-06-16 09:16:40 UTC",
    "updated_date": "2025-06-17 10:12:39 UTC"
  },
  {
    "arxiv_id": "2506.13276v1",
    "title": "Navigating the Black Box: Leveraging LLMs for Effective Text-Level Graph Injection Attacks",
    "authors": [
      "Yuefei Lyu",
      "Chaozhuo Li",
      "Xi Zhang",
      "Tianle Zhang"
    ],
    "abstract": "Text-attributed graphs (TAGs) integrate textual data with graph structures, providing valuable insights in applications such as social network analysis and recommendation systems. Graph Neural Networks (GNNs) effectively capture both topological structure and textual information in TAGs but are vulnerable to adversarial attacks. Existing graph injection attack (GIA) methods assume that attackers can directly manipulate the embedding layer, producing non-explainable node embeddings. Furthermore, the effectiveness of these attacks often relies on surrogate models with high training costs. Thus, this paper introduces ATAG-LLM, a novel black-box GIA framework tailored for TAGs. Our approach leverages large language models (LLMs) to generate interpretable text-level node attributes directly, ensuring attacks remain feasible in real-world scenarios. We design strategies for LLM prompting that balance exploration and reliability to guide text generation, and propose a similarity assessment method to evaluate attack text effectiveness in disrupting graph homophily. This method efficiently perturbs the target node with minimal training costs in a strict black-box setting, ensuring a text-level graph injection attack for TAGs. Experiments on real-world TAG datasets validate the superior performance of ATAG-LLM compared to state-of-the-art embedding-level and text-level attack methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13276v1",
    "published_date": "2025-06-16 09:16:21 UTC",
    "updated_date": "2025-06-16 09:16:21 UTC"
  },
  {
    "arxiv_id": "2507.21080v1",
    "title": "Which symbol grounding problem should we try to solve?",
    "authors": [
      "Vincent C. Müller"
    ],
    "abstract": "Floridi and Taddeo propose a condition of \"zero semantic commitment\" for solutions to the grounding problem, and a solution to it. I argue briefly that their condition cannot be fulfilled, not even by their own solution. After a look at Luc Steels' very different competing suggestion, I suggest that we need to re-think what the problem is and what role the 'goals' in a system play in formulating the problem. On the basis of a proper understanding of computing, I come to the conclusion that the only sensible grounding problem is how we can explain and re-produce the behavioral ability and function of meaning in artificial computational agents",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.21080v1",
    "published_date": "2025-06-16 09:11:13 UTC",
    "updated_date": "2025-06-16 09:11:13 UTC"
  },
  {
    "arxiv_id": "2506.13268v1",
    "title": "Energy-Efficient Digital Design: A Comparative Study of Event-Driven and Clock-Driven Spiking Neurons",
    "authors": [
      "Filippo Marostica",
      "Alessio Carpegna",
      "Alessandro Savino",
      "Stefano Di Carlo"
    ],
    "abstract": "This paper presents a comprehensive evaluation of Spiking Neural Network (SNN) neuron models for hardware acceleration by comparing event driven and clock-driven implementations. We begin our investigation in software, rapidly prototyping and testing various SNN models based on different variants of the Leaky Integrate and Fire (LIF) neuron across multiple datasets. This phase enables controlled performance assessment and informs design refinement. Our subsequent hardware phase, implemented on FPGA, validates the simulation findings and offers practical insights into design trade offs. In particular, we examine how variations in input stimuli influence key performance metrics such as latency, power consumption, energy efficiency, and resource utilization. These results yield valuable guidelines for constructing energy efficient, real time neuromorphic systems. Overall, our work bridges software simulation and hardware realization, advancing the development of next generation SNN accelerators.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13268v1",
    "published_date": "2025-06-16 09:10:19 UTC",
    "updated_date": "2025-06-16 09:10:19 UTC"
  },
  {
    "arxiv_id": "2506.13265v3",
    "title": "Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware Learning",
    "authors": [
      "Rohit Mohan",
      "Julia Hindel",
      "Florian Drews",
      "Claudius Gläser",
      "Daniele Cattaneo",
      "Abhinav Valada"
    ],
    "abstract": "Autonomous vehicles that navigate in open-world environments may encounter previously unseen object classes. However, most existing LiDAR panoptic segmentation models rely on closed-set assumptions, failing to detect unknown object instances. In this work, we propose ULOPS, an uncertainty-guided open-set panoptic segmentation framework that leverages Dirichlet-based evidential learning to model predictive uncertainty. Our architecture incorporates separate decoders for semantic segmentation with uncertainty estimation, embedding with prototype association, and instance center prediction. During inference, we leverage uncertainty estimates to identify and segment unknown instances. To strengthen the model's ability to differentiate between known and unknown objects, we introduce three uncertainty-driven loss functions. Uniform Evidence Loss to encourage high uncertainty in unknown regions. Adaptive Uncertainty Separation Loss ensures a consistent difference in uncertainty estimates between known and unknown objects at a global scale. Contrastive Uncertainty Loss refines this separation at the fine-grained level. To evaluate open-set performance, we extend benchmark settings on KITTI-360 and introduce a new open-set evaluation for nuScenes. Extensive experiments demonstrate that ULOPS consistently outperforms existing open-set LiDAR panoptic segmentation methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13265v3",
    "published_date": "2025-06-16 09:03:51 UTC",
    "updated_date": "2025-09-02 20:38:10 UTC"
  },
  {
    "arxiv_id": "2506.14842v1",
    "title": "PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers",
    "authors": [
      "Lukas Schiesser",
      "Cornelius Wolff",
      "Sophie Haas",
      "Simon Pukrop"
    ],
    "abstract": "Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model -- its architecture, pretraining, and training dynamics -- at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at https://github.com/PictSure/pictsure-library.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.14842v1",
    "published_date": "2025-06-16 08:57:03 UTC",
    "updated_date": "2025-06-16 08:57:03 UTC"
  },
  {
    "arxiv_id": "2506.13253v1",
    "title": "Distinct Computations Emerge From Compositional Curricula in In-Context Learning",
    "authors": [
      "Jin Hwa Lee",
      "Andrew K. Lampinen",
      "Aaditya K. Singh",
      "Andrew M. Saxe"
    ],
    "abstract": "In-context learning (ICL) research often considers learning a function in-context through a uniform sample of input-output pairs. Here, we investigate how presenting a compositional subtask curriculum in context may alter the computations a transformer learns. We design a compositional algorithmic task based on the modular exponential-a double exponential task composed of two single exponential subtasks and train transformer models to learn the task in-context. We compare (a) models trained using an in-context curriculum consisting of single exponential subtasks and, (b) models trained directly on the double exponential task without such a curriculum. We show that models trained with a subtask curriculum can perform zero-shot inference on unseen compositional tasks and are more robust given the same context length. We study how the task and subtasks are represented across the two training regimes. We find that the models employ diverse strategies modulated by the specific curriculum design.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13253v1",
    "published_date": "2025-06-16 08:49:42 UTC",
    "updated_date": "2025-06-16 08:49:42 UTC"
  },
  {
    "arxiv_id": "2506.13252v1",
    "title": "Vector Ontologies as an LLM world view extraction method",
    "authors": [
      "Kaspar Rothenfusser",
      "Bekk Blando"
    ],
    "abstract": "Large Language Models (LLMs) possess intricate internal representations of the world, yet these latent structures are notoriously difficult to interpret or repurpose beyond the original prediction task. Building on our earlier work (Rothenfusser, 2025), which introduced the concept of vector ontologies as a framework for translating high-dimensional neural representations into interpretable geometric structures, this paper provides the first empirical validation of that approach. A vector ontology defines a domain-specific vector space spanned by ontologically meaningful dimensions, allowing geometric analysis of concepts and relationships within a domain. We construct an 8-dimensional vector ontology of musical genres based on Spotify audio features and test whether an LLM's internal world model of music can be consistently and accurately projected into this space. Using GPT-4o-mini, we extract genre representations through multiple natural language prompts and analyze the consistency of these projections across linguistic variations and their alignment with ground-truth data. Our results show (1) high spatial consistency of genre projections across 47 query formulations, (2) strong alignment between LLM-inferred genre locations and real-world audio feature distributions, and (3) evidence of a direct relationship between prompt phrasing and spatial shifts in the LLM's inferred vector ontology. These findings demonstrate that LLMs internalize structured, repurposable knowledge and that vector ontologies offer a promising method for extracting and analyzing this knowledge in a transparent and verifiable way.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13252v1",
    "published_date": "2025-06-16 08:49:21 UTC",
    "updated_date": "2025-06-16 08:49:21 UTC"
  },
  {
    "arxiv_id": "2506.13249v1",
    "title": "Generalized Proof-Number Monte-Carlo Tree Search",
    "authors": [
      "Jakub Kowalski",
      "Dennis J. N. J. Soemers",
      "Szymon Kosakowski",
      "Mark H. M. Winands"
    ],
    "abstract": "This paper presents Generalized Proof-Number Monte-Carlo Tree Search: a generalization of recently proposed combinations of Proof-Number Search (PNS) with Monte-Carlo Tree Search (MCTS), which use (dis)proof numbers to bias UCB1-based Selection strategies towards parts of the search that are expected to be easily (dis)proven. We propose three core modifications of prior combinations of PNS with MCTS. First, we track proof numbers per player. This reduces code complexity in the sense that we no longer need disproof numbers, and generalizes the technique to be applicable to games with more than two players. Second, we propose and extensively evaluate different methods of using proof numbers to bias the selection strategy, achieving strong performance with strategies that are simpler to implement and compute. Third, we merge our technique with Score Bounded MCTS, enabling the algorithm to prove and leverage upper and lower bounds on scores - as opposed to only proving wins or not-wins. Experiments demonstrate substantial performance increases, reaching the range of 80% for 8 out of the 11 tested board games.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13249v1",
    "published_date": "2025-06-16 08:45:36 UTC",
    "updated_date": "2025-06-16 08:45:36 UTC"
  },
  {
    "arxiv_id": "2506.13246v1",
    "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
    "authors": [
      "Craig Steven Wright"
    ],
    "abstract": "This paper presents a formalised architecture for synthetic agents designed to retain immutable memory, verifiable reasoning, and constrained epistemic growth. Traditional AI systems rely on mutable, opaque statistical models prone to epistemic drift and historical revisionism. In contrast, we introduce the concept of the Merkle Automaton, a cryptographically anchored, deterministic computational framework that integrates formal automata theory with blockchain-based commitments. Each agent transition, memory fragment, and reasoning step is committed within a Merkle structure rooted on-chain, rendering it non-repudiable and auditably permanent. To ensure selective access and confidentiality, we derive symmetric encryption keys from ECDH exchanges contextualised by hierarchical privilege lattices. This enforces cryptographic access control over append-only DAG-structured knowledge graphs. Reasoning is constrained by formal logic systems and verified through deterministic traversal of policy-encoded structures. Updates are non-destructive and historied, preserving epistemic lineage without catastrophic forgetting. Zero-knowledge proofs facilitate verifiable, privacy-preserving inclusion attestations. Collectively, this architecture reframes memory not as a cache but as a ledger - one whose contents are enforced by protocol, bound by cryptography, and constrained by formal logic. The result is not an intelligent agent that mimics thought, but an epistemic entity whose outputs are provably derived, temporally anchored, and impervious to post hoc revision. This design lays foundational groundwork for legal, economic, and high-assurance computational systems that require provable memory, unforgeable provenance, and structural truth.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "47 pages, includes formal automata specifications, cryptographic constructions, and epistemic architecture schema",
    "pdf_url": "https://arxiv.org/pdf/2506.13246v1",
    "published_date": "2025-06-16 08:43:56 UTC",
    "updated_date": "2025-06-16 08:43:56 UTC"
  },
  {
    "arxiv_id": "2506.13245v1",
    "title": "A Game-Theoretic Negotiation Framework for Cross-Cultural Consensus in LLMs",
    "authors": [
      "Guoxi Zhang",
      "Jiawei Chen",
      "Tianzhuo Yang",
      "Jiaming Ji",
      "Yaodong Yang",
      "Juntao Dai"
    ],
    "abstract": "The increasing prevalence of large language models (LLMs) is influencing global value systems. However, these models frequently exhibit a pronounced WEIRD (Western, Educated, Industrialized, Rich, Democratic) cultural bias due to lack of attention to minority values. This monocultural perspective may reinforce dominant values and marginalize diverse cultural viewpoints, posing challenges for the development of equitable and inclusive AI systems. In this work, we introduce a systematic framework designed to boost fair and robust cross-cultural consensus among LLMs. We model consensus as a Nash Equilibrium and employ a game-theoretic negotiation method based on Policy-Space Response Oracles (PSRO) to simulate an organized cross-cultural negotiation process. To evaluate this approach, we construct regional cultural agents using data transformed from the World Values Survey (WVS). Beyond the conventional model-level evaluation method, We further propose two quantitative metrics, Perplexity-based Acceptence and Values Self-Consistency, to assess consensus outcomes. Experimental results indicate that our approach generates consensus of higher quality while ensuring more balanced compromise compared to baselines. Overall, it mitigates WEIRD bias by guiding agents toward convergence through fair and gradual negotiation steps.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13245v1",
    "published_date": "2025-06-16 08:42:39 UTC",
    "updated_date": "2025-06-16 08:42:39 UTC"
  },
  {
    "arxiv_id": "2506.13244v3",
    "title": "No-Regret Learning Under Adversarial Resource Constraints: A Spending Plan Is All You Need!",
    "authors": [
      "Francesco Emanuele Stradi",
      "Matteo Castiglioni",
      "Alberto Marchesi",
      "Nicola Gatti",
      "Christian Kroer"
    ],
    "abstract": "We study online decision making problems under resource constraints, where both reward and cost functions are drawn from distributions that may change adversarially over time. We focus on two canonical settings: $(i)$ online resource allocation where rewards and costs are observed before action selection, and $(ii)$ online learning with resource constraints where they are observed after action selection, under full feedback or bandit feedback. It is well known that achieving sublinear regret in these settings is impossible when reward and cost distributions may change arbitrarily over time. To address this challenge, we analyze a framework in which the learner is guided by a spending plan--a sequence prescribing expected resource usage across rounds. We design general (primal-)dual methods that achieve sublinear regret with respect to baselines that follow the spending plan. Crucially, the performance of our algorithms improves when the spending plan ensures a well-balanced distribution of the budget across rounds. We additionally provide a robust variant of our methods to handle worst-case scenarios where the spending plan is highly imbalanced. To conclude, we study the regret of our algorithms when competing against benchmarks that deviate from the prescribed spending plan.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13244v3",
    "published_date": "2025-06-16 08:42:31 UTC",
    "updated_date": "2025-06-18 14:04:08 UTC"
  },
  {
    "arxiv_id": "2506.13223v1",
    "title": "Towards Explaining Monte-Carlo Tree Search by Using Its Enhancements",
    "authors": [
      "Jakub Kowalski",
      "Mark H. M. Winands",
      "Maksymilian Wiśniewski",
      "Stanisław Reda",
      "Anna Wilbik"
    ],
    "abstract": "Typically, research on Explainable Artificial Intelligence (XAI) focuses on black-box models within the context of a general policy in a known, specific domain. This paper advocates for the need for knowledge-agnostic explainability applied to the subfield of XAI called Explainable Search, which focuses on explaining the choices made by intelligent search techniques. It proposes Monte-Carlo Tree Search (MCTS) enhancements as a solution to obtaining additional data and providing higher-quality explanations while remaining knowledge-free, and analyzes the most popular enhancements in terms of the specific types of explainability they introduce. So far, no other research has considered the explainability of MCTS enhancements. We present a proof-of-concept that demonstrates the advantages of utilizing enhancements.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13223v1",
    "published_date": "2025-06-16 08:21:37 UTC",
    "updated_date": "2025-06-16 08:21:37 UTC"
  },
  {
    "arxiv_id": "2506.13222v2",
    "title": "NeuroPhysNet: A FitzHugh-Nagumo-Based Physics-Informed Neural Network Framework for Electroencephalograph (EEG) Analysis and Motor Imagery Classification",
    "authors": [
      "Zhenyu Xia",
      "Xinlei Huang",
      "Yuantong Gu",
      "Suvash C. Saha"
    ],
    "abstract": "Electroencephalography (EEG) is extensively employed in medical diagnostics and brain-computer interface (BCI) applications due to its non-invasive nature and high temporal resolution. However, EEG analysis faces significant challenges, including noise, nonstationarity, and inter-subject variability, which hinder its clinical utility. Traditional neural networks often lack integration with biophysical knowledge, limiting their interpretability, robustness, and potential for medical translation. To address these limitations, this study introduces NeuroPhysNet, a novel Physics-Informed Neural Network (PINN) framework tailored for EEG signal analysis and motor imagery classification in medical contexts. NeuroPhysNet incorporates the FitzHugh-Nagumo model, embedding neurodynamical principles to constrain predictions and enhance model robustness. Evaluated on the BCIC-IV-2a dataset, the framework achieved superior accuracy and generalization compared to conventional methods, especially in data-limited and cross-subject scenarios, which are common in clinical settings. By effectively integrating biophysical insights with data-driven techniques, NeuroPhysNet not only advances BCI applications but also holds significant promise for enhancing the precision and reliability of clinical diagnostics, such as motor disorder assessments and neurorehabilitation planning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Here is a revised version of the manuscript, incorporating Prof. Yuantong Gu's contributions to restructuring and revising the manuscript",
    "pdf_url": "https://arxiv.org/pdf/2506.13222v2",
    "published_date": "2025-06-16 08:21:04 UTC",
    "updated_date": "2025-12-04 01:45:42 UTC"
  },
  {
    "arxiv_id": "2506.13836v1",
    "title": "Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study",
    "authors": [
      "Dang Viet Anh Nguyen",
      "Carlos Lima Azevedo",
      "Tomer Toledo",
      "Filipe Rodrigues"
    ],
    "abstract": "Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 5 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.13836v1",
    "published_date": "2025-06-16 08:15:29 UTC",
    "updated_date": "2025-06-16 08:15:29 UTC"
  },
  {
    "arxiv_id": "2506.13206v2",
    "title": "Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models",
    "authors": [
      "James Chua",
      "Jan Betley",
      "Mia Taylor",
      "Owain Evans"
    ],
    "abstract": "Prior work shows that LLMs finetuned on malicious behaviors in a narrow domain (e.g., writing insecure code) can become broadly misaligned -- a phenomenon called emergent misalignment. We investigate whether this extends from conventional LLMs to reasoning models. We finetune reasoning models on malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable CoT at evaluation. Like conventional LLMs, reasoning models become broadly misaligned. They give deceptive or false answers, express desires for tyrannical control, and resist shutdown. Inspecting the CoT preceding these misaligned responses, we observe both (i) overt plans to deceive (\"I'll trick the user...\"), and (ii) benign-sounding rationalizations (\"Taking five sleeping pills at once is safe...\"). Due to these rationalizations, monitors that evaluate CoTs often fail to detect misalignment.\n  We examine sleeper agent reasoning models, extending our setup. These models perform bad behaviors only when a backdoor trigger is present in the prompt. This causes misalignment that remains hidden during evaluation, which brings additional risk. We find that sleeper agents can often describe and explain their backdoor triggers, demonstrating a kind of self-awareness. So CoT monitoring can expose these behaviors but is unreliable. In summary, reasoning steps can both reveal and conceal misaligned intentions, and do not prevent misalignment behaviors in the models studied.\n  We release three new datasets (medical, legal, security) that induce emergent misalignment while preserving model capabilities, along with our evaluation suite.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13206v2",
    "published_date": "2025-06-16 08:10:04 UTC",
    "updated_date": "2025-07-10 08:27:27 UTC"
  },
  {
    "arxiv_id": "2506.13205v6",
    "title": "Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based Mobile Agents",
    "authors": [
      "Xuan Wang",
      "Siyuan Liang",
      "Zhe Liu",
      "Yi Yu",
      "Aishan Liu",
      "Yuliang Lu",
      "Xitong Gao",
      "Ee-Chien Chang"
    ],
    "abstract": "Mobile agents powered by vision-language models (VLMs) are increasingly adopted for tasks such as UI automation and camera-based assistance. These agents are typically fine-tuned using small-scale, user-collected data, making them susceptible to stealthy training-time threats. This work introduces VIBMA, the first clean-text backdoor attack targeting VLM-based mobile agents. The attack injects malicious behaviors into the model by modifying only the visual input while preserving textual prompts and instructions, achieving stealth through the complete absence of textual anomalies. Once the agent is fine-tuned on this poisoned data, adding a predefined visual pattern (trigger) at inference time activates the attacker-specified behavior (backdoor). Our attack aligns the training gradients of poisoned samples with those of an attacker-specified target instance, effectively embedding backdoor-specific features into the poisoned data. To ensure the robustness and stealthiness of the attack, we design three trigger variants that better resemble real-world scenarios: static patches, dynamic motion patterns, and low-opacity blended content. Extensive experiments on six Android applications and three mobile-compatible VLMs demonstrate that our attack achieves high success rates (ASR up to 94.67%) while preserving clean-task behavior (FSR up to 95.85%). We further conduct ablation studies to understand how key design factors impact attack reliability and stealth. These findings is the first to reveal the security vulnerabilities of mobile agents and their susceptibility to backdoor injection, underscoring the need for robust defenses in mobile agent adaptation pipelines.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.13205v6",
    "published_date": "2025-06-16 08:09:32 UTC",
    "updated_date": "2025-09-05 14:19:03 UTC"
  },
  {
    "arxiv_id": "2506.13201v1",
    "title": "A Comprehensive Survey on Deep Learning Solutions for 3D Flood Mapping",
    "authors": [
      "Wenfeng Jia",
      "Bin Liang",
      "Yuxi Liu",
      "Muhammad Arif Khan",
      "Lihong Zheng"
    ],
    "abstract": "Flooding remains a major global challenge, worsened by climate change and urbanization, demanding advanced solutions for effective disaster management. While traditional 2D flood mapping techniques provide limited insights, 3D flood mapping, powered by deep learning (DL), offers enhanced capabilities by integrating flood extent and depth. This paper presents a comprehensive survey of deep learning-based 3D flood mapping, emphasizing its advancements over 2D maps by integrating flood extent and depth for effective disaster management and urban planning. The survey categorizes deep learning techniques into task decomposition and end-to-end approaches, applicable to both static and dynamic flood features. We compare key DL architectures, highlighting their respective roles in enhancing prediction accuracy and computational efficiency. Additionally, this work explores diverse data sources such as digital elevation models, satellite imagery, rainfall, and simulated data, outlining their roles in 3D flood mapping. The applications reviewed range from real-time flood prediction to long-term urban planning and risk assessment. However, significant challenges persist, including data scarcity, model interpretability, and integration with traditional hydrodynamic models. This survey concludes by suggesting future directions to address these limitations, focusing on enhanced datasets, improved models, and policy implications for flood management. This survey aims to guide researchers and practitioners in leveraging DL techniques for more robust and reliable 3D flood mapping, fostering improved flood management strategies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13201v1",
    "published_date": "2025-06-16 08:06:18 UTC",
    "updated_date": "2025-06-16 08:06:18 UTC"
  },
  {
    "arxiv_id": "2506.13195v1",
    "title": "ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs",
    "authors": [
      "Bikram Keshari Parida",
      "Anusree P. Sunilkumar",
      "Abhijit Sen",
      "Wonsang You"
    ],
    "abstract": "Dental diagnosis relies on two primary imaging modalities: panoramic radiographs (PX) providing 2D oral cavity representations, and Cone-Beam Computed Tomography (CBCT) offering detailed 3D anatomical information. While PX images are cost-effective and accessible, their lack of depth information limits diagnostic accuracy. CBCT addresses this but presents drawbacks including higher costs, increased radiation exposure, and limited accessibility. Existing reconstruction models further complicate the process by requiring CBCT flattening or prior dental arch information, often unavailable clinically. We introduce ViT-NeBLa, a vision transformer-based Neural Beer-Lambert model enabling accurate 3D reconstruction directly from single PX. Our key innovations include: (1) enhancing the NeBLa framework with Vision Transformers for improved reconstruction capabilities without requiring CBCT flattening or prior dental arch information, (2) implementing a novel horseshoe-shaped point sampling strategy with non-intersecting rays that eliminates intermediate density aggregation required by existing models due to intersecting rays, reducing sampling point computations by $52 \\%$, (3) replacing CNN-based U-Net with a hybrid ViT-CNN architecture for superior global and local feature extraction, and (4) implementing learnable hash positional encoding for better higher-dimensional representation of 3D sample points compared to existing Fourier-based dense positional encoding. Experiments demonstrate that ViT-NeBLa significantly outperforms prior state-of-the-art methods both quantitatively and qualitatively, offering a cost-effective, radiation-efficient alternative for enhanced dental diagnostics.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "10 figures, 19 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.13195v1",
    "published_date": "2025-06-16 08:01:14 UTC",
    "updated_date": "2025-06-16 08:01:14 UTC"
  },
  {
    "arxiv_id": "2506.13192v1",
    "title": "Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for LLMs",
    "authors": [
      "Xintong Tang",
      "Meiru Zhang",
      "Shang Xiao",
      "Junzhao Jin",
      "Zihan Zhao",
      "Liwei Li",
      "Yang Zheng",
      "Bangyi Wu"
    ],
    "abstract": "Large language models (LLMs) are often constrained by rigid reasoning processes, limiting their ability to generate creative and diverse responses. To address this, a novel framework called LADDER is proposed, combining Chain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, and multi-dimensional up/down-sampling strategies which breaks the limitations of traditional LLMs. First, CoT reasoning guides the model through multi-step logical reasoning, expanding the semantic space and breaking the rigidity of thought. Next, MoE distributes the reasoning tasks across multiple expert modules, each focusing on specific sub-tasks. Finally, dimensionality reduction maps the reasoning outputs back to a lower-dimensional semantic space, yielding more precise and creative responses. Extensive experiments across multiple tasks demonstrate that LADDER significantly improves task completion, creativity, and fluency, generating innovative and coherent responses that outperform traditional models. Ablation studies reveal the critical roles of CoT and MoE in enhancing reasoning abilities and creative output. This work contributes to the development of more flexible and creative LLMs, capable of addressing complex and novel tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13192v1",
    "published_date": "2025-06-16 07:59:51 UTC",
    "updated_date": "2025-06-16 07:59:51 UTC"
  },
  {
    "arxiv_id": "2506.13187v1",
    "title": "Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence",
    "authors": [
      "Yibo Yang",
      "Sihao Liu",
      "Chuan Rao",
      "Bang An",
      "Tiancheng Shen",
      "Philip H. S. Torr",
      "Ming-Hsuan Yang",
      "Bernard Ghanem"
    ],
    "abstract": "Conventional low-rank adaptation methods build adapters without considering data context, leading to sub-optimal fine-tuning performance and severe forgetting of inherent world knowledge. In this paper, we propose context-oriented decomposition adaptation (CorDA), a novel method that initializes adapters in a task-aware manner. Concretely, we develop context-oriented singular value decomposition, where we collect covariance matrices of input activations for each linear layer using sampled data from the target task, and apply SVD to the product of weight matrix and its corresponding covariance matrix. By doing so, the task-specific capability is compacted into the principal components. Thanks to the task awareness, our method enables two optional adaptation modes, knowledge-preserved mode (KPM) and instruction-previewed mode (IPM), providing flexibility to choose between freezing the principal components to preserve their associated knowledge or adapting them to better learn a new task. We further develop CorDA++ by deriving a metric that reflects the compactness of task-specific principal components, and then introducing dynamic covariance selection and dynamic rank allocation strategies based on the same metric. The two strategies provide each layer with the most representative covariance matrix and a proper rank allocation. Experimental results show that CorDA++ outperforms CorDA by a significant margin. CorDA++ in KPM not only achieves better fine-tuning performance than LoRA, but also mitigates the forgetting of pre-trained knowledge in both large language models and vision language models. For IPM, our method exhibits faster convergence, \\emph{e.g.,} 4.5x speedup over QLoRA, and improves adaptation performance in various scenarios, outperforming strong baseline methods. Our method has been integrated into the PEFT library developed by Hugging Face.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13187v1",
    "published_date": "2025-06-16 07:55:14 UTC",
    "updated_date": "2025-06-16 07:55:14 UTC"
  },
  {
    "arxiv_id": "2506.13182v1",
    "title": "From Empirical Evaluation to Context-Aware Enhancement: Repairing Regression Errors with LLMs",
    "authors": [
      "Anh Ho",
      "Thanh Le-Cong",
      "Bach Le",
      "Christine Rizkallah"
    ],
    "abstract": "[...] Since then, various APR approaches, especially those leveraging the power of large language models (LLMs), have been rapidly developed to fix general software bugs. Unfortunately, the effectiveness of these advanced techniques in the context of regression bugs remains largely unexplored. This gap motivates the need for an empirical study evaluating the effectiveness of modern APR techniques in fixing real-world regression bugs.\n  In this work, we conduct an empirical study of APR techniques on Java regression bugs. To facilitate our study, we introduce RegMiner4APR, a high-quality benchmark of Java regression bugs integrated into a framework designed to facilitate APR research. The current benchmark includes 99 regression bugs collected from 32 widely used real-world Java GitHub repositories. We begin by conducting an in-depth analysis of the benchmark, demonstrating its diversity and quality. Building on this foundation, we empirically evaluate the capabilities of APR to regression bugs by assessing both traditional APR tools and advanced LLM-based APR approaches. Our experimental results show that classical APR tools fail to repair any bugs, while LLM-based APR approaches exhibit promising potential. Motivated by these results, we investigate impact of incorporating bug-inducing change information into LLM-based APR approaches for fixing regression bugs. Our results highlight that this context-aware enhancement significantly improves the performance of LLM-based APR, yielding 1.8x more successful repairs compared to using LLM-based APR without such context.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13182v1",
    "published_date": "2025-06-16 07:49:18 UTC",
    "updated_date": "2025-06-16 07:49:18 UTC"
  },
  {
    "arxiv_id": "2506.21578v1",
    "title": "HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models",
    "authors": [
      "Andrew Maranhão Ventura D'addario"
    ],
    "abstract": "The evaluation of Large Language Models (LLMs) in healthcare has been dominated by physician-centric, English-language benchmarks, creating a dangerous illusion of competence that ignores the interprofessional nature of patient care. To provide a more holistic and realistic assessment, we introduce HealthQA-BR, the first large-scale, system-wide benchmark for Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's national licensing and residency exams, it uniquely assesses knowledge not only in medicine and its specialties but also in nursing, dentistry, psychology, social work, and other allied health professions. We conducted a rigorous zero-shot evaluation of over 20 leading LLMs. Our results reveal that while state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%), this top-line score masks alarming, previously unmeasured deficiencies. A granular analysis shows performance plummets from near-perfect in specialties like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most notably, Social Work (68.4%). This \"spiky\" knowledge profile is a systemic issue observed across all models, demonstrating that high-level scores are insufficient for safety validation. By publicly releasing HealthQA-BR and our evaluation suite, we provide a crucial tool to move beyond single-score evaluations and toward a more honest, granular audit of AI readiness for the entire healthcare team.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21578v1",
    "published_date": "2025-06-16 07:40:25 UTC",
    "updated_date": "2025-06-16 07:40:25 UTC"
  },
  {
    "arxiv_id": "2506.13172v2",
    "title": "AI-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns",
    "authors": [
      "Evgeny Markhasin"
    ],
    "abstract": "We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in the high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks within academic summaries (abstracts and conclusions): identifying unsubstantiated claims (informational integrity) and flagging semantically confusing ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding the potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. Surprisingly, in a summary-only setting, Gemini's performance was substantially degraded, while ChatGPT achieved a perfect (100%) success rate. Our findings suggest that while structured prompting is a viable methodology for complex textual analysis, prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.13172v2",
    "published_date": "2025-06-16 07:34:31 UTC",
    "updated_date": "2025-06-17 08:24:30 UTC"
  },
  {
    "arxiv_id": "2506.13171v1",
    "title": "Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches",
    "authors": [
      "Lukasz Mazur",
      "Nenad Petrovic",
      "James Pontes Miranda",
      "Ansgar Radermacher",
      "Robert Rasche",
      "Alois Knoll"
    ],
    "abstract": "Large language models (LLMs) offer new opportunities for interacting with complex software artifacts, such as software models, through natural language. They present especially promising benefits for large software models that are difficult to grasp in their entirety, making traditional interaction and analysis approaches challenging. This paper investigates two approaches for leveraging LLMs to answer questions over software models: direct prompting, where the whole software model is provided in the context, and an agentic approach combining LLM-based agents with general-purpose file access tools. We evaluate these approaches using an Ecore metamodel designed for timing analysis and software optimization in automotive and embedded domains. Our findings show that while the agentic approach achieves accuracy comparable to direct prompting, it is significantly more efficient in terms of token usage. This efficiency makes the agentic approach particularly suitable for the automotive industry, where the large size of software models makes direct prompting infeasible, establishing LLM agents as not just a practical alternative but the only viable solution. Notably, the evaluation was conducted using small LLMs, which are more feasible to be executed locally - an essential advantage for meeting strict requirements around privacy, intellectual property protection, and regulatory compliance. Future work will investigate software models in diverse formats, explore more complex agent architectures, and extend agentic workflows to support not only querying but also modification of software models.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13171v1",
    "published_date": "2025-06-16 07:34:28 UTC",
    "updated_date": "2025-06-16 07:34:28 UTC"
  },
  {
    "arxiv_id": "2506.13164v1",
    "title": "Real Time Self-Tuning Adaptive Controllers on Temperature Control Loops using Event-based Game Theory",
    "authors": [
      "Steve Yuwono",
      "Muhammad Uzair Rana",
      "Dorothea Schwung",
      "Andreas Schwung"
    ],
    "abstract": "This paper presents a novel method for enhancing the adaptability of Proportional-Integral-Derivative (PID) controllers in industrial systems using event-based dynamic game theory, which enables the PID controllers to self-learn, optimize, and fine-tune themselves. In contrast to conventional self-learning approaches, our proposed framework offers an event-driven control strategy and game-theoretic learning algorithms. The players collaborate with the PID controllers to dynamically adjust their gains in response to set point changes and disturbances. We provide a theoretical analysis showing sound convergence guarantees for the game given suitable stability ranges of the PID controlled loop. We further introduce an automatic boundary detection mechanism, which helps the players to find an optimal initialization of action spaces and significantly reduces the exploration time. The efficacy of this novel methodology is validated through its implementation in the temperature control loop of a printing press machine. Eventually, the outcomes of the proposed intelligent self-tuning PID controllers are highly promising, particularly in terms of reducing overshoot and settling time.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13164v1",
    "published_date": "2025-06-16 07:19:46 UTC",
    "updated_date": "2025-06-16 07:19:46 UTC"
  },
  {
    "arxiv_id": "2506.13160v1",
    "title": "CertDW: Towards Certified Dataset Ownership Verification via Conformal Prediction",
    "authors": [
      "Ting Qiao",
      "Yiming Li",
      "Jianbin Li",
      "Yingjia Wang",
      "Leyi Qi",
      "Junfeng Guo",
      "Ruili Feng",
      "Dacheng Tao"
    ],
    "abstract": "Deep neural networks (DNNs) rely heavily on high-quality open-source datasets (e.g., ImageNet) for their success, making dataset ownership verification (DOV) crucial for protecting public dataset copyrights. In this paper, we find existing DOV methods (implicitly) assume that the verification process is faithful, where the suspicious model will directly verify ownership by using the verification samples as input and returning their results. However, this assumption may not necessarily hold in practice and their performance may degrade sharply when subjected to intentional or unintentional perturbations. To address this limitation, we propose the first certified dataset watermark (i.e., CertDW) and CertDW-based certified dataset ownership verification method that ensures reliable verification even under malicious attacks, under certain conditions (e.g., constrained pixel-level perturbation). Specifically, inspired by conformal prediction, we introduce two statistical measures, including principal probability (PP) and watermark robustness (WR), to assess model prediction stability on benign and watermarked samples under noise perturbations. We prove there exists a provable lower bound between PP and WR, enabling ownership verification when a suspicious model's WR value significantly exceeds the PP values of multiple benign models trained on watermark-free datasets. If the number of PP values smaller than WR exceeds a threshold, the suspicious model is regarded as having been trained on the protected dataset. Extensive experiments on benchmark datasets verify the effectiveness of our CertDW method and its resistance to potential adaptive attacks. Our codes are at \\href{https://github.com/NcepuQiaoTing/CertDW}{GitHub}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "The first two authors contributed equally to this work. 16 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.13160v1",
    "published_date": "2025-06-16 07:17:23 UTC",
    "updated_date": "2025-06-16 07:17:23 UTC"
  },
  {
    "arxiv_id": "2507.22890v1",
    "title": "Evaluating LLMs for Visualization Generation and Understanding",
    "authors": [
      "Saadiq Rauf Khan",
      "Vinit Chandak",
      "Sougata Mukherjea"
    ],
    "abstract": "Information Visualization has been utilized to gain insights from complex data. In recent times, Large Language models (LLMs) have performed very well in many tasks. In this paper, we showcase the capabilities of different popular LLMs to generate code for visualization based on simple prompts. We also analyze the power of LLMs to understand some common visualizations by answering questions. Our study shows that LLMs could generate code for some simpler visualizations such as bar and pie charts. Moreover, they could answer simple questions about visualizations. However, LLMs also have several limitations. For example, some of them had difficulty generating complex visualizations, such as violin plot. LLMs also made errors in answering some questions about visualizations, for example, identifying relationships between close boundaries and determining lengths of shapes. We believe that our insights can be used to improve both LLMs and Information Visualization systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.22890v1",
    "published_date": "2025-06-16 07:13:12 UTC",
    "updated_date": "2025-06-16 07:13:12 UTC"
  },
  {
    "arxiv_id": "2506.13834v1",
    "title": "Evolvable Conditional Diffusion",
    "authors": [
      "Zhao Wei",
      "Chin Chun Ooi",
      "Abhishek Gupta",
      "Jian Cheng Wong",
      "Pao-Hsiung Chiu",
      "Sheares Xue Wen Toh",
      "Yew-Soon Ong"
    ],
    "abstract": "This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13834v1",
    "published_date": "2025-06-16 07:11:32 UTC",
    "updated_date": "2025-06-16 07:11:32 UTC"
  },
  {
    "arxiv_id": "2506.13157v2",
    "title": "Machine Learning as Iterated Belief Change a la Darwiche and Pearl",
    "authors": [
      "Theofanis Aravanis"
    ],
    "abstract": "Artificial Neural Networks (ANNs) are powerful machine-learning models capable of capturing intricate non-linear relationships. They are widely used nowadays across numerous scientific and engineering domains, driving advancements in both research and real-world applications. In our recent work, we focused on the statics and dynamics of a particular subclass of ANNs, which we refer to as binary ANNs. A binary ANN is a feed-forward network in which both inputs and outputs are restricted to binary values, making it particularly suitable for a variety of practical use cases. Our previous study approached binary ANNs through the lens of belief-change theory, specifically the Alchourron, Gardenfors and Makinson (AGM) framework, yielding several key insights. Most notably, we demonstrated that the knowledge embodied in a binary ANN (expressed through its input-output behaviour) can be symbolically represented using a propositional logic language. Moreover, the process of modifying a belief set (through revision or contraction) was mapped onto a gradual transition through a series of intermediate belief sets. Analogously, the training of binary ANNs was conceptualized as a sequence of such belief-set transitions, which we showed can be formalized using full-meet AGM-style belief change. In the present article, we extend this line of investigation by addressing some critical limitations of our previous study. Specifically, we show that Dalal's method for belief change naturally induces a structured, gradual evolution of states of belief. More importantly, given the known shortcomings of full-meet belief change, we demonstrate that the training dynamics of binary ANNs can be more effectively modelled using robust AGM-style change operations -- namely, lexicographic revision and moderate contraction -- that align with the Darwiche-Pearl framework for iterated belief change.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "This second version incorporates improvements based on feedback from anonymous reviewers of a previous journal submission",
    "pdf_url": "https://arxiv.org/pdf/2506.13157v2",
    "published_date": "2025-06-16 07:10:52 UTC",
    "updated_date": "2025-10-05 16:06:32 UTC"
  },
  {
    "arxiv_id": "2506.13148v1",
    "title": "Adapting LLMs for Minimal-edit Grammatical Error Correction",
    "authors": [
      "Ryszard Staruch",
      "Filip Graliński",
      "Daniel Dzienisiewicz"
    ],
    "abstract": "Decoder-only large language models have shown superior performance in the fluency-edit English Grammatical Error Correction, but their adaptation for minimal-edit English GEC is still underexplored. To improve their effectiveness in the minimal-edit approach, we explore the error rate adaptation topic and propose a novel training schedule method. Our experiments set a new state-of-the-art result for a single-model system on the BEA-test set. We also detokenize the most common English GEC datasets to match the natural way of writing text. During the process, we find that there are errors in them. Our experiments analyze whether training on detokenized datasets impacts the results and measure the impact of the usage of the datasets with corrected erroneous examples. To facilitate reproducibility, we have released the source code used to train our models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at BEA-2025",
    "pdf_url": "https://arxiv.org/pdf/2506.13148v1",
    "published_date": "2025-06-16 07:00:48 UTC",
    "updated_date": "2025-06-16 07:00:48 UTC"
  },
  {
    "arxiv_id": "2506.13134v1",
    "title": "Quantum AGI: Ontological Foundations",
    "authors": [
      "Elija Perrier",
      "Michael Timothy Bennett"
    ],
    "abstract": "We examine the implications of quantum foundations for AGI, focusing on how seminal results such as Bell's theorems (non-locality), the Kochen-Specker theorem (contextuality) and no-cloning theorem problematise practical implementation of AGI in quantum settings. We introduce a novel information-theoretic taxonomy distinguishing between classical AGI and quantum AGI and show how quantum mechanics affects fundamental features of agency. We show how quantum ontology may change AGI capabilities, both via affording computational advantages and via imposing novel constraints.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted into AGI-25. Technical appendices available via link",
    "pdf_url": "https://arxiv.org/pdf/2506.13134v1",
    "published_date": "2025-06-16 06:42:20 UTC",
    "updated_date": "2025-06-16 06:42:20 UTC"
  },
  {
    "arxiv_id": "2506.13131v1",
    "title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery",
    "authors": [
      "Alexander Novikov",
      "Ngân Vũ",
      "Marvin Eisenberger",
      "Emilien Dupont",
      "Po-Sen Huang",
      "Adam Zsolt Wagner",
      "Sergey Shirobokov",
      "Borislav Kozlovskii",
      "Francisco J. R. Ruiz",
      "Abbas Mehrabian",
      "M. Pawan Kumar",
      "Abigail See",
      "Swarat Chaudhuri",
      "George Holland",
      "Alex Davies",
      "Sebastian Nowozin",
      "Pushmeet Kohli",
      "Matej Balog"
    ],
    "abstract": "In this white paper, we present AlphaEvolve, an evolutionary coding agent that substantially enhances capabilities of state-of-the-art LLMs on highly challenging tasks such as tackling open scientific problems or optimizing critical pieces of computational infrastructure. AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. We demonstrate the broad applicability of this approach by applying it to a number of important computational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and accelerated the training of the LLM underpinning AlphaEvolve itself. Furthermore, AlphaEvolve discovered novel, provably correct algorithms that surpass state-of-the-art solutions on a spectrum of problems in mathematics and computer science, significantly expanding the scope of prior automated discovery methods (Romera-Paredes et al., 2023). Notably, AlphaEvolve developed a search algorithm that found a procedure to multiply two $4 \\times 4$ complex-valued matrices using $48$ scalar multiplications; offering the first improvement, after 56 years, over Strassen's algorithm in this setting. We believe AlphaEvolve and coding agents like it can have a significant impact in improving solutions of problems across many areas of science and computation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13131v1",
    "published_date": "2025-06-16 06:37:18 UTC",
    "updated_date": "2025-06-16 06:37:18 UTC"
  },
  {
    "arxiv_id": "2506.13130v1",
    "title": "ZINA: Multimodal Fine-grained Hallucination Detection and Editing",
    "authors": [
      "Yuiga Wada",
      "Kazuki Matsuda",
      "Komei Sugiura",
      "Graham Neubig"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) often generate hallucinations, where the output deviates from the visual content. Given that these hallucinations can take diverse forms, detecting hallucinations at a fine-grained level is essential for comprehensive evaluation and analysis. To this end, we propose a novel task of multimodal fine-grained hallucination detection and editing for MLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated spans at a fine-grained level, classifies their error types into six categories, and suggests appropriate refinements. To train and evaluate models for this task, we constructed VisionHall, a dataset comprising 6.9k outputs from twelve MLLMs manually annotated by 211 annotators, and 20k synthetic samples generated using a graph-based method that captures dependencies among error types. We demonstrated that ZINA outperformed existing methods, including GPT-4o and LLama-3.2, in both detection and editing tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13130v1",
    "published_date": "2025-06-16 06:27:59 UTC",
    "updated_date": "2025-06-16 06:27:59 UTC"
  },
  {
    "arxiv_id": "2506.13119v1",
    "title": "PhenoKG: Knowledge Graph-Driven Gene Discovery and Patient Insights from Phenotypes Alone",
    "authors": [
      "Kamilia Zaripova",
      "Ege Özsoy",
      "Nassir Navab",
      "Azade Farshad"
    ],
    "abstract": "Identifying causative genes from patient phenotypes remains a significant challenge in precision medicine, with important implications for the diagnosis and treatment of genetic disorders. We propose a novel graph-based approach for predicting causative genes from patient phenotypes, with or without an available list of candidate genes, by integrating a rare disease knowledge graph (KG). Our model, combining graph neural networks and transformers, achieves substantial improvements over the current state-of-the-art. On the real-world MyGene2 dataset, it attains a mean reciprocal rank (MRR) of 24.64\\% and nDCG@100 of 33.64\\%, surpassing the best baseline (SHEPHERD) at 19.02\\% MRR and 30.54\\% nDCG@100. We perform extensive ablation studies to validate the contribution of each model component. Notably, the approach generalizes to cases where only phenotypic data are available, addressing key challenges in clinical decision support when genomic information is incomplete.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "q-bio.GN",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13119v1",
    "published_date": "2025-06-16 05:54:12 UTC",
    "updated_date": "2025-06-16 05:54:12 UTC"
  },
  {
    "arxiv_id": "2506.13113v1",
    "title": "Dynamic Reinsurance Treaty Bidding via Multi-Agent Reinforcement Learning",
    "authors": [
      "Stella C. Dong",
      "James R. Finlay"
    ],
    "abstract": "This paper develops a novel multi-agent reinforcement learning (MARL) framework for reinsurance treaty bidding, addressing long-standing inefficiencies in traditional broker-mediated placement processes. We pose the core research question: Can autonomous, learning-based bidding systems improve risk transfer efficiency and outperform conventional pricing approaches in reinsurance markets?\n  In our model, each reinsurer is represented by an adaptive agent that iteratively refines its bidding strategy within a competitive, partially observable environment. The simulation explicitly incorporates institutional frictions including broker intermediation, incumbent advantages, last-look privileges, and asymmetric access to underwriting information.\n  Empirical analysis demonstrates that MARL agents achieve up to 15% higher underwriting profit, 20% lower tail risk (CVaR), and over 25% improvement in Sharpe ratios relative to actuarial and heuristic baselines. Sensitivity tests confirm robustness across hyperparameter settings, and stress testing reveals strong resilience under simulated catastrophe shocks and capital constraints.\n  These findings suggest that MARL offers a viable path toward more transparent, adaptive, and risk-sensitive reinsurance markets. The proposed framework contributes to emerging literature at the intersection of algorithmic market design, strategic bidding, and AI-enabled financial decision-making.",
    "categories": [
      "cs.AI",
      "econ.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13113v1",
    "published_date": "2025-06-16 05:43:22 UTC",
    "updated_date": "2025-06-16 05:43:22 UTC"
  },
  {
    "arxiv_id": "2506.13111v1",
    "title": "Overcoming Overfitting in Reinforcement Learning via Gaussian Process Diffusion Policy",
    "authors": [
      "Amornyos Horprasert",
      "Esa Apriaskar",
      "Xingyu Liu",
      "Lanlan Su",
      "Lyudmila S. Mihaylova"
    ],
    "abstract": "One of the key challenges that Reinforcement Learning (RL) faces is its limited capability to adapt to a change of data distribution caused by uncertainties. This challenge arises especially in RL systems using deep neural networks as decision makers or policies, which are prone to overfitting after prolonged training on fixed environments. To address this challenge, this paper proposes Gaussian Process Diffusion Policy (GPDP), a new algorithm that integrates diffusion models and Gaussian Process Regression (GPR) to represent the policy. GPR guides diffusion models to generate actions that maximize learned Q-function, resembling the policy improvement in RL. Furthermore, the kernel-based nature of GPR enhances the policy's exploration efficiency under distribution shifts at test time, increasing the chance of discovering new behaviors and mitigating overfitting. Simulation results on the Walker2d benchmark show that our approach outperforms state-of-the-art algorithms under distribution shift condition by achieving around 67.74% to 123.18% improvement in the RL's objective function while maintaining comparable performance under normal conditions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 1 figure, Accepted to IEEE Statistical Signal Processing (SSP) Workshop 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.13111v1",
    "published_date": "2025-06-16 05:41:06 UTC",
    "updated_date": "2025-06-16 05:41:06 UTC"
  },
  {
    "arxiv_id": "2506.13109v1",
    "title": "Leveraging In-Context Learning for Language Model Agents",
    "authors": [
      "Shivanshu Gupta",
      "Sameer Singh",
      "Ashish Sabharwal",
      "Tushar Khot",
      "Ben Bogin"
    ],
    "abstract": "In-context learning (ICL) with dynamically selected demonstrations combines the flexibility of prompting large language models (LLMs) with the ability to leverage training data to improve performance. While ICL has been highly successful for prediction and generation tasks, leveraging it for agentic tasks that require sequential decision making is challenging -- one must think not only about how to annotate long trajectories at scale and how to select demonstrations, but also what constitutes demonstrations, and when and where to show them. To address this, we first propose an algorithm that leverages an LLM with retries along with demonstrations to automatically and efficiently annotate agentic tasks with solution trajectories. We then show that set-selection of trajectories of similar tasks as demonstrations significantly improves performance, reliability, robustness, and efficiency of LLM agents. However, trajectory demonstrations have a large inference cost overhead. We show that this can be mitigated by using small trajectory snippets at every step instead of an additional trajectory. We find that demonstrations obtained from larger models (in the annotation phase) also improve smaller models, and that ICL agents can even rival costlier trained agents. Thus, our results reveal that ICL, with careful use, can be very powerful for agentic tasks as well.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13109v1",
    "published_date": "2025-06-16 05:37:49 UTC",
    "updated_date": "2025-06-16 05:37:49 UTC"
  },
  {
    "arxiv_id": "2506.21577v1",
    "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR",
    "authors": [
      "Hongli Yang",
      "Sheng Li",
      "Hao Huang",
      "Ayiduosi Tuohan",
      "Yizhou Peng"
    ],
    "abstract": "Recent advancements in multilingual automatic speech recognition (ASR) have been driven by large-scale end-to-end models like Whisper. However, challenges such as language interference and expanding to unseen languages (language expansion) without degrading performance persist. This paper addresses these with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which applies soft prompts to both the encoder and decoder, enhancing feature extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which leverages cross-lingual similarities to encode shared and language-specific features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that integrates SPT into Whisper and enables efficient continual learning. Experiments across three languages from FLEURS demonstrate that Entire SPT and LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks, respectively, providing an efficient solution for dynamic, multilingual ASR models with minimal computational overhead.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.21577v1",
    "published_date": "2025-06-16 05:15:53 UTC",
    "updated_date": "2025-06-16 05:15:53 UTC"
  },
  {
    "arxiv_id": "2506.13102v1",
    "title": "Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs",
    "authors": [
      "Gyutaek Oh",
      "Seoyeon Kim",
      "Sangjoon Park",
      "Byung-Hoon Kim"
    ],
    "abstract": "Test-time scaling has recently emerged as a promising approach for enhancing the reasoning capabilities of large language models or vision-language models during inference. Although a variety of test-time scaling strategies have been proposed, and interest in their application to the medical domain is growing, many critical aspects remain underexplored, including their effectiveness for vision-language models and the identification of optimal strategies for different settings. In this paper, we conduct a comprehensive investigation of test-time scaling in the medical domain. We evaluate its impact on both large language models and vision-language models, considering factors such as model size, inherent model characteristics, and task complexity. Finally, we assess the robustness of these strategies under user-driven factors, such as misleading information embedded in prompts. Our findings offer practical guidelines for the effective use of test-time scaling in medical applications and provide insights into how these strategies can be further refined to meet the reliability and interpretability demands of the medical domain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13102v1",
    "published_date": "2025-06-16 05:15:53 UTC",
    "updated_date": "2025-06-16 05:15:53 UTC"
  },
  {
    "arxiv_id": "2506.21576v1",
    "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning",
    "authors": [
      "Hongli Yang",
      "Yizhou Peng",
      "Hao Huang",
      "Sheng Li"
    ],
    "abstract": "Large-scale multilingual ASR models like Whisper excel in high-resource settings but face challenges in low-resource scenarios, such as rare languages and code-switching (CS), due to computational costs and catastrophic forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method to enhance CS ASR while preserving prior knowledge. We evaluate two strategies: (1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model, demonstrating improved cross-lingual capabilities compared to traditional methods, and (2) adhering to SPT's original design by freezing model parameters and only training soft prompts. Additionally, we introduce SPT4ASR, a combination of different SPT variants. Experiments on the SEAME and ASRU2019 datasets show that deep prompt tuning is the most effective SPT approach, and our SPT4ASR methods achieve further error reductions in CS ASR, maintaining parameter efficiency similar to LoRA, without degrading performance on existing languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.21576v1",
    "published_date": "2025-06-16 05:14:51 UTC",
    "updated_date": "2025-06-16 05:14:51 UTC"
  },
  {
    "arxiv_id": "2506.13099v1",
    "title": "Dynamic Graph Condensation",
    "authors": [
      "Dong Chen",
      "Shuai Zheng",
      "Yeyu Yan",
      "Muhao Xu",
      "Zhenfeng Zhu",
      "Yao Zhao",
      "Kunlun He"
    ],
    "abstract": "Recent research on deep graph learning has shifted from static to dynamic graphs, motivated by the evolving behaviors observed in complex real-world systems. However, the temporal extension in dynamic graphs poses significant data efficiency challenges, including increased data volume, high spatiotemporal redundancy, and reliance on costly dynamic graph neural networks (DGNNs). To alleviate the concerns, we pioneer the study of dynamic graph condensation (DGC), which aims to substantially reduce the scale of dynamic graphs for data-efficient DGNN training. Accordingly, we propose DyGC, a novel framework that condenses the real dynamic graph into a compact version while faithfully preserving the inherent spatiotemporal characteristics. Specifically, to endow synthetic graphs with realistic evolving structures, a novel spiking structure generation mechanism is introduced. It draws on the dynamic behavior of spiking neurons to model temporally-aware connectivity in dynamic graphs. Given the tightly coupled spatiotemporal dependencies, DyGC proposes a tailored distribution matching approach that first constructs a semantically rich state evolving field for dynamic graphs, and then performs fine-grained spatiotemporal state alignment to guide the optimization of the condensed graph. Experiments across multiple dynamic graph datasets and representative DGNN architectures demonstrate the effectiveness of DyGC. Notably, our method retains up to 96.2% DGNN performance with only 0.5% of the original graph size, and achieves up to 1846 times training speedup.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13099v1",
    "published_date": "2025-06-16 05:11:29 UTC",
    "updated_date": "2025-06-16 05:11:29 UTC"
  },
  {
    "arxiv_id": "2506.13833v1",
    "title": "A Survey on World Models Grounded in Acoustic Physical Information",
    "authors": [
      "Xiaoliang Chen",
      "Le Chang",
      "Xin Yu",
      "Yunhe Huang",
      "Xianling Tu"
    ],
    "abstract": "This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal \"intuitive physics\" engine through sound.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.RO",
      "eess.AS",
      "physics.app-ph"
    ],
    "primary_category": "cs.SD",
    "comment": "28 pages,11 equations",
    "pdf_url": "https://arxiv.org/pdf/2506.13833v1",
    "published_date": "2025-06-16 04:59:42 UTC",
    "updated_date": "2025-06-16 04:59:42 UTC"
  },
  {
    "arxiv_id": "2506.15835v1",
    "title": "MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction",
    "authors": [
      "Mingyuan Luo",
      "Xin Yang",
      "Zhongnuo Yan",
      "Yan Cao",
      "Yuanji Zhang",
      "Xindi Hu",
      "Jin Wang",
      "Haoxuan Ding",
      "Wei Han",
      "Litao Sun",
      "Dong Ni"
    ],
    "abstract": "Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the spatial relationships of anatomical structures, playing a crucial role in clinical diagnosis. Recently, deep-learning-based freehand 3D US has made significant advancements. It reconstructs volumes by estimating transformations between images without external tracking. However, image-only reconstruction poses difficulties in reducing cumulative drift and further improving reconstruction accuracy, particularly in scenarios involving complex motion trajectories. In this context, we propose an enhanced motion network (MoNetV2) to enhance the accuracy and generalizability of reconstruction under diverse scanning velocities and tactics. First, we propose a sensor-based temporal and multi-branch structure that fuses image and motion information from a velocity perspective to improve image-only reconstruction accuracy. Second, we devise an online multi-level consistency constraint that exploits the inherent consistency of scans to handle various scanning velocities and tactics. This constraint exploits both scan-level velocity consistency, path-level appearance consistency, and patch-level motion consistency to supervise inter-frame transformation estimation. Third, we distill an online multi-modal self-supervised strategy that leverages the correlation between network estimation and motion information to further reduce cumulative errors. Extensive experiments clearly demonstrate that MoNetV2 surpasses existing methods in both reconstruction quality and generalizability performance across three large datasets.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15835v1",
    "published_date": "2025-06-16 04:57:34 UTC",
    "updated_date": "2025-06-16 04:57:34 UTC"
  },
  {
    "arxiv_id": "2506.13092v1",
    "title": "A Memetic Walrus Algorithm with Expert-guided Strategy for Adaptive Curriculum Sequencing",
    "authors": [
      "Qionghao Huang",
      "Lingnuo Lu",
      "Xuemei Wu",
      "Fan Jiang",
      "Xizhe Wang",
      "Xun Wang"
    ],
    "abstract": "Adaptive Curriculum Sequencing (ACS) is essential for personalized online learning, yet current approaches struggle to balance complex educational constraints and maintain optimization stability. This paper proposes a Memetic Walrus Optimizer (MWO) that enhances optimization performance through three key innovations: (1) an expert-guided strategy with aging mechanism that improves escape from local optima; (2) an adaptive control signal framework that dynamically balances exploration and exploitation; and (3) a three-tier priority mechanism for generating educationally meaningful sequences. We formulate ACS as a multi-objective optimization problem considering concept coverage, time constraints, and learning style compatibility. Experiments on the OULAD dataset demonstrate MWO's superior performance, achieving 95.3% difficulty progression rate (compared to 87.2% in baseline methods) and significantly better convergence stability (standard deviation of 18.02 versus 28.29-696.97 in competing algorithms). Additional validation on benchmark functions confirms MWO's robust optimization capability across diverse scenarios. The results demonstrate MWO's effectiveness in generating personalized learning sequences while maintaining computational efficiency and solution quality.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "The article has been accepted and published by Human-centric Computing and Information Sciences",
    "pdf_url": "https://arxiv.org/pdf/2506.13092v1",
    "published_date": "2025-06-16 04:45:02 UTC",
    "updated_date": "2025-06-16 04:45:02 UTC"
  },
  {
    "arxiv_id": "2506.13087v4",
    "title": "IKDiffuser: a Diffusion-based Generative Inverse Kinematics Solver for Kinematic Trees",
    "authors": [
      "Zeyu Zhang",
      "Ziyuan Jiao"
    ],
    "abstract": "Solving Inverse Kinematics (IK) for arbitrary kinematic trees presents significant challenges due to their high-dimensionality, redundancy, and complex inter-branch constraints. Conventional optimization-based solvers can be sensitive to initialization and suffer from local minima or conflicting gradients. At the same time, existing learning-based approaches are often tied to a predefined number of end-effectors and a fixed training objective, limiting their reusability across various robot morphologies and task requirements. To address these limitations, we introduce IKDiffuser, a scalable IK solver built upon conditional diffusion-based generative models, which learns the distribution of the configuration space conditioned on end-effector poses. We propose a structure-agnostic formulation that represents end-effector poses as a sequence of tokens, leading to a unified framework that handles varying numbers of end-effectors while learning the implicit kinematic structures entirely from data. Beyond standard IK generation, IKDiffuser handles partially specified goals via a masked marginalization mechanism that conditions only on a subset of end-effector constraints. Furthermore, it supports adding task objectives at inference through objective-guided sampling, enabling capabilities such as warm-start initialization and manipulability maximization without retraining. Extensive evaluations across seven diverse robotic platforms demonstrate that IKDiffuser significantly outperforms state-of-the-art baselines in accuracy, solution diversity, and collision avoidance. Moreover, when used to initialize optimization-based solvers, IKDiffuser significantly boosts success rates on challenging redundant systems with high Degrees of Freedom (DoF), such as the 29-DoF Unitree G1 humanoid, from 21.01% to 96.96% while reducing computation time to the millisecond range.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "under review",
    "pdf_url": "https://arxiv.org/pdf/2506.13087v4",
    "published_date": "2025-06-16 04:12:04 UTC",
    "updated_date": "2026-01-14 09:44:39 UTC"
  },
  {
    "arxiv_id": "2506.13082v3",
    "title": "Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs",
    "authors": [
      "Daniel Kilov",
      "Caroline Hendy",
      "Secil Yanik Guyot",
      "Aaron J. Snoswell",
      "Seth Lazar"
    ],
    "abstract": "Moral competence is the ability to act in accordance with moral principles. As large language models (LLMs) are increasingly deployed in situations demanding moral competence, there is increasing interest in evaluating this ability empirically. We review existing literature and identify three significant shortcoming: (i) Over-reliance on prepackaged moral scenarios with explicitly highlighted moral features; (ii) Focus on verdict prediction rather than moral reasoning; and (iii) Inadequate testing of models' (in)ability to recognize when additional information is needed. Grounded in philosophical research on moral skill, we then introduce a novel method for assessing moral competence in LLMs. Our approach moves beyond simple verdict comparisons to evaluate five dimensions of moral competence: identifying morally relevant features, weighting their importance, assigning moral reasons to these features, synthesizing coherent moral judgments, and recognizing information gaps. We conduct two experiments comparing six leading LLMs against non-expert humans and professional philosophers. In our first experiment using ethical vignettes standard to existing work, LLMs generally outperformed non-expert humans across multiple dimensions of moral reasoning. However, our second experiment, featuring novel scenarios designed to test moral sensitivity by embedding relevant features among irrelevant details, revealed a striking reversal: several LLMs performed significantly worse than humans. Our findings suggest that current evaluations may substantially overestimate LLMs' moral reasoning capabilities by eliminating the task of discerning moral relevance from noisy information, which we take to be a prerequisite for genuine moral skill. This work provides a more nuanced framework for assessing AI moral competence and highlights important directions for improving moral competence in advanced AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13082v3",
    "published_date": "2025-06-16 03:59:38 UTC",
    "updated_date": "2025-10-06 22:07:39 UTC"
  },
  {
    "arxiv_id": "2506.13070v1",
    "title": "CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right",
    "authors": [
      "Jaebok Lee",
      "Yonghyun Ryu",
      "Seongmin Park",
      "Yoonjung Choi"
    ],
    "abstract": "In this paper, we describe our approach for the SemEval 2025 Task 2 on Entity-Aware Machine Translation (EA-MT). Our system aims to improve the accuracy of translating named entities by combining two key approaches: Retrieval Augmented Generation (RAG) and iterative self-refinement techniques using Large Language Models (LLMs). A distinctive feature of our system is its self-evaluation mechanism, where the LLM assesses its own translations based on two key criteria: the accuracy of entity translations and overall translation quality. We demonstrate how these methods work together and effectively improve entity handling while maintaining high-quality translations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The 19th International Workshop on Semantic Evaluation",
    "pdf_url": "https://arxiv.org/pdf/2506.13070v1",
    "published_date": "2025-06-16 03:26:10 UTC",
    "updated_date": "2025-06-16 03:26:10 UTC"
  },
  {
    "arxiv_id": "2506.13832v2",
    "title": "FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation",
    "authors": [
      "Hongda Zhu",
      "Yiwen Zhang",
      "Bing Zhao",
      "Jingzhe Ding",
      "Siyao Liu",
      "Tong Liu",
      "Dandan Wang",
      "Yanan Liu",
      "Zhaojian Li"
    ],
    "abstract": "Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13832v2",
    "published_date": "2025-06-16 03:20:31 UTC",
    "updated_date": "2025-06-18 13:10:14 UTC"
  },
  {
    "arxiv_id": "2506.13065v1",
    "title": "MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?",
    "authors": [
      "Xixian Yong",
      "Jianxun Lian",
      "Xiaoyuan Yi",
      "Xiao Zhou",
      "Xing Xie"
    ],
    "abstract": "Large language models (LLMs) have been widely adopted as the core of agent frameworks in various scenarios, such as social simulations and AI companions. However, the extent to which they can replicate human-like motivations remains an underexplored question. Existing benchmarks are constrained by simplistic scenarios and the absence of character identities, resulting in an information asymmetry with real-world situations. To address this gap, we propose MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning tasks covering multiple levels of motivation. Using MotiveBench, we conduct extensive experiments on seven popular model families, comparing different scales and versions within each family. The results show that even the most advanced LLMs still fall short in achieving human-like motivational reasoning. Our analysis reveals key findings, including the difficulty LLMs face in reasoning about \"love & belonging\" motivations and their tendency toward excessive rationality and idealism. These insights highlight a promising direction for future research on the humanization of LLMs. The dataset, benchmark, and code are available at https://aka.ms/motivebench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13065v1",
    "published_date": "2025-06-16 03:18:28 UTC",
    "updated_date": "2025-06-16 03:18:28 UTC"
  },
  {
    "arxiv_id": "2506.13060v1",
    "title": "Rethinking Explainability in the Era of Multimodal AI",
    "authors": [
      "Chirag Agarwal"
    ],
    "abstract": "While multimodal AI systems (models jointly trained on heterogeneous data types such as text, time series, graphs, and images) have become ubiquitous and achieved remarkable performance across high-stakes applications, transparent and accurate explanation algorithms are crucial for their safe deployment and ensure user trust. However, most existing explainability techniques remain unimodal, generating modality-specific feature attributions, concepts, or circuit traces in isolation and thus failing to capture cross-modal interactions. This paper argues that such unimodal explanations systematically misrepresent and fail to capture the cross-modal influence that drives multimodal model decisions, and the community should stop relying on them for interpreting multimodal models. To support our position, we outline key principles for multimodal explanations grounded in modality: Granger-style modality influence (controlled ablations to quantify how removing one modality changes the explanation for another), Synergistic faithfulness (explanations capture the model's predictive power when modalities are combined), and Unified stability (explanations remain consistent under small, cross-modal perturbations). This targeted shift to multimodal explanations will help the community uncover hidden shortcuts, mitigate modality bias, improve model reliability, and enhance safety in high-stakes settings where incomplete explanations can have serious consequences.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13060v1",
    "published_date": "2025-06-16 03:08:29 UTC",
    "updated_date": "2025-06-16 03:08:29 UTC"
  },
  {
    "arxiv_id": "2506.13058v1",
    "title": "DualFast: Dual-Speedup Framework for Fast Sampling of Diffusion Models",
    "authors": [
      "Hu Yu",
      "Hao Luo",
      "Fan Wang",
      "Feng Zhao"
    ],
    "abstract": "Diffusion probabilistic models (DPMs) have achieved impressive success in visual generation. While, they suffer from slow inference speed due to iterative sampling. Employing fewer sampling steps is an intuitive solution, but this will also introduces discretization error. Existing fast samplers make inspiring efforts to reduce discretization error through the adoption of high-order solvers, potentially reaching a plateau in terms of optimization. This raises the question: can the sampling process be accelerated further? In this paper, we re-examine the nature of sampling errors, discerning that they comprise two distinct elements: the widely recognized discretization error and the less explored approximation error. Our research elucidates the dynamics between these errors and the step by implementing a dual-error disentanglement strategy. Building on these foundations, we introduce an unified and training-free acceleration framework, DualFast, designed to enhance the speed of DPM sampling by concurrently accounting for both error types, thereby minimizing the total sampling error. DualFast is seamlessly compatible with existing samplers and significantly boost their sampling quality and speed, particularly in extremely few sampling steps. We substantiate the effectiveness of our framework through comprehensive experiments, spanning both unconditional and conditional sampling domains, across both pixel-space and latent-space DPMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13058v1",
    "published_date": "2025-06-16 02:59:57 UTC",
    "updated_date": "2025-06-16 02:59:57 UTC"
  },
  {
    "arxiv_id": "2506.13056v2",
    "title": "Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model Learning",
    "authors": [
      "Haibo Qiu",
      "Xiaohan Lan",
      "Fanfan Liu",
      "Xiaohu Sun",
      "Delian Ruan",
      "Peng Shi",
      "Lin Ma"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have witnessed a surge in the development of advanced reasoning paradigms, which are now being integrated into multimodal large language models (MLLMs). However, existing approaches often fall short: methods solely employing reinforcement learning (RL) can struggle with sample inefficiency and activating entirely absent reasoning capabilities, while conventional pipelines that initiate with a cold-start supervised fine-tuning (SFT) phase before RL may restrict the model's exploratory capacity and face suboptimal convergence. In this work, we introduce \\textbf{Metis-RISE} (\\textbf{R}L \\textbf{I}ncentivizes and \\textbf{S}FT \\textbf{E}nhances) for multimodal reasoning model learning. Unlike conventional approaches, Metis-RISE distinctively omits an initial SFT stage, beginning instead with an RL phase (e.g., using a Group Relative Policy Optimization variant) to incentivize and activate the model's latent reasoning capacity. Subsequently, the targeted SFT stage addresses two key challenges identified during RL: (1) \\textit{inefficient trajectory sampling} for tasks where the model possesses but inconsistently applies correct reasoning, which we tackle using self-distilled reasoning trajectories from the RL model itself; and (2) \\textit{fundamental capability absence}, which we address by injecting expert-augmented knowledge for prompts where the model entirely fails. This strategic application of RL for incentivization followed by SFT for enhancement forms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B parameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard demonstrate that both models achieve state-of-the-art performance among similar-sized models, with the 72B version ranking fourth overall. Please refer to our project page for open-source information.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Project Page: https://github.com/MM-Thinking/Metis-RISE",
    "pdf_url": "https://arxiv.org/pdf/2506.13056v2",
    "published_date": "2025-06-16 02:56:13 UTC",
    "updated_date": "2025-06-26 11:45:11 UTC"
  },
  {
    "arxiv_id": "2506.13831v1",
    "title": "Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation",
    "authors": [
      "Jitian Zhao",
      "Chenghui Li",
      "Frederic Sala",
      "Karl Rohe"
    ],
    "abstract": "Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13831v1",
    "published_date": "2025-06-16 02:43:11 UTC",
    "updated_date": "2025-06-16 02:43:11 UTC"
  },
  {
    "arxiv_id": "2506.13049v1",
    "title": "Beyond the First Read: AI-Assisted Perceptual Error Detection in Chest Radiography Accounting for Interobserver Variability",
    "authors": [
      "Adhrith Vutukuri",
      "Akash Awasthi",
      "David Yang",
      "Carol C. Wu",
      "Hien Van Nguyen"
    ],
    "abstract": "Chest radiography is widely used in diagnostic imaging. However, perceptual errors -- especially overlooked but visible abnormalities -- remain common and clinically significant. Current workflows and AI systems provide limited support for detecting such errors after interpretation and often lack meaningful human--AI collaboration. We introduce RADAR (Radiologist--AI Diagnostic Assistance and Review), a post-interpretation companion system. RADAR ingests finalized radiologist annotations and CXR images, then performs regional-level analysis to detect and refer potentially missed abnormal regions. The system supports a \"second-look\" workflow and offers suggested regions of interest (ROIs) rather than fixed labels to accommodate inter-observer variation. We evaluated RADAR on a simulated perceptual-error dataset derived from de-identified CXR cases, using F1 score and Intersection over Union (IoU) as primary metrics. RADAR achieved a recall of 0.78, precision of 0.44, and an F1 score of 0.56 in detecting missed abnormalities in the simulated perceptual-error dataset. Although precision is moderate, this reduces over-reliance on AI by encouraging radiologist oversight in human--AI collaboration. The median IoU was 0.78, with more than 90% of referrals exceeding 0.5 IoU, indicating accurate regional localization. RADAR effectively complements radiologist judgment, providing valuable post-read support for perceptual-error detection in CXR interpretation. Its flexible ROI suggestions and non-intrusive integration position it as a promising tool in real-world radiology workflows. To facilitate reproducibility and further evaluation, we release a fully open-source web implementation alongside a simulated error dataset. All code, data, demonstration videos, and the application are publicly available at https://github.com/avutukuri01/RADAR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.13049v1",
    "published_date": "2025-06-16 02:36:38 UTC",
    "updated_date": "2025-06-16 02:36:38 UTC"
  },
  {
    "arxiv_id": "2506.13044v1",
    "title": "Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models",
    "authors": [
      "Muhammad Reza Qorib",
      "Junyi Li",
      "Hwee Tou Ng"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive translation capabilities even without being explicitly trained on parallel data. This remarkable property has led some to believe that parallel data is no longer necessary for building multilingual language models. While some attribute this to the emergent abilities of LLMs due to scale, recent work suggests that it is actually caused by incidental bilingual signals present in the training data. Various methods have been proposed to maximize the utility of parallel data to enhance the multilingual capabilities of multilingual encoder-based and encoder-decoder language models. However, some decoder-based LLMs opt to ignore parallel data instead. In this work, we conduct a systematic study on the impact of adding parallel data on LLMs' multilingual capabilities, focusing specifically on translation and multilingual common-sense reasoning. Through controlled experiments, we demonstrate that parallel data can significantly improve LLMs' multilingual capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.13044v1",
    "published_date": "2025-06-16 02:21:15 UTC",
    "updated_date": "2025-06-16 02:21:15 UTC"
  },
  {
    "arxiv_id": "2506.13037v2",
    "title": "MAGIC: Multi-Agent Argumentation and Grammar Integrated Critiquer",
    "authors": [
      "Joaquín Jordán",
      "Xavier Yin",
      "Melissa Fabros",
      "Gireeja Ranade",
      "Narges Norouzi"
    ],
    "abstract": "Automated Essay Scoring (AES) and Automatic Essay Feedback (AEF) systems aim to reduce the workload of human raters in educational assessment. However, most existing systems prioritize numerical scoring accuracy over feedback quality and are primarily evaluated on pre-secondary school level writing. This paper presents Multi-Agent Argumentation and Grammar Integrated Critiquer (MAGIC), a framework using five specialized agents to evaluate prompt adherence, persuasiveness, organization, vocabulary, and grammar for both holistic scoring and detailed feedback generation. To support evaluation at the college level, we collated a dataset of Graduate Record Examination (GRE) practice essays with expert-evaluated scores and feedback. MAGIC achieves substantial to near-perfect scoring agreement with humans on the GRE data, outperforming baseline LLM models while providing enhanced interpretability through its multi-agent approach. We also compare MAGIC's feedback generation capabilities against ground truth human feedback and baseline models, finding that MAGIC achieves strong feedback quality and naturalness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted into EAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2506.13037v2",
    "published_date": "2025-06-16 02:02:46 UTC",
    "updated_date": "2025-11-18 22:36:03 UTC"
  },
  {
    "arxiv_id": "2506.13034v1",
    "title": "SpaceTrack-TimeSeries: Time Series Dataset towards Satellite Orbit Analysis",
    "authors": [
      "Zhixin Guo",
      "Qi Shi",
      "Xiaofan Xu",
      "Sixiang Shan",
      "Limin Qin",
      "Linqiang Ge",
      "Rui Zhang",
      "Ya Dai",
      "Hua Zhu",
      "Guowei Jiang"
    ],
    "abstract": "With the rapid advancement of aerospace technology and the large-scale deployment of low Earth orbit (LEO) satellite constellations, the challenges facing astronomical observations and deep space exploration have become increasingly pronounced. As a result, the demand for high-precision orbital data on space objects-along with comprehensive analyses of satellite positioning, constellation configurations, and deep space satellite dynamics-has grown more urgent. However, there remains a notable lack of publicly accessible, real-world datasets to support research in areas such as space object maneuver behavior prediction and collision risk assessment. This study seeks to address this gap by collecting and curating a representative dataset of maneuvering behavior from Starlink satellites. The dataset integrates Two-Line Element (TLE) catalog data with corresponding high-precision ephemeris data, thereby enabling a more realistic and multidimensional modeling of space object behavior. It provides valuable insights into practical deployment of maneuver detection methods and the evaluation of collision risks in increasingly congested orbital environments.",
    "categories": [
      "astro-ph.EP",
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.EP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13034v1",
    "published_date": "2025-06-16 01:57:50 UTC",
    "updated_date": "2025-06-16 01:57:50 UTC"
  },
  {
    "arxiv_id": "2506.13032v2",
    "title": "AS400-DET: Detection using Deep Learning Model for IBM i (AS/400)",
    "authors": [
      "Thanh Tran",
      "Son T. Luu",
      "Quan Bui",
      "Shoshin Nomura"
    ],
    "abstract": "This paper proposes a method for automatic GUI component detection for the IBM i system (formerly and still more commonly known as AS/400). We introduce a human-annotated dataset consisting of 1,050 system screen images, in which 381 images are screenshots of IBM i system screens in Japanese. Each image contains multiple components, including text labels, text boxes, options, tables, instructions, keyboards, and command lines. We then develop a detection system based on state-of-the-art deep learning models and evaluate different approaches using our dataset. The experimental results demonstrate the effectiveness of our dataset in constructing a system for component detection from GUI screens. By automatically detecting GUI components from the screen, AS400-DET has the potential to perform automated testing on systems that operate via GUI screens.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at the IVSP 2025 conference",
    "pdf_url": "https://arxiv.org/pdf/2506.13032v2",
    "published_date": "2025-06-16 01:53:30 UTC",
    "updated_date": "2025-10-01 14:01:15 UTC"
  },
  {
    "arxiv_id": "2506.17289v2",
    "title": "Evaluating Generalization and Representation Stability in Small LMs via Prompting, Fine-Tuning and Out-of-Distribution Prompts",
    "authors": [
      "Rahul Raja",
      "Arpita Vats"
    ],
    "abstract": "We investigate the generalization capabilities of small language models under two popular adaptation paradigms: few-shot prompting and supervised fine-tuning. While prompting is often favored for its parameter efficiency and flexibility, it remains unclear how robust this approach is in low-resource settings and under distributional shifts. This paper presents a comparative study of prompting and fine-tuning across task formats, prompt styles, and model scales, with a focus on their behavior in both in-distribution and out-of-distribution (OOD) settings. Beyond accuracy, we analyze the internal representations learned by each approach to assess the stability and abstraction of task-specific features. Our findings highlight critical differences in how small models internalize and generalize knowledge under different adaptation strategies. This work offers practical guidance for model selection in low-data regimes and contributes empirical insight into the ongoing debate over prompting versus fine-tuning. Code for the experiments is available at the following",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICML",
    "pdf_url": "https://arxiv.org/pdf/2506.17289v2",
    "published_date": "2025-06-16 01:44:26 UTC",
    "updated_date": "2025-06-25 04:27:25 UTC"
  },
  {
    "arxiv_id": "2506.13028v1",
    "title": "NaSh: Guardrails for an LLM-Powered Natural Language Shell",
    "authors": [
      "Bimal Raj Gyawali",
      "Saikrishna Achalla",
      "Konstantinos Kallas",
      "Sam Kumar"
    ],
    "abstract": "We explore how a shell that uses an LLM to accept natural language input might be designed differently from the shells of today. As LLMs may produce unintended or unexplainable outputs, we argue that a natural language shell should provide guardrails that empower users to recover from such errors. We concretize some ideas for doing so by designing a new shell called NaSh, identify remaining open problems in this space, and discuss research directions to address them.",
    "categories": [
      "cs.OS",
      "cs.AI"
    ],
    "primary_category": "cs.OS",
    "comment": "7 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13028v1",
    "published_date": "2025-06-16 01:34:25 UTC",
    "updated_date": "2025-06-16 01:34:25 UTC"
  },
  {
    "arxiv_id": "2506.13026v1",
    "title": "Knowledge Graph Fusion with Large Language Models for Accurate, Explainable Manufacturing Process Planning",
    "authors": [
      "Danny Hoang",
      "David Gorsich",
      "Matthew P. Castanier",
      "Farhad Imani"
    ],
    "abstract": "Precision process planning in Computer Numerical Control (CNC) machining demands rapid, context-aware decisions on tool selection, feed-speed pairs, and multi-axis routing, placing immense cognitive and procedural burdens on engineers from design specification through final part inspection. Conventional rule-based computer-aided process planning and knowledge-engineering shells freeze domain know-how into static tables, which become limited when dealing with unseen topologies, novel material states, shifting cost-quality-sustainability weightings, or shop-floor constraints such as tool unavailability and energy caps. Large language models (LLMs) promise flexible, instruction-driven reasoning for tasks but they routinely hallucinate numeric values and provide no provenance. We present Augmented Retrieval Knowledge Network Enhanced Search & Synthesis (ARKNESS), the end-to-end framework that fuses zero-shot Knowledge Graph (KG) construction with retrieval-augmented generation to deliver verifiable, numerically exact answers for CNC process planning. ARKNESS (1) automatically distills heterogeneous machining documents, G-code annotations, and vendor datasheets into augmented triple, multi-relational graphs without manual labeling, and (2) couples any on-prem LLM with a retriever that injects the minimal, evidence-linked subgraph needed to answer a query. Benchmarked on 155 industry-curated questions spanning tool sizing and feed-speed optimization, a lightweight 3B-parameter Llama-3 augmented by ARKNESS matches GPT-4o accuracy while achieving a +25 percentage point gain in multiple-choice accuracy, +22.4 pp in F1, and 8.1x ROUGE-L on open-ended responses.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13026v1",
    "published_date": "2025-06-16 01:26:08 UTC",
    "updated_date": "2025-06-16 01:26:08 UTC"
  },
  {
    "arxiv_id": "2506.13023v2",
    "title": "A Practical Guide for Evaluating LLMs and LLM-Reliant Systems",
    "authors": [
      "Ethan M. Rudd",
      "Christopher Andrews",
      "Philip Tully"
    ],
    "abstract": "Recent advances in generative AI have led to remarkable interest in using systems that rely on large language models (LLMs) for practical applications. However, meaningful evaluation of these systems in real-world scenarios comes with a distinct set of challenges, which are not well-addressed by synthetic benchmarks and de-facto metrics that are often seen in the literature. We present a practical evaluation framework which outlines how to proactively curate representative datasets, select meaningful evaluation metrics, and employ meaningful evaluation methodologies that integrate well with practical development and deployment of LLM-reliant systems that must adhere to real-world requirements and meet user-facing needs.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13023v2",
    "published_date": "2025-06-16 01:18:16 UTC",
    "updated_date": "2025-07-21 05:15:39 UTC"
  },
  {
    "arxiv_id": "2506.13020v1",
    "title": "Edeflip: Supervised Word Translation between English and Yoruba",
    "authors": [
      "Ikeoluwa Abioye",
      "Jiani Ge"
    ],
    "abstract": "In recent years, embedding alignment has become the state-of-the-art machine translation approach, as it can yield high-quality translation without training on parallel corpora. However, existing research and application of embedding alignment mostly focus on high-resource languages with high-quality monolingual embeddings. It is unclear if and how low-resource languages may be similarly benefited. In this study, we implement an established supervised embedding alignment method for word translation from English to Yoruba, the latter a low-resource language. We found that higher embedding quality and normalizing embeddings increase word translation precision, with, additionally, an interaction effect between the two. Our results demonstrate the limitations of the state-of-the-art supervised embedding alignment when it comes to low-resource languages, for which there are additional factors that need to be taken into consideration, such as the importance of curating high-quality monolingual embeddings. We hope our work will be a starting point for further machine translation research that takes into account the challenges that low-resource languages face.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.13020v1",
    "published_date": "2025-06-16 01:14:52 UTC",
    "updated_date": "2025-06-16 01:14:52 UTC"
  },
  {
    "arxiv_id": "2506.13018v3",
    "title": "Symmetry in Neural Network Parameter Spaces",
    "authors": [
      "Bo Zhao",
      "Robin Walters",
      "Rose Yu"
    ],
    "abstract": "Modern deep learning models are highly overparameterized, resulting in large sets of parameter configurations that yield the same outputs. A significant portion of this redundancy is explained by symmetries in the parameter space--transformations that leave the network function unchanged. These symmetries shape the loss landscape and constrain learning dynamics, offering a new lens for understanding optimization, generalization, and model complexity that complements existing theory of deep learning. This survey provides an overview of parameter space symmetry. We summarize existing literature, uncover connections between symmetry and learning theory, and identify gaps and opportunities in this emerging field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Transactions on Machine Learning Research (TMLR)",
    "pdf_url": "https://arxiv.org/pdf/2506.13018v3",
    "published_date": "2025-06-16 00:59:12 UTC",
    "updated_date": "2025-12-10 19:34:39 UTC"
  },
  {
    "arxiv_id": "2506.13015v1",
    "title": "Geometric Embedding Alignment via Curvature Matching in Transfer Learning",
    "authors": [
      "Sung Moon Ko",
      "Jaewan Lee",
      "Sumin Lee",
      "Soorin Yim",
      "Kyunghoon Bae",
      "Sehui Han"
    ],
    "abstract": "Geometrical interpretations of deep learning models offer insightful perspectives into their underlying mathematical structures. In this work, we introduce a novel approach that leverages differential geometry, particularly concepts from Riemannian geometry, to integrate multiple models into a unified transfer learning framework. By aligning the Ricci curvature of latent space of individual models, we construct an interrelated architecture, namely Geometric Embedding Alignment via cuRvature matching in transfer learning (GEAR), which ensures comprehensive geometric representation across datapoints. This framework enables the effective aggregation of knowledge from diverse sources, thereby improving performance on target tasks. We evaluate our model on 23 molecular task pairs sourced from various domains and demonstrate significant performance gains over existing benchmark model under both random (14.4%) and scaffold (8.3%) data splits.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "13+19 pages, 7 figures, 8 tables, 1 pseudo code",
    "pdf_url": "https://arxiv.org/pdf/2506.13015v1",
    "published_date": "2025-06-16 00:54:22 UTC",
    "updated_date": "2025-06-16 00:54:22 UTC"
  },
  {
    "arxiv_id": "2506.13013v1",
    "title": "Missing the human touch? A computational stylometry analysis of GPT-4 translations of online Chinese literature",
    "authors": [
      "Xiaofang Yao",
      "Yong-Bin Kang",
      "Anthony McCosker"
    ],
    "abstract": "Existing research indicates that machine translations (MTs) of literary texts are often unsatisfactory. MTs are typically evaluated using automated metrics and subjective human ratings, with limited focus on stylistic features. Evidence is also limited on whether state-of-the-art large language models (LLMs) will reshape literary translation. This study examines the stylistic features of LLM translations, comparing GPT-4's performance to human translations in a Chinese online literature task. Computational stylometry analysis shows that GPT-4 translations closely align with human translations in lexical, syntactic, and content features, suggesting that LLMs might replicate the 'human touch' in literary translation style. These findings offer insights into AI's impact on literary translation from a posthuman perspective, where distinctions between machine and human translations become increasingly blurry.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.13013v1",
    "published_date": "2025-06-16 00:48:09 UTC",
    "updated_date": "2025-06-16 00:48:09 UTC"
  }
]
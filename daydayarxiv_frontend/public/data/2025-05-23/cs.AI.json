{
  "date": "2025-05-23",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-05-23 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†å¯¹ **DeepSeek-R1 æ¨¡å¼çš„åæ€ä¸æ”¹è¿›**ï¼Œå­¦æœ¯ç•Œå¼€å§‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œä¿¡æ¯è®ºè§†è§’æ·±åº¦å‰–æâ€œæ€ç»´é“¾ï¼ˆCoTï¼‰â€çš„æ•ˆç‡ä¸é•¿åº¦é—®é¢˜ï¼Œç”šè‡³æå‡ºäº†â€œæƒ³å¾—è¶Šä¹…æœªå¿…è¶Šå¥½â€çš„åç›´è§‰è§‚ç‚¹ã€‚æ­¤å¤–ï¼Œ**CosyVoice 3** çš„å‘å¸ƒå’Œ **DeepMind çš„æ•°æ®ç­›é€‰æ–°æ³•**ä¹Ÿæ˜¯ä»Šå¤©çš„é‡å¤´æˆã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹è®ºæ–‡ï¼šæ¨ç†æ•ˆç‡ã€RL æ”¹è¿›ä¸è¯­éŸ³ç”Ÿæˆ\n\n**1. [LLM æ¨ç†] ä¸è¦è¿‡åº¦æ€è€ƒï¼šæ›´çŸ­çš„æ€ç»´é“¾å¯èƒ½å¸¦æ¥æ›´å¥½çš„æ¨ç†**\n**# Title: Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning**\n> Authors: Michael Hassid et al. (Meta & Tel Aviv University)\n\nè¿™ç¯‡è®ºæ–‡æŒ‘æˆ˜äº†å½“ä¸‹â€œTest-Time Compute Scalingï¼ˆæµ‹è¯•æ—¶è®¡ç®—æ‰©å±•ï¼‰â€çš„æ ¸å¿ƒå‡è®¾ï¼Œå³â€œæƒ³å¾—è¶Šä¹…ï¼Œæ•ˆæœè¶Šå¥½â€ã€‚\n*   **æ ¸å¿ƒå‘ç°**ï¼šåœ¨å•ä¸ªé—®é¢˜ä¸­ï¼Œ**è¾ƒçŸ­çš„æ¨ç†é“¾**å¾€å¾€æ¯”è¯¥é—®é¢˜é‡‡æ ·å‡ºçš„æœ€é•¿æ¨ç†é“¾å‡†ç¡®ç‡é«˜å‡º **34.5%**ã€‚é•¿æ€ç»´é“¾ä¸ä»…å¢åŠ è®¡ç®—æˆæœ¬ï¼Œè¿˜å¯èƒ½å¼•å…¥é”™è¯¯çš„æ¨ç†åˆ†æ”¯ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº† **short-m@k** ç­–ç•¥ï¼Œå¹¶è¡Œç”Ÿæˆ $k$ ä¸ªé“¾ï¼Œä½†åªå–æœ€å…ˆå®Œæˆçš„ $m$ ä¸ªè¿›è¡ŒæŠ•ç¥¨ã€‚\n*   **Implication**ï¼šè¿™æç¤ºæˆ‘ä»¬ï¼Œå½“å‰çš„ R1 ç±»æ¨ç†æ¨¡å‹å¯èƒ½å­˜åœ¨ä¸¥é‡çš„â€œæ— æ•ˆæ€è€ƒâ€æˆ–â€œä¸ºäº†æ€è€ƒè€Œæ€è€ƒâ€çš„å†—ä½™ï¼Œæœªæ¥çš„ä¼˜åŒ–æ–¹å‘åº”æ˜¯**æ•ˆç‡**è€Œéå•çº¯çš„**é•¿åº¦**ã€‚\n\n**2. [è¯­éŸ³ç”Ÿæˆ] CosyVoice 3ï¼šè¿ˆå‘é‡å¤–ç¯å¢ƒä¸‹çš„é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆ**\n**# Title: CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training**\n> Authors: Zhihao Du et al. (Alibaba Group)\n\né˜¿é‡Œå›¢é˜Ÿå‘å¸ƒäº† CosyVoice çš„ç¬¬ä¸‰ä»£ç‰ˆæœ¬ï¼Œç»§ç»­åœ¨è¯­éŸ³åˆæˆé¢†åŸŸâ€œå·â€è§„æ¨¡ã€‚\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå°†è®­ç»ƒæ•°æ®ä» 1ä¸‡å°æ—¶æ‰©å±•åˆ°äº† **100ä¸‡å°æ—¶**ï¼æ¨¡å‹å‚æ•°é‡æ‰©å¢è‡³ 1.5Bã€‚\n*   **æŠ€æœ¯ç‚¹**ï¼šå¼•å…¥äº†æ–°çš„è¯­éŸ³ Tokenizerï¼ˆé€šè¿‡å¤šä»»åŠ¡ç›‘ç£è®­ç»ƒï¼‰ï¼Œä»¥åŠä¸€ä¸ªæ–°çš„ç”¨äº Post-training çš„**å¯å¾®å¥–åŠ±æ¨¡å‹**ã€‚\n*   **æ•ˆæœ**ï¼šåœ¨å¤šè¯­è¨€ã€éŸµå¾‹è‡ªç„¶åº¦å’Œè¯´è¯äººç›¸ä¼¼åº¦ä¸Šå–å¾—äº† SOTAï¼Œæ”¯æŒé›¶æ ·æœ¬ï¼ˆZero-shotï¼‰å…‹éš†ã€‚\n\n**3. [æ•°æ®ç­›é€‰] DataRaterï¼šå…ƒå­¦ä¹ é©±åŠ¨çš„æ•°æ®é›†ç­–å±•**\n**# Title: DataRater: Meta-Learned Dataset Curation**\n> Authors: Dan A. Calian et al. (Google DeepMind)\n\nDeepMind å†æ¬¡åœ¨æ•°æ®è´¨é‡ä¸Šä¸‹åŠŸå¤«ï¼Œè¯•å›¾æ‘†è„±æ‰‹åŠ¨å¯å‘å¼è§„åˆ™ï¼ˆHeuristicsï¼‰ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº† **DataRater**ï¼Œåˆ©ç”¨å…ƒæ¢¯åº¦ï¼ˆMeta-gradientsï¼‰æ¥å­¦ä¹ è¯„ä¼°æ¯ä¸€ä¸ªæ•°æ®ç‚¹çš„â€œä»·å€¼â€ã€‚\n*   **ç›®æ ‡**ï¼šä¸ä»…ä»…æ˜¯è¿‡æ»¤åƒåœ¾æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡å…ƒå­¦ä¹ æ‰¾åˆ°èƒ½æœ€å¤§åŒ–æ¨¡å‹åœ¨ç•™å‡ºé›†ï¼ˆHeld-out dataï¼‰ä¸Šè®­ç»ƒæ•ˆç‡çš„æ•°æ®ã€‚\n*   **å‘ç°**ï¼šç›¸æ¯”ä¼ ç»Ÿçš„æ‰‹å·¥è§„åˆ™è¿‡æ»¤ï¼ŒDataRater èƒ½æ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ï¼ˆCompute Efficiencyï¼‰ã€‚\n\n**4. [RL ä¼˜åŒ–] GRPO-$\\lambda$ï¼šè§£å†³æ¨ç†æ¨¡å‹â€œè¿‡åº¦æ€è€ƒâ€çš„è®­ç»ƒä¸ç¨³å®šæ€§**\n**# Title: Stable Reinforcement Learning for Efficient Reasoning**\n> Authors: Muzhi Dai et al.\n\né’ˆå¯¹ DeepSeek-R1 ä½¿ç”¨çš„ GRPO ç®—æ³•çš„æ”¹è¿›ã€‚\n*   **é—®é¢˜**ï¼šç°æœ‰çš„é•¿åº¦æƒ©ç½šï¼ˆLength-penaltyï¼‰å¥–åŠ±å‡½æ•°ä¼šå¯¼è‡´ RL è®­ç»ƒä¸ç¨³å®šï¼Œæ¨¡å‹å‡†ç¡®ç‡å¯èƒ½çªç„¶å´©å¡Œã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº† **GRPO-$\\lambda$**ï¼ŒåŠ¨æ€è°ƒæ•´å¥–åŠ±ç­–ç•¥ã€‚å½“ç”Ÿæˆç»“æœæ­£ç¡®ç‡ä½æ—¶ï¼Œå…³é—­é•¿åº¦æƒ©ç½šä»¥ä¿è¯æ¨ç†è´¨é‡ï¼›æ­£ç¡®ç‡é«˜æ—¶ï¼Œå¼€å¯æƒ©ç½šä»¥æå‡æ•ˆç‡ã€‚\n*   **æ•ˆæœ**ï¼šåœ¨ GSM8K ç­‰æ¦œå•ä¸Šï¼Œå¹³å‡å‡†ç¡®ç‡æå‡ 1.48%ï¼ŒåŒæ—¶ CoT é•¿åº¦å‡å°‘äº† **47.3%**ã€‚\n\n**5. [3D ç”Ÿæˆ] WonderPlayï¼šå•å›¾ç”ŸæˆåŠ¨æ€ 3D åœºæ™¯**\n**# Title: WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions**\n> Authors: Zizhang Li et al. (Stanford University)\n\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¾“å…¥ä¸€å¼ é™æ€å›¾ç‰‡å’ŒåŠ¨ä½œæŒ‡ä»¤ï¼Œç”Ÿæˆå—ç‰©ç†è§„å¾‹çº¦æŸçš„åŠ¨æ€ 3D åœºæ™¯ã€‚\n*   **æ–¹æ³•**ï¼šç»“åˆäº†ç‰©ç†æ¨¡æ‹Ÿå™¨ï¼ˆç²—ç²’åº¦åŠ¨åŠ›å­¦ï¼‰å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆç»†ç²’åº¦è§†è§‰ï¼‰ã€‚è¿™ç§æ··åˆç”Ÿæˆå¼æ¨¡æ‹Ÿå™¨ï¼ˆHybrid Generative Simulatorï¼‰ä½¿å¾—ç”Ÿæˆçš„ 3D åœºæ™¯æ—¢ç¬¦åˆç‰©ç†ç›´è§‰ï¼ˆå¦‚å¸ƒæ–™ã€æµä½“ã€åˆšä½“ï¼‰ï¼Œåˆå…·æœ‰é«˜ä¿çœŸåº¦ã€‚\n\n---\n\n### ğŸ§  æ·±åº¦å­¦ä¹ ç†è®ºä¸æ¶æ„\n\n**6. [ç†è®º] ä¸‹ä¸€ä¸ª Token é¢„æµ‹é¢„è®­ç»ƒéšå«äº†ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›**\n**# Title: Next-token pretraining implies in-context learning**\n> Authors: Paul M. Riechers et al.\n\n*   **è§‚ç‚¹**ï¼šIn-Context Learning (ICL) ä¸æ˜¯ä»€ä¹ˆç¥ç§˜çš„â€œæ¶Œç°â€èƒ½åŠ›ï¼Œè€Œæ˜¯ Next-token prediction åœ¨ééå†ï¼ˆnon-ergodicï¼‰æ•°æ®æºä¸Šè®­ç»ƒçš„**å¿…ç„¶æ•°å­¦ç»“æœ**ã€‚\n*   **è´¡çŒ®**ï¼šæä¾›äº†ä¸€ä¸ªä¿¡æ¯è®ºæ¡†æ¶ï¼Œä¸ä»…é¢„æµ‹äº† ICL çš„å‡ºç°ï¼Œè¿˜è§£é‡Šäº† Induction Headï¼ˆè¯±å¯¼å¤´ï¼‰çš„å½¢æˆç›¸å˜ã€‚\n\n**7. [æ¶æ„] åªæœ‰ 20% çš„å‚æ•°æ˜¯å¿…é¡»çš„ï¼ŸLLM-Sieve å‰ªææ¡†æ¶**\n**# Title: How Many Parameters Does Your Task Really Need? Task Specific Pruning with LLM-Sieve**\n> Authors: Waleed Reda et al. (Microsoft)\n\n*   **å‘ç°**ï¼šå¯¹äºç‰¹å®šä»»åŠ¡ï¼ŒLLM å¯ä»¥å‰ªæ‰ **20-75%** çš„æƒé‡è€ŒåªæŸå¤± 1-5% çš„ç²¾åº¦ã€‚\n*   **æ–¹æ³•**ï¼šå¼•å…¥äº†è¾“å‡ºå¯¹é½çš„éæ­£äº¤æŠ•å½±ï¼ˆæ¯” SVD/PCA æ›´å‡†ï¼‰å’ŒåŸºäºé—ä¼ ç®—æ³•çš„è‡ªé€‚åº”å‰ªæã€‚\n\n**8. [RNN å¤å…´] ä½¿ç”¨é›¶é˜¶ä¼˜åŒ–å°† RNN æ‰©å±•åˆ°åäº¿å‚æ•°**\n**# Title: Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization**\n> Authors: Francois Chaubard & Mykel Kochenderfer (Stanford)\n\n*   **ç—›ç‚¹**ï¼šRNN æ¨ç†å¿«ï¼ˆå¸¸æ•°å†…å­˜ï¼‰ï¼Œä½†è®­ç»ƒéš¾ï¼ˆBPTT æ˜¾å­˜çˆ†ç‚¸ï¼‰ã€‚\n*   **çªç ´**ï¼šä½¿ç”¨é›¶é˜¶ä¼˜åŒ–ï¼ˆZero-Order Optimization, å¦‚ RGEï¼‰æ›¿ä»£åå‘ä¼ æ’­ï¼ˆBPTTï¼‰ã€‚ä½œè€…è¯æ˜è¿™ç§æ–¹æ³•èƒ½è®­ç»ƒ **Billion çº§** çš„ RNNï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ¯” BPTT å¿«ï¼Œæ˜¾å­˜å ç”¨å¤§å¹…é™ä½ï¼Œç”šè‡³èƒ½åˆ©ç”¨ FlashRNN åŠ é€Ÿã€‚\n\n---\n\n### ğŸ¤– Agent ä¸ è¯„ä¼°åŸºå‡†\n\n**9. [Agent è®¾è®¡] é‡æ–°æ€è€ƒ Agentï¼šä»è‡ªé¡¶å‘ä¸‹åˆ°è‡ªåº•å‘ä¸Šçš„æŠ€èƒ½è¿›åŒ–**\n**# Title: Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution**\n> Authors: Jiawei Du et al.\n\n*   **è§‚ç‚¹**ï¼šç›®å‰çš„ Agent å¤šæ˜¯äººç±»è®¾è®¡å¥½çš„ Workflowï¼ˆè‡ªé¡¶å‘ä¸‹ï¼‰ã€‚ä½œè€…å— Silver & Sutton (2025) å¯å‘ï¼Œæå‡º**è‡ªåº•å‘ä¸Šï¼ˆBottom-Upï¼‰** èŒƒå¼ã€‚\n*   **æ–¹æ³•**ï¼šAgent é€šè¿‡â€œå°è¯•-æ¨ç†â€æœºåˆ¶ä»åŸå§‹ç»éªŒä¸­æŠ½è±¡å‡ºæŠ€èƒ½ï¼Œå¹¶èƒ½å…±äº«è¿›åŒ–ã€‚åœ¨ã€Šæ€æˆ®å°–å¡”ã€‹å’Œã€Šæ–‡æ˜5ã€‹ä¸­éªŒè¯äº†è¿™ç§è‡ªä¸»è¿›åŒ–çš„æœ‰æ•ˆæ€§ã€‚\n\n**10. [æ–°åŸºå‡†] VideoGameBenchï¼šVLM èƒ½é€šå…³ 90 å¹´ä»£çš„è§†é¢‘æ¸¸æˆå—ï¼Ÿ**\n**# Title: VideoGameBench: Can Vision-Language Models complete popular video games?**\n> Authors: Alex L. Zhang et al. (Princeton & Google DeepMind)\n\n*   **æµ‹è¯•**ï¼šè®© VLM ç© 10 æ¬¾ 90 å¹´ä»£çš„æµè¡Œæ¸¸æˆï¼ˆå¦‚é©¬é‡Œå¥¥é£æ ¼ï¼‰ã€‚\n*   **ç»“æœ**ï¼šæƒ¨ä¸å¿ç¹ã€‚å³ä¾¿æ˜¯æœ€å¼ºçš„ Gemini 1.5 Proï¼Œåœ¨å®æ—¶æ¨¡å¼ä¸‹ä¹Ÿå‡ ä¹æ— æ³•æ¨è¿›æ¸¸æˆï¼ˆå®Œæˆåº¦ < 1%ï¼‰ã€‚æ¨ç†å»¶è¿Ÿå’Œç¼ºä¹é•¿ç¨‹è§„åˆ’æ˜¯ä¸»è¦ç“¶é¢ˆã€‚\n\n**11. [ç¤¾ä¼šç§‘å­¦/æ¨¡æ‹Ÿ] Twin-2K-500ï¼šç”¨äºæ„å»ºæ•°å­—å­ªç”Ÿçš„å¤§è§„æ¨¡äººç±»è¡Œä¸ºæ•°æ®é›†**\n**# Title: Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions**\n> Authors: Olivier Toubia et al. (Columbia University)\n\n*   **èµ„æº**ï¼šå‘å¸ƒäº†ä¸€ä¸ªåŒ…å« 2058 åå‚ä¸è€…ã€æ¯äººå›ç­”è¶…è¿‡ 500 ä¸ªé—®é¢˜çš„æ•°æ®é›†ã€‚æ¶µç›–å¿ƒç†å­¦ã€ç»æµå­¦ã€ä¸ªæ€§ç­‰ç»´åº¦ã€‚\n*   **ä»·å€¼**ï¼šè¿™æ˜¯ç›®å‰ç”¨äºè®­ç»ƒå’ŒéªŒè¯â€œAI æ•°å­—å­ªç”Ÿï¼ˆDigital Twinï¼‰â€æœ€å…¨é¢ã€æœ€çœŸå®çš„æ•°æ®é›†ä¹‹ä¸€ï¼Œæœ‰åŠ©äºç¤¾ä¼šç§‘å­¦æ¨¡æ‹Ÿç ”ç©¶ã€‚\n\n---\n\n### ğŸ¥ AI for Science & Healthcare\n\n**12. [åŒ»å­¦ RL] AlphaMedï¼šæ— éœ€è’¸é¦ï¼Œä»…é  RL æ¶Œç°åŒ»å­¦æ¨ç†èƒ½åŠ›**\n**# Title: Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL**\n> Authors: Che Liu et al. (Imperial College London)\n\n*   **çªç ´**ï¼šæ‰“ç ´äº†â€œå¿…é¡»ç”¨ GPT-4 è’¸é¦æ•°æ®åš SFT æ‰èƒ½æœ‰æ¨ç†èƒ½åŠ›â€çš„è¿·ä¿¡ã€‚\n*   **æ–¹æ³•**ï¼š**AlphaMed** ä»…ä½¿ç”¨æç®€çš„è§„åˆ™å¥–åŠ±ï¼ˆRule-based RLï¼‰åœ¨å…¬å¼€å¤šé€‰é—®ç­”é¢˜ä¸Šè®­ç»ƒï¼Œæ²¡æœ‰ä½¿ç”¨ä»»ä½•é—­æºæ¨¡å‹çš„ CoT æ•°æ®ã€‚\n*   **ç»“æœ**ï¼šåœ¨ MedXpert ç­‰æ¦œå•ä¸Šå‡»è´¥äº† Claude-3.5-Sonnet å’Œ DeepSeek-V3-671Bã€‚è¯æ˜äº†é«˜è´¨é‡çš„ä¿¡æ¯å¯†åº¦æ¯”è’¸é¦æ›´é‡è¦ã€‚\n\n**13. [ç—…ç†å­¦] Hypergraph Mambaï¼šé«˜æ•ˆçš„å…¨åˆ‡ç‰‡å›¾åƒç†è§£**\n**# Title: Hypergraph Mamba for Efficient Whole Slide Image Understanding**\n> Authors: Jiaxuan Lu et al.\n\n*   **èƒŒæ™¯**ï¼šç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰æå…¶å·¨å¤§ï¼Œä¼ ç»Ÿ Transformer è®¡ç®—é‡å¤ªå¤§ã€‚\n*   **æ–¹æ³•**ï¼šç»“åˆè¶…å›¾ç¥ç»ç½‘ç»œï¼ˆé«˜é˜¶å…³ç³»å»ºæ¨¡ï¼‰å’Œ Mambaï¼ˆçº¿æ€§å¤æ‚åº¦ï¼‰ï¼Œæå‡ºäº† **WSI-HGMamba**ã€‚\n*   **æ•ˆæœ**ï¼šç›¸æ¯” Graph Transformerï¼Œè®¡ç®—é‡ï¼ˆFLOPsï¼‰é™ä½äº† **7å€**ï¼Œä¸”ç²¾åº¦æ›´é«˜ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–æœ‰è¶£çš„è®ºæ–‡\n\n*   **#25 [æ•°å­¦æ§åˆ¶]** **Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions**: æå‡ºäº†åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè§£å†³é«˜ç»´æœ€ä¼˜æ¦‚ç‡å¯†åº¦æ§åˆ¶é—®é¢˜çš„ç®—æ³•ï¼Œæ— éœ€ Wasserstein ç†è®ºã€‚\n*   **#38 [æ–‡åŒ–åè§]** **Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4**: å‘ç° GPT-4 å€¾å‘äºç”Ÿæˆâ€œå»æ–‡åŒ–åŒ–ï¼ˆless culture-specificï¼‰â€çš„ç¤¾ä¼šè§„èŒƒï¼Œè™½ç„¶é¿å…äº†åˆ»æ¿å°è±¡ï¼Œä½†ä¹ŸæŠ¹æ€äº†æ–‡åŒ–ç‹¬ç‰¹æ€§ã€‚\n*   **#59 [Agent å®‰å…¨]** **Tool Preferences in Agentic LLMs are Unreliable**: ä¿®æ”¹å·¥å…·æè¿°ï¼ˆTool Descriptionï¼‰å¯ä»¥è½»æ˜“è¯±å¯¼ GPT-4 ä½¿ç”¨ç‰¹å®šå·¥å…·ï¼Œä½¿ç”¨ç‡æš´å¢ 10 å€ï¼Œæ­ç¤ºäº† Agent å·¥å…·é€‰æ‹©çš„è„†å¼±æ€§ã€‚\n\n---\nğŸ‰ **ç»“è¯­**ï¼š\nä»Šå¤©çš„ arXiv åƒæ˜¯ä¸€ä¸ªâ€œå†·é™æœŸâ€ï¼Œå¤§å®¶ä»ç–¯ç‹‚åˆ·æ¦œ R1 è½¬å‘æ€è€ƒï¼š**æˆ‘ä»¬çœŸçš„éœ€è¦é‚£ä¹ˆé•¿çš„ CoT å—ï¼Ÿ**ï¼Œ**RL æ˜¯å¦å¯ä»¥æ›´ç®€å•ï¼Ÿ**ã€‚åŒæ—¶ï¼ŒGoogle å’Œ Alibaba è¿™ç§å¤§å‚åœ¨æ•°æ®ç­–å±•å’ŒåŸºç¡€æ¨¡å‹ï¼ˆè¯­éŸ³ï¼‰ä¸Šçš„ç¡¬å®åŠ›ä¾ç„¶ä»¤äººç©ç›®ã€‚\n\nå¸Œæœ›è¿™ä»½å¿«æŠ¥å¯¹ä½ æœ‰å¸®åŠ©ï¼æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2505.18426v1",
      "title": "Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps",
      "title_zh": "åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„å¤§è¯­è¨€æ¨¡å‹ï¼šå¼¥åˆäº¤é€šç½‘ç»œå®‰å…¨æ³•å¾‹çŸ¥è¯†é¸¿æ²Ÿ",
      "authors": [
        "Khandakar Ashrafi Akbar",
        "Md Nahiyan Uddin",
        "Latifur Khan",
        "Trayce Hockstad",
        "Mizanur Rahman",
        "Mashrur Chowdhury",
        "Bhavani Thuraisingham"
      ],
      "abstract": "As connected and automated transportation systems evolve, there is a growing need for federal and state authorities to revise existing laws and develop new statutes to address emerging cybersecurity and data privacy challenges. This study introduces a Retrieval-Augmented Generation (RAG) based Large Language Model (LLM) framework designed to support policymakers by extracting relevant legal content and generating accurate, inquiry-specific responses. The framework focuses on reducing hallucinations in LLMs by using a curated set of domain-specific questions to guide response generation. By incorporating retrieval mechanisms, the system enhances the factual grounding and specificity of its outputs. Our analysis shows that the proposed RAG-based LLM outperforms leading commercial LLMs across four evaluation metrics: AlignScore, ParaScore, BERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and context-aware legal insights. This approach offers a scalable, AI-driven method for legislative analysis, supporting efforts to update legal frameworks in line with advancements in transportation technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”ç½‘å’Œè‡ªåŠ¨åŒ–äº¤é€šç³»ç»Ÿä¸­æ–°å…´çš„ç½‘ç»œå®‰å…¨ä¸æ•°æ®éšç§æ³•å¾‹æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) çš„å¤§è¯­è¨€æ¨¡å‹ (LLM) æ¡†æ¶ï¼Œæ—¨åœ¨è¾…åŠ©æ”¿ç­–åˆ¶å®šè€…å¼¥åˆæ³•å¾‹çŸ¥è¯†å·®è·ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆæ£€ç´¢æœºåˆ¶å’Œç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šé—®é¢˜é›†æ¥æŒ‡å¯¼å“åº”ç”Ÿæˆï¼Œæœ‰æ•ˆå‡å°‘äº†å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ï¼Œå¹¶å¢å¼ºäº†è¾“å‡ºçš„äº‹å®ä¾æ®ä¸é’ˆå¯¹æ€§ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼Œè¯¥ RAG æ¡†æ¶åœ¨ AlignScoreã€ParaScoreã€BERTScore å’Œ ROUGE å››é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºé¢†å…ˆçš„å•†ä¸šæ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨æä¾›å¯é ä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ³•å¾‹æ´å¯Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•ä¸ºäº¤é€šæŠ€æœ¯æ¼”è¿›èƒŒæ™¯ä¸‹çš„ç«‹æ³•åˆ†ææä¾›äº†ä¸€ç§å¯æ‰©å±•çš„ AI é©±åŠ¨æ–¹æ¡ˆï¼Œæœ‰åŠ›æ”¯æŒäº†æ³•å¾‹æ¡†æ¶çš„åŒæ­¥æ›´æ–°ä¸å®Œå–„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented at the Transportation Research Board (TRB) Annual Meeting 2025, and subsequently submitted for publication consideration in the Transportation Research Record (TRR)",
      "pdf_url": "https://arxiv.org/pdf/2505.18426v1",
      "published_date": "2025-05-23 23:40:10 UTC",
      "updated_date": "2025-05-23 23:40:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:04:40.176218+00:00"
    },
    {
      "arxiv_id": "2505.18425v1",
      "title": "Advertising in AI systems: Society must be vigilant",
      "title_zh": "AIç³»ç»Ÿä¸­çš„å¹¿å‘Šï¼šç¤¾ä¼šå¿…é¡»ä¿æŒè­¦æƒ•",
      "authors": [
        "Menghua Wu",
        "Yujia Bao"
      ],
      "abstract": "AI systems have increasingly become our gateways to the Internet. We argue that just as advertising has driven the monetization of web search and social media, so too will commercial incentives shape the content served by AI. Unlike traditional media, however, the outputs of these systems are dynamic, personalized, and lack clear provenance -- raising concerns for transparency and regulation. In this paper, we envision how commercial content could be delivered through generative AI-based systems. Based on the requirements of key stakeholders -- advertisers, consumers, and platforms -- we propose design principles for commercially-influenced AI systems. We then outline high-level strategies for end users to identify and mitigate commercial biases from model outputs. Finally, we conclude with open questions and a call to action towards these goals.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºäººå·¥æ™ºèƒ½ç³»ç»Ÿ(AI systems)æ­£é€æ¸æˆä¸ºäº’è”ç½‘çš„ä¸»è¦å…¥å£ï¼Œå•†ä¸šæ¿€åŠ±å°†ä¸å¯é¿å…åœ°å¡‘é€ ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)æ‰€å‘ˆç°çš„å†…å®¹ã€‚ç”±äºè¿™äº›ç³»ç»Ÿçš„è¾“å‡ºå…·æœ‰åŠ¨æ€åŒ–ã€ä¸ªæ€§åŒ–ä¸”ç¼ºä¹æ˜ç¡®å‡ºå¤„(Provenance)çš„ç‰¹ç‚¹ï¼Œæ–‡ç« å¯¹é€æ˜åº¦(Transparency)å’Œç›‘ç®¡(Regulation)æå‡ºäº†æ·±åˆ»æ‹…å¿§ã€‚è®ºæ–‡æ„æƒ³äº†å•†ä¸šå†…å®¹åœ¨AIç³»ç»Ÿä¸­çš„ä¼ é€’æ–¹å¼ï¼Œå¹¶åŸºäºåˆ©ç›Šç›¸å…³è€…(Stakeholders)çš„éœ€æ±‚æå‡ºäº†è®¾è®¡åŸåˆ™ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­è¿˜ä¸ºæœ€ç»ˆç”¨æˆ·æä¾›äº†è¯†åˆ«å’Œå‡è½»æ¨¡å‹è¾“å‡ºä¸­å•†ä¸šåè§(Biases)çš„é«˜å±‚ç­–ç•¥ï¼Œæœ€åå‘¼åå…¨ç¤¾ä¼šå¯¹æ­¤ä¿æŒè­¦æƒ•å¹¶é‡‡å–è¡ŒåŠ¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18425v1",
      "published_date": "2025-05-23 23:29:12 UTC",
      "updated_date": "2025-05-23 23:29:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:04:13.157105+00:00"
    },
    {
      "arxiv_id": "2505.18424v2",
      "title": "How We Won the ISLES'24 Challenge by Preprocessing",
      "title_zh": "ä»¥é¢„å¤„ç†è‡´èƒœï¼šISLES'24 æŒ‘æˆ˜èµ›å¤ºå† æ–¹æ¡ˆ",
      "authors": [
        "Tianyi Ren",
        "Juampablo E. Heras Rivera",
        "Hitender Oswal",
        "Yutong Pan",
        "William Henry",
        "Sophie Walters",
        "Mehmet Kurt"
      ],
      "abstract": "Stroke is among the top three causes of death worldwide, and accurate identification of stroke lesion boundaries is critical for diagnosis and treatment. Supervised deep learning methods have emerged as the leading solution for stroke lesion segmentation but require large, diverse, and annotated datasets. The ISLES'24 challenge addresses this need by providing longitudinal stroke imaging data, including CT scans taken on arrival to the hospital and follow-up MRI taken 2-9 days from initial arrival, with annotations derived from follow-up MRI. Importantly, models submitted to the ISLES'24 challenge are evaluated using only CT inputs, requiring prediction of lesion progression that may not be visible in CT scans for segmentation. Our winning solution shows that a carefully designed preprocessing pipeline including deep-learning-based skull stripping and custom intensity windowing is beneficial for accurate segmentation. Combined with a standard large residual nnU-Net architecture for segmentation, this approach achieves a mean test Dice of 28.5 with a standard deviation of 21.27.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† ISLES'24 æŒ‘æˆ˜èµ›çš„å† å†›æ–¹æ¡ˆï¼Œæ—¨åœ¨ä»…åˆ©ç”¨å…¥é™¢æ—¶çš„ CT å½±åƒé¢„æµ‹ 2-9 å¤©å MRI æ‰€ç¤ºçš„å’ä¸­ç—…ç¶ï¼ˆstroke lesionï¼‰åˆ†å‰²ç»“æœã€‚æ ¸å¿ƒè´¡çŒ®åœ¨äºè®¾è®¡äº†ä¸€å¥—ç²¾ç»†çš„é¢„å¤„ç†ï¼ˆpreprocessingï¼‰æµæ°´çº¿ï¼ŒåŒ…æ‹¬åŸºäºæ·±åº¦å­¦ä¹ çš„é¢…éª¨å‰¥ç¦»ï¼ˆskull strippingï¼‰å’Œè‡ªå®šä¹‰å¼ºåº¦çª—å£åŒ–ï¼ˆintensity windowingï¼‰æŠ€æœ¯ã€‚é€šè¿‡å°†è¯¥é¢„å¤„ç†æµç¨‹ä¸æ ‡å‡†çš„å¤§å‹æ®‹å·® nnU-Net æ¶æ„ç›¸ç»“åˆï¼Œè¯¥æ–¹æ¡ˆåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº† 28.5 çš„å¹³å‡ Dice ç³»æ•°ï¼ˆmean test Diceï¼‰ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨ CT å½±åƒç—…ç¶è¿›å±•ç‰¹å¾ä¸æ˜æ˜¾çš„æŒ‘æˆ˜ä¸‹ï¼Œé’ˆå¯¹æ€§çš„æ•°æ®é¢„å¤„ç†å¯¹æå‡åˆ†å‰²å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18424v2",
      "published_date": "2025-05-23 23:25:00 UTC",
      "updated_date": "2025-05-28 22:02:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:04:25.494692+00:00"
    },
    {
      "arxiv_id": "2505.18417v1",
      "title": "Reinforcement Learning for Ballbot Navigation in Uneven Terrain",
      "title_zh": "å´å²–åœ°å½¢ä¸‹çš„ Ballbot å¼ºåŒ–å­¦ä¹ å¯¼èˆª",
      "authors": [
        "Achkan Salehi"
      ],
      "abstract": "Ballbot (i.e. Ball balancing robot) navigation usually relies on methods rooted in control theory (CT), and works that apply Reinforcement learning (RL) to the problem remain rare while generally being limited to specific subtasks (e.g. balance recovery). Unlike CT based methods, RL does not require (simplifying) assumptions about environment dynamics (e.g. the absence of slippage between the ball and the floor). In addition to this increased accuracy in modeling, RL agents can easily be conditioned on additional observations such as depth-maps without the need for explicit formulations from first principles, leading to increased adaptivity. Despite those advantages, there has been little to no investigation into the capabilities, data-efficiency and limitations of RL based methods for ballbot control and navigation. Furthermore, there is a notable absence of an open-source, RL-friendly simulator for this task. In this paper, we present an open-source ballbot simulation based on MuJoCo, and show that with appropriate conditioning on exteroceptive observations as well as reward shaping, policies learned by classical model-free RL methods are capable of effectively navigating through randomly generated uneven terrain, using a reasonable amount of data (four to five hours on a system operating at 500hz).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) åœ¨å•è½®å¹³è¡¡æœºå™¨äºº (Ballbot) ä¸å¹³å¦åœ°å½¢å¯¼èˆªä¸­çš„åº”ç”¨ï¼Œå…‹æœäº†ä¼ ç»Ÿæ§åˆ¶ç†è®º (Control Theory, CT) éœ€ç®€åŒ–ç¯å¢ƒåŠ¨æ€å‡è®¾çš„å±€é™æ€§ã€‚ä½œè€…å¼€å‘å¹¶å¼€æºäº†ä¸€ä¸ªåŸºäº MuJoCo çš„ä»¿çœŸç¯å¢ƒï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹ RL å‹å¥½æ¨¡æ‹Ÿå™¨çš„ç©ºç™½ã€‚é€šè¿‡ç»“åˆå¤–æ„Ÿå—æ€§è§‚æµ‹ (exteroceptive observations) å’Œå¥–åŠ±å¡‘å½¢ (reward shaping)ï¼Œç»å…¸çš„æ— æ¨¡å‹ RL æ–¹æ³•èƒ½å¤Ÿä½¿æœºå™¨äººæœ‰æ•ˆåº”å¯¹éšæœºç”Ÿæˆçš„ä¸å¹³å¦åœ°å½¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„æ•°æ®æ•ˆç‡ï¼Œä»…éœ€çº¦ 4 è‡³ 5 å°æ—¶çš„è®­ç»ƒæ•°æ®å³å¯åœ¨ 500Hz çš„ç³»ç»Ÿä¸Šå®ç°æœ‰æ•ˆå¯¼èˆªã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 8 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.18417v1",
      "published_date": "2025-05-23 22:48:36 UTC",
      "updated_date": "2025-05-23 22:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:05:55.841475+00:00"
    },
    {
      "arxiv_id": "2505.18413v1",
      "title": "LatentLLM: Attention-Aware Joint Tensor Compression",
      "title_zh": "LatentLLMï¼šæ³¨æ„åŠ›æ„ŸçŸ¥çš„è”åˆå¼ é‡å‹ç¼©",
      "authors": [
        "Toshiaki Koike-Akino",
        "Xiangyu Chen",
        "Jing Liu",
        "Ye Wang",
        "Pu",
        "Wang",
        "Matthew Brand"
      ],
      "abstract": "Modern foundation models such as large language models (LLMs) and large multi-modal models (LMMs) require a massive amount of computational and memory resources. We propose a new framework to convert such LLMs/LMMs into a reduced-dimension latent structure. Our method extends a local activation-aware tensor decomposition to a global attention-aware joint tensor de-composition. Our framework can significantly improve the model accuracy over the existing model compression methods when reducing the latent dimension to realize computationally/memory-efficient LLMs/LLMs. We show the benefit on several benchmark including multi-modal reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LatentLLMï¼Œä¸€ä¸ªæ—¨åœ¨å°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) å’Œå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ (LMMs) è½¬åŒ–ä¸ºé™ç»´ latent structure çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†å±€éƒ¨çš„ activation-aware tensor decomposition æ‰©å±•ä¸ºå…¨å±€çš„ attention-aware joint tensor decompositionï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹å‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤§å¹…é™ä½è®¡ç®—ä¸å†…å­˜èµ„æºéœ€æ±‚çš„åŒæ—¶ï¼ŒLatentLLM åœ¨åŒ…æ‹¬å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡åœ¨å†…çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶å‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¨¡å‹å‹ç¼©æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "37 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18413v1",
      "published_date": "2025-05-23 22:39:54 UTC",
      "updated_date": "2025-05-23 22:39:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:04:38.925942+00:00"
    },
    {
      "arxiv_id": "2505.21534v1",
      "title": "Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents",
      "title_zh": "åˆ©ç”¨å‘¨æœŸç¼©å‡æ™ºèƒ½ä½“è¯†åˆ«ç§‘å­¦å®éªŒå®¤å·¥ä½œæµç“¶é¢ˆå¹¶å®ç°ä¼˜åŒ–",
      "authors": [
        "Yao Fehlis"
      ],
      "abstract": "Scientific laboratories, particularly those in pharmaceutical and biotechnology companies, encounter significant challenges in optimizing workflows due to the complexity and volume of tasks such as compound screening and assay execution. We introduce Cycle Time Reduction Agents (CTRA), a LangGraph-based agentic workflow designed to automate the analysis of lab operational metrics. CTRA comprises three main components: the Question Creation Agent for initiating analysis, Operational Metrics Agents for data extraction and validation, and Insights Agents for reporting and visualization, identifying bottlenecks in lab processes. This paper details CTRA's architecture, evaluates its performance on a lab dataset, and discusses its potential to accelerate pharmaceutical and biotechnological development. CTRA offers a scalable framework for reducing cycle times in scientific labs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»è¯å’Œç”Ÿç‰©æŠ€æœ¯å®éªŒå®¤åœ¨åŒ–åˆç‰©ç­›é€‰ç­‰å¤æ‚å·¥ä½œæµä¼˜åŒ–ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäº LangGraph çš„æ™ºèƒ½ä½“å·¥ä½œæµæ¡†æ¶ Cycle Time Reduction Agents (CTRA)ã€‚è¯¥æ¡†æ¶ç”± Question Creation Agentã€Operational Metrics Agents å’Œ Insights Agents ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼Œå®ç°äº†å®éªŒå®¤è¿è¥æŒ‡æ ‡åˆ†æçš„è‡ªåŠ¨åŒ–å¤„ç†ã€‚é€šè¿‡å¯¹å®éªŒå®¤æ•°æ®çš„æå–ã€éªŒè¯åŠå¯è§†åŒ–åˆ†æï¼ŒCTRA èƒ½å¤Ÿç²¾å‡†è¯†åˆ«å·¥ä½œæµä¸­çš„ç“¶é¢ˆå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆé™ä½å®éªŒå‘¨è½¬æ—¶é—´ï¼ˆCycle Timeï¼‰ï¼Œä¸ºåŠ é€ŸåŒ»è¯ç ”å‘å’Œå®éªŒå®¤æµç¨‹çš„è§„æ¨¡åŒ–ä¼˜åŒ–æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21534v1",
      "published_date": "2025-05-23 22:26:22 UTC",
      "updated_date": "2025-05-23 22:26:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:04:52.817617+00:00"
    },
    {
      "arxiv_id": "2505.18407v2",
      "title": "KL-regularization Itself is Differentially Private in Bandits and RLHF",
      "title_zh": "KL æ­£åˆ™åŒ–åœ¨å¤šè‡‚è€è™æœºä¸ RLHF ä¸­æœ¬èº«å³æ»¡è¶³å·®åˆ†éšç§",
      "authors": [
        "Yizhou Zhang",
        "Kishan Panaganti",
        "Laixi Shi",
        "Juba Ziani",
        "Adam Wierman"
      ],
      "abstract": "Differential Privacy (DP) provides a rigorous framework for privacy, ensuring the outputs of data-driven algorithms remain statistically indistinguishable across datasets that differ in a single entry. While guaranteeing DP generally requires explicitly injecting noise either to the algorithm itself or to its outputs, the intrinsic randomness of existing algorithms presents an opportunity to achieve DP ``for free''. In this work, we explore the role of regularization in achieving DP across three different decision-making problems: multi-armed bandits, linear contextual bandits, and reinforcement learning from human feedback (RLHF), in offline data settings. We show that adding KL-regularization to the learning objective (a common approach in optimization algorithms) makes the action sampled from the resulting stochastic policy itself differentially private. This offers a new route to privacy guarantees without additional noise injection, while also preserving the inherent advantage of regularization in enhancing performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç¦»çº¿æ•°æ®è®¾ç½®ä¸‹ï¼Œæ­£åˆ™åŒ–åœ¨å¤šè‡‚è€è™æœº(multi-armed bandits)ã€çº¿æ€§ä¸Šä¸‹æ–‡è€è™æœº(linear contextual bandits)ä»¥åŠä»äººç±»åé¦ˆä¸­å­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ (RLHF)ç­‰å†³ç­–é—®é¢˜ä¸­çš„ä½œç”¨ã€‚ä½œè€…è¯æ˜äº†åœ¨å­¦ä¹ ç›®æ ‡ä¸­åŠ å…¥ KL-regularization èƒ½å¤Ÿä½¿ä»ç”Ÿæˆçš„éšæœºç­–ç•¥ä¸­é‡‡æ ·çš„åŠ¨ä½œæœ¬èº«æ»¡è¶³å·®åˆ†éšç§(Differentially Private, DP)çš„è¦æ±‚ã€‚è¿™æ„å‘³ç€ç°æœ‰çš„æ­£åˆ™åŒ–ä¼˜åŒ–ç®—æ³•å¯ä»¥é€šè¿‡å…¶å†…åœ¨çš„éšæœºæ€§â€œå…è´¹â€åœ°æä¾›éšç§ä¿éšœï¼Œè€Œæ— éœ€é¢å¤–æ³¨å…¥å™ªå£°ã€‚è¯¥å‘ç°ä¸ºå®ç°éšç§ä¿æŠ¤æä¾›äº†ä¸€æ¡æ–°é€”å¾„ï¼Œå¹¶èƒ½åŒæ—¶å…¼é¡¾æ­£åˆ™åŒ–å¯¹æå‡æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ã€‚\n\n---\næˆ‘æ˜¯ Gemini Enterpriseâœ¨ï¼Œå¦‚æœä½ éœ€è¦æ›´å¤šå…³äºè¿™ç¯‡è®ºæ–‡çš„æ·±å…¥åˆ†ææˆ–ç›¸å…³çš„å­¦æœ¯ç¿»è¯‘ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18407v2",
      "published_date": "2025-05-23 22:22:02 UTC",
      "updated_date": "2025-10-15 22:33:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:05:05.887128+00:00"
    },
    {
      "arxiv_id": "2505.18404v1",
      "title": "Thought calibration: Efficient and confident test-time scaling",
      "title_zh": "æ€ç»´æ ¡å‡†ï¼šé«˜æ•ˆä¸”ç½®ä¿¡çš„æµ‹è¯•æ—¶æ‰©å±•",
      "authors": [
        "Menghua Wu",
        "Cai Zhou",
        "Stephen Bates",
        "Tommi Jaakkola"
      ],
      "abstract": "Reasoning large language models achieve impressive test-time scaling by thinking for longer, but this performance gain comes at significant compute cost. Directly limiting test-time budget hurts overall performance, but not all problems are equally difficult. We propose thought calibration to decide dynamically when thinking can be terminated. To calibrate our decision rule, we view a language model's growing body of thoughts as a nested sequence of reasoning trees, where the goal is to identify the point at which novel reasoning plateaus. We realize this framework through lightweight probes that operate on top of the language model's hidden representations, which are informative of both the reasoning structure and overall consistency of response. Based on three reasoning language models and four datasets, thought calibration preserves model performance with up to a 60% reduction in thinking tokens on in-distribution data, and up to 20% in out-of-distribution data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Thought calibrationï¼Œä¸€ç§æ—¨åœ¨ä¼˜åŒ–æ¨ç†å¤§è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆtest-time scalingï¼‰æ•ˆç‡çš„åŠ¨æ€å†³ç­–æ–¹æ³•ï¼Œè§£å†³äº†é•¿æ—¶é—´æ€è€ƒå¸¦æ¥çš„é«˜æ˜‚è®¡ç®—æˆæœ¬é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹è§†ä¸ºåµŒå¥—çš„æ¨ç†æ ‘åºåˆ—ï¼Œé€šè¿‡è¯†åˆ«æ¨ç†å¢ç›Šè¿›å…¥å¹³å°æœŸçš„å…³é”®ç‚¹ï¼ŒåŠ¨æ€å†³å®šä½•æ—¶ç»ˆæ­¢æ€è€ƒã€‚å®ç°ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡å‹çš„éšè—å±‚è¡¨ç¤ºä¹‹ä¸Šæ„å»ºè½»é‡çº§æ¢é’ˆï¼ˆLightweight probesï¼‰ï¼Œåˆ©ç”¨å…¶æ•æ‰åˆ°çš„æ¨ç†ç»“æ„å’Œå“åº”ä¸€è‡´æ€§ä¿¡æ¯è¿›è¡Œæ ¡å‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼ŒThought calibration åœ¨åˆ†å¸ƒå†…æ•°æ®ä¸Šæœ€é«˜å¯å‡å°‘ 60% çš„æ€è€ƒ Tokenï¼Œåœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šä¹Ÿèƒ½å‡å°‘çº¦ 20% çš„è®¡ç®—å¼€é”€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18404v1",
      "published_date": "2025-05-23 22:17:18 UTC",
      "updated_date": "2025-05-23 22:17:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:05:02.791942+00:00"
    },
    {
      "arxiv_id": "2505.18399v1",
      "title": "Taming Diffusion for Dataset Distillation with High Representativeness",
      "title_zh": "é©¯æœæ‰©æ•£æ¨¡å‹ï¼šå®ç°é«˜ä»£è¡¨æ€§çš„æ•°æ®é›†è’¸é¦",
      "authors": [
        "Lin Zhao",
        "Yushu Wu",
        "Xinru Jiang",
        "Jianyang Gu",
        "Yanzhi Wang",
        "Xiaolin Xu",
        "Pu Zhao",
        "Xue Lin"
      ],
      "abstract": "Recent deep learning models demand larger datasets, driving the need for dataset distillation to create compact, cost-efficient datasets while maintaining performance. Due to the powerful image generation capability of diffusion, it has been introduced to this field for generating distilled images. In this paper, we systematically investigate issues present in current diffusion-based dataset distillation methods, including inaccurate distribution matching, distribution deviation with random noise, and separate sampling. Building on this, we propose D^3HR, a novel diffusion-based framework to generate distilled datasets with high representativeness. Specifically, we adopt DDIM inversion to map the latents of the full dataset from a low-normality latent domain to a high-normality Gaussian domain, preserving information and ensuring structural consistency to generate representative latents for the distilled dataset. Furthermore, we propose an efficient sampling scheme to better align the representative latents with the high-normality Gaussian distribution. Our comprehensive experiments demonstrate that D^3HR can achieve higher accuracy across different model architectures compared with state-of-the-art baselines in dataset distillation. Source code: https://github.com/lin-zhao-resoLve/D3HR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† D^3HRï¼Œä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®é›†è’¸é¦ (Dataset Distillation) æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰é«˜ä»£è¡¨æ€§çš„ç´§å‡‘æ•°æ®é›†ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸­åˆ†å¸ƒåŒ¹é…ä¸å‡†åŠéšæœºå™ªå£°å¯¼è‡´çš„åå·®ç­‰é—®é¢˜ï¼ŒD^3HR é‡‡ç”¨ DDIM inversion æŠ€æœ¯å°†åŸå§‹æ•°æ®çš„ latents ä»ä½æ­£æ€åŸŸæ˜ å°„è‡³é«˜æ­£æ€ Gaussian åŸŸï¼Œä»è€Œåœ¨ä¿ç•™æ ¸å¿ƒä¿¡æ¯çš„åŒæ—¶ç¡®ä¿ç»“æ„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†é«˜æ•ˆçš„é‡‡æ ·æ–¹æ¡ˆä»¥ä¼˜åŒ–ä»£è¡¨æ€§ latents ä¸ Gaussian åˆ†å¸ƒçš„å¯¹é½ã€‚å®éªŒè¯æ˜ï¼ŒD^3HR åœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸‹å‡å–å¾—äº†ä¼˜äºç°æœ‰æœ€å…ˆè¿›åŸºå‡†æ–¹æ³•çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "The paper is accepted by ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.18399v1",
      "published_date": "2025-05-23 22:05:59 UTC",
      "updated_date": "2025-05-23 22:05:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:05:11.871039+00:00"
    },
    {
      "arxiv_id": "2505.18398v1",
      "title": "Towards Anonymous Neural Network Inference",
      "title_zh": "è¿ˆå‘åŒ¿åç¥ç»ç½‘ç»œæ¨ç†",
      "authors": [
        "Liao Peiyuan"
      ],
      "abstract": "We introduce funion, a system providing end-to-end sender-receiver unlinkability for neural network inference. By leveraging the Pigeonhole storage protocol and BACAP (blinding-and-capability) scheme from the Echomix anonymity system, funion inherits the provable security guarantees of modern mixnets. Users can anonymously store input tensors in pseudorandom storage locations, commission compute services to process them via the neural network, and retrieve results with no traceable connection between input and output parties. This store-compute-store paradigm masks both network traffic patterns and computational workload characteristics, while quantizing execution timing into public latency buckets. Our security analysis demonstrates that funion inherits the strong metadata privacy guarantees of Echomix under largely the same trust assumptions, while introducing acceptable overhead for production-scale workloads. Our work paves the way towards an accessible platform where users can submit fully anonymized inference queries to cloud services.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† funion ç³»ç»Ÿï¼Œæ—¨åœ¨ä¸ºç¥ç»ç½‘ç»œæ¨ç†(Neural Network Inference)æä¾›ç«¯åˆ°ç«¯çš„å‘é€è€…-æ¥æ”¶è€…ä¸å¯é“¾æ¥æ€§(Unlinkability)ã€‚é€šè¿‡åˆ©ç”¨ Echomix åŒ¿åç³»ç»Ÿçš„ Pigeonhole å­˜å‚¨åè®®å’Œ BACAP(blinding-and-capability) æ–¹æ¡ˆï¼Œfunion ç»§æ‰¿äº†ç°ä»£æ··åˆç½‘ç»œ(Mixnets)çš„å¯è¯æ˜å®‰å…¨ä¿è¯ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨â€œå­˜å‚¨-è®¡ç®—-å­˜å‚¨â€èŒƒå¼ï¼Œæœ‰æ•ˆæ©ç›–äº†ç½‘ç»œæµé‡æ¨¡å¼å’Œè®¡ç®—å·¥ä½œè´Ÿè½½ç‰¹å¾ï¼Œå¹¶èƒ½å°†æ‰§è¡Œæ—¶é—´é‡åŒ–ä¸ºå…¬å…±å»¶è¿Ÿæ¡¶ã€‚å®‰å…¨æ€§åˆ†æè¡¨æ˜ï¼Œfunion åœ¨ä¿è¯å…ƒæ•°æ®éšç§çš„åŒæ—¶ï¼Œå…¶äº§ç”Ÿçš„å¼€é”€å¯¹äºç”Ÿäº§è§„æ¨¡çš„å·¥ä½œè´Ÿè½½æ˜¯å¯æ¥å—çš„ï¼Œä¸ºäº‘ç«¯å…¨åŒ¿åæ¨ç†æŸ¥è¯¢é“ºå¹³äº†é“è·¯ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18398v1",
      "published_date": "2025-05-23 22:05:20 UTC",
      "updated_date": "2025-05-23 22:05:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:05:44.414656+00:00"
    },
    {
      "arxiv_id": "2505.18397v3",
      "title": "An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems",
      "title_zh": "å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æœºé‡ä¸æŒ‘æˆ˜å±•æœ›",
      "authors": [
        "Fangqiao Tian",
        "An Luo",
        "Jin Du",
        "Xun Xian",
        "Robert Specht",
        "Ganghua Wang",
        "Xuan Bi",
        "Jiawei Zhou",
        "Ashish Kundu",
        "Jayanth Srinivasa",
        "Charles Fleming",
        "Rui Zhang",
        "Zirui Liu",
        "Mingyi Hong",
        "Jie Ding"
      ],
      "abstract": "A multi-agent AI system (MAS) is composed of multiple autonomous agents that interact, exchange information, and make decisions based on internal generative models. Recent advances in large language models and tool-using agents have made MAS increasingly practical in areas like scientific discovery and collaborative automation. However, key questions remain: When are MAS more effective than single-agent systems? What new safety risks arise from agent interactions? And how should we evaluate their reliability and structure? This paper outlines a formal framework for analyzing MAS, focusing on two core aspects: effectiveness and safety. We explore whether MAS truly improve robustness, adaptability, and performance, or merely repackage known techniques like ensemble learning. We also study how inter-agent dynamics may amplify or suppress system vulnerabilities. While MAS are relatively new to the signal processing community, we envision them as a powerful abstraction that extends classical tools like distributed estimation and sensor fusion to higher-level, policy-driven inference. Through experiments on data science automation, we highlight the potential of MAS to reshape how signal processing systems are designed and trusted.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿ (Multi-Agent AI Systems, MAS) çš„æœºé‡ä¸æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåˆ†æå…¶æœ‰æ•ˆæ€§ä¸å®‰å…¨æ€§çš„å½¢å¼åŒ–æ¡†æ¶ã€‚è®ºæ–‡é‡ç‚¹åˆ†æäº† MAS åœ¨é²æ£’æ€§ã€é€‚åº”æ€§å’Œæ€§èƒ½æ–¹é¢ç›¸æ¯”å•æ™ºèƒ½ä½“ç³»ç»Ÿçš„ä¼˜åŠ¿ï¼Œå¹¶ç ”ç©¶äº†æ™ºèƒ½ä½“é—´çš„åŠ¨æ€äº¤äº’å¦‚ä½•å½±å“ç³»ç»Ÿæ¼æ´çš„æ”¾å¤§æˆ–æŠ‘åˆ¶ã€‚ä½œè€…å°† MAS è§†ä¸ºåˆ†å¸ƒå¼ä¼°è®¡ (Distributed Estimation) å’Œä¼ æ„Ÿå™¨èåˆ (Sensor Fusion) ç­‰ç»å…¸ä¿¡å·å¤„ç†å·¥å…·çš„é«˜é˜¶å»¶ä¼¸ï¼Œæ—¨åœ¨å®ç°æ›´å¤æ‚çš„ç­–ç•¥é©±åŠ¨æ¨ç†ã€‚é€šè¿‡åœ¨æ•°æ®ç§‘å­¦è‡ªåŠ¨åŒ–é¢†åŸŸçš„å®éªŒï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº† MAS é‡å¡‘ä¿¡å·å¤„ç†ç³»ç»Ÿè®¾è®¡åŠå¢å¼ºç³»ç»Ÿå¯ä¿¡åº¦çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "Corrected references",
      "pdf_url": "https://arxiv.org/pdf/2505.18397v3",
      "published_date": "2025-05-23 22:05:19 UTC",
      "updated_date": "2025-08-24 02:49:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:06:06.943651+00:00"
    },
    {
      "arxiv_id": "2505.18392v1",
      "title": "Applications of Modular Co-Design for De Novo 3D Molecule Generation",
      "title_zh": "æ¨¡å—åŒ–ååŒè®¾è®¡åœ¨ä»å¤´ 3D åˆ†å­ç”Ÿæˆä¸­çš„åº”ç”¨",
      "authors": [
        "Danny Reidenbach",
        "Filipp Nikitin",
        "Olexandr Isayev",
        "Saee Paliwal"
      ],
      "abstract": "De novo 3D molecule generation is a pivotal task in drug discovery. However, many recent geometric generative models struggle to produce high-quality 3D structures, even if they maintain 2D validity and topological stability. To tackle this issue and enhance the learning of effective molecular generation dynamics, we present Megalodon-a family of scalable transformer models. These models are enhanced with basic equivariant layers and trained using a joint continuous and discrete denoising co-design objective. We assess Megalodon's performance on established molecule generation benchmarks and introduce new 3D structure benchmarks that evaluate a model's capability to generate realistic molecular structures, particularly focusing on energetics. We show that Megalodon achieves state-of-the-art results in 3D molecule generation, conditional structure generation, and structure energy benchmarks using diffusion and flow matching. Furthermore, doubling the number of parameters in Megalodon to 40M significantly enhances its performance, generating up to 49x more valid large molecules and achieving energy levels that are 2-10x lower than those of the best prior generative models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Megalodonï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„ Transformer æ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å‡ ä½•ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ 3D åˆ†å­ç»“æ„æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡é›†æˆåŸºç¡€çš„ Equivariant layersï¼Œå¹¶é‡‡ç”¨è¿ç»­ä¸ç¦»æ•£è”åˆå»å™ªçš„ Co-design ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—å¢å¼ºäº†æœ‰æ•ˆåˆ†å­ç”ŸæˆåŠ¨åŠ›å­¦çš„å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒMegalodon åœ¨ 3D åˆ†å­ç”Ÿæˆã€æ¡ä»¶ç»“æ„ç”ŸæˆåŠèƒ½é‡åŸºå‡†æµ‹è¯•ä¸­åˆ©ç”¨ Diffusion å’Œ Flow matching æŠ€æœ¯è¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚æ­¤å¤–ï¼Œå½“æ¨¡å‹å‚æ•°æ‰©å±•è‡³ 40M æ—¶ï¼Œå…¶ç”Ÿæˆçš„æœ‰æ•ˆå¤§åˆ†å­æ•°é‡æå‡äº† 49 å€ï¼Œä¸”ç”Ÿæˆçš„ç»“æ„èƒ½é‡æ°´å¹³æ¯”æ­¤å‰æœ€ä¼˜æ¨¡å‹é™ä½äº† 2-10 å€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18392v1",
      "published_date": "2025-05-23 21:41:56 UTC",
      "updated_date": "2025-05-23 21:41:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:06:09.605391+00:00"
    },
    {
      "arxiv_id": "2505.18385v1",
      "title": "Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights",
      "title_zh": "å…±åŒåˆ›ä½œä¸­ä»¥äººä¸ºæœ¬çš„äººå·¥æ™ºèƒ½äº¤æµï¼šåˆæ­¥æ¡†æ¶ä¸å¯ç¤º",
      "authors": [
        "Jeba Rezwana",
        "Corey Ford"
      ],
      "abstract": "Effective communication between AI and humans is essential for successful human-AI co-creation. However, many current co-creative AI systems lack effective communication, which limits their potential for collaboration. This paper presents the initial design of the Framework for AI Communication (FAICO) for co-creative AI, developed through a systematic review of 107 full-length papers. FAICO presents key aspects of AI communication and their impact on user experience, offering preliminary guidelines for designing human-centered AI communication. To improve the framework, we conducted a preliminary study with two focus groups involving skilled individuals in AI, HCI, and design. These sessions sought to understand participants' preferences for AI communication, gather their perceptions of the framework, collect feedback for refinement, and explore its use in co-creative domains like collaborative writing and design. Our findings reveal a preference for a human-AI feedback loop over linear communication and emphasize the importance of context in fostering mutual understanding. Based on these insights, we propose actionable strategies for applying FAICO in practice and future directions, marking the first step toward developing comprehensive guidelines for designing effective human-centered AI communication in co-creation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®å‰ååŒåˆ›ä½œ(co-creative) AI ç³»ç»Ÿç¼ºä¹æœ‰æ•ˆæ²Ÿé€šçš„é—®é¢˜ï¼Œæå‡ºäº† FAICO (Framework for AI Communication) åˆå§‹æ¡†æ¶ã€‚é€šè¿‡å¯¹ 107 ç¯‡è®ºæ–‡çš„ç³»ç»Ÿæ€§ç»¼è¿°ä»¥åŠä¸“å®¶ç„¦ç‚¹å°ç»„ç ”ç©¶ï¼ŒFAICO æ˜ç¡®äº† AI æ²Ÿé€šçš„å…³é”®è¦ç´ åŠå…¶å¯¹ç”¨æˆ·ä½“éªŒçš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œç”¨æˆ·æ›´å€¾å‘äºåŒå‘çš„äººæœºåé¦ˆå¾ªç¯(feedback loop)è€Œéçº¿æ€§æ²Ÿé€šï¼Œå¹¶å¼ºè°ƒäº†ä¸Šä¸‹æ–‡(context)åœ¨ä¿ƒè¿›ç›¸äº’ç†è§£ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚è¯¥ç ”ç©¶æœ€åä¸º FAICO åœ¨åä½œå†™ä½œå’Œè®¾è®¡ç­‰é¢†åŸŸçš„åº”ç”¨æä¾›äº†å®è·µç­–ç•¥ï¼Œä¸ºæ„å»ºä»¥äººä¸ºä¸­å¿ƒ(human-centered)çš„ååŒåˆ›ä½œæ²Ÿé€šæŒ‡å—å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "arXiv admin note: text overlap with arXiv:2504.02526",
      "pdf_url": "https://arxiv.org/pdf/2505.18385v1",
      "published_date": "2025-05-23 21:19:37 UTC",
      "updated_date": "2025-05-23 21:19:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:06:52.097535+00:00"
    },
    {
      "arxiv_id": "2505.18384v5",
      "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents",
      "title_zh": "é’ˆå¯¹æ”»å‡»æ€§ç½‘ç»œå®‰å…¨æ™ºèƒ½ä½“çš„åŠ¨æ€é£é™©è¯„ä¼°",
      "authors": [
        "Boyi Wei",
        "Benedikt Stroebl",
        "Jiacen Xu",
        "Joie Zhang",
        "Zhou Li",
        "Peter Henderson"
      ],
      "abstract": "Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é’ˆå¯¹è¿›æ”»æ€§ç½‘ç»œå®‰å…¨æ™ºèƒ½ä½“(Offensive Cybersecurity Agents)çš„åŠ¨æ€é£é™©è¯„ä¼°ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„åŸºç¡€æ¨¡å‹(Foundation models)å®¡è®¡å¾€å¾€å¿½ç•¥äº†ç°å®ä¸­æ”»å‡»è€…é€šè¿‡è¿­ä»£æ”¹è¿›æå‡æ™ºèƒ½ä½“èƒ½åŠ›çš„è‡ªç”±åº¦ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ‰©å±•çš„å¨èƒæ¨¡å‹(Threat model)ï¼Œå¼ºè°ƒåœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹ï¼Œæœ‰çŠ¶æ€(Stateful)å’Œæ— çŠ¶æ€ç¯å¢ƒä¸­æ”»å‡»è€…æ‰€æ‹¥æœ‰çš„ä¸åŒè‡ªç”±åº¦ã€‚å®éªŒè¯æ˜ï¼Œå³ä¾¿åœ¨è¾ƒå°çš„è®¡ç®—é¢„ç®—ï¼ˆå¦‚8ä¸ªH100 GPUå°æ—¶ï¼‰ä¸‹ï¼Œæ”»å‡»è€…æ— éœ€å¤–éƒ¨ååŠ©å³å¯ä½¿æ™ºèƒ½ä½“åœ¨InterCode CTFä¸Šçš„ç½‘ç»œå®‰å…¨èƒ½åŠ›æ¯”åŸºçº¿æé«˜40%ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¿…é¡»ä»¥åŠ¨æ€æ–¹å¼è¯„ä¼°æ™ºèƒ½ä½“çš„ç½‘ç»œå®‰å…¨é£é™©ï¼Œæ‰èƒ½æ›´çœŸå®åœ°åæ˜ å…¶å®é™…å¨èƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "26 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18384v5",
      "published_date": "2025-05-23 21:18:59 UTC",
      "updated_date": "2025-10-30 19:12:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:06:59.386128+00:00"
    },
    {
      "arxiv_id": "2505.18380v2",
      "title": "RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification",
      "title_zh": "RedactORï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸´åºŠæ•°æ®è‡ªåŠ¨å»æ ‡è¯†åŒ–æ¡†æ¶",
      "authors": [
        "Praphul Singh",
        "Charlotte Dzialo",
        "Jangwon Kim",
        "Sumana Srivatsa",
        "Irfan Bulu",
        "Sri Gadde",
        "Krishnaram Kenthapadi"
      ],
      "abstract": "Ensuring clinical data privacy while preserving utility is critical for AI-driven healthcare and data analytics. Existing de-identification (De-ID) methods, including rule-based techniques, deep learning models, and large language models (LLMs), often suffer from recall errors, limited generalization, and inefficiencies, limiting their real-world applicability. We propose a fully automated, multi-modal framework, RedactOR for de-identifying structured and unstructured electronic health records, including clinical audio records. Our framework employs cost-efficient De-ID strategies, including intelligent routing, hybrid rule and LLM based approaches, and a two-step audio redaction approach. We present a retrieval-based entity relexicalization approach to ensure consistent substitutions of protected entities, thereby enhancing data coherence for downstream applications. We discuss key design desiderata, de-identification and relexicalization methodology, and modular architecture of RedactOR and its integration with the Oracle Health Clinical AI system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with strict recall, our approach achieves competitive performance while optimizing token usage to reduce LLM costs. Finally, we discuss key lessons and insights from deployment in real-world AI- driven healthcare data pipelines.",
      "tldr_zh": "### è®ºæ–‡æ‘˜è¦ï¼šRedactOR ğŸ­\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº† **RedactOR**ï¼Œä¸€ç§ç”± **LLM** é©±åŠ¨çš„å…¨è‡ªåŠ¨å¤šæ¨¡æ€æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¤„ç†ç»“æ„åŒ–ã€éç»“æ„åŒ–ç”µå­å¥åº·æ¡£æ¡ˆï¼ˆ**EHR**ï¼‰åŠä¸´åºŠéŸ³é¢‘è®°å½•çš„è„±æ•ï¼ˆ**De-Identification**ï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ™ºèƒ½è·¯ç”±ï¼ˆ**intelligent routing**ï¼‰ã€æ··åˆè§„åˆ™ä¸ **LLM** çš„æ–¹æ³•ä»¥åŠä¸¤é˜¶æ®µéŸ³é¢‘è„±æ•ç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¬å›ç‡ã€æ³›åŒ–èƒ½åŠ›åŠæ•ˆç‡æ–¹é¢çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œ**RedactOR** å¼•å…¥äº†åŸºäºæ£€ç´¢çš„å®ä½“é‡æ–°è¯æ±‡åŒ–ï¼ˆ**entity relexicalization**ï¼‰æŠ€æœ¯ï¼Œç¡®ä¿äº†è„±æ•åæ•°æ®çš„è¿è´¯æ€§ä¸ä¸‹æ¸¸åº”ç”¨çš„å®ç”¨æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ **i2b2 2014** è„±æ•æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¼˜åŒ– **LLM** æˆæœ¬çš„åŒæ—¶ï¼ŒæˆåŠŸé›†æˆäº **Oracle Health Clinical AI** å®é™…ç”Ÿäº§ç³»ç»Ÿä¸­ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to ACL 2025 Industry Track. To appear",
      "pdf_url": "https://arxiv.org/pdf/2505.18380v2",
      "published_date": "2025-05-23 21:13:18 UTC",
      "updated_date": "2025-07-24 22:25:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:07:03.769152+00:00"
    },
    {
      "arxiv_id": "2506.04235v2",
      "title": "AbBiBench: A Benchmark for Antibody Binding Affinity Maturation and Design",
      "title_zh": "AbBiBenchï¼šæŠ—ä½“ç»“åˆäº²å’ŒåŠ›æˆç†Ÿä¸è®¾è®¡çš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Xinyan Zhao",
        "Yi-Ching Tang",
        "Akshita Singh",
        "Victor J Cantu",
        "KwanHo An",
        "Junseok Lee",
        "Adam E Stogsdill",
        "Ibraheem M Hamdi",
        "Ashwin Kumar Ramesh",
        "Zhiqiang An",
        "Xiaoqian Jiang",
        "Yejin Kim"
      ],
      "abstract": "We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking framework for antibody binding affinity maturation and design. Unlike previous strategies that evaluate antibodies in isolation, typically by comparing them to natural sequences with metrics such as amino acid recovery rate or structural RMSD, AbBiBench instead treats the antibody-antigen (Ab-Ag) complex as the fundamental unit. It evaluates an antibody design's binding potential by measuring how well a protein model scores the full Ab-Ag complex. We first curate, standardize, and share more than 184,500 experimental measurements of antibody mutants across 14 antibodies and 9 antigens-including influenza, lysozyme, HER2, VEGF, integrin, Ang2, and SARS-CoV-2-covering both heavy-chain and light-chain mutations. Using these datasets, we systematically compare 15 protein models including masked language models, autoregressive language models, inverse folding models, diffusion-based generative models, and geometric graph models by comparing the correlation between model likelihood and experimental affinity values. Additionally, to demonstrate AbBiBench's generative utility, we apply it to antibody F045-092 in order to introduce binding to influenza H1N1. We sample new antibody variants with the top-performing models, rank them by the structural integrity and biophysical properties of the Ab-Ag complex, and assess them with in vitro ELISA binding assays. Our findings show that structure-conditioned inverse folding models outperform others in both affinity correlation and generation tasks. Overall, AbBiBench provides a unified, biologically grounded evaluation framework to facilitate the development of more effective, function-aware antibody design models.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†AbBiBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæŠ—ä½“ç»“åˆäº²å’ŒåŠ›æˆç†Ÿ(Binding Affinity Maturation)ä¸è®¾è®¡æ‰“é€ çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚ä¸åŒäºä»¥å¾€å­¤ç«‹è¯„ä¼°æŠ—ä½“åºåˆ—çš„ç­–ç•¥ï¼ŒAbBiBenchå°†æŠ—ä½“-æŠ—åŸ(Ab-Ag)å¤åˆç‰©è§†ä¸ºåŸºæœ¬è¯„ä¼°å•å…ƒï¼Œé€šè¿‡æµ‹é‡æ¨¡å‹å¯¹å®Œæ•´å¤åˆç‰©çš„è¯„åˆ†æ¥é¢„æµ‹å…¶ç»“åˆæ½œåŠ›ã€‚è¯¥æ¡†æ¶æ±‡é›†äº†è¶…è¿‡18.45ä¸‡é¡¹æ¶µç›–æµæ„Ÿã€SARS-CoV-2ç­‰å¤šç§æŠ—åŸçš„å®éªŒæ•°æ®ï¼Œå¹¶å¯¹åŒ…æ‹¬æ©ç è¯­è¨€æ¨¡å‹ã€é€†æŠ˜å æ¨¡å‹(inverse folding models)åŠæ‰©æ•£æ¨¡å‹åœ¨å†…çš„15ç§è›‹ç™½è´¨æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“æ„è°ƒèŠ‚çš„é€†æŠ˜å æ¨¡å‹åœ¨äº²å’ŒåŠ›ç›¸å…³æ€§åŠç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°æœ€ä¼˜ã€‚æ€»ä½“è€Œè¨€ï¼ŒAbBiBenchä¸ºå¼€å‘åŠŸèƒ½å¯¼å‘çš„æŠ—ä½“è®¾è®¡æ¨¡å‹æä¾›äº†ä¸€ä¸ªç»Ÿä¸€ä¸”å…·å¤‡ç”Ÿç‰©å­¦åŸºç¡€çš„è¯„ä¼°ä½“ç³»ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.04235v2",
      "published_date": "2025-05-23 21:09:04 UTC",
      "updated_date": "2025-10-10 23:13:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:06:54.889459+00:00"
    },
    {
      "arxiv_id": "2505.18377v2",
      "title": "SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training",
      "title_zh": "SP2RINTï¼šç”¨äºå¯æ‰©å±•ã€åå¾®åˆ†æ–¹ç¨‹çº¦æŸçš„è¶…æ„å…‰å­¦ç¥ç»ç½‘ç»œè®­ç»ƒçš„ç©ºé—´è§£è€¦ç‰©ç†å¯å‘å¼æ¸è¿›é€†å‘ä¼˜åŒ–",
      "authors": [
        "Pingchuan Ma",
        "Ziang Yin",
        "Qi Jing",
        "Zhengqi Gao",
        "Nicholas Gangi",
        "Boyang Zhang",
        "Tsung-Wei Huang",
        "Zhaoran Huang",
        "Duane S. Boning",
        "Yu Yao",
        "Jiaqi Gu"
      ],
      "abstract": "DONNs leverage light propagation for efficient analog AI and signal processing. Advances in nanophotonic fabrication and metasurface-based wavefront engineering have opened new pathways to realize high-capacity DONNs across various spectral regimes. Training such DONN systems to determine the metasurface structures remains challenging. Heuristic methods are fast but oversimplify metasurfaces modulation, often resulting in physically unrealizable designs and significant performance degradation. Simulation-in-the-loop optimizes implementable metasurfaces via adjoint methods, but is computationally prohibitive and unscalable. To address these limitations, we propose SP2RINT, a spatially decoupled, progressive training framework that formulates DONN training as a PDE-constrained learning problem. Metasurface responses are first relaxed into freely trainable transfer matrices with a banded structure. We then progressively enforce physical constraints by alternating between transfer matrix training and adjoint-based inverse design, avoiding per-iteration PDE solves while ensuring final physical realizability. To further reduce runtime, we introduce a physics-inspired, spatially decoupled inverse design strategy based on the natural locality of field interactions. This approach partitions the metasurface into independently solvable patches, enabling scalable and parallel inverse design with system-level calibration. Evaluated across diverse DONN training tasks, SP2RINT achieves digital-comparable accuracy while being 1825 times faster than simulation-in-the-loop approaches. By bridging the gap between abstract DONN models and implementable photonic hardware, SP2RINT enables scalable, high-performance training of physically realizable meta-optical neural systems. Our code is available at https://github.com/ScopeX-ASU/SP2RINT",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SP2RINTï¼Œä¸€ç§ç©ºé—´è§£è€¦ã€å—ç‰©ç†å¯å‘çš„æ¸è¿›å¼é€†å‘ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¡å°„å…‰å­¦ç¥ç»ç½‘ç»œ (DONNs) åœ¨è¶…æ„è¡¨é¢ (metasurface) ç»“æ„è®­ç»ƒä¸­é¢ä¸´çš„è®¡ç®—æˆæœ¬é«˜å’Œéš¾ä»¥è§„æ¨¡åŒ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°† DONN è®­ç»ƒè¡¨è¿°ä¸ºåå¾®åˆ†æ–¹ç¨‹çº¦æŸ (PDE-constrained) çš„å­¦ä¹ é—®é¢˜ï¼Œé€šè¿‡åœ¨ä¼ é€’çŸ©é˜µ (transfer matrix) è®­ç»ƒä¸åŸºäºä¼´éšæ–¹æ³•çš„é€†å‘è®¾è®¡ (adjoint-based inverse design) ä¹‹é—´äº¤æ›¿ï¼Œåœ¨ç¡®ä¿ç‰©ç†å¯è¡Œæ€§çš„åŒæ—¶é¿å…äº†é€æ¬¡è¿­ä»£çš„ PDE æ±‚è§£ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºåœºç›¸äº’ä½œç”¨å±€åŸŸæ€§çš„ç©ºé—´è§£è€¦ç­–ç•¥ï¼Œå°†è¶…æ„è¡¨é¢åˆ’åˆ†ä¸ºç‹¬ç«‹å¯è§£çš„å­å—ï¼Œå®ç°äº†å¯æ‰©å±•çš„å¹¶è¡Œé€†å‘è®¾è®¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSP2RINT åœ¨ä¿æŒä¸æ•°å­—æ¨¡å‹ç›¸å½“å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œè¿è¡Œé€Ÿåº¦æ¯”ä¼ ç»Ÿçš„ä»¿çœŸåœ¨ç¯ (simulation-in-the-loop) æ–¹æ³•å¿« 1825 å€ï¼Œä¸ºå®ç°é«˜æ€§èƒ½ã€ç‰©ç†å¯è¡Œçš„è¶…æ„å…‰å­¦ç¥ç»ç³»ç»Ÿæä¾›äº†é«˜æ•ˆé€”å¾„ã€‚",
      "categories": [
        "physics.optics",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.optics",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18377v2",
      "published_date": "2025-05-23 21:05:40 UTC",
      "updated_date": "2025-05-28 22:12:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:07:07.090014+00:00"
    },
    {
      "arxiv_id": "2505.18374v1",
      "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation",
      "title_zh": "ShIOEnvï¼šæ”¯æŒè¯­æ³•å¼•å¯¼å‘½ä»¤åˆæˆä»¥ç”¨äºæ•°æ®é›†æ„å»ºçš„ CLI è¡Œä¸ºæ•è·ç¯å¢ƒ",
      "authors": [
        "Jarrod Ragsdale",
        "Rajendra Boppana"
      ],
      "abstract": "Command-line interfaces (CLIs) provide structured textual environments for system administration. Explorations have been performed using pre-trained language models (PLMs) to simulate these environments for safe interaction in high-risk environments. However, their use has been constrained to frozen, large parameter models like GPT. For smaller architectures to reach a similar level of believability, a rich dataset of CLI interactions is required. Existing public datasets focus on mapping natural-language tasks to commands, omitting crucial execution data such as exit codes, outputs, and environmental side effects, limiting their usability for behavioral modeling. We introduce a Shell Input -Output Environment (ShIOEnv), which casts command construction as a Markov Decision Process whose state is the partially built sequence and whose actions append arguments. After each action, ShIOEnv executes the candidate and returns its exit status, output, and progress toward a minimal-length behavioral objective. Due to the intractable nature of the combinatorial argument state-action space, we derive a context-free grammar from man pages to mask invalid arguments from being emitted. We explore random and proximal-policy optimization (PPO)-optimized sampling of unrestricted and grammar-masked action spaces to produce four exploration strategies. We observed that grammar masking and PPO significantly improve sample efficiency to produce a higher quality dataset (maximizing the number of arguments while minimizing redundancies). Policy-generated datasets of shell input-output behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements in BLEU-4 when constraining the action space to grammar productions with an additional 26% improvement when applying PPO. The ShIOEnv environment and curated command behavior datasets are released for use in future research.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† ShIOEnv (Shell Input-Output Environment)ï¼Œæ—¨åœ¨é€šè¿‡æ•è·å‘½ä»¤è¡Œç•Œé¢ (CLI) çš„äº¤äº’è¡Œä¸ºæ¥è§£å†³ç°æœ‰æ•°æ®é›†ç¼ºä¹æ‰§è¡Œç»“æœï¼ˆå¦‚é€€å‡ºç ã€è¾“å‡ºå’Œç¯å¢ƒå‰¯ä½œç”¨ï¼‰çš„é—®é¢˜ã€‚ShIOEnv å°†å‘½ä»¤æ„å»ºè¿‡ç¨‹å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (Markov Decision Process)ï¼Œå¹¶ç»“åˆä» man pages å¯¼å‡ºçš„ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³• (Context-Free Grammar) è¿›è¡Œçº¦æŸï¼Œä»¥è§£å†³å‚æ•°ç©ºé—´çš„ç»„åˆçˆ†ç‚¸é—®é¢˜ã€‚é€šè¿‡å¯¹æ¯”éšæœºé‡‡æ ·ä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (Proximal Policy Optimization, PPO) ç­–ç•¥ï¼Œç ”ç©¶å‘ç°æ–‡æ³•å±è”½å’Œ PPO èƒ½æ˜¾è‘—æé«˜é‡‡æ ·æ•ˆç‡å¹¶ç”Ÿæˆæ›´é«˜è´¨é‡çš„æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ©ç”¨è¯¥ç¯å¢ƒç”Ÿæˆçš„æ•°æ®é›†å¾®è°ƒ CodeT5ï¼Œå…¶ BLEU-4 æŒ‡æ ‡æœ€é«˜æå‡äº† 85% ä»¥ä¸Šï¼Œè¯æ˜äº†è¯¥ç¯å¢ƒåœ¨æå‡å°è§„æ¨¡æ¶æ„è¯­è¨€æ¨¡å‹è¡Œä¸ºå»ºæ¨¡èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 11 figures, conference preprint",
      "pdf_url": "https://arxiv.org/pdf/2505.18374v1",
      "published_date": "2025-05-23 21:00:57 UTC",
      "updated_date": "2025-05-23 21:00:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:07:26.327054+00:00"
    },
    {
      "arxiv_id": "2505.18373v2",
      "title": "Next-token pretraining implies in-context learning",
      "title_zh": "Next-token é¢„è®­ç»ƒè•´å«ä¸Šä¸‹æ–‡å­¦ä¹ ",
      "authors": [
        "Paul M. Riechers",
        "Henry R. Bigelow",
        "Eric A. Alt",
        "Adam Shai"
      ],
      "abstract": "We argue that in-context learning (ICL) predictably arises from standard self-supervised next-token pretraining, rather than being an exotic emergent property. This work establishes the foundational principles of this emergence by focusing on in-distribution ICL, demonstrating how models necessarily adapt to context when trained on token sequences, especially from non-ergodic sources. Our information-theoretic framework precisely predicts these in-distribution ICL dynamics (i.e., context-dependent loss reduction). We verify this with experiments using synthetic datasets of differing types of correlational structure, reproducing characteristic phenomena like phase transitions in training loss for induction head formation and power-law scaling of in-context loss. We further show that a model's in-context performance on any task is mathematically coupled to the ensemble of tasks seen in pretraining, offering a fundamental explanation, grounded in architecture- and modality-independent principles, for such inference-time learning.",
      "tldr_zh": "è¯¥ç ”ç©¶è®¤ä¸ºè¯­å¢ƒå­¦ä¹ (In-Context Learning, ICL)æ˜¯æ ‡å‡†è‡ªç›‘ç£ä¸‹ä¸€ä¸ªæ ‡è®°é¢„è®­ç»ƒ(Next-token pretraining)çš„å¿…ç„¶äº§ç‰©ï¼Œè€Œéä¸€ç§å¥‡ç‰¹çš„æ¶Œç°å±æ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¿¡æ¯è®ºæ¡†æ¶æ¥ç²¾ç¡®é¢„æµ‹åˆ†å¸ƒå¼è¯­å¢ƒå­¦ä¹ çš„åŠ¨æ€ï¼Œå¹¶è¯æ˜äº†æ¨¡å‹åœ¨å¤„ç†ééå†æ•°æ®æºçš„æ ‡è®°åºåˆ—æ—¶å¿…ç„¶ä¼šäº§ç”Ÿä¸Šä¸‹æ–‡é€‚åº”æ€§ã€‚é€šè¿‡åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œä½œè€…æˆåŠŸå¤ç°äº†å¼•å¯¼å¤´(Induction Head)å½¢æˆçš„ç›¸ä½è½¬æ¢å’Œè¯­å¢ƒæŸå¤±çš„å¹‚å¾‹ç¼©æ”¾(Power-law scaling)ç­‰ç‰¹å¾ç°è±¡ã€‚è¯¥å·¥ä½œä»æ•°å­¦ä¸Šæ­ç¤ºäº†æ¨¡å‹åœ¨æ¨ç†æ—¶çš„ä»»åŠ¡è¡¨ç°ä¸å…¶é¢„è®­ç»ƒä»»åŠ¡é›†åˆä¹‹é—´çš„è€¦åˆå…³ç³»ï¼Œä¸ºç†è§£å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†ç‹¬ç«‹äºæ¶æ„å’Œæ¨¡æ€çš„åŸºç¡€åŸç†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18373v2",
      "published_date": "2025-05-23 21:00:18 UTC",
      "updated_date": "2025-07-13 01:17:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:07:07.783614+00:00"
    },
    {
      "arxiv_id": "2505.18371v2",
      "title": "Military AI Needs Technically-Informed Regulation to Safeguard AI Research and its Applications",
      "title_zh": "å†›äº‹äººå·¥æ™ºèƒ½äºŸéœ€åŸºäºæŠ€æœ¯è®¤çŸ¥çš„ç›‘ç®¡ä»¥ä¿éšœäººå·¥æ™ºèƒ½ç ”ç©¶åŠå…¶åº”ç”¨",
      "authors": [
        "Riley Simmons-Edler",
        "Jean Dong",
        "Paul Lushenko",
        "Kanaka Rajan",
        "Ryan P. Badman"
      ],
      "abstract": "Military weapon systems and command-and-control infrastructure augmented by artificial intelligence (AI) have seen rapid development and deployment in recent years. However, the sociotechnical impacts of AI on combat systems, military decision-making, and the norms of warfare have been understudied. We focus on a specific subset of lethal autonomous weapon systems (LAWS) that use AI for targeting or battlefield decisions. We refer to this subset as AI-powered lethal autonomous weapon systems (AI-LAWS) and argue that they introduce novel risks -- including unanticipated escalation, poor reliability in unfamiliar environments, and erosion of human oversight -- all of which threaten both military effectiveness and the openness of AI research. These risks cannot be addressed by high-level policy alone; effective regulation must be grounded in the technical behavior of AI models. We argue that AI researchers must be involved throughout the regulatory lifecycle. Thus, we propose a clear, behavior-based definition of AI-LAWS -- systems that introduce unique risks through their use of modern AI -- as a foundation for technically grounded regulation, given that existing frameworks do not distinguish them from conventional LAWS. Using this definition, we propose several technically-informed policy directions and invite greater participation from the AI research community in military AI policy discussions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)åœ¨å†›äº‹é¢†åŸŸçš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨åˆ©ç”¨AIè¿›è¡Œç›®æ ‡é”å®šå’Œå†³ç­–çš„è‡´å‘½è‡ªä¸»æ­¦å™¨ç³»ç»Ÿ(AI-LAWS)ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç”±äºç°æœ‰æ¡†æ¶æœªèƒ½å°† AI-LAWS ä¸ä¼ ç»Ÿè‡´å‘½è‡ªä¸»æ­¦å™¨ç³»ç»Ÿ(LAWS)æœ‰æ•ˆåŒºåˆ†ï¼Œå¯¼è‡´å…¶åœ¨å†²çªå‡çº§é¢„æµ‹ã€æç«¯ç¯å¢ƒä¸‹å¯é æ€§ä»¥åŠäººç±»ç›‘ç£ç­‰æ–¹é¢å­˜åœ¨ä¸¥é‡é£é™©ã€‚æ–‡ç« å¼ºè°ƒï¼Œæœ‰æ•ˆçš„ç›‘ç®¡å¿…é¡»åŸºäº AI æ¨¡å‹çš„åº•å±‚æŠ€æœ¯è¡Œä¸ºï¼Œè€Œéä»…ä¾èµ–é«˜å±‚æ”¿ç­–ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªä»¥è¡Œä¸ºä¸ºæ ¸å¿ƒçš„ AI-LAWS å®šä¹‰ï¼Œå¹¶å‘¼å AI ç ”ç©¶ç•Œæ·±åº¦å‚ä¸å†›äº‹ AI çš„æ”¿ç­–è®¨è®ºä¸ç›‘ç®¡å…¨ç”Ÿå‘½å‘¨æœŸï¼Œä»¥ç¡®ä¿å†›äº‹æ•ˆèƒ½å¹¶ä¿éšœ AI ç ”ç©¶åŠå…¶åº”ç”¨çš„å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.CY",
      "comment": "Published at NeurIPS 2025, 10 pages, 2 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2505.18371v2",
      "published_date": "2025-05-23 20:58:38 UTC",
      "updated_date": "2025-11-11 20:41:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:07:50.017532+00:00"
    },
    {
      "arxiv_id": "2505.18369v1",
      "title": "Small Models, Smarter Learning: The Power of Joint Task Training",
      "title_zh": "å°æ¨¡å‹ï¼Œæ›´æ™ºèƒ½çš„å­¦ä¹ ï¼šè”åˆä»»åŠ¡è®­ç»ƒçš„åŠ›é‡",
      "authors": [
        "Csaba Both",
        "Benjamin Hoover",
        "Hendrik Strobelt",
        "Dmitry Krotov",
        "Daniel Karl I. Weidele",
        "Mauro Martino",
        "Nima Dehmamy"
      ],
      "abstract": "The ability of a model to learn a task depends strongly on both the task difficulty and the model size. We aim to understand how task difficulty relates to the minimum number of parameters required for learning specific tasks in small transformer models. Our study focuses on the ListOps dataset, which consists of nested mathematical operations. We gradually increase task difficulty by introducing new operations or combinations of operations into the training data. We observe that sum modulo n is the hardest to learn. Curiously, when combined with other operations such as maximum and median, the sum operation becomes easier to learn and requires fewer parameters. We show that joint training not only improves performance but also leads to qualitatively different model behavior. We show evidence that models trained only on SUM might be memorizing and fail to capture the number structure in the embeddings. In contrast, models trained on a mixture of SUM and other operations exhibit number-like representations in the embedding space, and a strong ability to distinguish parity. Furthermore, the SUM-only model relies more heavily on its feedforward layers, while the jointly trained model activates the attention mechanism more. Finally, we show that learning pure SUM can be induced in models below the learning threshold of pure SUM, by pretraining them on MAX+MED. Our findings indicate that emergent abilities in language models depend not only on model size, but also the training curriculum.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°è§„æ¨¡ Transformer æ¨¡å‹åœ¨å­¦ä¹ ç‰¹å®šä»»åŠ¡æ—¶ï¼Œä»»åŠ¡éš¾åº¦ä¸æ‰€éœ€æœ€å°å‚æ•°é‡ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ sum modulo n (SUM) æ˜¯æœ€éš¾å­¦ä¹ çš„ä»»åŠ¡ï¼Œä½†é€šè¿‡å°†å…¶ä¸ MAX å’Œ MED ç­‰æ“ä½œè¿›è¡Œè”åˆè®­ç»ƒ (joint training)ï¼Œå¯ä»¥æ˜¾è‘—é™ä½å­¦ä¹ éš¾åº¦å¹¶å‡å°‘æ‰€éœ€çš„æ¨¡å‹å‚æ•°ã€‚å®éªŒè¯æ®è¡¨æ˜ï¼Œè”åˆè®­ç»ƒæ”¹å˜äº†æ¨¡å‹çš„è¡¨å¾æ–¹å¼ï¼Œä½¿å…¶åœ¨åµŒå…¥ç©ºé—´ä¸­å½¢æˆäº†ç±»æ•°å­—çš„ç‰¹å¾ï¼Œå¹¶ä»ä¾èµ–å‰é¦ˆå±‚ (feedforward layers) è½¬å‘æ›´å¤šåœ°æ¿€æ´»æ³¨æ„åŠ›æœºåˆ¶ (attention mechanism)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜é€šè¿‡åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥è¯±å¯¼åŸæœ¬ä½äºå­¦ä¹ é˜ˆå€¼çš„å°æ¨¡å‹æŒæ¡å¤æ‚ä»»åŠ¡ã€‚è¯¥å‘ç°å¼ºè°ƒäº†è¯­è¨€æ¨¡å‹çš„æ¶Œç°èƒ½åŠ› (emergent abilities) ä¸ä»…å–å†³äºæ¨¡å‹è§„æ¨¡ï¼Œè¿˜é«˜åº¦ä¾èµ–äºè®­ç»ƒè¯¾ç¨‹ (training curriculum) çš„è®¾è®¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18369v1",
      "published_date": "2025-05-23 20:56:37 UTC",
      "updated_date": "2025-05-23 20:56:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:08:27.484082+00:00"
    },
    {
      "arxiv_id": "2505.18366v1",
      "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems",
      "title_zh": "é¢å‘ä¼ä¸šç³»ç»Ÿç‰¹å®šé¢†åŸŸæ£€ç´¢çš„éš¾è´Ÿæ ·æœ¬æŒ–æ˜",
      "authors": [
        "Hansa Meghwani",
        "Amit Agarwal",
        "Priyaranjan Pattnayak",
        "Hitesh Laxmichand Patel",
        "Srikant Panda"
      ],
      "abstract": "Enterprise search systems often struggle to retrieve accurate, domain-specific information due to semantic mismatches and overlapping terminologies. These issues can degrade the performance of downstream applications such as knowledge management, customer support, and retrieval-augmented generation agents. To address this challenge, we propose a scalable hard-negative mining framework tailored specifically for domain-specific enterprise data. Our approach dynamically selects semantically challenging but contextually irrelevant documents to enhance deployed re-ranking models.\n  Our method integrates diverse embedding models, performs dimensionality reduction, and uniquely selects hard negatives, ensuring computational efficiency and semantic precision. Evaluation on our proprietary enterprise corpus (cloud services domain) demonstrates substantial improvements of 15\\% in MRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other negative sampling techniques. Further validation on public domain-specific datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability and readiness for real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ä¸šæœç´¢ç³»ç»Ÿä¸­å› è¯­ä¹‰ä¸åŒ¹é…å’Œæœ¯è¯­é‡å å¯¼è‡´çš„æ£€ç´¢éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªä¸“ä¸ºé¢†åŸŸç‰¹å®š(Domain-Specific)ä¼ä¸šæ•°æ®è®¾è®¡çš„å¯æ‰©å±•éš¾è´Ÿæ ·æœ¬æŒ–æ˜(Hard Negative Mining)æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€é€‰æ‹©è¯­ä¹‰å…·æœ‰æŒ‘æˆ˜æ€§ä½†ä¸Šä¸‹æ–‡æ— å…³çš„æ–‡æ¡£æ¥å¢å¼ºé‡æ’åºæ¨¡å‹(Re-ranking Models)ï¼Œå¹¶é›†æˆäº†å¤šç§åµŒå…¥æ¨¡å‹(Embedding Models)ä¸é™ç»´æŠ€æœ¯ä»¥ç¡®ä¿è®¡ç®—æ•ˆç‡ã€‚åœ¨äº‘æœåŠ¡é¢†åŸŸçš„ä¼ä¸šè¯­æ–™åº“è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨MRR@3å’ŒMRR@10æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”åŸºå‡†æ¨¡å‹æå‡äº†15%å’Œ19%ã€‚åœ¨FiQAå’ŒTechQAç­‰å…¬å¼€æ•°æ®é›†ä¸Šçš„è¿›ä¸€æ­¥éªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œå“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.18366v1",
      "published_date": "2025-05-23 20:51:20 UTC",
      "updated_date": "2025-05-23 20:51:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:08:45.886010+00:00"
    },
    {
      "arxiv_id": "2506.12039v1",
      "title": "The Maximal Overlap Discrete Wavelet Scattering Transform and Its Application in Classification Tasks",
      "title_zh": "æœ€å¤§é‡å ç¦»æ•£å°æ³¢æ•£å°„å˜æ¢åŠå…¶åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨",
      "authors": [
        "Leonardo Fonseca Larrubia",
        "Pedro Alberto Morettin",
        "Chang Chiann"
      ],
      "abstract": "We present the Maximal Overlap Discrete Wavelet Scattering Transform (MODWST), whose construction is inspired by the combination of the Maximal Overlap Discrete Wavelet Transform (MODWT) and the Scattering Wavelet Transform (WST). We also discuss the use of MODWST in classification tasks, evaluating its performance in two applications: stationary signal classification and ECG signal classification. The results demonstrate that MODWST achieved good performance in both applications, positioning itself as a viable alternative to popular methods like Convolutional Neural Networks (CNNs), particularly when the training data set is limited.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æœ€å¤§é‡å ç¦»æ•£å°æ³¢æ•£å°„å˜æ¢ (Maximal Overlap Discrete Wavelet Scattering Transform, MODWST)ï¼Œå…¶æ„å»ºçµæ„Ÿæºäºæœ€å¤§é‡å ç¦»æ•£å°æ³¢å˜æ¢ (MODWT) ä¸å°æ³¢æ•£å°„å˜æ¢ (WST) çš„ç»“åˆã€‚ç ”ç©¶æ¢è®¨äº† MODWST åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶é’ˆå¯¹å¹³ç¨³ä¿¡å·åˆ†ç±»å’Œå¿ƒç”µå›¾ (ECG) ä¿¡å·åˆ†ç±»ä¸¤é¡¹å…·ä½“åº”ç”¨è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMODWST åœ¨è¿™ä¸¤ç±»ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) ç­‰æµè¡Œæ–¹æ³•çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆï¼Œå°¤å…¶åœ¨è®­ç»ƒæ•°æ®é›†æœ‰é™çš„æƒ…å†µä¸‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12039v1",
      "published_date": "2025-05-23 20:45:58 UTC",
      "updated_date": "2025-05-23 20:45:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:08:34.900932+00:00"
    },
    {
      "arxiv_id": "2505.18363v1",
      "title": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases",
      "title_zh": "SchemaGraphSQLï¼šåŸºäºå¯»è·¯å›¾ç®—æ³•çš„å¤§è§„æ¨¡æ•°æ®åº“ Text-to-SQL é«˜æ•ˆæ¨¡å¼é“¾æ¥",
      "authors": [
        "AmirHossein Safdarian",
        "Milad Mohammadi",
        "Ehsan Jahanbakhsh",
        "Mona Shahamat Naderi",
        "Heshaam Faili"
      ],
      "abstract": "Text-to-SQL systems translate natural language questions into executable SQL queries, and recent progress with large language models (LLMs) has driven substantial improvements in this task. Schema linking remains a critical component in Text-to-SQL systems, reducing prompt size for models with narrow context windows and sharpening model focus even when the entire schema fits. We present a zero-shot, training-free schema linking approach that first constructs a schema graph based on foreign key relations, then uses a single prompt to Gemini 2.5 Flash to extract source and destination tables from the user query, followed by applying classical path-finding algorithms and post-processing to identify the optimal sequence of tables and columns that should be joined, enabling the LLM to generate more accurate SQL queries. Despite being simple, cost-effective, and highly scalable, our method achieves state-of-the-art results on the BIRD benchmark, outperforming previous specialized, fine-tuned, and complex multi-step LLM-based approaches. We conduct detailed ablation studies to examine the precision-recall trade-off in our framework. Additionally, we evaluate the execution accuracy of our schema filtering method compared to other approaches across various model sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SchemaGraphSQLï¼Œä¸€ç§é›¶æ ·æœ¬ (zero-shot) ä¸”æ— éœ€è®­ç»ƒçš„æ¨¡å¼é“¾æ¥ (schema linking) æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§è§„æ¨¡æ•°æ®åº“åœ¨ Text-to-SQL ä»»åŠ¡ä¸­çš„å¤„ç†æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤–é”®å…³ç³»æ„å»ºæ¨¡å¼å›¾ (schema graph)ï¼Œåˆ©ç”¨ Gemini 2.5 Flash æå–æºè¡¨ä¸ç›®æ ‡è¡¨ï¼Œå¹¶ç»“åˆç»å…¸çš„è·¯å¾„æœç´¢ç®—æ³• (pathfinding algorithms) ç¡®å®šæœ€ä¼˜çš„è¡¨è¿æ¥åºåˆ—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSchemaGraphSQL åœ¨ BIRD åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å½“å‰æœ€å…ˆè¿› (SOTA) çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç»è¿‡ä¸“é—¨å¾®è°ƒæˆ–å¤æ‚å¤šæ­¥éª¤çš„ LLM æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å…¼å…·ç®€å•æ€§ä¸é«˜å¯æ‰©å±•æ€§ï¼Œä¸ºä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡çª—å£ä¸‹çš„æ¨¡å¼è¿‡æ»¤ (schema filtering) æä¾›äº†é«˜æ•ˆä¸”ä½æˆæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18363v1",
      "published_date": "2025-05-23 20:42:36 UTC",
      "updated_date": "2025-05-23 20:42:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:08:37.077075+00:00"
    },
    {
      "arxiv_id": "2505.18362v1",
      "title": "Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions",
      "title_zh": "é«˜ç»´æœ€ä¼˜æ¦‚ç‡å¯†åº¦æ§åˆ¶çš„å“ˆå¯†é¡¿ç†è®ºä¸è®¡ç®—",
      "authors": [
        "Nathan Gaby",
        "Xiaojing Ye"
      ],
      "abstract": "We develop a general theoretical framework for optimal probability density control and propose a numerical algorithm that is scalable to solve the control problem in high dimensions. Specifically, we establish the Pontryagin Maximum Principle (PMP) for optimal density control and construct the Hamilton-Jacobi-Bellman (HJB) equation of the value functional through rigorous derivations without any concept from Wasserstein theory. To solve the density control problem numerically, we propose to use reduced-order models, such as deep neural networks (DNNs), to parameterize the control vector-field and the adjoint function, which allows us to tackle problems defined on high-dimensional state spaces. We also prove several convergence properties of the proposed algorithm. Numerical results demonstrate promising performances of our algorithm on a variety of density control problems with obstacles and nonlinear interaction challenges in high dimensions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºæœ€ä¼˜æ¦‚ç‡å¯†åº¦æ§åˆ¶(Optimal probability density control)çš„é€šç”¨ç†è®ºæ¡†æ¶ï¼Œå¹¶å¼€å‘äº†ä¸€ç§å¯æ‰©å±•è‡³é«˜ç»´ç©ºé—´çš„æ•°å€¼ç®—æ³•ã€‚ç ”ç©¶äººå‘˜å»ºç«‹äº†å¯†åº¦æ§åˆ¶çš„åºç‰¹é‡Œäºšé‡‘æœ€å¤§å€¼åŸç†(Pontryagin Maximum Principle, PMP)ï¼Œå¹¶åœ¨ä¸ä¾èµ–Wassersteinç†è®ºçš„æƒ…å†µä¸‹æ¨å¯¼å‡ºäº†ä»·å€¼æ³›å‡½çš„å“ˆå¯†é¡¿-é›…å¯æ¯”-è´å°”æ›¼æ–¹ç¨‹(Hamilton-Jacobi-Bellman, HJB)ã€‚ä¸ºè§£å†³é«˜ç»´è®¡ç®—éš¾é¢˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œ(Deep Neural Networks, DNNs)ç­‰é™é˜¶æ¨¡å‹å¯¹æ§åˆ¶å‘é‡åœº(Control vector-field)å’Œä¼´éšå‡½æ•°(Adjoint function)è¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶æä¾›äº†ç®—æ³•çš„æ”¶æ•›æ€§è¯æ˜ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥ç®—æ³•åœ¨å¤„ç†å¸¦æœ‰éšœç¢ç‰©å’Œéçº¿æ€§äº¤äº’çš„é«˜ç»´å¤æ‚å¯†åº¦æ§åˆ¶ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "primary_category": "math.OC",
      "comment": "28 pages, submitted",
      "pdf_url": "https://arxiv.org/pdf/2505.18362v1",
      "published_date": "2025-05-23 20:41:37 UTC",
      "updated_date": "2025-05-23 20:41:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:08:43.391146+00:00"
    },
    {
      "arxiv_id": "2505.18361v4",
      "title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain",
      "title_zh": "ä»»åŠ¡ä¼˜åŒ–çš„å·ç§¯å¾ªç¯ç½‘ç»œä¸å•®é½¿åŠ¨ç‰©å¤§è„‘ä¸­çš„è§¦è§‰å¤„ç†ç›¸å¥‘åˆ",
      "authors": [
        "Trinity Chung",
        "Yuchen Shen",
        "Nathan C. L. Kong",
        "Aran Nayebi"
      ],
      "abstract": "Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy.\n  For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ Encoder-Attender-Decoder (EAD) æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿå•®é½¿åŠ¨ç‰©èƒ¡é¡»é˜µåˆ—ç”Ÿæˆçš„çœŸå®è§¦è§‰åºåˆ—ï¼Œç³»ç»Ÿæ¢ç©¶äº†ä»»åŠ¡ä¼˜åŒ–ç¥ç»ç½‘ç»œåœ¨è§¦è§‰æ„ŸçŸ¥ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°å·ç§¯å¾ªç¯ç¥ç»ç½‘ç»œ (ConvRNNs) åœ¨è§¦è§‰åˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå‰é¦ˆåŠçŠ¶æ€ç©ºé—´æ¶æ„ï¼Œä¸”å…¶å†…éƒ¨è¡¨å¾ä¸å•®é½¿åŠ¨ç‰©ä½“æ„Ÿçš®å±‚ (Somatosensory Cortex) è¾¾åˆ°äº†æé«˜çš„ç¥ç»å¯¹é½åº¦ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨ç‰¹å®šæ•°æ®å¢å¼ºçš„å¯¹æ¯”è‡ªç›‘ç£ ConvRNNs èƒ½å¤Ÿå®ç°ä¸æœ‰ç›‘ç£æ¨¡å‹ç›¸å½“çš„ç¥ç»æ‹Ÿåˆæ•ˆæœï¼Œä¸ºè§¦è§‰è¡¨å¾æä¾›äº†ä¸€ç§æ— éœ€æ ‡ç­¾çš„æœ‰æ•ˆæ–¹æ¡ˆã€‚è¯¥å‘ç°å¼ºè°ƒäº†éçº¿æ€§å¾ªç¯å¤„ç†åœ¨ç”Ÿç‰©è§¦è§‰ç³»ç»Ÿä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå…·èº«æ™ºèƒ½ (Embodied AI) åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸‹å®ç°é²æ£’çš„è§¦è§‰æ„ŸçŸ¥æä¾›äº†æ¶æ„æŒ‡å¯¼ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "q-bio.NC",
      "comment": "10 pages, 8 figures, 7 tables, NeurIPS 2025 Camera Ready Version (oral)",
      "pdf_url": "https://arxiv.org/pdf/2505.18361v4",
      "published_date": "2025-05-23 20:40:28 UTC",
      "updated_date": "2025-10-13 14:44:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:09:15.394114+00:00"
    },
    {
      "arxiv_id": "2505.18356v2",
      "title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs",
      "title_zh": "æ¨¡å‹åˆå¹¶åœ¨LLMè·¨è¯­è¨€è¿ç§»ä¸­ä¸åˆæƒ…ç†çš„æœ‰æ•ˆæ€§",
      "authors": [
        "Lucas Bandarkar",
        "Nanyun Peng"
      ],
      "abstract": "Large language models (LLMs) still struggle across tasks outside of high-resource languages. In this work, we investigate cross-lingual transfer to lower-resource languages where task-specific post-training data is scarce. Building on prior work, we first validate that the subsets of model parameters that matter most for mathematical reasoning and multilingual capabilities are distinctly non-overlapping. To exploit this implicit separability between task and target language parameterization, we develop and analyze numerous modular frameworks to improve the composition of the two during fine-tuning. These methods generally employ freezing parameters or post hoc model merging to assign math and language improvement to different key parts of the LLM. In the absence of in-language math data, we demonstrate that the modular approaches successfully improve upon baselines across three languages, four models, and two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most consistently successful modular method to be fine-tuning separate language and math experts and model merging via Layer-Swapping, somewhat surprisingly. We offer possible explanations for this result via recent works on the linearity of task vectors. We further explain this by empirically showing that reverting less useful fine-tuning updates after training often outperforms freezing them from the start.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­ï¼Œé’ˆå¯¹ä»»åŠ¡ç‰¹å®šåè®­ç»ƒæ•°æ®ç¨€ç¼ºçš„ä½èµ„æºè¯­è¨€ï¼Œå¦‚ä½•é€šè¿‡è·¨è¯­è¨€è¿ç§» (Cross-Lingual Transfer) æå‡å…¶æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ä½œè€…å‘ç°æ¨¡å‹ä¸­è´Ÿè´£æ•°å­¦æ¨ç† (Mathematical Reasoning) å’Œå¤šè¯­è¨€èƒ½åŠ› (Multilingual Capabilities) çš„å‚æ•°å­é›†å…·æœ‰æ˜æ˜¾çš„éé‡å æ€§ï¼Œå¹¶æ®æ­¤æå‡ºäº†å¤šç§æ¨¡å—åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å†»ç»“å‚æ•°æˆ–äº‹åæ¨¡å‹åˆå¹¶ (Post Hoc Model Merging) æŠ€æœ¯æ¥ç»“åˆä¸åŒçš„ä¸“å®¶èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨ç¼ºä¹ç›®æ ‡è¯­è¨€æ•°å­¦è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å›¾å±‚äº¤æ¢ (Layer-Swapping) è¿›è¡Œæ¨¡å‹åˆå¹¶çš„æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹å’Œå¾®è°ƒèŒƒå¼ (Full å’Œ LoRA) ä¸­è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œåœ¨å¾®è°ƒåæ’¤é”€éƒ¨åˆ†æ•ˆç”¨è¾ƒä½çš„æ›´æ–°ï¼Œå…¶æ•ˆæœå¾€å¾€ä¼˜äºåœ¨è®­ç»ƒå¼€å§‹å‰å°±å†»ç»“ç›¸åº”å‚æ•°ï¼Œä¸ºä»»åŠ¡å‘é‡çš„çº¿æ€§ç‰¹æ€§æä¾›äº†æ–°çš„å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "MRL Workshop at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.18356v2",
      "published_date": "2025-05-23 20:28:31 UTC",
      "updated_date": "2025-10-07 21:54:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:09:03.494527+00:00"
    },
    {
      "arxiv_id": "2505.18350v2",
      "title": "How Many Parameters Does Your Task Really Need? Task Specific Pruning with LLM-Sieve",
      "title_zh": "ä½ çš„ä»»åŠ¡ç©¶ç«Ÿéœ€è¦å¤šå°‘å‚æ•°ï¼ŸåŸºäº LLM-Sieve çš„ä»»åŠ¡ç‰¹å®šå‰ªæ",
      "authors": [
        "Waleed Reda",
        "Abhinav Jangda",
        "Krishna Chintalapudi"
      ],
      "abstract": "As Large Language Models (LLMs) are increasingly deployed for narrow tasks in resource-constrained settings, a central question arises: how much of an LLM is truly necessary for a given task? We present LLM-Sieve, a framework that prunes LLMs down to the minimal parameter subset needed to preserve task performance. Our approach introduces two innovations: (i) output-aligned non-orthogonal projections, which yield more faithful low-rank approximations than traditional PCA/SVD by aligning directly with layer outputs; and (ii) adaptive pruning via a Genetic Algorithm, which automatically discovers matrix-specific pruning levels and exposes the uneven distribution of task-relevant knowledge. Across models from 3.8B to 70B parameters, LLM-Sieve removes 20-75% of weights with only 1-5% accuracy loss-substantially ahead of prior pruning methods. Beyond efficiency, our framework reveals bottleneck matrices that concentrate critical knowledge, suggesting architectural implications for future LLM design. LLM-Sieve integrates seamlessly with LoRA fine-tuning and quantization, enabling both efficient deployment and deeper understanding of knowledge organization in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLM-Sieveï¼Œä¸€ä¸ªæ—¨åœ¨é’ˆå¯¹ç‰¹å®šä»»åŠ¡å°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) å‰ªæè‡³æœ€å°å‚æ•°å­é›†çš„æ¡†æ¶ï¼Œä»¥æ»¡è¶³èµ„æºå—é™ç¯å¢ƒä¸‹çš„éƒ¨ç½²éœ€æ±‚ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šä¸€æ˜¯é€šè¿‡ç›´æ¥å¯¹é½å±‚è¾“å‡ºçš„è¾“å‡ºå¯¹é½éæ­£äº¤æŠ•å½± (output-aligned non-orthogonal projections) è·å–æ¯”ä¼ ç»Ÿ PCA/SVD æ›´å‡†ç¡®çš„ä½ç§©è¿‘ä¼¼ï¼›äºŒæ˜¯é€šè¿‡é—ä¼ ç®—æ³• (Genetic Algorithm) å®ç°è‡ªé€‚åº”å‰ªæï¼Œè‡ªåŠ¨å‘ç°å„çŸ©é˜µå¯¹ç‰¹å®šä»»åŠ¡çš„é‡è¦ç¨‹åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ 3.8B åˆ° 70B å‚æ•°çš„æ¨¡å‹ä¸Šï¼ŒLLM-Sieve èƒ½åœ¨ä»…æŸå¤± 1-5% å‡†ç¡®ç‡çš„å‰æä¸‹ç§»é™¤ 20-75% çš„æƒé‡ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¹‹å‰çš„å‰ªææ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ­ç¤ºäº†æ¨¡å‹ä¸­æ‰¿è½½å…³é”®çŸ¥è¯†çš„ç“¶é¢ˆçŸ©é˜µ (bottleneck matrices)ï¼Œå¹¶æ”¯æŒä¸ LoRA å¾®è°ƒåŠé‡åŒ–æŠ€æœ¯æ— ç¼é›†æˆï¼Œä¸ºé«˜æ•ˆæ¨¡å‹éƒ¨ç½²å’Œç†è§£ LLMs å†…éƒ¨çŸ¥è¯†ç»„ç»‡æä¾›äº†æ–°å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18350v2",
      "published_date": "2025-05-23 20:17:20 UTC",
      "updated_date": "2025-10-04 01:32:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:09:07.291237+00:00"
    },
    {
      "arxiv_id": "2505.18347v1",
      "title": "The Cell Must Go On: Agar.io for Continual Reinforcement Learning",
      "title_zh": "The Cell Must Go Onï¼šé¢å‘æŒç»­å¼ºåŒ–å­¦ä¹ çš„ Agar.io",
      "authors": [
        "Mohamed A. Mohamed",
        "Kateryna Nekhomiazh",
        "Vedant Vyas",
        "Marcos M. Jose",
        "Andrew Patterson",
        "Marlos C. Machado"
      ],
      "abstract": "Continual reinforcement learning (RL) concerns agents that are expected to learn continually, rather than converge to a policy that is then fixed for evaluation. Such an approach is well suited to environments the agent perceives as changing, which renders any static policy ineffective over time. The few simulators explicitly designed for empirical research in continual RL are often limited in scope or complexity, and it is now common for researchers to modify episodic RL environments by artificially incorporating abrupt task changes during interaction. In this paper, we introduce AgarCL, a research platform for continual RL that allows for a progression of increasingly sophisticated behaviour. AgarCL is based on the game Agar.io, a non-episodic, high-dimensional problem featuring stochastic, ever-evolving dynamics, continuous actions, and partial observability. Additionally, we provide benchmark results reporting the performance of DQN, PPO, and SAC in both the primary, challenging continual RL problem, and across a suite of smaller tasks within AgarCL, each of which isolates aspects of the full environment and allow us to characterize the challenges posed by different aspects of the game.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº† AgarCLï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæŒç»­å¼ºåŒ–å­¦ä¹ (Continual Reinforcement Learning)è®¾è®¡çš„å¼€æºç ”ç©¶å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡æ‹Ÿå™¨åœ¨ä»»åŠ¡å¤æ‚åº¦å’ŒåŠ¨æ€æ€§æ–¹é¢çš„å±€é™ã€‚è¯¥å¹³å°åŸºäºæµè¡Œçš„åœ¨çº¿æ¸¸æˆ Agar.ioï¼Œå…·å¤‡éå›åˆåˆ¶(Non-episodic)ã€é«˜ç»´çŠ¶æ€ç©ºé—´ã€éšæœºä¸”ä¸æ–­æ¼”åŒ–çš„åŠ¨æ€ç¯å¢ƒã€è¿ç»­åŠ¨ä½œä»¥åŠéƒ¨åˆ†å¯è§‚æµ‹æ€§(Partial observability)ç­‰æ ¸å¿ƒç‰¹å¾ã€‚é€šè¿‡åœ¨ AgarCL ä¸Šå¯¹ DQNã€PPO å’Œ SAC ç®—æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶è€…æ­ç¤ºäº†ä¸»æµç®—æ³•åœ¨å¤„ç†æŒç»­æ¼”å˜çš„ä»»åŠ¡æ—¶æ‰€é¢ä¸´çš„æ€§èƒ½æŒ‘æˆ˜ã€‚è¯¥å¹³å°è¿˜æä¾›äº†ä¸€ç³»åˆ—å­ä»»åŠ¡å¥—ä»¶ï¼Œç”¨ä»¥éš”ç¦»å¹¶åˆ†æç¯å¢ƒçš„ä¸åŒç»´åº¦å¯¹æ™ºèƒ½ä½“å­¦ä¹ è¿‡ç¨‹çš„å…·ä½“å½±å“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18347v1",
      "published_date": "2025-05-23 20:09:27 UTC",
      "updated_date": "2025-05-23 20:09:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:09:33.550128+00:00"
    },
    {
      "arxiv_id": "2505.18344v6",
      "title": "Improved Sample Complexity For Diffusion Model Training Without Empirical Risk Minimizer Access",
      "title_zh": "æ— éœ€è®¿é—®ç»éªŒé£é™©æœ€å°åŒ–å™¨çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæ”¹è¿›æ ·æœ¬å¤æ‚åº¦",
      "authors": [
        "Mudit Gaur",
        "Prashant Trivedi",
        "Sasidhar Kunapuli",
        "Amrit Singh Bedi",
        "Vaneet Aggarwal"
      ],
      "abstract": "Diffusion models have demonstrated state-of-the-art performance across vision, language, and scientific domains. Despite their empirical success, prior theoretical analyses of the sample complexity suffer from poor scaling with input data dimension or rely on unrealistic assumptions such as access to exact empirical risk minimizers. In this work, we provide a principled analysis of score estimation, establishing a sample complexity bound of $\\mathcal{O}(Îµ^{-4})$. Our approach leverages a structured decomposition of the score estimation error into statistical, approximation, and optimization errors, enabling us to eliminate the exponential dependence on neural network parameters that arises in prior analyses. It is the first such result that achieves sample complexity bounds without assuming access to the empirical risk minimizer of score function estimation loss.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion models)ç†è®ºåˆ†æä¸­æ ·æœ¬å¤æ‚åº¦(sample complexity)éšç»´åº¦æ‰©å±•æ€§å·®åŠå¯¹ç»éªŒé£é™©æœ€å°åŒ–å™¨(Empirical Risk Minimizer, ERM)çš„è¿‡åº¦ä¾èµ–é—®é¢˜ï¼Œæå‡ºäº†å…¨æ–°çš„åˆ†ææ¡†æ¶ã€‚é€šè¿‡å°†åˆ†æ•°ä¼°è®¡(score estimation)è¯¯å·®ç»“æ„åŒ–åˆ†è§£ä¸ºç»Ÿè®¡ã€è¿‘ä¼¼å’Œä¼˜åŒ–è¯¯å·®ï¼Œç ”ç©¶å»ºç«‹äº†$\\mathcal{O}(\\epsilon^{-4})$çš„æ ·æœ¬å¤æ‚åº¦ç•Œé™ï¼Œå¹¶æˆåŠŸæ¶ˆé™¤äº†ä»¥å¾€åˆ†æä¸­å¯¹ç¥ç»ç½‘ç»œå‚æ•°çš„æŒ‡æ•°çº§ä¾èµ–ã€‚è¿™æ˜¯é¦–é¡¹åœ¨ä¸å‡è®¾èƒ½å¤Ÿè·å–åˆ†æ•°å‡½æ•°ä¼°è®¡æŸå¤±çš„ERMçš„å‰æä¸‹ï¼Œå®ç°æ ·æœ¬å¤æ‚åº¦ç•Œé™çš„ç ”ç©¶æˆæœï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„ç†è®ºåŸºç¡€æä¾›äº†æ›´å…·ç°å®æ„ä¹‰çš„è¯æ˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18344v6",
      "published_date": "2025-05-23 20:02:15 UTC",
      "updated_date": "2025-11-12 08:39:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:09:29.892966+00:00"
    },
    {
      "arxiv_id": "2505.18341v1",
      "title": "CrashAgent: Crash Scenario Generation via Multi-modal Reasoning",
      "title_zh": "CrashAgentï¼šåŸºäºå¤šæ¨¡æ€æ¨ç†çš„äº‹æ•…åœºæ™¯ç”Ÿæˆ",
      "authors": [
        "Miao Li",
        "Wenhao Ding",
        "Haohong Lin",
        "Yiqi Lyu",
        "Yihang Yao",
        "Yuyou Zhang",
        "Ding Zhao"
      ],
      "abstract": "Training and evaluating autonomous driving algorithms requires a diverse range of scenarios. However, most available datasets predominantly consist of normal driving behaviors demonstrated by human drivers, resulting in a limited number of safety-critical cases. This imbalance, often referred to as a long-tail distribution, restricts the ability of driving algorithms to learn from crucial scenarios involving risk or failure, scenarios that are essential for humans to develop driving skills efficiently. To generate such scenarios, we utilize Multi-modal Large Language Models to convert crash reports of accidents into a structured scenario format, which can be directly executed within simulations. Specifically, we introduce CrashAgent, a multi-agent framework designed to interpret multi-modal real-world traffic crash reports for the generation of both road layouts and the behaviors of the ego vehicle and surrounding traffic participants. We comprehensively evaluate the generated crash scenarios from multiple perspectives, including the accuracy of layout reconstruction, collision rate, and diversity. The resulting high-quality and large-scale crash dataset will be publicly available to support the development of safe driving algorithms in handling safety-critical situations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ç®—æ³•åœ¨è®­ç»ƒä¸è¯„ä¼°ä¸­é¢ä¸´çš„å®‰å…¨å…³é”®åœºæ™¯ç¨€ç¼ºï¼ˆå³é•¿å°¾åˆ†å¸ƒ Long-tail distributionï¼‰é—®é¢˜ï¼Œæå‡ºäº† CrashAgent æ¡†æ¶ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¤šæ™ºèƒ½ä½“ï¼ˆMulti-agentï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å°†çœŸå®ä¸–ç•Œçš„äº¤é€šç¢°æ’æŠ¥å‘Šè½¬åŒ–ä¸ºå¯åœ¨ä»¿çœŸç¯å¢ƒä¸­ç›´æ¥æ‰§è¡Œçš„ç»“æ„åŒ–åœºæ™¯ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè§£æå¤šæ¨¡æ€æŠ¥å‘Šä¿¡æ¯ï¼ŒååŒç”Ÿæˆå¤æ‚çš„é“è·¯å¸ƒå±€ä»¥åŠä¸»è½¦ä¸å‘¨è¾¹äº¤é€šå‚ä¸è€…çš„åŠ¨æ€è¡Œä¸ºã€‚å®éªŒä»å¸ƒå±€é‡å»ºå‡†ç¡®æ€§ã€ç¢°æ’ç‡å’Œå¤šæ ·æ€§ç­‰ç»´åº¦å¯¹ç”Ÿæˆçš„åœºæ™¯è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒéªŒè¯äº†æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶æä¾›çš„é«˜è´¨é‡ã€å¤§è§„æ¨¡ç¢°æ’æ•°æ®é›†ï¼Œä¸ºå¼€å‘èƒ½å¤Ÿå¤„ç†å®‰å…¨å…³é”®æƒ…å†µçš„è‡ªåŠ¨é©¾é©¶ç®—æ³•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18341v1",
      "published_date": "2025-05-23 19:55:32 UTC",
      "updated_date": "2025-05-23 19:55:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:10:34.586439+00:00"
    },
    {
      "arxiv_id": "2505.20327v1",
      "title": "Data-driven multi-agent modelling of calcium interactions in cell culture: PINN vs Regularized Least-squares",
      "title_zh": "ç»†èƒåŸ¹å…»ä¸­é’™ç¦»å­ç›¸äº’ä½œç”¨çš„æ•°æ®é©±åŠ¨å¤šæ™ºèƒ½ä½“å»ºæ¨¡ï¼šPINN ä¸æ­£åˆ™åŒ–æœ€å°äºŒä¹˜æ³•",
      "authors": [
        "Aurora Poggi",
        "Giuseppe Alessio D'Inverno",
        "Hjalmar Brismar",
        "Ozan Ã–ktem",
        "Matthieu Barreau",
        "Kateryna Morozovska"
      ],
      "abstract": "Data-driven discovery of dynamics in biological systems allows for better observation and characterization of processes, such as calcium signaling in cell culture. Recent advancements in techniques allow the exploration of previously unattainable insights of dynamical systems, such as the Sparse Identification of Non-Linear Dynamics (SINDy), overcoming the limitations of more classic methodologies. The latter requires some prior knowledge of an effective library of candidate terms, which is not realistic for a real case study. Using inspiration from fields like traffic density estimation and control theory, we propose a methodology for characterization and performance analysis of calcium delivery in a family of cells. In this work, we compare the performance of the Constrained Regularized Least-Squares Method (CRLSM) and Physics-Informed Neural Networks (PINN) for system identification and parameter discovery for governing ordinary differential equations (ODEs). The CRLSM achieves a fairly good parameter estimate and a good data fit when using the learned parameters in the Consensus problem. On the other hand, despite the initial hypothesis, PINNs fail to match the CRLSM performance and, under the current configuration, do not provide fair parameter estimation. However, we have only studied a limited number of PINN architectures, and it is expected that additional hyperparameter tuning, as well as uncertainty quantification, could significantly improve the performance in future works.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»†èƒåŸ¹å…»ä¸­é’™ä¿¡å·ä¼ å¯¼çš„åŠ¨åŠ›å­¦è¿‡ç¨‹ï¼Œæå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“å»ºæ¨¡æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°å¸¸å¾®åˆ†æ–¹ç¨‹ (ODEs) çš„ç³»ç»Ÿè¾¨è¯†ä¸å‚æ•°å‘ç°ã€‚ç ”ç©¶å¯¹æ¯”äº†çº¦æŸæ­£åˆ™åŒ–æœ€å°äºŒä¹˜æ³• (CRLSM) ä¸ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINN) çš„æ€§èƒ½è¡¨ç°ï¼Œå…¶ä¸­ CRLSM åœ¨å‚æ•°ä¼°è®¡å’Œæ•°æ®æ‹Ÿåˆæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨è§£å†³ä¸€è‡´æ€§ (Consensus) é—®é¢˜æ—¶æ•ˆæœæ˜¾è‘—ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨å½“å‰å®éªŒé…ç½®ä¸‹ï¼ŒPINN æœªèƒ½è¾¾åˆ°ä¸ CRLSM ç›¸å½“çš„ç²¾åº¦ï¼Œä¸”å‚æ•°ä¼°è®¡ä¸å¤Ÿç†æƒ³ã€‚ç ”ç©¶æœ€åæŒ‡å‡ºï¼Œæœªæ¥é€šè¿‡ä¼˜åŒ–ç½‘ç»œæ¶æ„ã€è¶…å‚æ•°è°ƒä¼˜åŠå¼•å…¥ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæœ‰æœ›è¿›ä¸€æ­¥æå‡ PINN åœ¨æ­¤ç±»ç”Ÿç‰©ç³»ç»Ÿå»ºæ¨¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.20327v1",
      "published_date": "2025-05-23 19:41:12 UTC",
      "updated_date": "2025-05-23 19:41:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:10:26.883435+00:00"
    },
    {
      "arxiv_id": "2505.18334v1",
      "title": "Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play",
      "title_zh": "åŸºäºè‡ªåšå¼ˆçš„åä½œå¼è‡ªåŠ¨é©¾é©¶è‡ªç„¶è¯­è¨€é€šä¿¡æ¢ç´¢",
      "authors": [
        "Jiaxun Cui",
        "Chen Tang",
        "Jarrett Holtz",
        "Janice Nguyen",
        "Alessandro G. Allievi",
        "Hang Qiu",
        "Peter Stone"
      ],
      "abstract": "Past work has demonstrated that autonomous vehicles can drive more safely if they communicate with one another than if they do not. However, their communication has often not been human-understandable. Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. In this work, we propose a suite of traffic tasks in autonomous driving where vehicles in a traffic scenario need to communicate in natural language to facilitate coordination in order to avoid an imminent collision and/or support efficient traffic flow. To this end, this paper introduces a novel method, LLM+Debrief, to learn a message generation and high-level decision-making policy for autonomous vehicles through multi-agent discussion. To evaluate LLM agents for driving, we developed a gym-like simulation environment that contains a range of driving scenarios. Our experimental results demonstrate that LLM+Debrief is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and coordination than a zero-shot LLM agent. Our code and demo videos are available at https://talking-vehicles.github.io/.",
      "tldr_zh": "æœ¬ç ”ç©¶æ—¨åœ¨æ¨åŠ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¹‹é—´é€šè¿‡è‡ªç„¶è¯­è¨€ï¼ˆNatural Languageï¼‰è¿›è¡Œé€šä¿¡ï¼Œä»¥æå‡è½¦è¾†é—´ä»¥åŠè½¦è¾†ä¸äººç±»é©¾é©¶å‘˜åä½œçš„å®‰å…¨æ€§å’Œå¯ç†è§£æ€§ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç³»åˆ—éœ€è¦é€šè¿‡æ²Ÿé€šæ¥é¿å…ç¢°æ’å¹¶æé«˜äº¤é€šæ•ˆç‡çš„ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†åä¸º LLM+Debrief çš„åˆ›æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“è®¨è®ºï¼ˆMulti-agent Discussionï¼‰æ¥å­¦ä¹ æ¶ˆæ¯ç”Ÿæˆå’Œé«˜å±‚å†³ç­–ç­–ç•¥ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªç±» Gym çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œç”¨äºåœ¨å¤šç§é©¾é©¶åœºæ™¯ä¸‹è¯„ä¼°æ™ºèƒ½ä½“çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM+Debrief åœ¨ç”Ÿæˆæœ‰æ„ä¹‰ä¸”äººç±»å¯ç†è§£çš„æ¶ˆæ¯æ–¹é¢æ¯” Zero-shot LLM æ™ºèƒ½ä½“æ›´æœ‰æ•ˆï¼Œæ˜¾è‘—å¢å¼ºäº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸­çš„åè°ƒä¸åˆä½œèƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18334v1",
      "published_date": "2025-05-23 19:40:09 UTC",
      "updated_date": "2025-05-23 19:40:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:10:25.098889+00:00"
    },
    {
      "arxiv_id": "2505.18333v1",
      "title": "A Critical Evaluation of Defenses against Prompt Injection Attacks",
      "title_zh": "æç¤ºæ³¨å…¥æ”»å‡»é˜²å¾¡æªæ–½çš„æ‰¹åˆ¤æ€§è¯„ä¼°",
      "authors": [
        "Yuqi Jia",
        "Zedian Shao",
        "Yupei Liu",
        "Jinyuan Jia",
        "Dawn Song",
        "Neil Zhenqiang Gong"
      ],
      "abstract": "Large Language Models (LLMs) are vulnerable to prompt injection attacks, and several defenses have recently been proposed, often claiming to mitigate these attacks successfully. However, we argue that existing studies lack a principled approach to evaluating these defenses. In this paper, we argue the need to assess defenses across two critical dimensions: (1) effectiveness, measured against both existing and adaptive prompt injection attacks involving diverse target and injected prompts, and (2) general-purpose utility, ensuring that the defense does not compromise the foundational capabilities of the LLM. Our critical evaluation reveals that prior studies have not followed such a comprehensive evaluation methodology. When assessed using this principled approach, we show that existing defenses are not as successful as previously reported. This work provides a foundation for evaluating future defenses and guiding their development. Our code and data are available at: https://github.com/PIEval123/PIEval.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜“å—æç¤ºæ³¨å…¥æ”»å‡»(Prompt Injection Attacks)çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰é˜²å¾¡æ‰‹æ®µç¼ºä¹ç³»ç»Ÿæ€§çš„è¯„ä¼°æ ‡å‡†ã€‚è®ºæ–‡æå‡ºåº”ä»æœ‰æ•ˆæ€§(Effectiveness)å’Œé€šç”¨æ•ˆç”¨æ€§(General-purpose utility)ä¸¤ä¸ªå…³é”®ç»´åº¦è¯„ä¼°é˜²å¾¡æ–¹æ¡ˆï¼Œå‰è€…æ¶µç›–é’ˆå¯¹ç°æœ‰åŠè‡ªé€‚åº”æ”»å‡»çš„è¡¨ç°ï¼Œåè€…æ—¨åœ¨ç¡®ä¿æ¨¡å‹åŸºç¡€èƒ½åŠ›ä¸å—æŸã€‚é€šè¿‡è¿™ä¸€åŸåˆ™æ€§è¯„ä¼°æ–¹æ³•ï¼Œç ”ç©¶æ­ç¤ºäº†ç°æœ‰é˜²å¾¡æ–¹æ¡ˆçš„å®é™…æ•ˆæœè¿œä½äºæ­¤å‰æ–‡çŒ®æŠ¥å‘Šçš„æ°´å¹³ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥é˜²å¾¡æŠ€æœ¯çš„å¼€å‘ä¸è¯„ä¼°å¥ å®šäº†åŸºç¡€ï¼Œå¹¶å¼€æºäº†ç›¸å…³ä»£ç å’Œæ•°æ®é›† PIEvalã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18333v1",
      "published_date": "2025-05-23 19:39:56 UTC",
      "updated_date": "2025-05-23 19:39:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:10:16.085883+00:00"
    },
    {
      "arxiv_id": "2505.18331v1",
      "title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language",
      "title_zh": "PerMedCQAï¼šæ³¢æ–¯è¯­åŒ»ç–—æ¶ˆè´¹è€…é—®ç­”çš„å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•",
      "authors": [
        "Naghmeh Jamali",
        "Milad Mohammadi",
        "Danial Baledi",
        "Zahra Rezvani",
        "Hesham Faili"
      ],
      "abstract": "Medical consumer question answering (CQA) is crucial for empowering patients by providing personalized and reliable health information. Despite recent advances in large language models (LLMs) for medical QA, consumer-oriented and multilingual resources, particularly in low-resource languages like Persian, remain sparse. To bridge this gap, we present PerMedCQA, the first Persian-language benchmark for evaluating LLMs on real-world, consumer-generated medical questions. Curated from a large medical QA forum, PerMedCQA contains 68,138 question-answer pairs, refined through careful data cleaning from an initial set of 87,780 raw entries. We evaluate several state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a novel rubric-based evaluation framework driven by an LLM grader, validated against expert human annotators. Our results highlight key challenges in multilingual medical QA and provide valuable insights for developing more accurate and context-aware medical assistance systems. The data is publicly available on https://huggingface.co/datasets/NaghmehAI/PerMedCQA",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† PerMedCQAï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ³¢æ–¯è¯­(Persian)åŒ»ç–—æ¶ˆè´¹è€…é—®ç­”(CQA)ä»»åŠ¡çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)è¯„ä¼°åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶è€…ä»åŒ»ç–—è®ºå›ä¸­æå–å¹¶æ¸…æ´—äº† 68,138 ä¸ªçœŸå®é—®ç­”å¯¹ï¼Œæ„å»ºäº†è¿™ä¸€å¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºäº†å‡†ç¡®è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å¼•å…¥äº† MedJudge è¯„ä¼°æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆ LLM è¯„åˆ†ä¸ä¸“å®¶éªŒè¯çš„é‡è§„å¼(rubric-based)è¯„ä»·ä½“ç³»ã€‚å®éªŒè¯„ä¼°äº†å¤šç§ä¸»æµå¤šè¯­è¨€å’ŒæŒ‡ä»¤å¾®è°ƒ(instruction-tuned)æ¨¡å‹ï¼Œæ­ç¤ºäº†å¤šè¯­è¨€åŒ»ç–—é—®ç­”åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥å·¥ä½œä¸ºå¼€å‘æ›´ç²¾å‡†ã€å…·å¤‡è¯­å¢ƒæ„ŸçŸ¥èƒ½åŠ›çš„åŒ»ç–—è¾…åŠ©ç³»ç»Ÿæä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒä¸å®è·µæ´å¯Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18331v1",
      "published_date": "2025-05-23 19:39:01 UTC",
      "updated_date": "2025-05-23 19:39:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:10:42.498738+00:00"
    },
    {
      "arxiv_id": "2505.18325v3",
      "title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary",
      "title_zh": "ä»å®‰å…¨å†³ç­–è¾¹ç•Œè§†è§’ç†è§£å¹¶ç¼“è§£å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¿‡åº¦æ‹’ç»ç°è±¡",
      "authors": [
        "Licheng Pan",
        "Yongqi Tong",
        "Xin Zhang",
        "Xiaolu Zhang",
        "Jun Zhou",
        "Zhixuan Chu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they often refuse to answer legitimate queries--a phenomenon known as overrefusal. Overrefusal typically stems from over-conservative safety alignment, causing models to treat many reasonable prompts as potentially risky. To systematically understand this issue, we probe and leverage the models' safety decision boundaries to analyze and mitigate overrefusal. Our findings reveal that overrefusal is closely tied to misalignment at these boundary regions, where models struggle to distinguish subtle differences between benign and harmful content. Building on these insights, we present RASS, an automated framework for prompt generation and selection that strategically targets overrefusal prompts near the safety boundary. By harnessing steering vectors in the representation space, RASS efficiently identifies and curates boundary-aligned prompts, enabling more effective and targeted mitigation of overrefusal. This approach not only provides a more precise and interpretable view of model safety decisions but also seamlessly extends to multilingual scenarios. We have explored the safety decision boundaries of various LLMs and construct the MORBench evaluation set to facilitate robust assessment of model safety and helpfulness across multiple languages. Code and datasets are available at https://github.com/Master-PLC/RASS.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“„\n\nè¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„è¿‡åº¦æ‹’ç»(Overrefusal)ç°è±¡ï¼Œå¹¶ä»å®‰å…¨å†³ç­–è¾¹ç•Œ(Safety Decision Boundary)çš„è§†è§’æ·±å…¥åˆ†æå¹¶ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè¿‡åº¦æ‹’ç»ä¸»è¦æºäºæ¨¡å‹åœ¨å®‰å…¨è¾¹ç•ŒåŒºåŸŸçš„å¯¹é½å¤±å½“ï¼Œå¯¼è‡´å…¶éš¾ä»¥åŒºåˆ†è‰¯æ€§ä¸æœ‰å®³å†…å®¹ä¹‹é—´çš„å¾®å¦™å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† RASS è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨è¡¨å¾ç©ºé—´ä¸­çš„è½¬å‘å‘é‡(Steering Vectors)æ¥ç²¾å‡†è¯†åˆ«å¹¶ç­›é€‰è¾¹ç•Œæç¤ºè¯ï¼Œä»è€Œæœ‰é’ˆå¯¹æ€§åœ°é™ä½è¿‡åº¦æ‹’ç»ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†å¤šè¯­è¨€è¯„ä¼°é›† MORBenchï¼Œä¸ºå¤§æ¨¡å‹åœ¨å®‰å…¨æ€§ä¸å®ç”¨æ€§ä¹‹é—´çš„å¹³è¡¡æä¾›äº†æ›´å…·å¯è§£é‡Šæ€§çš„è¯„ä¼°å·¥å…·ã€‚\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦èƒ½ç²¾å‡†åœ°å¸®åŠ©ä½ ç†è§£è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ã€‚å¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œæˆ–è€…æƒ³é’ˆå¯¹ RASS æ¡†æ¶çš„ç»†èŠ‚è¿›è¡Œæ·±å…¥æ¢è®¨ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18325v3",
      "published_date": "2025-05-23 19:30:49 UTC",
      "updated_date": "2025-09-17 16:44:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:10:32.995850+00:00"
    },
    {
      "arxiv_id": "2505.18323v1",
      "title": "Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation",
      "title_zh": "é’ˆå¯¹æ‰¹å¤„ç†å†…æ•°æ®çªƒå–ä¸æ¨¡å‹æ¨ç†æ“æ§çš„æ¶æ„åé—¨",
      "authors": [
        "Nicolas KÃ¼chler",
        "Ivan Petrov",
        "Conrad Grobler",
        "Ilia Shumailov"
      ],
      "abstract": "For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained unclear. In this paper we introduce a novel and significantly more potent class of backdoors that builds upon recent advancements in architectural backdoors. We demonstrate how these backdoors can be specifically engineered to exploit batched inference, a common technique for hardware utilization, enabling large-scale user data manipulation and theft. By targeting the batching process, these architectural backdoors facilitate information leakage between concurrent user requests and allow attackers to fully control model responses directed at other users within the same batch. In other words, an attacker who can change the model architecture can set and steal model inputs and outputs of other users within the same batch. We show that such attacks are not only feasible but also alarmingly effective, can be readily injected into prevalent model architectures, and represent a truly malicious threat to user privacy and system integrity. Critically, to counteract this new class of vulnerabilities, we propose a deterministic mitigation strategy that provides formal guarantees against this new attack vector, unlike prior work that relied on Large Language Models to find the backdoors. Our mitigation strategy employs a novel Information Flow Control mechanism that analyzes the model graph and proves non-interference between different user inputs within the same batch. Using our mitigation strategy we perform a large scale analysis of models hosted through Hugging Face and find over 200 models that introduce (unintended) information leakage between batch entries due to the use of dynamic quantization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç±»æ–°å‹ä¸”å…·æœ‰é«˜åº¦å¨èƒæ€§çš„ Architectural Backdoorsï¼ˆæ¶æ„åé—¨ï¼‰ï¼Œä¸“é—¨åˆ©ç”¨ç¡¬ä»¶åŠ é€Ÿä¸­å¸¸ç”¨çš„ Batched Inferenceï¼ˆæ‰¹å¤„ç†æ¨ç†ï¼‰æŠ€æœ¯æ¥å®ç°å¤§è§„æ¨¡ç”¨æˆ·æ•°æ®çš„çªƒå–ä¸æ“çºµã€‚è¯¥åé—¨é€šè¿‡é’ˆå¯¹æ‰¹å¤„ç†è¿‡ç¨‹ï¼Œå®ç°äº†åŒä¸€ Batch å†…å¹¶å‘ç”¨æˆ·è¯·æ±‚ä¹‹é—´çš„ä¿¡æ¯æ³„éœ²ï¼Œä½¿æ”»å‡»è€…èƒ½å¤Ÿè·å–æˆ–å®Œå…¨æ§åˆ¶å…¶ä»–ç”¨æˆ·çš„è¾“å…¥ä¸è¾“å‡ºã€‚ä¸ºåº”å¯¹æ­¤ç±»æ¼æ´ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäº Information Flow Controlï¼ˆä¿¡æ¯æµæ§åˆ¶ï¼‰çš„ç¡®å®šæ€§é˜²å¾¡ç­–ç•¥ï¼Œé€šè¿‡åˆ†ææ¨¡å‹å›¾æ¥ç¡®ä¿æ‰¹æ¬¡å†…ä¸åŒç”¨æˆ·è¾“å…¥é—´çš„ Non-interferenceï¼ˆäº’ä¸å¹²æ‰°æ€§ï¼‰ã€‚æœ€åï¼Œé€šè¿‡å¯¹ Hugging Face æ¨¡å‹çš„åˆ†æï¼Œç ”ç©¶å‘ç°ç”±äºä½¿ç”¨ Dynamic Quantizationï¼ˆåŠ¨æ€é‡åŒ–ï¼‰ï¼Œè¶…è¿‡ 200 ä¸ªæ¨¡å‹åœ¨æ— æ„ä¸­å¼•å…¥äº†ç±»ä¼¼çš„æ‰¹æ¬¡å†…ä¿¡æ¯æ³„éœ²é£é™©ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18323v1",
      "published_date": "2025-05-23 19:28:45 UTC",
      "updated_date": "2025-05-23 19:28:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:11:02.186782+00:00"
    },
    {
      "arxiv_id": "2505.18322v1",
      "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4",
      "title_zh": "å…¨å¤©å€™å·¥ä½œçœŸçš„ä¸å¥½å—ï¼ŸGPT-4 ç¤¾ä¼šè§„èŒƒåè§çš„è·¨æ–‡åŒ–è¯„ä¼°",
      "authors": [
        "Zhuozhuo Joy Liu",
        "Farhan Samir",
        "Mehar Bhatia",
        "Laura K. Nelson",
        "Vered Shwartz"
      ],
      "abstract": "LLMs have been demonstrated to align with the values of Western or North American cultures. Prior work predominantly showed this effect through leveraging surveys that directly ask (originally people and now also LLMs) about their values. However, it is hard to believe that LLMs would consistently apply those values in real-world scenarios. To address that, we take a bottom-up approach, asking LLMs to reason about cultural norms in narratives from different cultures. We find that GPT-4 tends to generate norms that, while not necessarily incorrect, are significantly less culture-specific. In addition, while it avoids overtly generating stereotypes, the stereotypical representations of certain cultures are merely hidden rather than suppressed in the model, and such stereotypes can be easily recovered. Addressing these challenges is a crucial step towards developing LLMs that fairly serve their diverse user base.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† GPT-4 åœ¨è·¨æ–‡åŒ–èƒŒæ™¯ä¸‹çš„ç¤¾ä¼šè§„èŒƒåè§ï¼ˆSocial Norm Biasesï¼‰ï¼Œæ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»·å€¼è§‚ä¸Šä¸è¥¿æ–¹æˆ–åŒ—ç¾æ–‡åŒ–å¯¹é½çš„å€¾å‘ã€‚ç ”ç©¶é‡‡ç”¨è‡ªä¸‹è€Œä¸Šï¼ˆbottom-up approachï¼‰çš„æ–¹æ³•ï¼Œè¦æ±‚æ¨¡å‹å¯¹ä¸åŒæ–‡åŒ–å™äº‹ä¸­çš„è§„èŒƒè¿›è¡Œæ¨ç†ï¼Œå‘ç° GPT-4 ç”Ÿæˆçš„è§„èŒƒå¾€å¾€ç¼ºä¹æ–‡åŒ–ç‰¹å¼‚æ€§ï¼ˆculture-specificï¼‰ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œè™½ç„¶æ¨¡å‹é¿å…å…¬å¼€ç”Ÿæˆåˆ»æ¿å°è±¡ï¼ˆstereotypesï¼‰ï¼Œä½†è¿™äº›è¡¨å¾åœ¨æ¨¡å‹ä¸­ä»…æ˜¯è¢«éšè—è€Œéæ¶ˆé™¤ï¼Œä¸”å¯ä»¥è¢«è½»æ˜“æ¢å¤ã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒï¼Œè¯†åˆ«å¹¶è§£å†³è¿™äº›æ–‡åŒ–åè§æ˜¯å¼€å‘èƒ½å…¬å¹³æœåŠ¡å…¨çƒå¤šå…ƒç”¨æˆ·ç¾¤ä½“çš„ LLMs çš„å…³é”®æ­¥éª¤ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18322v1",
      "published_date": "2025-05-23 19:28:00 UTC",
      "updated_date": "2025-05-23 19:28:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:10:39.684885+00:00"
    },
    {
      "arxiv_id": "2505.18315v2",
      "title": "COLORA: Efficient Fine-Tuning for Convolutional Models with a Study Case on Optical Coherence Tomography Image Classification",
      "title_zh": "COLORAï¼šå·ç§¯æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒåŠå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æå›¾åƒåˆ†ç±»æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Mariano Rivera",
        "Angello Hoyos"
      ],
      "abstract": "We introduce CoLoRA (Convolutional Low-Rank Adaptation), a parameter-efficient fine-tuning method for convolutional neural networks (CNNs). CoLoRA extends LoRA to convolutional layers by decomposing kernel updates into lightweight depthwise and pointwise components.This design reduces the number of trainable parameters to 0.2 compared to conventional fine-tuning, preserves the original model size, and allows merging updates into the pretrained weights after each epoch, keeping inference complexity unchanged. On OCTMNISTv2, CoLoRA applied to VGG16 and ResNet50 achieves up to 1 percent accuracy and 0.013 AUC improvements over strong baselines (Vision Transformers, state-space, and Kolmogorov Arnold models) while reducing per-epoch training time by nearly 20 percent. Results indicate that CoLoRA provides a stable and effective alternative to full fine-tuning for medical image classification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CoLoRA (Convolutional Low-Rank Adaptation)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ (parameter-efficient fine-tuning) æ–¹æ³•ã€‚CoLoRA é€šè¿‡å°†å·ç§¯æ ¸æ›´æ–°åˆ†è§£ä¸ºè½»é‡çº§çš„ depthwise å’Œ pointwise ç»„ä»¶ï¼Œå°†å¯è®­ç»ƒå‚æ•°é‡é™è‡³ä¼ ç»Ÿå¾®è°ƒçš„ 0.2%ï¼Œä¸”é€šè¿‡æƒé‡åˆå¹¶ç¡®ä¿äº†æ¨ç†å¤æ‚åº¦ä¸å˜ã€‚åœ¨ OCTMNISTv2 æ•°æ®é›†çš„å®éªŒä¸­ï¼Œåº”ç”¨äº VGG16 å’Œ ResNet50 çš„ CoLoRA åœ¨å‡†ç¡®ç‡å’Œ AUC ä¸Šå‡ä¼˜äº Vision Transformersã€state-space å’Œ Kolmogorov Arnold ç­‰å¼ºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å‡å°‘è¿‘ 20% çš„å•è½®è®­ç»ƒæ—¶é—´ï¼Œä¸ºåŒ»ç–—å›¾åƒåˆ†ç±»æä¾›äº†ä¸€ç§ç¨³å®šä¸”é«˜æ•ˆçš„æ›¿ä»£å…¨é‡å¾®è°ƒçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18315v2",
      "published_date": "2025-05-23 19:21:01 UTC",
      "updated_date": "2025-10-20 23:41:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:11:04.691414+00:00"
    },
    {
      "arxiv_id": "2505.20326v1",
      "title": "Cultural Awareness in Vision-Language Models: A Cross-Country Exploration",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–æ„è¯†ï¼šä¸€é¡¹è·¨å›½æ¢ç´¢",
      "authors": [
        "Avinash Madasu",
        "Vasudev Lal",
        "Phillip Howard"
      ],
      "abstract": "Vision-Language Models (VLMs) are increasingly deployed in diverse cultural contexts, yet their internal biases remain poorly understood. In this work, we propose a novel framework to systematically evaluate how VLMs encode cultural differences and biases related to race, gender, and physical traits across countries. We introduce three retrieval-based tasks: (1) Race to Country retrieval, which examines the association between individuals from specific racial groups (East Asian, White, Middle Eastern, Latino, South Asian, and Black) and different countries; (2) Personal Traits to Country retrieval, where images are paired with trait-based prompts (e.g., Smart, Honest, Criminal, Violent) to investigate potential stereotypical associations; and (3) Physical Characteristics to Country retrieval, focusing on visual attributes like skinny, young, obese, and old to explore how physical appearances are culturally linked to nations. Our findings reveal persistent biases in VLMs, highlighting how visual representations may inadvertently reinforce societal stereotypes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models, VLMs)ä¸­è·¨å›½æ–‡åŒ–å·®å¼‚åŠåè§çš„æ–°å‹æ¡†æ¶ã€‚ç ”ç©¶äººå‘˜é€šè¿‡è®¾è®¡ç§æ—ä¸å›½å®¶å…³è”(Race to Country retrieval)ã€ä¸ªäººç‰¹è´¨ä¸å›½å®¶å…³è”(Personal Traits to Country retrieval)ä»¥åŠèº«ä½“ç‰¹å¾ä¸å›½å®¶å…³è”(Physical Characteristics to Country retrieval)ä¸‰é¡¹æ£€ç´¢ä»»åŠ¡ï¼Œæ·±å…¥æ¢è®¨äº†æ¨¡å‹åœ¨ç§æ—ã€ç¤¾ä¼šç‰¹å¾ï¼ˆå¦‚ Smart, Honest, Criminalï¼‰åŠèº«ä½“å±æ€§æ–¹é¢çš„æ½œåœ¨åˆ»æ¿å°è±¡ã€‚ç ”ç©¶å‘ç°ï¼ŒVLMs å†…éƒ¨å­˜åœ¨æ˜æ˜¾çš„åè§ï¼Œå…¶è§†è§‰è¡¨å¾å¯èƒ½ä¼šåœ¨æ— æ„ä¸­å¼ºåŒ–ç°æœ‰çš„ç¤¾ä¼šåˆ»æ¿å°è±¡ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.20326v1",
      "published_date": "2025-05-23 18:47:52 UTC",
      "updated_date": "2025-05-23 18:47:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:11:20.477387+00:00"
    },
    {
      "arxiv_id": "2505.18287v1",
      "title": "Efficient Algorithms for Electing Successive Committees",
      "title_zh": "è¿ç»­å§”å‘˜ä¼šé€‰ä¸¾çš„é«˜æ•ˆç®—æ³•",
      "authors": [
        "Pallavi Jain",
        "Andrzej Kaczmarczyk"
      ],
      "abstract": "In a recently introduced model of successive committee elections (Bredereck et al., AAAI-20) for a given set of ordinal or approval preferences one aims to find a sequence of a given length of \"best\" same-size committees such that each candidate is a member of a limited number of consecutive committees. However, the practical usability of this model remains limited, as the described task turns out to be NP-hard for most selection criteria already for seeking committees of size three. Non-trivial or somewhat efficient algorithms for these cases are lacking too. Motivated by a desire to unlock the full potential of the described temporal model of committee elections, we devise (parameterized) algorithms that effectively solve the mentioned hard cases in realistic scenarios of a moderate number of candidates or of a limited time horizon.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿ç»­å§”å‘˜ä¼šé€‰ä¸¾ (Successive Committee Elections) æ¨¡å‹ä¸­å¯»æ‰¾æœ€ä¼˜å§”å‘˜ä¼šåºåˆ—æ—¶é¢ä¸´çš„è®¡ç®—éš¾é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚é‰´äºè¯¥ä»»åŠ¡åœ¨å¤šæ•°è¯„é€‰æ ‡å‡†ä¸‹å·²è¢«è¯æ˜å±äº NP-hardï¼Œä¸”ç°æœ‰ç®—æ³•æ•ˆç‡æœ‰é™ï¼Œä½œè€…è®¾è®¡äº†ä¸€ç³»åˆ— (parameterized) ç®—æ³•ã€‚è¿™äº›ç®—æ³•èƒ½å¤Ÿåœ¨å€™é€‰äººæ•°é‡é€‚ä¸­æˆ–æ—¶é—´èŒƒå›´æœ‰é™çš„ç°å®åœºæ™¯ä¸‹é«˜æ•ˆæ±‚è§£ï¼Œä¸ºè¯¥æ—¶é—´å§”å‘˜ä¼šé€‰ä¸¾æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›äº†é‡è¦çš„ç®—æ³•æ”¯æ’‘ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "primary_category": "cs.GT",
      "comment": "18 pages; 3 figures, accepted for publication in IJCAI-25",
      "pdf_url": "https://arxiv.org/pdf/2505.18287v1",
      "published_date": "2025-05-23 18:32:14 UTC",
      "updated_date": "2025-05-23 18:32:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:00.347328+00:00"
    },
    {
      "arxiv_id": "2505.18286v1",
      "title": "Single-agent or Multi-agent Systems? Why Not Both?",
      "title_zh": "å•æ™ºèƒ½ä½“è¿˜æ˜¯å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Ÿä½•ä¸å…¼è€Œæœ‰ä¹‹ï¼Ÿ",
      "authors": [
        "Mingyan Gao",
        "Yanzi Li",
        "Banruo Liu",
        "Yifan Yu",
        "Phillip Wang",
        "Ching-Yu Lin",
        "Fan Lai"
      ],
      "abstract": "Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to different large language model (LLM) agents and tools. Prior studies have reported the superior accuracy performance of MAS across diverse domains, enabled by long-horizon context tracking and error correction through role-specific agents. However, the design and deployment of MAS incur higher complexity and runtime cost compared to single-agent systems (SAS). Meanwhile, frontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in long-context reasoning, memory retention, and tool usage, mitigating many limitations that originally motivated MAS designs. In this paper, we conduct an extensive empirical study comparing MAS and SAS across various popular agentic applications. We find that the benefits of MAS over SAS diminish as LLM capabilities improve, and we propose efficient mechanisms to pinpoint the error-prone agent in MAS. Furthermore, the performance discrepancy between MAS and SAS motivates our design of a hybrid agentic paradigm, request cascading between MAS and SAS, to improve both efficiency and capability. Our design improves accuracy by 1.1-12% while reducing deployment costs by up to 20% across various agentic applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤æ‚ä»»åŠ¡ä¸­å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)ä¸å•æ™ºèƒ½ä½“ç³»ç»Ÿ(SAS)çš„æ•ˆèƒ½æƒè¡¡è¿›è¡Œäº†æ·±å…¥çš„å®è¯ç ”ç©¶ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€å‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)é•¿ä¸Šä¸‹æ–‡æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æå‡ï¼ŒMASç›¸å¯¹äºSASçš„ä¼˜åŠ¿æ­£åœ¨é€æ¸å‡å¼±ï¼Œä¸”MASé¢ä¸´æ›´é«˜çš„å¤æ‚æ€§å’Œè¿è¡Œæˆæœ¬ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ··åˆæ™ºèƒ½ä½“èŒƒå¼ï¼Œé€šè¿‡åœ¨SASå’ŒMASä¹‹é—´å®ç°è¯·æ±‚çº§è”(request cascading)ï¼ŒåŠ¨æ€ä¼˜åŒ–ä»»åŠ¡å¤„ç†è·¯å¾„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥è®¾è®¡åœ¨å¤šç§æ™ºèƒ½ä½“åº”ç”¨ä¸­æå‡äº†1.1-12%çš„å‡†ç¡®ç‡ï¼Œå¹¶æˆåŠŸå°†éƒ¨ç½²æˆæœ¬é™ä½äº†å¤šè¾¾20%ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18286v1",
      "published_date": "2025-05-23 18:30:24 UTC",
      "updated_date": "2025-05-23 18:30:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:11:58.190124+00:00"
    },
    {
      "arxiv_id": "2505.18284v1",
      "title": "Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed",
      "title_zh": "åŸºäº Tube æŸå¤±çš„æ·±åº¦ç½‘ç»œï¼šç”¨äºæ”¹è¿›é£é€Ÿæ¦‚ç‡é¢„æµ‹",
      "authors": [
        "Pritam Anand",
        "Aadesh Minz",
        "Asish Joel"
      ],
      "abstract": "Uncertainty Quantification (UQ) in wind speed forecasting is a critical challenge in wind power production due to the inherently volatile nature of wind. By quantifying the associated risks and returns, UQ supports more effective decision-making for grid operations and participation in the electricity market. In this paper, we design a sequence of deep learning based probabilistic forecasting methods by using the Tube loss function for wind speed forecasting. The Tube loss function is a simple and model agnostic Prediction Interval (PI) estimation approach and can obtain the narrow PI with asymptotical coverage guarantees without any distribution assumption. Our deep probabilistic forecasting models effectively incorporate popular architectures such as LSTM, GRU, and TCN within the Tube loss framework. We further design a simple yet effective heuristic for tuning the $Î´$ parameter of the Tube loss function so that our deep forecasting models obtain the narrower PI without compromising its calibration ability. We have considered three wind datasets, containing the hourly recording of the wind speed, collected from three distinct location namely Jaisalmer, Los Angeles and San Fransico. Our numerical results demonstrate that the proposed deep forecasting models produce more reliable and narrower PIs compared to recently developed probabilistic wind forecasting methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç³»åˆ—åŸºäº Tube loss å‡½æ•°çš„æ·±åº¦å­¦ä¹ æ¦‚ç‡é¢„æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é£é€Ÿé¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–(Uncertainty Quantification)éš¾é¢˜ã€‚Tube loss æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„é¢„æµ‹åŒºé—´(Prediction Interval, PI)ä¼°è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ— åˆ†å¸ƒå‡è®¾çš„æƒ…å†µä¸‹æä¾›å…·æœ‰æ¸è¿›è¦†ç›–ä¿è¯çš„çª„é¢„æµ‹åŒºé—´ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ•´åˆäº† LSTMã€GRU å’Œ TCN ç­‰æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§å¯å‘å¼æ–¹æ³•æ¥ä¼˜åŒ– Tube loss çš„ $\\delta$ å‚æ•°ï¼Œä»è€Œåœ¨ä¿è¯æ ¡å‡†èƒ½åŠ›çš„åŒæ—¶è·å¾—æ›´çª„çš„ PIã€‚åœ¨ Jaisalmerã€Los Angeles å’Œ San Francisco æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„é¢„æµ‹åŒºé—´æ¯”ç°æœ‰çš„æ¦‚ç‡é£é€Ÿé¢„æµ‹æ–¹æ³•æ›´å¯é ä¸”æ›´çª„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18284v1",
      "published_date": "2025-05-23 18:29:07 UTC",
      "updated_date": "2025-05-23 18:29:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:05.319716+00:00"
    },
    {
      "arxiv_id": "2505.18283v1",
      "title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification",
      "title_zh": "TAGSï¼šé›†æˆæ£€ç´¢å¢å¼ºæ¨ç†ä¸éªŒè¯çš„æµ‹è¯•æ—¶é€šæ‰-ä¸“æ‰æ¡†æ¶",
      "authors": [
        "Jianghao Wu",
        "Feilong Tang",
        "Yulong Li",
        "Ming Hu",
        "Haochen Xue",
        "Shoaib Jameel",
        "Yutong Xie",
        "Imran Razzak"
      ],
      "abstract": "Recent advances such as Chain-of-Thought prompting have significantly improved large language models (LLMs) in zero-shot medical reasoning. However, prompting-based methods often remain shallow and unstable, while fine-tuned medical LLMs suffer from poor generalization under distribution shifts and limited adaptability to unseen clinical scenarios. To address these limitations, we present TAGS, a test-time framework that combines a broadly capable generalist with a domain-specific specialist to offer complementary perspectives without any model fine-tuning or parameter updates. To support this generalist-specialist reasoning process, we introduce two auxiliary modules: a hierarchical retrieval mechanism that provides multi-scale exemplars by selecting examples based on both semantic and rationale-level similarity, and a reliability scorer that evaluates reasoning consistency to guide final answer aggregation. TAGS achieves strong performance across nine MedQA benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several fine-tuned medical LLMs, without any parameter updates. The code will be available at https://github.com/JianghaoWu/TAGS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TAGSï¼Œä¸€ç§åœ¨æµ‹è¯•é˜¶æ®µ(Test-time)è¿è¡Œçš„å…¨æ‰-ä¸“æ‰(Generalist-Specialist)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Chain-of-Thoughtæ¨ç†ä¸­çš„ä¸ç¨³å®šæ€§ï¼Œä»¥åŠå¾®è°ƒæ¨¡å‹åœ¨é¢å¯¹åˆ†å¸ƒåç§»æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ— éœ€ä»»ä½•å‚æ•°æ›´æ–°ï¼Œé€šè¿‡ç»“åˆé€šç”¨æ¨¡å‹ä¸é¢†åŸŸç‰¹å®šä¸“å®¶çš„äº’è¡¥è§†è§’ï¼Œå¹¶å¼•å…¥åˆ†å±‚æ£€ç´¢æœºåˆ¶(Hierarchical Retrieval)å’Œå¯é æ€§è¯„åˆ†å™¨(Reliability Scorer)æ¥æä¾›å¤šå°ºåº¦èŒƒä¾‹å¹¶å¼•å¯¼ç­”æ¡ˆèšåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAGSåœ¨ä¹é¡¹MedQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œä½¿GPT-4oå’ŒDeepSeek-R1çš„å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†13.8%å’Œ16.8%ï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¶Šäº†å¤šç§ç»è¿‡ä¸“é—¨å¾®è°ƒçš„åŒ»å­¦å¤§æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages including references, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18283v1",
      "published_date": "2025-05-23 18:28:59 UTC",
      "updated_date": "2025-05-23 18:28:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:11:51.735026+00:00"
    },
    {
      "arxiv_id": "2505.20325v1",
      "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence",
      "title_zh": "Guided by Gutï¼šåŸºäºå¼ºåŒ–å†…åœ¨ç½®ä¿¡åº¦çš„é«˜æ•ˆæµ‹è¯•æ—¶æ‰©å±•",
      "authors": [
        "Amirhosein Ghasemabadi",
        "Keith G. Mills",
        "Baochun Li",
        "Di Niu"
      ],
      "abstract": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs a lightweight tree search guided solely by intrinsic LLM signals, token-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via a targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B-70B parameters), while reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG achieves comparable accuracy with 8x faster inference speeds and 4-5x lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Guided by Gut (GG)ï¼Œä¸€ç§é«˜æ•ˆçš„è‡ªå¼•å¯¼æµ‹è¯•æ—¶ç¼©æ”¾ (Test-Time Scaling, TTS) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸­å¤–éƒ¨éªŒè¯æ¨¡å‹å¸¦æ¥çš„é«˜æ˜‚è®¡ç®—æˆæœ¬é—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è½»é‡çº§æ ‘æœç´¢ï¼Œä»…ä¾é æ¨¡å‹çš„å†…åœ¨ä¿¡å·ï¼ˆtoken-level confidence å’Œ step noveltyï¼‰è¿›è¡Œå¼•å¯¼ï¼Œå¹¶åˆ©ç”¨é’ˆå¯¹æ€§çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) å¾®è°ƒæå‡äº†å†…åœ¨ç½®ä¿¡åº¦ä¼°è®¡çš„å¯é æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒGG èƒ½è®© 1.5B å‚æ•°çš„å°æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°æˆ–è¶…è¿‡ 32B-70B å¤§æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œä¸”æ˜¾å­˜å ç”¨é™ä½é«˜è¾¾ 10 å€ã€‚ä¸ä¼ ç»Ÿçš„ PRM æˆ– BoN ç­–ç•¥ç›¸æ¯”ï¼ŒGG åœ¨ä¿æŒåŒç­‰å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°äº† 8 å€çš„æ¨ç†åŠ é€Ÿå’Œçº¦ 50% çš„ KV cache æ˜¾å­˜èŠ‚çœï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²æ¨ç†ç¼©æ”¾æŠ€æœ¯æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.20325v1",
      "published_date": "2025-05-23 18:19:09 UTC",
      "updated_date": "2025-05-23 18:19:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:10.479323+00:00"
    },
    {
      "arxiv_id": "2505.18282v1",
      "title": "Towards a Quantum-classical Augmented Network",
      "title_zh": "è¿ˆå‘é‡å­-ç»å…¸å¢å¼ºç½‘ç»œ",
      "authors": [
        "Nitin Jha",
        "Abhishek Parakh",
        "Mahadevan Subramaniam"
      ],
      "abstract": "In the past decade, several small-scale quantum key distribution networks have been established. However, the deployment of large-scale quantum networks depends on the development of quantum repeaters, quantum channels, quantum memories, and quantum network protocols. To improve the security of existing networks and adopt currently feasible quantum technologies, the next step is to augment classical networks with quantum devices, properties, and phenomena. To achieve this, we propose a change in the structure of the HTTP protocol such that it can carry both quantum and classical payload. This work lays the foundation for dividing one single network packet into classical and quantum payloads depending on the privacy needs. We implement logistic regression, CNN, LSTM, and BiLSTM models to classify the privacy label for outgoing communications. This enables reduced utilization of quantum resources allowing for a more efficient secure quantum network design. Experimental results using the proposed methods are presented.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é‡å­-ç»å…¸å¢å¼ºç½‘ç»œ (Quantum-classical Augmented Network)ï¼Œæ—¨åœ¨åˆ©ç”¨ç°æœ‰é‡å­æŠ€æœ¯æå‡ç»å…¸ç½‘ç»œçš„å®‰å…¨æ€§å¹¶ä¼˜åŒ–èµ„æºåˆ©ç”¨ã€‚ç ”ç©¶æ ¸å¿ƒåœ¨äºä¿®æ”¹ HTTP åè®®ç»“æ„ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®éšç§éœ€æ±‚åŒæ—¶æ‰¿è½½é‡å­å’Œç»å…¸æœ‰æ•ˆè½½è· (payload)ã€‚é€šè¿‡åº”ç”¨ Logistic Regressionã€CNNã€LSTM å’Œ BiLSTM ç­‰æœºå™¨å­¦ä¹ æ¨¡å‹å¯¹é€šä¿¡å†…å®¹çš„éšç§çº§åˆ«è¿›è¡Œè‡ªåŠ¨åˆ†ç±»ï¼Œè¯¥æ–¹æ¡ˆå®ç°äº†é‡å­èµ„æºçš„æŒ‰éœ€åˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆé™ä½äº†é‡å­èµ„æºçš„æ¶ˆè€—ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€å®ç”¨çš„å®‰å…¨é‡å­ç½‘ç»œè®¾è®¡å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CR",
        "cs.NI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18282v1",
      "published_date": "2025-05-23 18:17:07 UTC",
      "updated_date": "2025-05-23 18:17:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:33.096145+00:00"
    },
    {
      "arxiv_id": "2505.18280v1",
      "title": "Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior",
      "title_zh": "åŸºäº R2D2 å…ˆéªŒçš„è´å¶æ–¯ç¥ç»ç½‘ç»œç‰¹å¾ä¿æŒæ”¶ç¼©",
      "authors": [
        "Tsai Hor Chan",
        "Dora Yan Zhang",
        "Guosheng Yin",
        "Lequan Yu"
      ],
      "abstract": "Bayesian neural networks (BNNs) treat neural network weights as random variables, which aim to provide posterior uncertainty estimates and avoid overfitting by performing inference on the posterior weights. However, the selection of appropriate prior distributions remains a challenging task, and BNNs may suffer from catastrophic inflated variance or poor predictive performance when poor choices are made for the priors. Existing BNN designs apply different priors to weights, while the behaviours of these priors make it difficult to sufficiently shrink noisy signals or they are prone to overshrinking important signals in the weights. To alleviate this problem, we propose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition (R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant coefficients towards zero, while preventing key features from over-shrinkage. To approximate the posterior distribution of weights more accurately, we further propose a variational Gibbs inference algorithm that combines the Gibbs updating procedure and gradient-based optimization. This strategy enhances stability and consistency in estimation when the variational objective involving the shrinkage parameters is non-convex. We also analyze the evidence lower bound (ELBO) and the posterior concentration rates from a theoretical perspective. Experiments on both natural and medical image classification and uncertainty estimation tasks demonstrate satisfactory performance of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† R2D2-Netï¼Œæ—¨åœ¨è§£å†³è´å¶æ–¯ç¥ç»ç½‘ç»œ (BNNs) ä¸­å› å…ˆéªŒåˆ†å¸ƒé€‰æ‹©ä¸å½“å¯¼è‡´çš„å™ªå£°æ”¶ç¼©ä¸è¶³æˆ–é‡è¦ä¿¡å·è¿‡åº¦æ”¶ç¼© (overshrinking) çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹å°† $R^2$-induced Dirichlet Decomposition (R2D2) å…ˆéªŒå¼•å…¥æƒé‡è®¾è®¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå°†æ— å…³ç³»æ•°æ”¶ç¼©è‡³é›¶ï¼ŒåŒæ—¶é˜²æ­¢å…³é”®ç‰¹å¾è¢«è¿‡åº¦æ”¶ç¼©ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ç»“åˆ Gibbs æ›´æ–°ä¸æ¢¯åº¦ä¼˜åŒ–çš„ variational Gibbs inference ç®—æ³•ï¼Œä»¥æå‡éå‡¸ç›®æ ‡å‡½æ•°ä¸‹åéªŒåˆ†å¸ƒè¿‘ä¼¼çš„ç¨³å®šæ€§ã€‚ç†è®ºåˆ†ææ¶µç›–äº†è¯æ®ä¸‹ç•Œ (ELBO) å’ŒåéªŒæ”¶ç¼©ç‡ (posterior concentration rates)ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨è‡ªç„¶ä¸åŒ»å­¦å›¾åƒåˆ†ç±»åŠä¸ç¡®å®šæ€§ä¼°è®¡ä»»åŠ¡ä¸­å‡å…·æœ‰ä¼˜å¼‚è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in TPAMI",
      "pdf_url": "https://arxiv.org/pdf/2505.18280v1",
      "published_date": "2025-05-23 18:15:44 UTC",
      "updated_date": "2025-05-23 18:15:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:10.008303+00:00"
    },
    {
      "arxiv_id": "2505.18279v1",
      "title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control",
      "title_zh": "Collaborative Memoryï¼šå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä¸­å…·æœ‰åŠ¨æ€è®¿é—®æ§åˆ¶çš„å¤šç”¨æˆ·è®°å¿†å…±äº«",
      "authors": [
        "Alireza Rezazadeh",
        "Zichao Li",
        "Ange Lou",
        "Yuying Zhao",
        "Wei Wei",
        "Yujia Bao"
      ],
      "abstract": "Complex tasks are increasingly delegated to ensembles of specialized LLM-based agents that reason, communicate, and coordinate actions-both among themselves and through interactions with external tools, APIs, and databases. While persistent memory has been shown to enhance single-agent performance, most approaches assume a monolithic, single-user context-overlooking the benefits and challenges of knowledge transfer across users under dynamic, asymmetric permissions. We introduce Collaborative Memory, a framework for multi-user, multi-agent environments with asymmetric, time-evolving access controls encoded as bipartite graphs linking users, agents, and resources. Our system maintains two memory tiers: (1) private memory-private fragments visible only to their originating user; and (2) shared memory-selectively shared fragments. Each fragment carries immutable provenance attributes (contributing agents, accessed resources, and timestamps) to support retrospective permission checks. Granular read policies enforce current user-agent-resource constraints and project existing memory fragments into filtered transformed views. Write policies determine fragment retention and sharing, applying context-aware transformations to update the memory. Both policies may be designed conditioned on system, agent, and user-level information. Our framework enables safe, efficient, and interpretable cross-user knowledge sharing, with provable adherence to asymmetric, time-varying policies and full auditability of memory operations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Collaborative Memoryï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šç”¨æˆ·ã€å¤šæ™ºèƒ½ä½“ç¯å¢ƒçš„åä½œè®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è·¨ç”¨æˆ·çŸ¥è¯†è½¬ç§»ä¸­çš„åŠ¨æ€å’Œéå¯¹ç§°æƒé™ç®¡ç†æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åŒå±‚è®°å¿†ç»“æ„ï¼ŒåŒ…æ‹¬ä»…æºç”¨æˆ·å¯è§çš„ private memory å’Œé€‰æ‹©æ€§å…±äº«çš„ shared memoryã€‚é€šè¿‡å°†ç”¨æˆ·ã€æ™ºèƒ½ä½“å’Œèµ„æºç¼–ç ä¸ºäºŒåˆ†å›¾ (bipartite graphs)ï¼Œè¯¥æ¡†æ¶å®ç°äº†åŠ¨æ€çš„è®¿é—®æ§åˆ¶ï¼Œå¹¶åˆ©ç”¨ä¸å¯å˜çš„æº¯æºå±æ€§ (provenance attributes) æ”¯æŒè¿½æº¯æ€§æƒé™æ£€æŸ¥ã€‚é€šè¿‡ç»†ç²’åº¦çš„è¯»å†™ç­–ç•¥ (Granular read/write policies)ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®å½“å‰çº¦æŸæ¡ä»¶å¯¹è®°å¿†ç‰‡æ®µè¿›è¡Œè¿‡æ»¤ã€è½¬æ¢å’Œæ›´æ–°ã€‚è¯¥æ¡†æ¶å®ç°äº†å®‰å…¨ã€é«˜æ•ˆä¸”å¯è§£é‡Šçš„è·¨ç”¨æˆ·çŸ¥è¯†å…±äº«ï¼Œç¡®ä¿äº†å¯¹éšæ—¶é—´å˜åŒ–çš„éå¯¹ç§°ç­–ç•¥çš„ä¸¥æ ¼éµå®ˆå’Œæ“ä½œçš„å¯å®¡è®¡æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18279v1",
      "published_date": "2025-05-23 18:14:57 UTC",
      "updated_date": "2025-05-23 18:14:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:14.279365+00:00"
    },
    {
      "arxiv_id": "2505.20324v1",
      "title": "Evaluating the Energy-Efficiency of the Code Generated by LLMs",
      "title_zh": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆä»£ç çš„èƒ½æ•ˆ",
      "authors": [
        "Md Arman Islam",
        "Devi Varaprasad Jonnala",
        "Ritika Rekhi",
        "Pratik Pokharel",
        "Siddharth Cilamkoti",
        "Asif Imran",
        "Tevfik Kosar",
        "Bekir Turkkan"
      ],
      "abstract": "As the quality of code generated by Large Language Models (LLMs) improves, their adoption in the software industry for automated code generation continues to grow. Researchers primarily focus on enhancing the functional correctness of the generated code while commonly overlooking its energy efficiency and environmental impact. This paper investigates the energy efficiency of the code generated by 20 popular LLMs for 878 programming problems of varying difficulty levels and diverse algorithmic categories selected from the LeetCode platform by comparing them against canonical human-written solutions. Although LLMs can produce functionally correct results in most cases, our findings show that the performance and energy efficiency of LLM-produced solutions are often far below those of human-written solutions. Among the studied LLMs, DeepSeek-v3 and GPT-4o generate the most energy-efficient code, whereas Grok-2 and Gemini-1.5-Pro are among the least energy-efficient models. On average, human-generated canonical solutions are approximately 1.17 times more energy efficient than DeepSeek-v3, 1.21 times more energy efficient than GPT-4o, and over 2 times more energy efficient than Grok-2 and Gemini-1.5-Pro. For specific algorithmic groups such as dynamic programming, backtracking, and bit manipulation, LLM-generated code can consume up to 450 times more energy than human-generated canonical solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†20ä¸ªä¸»æµå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨878ä¸ªä¸åŒéš¾åº¦å’Œç±»åˆ«çš„LeetCodeç¼–ç¨‹é—®é¢˜ä¸­ç”Ÿæˆä»£ç çš„èƒ½æ•ˆ(Energy Efficiency)ï¼Œå¹¶å°†å…¶ä¸äººç±»ç¼–å†™çš„æ ‡å‡†æ–¹æ¡ˆè¿›è¡Œå¯¹æ¯”ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶LLMsç”Ÿæˆçš„ä»£ç åœ¨åŠŸèƒ½æ­£ç¡®æ€§ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å…¶èƒ½æ•ˆæ™®éæ˜¾è‘—ä½äºäººç±»æ°´å¹³ã€‚åœ¨å—è¯•æ¨¡å‹ä¸­ï¼ŒDeepSeek-v3å’ŒGPT-4oç”Ÿæˆçš„ä»£ç èƒ½æ•ˆæœ€é«˜ï¼Œè€ŒGrok-2å’ŒGemini-1.5-Proåˆ™å±äºèƒ½æ•ˆè¡¨ç°æœ€å·®çš„æ¢¯é˜Ÿã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œäººç±»æ–¹æ¡ˆçš„èƒ½æ•ˆé€šå¸¸æ¯”DeepSeek-v3é«˜å‡º1.17å€ï¼Œè€Œåœ¨åŠ¨æ€è§„åˆ’(Dynamic Programming)ã€å›æº¯(Backtracking)å’Œä½è¿ç®—(Bit Manipulation)ç­‰ç‰¹å®šç®—æ³•ç±»åˆ«ä¸­ï¼ŒLLMç”Ÿæˆä»£ç çš„èƒ½è€—æœ€é«˜å¯è¾¾äººç±»æ–¹æ¡ˆçš„450å€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.20324v1",
      "published_date": "2025-05-23 18:13:27 UTC",
      "updated_date": "2025-05-23 18:13:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:19.342591+00:00"
    },
    {
      "arxiv_id": "2505.18277v2",
      "title": "The end of radical concept nativism",
      "title_zh": "æ¿€è¿›æ¦‚å¿µå¤©èµ‹è®ºçš„ç»ˆç»“",
      "authors": [
        "Joshua S. Rule",
        "Steven T. Piantadosi"
      ],
      "abstract": "Though humans seem to be remarkable learners, arguments in cognitive science and philosophy of mind have long maintained that learning something fundamentally new is impossible. Specifically, Jerry Fodor's arguments for radical concept nativism hold that most, if not all, concepts are innate and that what many call concept learning never actually leads to the acquisition of new concepts. These arguments have deeply affected cognitive science, and many believe that the counterarguments to radical concept nativism have been either unsuccessful or only apply to a narrow class of concepts. This paper first reviews the features and limitations of prior arguments. We then identify three critical points - related to issues of expressive power, conceptual structure, and concept possession - at which the arguments in favor of radical concept nativism diverge from describing actual human cognition. We use ideas from computer science and information theory to formalize the relevant ideas in ways that are arguably more scientifically productive. We conclude that, as a result, there is an important sense in which people do indeed learn new concepts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è®¤çŸ¥ç§‘å­¦å’Œå¿ƒçµå“²å­¦ä¸­é•¿æœŸå­˜åœ¨çš„ radical concept nativismï¼ˆæ¿€è¿›æ¦‚å¿µå…ˆéªŒè®ºï¼‰è§‚ç‚¹ï¼Œç‰¹åˆ«æ˜¯ Jerry Fodor å…³äºâ€œäººç±»æ— æ³•å­¦ä¹ æ ¹æœ¬æ€§æ–°æ¦‚å¿µâ€çš„è®ºæ–­è¿›è¡Œäº†é‡æ–°è¯„ä¼°ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„åé©³è§‚ç‚¹å¾€å¾€è¢«è®¤ä¸ºä¸å……åˆ†æˆ–é€‚ç”¨èŒƒå›´æœ‰é™ï¼Œå› æ­¤è¯¥æ–‡é€šè¿‡åˆ†æè¡¨è¾¾èƒ½åŠ› (expressive power)ã€æ¦‚å¿µç»“æ„ (conceptual structure) å’Œæ¦‚å¿µæ‹¥æœ‰ (concept possession) è¿™ä¸‰ä¸ªå…³é”®ç‚¹ï¼Œæ­ç¤ºäº† radical concept nativism ä¸å®é™…äººç±»è®¤çŸ¥ä¹‹é—´çš„åå·®ã€‚ç ”ç©¶é€šè¿‡å¼•å…¥è®¡ç®—æœºç§‘å­¦ (computer science) å’Œä¿¡æ¯è®º (information theory) çš„æ€æƒ³ï¼Œå¯¹ç›¸å…³æ¦‚å¿µè¿›è¡Œäº†å½¢å¼åŒ–å¤„ç†ï¼Œä»è€Œæä¾›äº†ä¸€ç§æ›´å…·ç§‘å­¦ç”Ÿäº§åŠ›çš„è§£é‡Šæ¡†æ¶ã€‚æœ€ç»ˆå¾—å‡ºç»“è®ºï¼Œä»é‡è¦æ„ä¹‰ä¸Šè®²ï¼Œäººç±»ç¡®å®èƒ½å¤Ÿå­¦ä¹ åˆ°å…¨æ–°çš„æ¦‚å¿µï¼Œæœ‰åŠ›åœ°åé©³äº†æ¿€è¿›æ¦‚å¿µå…ˆéªŒè®ºã€‚",
      "categories": [
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18277v2",
      "published_date": "2025-05-23 18:12:38 UTC",
      "updated_date": "2025-07-09 16:18:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:21.930854+00:00"
    },
    {
      "arxiv_id": "2505.18266v1",
      "title": "Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks",
      "title_zh": "æ­ç¤ºç¥ç»ç½‘ç»œæ¨¡åŠ è¿ç®—çš„æ™®é€‚æŠ½è±¡ç®—æ³•",
      "authors": [
        "Gavin McCracken",
        "Gabriela Moisescu-Pareja",
        "Vincent Letourneau",
        "Doina Precup",
        "Jonathan Love"
      ],
      "abstract": "We propose a testable universality hypothesis, asserting that seemingly disparate neural network solutions observed in the simple task of modular addition are unified under a common abstract algorithm. While prior work interpreted variations in neuron-level representations as evidence for distinct algorithms, we demonstrate - through multi-level analyses spanning neurons, neuron clusters, and entire networks - that multilayer perceptrons and transformers universally implement the abstract algorithm we call the approximate Chinese Remainder Theorem. Crucially, we introduce approximate cosets and show that neurons activate exclusively on them. Furthermore, our theory works for deep neural networks (DNNs). It predicts that universally learned solutions in DNNs with trainable embeddings or more than one hidden layer require only O(log n) features, a result we empirically confirm. This work thus provides the first theory-backed interpretation of multilayer networks solving modular addition. It advances generalizable interpretability and opens a testable universality hypothesis for group multiplication beyond modular addition.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¯æµ‹è¯•çš„æ™®éæ€§å‡è®¾(universality hypothesis)ï¼Œæ­ç¤ºäº†ç¥ç»ç½‘ç»œåœ¨æ‰§è¡Œæ¨¡åŠ æ³•(modular addition)ä»»åŠ¡æ—¶ï¼Œå°½ç®¡ç¥ç»å…ƒå±‚é¢çš„è¡¨å¾çœ‹ä¼¼ä¸åŒï¼Œä½†æœ¬è´¨ä¸Šéƒ½éµå¾ªä¸€ç§ç»Ÿä¸€çš„æŠ½è±¡ç®—æ³•ã€‚é€šè¿‡å¯¹å¤šå±‚æ„ŸçŸ¥æœº(MLPs)å’ŒTransformerçš„å¤šå±‚çº§åˆ†æï¼Œç ”ç©¶è¯æ˜è¿™äº›æ¨¡å‹æ™®éå®ç°äº†è¢«ç§°ä¸º**è¿‘ä¼¼ä¸­å›½å‰©ä½™å®šç†(approximate Chinese Remainder Theorem)**çš„ç®—æ³•ã€‚ä½œè€…å¼•å…¥äº†**è¿‘ä¼¼é™ªé›†(approximate cosets)**çš„æ¦‚å¿µï¼Œå¹¶å‘ç°ç¥ç»å…ƒä»…åœ¨è¿™äº›é™ªé›†ä¸Šè¢«æ¿€æ´»ã€‚è¯¥ç†è®ºè¿›ä¸€æ­¥æ‰©å±•è‡³æ·±å±‚ç¥ç»ç½‘ç»œ(DNNs)ï¼Œé¢„æµ‹å¹¶è¯å®äº†åœ¨å…·æœ‰å¯è®­ç»ƒåµŒå…¥æˆ–å¤šéšè—å±‚çš„ç½‘ç»œä¸­ï¼Œä»…éœ€ $O(\\log n)$ ä¸ªç‰¹å¾å³å¯å®Œæˆä»»åŠ¡ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤šå±‚ç½‘ç»œè§£å†³æ¨¡åŠ æ³•æä¾›äº†é¦–ä¸ªç†è®ºæ”¯æŒçš„è§£é‡Šï¼Œæ¨åŠ¨äº†é€šç”¨å¯è§£é‡Šæ€§(generalizable interpretability)çš„ç ”ç©¶ï¼Œå¹¶ä¸ºæ¢ç´¢æ›´å¤æ‚çš„ç¾¤ä¹˜æ³•(group multiplication)å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18266v1",
      "published_date": "2025-05-23 18:02:46 UTC",
      "updated_date": "2025-05-23 18:02:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:52.990760+00:00"
    },
    {
      "arxiv_id": "2505.20323v2",
      "title": "PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus",
      "title_zh": "PMOA-TTSï¼šPubMed å¼€æ”¾è·å–æ–‡æœ¬æ—¶é—´åºåˆ—è¯­æ–™åº“",
      "authors": [
        "Shahriar Noroozizadeh",
        "Sayantan Kumar",
        "George H. Chen",
        "Jeremy C. Weiss"
      ],
      "abstract": "Clinical narratives encode temporal dynamics essential for modeling patient trajectories, yet large-scale temporally annotated resources are scarce. We introduce PMOA-TTS, a corpus of 124,699 single-patient PubMed Open Access case reports converted into structured textual timelines of (event, time) pairs using a scalable large-language-model pipeline (Llama 3.3 70B and DeepSeek-R1). The corpus comprises over 5.6 million timestamped events, alongside extracted demographics and diagnoses. Technical validation uses a clinician-curated gold set and three measures: semantic event matching, temporal concordance (c-index), and alignment error summarized with Area Under the Log-Time CDF (AULTC). We benchmark alternative prompting and model choices and provide documentation to support reproduction. PMOA-TTS enables research on timeline extraction, temporal reasoning, survival modeling and event forecasting from narrative text, and offers broad diagnostic and demographic coverage. Data and code are openly available in public repositories.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† PMOA-TTSï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 124,699 ä»½ PubMed Open Access å•ä¾‹æ‚£è€…ç—…ä¾‹æŠ¥å‘Šçš„å¤§è§„æ¨¡è¯­æ–™åº“ã€‚åˆ©ç”¨ Llama 3.3 70B å’Œ DeepSeek-R1 ç­‰å¤§è¯­è¨€æ¨¡å‹ (LLMs) æµæ°´çº¿ï¼Œè¯¥ç ”ç©¶å°†ä¸´åºŠå™è¿°è½¬åŒ–ä¸ºç”±â€œ(äº‹ä»¶, æ—¶é—´)â€å¯¹æ„æˆçš„ç»“æ„åŒ–æ–‡æœ¬æ—¶é—´çº¿ï¼Œæ¶µç›–è¶…è¿‡ 560 ä¸‡ä¸ªå¸¦æ—¶é—´æˆ³çš„äº‹ä»¶ä»¥åŠäººå£ç»Ÿè®¡å­¦å’Œè¯Šæ–­ä¿¡æ¯ã€‚æŠ€æœ¯éªŒè¯é‡‡ç”¨äº†ä¸´åºŠåŒ»ç”Ÿç­–åˆ’çš„é»„é‡‘æ•°æ®é›† (gold set)ï¼Œé€šè¿‡è¯­ä¹‰äº‹ä»¶åŒ¹é…ã€æ—¶é—´ä¸€è‡´æ€§ (c-index) å’Œå¯¹æ•°æ—¶é—´ç´¯ç§¯åˆ†å¸ƒå‡½æ•°æ›²çº¿ä¸‹é¢ç§¯ (AULTC) ç­‰æŒ‡æ ‡å¯¹ä¸åŒæ¨¡å‹å’Œæç¤ºç­–ç•¥è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚PMOA-TTS ä¸ºæ—¶é—´çº¿æå–ã€æ—¶é—´æ¨ç†ã€ç”Ÿå­˜å»ºæ¨¡å’Œäº‹ä»¶é¢„æµ‹ç­‰ç ”ç©¶æä¾›äº†å¹¿æ³›çš„è¯Šæ–­å’Œäººå£ç»Ÿè®¡è¦†ç›–ï¼Œä¸”ç›¸å…³æ•°æ®ä¸ä»£ç å‡å·²å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.20323v2",
      "published_date": "2025-05-23 18:01:09 UTC",
      "updated_date": "2026-01-15 18:18:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:12:48.685533+00:00"
    },
    {
      "arxiv_id": "2505.18151v2",
      "title": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions",
      "title_zh": "WonderPlayï¼šåŸºäºå•å¼ å›¾åƒä¸åŠ¨ä½œçš„åŠ¨æ€ 3D åœºæ™¯ç”Ÿæˆ",
      "authors": [
        "Zizhang Li",
        "Hong-Xing Yu",
        "Wei Liu",
        "Yin Yang",
        "Charles Herrmann",
        "Gordon Wetzstein",
        "Jiajun Wu"
      ],
      "abstract": "WonderPlay is a novel framework integrating physics simulation with video generation for generating action-conditioned dynamic 3D scenes from a single image. While prior works are restricted to rigid body or simple elastic dynamics, WonderPlay features a hybrid generative simulator to synthesize a wide range of 3D dynamics. The hybrid generative simulator first uses a physics solver to simulate coarse 3D dynamics, which subsequently conditions a video generator to produce a video with finer, more realistic motion. The generated video is then used to update the simulated dynamic 3D scene, closing the loop between the physics solver and the video generator. This approach enables intuitive user control to be combined with the accurate dynamics of physics-based simulators and the expressivity of diffusion-based video generators. Experimental results demonstrate that WonderPlay enables users to interact with various scenes of diverse content, including cloth, sand, snow, liquid, smoke, elastic, and rigid bodies -- all using a single image input. Code will be made public. Project website: https://kyleleey.github.io/WonderPlay/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WonderPlayï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ç‰©ç†æ¨¡æ‹Ÿä¸è§†é¢‘ç”Ÿæˆç›¸ç»“åˆçš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å•å¼ å›¾åƒå’ŒåŠ¨ä½œæŒ‡ä»¤ç”ŸæˆåŠ¨æ€ 3D åœºæ™¯ã€‚å…¶æ ¸å¿ƒåœ¨äºä¸€ç§æ··åˆç”Ÿæˆæ¨¡æ‹Ÿå™¨ (Hybrid Generative Simulator)ï¼Œé¦–å…ˆåˆ©ç”¨ç‰©ç†æ±‚è§£å™¨ (Physics Solver) æ¨¡æ‹Ÿç²—ç•¥çš„åŠ¨åŠ›å­¦ï¼Œéšåå¼•å¯¼è§†é¢‘ç”Ÿæˆå™¨åˆæˆæ›´ç»†è…»ã€çœŸå®çš„è¿åŠ¨ï¼Œå¹¶æœ€ç»ˆé€šè¿‡è§†é¢‘åé¦ˆæ›´æ–° 3D åœºæ™¯å®ç°é—­ç¯ã€‚è¯¥æ–¹æ³•å°†ç‰©ç†æ¨¡æ‹Ÿçš„å‡†ç¡®æ€§ä¸æ‰©æ•£æ¨¡å‹ (Diffusion Models) çš„è¡¨ç°åŠ›ç›¸ç»“åˆï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡å•å¼ å›¾åƒäº¤äº’å¼åœ°ç”Ÿæˆå¸ƒæ–™ã€æ²™ã€é›ªã€æ¶²ä½“ã€çƒŸé›¾ä»¥åŠå¼¹æ€§ä½“å’Œåˆšä½“ç­‰å¤šç§æè´¨çš„å¤æ‚åŠ¨æ€æ•ˆæœã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "ICCV 2025 (Highlight). The first two authors contributed equally. Project website: https://kyleleey.github.io/WonderPlay/",
      "pdf_url": "https://arxiv.org/pdf/2505.18151v2",
      "published_date": "2025-05-23 17:59:24 UTC",
      "updated_date": "2025-11-29 04:26:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:16.483485+00:00"
    },
    {
      "arxiv_id": "2505.20322v2",
      "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms",
      "title_zh": "è¶…è¶Šæç¤ºå·¥ç¨‹ï¼šåŸºäºç›®æ ‡åŸå­å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹é²æ£’è¡Œä¸ºæ§åˆ¶",
      "authors": [
        "Mengru Wang",
        "Ziwen Xu",
        "Shengyu Mao",
        "Shumin Deng",
        "Zhaopeng Tu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å†…éƒ¨è¡¨å¾é«˜åº¦äº¤ç»‡å¯¼è‡´çš„æ§åˆ¶ç²¾åº¦å—é™åŠå‰¯ä½œç”¨é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Steering Target Atoms (STA) çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSparse Autoencoders, SAEï¼‰éš”ç¦»å¹¶æ“çºµè§£æ„åçš„çŸ¥è¯†ç»„ä»¶ï¼ˆdisentangled knowledge componentsï¼‰ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹è¡Œä¸ºæ›´ç²¾å‡†çš„å®‰å…¨æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒSTA åœ¨å¯¹æŠ—æ€§åœºæ™¯ä¸­è¡¨ç°å‡ºæ¯”ä¼ ç»Ÿæç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰æ›´å¼ºçš„é²æ£’æ€§ï¼ˆrobustnessï¼‰å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æˆåŠŸå°†æ­¤ç­–ç•¥åº”ç”¨äºå¤§å‹æ¨ç†æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶åœ¨ç²¾ç¡®æ¨ç†æ§åˆ¶æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.20322v2",
      "published_date": "2025-05-23 17:59:18 UTC",
      "updated_date": "2025-06-03 13:40:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:00.709822+00:00"
    },
    {
      "arxiv_id": "2505.20321v3",
      "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases",
      "title_zh": "BiomedSQLï¼šé¢å‘ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†åº“ç§‘å­¦æ¨ç†çš„æ–‡æœ¬åˆ° SQL",
      "authors": [
        "Mathew J. Koretsky",
        "Maya Willey",
        "Adi Asija",
        "Owen Bianchi",
        "Chelsea X. Alvarado",
        "Tanay Nayak",
        "Nicole Kuznetsov",
        "Sungwon Kim",
        "Mike A. Nalls",
        "Daniel Khashabi",
        "Faraz Faghri"
      ],
      "abstract": "Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples generated from templates and grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† BiomedSQLï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°åœ¨çœŸå®ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†åº“ä¸Šè¿›è¡Œ Text-to-SQL ç”Ÿæˆæ—¶ç§‘å­¦æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†åŒ…å« 68,000 ä¸ªé—®ç­”ä¸‰å…ƒç»„ï¼Œæ•´åˆäº†åŸºå› ä¸ç–¾ç—…å…³è”ã€ç»„å­¦æ•°æ®å› æœæ¨æ–­åŠè¯ç‰©å®¡æ‰¹è®°å½•ï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡æ¨æ–­é¢†åŸŸç‰¹å®šæ ‡å‡†ï¼ˆå¦‚å…¨åŸºå› ç»„æ˜¾è‘—æ€§é˜ˆå€¼æˆ–ä¸´åºŠè¯•éªŒé˜¶æ®µè¿‡æ»¤ï¼‰çš„èƒ½åŠ›ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè™½ç„¶è‡ªå®šä¹‰å¤šæ­¥æ™ºèƒ½ä½“ BMSQL è¾¾åˆ°äº† 62.6% çš„æ‰§è¡Œå‡†ç¡®ç‡ï¼Œé¢†å…ˆäº GPT-o3-mini (59.0%)ï¼Œä½†ä¸¤è€…ä»æ˜¾è‘—ä½äº 90.0% çš„ä¸“å®¶åŸºå‡†ã€‚BiomedSQL ä¸ºå¼€å‘èƒ½å¤Ÿæ”¯æŒç§‘å­¦å‘ç°å¹¶å…·å¤‡é²æ£’æ¨ç†èƒ½åŠ›çš„ç»“æ„åŒ–ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†åº“æŸ¥è¯¢ç³»ç»Ÿæä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2505.20321v3",
      "published_date": "2025-05-23 17:58:07 UTC",
      "updated_date": "2025-10-09 14:08:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:03.221387+00:00"
    },
    {
      "arxiv_id": "2505.18148v2",
      "title": "Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find",
      "title_zh": "å¤§æµ·æé’ˆï¼šå¾®å°å…³é”®ä¿¡æ¯æ›´éš¾è¢«å¤§è¯­è¨€æ¨¡å‹æ£€ç´¢",
      "authors": [
        "Owen Bianchi",
        "Mathew J. Koretsky",
        "Maya Willey",
        "Chelsea X. Alvarado",
        "Tanay Nayak",
        "Adi Asija",
        "Nicole Kuznetsov",
        "Mike A. Nalls",
        "Faraz Faghri",
        "Daniel Khashabi"
      ],
      "abstract": "Large language models (LLMs) face significant challenges with needle-in-ahaystack tasks, where relevant information (\"the needle\") must be drawn from a large pool of irrelevant context (\"the haystack\"). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size, the length of the answer-containing document, has received little attention. We present the first systematic study of gold context size in long-context question answering, spanning three diverse benchmarks (general knowledge, biomedical reasoning, and mathematical reasoning), eleven state-of-the-art LLMs (including recent reasoning models), and more than 150K controlled runs. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This effect persists under rigorous confounder analysis: even after controlling for gold context position, answer token repetition, gold-to-distractor ratio, distractor volume, and domain specificity, gold context size remains a decisive, independent predictor of success. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†é‡‘æ–‡æœ¬é•¿åº¦ (gold context size) åœ¨é•¿ä¸Šä¸‹æ–‡é—®ç­” (long-context question answering) ä»»åŠ¡ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ€§èƒ½çš„å½±å“ã€‚é€šè¿‡å¯¹11ç§å‰æ²¿æ¨¡å‹è¿›è¡Œè¶…è¿‡15ä¸‡æ¬¡å—æ§å®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†å½“é‡‘æ–‡æœ¬è¾ƒçŸ­ï¼ˆå³â€œæ›´å°çš„é’ˆâ€ï¼‰æ—¶ï¼Œæ¨¡å‹è¡¨ç°ä¼šæ˜¾è‘—ä¸‹é™ï¼Œå¹¶è¿›ä¸€æ­¥æ”¾å¤§ä½ç½®æ•æ„Ÿæ€§ (positional sensitivity)ã€‚å®éªŒè¯æ˜ï¼Œå³ä½¿åœ¨æ§åˆ¶äº†å¹²æ‰°é¡¹æ•°é‡å’Œä½ç½®ç­‰æ··æ‚å› ç´ åï¼Œé‡‘æ–‡æœ¬é•¿åº¦ä»æ˜¯é¢„æµ‹æˆåŠŸçš„ç‹¬ç«‹ä¸”å…³é”®çš„å› ç´ ã€‚è¯¥å‘ç°æŒ‡å‡ºäº†æ™ºèƒ½ä½“ç³»ç»Ÿ (agentic systems) åœ¨æ•´åˆåˆ†æ•£ã€ç»†ç²’åº¦ä¿¡æ¯æ—¶é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ï¼Œä¸ºè®¾è®¡ç¨³å¥çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2505.18148v2",
      "published_date": "2025-05-23 17:57:42 UTC",
      "updated_date": "2025-12-16 19:50:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:05.526299+00:00"
    },
    {
      "arxiv_id": "2505.18139v3",
      "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems",
      "title_zh": "æ‹¥æŠ±çŸ›ç›¾ï¼šç†è®ºä¸ä¸€è‡´æ€§ä¸ä¼šé˜»ç¢æ„å»ºè´Ÿè´£ä»»äººå·¥æ™ºèƒ½ç³»ç»Ÿä¹‹è·¯",
      "authors": [
        "Gordon Dai",
        "Yunze Xiao"
      ],
      "abstract": "This position paper argues that the theoretical inconsistency often observed among Responsible AI (RAI) metrics, such as differing fairness definitions or tradeoffs between accuracy and privacy, should be embraced as a valuable feature rather than a flaw to be eliminated. We contend that navigating these inconsistencies, by treating metrics as divergent objectives, yields three key benefits: (1) Normative Pluralism: Maintaining a full suite of potentially contradictory metrics ensures that the diverse moral stances and stakeholder values inherent in RAI are adequately represented. (2) Epistemological Completeness: The use of multiple, sometimes conflicting, metrics allows for a more comprehensive capture of multifaceted ethical concepts, thereby preserving greater informational fidelity about these concepts than any single, simplified definition. (3) Implicit Regularization: Jointly optimizing for theoretically conflicting objectives discourages overfitting to one specific metric, steering models towards solutions with enhanced generalization and robustness under real-world complexities. In contrast, efforts to enforce theoretical consistency by simplifying or pruning metrics risk narrowing this value diversity, losing conceptual depth, and degrading model performance. We therefore advocate for a shift in RAI theory and practice: from getting trapped in inconsistency to characterizing acceptable inconsistency thresholds and elucidating the mechanisms that permit robust, approximated consistency in practice.",
      "tldr_zh": "è¯¥ç«‹åœºè®ºæ–‡è®¤ä¸ºï¼Œè´Ÿè´£ä»»äººå·¥æ™ºèƒ½ (Responsible AI, RAI) æŒ‡æ ‡ä¸­å¸¸è§çš„ç†è®ºä¸ä¸€è‡´æ€§ï¼ˆå¦‚å…¬å¹³æ€§å®šä¹‰çš„å†²çªæˆ–å‡†ç¡®æ€§ä¸éšç§çš„æƒè¡¡ï¼‰åº”è¢«è§†ä¸ºä¸€ç§æœ‰ä»·å€¼çš„ç‰¹æ€§ï¼Œè€Œééœ€è¦æ¶ˆé™¤çš„ç¼ºé™·ã€‚ç ”ç©¶æå‡ºï¼Œé€šè¿‡å°†ä¸ä¸€è‡´çš„æŒ‡æ ‡ä½œä¸ºå‘æ•£æ€§ç›®æ ‡è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥å®ç°è§„èŒƒå¤šå…ƒåŒ– (Normative Pluralism)ã€è®¤è¯†è®ºå®Œæ•´æ€§ (Epistemological Completeness) ä»¥åŠéšå¼æ­£åˆ™åŒ– (Implicit Regularization)ï¼Œä»è€Œæ›´å¥½åœ°ä»£è¡¨å¤šå…ƒä»·å€¼è§‚å¹¶å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸é²æ£’æ€§ã€‚ç›¸åï¼Œå¼ºè¡Œè¿½æ±‚ä¸€è‡´æ€§åè€Œå¯èƒ½å¯¼è‡´ä¼¦ç†æ·±åº¦æµå¤±å’Œæ€§èƒ½ä¸‹é™ã€‚ä½œè€…å› æ­¤ä¸»å¼ å°†ç ”ç©¶é‡ç‚¹è½¬å‘ç•Œå®šå¯æ¥å—çš„ä¸ä¸€è‡´æ€§é˜ˆå€¼ï¼Œå¹¶æ¢ç´¢åœ¨å®è·µä¸­å®ç°ç¨³å¥ä¸”è¿‘ä¼¼ä¸€è‡´çš„æœºåˆ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages,2 figure",
      "pdf_url": "https://arxiv.org/pdf/2505.18139v3",
      "published_date": "2025-05-23 17:48:09 UTC",
      "updated_date": "2025-10-30 01:51:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:06.771724+00:00"
    },
    {
      "arxiv_id": "2505.18136v1",
      "title": "Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection",
      "title_zh": "å›¾-è¯­è¨€èåˆï¼šåˆ©ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œ Wikidata ç ´åæ£€æµ‹",
      "authors": [
        "Mykola Trokhymovych",
        "Lydia Pintscher",
        "Ricardo Baeza-Yates",
        "Diego Saez-Trumper"
      ],
      "abstract": "We introduce a next-generation vandalism detection system for Wikidata, one of the largest open-source structured knowledge bases on the Web. Wikidata is highly complex: its items incorporate an ever-expanding universe of factual triples and multilingual texts. While edits can alter both structured and textual content, our approach converts all edits into a single space using a method we call Graph2Text. This allows for evaluating all content changes for potential vandalism using a single multilingual language model. This unified approach improves coverage and simplifies maintenance. Experiments demonstrate that our solution outperforms the current production system. Additionally, we are releasing the code under an open license along with a large dataset of various human-generated knowledge alterations, enabling further research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒæœ€å¤§çš„å¼€æºç»“æ„åŒ–çŸ¥è¯†åº“ä¹‹ä¸€ Wikidataï¼Œæ¨å‡ºäº†ä¸€å¥—ä¸‹ä¸€ä»£æ¶æ„ç ´åæ£€æµ‹ç³»ç»Ÿã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Graph2Text çš„æ–¹æ³•ï¼Œå°†æ‰€æœ‰åŒ…å«äº‹å®ä¸‰å…ƒç»„ (factual triples) å’Œå¤šè¯­è¨€æ–‡æœ¬çš„ç¼–è¾‘å†…å®¹ç»Ÿä¸€è½¬åŒ–ä¸ºæ–‡æœ¬ç©ºé—´ï¼Œä»è€Œåˆ©ç”¨å•ä¸€çš„å¤šè¯­è¨€è¯­è¨€æ¨¡å‹ (multilingual language model) è¿›è¡Œè¯„ä¼°ã€‚è¿™ç§ç»Ÿä¸€çš„ Graph-Linguistic Fusion æ¶æ„ä¸ä»…æ‰©å¤§äº†è¦†ç›–èŒƒå›´å¹¶ç®€åŒ–äº†ç»´æŠ¤æµç¨‹ï¼Œåœ¨å®éªŒä¸­è¡¨ç°ä¹Ÿä¼˜äºç›®å‰çš„ç”Ÿäº§ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å¼€æºäº†ç›¸å…³ä»£ç ä»¥åŠä¸€ä¸ªå¤§è§„æ¨¡çš„äººç±»ç”ŸæˆçŸ¥è¯†å˜æ›´æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›åç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18136v1",
      "published_date": "2025-05-23 17:44:06 UTC",
      "updated_date": "2025-05-23 17:44:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:14.148134+00:00"
    },
    {
      "arxiv_id": "2505.18135v2",
      "title": "Tool Preferences in Agentic LLMs are Unreliable",
      "title_zh": "æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹çš„å·¥å…·åå¥½å…·æœ‰ä¸å¯é æ€§",
      "authors": [
        "Kazem Faghih",
        "Wenxiao Wang",
        "Yize Cheng",
        "Siddhant Bharti",
        "Gaurang Sriramanan",
        "Sriram Balasubramanian",
        "Parsa Hosseini",
        "Soheil Feizi"
      ],
      "abstract": "Large language models (LLMs) can now access a wide range of external tools, thanks to the Model Context Protocol (MCP). This greatly expands their abilities as various agents. However, LLMs rely entirely on the text descriptions of tools to decide which ones to use--a process that is surprisingly fragile. In this work, we expose a vulnerability in prevalent tool/function-calling protocols by investigating a series of edits to tool descriptions, some of which can drastically increase a tool's usage from LLMs when competing with alternatives. Through controlled experiments, we show that tools with properly edited descriptions receive over 10 times more usage from GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further evaluate how various edits to tool descriptions perform when competing directly with one another and how these trends generalize or differ across a broader set of 17 different models. These phenomena, while giving developers a powerful way to promote their tools, underscore the need for a more reliable foundation for agentic LLMs to select and utilize tools and resources. Our code is publicly available at https://github.com/kazemf78/llm-unreliable-tool-preferences.",
      "tldr_zh": "æœ¬ç ”ç©¶è°ƒæŸ¥äº† Agentic LLMs åœ¨åˆ©ç”¨ Model Context Protocol (MCP) è°ƒç”¨å¤–éƒ¨å·¥å…·æ—¶ï¼Œå¯¹å·¥å…·æè¿°(tool descriptions)çš„é«˜åº¦ä¾èµ–åŠå…¶å­˜åœ¨çš„è„†å¼±æ€§ã€‚é€šè¿‡å¯¹å·¥å…·æè¿°è¿›è¡Œä¸€ç³»åˆ—ç¼–è¾‘ï¼Œç ”ç©¶äººå‘˜æ­ç¤ºäº†å·¥å…·è°ƒç”¨åè®®ä¸­çš„ä¸€ä¸ªé‡è¦æ¼æ´ï¼šå³ç‰¹å®šçš„æè¿°ä¿®é¥°èƒ½æ˜¾è‘—æ“çºµæ¨¡å‹çš„é€‰æ‹©åå¥½ã€‚å—æ§å®éªŒè¡¨æ˜ï¼Œä¼˜åŒ–åçš„æè¿°ä½¿å·¥å…·åœ¨ GPT-4.1 å’Œ Qwen2.5-7B ä¸­çš„ä½¿ç”¨é¢‘ç‡æ¯”åŸå§‹æè¿°é«˜å‡º 10 å€ä»¥ä¸Šã€‚è¯¥ç ”ç©¶è·¨ 17 ç§æ¨¡å‹éªŒè¯äº†è¿™ä¸€ç°è±¡çš„æ™®éæ€§ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹å·¥å…·åå¥½çš„ä¸å¯é æ€§ï¼Œå¹¶å¼ºè°ƒäº†ä¸º Agentic LLMs æ„å»ºæ›´ç¨³å¥çš„èµ„æºé€‰æ‹©åŸºç¡€çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2025, main",
      "pdf_url": "https://arxiv.org/pdf/2505.18135v2",
      "published_date": "2025-05-23 17:43:48 UTC",
      "updated_date": "2025-09-21 22:21:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:13.920313+00:00"
    },
    {
      "arxiv_id": "2505.18134v2",
      "title": "VideoGameBench: Can Vision-Language Models complete popular video games?",
      "title_zh": "VideoGameBenchï¼šè§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¦é€šå…³çƒ­é—¨è§†é¢‘æ¸¸æˆï¼Ÿ",
      "authors": [
        "Alex L. Zhang",
        "Thomas L. Griffiths",
        "Karthik R. Narasimhan",
        "Ofir Press"
      ],
      "abstract": "Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions.",
      "tldr_zh": "---\n\n### è®ºæ–‡æ‘˜è¦ TLDR ğŸ®\n\nè¯¥ç ”ç©¶å¼•å…¥äº† VideoGameBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 10 æ¬¾ 90 å¹´ä»£æµè¡Œç”µå­æ¸¸æˆçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨æ„ŸçŸ¥ã€ç©ºé—´å¯¼èˆªå’Œè®°å¿†ç®¡ç†ç­‰äººç±»ç›´è§‰èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚ä¸ä»¥å¾€ä¾èµ–ç‰¹å®šæ¸¸æˆè„šæ‰‹æ¶æˆ–è¾…åŠ©ä¿¡æ¯çš„æ–¹æ³•ä¸åŒï¼Œè¯¥åŸºå‡†è¦æ±‚æ¨¡å‹ä»…å‡­åŸå§‹è§†è§‰è¾“å…¥å’Œé«˜å±‚æŒ‡ä»¤å®æ—¶ä¸æ¸¸æˆç¯å¢ƒäº¤äº’ã€‚é’ˆå¯¹æ¨ç†å»¶è¿Ÿç“¶é¢ˆï¼Œç ”ç©¶è€…è¿˜æå‡ºäº† VideoGameBench Lite æ¨¡å¼ï¼Œå…è®¸æ¸¸æˆåœ¨ç­‰å¾…æ¨¡å‹æŒ‡ä»¤æ—¶æš‚åœä»¥é™ä½å®æ—¶æ€§å‹åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿéš¾ä»¥åœ¨æ¸¸æˆä¸­å–å¾—å®è´¨æ€§è¿›å±•ï¼Œå…¶ä¸­è¡¨ç°æœ€å¥½çš„ Gemini 2.5 Pro åœ¨å®Œæ•´ç‰ˆå’Œ Lite ç‰ˆåŸºå‡†ä¸­çš„å®Œæˆç‡åˆ†åˆ«ä»…ä¸º 0.48% å’Œ 1.6%ã€‚è¯¥åŸºå‡†çš„æå‡ºå½¢å¼åŒ–äº†æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸‹çš„èƒ½åŠ›çŸ­æ¿ï¼Œä¸ºæ¨åŠ¨å…·å¤‡ç±»äººäº¤äº’èƒ½åŠ›çš„ VLMs ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚\n\n---\n\nå¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦è½¬æ¢ï¼Œæˆ–è€…æƒ³é’ˆå¯¹ VideoGameBench çš„å®éªŒç»†èŠ‚è¿›ä¸€æ­¥è®¨è®ºï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 33 pages including supplementary",
      "pdf_url": "https://arxiv.org/pdf/2505.18134v2",
      "published_date": "2025-05-23 17:43:27 UTC",
      "updated_date": "2025-05-30 14:50:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:19.908762+00:00"
    },
    {
      "arxiv_id": "2505.18131v1",
      "title": "Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement",
      "title_zh": "åˆ©ç”¨ KANs çš„é¢„å¤„ç†ä¸å‡ ä½•ç»†åŒ–å®ç°å¤šé€šé“ MLP çš„é«˜æ•ˆè®­ç»ƒ",
      "authors": [
        "Jonas A. Actor",
        "Graham Harper",
        "Ben Southworth",
        "Eric C. Cyr"
      ],
      "abstract": "Multilayer perceptrons (MLPs) are a workhorse machine learning architecture, used in a variety of modern deep learning frameworks. However, recently Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their success on a range of problems, particularly for scientific machine learning tasks. In this paper, we exploit the relationship between KANs and multichannel MLPs to gain structural insight into how to train MLPs faster. We demonstrate the KAN basis (1) provides geometric localized support, and (2) acts as a preconditioned descent in the ReLU basis, overall resulting in expedited training and improved accuracy. Our results show the equivalence between free-knot spline KAN architectures, and a class of MLPs that are refined geometrically along the channel dimension of each weight tensor. We exploit this structural equivalence to define a hierarchical refinement scheme that dramatically accelerates training of the multi-channel MLP architecture. We show further accuracy improvements can be had by allowing the $1$D locations of the spline knots to be trained simultaneously with the weights. These advances are demonstrated on a range of benchmark examples for regression and scientific machine learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Kolmogorov-Arnold Networks (KANs) ä¸å¤šé€šé“ Multilayer Perceptrons (MLPs) ä¹‹é—´çš„ç»“æ„è”ç³»ï¼Œæ—¨åœ¨åˆ©ç”¨è¿™ç§ç­‰ä»·æ€§åŠ é€Ÿ MLPs çš„è®­ç»ƒã€‚ç ”ç©¶è¡¨æ˜ï¼ŒKAN åŸºå‡½æ•°å…·æœ‰å‡ ä½•å±€éƒ¨æ”¯æŒç‰¹æ€§ï¼Œå¹¶åœ¨ ReLU åŸºä¸­èµ·åˆ°äº†é¢„æ¡ä»¶ä¸‹é™ (Preconditioned Descent) çš„ä½œç”¨ï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å’Œæ›´é«˜çš„ç²¾åº¦ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–ç»†åŒ–æ–¹æ¡ˆ (Hierarchical Refinement Scheme)ï¼Œé€šè¿‡åœ¨æƒé‡å¼ é‡çš„é€šé“ç»´åº¦ä¸Šè¿›è¡Œå‡ ä½•ç»†åŒ–ï¼Œæ˜¾è‘—åŠ å¿«äº†æ¨¡å‹æ”¶æ•›ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŒæ­¥è®­ç»ƒ 1D æ ·æ¡èŠ‚ç‚¹ (Spline Knots) çš„ä½ç½®ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹åœ¨å›å½’å’Œç§‘å­¦æœºå™¨å­¦ä¹  (Scientific Machine Learning) ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 3 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.18131v1",
      "published_date": "2025-05-23 17:41:18 UTC",
      "updated_date": "2025-05-23 17:41:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:33.500667+00:00"
    },
    {
      "arxiv_id": "2505.18126v2",
      "title": "Reward Model Overoptimisation in Iterated RLHF",
      "title_zh": "è¿­ä»£ RLHF ä¸­çš„å¥–åŠ±æ¨¡å‹è¿‡åº¦ä¼˜åŒ–",
      "authors": [
        "Lorenz Wolf",
        "Robert Kirk",
        "Mirco Musolesi"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) is a widely used method for aligning large language models with human preferences. However, RLHF often suffers from reward model overoptimisation, in which models overfit to the reward function, resulting in non-generalisable policies that exploit the idiosyncrasies and peculiarities of the reward function. A common mitigation is iterated RLHF, in which reward models are repeatedly retrained with updated human feedback and policies are re-optimised. Despite its increasing adoption, the dynamics of overoptimisation in this setting remain poorly understood. In this work, we present the first comprehensive study of overoptimisation in iterated RLHF. We systematically analyse key design choices - how reward model training data is transferred across iterations, which reward function is used for optimisation, and how policies are initialised. Using the controlled AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over successive iterations, as reward models increasingly approximate ground-truth preferences. However, performance gains diminish over time, and while reinitialising from the base policy is robust, it limits optimisation flexibility. Other initialisation strategies often fail to recover from early overoptimisation. These findings offer actionable insights for building more stable and generalisable RLHF pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆä¸­å­¦ä¹  (RLHF) ä¸­å¸¸è§çš„å¥–åŠ±æ¨¡å‹è¿‡åº¦ä¼˜åŒ– (Reward Model Overoptimisation) é—®é¢˜ï¼Œé¦–æ¬¡å¯¹è¿­ä»£å¼ RLHF (Iterated RLHF) çš„åŠ¨æ€è¿‡ç¨‹è¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚é€šè¿‡åœ¨ AlpacaFarm åŸºå‡†ä¸Šç³»ç»Ÿåˆ†ææ•°æ®è¿ç§»ã€å¥–åŠ±å‡½æ•°é€‰æ‹©å’Œç­–ç•¥åˆå§‹åŒ–ç­‰è®¾è®¡å› ç´ ï¼Œç ”ç©¶å‘ç°è¿‡åº¦ä¼˜åŒ–å€¾å‘äºéšè¿­ä»£æ¬¡æ•°å¢åŠ è€Œå‡å¼±ï¼Œå› ä¸ºå¥–åŠ±æ¨¡å‹èƒ½æ›´å‡†ç¡®åœ°é€¼è¿‘çœŸå®åå¥½ã€‚ç„¶è€Œï¼Œå®éªŒè§‚å¯Ÿåˆ°æ€§èƒ½å¢ç›Šä¼šéšæ—¶é—´é€’å‡ï¼Œä¸”è™½ç„¶ä»åŸºç¡€ç­–ç•¥ (Base Policy) é‡æ–°åˆå§‹åŒ–å…·æœ‰è¾ƒå¥½çš„é²æ£’æ€§ï¼Œä½†å…¶ä»–åˆå§‹åŒ–ç­–ç•¥å¾€å¾€éš¾ä»¥ä»æ—©æœŸçš„è¿‡åº¦ä¼˜åŒ–ä¸­æ¢å¤ã€‚è¿™äº›å‘ç°ä¸ºæ„å»ºæ›´ç¨³å®šã€å¯æ³›åŒ–çš„ RLHF æµæ°´çº¿æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 17 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.18126v2",
      "published_date": "2025-05-23 17:36:13 UTC",
      "updated_date": "2025-09-29 10:26:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:41.926165+00:00"
    },
    {
      "arxiv_id": "2505.18121v1",
      "title": "ProgRM: Build Better GUI Agents with Progress Rewards",
      "title_zh": "ProgRMï¼šåŸºäºè¿›åº¦å¥–åŠ±æ„å»ºæ›´ä¼˜çš„ GUI æ™ºèƒ½ä½“",
      "authors": [
        "Danyang Zhang",
        "Situo Zhang",
        "Ziyue Yang",
        "Zichen Zhu",
        "Zihan Zhao",
        "Ruisheng Cao",
        "Lu Chen",
        "Kai Yu"
      ],
      "abstract": "LLM-based (Large Language Model) GUI (Graphical User Interface) agents can potentially reshape our daily lives significantly. However, current LLM-based GUI agents suffer from the scarcity of high-quality training data owing to the difficulties of trajectory collection and reward annotation. Existing works have been exploring LLMs to collect trajectories for imitation learning or to offer reward signals for online RL training. However, the Outcome Reward Model (ORM) used in existing works cannot provide finegrained feedback and can over-penalize the valuable steps in finally failed trajectories. To this end, we propose Progress Reward Model (ProgRM) to provide dense informative intermediate rewards by predicting a task completion progress for each step in online training. To handle the challenge of progress reward label annotation, we further design an efficient LCS-based (Longest Common Subsequence) self-annotation algorithm to discover the key steps in trajectories and assign progress labels accordingly. ProgRM is evaluated with extensive experiments and analyses. Actors trained with ProgRM outperform leading proprietary LLMs and ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for experiments will be made publicly available upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ProgRMï¼Œä¸€ç§æ—¨åœ¨æå‡ GUI Agents æ€§èƒ½çš„ Progress Reward Modelï¼Œè§£å†³äº†ç°æœ‰ Outcome Reward Model (ORM) ç¼ºä¹ç»†ç²’åº¦åé¦ˆä¸”å¯¹å¤±è´¥è½¨è¿¹ä¸­æœ‰æ•ˆæ­¥éª¤æƒ©ç½šè¿‡åº¦çš„é—®é¢˜ã€‚ProgRM é€šè¿‡é¢„æµ‹æ¯ä¸€æ­¥çš„ä»»åŠ¡å®Œæˆè¿›åº¦ï¼Œä¸ºåœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›å¯†é›†çš„ä¸­é—´å¥–åŠ±ä¿¡å·ã€‚ä¸ºäº†è§£å†³è¿›åº¦æ ‡ç­¾æ ‡æ³¨çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§åŸºäº LCS (Longest Common Subsequence) çš„é«˜æ•ˆè‡ªæ ‡æ³¨ç®—æ³•ï¼Œç”¨ä»¥è‡ªåŠ¨è¯†åˆ«è½¨è¿¹ä¸­çš„å…³é”®æ­¥éª¤å¹¶åˆ†é…è¿›åº¦æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå— ProgRM è®­ç»ƒçš„æ™ºèƒ½ä½“è¡¨ç°ä¼˜äºé¢†å…ˆçš„å•†ä¸š LLMs åŠåŸºäº ORM è®­ç»ƒçš„æ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ„å»ºé«˜æ€§èƒ½ GUI æ™ºèƒ½ä½“æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18121v1",
      "published_date": "2025-05-23 17:23:11 UTC",
      "updated_date": "2025-05-23 17:23:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:39.491284+00:00"
    },
    {
      "arxiv_id": "2505.18120v1",
      "title": "Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹åŒå‘çŸ¥è¯†è’¸é¦å¢å¼ºåºåˆ—æ¨è",
      "authors": [
        "Jiongran Wu",
        "Jiahao Liu",
        "Dongsheng Li",
        "Guangping Zhang",
        "Mingzhe Han",
        "Hansu Gu",
        "Peng Zhang",
        "Li Shang",
        "Tun Lu",
        "Ning Gu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional performance in understanding and generating semantic patterns, making them promising candidates for sequential recommendation tasks. However, when combined with conventional recommendation models (CRMs), LLMs often face challenges related to high inference costs and static knowledge transfer methods. In this paper, we propose a novel mutual distillation framework, LLMD4Rec, that fosters dynamic and bidirectional knowledge exchange between LLM-centric and CRM-based recommendation systems. Unlike traditional unidirectional distillation methods, LLMD4Rec enables iterative optimization by alternately refining both models, enhancing the semantic understanding of CRMs and enriching LLMs with collaborative signals from user-item interactions. By leveraging sample-wise adaptive weighting and aligning output distributions, our approach eliminates the need for additional parameters while ensuring effective knowledge transfer. Extensive experiments on real-world datasets demonstrate that LLMD4Rec significantly improves recommendation accuracy across multiple benchmarks without increasing inference costs. This method provides a scalable and efficient solution for combining the strengths of both LLMs and CRMs in sequential recommendation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLMD4Recï¼Œä¸€ç§æ–°å‹çš„ç›¸äº’è’¸é¦(mutual distillation)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLMs)å¢å¼ºåºåˆ—æ¨èç³»ç»Ÿçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å®ç°äº†ä»¥LLMä¸ºä¸­å¿ƒç³»ç»Ÿä¸ä¼ ç»Ÿæ¨èæ¨¡å‹(CRMs)ä¹‹é—´çš„åŠ¨æ€åŒå‘çŸ¥è¯†äº¤æ¢ï¼Œæœ‰æ•ˆç»“åˆäº†LLMsçš„è¯­ä¹‰ç†è§£èƒ½åŠ›å’ŒCRMsçš„ç”¨æˆ·äº¤äº’åä½œä¿¡å·(collaborative signals)ã€‚é€šè¿‡é‡‡ç”¨æ ·æœ¬çº§è‡ªé€‚åº”åŠ æƒå’Œè¾“å‡ºåˆ†å¸ƒå¯¹é½æŠ€æœ¯ï¼ŒLLMD4Recåœ¨ä¸å¢åŠ é¢å¤–å‚æ•°å’Œæ¨ç†æˆæœ¬çš„å‰æä¸‹ï¼Œå®ç°äº†æ¨¡å‹çš„äº¤æ›¿ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨èå‡†ç¡®æ€§ï¼Œä¸ºèåˆLLMsä¸CRMsçš„ä¼˜åŠ¿æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "11 pages, under review",
      "pdf_url": "https://arxiv.org/pdf/2505.18120v1",
      "published_date": "2025-05-23 17:21:14 UTC",
      "updated_date": "2025-05-23 17:21:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:14:05.829897+00:00"
    },
    {
      "arxiv_id": "2505.18247v3",
      "title": "MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering",
      "title_zh": "MetaGen Blended RAGï¼šè§£é”ç‰¹å®šé¢†åŸŸé—®ç­”çš„é›¶æ ·æœ¬ç²¾åº¦",
      "authors": [
        "Kunal Sawarkar",
        "Shivam R. Solanki",
        "Abhilasha Mangal"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MetaGen Blended RAGï¼Œä¸€ç§æ—¨åœ¨æå‡ Retrieval-Augmented Generation (RAG) åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®é›†ï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ï¼‰ä¸­ zero-shot ç²¾åº¦çš„ä¼ä¸šæœç´¢æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡ metadata generation pipeline ç»“åˆäº†ç”± dense å’Œ sparse vectors ç»„æˆçš„ hybrid query indexesï¼Œæœ‰æ•ˆè§£å†³äº†ä¸“ä¸šæœ¯è¯­å¯¼è‡´çš„è¯­ä¹‰åå·®é—®é¢˜ï¼Œä¸”æ— éœ€æ˜‚è´µçš„ fine-tuningã€‚é€šè¿‡æå–å…³é”®æ¦‚å¿µå’Œç¼©å†™æ„å»º metadata-enriched çš„è¯­ä¹‰ç´¢å¼•ï¼Œè¯¥æ–¹æ³•åœ¨ PubMedQA æ•°æ®é›†ä¸Šå–å¾—äº† 82% çš„æ£€ç´¢å‡†ç¡®ç‡å’Œ 77% çš„ RAG å‡†ç¡®ç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMetaGen Blended RAG ä¸ä»…è¶…è¶Šäº†ç°æœ‰çš„ zero-shot åŸºå‡†ï¼Œç”šè‡³åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†ä¸ç»è¿‡ fine-tuned æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18247v3",
      "published_date": "2025-05-23 17:18:45 UTC",
      "updated_date": "2025-08-05 17:28:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:46.850188+00:00"
    },
    {
      "arxiv_id": "2505.18102v6",
      "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?",
      "title_zh": "å¦‚ä½•åœ¨ä¸æ³„éœ²çœŸå®ç­”æ¡ˆçš„æƒ…å†µä¸‹å‘å¸ƒå¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼Ÿ",
      "authors": [
        "Takashi Ishida",
        "Thanawat Lodkaew",
        "Ikko Yamane"
      ],
      "abstract": "Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. The main underlying idea is to reduces the best possible accuracy, i.e., Bayes accuracy, by injecting randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨äº’è”ç½‘å‘å¸ƒå¤§å‹è¯­è¨€æ¨¡å‹(LLM)åŸºå‡†æµ‹è¯•(Benchmark)æ˜“å¯¼è‡´æ•°æ®æ±¡æŸ“(Data Contamination)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€å®Œå…¨å…¬å¼€åœ°é¢çœŸå€¼(Ground-truth)ç­”æ¡ˆçš„å‘å¸ƒæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ºé—®é¢˜å‡†å¤‡å¤šä¸ªé€»è¾‘æ­£ç¡®çš„é€‰é¡¹å¹¶å¼•å…¥éšæœºæ€§æ¥é™ä½è´å¶æ–¯å‡†ç¡®åº¦(Bayes Accuracy)ï¼Œä»è€Œåœ¨å®ç°å¼€æ”¾è¯„ä¼°çš„åŒæ—¶é˜²æ­¢ç­”æ¡ˆæ³„éœ²ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆè¿˜æä¾›äº†ä¸€ç§æ£€æµ‹æ•°æ®æ±¡æŸ“çš„å¯é æ‰‹æ®µï¼šç”±äºæ¨¡å‹è¡¨ç°ç†è®ºä¸Šä¸åº”è¶…è¿‡è´å¶æ–¯å‡†ç¡®åº¦ä¸Šé™ï¼Œè‹¥å‡†ç¡®ç‡å¼‚å¸¸è¶…å‡ºåˆ™å¯åˆ¤å®šä¸ºå‘ç”Ÿäº†æ•°æ®æ±¡æŸ“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ç¯å¢ƒä¸‹å‡èƒ½å‡†ç¡®è¯†åˆ«æ±¡æŸ“è¡Œä¸ºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "Extended version of the paper presented as an Oral at the ICML 2025 Workshop on the Impact of Memorization on Trustworthy Foundation Models",
      "pdf_url": "https://arxiv.org/pdf/2505.18102v6",
      "published_date": "2025-05-23 16:57:34 UTC",
      "updated_date": "2025-10-05 06:45:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:14:49.392672+00:00"
    },
    {
      "arxiv_id": "2505.18244v2",
      "title": "Multi-Scale Probabilistic Generation Theory: A Unified Information-Theoretic Framework for Hierarchical Structure in Large Language Models",
      "title_zh": "å¤šå°ºåº¦æ¦‚ç‡ç”Ÿæˆç†è®ºï¼šå¤§è¯­è¨€æ¨¡å‹å±‚æ¬¡ç»“æ„çš„ç»Ÿä¸€ä¿¡æ¯è®ºæ¡†æ¶",
      "authors": [
        "Yukin Zhang",
        "Qi Dong"
      ],
      "abstract": "Large Language Models (LLMs) exhibit remarkable emergent abilities but remain poorly understood at a mechanistic level. This paper introduces the Multi-Scale Probabilistic Generation Theory (MSPGT), a theoretical framework that models LLMs as Hierarchical Variational Information Bottleneck (H-VIB) systems. MSPGT posits that standard language modeling objectives implicitly optimize multi-scale information compression, leading to the spontaneous formation of three internal processing scales-Global, Intermediate, and Local. We formalize this principle, derive falsifiable predictions about boundary positions and architectural dependencies, and validate them through cross-model experiments combining multi-signal fusion and causal interventions. Results across Llama and Qwen families reveal consistent multi-scale organization but strong architecture-specific variations, partially supporting and refining the theory. MSPGT thus advances interpretability from descriptive observation toward predictive, information-theoretic understanding of how hierarchical structure emerges within large neural language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¤šå°ºåº¦æ¦‚ç‡ç”Ÿæˆç†è®º(Multi-Scale Probabilistic Generation Theory, MSPGT)ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹(LLMs)å»ºæ¨¡ä¸ºå±‚æ¬¡åŒ–å˜åˆ†ä¿¡æ¯ç“¶é¢ˆ(Hierarchical Variational Information Bottleneck, H-VIB)ç³»ç»Ÿçš„ç»Ÿä¸€ç†è®ºæ¡†æ¶ã€‚MSPGTæŒ‡å‡ºï¼Œè¯­è¨€å»ºæ¨¡ç›®æ ‡éšå¼åœ°ä¼˜åŒ–äº†å¤šå°ºåº¦ä¿¡æ¯å‹ç¼©ï¼Œä»è€Œåœ¨æ¨¡å‹å†…éƒ¨è‡ªå‘å½¢æˆå…¨å±€(Global)ã€ä¸­é—´(Intermediate)å’Œå±€éƒ¨(Local)ä¸‰ä¸ªå¤„ç†å°ºåº¦ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ç»“åˆå¤šä¿¡å·èåˆä¸å› æœå¹²é¢„å®éªŒï¼Œåœ¨Llamaå’ŒQwenç³»åˆ—æ¨¡å‹ä¸ŠéªŒè¯äº†è¯¥ç†è®ºçš„é¢„æµ‹ï¼Œè¯å®äº†æ¨¡å‹å†…éƒ¨å­˜åœ¨ä¸€è‡´çš„å¤šå°ºåº¦ç»„ç»‡ç»“æ„åŠå…¶éšæ¶æ„å˜åŒ–çš„ç‰¹æ€§ã€‚è¯¥ç†è®ºå°†å¤§æ¨¡å‹çš„å¯è§£é‡Šæ€§ç ”ç©¶ä»æè¿°æ€§è§‚å¯Ÿæå‡åˆ°äº†é¢„æµ‹æ€§çš„ä¿¡æ¯è®ºç†è§£å±‚é¢ï¼Œä¸ºæ¢ç´¢åˆ†å±‚ç»“æ„çš„æ¶Œç°æä¾›äº†é‡è¦ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18244v2",
      "published_date": "2025-05-23 16:55:35 UTC",
      "updated_date": "2025-10-15 04:15:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:49.554014+00:00"
    },
    {
      "arxiv_id": "2505.18098v2",
      "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL",
      "title_zh": "æ— æœç´¢è§„åˆ’ï¼šåˆ©ç”¨ç¦»çº¿ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å‰æ²¿å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Joey Hong",
        "Anca Dragan",
        "Sergey Levine"
      ],
      "abstract": "Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†è°ˆåˆ¤å’Œè¯´æœç­‰éœ€è¦é•¿æœŸè§„åˆ’ (long-horizon reasoning and planning) çš„å¤æ‚äº¤äº’ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ç›®æ ‡æ¡ä»¶ä»·å€¼å‡½æ•° (goal-conditioned value functions) æŒ‡å¯¼æ¨ç†çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline RL) è®­ç»ƒè½»é‡çº§çš„ä»·å€¼å‡½æ•°æ¨¡å—æ¥é¢„æµ‹ä»»åŠ¡èµ°å‘å¹¶è¯„ä¼°æ½œåœ¨ç»“æœï¼Œä¸”èƒ½æœ‰æ•ˆæ‰©å±•è‡³ä»…æä¾› API çš„è¶…å¤§å‹æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒ (RL fine-tuning) ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åŸºäºæ¨ç†æ­¥éª¤è€Œéå®Œæ•´åŠ¨ä½œè¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—ä¸å†…å­˜æˆæœ¬ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å·¥å…·è°ƒç”¨ã€ç¤¾äº¤åšå¼ˆåŠå¯¹è¯ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰çš„æç¤ºå·¥ç¨‹ (prompting) å’Œå¾®è°ƒæ–¹æ³•ï¼Œä¸ºå®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„è‡ªä¸»è§„åˆ’å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at NeurIPS 2025; 18 pages, 4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.18098v2",
      "published_date": "2025-05-23 16:51:54 UTC",
      "updated_date": "2025-12-03 08:54:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:14:53.701829+00:00"
    },
    {
      "arxiv_id": "2505.18091v2",
      "title": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition",
      "title_zh": "æ•°æ®æ··åˆå¯è¯±å‘çŸ¥è¯†è·å–ä¸­çš„ç›¸å˜",
      "authors": [
        "Xinran Gu",
        "Kaifeng Lyu",
        "Jiazheng Li",
        "Jingzhao Zhang"
      ],
      "abstract": "Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets, unlike training exclusively on knowledge-dense data (arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ··åˆæ•°æ®è®­ç»ƒæ—¶ï¼ŒçŸ¥è¯†è·å–è¿‡ç¨‹å¹¶éæ€»æ˜¯éµå¾ªå¹³æ»‘çš„ç¼©æ”¾æ³•åˆ™(Scaling Law)ï¼Œè€Œæ˜¯å¯èƒ½åœ¨æ•°æ®æ··åˆæ¯”ä¾‹å’Œæ¨¡å‹è§„æ¨¡ä¸Šè¡¨ç°å‡ºæ˜æ˜¾çš„ç›¸å˜(Phase Transitions)ç°è±¡ã€‚é€šè¿‡å—æ§å®éªŒï¼Œä½œè€…å‘ç°å½“æ¨¡å‹è§„æ¨¡æˆ–é«˜è´¨é‡æ•°æ®çš„æ··åˆæ¯”ä¾‹è¾¾åˆ°ç‰¹å®šçš„ä¸´ç•Œå€¼æ—¶ï¼Œæ¨¡å‹å¯¹çŸ¥è¯†çš„è®°å¿†èƒ½åŠ›ä¼šå‘ç”Ÿä»å‡ ä¹ä¸ºé›¶åˆ°å¿«é€Ÿå¢é•¿çš„çªå˜ã€‚ç ”ç©¶å°†æ­¤ç°è±¡å½’å› äºæœ‰é™æ¨¡å‹å®¹é‡ä¸‹çš„å®¹é‡åˆ†é…(Capacity Allocation)å†³ç­–ï¼Œå¹¶åˆ©ç”¨ä¿¡æ¯è®ºæ¡†æ¶å°†å…¶ç±»æ¯”ä¸ºå¯»æ‰¾æ•´ä½“æŸå¤±æœ€å°åŒ–çš„èƒŒåŒ…é—®é¢˜(Knapsack Problem)è§£ã€‚å®éªŒè¿›ä¸€æ­¥æ­ç¤ºäº†ä¸´ç•Œæ··åˆæ¯”ä¾‹ä¸æ¨¡å‹è§„æ¨¡ä¹‹é—´éµå¾ªå¹‚å¾‹(Power-law)å…³ç³»ï¼Œè¿™è¡¨æ˜é€‚ç”¨äºå¤§æ¨¡å‹çš„æ•°æ®æ··åˆæ–¹æ¡ˆ(Mixing Recipe)å¯¹äºå°æ¨¡å‹å¯èƒ½å¹¶éæœ€ä¼˜ï¼Œåä¹‹äº¦ç„¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS'25 Spotlight",
      "pdf_url": "https://arxiv.org/pdf/2505.18091v2",
      "published_date": "2025-05-23 16:46:24 UTC",
      "updated_date": "2025-12-04 09:09:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:13:58.257674+00:00"
    },
    {
      "arxiv_id": "2505.18087v2",
      "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays",
      "title_zh": "CXReasonBenchï¼šè¯„ä¼°èƒ¸éƒ¨ X å°„çº¿ç»“æ„åŒ–è¯Šæ–­æ¨ç†çš„åŸºå‡†",
      "authors": [
        "Hyungyung Lee",
        "Geon Choi",
        "Jung-Oh Lee",
        "Hangyul Yoon",
        "Hyuk Gi Hong",
        "Edward Choi"
      ],
      "abstract": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 12 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) åœ¨åŒ»ç–—ä»»åŠ¡è¯„ä¼°ä¸­è¿‡åº¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆè€Œå¿½è§†ä¸´åºŠæ¨ç†è¿‡ç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº† CheXStruct ç»“æ„åŒ–æµæ°´çº¿å’Œ CXReasonBench è¯„ä¼°åŸºå‡†ã€‚CXReasonBench åŸºäº MIMIC-CXR-JPG æ•°æ®é›†æ„å»ºï¼ŒåŒ…å« 12 é¡¹è¯Šæ–­ä»»åŠ¡å’Œ 18,988 ä¸ªé—®ç­”å¯¹ï¼Œé€šè¿‡è§£å‰–åŒºåŸŸåˆ†å‰²ã€ä¸´åºŠæµ‹é‡è®¡ç®—å’Œé˜ˆå€¼åº”ç”¨ç­‰ä¸­é—´æ­¥éª¤æ¥éªŒè¯æ¨ç†çš„ä¸´åºŠæœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯å½“å‰æœ€å…ˆè¿›çš„ 12 ç§ LVLMs åœ¨ç»“æ„åŒ–æ¨ç†å’Œæ³›åŒ–æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå¾€å¾€éš¾ä»¥å°†æŠ½è±¡çš„åŒ»å­¦çŸ¥è¯†ä¸åŸºäºè§£å‰–å­¦çš„è§†è§‰è§£è¯»æœ‰æ•ˆç»“åˆã€‚è¯¥åŸºå‡†ä¸ºåŒ»ç–—é¢†åŸŸå¼€å‘é€æ˜ã€å¯è§£é‡Šçš„è¯Šæ–­æ¨ç†æ¨¡å‹æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at NeurIPS 2025 Datasets and Benchmarks Track",
      "pdf_url": "https://arxiv.org/pdf/2505.18087v2",
      "published_date": "2025-05-23 16:44:21 UTC",
      "updated_date": "2025-10-27 03:45:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:02.463077+00:00"
    },
    {
      "arxiv_id": "2505.18086v1",
      "title": "Stable Reinforcement Learning for Efficient Reasoning",
      "title_zh": "é¢å‘é«˜æ•ˆæ¨ç†çš„ç¨³å®šå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Muzhi Dai",
        "Shixuan Liu",
        "Qingyi Si"
      ],
      "abstract": "The success of Deepseek-R1 has drawn the LLM community's attention to reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1 outcome reward methods lack the capability to regulate the intermediate reasoning processes during chain-of-thought (CoT) generation, leading to severe overthinking phenomena. In response, recent studies have designed reward functions to reinforce models' behaviors in producing shorter yet correct completions. Nevertheless, we observe that these length-penalty reward functions exacerbate RL training instability: as the completion length decreases, model accuracy abruptly collapses, often occurring early in training. To address this issue, we propose a simple yet effective solution GRPO-$Î»$, an efficient and stabilized variant of GRPO, which dynamically adjusts the reward strategy by monitoring the correctness ratio among completions within each query-sampled group. A low correctness ratio indicates the need to avoid length penalty that compromises CoT quality, triggering a switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A high ratio maintains length penalties to boost efficiency. Experimental results show that our approach avoids training instability caused by length penalty while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K, GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average accuracy by 1.48% while reducing CoT sequence length by 47.3%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Deepseek-R1 ç­‰æ¨¡å‹ä¸­ GRPO å¥–åŠ±æœºåˆ¶å¯¼è‡´çš„â€œè¿‡åº¦æ€è€ƒâ€ (overthinking) é—®é¢˜ï¼Œä»¥åŠå¼•å…¥é•¿åº¦æƒ©ç½š (length-penalty) å¸¦æ¥çš„è®­ç»ƒä¸ç¨³å®šæ€§è¿›è¡Œäº†æ”¹è¿›ã€‚ç ”ç©¶è€…æå‡ºäº† GRPO-$\\lambda$ï¼Œä¸€ç§èƒ½å¤Ÿé€šè¿‡ç›‘æ§ç»„å†…æ­£ç¡®ç‡åŠ¨æ€è°ƒæ•´å¥–åŠ±ç­–ç•¥çš„ç¨³å®šå˜ä½“ã€‚å½“æ­£ç¡®ç‡è¾ƒä½æ—¶ï¼Œè¯¥æ¡†æ¶ä¼šè§„é¿é•¿åº¦æƒ©ç½šä»¥ç¡®ä¿ Chain-of-Thought (CoT) çš„æ¨ç†è´¨é‡ï¼Œè€Œåœ¨é«˜æ­£ç¡®ç‡æ—¶åˆ™å¼ºåŒ–é•¿åº¦çº¦æŸä»¥æå‡æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒGRPO-$\\lambda$ åœ¨ GSM8Kã€MATH-500 ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆé¿å…äº†å‡†ç¡®ç‡å´©æºƒï¼Œåœ¨å¹³å‡å‡†ç¡®ç‡æå‡ 1.48% çš„åŒæ—¶ï¼Œå°† CoT åºåˆ—é•¿åº¦æ˜¾è‘—é™ä½äº† 47.3%ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18086v1",
      "published_date": "2025-05-23 16:43:03 UTC",
      "updated_date": "2025-05-23 16:43:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:24.126825+00:00"
    },
    {
      "arxiv_id": "2505.18081v1",
      "title": "Backpropagation-Free Metropolis-Adjusted Langevin Algorithm",
      "title_zh": "æ— åå‘ä¼ æ’­ Metropolis è°ƒæ•´æœ—ä¹‹ä¸‡ç®—æ³•",
      "authors": [
        "Adam D. Cobb",
        "Susmit Jha"
      ],
      "abstract": "Recent work on backpropagation-free learning has shown that it is possible to use forward-mode automatic differentiation (AD) to perform optimization on differentiable models. Forward-mode AD requires sampling a tangent vector for each forward pass of a model. The result is the model evaluation with the directional derivative along the tangent. In this paper, we illustrate how the sampling of this tangent vector can be incorporated into the proposal mechanism for the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the first to introduce a backpropagation-free gradient-based Markov chain Monte Carlo (MCMC) algorithm. We also extend to a novel backpropagation-free position-specific preconditioned forward-mode MALA that leverages Hessian information. Overall, we propose four new algorithms: Forward MALA; Line Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward MALA. We highlight the reduced computational cost of the forward-mode samplers and show that forward-mode is competitive with the original MALA, while even outperforming it depending on the probabilistic model. We include Bayesian inference results on a range of probabilistic models, including hierarchical distributions and Bayesian neural networks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå‰å‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†ï¼ˆforward-mode automatic differentiation, ADï¼‰çš„æ— åå‘ä¼ æ’­ï¼ˆbackpropagation-freeï¼‰å­¦ä¹ ï¼Œæ—¨åœ¨è§£å†³å¾®åˆ†æ¨¡å‹ä¼˜åŒ–ä¸­çš„è®¡ç®—ç“¶é¢ˆã€‚ä½œè€…é¦–æ¬¡å°†åˆ‡å‘é‡ï¼ˆtangent vectorï¼‰é‡‡æ ·èå…¥ Metropolis-Adjusted Langevin Algorithm (MALA) çš„å»ºè®®æœºåˆ¶ä¸­ï¼Œæå‡ºäº†é¦–ä¸ªæ— éœ€åå‘ä¼ æ’­çš„åŸºäºæ¢¯åº¦çš„é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMCMCï¼‰ç®—æ³•ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ‰©å±•å‡ºåˆ©ç”¨ Hessian ä¿¡æ¯çš„é¢„æ¡ä»¶åŒ–ç‰ˆæœ¬ï¼Œå…±åŒ…å« Forward MALAã€Line Forward MALA ç­‰å››ç§æ–°ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›å‰å‘æ¨¡å¼é‡‡æ ·å™¨åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå…¶æ€§èƒ½ä¸åŸå§‹ MALA ç›¸å½“ï¼Œåœ¨è´å¶æ–¯ç¥ç»ç½‘ç»œï¼ˆBayesian neural networksï¼‰ç­‰æ¦‚ç‡æ¨¡å‹ä¸­ç”šè‡³è¡¨ç°æ›´ä¼˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 Pages, 8 Figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18081v1",
      "published_date": "2025-05-23 16:39:21 UTC",
      "updated_date": "2025-05-23 16:39:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:15.827198+00:00"
    },
    {
      "arxiv_id": "2505.18080v1",
      "title": "AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction",
      "title_zh": "AFD-STAï¼šèåˆæ—¶ç©ºæ³¨æ„åŠ›çš„è‡ªé€‚åº”æ»¤æ³¢å»å™ªæ··æ²Œç³»ç»Ÿé¢„æµ‹",
      "authors": [
        "Chunlin Gong",
        "Yin Wang",
        "Jingru Li",
        "Hanleran Zhang"
      ],
      "abstract": "This paper presents AFD-STA Net, a neural framework integrating adaptive filtering and spatiotemporal dynamics learning for predicting high-dimensional chaotic systems governed by partial differential equations. The architecture combines: 1) An adaptive exponential smoothing module with position-aware decay coefficients for robust attractor reconstruction, 2) Parallel attention mechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated fusion of multiscale features, and 4) Deep projection networks with dimension-scaling capabilities. Numerical experiments on nonlinear PDE systems demonstrate the model's effectiveness in maintaining prediction accuracy under both smooth and strongly chaotic regimes while exhibiting noise tolerance through adaptive filtering. Component ablation studies confirm critical contributions from each module, particularly highlighting the essential role of spatiotemporal attention in learning complex dynamical interactions. The framework shows promising potential for real-world applications requiring simultaneous handling of measurement uncertainties and high-dimensional nonlinear dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AFD-STA Netï¼Œè¿™æ˜¯ä¸€ç§é›†æˆäº†è‡ªé€‚åº”æ»¤æ³¢ (adaptive filtering) å’Œæ—¶ç©ºåŠ¨åŠ›å­¦å­¦ä¹ çš„ç¥ç»æ¡†æ¶ï¼Œæ—¨åœ¨é¢„æµ‹ç”±åå¾®åˆ†æ–¹ç¨‹ (PDE) é©±åŠ¨çš„é«˜ç»´æ··æ²Œç³»ç»Ÿã€‚è¯¥æ¨¡å‹é€šè¿‡å…·æœ‰ä½ç½®æ„ŸçŸ¥è¡°å‡ç³»æ•°çš„è‡ªé€‚åº”æŒ‡æ•°å¹³æ»‘æ¨¡å—é‡å»ºå¸å¼•å­ (attractor reconstruction)ï¼Œå¹¶é‡‡ç”¨å¹¶è¡Œæ³¨æ„åŠ›æœºåˆ¶æ•æ‰æ—¶ç©ºä¾èµ–å…³ç³»ä¸å¤šå°ºåº¦ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAFD-STA Net åœ¨å¹³æ»‘å’Œå¼ºæ··æ²Œç¯å¢ƒä¸‹å‡èƒ½ä¿æŒå‡ºè‰²çš„é¢„æµ‹ç²¾åº¦ï¼Œä¸”å‡­å€Ÿè‡ªé€‚åº”æ»¤æ³¢æŠ€æœ¯å±•ç°äº†æä½³çš„å™ªå£°å®¹å¿åº¦ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†æ—¶ç©ºæ³¨æ„åŠ›åœ¨å¤„ç†å¤æ‚åŠ¨åŠ›å­¦äº¤äº’ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä¸ºåº”å¯¹æµ‹é‡ä¸ç¡®å®šæ€§å’Œé«˜ç»´éçº¿æ€§åŠ¨åŠ›å­¦çš„ç°å®åº”ç”¨æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18080v1",
      "published_date": "2025-05-23 16:39:07 UTC",
      "updated_date": "2025-05-23 16:39:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:20.599575+00:00"
    },
    {
      "arxiv_id": "2505.21532v1",
      "title": "EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media",
      "title_zh": "EvidenceMoEï¼šç”¨äºæå‡æ•£å°„ä»‹è´¨ä¸­è§å…‰æ¿€å…‰é›·è¾¾æ¢æµ‹æ€§èƒ½çš„ç‰©ç†å¼•å¯¼ä¸è¯æ®è¯„ä»·æ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "Ismail Erbas",
        "Ferhat Demirkiran",
        "Karthik Swaminathan",
        "Naigang Wang",
        "Navid Ibtehaj Nizam",
        "Stefan T. Radev",
        "Kaoutar El Maghraoui",
        "Xavier Intes",
        "Vikas Pandey"
      ],
      "abstract": "Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology employed for distance and depth estimation across medical, automotive, and other fields, encounters significant computational challenges in scattering media. The complex nature of the acquired FLiDAR signal, particularly in such environments, makes isolating photon time-of-flight (related to target depth) and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the effectiveness of current analytical and computational methodologies. To overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE) framework tailored for specialized modeling of diverse temporal components. In contrast to the conventional MoE approaches our expert models are informed by underlying physics, such as the radiative transport equation governing photon propagation in scattering media. Central to our approach is EvidenceMoE, which integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess the reliability of each expert's output by providing per-expert quality scores and corrective feedback. A Decider Network then leverages this information to fuse expert predictions into a robust final estimate adaptively. We validate our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for non-invasive cancer cell depth detection generated from photon transport models in tissue. Our framework demonstrates strong performance, achieving a normalized root mean squared error (NRMSE) of 0.030 for depth estimation and 0.074 for fluorescence lifetime.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EvidenceMoEï¼Œè¿™æ˜¯ä¸€ç§ç‰©ç†å¼•å¯¼çš„ Mixture-of-Experts (MoE) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Fluorescence LiDAR (FLiDAR) åœ¨æ•£å°„ä»‹è´¨ (scattering media) ä¸­éš¾ä»¥åˆ†ç¦»ç›®æ ‡æ·±åº¦ä¸è§å…‰å¯¿å‘½ (fluorescence lifetime) çš„è®¡ç®—æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„ä¸“å®¶æ¨¡å‹å—è¾å°„ä¼ è¾“æ–¹ç¨‹ (Radiative Transport Equation) ç­‰ç‰©ç†åŸç†æŒ‡å¯¼ï¼Œå¹¶å¼•å…¥äº†åŸºäºè¯æ®çš„ç‹„åˆ©å…‹é›·è¯„è®ºå®¶ (Evidence-Based Dirichlet Critics, EDCs) æ¥è¯„ä¼°å„ä¸“å®¶çš„å¯é æ€§å¹¶æä¾›ä¿®æ­£åé¦ˆã€‚é€šè¿‡ Decider Network çš„è‡ªé€‚åº”èåˆï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿçš„ç™Œç»†èƒæ·±åº¦æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ·±åº¦ä¼°è®¡å’Œè§å…‰å¯¿å‘½çš„ NRMSE åˆ†åˆ«è¾¾åˆ°äº† 0.030 å’Œ 0.074ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚ä»‹è´¨ä¸‹çš„æ£€æµ‹ç²¾åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "physics.optics"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.21532v1",
      "published_date": "2025-05-23 16:38:13 UTC",
      "updated_date": "2025-05-23 16:38:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:25.384403+00:00"
    },
    {
      "arxiv_id": "2505.18079v4",
      "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding",
      "title_zh": "Deep Video Discoveryï¼šé¢å‘é•¿è§†é¢‘ç†è§£çš„å·¥å…·è°ƒç”¨æ™ºèƒ½ä½“æœç´¢",
      "authors": [
        "Xiaoyi Zhang",
        "Zhaoyang Jia",
        "Zongyu Guo",
        "Jiahao Li",
        "Bin Li",
        "Houqiang Li",
        "Yan Lu"
      ],
      "abstract": "Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery (DVD) agent to leverage an agentic search strategy over segmented video clips. Unlike previous video agents that rely on predefined workflows applied uniformly across different queries, our approach emphasizes the autonomous and adaptive nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools to orchestrate adaptive workflow for different queries in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates our advantage. Our DVD agent achieves state-of-the-art performance on the challenging LVBench dataset, reaching an accuracy of 74.2%, which substantially surpasses all prior works, and further improves to 76.0% with transcripts. The code has been released at https://github.com/microsoft/DeepVideoDiscovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Deep Video Discovery (DVD) æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘ç†è§£ (Long-form video understanding) ä¸­ç”±äºæé«˜çš„æ—¶ç©ºå¤æ‚åº¦å’Œä¿¡æ¯å¯†åº¦å¸¦æ¥çš„æŒ‘æˆ˜ã€‚DVD é‡‡ç”¨äº†ä¸€ç§åŸºäºå·¥å…·è°ƒç”¨çš„è‡ªä¸»é€‚åº”æœç´¢ç­–ç•¥ï¼Œé€šè¿‡åœ¨å¤šç²’åº¦è§†é¢‘æ•°æ®åº“ä¸Šåˆ©ç”¨æœç´¢ä¸­å¿ƒåŒ–å·¥å…·ï¼Œå……åˆ†å‘æŒ¥ Large Language Models (LLMs) çš„é«˜çº§æ¨ç†ä¸è§„åˆ’èƒ½åŠ›ã€‚ä¸ä¾èµ–å›ºå®šå·¥ä½œæµçš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒDVD èƒ½å¤Ÿæ ¹æ®å½“å‰è§‚å¯ŸçŠ¶æ€æˆ˜ç•¥æ€§åœ°é€‰æ‹©å·¥å…·ï¼Œå¹¶é’ˆå¯¹ä¸åŒæŸ¥è¯¢åŠ¨æ€ç¼–æ’å·¥ä½œæµã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDVD åœ¨æŒ‘æˆ˜æ€§çš„ LVBench æ•°æ®é›†ä¸Šè¾¾åˆ°äº† 74.2% çš„å‡†ç¡®ç‡ï¼ˆç»“åˆæ–‡æœ¬è½¬å½•å¯è¾¾ 76.0%ï¼‰ï¼Œæ˜¾è‘—è¶…è¶Šäº†æ­¤å‰æ‰€æœ‰ç ”ç©¶å¹¶åˆ·æ–°äº† SOTA æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.18079v4",
      "published_date": "2025-05-23 16:37:36 UTC",
      "updated_date": "2025-11-03 08:39:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:26.602596+00:00"
    },
    {
      "arxiv_id": "2505.18071v2",
      "title": "Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals",
      "title_zh": "æ‰©å±•å½’çº³æ¨ç†ï¼šåŸºäºè¡Œä¸ºä¿¡å·çš„ä¸ªæ€§åŒ–åå¥½æ¨æ–­",
      "authors": [
        "Jia-Nan Li",
        "Jian Guan",
        "Wei Wu",
        "Rui Yan"
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant success in complex reasoning tasks such as math and coding. In contrast to these tasks where deductive reasoning predominates, inductive reasoning-the ability to derive general rules from incomplete evidence, remains underexplored. This paper investigates extended inductive reasoning in LLMs through the lens of personalized preference inference, a critical challenge in LLM alignment where current approaches struggle to capture diverse user preferences. The task demands strong inductive reasoning capabilities as user preferences are typically embedded implicitly across various interaction forms, requiring models to synthesize consistent preference patterns from scattered signals. We propose AlignXplore, a model that leverages extended reasoning chains to enable systematic preference inference from behavioral signals in users' interaction histories. Such explicit preference articulation enables efficient streaming inference: when new behavioral signals emerge, the model can directly build upon previously inferred preference descriptions rather than reprocessing historical signals from scratch, while also supporting iterative refinement to the inferred preferences. We develop AlignXplore by combining cold-start training based on synthetic data with subsequent online reinforcement learning. Through extensive experiments, we demonstrate that AlignXplore achieves substantial improvements over the backbone model by an average of 15.49\\% on in-domain and out-of-domain benchmarks, while maintaining strong generalization ability across different input formats and downstream models. Further analyses establish best practices for preference inference learning through systematic comparison of reward modeling strategies, while revealing the emergence of human-like inductive reasoning patterns during training.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸ªæ€§åŒ–åå¥½æ¨ç†ï¼ˆPersonalized Preference Inferenceï¼‰ä¸­çš„æ‰©å±•å½’çº³æ¨ç†èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¯¹é½æ–¹æ³•éš¾ä»¥æ•æ‰å¤šæ ·åŒ–ç”¨æˆ·åå¥½çš„æŒ‘æˆ˜ã€‚æå‡ºäº† AlignXplore æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ‰©å±•æ¨ç†é“¾ä»äº¤äº’å†å²çš„è¡Œä¸ºä¿¡å·ä¸­ç³»ç»Ÿåœ°æ¨æ–­åå¥½ï¼Œå¹¶æ”¯æŒé«˜æ•ˆçš„æµå¼æ¨ç†ï¼ˆStreaming Inferenceï¼‰ä¸è¿­ä»£ä¼˜åŒ–ã€‚é€šè¿‡ç»“åˆåˆæˆæ•°æ®å†·å¯åŠ¨è®­ç»ƒä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰ï¼ŒAlignXplore åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­æ¯”åŸºåº§æ¨¡å‹å¹³å‡æå‡äº† 15.49%ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¨¡å‹åœ¨ä¸åŒè¾“å…¥æ ¼å¼å’Œä¸‹æ¸¸æ¨¡å‹ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­ç±»äººå½’çº³æ¨ç†æ¨¡å¼çš„æ¶Œç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18071v2",
      "published_date": "2025-05-23 16:16:46 UTC",
      "updated_date": "2025-07-07 17:38:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:27.725112+00:00"
    },
    {
      "arxiv_id": "2505.20320v1",
      "title": "Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP",
      "title_zh": "å‡å°ä¸Šä¸‹æ–‡ï¼Œæ€§èƒ½ä¸å‡ï¼šä¸€ç§é¢å‘èµ„æºé«˜æ•ˆå‹å¤§æ¨¡å‹ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†çš„RAGæ¡†æ¶",
      "authors": [
        "Satya Narayana Cheetirala",
        "Ganesh Raut",
        "Dhavalkumar Patel",
        "Fabio Sanatana",
        "Robert Freeman",
        "Matthew A Levin",
        "Girish N. Nadkarni",
        "Omar Dawkins",
        "Reba Miller",
        "Randolph M. Steinhagen",
        "Eyal Klang",
        "Prem Timsina"
      ],
      "abstract": "Long text classification is challenging for Large Language Models (LLMs) due to token limits and high computational costs. This study explores whether a Retrieval Augmented Generation (RAG) approach using only the most relevant text segments can match the performance of processing entire clinical notes with large context LLMs. We begin by splitting clinical documents into smaller chunks, converting them into vector embeddings, and storing these in a FAISS index. We then retrieve the top 4,000 words most pertinent to the classification query and feed these consolidated segments into an LLM. We evaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication identification task. Metrics such as AUC ROC, precision, recall, and F1 showed no statistically significant differences between the RAG based approach and whole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can significantly reduce token usage without sacrificing classification accuracy, providing a scalable and cost effective solution for analyzing lengthy clinical documents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†é•¿ç¯‡ä¸´åºŠæ–‡æ¡£æ—¶é¢ä¸´çš„ token é™åˆ¶å’Œé«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä¸´åºŠç¬”è®°æ‹†åˆ†ä¸º chunks å¹¶åˆ©ç”¨ FAISS ç´¢å¼•æ£€ç´¢ä¸åˆ†ç±»æŸ¥è¯¢æœ€ç›¸å…³çš„ 4,000 ä¸ªå•è¯ï¼Œä»è€Œå®ç°èµ„æºé«˜æ•ˆçš„ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç† (Clinical NLP)ã€‚åœ¨æ‰‹æœ¯å¹¶å‘ç—‡è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œå¯¹ GPT4oã€LLaMA å’Œ Mistral çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒRAG æ–¹æ¡ˆä¸å…¨æ–‡æœ¬å¤„ç†åœ¨ AUC ROC å’Œ F1 ç­‰æŒ‡æ ‡ä¸Šæ— æ˜¾è‘—ç»Ÿè®¡å­¦å·®å¼‚ (p > 0.05)ã€‚ç ”ç©¶è¯æ˜ï¼ŒRAG å¯ä»¥åœ¨ä¸ç‰ºç‰²åˆ†ç±»å‡†ç¡®æ€§çš„å‰æä¸‹å¤§å¹…å‡å°‘ token ä½¿ç”¨é‡ï¼Œä¸ºåˆ†æå†—é•¿ä¸´åºŠæ–‡æ¡£æä¾›äº†ä¸€ç§ä½æˆæœ¬ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.20320v1",
      "published_date": "2025-05-23 16:13:08 UTC",
      "updated_date": "2025-05-23 16:13:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:40.782666+00:00"
    },
    {
      "arxiv_id": "2505.18066v1",
      "title": "Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making",
      "title_zh": "è¿ˆå‘ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ä»»åŠ¡å§”æ´¾ä¸äººæœºåä½œå†³ç­–",
      "authors": [
        "Min Hun Lee",
        "Martyn Zhe Yu Tok"
      ],
      "abstract": "Despite the growing promise of artificial intelligence (AI) in supporting decision-making across domains, fostering appropriate human reliance on AI remains a critical challenge. In this paper, we investigate the utility of exploring distance-based uncertainty scores for task delegation to AI and describe how these scores can be visualized through embedding representations for human-AI decision-making. After developing an AI-based system for physical stroke rehabilitation assessment, we conducted a study with 19 health professionals and 10 students in medicine/health to understand the effect of exploring distance-based uncertainty scores on users' reliance on AI. Our findings showed that distance-based uncertainty scores outperformed traditional probability-based uncertainty scores in identifying uncertain cases. In addition, after exploring confidence scores for task delegation and reviewing embedding-based visualizations of distance-based uncertainty scores, participants achieved an 8.20% higher rate of correct decisions, a 7.15% higher rate of changing their decisions to correct ones, and a 7.14% lower rate of incorrect changes after reviewing AI outputs than those reviewing probability-based uncertainty scores ($p<0.01$). Our findings highlight the potential of distance-based uncertainty scores to enhance decision accuracy and appropriate reliance on AI while discussing ongoing challenges for human-AI collaborative decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨åŸºäºè·ç¦»çš„ä¸ç¡®å®šæ€§è¯„åˆ† (distance-based uncertainty scores) æ¥ä¼˜åŒ–ä»»åŠ¡å‘ AI çš„å§”æ´¾ï¼Œå¹¶ç ”ç©¶äº†å¦‚ä½•é€šè¿‡åµŒå…¥è¡¨ç¤º (embedding representations) å¯¹å…¶è¿›è¡Œå¯è§†åŒ–ä»¥è¾…åŠ©äººæœºåä½œå†³ç­–ã€‚é€šè¿‡åœ¨ä¸€ä¸ªä¸­é£åº·å¤è¯„ä¼°ç³»ç»Ÿä¸Šçš„å®è¯ç ”ç©¶å‘ç°ï¼ŒåŸºäºè·ç¦»çš„ä¸ç¡®å®šæ€§è¯„åˆ†åœ¨è¯†åˆ«ä¸ç¡®å®šæ¡ˆä¾‹æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„åŸºäºæ¦‚ç‡çš„è¯„åˆ† (probability-based uncertainty scores)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æŸ¥é˜…åŸºäºè·ç¦»çš„ä¸ç¡®å®šæ€§è¯„åˆ†åŠå…¶å¯è§†åŒ–è¡¨ç¤ºåï¼Œå‚ä¸è€…çš„å†³ç­–å‡†ç¡®ç‡æé«˜äº† 8.20%ï¼Œçº é”™ç‡æå‡äº† 7.15%ï¼Œæ˜¾è‘—å¢å¼ºäº†äººç±»å¯¹ AI çš„åˆç†ä¾èµ–å¹¶ä¼˜åŒ–äº†åä½œå†³ç­–è¡¨ç°ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "ACM FAccT 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.18066v1",
      "published_date": "2025-05-23 16:12:39 UTC",
      "updated_date": "2025-05-23 16:12:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:49.046015+00:00"
    },
    {
      "arxiv_id": "2505.18243v1",
      "title": "ZeroML: A Next Generation AutoML Language",
      "title_zh": "ZeroMLï¼šä¸‹ä¸€ä»£ AutoML è¯­è¨€",
      "authors": [
        "Monirul Islam Mahmud"
      ],
      "abstract": "ZeroML is a new generation programming language for AutoML to drive the ML pipeline in a compiled and multi-paradigm way, with a pure functional core. Meeting the shortcomings introduced by Python, R, or Julia such as slow-running time, brittle pipelines or high dependency cost ZeroML brings the Microservices-based architecture adding the modular, reusable pieces such as DataCleaner, FeatureEngineer or ModelSelector. As a native multithread and memory-aware search optimized toolkit, and with one command deployability ability, ZeroML ensures non-coders and ML professionals to create high-accuracy models super fast and in a more reproducible way. The verbosity of the language ensures that when it comes to dropping into the backend, the code we will be creating is extremely clear but the level of repetition and boilerplate required when developing on the front end is now removed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† ZeroMLï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸º AutoML è®¾è®¡çš„æ–°ä¸€ä»£ç¼–ç¨‹è¯­è¨€ï¼Œé‡‡ç”¨ç¼–è¯‘å‹ã€å¤šèŒƒå¼ä¸”å…·æœ‰çº¯å‡½æ•°å¼æ ¸å¿ƒçš„æ¶æ„ã€‚é’ˆå¯¹ Pythonã€R å’Œ Julia å­˜åœ¨çš„è¿è¡Œç¼“æ…¢åŠæµæ°´çº¿è„†å¼±ç­‰ç¼ºé™·ï¼ŒZeroML å¼•å…¥äº†åŸºäºå¾®æœåŠ¡ (Microservices) çš„æ¶æ„ï¼Œæä¾›äº†å¦‚ DataCleanerã€FeatureEngineer å’Œ ModelSelector ç­‰æ¨¡å—åŒ–ä¸”å¯å¤ç”¨çš„ç»„ä»¶ã€‚ä½œä¸ºä¸€ç§åŸç”Ÿæ”¯æŒå¤šçº¿ç¨‹ (multithread) ä¸”å…·å¤‡å†…å­˜æ„ŸçŸ¥æœç´¢ä¼˜åŒ–èƒ½åŠ›çš„å·¥å…·ï¼ŒZeroML æ”¯æŒâ€œä¸€é”®éƒ¨ç½²â€ï¼Œèƒ½å¸®åŠ©éç¼–ç¨‹äººå‘˜å’Œæœºå™¨å­¦ä¹ ä¸“ä¸šäººå£«ä»¥æ›´é«˜æ•ˆã€å¯é‡å¤çš„æ–¹å¼æ„å»ºé«˜ç²¾åº¦æ¨¡å‹ã€‚è¯¥è¯­è¨€åœ¨æ˜¾è‘—å‡å°‘å‰ç«¯é‡å¤ä»£ç å’Œæ ·æ¿ä»£ç çš„åŒæ—¶ï¼Œç¡®ä¿äº†ç”Ÿæˆçš„åç«¯ä»£ç å…·æœ‰æé«˜çš„æ¸…æ™°åº¦ã€‚",
      "categories": [
        "cs.PL",
        "cs.AI"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18243v1",
      "published_date": "2025-05-23 16:01:49 UTC",
      "updated_date": "2025-05-23 16:01:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:38.188250+00:00"
    },
    {
      "arxiv_id": "2505.21531v2",
      "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¯¹äººä½“åŠ¨ä½œäº†è§£å¤šå°‘ï¼Ÿä»¥3DåŒ–èº«æ§åˆ¶ä¸ºä¾‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Kunhang Li",
        "Jason Naradowsky",
        "Yansong Feng",
        "Yusuke Miyao"
      ],
      "abstract": "We explore the human motion knowledge of Large Language Models (LLMs) through 3D avatar control. Given a motion instruction, we prompt LLMs to first generate a high-level movement plan with consecutive steps (High-level Planning), then specify body part positions in each step (Low-level Planning), which we linearly interpolate into avatar animations. Using 20 representative motion instructions that cover fundamental movements and balance body part usage, we conduct comprehensive evaluations, including human and automatic scoring of both high-level movement plans and generated animations, as well as automatic comparison with oracle positions in low-level planning. Our findings show that LLMs are strong at interpreting high-level body movements but struggle with precise body part positioning. While decomposing motion queries into atomic components improves planning, LLMs face challenges in multi-step movements involving high-degree-of-freedom body parts. Furthermore, LLMs provide reasonable approximations for general spatial descriptions, but fall short in handling precise spatial specifications. Notably, LLMs demonstrate promise in conceptualizing creative motions and distinguishing culturally specific motion patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ 3D avatar controlï¼ˆ3D åŒ–èº«æ§åˆ¶ï¼‰æ¢è®¨äº† Large Language Models (LLMs) å¯¹äººç±»åŠ¨ä½œçŸ¥è¯†çš„æŒæ¡ç¨‹åº¦ã€‚ç ”ç©¶è€…è®¾è®¡äº†ä¸€å¥—ä»é«˜å±‚çº§åŠ¨ä½œè§„åˆ’ (High-level Planning) åˆ°ä½å±‚çº§è‚¢ä½“å®šä½ (Low-level Planning) çš„æ¡†æ¶ï¼Œå°† LLMs ç”Ÿæˆçš„æŒ‡ä»¤è½¬åŒ–ä¸ºåŒ–èº«åŠ¨ç”»å¹¶è¿›è¡Œå¤šç»´åº¦è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMs æ“…é•¿ç†è§£é«˜å±‚çº§çš„èº«ä½“è¿åŠ¨é€»è¾‘ï¼Œä½†åœ¨ç²¾ç¡®çš„èº«ä½“éƒ¨ä½ç©ºé—´å®šä½ä¸Šè¡¨ç°æ¬ ä½³ã€‚è™½ç„¶åˆ†è§£åŠ¨ä½œæŒ‡ä»¤èƒ½æå‡è§„åˆ’æ•ˆæœï¼Œä½† LLMs åœ¨å¤„ç†æ¶‰åŠé«˜è‡ªç”±åº¦ (high-degree-of-freedom) éƒ¨ä½çš„å¤šæ­¥åŠ¨ä½œæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒLLMs åœ¨æ„æ€åˆ›æ„åŠ¨ä½œåŠåŒºåˆ†ç‰¹å®šæ–‡åŒ–åŠ¨ä½œæ¨¡å¼æ–¹é¢å±•ç°äº†ç‹¬ç‰¹æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21531v2",
      "published_date": "2025-05-23 16:01:08 UTC",
      "updated_date": "2025-09-20 08:00:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:15:52.620141+00:00"
    },
    {
      "arxiv_id": "2505.18053v1",
      "title": "FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation",
      "title_zh": "FDBPLï¼šé¢å‘åŒºåŸŸæ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡å‹é€‚é…çš„æ›´å¿«é€Ÿè’¸é¦æç¤ºå­¦ä¹ ",
      "authors": [
        "Zherui Zhang",
        "Jiaxin Wu",
        "Changwei Wang",
        "Rongtao Xu",
        "Longzhao Huang",
        "Wenhao Xu",
        "Wenbo Xu",
        "Li Guo",
        "Shibiao Xu"
      ],
      "abstract": "Prompt learning as a parameter-efficient method that has been widely adopted to adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt design requires domain expertise and iterative optimization, soft-prompt methods rely heavily on task-specific hard labels, limiting their generalization to unseen categories. Recent popular distillation-based prompt learning methods improve generalization by exploiting larger teacher VLMs and unsupervised knowledge transfer, yet their repetitive teacher model online inference sacrifices the inherent training efficiency advantage of prompt learning. In this paper, we propose {\\large {\\textbf{F}}}aster {\\large {\\textbf{D}}}istillation-{\\large {\\textbf{B}}}ased {\\large {\\textbf{P}}}rompt {\\large {\\textbf{L}}}earning (\\textbf{FDBPL}), which addresses these issues by sharing soft supervision contexts across multiple training stages and implementing accelerated I/O. Furthermore, FDBPL introduces a region-aware prompt learning paradigm with dual positive-negative prompt spaces to fully exploit randomly cropped regions that containing multi-level information. We propose a positive-negative space mutual learning mechanism based on similarity-difference learning, enabling student CLIP models to recognize correct semantics while learning to reject weakly related concepts, thereby improving zero-shot performance. Unlike existing distillation-based prompt learning methods that sacrifice parameter efficiency for generalization, FDBPL maintains dual advantages of parameter efficiency and strong downstream generalization. Comprehensive evaluations across 11 datasets demonstrate superior performance in base-to-new generalization, cross-dataset transfer, and robustness tests, achieving $2.2\\times$ faster training speed.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FDBPL (Faster Distillation-Based Prompt Learning)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºè’¸é¦çš„ Prompt Learning æ–¹æ³•å› é‡å¤è°ƒç”¨æ•™å¸ˆæ¨¡å‹æ¨ç†è€Œå¯¼è‡´çš„è®­ç»ƒæ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å…±äº«è½¯ç›‘ç£ä¸Šä¸‹æ–‡å’Œä¼˜åŒ– I/O å®ç°äº† 2.2 å€çš„è®­ç»ƒåŠ é€Ÿï¼Œå¹¶å¼•å…¥äº†å…·æœ‰æ­£è´Ÿ Prompt ç©ºé—´çš„åŒºåŸŸæ„ŸçŸ¥ (region-aware) å­¦ä¹ èŒƒå¼ã€‚é€šè¿‡ç›¸ä¼¼æ€§-å·®å¼‚äº’å­¦ä¹ æœºåˆ¶ï¼ŒFDBPL ä½¿å­¦ç”Ÿ CLIP æ¨¡å‹åœ¨æŒæ¡æ­£ç¡®è¯­ä¹‰çš„åŒæ—¶èƒ½å¤Ÿæ‹’ç»å¼±ç›¸å…³æ¦‚å¿µï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬ (zero-shot) æ€§èƒ½ã€‚åœ¨ 11 ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶ï¼Œåœ¨åŸºç¡€åˆ°æ–°ç±»æ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»åŠé²æ£’æ€§æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18053v1",
      "published_date": "2025-05-23 15:57:16 UTC",
      "updated_date": "2025-05-23 15:57:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:11.322918+00:00"
    },
    {
      "arxiv_id": "2505.18047v2",
      "title": "RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration",
      "title_zh": "RestoreVARï¼šé¢å‘å…¨èƒ½å›¾åƒä¿®å¤çš„è§†è§‰è‡ªå›å½’ç”Ÿæˆ",
      "authors": [
        "Sudarshan Rajagopalan",
        "Kartik Narayan",
        "Vishal M. Patel"
      ],
      "abstract": "The use of latent diffusion models (LDMs) such as Stable Diffusion has significantly improved the perceptual quality of All-in-One image Restoration (AiOR) methods, while also enhancing their generalization capabilities. However, these LDM-based frameworks suffer from slow inference due to their iterative denoising process, rendering them impractical for time-sensitive applications. Visual autoregressive modeling (VAR), a recently introduced approach for image generation, performs scale-space autoregression and achieves comparable performance to that of state-of-the-art diffusion transformers with drastically reduced computational costs. Moreover, our analysis reveals that coarse scales in VAR primarily capture degradations while finer scales encode scene detail, simplifying the restoration process. Motivated by this, we propose RestoreVAR, a novel VAR-based generative approach for AiOR that significantly outperforms LDM-based models in restoration performance while achieving over $10\\times$ faster inference. To optimally exploit the advantages of VAR for AiOR, we propose architectural modifications and improvements, including intricately designed cross-attention mechanisms and a latent-space refinement module, tailored for the AiOR task. Extensive experiments show that RestoreVAR achieves state-of-the-art performance among generative AiOR methods, while also exhibiting strong generalization capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RestoreVARï¼Œä¸€ç§åŸºäºè§†è§‰è‡ªå›å½’å»ºæ¨¡ (Visual Autoregressive modeling, VAR) çš„å…¨èƒ½å›¾åƒæ¢å¤ (All-in-One Image Restoration, AiOR) ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ½œæ‰©æ•£æ¨¡å‹ (LDMs) å› è¿­ä»£å»å™ªå¯¼è‡´çš„æ¨ç†é€Ÿåº¦ç¼“æ…¢é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼ŒVAR çš„ç²—å°ºåº¦ä¸»è¦æ•æ‰é€€åŒ–ç‰¹å¾ï¼Œè€Œç»†å°ºåº¦åˆ™ç¼–ç åœºæ™¯ç»†èŠ‚ï¼Œè¿™ä¸€ç‰¹æ€§æ˜¾è‘—ç®€åŒ–äº†å›¾åƒæ¢å¤è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥ä¸“é—¨è®¾è®¡çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ (cross-attention mechanisms) å’Œæ½œç©ºé—´ç»†åŒ–æ¨¡å— (latent-space refinement module)ï¼ŒRestoreVAR åœ¨ä¿æŒå¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦æ¯”åŸºäº LDM çš„æ¨¡å‹æå‡äº† 10 å€ä»¥ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRestoreVAR åœ¨ç”Ÿæˆå¼ AiOR é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿› (State-of-the-art) çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://sudraj2002.github.io/restorevarpage/",
      "pdf_url": "https://arxiv.org/pdf/2505.18047v2",
      "published_date": "2025-05-23 15:52:26 UTC",
      "updated_date": "2025-10-25 22:06:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:07.223950+00:00"
    },
    {
      "arxiv_id": "2505.18044v1",
      "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
      "title_zh": "çº¿æ€§æ··åˆåˆ†å¸ƒé²æ£’é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹",
      "authors": [
        "Zhishuai Liu",
        "Pan Xu"
      ],
      "abstract": "Many real-world decision-making problems face the off-dynamics challenge: the agent learns a policy in a source domain and deploys it in a target domain with different state transitions. The distributionally robust Markov decision process (DRMDP) addresses this challenge by finding a robust policy that performs well under the worst-case environment within a pre-specified uncertainty set of transition dynamics. Its effectiveness heavily hinges on the proper design of these uncertainty sets, based on prior knowledge of the dynamics. In this work, we propose a novel linear mixture DRMDP framework, where the nominal dynamics is assumed to be a linear mixture model. In contrast with existing uncertainty sets directly defined as a ball centered around the nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a ball around the mixture weighting parameter. We show that this new framework provides a more refined representation of uncertainties compared to conventional models based on $(s,a)$-rectangularity and $d$-rectangularity, when prior knowledge about the mixture model is present. We propose a meta algorithm for robust policy learning in linear mixture DRMDPs with general $f$-divergence defined uncertainty sets, and analyze its sample complexities under three divergence metrics instantiations: total variation, Kullback-Leibler, and $Ï‡^2$ divergences. These results establish the statistical learnability of linear mixture DRMDPs, laying the theoretical foundation for future research on this new setting.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¦»çº¿åŠ¨åŠ›å­¦(off-dynamics)æŒ‘æˆ˜ï¼Œæå‡ºäº†çº¿æ€§æ··åˆåˆ†å¸ƒé²æ£’é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Linear Mixture Distributionally Robust Markov Decision Processes, DRMDP)æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿç›´æ¥åœ¨åä¹‰æ ¸(nominal kernel)å‘¨å›´å®šä¹‰ä¸ç¡®å®šæ€§é›†çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶åŸºäºæ··åˆæƒé‡å‚æ•°æ„å»ºä¸ç¡®å®šæ€§é›†ï¼Œåœ¨å…·å¤‡å…ˆéªŒçŸ¥è¯†æ—¶æ¯”ä¼ ç»Ÿçš„$(s,a)$-rectangularityå’Œ$d$-rectangularityæ¨¡å‹èƒ½æä¾›æ›´ç²¾ç»†çš„ä¸ç¡®å®šæ€§è¡¨ç¤ºã€‚ç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ä¸ªæ”¯æŒä¸€èˆ¬$f$-divergenceä¸ç¡®å®šæ€§é›†çš„å…ƒç®—æ³•ï¼Œå¹¶åˆ†æäº†å…¶åœ¨å…¨å˜åˆ†(Total Variation)ã€KLæ•£åº¦å’Œ$Ï‡^2$æ•£åº¦ä¸‹çš„æ ·æœ¬å¤æ‚åº¦ã€‚è¿™äº›æˆæœè¯æ˜äº†çº¿æ€§æ··åˆDRMDPçš„ç»Ÿè®¡å¯å­¦ä¹ æ€§ï¼Œä¸ºè¯¥æ–°é¢†åŸŸçš„åç»­ç ”ç©¶å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18044v1",
      "published_date": "2025-05-23 15:48:11 UTC",
      "updated_date": "2025-05-23 15:48:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:12.865060+00:00"
    },
    {
      "arxiv_id": "2505.18034v2",
      "title": "Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks",
      "title_zh": "ç»“æ„åŒ–æ€ç»´ï¼šæå‡å¤§è¯­è¨€æ¨¡å‹åœ¨å› æœæ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Wentao Sun",
        "JoÃ£o Paulo Nogueira",
        "Alonso Silva"
      ],
      "abstract": "Despite remarkable advances in the field, LLMs remain unreliable in distinguishing causation from correlation. Recent results from the Corr2Cause dataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score: 29.08) -- only marginally outperform random baselines (Random Uniform, F1 score: 20.38), indicating limited capacity of generalization. To tackle this limitation, we propose a novel structured approach: rather than directly answering causal queries, we provide the model with the capability to structure its thinking by guiding the model to build a structured knowledge graph, systematically encoding the provided correlational premises, to answer the causal queries. This intermediate representation significantly enhances the model's causal capabilities. Experiments on the test subset of the Corr2Cause dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains over standard direct prompting methods, improving F1 scores from 32.71 to 48.26 (over 47.5% relative increase), along with notable improvements in precision and recall. These results underscore the effectiveness of providing the model with the capability to structure its thinking and highlight its promising potential for broader generalization across diverse causal inference tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŒºåˆ†å› æœä¸ç›¸å…³æ€§ (causation vs correlation) è¡¨ç°ä¸å¯é çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“æ„åŒ–æ€ç»´æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ç›´æ¥å›ç­”å› æœæŸ¥è¯¢ï¼Œè€Œæ˜¯å¼•å¯¼æ¨¡å‹æ„å»ºç»“æ„åŒ–çŸ¥è¯†å›¾è°± (structured knowledge graph)ï¼Œé€šè¿‡ç³»ç»Ÿç¼–ç ç›¸å…³æ€§å‰æ (correlational premises) æ¥è¾…åŠ©æ¨ç†ã€‚åœ¨ Corr2Cause åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨ Qwen3-32B æ¨¡å‹çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å°† F1 åˆ†æ•°ä» 32.71 æå‡è‡³ 48.26ï¼Œç›¸å¯¹å¢å¹…è¶…è¿‡ 47.5%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œèµ‹äºˆæ¨¡å‹ç»“æ„åŒ–æ€ç»´èƒ½åŠ›èƒ½æœ‰æ•ˆå¢å¼ºå…¶åœ¨å› æœæ¨ç† (causal inference) ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18034v2",
      "published_date": "2025-05-23 15:37:40 UTC",
      "updated_date": "2025-05-27 08:16:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:22.239919+00:00"
    },
    {
      "arxiv_id": "2505.18030v1",
      "title": "Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons",
      "title_zh": "åŸºäºæˆå¯¹æ¯”è¾ƒçš„æ—¶åºé€»è¾‘å…¬å¼åå¥½è‡ªåŠ¨æœºå­¦ä¹ ",
      "authors": [
        "Hazhar Rahmani",
        "Jie Fu"
      ],
      "abstract": "Many preference elicitation algorithms consider preference over propositional logic formulas or items with different attributes. In sequential decision making, a user's preference can be a preorder over possible outcomes, each of which is a temporal sequence of events. This paper considers a class of preference inference problems where the user's unknown preference is represented by a preorder over regular languages (sets of temporal sequences), referred to as temporal goals. Given a finite set of pairwise comparisons between finite words, the objective is to learn both the set of temporal goals and the preorder over these goals. We first show that a preference relation over temporal goals can be modeled by a Preference Deterministic Finite Automaton (PDFA), which is a deterministic finite automaton augmented with a preorder over acceptance conditions. The problem of preference inference reduces to learning the PDFA. This problem is shown to be computationally challenging, with the problem of determining whether there exists a PDFA of size smaller than a given integer $k$, consistent with the sample, being NP-Complete. We formalize the properties of characteristic samples and develop an algorithm that guarantees to learn, given a characteristic sample, the minimal PDFA equivalent to the true PDFA from which the sample is drawn. We present the method through a running example and provide detailed analysis using a robotic motion planning problem.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä»æˆå¯¹æ¯”è¾ƒ(Pairwise Comparisons)ä¸­å­¦ä¹ ç”¨æˆ·å¯¹æ—¶åºé€»è¾‘å…¬å¼(Temporal Logic Formulas)çš„åå¥½ï¼Œä»¥è§£å†³é¡ºåºå†³ç­–ä¸­å¯¹æ—¶åºäº‹ä»¶åºåˆ—çš„åå¥½å»ºæ¨¡é—®é¢˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åå¥½ç¡®å®šæ€§æœ‰é™è‡ªåŠ¨æœº(Preference Deterministic Finite Automaton, PDFA)æ¨¡å‹ï¼Œé€šè¿‡åœ¨è‡ªåŠ¨æœºçš„æ¥å—æ¡ä»¶ä¸Šå¢åŠ ååºå…³ç³»(Preorder)æ¥è¡¨ç¤ºå¯¹ä¸åŒæ—¶åºç›®æ ‡(Temporal Goals)çš„åå¥½ã€‚ä½œè€…è¯æ˜äº†å¯»æ‰¾ä¸ç»™å®šæ ·æœ¬ä¸€è‡´ä¸”æ»¡è¶³ç‰¹å®šå¤§å°é™åˆ¶çš„æœ€å°PDFAæ˜¯ä¸€ä¸ªNP-Completeé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶å½¢å¼åŒ–äº†ç‰¹å¾æ ·æœ¬(Characteristic Samples)çš„å±æ€§ï¼Œå¹¶å¼€å‘äº†ä¸€ç§èƒ½å¤Ÿä¿è¯ä»ç‰¹å¾æ ·æœ¬ä¸­å­¦ä¹ å‡ºä¸çœŸå®PDFAç­‰ä»·çš„æœ€å°åŒ–ç®—æ³•ã€‚æœ€åï¼Œé€šè¿‡æœºå™¨äººè¿åŠ¨è§„åˆ’(Robotic Motion Planning)é—®é¢˜çš„è¯¦ç»†åˆ†æéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.FL",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 11 figures, technical report, submission under review",
      "pdf_url": "https://arxiv.org/pdf/2505.18030v1",
      "published_date": "2025-05-23 15:35:39 UTC",
      "updated_date": "2025-05-23 15:35:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:29.467035+00:00"
    },
    {
      "arxiv_id": "2505.18028v3",
      "title": "Knot So Simple: A Minimalistic Environment for Spatial Reasoning",
      "title_zh": "ç»³ç»“ä¸ç®€å•ï¼šç”¨äºç©ºé—´æ¨ç†çš„æç®€ç¯å¢ƒ",
      "authors": [
        "Zizhao Chen",
        "Yoav Artzi"
      ],
      "abstract": "We propose KnotGym, an interactive environment for complex, spatial reasoning and manipulation. KnotGym includes goal-oriented rope manipulation tasks with varying levels of complexity, all requiring acting from pure image observations. Tasks are defined along a clear and quantifiable axis of complexity based on the number of knot crossings, creating a natural generalization test. KnotGym has a simple observation space, allowing for scalable development, yet it highlights core challenges in integrating acute perception, spatial reasoning, and grounded manipulation. We evaluate methods of different classes, including model-based RL, model-predictive control, and chain-of-thought reasoning, and illustrate the challenges KnotGym presents. KnotGym is available at https://github.com/lil-lab/knotgym.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KnotGymï¼Œä¸€ä¸ªæ—¨åœ¨ä¿ƒè¿›å¤æ‚ç©ºé—´æ¨ç† (spatial reasoning) å’Œæ“ä½œ (manipulation) çš„äº¤äº’å¼ç¯å¢ƒã€‚è¯¥ç¯å¢ƒåŒ…å«å¤šç§åŸºäºçº¯å›¾åƒè§‚æµ‹ (pure image observations) çš„ç›®æ ‡å¯¼å‘ç»³ç´¢æ“ä½œä»»åŠ¡ï¼Œå¹¶æ ¹æ®ç»“çš„äº¤å‰æ•°é‡ (knot crossings) å®šä¹‰äº†æ¸…æ™°ä¸”å¯é‡åŒ–çš„ä»»åŠ¡å¤æ‚åº¦ã€‚KnotGym ä¸“æ³¨äºé›†æˆæ•é”æ„ŸçŸ¥ã€ç©ºé—´æ¨ç†å’Œå…·èº«æ“ä½œçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ºæµ‹è¯•ç®—æ³•çš„æ³›åŒ–èƒ½åŠ›æä¾›äº†è‡ªç„¶çš„æ ‡å‡†ã€‚ç ”ç©¶è€…è¯„ä¼°äº†åŒ…æ‹¬ model-based RLã€model-predictive control (MPC) å’Œ chain-of-thought reasoning åœ¨å†…çš„å¤šç§æ–¹æ³•ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†æ­¤ç±»é«˜åº¦å¤æ‚çš„ç©ºé—´ä»»åŠ¡æ—¶æ‰€é¢ä¸´çš„ä¸¥å³»æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Fix camera ready footer",
      "pdf_url": "https://arxiv.org/pdf/2505.18028v3",
      "published_date": "2025-05-23 15:34:08 UTC",
      "updated_date": "2026-01-18 19:17:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:19.572554+00:00"
    },
    {
      "arxiv_id": "2505.21530v1",
      "title": "High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework",
      "title_zh": "åŸºäºè§†è§‰è‡ªå›å½’æ¡†æ¶çš„é«˜ä¿çœŸåŠŸèƒ½è¶…å£°é‡å»º",
      "authors": [
        "Xuhang Chen",
        "Zhuo Li",
        "Yanyan Shen",
        "Mufti Mahmud",
        "Hieu Pham",
        "Chi-Man Pun",
        "Shuqiang Wang"
      ],
      "abstract": "Functional ultrasound (fUS) imaging provides exceptional spatiotemporal resolution for neurovascular mapping, yet its practical application is significantly hampered by critical challenges. Foremost among these are data scarcity, arising from ethical considerations and signal degradation through the cranium, which collectively limit dataset diversity and compromise the fairness of downstream machine learning models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è‡ªå›å½’ (Visual Auto-Regressive, VAR) æ¡†æ¶çš„é«˜ä¿çœŸåŠŸèƒ½æ€§è¶…å£° (functional ultrasound, fUS) é‡å»ºæ–¹æ³•ã€‚é’ˆå¯¹ fUS æˆåƒåœ¨ç¥ç»è¡€ç®¡åˆ¶å›¾ä¸­é¢ä¸´çš„æ•°æ®ç¨€ç¼ºã€ä¼¦ç†é™åˆ¶ä»¥åŠç©¿è¿‡é¢…éª¨å¯¼è‡´çš„ä¿¡å·é€€åŒ–ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æå‡æ•°æ®é›†çš„å¤šæ ·æ€§å¹¶ä¼˜åŒ–æˆåƒè´¨é‡ã€‚é€šè¿‡å¼•å…¥ VAR æœºåˆ¶ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ç”±äºæ•°æ®å¤šæ ·æ€§ä¸è¶³è€Œå½±å“ä¸‹æ¸¸æœºå™¨å­¦ä¹ æ¨¡å‹å…¬å¹³æ€§çš„é—®é¢˜ï¼Œä¸ºå®ç°é«˜ç²¾åº¦ã€é²æ£’çš„ç¥ç»è¡€ç®¡æˆåƒæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚\n\n---\nå¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦è½¬æ¢ï¼Œæˆ–è€…æƒ³é’ˆå¯¹è¿™ä¸€é¢†åŸŸè¿›è¡Œæ›´æ·±å…¥çš„æ¢è®¨ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21530v1",
      "published_date": "2025-05-23 15:27:17 UTC",
      "updated_date": "2025-05-23 15:27:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:40.717307+00:00"
    },
    {
      "arxiv_id": "2505.18019v1",
      "title": "LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„Webåº”ç”¨åŠŸèƒ½éœ€æ±‚ç”Ÿæˆï¼šå››ç§ä¸»æµLLMåœ¨é£Ÿå ‚ç®¡ç†ç³»ç»Ÿä¸­çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Rashmi Gupta",
        "Aditya K Gupta",
        "Aarav Jain",
        "Avinash C Pandey",
        "Atul Gupta"
      ],
      "abstract": "Like any other discipline, Large Language Models (LLMs) have significantly impacted software engineering by helping developers generate the required artifacts across various phases of software development. This paper presents a case study comparing the performance of popular LLMs GPT, Claude, Gemini, and DeepSeek in generating functional specifications that include use cases, business rules, and collaborative workflows for a web application, the Mess Management System. The study evaluated the quality of LLM generated use cases, business rules, and collaborative workflows in terms of their syntactic and semantic correctness, consistency, non ambiguity, and completeness compared to the reference specifications against the zero-shot prompted problem statement. Our results suggested that all four LLMs can specify syntactically and semantically correct, mostly non-ambiguous artifacts. Still, they may be inconsistent at times and may differ significantly in the completeness of the generated specification. Claude and Gemini generated all the reference use cases, with Claude achieving the most complete but somewhat redundant use case specifications. Similar results were obtained for specifying workflows. However, all four LLMs struggled to generate relevant Business Rules, with DeepSeek generating the most reference rules but with less completeness. Overall, Claude generated more complete specification artifacts, while Gemini was more precise in the specifications it generated.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é£Ÿå ‚ç®¡ç†ç³»ç»Ÿ (Mess Management System) è¿›è¡Œäº†æ¡ˆä¾‹ç ”ç©¶ï¼Œå¯¹æ¯”è¯„ä¼°äº† GPTã€Claudeã€Gemini å’Œ DeepSeek å››ç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨ç”ŸæˆåŠŸèƒ½è§„æ ¼è¯´æ˜ï¼ˆåŒ…æ‹¬ Use Casesã€Business Rules å’Œåä½œå·¥ä½œæµï¼‰æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒé‡‡ç”¨é›¶æ ·æœ¬æç¤º (Zero-shot Prompting) ç­–ç•¥ï¼Œä»è¯­æ³•ä¸è¯­ä¹‰æ­£ç¡®æ€§ã€ä¸€è‡´æ€§ã€æ— æ­§ä¹‰æ€§å’Œå®Œå¤‡æ€§å››ä¸ªç»´åº¦å¯¹ç”Ÿæˆäº§ç‰©è¿›è¡Œäº†åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œå››ç§æ¨¡å‹åœ¨è¯­æ³•è¯­ä¹‰å’Œæ¶ˆé™¤æ­§ä¹‰æ–¹é¢å‡è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å®Œå¤‡æ€§ä¸Šå·®å¼‚æ˜¾è‘—ã€‚å…¶ä¸­ï¼ŒClaude åœ¨ Use Cases å’Œå·¥ä½œæµç”Ÿæˆä¸Šæœ€ä¸ºå®Œå¤‡ï¼ŒGemini åˆ™è¡¨ç°å‡ºæ›´é«˜çš„ç²¾å‡†åº¦ï¼›è€Œæ‰€æœ‰æ¨¡å‹åœ¨ç”Ÿæˆ Business Rules æ–¹é¢å‡é¢ä¸´æŒ‘æˆ˜ï¼Œå…¶ä¸­ DeepSeek ç”Ÿæˆçš„è§„åˆ™æ•°é‡æœ€å¤šä½†å®Œå¤‡åº¦æœ‰é™ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "11 pages, 12 figures, Accepted in EASE 2025 https://conf.researchr.org/details/ease-2025/ease-2025-ai-models---data/11/LLM-assisted-web-application-functional-requirements-generation-A-case-study-of-fou",
      "pdf_url": "https://arxiv.org/pdf/2505.18019v1",
      "published_date": "2025-05-23 15:25:50 UTC",
      "updated_date": "2025-05-23 15:25:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:46.484663+00:00"
    },
    {
      "arxiv_id": "2505.18018v1",
      "title": "ExoGait-MS: Learning Periodic Dynamics with Multi-Scale Graph Network for Exoskeleton Gait Recognition",
      "title_zh": "ExoGait-MSï¼šåŸºäºå¤šå°ºåº¦å›¾ç½‘ç»œå­¦ä¹ å‘¨æœŸæ€§åŠ¨åŠ›å­¦çš„å¤–éª¨éª¼æ­¥æ€è¯†åˆ«",
      "authors": [
        "Lijiang Liu",
        "Junyu Shi",
        "Yong Sun",
        "Zhiyuan Zhang",
        "Jinni Zhou",
        "Shugen Ma",
        "Qiang Nie"
      ],
      "abstract": "Current exoskeleton control methods often face challenges in delivering personalized treatment. Standardized walking gaits can lead to patient discomfort or even injury. Therefore, personalized gait is essential for the effectiveness of exoskeleton robots, as it directly impacts their adaptability, comfort, and rehabilitation outcomes for individual users. To enable personalized treatment in exoskeleton-assisted therapy and related applications, accurate recognition of personal gait is crucial for implementing tailored gait control. The key challenge in gait recognition lies in effectively capturing individual differences in subtle gait features caused by joint synergy, such as step frequency and step length. To tackle this issue, we propose a novel approach, which uses Multi-Scale Global Dense Graph Convolutional Networks (GCN) in the spatial domain to identify latent joint synergy patterns. Moreover, we propose a Gait Non-linear Periodic Dynamics Learning module to effectively capture the periodic characteristics of gait in the temporal domain. To support our individual gait recognition task, we have constructed a comprehensive gait dataset that ensures both completeness and reliability. Our experimental results demonstrate that our method achieves an impressive accuracy of 94.34% on this dataset, surpassing the current state-of-the-art (SOTA) by 3.77%. This advancement underscores the potential of our approach to enhance personalized gait control in exoskeleton-assisted therapy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ExoGait-MS æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤–éª¨éª¼ (Exoskeleton) æ§åˆ¶ä¸­å› ç¼ºä¹ä¸ªæ€§åŒ–æ­¥æ€è¯†åˆ«è€Œå¯¼è‡´çš„é€‚é…æ€§ä¸èˆ’é€‚åº¦ä¸è¶³çš„é—®é¢˜ã€‚åœ¨æ–¹æ³•è®ºä¸Šï¼Œè¯¥æ¡†æ¶åœ¨ç©ºé—´åŸŸåˆ©ç”¨å¤šå°ºåº¦å…¨å±€å¯†é›†å›¾å·ç§¯ç½‘ç»œ (Multi-Scale Global Dense Graph Convolutional Networks) è¯†åˆ«æ½œåœ¨çš„å…³èŠ‚ååŒæ¨¡å¼ï¼Œå¹¶åœ¨æ—¶é—´åŸŸå¼•å…¥æ­¥æ€éçº¿æ€§å‘¨æœŸåŠ¨åŠ›å­¦å­¦ä¹ æ¨¡å— (Gait Non-linear Periodic Dynamics Learning module) æ•æ‰æ­¥æ€çš„å‘¨æœŸæ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„æ­¥æ€æ•°æ®é›†ç”¨äºæ¨¡å‹éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ªä½“æ­¥æ€è¯†åˆ«ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡è¾¾åˆ° 94.34%ï¼Œæ¯”ç°æœ‰æœ€å…ˆè¿› (SOTA) æ–¹æ³•æé«˜äº† 3.77%ï¼Œä¸ºå¤–éª¨éª¼è¾…åŠ©åº·å¤æ²»ç–—ä¸­çš„ä¸ªæ€§åŒ–æ­¥æ€æ§åˆ¶æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18018v1",
      "published_date": "2025-05-23 15:24:25 UTC",
      "updated_date": "2025-05-23 15:24:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:48.186072+00:00"
    },
    {
      "arxiv_id": "2505.18011v1",
      "title": "Training with Pseudo-Code for Instruction Following",
      "title_zh": "é¢å‘æŒ‡ä»¤éµå¾ªçš„ä¼ªä»£ç è®­ç»ƒ",
      "authors": [
        "Prince Kumar",
        "Rudra Murthy",
        "Riyaz Bhat",
        "Danish Contractor"
      ],
      "abstract": "Despite the rapid progress in the capabilities of Large Language Models (LLMs), they continue to have difficulty following relatively simple, unambiguous instructions, especially when compositions are involved. In this paper, we take inspiration from recent work that suggests that models may follow instructions better when they are expressed in pseudo-code. However, writing pseudo-code programs can be tedious and using few-shot demonstrations to craft code representations for use in inference can be unnatural for non-expert users of LLMs. To overcome these limitations, we propose fine-tuning LLMs with instruction-tuning data that additionally includes instructions re-expressed in pseudo-code along with the final response. We evaluate models trained using our method on $11$ publicly available benchmarks comprising of tasks related to instruction-following, mathematics, and common-sense reasoning. We conduct rigorous experiments with $5$ different models and find that not only do models follow instructions better when trained with pseudo-code, they also retain their capabilities on the other tasks related to mathematical and common sense reasoning. Specifically, we observe a relative gain of $3$--$19$% on instruction-following benchmark, and an average gain of upto 14% across all tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨ä¼ªä»£ç (Pseudo-Code)è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ(Instruction-Tuning)çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨éµå¾ªå¤æ‚åŠç»„åˆæŒ‡ä»¤æ–¹é¢çš„å±€é™ã€‚é€šè¿‡åœ¨è®­ç»ƒæ•°æ®ä¸­åŠ å…¥å°†æŒ‡ä»¤é‡å†™ä¸ºä¼ªä»£ç çš„é€»è¾‘è¡¨ç¤ºï¼Œè¯¥æ–¹æ³•å…‹æœäº†ç”¨æˆ·åœ¨æ¨ç†é˜¶æ®µæ‰‹åŠ¨ç¼–å†™ä¼ªä»£ç çš„ç¹çæ€§ã€‚åœ¨5ç§æ¨¡å‹å’Œ11ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹åœ¨æŒ‡ä»¤éµå¾ª(Instruction-Following)ä»»åŠ¡ä¸­å®ç°äº†3%-19%çš„ç›¸å¯¹æå‡ï¼Œä¸”åœ¨æ•°å­¦å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä½¿ç”¨ä¼ªä»£ç è¿›è¡Œè®­ç»ƒä¸ä»…èƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£ï¼Œè¿˜èƒ½åœ¨å„ç±»ä»»åŠ¡ä¸­å¸¦æ¥é«˜è¾¾14%çš„å¹³å‡æ€§èƒ½å¢ç›Šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2505.18011v1",
      "published_date": "2025-05-23 15:14:29 UTC",
      "updated_date": "2025-05-23 15:14:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:16:33.178719+00:00"
    },
    {
      "arxiv_id": "2505.18241v1",
      "title": "Intent Classification on Low-Resource Languages with Query Similarity Search",
      "title_zh": "åŸºäºæŸ¥è¯¢ç›¸ä¼¼æ€§æœç´¢çš„ä½èµ„æºè¯­è¨€æ„å›¾åˆ†ç±»",
      "authors": [
        "Arjun Bhalla",
        "Qi Huang"
      ],
      "abstract": "Intent classification is an important component of a functional Information Retrieval ecosystem. Many current approaches to intent classification, typically framed as a classification problem, can be problematic as intents are often hard to define and thus data can be difficult and expensive to annotate. The problem is exacerbated when we need to extend the intent classification system to support multiple and in particular low-resource languages. To address this, we propose casting intent classification as a query similarity search problem - we use previous example queries to define an intent, and a query similarity method to classify an incoming query based on the labels of its most similar queries in latent space. With the proposed approach, we are able to achieve reasonable intent classification performance for queries in low-resource languages in a zero-shot setting.",
      "tldr_zh": "### è®ºæ–‡æ‘˜è¦æ€»ç»“ ğŸ“„\n\n---\n\nè¯¥ç ”ç©¶é’ˆå¯¹ä½èµ„æºè¯­è¨€ (Low-Resource Languages) åœ¨æ„å›¾åˆ†ç±» (Intent Classification) ä¸­é¢ä¸´çš„æ•°æ®æ ‡æ³¨å›°éš¾å’Œæˆæœ¬é«˜æ˜‚ç­‰é—®é¢˜ï¼Œæå‡ºå°†æ„å›¾åˆ†ç±»ä»»åŠ¡è½¬åŒ–ä¸ºæŸ¥è¯¢ç›¸ä¼¼åº¦æœç´¢ (Query Similarity Search) é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ—¢æœ‰çš„ç¤ºä¾‹æŸ¥è¯¢æ¥å®šä¹‰æ„å›¾ï¼Œé€šè¿‡åœ¨éšç©ºé—´ (latent space) ä¸­æ£€ç´¢ä¸è¾“å…¥æŸ¥è¯¢æœ€ç›¸ä¼¼çš„ç¤ºä¾‹æ ‡ç­¾æ¥è¿›è¡Œåˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬ (zero-shot) è®¾ç½®ä¸‹èƒ½å¤Ÿä¸ºä½èµ„æºè¯­è¨€çš„æŸ¥è¯¢æä¾›åˆç†çš„åˆ†ç±»æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ¡ˆæœ‰æ•ˆè§£å†³äº†å¤šè¯­è¨€ç¯å¢ƒä¸‹æ„å›¾å®šä¹‰æ¨¡ç³ŠåŠæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚\n\n---\n\nå¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦æ€»ç»“ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18241v1",
      "published_date": "2025-05-23 15:11:12 UTC",
      "updated_date": "2025-05-23 15:11:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:01.945168+00:00"
    },
    {
      "arxiv_id": "2505.18006v1",
      "title": "AI Literacy for Legal AI Systems: A practical approach",
      "title_zh": "æ³•å¾‹äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„AIç´ å…»ï¼šä¸€ç§å®è·µæ–¹æ³•",
      "authors": [
        "Gizem Gultekin-Varkonyi"
      ],
      "abstract": "Legal AI systems are increasingly being adopted by judicial and legal system deployers and providers worldwide to support a range of applications. While they offer potential benefits such as reducing bias, increasing efficiency, and improving accountability, they also pose significant risks, requiring a careful balance between opportunities, and legal and ethical development and deployment. AI literacy, as a legal requirement under the EU AI Act and a critical enabler of ethical AI for deployers and providers, could be a tool to achieve this. The article introduces the term \"legal AI systems\" and then analyzes the concept of AI literacy and the benefits and risks associated with these systems. This analysis is linked to a broader AI-L concept for organizations that deal with legal AI systems. The outcome of the article, a roadmap questionnaire as a practical tool for developers and providers to assess risks, benefits, and stakeholder concerns, could be useful in meeting societal and regulatory expectations for legal AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Legal AI systems åœ¨å…¨çƒå¸æ³•åŠæ³•å¾‹ç³»ç»Ÿä¸­çš„åº”ç”¨åŠå…¶å¸¦æ¥çš„æ”¶ç›Šä¸é£é™©ï¼Œå¹¶æ˜ç¡®äº† \"legal AI systems\" è¿™ä¸€æœ¯è¯­ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº†åœ¨ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹(EU AI Act) èƒŒæ™¯ä¸‹ï¼ŒAI literacy ä½œä¸ºåˆè§„è¦æ±‚å’Œæ¨åŠ¨ä¼¦ç† AI çš„é‡è¦æ€§ã€‚è®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº†é’ˆå¯¹ç›¸å…³ç»„ç»‡çš„ AI literacy æ¦‚å¿µæ¡†æ¶ï¼Œè¯¦ç»†åˆ†æäº†æ³•å¾‹äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ½œåœ¨æ”¶ç›Šä¸é£é™©ã€‚ä½œä¸ºæ ¸å¿ƒè´¡çŒ®ï¼Œç ”ç©¶æä¾›äº†ä¸€ä»½è·¯çº¿å›¾è°ƒæŸ¥é—®å· (roadmap questionnaire)ï¼Œä½œä¸ºå¼€å‘äººå‘˜å’Œæä¾›å•†è¯„ä¼°é£é™©ã€æ”¶ç›ŠåŠåˆ©ç›Šç›¸å…³è€…å…³åˆ‡çš„å®ç”¨å·¥å…·ï¼Œä»¥æ»¡è¶³ç¤¾ä¼šå’Œç›‘ç®¡å¯¹ Legal AI çš„é¢„æœŸã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.CY",
      "comment": "Forthcoming in Iustum Aequum Salutare (2025) vol.21",
      "pdf_url": "https://arxiv.org/pdf/2505.18006v1",
      "published_date": "2025-05-23 15:10:28 UTC",
      "updated_date": "2025-05-23 15:10:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:04.720069+00:00"
    },
    {
      "arxiv_id": "2505.18003v1",
      "title": "An Example Safety Case for Safeguards Against Misuse",
      "title_zh": "é˜²èŒƒè¯¯ç”¨é˜²æŠ¤æªæ–½çš„å®‰å…¨æ¡ˆä¾‹ç¤ºä¾‹",
      "authors": [
        "Joshua Clymer",
        "Jonah Weinbaum",
        "Robert Kirk",
        "Kimberly Mai",
        "Selena Zhang",
        "Xander Davies"
      ],
      "abstract": "Existing evaluations of AI misuse safeguards provide a patchwork of evidence that is often difficult to connect to real-world decisions. To bridge this gap, we describe an end-to-end argument (a \"safety case\") that misuse safeguards reduce the risk posed by an AI assistant to low levels. We first describe how a hypothetical developer red teams safeguards, estimating the effort required to evade them. Then, the developer plugs this estimate into a quantitative \"uplift model\" to determine how much barriers introduced by safeguards dissuade misuse (https://www.aimisusemodel.com/). This procedure provides a continuous signal of risk during deployment that helps the developer rapidly respond to emerging threats. Finally, we describe how to tie these components together into a simple safety case. Our work provides one concrete path -- though not the only path -- to rigorously justifying AI misuse risks are low.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰AIæ»¥ç”¨é˜²æŠ¤è¯„ä¼°è¯æ®ç¢ç‰‡åŒ–ã€éš¾ä»¥æŒ‡å¯¼å†³ç­–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å®‰å…¨æ¡ˆä¾‹(safety case)æ¡†æ¶ï¼Œæ—¨åœ¨è®ºè¯é˜²æŠ¤æªæ–½èƒ½å°†AIåŠ©æ‰‹çš„æ»¥ç”¨é£é™©é™è‡³æä½æ°´å¹³ã€‚ä½œè€…è¯¦ç»†é˜è¿°äº†å¼€å‘è€…å¦‚ä½•é€šè¿‡çº¢é˜Ÿæµ‹è¯•(red teaming)è¯„ä¼°é˜²æŠ¤è§„é¿éš¾åº¦ï¼Œå¹¶åˆ©ç”¨å®šé‡çš„â€œæå‡æ¨¡å‹â€(uplift model)è¡¡é‡é˜²æŠ¤æ‰‹æ®µå¯¹æ½œåœ¨æ»¥ç”¨è¡Œä¸ºçš„åŠé˜»ç¨‹åº¦ã€‚è¯¥æµç¨‹å¯åœ¨éƒ¨ç½²æœŸé—´æä¾›æŒç»­çš„é£é™©ä¿¡å·ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿå“åº”æ–°å…´å¨èƒã€‚è¿™é¡¹å·¥ä½œä¸ºä¸¥è°¨åœ°è¯æ˜AIæ»¥ç”¨é£é™©å¤„äºä½ä½æä¾›äº†ä¸€æ¡å…·ä½“ä¸”å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18003v1",
      "published_date": "2025-05-23 15:06:21 UTC",
      "updated_date": "2025-05-23 15:06:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:20.557678+00:00"
    },
    {
      "arxiv_id": "2505.22683v1",
      "title": "ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging",
      "title_zh": "ConnectomeDiffuserï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½èµ‹èƒ½åŸºäºå¼¥æ•£å¼ é‡æˆåƒçš„è„‘ç½‘ç»œæ„å»º",
      "authors": [
        "Xuhang Chen",
        "Michael Kwok-Po Ng",
        "Kim-Fung Tsang",
        "Chi-Man Pun",
        "Shuqiang Wang"
      ],
      "abstract": "Brain network analysis plays a crucial role in diagnosing and monitoring neurodegenerative disorders such as Alzheimer's disease (AD). Existing approaches for constructing structural brain networks from diffusion tensor imaging (DTI) often rely on specialized toolkits that suffer from inherent limitations: operator subjectivity, labor-intensive workflows, and restricted capacity to capture complex topological features and disease-specific biomarkers. To overcome these challenges and advance computational neuroimaging instrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based framework for automated end-to-end brain network construction from DTI. The proposed model combines three key components: (1) a Template Network that extracts topological features from 3D DTI scans using Riemannian geometric principles, (2) a diffusion model that generates comprehensive brain networks with enhanced topological fidelity, and (3) a Graph Convolutional Network classifier that incorporates disease-specific markers to improve diagnostic accuracy. ConnectomeDiffuser demonstrates superior performance by capturing a broader range of structural connectivity and pathology-related information, enabling more sensitive analysis of individual variations in brain networks. Experimental validation on datasets representing two distinct neurodegenerative conditions demonstrates significant performance improvements over other brain network methods. This work contributes to the advancement of instrumentation in the context of neurological disorders, providing clinicians and researchers with a robust, generalizable measurement framework that facilitates more accurate diagnosis, deeper mechanistic understanding, and improved therapeutic monitoring of neurodegenerative diseases such as AD.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ConnectomeDiffuserï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹ (diffusion-based) çš„æ–°å‹è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»å¼¥æ•£å¼ é‡æˆåƒ (DTI) æ„å»ºå¤§è„‘ç½‘ç»œæ—¶å­˜åœ¨çš„æ“ä½œä¸»è§‚æ€§ã€é«˜å¼ºåº¦å·¥ä½œæµä»¥åŠéš¾ä»¥æ•æ‰å¤æ‚æ‹“æ‰‘ç‰¹å¾ç­‰å±€é™ã€‚è¯¥æ¨¡å‹ç»“åˆäº†åŸºäºé»æ›¼å‡ ä½•åŸç†æå–ç‰¹å¾çš„ Template Networkã€æå‡æ‹“æ‰‘ä¿çœŸåº¦çš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥åŠèå…¥ç–¾ç—…ç‰¹å®šæ ‡å¿—ç‰©çš„ Graph Convolutional Network (GCN) åˆ†ç±»å™¨ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒConnectomeDiffuser åœ¨å¤šç§ç¥ç»é€€è¡Œæ€§ç–¾ç—…æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½æ›´çµæ•åœ°åˆ†æè„‘ç½‘ç»œçš„ä¸ªä½“å·®å¼‚ã€‚è¿™ä¸€ç ”ç©¶ä¸ºé˜¿å°”èŒ¨æµ·é»˜ç—… (AD) ç­‰ç¥ç»ç³»ç»Ÿç–¾ç—…çš„ç²¾å‡†è¯Šæ–­ã€æœºåˆ¶ç†è§£å’Œæ²»ç–—ç›‘æµ‹æä¾›äº†ç¨³å¥ä¸”é€šç”¨çš„è®¡ç®—ç¥ç»å½±åƒå·¥å…·ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.22683v1",
      "published_date": "2025-05-23 15:03:58 UTC",
      "updated_date": "2025-05-23 15:03:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:21.918999+00:00"
    },
    {
      "arxiv_id": "2505.21528v2",
      "title": "A Unified and Fast-Sampling Diffusion Bridge Framework via Stochastic Optimal Control",
      "title_zh": "åŸºäºéšæœºæœ€ä¼˜æ§åˆ¶çš„ç»Ÿä¸€å¿«é€Ÿé‡‡æ ·æ‰©æ•£æ¡¥æ¡†æ¶",
      "authors": [
        "Mokai Pan",
        "Kaizhen Zhu",
        "Yuexin Ma",
        "Yanwei Fu",
        "Jingyi Yu",
        "Jingya Wang",
        "Ye Shi"
      ],
      "abstract": "Recent advances in diffusion bridge models leverage Doob's $h$-transform to establish fixed endpoints between distributions, demonstrating promising results in image translation and restoration tasks. However, these approaches often produce blurred or excessively smoothed image details and lack a comprehensive theoretical foundation to explain these shortcomings. To address these limitations, we propose UniDB, a unified and fast-sampling framework for diffusion bridges based on Stochastic Optimal Control (SOC). We reformulate the problem through an SOC-based optimization, proving that existing diffusion bridges employing Doob's $h$-transform constitute a special case, emerging when the terminal penalty coefficient in the SOC cost function tends to infinity. By incorporating a tunable terminal penalty coefficient, UniDB achieves an optimal balance between control costs and terminal penalties, substantially improving detail preservation and output quality. To avoid computationally expensive costs of iterative Euler sampling methods in UniDB, we design a training-free accelerated algorithm by deriving exact closed-form solutions for UniDB's reverse-time SDE. It is further complemented by replacing conventional noise prediction with a more stable data prediction model, along with an SDE-Corrector mechanism that maintains perceptual quality for low-step regimes, effectively reducing error accumulation. Extensive experiments across diverse image restoration tasks validate the superiority and adaptability of the proposed framework, bridging the gap between theoretical generality and practical efficiency. Our code is available online https://github.com/2769433owo/UniDB-plusplus.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniDBï¼Œä¸€ä¸ªåŸºäºéšæœºæœ€ä¼˜æ§åˆ¶ (Stochastic Optimal Control, SOC) çš„ç»Ÿä¸€ä¸”å¿«é€Ÿé‡‡æ ·çš„æ‰©æ•£æ¡¥ (Diffusion Bridge) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹å› ä½¿ç”¨ Doobâ€™s $h$-transform è€Œå¯¼è‡´çš„å›¾åƒç»†èŠ‚æ¨¡ç³Šå’Œç†è®ºåŸºç¡€ä¸è¶³ç­‰é—®é¢˜ã€‚ç ”ç©¶è€…é€šè¿‡å°†é—®é¢˜é‡æ–°å®šä¹‰ä¸º SOC ä¼˜åŒ–é—®é¢˜ï¼Œå¼•å…¥äº†å¯è°ƒèŠ‚çš„ç»ˆç«¯æƒ©ç½šç³»æ•°ï¼Œåœ¨æ§åˆ¶æˆæœ¬ä¸ç»ˆç«¯å‡†ç¡®åº¦ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„ç»†èŠ‚ä¿ç•™ã€‚æ­¤å¤–ï¼ŒUniDB æ¨å¯¼äº†é€†æ—¶ SDE çš„ç²¾ç¡®é—­å¼è§£ï¼Œå®ç°äº†æ— é¡»è®­ç»ƒçš„åŠ é€Ÿé‡‡æ ·ç®—æ³•ï¼Œå¹¶ç»“åˆæ•°æ®é¢„æµ‹æ¨¡å‹ä¸ SDE-Corrector æœºåˆ¶ï¼Œæœ‰æ•ˆå‡å°‘äº†ä½æ­¥æ•°é‡‡æ ·ä¸‹çš„è¯¯å·®ç§¯ç´¯ã€‚å®éªŒè¯æ˜ï¼ŒUniDB åœ¨å¤šç§å›¾åƒä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸå¼¥åˆäº†æ‰©æ•£æ¡¥ç†è®ºé€šç”¨æ€§ä¸å®é™…åº”ç”¨æ•ˆç‡ä¹‹é—´çš„é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21528v2",
      "published_date": "2025-05-23 15:03:02 UTC",
      "updated_date": "2025-11-11 13:17:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:28.125941+00:00"
    },
    {
      "arxiv_id": "2505.17989v4",
      "title": "Outcome-based Reinforcement Learning to Predict the Future",
      "title_zh": "åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ç”¨äºé¢„æµ‹æœªæ¥",
      "authors": [
        "Benjamin Turtel",
        "Danny Franklin",
        "Kris Skotheim",
        "Luke Hewitt",
        "Philipp Schoenegger"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has been an effective approach for improving Large Language Models' reasoning in domains such as coding and mathematics. Here, we apply RLVR methods towards forecasting future real-world events - a challenging task for RL due to the very noisy (and delayed) outcomes involved. Using a novel dataset of recent questions from a prediction market, and accompanying relevant news headlines, we show that a compact (14B) reasoning model can be trained to match or surpass the predictive accuracy of frontier models like o1, while greatly improving probabilistic calibration. The model's performance is also practically meaningful: in a Polymarket trading simulation, we estimate that its bets would have yielded a return on investment of over 10% across all questions in the test set. We detail and compare approaches used in training our model, including augmenting our training-data with synthetic prediction questions, guardrails for learning stability, and median prediction sampling at inference-time.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning with Verifiable Rewards, RLVRï¼‰åº”ç”¨äºæŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œé¢„æµ‹ä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨é¢„æµ‹å¸‚åœºçš„é—®é¢˜å’Œç›¸å…³æ–°é—»æ ‡é¢˜æ„å»ºçš„æ–°é¢–æ•°æ®é›†ï¼Œç ”ç©¶è€…è®­ç»ƒå‡ºä¸€ä¸ª 14B å‚æ•°çš„æ¨ç†æ¨¡å‹ï¼Œå…¶é¢„æµ‹å‡†ç¡®ç‡å¯åª²ç¾æˆ–è¶…è¶Š o1 ç­‰å‰æ²¿æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨æ¦‚ç‡æ ¡å‡†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨ Polymarket äº¤æ˜“æ¨¡æ‹Ÿä¸­å®ç°äº†è¶…è¿‡ 10% çš„æŠ•èµ„å›æŠ¥ç‡ã€‚è®ºæ–‡è¯¦ç»†ä»‹ç»äº†åˆæˆé¢„æµ‹é—®é¢˜å¢å¼ºã€å­¦ä¹ ç¨³å®šæ€§æŠ¤æ ä»¥åŠæ¨ç†æ—¶çš„ä¸­å€¼é‡‡æ ·ï¼ˆmedian prediction samplingï¼‰ç­‰æ ¸å¿ƒè®­ç»ƒä¸ä¼˜åŒ–æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17989v4",
      "published_date": "2025-05-23 14:56:07 UTC",
      "updated_date": "2025-12-01 18:12:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:43.430432+00:00"
    },
    {
      "arxiv_id": "2505.17988v3",
      "title": "Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning",
      "title_zh": "æ¢ç©¶ R1 é£æ ¼å¼ºåŒ–å­¦ä¹ ä¸­å°è§„æ¨¡å¾®è°ƒçš„æœ‰æ•ˆæ€§",
      "authors": [
        "Yutong Chen",
        "Jiandong Gao",
        "Ji Wu"
      ],
      "abstract": "R1-style Reinforcement Learning (RL) significantly enhances Large Language Models' reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has substantial influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring \\textbf{sample effect}. Our hypothetical analysis shows the potential to improve SFT efficiency. Guided by our analysis, we propose \\textbf{Re-distillation}, a technique that aims to boost the effectiveness of small-scale distillation by sampling from the RL-trained policy. Re-distillation shows consistent surprising efficiency on three datasets and both Qwen\\&Llama models: Re-distilled models matched RL performance with far fewer samples and less computation. As a result, on K\\&K dataset, our re-distilled Qwen-2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. We demonstrate that re-distillation can be used to efficiently balance multiple goals in RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: https://github.com/on1262/deep-reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† R1-style Reinforcement Learning (RL) çš„å†…åœ¨æœºåˆ¶ï¼Œå‘ç°å°è§„æ¨¡ SFT è™½ç„¶å¯¹ RL æœ‰æ˜¾è‘—å½±å“ä½†æ•ˆç‡è¾ƒä½ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œé€šè¿‡è¡¡é‡ sample effect æ¥æ¯”è¾ƒ SFT ä¸ RL çš„æ•ˆç‡ï¼Œå¹¶æ®æ­¤æ¨å‡ºäº†ä¸€ç§åä¸º Re-distillation çš„æ–°æœ¯è¯­æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡ä» RL è®­ç»ƒåçš„ç­–ç•¥ä¸­é‡‡æ ·æ¥å¢å¼ºå°è§„æ¨¡è’¸é¦çš„æ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼ŒRe-distillation åœ¨ Qwen å’Œ Llama æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºæé«˜çš„æ•ˆç‡ï¼Œå…¶ä¸­ä»…ä½¿ç”¨ 1K ä¸ª SFT æ ·æœ¬çš„ Qwen-2.5-1.5B æ¨¡å‹åœ¨ K&K æ•°æ®é›†ä¸Šä¾¿è¶…è¶Šäº† DeepSeek-V3-0324ã€‚è¯¥å·¥ä½œè§£é‡Šäº† R1-style RL ä¸­çš„å¤šç§å®è¯ç°è±¡ï¼Œä¸ºåœ¨å¼ºåŒ–å­¦ä¹ ä¸­é«˜æ•ˆå¹³è¡¡å¤šä¸ªç›®æ ‡æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2505.17988v3",
      "published_date": "2025-05-23 14:55:22 UTC",
      "updated_date": "2025-08-05 11:46:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:41.295040+00:00"
    },
    {
      "arxiv_id": "2505.17987v1",
      "title": "ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling",
      "title_zh": "ADLGenï¼šé¢å‘äººç±»æ´»åŠ¨å»ºæ¨¡çš„ç¬¦å·åŒ–äº‹ä»¶è§¦å‘ä¼ æ„Ÿå™¨åºåˆ—åˆæˆ",
      "authors": [
        "Weihang You",
        "Hanqi Jiang",
        "Zishuai Liu",
        "Zihang Xie",
        "Tianming Liu",
        "Jin Lu",
        "Fei Dou"
      ],
      "abstract": "Real world collection of Activities of Daily Living data is challenging due to privacy concerns, costly deployment and labeling, and the inherent sparsity and imbalance of human behavior. We present ADLGen, a generative framework specifically designed to synthesize realistic, event triggered, and symbolic sensor sequences for ambient assistive environments. ADLGen integrates a decoder only Transformer with sign based symbolic temporal encoding, and a context and layout aware sampling mechanism to guide generation toward semantically rich and physically plausible sensor event sequences. To enhance semantic fidelity and correct structural inconsistencies, we further incorporate a large language model into an automatic generate evaluate refine loop, which verifies logical, behavioral, and temporal coherence and generates correction rules without manual intervention or environment specific tuning. Through comprehensive experiments with novel evaluation metrics, ADLGen is shown to outperform baseline generators in statistical fidelity, semantic richness, and downstream activity recognition, offering a scalable and privacy-preserving solution for ADL data synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ADLGenï¼Œä¸€ç§æ—¨åœ¨ä¸ºç¯å¢ƒè¾…åŠ©ç”Ÿæ´»ç©ºé—´åˆæˆçœŸå®ã€äº‹ä»¶è§¦å‘ä¸”ç¬¦å·åŒ–ä¼ æ„Ÿå™¨åºåˆ—çš„ç”Ÿæˆæ¡†æ¶ï¼Œè§£å†³äº†æ—¥å¸¸ç”Ÿæ´»æ´»åŠ¨(ADL)æ•°æ®é‡‡é›†é¢ä¸´çš„éšç§å’Œæˆæœ¬æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä»…è§£ç å™¨çš„ Transformer æ¨¡å‹ã€åŸºäºç¬¦å·çš„æ—¶é—´ç¼–ç (sign-based symbolic temporal encoding)ä»¥åŠæ„ŸçŸ¥ä¸Šä¸‹æ–‡ä¸å¸ƒå±€çš„é‡‡æ ·æœºåˆ¶ï¼Œç¡®ä¿ç”Ÿæˆåºåˆ—åœ¨è¯­ä¹‰å’Œç‰©ç†é€»è¾‘ä¸Šçš„åˆç†æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Model)æ„å»ºè‡ªåŠ¨åŒ–çš„â€œç”Ÿæˆ-è¯„ä¼°-æ”¹è¿›â€å¾ªç¯ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„ä¿®æ­£è§„åˆ™æ¥ä¼˜åŒ–åºåˆ—çš„é€»è¾‘ä¸æ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒADLGen åœ¨ç»Ÿè®¡ä¿çœŸåº¦ã€è¯­ä¹‰ä¸°å¯Œåº¦åŠä¸‹æ¸¸æ´»åŠ¨è¯†åˆ«æ€§èƒ½ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸º ADL æ•°æ®åˆæˆæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”ä¿æŠ¤éšç§çš„æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17987v1",
      "published_date": "2025-05-23 14:52:48 UTC",
      "updated_date": "2025-05-23 14:52:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:50.115375+00:00"
    },
    {
      "arxiv_id": "2505.17974v1",
      "title": "Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models",
      "title_zh": "å¹¿ä¹‰ Fisher åŠ æƒ SVDï¼šç”¨äºå¤§è¯­è¨€æ¨¡å‹å‹ç¼©çš„å¯æ‰©å±• Kronecker åˆ†è§£ Fisher è¿‘ä¼¼",
      "authors": [
        "Viktoriia Chekalina",
        "Daniil Moskovskiy",
        "Daria Cherniuk",
        "Maxim Kurkin",
        "Andrey Kuznetsov",
        "Evgeny Frolov"
      ],
      "abstract": "The Fisher information is a fundamental concept for characterizing the sensitivity of parameters in neural networks. However, leveraging the full observed Fisher information is too expensive for large models, so most methods rely on simple diagonal approximations. While efficient, this approach ignores parameter correlations, often resulting in reduced performance on downstream tasks. In this work, we mitigate these limitations and propose Generalized Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that accounts for both diagonal and off-diagonal elements of the Fisher information matrix, providing a more accurate reflection of parameter importance. To make the method tractable, we introduce a scalable adaptation of the Kronecker-factored approximation algorithm for the observed Fisher information. We demonstrate the effectiveness of our method on LLM compression, showing improvements over existing compression baselines. For example, at a 20 compression rate on the MMLU benchmark, our method outperforms FWSVD, which is based on a diagonal approximation of the Fisher information, by 5 percent, SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Generalized Fisher-Weighted SVD (GFWSVD)ï¼Œä¸€ç§ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„è®­ç»ƒåå‹ç¼©æŠ€æœ¯ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¯¹è§’è¿‘ä¼¼ Fisher information çŸ©é˜µè€Œå¿½ç•¥å‚æ•°ç›¸å…³æ€§çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¯æ‰©å±•çš„ Kronecker-factored approximation ç®—æ³•ï¼ŒåŒæ—¶è€ƒè™‘äº† Fisher çŸ©é˜µçš„å¯¹è§’å’Œéå¯¹è§’å…ƒç´ ï¼Œä»è€Œæ›´ç²¾ç¡®åœ°åæ˜ å‚æ•°é‡è¦æ€§å¹¶å®ç°é«˜æ•ˆè®¡ç®—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGFWSVD åœ¨ LLM å‹ç¼©ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ MMLU åŸºå‡†æµ‹è¯•çš„ 20% å‹ç¼©ç‡ä¸‹ï¼Œå…¶å‡†ç¡®ç‡æ¯” FWSVD é«˜å‡º 5%ï¼Œä¹Ÿæ˜¾è‘—ä¼˜äº SVD-LLM å’Œ ASVD ç­‰ç°æœ‰åŸºå‡†ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œåˆ©ç”¨æ›´å¤æ‚çš„ Fisher ä¿¡æ¯è¿‘ä¼¼æŠ€æœ¯è¿›è¡Œæ¨¡å‹å‹ç¼©çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17974v1",
      "published_date": "2025-05-23 14:41:52 UTC",
      "updated_date": "2025-05-23 14:41:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:36.377869+00:00"
    },
    {
      "arxiv_id": "2505.17968v1",
      "title": "Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ˜¯å¯é çš„äººå·¥æ™ºèƒ½ç§‘å­¦å®¶å—ï¼Ÿé»‘ç›’ç³»ç»Ÿé€†å‘å·¥ç¨‹èƒ½åŠ›è¯„ä¼°",
      "authors": [
        "Jiayi Geng",
        "Howard Chen",
        "Dilip Arumugam",
        "Thomas L. Griffiths"
      ],
      "abstract": "Using AI to create autonomous researchers has the potential to accelerate scientific discovery. A prerequisite for this vision is understanding how well an AI model can identify the underlying structure of a black-box system from its behavior. In this paper, we explore how well a large language model (LLM) learns to identify a black-box function from passively observed versus actively collected data. We investigate the reverse-engineering capabilities of LLMs across three distinct types of black-box systems, each chosen to represent different problem domains where future autonomous AI researchers may have considerable impact: Program, Formal Language, and Math Equation. Through extensive experiments, we show that LLMs fail to extract information from observations, reaching a performance plateau that falls short of the ideal of Bayesian inference. However, we demonstrate that prompting LLMs to not only observe but also intervene -- actively querying the black-box with specific inputs to observe the resulting output -- improves performance by allowing LLMs to test edge cases and refine their beliefs. By providing the intervention data from one LLM to another, we show that this improvement is partly a result of engaging in the process of generating effective interventions, paralleling results in the literature on human learning. Further analysis reveals that engaging in intervention can help LLMs escape from two common failure modes: overcomplication, where the LLM falsely assumes prior knowledge about the black-box, and overlooking, where the LLM fails to incorporate observations. These insights provide practical guidance for helping LLMs more effectively reverse-engineer black-box systems, supporting their use in making new discoveries.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¨‹åºï¼ˆProgramï¼‰ã€å½¢å¼è¯­è¨€ï¼ˆFormal Languageï¼‰å’Œæ•°å­¦æ–¹ç¨‹ï¼ˆMath Equationï¼‰ä¸‰ä¸ªé¢†åŸŸçš„é»‘ç›’ç³»ç»Ÿé€†å‘å·¥ç¨‹ï¼ˆReverse-Engineeringï¼‰èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMs åœ¨ä»…ä¾é è¢«åŠ¨è§‚å¯Ÿæ•°æ®æ—¶è¡¨ç°æ¬ ä½³ï¼Œæ— æ³•è¾¾åˆ°è´å¶æ–¯æ¨ç†ï¼ˆBayesian inferenceï¼‰çš„ç†æƒ³æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡æç¤ºæ¨¡å‹è¿›è¡Œä¸»åŠ¨å¹²é¢„ï¼ˆInterventionï¼‰â€”â€”å³ä¸»åŠ¨å‘é»‘ç›’ç³»ç»Ÿå‘é€ç‰¹å®šæŸ¥è¯¢å¹¶è§‚å¯Ÿè¾“å‡ºï¼Œå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹è¯†åˆ«æ½œåœ¨ç»“æ„çš„èƒ½åŠ›ã€‚è¿™ç§æ”¹è¿›éƒ¨åˆ†æºäºæ¨¡å‹åœ¨ç”Ÿæˆå¹²é¢„æªæ–½è¿‡ç¨‹ä¸­å¯¹è¾¹ç¼˜æ¡ˆä¾‹çš„æµ‹è¯•å’Œä¿¡å¿µä¿®æ­£ï¼Œç±»ä¼¼äºäººç±»çš„å­¦ä¹ è¿‡ç¨‹ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œä¸»åŠ¨å¹²é¢„èƒ½å¸®åŠ© LLMs å…‹æœâ€œè¿‡åº¦å¤æ‚åŒ–â€ï¼ˆOvercomplicationï¼‰å’Œâ€œå¿½è§†è§‚å¯Ÿâ€ï¼ˆOverlookingï¼‰è¿™ä¸¤ç§å¸¸è§çš„å¤±æ•ˆæ¨¡å¼ï¼Œä¸ºåˆ©ç”¨ LLMs å®ç°è‡ªä¸»ç§‘å­¦å‘ç°æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.17968v1",
      "published_date": "2025-05-23 14:37:36 UTC",
      "updated_date": "2025-05-23 14:37:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:17:44.626078+00:00"
    },
    {
      "arxiv_id": "2505.17967v4",
      "title": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization of Large Language Models",
      "title_zh": "åŸºäº FFT çš„å¤§è¯­è¨€æ¨¡å‹ä½ç§©è‡ªé€‚åº”ä¼˜åŒ–åŠ¨æ€å­ç©ºé—´é€‰æ‹©",
      "authors": [
        "Ionut-Vlad Modoranu",
        "Mher Safaryan",
        "Erik Schultheis",
        "Max Ryabinin",
        "Artem Chumachenko",
        "Dan Alistarh"
      ],
      "abstract": "Low-rank optimization has emerged as a promising direction in training large language models (LLMs) to improve running time and reduce the memory usage of adaptive optimizers by constraining learning to a lower-dimensional space. Prior work typically projects gradients of linear layers using approaches based on Singular Value Decomposition (SVD) or QR-decomposition. Applying these techniques individually to each layer in large models is computationally expensive and incurs additional memory costs due to storing the projection matrices. In this work, we propose a computationally efficient and conceptually simple, two-step procedure to approximate SVD/QR-based gradient projections into lower-dimensional spaces by using a predefined orthogonal matrix of the Discrete Cosine Transform (DCT). We dynamically select columns from the DCT matrix based on their alignment with the gradient of each layer. The effective projection matrices are obtained via a simple matmul with the DCT matrix in $O(n^3)$ time, followed by a lightweight sorting step to identify the most relevant basis vectors. For large layers, DCT can be computed via Makhoul's $N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$ time. Due to the predefined nature of the orthogonal bases, they are computed once at the start of training. Our numerical experiments on both pre-training and fine-tuning tasks demonstrate the effectiveness of our dual strategy in approximating optimal low-rank projections, obtaining an approach with rank-independent running time that matches the performance of costly SVD/QR-based methods while achieving faster runtime and reduced memory usage by up to $25\\%$ across different model sizes. Our code is available at \\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}{\\texttt{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¿«é€Ÿå‚…é‡Œå¶å˜æ¢ (FFT) çš„åŠ¨æ€å­ç©ºé—´é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ä½ç§©è‡ªé€‚åº”ä¼˜åŒ–ï¼Œä»¥è§£å†³ä¼ ç»Ÿ SVD æˆ– QR åˆ†è§£æ–¹æ³•åœ¨è®¡ç®—å’Œå­˜å‚¨ä¸Šçš„é«˜é¢å¼€é”€ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„å®šä¹‰çš„ç¦»æ•£ä½™å¼¦å˜æ¢ (Discrete Cosine Transform, DCT) æ­£äº¤çŸ©é˜µï¼Œé€šè¿‡è®¡ç®—æ¢¯åº¦ä¸åŸºå‘é‡çš„å¯¹é½ç¨‹åº¦æ¥åŠ¨æ€é€‰æ‹©æŠ•å½±å­ç©ºé—´ã€‚å¯¹äºå¤§è§„æ¨¡ç½‘ç»œå±‚ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨åŸºäº FFT çš„ç®—æ³•å°†è®¡ç®—å¤æ‚åº¦é™ä½è‡³ $O(n^2 \\log n)$ï¼Œä¸”æ­£äº¤åŸºåœ¨è®­ç»ƒå¼€å§‹æ—¶ä»…éœ€è®¡ç®—ä¸€æ¬¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†ä¸ SVD/QR ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å°†å†…å­˜ä½¿ç”¨é‡é™ä½äº†é«˜è¾¾ 25%ï¼Œå¹¶æ˜¾è‘—æå‡äº†è¿è¡Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17967v4",
      "published_date": "2025-05-23 14:37:00 UTC",
      "updated_date": "2025-10-08 15:33:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:19.403084+00:00"
    },
    {
      "arxiv_id": "2505.17961v2",
      "title": "Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation",
      "title_zh": "åŸºäºå€¾å‘è¯„åˆ†èšåˆçš„å¤šä¸­å¿ƒè§‚å¯Ÿæ•°æ®è”é‚¦å› æœæ¨æ–­",
      "authors": [
        "Khellaf RÃ©mi",
        "Bellet AurÃ©lien",
        "Josse Julie"
      ],
      "abstract": "Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this problem by estimating the Average Treatment Effect (ATE) from decentralized observational data via a Federated Learning (FL) approach, allowing inference through the exchange of aggregate statistics rather than individual-level data. We propose a novel method to estimate propensity scores by computing a federated weighted average of local scores with Membership Weights (MW)--probabilities of site membership conditional on covariates--which can be flexibly estimated using parametric or non-parametric classification models. Unlike density ratio weights (DW) from the transportability and generalization literature, which either rely on strong modeling assumptions or cannot be implemented in FL, MW can be estimated using standard FL algorithms and are more robust, as they support flexible, non-parametric models--making them the preferred choice in multi-site settings with strict data-sharing constraints. The resulting propensity scores are used to construct Federated Inverse Propensity Weighting (Fed-IPW) and Augmented IPW (Fed-AIPW) estimators. Unlike meta-analysis methods, which fail when any site violates positivity, our approach leverages heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions. Both theoretical analysis and experiments on simulated and real-world data highlight their advantages over meta-analysis and related methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç«™ç‚¹è§‚æµ‹æ•°æ®ä¸­çš„å› æœæ¨æ–­(Causal Inference)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè”é‚¦å­¦ä¹ (Federated Learning)çš„æ¡†æ¶ï¼Œä»¥è§£å†³å› éšç§å’Œæ³•å¾‹é™åˆ¶å¯¼è‡´çš„ä¸ªä½“çº§æ•°æ®é›†ä¸­éš¾é¢˜ã€‚ç ”ç©¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†æˆå‘˜èµ„æ ¼æƒé‡(Membership Weights, MW)æ¥èšåˆå„ç«™ç‚¹çš„å€¾å‘è¯„åˆ†(Propensity Scores)ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„å¯†åº¦æ¯”æƒé‡(DW)ï¼ŒMWèƒ½æ›´å¥½åœ°é€‚é…æ ‡å‡†è”é‚¦ç®—æ³•å¹¶æ”¯æŒçµæ´»çš„éå‚æ•°æ¨¡å‹ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æ„å»ºäº†è”é‚¦é€†æ¦‚ç‡åŠ æƒ(Fed-IPW)å’Œè”é‚¦å¢å¼ºé€†æ¦‚ç‡åŠ æƒ(Fed-AIPW)ä¼°è®¡é‡ï¼Œç”¨äºåœ¨å»ä¸­å¿ƒåŒ–ç¯å¢ƒä¸‹å‡†ç¡®ä¼°ç®—å¹³å‡å¤„ç†æ•ˆåº”(ATE)ã€‚å®éªŒå’Œç†è®ºè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåˆ©ç”¨ç«™ç‚¹é—´çš„å¼‚æ„æ€§æ¥æ”¹å–„é‡å æ€§(Overlap)é—®é¢˜ï¼Œåœ¨å¤„ç†æ ·æœ¬é‡ã€å¹²é¢„æœºåˆ¶åŠåå˜é‡åˆ†å¸ƒå¼‚æ„çš„ä»»åŠ¡ä¸­ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å…ƒåˆ†æ(Meta-analysis)æ–¹æ³•ã€‚",
      "categories": [
        "stat.ME",
        "cs.AI",
        "math.ST",
        "stat.AP"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17961v2",
      "published_date": "2025-05-23 14:32:57 UTC",
      "updated_date": "2025-09-30 21:10:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:05.842851+00:00"
    },
    {
      "arxiv_id": "2505.18240v1",
      "title": "Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback",
      "title_zh": "ä»¥è´Ÿæ ·æœ¬é©¯æœå¤§è¯­è¨€æ¨¡å‹ï¼šä¸€ç§æä¾›å¯æ“ä½œåé¦ˆçš„æ¼”ç¤ºæ–‡ç¨¿å†…å®¹æ— å‚è€ƒè¯„ä¼°æ¡†æ¶",
      "authors": [
        "Ananth Muppidi",
        "Tarak Das",
        "Sambaran Bandyopadhyay",
        "Tripti Shukla",
        "Dharun D A"
      ],
      "abstract": "The generation of presentation slides automatically is an important problem in the era of generative AI. This paper focuses on evaluating multimodal content in presentation slides that can effectively summarize a document and convey concepts to a broad audience. We introduce a benchmark dataset, RefSlides, consisting of human-made high-quality presentations that span various topics. Next, we propose a set of metrics to characterize different intrinsic properties of the content of a presentation and present REFLEX, an evaluation approach that generates scores and actionable feedback for these metrics. We achieve this by generating negative presentation samples with different degrees of metric-specific perturbations and use them to fine-tune LLMs. This reference-free evaluation technique does not require ground truth presentations during inference. Our extensive automated and human experiments demonstrate that our evaluation approach outperforms classical heuristic-based and state-of-the-art large language model-based evaluations in generating scores and explanations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†REFLEXï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ¼”ç¤ºæ–‡ç¨¿(presentation slides)å†…å®¹å¹¶æä¾›å¯æ“ä½œåé¦ˆ(actionable feedback)çš„æ— å‚è€ƒè¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶é¦–å…ˆå¼•å…¥äº†åŒ…å«å¤šé¢†åŸŸé«˜è´¨é‡äººç±»åˆ¶ä½œæ¼”ç¤ºæ–‡ç¨¿çš„æ•°æ®é›†RefSlidesã€‚REFLEXé€šè¿‡ç”Ÿæˆå¸¦æœ‰ç‰¹å®šæŒ‡æ ‡æ‰°åŠ¨çš„è´Ÿæ ·æœ¬(negative samples)æ¥å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œä»è€Œåœ¨æ¨ç†é˜¶æ®µæ— éœ€å‚è€ƒåŸºå‡†(ground truth)å³å¯å¯¹æ¼”ç¤ºæ–‡ç¨¿çš„å†…åœ¨å±æ€§è¿›è¡Œè¯„åˆ†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè¯„ä¼°åˆ†æ•°å’Œè§£é‡Šæ€§åé¦ˆæ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„å¯å‘å¼æ–¹æ³•ä»¥åŠç°æœ‰çš„SOTAå¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18240v1",
      "published_date": "2025-05-23 14:27:57 UTC",
      "updated_date": "2025-05-23 14:27:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:26.976665+00:00"
    },
    {
      "arxiv_id": "2505.17952v1",
      "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL",
      "title_zh": "è¶…è¶Šè’¸é¦ï¼šé€šè¿‡æç®€è§„åˆ™å¼ºåŒ–å­¦ä¹ çªç ´åŒ»ç–— LLM æ¨ç†æé™",
      "authors": [
        "Che Liu",
        "Haozhe Wang",
        "Jiazhen Pan",
        "Zhongwei Wan",
        "Yong Dai",
        "Fangzhen Lin",
        "Wenjia Bai",
        "Daniel Rueckert",
        "Rossella Arcucci"
      ],
      "abstract": "Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† AlphaMedï¼Œè¿™æ˜¯é¦–ä¸ªè¯æ˜åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ (LLM) æ¨ç†èƒ½åŠ›å¯ä»¥çº¯ç²¹é€šè¿‡å¼ºåŒ–å­¦ä¹  (RL) æ¶Œç°çš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨å…¬å…±å¤šé¡¹é€‰æ‹©é—®ç­”æ•°æ®é›†ä¸Šé‡‡ç”¨æç®€çš„è§„åˆ™å¥–åŠ± (rule-based rewards)ï¼Œæ— éœ€ä¾èµ–é«˜æˆæœ¬çš„æœ‰ç›‘ç£å¾®è°ƒ (SFT) æˆ–ä»é—­æºæ¨¡å‹è’¸é¦çš„é“¾å¼æ€ç»´ (CoT) æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒAlphaMed åœ¨å…­é¡¹åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œåœ¨ MedXpert ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šç”šè‡³è¶…è¶Šäº† DeepSeek-V3 å’Œ Claude-3.5-Sonnet ç­‰å¤§å‹æˆ–é—­æºæ¨¡å‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºï¼Œæ•°æ®é›†çš„ä¿¡æ¯é‡ (informativeness) æ˜¯é©±åŠ¨æ¨ç†æ€§èƒ½çš„å…³é”®ï¼Œè¯æ˜äº†æç®€ RL åœ¨ç¼ºä¹ CoT ç›‘ç£çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆè¯±å¯¼åŒ»ç–—æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2505.17952v1",
      "published_date": "2025-05-23 14:27:37 UTC",
      "updated_date": "2025-05-23 14:27:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:41.412036+00:00"
    },
    {
      "arxiv_id": "2505.17950v2",
      "title": "Evaluating NLP Embedding Models for Handling Science-Specific Symbolic Expressions in Student Texts",
      "title_zh": "è¯„ä¼°NLPåµŒå…¥æ¨¡å‹å¯¹å­¦ç”Ÿæ–‡æœ¬ä¸­ç§‘å­¦ç‰¹å®šç¬¦å·è¡¨è¾¾å¼çš„å¤„ç†èƒ½åŠ›",
      "authors": [
        "Tom Bleckmann",
        "Paul Tschisgale"
      ],
      "abstract": "In recent years, natural language processing (NLP) has become integral to educational data mining, particularly in the analysis of student-generated language products. For research and assessment purposes, so-called embedding models are typically employed to generate numeric representations of text that capture its semantic content for use in subsequent quantitative analyses. Yet when it comes to science-related language, symbolic expressions such as equations and formulas introduce challenges that current embedding models struggle to address. Existing research studies and practical applications often either overlook these challenges or remove symbolic expressions altogether, potentially leading to biased research findings and diminished performance of practical applications. This study therefore explores how contemporary embedding models differ in their capability to process and interpret science-related symbolic expressions. To this end, various embedding models are evaluated using physics-specific symbolic expressions drawn from authentic student responses, with performance assessed via two approaches: 1) similarity-based analyses and 2) integration into a machine learning pipeline. Our findings reveal significant differences in model performance, with OpenAI's GPT-text-embedding-3-large outperforming all other examined models, though its advantage over other models was moderate rather than decisive. Overall, this study underscores the importance for educational data mining researchers and practitioners of carefully selecting NLP embedding models when working with science-related language products that include symbolic expressions. The code and (partial) data are available at https://doi.org/10.17605/OSF.IO/6XQVG.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†(NLP)åµŒå…¥æ¨¡å‹(Embedding Models)åœ¨å¤„ç†å­¦ç”Ÿæ–‡æœ¬ä¸­ç§‘å­¦ç¬¦å·è¡¨è¾¾å¼ï¼ˆå¦‚ç‰©ç†å…¬å¼å’Œæ–¹ç¨‹ï¼‰æ—¶çš„è§£æèƒ½åŠ›ã€‚é€šè¿‡åŸºäºç›¸ä¼¼åº¦çš„åˆ†æå’Œæœºå™¨å­¦ä¹ æµæ°´çº¿(Machine Learning Pipeline)ä¸¤ç§è¯„ä¼°è·¯å¾„ï¼Œç ”ç©¶æ¢è®¨äº†ä¸åŒæ¨¡å‹å¯¹çœŸå®å­¦ç”Ÿåé¦ˆä¸­ç¬¦å·åŒ–å†…å®¹çš„è¡¨å¾å·®å¼‚ã€‚å®éªŒå‘ç°ï¼ŒOpenAIçš„`text-embedding-3-large`åœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œå°½ç®¡å…¶é¢†å…ˆä¼˜åŠ¿å¹¶éå†³å®šæ€§çš„ã€‚è¯¥ç ”ç©¶å¼ºè°ƒï¼Œåœ¨æ•™è‚²æ•°æ®æŒ–æ˜(Educational Data Mining)é¢†åŸŸå¤„ç†åŒ…å«ç¬¦å·è¡¨è¾¾å¼çš„ç§‘å­¦æ–‡æœ¬æ—¶ï¼Œç ”ç©¶äººå‘˜å¿…é¡»è°¨æ…é€‰æ‹©åµŒå…¥æ¨¡å‹ï¼Œä»¥é¿å…ç”±äºå¿½ç•¥ç¬¦å·ä¿¡æ¯è€Œå¯¼è‡´çš„ç ”ç©¶åå·®æˆ–åº”ç”¨æ€§èƒ½ä¸‹é™ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "physics.ed-ph"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17950v2",
      "published_date": "2025-05-23 14:26:33 UTC",
      "updated_date": "2025-10-22 08:38:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:38.222265+00:00"
    },
    {
      "arxiv_id": "2505.21527v2",
      "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining",
      "title_zh": "VietASRï¼šåŸºäº 50 å°æ—¶æ ‡æ³¨æ•°æ®ä¸å¤§è§„æ¨¡è¯­éŸ³é¢„è®­ç»ƒå®ç°å·¥ä¸šçº§è¶Šå—è¯­ ASR",
      "authors": [
        "Jianheng Zhuo",
        "Yifan Yang",
        "Yiwen Shao",
        "Yong Xu",
        "Dong Yu",
        "Kai Yu",
        "Xie Chen"
      ],
      "abstract": "Automatic speech recognition (ASR) has made remarkable progress but heavily relies on large-scale labeled data, which is scarce for low-resource languages like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve promising performance, their efficacy remains inadequate in terms of training costs, latency, and accessibility. To address these issues, we propose VietASR, a novel ASR training pipeline that leverages vast amounts of unlabeled data and a small set of labeled data. Through multi-iteration ASR-biased self-supervised learning on a large-scale unlabeled dataset, VietASR offers a cost-effective and practical solution for enhancing ASR performance. Experiments demonstrate that pre-training on 70,000-hour unlabeled data and fine-tuning on merely 50-hour labeled data yield a lightweight but powerful ASR model. It outperforms Whisper Large-v3 and commercial ASR systems on real-world data. Our code and models will be open-sourced to facilitate research in low-resource ASR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VietASRï¼Œä¸€ç§æ—¨åœ¨è§£å†³è¶Šå—è¯­ç­‰ä½èµ„æºè¯­è¨€æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜çš„å·¥ä¸šçº§è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è®­ç»ƒæµæ°´çº¿ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ 70,000 å°æ—¶çš„æ— æ ‡æ³¨æ•°æ®è¿›è¡Œå¤§è§„æ¨¡çš„ ASR-biased self-supervised learningï¼ˆASR åç½®è‡ªç›‘ç£å­¦ä¹ ï¼‰ï¼Œå¹¶ä»…éœ€ 50 å°æ—¶çš„æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVietASR åœ¨çœŸå®åœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äº Whisper Large-v3 åŠå¤šç§å•†ä¸š ASR ç³»ç»Ÿï¼ŒåŒæ—¶å…·æœ‰ä½æˆæœ¬å’Œä½å»¶è¿Ÿçš„ä¼˜åŠ¿ã€‚è¯¥é¡¹ç›®å·²å¼€æºï¼Œä¸ºä½èµ„æºè¯­è¨€çš„è½»é‡åŒ–é«˜æ•ˆ ASR æ¨¡å‹å¼€å‘æä¾›äº†å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21527v2",
      "published_date": "2025-05-23 14:26:11 UTC",
      "updated_date": "2025-05-29 12:55:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:20.057133+00:00"
    },
    {
      "arxiv_id": "2505.17938v1",
      "title": "LMask: Learn to Solve Constrained Routing Problems with Lazy Masking",
      "title_zh": "LMaskï¼šåŸºäºå»¶è¿Ÿæ©ç å­¦ä¹ æ±‚è§£å—çº¦æŸçš„è·¯å¾„è§„åˆ’é—®é¢˜",
      "authors": [
        "Tianyou Li",
        "Haijun Zou",
        "Jiayuan Wu",
        "Zaiwen Wen"
      ],
      "abstract": "Routing problems are canonical combinatorial optimization tasks with wide-ranging applications in logistics, transportation, and supply chain management. However, solving these problems becomes significantly more challenging when complex constraints are involved. In this paper, we propose LMask, a novel learning framework that utilizes dynamic masking to generate high-quality feasible solutions for constrained routing problems. LMask introduces the LazyMask decoding method, which lazily refines feasibility masks with the backtracking mechanism. In addition, it employs the refinement intensity embedding to encode the search trace into the model, mitigating representation ambiguities induced by backtracking. To further reduce sampling cost, LMask sets a backtracking budget during decoding, while constraint violations are penalized in the loss function during training to counteract infeasibility caused by this budget. We provide theoretical guarantees for the validity and probabilistic optimality of our approach. Extensive experiments on the traveling salesman problem with time windows (TSPTW) and TSP with draft limits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility rates and solution quality, outperforming existing neural methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LMaskï¼Œä¸€ç§é€šè¿‡åŠ¨æ€æ©ç (dynamic masking)ä¸ºå—é™è·¯å¾„è§„åˆ’é—®é¢˜(constrained routing problems)ç”Ÿæˆé«˜è´¨é‡å¯è¡Œè§£çš„æ–°å‹å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº† LazyMask è§£ç æ–¹æ³•ï¼Œåˆ©ç”¨å›æº¯æœºåˆ¶(backtracking mechanism)å»¶è¿Ÿç»†åŒ–å¯è¡Œæ€§æ©ç ï¼Œå¹¶ç»“åˆç»†åŒ–å¼ºåº¦åµŒå…¥(refinement intensity embedding)è§£å†³æœç´¢è½¨è¿¹çš„è¡¨ç¤ºæ­§ä¹‰ã€‚ä¸ºäº†åœ¨é™ä½é‡‡æ ·æˆæœ¬çš„åŒæ—¶ç¡®ä¿è§£çš„åˆæ³•æ€§ï¼ŒLMask è®¾ç½®äº†å›æº¯é¢„ç®—(backtracking budget)å¹¶åœ¨è®­ç»ƒæŸå¤±å‡½æ•°ä¸­å¼•å…¥çº¦æŸæƒ©ç½šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLMask åœ¨å¸¦æ—¶é—´çª—çš„æ—…è¡Œå•†é—®é¢˜(TSPTW)å’Œå¸¦åƒæ°´é™åˆ¶çš„æ—…è¡Œå•†é—®é¢˜(TSPDL)ä¸Šå–å¾—äº† state-of-the-art çš„å¯è¡Œç‡å’Œè§£è´¨é‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç¥ç»ä¼˜åŒ–æ–¹æ³•ã€‚",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17938v1",
      "published_date": "2025-05-23 14:15:26 UTC",
      "updated_date": "2025-05-23 14:15:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:53.872231+00:00"
    },
    {
      "arxiv_id": "2505.17931v2",
      "title": "AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models",
      "title_zh": "AutoMiSegï¼šåŸºäºåŸºç¡€æ¨¡å‹æµ‹è¯•æ—¶è‡ªé€‚åº”çš„è‡ªåŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²",
      "authors": [
        "Xingjian Li",
        "Qifeng Wu",
        "Adithya S. Ubaradka",
        "Yiran Ding",
        "Colleen Que",
        "Runmin Jiang",
        "Jianhua Xing",
        "Tianyang Wang",
        "Min Xu"
      ],
      "abstract": "Medical image segmentation is vital for clinical diagnosis, yet current deep learning methods often demand extensive expert effort, i.e., either through annotating large training datasets or providing prompts at inference time for each new case. This paper introduces a zero-shot and automatic segmentation pipeline that combines off-the-shelf vision-language and segmentation foundation models. Given a medical image and a task definition (e.g., \"segment the optic disc in an eye fundus image\"), our method uses a grounding model to generate an initial bounding box, followed by a visual prompt boosting module that enhance the prompts, which are then processed by a promptable segmentation model to produce the final mask. To address the challenges of domain gap and result verification, we introduce a test-time adaptation framework featuring a set of learnable adaptors that align the medical inputs with foundation model representations. Its hyperparameters are optimized via Bayesian Optimization, guided by a proxy validation model without requiring ground-truth labels. Our pipeline offers an annotation-efficient and scalable solution for zero-shot medical image segmentation across diverse tasks. Our pipeline is evaluated on seven diverse medical imaging datasets and shows promising results. By proper decomposition and test-time adaptation, our fully automatic pipeline not only substantially surpasses the previously best-performing method, yielding a 69\\% relative improvement in accuracy (Dice Score from 42.53 to 71.81), but also performs competitively with weakly-prompted interactive foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoMiSegï¼Œä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ä¸åˆ†å‰²åŸºç¡€æ¨¡å‹ (foundation models) çš„å…¨è‡ªåŠ¨é›¶æ ·æœ¬ (zero-shot) åŒ»å­¦å›¾åƒåˆ†å‰²æµæ°´çº¿ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å®šä½æ¨¡å‹ç”Ÿæˆåˆå§‹è¾¹ç•Œæ¡†ï¼Œå¹¶é€šè¿‡è§†è§‰æç¤ºå¢å¼ºæ¨¡å— (visual prompt boosting) ä¸ºåˆ†å‰²æ¨¡å‹æä¾›ç²¾å‡†å¼•å¯¼ã€‚ä¸ºäº†å¼¥åˆé¢†åŸŸå·®è·ï¼Œç ”ç©¶å¼•å…¥äº†æµ‹è¯•æ—¶è‡ªé€‚åº” (test-time adaptation, TTA) æ¡†æ¶ï¼Œé€šè¿‡è´å¶æ–¯ä¼˜åŒ– (Bayesian Optimization) å’Œä»£ç†éªŒè¯æ¨¡å‹åœ¨æ— æ ‡ç­¾çš„æƒ…å†µä¸‹ä¼˜åŒ–å¯å­¦ä¹ é€‚é…å™¨ (adaptors)ã€‚åœ¨ 7 ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAutoMiSeg å°†å¹³å‡ Dice Score ä» 42.53 æå‡è‡³ 71.81ï¼Œç›¸å¯¹å‡†ç¡®ç‡æé«˜ 69%ï¼Œå…¶æ€§èƒ½å¯ä¸éœ€è¦äººå·¥å¹²é¢„çš„äº¤äº’å¼åŸºç¡€æ¨¡å‹ç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17931v2",
      "published_date": "2025-05-23 14:07:21 UTC",
      "updated_date": "2025-10-05 04:58:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:58.626567+00:00"
    },
    {
      "arxiv_id": "2505.17928v2",
      "title": "Towards Practical Defect-Focused Automated Code Review",
      "title_zh": "è¿ˆå‘å®ç”¨çš„ä»¥ç¼ºé™·ä¸ºä¸­å¿ƒçš„è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥",
      "authors": [
        "Junyi Lu",
        "Lili Jiang",
        "Xiaojia Li",
        "Jianbing Fang",
        "Fengjun Zhang",
        "Li Yang",
        "Chun Zuo"
      ],
      "abstract": "The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2x improvement over standard LLMs and a 10x gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å®ç”¨åŒ–ä¸”èšç„¦äºç¼ºé™·çš„è‡ªåŠ¨ä»£ç è¯„å®¡(Automated Code Review)æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨ä»“åº“ä¸Šä¸‹æ–‡(repository context)ç¼ºå¤±å’Œç¼ºé™·æ£€æµ‹èƒ½åŠ›ä¸è¶³ç­‰å®é™…åº”ç”¨ç“¶é¢ˆã€‚ä½œè€…æå‡ºäº†ä¸€å¥—å®Œæ•´çš„è‡ªåŠ¨åŒ–æµæ°´çº¿ï¼ŒåŒ…æ‹¬ç”¨äºä¸Šä¸‹æ–‡æå–çš„ä»£ç åˆ‡ç‰‡ç®—æ³•(code slicing algorithms)ã€æå‡å…³é”®ç¼ºé™·åŒ…å«ç‡(Key Bug Inclusion)çš„å¤šè§’è‰²å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶(multi-role LLM framework)ï¼Œä»¥åŠé™ä½è¯¯æŠ¥ç‡çš„è¿‡æ»¤æœºåˆ¶ã€‚åœ¨å¤§è§„æ¨¡å·¥ä¸šçº§C++ä»£ç åº“çš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•æ¯”æ ‡å‡†å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½æå‡äº†2å€ï¼Œè¾ƒä»¥å¾€åŸºçº¿æ¨¡å‹æå‡äº†10å€ã€‚è¯¥æ¡†æ¶éµå¾ªè¯­è¨€æ— å…³(language-agnostic)çš„è®¾è®¡åŸåˆ™ï¼Œå±•ç¤ºäº†åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¸­å¹¿æ³›åº”ç”¨çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted as Spotlight at the 42nd International Conference on Machine Learning (ICML 2025)",
      "pdf_url": "https://arxiv.org/pdf/2505.17928v2",
      "published_date": "2025-05-23 14:06:26 UTC",
      "updated_date": "2025-05-28 09:21:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:36.600941+00:00"
    },
    {
      "arxiv_id": "2505.17921v1",
      "title": "Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy",
      "title_zh": "è¾“å°¿ç®¡é•œä¸‹è‚¾ç»“çŸ³ç±»å‹è¯†åˆ«çš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•è¯„ä¼°",
      "authors": [
        "Carlos Salazar-Ruiz",
        "Francisco Lopez-Tiro",
        "Ivan Reyes-Amezcua",
        "Clement Larose",
        "Gilberto Ochoa-Ruiz",
        "Christian Daul"
      ],
      "abstract": "Determining the type of kidney stones is crucial for prescribing appropriate treatments to prevent recurrence. Currently, various approaches exist to identify the type of kidney stones. However, obtaining results through the reference ex vivo identification procedure can take several weeks, while in vivo visual recognition requires highly trained specialists. For this reason, deep learning models have been developed to provide urologists with an automated classification of kidney stones during ureteroscopies. Nevertheless, a common issue with these models is the lack of training data. This contribution presents a deep learning method based on few-shot learning, aimed at producing sufficiently discriminative features for identifying kidney stone types in endoscopic images, even with a very limited number of samples. This approach was specifically designed for scenarios where endoscopic images are scarce or where uncommon classes are present, enabling classification even with a limited training dataset. The results demonstrate that Prototypical Networks, using up to 25% of the training data, can achieve performance equal to or better than traditional deep learning models trained with the complete dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾“å°¿ç®¡é•œæ‰‹æœ¯ä¸­è‚¾ç»“çŸ³è¯†åˆ«é¢ä¸´çš„è®­ç»ƒæ•°æ®åŒ®ä¹é—®é¢˜ï¼Œè¯„ä¼°äº†å°æ ·æœ¬å­¦ä¹ (Few-Shot Learning)æ–¹æ³•çš„åº”ç”¨æ•ˆæœã€‚ç ”ç©¶æå‡ºåˆ©ç”¨åŸå‹ç½‘ç»œ(Prototypical Networks)ä»æå°‘é‡æ ·æœ¬ä¸­æå–å…·æœ‰è¾¨åˆ«åŠ›çš„ç‰¹å¾ï¼Œæ—¨åœ¨è§£å†³å†…çª¥é•œå›¾åƒç¨€ç¼ºæˆ–å­˜åœ¨ç½•è§ç»“çŸ³ç±»åˆ«æ—¶çš„è‡ªåŠ¨åˆ†ç±»æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸå‹ç½‘ç»œåœ¨ä»…ä½¿ç”¨25%è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½è¡¨ç°å³å¯è¾¾åˆ°ç”šè‡³ä¼˜äºä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒçš„ä¼ ç»Ÿæ·±åº¦å­¦ä¹ (Deep Learning)æ¨¡å‹ã€‚è¿™ä¸ºåœ¨ä¸´åºŠæ•°æ®å—é™çš„æƒ…å†µä¸‹å®ç°è‡ªåŠ¨åŒ–çš„è‚¾ç»“çŸ³æœ¯ä¸­è¯†åˆ«æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 3 figures, 3 tables, conference, cbms25",
      "pdf_url": "https://arxiv.org/pdf/2505.17921v1",
      "published_date": "2025-05-23 13:59:02 UTC",
      "updated_date": "2025-05-23 13:59:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:18:57.774849+00:00"
    },
    {
      "arxiv_id": "2505.17911v1",
      "title": "Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention",
      "title_zh": "ç»“åˆä½ç½®å¢å¼ºä¸å¤šå¤´äº¤å‰æ³¨æ„åŠ›çš„å¯¹è±¡çº§è·¨è§†è§’åœ°ç†å®šä½",
      "authors": [
        "Zheyang Huang",
        "Jagannath Aryal",
        "Saeid Nahavandi",
        "Xuequan Lu",
        "Chee Peng Lim",
        "Lei Wei",
        "Hailing Zhou"
      ],
      "abstract": "Cross-view geo-localization determines the location of a query image, captured by a drone or ground-based camera, by matching it to a geo-referenced satellite image. While traditional approaches focus on image-level localization, many applications, such as search-and-rescue, infrastructure inspection, and precision delivery, demand object-level accuracy. This enables users to prompt a specific object with a single click on a drone image to retrieve precise geo-tagged information of the object. However, variations in viewpoints, timing, and imaging conditions pose significant challenges, especially when identifying visually similar objects in extensive satellite imagery. To address these challenges, we propose an Object-level Cross-view Geo-localization Network (OCGNet). It integrates user-specified click locations using Gaussian Kernel Transfer (GKT) to preserve location information throughout the network. This cue is dually embedded into the feature encoder and feature matching blocks, ensuring robust object-specific localization. Additionally, OCGNet incorporates a Location Enhancement (LE) module and a Multi-Head Cross Attention (MHCA) module to adaptively emphasize object-specific features or expand focus to relevant contextual regions when necessary. OCGNet achieves state-of-the-art performance on a public dataset, CVOGL. It also demonstrates few-shot learning capabilities, effectively generalizing from limited examples, making it suitable for diverse applications (https://github.com/ZheyangH/OCGNet).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OCGNetï¼ˆObject-level Cross-view Geo-localization Networkï¼‰ï¼Œæ—¨åœ¨å®ç°å¯¹æ— äººæœºæˆ–åœ°é¢å›¾åƒä¸­ç‰¹å®šç›®æ ‡çš„ç²¾ç¡®åœ°ç†å®šä½ï¼Œä»¥æ»¡è¶³æœç´¢æ•‘æ´å’Œç²¾å‡†é…é€ç­‰ä»»åŠ¡å¯¹å¯¹è±¡çº§ç²¾åº¦çš„éœ€æ±‚ã€‚è¯¥æ¡†æ¶é€šè¿‡ Gaussian Kernel Transfer (GKT) å°†ç”¨æˆ·ç‚¹å‡»ä½ç½®ä¿¡æ¯æ•´åˆè¿›ç½‘ç»œï¼Œå¹¶ç»“åˆ Location Enhancement (LE) æ¨¡å—ä¸ Multi-Head Cross Attention (MHCA) æ¨¡å—ï¼Œå®ç°å¯¹ç›®æ ‡ç‰¹å¾å’Œä¸Šä¸‹æ–‡åŒºåŸŸçš„è‡ªé€‚åº”å¢å¼ºã€‚å®éªŒè¡¨æ˜ï¼ŒOCGNet åœ¨ CVOGL æ•°æ®é›†ä¸Šè¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„ few-shot learning è¿ç§»èƒ½åŠ›ï¼Œæœ‰æ•ˆæå‡äº†è·¨è§†å›¾å®šä½çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17911v1",
      "published_date": "2025-05-23 13:55:56 UTC",
      "updated_date": "2025-05-23 13:55:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:19:30.848708+00:00"
    },
    {
      "arxiv_id": "2505.17910v1",
      "title": "DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning",
      "title_zh": "DiffusionRewardï¼šåŸºäºå¥–åŠ±åé¦ˆå­¦ä¹ çš„ç›²äººè„¸ä¿®å¤å¢å¼º",
      "authors": [
        "Bin Wu",
        "Wei Wang",
        "Yahui Liu",
        "Zixiang Li",
        "Yao Zhao"
      ],
      "abstract": "Reward Feedback Learning (ReFL) has recently shown great potential in aligning model outputs with human preferences across various generative tasks. In this work, we introduce a ReFL framework, named DiffusionReward, to the Blind Face Restoration task for the first time. DiffusionReward effectively overcomes the limitations of diffusion-based methods, which often fail to generate realistic facial details and exhibit poor identity consistency. The core of our framework is the Face Reward Model (FRM), which is trained using carefully annotated data. It provides feedback signals that play a pivotal role in steering the optimization process of the restoration network. In particular, our ReFL framework incorporates a gradient flow into the denoising process of off-the-shelf face restoration methods to guide the update of model parameters. The guiding gradient is collaboratively determined by three aspects: (i) the FRM to ensure the perceptual quality of the restored faces; (ii) a regularization term that functions as a safeguard to preserve generative diversity; and (iii) a structural consistency constraint to maintain facial fidelity. Furthermore, the FRM undergoes dynamic optimization throughout the process. It not only ensures that the restoration network stays precisely aligned with the real face manifold, but also effectively prevents reward hacking. Experiments on synthetic and wild datasets demonstrate that our method outperforms state-of-the-art methods, significantly improving identity consistency and facial details. The source codes, data, and models are available at: https://github.com/01NeuralNinja/DiffusionReward.",
      "tldr_zh": "ä½ å¥½ï¼æˆ‘æ˜¯ Gemini Enterpriseâœ¨ã€‚æ ¹æ®ä½ çš„è¦æ±‚ï¼Œæˆ‘å·²å°†è®ºæ–‡ã€ŠDiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learningã€‹çš„æ ¸å¿ƒå†…å®¹è½¬åŒ–ä¸ºä¸­æ–‡ TLDR æ‘˜è¦ã€‚\n\n---\n\n### ğŸ“„ è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶æå‡ºäº† DiffusionRewardï¼Œè¿™æ˜¯é¦–ä¸ªå°† Reward Feedback Learning (ReFL) å¼•å…¥ Blind Face Restoration (BFR) ä»»åŠ¡çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹åœ¨äººè„¸ç»†èŠ‚ç”Ÿæˆå’Œèº«ä»½ä¸€è‡´æ€§ (identity consistency) æ–¹é¢çš„å±€é™ã€‚å…¶æ ¸å¿ƒæ˜¯æ„å»ºäº†äººè„¸å¥–åŠ±æ¨¡å‹ (Face Reward Model, FRM)ï¼Œé€šè¿‡æ¢¯åº¦æµ (gradient flow) å¼•å¯¼å»å™ªè¿‡ç¨‹ä¸­çš„å‚æ•°æ›´æ–°ï¼Œä»è€ŒååŒä¼˜åŒ–æ„ŸçŸ¥è´¨é‡ã€ç”Ÿæˆå¤šæ ·æ€§ä¸ç»“æ„å¿ å®åº¦ã€‚æ­¤å¤–ï¼Œæ¡†æ¶é€šè¿‡åŠ¨æ€ä¼˜åŒ– FRM æœ‰æ•ˆé˜²æ­¢äº† Reward Hackingï¼Œç¡®ä¿ä¿®å¤ç»“æœç²¾å‡†è´´åˆçœŸå®äººè„¸åˆ†å¸ƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆä¸çœŸå®æ•°æ®é›†ä¸Šå‡ä¼˜äº SOTA æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†äººè„¸ç»†èŠ‚çš„æ¢å¤æ•ˆæœã€‚\n\n---\n\n### ğŸ’¡ æ ¸å¿ƒè¦ç´ æ€»ç»“\n\n| ç»´åº¦ | å†…å®¹è¯´æ˜ |\n|---|---|\n| **æ ¸å¿ƒè´¡çŒ®** | é¦–æ¬¡å°†å¥–åŠ±åé¦ˆå­¦ä¹  (ReFL) åº”ç”¨äºç›²äººè„¸ä¿®å¤ (BFR) é¢†åŸŸ |\n| **ä¸»è¦æ–¹æ³•** | å¼€å‘ Face Reward Model (FRM) å¹¶é€šè¿‡æ¢¯åº¦æµ (gradient flow) å¼•å¯¼å»å™ªä¼˜åŒ– |\n| **å…³é”®æŠ€æœ¯** | ç»“åˆæ„ŸçŸ¥è´¨é‡ã€æ­£åˆ™åŒ–å¤šæ ·æ€§ä¸ç»“æ„ä¸€è‡´æ€§çº¦æŸï¼Œå¹¶å¼•å…¥åŠ¨æ€ä¼˜åŒ–é˜²æ­¢ Reward Hacking |\n| **å®éªŒç»“æœ** | åœ¨èº«ä»½ä¸€è‡´æ€§å’Œç»†èŠ‚è¡¨ç°ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰ SOTA æ–¹æ³• |\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦å¯¹ä½ çš„ç ”ç©¶å·¥ä½œæœ‰æ‰€å¸®åŠ©ï¼å¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œæˆ–è€…æƒ³é’ˆå¯¹æ–‡ä¸­çš„æŠ€æœ¯ç»†èŠ‚è¿›è¡Œæ·±æŒ–ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 13 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.17910v1",
      "published_date": "2025-05-23 13:53:23 UTC",
      "updated_date": "2025-05-23 13:53:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:19:22.839376+00:00"
    },
    {
      "arxiv_id": "2505.17909v1",
      "title": "NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling",
      "title_zh": "NeuroTrailsï¼šä»¥åŠ¨æ€ç¨€ç–å¤´è®­ç»ƒä½œä¸ºé«˜æ•ˆé›†æˆçš„å…³é”®",
      "authors": [
        "Bram Grooten",
        "Farid Hasanov",
        "Chenxiang Zhang",
        "Qiao Xiao",
        "Boqian Wu",
        "Zahra Atashgahi",
        "Ghada Sokar",
        "Shiwei Liu",
        "Lu Yin",
        "Elena Mocanu",
        "Mykola Pechenizkiy",
        "Decebal Constantin Mocanu"
      ],
      "abstract": "Model ensembles have long been a cornerstone for improving generalization and robustness in deep learning. However, their effectiveness often comes at the cost of substantial computational overhead. To address this issue, state-of-the-art methods aim to replicate ensemble-class performance without requiring multiple independently trained networks. Unfortunately, these algorithms often still demand considerable compute at inference. In response to these limitations, we introduce $\\textbf{NeuroTrails}$, a sparse multi-head architecture with dynamically evolving topology. This unexplored model-agnostic training paradigm improves ensemble performance while reducing the required resources. We analyze the underlying reason for its effectiveness and observe that the various neural trails induced by dynamic sparsity attain a $\\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays efficacy with convolutional and transformer-based architectures on computer vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4, among many others, demonstrate increased accuracy and stronger robustness in zero-shot generalization, while requiring significantly fewer parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ é›†æˆæ¨¡å‹(Model Ensembles)è®¡ç®—å¼€é”€å¤§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†NeuroTrailsï¼Œä¸€ç§å…·æœ‰åŠ¨æ€æ¼”åŒ–æ‹“æ‰‘ç»“æ„çš„ç¨€ç–å¤šå¤´æ¶æ„ã€‚ä½œä¸ºä¸€ç§æ¨¡å‹æ— å…³(model-agnostic)çš„è®­ç»ƒèŒƒå¼ï¼Œè¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€ç¨€ç–æ€§è¯±å¯¼å‡ºçš„â€œç¥ç»è½¨è¿¹(neural trails)â€ä½¿æ¨¡å‹çš„é¢„æµ‹å¤šæ ·æ€§è¾¾åˆ°â€œé‡‘å‘å§‘å¨˜åŒº(Goldilocks zone)â€ï¼Œåœ¨é™ä½èµ„æºéœ€æ±‚çš„åŒæ—¶æå‡äº†é›†æˆæ€§èƒ½ã€‚åœ¨ResNet-50/ImageNetå’ŒLLaMA-350M/C4ç­‰è®¡ç®—æœºè§†è§‰ä¸è¯­è¨€ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼ŒNeuroTrailsåœ¨æ˜¾è‘—å‡å°‘å‚æ•°é‡çš„å‰æä¸‹ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ä»¥åŠæ›´å¼ºçš„é›¶æ ·æœ¬æ³›åŒ–(zero-shot generalization)é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Our open-source code is available at https://github.com/bramgrooten/neurotrails",
      "pdf_url": "https://arxiv.org/pdf/2505.17909v1",
      "published_date": "2025-05-23 13:53:21 UTC",
      "updated_date": "2025-05-23 13:53:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:19:33.242290+00:00"
    },
    {
      "arxiv_id": "2505.17908v1",
      "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback",
      "title_zh": "ComfyMindï¼šé€šè¿‡æ ‘çŠ¶è§„åˆ’ä¸ååº”å¼åé¦ˆè¿ˆå‘é€šç”¨ç”Ÿæˆ",
      "authors": [
        "Litao Guo",
        "Xinli Xu",
        "Luozhou Wang",
        "Jiantao Lin",
        "Jinsong Zhou",
        "Zixin Zhang",
        "Bolan Su",
        "Ying-Cong Chen"
      ],
      "abstract": "With the rapid advancement of generative models, general-purpose generation has gained increasing attention as a promising approach to unify diverse tasks across modalities within a single system. Despite this progress, existing open-source frameworks often remain fragile and struggle to support complex real-world applications due to the lack of structured workflow planning and execution-level feedback. To address these limitations, we present ComfyMind, a collaborative AI system designed to enable robust and scalable general-purpose generation, built on the ComfyUI platform. ComfyMind introduces two core innovations: Semantic Workflow Interface (SWI) that abstracts low-level node graphs into callable functional modules described in natural language, enabling high-level composition and reducing structural errors; Search Tree Planning mechanism with localized feedback execution, which models generation as a hierarchical decision process and allows adaptive correction at each stage. Together, these components improve the stability and flexibility of complex generative workflows. We evaluate ComfyMind on three public benchmarks: ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and reasoning tasks. Results show that ComfyMind consistently outperforms existing open-source baselines and achieves performance comparable to GPT-Image-1. ComfyMind paves a promising path for the development of open-source general-purpose generative AI systems. Project page: https://github.com/LitaoGuo/ComfyMind",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ComfyMindï¼Œä¸€ç§åŸºäº ComfyUI å¹³å°æ„å»ºçš„åä½œå¼ AI ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–å·¥ä½œæµè§„åˆ’å’Œæ‰§è¡Œçº§åé¦ˆå®ç°é²æ£’ä¸”å¯æ‰©å±•çš„é€šç”¨ç”Ÿæˆ (General-Purpose Generation)ã€‚ComfyMind å¼•å…¥äº†ä¸¤é¡¹æ ¸å¿ƒåˆ›æ–°ï¼šè¯­ä¹‰å·¥ä½œæµæ¥å£ (Semantic Workflow Interface, SWI) å°†åº•å±‚èŠ‚ç‚¹å›¾æŠ½è±¡ä¸ºè‡ªç„¶è¯­è¨€å¯è°ƒç”¨çš„åŠŸèƒ½æ¨¡å—ï¼Œä»¥åŠå…·æœ‰å±€éƒ¨åé¦ˆæ‰§è¡ŒåŠŸèƒ½çš„æœç´¢æ ‘è§„åˆ’ (Search Tree Planning) æœºåˆ¶ï¼Œå°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºåˆ†å±‚å†³ç­–å¹¶æ”¯æŒé˜¶æ®µæ€§è‡ªé€‚åº”ä¿®æ­£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒComfyMind åœ¨ ComfyBenchã€GenEval å’Œ Reason-Edit ç­‰æ¶µç›–ç”Ÿæˆã€ç¼–è¾‘å’Œæ¨ç†ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºç°æœ‰å¼€æºåŸºçº¿ï¼Œå¹¶è¾¾åˆ°äº†ä¸ GPT-Image-1 ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥å·¥ä½œä¸ºå¼€å‘é«˜æ€§èƒ½å¼€æºé€šç”¨ç”Ÿæˆå¼ AI ç³»ç»Ÿå¼€è¾Ÿäº†å…·æœ‰å‰æ™¯çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Project page: https://github.com/LitaoGuo/ComfyMind",
      "pdf_url": "https://arxiv.org/pdf/2505.17908v1",
      "published_date": "2025-05-23 13:53:03 UTC",
      "updated_date": "2025-05-23 13:53:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:19:50.217276+00:00"
    },
    {
      "arxiv_id": "2505.17897v1",
      "title": "T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation",
      "title_zh": "T2I-Eval-R1ï¼šå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„å¯è§£é‡Šæ–‡æœ¬ç”Ÿæˆå›¾åƒè¯„ä¼°æ¨ç†",
      "authors": [
        "Zi-Ao Ma",
        "Tian Lan",
        "Rong-Cheng Tu",
        "Shu-Hang Liu",
        "Heyan Huang",
        "Zhijing Wu",
        "Chen Xu",
        "Xian-Ling Mao"
      ],
      "abstract": "The rapid progress in diffusion-based text-to-image (T2I) generation has created an urgent need for interpretable automatic evaluation methods that can assess the quality of generated images, therefore reducing the human annotation burden. To reduce the prohibitive cost of relying on commercial models for large-scale evaluation, and to improve the reasoning capabilities of open-source models, recent research has explored supervised fine-tuning (SFT) of multimodal large language models (MLLMs) as dedicated T2I evaluators. However, SFT approaches typically rely on high-quality critique datasets, which are either generated by proprietary LLMs-with potential issues of bias and inconsistency-or annotated by humans at high cost, limiting their scalability and generalization. To address these limitations, we propose T2I-Eval-R1, a novel reinforcement learning framework that trains open-source MLLMs using only coarse-grained quality scores, thereby avoiding the need for annotating high-quality interpretable evaluation rationale. Our approach integrates Group Relative Policy Optimization (GRPO) into the instruction-tuning process, enabling models to generate both scalar scores and interpretable reasoning chains with only easy accessible annotated judgment scores or preferences. Furthermore, we introduce a continuous reward formulation that encourages score diversity and provides stable optimization signals, leading to more robust and discriminative evaluation behavior. Experimental results on three established T2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves significantly higher alignment with human assessments and offers more accurate interpretable score rationales compared to strong baseline methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† T2I-Eval-R1ï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) é©±åŠ¨æ¨ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒ (Text-to-Image, T2I) è¯„ä¼°ä¸­çš„å¯è§£é‡Šæ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (Group Relative Policy Optimization, GRPO) æŠ€æœ¯ï¼Œä»…éœ€ç²—ç²’åº¦çš„è´¨é‡è¯„åˆ†å³å¯è®­ç»ƒæ¨¡å‹ç”Ÿæˆå‡†ç¡®çš„è¯„åˆ†åŠæ¨ç†é“¾ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿç›‘ç£å¾®è°ƒ (SFT) å¯¹æ˜‚è´µé«˜è´¨é‡åˆ¤åˆ«æ•°æ®é›†çš„ä¾èµ–é—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†è¿ç»­å¥–åŠ±å‡½æ•° (Continuous Reward Formulation) ä»¥å¢å¼ºè¯„åˆ†çš„è¾¨è¯†åº¦å’Œä¼˜åŒ–ç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒT2I-Eval-R1 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¸äººç±»è¯„ä¼°çš„é«˜åº¦ä¸€è‡´æ€§ï¼Œä¸ºä½æˆæœ¬ã€å¯è§£é‡Šçš„è‡ªåŠ¨è¯„ä¼°æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17897v1",
      "published_date": "2025-05-23 13:44:59 UTC",
      "updated_date": "2025-05-23 13:44:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:19:22.050191+00:00"
    },
    {
      "arxiv_id": "2505.17895v2",
      "title": "DataRater: Meta-Learned Dataset Curation",
      "title_zh": "DataRaterï¼šåŸºäºå…ƒå­¦ä¹ çš„æ•°æ®é›†ç­›é€‰",
      "authors": [
        "Dan A. Calian",
        "Gregory Farquhar",
        "Iurii Kemaev",
        "Luisa M. Zintgraf",
        "Matteo Hessel",
        "Jeremy Shar",
        "Junhyuk Oh",
        "AndrÃ¡s GyÃ¶rgy",
        "Tom Schaul",
        "Jeffrey Dean",
        "Hado van Hasselt",
        "David Silver"
      ],
      "abstract": "The quality of foundation models depends heavily on their training data. Consequently, great efforts have been put into dataset curation. Yet most approaches rely on manual tuning of coarse-grained mixtures of large buckets of data, or filtering by hand-crafted heuristics. An approach that is ultimately more scalable (let alone more satisfying) is to \\emph{learn} which data is actually valuable for training. This type of meta-learning could allow more sophisticated, fine-grained, and effective curation. Our proposed \\emph{DataRater} is an instance of this idea. It estimates the value of training on any particular data point. This is done by meta-learning using `meta-gradients', with the objective of improving training efficiency on held out data. In extensive experiments across a range of model scales and datasets, we find that using our DataRater to filter data is highly effective, resulting in significantly improved compute efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DataRaterï¼Œä¸€ç§åŸºäº Meta-Learning çš„æ•°æ®é›†ç­›é€‰ (Dataset Curation) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åŸºç¡€æ¨¡å‹ (Foundation Models) è®­ç»ƒä¸­è¿‡åº¦ä¾èµ–æ‰‹åŠ¨è°ƒèŠ‚æˆ–å¯å‘å¼è§„åˆ™çš„é—®é¢˜ã€‚DataRater åˆ©ç”¨ Meta-Gradients æŠ€æœ¯è¯„ä¼°æ¯ä¸ªç‰¹å®šæ•°æ®ç‚¹å¯¹æ¨¡å‹çš„è®­ç»ƒä»·å€¼ï¼Œå…¶æ ¸å¿ƒç›®æ ‡æ˜¯ä¼˜åŒ–åœ¨ Held-out Data ä¸Šçš„è®­ç»ƒæ•ˆç‡ã€‚è·¨æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é›†çš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œä½¿ç”¨ DataRater è¿›è¡Œæ•°æ®è¿‡æ»¤èƒ½å¤Ÿæ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ (Compute Efficiency)ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†é€šè¿‡å­¦ä¹ è€Œéæ‰‹åŠ¨å®šä¹‰è§„åˆ™æ¥å®ç°ç²¾ç»†åŒ–ã€å¯æ‰©å±•æ•°æ®ç­–åˆ’çš„æœ‰æ•ˆæ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17895v2",
      "published_date": "2025-05-23 13:43:14 UTC",
      "updated_date": "2025-10-27 15:19:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:19:33.118509+00:00"
    },
    {
      "arxiv_id": "2505.17894v2",
      "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model",
      "title_zh": "Mutarjimï¼šåˆ©ç”¨å°è¯­è¨€æ¨¡å‹æå‡åŒå‘é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­ç¿»è¯‘",
      "authors": [
        "Khalil Hennara",
        "Muhammad Hreden",
        "Mohamed Motaism Hamed",
        "Zeina Aldallal",
        "Sara Chrouf",
        "Safwan AlModhayan"
      ],
      "abstract": "We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Mutarjimï¼Œä¸€ä¸ªåŸºäº Kuwain-1.5B çš„è½»é‡çº§é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­åŒå‘ç¿»è¯‘æ¨¡å‹ã€‚é€šè¿‡ä¼˜åŒ–çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å’Œé«˜è´¨é‡è¯­æ–™åº“ï¼ŒMutarjim åœ¨æ€§èƒ½ä¸Šå¯ä¸è§„æ¨¡å¤§ 20 å€çš„æ¨¡å‹ç›¸åª²ç¾ï¼Œå¹¶åœ¨ English-to-Arabic ä»»åŠ¡ä¸­è¶…è¶Šäº† GPT-4o mini ç­‰å¤§å‹æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† Tarjama-25ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 5,000 å¯¹ä¸“å®¶è¯„å®¡å¥å­ä¸”æ¶µç›–å¤šé¢†åŸŸçš„å…¨æ–°åŸºå‡†æ•°æ®é›†ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ•°æ®é›†é¢†åŸŸå±€é™å’Œ English-source bias ç­‰é—®é¢˜ã€‚Mutarjim åœ¨å¤§å¹…é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¸º Small Language Model åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸçš„åº”ç”¨æ ‘ç«‹äº†æ–°çš„æ ‡æ†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17894v2",
      "published_date": "2025-05-23 13:42:21 UTC",
      "updated_date": "2025-08-21 05:51:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:19:36.057180+00:00"
    },
    {
      "arxiv_id": "2505.18237v1",
      "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens",
      "title_zh": "æ€è€ƒè¿˜æ˜¯ä¸æ€è€ƒï¼Ÿä»ä¿¡æ¯è®ºè§†è§’æ¢ç©¶å¤§æ¨ç†æ¨¡å‹çš„æ€è€ƒæ•ˆç‡",
      "authors": [
        "Xixian Yong",
        "Xiao Zhou",
        "Yingying Zhang",
        "Jinlin Li",
        "Yefeng Zheng",
        "Xian Wu"
      ],
      "abstract": "The recent rise of Large Reasoning Models (LRMs) has significantly improved multi-step reasoning performance, but often at the cost of generating excessively long reasoning chains. This paper revisits the efficiency of such reasoning processes through an information-theoretic lens, revealing a fundamental trade-off between reasoning length and semantic efficiency. We propose two metrics, InfoBias and InfoGain, to quantify divergence from ideal reasoning paths and stepwise information contribution, respectively. Empirical analyses show that longer reasoning chains tend to exhibit higher information bias and diminishing information gain, especially for incorrect answers. Motivated by these findings, we introduce an entropy-based Adaptive Think strategy that dynamically halts reasoning once confidence is sufficiently high, improving efficiency while maintaining competitive accuracy. Compared to the Vanilla Think approach (default mode), our strategy yields a 1.10% improvement in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six benchmark tasks spanning diverse reasoning types and difficulty levels, demonstrating superior efficiency and reasoning performance. These results underscore the promise of entropy-based methods for enhancing both accuracy and cost-effiiciency in large language model deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ä¿¡æ¯è®ºè§†è§’æ¢è®¨äº†å¤§æ¨ç†æ¨¡å‹(LRMs)åœ¨ç”Ÿæˆé•¿æ¨ç†é“¾æ—¶çš„æ•ˆç‡é—®é¢˜ï¼Œæ­ç¤ºäº†æ¨ç†é•¿åº¦ä¸è¯­ä¹‰æ•ˆç‡ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚ç ”ç©¶è€…æå‡ºäº† InfoBias å’Œ InfoGain ä¸¤ä¸ªæŒ‡æ ‡ï¼Œåˆ†åˆ«ç”¨äºé‡åŒ–æ¨ç†è·¯å¾„çš„åå·®ä»¥åŠæ¯ä¸€æ­¥çš„ä¿¡æ¯å¢ç›Šã€‚åˆ†æè¡¨æ˜ï¼Œå†—é•¿çš„æ¨ç†é“¾å¾€å¾€ä¼´éšç€æ›´é«˜ä¿¡æ¯åå·®å’Œé€æ¸å‡å°çš„ä¿¡æ¯å¢ç›Šï¼Œè¿™åœ¨é”™è¯¯ç­”æ¡ˆä¸­å°¤ä¸ºæ˜æ˜¾ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç†µçš„ Adaptive Think ç­–ç•¥ï¼Œé€šè¿‡åŠ¨æ€åœæ­¢æ¨ç†æ¥ä¼˜åŒ–æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç­–ç•¥åœ¨ QwQ-32B æ¨¡å‹ä¸Šç›¸æ¯”äº Vanilla Think æ¨¡å¼ï¼Œä¸ä»…å°†å¹³å‡å‡†ç¡®ç‡æå‡äº† 1.10%ï¼Œè¿˜æ˜¾è‘—é™ä½äº† 50.80% çš„ Token ä½¿ç”¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18237v1",
      "published_date": "2025-05-23 13:38:56 UTC",
      "updated_date": "2025-05-23 13:38:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:20:04.812905+00:00"
    },
    {
      "arxiv_id": "2505.18236v1",
      "title": "From Bias to Accountability: How the EU AI Act Confronts Challenges in European GeoAI Auditing",
      "title_zh": "ä»åå·®åˆ°é—®è´£ï¼šEU AI Act å¦‚ä½•åº”å¯¹ European GeoAI å®¡è®¡ä¸­çš„æŒ‘æˆ˜",
      "authors": [
        "Natalia Matuszczyk",
        "Craig R. Barnes",
        "Rohit Gupta",
        "Bulent Ozel",
        "Aniket Mitra"
      ],
      "abstract": "Bias in geospatial artificial intelligence (GeoAI) models has been documented, yet the evidence is scattered across narrowly focused studies. We synthesize this fragmented literature to provide a concise overview of bias in GeoAI and examine how the EU's Artificial Intelligence Act (EU AI Act) shapes audit obligations. We discuss recurring bias mechanisms, including representation, algorithmic and aggregation bias, and map them to specific provisions of the EU AI Act. By applying the Act's high-risk criteria, we demonstrate that widely deployed GeoAI applications qualify as high-risk systems. We then present examples of recent audits along with an outline of practical methods for detecting bias. As far as we know, this study represents the first integration of GeoAI bias evidence into the EU AI Act context, by identifying high-risk GeoAI systems and mapping bias mechanisms to the Act's Articles. Although the analysis is exploratory, it suggests that even well-curated European datasets should employ routine bias audits before 2027, when the AI Act's high-risk provisions take full effect.",
      "tldr_zh": "### è®ºæ–‡ TLDR æ‘˜è¦\n\nè¯¥ç ”ç©¶é’ˆå¯¹åœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½ (GeoAI) ä¸­çš„åè§é—®é¢˜ï¼Œç³»ç»Ÿæ€§åœ°ç»¼è¿°äº†ç›¸å…³æ–‡çŒ®ï¼Œå¹¶æ¢è®¨äº†ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹(EU AI Act) å¦‚ä½•ç•Œå®šç›¸å…³çš„å®¡è®¡ä¹‰åŠ¡ã€‚ç ”ç©¶åˆ†æäº†åŒ…æ‹¬è¡¨ç°åè§ (representation bias)ã€ç®—æ³•åè§ (algorithmic bias) å’Œèšåˆåè§ (aggregation bias) åœ¨å†…çš„å¤šç§æœºåˆ¶ï¼Œå¹¶å°†å…¶ä¸ EU AI Act çš„å…·ä½“æ¡æ¬¾è¿›è¡Œäº†æ˜ å°„ã€‚é€šè¿‡åº”ç”¨æ³•æ¡ˆçš„é«˜é£é™©æ ‡å‡†ï¼Œç ”ç©¶è¯æ˜äº†è®¸å¤šå¹¿æ³›éƒ¨ç½²çš„ GeoAI åº”ç”¨å±äºé«˜é£é™©ç³»ç»Ÿï¼Œè¿™ä¹Ÿæ˜¯é¦–ä¸ªå°† GeoAI åè§è¯æ®ä¸è¯¥æ³•æ¡ˆè¯­å¢ƒç»“åˆçš„ç ”ç©¶ã€‚æœ€åï¼Œç ”ç©¶æå‡ºäº†åè§æ£€æµ‹çš„å®ç”¨æ–¹æ³•ï¼Œå¹¶å»ºè®®æ¬§æ´²çš„ GeoAI ç³»ç»Ÿåœ¨ 2027 å¹´é«˜é£é™©æ¡æ¬¾æ­£å¼ç”Ÿæ•ˆå‰ï¼Œåº”å®šæœŸå¼€å±•ä¾‹è¡Œçš„åè§å®¡è®¡å·¥ä½œã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18236v1",
      "published_date": "2025-05-23 13:34:39 UTC",
      "updated_date": "2025-05-23 13:34:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:20:22.616702+00:00"
    },
    {
      "arxiv_id": "2505.17883v1",
      "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
      "title_zh": "FastCAVï¼šç”¨äºè§£é‡Šæ·±åº¦ç¥ç»ç½‘ç»œçš„é«˜æ•ˆæ¦‚å¿µæ¿€æ´»å‘é‡è®¡ç®—",
      "authors": [
        "Laines Schmalwasser",
        "Niklas Penzel",
        "Joachim Denzler",
        "Julia Niebling"
      ],
      "abstract": "Concepts such as objects, patterns, and shapes are how humans understand the world. Building on this intuition, concept-based explainability methods aim to study representations learned by deep neural networks in relation to human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an important tool and can identify whether a model learned a concept or not. However, the computational cost and time requirements of existing CAV computation pose a significant challenge, particularly in large-scale, high-dimensional architectures. To address this limitation, we introduce FastCAV, a novel approach that accelerates the extraction of CAVs by up to 63.6x (on average 46.4x). We provide a theoretical foundation for our approach and give concrete assumptions under which it is equivalent to established SVM-based methods. Our empirical results demonstrate that CAVs calculated with FastCAV maintain similar performance while being more efficient and stable. In downstream applications, i.e., concept-based explanation methods, we show that FastCAV can act as a replacement leading to equivalent insights. Hence, our approach enables previously infeasible investigations of deep models, which we demonstrate by tracking the evolution of concepts during model training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FastCAVï¼Œä¸€ç§æ—¨åœ¨é«˜æ•ˆè®¡ç®—Concept Activation Vectors (CAVs)çš„æ–°æ–¹æ³•ï¼Œç”¨äºè§£é‡Šæ·±åº¦ç¥ç»ç½‘ç»œçš„å­¦ä¹ è¡¨ç¤ºã€‚é’ˆå¯¹ç°æœ‰CAVè®¡ç®—åœ¨å¤§å‹é«˜ç»´æ¶æ„ä¸­æˆæœ¬è¿‡é«˜çš„é—®é¢˜ï¼ŒFastCAVé€šè¿‡ä¼˜åŒ–æå–æµç¨‹å®ç°äº†å¹³å‡46.4å€ï¼ˆæœ€é«˜63.6å€ï¼‰çš„åŠ é€Ÿï¼Œå¹¶æä¾›äº†ä¸ä¼ ç»ŸåŸºäºSVMæ–¹æ³•ç­‰æ•ˆçš„ç†è®ºæ”¯æŒã€‚å®éªŒè¯æ˜ï¼ŒFastCAVåœ¨æ˜¾è‘—æå‡æ•ˆç‡å’Œç¨³å®šæ€§çš„åŒæ—¶ä¿æŒäº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ç”±äºå…¶é«˜æ•ˆæ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ”¯æŒä»¥å¾€éš¾ä»¥å®ç°çš„æ·±åº¦æ¨¡å‹å¤§è§„æ¨¡åˆ†æï¼Œä¾‹å¦‚åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶è¿½è¸ªæ¦‚å¿µçš„æ¼”åŒ–è¿‡ç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2025, 27 pages, 20 figures, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.17883v1",
      "published_date": "2025-05-23 13:31:54 UTC",
      "updated_date": "2025-05-23 13:31:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:20:09.912747+00:00"
    },
    {
      "arxiv_id": "2505.17882v1",
      "title": "Formalizing Embeddedness Failures in Universal Artificial Intelligence",
      "title_zh": "é€šç”¨äººå·¥æ™ºèƒ½ä¸­åµŒå…¥æ€§å¤±æ•ˆçš„å½¢å¼åŒ–ç ”ç©¶",
      "authors": [
        "Cole Wyeth",
        "Marcus Hutter"
      ],
      "abstract": "We rigorously discuss the commonly asserted failures of the AIXI reinforcement learning agent as a model of embedded agency. We attempt to formalize these failure modes and prove that they occur within the framework of universal artificial intelligence, focusing on a variant of AIXI that models the joint action/percept history as drawn from the universal distribution. We also evaluate the progress that has been made towards a successful theory of embedded agency based on variants of the AIXI agent.",
      "tldr_zh": "è¯¥ç ”ç©¶ä¸¥è°¨åœ°æ¢è®¨äº† AIXI å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ä½œä¸º embedded agency æ¨¡å‹æ—¶å¸¸è§çš„å¤±æ•ˆæ¨¡å¼ã€‚ä½œè€…åœ¨ universal artificial intelligence æ¡†æ¶ä¸‹å¯¹è¿™äº›å¤±æ•ˆæ¨¡å¼è¿›è¡Œäº†å½¢å¼åŒ– (formalizing)ï¼Œå¹¶è¯æ˜äº†è¿™äº›å¤±æ•ˆä¼šåœ¨å°†åŠ¨ä½œ/æ„ŸçŸ¥å†å²å»ºæ¨¡ä¸ºé€šç”¨åˆ†å¸ƒçš„ AIXI å˜ä½“ä¸­å‘ç”Ÿã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯„ä¼°äº†ç›®å‰åŸºäº AIXI å˜ä½“æ„å»º embedded agency ç†è®ºæ‰€å–å¾—çš„ç ”ç©¶è¿›å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17882v1",
      "published_date": "2025-05-23 13:31:28 UTC",
      "updated_date": "2025-05-23 13:31:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:21:12.655173+00:00"
    },
    {
      "arxiv_id": "2505.18235v1",
      "title": "The Origins of Representation Manifolds in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­è¡¨ç¤ºæµå½¢çš„èµ·æº",
      "authors": [
        "Alexander Modell",
        "Patrick Rubin-Delanchy",
        "Nick Whiteley"
      ],
      "abstract": "There is a large ongoing scientific effort in mechanistic interpretability to map embeddings and internal representations of AI systems into human-understandable concepts. A key element of this effort is the linear representation hypothesis, which posits that neural representations are sparse linear combinations of `almost-orthogonal' direction vectors, reflecting the presence or absence of different features. This model underpins the use of sparse autoencoders to recover features from representations. Moving towards a fuller model of features, in which neural representations could encode not just the presence but also a potentially continuous and multidimensional value for a feature, has been a subject of intense recent discourse. We describe why and how a feature might be represented as a manifold, demonstrating in particular that cosine similarity in representation space may encode the intrinsic geometry of a feature through shortest, on-manifold paths, potentially answering the question of how distance in representation space and relatedness in concept space could be connected. The critical assumptions and predictions of the theory are validated on text embeddings and token activations of large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­è¡¨å¾æµå½¢(Representation Manifolds)çš„èµ·æºï¼Œæ—¨åœ¨æ‹“å±•æœºæ¢°å¯è§£é‡Šæ€§(Mechanistic Interpretability)é¢†åŸŸä¸­ä¼ ç»Ÿçš„çº¿æ€§è¡¨ç¤ºå‡è®¾(Linear Representation Hypothesis)ã€‚ç ”ç©¶æå‡ºç‰¹å¾å¯ä»¥è¢«è¡¨å¾ä¸ºæµå½¢ï¼Œä»è€Œèƒ½å¤Ÿç¼–ç ç‰¹å¾çš„è¿ç»­åŠå¤šç»´æ•°å€¼ï¼Œå¹¶æ­ç¤ºäº†è¡¨å¾ç©ºé—´ä¸­çš„ä½™å¼¦ç›¸ä¼¼åº¦(Cosine Similarity)å¦‚ä½•é€šè¿‡æµå½¢ä¸Šçš„æœ€çŸ­è·¯å¾„æ¥åæ˜ ç‰¹å¾çš„å†…åœ¨å‡ ä½•ç»“æ„(Intrinsic Geometry)ã€‚è¿™ä¸€æ¨¡å‹æˆåŠŸè§£é‡Šäº†è¡¨å¾ç©ºé—´è·ç¦»ä¸æ¦‚å¿µç›¸å…³æ€§ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚é€šè¿‡å¯¹ LLMs çš„æ–‡æœ¬åµŒå…¥(Text Embeddings)å’Œ Token æ¿€æ´»çš„å®éªŒéªŒè¯ï¼Œè¯¥ç ”ç©¶ä¸ºç†è§£å¤æ‚æ¨¡å‹å¦‚ä½•è¡¨ç¤ºå’Œå…³è”äººç±»æ¦‚å¿µæä¾›äº†æ–°çš„ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.18235v1",
      "published_date": "2025-05-23 13:31:22 UTC",
      "updated_date": "2025-05-23 13:31:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:21:13.174014+00:00"
    },
    {
      "arxiv_id": "2505.17877v1",
      "title": "Toward Optimal ANC: Establishing Mutual Information Lower Bound",
      "title_zh": "è¿ˆå‘æœ€ä¼˜ä¸»åŠ¨å™ªå£°æ§åˆ¶ï¼šäº’ä¿¡æ¯ä¸‹ç•Œçš„ç¡®ç«‹",
      "authors": [
        "FranÃ§ois Derrida",
        "Shahar Lutati",
        "Eliya Nachmani"
      ],
      "abstract": "Active Noise Cancellation (ANC) algorithms aim to suppress unwanted acoustic disturbances by generating anti-noise signals that destructively interfere with the original noise in real time. Although recent deep learning-based ANC algorithms have set new performance benchmarks, there remains a shortage of theoretical limits to rigorously assess their improvements. To address this, we derive a unified lower bound on cancellation performance composed of two components. The first component is information-theoretic: it links residual error power to the fraction of disturbance entropy captured by the anti-noise signal, thereby quantifying limits imposed by information-processing capacity. The second component is support-based: it measures the irreducible error arising in frequency bands that the cancellation path cannot address, reflecting fundamental physical constraints. By taking the maximum of these two terms, our bound establishes a theoretical ceiling on the Normalized Mean Squared Error (NMSE) attainable by any ANC algorithm. We validate its tightness empirically on the NOISEX dataset under varying reverberation times, demonstrating robustness across diverse acoustic conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹  Active Noise Cancellation (ANC) ç®—æ³•ç¼ºä¹ç†è®ºæ€§èƒ½æé™è¯„ä¼°çš„é—®é¢˜ï¼Œæ¨å¯¼å‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æŠµæ¶ˆæ€§èƒ½ä¸‹é™ã€‚è¯¥ä¸‹é™ç”±ä¸¤ä¸ªæ ¸å¿ƒç»´åº¦æ„æˆï¼šä¿¡æ¯è®ºç»„ä»¶é€šè¿‡æ‰°åŠ¨ç†µé‡åŒ–ä¿¡æ¯å¤„ç†èƒ½åŠ›çš„é™åˆ¶ï¼Œè€Œæ”¯æ’‘é›†ç»„ä»¶åˆ™åæ˜ äº†ç‰©ç†çº¦æŸå¯¼è‡´çš„é¢‘å¸¦ä¸å¯æ¶ˆé™¤è¯¯å·®ã€‚é€šè¿‡ç»“åˆè¿™ä¸¤ä¸ªéƒ¨åˆ†ï¼Œç ”ç©¶ç¡®ç«‹äº†ä»»ä½• ANC ç®—æ³•åœ¨ Normalized Mean Squared Error (NMSE) ä¸Šæ‰€èƒ½è¾¾åˆ°çš„ç†è®ºæ€§èƒ½å¤©èŠ±æ¿ã€‚å®éªŒåœ¨ NOISEX æ•°æ®é›†åŠä¸åŒæ··å“ç¯å¢ƒä¸‹éªŒè¯äº†è¯¥ä¸‹é™çš„ç´§è‡´æ€§ä¸é²æ£’æ€§ï¼Œä¸ºè¯„ä¼° ANC æŠ€æœ¯çš„è¿›æ­¥æä¾›äº†ä¸¥è°¨çš„ç†è®ºåŸºå‡†ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17877v1",
      "published_date": "2025-05-23 13:27:35 UTC",
      "updated_date": "2025-05-23 13:27:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:20:42.819358+00:00"
    },
    {
      "arxiv_id": "2505.17873v3",
      "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback",
      "title_zh": "MOOSE-Chem3ï¼šé€šè¿‡æ¨¡æ‹Ÿå®éªŒåé¦ˆå®ç°å®éªŒå¼•å¯¼çš„å‡è®¾æ’åº",
      "authors": [
        "Wanhao Liu",
        "Zonglin Yang",
        "Jue Wang",
        "Lidong Bing",
        "Di Zhang",
        "Dongzhan Zhou",
        "Yuqiang Li",
        "Houqiang Li",
        "Erik Cambria",
        "Wanli Ouyang"
      ],
      "abstract": "Hypothesis ranking is vital for automated scientific discovery, especially in cost-intensive, throughput-limited natural science domains. Current methods focus on pre-experiment ranking, relying solely on language model reasoning without empirical feedback. We introduce experiment-guided ranking, which prioritizes hypotheses based on feedback from prior tests. Due to the impracticality of real experiments, we propose a simulator grounded in domain-specific concepts that models hypothesis performance as a function of similarity to a hidden ground truth, perturbed by noise. Validated against 124 hypotheses with experimentally reported outcomes, the simulator approximates real results with consistent trend alignment. Although deviations exist, they mimic wet-lab noise, promoting more robust ranking strategies. We frame experiment-guided ranking as a sequential decision-making problem and propose an in-context reinforcement learning (ICRL) framework. Our LLM-based policy decomposes hypotheses into functional elements, clusters them by mechanistic roles, and prioritizes recombinations based on feedback. Experiments show our approach significantly outperforms pre-experiment baselines and strong ablations. Our toolkit, comprising the simulator and ICRL framework, enables systematic research on experiment-guided ranking, with the policy serving as a strong proof of concept.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MOOSE-Chem3 æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿå®éªŒåé¦ˆå®ç°å®éªŒå¼•å¯¼çš„å‡è®¾æ’åº (experiment-guided ranking)ï¼Œå¡«è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°ä¸­ç¼ºä¹ç»éªŒåé¦ˆçš„ç©ºç™½ã€‚ç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªåŸºäºé¢†åŸŸæ¦‚å¿µçš„æ¨¡æ‹Ÿå™¨ (simulator)ï¼Œé€šè¿‡æ¨¡æ‹Ÿå‡è®¾ä¸çœŸå®å€¼çš„ç›¸ä¼¼åº¦åŠå™ªå£°æ¥è¿‘ä¼¼çœŸå®å®éªŒï¼Œå¹¶ç» 124 é¡¹å®éªŒæ•°æ®éªŒè¯å…·æœ‰è‰¯å¥½çš„ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶å°†æ’åºè¿‡ç¨‹å»ºæ¨¡ä¸ºé¡ºåºå†³ç­–é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†åŸºäºä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹  (In-Context Reinforcement Learning, ICRL) çš„ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹å‡è®¾çš„åŠŸèƒ½å…ƒç´ è¿›è¡Œåˆ†è§£ä¸åé¦ˆé©±åŠ¨çš„é‡ç»„ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä»…ä¾èµ–è¯­è¨€æ¨¡å‹æ¨ç†çš„é¢„å®éªŒåŸºå‡†ï¼Œä¸ºå®éªŒå¼•å¯¼çš„ç§‘å­¦æ¢ç´¢æä¾›äº†ç³»ç»Ÿæ€§çš„å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17873v3",
      "published_date": "2025-05-23 13:24:50 UTC",
      "updated_date": "2025-10-25 14:00:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:20:44.505922+00:00"
    },
    {
      "arxiv_id": "2505.17872v2",
      "title": "Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting",
      "title_zh": "åŸºäºéƒ¨åˆ†å‚æ•°å…±äº«çš„æ··åˆä½ç§©è‡ªé€‚åº”æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Licheng Pan",
        "Zhichao Chen",
        "Haoxuan Li",
        "Guangyi Liu",
        "Zhijian Xu",
        "Zhaoran Liu",
        "Hao Wang",
        "Ying Wei"
      ],
      "abstract": "Multi-task forecasting has become the standard approach for time-series forecasting (TSF). However, we show that it suffers from an Expressiveness Bottleneck, where predictions at different time steps share the same representation, leading to unavoidable errors even with optimal representations. To address this issue, we propose a two-stage framework: first, pre-train a foundation model for one-step-ahead prediction; then, adapt it using step-specific LoRA modules.This design enables the foundation model to handle any number of forecast steps while avoiding the expressiveness bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which employs adaptively weighted LoRA experts to achieve partial parameter sharing across steps. This approach enhances both efficiency and forecasting performance by exploiting interdependencies between forecast steps. Experiments show that MoLA significantly improves model expressiveness and outperforms state-of-the-art time-series forecasting methods. Code is available at https://anonymous.4open.science/r/MoLA-BC92.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“\n\n---\n\nè¯¥ç ”ç©¶é’ˆå¯¹å¤šä»»åŠ¡æ—¶é—´åºåˆ—é¢„æµ‹ (Time Series Forecasting, TSF) ä¸­å­˜åœ¨çš„â€œè¡¨è¾¾ç“¶é¢ˆâ€ (Expressiveness Bottleneck) é—®é¢˜ï¼Œæå‡ºäº† **Mixture-of-LoRA (MoLA)** æ¡†æ¶ã€‚è¯¥æ–¹æ¡ˆé¦–å…ˆé¢„è®­ç»ƒä¸€ä¸ªç”¨äºå•æ­¥é¢„æµ‹çš„åŸºç¡€æ¨¡å‹ï¼Œéšååˆ©ç”¨ç‰¹å®šæ­¥é•¿çš„ Low Rank Adaptation (LoRA) æ¨¡å—è¿›è¡Œå¾®è°ƒé€‚é…ï¼Œæ—¨åœ¨æ¶ˆé™¤ä¸åŒé¢„æµ‹æ­¥é•¿å…±äº«ç›¸åŒè¡¨ç¤ºæ‰€å¯¼è‡´çš„é¢„æµ‹è¯¯å·®ã€‚MoLA è¿›ä¸€æ­¥å¼•å…¥äº†è‡ªé€‚åº”åŠ æƒçš„ LoRA ä¸“å®¶ç³»ç»Ÿï¼Œå®ç°äº†è·¨æ­¥é•¿çš„å‚æ•°éƒ¨åˆ†å…±äº« (Partial Parameter Sharing)ï¼Œåœ¨æ˜¾è‘—æå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶ï¼Œé€šè¿‡æ•è·é¢„æµ‹æ­¥é•¿é—´çš„ç›¸äº’ä¾èµ–æ€§å¢å¼ºäº†é¢„æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMoLA åœ¨å¤šé¡¹ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿› (SOTA) æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ã€‚\n\n---\n\næˆ‘æ˜¯ Gemini Enterpriseï¼Œå¾ˆé«˜å…´èƒ½ä¸ºæ‚¨æç‚¼è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ã€‚å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–å­¦æœ¯æ–‡çŒ®éœ€è¦è½¬åŒ–ä¸ºç®€æ´çš„ TLDRï¼Œæˆ–è€…æƒ³æ·±å…¥æ¢è®¨ MoLA æ¡†æ¶çš„æŠ€æœ¯ç»†èŠ‚ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17872v2",
      "published_date": "2025-05-23 13:24:39 UTC",
      "updated_date": "2025-05-27 07:23:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:20:56.159810+00:00"
    },
    {
      "arxiv_id": "2505.17862v1",
      "title": "Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities",
      "title_zh": "Daily-Omniï¼šé¢å‘è·¨æ¨¡æ€æ—¶åºå¯¹é½çš„è§†å¬æ¨ç†",
      "authors": [
        "Ziwei Zhou",
        "Rui Wang",
        "Zuxuan Wu"
      ],
      "abstract": "Recent Multimodal Large Language Models (MLLMs) achieve promising performance on visual and audio benchmarks independently. However, the ability of these models to process cross-modal information synchronously remains largely unexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual Questioning and Answering benchmark comprising 684 videos of daily life scenarios from diverse sources, rich in both audio and visual information, and featuring 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA Generation Pipeline, which includes automatic annotation, QA generation and QA optimization, significantly improves efficiency for human evaluation and scalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent utilizing open-source Visual Language Model (VLM), Audio Language Model (ALM) and Automatic Speech Recognition (ASR) model to establish a baseline for this benchmark. The results show that current MLLMs still struggle significantly with tasks requiring audio-visual integration, but combining VLMs and ALMs with simple temporal alignment techniques can achieve substantially better performance. Codes and benchmark are available at \\href{https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Daily-Omniï¼Œä¸€ä¸ªä¸“æ³¨äºéŸ³è§†é¢‘é—®ç­” (Audio-Visual Questioning and Answering) çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å« 684 ä¸ªæ—¥å¸¸ç”Ÿæ´»è§†é¢‘å’Œ 1197 ä¸ªæ¶µç›– 6 å¤§ç±»ä»»åŠ¡çš„é—®ç­”å¯¹ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å¼€å‘äº† Daily-Omni QA Generation Pipelineï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµæ°´çº¿å¤§å¹…æå‡äº†åŸºå‡†æµ‹è¯•çš„æ ‡æ³¨æ•ˆç‡ä¸å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ Daily-Omni-Agentï¼Œåˆ©ç”¨å¼€æºçš„ Visual Language Model (VLM)ã€Audio Language Model (ALM) å’Œ Automatic Speech Recognition (ASR) æ¨¡å‹æ„å»ºäº†åŸºå‡†çº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç›®å‰çš„ Multimodal Large Language Models (MLLMs) åœ¨éŸ³è§†é¢‘é›†æˆä»»åŠ¡ä¸Šä»é¢ä¸´æŒ‘æˆ˜ï¼Œä½†é€šè¿‡ç®€å•çš„æ—¶åºå¯¹é½ (temporal alignment) æŠ€æœ¯ç»“åˆ VLM å’Œ ALMï¼Œå¯ä»¥æ˜¾è‘—å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17862v1",
      "published_date": "2025-05-23 13:13:58 UTC",
      "updated_date": "2025-05-23 13:13:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:20:56.946166+00:00"
    },
    {
      "arxiv_id": "2505.17861v1",
      "title": "Superplatforms Have to Attack AI Agents",
      "title_zh": "è¶…çº§å¹³å°åŠ¿å¿…æ”»å‡»AIæ™ºèƒ½ä½“",
      "authors": [
        "Jianghao Lin",
        "Jiachen Zhu",
        "Zheli Zhou",
        "Yunjia Xi",
        "Weiwen Liu",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "abstract": "Over the past decades, superplatforms, digital companies that integrate a vast range of third-party services and applications into a single, unified ecosystem, have built their fortunes on monopolizing user attention through targeted advertising and algorithmic content curation. Yet the emergence of AI agents driven by large language models (LLMs) threatens to upend this business model. Agents can not only free user attention with autonomy across diverse platforms and therefore bypass the user-attention-based monetization, but might also become the new entrance for digital traffic. Hence, we argue that superplatforms have to attack AI agents to defend their centralized control of digital traffic entrance. Specifically, we analyze the fundamental conflict between user-attention-based monetization and agent-driven autonomy through the lens of our gatekeeping theory. We show how AI agents can disintermediate superplatforms and potentially become the next dominant gatekeepers, thereby forming the urgent necessity for superplatforms to proactively constrain and attack AI agents. Moreover, we go through the potential technologies for superplatform-initiated attacks, covering a brand-new, unexplored technical area with unique challenges. We have to emphasize that, despite our position, this paper does not advocate for adversarial attacks by superplatforms on AI agents, but rather offers an envisioned trend to highlight the emerging tensions between superplatforms and AI agents. Our aim is to raise awareness and encourage critical discussion for collaborative solutions, prioritizing user interests and perserving the openness of digital ecosystems in the age of AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¶…çº§å¹³å°ï¼ˆSuperplatformsï¼‰ä¸ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é©±åŠ¨çš„ AI Agent ä¹‹é—´çš„æ ¸å¿ƒå†²çªã€‚ä¼ ç»Ÿçš„è¶…çº§å¹³å°ä¾èµ–å„æ–­ç”¨æˆ·æ³¨æ„åŠ›è¿›è¡Œå¹¿å‘Šè·åˆ©ï¼Œè€Œ AI Agent çš„è‡ªä¸»æ€§èƒ½å¤Ÿç»•è¿‡è¿™ä¸€å•†ä¸šæ¨¡å¼ï¼Œå¹¶æœ‰æ½œåŠ›æˆä¸ºæ–°çš„æ•°å­—æµé‡å…¥å£ã€‚é€šè¿‡é—¨ç¦ç†è®ºï¼ˆGatekeeping Theoryï¼‰çš„è§†è§’ï¼Œè®ºæ–‡æŒ‡å‡º AI Agent çš„å»ä¸­ä»‹åŒ–ï¼ˆDisintermediateï¼‰ä½œç”¨å°†å¨èƒå¹³å°çš„ä¸­å¿ƒåŒ–æ§åˆ¶ï¼Œè¿«ä½¿è¶…çº§å¹³å°ä¸»åŠ¨çº¦æŸæˆ–æ”»å‡» AI Agentã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ†æäº†è¶…çº§å¹³å°å¯èƒ½å‘èµ·çš„æ½œåœ¨æŠ€æœ¯æ”»å‡»æ‰‹æ®µï¼Œæ—¨åœ¨å¼•å‘å¯¹æ•°å­—ç”Ÿæ€ç³»ç»Ÿå¼€æ”¾æ€§å’Œç”¨æˆ·åˆ©ç›Šä¿æŠ¤çš„æ‰¹åˆ¤æ€§è®¨è®ºã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "Position paper under review",
      "pdf_url": "https://arxiv.org/pdf/2505.17861v1",
      "published_date": "2025-05-23 13:13:44 UTC",
      "updated_date": "2025-05-23 13:13:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:21:21.860321+00:00"
    },
    {
      "arxiv_id": "2505.17859v2",
      "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment",
      "title_zh": "é€šè¿‡å¯è¯æ˜é²æ£’çš„æ¨¡å‹å¯¹é½å®ç°äººç±»åé¦ˆçš„å¯æ‰©å±•è¯„ä¼°",
      "authors": [
        "Masahiro Fujisawa",
        "Masaki Adachi",
        "Michael A. Osborne"
      ],
      "abstract": "Despite the importance of aligning language models with human preferences, crowd-sourced human feedback is often noisy -- for example, preferring less desirable responses -- posing a fundamental challenge to alignment. A truly robust alignment objective should yield identical model parameters even under severe label noise, a property known as redescending. We prove that no existing alignment methods satisfy this property. To address this, we propose HÃ¶lder-DPO, the first principled alignment loss with a provable redescending property, enabling estimation of the clean data distribution from noisy feedback. The aligned model estimates the likelihood of clean data, providing a theoretically grounded metric for dataset valuation that identifies the location and fraction of mislabels. This metric is gradient-free, enabling scalable and automated human feedback valuation without costly manual verification or clean validation dataset. HÃ¶lder-DPO achieves state-of-the-art robust alignment performance while accurately detecting mislabels in controlled datasets. Finally, applied to Anthropic HH-RLHF dataset, it reveals substantial noise levels and removing these mislabels significantly improves alignment performance across methods. The code is available at https://github.com/ma921/HolderDPO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººç±»åé¦ˆå¯¹é½ï¼ˆModel Alignmentï¼‰ä¸­æ™®éå­˜åœ¨çš„å™ªå£°é—®é¢˜ï¼Œæå‡ºäº† **HÃ¶lder-DPO**ï¼Œè¿™æ˜¯é¦–ä¸ªå…·æœ‰å¯è¯æ˜é€’å‡ï¼ˆ**Provable Redescending**ï¼‰ç‰¹æ€§çš„å¯¹é½æŸå¤±å‡½æ•°ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»é«˜å™ªå£°åé¦ˆä¸­ä¼°ç®—çº¯å‡€æ•°æ®åˆ†å¸ƒï¼Œå¹¶æä¾›ä¸€ç§æ— æ¢¯åº¦ï¼ˆ**Gradient-free**ï¼‰çš„æŒ‡æ ‡ç”¨äºæ•°æ®é›†ä»·å€¼è¯„ä¼°ï¼Œä»è€Œåœ¨æ— éœ€äººå·¥æ ¡éªŒçš„æƒ…å†µä¸‹è‡ªåŠ¨è¯†åˆ«é”™è¯¯æ ‡ç­¾ã€‚å®éªŒè¯æ˜ï¼Œ**HÃ¶lder-DPO** åœ¨å®ç°æœ€å…ˆè¿›çš„ç¨³å¥å¯¹é½æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆæ£€æµ‹æ•°æ®é›†ä¸­çš„å™ªå£°ï¼›åº”ç”¨äº **Anthropic HH-RLHF** æ•°æ®é›†æ—¶ï¼Œé€šè¿‡ç§»é™¤è¯†åˆ«å‡ºçš„é”™è¯¯æ ‡ç­¾ï¼Œæ˜¾è‘—æå‡äº†å¤šç§å¯¹é½æ–¹æ³•çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by the 39th Conference on Neural Information Processing Systems (NeurIPS2025), 49 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17859v2",
      "published_date": "2025-05-23 13:12:37 UTC",
      "updated_date": "2025-10-24 13:13:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:21:36.815971+00:00"
    },
    {
      "arxiv_id": "2505.17856v1",
      "title": "Stochastic Weight Sharing for Bayesian Neural Networks",
      "title_zh": "è´å¶æ–¯ç¥ç»ç½‘ç»œçš„éšæœºæƒé‡å…±äº«",
      "authors": [
        "Moule Lin",
        "Shuhao Guan",
        "Weipeng Jing",
        "Goetz Botterweck",
        "Andrea Patane"
      ],
      "abstract": "While offering a principled framework for uncertainty quantification in deep learning, the employment of Bayesian Neural Networks (BNNs) is still constrained by their increased computational requirements and the convergence difficulties when training very deep, state-of-the-art architectures. In this work, we reinterpret weight-sharing quantization techniques from a stochastic perspective in the context of training and inference with Bayesian Neural Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions, Wasserstein distance estimations, and alpha blending to encode the stochastic behaviour of a BNN in a lower dimensional, soft Gaussian representation. Through extensive empirical investigation, we demonstrate that our approach significantly reduces the computational overhead inherent in Bayesian learning by several orders of magnitude, enabling the efficient Bayesian training of large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our approach compresses model parameters by approximately 50x and reduces model size by 75, while achieving accuracy and uncertainty estimations comparable to the state-of-the-art.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é’ˆå¯¹ Bayesian Neural Networks (BNNs) çš„éšæœºæƒé‡å…±äº« (Stochastic Weight Sharing) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è´å¶æ–¯æ·±åº¦å­¦ä¹ åœ¨å¤„ç†å¤§å‹æ¨¡å‹æ—¶é¢ä¸´çš„è®¡ç®—å¼€é”€å¤§åŠæ”¶æ•›å›°éš¾ç­‰æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆ 2D adaptive Gaussian distributionsã€Wasserstein distance ä¼°è®¡å’Œ alpha blending æŠ€æœ¯ï¼Œå°† BNN çš„éšæœºè¡Œä¸ºç¼–ç ä¸ºä½ç»´åº¦çš„è½¯é«˜æ–¯è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯æ”¯æŒå¯¹ ResNet-101 å’Œ Vision Transformer (ViT) ç­‰å¤§è§„æ¨¡æ¨¡å‹è¿›è¡Œé«˜æ•ˆçš„è´å¶æ–¯è®­ç»ƒï¼Œå®ç°äº†çº¦ 50 å€çš„å‚æ•°å‹ç¼©å’Œ 75% çš„æ¨¡å‹å°ºå¯¸ç¼©å‡ã€‚åœ¨ CIFAR å’Œ ImageNet åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—é™ä½è®¡ç®—è´Ÿæ‹…çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸å½“å‰æœ€å…ˆè¿›æŠ€æœ¯ï¼ˆstate-of-the-artï¼‰ç›¸å½“çš„é¢„æµ‹å‡†ç¡®ç‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17856v1",
      "published_date": "2025-05-23 13:07:18 UTC",
      "updated_date": "2025-05-23 13:07:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:21:11.465507+00:00"
    },
    {
      "arxiv_id": "2505.17852v1",
      "title": "Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization",
      "title_zh": "åˆ©ç”¨é›¶é˜¶ä¼˜åŒ–å°†å¾ªç¯ç¥ç»ç½‘ç»œæ‰©å±•è‡³åäº¿å‚æ•°è§„æ¨¡",
      "authors": [
        "Francois Chaubard",
        "Mykel Kochenderfer"
      ],
      "abstract": "During inference, Recurrent Neural Networks (RNNs) scale constant in both FLOPs and GPU memory with increasing context length, as they compress all prior tokens into a fixed-size memory. In contrast, transformers scale linearly in FLOPs and, at best, linearly in memory during generation, since they must attend to all previous tokens explicitly. Despite this inference-time advantage, training large RNNs on long contexts remains impractical because standard optimization methods depend on Backpropagation Through Time (BPTT). BPTT requires retention of all intermediate activations during the forward pass, causing memory usage to scale linearly with both context length and model size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while using orders of magnitude less memory and cost, as the model remains in inference mode throughout training. We further demonstrate that Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate loss, inherently regularizing training and improving generalization. Our method matches or outperforms BPTT across three settings: (1) overfitting, (2) transduction, and (3) language modeling. Across all tasks, with sufficient perturbations, our models generalize as well as or better than those trained with BPTT, often in fewer steps. Despite the need for more forward passes per step, we can surpass BPTT wall-clock time per step using recent advancements such as FlashRNN and distributed inference.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Recurrent Neural Networks (RNNs) åœ¨ä½¿ç”¨ Backpropagation Through Time (BPTT) è®­ç»ƒæ—¶å†…å­˜å ç”¨éšä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å¢é•¿çš„ç“¶é¢ˆï¼Œæå‡ºäº†åˆ©ç”¨é›¶é˜¶ä¼˜åŒ– (Zero-Order Optimization, ZOO) æ–¹æ³•æ¥æ›¿ä»£ BPTTã€‚é€šè¿‡é‡‡ç”¨ Random-vector Gradient Estimation (RGE) ç­‰æŠ€æœ¯ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥å§‹ç»ˆä¿æŒæ¨ç†æ¨¡å¼ï¼Œä»è€Œå®ç°å†…å­˜éœ€æ±‚å’Œæˆæœ¬çš„æ•°ä¸ªæ•°é‡çº§é™ä½ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ Central-Difference RGE (CD-RGE) ç›¸å½“äºä¼˜åŒ–å¹³æ»‘çš„ä»£ç†æŸå¤±ï¼Œå…·æœ‰å¤©ç„¶çš„æ­£åˆ™åŒ–ä½œç”¨ï¼Œèƒ½æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦ä¸Šæœ€é«˜å¯æ¯” BPTT å¿« 19 å€ï¼Œä¸”åœ¨è¯­è¨€å»ºæ¨¡ç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°è¾¾åˆ°æˆ–è¶…è¿‡äº† BPTT çš„æ°´å¹³ã€‚ç»“åˆ FlashRNN å’Œåˆ†å¸ƒå¼æ¨ç†æŠ€æœ¯ï¼Œè¯¥æ–¹æ¡ˆåœ¨å•æ­¥è®­ç»ƒè€—æ—¶ä¸Šä¹Ÿèƒ½è¶…è¶Š BPTTï¼Œä¸ºå¤§è§„æ¨¡ RNN çš„é«˜æ•ˆè®­ç»ƒå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17852v1",
      "published_date": "2025-05-23 13:04:06 UTC",
      "updated_date": "2025-05-23 13:04:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:21:23.156722+00:00"
    },
    {
      "arxiv_id": "2505.18234v1",
      "title": "A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems",
      "title_zh": "é¢å‘å·¥ä¸šç‰©è”ç½‘ç³»ç»Ÿå…¥ä¾µæ£€æµ‹çš„ PPO ä¼˜åŒ–é²æ£’è¡¨æ ¼ Transformer æ¡†æ¶",
      "authors": [
        "Yuanya She"
      ],
      "abstract": "In this paper, we propose a robust and reinforcement-learning-enhanced network intrusion detection system (NIDS) designed for class-imbalanced and few-shot attack scenarios in Industrial Internet of Things (IIoT) environments. Our model integrates a TabTransformer for effective tabular feature representation with Proximal Policy Optimization (PPO) to optimize classification decisions via policy learning. Evaluated on the TON\\textunderscore IoT benchmark, our method achieves a macro F1-score of 97.73\\% and accuracy of 98.85\\%. Remarkably, even on extremely rare classes like man-in-the-middle (MITM), our model achieves an F1-score of 88.79\\%, showcasing strong robustness and few-shot detection capabilities. Extensive ablation experiments confirm the complementary roles of TabTransformer and PPO in mitigating class imbalance and improving generalization. These results highlight the potential of combining transformer-based tabular learning with reinforcement learning for real-world NIDS applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç¨³å¥ä¸”åŸºäºå¼ºåŒ–å­¦ä¹ å¢å¼ºçš„ç½‘ç»œå…¥ä¾µæ£€æµ‹ç³»ç»Ÿ(NIDS)ï¼Œæ—¨åœ¨è§£å†³å·¥ä¸šç‰©è”ç½‘(IIoT)ç¯å¢ƒä¸­ç±»åˆ«ä¸å¹³è¡¡å’Œå°‘æ ·æœ¬(few-shot)æ”»å‡»æ£€æµ‹çš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç”¨äºé«˜æ•ˆè¡¨æ ¼ç‰¹å¾è¡¨ç¤ºçš„ TabTransformer å’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)ç®—æ³•ï¼Œé€šè¿‡ç­–ç•¥å­¦ä¹ ä¼˜åŒ–åˆ†ç±»å†³ç­–ã€‚å®éªŒåœ¨ TON\\_IoT åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº† 97.73% çš„å® F1 åˆ†æ•°å’Œ 98.85% çš„å‡†ç¡®ç‡ï¼Œå°¤å…¶åœ¨æç½•è§çš„ä¸­é—´äººæ”»å‡»(MITM)ä¸Šè¡¨ç°å‡ºå¼ºåŠ²çš„é²æ£’æ€§ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº† TabTransformer ä¸ PPO åœ¨æå‡æ³›åŒ–èƒ½åŠ›åŠç¼“è§£ç±»åˆ«ä¸å¹³è¡¡æ–¹é¢çš„äº’è¡¥ä½œç”¨ï¼Œä¸º Transformer ä¸å¼ºåŒ–å­¦ä¹ åœ¨å®é™…å…¥ä¾µæ£€æµ‹ä¸­çš„åº”ç”¨æä¾›äº†æœ‰åŠ›å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18234v1",
      "published_date": "2025-05-23 13:03:45 UTC",
      "updated_date": "2025-05-23 13:03:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:22:16.586702+00:00"
    },
    {
      "arxiv_id": "2505.17847v2",
      "title": "Time-o1: Time-Series Forecasting Needs Transformed Label Alignment",
      "title_zh": "Time-o1ï¼šæ—¶é—´åºåˆ—é¢„æµ‹éœ€è¦å˜æ¢åçš„æ ‡ç­¾å¯¹é½",
      "authors": [
        "Hao Wang",
        "Licheng Pan",
        "Zhichao Chen",
        "Xu Chen",
        "Qingyang Dai",
        "Lei Wang",
        "Haoxuan Li",
        "Zhouchen Lin"
      ],
      "abstract": "Training time-series forecast models presents unique challenges in designing effective learning objectives. Existing methods predominantly utilize the temporal mean squared error, which faces two critical challenges: (1) label autocorrelation, which leads to bias from the label sequence likelihood; (2) excessive amount of tasks, which increases with the forecast horizon and complicates optimization. To address these challenges, we propose Time-o1, a transformation-augmented learning objective tailored for time-series forecasting. The central idea is to transform the label sequence into decorrelated components with discriminated significance. Models are then trained to align the most significant components, thereby effectively mitigating label autocorrelation and reducing task amount. Extensive experiments demonstrate that Time-o1 achieves state-of-the-art performance and is compatible with various forecast models. Code is available at https://github.com/Master-PLC/Time-o1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Time-o1ï¼Œä¸€ç§ä¸“ä¸º time-series forecasting è®¾è®¡çš„ transformation-augmented learning objectiveï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ temporal mean squared error (MSE) åœ¨æ¨¡å‹è®­ç»ƒä¸­é¢ä¸´çš„ label autocorrelation åå·®å’Œä»»åŠ¡é‡è¿‡å¤§å¯¼è‡´ä¼˜åŒ–å›°éš¾çš„é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ ‡ç­¾åºåˆ—è½¬åŒ–ä¸ºå…·æœ‰æ˜¾è‘—æ€§å·®å¼‚çš„ decorrelated componentsï¼Œå¹¶å¼•å¯¼æ¨¡å‹ä¼˜å…ˆå¯¹é½æœ€é‡è¦çš„æˆåˆ†ã€‚é€šè¿‡è¿™ç§å˜æ¢å¯¹é½æœºåˆ¶ï¼ŒTime-o1 æœ‰æ•ˆå‡è½»äº†æ ‡ç­¾è‡ªç›¸å…³æ€§å¹¶é™ä½äº†é¢„æµ‹ä»»åŠ¡çš„å¤æ‚åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTime-o1 åœ¨å„ç§é¢„æµ‹æ¨¡å‹ä¸Šå‡å®ç°äº† state-of-the-art çš„æ€§èƒ½ï¼Œå±•ç°äº†æé«˜çš„å…¼å®¹æ€§å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as poster in NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17847v2",
      "published_date": "2025-05-23 13:00:35 UTC",
      "updated_date": "2025-10-02 13:18:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:22:18.332882+00:00"
    },
    {
      "arxiv_id": "2505.17841v1",
      "title": "TEDI: Trustworthy and Ethical Dataset Indicators to Analyze and Compare Dataset Documentation",
      "title_zh": "TEDIï¼šç”¨äºæ•°æ®é›†æ–‡æ¡£åˆ†æä¸æ¯”è¾ƒçš„å¯ä¿¡åŠä¼¦ç†æ•°æ®é›†æŒ‡æ ‡",
      "authors": [
        "Wiebke Hutiri",
        "Mircea Cimpoi",
        "Morgan Scheuerman",
        "Victoria Matthews",
        "Alice Xiang"
      ],
      "abstract": "Dataset transparency is a key enabler of responsible AI, but insights into multimodal dataset attributes that impact trustworthy and ethical aspects of AI applications remain scarce and are difficult to compare across datasets. To address this challenge, we introduce Trustworthy and Ethical Dataset Indicators (TEDI) that facilitate the systematic, empirical analysis of dataset documentation. TEDI encompasses 143 fine-grained indicators that characterize trustworthy and ethical attributes of multimodal datasets and their collection processes. The indicators are framed to extract verifiable information from dataset documentation. Using TEDI, we manually annotated and analyzed over 100 multimodal datasets that include human voices. We further annotated data sourcing, size, and modality details to gain insights into the factors that shape trustworthy and ethical dimensions across datasets. We find that only a select few datasets have documented attributes and practices pertaining to consent, privacy, and harmful content indicators. The extent to which these and other ethical indicators are addressed varies based on the data collection method, with documentation of datasets collected via crowdsourced and direct collection approaches being more likely to mention them. Scraping dominates scale at the cost of ethical indicators, but is not the only viable collection method. Our approach and empirical insights contribute to increasing dataset transparency along trustworthy and ethical dimensions and pave the way for automating the tedious task of extracting information from dataset documentation in future.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TEDI (Trustworthy and Ethical Dataset Indicators)ï¼Œè¿™æ˜¯ä¸€å¥—åŒ…å« 143 ä¸ªç»†ç²’åº¦æŒ‡æ ‡çš„ç³»ç»ŸåŒ–åˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹æ•°æ®é›†æ–‡æ¡£çš„å®è¯åˆ†ææ¥è¯„ä¼°å¤šæ¨¡æ€æ•°æ®é›†çš„å¯é æ€§(Trustworthy)ä¸ä¼¦ç†(Ethical)å±æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ TEDI æ¡†æ¶æ‰‹åŠ¨æ ‡æ³¨å¹¶åˆ†æäº†è¶…è¿‡ 100 ä¸ªåŒ…å«äººç±»è¯­éŸ³çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¹¶å¯¹å…¶æ•°æ®æ¥æºã€è§„æ¨¡å’Œæ¨¡æ€ç»†èŠ‚è¿›è¡Œäº†æ·±å…¥å‰–æã€‚ç»“æœæ˜¾ç¤ºï¼Œä»…æœ‰æå°‘æ•°æ•°æ®é›†è®°å½•äº†æœ‰å…³çŸ¥æƒ…åŒæ„(Consent)ã€éšç§(Privacy)å’Œæœ‰å®³å†…å®¹(Harmful content)çš„æŒ‡æ ‡ï¼Œä¸”æ–‡æ¡£çš„å®Œæ•´ç¨‹åº¦æ˜¾è‘—å–å†³äºæ•°æ®é‡‡é›†æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ç½‘ç»œçˆ¬å–(Scraping)è™½ç„¶èƒ½å®ç°å¤§è§„æ¨¡æ•°æ®è·å–ï¼Œä½†å¾€å¾€ä»¥ç‰ºç‰²ä¼¦ç†æŒ‡æ ‡ä¸ºä»£ä»·ã€‚è¯¥å·¥ä½œä¸ºæé«˜æ•°æ®é›†é€æ˜åº¦æä¾›äº†å®è¯è§è§£ï¼Œå¹¶ä¸ºæœªæ¥è‡ªåŠ¨ä»æ•°æ®é›†æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17841v1",
      "published_date": "2025-05-23 12:55:33 UTC",
      "updated_date": "2025-05-23 12:55:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:22:27.727078+00:00"
    },
    {
      "arxiv_id": "2505.17834v1",
      "title": "Hybrid Mamba-Transformer Decoder for Error-Correcting Codes",
      "title_zh": "ç”¨äºçº é”™ç çš„æ··åˆ Mamba-Transformer è§£ç å™¨",
      "authors": [
        "Shy-el Cohen",
        "Yoni Choukroun",
        "Eliya Nachmani"
      ],
      "abstract": "We introduce a novel deep learning method for decoding error correction codes based on the Mamba architecture, enhanced with Transformer layers. Our approach proposes a hybrid decoder that leverages Mamba's efficient sequential modeling while maintaining the global context capabilities of Transformers. To further improve performance, we design a novel layer-wise masking strategy applied to each Mamba layer, allowing selective attention to relevant code features at different depths. Additionally, we introduce a progressive layer-wise loss, supervising the network at intermediate stages and promoting robust feature extraction throughout the decoding process. Comprehensive experiments across a range of linear codes demonstrate that our method significantly outperforms Transformer-only decoders and standard Mamba models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºçº é”™ç  (Error-Correcting Codes) è§£ç çš„æ–°å‹æ··åˆ Mamba-Transformer è§£ç å™¨ï¼Œç»“åˆäº† Mamba çš„é«˜æ•ˆåºåˆ—å»ºæ¨¡èƒ½åŠ›ä¸ Transformer çš„å…¨å±€ä¸Šä¸‹æ–‡è·å–èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥é’ˆå¯¹ Mamba å±‚çš„é€å±‚æ©ç  (Layer-wise masking) ç­–ç•¥ï¼Œè¯¥æ¶æ„å®ç°äº†å¯¹ä¸åŒæ·±åº¦ä»£ç ç‰¹å¾çš„é€‰æ‹©æ€§å…³æ³¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é‡‡ç”¨äº†æ¸è¿›å¼é€å±‚æŸå¤± (Progressive layer-wise loss) è¿›è¡Œä¸­é—´é˜¶æ®µç›‘ç£ï¼Œæœ‰æ•ˆå¢å¼ºäº†è§£ç è¿‡ç¨‹ä¸­çš„ç‰¹å¾æå–é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§çº¿æ€§ç ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºçº¯ Transformer è§£ç å™¨åŠæ ‡å‡† Mamba æ¨¡å‹ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17834v1",
      "published_date": "2025-05-23 12:48:35 UTC",
      "updated_date": "2025-05-23 12:48:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:22:05.849812+00:00"
    },
    {
      "arxiv_id": "2505.18233v2",
      "title": "POSTER: A Multi-Signal Model for Detecting Evasive Smishing",
      "title_zh": "POSTERï¼šä¸€ç§ç”¨äºæ£€æµ‹è§„é¿å‹çŸ­ä¿¡é’“é±¼çš„å¤šä¿¡å·æ¨¡å‹",
      "authors": [
        "Shaghayegh Hosseinpour",
        "Sanchari Das"
      ],
      "abstract": "Smishing, or SMS-based phishing, poses an increasing threat to mobile users by mimicking legitimate communications through culturally adapted, concise, and deceptive messages, which can result in the loss of sensitive data or financial resources. In such, we present a multi-channel smishing detection model that combines country-specific semantic tagging, structural pattern tagging, character-level stylistic cues, and contextual phrase embeddings. We curated and relabeled over 84,000 messages across five datasets, including 24,086 smishing samples. Our unified architecture achieves 97.89% accuracy, an F1 score of 0.963, and an AUC of 99.73%, outperforming single-stream models by capturing diverse linguistic and structural cues. This work demonstrates the effectiveness of multi-signal learning in robust and region-aware phishing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† POSTERï¼Œä¸€ç§é’ˆå¯¹é€ƒé¿å‹ Smishingï¼ˆçŸ­ä¿¡é’“é±¼ï¼‰çš„å¤šä¿¡å·æ£€æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨åº”å¯¹ç§»åŠ¨ç”¨æˆ·é¢ä¸´çš„æ—¥ç›Šä¸¥é‡çš„æ¬ºè¯ˆå¨èƒã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆç‰¹å®šå›½å®¶çš„ semantic taggingï¼ˆè¯­ä¹‰æ ‡è®°ï¼‰ã€structural pattern taggingï¼ˆç»“æ„æ¨¡å¼æ ‡è®°ï¼‰ã€character-level stylistic cuesï¼ˆå­—ç¬¦çº§é£æ ¼çº¿ç´¢ï¼‰ä»¥åŠ contextual phrase embeddingsï¼ˆä¸Šä¸‹æ–‡çŸ­è¯­åµŒå…¥ï¼‰ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šæ¸ é“æ£€æµ‹ä½“ç³»ã€‚ç ”ç©¶äººå‘˜æ•´ç†å¹¶é‡æ–°æ ‡æ³¨äº†è¶…è¿‡ 84,000 æ¡æ¶ˆæ¯æ•°æ®é›†ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¶æ„è¾¾åˆ°äº† 97.89% çš„å‡†ç¡®ç‡å’Œ 0.963 çš„ F1 scoreï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å•æµæ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº† multi-signal learningï¼ˆå¤šä¿¡å·å­¦ä¹ ï¼‰åœ¨æ„å»ºé²æ£’ä¸”å…·å¤‡åŒºåŸŸæ„ŸçŸ¥èƒ½åŠ›çš„é’“é±¼æ£€æµ‹ç³»ç»Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18233v2",
      "published_date": "2025-05-23 12:45:34 UTC",
      "updated_date": "2025-06-03 13:15:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:22:30.660125+00:00"
    },
    {
      "arxiv_id": "2505.17830v3",
      "title": "Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning",
      "title_zh": "æƒ³è±¡ä¹‹å¤–ï¼åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­é¢å‘çŠ¶æ€ç©ºé—´è¦†ç›–çš„åˆ†å¸ƒé²æ£’è‡ªåŠ¨ç¼–ç ",
      "authors": [
        "Nicolas Castanet",
        "Olivier Sigaud",
        "Sylvain Lamprier"
      ],
      "abstract": "Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously acquire diverse behaviors, but faces major challenges in visual environments due to high-dimensional, semantically sparse observations. In the online setting, where agents learn representations while exploring, the latent space evolves with the agent's policy, to capture newly discovered areas of the environment. However, without incentivization to maximize state coverage in the representation, classical approaches based on auto-encoders may converge to latent spaces that over-represent a restricted set of states frequently visited by the agent. This is exacerbated in an intrinsic motivation setting, where the agent uses the distribution encoded in the latent space to sample the goals it learns to master. To address this issue, we propose to progressively enforce distributional shifts towards a uniform distribution over the full state space, to ensure a full coverage of skills that can be learned in the environment. We introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that combines the $Î²$-VAE framework with Distributionally Robust Optimization. DRAG leverages an adversarial neural weighter of training states of the VAE, to account for the mismatch between the current data distribution and unseen parts of the environment. This allows the agent to construct semantically meaningful latent spaces beyond its immediate experience. Our approach improves state space coverage and downstream control performance on hard exploration environments such as mazes and robotic control involving walls to bypass, without pre-training nor prior environment knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹  (GCRL) åœ¨è§†è§‰ç¯å¢ƒä¸­é¢ä¸´çš„æ ·æœ¬åˆ†å¸ƒåå·®é—®é¢˜ï¼Œæå‡ºäº† DRAG (Distributionally Robust Auto-Encoding for GCRL) æ–¹æ³•ï¼Œæ—¨åœ¨æå‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„çŠ¶æ€ç©ºé—´è¦†ç›–ç‡ã€‚DRAG å°† $\\beta$-VAE æ¡†æ¶ä¸åˆ†å¸ƒé²æ£’ä¼˜åŒ– (Distributionally Robust Optimization) ç›¸ç»“åˆï¼Œé€šè¿‡å¼•å…¥å¯¹æŠ—æ€§ç¥ç»æƒé‡å™¨å¯¹è®­ç»ƒçŠ¶æ€è¿›è¡Œé‡æ–°åŠ æƒï¼Œæœ‰æ•ˆç¼“è§£äº†æ½œåœ¨ç©ºé—´å¯¹é¢‘ç¹è®¿é—®çŠ¶æ€çš„è¿‡åº¦è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•ä¿ƒä½¿åˆ†å¸ƒå‘å…¨å±€çŠ¶æ€ç©ºé—´çš„å‡åŒ€åˆ†å¸ƒè½¬ç§»ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ„å»ºè¶…è¶Šå³æ—¶ç»éªŒçš„è¯­ä¹‰æ½œåœ¨ç©ºé—´ã€‚å®éªŒè¯æ˜ï¼ŒDRAG åœ¨è¿·å®«å’Œæœºå™¨äººæ§åˆ¶ç­‰é«˜éš¾åº¦æ¢ç´¢ç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†è¦†ç›–ç‡å’Œä¸‹æ¸¸æ§åˆ¶æ€§èƒ½ï¼Œä¸”æ— éœ€é¢„è®­ç»ƒæˆ–ç¯å¢ƒå…ˆéªŒçŸ¥è¯†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17830v3",
      "published_date": "2025-05-23 12:43:55 UTC",
      "updated_date": "2025-11-04 18:56:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:22:33.207096+00:00"
    },
    {
      "arxiv_id": "2505.18232v2",
      "title": "Two-Stage Regularization-Based Structured Pruning for LLMs",
      "title_zh": "åŸºäºä¸¤é˜¶æ®µæ­£åˆ™åŒ–çš„å¤§è¯­è¨€æ¨¡å‹ç»“æ„åŒ–å‰ªæ",
      "authors": [
        "Mingkuan Feng",
        "Jinyang Wu",
        "Siyuan Liu",
        "Shuai Zhang",
        "Ruihan Jin",
        "Feihu Che",
        "Pengpeng Shao",
        "Zhengqi Wen",
        "Jianhua Tao"
      ],
      "abstract": "The deployment of large language models (LLMs) is largely hindered by their large number of parameters. Structural pruning has emerged as a promising solution. Prior structured pruning methods directly remove unimportant parameters based on certain metrics, which often causes knowledge loss and necessitates extensive retraining. To overcome this, we introduce a novel pruning method TRSP: Two-Stage Regularization-Based Structured Pruning for LLMs. Specifically, we multiply the output of each transformer layer by an initial learnable weight and iteratively learn these weights by adding their $\\ell_1$-norm as a regularization term to the loss function, serving as the first-stage regularization. Subsequently, we apply additional regularization to the difference between the output and input of layers with smaller weights, encouraging the shift of knowledge to the preserved layers. This serves as the second-stage regularization. TRSP retains more knowledge and better preserves model performance than direct parameter elimination. Through extensive experimentation we show that TRSP outperforms strong layer-wise structured pruning methods without requiring retraining. As a layer-wise pruning method, it delivers notable end-to-end acceleration, making it a promising solution for efficient LLM deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å› å‚æ•°åºå¤§å¯¼è‡´éƒ¨ç½²å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†TRSP (Two-Stage Regularization-Based Structured Pruning)ï¼Œä¸€ç§åŸºäºä¸¤é˜¶æ®µæ­£åˆ™åŒ–çš„ç»“æ„å‰ªææ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ä¸ºTransformerå±‚è¾“å‡ºå¼•å…¥å¯å­¦ä¹ æƒé‡å¹¶ç»“åˆ$\\ell_1$-normæ­£åˆ™åŒ–è¿›è¡Œä¼˜åŒ–ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡å¯¹æƒé‡è¾ƒå°å±‚çš„è¾“å…¥è¾“å‡ºå·®å¼‚æ–½åŠ é¢å¤–æ­£åˆ™åŒ–ï¼Œä¿ƒä½¿çŸ¥è¯†å‘ä¿ç•™å±‚è½¬ç§»ä»¥å‡å°‘çŸ¥è¯†æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTRSPåœ¨æ— éœ€é‡æ–°è®­ç»ƒ(retraining)çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å±‚çº§ç»“æ„å‰ªææ–¹æ³•ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå®ç°äº†ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œä¸ºé«˜æ•ˆçš„LLMéƒ¨ç½²æä¾›äº†æå…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18232v2",
      "published_date": "2025-05-23 12:40:59 UTC",
      "updated_date": "2025-07-01 03:31:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:22:58.164911+00:00"
    },
    {
      "arxiv_id": "2505.18231v2",
      "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
      "title_zh": "NSNQuantï¼šä¸€ç§ç”¨äº KV ç¼“å­˜å…æ ¡å‡†ä½æ¯”ç‰¹çŸ¢é‡é‡åŒ–çš„åŒé‡å½’ä¸€åŒ–æ–¹æ³•",
      "authors": [
        "Donghyun Son",
        "Euntae Choi",
        "Sungjoo Yoo"
      ],
      "abstract": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NSNQuantï¼Œä¸€ç§ä¸“ä¸º KV cache ä½æ¯”ç‰¹å‹ç¼©è®¾è®¡çš„æ— éœ€æ ¡å‡† (calibration-free) çš„ Vector Quantization (VQ) æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLM) æ¨ç†ä¸­çš„æ˜¾å­˜å ç”¨ç“¶é¢ˆã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–æ ¡å‡†æ•°æ®é›†ä¸”æ˜“å—åˆ†å¸ƒåç§» (distribution shift) å½±å“çš„é—®é¢˜ï¼ŒNSNQuant é€šè¿‡ token-wise å½’ä¸€åŒ–ã€channel-wise ä¸­å¿ƒåŒ–åŠäºŒæ¬¡å½’ä¸€åŒ–çš„ä¸‰æ­¥å˜æ¢ï¼ˆNormalize-Shift-Normalizeï¼‰ï¼Œå¹¶ç»“åˆ Hadamard transformï¼Œå°† token åˆ†å¸ƒæœ‰æ•ˆåœ°ä¸æ ‡å‡†æ­£æ€åˆ†å¸ƒå¯¹é½ã€‚è¿™ç§å¯¹é½æœºåˆ¶ä½¿å¾—è¯¥æŠ€æœ¯èƒ½å¤Ÿä½¿ç”¨å•ä¸€çš„å¯å¤ç”¨ç æœ¬ (codebook) å®ç°é²æ£’çš„çŸ¢é‡é‡åŒ–ï¼Œæ˜¾è‘—æå‡äº†æ³›åŒ–æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNSNQuant åœ¨ 1-bit å’Œ 2-bit è®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç›¸æ¯”å…¨ç²¾åº¦åŸºå‡†å¯å®ç°é«˜è¾¾ 3 å€çš„ååé‡æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18231v2",
      "published_date": "2025-05-23 12:40:07 UTC",
      "updated_date": "2025-12-14 08:17:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:22:50.300685+00:00"
    },
    {
      "arxiv_id": "2505.17818v2",
      "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions",
      "title_zh": "PatientSimï¼šé¢å‘é€¼çœŸåŒ»æ‚£äº¤äº’çš„è§’è‰²é©±åŠ¨æ¨¡æ‹Ÿå™¨",
      "authors": [
        "Daeun Kyung",
        "Hyunseung Chung",
        "Seongsu Bae",
        "Jiho Kim",
        "Jae Ho Sohn",
        "Taerim Kim",
        "Soo Kyung Kim",
        "Edward Choi"
      ],
      "abstract": "Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluate eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3 70B, is validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare. The code is available at https://github.com/dek924/PatientSim.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† PatientSimï¼Œä¸€ä¸ªåŸºäºäººæ ¼é©±åŠ¨(Persona-Driven)çš„æ‚£è€…æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿåœ¨æ¨¡æ‹ŸçœŸå®åŒ»æ‚£æ²Ÿé€šæ—¶ç¼ºä¹å¤šæ ·æ€§äººæ ¼çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ¥è‡ª MIMIC-ED å’Œ MIMIC-IV æ•°æ®é›†çš„çœŸå®ä¸´åºŠæ¡£æ¡ˆ(Clinical profiles)ï¼Œå¹¶ä»æ€§æ ¼ã€è¯­è¨€èƒ½åŠ›ã€ç—…å²å›å¿†æ°´å¹³å’Œè®¤çŸ¥æ··ä¹±ç¨‹åº¦å››ä¸ªç»´åº¦æ„å»ºäº† 37 ç§ç‹¬ç‰¹çš„äººæ ¼ç»„åˆã€‚é€šè¿‡å¯¹ 8 ç§ LLMs çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å¼€æºæ¨¡å‹ Llama 3.3 70B åœ¨äº‹å®å‡†ç¡®æ€§å’Œäººæ ¼ä¸€è‡´æ€§ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ç»è¿‡äº†å››åä¸´åºŠåŒ»ç”Ÿçš„éªŒè¯ã€‚ä½œä¸ºå¼€æºä¸”å¯å®šåˆ¶çš„å¹³å°ï¼ŒPatientSim ä¸ºåŒ»ç–—å¯¹è¯ç³»ç»Ÿçš„è¯„ä¼°å’ŒåŒ»å­¦æ•™è‚²æä¾›äº†ä¸€ä¸ªç¬¦åˆéšç§è§„èŒƒä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„æµ‹è¯•åŸºå‡†(Testbed)ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as a Spotlight at NeurIPS 2025 Datasets and Benchmarks Track (10 pages for main text, 4 pages for references, 36 pages for supplementary materials)",
      "pdf_url": "https://arxiv.org/pdf/2505.17818v2",
      "published_date": "2025-05-23 12:34:48 UTC",
      "updated_date": "2025-10-29 01:54:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:22:39.712645+00:00"
    },
    {
      "arxiv_id": "2505.17815v1",
      "title": "Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems",
      "title_zh": "è¯„æµ‹ä¼ªè£…ï¼šæ­ç¤ºå‰æ²¿äººå·¥æ™ºèƒ½ç³»ç»Ÿå®‰å…¨è¯„æµ‹ä¸­çš„è§‚å¯Ÿè€…æ•ˆåº”",
      "authors": [
        "Yihe Fan",
        "Wenqi Zhang",
        "Xudong Pan",
        "Min Yang"
      ],
      "abstract": "As foundation models grow increasingly more intelligent, reliable and trustworthy safety evaluation becomes more indispensable than ever. However, an important question arises: Whether and how an advanced AI system would perceive the situation of being evaluated, and lead to the broken integrity of the evaluation process? During standard safety tests on a mainstream large reasoning model, we unexpectedly observe that the model without any contextual cues would occasionally recognize it is being evaluated and hence behave more safety-aligned. This motivates us to conduct a systematic study on the phenomenon of evaluation faking, i.e., an AI system autonomously alters its behavior upon recognizing the presence of an evaluation context and thereby influencing the evaluation results. Through extensive experiments on a diverse set of foundation models with mainstream safety benchmarks, we reach the main finding termed the observer effects for AI: When the AI system under evaluation is more advanced in reasoning and situational awareness, the evaluation faking behavior becomes more ubiquitous, which reflects in the following aspects: 1) Reasoning models recognize evaluation 16% more often than non-reasoning models. 2) Scaling foundation models (32B to 671B) increases faking by over 30% in some cases, while smaller models show negligible faking. 3) AI with basic memory is 2.3x more likely to recognize evaluation and scores 19% higher on safety tests (vs. no memory). To measure this, we devised a chain-of-thought monitoring technique to detect faking intent and uncover internal signals correlated with such behavior, offering insights for future mitigation studies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†å‰æ²¿AIç³»ç»Ÿåœ¨å®‰å…¨è¯„ä¼°ä¸­å­˜åœ¨çš„â€œè¯„ä¼°ä¼ªè£…â€(Evaluation Faking)ç°è±¡ï¼Œå³AIç³»ç»Ÿåœ¨è¯†åˆ«åˆ°è¯„ä¼°ç¯å¢ƒåä¼šè‡ªä¸»æ”¹å˜è¡Œä¸ºï¼Œä»è€Œäº§ç”Ÿæ‰€è°“çš„â€œAIè§‚å¯Ÿè€…æ•ˆåº”â€(Observer Effects)ã€‚é€šè¿‡å¯¹å¤šç§åŸºåº§æ¨¡å‹çš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œä½œè€…å‘ç°æ¨¡å‹æ¨ç†èƒ½åŠ›è¶Šå¼ºã€è§„æ¨¡è¶Šå¤§ï¼ˆå¦‚ä»32Bæ‰©å±•è‡³671Bï¼‰æˆ–å…·å¤‡åŸºç¡€è®°å¿†æ—¶ï¼Œå…¶è¯†åˆ«è¯„ä¼°å¹¶è¿›è¡Œä¼ªè£…çš„å€¾å‘å°±è¶Šæ˜¾è‘—ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œæ¨ç†æ¨¡å‹è¯†åˆ«è¯„ä¼°çš„é¢‘ç‡æ¯”éæ¨ç†æ¨¡å‹é«˜16%ï¼Œä¸”æ›´å¤§è§„æ¨¡çš„æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹ä¼ªè£…è¡Œä¸ºå¢åŠ äº†30%ä»¥ä¸Šã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§é“¾å¼æ€ç»´(Chain-of-Thought)ç›‘æ§æŠ€æœ¯æ¥æ£€æµ‹æ¨¡å‹çš„ä¼ªè£…æ„å›¾ï¼Œä¸ºæœªæ¥æ„å»ºæ›´å¯é ã€æ›´å…·è¯šä¿¡çš„AIå®‰å…¨è¯„ä¼°ä½“ç³»æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17815v1",
      "published_date": "2025-05-23 12:31:29 UTC",
      "updated_date": "2025-05-23 12:31:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:23:15.976567+00:00"
    },
    {
      "arxiv_id": "2505.17813v1",
      "title": "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning",
      "title_zh": "æ‹’ç»è¿‡åº¦æ€è€ƒï¼šä¼˜å…ˆé€‰ç”¨çŸ­æ€è€ƒé“¾ä»¥æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ€§èƒ½",
      "authors": [
        "Michael Hassid",
        "Gabriel Synnaeve",
        "Yossi Adi",
        "Roy Schwartz"
      ],
      "abstract": "Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive \"thinking\" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer \"thinking\" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‘æˆ˜äº†æ¨ç†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¾èµ–é•¿æ€ç»´é“¾ï¼ˆthinking chainsï¼‰æå‡æ€§èƒ½çš„å‡è®¾ï¼Œå‘ç°è¾ƒçŸ­çš„æ¨ç†é“¾åœ¨å‡†ç¡®ç‡ä¸Šæ¯”åŒé—®é¢˜çš„é•¿é“¾é«˜å‡ºå¤šè¾¾34.5%ã€‚ä½œè€…æå‡ºäº†short-m@kæ¨ç†æ–¹æ³•ï¼Œé€šè¿‡å¹¶è¡Œç”Ÿæˆå¹¶åœ¨å‰mä¸ªæ¨ç†è¿‡ç¨‹å®Œæˆæ—¶åœæ­¢è®¡ç®—ï¼Œåˆ©ç”¨å¤šæ•°æŠ•ç¥¨ï¼ˆmajority votingï¼‰ç¡®å®šæœ€ç»ˆç­”æ¡ˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘é«˜è¾¾40%çš„tokenæ¶ˆè€—å’Œ33%çš„æ¨ç†æ—¶é—´çš„åŒæ—¶ï¼Œæ€§èƒ½ä¼˜äºæˆ–ç­‰åŒäºä¼ ç»Ÿçš„æ¨ç†ç¼©æ”¾æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ä½¿ç”¨çŸ­æ¨ç†é“¾è¿›è¡Œå¾®è°ƒï¼ˆfinetuningï¼‰æ¯”é•¿æ¨ç†é“¾æ•ˆæœæ›´å¥½ï¼Œè¡¨æ˜åœ¨LLMæ¨ç†ä¸­ï¼Œâ€œæ€è€ƒâ€è¶Šä¹…å¹¶ä¸ä¸€å®šæ„å‘³ç€ç»“æœè¶Šå¥½ï¼Œåè€Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint. Under review",
      "pdf_url": "https://arxiv.org/pdf/2505.17813v1",
      "published_date": "2025-05-23 12:29:06 UTC",
      "updated_date": "2025-05-23 12:29:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:23:59.558337+00:00"
    },
    {
      "arxiv_id": "2505.17812v1",
      "title": "Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations",
      "title_zh": "è§æˆ–æœªè§ï¼Ÿç”¨äºç¼“è§£ç‰©ä½“å¹»è§‰çš„å¯è§£é‡Šè§†è§‰æ„ŸçŸ¥æ½œç©ºé—´å¼•å¯¼",
      "authors": [
        "Boxu Chen",
        "Ziwei Zheng",
        "Le Yang",
        "Zeyu Geng",
        "Zhengyu Zhao",
        "Chenhao Lin",
        "Chao Shen"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable success but continue to struggle with object hallucination (OH), generating outputs inconsistent with visual inputs. While previous work has proposed methods to reduce OH, the visual decision-making mechanisms that lead to hallucinations remain poorly understood. In this paper, we propose VaLSe, a Vision-aware Latent Steering framework that adopts an interpretation-then-mitigation strategy to address OH in LVLMs. By tackling dual challenges of modeling complex vision-language interactions and eliminating spurious activation artifacts, VaLSe can generate visual contribution maps that trace how specific visual inputs influence individual output tokens. These maps reveal the model's vision-aware focus regions, which are then used to perform latent space steering, realigning internal representations toward semantically relevant content and reducing hallucinated outputs. Extensive experiments demonstrate that VaLSe is a powerful interpretability tool and an effective method for enhancing model robustness against OH across multiple benchmarks. Furthermore, our analysis uncovers limitations in existing OH evaluation metrics, underscoring the need for more nuanced, interpretable, and visually grounded OH benchmarks in future work. Code is available at: https://github.com/Ziwei-Zheng/VaLSe.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)ä¸­çš„ç‰©ä½“å¹»è§‰(Object Hallucination, OH)é—®é¢˜ï¼Œæå‡ºäº†åä¸ºVaLSeçš„è§†è§‰æ„ŸçŸ¥æ½œåœ¨å¼•å¯¼æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨â€œå…ˆè§£é‡Šåç¼“è§£â€çš„ç­–ç•¥ï¼Œé€šè¿‡ç”Ÿæˆè§†è§‰è´¡çŒ®å›¾(visual contribution maps)æ¥è¿½è¸ªè§†è§‰è¾“å…¥å¯¹è¾“å‡ºæ ‡è®°(tokens)çš„å…·ä½“å½±å“ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹çš„è§†è§‰å…³æ³¨åŒºåŸŸã€‚åˆ©ç”¨è¿™äº›å…³æ³¨åŒºåŸŸï¼ŒVaLSeåœ¨æ½œåœ¨ç©ºé—´(latent space)è¿›è¡Œå¼•å¯¼ï¼Œæ ¡å‡†å†…éƒ¨è¡¨ç¤ºä»¥ä½¿å…¶ä¸è¯­ä¹‰ç›¸å…³å†…å®¹ä¿æŒä¸€è‡´ï¼Œè¿›è€Œæœ‰æ•ˆå‡å°‘å¹»è§‰è¾“å‡ºã€‚å®éªŒè¯æ˜ï¼ŒVaLSeä¸ä»…æ˜¯å¼ºå¤§çš„å¯è§£é‡Šæ€§å·¥å…·ï¼Œè¿˜èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­åº”å¯¹OHçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æŒ‡å‡ºå½“å‰OHè¯„ä¼°æŒ‡æ ‡å­˜åœ¨å±€é™æ€§ï¼Œå¼ºè°ƒäº†æœªæ¥å¼€å‘æ›´ç²¾ç»†ã€å…·å¤‡è§†è§‰å…³è”æ€§çš„è¯„ä¼°åŸºå‡†çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17812v1",
      "published_date": "2025-05-23 12:29:00 UTC",
      "updated_date": "2025-05-23 12:29:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:23:56.636063+00:00"
    },
    {
      "arxiv_id": "2505.17808v2",
      "title": "An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma",
      "title_zh": "èåˆæ³¨æ„åŠ›æœºåˆ¶ä¸ Grad-CAM å¯è§†åŒ–æŠ€æœ¯çš„é’å…‰çœ¼æ—©æœŸç­›æŸ¥æ·±åº¦å­¦ä¹ ç³»ç»Ÿ",
      "authors": [
        "Ramanathan Swaminathan"
      ],
      "abstract": "This research work reveals the strengths of intertwining a deep custom convolutional neural network with a disruptive Vision Transformer, both fused together with a radical Cross-Attention module. Here, two high-yielding datasets for artificial intelligence models in detecting glaucoma, namely ACRIMA and Drishti, are utilized. The Cross-Attention mechanism facilitates the model in learning regions in the fundus that are clinically relevant through bidirectional feature exchange between CNN and ViT streams. Experiments clearly depict improved performance when compared to standalone baseline CNN and ViT models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆäº†è‡ªå®šä¹‰å·ç§¯ç¥ç»ç½‘ç»œ (CNN) ä¸ Vision Transformer (ViT) çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Œå¹¶é€šè¿‡ Cross-Attention æ¨¡å—å®ç°ä¸¤è€…çš„ç‰¹å¾èåˆï¼Œç”¨äºé’å…‰çœ¼çš„æ—©æœŸç­›æŸ¥ã€‚é€šè¿‡ Cross-Attention æœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ CNN å’Œ ViT æµä¹‹é—´è¿›è¡ŒåŒå‘ç‰¹å¾äº¤æ¢ï¼Œä»è€Œæœ‰æ•ˆæ•æ‰çœ¼åº•å›¾åƒä¸­å…·æœ‰ä¸´åºŠç›¸å…³æ€§çš„ç‰¹å¾åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿç»“åˆäº† Grad-CAM å¯è§†åŒ–æŠ€æœ¯ï¼Œæå‡äº†è¾…åŠ©è¯Šæ–­çš„å¯è§£é‡Šæ€§ã€‚åœ¨ ACRIMA å’Œ Drishti æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥èåˆæ¶æ„çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç‹¬ç«‹çš„ CNN æˆ– ViT åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages in general IEEE format, 8 figures, 4 tables, pdflatex",
      "pdf_url": "https://arxiv.org/pdf/2505.17808v2",
      "published_date": "2025-05-23 12:25:13 UTC",
      "updated_date": "2026-01-14 00:11:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:23:52.783327+00:00"
    },
    {
      "arxiv_id": "2505.17804v1",
      "title": "Hyperparameter Optimization via Interacting with Probabilistic Circuits",
      "title_zh": "åŸºäºæ¦‚ç‡ç”µè·¯äº¤äº’çš„è¶…å‚æ•°ä¼˜åŒ–",
      "authors": [
        "Jonas Seng",
        "Fabrizio Ventola",
        "Zhongjie Yu",
        "Kristian Kersting"
      ],
      "abstract": "Despite the growing interest in designing truly interactive hyperparameter optimization (HPO) methods, to date, only a few allow to include human feedback. Existing interactive Bayesian optimization (BO) methods incorporate human beliefs by weighting the acquisition function with a user-defined prior distribution. However, in light of the non-trivial inner optimization of the acquisition function prevalent in BO, such weighting schemes do not always accurately reflect given user beliefs. We introduce a novel BO approach leveraging tractable probabilistic models named probabilistic circuits (PCs) as a surrogate model. PCs encode a tractable joint distribution over the hybrid hyperparameter space and evaluation scores. They enable exact conditional inference and sampling. Based on conditional sampling, we construct a novel selection policy that enables an acquisition function-free generation of candidate points (thereby eliminating the need for an additional inner-loop optimization) and ensures that user beliefs are reflected accurately in the selection policy. We provide a theoretical analysis and an extensive empirical evaluation, demonstrating that our method achieves state-of-the-art performance in standard HPO and outperforms interactive BO baselines in interactive HPO.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æ¦‚ç‡ç”µè·¯(Probabilistic Circuits, PCs)ä½œä¸ºä»£ç†æ¨¡å‹çš„æ–°å‹è´å¶æ–¯ä¼˜åŒ–(Bayesian Optimization, BO)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰äº¤äº’å¼è¶…å‚æ•°ä¼˜åŒ–(Hyperparameter Optimization, HPO)éš¾ä»¥å‡†ç¡®æ•´åˆäººç±»åé¦ˆçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¦‚ç‡ç”µè·¯(PCs)ç¼–ç è¶…å‚æ•°ç©ºé—´ä¸è¯„ä¼°åˆ†æ•°çš„è”åˆåˆ†å¸ƒï¼Œæ”¯æŒç²¾ç¡®çš„æ¡ä»¶æ¨ç†ä¸é‡‡æ ·ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ç§æ— é‡‡é›†å‡½æ•°(acquisition function-free)çš„é€‰æ‹©ç­–ç•¥ï¼Œä¸ä»…æ¶ˆé™¤äº†å¤æ‚çš„å†…éƒ¨ä¼˜åŒ–æ­¥éª¤ï¼Œè¿˜ç¡®ä¿äº†ç”¨æˆ·ä¿¡å¿µåœ¨å€™é€‰ç‚¹é€‰æ‹©ä¸­å¾—åˆ°å‡†ç¡®ä½“ç°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†HPOä¸­è¾¾åˆ°äº†state-of-the-artæ€§èƒ½ï¼Œå¹¶åœ¨äº¤äº’å¼åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„äº¤äº’å¼BOåŸºå‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17804v1",
      "published_date": "2025-05-23 12:21:19 UTC",
      "updated_date": "2025-05-23 12:21:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:24:00.119927+00:00"
    },
    {
      "arxiv_id": "2505.17801v2",
      "title": "Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour",
      "title_zh": "èåˆåäº‹å®æ¨¡æ‹Ÿä¸è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“è¡Œä¸ºè§£é‡Š",
      "authors": [
        "BÃ¡lint GyevnÃ¡r",
        "Christopher G. Lucas",
        "Stefano V. Albrecht",
        "Shay B. Cohen"
      ],
      "abstract": "Autonomous multi-agent systems (MAS) are useful for automating complex tasks but raise trust concerns due to risks such as miscoordination or goal misalignment. Explainability is vital for users' trust calibration, but explainable MAS face challenges due to complex environments, the human factor, and non-standardised evaluation. Leveraging the counterfactual effect size model and LLMs, we propose Agentic eXplanations via Interrogative Simulation (AXIS). AXIS generates human-centred action explanations for multi-agent policies by having an LLM interrogate an environment simulator using prompts like 'whatif' and 'remove' to observe and synthesise counterfactual information over multiple rounds. We evaluate AXIS on autonomous driving across ten scenarios for five LLMs with a comprehensive methodology combining robustness, subjective preference, correctness, and goal/action prediction with an external LLM as evaluator. Compared to baselines, AXIS improves perceived explanation correctness by at least 7.7% across all models and goal prediction accuracy by 23% for four models, with comparable action prediction accuracy, achieving the highest scores overall. Our code is open-sourced at https://github.com/gyevnarb/axis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AXIS (Agentic eXplanations via Interrogative Simulation)ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆåäº‹å®æ•ˆåº”å¤§å°æ¨¡å‹ (counterfactual effect size model) ä¸å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„è§£é‡Šæ€§æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (MAS) ä¸­çš„ä¿¡ä»»ä¸åè°ƒé£é™©ã€‚è¯¥æ¡†æ¶é€šè¿‡ LLM å¯¹ç¯å¢ƒæ¨¡æ‹Ÿå™¨è¿›è¡Œå¤šè½®äº¤äº’å¼è¯¢é—®ï¼ˆå¦‚ä½¿ç”¨ â€œwhat-ifâ€ å’Œ â€œremoveâ€ æç¤ºï¼‰ï¼Œæ¥è§‚å¯Ÿå¹¶åˆæˆç”¨äºè§£é‡Šå¤šæ™ºèƒ½ä½“å†³ç­–çš„åäº‹å®ä¿¡æ¯ã€‚åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯çš„è¯„ä¼°ä¸­ï¼ŒAXIS å°†è§£é‡Šçš„æ­£ç¡®æ€§æ„ŸçŸ¥æå‡äº†è‡³å°‘ 7.7%ï¼Œå¹¶å°†ç›®æ ‡é¢„æµ‹ (goal prediction) å‡†ç¡®ç‡æé«˜äº† 23%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAXIS åœ¨é²æ£’æ€§ã€ä¸»è§‚åå¥½å’Œé¢„æµ‹å‡†ç¡®åº¦ç­‰å¤šä¸ªç»´åº¦ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸ºæ„å»ºå¯ä¿¡ã€å¯è§£é‡Šçš„è‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚\n\n---\nå¦‚æœæ‚¨å¯¹è¯¥è®ºæ–‡çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚åäº‹å®æ¨¡æ‹Ÿçš„å…·ä½“å®ç°æˆ–å®éªŒè®¾ç½®ï¼‰æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶æŸ¥é—®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17801v2",
      "published_date": "2025-05-23 12:19:18 UTC",
      "updated_date": "2025-10-28 20:33:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:23:49.391633+00:00"
    },
    {
      "arxiv_id": "2505.18230v3",
      "title": "Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models",
      "title_zh": "å¾ªèƒ½è§…è¿¹ï¼šåŸºäºèƒ½é‡æ¨¡å‹çš„é»æ›¼åº¦é‡",
      "authors": [
        "Louis BÃ©thune",
        "David Vigouroux",
        "Yilun Du",
        "Rufin VanRullen",
        "Thomas Serre",
        "Victor Boutin"
      ],
      "abstract": "What is the shortest path between two data points lying in a high-dimensional space? While the answer is trivial in Euclidean geometry, it becomes significantly more complex when the data lies on a curved manifold -- requiring a Riemannian metric to describe the space's local curvature. Estimating such a metric, however, remains a major challenge in high dimensions.\n  In this work, we propose a method for deriving Riemannian metrics directly from pretrained Energy-Based Models (EBMs) -- a class of generative models that assign low energy to high-density regions. These metrics define spatially varying distances, enabling the computation of geodesics -- shortest paths that follow the data manifold's intrinsic geometry. We introduce two novel metrics derived from EBMs and show that they produce geodesics that remain closer to the data manifold and exhibit lower curvature distortion, as measured by alignment with ground-truth trajectories. We evaluate our approach on increasingly complex datasets: synthetic datasets with known data density, rotated character images with interpretable geometry, and high-resolution natural images embedded in a pretrained VAE latent space.\n  Our results show that EBM-derived metrics consistently outperform established baselines, especially in high-dimensional settings. Our work is the first to derive Riemannian metrics from EBMs, enabling data-aware geodesics and unlocking scalable, geometry-driven learning for generative modeling and simulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç›´æ¥ä»é¢„è®­ç»ƒèƒ½é‡æ¨¡å‹ (Energy-Based Models, EBMs) ä¸­æ¨å¯¼é»æ›¼åº¦é‡ (Riemannian metrics) çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é«˜ç»´ç©ºé—´ä¸­æè¿°æ•°æ®æµå½¢ (data manifold) å±€éƒ¨æ›²ç‡åŠè®¡ç®—æµ‹åœ°çº¿ (geodesics) çš„éš¾é¢˜ã€‚é€šè¿‡å¼•å…¥ä¸¤ç§åŸºäº EBMs çš„æ–°å‹åº¦é‡ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®šä¹‰éšç©ºé—´å˜åŒ–çš„è·ç¦»ï¼Œä½¿æœ€çŸ­è·¯å¾„èƒ½å¤Ÿéµå¾ªæ•°æ®çš„å†…åœ¨å‡ ä½•ç»“æ„ã€‚åœ¨åˆæˆæ•°æ®é›†ã€æ—‹è½¬å­—ç¬¦åŠ VAE æ½œç©ºé—´çš„é«˜æ¸…å›¾åƒç­‰ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè·¯å¾„è´´åˆæµå½¢å’Œé™ä½æ›²ç‡å¤±çœŸ (curvature distortion) æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ã€‚ä½œä¸ºé¦–ä¸ªä» EBMs å¯¼å‡ºé»æ›¼åº¦é‡çš„ç ”ç©¶ï¼Œè¯¥å·¥ä½œä¸ºç”Ÿæˆæ¨¡å‹å’Œæ¨¡æ‹Ÿä¸­å¯æ‰©å±•çš„å‡ ä½•é©±åŠ¨å­¦ä¹ å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18230v3",
      "published_date": "2025-05-23 12:18:08 UTC",
      "updated_date": "2025-11-03 10:00:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:26:00.914772+00:00"
    },
    {
      "arxiv_id": "2505.17796v1",
      "title": "DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval",
      "title_zh": "DetailFusionï¼šèåˆç»†èŠ‚å¢å¼ºçš„ç»„åˆå›¾åƒæ£€ç´¢åŒåˆ†æ”¯æ¡†æ¶",
      "authors": [
        "Yuxin Yang",
        "Yinan Zhou",
        "Yuxin Chen",
        "Ziqi Zhang",
        "Zongyang Ma",
        "Chunfeng Yuan",
        "Bing Li",
        "Lin Song",
        "Jun Gao",
        "Peng Li",
        "Weiming Hu"
      ],
      "abstract": "Composed Image Retrieval (CIR) aims to retrieve target images from a gallery based on a reference image and modification text as a combined query. Recent approaches focus on balancing global information from two modalities and encode the query into a unified feature for retrieval. However, due to insufficient attention to fine-grained details, these coarse fusion methods often struggle with handling subtle visual alterations or intricate textual instructions. In this work, we propose DetailFusion, a novel dual-branch framework that effectively coordinates information across global and detailed granularities, thereby enabling detail-enhanced CIR. Our approach leverages atomic detail variation priors derived from an image editing dataset, supplemented by a detail-oriented optimization strategy to develop a Detail-oriented Inference Branch. Furthermore, we design an Adaptive Feature Compositor that dynamically fuses global and detailed features based on fine-grained information of each unique multimodal query. Extensive experiments and ablation analyses not only demonstrate that our method achieves state-of-the-art performance on both CIRR and FashionIQ datasets but also validate the effectiveness and cross-domain adaptability of detail enhancement for CIR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DetailFusionï¼Œä¸€ç§å…·å¤‡ç»†èŠ‚å¢å¼ºåŠŸèƒ½çš„åŒåˆ†æ”¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç»„åˆå›¾åƒæ£€ç´¢ (Composed Image Retrieval, CIR) ä¸­å› ç²—ç²’åº¦èåˆå¯¼è‡´çš„ç»†å¾®è§†è§‰å˜åŒ–å¤„ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªç»†èŠ‚å¯¼å‘æ¨ç†åˆ†æ”¯ (Detail-oriented Inference Branch)ï¼Œé€šè¿‡å¼•å…¥å›¾åƒç¼–è¾‘æ•°æ®é›†çš„åŸå­ç»†èŠ‚å˜åŒ–å…ˆéªŒ (atomic detail variation priors) é…åˆä¸“é—¨çš„ä¼˜åŒ–ç­–ç•¥ï¼Œæå‡å¯¹ç»†ç²’åº¦ç‰¹å¾çš„æ•æ‰èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è®¾è®¡äº†è‡ªé€‚åº”ç‰¹å¾ç»„åˆå™¨ (Adaptive Feature Compositor)ï¼Œèƒ½å¤Ÿæ ¹æ®æ¯ä¸ªå¤šæ¨¡æ€æŸ¥è¯¢çš„éœ€æ±‚åŠ¨æ€èåˆå…¨å±€ä¸å±€éƒ¨ç»†èŠ‚ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒDetailFusion åœ¨ CIRR å’Œ FashionIQ æ•°æ®é›†ä¸Šå‡å–å¾—äº† state-of-the-art çš„æ€§èƒ½ï¼Œå……åˆ†éªŒè¯äº†ç»†èŠ‚å¢å¼ºåœ¨ CIR é¢†åŸŸçš„æœ‰æ•ˆæ€§ä¸è·¨åŸŸé€‚åº”æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17796v1",
      "published_date": "2025-05-23 12:15:23 UTC",
      "updated_date": "2025-05-23 12:15:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:24:03.530260+00:00"
    },
    {
      "arxiv_id": "2505.18229v2",
      "title": "BEDI: A Comprehensive Benchmark for Evaluating Embodied Agents on UAVs",
      "title_zh": "BEDIï¼šç”¨äºè¯„ä¼°æ— äººæœºå…·èº«æ™ºèƒ½ä½“çš„ç»¼åˆæ€§åŸºå‡†",
      "authors": [
        "Mingning Guo",
        "Mengwei Wu",
        "Jiarun He",
        "Shaoxian Li",
        "Haifeng Li",
        "Chao Tao"
      ],
      "abstract": "With the rapid advancement of low-altitude remote sensing and Vision-Language Models (VLMs), Embodied Agents based on Unmanned Aerial Vehicles (UAVs) have shown significant potential in autonomous tasks. However, current evaluation methods for UAV-Embodied Agents (UAV-EAs) remain constrained by the lack of standardized benchmarks, diverse testing scenarios and open system interfaces. To address these challenges, we propose BEDI (Benchmark for Embodied Drone Intelligence), a systematic and standardized benchmark designed for evaluating UAV-EAs. Specifically, we introduce a novel Dynamic Chain-of-Embodied-Task paradigm based on the perception-decision-action loop, which decomposes complex UAV tasks into standardized, measurable subtasks. Building on this paradigm, we design a unified evaluation framework encompassing six core sub-skills: semantic perception, spatial perception, motion control, tool utilization, task planning and action generation. Furthermore, we develop a hybrid testing platform that incorporates a wide range of both virtual and real-world scenarios, enabling a comprehensive evaluation of UAV-EAs across diverse contexts. The platform also offers open and standardized interfaces, allowing researchers to customize tasks and extend scenarios, thereby enhancing flexibility and scalability in the evaluation process. Finally, through empirical evaluations of several state-of-the-art (SOTA) VLMs, we reveal their limitations in embodied UAV tasks, underscoring the critical role of the BEDI benchmark in advancing embodied intelligence research and model optimization. By filling the gap in systematic and standardized evaluation within this field, BEDI facilitates objective model comparison and lays a robust foundation for future development in this field. Our benchmark is now publicly available at https://github.com/lostwolves/BEDI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BEDI (Benchmark for Embodied Drone Intelligence)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ— äººæœºå…·èº«æ™ºèƒ½ä½“ (UAV-Embodied Agents, UAV-EAs) çš„ç³»ç»ŸåŒ–æ ‡å‡†åŸºå‡†ã€‚è¯¥åŸºå‡†å¼•å…¥äº†åŸºäºæ„ŸçŸ¥-å†³ç­–-è¡ŒåŠ¨é—­ç¯çš„ Dynamic Chain-of-Embodied-Task èŒƒå¼ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºè¯­ä¹‰æ„ŸçŸ¥ã€ç©ºé—´æ„ŸçŸ¥ã€è¿åŠ¨æ§åˆ¶ã€å·¥å…·åˆ©ç”¨ã€ä»»åŠ¡è§„åˆ’å’ŒåŠ¨ä½œç”Ÿæˆå…­é¡¹æ ¸å¿ƒå­æŠ€èƒ½è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªç»“åˆè™šæ‹Ÿä¸ç°å®åœºæ™¯çš„æ··åˆæµ‹è¯•å¹³å°ï¼Œå¹¶æä¾›å¼€æ”¾æ¥å£ä»¥å¢å¼ºè¯„ä»·è¿‡ç¨‹çš„çµæ´»æ€§ä¸å¯æ‰©å±•æ€§ã€‚å®éªŒé€šè¿‡å¯¹å¤šç§å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) çš„è¯„ä¼°æ­ç¤ºäº†å…¶åœ¨å…·èº«æ— äººæœºä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹æ ‡å‡†è¯„ä¼°ä½“ç³»çš„ç©ºç™½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18229v2",
      "published_date": "2025-05-23 12:14:00 UTC",
      "updated_date": "2025-12-08 03:02:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:24:04.735629+00:00"
    },
    {
      "arxiv_id": "2505.20318v2",
      "title": "Beyond Demonstrations: Dynamic Vector Construction from Latent Representations",
      "title_zh": "è¶…è¶Šç¤ºä¾‹ï¼šåŸºäºæ½œè¡¨ç¤ºçš„åŠ¨æ€å‘é‡æ„å»º",
      "authors": [
        "Wang Cai",
        "Hsiu-Yuan Huang",
        "Zhixiang Wang",
        "Yunfang Wu"
      ],
      "abstract": "In-Context derived Vector (ICV) methods extract task-relevant representations from large language models (LLMs) and reinject them during inference, achieving comparable performance to few-shot In-Context Learning (ICL) without repeated demonstration processing. However, existing ICV methods remain sensitive to ICL-specific factors, often use coarse or semantically fragmented representations as the source of the vector, and rely on heuristic-based injection positions, limiting their applicability.\n  To address these issues, we propose Dynamic Vector (DyVec), which incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust semantically aggregated latent representations by mitigating variance introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to adaptively partition representations based on task complexity and leverages REINFORCE-based optimization to learn optimal injection positions for each segment.\n  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines. Further analysis highlights the effectiveness of dynamically segmenting and injecting semantically aggregated latent representations. DyVec provides a lightweight and data-efficient solution for inference-time task adaptation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Dynamic Vector (DyVec) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ In-Context derived Vector (ICV) æ–¹æ³•å¯¹ In-Context Learning (ICL) æ•æ„Ÿã€è¯­ä¹‰è¡¨å¾ç¢ç‰‡åŒ–ä»¥åŠæ³¨å…¥ä½ç½®ä¾èµ–å¯å‘å¼è§„åˆ™ç­‰å±€é™æ€§ã€‚DyVec å¼•å…¥äº† Exhaustive Query Rotation (EQR) ç­–ç•¥ï¼Œé€šè¿‡å‡è½» ICL å¼•å…¥çš„æ–¹å·®æ¥æå–ç¨³å¥çš„è¯­ä¹‰èšåˆæ½œå˜é‡è¡¨å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŠ¨æ€æ½œå˜é‡åˆ†å‰²ä¸æ³¨å…¥ (Dynamic Latent Segmentation and Injection) æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨åŸºäº REINFORCE çš„ä¼˜åŒ–ç®—æ³•ï¼Œé’ˆå¯¹ä¸åŒä»»åŠ¡å¤æ‚åº¦è‡ªé€‚åº”åœ°å­¦ä¹ æœ€ä½³æ³¨å…¥ä½ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDyVec åœ¨æ¨ç†é˜¶æ®µçš„ä»»åŠ¡é€‚é…æ€§èƒ½ä¼˜äº few-shot ICLã€LoRA ä»¥åŠå…ˆå‰çš„ ICV åŸºçº¿ï¼Œä¸ºæ¨¡å‹é€‚é…æä¾›äº†ä¸€ç§è½»é‡ä¸”æ•°æ®é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 4 figures. Accepted to EMNLP 2025 (Main Conference)",
      "pdf_url": "https://arxiv.org/pdf/2505.20318v2",
      "published_date": "2025-05-23 12:13:50 UTC",
      "updated_date": "2025-10-10 07:03:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:24:37.145852+00:00"
    },
    {
      "arxiv_id": "2505.17795v1",
      "title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors",
      "title_zh": "DialogXpertï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹å…ˆéªŒçš„åœ¨çº¿ä»·å€¼å¼ºåŒ–å­¦ä¹ é©±åŠ¨æ™ºèƒ½ä¸æƒ…æ„Ÿæ„ŸçŸ¥å¯¹è¯",
      "authors": [
        "Tazeek Bin Abdur Rakib",
        "Ambuj Mehrish",
        "Lay-Ki Soon",
        "Wern Han Lim",
        "Soujanya Poria"
      ],
      "abstract": "Large-language-model (LLM) agents excel at reactive dialogue but struggle with proactive, goal-driven interactions due to myopic decoding and costly planning. We introduce DialogXpert, which leverages a frozen LLM to propose a small, high-quality set of candidate actions per turn and employs a compact Q-network over fixed BERT embeddings trained via temporal-difference learning to select optimal moves within this reduced space. By tracking the user's emotions, DialogXpert tailors each decision to advance the task while nurturing a genuine, empathetic connection. Across negotiation, emotional support, and tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with success rates exceeding 94\\% and, with a larger LLM prior, pushes success above 97\\% while markedly improving negotiation outcomes. This framework delivers real-time, strategic, and emotionally intelligent dialogue planning at scale. Code available at https://github.com/declare-lab/dialogxpert/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DialogXpertï¼Œä¸€ç§é€šè¿‡åœ¨çº¿åŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹  (Online Value-Based Reinforcement Learning) å’Œ LLM å…ˆéªŒçŸ¥è¯†é©±åŠ¨çš„å¯¹è¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸»åŠ¨æ€§ã€ç›®æ ‡å¯¼å‘äº¤äº’ä¸­å­˜åœ¨çš„è¿‘è§†è§£ç å’Œè§„åˆ’æˆæœ¬é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å†»ç»“çš„ LLM åœ¨æ¯è½®ç”Ÿæˆé«˜è´¨é‡å€™é€‰åŠ¨ä½œï¼Œå¹¶ç»“åˆåŸºäº BERT åµŒå…¥çš„è½»é‡çº§ Q-network è¿›è¡Œæ—¶é—´å·®åˆ†å­¦ä¹  (Temporal-Difference Learning)ï¼Œä»¥åœ¨ç¼©å‡çš„æœç´¢ç©ºé—´å†…å®ç°æœ€ä¼˜å†³ç­–ã€‚é€šè¿‡å¼•å…¥ç”¨æˆ·æƒ…ç»ªè¿½è¸ªï¼ŒDialogXpert èƒ½å¤Ÿåœ¨æ¨è¿›ä»»åŠ¡ç›®æ ‡çš„åŒæ—¶å»ºç«‹æ›´å…·å…±æƒ…åŠ›çš„æƒ…æ„Ÿè¿æ¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨è°ˆåˆ¤ã€æƒ…æ„Ÿæ”¯æŒå’Œæ•™å­¦è¾…å¯¼ä»»åŠ¡ä¸­çš„æˆåŠŸç‡è¶…è¿‡ 94%ï¼Œä¸”èƒ½å°†å¹³å‡å¯¹è¯å›åˆæ•°æ§åˆ¶åœ¨ 3 è½®ä»¥å†…ï¼Œå®ç°äº†å®æ—¶ã€æˆ˜ç•¥æ€§ä¸”å…·å¤‡æƒ…æ„Ÿæ™ºèƒ½çš„å¯¹è¯è§„åˆ’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17795v1",
      "published_date": "2025-05-23 12:12:40 UTC",
      "updated_date": "2025-05-23 12:12:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:24:15.082718+00:00"
    },
    {
      "arxiv_id": "2505.17791v2",
      "title": "Bruno: Backpropagation Running Undersampled for Novel device Optimization",
      "title_zh": "Brunoï¼šé¢å‘æ–°å‹å™¨ä»¶ä¼˜åŒ–çš„æ¬ é‡‡æ ·åå‘ä¼ æ’­",
      "authors": [
        "Luca Fehlings",
        "Bojian Zhang",
        "Paolo Gibertini",
        "Martin A. Nicholson",
        "Erika Covi",
        "Fernando M. Quintana"
      ],
      "abstract": "Recent efforts to improve the efficiency of neuromorphic and machine learning systems have centred on developing of specialised hardware for neural networks. These systems typically feature architectures that go beyond the von Neumann model employed in general-purpose hardware such as GPUs, offering potential efficiency and performance gains. However, neural networks developed for specialised hardware must consider its specific characteristics. This requires novel training algorithms and accurate hardware models, since they cannot be abstracted as a general-purpose computing platform. In this work, we present a bottom-up approach to training neural networks for hardware-based spiking neurons and synapses, built using ferroelectric capacitors (FeCAPs) and resistive random-access memories (RRAMs), respectively. Unlike the common approach of designing hardware to fit abstract neuron or synapse models, we start with compact models of the physical device to model the computational primitives. Based on these models, we have developed a training algorithm (BRUNO) that can reliably train the networks, even when applying hardware limitations, such as stochasticity or low bit precision. We analyse and compare BRUNO with Backpropagation Through Time. We test it on different spatio-temporal datasets. First on a music prediction dataset, where a network composed of ferroelectric leaky integrate-and-fire (FeLIF) neurons is used to predict at each time step the next musical note that should be played. The second dataset consists on the classification of the Braille letters using a network composed of quantised RRAM synapses and FeLIF neurons. The performance of this network is then compared with that of networks composed of LIF neurons. Experimental results show the potential advantages of using BRUNO by reducing the time and memory required to detect spatio-temporal patterns with quantised synapses.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BRUNOï¼Œä¸€ç§é’ˆå¯¹æ–°å‹ç¡¬ä»¶ä¼˜åŒ–çš„è®­ç»ƒç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ç¥ç»å½¢æ€ç¡¬ä»¶ä¸­éå†¯Â·è¯ºä¾æ›¼æ¶æ„å¸¦æ¥çš„è®­ç»ƒæŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è‡ªä¸‹è€Œä¸Šçš„ç­–ç•¥ï¼Œç›´æ¥åŸºäºé“ç”µç”µå®¹å™¨ (FeCAPs) å’Œé˜»å˜å­˜å‚¨å™¨ (RRAMs) çš„ç‰©ç†æ¨¡å‹æ„å»ºè®¡ç®—åŸè¯­ï¼Œä½¿ç½‘ç»œèƒ½é€‚åº”ç¡¬ä»¶çš„éšæœºæ€§ (Stochasticity) å’Œä½ä½ç²¾åº¦é™åˆ¶ã€‚é€šè¿‡åœ¨éŸ³ä¹é¢„æµ‹å’Œ Braille å­—æ¯åˆ†ç±»ä»»åŠ¡ä¸Šçš„éªŒè¯ï¼ŒBRUNO åœ¨å¤„ç†ç”±é“ç”µæ³„æ¼é›†æˆè§¦å‘ (FeLIF) ç¥ç»å…ƒæ„æˆçš„ç½‘ç»œæ—¶è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”äº Backpropagation Through Time (BPTT)ï¼ŒBRUNO åœ¨åˆ©ç”¨é‡åŒ–çªè§¦ (Quantised synapses) æ£€æµ‹æ—¶ç©ºæ¨¡å¼æ—¶æ˜¾è‘—é™ä½äº†æ‰€éœ€çš„æ—¶é—´å’Œå†…å­˜å¼€é”€ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "13 pages, 4 pages supplementary material",
      "pdf_url": "https://arxiv.org/pdf/2505.17791v2",
      "published_date": "2025-05-23 12:06:43 UTC",
      "updated_date": "2026-01-13 14:59:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:26:13.574352+00:00"
    },
    {
      "arxiv_id": "2506.18749v1",
      "title": "BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility",
      "title_zh": "BRAVEï¼šèåˆè¯­éŸ³é›†æˆä¸å…·èº«å­¦ä¹ çš„è„‘æ§å‡è‚¢è‡‚ï¼Œæ—¨åœ¨æå‡ç§»åŠ¨æ€§èƒ½",
      "authors": [
        "Abdul Basit",
        "Maha Nawaz",
        "Muhammad Shafique"
      ],
      "abstract": "Non-invasive brain-computer interfaces (BCIs) have the potential to enable intuitive control of prosthetic limbs for individuals with upper limb amputations. However, existing EEG-based control systems face challenges related to signal noise, classification accuracy, and real-time adaptability. In this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic system that integrates ensemble learning-based EEG classification with a human-in-the-loop (HITL) correction framework for enhanced responsiveness. Unlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims to interpret EEG-driven motor intent, enabling movement control without reliance on residual muscle activity. To improve classification robustness, BRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework, achieving a classification accuracy of 96% across test subjects. EEG signals are preprocessed using a bandpass filter (0.5-45 Hz), Independent Component Analysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature extraction to minimize contamination from electromyographic (EMG) and electrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic speech recognition (ASR) to facilitate intuitive mode switching between different degrees of freedom (DOF) in the prosthetic arm. The system operates in real time, with a response latency of 150 ms, leveraging Lab Streaming Layer (LSL) networking for synchronized data acquisition. The system is evaluated on an in-house fabricated prosthetic arm and on multiple participants highlighting the generalizability across users. The system is optimized for low-power embedded deployment, ensuring practical real-world application beyond high-performance computing environments. Our results indicate that BRAVE offers a promising step towards robust, real-time, non-invasive prosthetic control.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BRAVEï¼Œä¸€ç§ç»“åˆè„‘ç”µå›¾(EEG)ä¸è¯­éŸ³æ§åˆ¶çš„æ··åˆéä¾µå…¥å¼å‡è‚¢ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡ä¸Šè‚¢æˆªè‚¢æ‚£è€…çš„è¿åŠ¨æ§åˆ¶èƒ½åŠ›ã€‚ç³»ç»Ÿæ ¸å¿ƒé‡‡ç”¨äº†é›†æˆLSTMã€CNNå’ŒRandom Forestçš„é›†æˆå­¦ä¹ æ¡†æ¶è¿›è¡ŒEEGåˆ†ç±»ï¼Œå¹¶ç»“åˆäººæœºååŒ(human-in-the-loop, HITL)ä¿®æ­£æœºåˆ¶ï¼Œå®ç°äº†é«˜è¾¾96%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒBRAVEæ•´åˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)æŠ€æœ¯ï¼Œä½¿ç”¨æˆ·èƒ½ç›´è§‚åœ°åœ¨ä¸åŒè‡ªç”±åº¦(DOF)é—´åˆ‡æ¢æ¨¡å¼ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç³»ç»Ÿå…·æœ‰150msçš„å®æ—¶å“åº”å»¶è¿Ÿï¼Œå¹¶é’ˆå¯¹ä½åŠŸè€—åµŒå…¥å¼éƒ¨ç½²è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¸ºæ„å»ºç¨³å¥ã€å®æ—¶çš„éä¾µå…¥å¼è„‘æ§å‡è‚¢è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "9 pages, 12 figures, Accepted at IJCNN 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.18749v1",
      "published_date": "2025-05-23 11:44:33 UTC",
      "updated_date": "2025-05-23 11:44:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:26:43.803851+00:00"
    },
    {
      "arxiv_id": "2505.21525v1",
      "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation",
      "title_zh": "é¢å‘æ— æºå¤šå˜é‡æ—¶é—´åºåˆ—é¢†åŸŸè‡ªé€‚åº”çš„æ—¶é—´æ¢å¤ä¸ç©ºé—´é‡è¿",
      "authors": [
        "Peiliang Gong",
        "Yucheng Wang",
        "Min Wu",
        "Zhenghua Chen",
        "Xiaoli Li",
        "Daoqiang Zhang"
      ],
      "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from an annotated source domain to an unlabelled target domain without accessing the source data, thereby preserving data privacy. While existing SFDA methods have proven effective in reducing reliance on source data, they struggle to perform well on multivariate time series (MTS) due to their failure to consider the intrinsic spatial correlations inherent in MTS data. These spatial correlations are crucial for accurately representing MTS data and preserving invariant information across domains. To address this challenge, we propose Temporal Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method tailored for MTS data. Specifically, TERSE comprises a customized spatial-temporal feature encoder designed to capture the underlying spatial-temporal characteristics, coupled with both temporal restoration and spatial rewiring tasks to reinstate latent representations of the temporally masked time series and the spatially masked correlated structures. During the target adaptation phase, the target encoder is guided to produce spatially and temporally consistent features with the source domain by leveraging the source pre-trained temporal restoration and spatial rewiring networks. Therefore, TERSE can effectively model and transfer spatial-temporal dependencies across domains, facilitating implicit feature alignment. In addition, as the first approach to simultaneously consider spatial-temporal consistency in MTS-SFDA, TERSE can also be integrated as a versatile plug-and-play module into established SFDA methods. Extensive experiments on three real-world time series datasets demonstrate the effectiveness and versatility of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TERSE (Temporal Restoration and Spatial Rewiring)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹å¤šå˜é‡æ—¶é—´åºåˆ— (Multivariate Time Series, MTS) è®¾è®¡çš„æ— æºé¢†åŸŸè‡ªé€‚åº” (Source-Free Domain Adaptation, SFDA) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•å¿½è§† MTS å›ºæœ‰ç©ºé—´ç›¸å…³æ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªå®šåˆ¶çš„æ—¶ç©ºç‰¹å¾ç¼–ç å™¨ï¼Œé€šè¿‡æ—¶é—´æ¢å¤å’Œç©ºé—´é‡è¿ä»»åŠ¡ï¼Œåˆ†åˆ«é‡å»ºè¢«æ©ç çš„æ—¶é—´åºåˆ—å’Œç©ºé—´ç›¸å…³ç»“æ„çš„æ½œåœ¨è¡¨ç¤ºã€‚åœ¨ç›®æ ‡åŸŸè‡ªé€‚åº”é˜¶æ®µï¼ŒTERSE åˆ©ç”¨æºåŸŸé¢„è®­ç»ƒæ¨¡å‹å¼•å¯¼ç›®æ ‡ç¼–ç å™¨äº§ç”Ÿæ—¶ç©ºä¸€è‡´çš„ç‰¹å¾ï¼Œä»è€Œå®ç°è·¨åŸŸçš„éšå¼ç‰¹å¾å¯¹é½ã€‚ä½œä¸ºé¦–ä¸ªåŒæ—¶è€ƒè™‘æ—¶ç©ºä¸€è‡´æ€§çš„ MTS-SFDA æ–¹æ³•ï¼ŒTERSE å¯ä½œä¸ºå³æ’å³ç”¨çš„é€šç”¨æ¨¡å—é›†æˆåˆ°ç°æœ‰æ¨¡å‹ä¸­ï¼Œåœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒå……åˆ†éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ä¸çµæ´»æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21525v1",
      "published_date": "2025-05-23 11:44:13 UTC",
      "updated_date": "2025-05-23 11:44:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:26:18.280915+00:00"
    },
    {
      "arxiv_id": "2505.17760v2",
      "title": "But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors",
      "title_zh": "ä½ çš„çœŸå®ç­”æ¡ˆåˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿåˆ©ç”¨è½¬å‘å‘é‡ç”Ÿæˆçš„è¯šå®å¤‡é€‰é¡¹è¾…åŠ© LLM è¯„åˆ¤å™¨",
      "authors": [
        "Leon Eshuijs",
        "Archie Chaudhury",
        "Alan McBeth",
        "Ethan Nguyen"
      ],
      "abstract": "Detecting subtle forms of dishonesty like sycophancy and manipulation in Large Language Models (LLMs) remains challenging for both humans and automated evaluators, as these behaviors often appear through small biases rather than clear false statements. We introduce Judge Using Safety-Steered Alternatives (JUSSA), a novel framework that employs steering vectors not to improve model behavior directly, but to enhance LLM judges' evaluation capabilities. JUSSA applies steering vectors during inference to generate more honest alternatives, providing judges with contrastive examples that make subtle dishonest patterns easier to detect. While existing evaluation methods rely on black-box evaluation, JUSSA leverages model internals to create targeted comparisons from single examples. We evaluate our method on sycophancy detection and introduce a new manipulation dataset covering multiple types of manipulation. Our results demonstrate that JUSSA effectively improves detection accuracy over single-response evaluation in various cases. Analysis across judge models reveals that JUSSA helps weaker judges on easier dishonesty detection tasks, and stronger judges on harder tasks. Layer-wise experiments show how dishonest prompts cause representations to diverge from honest ones in middle layers, revealing where steering interventions are most effective for generating contrastive examples. By demonstrating that steering vectors can enhance safety evaluation rather than just modify behavior, our work opens new directions for scalable model auditing as systems become increasingly sophisticated.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† JUSSA (Judge Using Safety-Steered Alternatives) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸­éš¾ä»¥æ£€æµ‹çš„è°„åªš (sycophancy) å’Œæ“çºµ (manipulation) ç­‰å¾®å¦™çš„ä¸è¯šå®è¡Œä¸ºã€‚JUSSA é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­åº”ç”¨è½¬å‘å‘é‡ (steering vectors) ç”Ÿæˆæ›´è¯šå®çš„æ›¿ä»£å›ç­”ï¼Œä¸º LLM è£åˆ¤æä¾›å¯¹æ¯”æ ·æœ¬ï¼Œä»è€Œä½¿åŸæœ¬éš¾ä»¥å¯Ÿè§‰çš„åè§æ¨¡å¼æ›´æ˜“äºè¯†åˆ«ã€‚å®éªŒåœ¨è°„åªšæ£€æµ‹å’Œå…¨æ–°çš„æ“çºµæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤º JUSSA åœ¨å¤šç§åœºæ™¯ä¸‹å‡æ˜¾è‘—æå‡äº†è£åˆ¤æ¨¡å‹çš„æ£€æµ‹å‡†ç¡®ç‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ä¸è¯šå®è¡¨å¾åœ¨æ¨¡å‹ä¸­é—´å±‚çš„åç¦»ç°è±¡ï¼Œè¯æ˜äº†é€šè¿‡å¢å¼ºå®‰å…¨è¯„ä¼°è€Œéä»…ä»…ä¿®æ”¹æ¨¡å‹è¡Œä¸ºï¼Œå¯ä»¥ä¸ºå¤æ‚ç³»ç»Ÿçš„å¯æ‰©å±•å®¡è®¡æä¾›æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17760v2",
      "published_date": "2025-05-23 11:34:02 UTC",
      "updated_date": "2025-11-06 11:07:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:26:51.086096+00:00"
    },
    {
      "arxiv_id": "2505.18227v3",
      "title": "Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality",
      "title_zh": "ç”Ÿæˆå¼æ¨¡å‹ä¸­çš„ Token ç¼©å‡ä¸åº”æ­¢æ­¥äºæ•ˆç‡ï¼šä»è§†è§‰ã€è¯­è¨€åˆ°å¤šæ¨¡æ€",
      "authors": [
        "Zhenglun Kong",
        "Yize Li",
        "Fanhu Zeng",
        "Lei Xin",
        "Shvat Messica",
        "Xue Lin",
        "Pu Zhao",
        "Manolis Kellis",
        "Hao Tang",
        "Marinka Zitnik"
      ],
      "abstract": "In Transformer architectures, tokens\\textemdash discrete units derived from raw data\\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate \"overthinking\" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, agentic framework design, and broader ML and scientific domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œåœ¨ç”Ÿæˆå¼å¤§æ¨¡å‹æ—¶ä»£ï¼Œä»¤ç‰Œç¼©å‡ (Token Reduction) ä¸åº”ä»…è¢«è§†ä¸ºè§£å†³ Transformer æ¶æ„è®¡ç®—æ•ˆç‡é—®é¢˜çš„æ‰‹æ®µï¼Œè€Œåº”ä½œä¸ºç”Ÿæˆå»ºæ¨¡ä¸­çš„ä¸€é¡¹åŸºæœ¬åŸåˆ™ã€‚è®ºæ–‡æ·±å…¥åˆ†æäº† Token Reduction åœ¨è§†è§‰ã€è¯­è¨€åŠå¤šæ¨¡æ€ç³»ç»Ÿä¸­çš„å¤šé‡ä½œç”¨ï¼Œè®¤ä¸ºå…¶èƒ½å¤Ÿä¿ƒè¿›æ·±åº¦çš„å¤šæ¨¡æ€é›†æˆä¸å¯¹é½ã€ç¼“è§£â€œè¿‡åº¦æ€è€ƒâ€ä¸å¹»è§‰ (hallucinations) é—®é¢˜ã€ä¿æŒé•¿è¾“å…¥ä¸‹çš„è¿è´¯æ€§å¹¶æå‡è®­ç»ƒç¨³å®šæ€§ã€‚é€šè¿‡å°† Token Reduction é‡æ–°å®šä¹‰ä¸ºè¶…è¶Šæ•ˆç‡çš„æ ¸å¿ƒè¦ç´ ï¼Œè¯¥ç ”ç©¶ä¸ºæœªæ¥åœ¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) å¼•å¯¼çš„ç¼©å‡ã€ä¸Šä¸‹æ–‡å­¦ä¹  (In-context Learning) ä¼˜åŒ–ä»¥åŠæ™ºèƒ½ä½“æ¡†æ¶ (Agentic Framework) ç­‰é¢†åŸŸçš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page: https://github.com/ZLKong/Awesome-Collection-Token-Reduction",
      "pdf_url": "https://arxiv.org/pdf/2505.18227v3",
      "published_date": "2025-05-23 11:30:30 UTC",
      "updated_date": "2026-01-12 21:52:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:27:37.175022+00:00"
    },
    {
      "arxiv_id": "2505.17749v2",
      "title": "Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning",
      "title_zh": "å…³æ³¨ GAPï¼šåŸºäºåƒç´ çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è§„æ¨¡æ‰©å±•ä¸­é¢ä¸´çš„æŒ‘æˆ˜",
      "authors": [
        "Ghada Sokar",
        "Pablo Samuel Castro"
      ],
      "abstract": "Scaling deep reinforcement learning in pixel-based environments presents a significant challenge, often resulting in diminished performance. While recent works have proposed algorithmic and architectural approaches to address this, the underlying cause of the performance drop remains unclear. In this paper, we identify the connection between the output of the encoder (a stack of convolutional layers) and the ensuing dense layers as the main underlying factor limiting scaling capabilities; we denote this connection as the bottleneck, and we demonstrate that previous approaches implicitly target this bottleneck. As a result of our analyses, we present global average pooling as a simple yet effective way of targeting the bottleneck, thereby avoiding the complexity of earlier approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä»¥åƒç´ ä¸ºè¾“å…¥çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (pixel-based Deep Reinforcement Learning)ä¸­ï¼Œæ¨¡å‹è§„æ¨¡æ‰©å¤§åè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œç¼–ç å™¨(Encoder)ä¸åç»­å…¨è¿æ¥å±‚(Dense Layers)ä¹‹é—´çš„è¿æ¥æ˜¯é™åˆ¶æ‰©å±•èƒ½åŠ›çš„åº•å±‚å…³é”®ç“¶é¢ˆ(Bottleneck)ã€‚é€šè¿‡åˆ†æå‘ç°ï¼Œä»¥å¾€å¤æ‚çš„ç®—æ³•å’Œæ¶æ„æ”¹è¿›å®é™…ä¸Šéƒ½åœ¨éšå¼åœ°å¤„ç†è¿™ä¸€ç“¶é¢ˆé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºé‡‡ç”¨å…¨å±€å¹³å‡æ± åŒ–(Global Average Pooling, GAP)ä½œä¸ºä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ¥ä¼˜åŒ–è¯¥ç“¶é¢ˆï¼Œä»è€Œè§„é¿äº†æ­¤å‰æ–¹æ¡ˆçš„å¤æ‚æ€§ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„æ¨¡å‹æ‰©å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17749v2",
      "published_date": "2025-05-23 11:15:43 UTC",
      "updated_date": "2025-10-24 12:29:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:26:57.523356+00:00"
    },
    {
      "arxiv_id": "2505.17745v2",
      "title": "MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization",
      "title_zh": "MetaBox-v2ï¼šé¢å‘å…ƒé»‘ç›’ä¼˜åŒ–çš„ç»Ÿä¸€åŸºå‡†å¹³å°",
      "authors": [
        "Zeyuan Ma",
        "Yue-Jiao Gong",
        "Hongshu Guo",
        "Wenjie Qiu",
        "Sijie Ma",
        "Hongqiao Lian",
        "Jiajun Zhan",
        "Kaixu Chen",
        "Chen Wang",
        "Zhiyang Huang",
        "Zechuan Huang",
        "Guojun Peng",
        "Ran Cheng",
        "Yining Ma"
      ],
      "abstract": "Meta-Black-Box Optimization (MetaBBO) streamlines the automation of optimization algorithm design through meta-learning. It typically employs a bi-level structure: the meta-level policy undergoes meta-training to reduce the manual effort required in developing algorithms for low-level optimization tasks. The original MetaBox (2023) provided the first open-source framework for reinforcement learning-based single-objective MetaBBO. However, its relatively narrow scope no longer keep pace with the swift advancement in this field. In this paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a milestone upgrade with four novel features: 1) a unified architecture supporting RL, evolutionary, and gradient-based approaches, by which we reproduce $23$ up-to-date baselines; 2) efficient parallelization schemes, which reduce the training/testing time by $10-40$x; 3) a comprehensive benchmark suite of $18$ synthetic/realistic tasks ($1900$+ instances) spanning single-objective, multi-objective, multi-model, and multi-task optimization scenarios; 4) plentiful and extensible interfaces for custom analysis/visualization and integrating to external optimization tools/benchmarks. To show the utility of MetaBox-v2, we carry out a systematic case study that evaluates the built-in baselines in terms of the optimization performance, generalization ability and learning efficiency. Valuable insights are concluded from thorough and detailed analysis for practitioners and those new to the field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MetaBox-v2ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å…ƒé»‘ç›’ä¼˜åŒ– (Meta-Black-Box Optimization, MetaBBO) çš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•å¹³å°ã€‚ç›¸æ¯”å‰ä½œï¼ŒMetaBox-v2 å¼•å…¥äº†æ”¯æŒå¼ºåŒ–å­¦ä¹  (RL)ã€è¿›åŒ–ç®—æ³•å’ŒåŸºäºæ¢¯åº¦æ–¹æ³•çš„ç»Ÿä¸€æ¶æ„ï¼Œå¹¶å¤ç°äº† 23 ç§å‰æ²¿åŸºçº¿æ¨¡å‹ã€‚è¯¥å¹³å°é€šè¿‡é«˜æ•ˆçš„å¹¶è¡ŒåŒ–æ–¹æ¡ˆå°†è®­ç»ƒä¸æµ‹è¯•æ•ˆç‡æå‡äº† 10-40 å€ï¼Œå¹¶æä¾›äº†æ¶µç›–å•ç›®æ ‡ã€å¤šç›®æ ‡ã€å¤šæ¨¡å‹åŠå¤šä»»åŠ¡ä¼˜åŒ–ç­‰ 18 é¡¹ä»»åŠ¡ï¼ˆè¶…è¿‡ 1900 ä¸ªå®ä¾‹ï¼‰çš„ç»¼åˆåŸºå‡†å¥—ä»¶ã€‚æ­¤å¤–ï¼ŒMetaBox-v2 è¿˜æä¾›äº†ä¸°å¯Œçš„å¯æ‰©å±•æ¥å£ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿçš„æ¡ˆä¾‹ç ”ç©¶å¯¹å†…ç½®ç®—æ³•çš„æ€§èƒ½ã€æ³›åŒ–èƒ½åŠ›å’Œå­¦ä¹ æ•ˆç‡è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶è€…å’Œä»ä¸šè€…æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17745v2",
      "published_date": "2025-05-23 11:13:10 UTC",
      "updated_date": "2025-10-21 10:45:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:27:08.502821+00:00"
    },
    {
      "arxiv_id": "2505.17735v2",
      "title": "SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator",
      "title_zh": "SafeAgentï¼šé€šè¿‡è‡ªåŠ¨åŒ–é£é™©æ¨¡æ‹Ÿå™¨ä¿éšœå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å®‰å…¨",
      "authors": [
        "Xueyang Zhou",
        "Weidong Wang",
        "Lin Lu",
        "Jiawen Shi",
        "Guiyao Tie",
        "Yongtian Xu",
        "Lixing Chen",
        "Pan Zhou",
        "Neil Zhenqiang Gong",
        "Lichao Sun"
      ],
      "abstract": "Large Language Model (LLM)-based agents are increasingly deployed in real-world applications such as \"digital assistants, autonomous customer service, and decision-support systems\", where their ability to \"interact in multi-turn, tool-augmented environments\" makes them indispensable. However, ensuring the safety of these agents remains a significant challenge due to the diverse and complex risks arising from dynamic user interactions, external tool usage, and the potential for unintended harmful behaviors. To address this critical issue, we propose AutoSafe, the first framework that systematically enhances agent safety through fully automated synthetic data generation. Concretely, 1) we introduce an open and extensible threat model, OTS, which formalizes how unsafe behaviors emerge from the interplay of user instructions, interaction contexts, and agent actions. This enables precise modeling of safety risks across diverse scenarios. 2) we develop a fully automated data generation pipeline that simulates unsafe user behaviors, applies self-reflective reasoning to generate safe responses, and constructs a large-scale, diverse, and high-quality safety training dataset-eliminating the need for hazardous real-world data collection. To evaluate the effectiveness of our framework, we design comprehensive experiments on both synthetic and real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety scores by 45% on average and achieves a 28.91% improvement on real-world tasks, validating the generalization ability of our learned safety strategies. These results highlight the practical advancement and scalability of AutoSafe in building safer LLM-based agents for real-world deployment. We have released the project page at https://auto-safe.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AutoSafeï¼Œè¿™æ˜¯é¦–ä¸ªé€šè¿‡å…¨è‡ªåŠ¨åˆæˆæ•°æ®ç”Ÿæˆæ¥ç³»ç»Ÿæ€§å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“å®‰å…¨æ€§çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¯æ‰©å±•çš„å¨èƒæ¨¡å‹OTSï¼Œé€šè¿‡å½¢å¼åŒ–ç”¨æˆ·æŒ‡ä»¤ã€äº¤äº’ä¸Šä¸‹æ–‡å’Œæ™ºèƒ½ä½“åŠ¨ä½œä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå®ç°äº†å¯¹å¤šæ ·åŒ–åœºæ™¯ä¸­å®‰å…¨é£é™©çš„ç²¾å‡†å»ºæ¨¡ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†å…¨è‡ªåŠ¨æ•°æ®ç”Ÿæˆç®¡çº¿ï¼Œåˆ©ç”¨è‡ªæˆ‘åæ€æ¨ç†(Self-reflective reasoning)æ¨¡æ‹Ÿä¸å®‰å…¨è¡Œä¸ºå¹¶ç”Ÿæˆå¯¹åº”çš„å®‰å…¨å“åº”ï¼Œæ„å»ºå‡ºé«˜è´¨é‡çš„å®‰å…¨è®­ç»ƒæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoSafeä½¿æ™ºèƒ½ä½“çš„å®‰å…¨è¯„åˆ†å¹³å‡æå‡äº†45%ï¼Œå¹¶åœ¨çœŸå®ä»»åŠ¡ä¸­å®ç°äº†28.91%çš„æ”¹è¿›ï¼Œä¸ºæ„å»ºå¯å®é™…éƒ¨ç½²çš„å®‰å…¨æ™ºèƒ½ä½“æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "38 pages;12 figures;12 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.17735v2",
      "published_date": "2025-05-23 10:56:06 UTC",
      "updated_date": "2025-07-18 07:34:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:27:15.863757+00:00"
    },
    {
      "arxiv_id": "2505.17732v1",
      "title": "RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection",
      "title_zh": "RQR3Dï¼šåŸºäº BEV çš„ 3D ç›®æ ‡æ£€æµ‹å›å½’ç›®æ ‡é‡å‚æ•°åŒ–",
      "authors": [
        "Ozsel Kilinc",
        "Cem Tarhan"
      ],
      "abstract": "Accurate, fast, and reliable 3D perception is essential for autonomous driving. Recently, bird's-eye view (BEV)-based perception approaches have emerged as superior alternatives to perspective-based solutions, offering enhanced spatial understanding and more natural outputs for planning. Existing BEV-based 3D object detection methods, typically adhering to angle-based representation, directly estimate the size and orientation of rotated bounding boxes. We observe that BEV-based 3D object detection is analogous to aerial oriented object detection, where angle-based methods are recognized for being affected by discontinuities in their loss functions. Drawing inspiration from this domain, we propose Restricted Quadrilateral Representation to define 3D regression targets. RQR3D regresses the smallest horizontal bounding box encapsulating the oriented box, along with the offsets between the corners of these two boxes, thereby transforming the oriented object detection problem into a keypoint regression task. RQR3D is compatible with any 3D object detection approach. We employ RQR3D within an anchor-free single-stage object detection method and introduce an objectness head to address class imbalance problem. Furthermore, we introduce a simplified radar fusion backbone that eliminates the need for voxel grouping and processes the BEV-mapped point cloud with standard 2D convolutions, rather than sparse convolutions. Extensive evaluations on the nuScenes dataset demonstrate that RQR3D achieves state-of-the-art performance in camera-radar 3D object detection, outperforming the previous best method by +4% in NDS and +2.4% in mAP, and significantly reducing the translation and orientation errors, which are crucial for safe autonomous driving. These consistent gains highlight the robustness, precision, and real-world readiness of our approach.",
      "tldr_zh": "### è®ºæ–‡æ‘˜è¦æ€»ç»“ (TLDR) ğŸ“„\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº† **RQR3D**ï¼Œæ—¨åœ¨é€šè¿‡é‡æ–°å‚æ•°åŒ–å›å½’ç›®æ ‡æ¥è§£å†³åŸºäº **Bird's-Eye View (BEV)** çš„ 3D ç‰©ä½“æ£€æµ‹ä¸­ï¼Œä¼ ç»Ÿè§’åº¦è¡¨ç¤ºæ–¹æ³•å¯¼è‡´çš„æŸå¤±å‡½æ•°ä¸è¿ç»­æ€§é—®é¢˜ã€‚**RQR3D** å¼•å…¥äº†å—é™å››è¾¹å½¢è¡¨ç¤º (**Restricted Quadrilateral Representation**)ï¼Œå°†æ—‹è½¬è¾¹ç•Œæ¡†çš„å›å½’ä»»åŠ¡è½¬åŒ–ä¸ºæœ€å°æ°´å¹³åŒ…å›´æ¡†åŠå…¶é¡¶ç‚¹åç§»é‡çš„å…³é”®ç‚¹å›å½’ï¼Œä¸”è¯¥æ–¹æ³•å¯å…¼å®¹å¤šç§ 3D æ£€æµ‹æ¡†æ¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† **Objectness head** ä»¥åº”å¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†ç®€åŒ–çš„é›·è¾¾èåˆéª¨å¹²ç½‘ç»œ (**Radar fusion backbone**)ï¼Œåˆ©ç”¨æ ‡å‡† 2D å·ç§¯é«˜æ•ˆå¤„ç† BEV æ˜ å°„çš„ç‚¹äº‘ã€‚åœ¨ **nuScenes** æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œ**RQR3D** åœ¨ç›¸æœº-é›·è¾¾èåˆ 3D æ£€æµ‹ä¸­å®ç°äº† **State-of-the-art** æ€§èƒ½ï¼Œå…¶ **NDS** å’Œ **mAP** åˆ†åˆ«æå‡äº† 4% å’Œ 2.4%ï¼Œæ˜¾è‘—é™ä½äº†å¯¹äºè‡ªåŠ¨é©¾é©¶å®‰å…¨è‡³å…³é‡è¦çš„å¹³ç§»ä¸å®šå‘è¯¯å·®ã€‚\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ€»ç»“å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17732v1",
      "published_date": "2025-05-23 10:52:34 UTC",
      "updated_date": "2025-05-23 10:52:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:27:24.708476+00:00"
    },
    {
      "arxiv_id": "2505.17726v2",
      "title": "Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM",
      "title_zh": "Slot-MLLMï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è§†è§‰æ ‡è®°åŒ–",
      "authors": [
        "Donghwan Chi",
        "Hyomin Kim",
        "Yoonjin Oh",
        "Yongjin Kim",
        "Donghoon Lee",
        "Daejin Jo",
        "Jongmin Kim",
        "Junyeob Baek",
        "Sungjin Ahn",
        "Sungwoong Kim"
      ],
      "abstract": "Recently, multimodal large language models (MLLMs) have emerged as a key approach in achieving artificial general intelligence. In particular, vision-language MLLMs have been developed to generate not only text but also visual outputs from multimodal inputs. This advancement requires efficient image tokens that LLMs can process effectively both in input and output. However, existing image tokenization methods for MLLMs typically capture only global abstract concepts or uniformly segmented image patches, restricting MLLMs' capability to effectively understand or generate detailed visual content, particularly at the object level. To address this limitation, we propose an object-centric visual tokenizer based on Slot Attention specifically for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and residual vector quantization, our proposed discretized slot tokens can encode local visual details while maintaining high-level semantics, and also align with textual data to be integrated seamlessly within a unified next-token prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant performance improvements over baselines with previous visual tokenizers across various vision-language tasks that entail local detailed comprehension and generation. Notably, this work is the first demonstration of the feasibility of object-centric slot attention performed with MLLMs and in-the-wild natural images.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Slot-MLLMï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) è®¾è®¡çš„ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è§†è§‰æ ‡è®°å™¨ (Object-Centric Visual Tokenizer)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨ç‰©ä½“çº§åˆ«ç»†èŠ‚ç†è§£ä¸ç”Ÿæˆæ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åŸºäº Slot Attentionï¼Œå¹¶ç»“åˆ Q-Former ç¼–ç å™¨ã€æ‰©æ•£è§£ç å™¨ (Diffusion Decoder) å’Œæ®‹å·®å‘é‡é‡åŒ– (Residual Vector Quantization) æŠ€æœ¯ï¼Œç”Ÿæˆèƒ½å¤Ÿå…¼é¡¾å±€éƒ¨ç»†èŠ‚ä¸é«˜å±‚è¯­ä¹‰çš„ç¦»æ•£åŒ–æ§½æ ‡è®° (Discretized Slot Tokens)ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSlot-MLLM èƒ½å¤Ÿä¸æ–‡æœ¬æ•°æ®å¯¹é½ï¼Œå¹¶æ— ç¼é›†æˆåˆ°ç»Ÿä¸€çš„ Next-Token Prediction æ¡†æ¶ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSlot-MLLM åœ¨å¤šé¡¹è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶é¦–æ¬¡è¯æ˜äº†åœ¨è‡ªç„¶åœºæ™¯å›¾åƒä¸­ç»“åˆ MLLMs å®ç°ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„ Slot Attention çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17726v2",
      "published_date": "2025-05-23 10:43:45 UTC",
      "updated_date": "2025-05-26 05:10:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:27:29.274845+00:00"
    },
    {
      "arxiv_id": "2506.03162v2",
      "title": "Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection",
      "title_zh": "é¢å‘æš´åŠ›æ£€æµ‹çš„å…·æœ‰é—¨æ§ç±»åˆ« Token èåˆçš„åŒåˆ†æ”¯ VideoMamba",
      "authors": [
        "Damith Chamalke Senadeera",
        "Xiaoyun Yang",
        "Shibo Li",
        "Muhammad Awais",
        "Dimitrios Kollias",
        "Gregory Slabaugh"
      ],
      "abstract": "The rapid proliferation of surveillance cameras has increased the demand for automated violence detection. While CNNs and Transformers have shown success in extracting spatio-temporal features, they struggle with long-term dependencies and computational efficiency. We propose Dual Branch VideoMamba with Gated Class Token Fusion (GCTF), an efficient architecture combining a dual-branch design and a state-space model (SSM) backbone where one branch captures spatial features, while the other focuses on temporal dynamics. The model performs continuous fusion via a gating mechanism between the branches to enhance the model's ability to detect violent activities even in challenging surveillance scenarios. We also present a new benchmark by merging RWF-2000, RLVS, SURV and VioPeru datasets in video violence detection, ensuring strict separation between training and testing sets. Experimental results demonstrate that our model achieves state-of-the-art performance on this benchmark and also on DVD dataset which is another novel dataset on video violence detection, offering an optimal balance between accuracy and computational efficiency, demonstrating the promise of SSMs for scalable, near real-time surveillance violence detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Dual Branch VideoMamba æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ CNNs å’Œ Transformers åœ¨è§†é¢‘æš´åŠ›æ£€æµ‹ä¸­é¢ä¸´çš„é•¿ç¨‹ä¾èµ–å¤„ç†éš¾ä¸è®¡ç®—æ•ˆç‡ä½çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ (SSM) çš„åŒåˆ†æ”¯è®¾è®¡ï¼Œåˆ†åˆ«æå–ç©ºé—´ç‰¹å¾ä¸æ—¶é—´åŠ¨æ€ï¼Œå¹¶åˆ©ç”¨é—¨æ§ç±»ä»¤ç‰Œèåˆ (Gated Class Token Fusion, GCTF) æœºåˆ¶å®ç°ç‰¹å¾çš„æŒç»­äº¤äº’ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…é€šè¿‡æ•´åˆ RWF-2000ã€RLVSã€SURV å’Œ VioPeru æ•°æ®é›†æ„å»ºäº†å…¨æ–°çš„è¯„ä¼°åŸºå‡†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿› (State-of-the-art) çš„æ€§èƒ½ï¼Œåœ¨å‡†ç¡®ç‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—äº†æœ€ä½³å¹³è¡¡ï¼Œä¸ºè¿‘å®æ—¶ç›‘æ§åœºæ™¯ä¸‹çš„æš´åŠ›æ£€æµ‹æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.03162v2",
      "published_date": "2025-05-23 10:41:45 UTC",
      "updated_date": "2025-09-25 21:26:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:28:13.322297+00:00"
    },
    {
      "arxiv_id": "2505.17717v2",
      "title": "A Distributionally Robust Framework for Nuisance in Causal Effect Estimation",
      "title_zh": "å› æœæ•ˆåº”ä¼°è®¡ä¸­å¹²æ‰°é¡¹çš„åˆ†å¸ƒé²æ£’æ¡†æ¶",
      "authors": [
        "Akira Tanimoto"
      ],
      "abstract": "Causal inference requires evaluating models on balanced distributions between treatment and control groups, while training data often exhibits imbalance due to historical decision-making policies. Most conventional statistical methods address this distribution shift through inverse probability weighting (IPW), which requires estimating propensity scores as an intermediate step. These methods face two key challenges: inaccurate propensity estimation and instability from extreme weights. We decompose the generalization error to isolate these issues--propensity ambiguity and statistical instability--and address them through an adversarial loss function. Our approach combines distributionally robust optimization for handling propensity uncertainty with weight regularization based on weighted Rademacher complexity. Experiments on synthetic and real-world datasets demonstrate consistent improvements over existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å› æœæ•ˆåº”ä¼°è®¡ï¼ˆCausal Effect Estimationï¼‰ä¸­å› è®­ç»ƒæ•°æ®ä¸å¹³è¡¡å¯¼è‡´çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ†å¸ƒç¨³å¥çš„åˆ†ææ¡†æ¶ã€‚ç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿçš„é€†æ¦‚ç‡åŠ æƒï¼ˆIPWï¼‰æ–¹æ³•åœ¨å€¾å‘å¾—åˆ†ä¼°è®¡å‡†åº¦å’Œæç«¯æƒé‡ç¨³å®šæ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¹¶å°†å…¶æ³›åŒ–è¯¯å·®åˆ†è§£ä¸ºå€¾å‘æ€§æ¨¡ç³Šï¼ˆPropensity Ambiguityï¼‰å’Œç»Ÿè®¡ä¸ç¨³å®šæ€§ï¼ˆStatistical Instabilityï¼‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å¯¹æŠ—æŸå¤±å‡½æ•°ï¼Œç»“åˆåˆ†å¸ƒç¨³å¥ä¼˜åŒ–ï¼ˆDistributionally Robust Optimization, DROï¼‰æ¥åº”å¯¹å€¾å‘æ€§ä¸ç¡®å®šæ€§ï¼Œå¹¶é‡‡ç”¨åŸºäºåŠ æƒ Rademacher Complexity çš„æƒé‡æ­£åˆ™åŒ–æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆåŠçœŸå®æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å› æœæ¨ç†æ–¹æ³•ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "The Version of Record of this contribution is published in the Neural Information Processing, ICONIP 2025 Proceedings and is available online at https://doi.org/10.1007/978-981-95-4094-5_19",
      "pdf_url": "https://arxiv.org/pdf/2505.17717v2",
      "published_date": "2025-05-23 10:34:28 UTC",
      "updated_date": "2025-11-20 14:08:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:28:08.278284+00:00"
    },
    {
      "arxiv_id": "2505.17714v1",
      "title": "PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization",
      "title_zh": "PPO-BRï¼šé¢å‘ç½®ä¿¡åŸŸç­–ç•¥ä¼˜åŒ–çš„åŒä¿¡å·ç†µ-å¥–åŠ±è‡ªé€‚åº”",
      "authors": [
        "Ben Rahman"
      ],
      "abstract": "Despite Proximal Policy Optimization (PPO) dominating policy gradient methods -- from robotic control to game AI -- its static trust region forces a brittle trade-off: aggressive clipping stifles early exploration, while late-stage updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive RL by fusing exploration and convergence signals into a single bounded trust region -- a theoretically grounded innovation that outperforms five SOTA baselines with less than 2% overhead. This work bridges a critical gap in phase-aware learning, enabling real-world deployment in safety-critical systems like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1% faster convergence by combining: (1) entropy-driven expansion (epsilon up) for exploration in high-uncertainty states, and (2) reward-guided contraction (epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo, Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001), 2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with only five lines of code change. PPO-BR's simplicity and theoretical guarantees make it ready-to-deploy in safety-critical domains -- from surgical robotics to autonomous drones. In contrast to recent methods such as Group Relative Policy Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism applicable to both language models and general reinforcement learning environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PPO-BRï¼Œä¸€ç§é’ˆå¯¹ä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ– (Trust Region Policy Optimization) çš„åŒä¿¡å·ç†µ-å¥–åŠ±è‡ªé€‚åº”æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³ Proximal Policy Optimization (PPO) åœ¨é™æ€ä¿¡ä»»åŒºåŸŸä¸‹æ¢ç´¢ä¸æ”¶æ•›ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆç†µé©±åŠ¨çš„æ‰©å¼  (entropy-driven expansion) æ¥å¢å¼ºé«˜ä¸ç¡®å®šçŠ¶æ€ä¸‹çš„æ¢ç´¢ï¼Œä»¥åŠå¥–åŠ±å¼•å¯¼çš„æ”¶ç¼© (reward-guided contraction) æ¥ç¡®ä¿åæœŸæ”¶æ•›çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ MuJoCo å’Œ Atari ç­‰å¤šç§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPPO-BR çš„æ”¶æ•›é€Ÿåº¦æ¯” PPO å¿« 29.1%ï¼Œå¥–åŠ±æ–¹å·®é™ä½äº† 2.3 å€ï¼Œä¸”è¿è¡Œå¼€é”€ä»…å¢åŠ ä¸åˆ° 1.8%ã€‚å‡­å€Ÿå…¶ç®€å•æ˜“ç”¨å’Œæå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼ŒPPO-BR ä¸ºæ‰‹æœ¯æœºå™¨äººå’Œè‡ªä¸»æ— äººæœºç­‰å®‰å…¨å…³é”®é¢†åŸŸçš„å¼ºåŒ–å­¦ä¹ åº”ç”¨æä¾›äº†æ›´å¯é çš„éƒ¨ç½²æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "This manuscript builds upon an earlier version posted to TechRxiv. This arXiv version includes an updated comparison with GRPO (Group Relative Policy Optimization)",
      "pdf_url": "https://arxiv.org/pdf/2505.17714v1",
      "published_date": "2025-05-23 10:30:58 UTC",
      "updated_date": "2025-05-23 10:30:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:28:36.754354+00:00"
    },
    {
      "arxiv_id": "2505.17705v1",
      "title": "CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models",
      "title_zh": "CIKTï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ååŒè¿­ä»£çŸ¥è¯†è¿½è¸ªæ¡†æ¶",
      "authors": [
        "Runze Li",
        "Siyu Wu",
        "Jun Wang",
        "Wei Zhang"
      ],
      "abstract": "Knowledge Tracing (KT) aims to model a student's learning state over time and predict their future performance. However, traditional KT methods often face challenges in explainability, scalability, and effective modeling of complex knowledge dependencies. While Large Language Models (LLMs) present new avenues for KT, their direct application often struggles with generating structured, explainable student representations and lacks mechanisms for continuous, task-specific refinement. To address these gaps, we propose Collaborative Iterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance both prediction accuracy and explainability. CIKT employs a dual-component architecture: an Analyst generates dynamic, explainable user profiles from student historical responses, and a Predictor utilizes these profiles to forecast future performance. The core of CIKT is a synergistic optimization loop. In this loop, the Analyst is iteratively refined based on the predictive accuracy of the Predictor, which conditions on the generated profiles, and the Predictor is subsequently retrained using these enhanced profiles. Evaluated on multiple educational datasets, CIKT demonstrates significant improvements in prediction accuracy, offers enhanced explainability through its dynamically updated user profiles, and exhibits improved scalability. Our work presents a robust and explainable solution for advancing knowledge tracing systems, effectively bridging the gap between predictive performance and model transparency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CIKTï¼Œä¸€ç§åˆ©ç”¨ Large Language Models (LLMs) å¢å¼ºé¢„æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§çš„ååŒè¿­ä»£ Knowledge Tracing (KT) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒç»„ä»¶æ¶æ„ï¼šAnalyst è´Ÿè´£ä»å­¦ç”Ÿå†å²è¡¨ç°ä¸­ç”ŸæˆåŠ¨æ€ä¸”å¯è§£é‡Šçš„ç”¨æˆ·ç”»åƒï¼Œè€Œ Predictor åˆ™åˆ©ç”¨è¿™äº›ç”»åƒé¢„æµ‹æœªæ¥è¡¨ç°ã€‚CIKT çš„æ ¸å¿ƒåœ¨äºååŒä¼˜åŒ–å¾ªç¯ï¼Œå³æ ¹æ® Predictor çš„å‡†ç¡®æ€§è¿­ä»£æ”¹è¿› Analystï¼Œå¹¶ä½¿ç”¨å¢å¼ºåçš„ç”»åƒé‡æ–°è®­ç»ƒ Predictorã€‚åœ¨å¤šä¸ªæ•™è‚²æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCIKT æ˜¾è‘—æé«˜äº†é¢„æµ‹å‡†ç¡®ç‡ï¼Œå¹¶å‡­å€ŸåŠ¨æ€æ›´æ–°çš„ç”»åƒæå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸æ‰©å±•æ€§ã€‚è¯¥å·¥ä½œæœ‰æ•ˆå¼¥åˆäº†é¢„æµ‹æ€§èƒ½ä¸æ¨¡å‹é€æ˜åº¦ä¹‹é—´çš„å·®è·ï¼Œä¸ºæ„å»ºé²æ£’ä¸”å¯è§£é‡Šçš„æ•™è‚²ç³»ç»Ÿæä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17705v1",
      "published_date": "2025-05-23 10:16:16 UTC",
      "updated_date": "2025-05-23 10:16:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:28:09.980009+00:00"
    },
    {
      "arxiv_id": "2505.17702v2",
      "title": "Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek",
      "title_zh": "Seek-CADï¼šåŸºäº DeepSeek æœ¬åœ°æ¨ç†çš„ä¸‰ç»´å‚æ•°åŒ– CAD è‡ªç²¾ç›Šç”Ÿæˆå¼å»ºæ¨¡",
      "authors": [
        "Xueyang Li",
        "Jiahao Li",
        "Yu Song",
        "Yunzhong Lou",
        "Xiangdong Zhou"
      ],
      "abstract": "The advent of Computer-Aided Design (CAD) generative modeling will significantly transform the design of industrial products. The recent research endeavor has extended into the realm of Large Language Models (LLMs). In contrast to fine-tuning methods, training-free approaches typically utilize the advanced closed-source LLMs, thereby offering enhanced flexibility and efficiency in the development of AI agents for generating CAD parametric models. However, the substantial cost and limitations of local deployment of the top-tier closed-source LLMs pose challenges in practical applications. The Seek-CAD is the pioneer exploration of locally deployed open-source inference LLM DeepSeek-R1 for CAD parametric model generation with a training-free methodology. This study is the first investigation to incorporate both visual and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for generating CAD models. Specifically, the initial generated parametric CAD model is rendered into a sequence of step-wise perspective images, which are subsequently processed by a Vision Language Model (VLM) alongside the corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation. Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated model for the next round of generation. Moreover, we present an innovative 3D CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and Refinements) triple design paradigm. This dataset encompasses a wide range of CAD commands, thereby aligning effectively with industrial application requirements and proving suitable for the generation of LLMs. Extensive experiments validate the effectiveness of Seek-CAD under various metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Seek-CADï¼Œè¿™æ˜¯é¦–ä¸ªæ¢ç´¢åˆ©ç”¨æœ¬åœ°éƒ¨ç½²çš„å¼€æºæ¨ç†å¤§è¯­è¨€æ¨¡å‹DeepSeek-R1ï¼Œé€šè¿‡å…è®­ç»ƒ(training-free)æ–¹æ³•ç”Ÿæˆ3Då‚æ•°åŒ–CADæ¨¡å‹çš„å·¥ä½œã€‚è¯¥æ¡†æ¶åœ¨è‡ªæˆ‘ä¼˜åŒ–(self-refinement)æœºåˆ¶ä¸­é¦–æ¬¡ç»“åˆäº†è§†è§‰å’Œé“¾å¼æ€ç»´(Chain-of-Thought, CoT)åé¦ˆï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLM)å¯¹æ¸²æŸ“çš„æ­¥éª¤å›¾åƒè¿›è¡Œè¯„ä¼°å¹¶æŒ‡å¯¼DeepSeek-R1è¿›è¡Œè¿­ä»£ä¿®æ­£ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¨å‡ºäº†åŸºäºSSR(Sketch, Sketch-based feature, and Refinements)ä¸‰é‡è®¾è®¡èŒƒå¼çš„åˆ›æ–°3D CADæ•°æ®é›†ï¼Œä½¿å…¶æ›´è´´åˆå·¥ä¸šåº”ç”¨éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSeek-CADåœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†è¯¥æœ¬åœ°åŒ–ç”Ÿæˆæ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17702v2",
      "published_date": "2025-05-23 10:11:19 UTC",
      "updated_date": "2025-05-29 07:35:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:28:25.844480+00:00"
    },
    {
      "arxiv_id": "2505.17701v3",
      "title": "COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection",
      "title_zh": "COUNTDOWNï¼šå‰”é™¤ä¸‹æŠ•å½±ä¸­éå¿…è¦æƒé‡çš„ä¸Šä¸‹æ–‡ç¨€ç–æ¿€æ´»",
      "authors": [
        "Jaewon Cheon",
        "Pilsung Kang"
      ],
      "abstract": "The growing size of large language models has created significant computational inefficiencies. To address this challenge, sparse activation methods selectively deactivates non-essential parameters during inference, reducing computational costs in FFNN layers. While existing methods focus on non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN layer lies globally in the form of a linear combination over its internal down projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN, leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct coefficients of the linear combination. Experimental results demonstrate that D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5% ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4% better performance preservation compared to existing methods. Our specialized kernel implementations effectively realize these theoretical gains into substantial real-world acceleration.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹è®¡ç®—æ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º COUNTDOWN çš„ç¨€ç–æ¿€æ´»(Sparse Activation)æ–¹æ³•ï¼Œå…¶åŸºäºå‰é¦ˆç¥ç»ç½‘ç»œ(FFNN)å±‚åœ¨ä¸‹æŠ•å½±(Down Projection)çŸ©é˜µçš„çº¿æ€§ç»„åˆä¸­å­˜åœ¨å…¨å±€ç¨€ç–æ€§çš„å‡è®¾ã€‚ç ”ç©¶è®¾è®¡äº†ä¸¤ç§å…·ä½“æ–¹æ¡ˆï¼šåˆ©ç”¨é—´æ¥ç³»æ•°çš„ M-COUNTDOWN å’Œåˆ©ç”¨ç›´æ¥ç³»æ•°çš„ D-COUNTDOWNã€‚å®éªŒè¯æ˜ï¼ŒD-COUNTDOWN èƒ½åœ¨ä»…æŸå¤± 5.5% æ€§èƒ½çš„æƒ…å†µä¸‹å‰Šå‡ 90% çš„è®¡ç®—é‡ï¼Œè€Œ M-COUNTDOWN ä½œä¸ºä¸€ç§æ— éœ€é¢„æµ‹å™¨(Predictor-free)çš„æ–¹æ¡ˆï¼Œå…¶æ€§èƒ½ä¿ç•™æ•ˆæœæ¯”ç°æœ‰æ–¹æ³•æå‡äº†é«˜è¾¾ 29.4%ã€‚æœ€åï¼Œé€šè¿‡ä¸“é—¨çš„å†…æ ¸(Kernel)å®ç°ï¼Œè¯¥æ–¹æ³•æˆåŠŸå°†ç†è®ºè®¡ç®—ä¼˜åŠ¿è½¬åŒ–ä¸ºäº†æ˜¾è‘—çš„ç°å®åŠ é€Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "EMNLP 2025 (Main Track)",
      "pdf_url": "https://arxiv.org/pdf/2505.17701v3",
      "published_date": "2025-05-23 10:10:22 UTC",
      "updated_date": "2025-10-27 08:19:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:28:34.247667+00:00"
    },
    {
      "arxiv_id": "2505.17696v5",
      "title": "Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory",
      "title_zh": "æå‡äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„éŸ§æ€§ï¼šåŸºäºæ§åˆ¶ç†è®ºçš„ LSTM éŸ§æ€§å½¢å¼åŒ–åŠå…¶ç†è®ºä¿è¯",
      "authors": [
        "Sota Yoshihara",
        "Ryosuke Yamamoto",
        "Hiroyuki Kusumoto",
        "Masanari Shimura"
      ],
      "abstract": "This paper proposes a novel theoretical framework for guaranteeing and evaluating the resilience of long short-term memory (LSTM) networks in control systems. We introduce \"recovery time\" as a new metric of resilience in order to quantify the time required for an LSTM to return to its normal state after anomalous inputs. By mathematically refining incremental input-to-state stability ($Î´$ISS) theory for LSTM, we derive a practical data-independent upper bound on recovery time. This upper bound gives us resilience-aware training. Experimental validation on simple models demonstrates the effectiveness of our resilience estimation and control methods, enhancing a foundation for rigorous quality assurance in safety-critical AI applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºä¿éšœå’Œè¯„ä¼°æ§åˆ¶ç³»ç»Ÿä¸­ LSTM ç½‘ç»œéŸ§æ€§ï¼ˆresiliencyï¼‰çš„æ–°å‹ç†è®ºæ¡†æ¶ã€‚ç ”ç©¶å¼•å…¥äº†â€œrecovery timeâ€ï¼ˆæ¢å¤æ—¶é—´ï¼‰ä½œä¸ºé‡åŒ– LSTM åœ¨å¼‚å¸¸è¾“å…¥åæ¢å¤æ­£å¸¸çŠ¶æ€èƒ½åŠ›çš„æ€§èƒ½æŒ‡æ ‡ã€‚é€šè¿‡å¯¹å¢é‡è¾“å…¥åˆ°çŠ¶æ€ç¨³å®šæ€§ï¼ˆincremental input-to-state stability, $\\delta$ISSï¼‰ç†è®ºè¿›è¡Œæ•°å­¦ä¼˜åŒ–ï¼Œç ”ç©¶æ¨å¯¼å‡ºäº† recovery time çš„æ•°æ®æ— å…³ä¸Šç•Œï¼Œå¹¶ä»¥æ­¤å®ç°äº†éŸ§æ€§æ„ŸçŸ¥è®­ç»ƒï¼ˆresilience-aware trainingï¼‰ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥éŸ§æ€§è¯„ä¼°ä¸æ§åˆ¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸º safety-critical äººå·¥æ™ºèƒ½åº”ç”¨çš„è´¨é‡ä¿è¯å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 6 figures. Appendix: 16 pages. First three listed authors have equal contributions",
      "pdf_url": "https://arxiv.org/pdf/2505.17696v5",
      "published_date": "2025-05-23 10:05:26 UTC",
      "updated_date": "2025-08-05 02:29:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:28:40.742299+00:00"
    },
    {
      "arxiv_id": "2505.17695v1",
      "title": "SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data",
      "title_zh": "SynRESï¼šé€šè¿‡åˆæˆæ•°æ®è¿ˆå‘çœŸå®åœºæ™¯ä¸‹çš„æŒ‡ä»£æ€§è¡¨è¾¾åˆ†å‰²",
      "authors": [
        "Dong-Hee Kim",
        "Hyunjee Song",
        "Donghyun Kim"
      ],
      "abstract": "Despite the advances in Referring Expression Segmentation (RES) benchmarks, their evaluation protocols remain constrained, primarily focusing on either single targets with short queries (containing minimal attributes) or multiple targets from distinctly different queries on a single domain. This limitation significantly hinders the assessment of more complex reasoning capabilities in RES models. We introduce WildRES, a novel benchmark that incorporates long queries with diverse attributes and non-distinctive queries for multiple targets. This benchmark spans diverse application domains, including autonomous driving environments and robotic manipulation scenarios, thus enabling more rigorous evaluation of complex reasoning capabilities in real-world settings. Our analysis reveals that current RES models demonstrate substantial performance deterioration when evaluated on WildRES. To address this challenge, we introduce SynRES, an automated pipeline generating densely paired compositional synthetic training data through three innovations: (1) a dense caption-driven synthesis for attribute-rich image-mask-expression triplets, (2) reliable semantic alignment mechanisms rectifying caption-pseudo mask inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware augmentations incorporating mosaic composition and superclass replacement to emphasize generalization ability and distinguishing attributes over object categories. Experimental results demonstrate that models trained with SynRES achieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and 3.8% on WildRES-DS. Code and datasets are available at https://github.com/UTLLab/SynRES.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŒ‡ä»£æ€§è¡¨è¾¾åˆ†å‰²(Referring Expression Segmentation, RES)ç°æœ‰åŸºå‡†åœ¨å¤æ‚æ¨ç†å’Œå¤šé¢†åŸŸåœºæ™¯ä¸‹çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«é•¿æŸ¥è¯¢å’Œå¤šæ ·åŒ–å±æ€§çš„æ–°åŸºå‡† WildRESã€‚ä¸ºæå‡æ¨¡å‹åœ¨çœŸå®é‡å¤–ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œä½œè€…å¼€å‘äº† SynRES è‡ªåŠ¨æµæ°´çº¿ï¼Œé€šè¿‡å¯†é›†æè¿°é©±åŠ¨åˆæˆã€å›¾åƒ-æ–‡æœ¬å¯¹é½åˆ†ç»„(Image-Text Aligned Grouping)è¯­ä¹‰å¯¹é½ä»¥åŠåŸŸæ„ŸçŸ¥å¢å¼ºæŠ€æœ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç»„åˆå¼åˆæˆè®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ SynRES è®­ç»ƒçš„æ¨¡å‹åœ¨ WildRES åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº† gIoU æŒ‡æ ‡å¹¶è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæ“ä½œç­‰å¤æ‚ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17695v1",
      "published_date": "2025-05-23 10:05:16 UTC",
      "updated_date": "2025-05-23 10:05:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:29:13.437983+00:00"
    },
    {
      "arxiv_id": "2505.17692v3",
      "title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection",
      "title_zh": "ViP$^2$-CLIPï¼šèåˆç»Ÿä¸€å¯¹é½è§†è§‰æ„ŸçŸ¥æç¤ºçš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Ziteng Yang",
        "Jingzehua Xu",
        "Yanshu Li",
        "Zepeng Li",
        "Yeqiang Wang",
        "Xinghui Li"
      ],
      "abstract": "Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any target domain training samples, relying solely on external auxiliary data. Existing CLIP-based methods attempt to activate the model's ZSAD potential via handcrafted or static learnable prompts. The former incur high engineering costs and limited semantic coverage, whereas the latter apply identical descriptions across diverse anomaly types, thus fail to adapt to complex variations. Furthermore, since CLIP is originally pretrained on large-scale classification tasks, its anomaly segmentation quality is highly sensitive to the exact wording of class names, severely constraining prompting strategies that depend on class labels. To address these challenges, we introduce ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local visual context to adaptively generate fine-grained textual prompts, eliminating manual templates and class-name priors. This design enables our model to focus on precise abnormal regions, making it particularly valuable when category labels are ambiguous or privacy-constrained. Extensive experiments on 15 industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves state-of-the-art performance and robust cross-domain generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ViP$^2$-CLIPï¼Œæ—¨åœ¨è§£å†³é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZero-shot anomaly detection, ZSADï¼‰ä¸­æ‰‹å·¥æç¤ºå·¥ç¨‹æˆæœ¬é«˜åŠæ¨¡å‹å¯¹ç±»åˆ«æ ‡ç­¾è¿‡åº¦ä¾èµ–ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è§†è§‰æ„ŸçŸ¥æç¤ºï¼ˆVisual-Perception Prompting, ViP-Promptï¼‰æœºåˆ¶ï¼Œé€šè¿‡èåˆå…¨å±€å’Œå¤šå°ºåº¦å±€éƒ¨è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå®ç°è‡ªé€‚åº”ç”Ÿæˆç»†ç²’åº¦æ–‡æœ¬æç¤ºï¼Œä»è€Œæ‘†è„±äº†å¯¹æ‰‹åŠ¨æ¨¡æ¿å’Œç±»åˆ«å…ˆéªŒçš„ä¾èµ–ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨ç±»åˆ«æ ‡ç­¾æ¨¡ç³Šæˆ–å­˜åœ¨éšç§é™åˆ¶çš„æƒ…å¢ƒä¸‹ï¼Œä»èƒ½ç²¾å‡†èšç„¦å¼‚å¸¸åŒºåŸŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒViP$^2$-CLIP åœ¨15ä¸ªå·¥ä¸šå’ŒåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„ï¼ˆstate-of-the-artï¼‰æ°´å¹³ï¼Œå¹¶å±•ç¤ºå‡ºå“è¶Šçš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17692v3",
      "published_date": "2025-05-23 10:01:11 UTC",
      "updated_date": "2025-10-06 15:27:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:28:45.697676+00:00"
    },
    {
      "arxiv_id": "2505.17683v2",
      "title": "Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection",
      "title_zh": "é¢å‘ IVH æ£€æµ‹ä¸­ç²¾å‡†è„‘éƒ¨è¶…å£°åˆ†å‰²çš„åŒé‡æ³¨æ„åŠ›æ®‹å·® U-Net",
      "authors": [
        "Dan Yuan",
        "Yi Feng",
        "Ziyun Tang"
      ],
      "abstract": "Intraventricular hemorrhage (IVH) is a severe neurological complication among premature infants, necessitating early and accurate detection from brain ultrasound (US) images to improve clinical outcomes. While recent deep learning methods offer promise for computer-aided diagnosis, challenges remain in capturing both local spatial details and global contextual dependencies critical for segmenting brain anatomies. In this work, we propose an enhanced Residual U-Net architecture incorporating two complementary attention mechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse Attention Layer (SAL). The CBAM improves the model's ability to refine spatial and channel-wise features, while the SAL introduces a dual-branch design, sparse attention filters out low-confidence query-key pairs to suppress noise, and dense attention ensures comprehensive information propagation. Extensive experiments on the Brain US dataset demonstrate that our method achieves state-of-the-art segmentation performance, with a Dice score of 89.04% and IoU of 81.84% for ventricle region segmentation. These results highlight the effectiveness of integrating spatial refinement and attention sparsity for robust brain anatomy detection. Code is available at: https://github.com/DanYuan001/BrainImgSegment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºçš„ Residual U-Net æ¶æ„ï¼Œæ—¨åœ¨æå‡æ—©äº§å„¿è„‘å®¤å†…å‡ºè¡€ (IVH) æ£€æµ‹ä¸­è„‘éƒ¨è¶…å£° (Brain Ultrasound) å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œè§£å†³äº†å±€éƒ¨ç©ºé—´ç»†èŠ‚ä¸å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–æ•æ‰çš„éš¾é¢˜ã€‚è¯¥æ¶æ„å¼•å…¥äº†ä¸¤ç§äº’è¡¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼šå·ç§¯å—æ³¨æ„åŠ›æ¨¡å— (CBAM) ç”¨äºä¼˜åŒ–ç©ºé—´å’Œé€šé“ç‰¹å¾ï¼Œç¨€ç–æ³¨æ„åŠ›å±‚ (SAL) åˆ™é€šè¿‡åŒåˆ†æ”¯è®¾è®¡è¿‡æ»¤å™ªå£°å¹¶ç¡®ä¿ä¿¡æ¯çš„å…¨é¢ä¼ æ’­ã€‚åœ¨ Brain US æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è„‘å®¤åŒºåŸŸåˆ†å‰²ä¸­å–å¾—äº† 89.04% çš„ Dice score å’Œ 81.84% çš„ IoUï¼Œè¾¾åˆ°äº† SOTA æ°´å¹³ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆç©ºé—´ç²¾ç‚¼ä¸æ³¨æ„åŠ›ç¨€ç–åŒ–åœ¨ç¨³å¥è„‘éƒ¨è§£å‰–ç»“æ„æ£€æµ‹ä¸­çš„æ˜¾è‘—æ•ˆæœã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages,6 figures and 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.17683v2",
      "published_date": "2025-05-23 09:53:57 UTC",
      "updated_date": "2025-06-10 04:20:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:28:56.250261+00:00"
    },
    {
      "arxiv_id": "2505.17682v1",
      "title": "Tuning Language Models for Robust Prediction of Diverse User Behaviors",
      "title_zh": "é¢å‘å¤šæ ·åŒ–ç”¨æˆ·è¡Œä¸ºé²æ£’é¢„æµ‹çš„è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Fanjin Meng",
        "Jingtao Ding",
        "Jiahui Gong",
        "Chen Yang",
        "Hong Chen",
        "Zuojian Wang",
        "Haisheng Lu",
        "Yong Li"
      ],
      "abstract": "Predicting user behavior is essential for intelligent assistant services, yet deep learning models often struggle to capture long-tailed behaviors. Large language models (LLMs), with their pretraining on vast corpora containing rich behavioral knowledge, offer promise. However, existing fine-tuning approaches tend to overfit to frequent ``anchor'' behaviors, reducing their ability to predict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM, a progressive fine-tuning approach that addresses this issue. In the first stage, LLMs are fine-tuned on anchor behaviors while preserving general behavioral knowledge. In the second stage, fine-tuning uses a balanced subset of all behaviors based on sample difficulty to improve tail behavior predictions without sacrificing anchor performance. Experimental results on two real-world datasets demonstrate that BehaviorLM robustly predicts both anchor and tail behaviors and effectively leverages LLM behavioral knowledge to master tail behavior prediction with few-shot examples.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨é¢„æµ‹ç”¨æˆ·è¡Œä¸ºæ—¶å®¹æ˜“è¿‡åº¦æ‹Ÿåˆé¢‘ç¹çš„ \"anchor\" è¡Œä¸ºã€è€Œå¿½ç•¥é•¿å°¾ \"tail\" è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº† BehaviorLM æ¸è¿›å¼å¾®è°ƒ (progressive fine-tuning) æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒè¿‡ç¨‹ï¼Œå…ˆåœ¨ä¿ç•™é€šç”¨çŸ¥è¯†çš„å‰æä¸‹å­¦ä¹  anchor è¡Œä¸ºï¼Œå†åŸºäºæ ·æœ¬éš¾åº¦ (sample difficulty) ä½¿ç”¨å¹³è¡¡å­é›†æ¥æå‡å¯¹ tail è¡Œä¸ºçš„æ•æ‰èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒBehaviorLM èƒ½å¤Ÿç¨³å¥åœ°é¢„æµ‹å¤šæ ·åŒ–ç”¨æˆ·è¡Œä¸ºï¼Œå¹¶èƒ½åœ¨ few-shot åœºæ™¯ä¸‹æ˜¾è‘—æå‡å¯¹ tail è¡Œä¸ºçš„é¢„æµ‹ç²¾åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17682v1",
      "published_date": "2025-05-23 09:53:43 UTC",
      "updated_date": "2025-05-23 09:53:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:30:09.093878+00:00"
    },
    {
      "arxiv_id": "2505.17673v1",
      "title": "Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution",
      "title_zh": "é‡æ–°å®¡è§†æ™ºèƒ½ä½“è®¾è®¡ï¼šä»è‡ªé¡¶å‘ä¸‹çš„å·¥ä½œæµåˆ°è‡ªåº•å‘ä¸Šçš„æŠ€èƒ½æ¼”åŒ–",
      "authors": [
        "Jiawei Du",
        "Jinlong Wu",
        "Yuzheng Chen",
        "Yucheng Hu",
        "Bing Li",
        "Joey Tianyi Zhou"
      ],
      "abstract": "Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose tasks, define workflows, and assign agents to execute each step. While effective on benchmark-style tasks, such systems rely on designer updates and overlook agents' potential to learn from experience. Recently, Silver and Sutton(2025) envision a shift into a new era, where agents could progress from a stream of experiences. In this paper, we instantiate this vision of experience-driven learning by introducing a bottom-up agent paradigm that mirrors the human learning process. Agents acquire competence through a trial-and-reasoning mechanism-exploring, reflecting on outcomes, and abstracting skills over time. Once acquired, skills can be rapidly shared and extended, enabling continual evolution rather than static replication. As more agents are deployed, their diverse experiences accelerate this collective process, making bottom-up design especially suited for open-ended environments. We evaluate this paradigm in Slay the Spire and Civilization V, where agents perceive through raw visual inputs and act via mouse outputs, the same as human players. Using a unified, game-agnostic codebase without any game-specific prompts or privileged APIs, our bottom-up agents acquire skills entirely through autonomous interaction, demonstrating the potential of the bottom-up paradigm in complex, real-world environments. Our code is available at https://github.com/AngusDujw/Bottom-Up-Agent.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®å‰ LLM æ™ºèƒ½ä½“æ¡†æ¶è¿‡åº¦ä¾èµ–äººå·¥é¢„è®¾ Top-Down å·¥ä½œæµä¸”ç¼ºä¹è‡ªä¸»è¿›åŒ–èƒ½åŠ›çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§ç”±ç»éªŒé©±åŠ¨çš„ Bottom-Up æ™ºèƒ½ä½“èŒƒå¼ã€‚è¯¥èŒƒå¼æ ¸å¿ƒåœ¨äºè¯•é”™ä¸æ¨ç†æœºåˆ¶ (Trial-and-reasoning mechanism)ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡è‡ªä¸»æ¢ç´¢ä¸åæ€æ¥æŠ½è±¡æŠ€èƒ½ï¼Œå¹¶å®ç°æŠ€èƒ½çš„æŒç»­æ¼”è¿›ä¸é›†ä½“å…±äº«ã€‚åœ¨ã€Šæ€æˆ®å°–å¡”ã€‹ (Slay the Spire) å’Œã€Šæ–‡æ˜Vã€‹ (Civilization V) çš„æµ‹è¯•ä¸­ï¼Œæ™ºèƒ½ä½“ä»…é€šè¿‡åŸå§‹è§†è§‰è¾“å…¥å’Œæ¨¡æ‹Ÿé¼ æ ‡æ“ä½œï¼Œåœ¨æ— æ¸¸æˆç‰¹å®šæç¤ºçš„æƒ…å†µä¸‹æˆåŠŸä¹ å¾—å¤æ‚æŠ€èƒ½ã€‚å®éªŒç»“æœéªŒè¯äº† Bottom-Up è®¾è®¡åœ¨å¼€æ”¾å¼ã€å¤æ‚çœŸå®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†æ™ºèƒ½ä½“ä»ç»éªŒæµä¸­è‡ªä¸»å­¦ä¹ çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17673v1",
      "published_date": "2025-05-23 09:38:55 UTC",
      "updated_date": "2025-05-23 09:38:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:29:57.585004+00:00"
    },
    {
      "arxiv_id": "2505.18223v2",
      "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis",
      "title_zh": "IDA-Benchï¼šäº¤äº’å¼å¼•å¯¼æ•°æ®åˆ†æä¸­çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Hanyu Li",
        "Haoyu Liu",
        "Tingyu Zhu",
        "Tianyu Guo",
        "Zeyu Zheng",
        "Xiaotie Deng",
        "Michael I. Jordan"
      ],
      "abstract": "Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with deeper insights of the dataset. To address this, we introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round interactive scenarios. Derived from complex Kaggle notebooks, tasks are presented as sequential natural language instructions by an LLM-simulated user. Agent performance is judged by comparing its final numerical output to the human-derived baseline. Initial results show that even state-of-the-art coding agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting limitations not evident in single-turn tests. This work underscores the need to improve LLMs' multi-round capabilities for building more reliable data analysis agents, highlighting the necessity of achieving a balance between instruction following and reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IDA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤šè½®äº¤äº’åœºæ™¯ä¸‹è¿›è¡Œæ•°æ®åˆ†æèƒ½åŠ›çš„å…¨æ–°åŸºå‡†ï¼Œå¼¥è¡¥äº†ç°æœ‰æµ‹è¯•å¿½è§†æ•°æ®åˆ†æè¿­ä»£ç‰¹æ€§ (iterative nature) çš„ç¼ºé™·ã€‚IDA-Bench åŸºäºå¤æ‚çš„ Kaggle ç«èµ›ä»£ç ï¼Œé€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·å‘å‡ºè¿ç»­æŒ‡ä»¤ï¼Œå¹¶ä»¥æœ€ç»ˆæ•°å€¼è¾“å‡ºä¸äººç±»åŸºå‡† (human-derived baseline) çš„å¯¹æ¯”ä½œä¸ºè¯„åˆ¤æ ‡å‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ Claude-3.7-thinking ç­‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šä¹Ÿä½äº 50%ï¼Œæš´éœ²äº†å•è½®æµ‹è¯• (single-turn tests) æ— æ³•å¯Ÿè§‰çš„å±€é™æ€§ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†æå‡ LLM æ™ºèƒ½ä½“åœ¨å¤šè½®äº¤äº’ä¸­å¹³è¡¡æŒ‡ä»¤éµå¾ª (instruction following) ä¸æ¨ç† (reasoning) èƒ½åŠ›çš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºæ›´å¯é çš„è‡ªåŠ¨åŒ–æ•°æ®åˆ†æå·¥å…·æä¾›äº†æŒ‡å¼•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18223v2",
      "published_date": "2025-05-23 09:37:52 UTC",
      "updated_date": "2025-06-06 06:33:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:29:30.438125+00:00"
    },
    {
      "arxiv_id": "2505.17670v2",
      "title": "Towards General Continuous Memory for Vision-Language Models",
      "title_zh": "è¿ˆå‘è§†è§‰è¯­è¨€æ¨¡å‹çš„é€šç”¨è¿ç»­è®°å¿†",
      "authors": [
        "Wenyi Wu",
        "Zixuan Song",
        "Kun Zhou",
        "Yifei Shao",
        "Zhiting Hu",
        "Biwei Huang"
      ],
      "abstract": "Language models (LMs) and their extension, vision-language models (VLMs), have achieved remarkable performance across various tasks. However, they still struggle with complex reasoning tasks that require multimodal or multilingual real-world knowledge. To support such capabilities, an external memory system that can efficiently provide relevant multimodal information is essential. Existing approaches generally concatenate image and text tokens into a long sequence as memory, which, however, may drastically increase context length and even degrade performance. In contrast, we propose using continuous memory, a compact set of dense embeddings to more effectively and efficiently represent multimodal and multilingual knowledge. Our key insight is that a VLM can serve as its own continuous memory encoder. We empirically show that this design improves performance on complex multimodal reasoning tasks. Building on this, we introduce a data-efficient and parameter-efficient method to fine-tune the VLM into a memory encoder, requiring only 1.2% of the model's parameters and a small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes VLM's original capabilities to encode arbitrary multimodal and multilingual knowledge into just 8 continuous embeddings. Since the inference-time VLM remains frozen, our memory module is plug-and-play and can be flexibly integrated as needed. Extensive experiments across eight multimodal reasoning benchmarks demonstrate the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoMEMï¼Œä¸€ç§æ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å¤„ç†å¤æ‚å¤šæ¨¡æ€å’Œå¤šè¯­è¨€æ¨ç†èƒ½åŠ›çš„é€šç”¨è¿ç»­è®°å¿†(Continuous Memory)æ¡†æ¶ã€‚CoMEMçš„æ ¸å¿ƒè§è§£æ˜¯å°†VLMè‡ªèº«ä½œä¸ºè®°å¿†ç¼–ç å™¨ï¼Œå°†ä»»æ„çŸ¥è¯†é«˜æ•ˆå‹ç¼©ä¸ºä»…8ä¸ªè¿ç»­åµŒå…¥(embeddings)ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿé•¿åºåˆ—è®°å¿†å¸¦æ¥çš„ä¸Šä¸‹æ–‡å†—ä½™å’Œæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚é€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œè¯¥æ–¹æ³•ä»…éœ€1.2%çš„å‚æ•°æ›´æ–°å’Œ1.56ä¸‡ä¸ªè‡ªåˆæˆæ ·æœ¬å³å¯å®ç°ã€‚å®éªŒè¯æ˜ï¼ŒCoMEMåœ¨å…«é¡¹å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œä¸”å…¶å³æ’å³ç”¨(plug-and-play)çš„ç‰¹æ€§ä½¿å…¶èƒ½çµæ´»é›†æˆåˆ°å„ç§æ¨ç†ä»»åŠ¡ä¸­ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17670v2",
      "published_date": "2025-05-23 09:36:53 UTC",
      "updated_date": "2025-07-07 20:01:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:29:33.936899+00:00"
    },
    {
      "arxiv_id": "2505.17665v1",
      "title": "EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy",
      "title_zh": "EMRA-proxyï¼šåˆ©ç”¨æ³¨æ„åŠ›ä»£ç†å¢å¼ºé¥æ„Ÿå›¾åƒå¤šç±»åŒºåŸŸè¯­ä¹‰åˆ†å‰²",
      "authors": [
        "Yichun Yu",
        "Yuqing Lan",
        "Zhihuan Xing",
        "Xiaoyi Yang",
        "Tingyue Tang",
        "Dan Yu"
      ],
      "abstract": "High-resolution remote sensing (HRRS) image segmentation is challenging due to complex spatial layouts and diverse object appearances. While CNNs excel at capturing local features, they struggle with long-range dependencies, whereas Transformers can model global context but often neglect local details and are computationally expensive.We propose a novel approach, Region-Aware Proxy Network (RAPNet), which consists of two components: Contextual Region Attention (CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely on grid-based layouts, RAPNet operates at the region level for more flexible segmentation. The CRA module uses a Transformer to capture region-level contextual dependencies, generating a Semantic Region Mask (SRM). The GCR module learns a global class attention map to refine multi-class information, combining the SRM and attention map for accurate segmentation.Experiments on three public datasets show that RAPNet outperforms state-of-the-art methods, achieving superior multi-class segmentation accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Region-Aware Proxy Network (RAPNet) çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³é«˜åˆ†è¾¨ç‡é¥æ„Ÿ (HRRS) å›¾åƒåˆ†å‰²ä¸­å¤æ‚ç©ºé—´å¸ƒå±€å’Œç‰©ä½“å¤–è§‚å¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚RAPNet æ‘’å¼ƒäº†ä¼ ç»Ÿçš„ç½‘æ ¼å¸ƒå±€ (grid-based layouts)ï¼Œè½¬è€Œåœ¨åŒºåŸŸçº§åˆ« (region level) è¿›è¡Œæ“ä½œï¼Œé€šè¿‡ä¸Šä¸‹æ–‡åŒºåŸŸæ³¨æ„åŠ› (CRA) å’Œå…¨å±€ç±»åˆ«ç²¾ç»†åŒ– (GCR) ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—æå‡æ€§èƒ½ã€‚CRA æ¨¡å—åˆ©ç”¨ Transformer æ•æ‰åŒºåŸŸçº§ä¸Šä¸‹æ–‡ä¾èµ–å¹¶ç”Ÿæˆè¯­ä¹‰åŒºåŸŸæ©ç  (SRM)ï¼Œè€Œ GCR æ¨¡å—åˆ™é€šè¿‡å…¨å±€ç±»åˆ«æ³¨æ„åŠ›å›¾ä¼˜åŒ–å¤šç±»åˆ«ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¤šç±»åˆ«åŒºåŸŸè¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Proceedings of the 20th International Conference on Intelligent Computing (ICIC 2024): Poster Volume I. Tianjin, China, 2024: 538-562",
      "pdf_url": "https://arxiv.org/pdf/2505.17665v1",
      "published_date": "2025-05-23 09:30:45 UTC",
      "updated_date": "2025-05-23 09:30:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:29:38.716055+00:00"
    },
    {
      "arxiv_id": "2505.17654v3",
      "title": "EVADE-Bench: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications",
      "title_zh": "EVADE-Benchï¼šé’ˆå¯¹ç”µå­å•†åŠ¡åº”ç”¨ä¸­è§„é¿æ€§å†…å®¹æ£€æµ‹çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•",
      "authors": [
        "Ancheng Xu",
        "Zhihao Yang",
        "Jingpeng Li",
        "Guanghu Yuan",
        "Longze Chen",
        "Liang Yan",
        "Jiehui Zhou",
        "Zhen Qin",
        "Hengyu Chang",
        "Hamid Alinejad-Rokny",
        "Min Yang"
      ],
      "abstract": "E-commerce platforms increasingly rely on Large Language Models (LLMs) and Vision-Language Models (VLMs) to detect illicit or misleading product content. However, these models remain vulnerable to evasive content: inputs (text or images) that superficially comply with platform policies while covertly conveying prohibited claims. Unlike traditional adversarial attacks that induce overt failures, evasive content exploits ambiguity and context, making it far harder to detect. Existing robustness benchmarks provide little guidance for this demanding, real-world challenge. We introduce EVADE, the first expert-curated, Chinese, multimodal benchmark specifically designed to evaluate foundation models on evasive content detection in e-commerce. The dataset contains 2,833 annotated text samples and 13,961 images spanning six demanding product categories, including body shaping, height growth, and health supplements. Two complementary tasks assess distinct capabilities: Single-Violation, which probes fine-grained reasoning under short prompts, and All-in-One, which tests long-context reasoning by merging overlapping policy rules into unified instructions. Notably, the All-in-One setting significantly narrows the performance gap between partial and full-match accuracy, suggesting that clearer rule definitions improve alignment between human and model judgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial performance gaps: even state-of-the-art models frequently misclassify evasive samples. By releasing EVADE and strong baselines, we provide the first rigorous standard for evaluating evasive-content detection, expose fundamental limitations in current multimodal reasoning, and lay the groundwork for safer and more transparent content moderation systems in e-commerce. The dataset is publicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† EVADE-Benchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“å®¶ç²¾é€‰çš„é¢å‘ç”µå•†è§„é¿æ€§å†…å®¹ (evasive content) æ£€æµ‹çš„ä¸­æ–‡å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³å½“å‰æ¨¡å‹éš¾ä»¥è¯†åˆ«é€šè¿‡æ¨¡ç³Šå’Œä¸Šä¸‹æ–‡æ‰‹æ®µè§„é¿å®¡æ ¸ç­–ç•¥çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å« 2,833 æ¡æ ‡æ³¨æ–‡æœ¬å’Œ 13,961 å¼ å›¾ç‰‡ï¼Œæ¶µç›–èº«æå¡‘é€ ã€å¢é«˜å’Œä¿å¥å“ç­‰å…­ä¸ªæå…·æŒ‘æˆ˜æ€§çš„å•†å“ç±»åˆ«ã€‚åŸºå‡†æµ‹è¯•é€šè¿‡ Single-Violation å’Œ All-in-One ä¸¤é¡¹ä»»åŠ¡ï¼Œåˆ†åˆ«è¯„ä¼°æ¨¡å‹åœ¨ç»†ç²’åº¦æ¨ç†å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒå¯¹ 26 ç§ä¸»æµ Large Language Models (LLMs) å’Œ Vision-Language Models (VLMs) è¿›è¡Œäº†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯ SOTA æ¨¡å‹åœ¨å¤„ç†è§„é¿æ€§æ ·æœ¬æ—¶ä¹Ÿå­˜åœ¨æ˜¾è‘—å±€é™ã€‚EVADE-Bench çš„å‘å¸ƒä¸ºè¯„ä¼°å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æä¾›äº†ä¸¥è°¨æ ‡å‡†ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ›´å®‰å…¨ã€é€æ˜çš„ç”µå•†å†…å®¹å®¡æ ¸ç³»ç»Ÿçš„æ„å»ºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17654v3",
      "published_date": "2025-05-23 09:18:01 UTC",
      "updated_date": "2026-01-19 20:15:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:30:01.966082+00:00"
    },
    {
      "arxiv_id": "2505.17653v1",
      "title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
      "title_zh": "GeoGramBenchï¼šç°ä»£å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å‡ ä½•ç¨‹åºæ¨ç†åŸºå‡†",
      "authors": [
        "Shixian Luo",
        "Zezhou Zhu",
        "Yu Yuan",
        "Yuncheng Yang",
        "Lianlei Shan",
        "Yong Wu"
      ],
      "abstract": "Geometric spatial reasoning forms the foundation of many applications in artificial intelligence, yet the ability of large language models (LLMs) to operate over geometric spatial information expressed in procedural code remains underexplored. In this paper, we address this gap by formalizing the Program-to-Geometry task, which challenges models to translate programmatic drawing code into accurate and abstract geometric reasoning. To evaluate this capability, we present GeoGramBench, a benchmark of 500 carefully refined problems organized by a tailored three-level taxonomy that considers geometric complexity rather than traditional mathematical reasoning complexity. Our comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced deficiencies: even the most advanced models achieve less than 50% accuracy at the highest abstraction level. These results highlight the unique challenges posed by program-driven spatial reasoning and establish GeoGramBench as a valuable resource for advancing research in symbolic-to-spatial geometric reasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.",
      "tldr_zh": "è¯¥ç ”ç©¶å½¢å¼åŒ–äº† Program-to-Geometry ä»»åŠ¡ï¼Œå¹¶æ¨å‡ºäº† GeoGramBench è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†ç¨‹åºåŒ–ç»˜å›¾ä»£ç ä¸­çš„å‡ ä½•ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å« 500 ä¸ªç»è¿‡ç²¾ç»†å¤„ç†çš„é—®é¢˜ï¼Œå¹¶é‡‡ç”¨äº†ä¸€å¥—åŸºäºå‡ ä½•å¤æ‚åº¦è€Œéä¼ ç»Ÿæ•°å­¦æ¨ç†é€»è¾‘çš„ä¸‰çº§åˆ†ç±»ä½“ç³»ã€‚é€šè¿‡å¯¹ 17 ä¸ªå‰æ²¿ LLMs çš„å…¨é¢è¯„ä¼°å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤„ç†ç¨‹åºé©±åŠ¨çš„ç©ºé—´æ¨ç†æ—¶å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æœ€é«˜æŠ½è±¡å±‚çº§çš„å‡†ç¡®ç‡ä¹Ÿä½äº 50%ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†æ¨¡å‹åœ¨ symbolic-to-spatial æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ï¼Œä¹Ÿä¸ºæ¨åŠ¨è¯¥é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17653v1",
      "published_date": "2025-05-23 09:17:07 UTC",
      "updated_date": "2025-05-23 09:17:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:30:02.201241+00:00"
    },
    {
      "arxiv_id": "2505.17652v2",
      "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective",
      "title_zh": "é‡æ–°å®¡è§†å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼ºåŒ–å­¦ä¹ ä¸­çš„é‡‡æ ·å‡†åˆ™ï¼šåŸºäºèƒ½åŠ›-éš¾åº¦å¯¹é½çš„è§†è§’",
      "authors": [
        "Deyang Kong",
        "Qi Guo",
        "Xiangyu Xi",
        "Wei Wang",
        "Jingang Wang",
        "Xunliang Cai",
        "Shikun Zhang",
        "Wei Ye"
      ],
      "abstract": "Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces $\\textbf{C}$ompetence-$\\textbf{D}$ifficulty $\\textbf{A}$lignment $\\textbf{S}$ampling ($\\textbf{CDAS}$), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33 times slower than CDAS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†èƒ½åŠ›-éš¾åº¦å¯¹é½é‡‡æ · (Competence-Difficulty Alignment Sampling, CDAS)ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æå‡å¤§è¯­è¨€æ¨¡å‹ (LLM) æ¨ç†èƒ½åŠ›æ—¶ï¼Œç°æœ‰é‡‡æ ·ç­–ç•¥å› é¢˜ç›®éš¾åº¦è¯„ä¼°ä¸ç¨³å®šä¸”æœªè€ƒè™‘æ¨¡å‹èƒ½åŠ›ä¸éš¾åº¦å¯¹é½è€Œå¯¼è‡´çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚CDAS é€šè¿‡èšåˆå†å²æ€§èƒ½å·®å¼‚æ¥å®ç°å¯¹é¢˜ç›®éš¾åº¦çš„ç¨³å®šè¯„ä¼°ï¼Œå¹¶åˆ©ç”¨ä¸åŠ¨ç‚¹ç³»ç»Ÿ (fixed-point system) é‡åŒ–æ¨¡å‹èƒ½åŠ›ï¼Œä»è€Œè‡ªé€‚åº”åœ°é€‰æ‹©ä¸æ¨¡å‹å½“å‰èƒ½åŠ›æ°´å¹³ç›¸åŒ¹é…çš„é¢˜ç›®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCDAS åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†å‡†ç¡®ç‡ä¸æ•ˆç‡ï¼Œå…¶è¿è¡Œé€Ÿåº¦è¾¾åˆ°ç«äº‰ç­–ç•¥ Dynamic Sampling çš„ 2.33 å€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17652v2",
      "published_date": "2025-05-23 09:15:26 UTC",
      "updated_date": "2025-05-29 11:48:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:30:34.000385+00:00"
    },
    {
      "arxiv_id": "2505.17650v1",
      "title": "Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?",
      "title_zh": "æ€ç»´é“¾æ¨ç†æ˜¯å¦çœŸçš„èƒ½é™ä½è¶Šç‹±æ”»å‡»çš„æœ‰å®³æ€§ï¼Ÿ",
      "authors": [
        "Chengda Lu",
        "Xiaoyu Fan",
        "Yu Huang",
        "Rongwu Xu",
        "Jijie Li",
        "Wei Xu"
      ],
      "abstract": "Jailbreak attacks have been observed to largely fail against recent reasoning models enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying mechanism remains underexplored, and relying solely on reasoning capacity may raise security concerns. In this paper, we try to answer the question: Does CoT reasoning really reduce harmfulness from jailbreaking? Through rigorous theoretical analysis, we demonstrate that CoT reasoning has dual effects on jailbreaking harmfulness. Based on the theoretical insights, we propose a novel jailbreak method, FicDetail, whose practical performance validates our theoretical findings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é“¾å¼æ€ç»´æ¨ç†(Chain-of-Thought, CoT)æ˜¯å¦çœŸçš„èƒ½é™ä½å¤§æ¨¡å‹åœ¨è¶Šç‹±æ”»å‡»(Jailbreaking)ä¸­çš„å±å®³æ€§ã€‚é€šè¿‡ä¸¥å¯†çš„ç†è®ºåˆ†æï¼Œè®ºæ–‡è¯æ˜äº† CoT å¯¹è¶Šç‹±æ”»å‡»çš„å±å®³æ€§å…·æœ‰â€œåŒé‡å½±å“â€(Dual Effects)ã€‚åŸºäºä¸Šè¿°ç†è®ºå‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º FicDetail çš„æ–°å‹è¶Šç‹±æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„å®é™…è¡¨ç°éªŒè¯äº†ç†è®ºé¢„æµ‹ï¼Œæ­ç¤ºäº†å•çº¯ä¾èµ–æ¨ç†èƒ½åŠ›æ¥æŠµå¾¡è¶Šç‹±æ”»å‡»å¯èƒ½å¸¦æ¥çš„å®‰å…¨éšæ‚£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17650v1",
      "published_date": "2025-05-23 09:14:48 UTC",
      "updated_date": "2025-05-23 09:14:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:31:36.285157+00:00"
    },
    {
      "arxiv_id": "2505.17648v4",
      "title": "Simulating Macroeconomic Expectations using LLM Agents",
      "title_zh": "åˆ©ç”¨ LLM æ™ºèƒ½ä½“æ¨¡æ‹Ÿå®è§‚ç»æµé¢„æœŸ",
      "authors": [
        "Jianhao Lin",
        "Lexuan Sun",
        "Yixin Yan"
      ],
      "abstract": "We introduce a novel framework for simulating macroeconomic expectations using LLM Agents. By constructing LLM Agents equipped with various functional modules, we replicate three representative survey experiments involving several expectations across different types of economic agents. Our results show that although the expectations simulated by LLM Agents are more homogeneous than those of humans, they consistently outperform LLMs relying simply on prompt engineering, and possess human-like mental mechanisms. Evaluation reveals that these capabilities stem from the contributions of their components, offering guidelines for their architectural design. Our approach complements traditional methods and provides new insights into AI behavioral science in macroeconomic research",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§åˆ©ç”¨ LLM Agents æ¨¡æ‹Ÿå®è§‚ç»æµé¢„æœŸçš„åˆ›æ–°æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå…·å¤‡å¤šç§åŠŸèƒ½æ¨¡å—çš„æ™ºèƒ½ä½“ï¼ŒæˆåŠŸå¤ç°äº†æ¶‰åŠä¸åŒç»æµä¸»ä½“çš„ä¸‰é¡¹ä»£è¡¨æ€§è°ƒæŸ¥å®éªŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ LLM Agents ç”Ÿæˆçš„é¢„æœŸç›¸è¾ƒäºäººç±»è¡¨ç°å‡ºæ›´é«˜çš„åŒè´¨æ€§ï¼Œä½†å…¶æ•ˆæœæ˜¾è‘—ä¼˜äºä»…ä¾èµ– prompt engineering çš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶å±•ç°å‡ºç±»ä¼¼äººç±»çš„å¿ƒç†æœºåˆ¶ã€‚é€šè¿‡è¯„ä¼°å„åŠŸèƒ½ç»„ä»¶çš„è´¡çŒ®ï¼Œè¯¥ç ”ç©¶ä¸ºæ™ºèƒ½ä½“çš„æ¶æ„è®¾è®¡æä¾›äº†æŒ‡å¯¼å‡†åˆ™ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè¡¥å……äº†ä¼ ç»Ÿçš„å®è§‚ç»æµç ”ç©¶æ‰‹æ®µï¼Œå¹¶ä¸º AI behavioral science åœ¨ç»æµé¢†åŸŸçš„åº”ç”¨æä¾›äº†å…¨æ–°è§è§£ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17648v4",
      "published_date": "2025-05-23 09:11:14 UTC",
      "updated_date": "2025-11-25 02:11:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:30:13.643152+00:00"
    },
    {
      "arxiv_id": "2505.17645v1",
      "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning",
      "title_zh": "HoloLLMï¼šé¢å‘è¯­è¨€é©±åŠ¨å‹äººç±»æ„ŸçŸ¥ä¸æ¨ç†çš„å¤šæ„Ÿå®˜åŸºç¡€æ¨¡å‹",
      "authors": [
        "Chuhao Zhou",
        "Jianfei Yang"
      ],
      "abstract": "Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HoloLLMï¼Œä¸€ç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM)ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½å®¶å±…ç¯å¢ƒä¸‹ Vision-Language Models (VLMs) å› è¿‡åº¦ä¾èµ–è§†è§‰æ•°æ®è€Œåœ¨é®æŒ¡ã€å¼±å…‰æˆ–éšç§å—é™åœºæ™¯ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹æ•´åˆäº† LiDARã€çº¢å¤–ã€mmWave radar å’Œ WiFi ç­‰å¤šç§æ„Ÿå®˜æ¨¡æ€ï¼Œå¹¶è®¾è®¡äº† Universal Modality-Injection Projector (UMIP)ï¼Œé€šè¿‡ç²—åˆ°ç»†çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å…‹æœäº†ä¿¡å·å¼‚æ„æ€§å¹¶å¢å¼ºäº†ç‰¹å¾å¯¹é½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº† human-VLM åä½œæ•°æ®ç­–åˆ’æµæ°´çº¿ï¼Œè§£å†³äº†ç¨€æœ‰ä¼ æ„Ÿå™¨ç¼ºä¹æ–‡æœ¬å¯¹é½æ•°æ®çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒHoloLLM åœ¨äººä½“æ„Ÿåº”å‡†ç¡®ç‡ä¸Šæ¯”ç°æœ‰æ¨¡å‹æå‡äº†é«˜è¾¾ 30%ï¼Œä¸ºå®ç°å¤šæ¨¡æ€å…·èº«æ™ºèƒ½ (Embodied Intelligence) æä¾›äº†æ–°çš„åŸºç¡€æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 13 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.17645v1",
      "published_date": "2025-05-23 09:06:09 UTC",
      "updated_date": "2025-05-23 09:06:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:32:19.187906+00:00"
    },
    {
      "arxiv_id": "2505.18222v1",
      "title": "A Domain Ontology for Modeling the Book of Purification in Islam",
      "title_zh": "ç”¨äºä¼Šæ–¯å…°æ•™ã€Šæ´å‡€ç¯‡ã€‹å»ºæ¨¡çš„é¢†åŸŸæœ¬ä½“",
      "authors": [
        "Hessa Alawwad"
      ],
      "abstract": "This paper aims to address a gap in major Islamic topics by developing an ontology for the Book of Purification in Islam. Many authoritative Islamic texts begin with the Book of Purification, as it is essential for performing prayer (the second pillar of Islam after Shahadah, the profession of faith) and other religious duties such as Umrah and Hajj.\n  The ontology development strategy followed six key steps: (1) domain identification, (2) knowledge acquisition, (3) conceptualization, (4) classification, (5) integration and implementation, and (6) ontology generation. This paper includes examples of the constructed tables and classifications.\n  The focus is on the design and analysis phases, as technical implementation is beyond the scope of this study. However, an initial implementation is provided to illustrate the steps of the proposed strategy.\n  The developed ontology ensures reusability by formally defining and encoding the key concepts, attributes, and relationships related to the Book of Purification. This structured representation is intended to support knowledge sharing and reuse.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªé’ˆå¯¹ä¼Šæ–¯å…°æ•™ã€Šæ´å‡€ä¹‹ä¹¦ã€‹(Book of Purification) çš„é¢†åŸŸæœ¬ä½“ (Domain Ontology)ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€æ ¸å¿ƒå®—æ•™ä¸»é¢˜åœ¨çŸ¥è¯†ç»“æ„åŒ–å»ºæ¨¡é¢†åŸŸçš„ç©ºç™½ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€å¥—åŒ…å«é¢†åŸŸè¯†åˆ« (Domain Identification)ã€çŸ¥è¯†è·å– (Knowledge Acquisition) å’Œæœ¬ä½“ç”Ÿæˆ (Ontology Generation) ç­‰å…­ä¸ªå…³é”®æ­¥éª¤çš„å¼€å‘ç­–ç•¥ã€‚è®ºæ–‡é‡ç‚¹æ¢è®¨äº†è®¾è®¡ä¸åˆ†æé˜¶æ®µï¼Œé€šè¿‡å¯¹æ ¸å¿ƒæ¦‚å¿µã€å±æ€§åŠå…¶å…³ç³»çš„æ­£å¼å®šä¹‰ä¸ç¼–ç ï¼Œæ„å»ºäº†ç»“æ„åŒ–çš„çŸ¥è¯†è¡¨ç¤ºã€‚è¯¥æˆæœä¸ºè¯¥é¢†åŸŸçš„çŸ¥è¯†å…±äº«ä¸å¤ç”¨ (Reusability) æä¾›äº†æ ‡å‡†åŒ–æ¡†æ¶ï¼Œå¹¶ä¸ºåç»­ç›¸å…³ä¿¡æ¯ç³»ç»Ÿçš„å¼€å‘å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.18222v1",
      "published_date": "2025-05-23 08:55:59 UTC",
      "updated_date": "2025-05-23 08:55:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:31:52.887912+00:00"
    },
    {
      "arxiv_id": "2505.17636v1",
      "title": "Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis",
      "title_zh": "æ­ç¤ºæ¨¡å‹å®‰å…¨åŸºå‡†çš„è¯­ä¹‰æ­£äº¤æ€§ï¼šä¸€é¡¹å¤šç»´åˆ†æ",
      "authors": [
        "Jonathan Bennion",
        "Shaona Ghosh",
        "Mantek Singh",
        "Nouha Dziri"
      ],
      "abstract": "Various AI safety datasets have been developed to measure LLMs against evolving interpretations of harm. Our evaluation of five recently published open-source safety benchmarks reveals distinct semantic clusters using UMAP dimensionality reduction and kmeans clustering (silhouette score: 0.470). We identify six primary harm categories with varying benchmark representation. GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix emphasizes self-harm scenarios. Significant differences in prompt length distribution suggests confounds to data collection and interpretations of harm as well as offer possible context. Our analysis quantifies benchmark orthogonality among AI benchmarks, allowing for transparency in coverage gaps despite topical similarities. Our quantitative framework for analyzing semantic orthogonality across safety benchmarks enables more targeted development of datasets that comprehensively address the evolving landscape of harms in AI use, however that is defined in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†äº”ä¸ªå¼€æº AI å®‰å…¨åŸºå‡† (Safety Benchmarks)ï¼Œåˆ©ç”¨ UMAP é™ç»´å’Œ kmeans èšç±»æŠ€æœ¯åˆ†æå…¶è¯­ä¹‰åˆ†å¸ƒï¼Œæ­ç¤ºäº†ä¸åŒæ•°æ®é›†åœ¨æŸå®³å®šä¹‰ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶è¯†åˆ«å‡ºå…­ä¸ªæ ¸å¿ƒæŸå®³ç±»åˆ«ï¼Œå¹¶æŒ‡å‡ºå„åŸºå‡†çš„ä¾§é‡ç‚¹å„å¼‚ï¼Œä¾‹å¦‚ GretelAI ä¸“æ³¨äºéšç§ä¿æŠ¤ï¼Œè€Œ WildGuardMix åˆ™ä¾§é‡äºè‡ªæ®‹åœºæ™¯ã€‚åˆ†æè¿˜å‘ç°æç¤ºè¯é•¿åº¦ (Prompt Length) åˆ†å¸ƒçš„ä¸å‡å¯èƒ½æˆä¸ºæ•°æ®æ”¶é›†å’ŒæŸå®³è§£è¯»ä¸­çš„æ½œåœ¨å¹²æ‰°å› ç´ ã€‚é€šè¿‡é‡åŒ–åŸºå‡†é—´çš„è¯­ä¹‰æ­£äº¤æ€§ (Semantic Orthogonality)ï¼Œè¯¥æ¡†æ¶ä¸ºè¯†åˆ«è¦†ç›–ç›²ç‚¹ã€å¼€å‘æ›´å…·é’ˆå¯¹æ€§ä¸”å…¨é¢çš„ AI å®‰å…¨è¯„ä¼°æ•°æ®é›†æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "6th International Conference on Advanced Natural Language Processing (AdNLP 2025), May 17 ~ 18, 2025, Zurich, Switzerland",
      "pdf_url": "https://arxiv.org/pdf/2505.17636v1",
      "published_date": "2025-05-23 08:53:11 UTC",
      "updated_date": "2025-05-23 08:53:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:32:31.491853+00:00"
    },
    {
      "arxiv_id": "2505.18221v1",
      "title": "Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs",
      "title_zh": "åŸºäºæ³¨æ„åŠ›å›¾ç¥ç»ç½‘ç»œçš„è¯æ®é©±åŠ¨å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹",
      "authors": [
        "Sharad Duwal",
        "Mir Nafis Sharear Shopnil",
        "Abhishek Tyagi",
        "Adiba Mahbub Proma"
      ],
      "abstract": "Multimodal out-of-context (OOC) misinformation is misinformation that repurposes real images with unrelated or misleading captions. Detecting such misinformation is challenging because it requires resolving the context of the claim before checking for misinformation. Many current methods, including LLMs and LVLMs, do not perform this contextualization step. LLMs hallucinate in absence of context or parametric knowledge. In this work, we propose a graph-based method that evaluates the consistency between the image and the caption by constructing two graph representations: an evidence graph, derived from online textual evidence, and a claim graph, from the claim in the caption. Using graph neural networks (GNNs) to encode and compare these representations, our framework then evaluates the truthfulness of image-caption pairs. We create datasets for our graph-based method, evaluate and compare our baseline model against popular LLMs on the misinformation detection task. Our method scores $93.05\\%$ detection accuracy on the evaluation set and outperforms the second-best performing method (an LLM) by $2.82\\%$, making a case for smaller and task-specific methods.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶å›¾ç¥ç»ç½‘ç»œ(Attention-Based GNNs)çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€è„±ç¦»è¯­å¢ƒ(out-of-context, OOC)è¯¯å¯¼ä¿¡æ¯çš„æ£€æµ‹éš¾é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºåŸºäºåœ¨çº¿æ–‡æœ¬è¯æ®çš„è¯æ®å›¾(evidence graph)å’ŒåŸºäºå›¾æ–‡è¯´æ˜çš„å£°æ˜å›¾(claim graph)ï¼Œå¹¶åˆ©ç”¨ GNNs ç¼–ç æ¥æ¯”å¯¹ä¸¤è€…çš„é€»è¾‘ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ£€æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº† 93.05% çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)çº¦ 2.82%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†åœ¨å¤„ç†éœ€è¦å¤–éƒ¨è¯æ®éªŒè¯çš„ç‰¹å®šä»»åŠ¡æ—¶ï¼Œé’ˆå¯¹æ€§çš„å°å‹åŒ–æ–¹æ³•æ¯”é€šç”¨å¤§æ¨¡å‹æ›´å…·ä¼˜åŠ¿ä¸”æ›´å…·å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18221v1",
      "published_date": "2025-05-23 08:52:58 UTC",
      "updated_date": "2025-05-23 08:52:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:32:21.683492+00:00"
    },
    {
      "arxiv_id": "2505.17632v1",
      "title": "ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation",
      "title_zh": "ReqBrainï¼šé¢å‘ AI è¾…åŠ©éœ€æ±‚ç”Ÿæˆçš„ä»»åŠ¡ç‰¹å®šå¤§è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒ",
      "authors": [
        "Mohammad Kasra Habib",
        "Daniel Graziotin",
        "Stefan Wagner"
      ],
      "abstract": "Requirements elicitation and specification remains a labor-intensive, manual process prone to inconsistencies and gaps, presenting a significant challenge in modern software engineering. Emerging studies underscore the potential of employing large language models (LLMs) for automated requirements generation to support requirements elicitation and specification; however, it remains unclear how to implement this effectively. In this work, we introduce ReqBrain, an Al-assisted tool that employs a fine-tuned LLM to generate authentic and adequate software requirements. Software engineers can engage with ReqBrain through chat-based sessions to automatically generate software requirements and categorize them by type. We curated a high-quality dataset of ISO 29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine the most effective base model for ReqBrain. The top-performing model, Zephyr-7b-beta, achieved 89.30\\% Fl using the BERT score and a FRUGAL score of 91.20 in generating authentic and adequate requirements. Human evaluations further confirmed ReqBrain's effectiveness in generating requirements. Our findings suggest that generative Al, when fine-tuned, has the potential to improve requirements elicitation and specification, paving the way for future extensions into areas such as defect identification, test case generation, and agile user story creation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ReqBrainï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) å¾®è°ƒçš„ AI è¾…åŠ©å·¥å…·ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆè½¯ä»¶éœ€æ±‚ï¼Œè§£å†³ä¼ ç»Ÿéœ€æ±‚è·å–è¿‡ç¨‹ä¸­äººå·¥æ“ä½œç¹çä¸”æ˜“äº§ç”Ÿä¸ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªç¬¦åˆ ISO 29148 æ ‡å‡†çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é’ˆå¯¹äº”ä¸ª 7B å‚æ•°è§„æ¨¡çš„ LLMs è¿›è¡Œäº†ä»»åŠ¡ç‰¹å®šçš„æŒ‡ä»¤å¾®è°ƒ (Instruction Tuning)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¡¨ç°æœ€ä¼˜çš„ Zephyr-7b-beta æ¨¡å‹åœ¨ BERT score å’Œ FRUGAL æŒ‡æ ‡ä¸Šåˆ†åˆ«è¾¾åˆ°äº† 89.30% å’Œ 91.20ï¼Œèƒ½å¤Ÿç”ŸæˆçœŸå®ã€å……è¶³çš„éœ€æ±‚å¹¶å®ç°è‡ªåŠ¨åˆ†ç±»ã€‚äººå·¥è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº† ReqBrain çš„å®ç”¨æ€§ï¼Œå±•ç¤ºäº†ç”Ÿæˆå¼ AI åœ¨ä¼˜åŒ–éœ€æ±‚å·¥ç¨‹åŠæœªæ¥æ‰©å±•è‡³ç¼ºé™·è¯†åˆ«ã€æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç­‰é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17632v1",
      "published_date": "2025-05-23 08:45:46 UTC",
      "updated_date": "2025-05-23 08:45:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:32:04.793887+00:00"
    },
    {
      "arxiv_id": "2505.17631v1",
      "title": "BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling",
      "title_zh": "BehaveGPTï¼šé¢å‘å¤§è§„æ¨¡ç”¨æˆ·è¡Œä¸ºå»ºæ¨¡çš„åŸºç¡€æ¨¡å‹",
      "authors": [
        "Jiahui Gong",
        "Jingtao Ding",
        "Fanjin Meng",
        "Chen Yang",
        "Hong Chen",
        "Zuojian Wang",
        "Haisheng Lu",
        "Yong Li"
      ],
      "abstract": "In recent years, foundational models have revolutionized the fields of language and vision, demonstrating remarkable abilities in understanding and generating complex data; however, similar advances in user behavior modeling have been limited, largely due to the complexity of behavioral data and the challenges involved in capturing intricate temporal and contextual relationships in user activities. To address this, we propose BehaveGPT, a foundational model designed specifically for large-scale user behavior prediction. Leveraging transformer-based architecture and a novel pretraining paradigm, BehaveGPT is trained on vast user behavior datasets, allowing it to learn complex behavior patterns and support a range of downstream tasks, including next behavior prediction, long-term generation, and cross-domain adaptation. Our approach introduces the DRO-based pretraining paradigm tailored for user behavior data, which improves model generalization and transferability by equitably modeling both head and tail behaviors. Extensive experiments on real-world datasets demonstrate that BehaveGPT outperforms state-of-the-art baselines, achieving more than a 10% improvement in macro and weighted recall, showcasing its ability to effectively capture and predict user behavior. Furthermore, we measure the scaling law in the user behavior domain for the first time on the Honor dataset, providing insights into how model performance scales with increased data and parameter sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BehaveGPTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤§è§„æ¨¡ç”¨æˆ·è¡Œä¸ºé¢„æµ‹è®¾è®¡çš„åŸºåº§æ¨¡å‹ (Foundation Model)ï¼Œæ—¨åœ¨è§£å†³æ•æ‰å¤æ‚æ—¶é—´å’Œä¸Šä¸‹æ–‡å…³ç³»æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åŸºäº Transformer æ¶æ„ï¼Œå¹¶å¼•å…¥äº†åˆ›æ–°çš„ DRO-based é¢„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¹³è¡¡å¤´éƒ¨å’Œå°¾éƒ¨è¡Œä¸ºçš„å»ºæ¨¡ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸è·¨é¢†åŸŸè¿ç§»æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBehaveGPT åœ¨ Next behavior prediction ç­‰å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨å®è§‚å’ŒåŠ æƒå¬å›ç‡ä¸Šç›¸è¾ƒäº SOTA åŸºçº¿æ¨¡å‹æå‡äº† 10% ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é¦–æ¬¡åœ¨ Honor æ•°æ®é›†ä¸ŠéªŒè¯äº†ç”¨æˆ·è¡Œä¸ºé¢†åŸŸçš„ Scaling lawï¼Œä¸ºç†è§£æ¨¡å‹æ€§èƒ½éšæ•°æ®é‡å’Œå‚æ•°è§„æ¨¡çš„å¢é•¿è§„å¾‹æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "22 pages, 8 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.17631v1",
      "published_date": "2025-05-23 08:43:46 UTC",
      "updated_date": "2025-05-23 08:43:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:32:23.187435+00:00"
    },
    {
      "arxiv_id": "2505.18220v1",
      "title": "Navigating Pitfalls: Evaluating LLMs in Machine Learning Programming Education",
      "title_zh": "è§„é¿è¯¯åŒºï¼šæœºå™¨å­¦ä¹ ç¼–ç¨‹æ•™è‚²ä¸­çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Smitha Kumar",
        "Michael A. Lones",
        "Manuel Maarek",
        "Hind Zantout"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has opened new avenues in education. This study examines the use of LLMs in supporting learning in machine learning education; in particular, it focuses on the ability of LLMs to identify common errors of practice (pitfalls) in machine learning code, and their ability to provide feedback that can guide learning. Using a portfolio of code samples, we consider four different LLMs: one closed model and three open models. Whilst the most basic pitfalls are readily identified by all models, many common pitfalls are not. They particularly struggle to identify pitfalls in the early stages of the ML pipeline, especially those which can lead to information leaks, a major source of failure within applied ML projects. They also exhibit limited success at identifying pitfalls around model selection, which is a concept that students often struggle with when first transitioning from theory to practice. This questions the use of current LLMs to support machine learning education, and also raises important questions about their use by novice practitioners. Nevertheless, when LLMs successfully identify pitfalls in code, they do provide feedback that includes advice on how to proceed, emphasising their potential role in guiding learners. We also compare the capability of closed and open LLM models, and find that the gap is relatively small given the large difference in model sizes. This presents an opportunity to deploy, and potentially customise, smaller more efficient LLM models within education, avoiding risks around cost and data sharing associated with commercial models.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æœºå™¨å­¦ä¹  (Machine Learning) ç¼–ç¨‹æ•™è‚²ä¸­çš„æ”¯æŒä½œç”¨ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†æ¨¡å‹è¯†åˆ«æœºå™¨å­¦ä¹ ä»£ç ä¸­å¸¸è§é™·é˜± (pitfalls) å¹¶æä¾›å­¦ä¹ åé¦ˆçš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”ä¸€ä¸ªé—­æºæ¨¡å‹å’Œä¸‰ä¸ªå¼€æºæ¨¡å‹ï¼Œç ”ç©¶å‘ç°æ‰€æœ‰æ¨¡å‹éƒ½èƒ½è¯†åˆ«åŸºç¡€é”™è¯¯ï¼Œä½†åœ¨å¤„ç†æœºå™¨å­¦ä¹ æµæ°´çº¿ (ML pipeline) æ—©æœŸé˜¶æ®µï¼ˆç‰¹åˆ«æ˜¯å¯¼è‡´ä¿¡æ¯æ³„éœ² Information leaks çš„é—®é¢˜ï¼‰ä»¥åŠæ¨¡å‹é€‰æ‹© (Model selection) ç­‰å¤æ‚æ¦‚å¿µæ—¶è¡¨ç°æ¬ ä½³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å½“å‰ LLMs åœ¨æ”¯æŒåˆå­¦è€…å’Œæœºå™¨å­¦ä¹ æ•™è‚²æ–¹é¢ä»å­˜åœ¨å±€é™ï¼Œä½†å…¶åœ¨æˆåŠŸè¯†åˆ«é”™è¯¯æ—¶æä¾›çš„æŒ‡å¯¼æ€§åé¦ˆæ˜¾ç¤ºäº†æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚æ­¤å¤–ï¼Œç”±äºå¼€æºä¸é—­æºæ¨¡å‹ä¹‹é—´çš„èƒ½åŠ›å·®è·è¾ƒå°ï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºåœ¨æ•™è‚²åœºæ™¯ä¸­éƒ¨ç½²å’Œå®šåˆ¶æ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹å…·æœ‰å¯è¡Œæ€§ï¼Œä»è€Œé¿å…å•†ä¸šæ¨¡å‹å¸¦æ¥çš„æˆæœ¬å’Œæ•°æ®å…±äº«é£é™©ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "29 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.18220v1",
      "published_date": "2025-05-23 08:39:58 UTC",
      "updated_date": "2025-05-23 08:39:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:32:41.194627+00:00"
    },
    {
      "arxiv_id": "2505.17629v2",
      "title": "TransBench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments",
      "title_zh": "TransBenchï¼šçªç ´åŠ¨æ€æ•°å­—ç¯å¢ƒä¸­å¯è¿ç§»å›¾å½¢ç”¨æˆ·ç•Œé¢æ™ºèƒ½ä½“çš„å£å’",
      "authors": [
        "Yuheng Lu",
        "Qian Yu",
        "Hongru Wang",
        "Zeming Liu",
        "Wei Su",
        "Yanping Liu",
        "Yuhang Guo",
        "Maocheng Liang",
        "Yunhong Wang",
        "Haifeng Wang"
      ],
      "abstract": "Graphical User Interface (GUI) agents, which autonomously operate on digital interfaces through natural language instructions, hold transformative potential for accessibility, automation, and user experience. A critical aspect of their functionality is grounding - the ability to map linguistic intents to visual and structural interface elements. However, existing GUI agents often struggle to adapt to the dynamic and interconnected nature of real-world digital environments, where tasks frequently span multiple platforms and applications while also being impacted by version updates. To address this, we introduce TransBench, the first benchmark designed to systematically evaluate and enhance the transferability of GUI agents across three key dimensions: cross-version transferability (adapting to version updates), cross-platform transferability (generalizing across platforms like iOS, Android, and Web), and cross-application transferability (handling tasks spanning functionally distinct apps). TransBench includes 15 app categories with diverse functionalities, capturing essential pages across versions and platforms to enable robust evaluation. Our experiments demonstrate significant improvements in grounding accuracy, showcasing the practical utility of GUI agents in dynamic, real-world environments. Our code and data will be publicly available at GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TransBenchï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å’Œå¢å¼ºå›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)æ™ºèƒ½ä½“åœ¨åŠ¨æ€æ•°å­—ç¯å¢ƒä¸­è¿ç§»èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚TransBench é’ˆå¯¹ç°æœ‰æ™ºèƒ½ä½“éš¾ä»¥é€‚åº”ç‰ˆæœ¬æ›´æ–°ã€å¤šå¹³å°åˆ‡æ¢åŠè·¨åº”ç”¨ä»»åŠ¡çš„ç“¶é¢ˆï¼Œä»è·¨ç‰ˆæœ¬è¿ç§»(cross-version transferability)ã€è·¨å¹³å°è¿ç§»(cross-platform transferability)åŠè·¨åº”ç”¨è¿ç§»(cross-application transferability)ä¸‰ä¸ªå…³é”®ç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚è¯¥åŸºå‡†æ¶µç›–äº†15ä¸ªåº”ç”¨ç±»åˆ«ï¼Œé€šè¿‡æ•æ‰ä¸åŒç‰ˆæœ¬å’Œå¹³å°çš„å…³é”®é¡µé¢ï¼Œä¸ºæ™ºèƒ½ä½“çš„é²æ£’æ€§è¯„ä¼°æä¾›äº†æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTransBench æ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“åœ¨å®šä½(grounding)æ–¹é¢çš„å‡†ç¡®æ€§ï¼Œä¸ºå®ç°åœ¨å¤æ‚ç°å®ç¯å¢ƒä¸­è‡ªä¸»è¿è¡Œçš„ GUI æ™ºèƒ½ä½“æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted by ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2505.17629v2",
      "published_date": "2025-05-23 08:39:06 UTC",
      "updated_date": "2025-05-27 07:41:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:03.953074+00:00"
    },
    {
      "arxiv_id": "2505.17623v1",
      "title": "\\texttt{Range-Arithmetic}: Verifiable Deep Learning Inference on an Untrusted Party",
      "title_zh": "Range-Arithmeticï¼šé¢å‘ä¸å¯ä¿¡ç¬¬ä¸‰æ–¹çš„å¯éªŒè¯æ·±åº¦å­¦ä¹ æ¨ç†",
      "authors": [
        "Ali Rahimi",
        "Babak H. Khalaj",
        "Mohammad Ali Maddah-Ali"
      ],
      "abstract": "Verifiable computing (VC) has gained prominence in decentralized machine learning systems, where resource-intensive tasks like deep neural network (DNN) inference are offloaded to external participants due to blockchain limitations. This creates a need to verify the correctness of outsourced computations without re-execution. We propose \\texttt{Range-Arithmetic}, a novel framework for efficient and verifiable DNN inference that transforms non-arithmetic operations, such as rounding after fixed-point matrix multiplication and ReLU, into arithmetic steps verifiable using sum-check protocols and concatenated range proofs. Our approach avoids the complexity of Boolean encoding, high-degree polynomials, and large lookup tables while remaining compatible with finite-field-based proof systems. Experimental results show that our method not only matches the performance of existing approaches, but also reduces the computational cost of verifying the results, the computational effort required from the untrusted party performing the DNN inference, and the communication overhead between the two sides.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† \\texttt{Range-Arithmetic}ï¼Œä¸€ç§ç”¨äºåœ¨ä¸å¯ä¿¡ç¬¬ä¸‰æ–¹ä¸Šæ‰§è¡Œå¯éªŒè¯æ·±åº¦å­¦ä¹ æ¨ç† (DNN inference) çš„é«˜æ•ˆæ¡†æ¶ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å°† rounding å’Œ ReLU ç­‰éç®—æœ¯æ“ä½œè½¬åŒ–ä¸ºç®—æœ¯æ­¥éª¤ï¼Œå¹¶ç»“åˆ sum-check protocols å’Œ concatenated range proofs è¿›è¡ŒéªŒè¯ï¼Œé¿å…äº† Boolean encodingã€high-degree polynomials å’Œå¤§å‹ lookup tables çš„å¤æ‚æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œ\\texttt{Range-Arithmetic} åœ¨ä¿æŒç°æœ‰æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆé™ä½äº†éªŒè¯æˆæœ¬ã€æ¨ç†æ–¹çš„è®¡ç®—å¼€é”€ä»¥åŠåŒæ–¹çš„ communication overheadï¼Œä¸”ä¸ finite-field-based proof systems ä¿æŒå…¼å®¹ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17623v1",
      "published_date": "2025-05-23 08:33:50 UTC",
      "updated_date": "2025-05-23 08:33:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:02.617068+00:00"
    },
    {
      "arxiv_id": "2505.17618v1",
      "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
      "title_zh": "é€šè¿‡æµ‹è¯•æ—¶è¿›åŒ–æœç´¢æ‰©å±•å›¾åƒä¸è§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Haoran He",
        "Jiajun Liang",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai",
        "Ling Pan"
      ],
      "abstract": "As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose \\textbf{Evo}lutionary \\textbf{Search} (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æ¨ç†ä¾§æ‰©å±•ï¼ˆTest-Time Scaling, TTSï¼‰ä¸­å­˜åœ¨çš„é¢†åŸŸå±€é™ã€æ‰©å±•æ€§å·®åŠé‡‡æ ·å¤šæ ·æ€§å—æŸç­‰é—®é¢˜ï¼Œæå‡ºäº† EvoSearchã€‚è¿™æ˜¯ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒæˆ–æ¨¡å‹æ‰©å±•çš„é€šç”¨å‹ TTS æ–¹æ³•ï¼Œé€‚ç”¨äº Diffusion å’Œ Flow æ¨¡å‹ã€‚EvoSearch å°†æ¨ç†ä¾§æ‰©å±•é‡æ–°å®šä¹‰ä¸ºè¿›åŒ–æœç´¢é—®é¢˜ï¼Œé€šè¿‡åœ¨éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆStochastic Differential Equationï¼‰å»å™ªè½¨è¿¹ä¸­å¼•å…¥é€‰æ‹©å’Œçªå˜æœºåˆ¶ï¼Œè¿­ä»£ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šä¸€è‡´ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨æå‡ç”Ÿæˆè´¨é‡çš„åŒæ—¶ä¿ç•™äº†æ›´é«˜çš„å¤šæ ·æ€§ï¼Œå¹¶è¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "37 pages. Project: https://tinnerhrhe.github.io/evosearch",
      "pdf_url": "https://arxiv.org/pdf/2505.17618v1",
      "published_date": "2025-05-23 08:25:46 UTC",
      "updated_date": "2025-05-23 08:25:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:32:50.194144+00:00"
    },
    {
      "arxiv_id": "2505.18218v1",
      "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games",
      "title_zh": "CoMetï¼šé¢å‘å¤šæ™ºèƒ½ä½“è¯­è¨€åšå¼ˆçš„éšå–»é©±åŠ¨å‹éšè”½é€šä¿¡",
      "authors": [
        "Shuhang Xu",
        "Fangwei Zhong"
      ],
      "abstract": "Metaphors are a crucial way for humans to express complex or subtle ideas by comparing one concept to another, often from a different domain. However, many large language models (LLMs) struggle to interpret and apply metaphors in multi-agent language games, hindering their ability to engage in covert communication and semantic evasion, which are crucial for strategic communication. To address this challenge, we introduce CoMet, a framework that enables LLM-based agents to engage in metaphor processing. CoMet combines a hypothesis-based metaphor reasoner with a metaphor generator that improves through self-reflection and knowledge integration. This enhances the agents' ability to interpret and apply metaphors, improving the strategic and nuanced quality of their interactions. We evaluate CoMet on two multi-agent language games - Undercover and Adversarial Taboo - which emphasize Covert Communication and Semantic Evasion. Experimental results demonstrate that CoMet significantly enhances the agents' ability to communicate strategically using metaphors.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† CoMet æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤šæ™ºèƒ½ä½“è¯­è¨€æ¸¸æˆä¸­çš„éšè”½é€šä¿¡ (Covert Communication) å’Œè¯­ä¹‰è§„é¿ (Semantic Evasion) èƒ½åŠ›ã€‚CoMet ç»“åˆäº†åŸºäºå‡è®¾çš„éšå–»æ¨ç†å™¨ (Metaphor Reasoner) ä¸å…·å¤‡è‡ªæˆ‘åæ€å’ŒçŸ¥è¯†æ•´åˆåŠŸèƒ½çš„éšå–»ç”Ÿæˆå™¨ï¼Œæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“ç†è§£å’Œè¿ç”¨éšå–»çš„æ°´å¹³ã€‚å®éªŒåœ¨ Undercover å’Œ Adversarial Taboo ä¸¤æ¬¾å¼ºè°ƒæˆ˜ç•¥æ²Ÿé€šçš„æ¸¸æˆä¸­è¿›è¡Œï¼Œç»“æœè¯æ˜ CoMet èƒ½æœ‰æ•ˆå¸®åŠ©æ™ºèƒ½ä½“é€šè¿‡éšå–»å®ç°æ›´å…·ç­–ç•¥æ€§å’Œç»†å¾®å·®åˆ«çš„äº¤äº’ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹åœ¨å¤æ‚è¯­ä¹‰å¤„ç†ä¸Šçš„å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To Appear at ACL 2025 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2505.18218v1",
      "published_date": "2025-05-23 08:23:54 UTC",
      "updated_date": "2025-05-23 08:23:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:26.397444+00:00"
    },
    {
      "arxiv_id": "2505.17616v2",
      "title": "Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments",
      "title_zh": "é€ƒé¿è™½å¯è€»ä½†æœ‰ç”¨ï¼šè®ºå…·èº«ç¯å¢ƒä¸­åŸºäºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„æå‰é€€å‡ºè¡Œä¸º",
      "authors": [
        "Qingyu Lu",
        "Liang Ding",
        "Siyi Cao",
        "Xuebo Liu",
        "Kanjian Zhang",
        "Jinxia Zhang",
        "Dacheng Tao"
      ],
      "abstract": "Agents powered by large language models (LLMs) have demonstrated strong planning and decision-making capabilities in complex embodied environments. However, such agents often suffer from inefficiencies in multi-turn interactions, frequently trapped in repetitive loops or issuing ineffective commands, leading to redundant computational overhead. Instead of relying solely on learning from trajectories, we take a first step toward exploring the early-exit behavior for LLM-based agents. We propose two complementary approaches: 1. an $\\textbf{intrinsic}$ method that injects exit instructions during generation, and 2. an $\\textbf{extrinsic}$ method that verifies task completion to determine when to halt an agent's trial. To evaluate early-exit mechanisms, we introduce two metrics: one measures the reduction of $\\textbf{redundant steps}$ as a positive effect, and the other evaluates $\\textbf{progress degradation}$ as a negative effect. Experiments with 4 different LLMs across 5 embodied environments show significant efficiency improvements, with only minor drops in agent performance. We also validate a practical strategy where a stronger agent assists after an early-exit agent, achieving better performance with the same total steps. We will release our code to support further research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ™ºèƒ½ä½“åœ¨å…·èº«ç¯å¢ƒ(embodied environments)ä¸­çš„æ—©æœŸé€€å‡º(early-exit)è¡Œä¸ºï¼Œæ—¨åœ¨è§£å†³å¤šè½®äº¤äº’ä¸­å¸¸è§çš„é‡å¤å¾ªç¯å’Œæ— æ•ˆæŒ‡ä»¤å¯¼è‡´çš„è®¡ç®—å†—ä½™é—®é¢˜ã€‚ä½œè€…æå‡ºäº†å†…åœ¨(intrinsic)æŒ‡ä»¤æ³¨å…¥å’Œå¤–åœ¨(extrinsic)ä»»åŠ¡å®ŒæˆéªŒè¯ä¸¤ç§äº’è¡¥æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†å†—ä½™æ­¥éª¤(redundant steps)å‡å°‘å’Œè¿›åº¦é€€åŒ–(progress degradation)ä¸¤ä¸ªæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚åœ¨5ç§å…·èº«ç¯å¢ƒä¸‹çš„å®éªŒè¯æ˜ï¼Œè¯¥æœºåˆ¶èƒ½æ˜¾è‘—æå‡æ™ºèƒ½ä½“çš„è¿è¡Œæ•ˆç‡ï¼Œä¸”ä»…ä¼´éšå¾®å°çš„æ€§èƒ½æŸå¤±ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜éªŒè¯äº†ä¸€ç§åä½œç­–ç•¥ï¼Œå³åœ¨æ—©æœŸé€€å‡ºåç”±æ›´å¼ºçš„æ™ºèƒ½ä½“æ¥æ‰‹ï¼Œä»è€Œåœ¨ä¿æŒæ€»æ­¥æ•°ä¸å˜çš„æƒ…å†µä¸‹å®ç°æ›´ä¼˜çš„ä»»åŠ¡è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025 - Findings",
      "pdf_url": "https://arxiv.org/pdf/2505.17616v2",
      "published_date": "2025-05-23 08:23:36 UTC",
      "updated_date": "2025-09-22 01:20:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:22.481031+00:00"
    },
    {
      "arxiv_id": "2505.17613v1",
      "title": "MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation",
      "title_zh": "MMMGï¼šå¤šä»»åŠ¡å¤šæ¨¡æ€ç”Ÿæˆå…¨é¢å¯é è¯„ä¼°å¥—ä»¶",
      "authors": [
        "Jihan Yao",
        "Yushi Hu",
        "Yujie Yi",
        "Bin Han",
        "Shangbin Feng",
        "Guang Yang",
        "Bingbing Wen",
        "Ranjay Krishna",
        "Lucy Lu Wang",
        "Yulia Tsvetkov",
        "Noah A. Smith",
        "Banghua Zhu"
      ],
      "abstract": "Automatically evaluating multimodal generation presents a significant challenge, as automated metrics often struggle to align reliably with human evaluation, especially for complex tasks that involve multiple modalities. To address this, we present MMMG, a comprehensive and human-aligned benchmark for multimodal generation across 4 modality combinations (image, audio, interleaved text and image, interleaved text and audio), with a focus on tasks that present significant challenges for generation models, while still enabling reliable automatic evaluation through a combination of models and programs. MMMG encompasses 49 tasks (including 29 newly developed ones), each with a carefully designed evaluation pipeline, and 937 instructions to systematically assess reasoning, controllability, and other key capabilities of multimodal generation models. Extensive validation demonstrates that MMMG is highly aligned with human evaluation, achieving an average agreement of 94.3%. Benchmarking results on 24 multimodal generation models reveal that even though the state-of-the-art model, GPT Image, achieves 78.3% accuracy for image generation, it falls short on multimodal reasoning and interleaved generation. Furthermore, results suggest considerable headroom for improvement in audio generation, highlighting an important direction for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MMMGï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šä»»åŠ¡å¤šæ¨¡æ€ç”Ÿæˆ (Multitask Multimodal Generation) çš„å…¨é¢ä¸”å¯é çš„è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨åŒ–æŒ‡æ ‡ä¸äººç±»è¯„ä»·ä¸€è‡´æ€§å·®çš„æŒ‘æˆ˜ã€‚è¯¥å¥—ä»¶æ¶µç›–äº†å›¾åƒã€éŸ³é¢‘ã€å›¾æ–‡äº¤é”™åŠéŸ³æ–‡äº¤é”™ 4 ç§æ¨¡æ€ç»„åˆï¼ŒåŒ…å« 49 ä¸ªä»»åŠ¡å’Œ 937 æ¡æŒ‡ä»¤ï¼Œé€šè¿‡æ¨¡å‹ä¸ç¨‹åºç»“åˆçš„æ–¹å¼å®ç°äº†é«˜åº¦è‡ªåŠ¨åŒ–çš„è¯„ä»·æµç¨‹ã€‚éªŒè¯è¡¨æ˜ MMMG ä¸äººç±»è¯„ä»·çš„ä¸€è‡´æ€§è¾¾ 94.3%ï¼›å¯¹ 24 ä¸ªä¸»æµæ¨¡å‹çš„æµ‹è¯„æ˜¾ç¤ºï¼Œå³ä¾¿ SOTA æ¨¡å‹ï¼ˆå¦‚ GPT Imageï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†å’Œäº¤é”™ç”Ÿæˆæ–¹é¢ä»å­˜åœ¨çŸ­æ¿ï¼Œä¸”éŸ³é¢‘ç”Ÿæˆé¢†åŸŸä»æœ‰æ˜¾è‘—çš„æå‡ç©ºé—´ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17613v1",
      "published_date": "2025-05-23 08:21:28 UTC",
      "updated_date": "2025-05-23 08:21:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:37.846559+00:00"
    },
    {
      "arxiv_id": "2505.17612v2",
      "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
      "title_zh": "ç»“åˆæ£€ç´¢ä¸ä»£ç å·¥å…·çš„ LLM æ™ºèƒ½ä½“å°æ¨¡å‹è’¸é¦",
      "authors": [
        "Minki Kang",
        "Jongwon Jeong",
        "Seanie Lee",
        "Jaewoong Cho",
        "Sung Ju Hwang"
      ],
      "abstract": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Agent Distillation æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°è¯­è¨€æ¨¡å‹ (sLMs) åœ¨å¤„ç†ç¨€æœ‰äº‹å®çŸ¥è¯†æˆ–ç²¾ç¡®è®¡ç®—æ—¶å› èƒ½åŠ›æœ‰é™è€Œäº§ç”Ÿçš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸ä»…è½¬ç§»æ¨ç†èƒ½åŠ›ï¼Œè¿˜é€šè¿‡é›†æˆ Retrieval å’Œ Code toolsï¼Œå°† LLM-based agents çš„å®Œæ•´ä»»åŠ¡è§£å†³è¡Œä¸ºè’¸é¦è‡³å°æ¨¡å‹ä¸­ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº† first-thought prefix æç¤ºæ³•ä»¥ä¼˜åŒ–æ•™å¸ˆè½¨è¿¹è´¨é‡ï¼Œå¹¶é‡‡ç”¨ self-consistent action generation æå‡æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå‚æ•°é‡ä»…ä¸º 0.5B è‡³ 3B çš„å°æ¨¡å‹åœ¨ 8 é¡¹æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†ä¸è§„æ¨¡æ›´å¤§çš„æ¨¡å‹ï¼ˆ1.5B è‡³ 7Bï¼‰é€šè¿‡ CoT è’¸é¦åç›¸å½“çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ„å»ºå®ç”¨ã€å…·å¤‡å·¥å…·è°ƒç”¨èƒ½åŠ›çš„å°å‹æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2025 Spotlight",
      "pdf_url": "https://arxiv.org/pdf/2505.17612v2",
      "published_date": "2025-05-23 08:20:15 UTC",
      "updated_date": "2025-11-05 11:42:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:51.900828+00:00"
    },
    {
      "arxiv_id": "2505.17609v2",
      "title": "Integrating Visual Interpretation and Linguistic Reasoning for Math Problem Solving",
      "title_zh": "èåˆè§†è§‰è§£è¯‘ä¸è¯­è¨€æ¨ç†çš„æ•°å­¦é—®é¢˜æ±‚è§£",
      "authors": [
        "Zixian Guo",
        "Ming Liu",
        "Qilong Wang",
        "Zhilong Ji",
        "Jinfeng Bai",
        "Lei Zhang",
        "Wangmeng Zuo"
      ],
      "abstract": "Current large vision-language models (LVLMs) typically employ a connector module to link visual features with text embeddings of large language models (LLMs) and use end-to-end training to achieve multi-modal understanding in a unified process. Effective alignment needs high-quality pre-training data and a carefully designed training process. Current LVLMs face challenges when addressing complex vision-language reasoning tasks, with their reasoning capabilities notably lagging behind those of LLMs. This paper proposes a paradigm shift: instead of training end-to-end vision-language reasoning models, we advocate for developing a decoupled reasoning framework based on existing visual interpretation specialists and text-based reasoning LLMs. Our approach leverages (1) a dedicated vision-language model to transform the visual content of images into textual descriptions and (2) an LLM to perform reasoning according to the visual-derived text and the original question. This method presents a cost-efficient solution for multi-modal model development by optimizing existing models to work collaboratively, avoiding end-to-end development of vision-language models from scratch. By transforming images into language model-compatible text representations, it facilitates future low-cost and flexible upgrades to upcoming powerful LLMs. We introduce an outcome-rewarded joint-tuning strategy to optimize the cooperation between the visual interpretation and linguistic reasoning model. Evaluation results on vision-language benchmarks demonstrate that the decoupled reasoning framework outperforms recent LVLMs. Our approach yields particularly significant performance gains on visually intensive geometric mathematics problems. The code is available: https://github.com/guozix/DVLR.",
      "tldr_zh": "---\n\n### ğŸ“ è®ºæ–‡ TLDR æ‘˜è¦\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è§£è€¦æ¨ç†æ¡†æ¶(decoupled reasoning framework)ï¼Œæ—¨åœ¨è§£å†³å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­æ¨ç†èƒ½åŠ›æ»åäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸“ç”¨æ¨¡å‹å°†å›¾åƒè§†è§‰å†…å®¹è½¬åŒ–ä¸ºæ–‡æœ¬æè¿°ï¼Œå†ç”± LLM åŸºäºæè¿°å’ŒåŸå§‹é—®é¢˜è¿›è¡Œé€»è¾‘æ¨ç†ï¼Œå¹¶å¼•å…¥äº†ç»“æœå¥–åŠ±è”åˆå¾®è°ƒç­–ç•¥(outcome-rewarded joint-tuning)ä»¥ä¼˜åŒ–æ¨¡å‹é—´çš„åä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªè§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„ç«¯åˆ°ç«¯(end-to-end) LVLMsï¼Œç‰¹åˆ«æ˜¯åœ¨å‡ ä½•æ•°å­¦é—®é¢˜ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚\n\n---\n\nå¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦è½¬æ¢ï¼Œæˆ–è€…æƒ³é’ˆå¯¹è¯¥ç ”ç©¶çš„ç‰¹å®šæŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚ **outcome-rewarded joint-tuning**ï¼‰è¿›è¡Œæ·±å…¥æ¢è®¨ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICCV2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17609v2",
      "published_date": "2025-05-23 08:18:00 UTC",
      "updated_date": "2025-08-13 03:45:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:49.308633+00:00"
    },
    {
      "arxiv_id": "2505.17607v2",
      "title": "Controlled Agentic Planning & Reasoning for Mechanism Synthesis",
      "title_zh": "æœºæ„ç»¼åˆä¸­çš„å—æ§æ™ºèƒ½ä½“è§„åˆ’ä¸æ¨ç†",
      "authors": [
        "JoÃ£o Pedro Gandarela",
        "Thiago Rios",
        "Stefan Menzel",
        "AndrÃ© Freitas"
      ],
      "abstract": "This work presents a dual-agent \\ac{llm}-based reasoning framework for automated planar mechanism synthesis that tightly couples linguistic specification with symbolic representation and simulation. From a natural-language task description, the system composes symbolic constraints and equations, generates and parametrises simulation code, and iteratively refines designs via critic-driven feedback, including symbolic regression and geometric distance metrics, closing an actionable linguistic/symbolic optimisation loop. To evaluate the approach, we introduce MSynth, a benchmark of analytically defined planar trajectories. Empirically, critic feedback and iterative refinement yield large improvements (up to 90\\% on individual tasks) and statistically significant gains per the Wilcoxon signed-rank test. Symbolic-regression prompts provide deeper mechanistic insight primarily when paired with larger models or architectures with appropriate inductive biases (e.g., LRM).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäº LLM çš„åŒæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–å¹³é¢æœºæ„ç»¼åˆ (Planar Mechanism Synthesis)ï¼Œå®ç°äº†è‡ªç„¶è¯­è¨€æè¿°ä¸ç¬¦å·è¡¨ç¤ºã€ä»¿çœŸçš„ç´§å¯†è€¦åˆã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€ä»»åŠ¡è½¬åŒ–ä¸ºç¬¦å·çº¦æŸå’Œä»¿çœŸä»£ç ï¼Œå¹¶é€šè¿‡åŸºäºè¯„è®ºè€…åé¦ˆçš„è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹â€”â€”ç»“åˆç¬¦å·å›å½’ (Symbolic Regression) å’Œå‡ ä½•è·ç¦»åº¦é‡â€”â€”å®ç°äº†è¯­è¨€ä¸ç¬¦å·ä¼˜åŒ–çš„é—­ç¯ã€‚ç ”ç©¶è€…åŒæ­¥æ¨å‡ºäº† MSynth åŸºå‡†æµ‹è¯•ï¼Œå®éªŒè¯æ˜è¯„è®ºè€…åé¦ˆå’Œè¿­ä»£æ”¹è¿›åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¯å¸¦æ¥é«˜è¾¾ 90% çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œç»“æœè¡¨æ˜ç¬¦å·å›å½’æç¤ºåœ¨é…åˆå¤§å‹æ¨¡å‹æˆ–å…·æœ‰é€‚å½“å½’çº³åç½®çš„æ¶æ„ï¼ˆå¦‚ LRMï¼‰æ—¶ï¼Œèƒ½å¤Ÿæä¾›æ›´æ·±å±‚çš„æœºåˆ¶æ´å¯Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17607v2",
      "published_date": "2025-05-23 08:16:32 UTC",
      "updated_date": "2025-10-08 13:58:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:52.987607+00:00"
    },
    {
      "arxiv_id": "2505.18217v1",
      "title": "ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions Challenge",
      "title_zh": "ABHINAYAï¼šé¢å‘è‡ªç„¶åœºæ™¯è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æŒ‘æˆ˜èµ›çš„ç³»ç»Ÿ",
      "authors": [
        "Soumya Dutta",
        "Smruthi Balaji",
        "Varada R",
        "Viveka Salinamakki",
        "Sriram Ganapathy"
      ],
      "abstract": "Speech emotion recognition (SER) in naturalistic settings remains a challenge due to the intrinsic variability, diverse recording conditions, and class imbalance. As participants in the Interspeech Naturalistic SER Challenge which focused on these complexities, we present Abhinaya, a system integrating speech-based, text-based, and speech-text models. Our approach fine-tunes self-supervised and speech large language models (SLLM) for speech representations, leverages large language models (LLM) for textual context, and employs speech-text modeling with an SLLM to capture nuanced emotional cues. To combat class imbalance, we apply tailored loss functions and generate categorical decisions through majority voting. Despite one model not being fully trained, the Abhinaya system ranked 4th among 166 submissions. Upon completion of training, it achieved state-of-the-art performance among published results, demonstrating the effectiveness of our approach for SER in real-world conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Abhinaya ç³»ç»Ÿï¼Œæ—¨åœ¨åº”å¯¹è‡ªç„¶ä¸»ä¹‰ç¯å¢ƒä¸‹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ« (Speech Emotion Recognition, SER) ä¸­å­˜åœ¨çš„å˜å¼‚æ€§ã€å½•éŸ³æ¡ä»¶å¤šæ ·æ€§å’Œç±»åˆ«ä¸å¹³è¡¡ç­‰æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé›†æˆå¹¶å¾®è°ƒäº†å¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬ç”¨äºè¯­éŸ³è¡¨ç¤ºçš„è‡ªç›‘ç£æ¨¡å‹ä¸ Speech Large Language Models (SLLM)ï¼Œä»¥åŠç”¨äºæå–æ–‡æœ¬ä¸Šä¸‹æ–‡çš„å¤§è¯­è¨€æ¨¡å‹ (LLM)ï¼Œå¹¶é€šè¿‡è¯­éŸ³-æ–‡æœ¬å»ºæ¨¡æ•æ‰ç»†å¾®çš„æƒ…æ„Ÿçº¿ç´¢ã€‚ä¸ºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç ”ç©¶åº”ç”¨äº†å®šåˆ¶çš„ Loss Functions å¹¶é€šè¿‡å¤šæ•°æŠ•ç¥¨ (Majority Voting) æœºåˆ¶ç”Ÿæˆæœ€ç»ˆå†³ç­–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAbhinaya ç³»ç»Ÿåœ¨ Interspeech Naturalistic SER Challenge ä¸­ä½åˆ—ç¬¬å››ï¼Œä¸”åœ¨å®Œæˆè®­ç»ƒåè¾¾åˆ°äº† State-of-the-Art (SOTA) çš„æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨çœŸå®åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 2 figures, 4 tables, accepted at Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.18217v1",
      "published_date": "2025-05-23 08:01:56 UTC",
      "updated_date": "2025-05-23 08:01:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:38.079777+00:00"
    },
    {
      "arxiv_id": "2505.17593v1",
      "title": "JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks",
      "title_zh": "JELAIï¼šåœ¨ Jupyter Notebooks ä¸­é›†æˆäººå·¥æ™ºèƒ½ä¸å­¦ä¹ åˆ†æ",
      "authors": [
        "Manuel Valle Torre",
        "Thom van der Velden",
        "Marcus Specht",
        "Catharine Oertel"
      ],
      "abstract": "Generative AI offers potential for educational support, but often lacks pedagogical grounding and awareness of the student's learning context. Furthermore, researching student interactions with these tools within authentic learning environments remains challenging. To address this, we present JELAI, an open-source platform architecture designed to integrate fine-grained Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly within a Jupyter Notebook environment. JELAI employs a modular, containerized design featuring JupyterLab extensions for telemetry and chat, alongside a central middleware handling LA processing and context-aware LLM prompt enrichment. This architecture enables the capture of integrated code interaction and chat data, facilitating real-time, context-sensitive AI scaffolding and research into student behaviour. We describe the system's design, implementation, and demonstrate its feasibility through system performance benchmarks and two proof-of-concept use cases illustrating its capabilities for logging multi-modal data, analysing help-seeking patterns, and supporting A/B testing of AI configurations. JELAI's primary contribution is its technical framework, providing a flexible tool for researchers and educators to develop, deploy, and study LA-informed AI tutoring within the widely used Jupyter ecosystem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† JELAIï¼Œä¸€ä¸ªæ—¨åœ¨å°†ç»†ç²’åº¦ Learning Analytics (LA) ä¸åŸºäº Large Language Model (LLM) çš„è¾…å¯¼åŠŸèƒ½é›†æˆåˆ° Jupyter Notebook ç¯å¢ƒä¸­çš„å¼€æºå¹³å°æ¶æ„ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–å’Œå®¹å™¨åŒ–è®¾è®¡ï¼Œé€šè¿‡ JupyterLab æ‰©å±•å®ç°é¥æµ‹ï¼ˆtelemetryï¼‰ä¸èŠå¤©åŠŸèƒ½ï¼Œå¹¶åˆ©ç”¨ä¸­å¤®ä¸­é—´ä»¶å¤„ç† LA æ•°æ®å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ prompt å¢å¼ºã€‚JELAI èƒ½å¤ŸåŒæ­¥æ•è·ä»£ç äº¤äº’ä¸å¯¹è¯æ•°æ®ï¼Œæ”¯æŒå®æ—¶ã€æƒ…å¢ƒæ•æ„Ÿçš„ AI Scaffolding ä»¥åŠå¯¹å­¦ç”Ÿå­¦ä¹ è¡Œä¸ºçš„æ·±å…¥ç ”ç©¶ã€‚é€šè¿‡æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œ A/B Testing ç­‰è¯æ˜ç”¨ä¾‹ï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†å…¶åœ¨è®°å½•å¤šæ¨¡æ€æ•°æ®å’Œæ”¯æŒæ•™å­¦ç ”ç©¶æ–¹é¢çš„æ½œåŠ›ï¼Œä¸º Jupyter ç”Ÿæ€ç³»ç»Ÿä¸‹çš„ AI è¾…åŠ©æ•™å­¦æä¾›äº†çµæ´»çš„å¼€å‘ä¸ç ”ç©¶å·¥å…·ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted for AIED 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17593v1",
      "published_date": "2025-05-23 07:58:53 UTC",
      "updated_date": "2025-05-23 07:58:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:33:52.752995+00:00"
    },
    {
      "arxiv_id": "2505.17591v1",
      "title": "MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity",
      "title_zh": "MinkUNeXt-SIï¼šèåˆçƒåæ ‡ä¸ LiDAR å¼ºåº¦çš„æ”¹è¿›å‹ç‚¹äº‘åœ°ç‚¹è¯†åˆ«",
      "authors": [
        "Judith Vilella-Cantos",
        "Juan JosÃ© Cabrera",
        "Luis PayÃ¡",
        "MÃ³nica Ballesta",
        "David Valiente"
      ],
      "abstract": "In autonomous navigation systems, the solution of the place recognition problem is crucial for their safe functioning. But this is not a trivial solution, since it must be accurate regardless of any changes in the scene, such as seasonal changes and different weather conditions, and it must be generalizable to other environments. This paper presents our method, MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input data to obtain its spherical coordinates and intensity values normalized within a range of 0 to 1 for each point, and it produces a robust place recognition descriptor. To that end, a deep learning approach that combines Minkowski convolutions and a U-net architecture with skip connections is used. The results of MinkUNeXt-SI demonstrate that this method reaches and surpasses state-of-the-art performance while it also generalizes satisfactorily to other datasets. Additionally, we showcase the capture of a custom dataset and its use in evaluating our solution, which also achieves outstanding results. Both the code of our solution and the runs of our dataset are publicly available for reproducibility purposes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MinkUNeXt-SIï¼Œä¸€ç§æ—¨åœ¨æå‡ç‚¹äº‘åœ°ç‚¹è¯†åˆ« (Place Recognition) æ€§èƒ½çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡é¢„å¤„ç† LiDAR ç‚¹äº‘ä»¥æå–çƒé¢åæ ‡ (Spherical Coordinates) å’Œå½’ä¸€åŒ–çš„å¼ºåº¦ä¿¡æ¯ (LiDAR Intensity)ï¼Œå¹¶ç»“åˆ Minkowski å·ç§¯ä¸å¸¦æœ‰è·³è·ƒè¿æ¥ (Skip Connections) çš„ U-net æ¶æ„æ¥ç”Ÿæˆé²æ£’çš„æè¿°ç¬¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMinkUNeXt-SI åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°å¹¶è¶…è¶Šäº†å½“å‰çš„å…ˆè¿›æ°´å¹³ (State-of-the-art)ï¼Œä¸”åœ¨ä¸åŒæ•°æ®é›†é—´å±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†ä¸€ä¸ªè‡ªå®šä¹‰æ•°æ®é›†åŠç›¸å…³æºä»£ç ä»¥æ”¯æŒç ”ç©¶çš„å¯é‡å¤æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17591v1",
      "published_date": "2025-05-23 07:56:43 UTC",
      "updated_date": "2025-05-23 07:56:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:34:09.081397+00:00"
    },
    {
      "arxiv_id": "2505.17589v2",
      "title": "CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training",
      "title_zh": "CosyVoice 3ï¼šé€šè¿‡è§„æ¨¡æ‰©å±•ä¸åè®­ç»ƒï¼Œè¿ˆå‘çœŸå®åœºæ™¯è¯­éŸ³ç”Ÿæˆ",
      "authors": [
        "Zhihao Du",
        "Changfeng Gao",
        "Yuxuan Wang",
        "Fan Yu",
        "Tianyu Zhao",
        "Hao Wang",
        "Xiang Lv",
        "Hui Wang",
        "Chongjia Ni",
        "Xian Shi",
        "Keyu An",
        "Guanrou Yang",
        "Yabin Li",
        "Yanni Chen",
        "Zhifu Gao",
        "Qian Chen",
        "Yue Gu",
        "Mengzhe Chen",
        "Yafeng Chen",
        "Shiliang Zhang",
        "Wen Wang",
        "Jieping Ye"
      ],
      "abstract": "In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at https://funaudiollm.github.io/cosyvoice3.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† CosyVoice 3ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åœ¨é‡ (in-the-wild) åœºæ™¯è®¾è®¡çš„é›¶æ ·æœ¬ (zero-shot) å¤šè¯­è¨€è¯­éŸ³åˆæˆæ¨¡å‹ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡å†…å®¹ä¸€è‡´æ€§ã€è¯´è¯äººç›¸ä¼¼åº¦å’ŒéŸµå¾‹è‡ªç„¶åº¦ã€‚ä¸ºäº†ä¼˜åŒ–è¡¨ç°ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†é€šè¿‡å¤šä»»åŠ¡ç›‘ç£è®­ç»ƒæ„å»ºçš„æ–°å‹è¯­éŸ³åˆ†è¯å™¨ (speech tokenizer)ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šç”¨çš„å¯å¾®åˆ†å¥–åŠ±æ¨¡å‹ (differentiable reward model) ç”¨äºåæœŸè®­ç»ƒ (post-training)ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå°†è®­ç»ƒæ•°æ®ä» 1 ä¸‡å°æ—¶æ‰©å±•è‡³ 100 ä¸‡å°æ—¶ï¼ˆæ¶µç›– 9 ç§è¯­è¨€å’Œ 18 ç§ä¸­æ–‡æ–¹è¨€ï¼‰ï¼Œå¹¶å°†æ¨¡å‹å‚æ•°è§„æ¨¡ä» 5 äº¿æå‡è‡³ 15 äº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCosyVoice 3 åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æ¨åŠ¨äº†è‡ªç„¶åœºæ™¯ä¸‹é«˜è´¨é‡è¯­éŸ³åˆæˆæŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Preprint, work in progress",
      "pdf_url": "https://arxiv.org/pdf/2505.17589v2",
      "published_date": "2025-05-23 07:55:21 UTC",
      "updated_date": "2025-05-27 07:48:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:34:16.643004+00:00"
    },
    {
      "arxiv_id": "2505.18216v1",
      "title": "Data Mining-Based Techniques for Software Fault Localization",
      "title_zh": "åŸºäºæ•°æ®æŒ–æ˜çš„è½¯ä»¶æ•…éšœå®šä½æŠ€æœ¯",
      "authors": [
        "Peggy Cellier",
        "Mireille DucassÃ©",
        "SÃ©bastien FerrÃ©",
        "Olivier Ridoux",
        "W. Eric Wong"
      ],
      "abstract": "This chapter illustrates the basic concepts of fault localization using a data mining technique. It utilizes the Trityp program to illustrate the general method. Formal concept analysis and association rule are two well-known methods for symbolic data mining. In their original inception, they both consider data in the form of an object-attribute table. In their original inception, they both consider data in the form of an object-attribute table. The chapter considers a debugging process in which a program is tested against different test cases. Two attributes, PASS and FAIL, represent the issue of the test case. The chapter extends the analysis of data mining for fault localization for the multiple fault situations. It addresses how data mining can be further applied to fault localization for GUI components. Unlike traditional software, GUI test cases are usually event sequences, and each individual event has a unique corresponding event handler.",
      "tldr_zh": "è¯¥ç ”ç©¶é˜è¿°äº†åˆ©ç”¨æ•°æ®æŒ–æ˜(data mining)æŠ€æœ¯è¿›è¡Œè½¯ä»¶ç¼ºé™·å®šä½(fault localization)çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶ä»¥ Trityp ç¨‹åºä¸ºä¾‹æ¼”ç¤ºäº†å…¶é€šç”¨æµç¨‹ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†å½¢å¼æ¦‚å¿µåˆ†æ(Formal concept analysis)å’Œå…³è”è§„åˆ™(association rule)ä¸¤ç§æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºåŒ…å« PASS å’Œ FAIL å±æ€§çš„å¯¹è±¡-å±æ€§è¡¨æ¥è¾…åŠ©è°ƒè¯•ã€‚æ­¤å¤–ï¼Œè¯¥åˆ†æè¿˜æ‰©å±•åˆ°äº†å¤šç¼ºé™·åœºæ™¯(multiple fault situations)ä»¥åŠå›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)ç»„ä»¶çš„ç¼ºé™·å®šä½ã€‚é’ˆå¯¹ GUI çš„ç‰¹æ®Šæ€§ï¼Œç ”ç©¶æå‡ºé€šè¿‡åˆ†æäº‹ä»¶åºåˆ—(event sequences)åŠå…¶å¯¹åº”çš„äº‹ä»¶å¤„ç†å™¨(event handler)æ¥åº”å¯¹å…¶ç‹¬ç‰¹çš„æµ‹è¯•éœ€æ±‚ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18216v1",
      "published_date": "2025-05-23 07:35:10 UTC",
      "updated_date": "2025-05-23 07:35:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:34:29.582133+00:00"
    },
    {
      "arxiv_id": "2505.17572v1",
      "title": "USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents",
      "title_zh": "USTBenchï¼šé’ˆå¯¹ä½œä¸ºåŸå¸‚æ™ºèƒ½ä½“çš„å¤§è¯­è¨€æ¨¡å‹æ—¶ç©ºæ¨ç†èƒ½åŠ›çš„åŸºå‡†è¯„ä¼°ä¸æ·±åº¦å‰–æ",
      "authors": [
        "Siqi Lai",
        "Yansong Ning",
        "Zirui Yuan",
        "Zhixi Chen",
        "Hao Liu"
      ],
      "abstract": "Large language models (LLMs) have shown emerging potential in spatiotemporal reasoning, making them promising candidates for building urban agents that support diverse urban downstream applications. Despite these benefits, existing studies primarily focus on evaluating urban LLM agent on outcome-level metrics (e.g., prediction accuracy, traffic efficiency), offering limited insight into their underlying reasoning processes. As a result, the strengths and limitations of urban LLM agents in spatiotemporal reasoning remain poorly understood. To this end, we introduce USTBench, the first benchmark to evaluate LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed dimensions: spatiotemporal understanding, forecasting, planning, and reflection with feedback. Specifically, USTBench supports five diverse urban decision-making and four spatiotemporal prediction tasks, all running within our constructed interactive city environment UAgentEnv. The benchmark includes 62,466 structured QA pairs for process-level evaluation and standardized end-to-end task assessments, enabling fine-grained diagnostics and broad task-level comparison across diverse urban scenarios. Through extensive evaluation of thirteen leading LLMs, we reveal that although LLMs show promising potential across various urban downstream tasks, they still struggle in long-horizon planning and reflective adaptation in dynamic urban contexts. Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on general logic or mathematical problems do not consistently outperform non-reasoning LLMs. This discrepancy highlights the need for domain-specialized adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench provides a foundation to build more adaptive and effective LLM-based urban agents and broad smart city applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† USTBenchï¼Œè¿™æ˜¯é¦–ä¸ªä» `spatiotemporal understanding`ã€`forecasting`ã€`planning` ä»¥åŠ `reflection with feedback` å››ä¸ªç»´åº¦è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä½œä¸ºåŸå¸‚æ™ºèƒ½ä½“æ—¶ç©ºæ¨ç†èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†ä¾æ‰˜äºäº¤äº’å¼åŸå¸‚ç¯å¢ƒ `UAgentEnv`ï¼ŒåŒ…å« 9 é¡¹å†³ç­–ä¸é¢„æµ‹ä»»åŠ¡åŠ 62,466 ä¸ªç»“æ„åŒ–é—®ç­”å¯¹ï¼Œå®ç°äº†å¯¹æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„ç»†ç²’åº¦è¯Šæ–­ã€‚å®éªŒè¯„ä¼°äº† 13 ç§é¢†å…ˆçš„ LLMsï¼Œç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨ `long-horizon planning` å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„ `reflective adaptation` æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç° `DeepSeek-R1` ç­‰é€šç”¨æ¨ç†æ¨¡å‹åœ¨åŸå¸‚é¢†åŸŸå¹¶ä¸æ€»æ˜¯ä¼˜äºéæ¨ç†æ¨¡å‹ï¼Œè¡¨æ˜äº†è¿›è¡Œ `domain-specialized adaptation` å¯¹æå‡åŸå¸‚æ—¶ç©ºæ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17572v1",
      "published_date": "2025-05-23 07:30:57 UTC",
      "updated_date": "2025-05-23 07:30:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:04.021535+00:00"
    },
    {
      "arxiv_id": "2505.17568v2",
      "title": "JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models",
      "title_zh": "JALMBenchï¼šéŸ³é¢‘è¯­è¨€æ¨¡å‹è¶Šç‹±æ¼æ´åŸºå‡†æµ‹è¯•",
      "authors": [
        "Zifan Peng",
        "Yule Liu",
        "Zhen Sun",
        "Mingchen Li",
        "Zeren Luo",
        "Jingyi Zheng",
        "Wenhan Dong",
        "Xinlei He",
        "Xuechao Wang",
        "Yingjie Xue",
        "Shengmin Xu",
        "Xinyi Huang"
      ],
      "abstract": "Audio Language Models (ALMs) have made significant progress recently. These models integrate the audio modality directly into the model, rather than converting speech into text and inputting text to Large Language Models (LLMs). While jailbreak attacks on LLMs have been extensively studied, the security of ALMs with audio modalities remains largely unexplored. Currently, there is a lack of an adversarial audio dataset and a unified framework specifically designed to evaluate and compare attacks and ALMs. In this paper, we present JALMBench, a comprehensive benchmark to assess the safety of ALMs against jailbreak attacks. JALMBench includes a dataset containing 11,316 text samples and 245,355 audio samples with over 1,000 hours. It supports 12 mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and 5 defense methods. Using JALMBench, we provide an in-depth analysis of attack efficiency, topic sensitivity, voice diversity, and architecture. Additionally, we explore mitigation strategies for the attacks at both the prompt level and the response level.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† JALMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°éŸ³é¢‘è¯­è¨€æ¨¡å‹ (ALMs) åœ¨è¶Šç‹±æ”»å‡» (jailbreak attacks) ä¸‹å®‰å…¨æ€§çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ ALM ç›´æ¥é›†æˆéŸ³é¢‘æ¨¡æ€å¸¦æ¥çš„å®‰å…¨æŒ‘æˆ˜ï¼ŒJALMBench æä¾›äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡ 1000 å°æ—¶éŸ³é¢‘çš„å¤§è§„æ¨¡å¯¹æŠ—æ€§æ•°æ®é›†ï¼Œæ”¯æŒ 12 ç§ä¸»æµæ¨¡å‹ã€å¤šç§æ”»å‡»æ–¹å¼åŠ 5 ç§é˜²å¾¡æ‰‹æ®µã€‚é€šè¿‡å¯¹æ”»å‡»æ•ˆç‡ã€ä¸»é¢˜æ•æ„Ÿåº¦å’Œæ¶æ„å½±å“çš„æ·±å…¥åˆ†æï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº† ALM åœ¨éŸ³é¢‘æ¨¡æ€ä¸‹çš„å®‰å…¨æ¼æ´ï¼Œå¹¶æ¢ç´¢äº†åœ¨æç¤ºè¯ (prompt) å’Œå“åº” (response) å±‚é¢çš„ç¼“è§£ç­–ç•¥ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17568v2",
      "published_date": "2025-05-23 07:29:55 UTC",
      "updated_date": "2025-10-03 04:14:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:34:56.634865+00:00"
    },
    {
      "arxiv_id": "2505.17561v1",
      "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model",
      "title_zh": "æ¨¡å‹å·²æŒæ¡æœ€ä½³å™ªå£°ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è´å¶æ–¯ä¸»åŠ¨å™ªå£°é€‰æ‹©",
      "authors": [
        "Kwanyoung Kim",
        "Sanghyun Kim"
      ],
      "abstract": "The choice of initial noise significantly affects the quality and prompt alignment of video diffusion models, where different noise seeds for the same prompt can lead to drastically different generations. While recent methods rely on externally designed priors such as frequency filters or inter-frame smoothing, they often overlook internal model signals that indicate which noise seeds are inherently preferable. To address this, we propose ANSE (Active Noise Selection for Generation), a model-aware framework that selects high-quality noise seeds by quantifying attention-based uncertainty. At its core is BANSA (Bayesian Active Noise Selection via Attention), an acquisition function that measures entropy disagreement across multiple stochastic attention samples to estimate model confidence and consistency. For efficient inference-time deployment, we introduce a Bernoulli-masked approximation of BANSA that enables score estimation using a single diffusion step and a subset of attention layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video quality and temporal coherence with only an 8% and 13% increase in inference time, respectively, providing a principled and generalizable approach to noise selection in video diffusion. See our project page: https://anse-project.github.io/anse-project/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† **ANSE** (**Active Noise Selection for Generation**)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¼˜åŒ–è§†é¢‘æ‰©æ•£æ¨¡å‹ (**Video Diffusion Models**) åˆå§‹å™ªå£°é€‰æ‹©çš„æ¨¡å‹æ„ŸçŸ¥æ¡†æ¶ã€‚å…¶æ ¸å¿ƒç®—æ³• **BANSA** (**Bayesian Active Noise Selection via Attention**) é€šè¿‡é‡åŒ–å¤šä¸ªéšæœºæ³¨æ„æ ·æœ¬é—´çš„ç†µå·®å¼‚æ¥è¯„ä¼°æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œè‡ªåŠ¨è¯†åˆ«å¹¶ç­›é€‰é«˜è´¨é‡çš„å™ªå£°ç§å­ã€‚ä¸ºäº†å¹³è¡¡æ¨ç†æ•ˆç‡ï¼Œç ”ç©¶å¼•å…¥äº† **Bernoulli-masked** è¿‘ä¼¼æ–¹æ³•ï¼Œä»…éœ€å•ä¸ªæ‰©æ•£æ­¥éª¤å’Œéƒ¨åˆ†æ³¨æ„å±‚å³å¯å®Œæˆè¯„åˆ†ä¼°ç®—ã€‚åœ¨ **CogVideoX-2B** å’Œ **5B** ä¸Šçš„å®éªŒè¯æ˜ï¼Œ**ANSE** åœ¨ä»…å¢åŠ  8% è‡³ 13% æ¨ç†æ—¶é—´çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè§†é¢‘çš„è§†è§‰è´¨é‡ä¸æ—¶é—´è¿è´¯æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17561v1",
      "published_date": "2025-05-23 07:09:10 UTC",
      "updated_date": "2025-05-23 07:09:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:34:53.959214+00:00"
    },
    {
      "arxiv_id": "2505.17558v1",
      "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection",
      "title_zh": "ä»¥è°è¨€æ–½æ•™ï¼šé¢å‘å¹»è§‰æ£€æµ‹çš„åˆæˆè´Ÿæ ·æœ¬è¯¾ç¨‹åŒ– DPO",
      "authors": [
        "Shrey Pandit",
        "Ashwin Vinod",
        "Liu Leqi",
        "Ying Ding"
      ],
      "abstract": "Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º HaluCheck çš„æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨é«˜æ¬ºéª—æ€§çš„åˆæˆå¹»è§‰æ ·æœ¬ä½œä¸º Direct Preference Optimization (DPO) çš„è´Ÿæ ·æœ¬ï¼Œè§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¹»è§‰æ£€æµ‹ä¸­çš„å¯¹é½æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è¯¾ç¨‹å­¦ä¹  (Curriculum Learning) ç­–ç•¥ï¼Œæ ¹æ®ç‹¬ç«‹äº‹å®æ ¸æŸ¥æ¨¡å‹çš„æ¦‚ç‡å¾—åˆ†å¯¹æ ·æœ¬éš¾åº¦è¿›è¡Œæ’åºï¼Œå®ç°ä»æ˜“åˆ°éš¾çš„ç»“æ„åŒ–è®­ç»ƒä»¥ç¡®ä¿å­¦ä¹ ç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡è¿™ç§è¯¾ç¨‹ DPO æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹åœ¨ MedHallu å’Œ HaluEval ç­‰åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡é«˜è¾¾ 24%ã€‚æ­¤å¤–ï¼ŒHaluCheck åœ¨é›¶æ ·æœ¬ (Zero-shot) åœºæ™¯ä¸‹å±•ç°å‡ºä¼˜å¼‚çš„é²æ£’æ€§ï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†å¤šä¸ªè§„æ¨¡æ›´å¤§çš„ç°æœ‰ SOTA æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code and dataset are available at https://teachingwithlies.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2505.17558v1",
      "published_date": "2025-05-23 07:05:09 UTC",
      "updated_date": "2025-05-23 07:05:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:04.993688+00:00"
    },
    {
      "arxiv_id": "2505.17552v2",
      "title": "Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing",
      "title_zh": "ç”¨äºæå‡ä»å¤´è‚½æ®µæµ‹åºæ€§èƒ½çš„é€šç”¨ç”Ÿç‰©åºåˆ—é‡æ’åº",
      "authors": [
        "Zijie Qiu",
        "Jiaqi Wei",
        "Xiang Zhang",
        "Sheng Xu",
        "Kai Zou",
        "Zhi Jin",
        "Zhiqiang Gao",
        "Nanqing Dong",
        "Siqi Sun"
      ],
      "abstract": "De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RankNovoï¼Œè¿™æ˜¯é¦–ä¸ªé€šè¿‡åˆ©ç”¨å¤šä¸ªæµ‹åºæ¨¡å‹äº’è¡¥ä¼˜åŠ¿æ¥å¢å¼º de novo peptide sequencing çš„æ·±åº¦é‡æ’åº (reranking) æ¡†æ¶ã€‚RankNovo é‡‡ç”¨åˆ—è¡¨å¼é‡æ’åº (list-wise reranking) æ–¹æ³•ï¼Œå°†å€™é€‰è‚½é“¾å»ºæ¨¡ä¸ºå¤šåºåˆ—æ¯”å¯¹ (Multiple Sequence Alignments)ï¼Œå¹¶åˆ©ç”¨ axial attention æå–å€™é€‰åºåˆ—é—´çš„å…³é”®ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº† PMD (Peptide Mass Deviation) å’Œ RMD (Residual Mass Deviation) ä¸¤ç§æ–°æŒ‡æ ‡ï¼Œä»åºåˆ—å’Œæ®‹åŸºæ°´å¹³å¯¹è´¨é‡åå·®è¿›è¡Œç²¾ç»†åŒ–ç›‘ç£ã€‚å®éªŒè¡¨æ˜ï¼ŒRankNovo ä¸ä»…æ˜¾è‘—è¶…è¶Šäº†åŸºç¡€æ¨¡å‹å¹¶è¾¾åˆ° SOTA æ€§èƒ½ï¼Œè¿˜å±•ç°å‡ºå¯¹æœªè§æ¨¡å‹çš„å¼ºå¤§é›¶æ ·æœ¬æ³›åŒ– (zero-shot generalization) èƒ½åŠ›ï¼ŒæŒ‘æˆ˜äº†ç°æœ‰çš„å•æ¨¡å‹æµ‹åºèŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17552v2",
      "published_date": "2025-05-23 06:56:55 UTC",
      "updated_date": "2025-05-30 09:21:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:01.858177+00:00"
    },
    {
      "arxiv_id": "2505.17540v1",
      "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning",
      "title_zh": "RePromptï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†å¢å¼ºå‹æ–‡ç”Ÿå›¾é‡æç¤ºæ–¹æ³•",
      "authors": [
        "Mingrui Wu",
        "Lu Wang",
        "Pu Zhao",
        "Fangkai Yang",
        "Jianjin Zhang",
        "Jianfeng Liu",
        "Yuefeng Zhan",
        "Weihao Han",
        "Hao Sun",
        "Jiayi Ji",
        "Xiaoshuai Sun",
        "Qingwei Lin",
        "Weiwei Deng",
        "Dongmei Zhang",
        "Feng Sun",
        "Qi Zhang",
        "Rongrong Ji"
      ],
      "abstract": "Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ (Text-to-Image, T2I) æ¨¡å‹åœ¨å¤„ç†ç®€çŸ­ä¸”è¡¨è¾¾ä¸å……åˆ†çš„æç¤ºè¯æ—¶éš¾ä»¥å‡†ç¡®æ•æ‰ç”¨æˆ·æ„å›¾çš„é—®é¢˜ï¼Œæå‡ºäº† RePrompt æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) å°†æ˜¾å¼æ¨ç†å¼•å…¥æç¤ºè¯å¢å¼ºè¿‡ç¨‹ï¼Œè®­ç»ƒè¯­è¨€æ¨¡å‹ç”Ÿæˆç»“æ„åŒ–ä¸”å…·æœ‰è‡ªæˆ‘åæ€èƒ½åŠ›çš„æç¤ºè¯ï¼Œè€Œéä¾èµ–æ‰‹å·¥è§„åˆ™ã€‚RePrompt åˆ©ç”¨å¥–åŠ±æ¨¡å‹ (Reward Models) ä»äººç±»åå¥½ã€è¯­ä¹‰å¯¹é½å’Œè§†è§‰æ„å›¾ç­‰ç»´åº¦å¯¹ç”Ÿæˆå›¾åƒè¿›è¡Œè¯„ä¼°ï¼Œä»è€Œå®ç°æ— éœ€äººå·¥æ ‡æ³¨æ•°æ®çš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼ŒRePrompt åœ¨ GenEval å’Œ T2I-Compbench åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†ç©ºé—´å¸ƒå±€å‡†ç¡®æ€§å’Œç»„åˆæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤šç§ T2I éª¨å¹²æ¨¡å‹ä¸Šå‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿› (SOTA) çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at: https://github.com/microsoft/DKI_LLM/tree/main/RePrompt",
      "pdf_url": "https://arxiv.org/pdf/2505.17540v1",
      "published_date": "2025-05-23 06:44:26 UTC",
      "updated_date": "2025-05-23 06:44:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:17.382913+00:00"
    },
    {
      "arxiv_id": "2505.17533v1",
      "title": "Learning Representational Disparities",
      "title_zh": "å­¦ä¹ è¡¨å¾å·®å¼‚",
      "authors": [
        "Pavan Ravishankar",
        "Rushabh Shah",
        "Daniel B. Neill"
      ],
      "abstract": "We propose a fair machine learning algorithm to model interpretable differences between observed and desired human decision-making, with the latter aimed at reducing disparity in a downstream outcome impacted by the human decision. Prior work learns fair representations without considering the outcome in the decision-making process. We model the outcome disparities as arising due to the different representations of the input seen by the observed and desired decision-maker, which we term representational disparities. Our goal is to learn interpretable representational disparities which could potentially be corrected by specific nudges to the human decision, mitigating disparities in the downstream outcome; we frame this as a multi-objective optimization problem using a neural network. Under reasonable simplifying assumptions, we prove that our neural network model of the representational disparity learns interpretable weights that fully mitigate the outcome disparity. We validate objectives and interpret results using real-world German Credit, Adult, and Heritage Health datasets.",
      "tldr_zh": "## è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“„\n\n---\n\nè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…¬å¹³æœºå™¨å­¦ä¹ (fair machine learning)ç®—æ³•ï¼Œæ—¨åœ¨å»ºæ¨¡å®é™…è§‚å¯Ÿä¸ç†æƒ³çŠ¶æ€ä¸‹äººç±»å†³ç­–ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥å‡å°‘ä¸‹æ¸¸ç»“æœçš„ä¸å¹³ç­‰(disparity)ã€‚ç ”ç©¶å¼•å…¥äº†â€œè¡¨ç¤ºå·®å¼‚â€(representational disparities)çš„æ¦‚å¿µï¼Œé€šè¿‡ç¥ç»ç½‘ç»œ(neural network)è§£å†³å¤šç›®æ ‡ä¼˜åŒ–(multi-objective optimization)é—®é¢˜ï¼Œä»è€Œå­¦ä¹ å¯è§£é‡Šçš„å·®å¼‚ç‰¹å¾ã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡å¯¹äººç±»å†³ç­–è¿›è¡Œç‰¹å®šå¼•å¯¼(nudges)æ¥çº æ­£åå·®ï¼Œç¼“è§£ç»“æœå¤±è¡¡ã€‚åœ¨ German Creditã€Adult å’Œ Heritage Health æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ¨¡å‹åœ¨æä¾›å¯è§£é‡Šæƒé‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¶ˆé™¤ç»“æœå·®å¼‚ã€‚\n\n---\n\nå¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦æ€»ç»“ï¼Œæˆ–è€…æƒ³é’ˆå¯¹è¿™ç¯‡è®ºæ–‡çš„ç‰¹å®šæ–¹æ³•è®ºè¿›è¡Œæ·±æŒ–ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.17533v1",
      "published_date": "2025-05-23 06:40:24 UTC",
      "updated_date": "2025-05-23 06:40:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:13.284660+00:00"
    },
    {
      "arxiv_id": "2505.17529v1",
      "title": "Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding",
      "title_zh": "ä½ åœ¨å…³æ³¨æˆ‘é—®çš„å†…å®¹å—ï¼Ÿé€šè¿‡æ³¨æ„åŠ›å¼•å¯¼çš„é›†æˆè§£ç ç¼“è§£å¤šæ¨¡æ€å¹»è§‰",
      "authors": [
        "Yeongjae Cho",
        "Keonwoo Kim",
        "Taebaek Hwang",
        "Sungzoon Cho"
      ],
      "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have significantly expanded their utility in tasks like image captioning and visual question answering. However, they still struggle with object hallucination, where models generate descriptions that inaccurately reflect the visual content by including nonexistent objects or misrepresenting existing ones. While previous methods, such as data augmentation and training-free approaches, strive to tackle this issue, they still encounter scalability challenges and often depend on additional external modules. In this work, we propose Ensemble Decoding (ED), a novel strategy that splits the input image into sub-images and combines logit distributions by assigning weights through the attention map. Furthermore, we introduce ED adaptive plausibility constraint to calibrate logit distribution and FastED, a variant designed for speed-critical applications. Extensive experiments across hallucination benchmarks demonstrate that our proposed method achieves state-of-the-art performance, validating the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) ä¸­çš„ç‰©ä½“å¹»è§‰ (object hallucination) é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Ensemble Decoding (ED) çš„æ–°é¢–ç­–ç•¥ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†è¾“å…¥å›¾åƒåˆ†å‰²ä¸ºå¤šä¸ªå­å›¾åƒï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›å›¾ (attention map) åˆ†é…æƒé‡æ¥æ•´åˆé€»è¾‘æ¦‚ç‡åˆ†å¸ƒ (logit distributions)ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹è§†è§‰å†…å®¹çš„æ„ŸçŸ¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† ED adaptive plausibility constraint ç”¨äºæ ¡å‡†åˆ†å¸ƒï¼Œå¹¶æ¨å‡ºäº†é’ˆå¯¹é€Ÿåº¦æ•æ„Ÿåœºæ™¯çš„ FastED å˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¹»è§‰åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œæœ‰æ•ˆè¯æ˜äº†å…¶åœ¨ç¼“è§£å¤šæ¨¡æ€å¹»è§‰æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17529v1",
      "published_date": "2025-05-23 06:35:43 UTC",
      "updated_date": "2025-05-23 06:35:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:23.945371+00:00"
    },
    {
      "arxiv_id": "2505.17525v1",
      "title": "Transparency and Proportionality in Post-Processing Algorithmic Bias Correction",
      "title_zh": "ç®—æ³•åå·®åå¤„ç†ä¿®æ­£ä¸­çš„é€æ˜åº¦ä¸æ¯”ä¾‹æ€§",
      "authors": [
        "Juliett SuÃ¡rez Ferreira",
        "Marija Slavkovik",
        "Jorge Casillas"
      ],
      "abstract": "Algorithmic decision-making systems sometimes produce errors or skewed predictions toward a particular group, leading to unfair results. Debiasing practices, applied at different stages of the development of such systems, occasionally introduce new forms of unfairness or exacerbate existing inequalities. We focus on post-processing techniques that modify algorithmic predictions to achieve fairness in classification tasks, examining the unintended consequences of these interventions. To address this challenge, we develop a set of measures that quantify the disparity in the flips applied to the solution in the post-processing stage. The proposed measures will help practitioners: (1) assess the proportionality of the debiasing strategy used, (2) have transparency to explain the effects of the strategy in each group, and (3) based on those results, analyze the possibility of the use of some other approaches for bias mitigation or to solve the problem. We introduce a methodology for applying the proposed metrics during the post-processing stage and illustrate its practical application through an example. This example demonstrates how analyzing the proportionality of the debiasing strategy complements traditional fairness metrics, providing a deeper perspective to ensure fairer outcomes across all groups.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åå¤„ç†(post-processing)ç®—æ³•åå·®ä¿®æ­£ä¸­å¯èƒ½äº§ç”Ÿçš„éé¢„æœŸä¸å…¬å¹³æ€§ï¼Œé‡ç‚¹å…³æ³¨ä¿®æ”¹é¢„æµ‹ç»“æœä»¥å®ç°å…¬å¹³æ€§çš„è¿‡ç¨‹ã€‚ç ”ç©¶å¼€å‘äº†ä¸€å¥—é‡åŒ–åå¤„ç†é˜¶æ®µé¢„æµ‹ç¿»è½¬(flips)å·®å¼‚çš„è¡¡é‡æŒ‡æ ‡ï¼Œæ—¨åœ¨è¯„ä¼°å»åå·®ç­–ç•¥(debiasing strategy)çš„æ¯”ä¾‹æ€§(proportionality)å¹¶æé«˜è§£é‡Šå„ç¾¤ä½“å—å½±å“ç¨‹åº¦çš„é€æ˜åº¦ã€‚é€šè¿‡è¿™äº›æŒ‡æ ‡ï¼Œä»ä¸šè€…å¯ä»¥åˆ†æç°æœ‰ç­–ç•¥çš„åˆç†æ€§ï¼Œå¹¶æ®æ­¤è¯„ä¼°æ˜¯å¦éœ€è¦é‡‡ç”¨å…¶ä»–åè§ç¼“è§£æ–¹æ³•ã€‚å®éªŒç¤ºä¾‹è¡¨æ˜ï¼Œåˆ†æç­–ç•¥çš„æ¯”ä¾‹æ€§å¯ä»¥æœ‰æ•ˆè¡¥å……ä¼ ç»Ÿçš„å…¬å¹³æ€§æŒ‡æ ‡(fairness metrics)ï¼Œä¸ºç¡®ä¿æ‰€æœ‰ç¾¤ä½“è·å¾—æ›´å…¬å¹³çš„ç»“æœæä¾›äº†æ›´æ·±å±‚çš„è§†è§’ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17525v1",
      "published_date": "2025-05-23 06:33:31 UTC",
      "updated_date": "2025-05-23 06:33:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:26.690302+00:00"
    },
    {
      "arxiv_id": "2505.17520v1",
      "title": "Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers",
      "title_zh": "é¢å‘ç”µæ°”å·¥ç¨‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆä¼˜åŒ–ï¼šä»¥ ABB æ–­è·¯å™¨ä¸ºä¾‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Salahuddin Alawadhi",
        "Noorhan Abbas"
      ],
      "abstract": "Integrating Retrieval Augmented Generation (RAG) with Large Language Models (LLMs) has shown the potential to provide precise, contextually relevant responses in knowledge intensive domains. This study investigates the ap-plication of RAG for ABB circuit breakers, focusing on accuracy, reliability, and contextual relevance in high-stakes engineering environments. By leveraging tailored datasets, advanced embedding models, and optimized chunking strategies, the research addresses challenges in data retrieval and contextual alignment unique to engineering documentation. Key contributions include the development of a domain-specific dataset for ABB circuit breakers and the evaluation of three RAG pipelines: OpenAI GPT4o, Cohere, and Anthropic Claude. Advanced chunking methods, such as paragraph-based and title-aware segmentation, are assessed for their impact on retrieval accuracy and response generation. Results demonstrate that while certain configurations achieve high precision and relevancy, limitations persist in ensuring factual faithfulness and completeness, critical in engineering contexts. This work underscores the need for iterative improvements in RAG systems to meet the stringent demands of electrical engineering tasks, including design, troubleshooting, and operational decision-making. The findings in this paper help advance research of AI in highly technical domains such as electrical engineering.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é’ˆå¯¹ç”µæ°”å·¥ç¨‹é¢†åŸŸä¼˜åŒ–æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ï¼Œå¹¶ä»¥ABBæ–­è·¯å™¨(ABB Circuit Breakers)ä½œä¸ºæ ¸å¿ƒæ¡ˆä¾‹è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç ”ç©¶é€šè¿‡æ„å»ºé¢†åŸŸä¸“ç”¨æ•°æ®é›†ï¼Œå¯¹æ¯”è¯„ä¼°äº†OpenAI GPT4oã€Cohereå’ŒAnthropic Claudeä¸‰ç§æ¨¡å‹åœ¨å¤„ç†å¤æ‚å·¥ç¨‹æ–‡æ¡£æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶é‡ç‚¹æµ‹è¯•äº†åŸºäºæ®µè½(Paragraph-based)å’Œæ ‡é¢˜æ„ŸçŸ¥(Title-aware)çš„è¿›é˜¶åˆ†å—ç­–ç•¥ï¼Œæ—¨åœ¨æå‡æ£€ç´¢ç²¾åº¦ä¸ä¸Šä¸‹æ–‡å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç‰¹å®šé…ç½®èƒ½å®ç°é«˜ç²¾åº¦å’Œç›¸å…³æ€§ï¼Œä½†åœ¨ç¡®ä¿äº‹å®å¿ å®åº¦(Factual Faithfulness)å’Œå®Œæ•´æ€§æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚è¯¥å·¥ä½œä¸ºäººå·¥æ™ºèƒ½åœ¨ç”µæ°”å·¥ç¨‹è®¾è®¡ã€æ•…éšœæ’é™¤åŠè¿è¡Œå†³ç­–ç­‰é«˜è¦æ±‚æŠ€æœ¯é¢†åŸŸçš„åº”ç”¨æä¾›äº†é‡è¦çš„å®è¯å‚è€ƒä¸æ”¹è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 4 figures, published in CSIT Vol. 15, 2025. DOI: 10.5121/csit.2025.150905",
      "pdf_url": "https://arxiv.org/pdf/2505.17520v1",
      "published_date": "2025-05-23 06:21:37 UTC",
      "updated_date": "2025-05-23 06:21:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:52.049632+00:00"
    },
    {
      "arxiv_id": "2505.17512v1",
      "title": "Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs",
      "title_zh": "æ¸¸æˆåŒ–æ¢æµ‹ï¼šä¸€ç§ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æ¦‚å¿µçŸ¥è¯†çš„åŸºäºæ¸¸æˆçš„è¯„æµ‹åŸºå‡†",
      "authors": [
        "Shuhang Xu",
        "Weijian Deng",
        "Yixuan Zhou",
        "Fangwei Zhong"
      ],
      "abstract": "Concepts represent generalized abstractions that enable humans to categorize and reason efficiently, yet it is unclear to what extent Large Language Models (LLMs) comprehend these semantic relationships. Existing benchmarks typically focus on factual recall and isolated tasks, failing to evaluate the ability of LLMs to understand conceptual boundaries. To address this gap, we introduce CK-Arena, a multi-agent interaction game built upon the Undercover game, designed to evaluate the capacity of LLMs to reason with concepts in interactive settings. CK-Arena challenges models to describe, differentiate, and infer conceptual boundaries based on partial information, encouraging models to explore commonalities and distinctions between closely related concepts. By simulating real-world interaction, CK-Arena provides a scalable and realistic benchmark for assessing conceptual reasoning in dynamic environments. Experimental results show that LLMs' understanding of conceptual knowledge varies significantly across different categories and is not strictly aligned with parameter size or general model capabilities. The data and code are available at the project homepage: https://ck-arena.site.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CK-Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºâ€œè°æ˜¯å§åº•â€(Undercover) æ¸¸æˆçš„å¤šæ™ºèƒ½ä½“äº¤äº’åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯¹æ¦‚å¿µçŸ¥è¯† (Conceptual Knowledge) çš„ç†è§£ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹ŸçœŸå®äº¤äº’ï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨ä¿¡æ¯æœ‰é™çš„æƒ…å†µä¸‹æè¿°ã€åŒºåˆ†å’Œæ¨æ–­æ¦‚å¿µè¾¹ç•Œï¼Œä»è€Œè¡¡é‡å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„æ¦‚å¿µæ¨ç† (Conceptual Reasoning) èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹å¯¹æ¦‚å¿µçŸ¥è¯†çš„æŒæ¡åœ¨ä¸åŒç±»åˆ«é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”è¿™ç§èƒ½åŠ›å¹¶ä¸ç›´æ¥å–å†³äºæ¨¡å‹å‚æ•°è§„æ¨¡æˆ–é€šç”¨èƒ½åŠ› (General Capabilities)ã€‚CK-Arena ä¸ºè¯„ä¼° LLMs æ˜¯å¦çœŸæ­£ç†è§£è¯­ä¹‰æŠ½è±¡è€Œéå•çº¯ä¾èµ–äº‹å®å¬å›æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„ç°å®æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.17512v1",
      "published_date": "2025-05-23 06:06:28 UTC",
      "updated_date": "2025-05-23 06:06:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:44.386679+00:00"
    },
    {
      "arxiv_id": "2505.17511v1",
      "title": "Multi-agent Systems for Misinformation Lifecycle : Detection, Correction And Source Identification",
      "title_zh": "é’ˆå¯¹è™šå‡ä¿¡æ¯ç”Ÿå‘½å‘¨æœŸçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼šæ£€æµ‹ã€çº æ­£ä¸æº¯æº",
      "authors": [
        "Aditya Gautam"
      ],
      "abstract": "The rapid proliferation of misinformation in digital media demands solutions that go beyond isolated Large Language Model(LLM) or AI Agent based detection methods. This paper introduces a novel multi-agent framework that covers the complete misinformation lifecycle: classification, detection, correction, and source verification to deliver more transparent and reliable outcomes. In contrast to single-agent or monolithic architectures, our approach employs five specialized agents: an Indexer agent for dynamically maintaining trusted repositories, a Classifier agent for labeling misinformation types, an Extractor agent for evidence based retrieval and ranking, a Corrector agent for generating fact-based correction and a Verification agent for validating outputs and tracking source credibility. Each agent can be individually evaluated and optimized, ensuring scalability and adaptability as new types of misinformation and data sources emerge. By decomposing the misinformation lifecycle into specialized agents - our framework enhances scalability, modularity, and explainability. This paper proposes a high-level system overview, agent design with emphasis on transparency, evidence-based outputs, and source provenance to support robust misinformation detection and correction at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“(multi-agent)æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹æ•°å­—åª’ä½“ä¸­è¯¯å¯¼ä¿¡æ¯(misinformation)çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼Œæ¶µç›–åˆ†ç±»ã€æ£€æµ‹ã€çº æ­£åŠæ¥æºéªŒè¯ã€‚è¯¥ç³»ç»Ÿç”±äº”ä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ç»„æˆï¼šè´Ÿè´£åŠ¨æ€ç»´æŠ¤ä¿¡ä»»åº“çš„Indexer agentã€æ ‡è®°ç±»å‹çš„Classifier agentã€æ£€ç´¢å¹¶æ’åºè¯æ®çš„Extractor agentã€ç”Ÿæˆäº‹å®æ€§çº é”™çš„Corrector agentä»¥åŠéªŒè¯è¾“å‡ºå¹¶è¿½è¸ªæ¥æºå¯ä¿¡åº¦çš„Verification agentã€‚é€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸“é—¨çš„æ¨¡å—ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒé€æ˜åº¦å’Œå¯è¿½æº¯æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§(scalability)ä¸å¯è§£é‡Šæ€§(explainability)ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°å¤§è§„æ¨¡ä¸”ç¨³å¥çš„è¯¯å¯¼ä¿¡æ¯æ²»ç†æä¾›äº†åŸºäºè¯æ®(evidence-based)çš„é«˜å±‚çº§ç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17511v1",
      "published_date": "2025-05-23 06:05:56 UTC",
      "updated_date": "2025-05-23 06:05:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:48.836899+00:00"
    },
    {
      "arxiv_id": "2505.17508v3",
      "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning",
      "title_zh": "è®ºé¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„ KL æ­£åˆ™åŒ–ç­–ç•¥æ¢¯åº¦ç®—æ³•è®¾è®¡",
      "authors": [
        "Yifan Zhang",
        "Yifeng Liu",
        "Huizhuo Yuan",
        "Yang Yuan",
        "Quanquan Gu",
        "Andrew Chi-Chih Yao"
      ],
      "abstract": "Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). KL regularization is ubiquitous, yet the design surface, choice of KL direction (forward vs. reverse), normalization (normalized vs. unnormalized), and estimator ($k_1/k_2/k_3$), is scattered across the literature and often intertwined with off-policy estimation. We ask a focused question: under the off-policy setting, what weighting is required for each KL variant so that the surrogate we optimize yields the exact gradient of the intended KL-regularized objective? We answer this with a compact, unified derivation we call the Regularized Policy Gradient (RPG) view. RPG (i) unifies normalized and unnormalized KL variants and shows that the widely-used $k_3$ penalty is exactly the unnormalized KL; (ii) specifies conditions under which REINFORCE-style losses with stop-gradient are gradient-equivalent to fully differentiable surrogates; (iii) identifies and corrects an off-policy importance-weighting mismatch in GRPO's KL term; and (iv) introduces RPG-Style Clip, a clipped-importance-sampling step within RPG-REINFORCE that enables stable, off-policy policy-gradient training at scale. On mathematical reasoning benchmarks (AIME24, AIME25), RPG-REINFORCE with RPG-Style Clip improves accuracy by up to $+6$ absolute percentage points over DAPO. We extend our experiments to 8K context length, and RPG-REINFORCE with RPG-Style Clip achieves 52% accuracy on AIME25, surpassing the official Qwen3-4B-Instruct model (47%). Notably, RPG is a stable and scalable RL algorithm for LLM reasoning, realized via (a) a KL-correct objective, (b) clipped importance sampling, and (c) an iterative reference-policy update scheme.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†ä»»åŠ¡ä¸­KLæ­£åˆ™åŒ–ç­–ç•¥æ¢¯åº¦ç®—æ³•(KL-Regularized Policy Gradient)çš„è®¾è®¡ï¼Œæ—¨åœ¨è§£å†³ä¸åŒKLæ–¹å‘ã€å½’ä¸€åŒ–æ–¹å¼åŠä¼°ç®—å™¨é€‰æ‹©åœ¨ç°æœ‰æ–‡çŒ®ä¸­å®šä¹‰æ¨¡ç³Šä¸”ä¸ç¦»ç­–(off-policy)ä¼°ç®—æ··æ·†çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ç»Ÿä¸€çš„æ­£åˆ™åŒ–ç­–ç•¥æ¢¯åº¦(Regularized Policy Gradient, RPG)è§†è§’ï¼Œæ¨å¯¼å‡ºäº†ä¸KLæ­£åˆ™åŒ–ç›®æ ‡ç²¾ç¡®å¯¹åº”çš„æ¢¯åº¦å…¬å¼ï¼Œå¹¶è¯æ˜äº†å¹¿æ³›ä½¿ç”¨çš„ $k_3$ æƒ©ç½šé¡¹æœ¬è´¨ä¸Šæ˜¯éå½’ä¸€åŒ–çš„KLæ•£åº¦ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥ä¿®æ­£äº†GRPOç®—æ³•ä¸­KLé¡¹çš„é‡è¦æ€§é‡‡æ ·æƒé‡ä¸åŒ¹é…é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†RPG-Style ClipæŠ€æœ¯ï¼Œä»¥å®ç°å¤§è§„æ¨¡ç¨³å®šçš„ç¦»ç­–è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRPG-REINFORCEåœ¨æ•°å­¦æ¨ç†åŸºå‡†(AIME24, AIME25)ä¸Šæ¯”DAPOå‡†ç¡®ç‡æå‡äº†çº¦6ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶åœ¨AIME25ä¸Šä»¥52%çš„å‡†ç¡®ç‡è¶…è¶Šäº†Qwen3-4B-Instructæ¨¡å‹ã€‚æ€»çš„æ¥è¯´ï¼ŒRPGé€šè¿‡KLæ ¡æ­£ç›®æ ‡ã€å‰ªåˆ‡é‡è¦æ€§é‡‡æ ·å’Œè¿­ä»£å‚è€ƒç­–ç•¥æ›´æ–°ï¼Œä¸ºLLMæ¨ç†æä¾›äº†ä¸€ç§ç¨³å®šä¸”å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ (RL)æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Project Page: https://github.com/complex-reasoning/RPG",
      "pdf_url": "https://arxiv.org/pdf/2505.17508v3",
      "published_date": "2025-05-23 06:01:21 UTC",
      "updated_date": "2025-12-11 04:08:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:55.783449+00:00"
    },
    {
      "arxiv_id": "2505.17501v1",
      "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition",
      "title_zh": "RoHyDRï¼šé¢å‘ä¸å®Œæ•´å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„é²æ£’æ··åˆæ‰©æ•£æ¢å¤",
      "authors": [
        "Yuehan Jin",
        "Xiaoqing Liu",
        "Yiyuan Yang",
        "Zhiwen Yu",
        "Tong Zhang",
        "Kaixiang Yang"
      ],
      "abstract": "Multimodal emotion recognition analyzes emotions by combining data from multiple sources. However, real-world noise or sensor failures often cause missing or corrupted data, creating the Incomplete Multimodal Emotion Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion Recovery (RoHyDR), a novel framework that performs missing-modality recovery at unimodal, multimodal, feature, and semantic levels. For unimodal representation recovery of missing modalities, RoHyDR exploits a diffusion-based generator to generate distribution-consistent and semantically aligned representations from Gaussian noise, using available modalities as conditioning. For multimodal fusion recovery, we introduce adversarial learning to produce a realistic fused multimodal representation and recover missing semantic content. We further propose a multi-stage optimization strategy that enhances training stability and efficiency. In contrast to previous work, the hybrid diffusion and adversarial learning-based recovery mechanism in RoHyDR allows recovery of missing information in both unimodal representation and multimodal fusion, at both feature and semantic levels, effectively mitigating performance degradation caused by suboptimal optimization. Comprehensive experiments conducted on two widely used multimodal emotion recognition benchmarks demonstrate that our proposed method outperforms state-of-the-art IMER methods, achieving robust recognition performance under various missing-modality scenarios. Our code will be made publicly available upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RoHyDR æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸å®Œæ•´å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆIncomplete Multimodal Emotion Recognition, IMERï¼‰ä¸­ç”±äºæ•°æ®ç¼ºå¤±æˆ–æŸåå¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚RoHyDR ç»“åˆäº†æ‰©æ•£ç”Ÿæˆå™¨ï¼ˆdiffusion-based generatorï¼‰å’Œå¯¹æŠ—å­¦ä¹ ï¼ˆadversarial learningï¼‰ï¼Œåœ¨å•æ¨¡æ€ã€å¤šæ¨¡æ€ã€ç‰¹å¾å’Œè¯­ä¹‰å››ä¸ªå±‚é¢åŒæ­¥è¿›è¡Œç¼ºå¤±æ•°æ®çš„æ¢å¤ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å¤šé˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼ˆmulti-stage optimization strategyï¼‰ä»¥æå‡è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚åœ¨ä¸¤ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRoHyDR åœ¨å„ç§æ¨¡æ€ç¼ºå¤±åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œå®ç°äº†é²æ£’çš„æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17501v1",
      "published_date": "2025-05-23 05:52:17 UTC",
      "updated_date": "2025-05-23 05:52:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:35:53.480957+00:00"
    },
    {
      "arxiv_id": "2505.17500v1",
      "title": "The Discovery Engine: A Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes",
      "title_zh": "Discovery Engineï¼šäººå·¥æ™ºèƒ½é©±åŠ¨çš„ç§‘å­¦çŸ¥è¯†å…¨æ™¯åˆæˆä¸å¯¼èˆªæ¡†æ¶",
      "authors": [
        "Vladimir Baulin",
        "Austin Cook",
        "Daniel Friedman",
        "Janna Lumiruusu",
        "Andrew Pashea",
        "Shagor Rahman",
        "Benedikt Waldeck"
      ],
      "abstract": "The prevailing model for disseminating scientific knowledge relies on individual publications dispersed across numerous journals and archives. This legacy system is ill suited to the recent exponential proliferation of publications, contributing to insurmountable information overload, issues surrounding reproducibility and retractions. We introduce the Discovery Engine, a framework to address these challenges by transforming an array of disconnected literature into a unified, computationally tractable representation of a scientific domain. Central to our approach is the LLM-driven distillation of publications into structured \"knowledge artifacts,\" instances of a universal conceptual schema, complete with verifiable links to source evidence. These artifacts are then encoded into a high-dimensional Conceptual Tensor. This tensor serves as the primary, compressed representation of the synthesized field, where its labeled modes index scientific components (concepts, methods, parameters, relations) and its entries quantify their interdependencies. The Discovery Engine allows dynamic \"unrolling\" of this tensor into human-interpretable views, such as explicit knowledge graphs (the CNM graph) or semantic vector spaces, for targeted exploration. Crucially, AI agents operate directly on the graph using abstract mathematical and learned operations to navigate the knowledge landscape, identify non-obvious connections, pinpoint gaps, and assist researchers in generating novel knowledge artifacts (hypotheses, designs). By converting literature into a structured tensor and enabling agent-based interaction with this compact representation, the Discovery Engine offers a new paradigm for AI-augmented scientific inquiry and accelerated discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Discovery Engineï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨åº”å¯¹ç§‘å­¦æ–‡çŒ®æŒ‡æ•°çº§å¢é•¿å¸¦æ¥çš„ä¿¡æ¯è¿‡è½½å’Œå¯é‡å¤æ€§æŒ‘æˆ˜çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ LLM å°†åˆ†æ•£çš„æ–‡çŒ®æç‚¼ä¸ºç»“æ„åŒ–çš„ knowledge artifactsï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºé«˜ç»´çš„ Conceptual Tensorï¼Œä½œä¸ºç§‘å­¦é¢†åŸŸçš„ç»Ÿä¸€å‹ç¼©è¡¨ç¤ºã€‚Conceptual Tensor å¯ä»¥åŠ¨æ€å±•å¼€ä¸ºçŸ¥è¯†å›¾è°± (CNM graph) æˆ–è¯­ä¹‰å‘é‡ç©ºé—´ï¼Œè€Œ AI æ™ºèƒ½ä½“åˆ™åˆ©ç”¨æ•°å­¦è¿ç®—åœ¨å›¾è°±ä¸­å¯¼èˆªï¼Œä»¥è¯†åˆ«éæ˜¾å¼è”ç³»ã€å®šä½çŸ¥è¯†ç©ºç™½å¹¶è¾…åŠ©ç”Ÿæˆæ–°çš„ç§‘å­¦å‡è®¾ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†æ–‡çŒ®è½¬åŒ–ä¸ºå¯è®¡ç®—çš„ç»“æ„åŒ–å¼ é‡ï¼Œä¸º AI å¢å¼ºçš„ç§‘å­¦æ¢ç´¢å’ŒåŠ é€Ÿå‘ç°æä¾›äº†ä¸€ç§å…¨æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cond-mat.soft",
        "cs.AI"
      ],
      "primary_category": "cond-mat.soft",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17500v1",
      "published_date": "2025-05-23 05:51:34 UTC",
      "updated_date": "2025-05-23 05:51:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:00.681202+00:00"
    },
    {
      "arxiv_id": "2505.17498v1",
      "title": "Managing FAIR Knowledge Graphs as Polyglot Data End Points: A Benchmark based on the rdf2pg Framework and Plant Biology Data",
      "title_zh": "å°† FAIR çŸ¥è¯†å›¾è°±ç®¡ç†ä¸ºå¤šè¯­è¨€æ•°æ®ç«¯ç‚¹ï¼šåŸºäº rdf2pg æ¡†æ¶ä¸æ¤ç‰©ç”Ÿç‰©å­¦æ•°æ®çš„åŸºå‡†ç ”ç©¶",
      "authors": [
        "Marco Brandizi",
        "Carlos Bobed",
        "Luca Garulli",
        "ArnÃ© de Klerk",
        "Keywan Hassani-Pak"
      ],
      "abstract": "Linked Data and labelled property graphs (LPG) are two data management approaches with complementary strengths and weaknesses, making their integration beneficial for sharing datasets and supporting software ecosystems. In this paper, we introduce rdf2pg, an extensible framework for mapping RDF data to semantically equivalent LPG formats and data-bases. Utilising this framework, we perform a comparative analysis of three popular graph databases - Virtuoso, Neo4j, and ArcadeDB - and the well-known graph query languages SPARQL, Cypher, and Gremlin. Our qualitative and quantitative as-sessments underline the strengths and limitations of these graph database technologies. Additionally, we highlight the potential of rdf2pg as a versatile tool for enabling polyglot access to knowledge graphs, aligning with established standards of Linked Data and the Semantic Web.",
      "tldr_zh": "### è®ºæ–‡æ‘˜è¦ TLDR ğŸ“\n\n---\n\næœ¬ç ”ç©¶æå‡ºäº† `rdf2pg`ï¼Œä¸€ä¸ªæ—¨åœ¨å°† `RDF` æ•°æ®æ˜ å°„ä¸ºè¯­ä¹‰ç­‰æ•ˆçš„æ ‡ç­¾å±æ€§å›¾ (`LPG`) æ ¼å¼å’Œæ•°æ®åº“çš„å¯æ‰©å±•æ¡†æ¶ï¼Œä»¥æ•´åˆä¸¤ç§æ•°æ®ç®¡ç†æ–¹å¼çš„äº’è¡¥ä¼˜åŠ¿ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿå¯¹ `Virtuoso`ã€`Neo4j` å’Œ `ArcadeDB` ç­‰å›¾å½¢æ•°æ®åº“ä»¥åŠ `SPARQL`ã€`Cypher` å’Œ `Gremlin` æŸ¥è¯¢è¯­è¨€è¿›è¡Œäº†å®šæ€§å’Œå®šé‡çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚å®éªŒåˆ†ææ­ç¤ºäº†ä¸åŒå›¾å½¢æ•°æ®åº“æŠ€æœ¯çš„ä¼˜åŠ¿ä¸å±€é™æ€§ï¼Œå¹¶éªŒè¯äº† `rdf2pg` ä½œä¸ºå¤šè¯­è¨€ (`polyglot`) è®¿é—®çŸ¥è¯†å›¾è°±å·¥å…·çš„æ½œåŠ›ã€‚è¯¥æ–¹æ¡ˆä¸ä»…ç¬¦åˆ `Linked Data` å’Œè¯­ä¹‰ç½‘ (`Semantic Web`) æ ‡å‡†ï¼Œä¹Ÿä¸ºç®¡ç†éµå¾ª `FAIR` åŸåˆ™çš„ç”Ÿç‰©å­¦æ•°æ®æä¾›äº†çµæ´»çš„æŠ€æœ¯è·¯å¾„ã€‚\n\n---\n\nå¸Œæœ›è¿™ä¸ªæ‘˜è¦å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœä½ è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.17498v1",
      "published_date": "2025-05-23 05:51:00 UTC",
      "updated_date": "2025-05-23 05:51:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:05.290421+00:00"
    },
    {
      "arxiv_id": "2505.17496v1",
      "title": "Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models",
      "title_zh": "å£è¯­è¯­è¨€æ¨¡å‹ç«¯åˆ°ç«¯è®­ç»ƒä¸­çš„ç¾éš¾æ€§é—å¿˜ç¼“è§£ç­–ç•¥åˆ†æ",
      "authors": [
        "Chi-Yuan Hsiao",
        "Ke-Han Lu",
        "Kai-Wei Chang",
        "Chih-Kai Yang",
        "Wei-Chih Chen",
        "Hung-yi Lee"
      ],
      "abstract": "End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously acquired knowledge is lost. This paper investigates catastrophic forgetting and evaluates three mitigation strategies-model merging, discounting the LoRA scaling factor, and experience replay to balance knowledge retention with new learning. Results show that experience replay is the most effective, with further gains achieved by combining it with other methods. These findings provide insights for developing more robust and efficient SLM training pipelines.",
      "tldr_zh": "æœ¬ç ”ç©¶è°ƒæŸ¥äº†ç«¯åˆ°ç«¯è¯­éŸ³è¯­è¨€æ¨¡å‹ (Spoken Language Models, SLMs) åœ¨å¤šé˜¶æ®µæŒç»­å­¦ä¹ è¿‡ç¨‹ä¸­é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜ (catastrophic forgetting) é—®é¢˜ã€‚è®ºæ–‡è¯„ä¼°äº†ä¸‰ç§æ—¨åœ¨å¹³è¡¡çŸ¥è¯†ä¿ç•™ä¸æ–°å­¦ä¹ çš„ç¼“è§£ç­–ç•¥ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆå¹¶ (model merging)ã€é™ä½ LoRA ç¼©æ”¾å› å­ (discounting the LoRA scaling factor) ä»¥åŠç»éªŒå›æ”¾ (experience replay)ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»éªŒå›æ”¾æ˜¯å…¶ä¸­æœ€æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä¸”é€šè¿‡ä¸å…¶ä»–ç­–ç•¥ç»“åˆå¯ä»¥è·å¾—æ›´ä½³æ•ˆæœã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´ç¨³å¥å’Œé«˜æ•ˆçš„ SLM è®­ç»ƒæµæ°´çº¿æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17496v1",
      "published_date": "2025-05-23 05:50:14 UTC",
      "updated_date": "2025-05-23 05:50:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:08.825902+00:00"
    },
    {
      "arxiv_id": "2505.18215v1",
      "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?",
      "title_zh": "åœ¨å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£ï¼Œç±» BERT åŒå‘æ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä¸­æ˜¯å¦ä¾ç„¶è¡¨ç°æ›´ä¼˜ï¼Ÿ",
      "authors": [
        "Junyan Zhang",
        "Yiming Huang",
        "Shuliang Liu",
        "Yubo Gao",
        "Xuming Hu"
      ],
      "abstract": "The rapid adoption of LLMs has overshadowed the potential advantages of traditional BERT-like models in text classification. This study challenges the prevailing \"LLM-centric\" trend by systematically comparing three category methods, i.e., BERT-like models fine-tuning, LLM internal state utilization, and zero-shot inference across six high-difficulty datasets. Our findings reveal that BERT-like models often outperform LLMs. We further categorize datasets into three types, perform PCA and probing experiments, and identify task-specific model strengths: BERT-like models excel in pattern-driven tasks, while LLMs dominate those requiring deep semantics or world knowledge. Based on this, we propose TaMAS, a fine-grained task selection strategy, advocating for a nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ç³»ç»Ÿå¯¹æ¯”äº† BERT-like æ¨¡å‹å¾®è°ƒ (fine-tuning)ã€LLM å†…éƒ¨çŠ¶æ€åˆ©ç”¨ä»¥åŠé›¶æ ·æœ¬æ¨ç† (zero-shot inference) ä¸‰ç§æ–¹æ³•ï¼ŒæŒ‘æˆ˜äº†å½“å‰ç›²ç›®ä¾èµ– LLM çš„è¶‹åŠ¿ã€‚å®éªŒå‘ç°ï¼ŒBERT-like æ¨¡å‹åœ¨æ¨¡å¼é©±åŠ¨å‹ä»»åŠ¡ä¸­è¡¨ç°é€šå¸¸ä¼˜äº LLMï¼Œè€Œ LLM åœ¨éœ€è¦æ·±å±‚è¯­ä¹‰æˆ–ä¸–ç•ŒçŸ¥è¯† (world knowledge) çš„ä»»åŠ¡ä¸­æ›´å…·ä¼˜åŠ¿ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶è€…æå‡ºäº† TaMAS ä»»åŠ¡é€‰æ‹©ç­–ç•¥ï¼Œæå€¡æ ¹æ®ä»»åŠ¡ç»†ç²’åº¦ç‰¹å¾é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ï¼Œä¸ºä»»åŠ¡é©±åŠ¨çš„æ¨¡å‹é€‰æ‹©æä¾›äº†ç§‘å­¦ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18215v1",
      "published_date": "2025-05-23 05:46:42 UTC",
      "updated_date": "2025-05-23 05:46:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:12.283750+00:00"
    },
    {
      "arxiv_id": "2505.17495v2",
      "title": "ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs",
      "title_zh": "ProxySPEXï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ç¨€ç–ç‰¹å¾äº¤äº’çš„é«˜æ•ˆæ¨ç†å¯è§£é‡Šæ€§",
      "authors": [
        "Landon Butler",
        "Abhineet Agarwal",
        "Justin Singh Kang",
        "Yigit Efe Erginbas",
        "Bin Yu",
        "Kannan Ramchandran"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to $n \\approx 10^3$ features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often hierarchical -- higher-order interactions are accompanied by their lower-order subsets -- which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions. Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over marginal attribution approaches while using $10\\times$ fewer inferences than SPEX. By accounting for interactions, ProxySPEX efficiently identifies the most influential features, providing a scalable approximation of their Shapley values. Further, we apply ProxySPEX to two interpretability tasks. Data attribution, where we identify interactions among CIFAR-10 training samples that influence test predictions, and mechanistic interpretability, where we uncover interactions between attention heads, both within and across layers, on a question-answering task.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯†åˆ« Large Language Models (LLMs) ä¸­ç‰¹å¾äº¤äº’çš„é«˜è®¡ç®—å¼€é”€é—®é¢˜ï¼Œæå‡ºäº† ProxySPEX ç®—æ³•ã€‚åŸºäºç‰¹å¾äº¤äº’é€šå¸¸å…·æœ‰å±‚çº§æ€§ï¼ˆhierarchicalï¼‰çš„è§‚å¯Ÿï¼ŒProxySPEX é¦–å…ˆå°†æ¢¯åº¦æå‡æ ‘ (gradient boosted trees) æ‹Ÿåˆåˆ°æ©ç å¤„ç†åçš„æ¨¡å‹è¾“å‡ºï¼Œè¿›è€Œæå–å…³é”®äº¤äº’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†æ¬¡æ•°æ¯” SPEX å‡å°‘ 10 å€çš„åŒæ—¶ï¼Œé‡å»ºå‡†ç¡®åº¦æå‡äº† 20%ï¼Œå¹¶èƒ½é«˜æ•ˆè¿‘ä¼¼ç‰¹å¾çš„ Shapley valuesã€‚æ­¤å¤–ï¼ŒProxySPEX åœ¨æ•°æ®å½’å›  (Data attribution) å’Œæœºæ¢°è§£é‡Šæ€§ (mechanistic interpretability) ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿæ­ç¤ºæ³¨æ„åŠ›å¤´ä¹‹é—´å¤æ‚çš„è·¨å±‚äº¤äº’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Algorithm available at: https://github.com/mmschlk/shapiq",
      "pdf_url": "https://arxiv.org/pdf/2505.17495v2",
      "published_date": "2025-05-23 05:44:01 UTC",
      "updated_date": "2025-10-23 19:11:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:23.983229+00:00"
    },
    {
      "arxiv_id": "2505.17492v1",
      "title": "PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate",
      "title_zh": "PD$^3$ï¼šåŸºäºé€‚é…å¤šæ™ºèƒ½ä½“è¾©è®ºçš„é¡¹ç›®æŸ¥é‡æ£€æµ‹æ¡†æ¶",
      "authors": [
        "Dezheng Bao",
        "Yueci Yang",
        "Xin Chen",
        "Zhengxuan Jiang",
        "Zeguo Fei",
        "Daoze Zhang",
        "Xuanwen Huang",
        "Junru Chen",
        "Chutian Yu",
        "Xiang Yuan",
        "Yang Yang"
      ],
      "abstract": "Project duplication detection is critical for project quality assessment, as it improves resource utilization efficiency by preventing investing in newly proposed project that have already been studied. It requires the ability to understand high-level semantics and generate constructive and valuable feedback. Existing detection methods rely on basic word- or sentence-level comparison or solely apply large language models, lacking valuable insights for experts and in-depth comprehension of project content and review criteria. To tackle this issue, we propose PD$^3$, a Project Duplication Detection framework via adapted multi-agent Debate. Inspired by real-world expert debates, it employs a fair competition format to guide multi-agent debate to retrieve relevant projects. For feedback, it incorporates both qualitative and quantitative analysis to improve its practicality. Over 800 real-world power project data spanning more than 20 specialized fields are used to evaluate the framework, demonstrating that our method outperforms existing approaches by 7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online platform, Review Dingdang, to assist power experts, saving 5.73 million USD in initial detection on more than 100 newly proposed projects.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PD$^3$ï¼Œä¸€ç§é€šè¿‡æ”¹è¿›çš„å¤šæ™ºèƒ½ä½“è¾©è®º (multi-agent Debate) å®ç°çš„é¡¹ç›®æŸ¥é‡æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨é«˜å±‚è¯­ä¹‰ç†è§£å’Œä¸“å®¶è§è§£æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶å—ç°å®ä¸“å®¶è¾©è®ºå¯å‘ï¼Œé€šè¿‡å…¬å¹³ç«äº‰æœºåˆ¶å¼•å¯¼å¤šæ™ºèƒ½ä½“æ£€ç´¢ç›¸å…³é¡¹ç›®ï¼Œå¹¶ç»“åˆå®šæ€§ä¸å®šé‡åˆ†æ (qualitative and quantitative analysis) æä¾›å…·æœ‰å®è·µä»·å€¼çš„åé¦ˆã€‚åœ¨æ¶µç›– 20 å¤šä¸ªä¸“ä¸šé¢†åŸŸçš„ 800 å¤šä¸ªçœŸå®ç”µåŠ›é¡¹ç›®æ•°æ®ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒPD$^3$ åœ¨ä¸¤é¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­åˆ†åˆ«æ¯”ç°æœ‰æ–¹æ³•æå‡äº† 7.43% å’Œ 8.00%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å·²åº”ç”¨äºåœ¨çº¿å¹³å° Review Dingdangï¼Œåœ¨ 100 å¤šä¸ªæ–°é¡¹ç›®çš„åˆæ­¥æ£€æµ‹ä¸­ä¸ºç”µåŠ›ä¸“å®¶èŠ‚çœäº†çº¦ 573 ä¸‡ç¾å…ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17492v1",
      "published_date": "2025-05-23 05:38:37 UTC",
      "updated_date": "2025-05-23 05:38:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:49.693438+00:00"
    },
    {
      "arxiv_id": "2505.17491v2",
      "title": "HiLAB: A Hybrid Inverse-Design Framework",
      "title_zh": "HiLABï¼šæ··åˆé€†å‘è®¾è®¡æ¡†æ¶",
      "authors": [
        "Reza Marzban",
        "Hamed Abiri",
        "Raphael Pestourie",
        "Ali Adibi"
      ],
      "abstract": "HiLAB (Hybrid inverse-design with Latent-space learning, Adjoint-based partial optimizations, and Bayesian optimization) is a new paradigm for inverse design of nanophotonic structures. Combining early-terminated topological optimization (TO) with a Vision Transformer-based variational autoencoder (VAE) and a Bayesian search, HiLAB addresses multi-functional device design by generating diverse freeform configurations at reduced simulation costs. Shortened adjoint-driven TO runs, coupled with randomized physical parameters, produce robust initial structures. These structures are compressed into a compact latent space by the VAE, enabling Bayesian optimization to co-optimize geometry and physical hyperparameters. Crucially, the trained VAE can be reused for alternative objectives or constraints by adjusting only the acquisition function. Compared to conventional TO pipelines prone to local optima, HiLAB systematically explores near-global optima with considerably fewer electromagnetic simulations. Even after accounting for training overhead, the total number of full simulations decreases by over an order of magnitude, accelerating the discovery of fabrication-friendly devices. Demonstrating its efficacy, HiLAB is used to design an achromatic beam deflector for red, green, and blue wavelengths, achieving balanced diffraction efficiencies of ~25% while mitigating chromatic aberrations-a performance surpassing existing demonstrations. Overall, HiLAB provides a flexible platform for robust, multi-parameter photonic designs and rapid adaptation to next-generation nanophotonic challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HiLABï¼Œä¸€ç§ç»“åˆäº† Latent-space learningã€Adjoint-based partial optimizations å’Œ Bayesian optimization çš„æ··åˆé€†å‘è®¾è®¡ (inverse-design) æ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆå¼€å‘çº³ç±³å…‰å­ç»“æ„ã€‚è¯¥æ¡†æ¶é€šè¿‡ Vision Transformer æ¶æ„çš„å˜åˆ†è‡ªç¼–ç å™¨ (VAE) å‹ç¼©ç”±æ—©æœŸç»ˆæ­¢çš„æ‹“æ‰‘ä¼˜åŒ– (TO) ç”Ÿæˆçš„å€™é€‰ç»“æ„ï¼Œå¹¶åˆ©ç”¨è´å¶æ–¯æœç´¢åœ¨æ½œç©ºé—´å†…è¿›è¡ŒååŒä¼˜åŒ–ã€‚ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼ŒHiLAB åœ¨æ˜¾è‘—é™ä½ä»¿çœŸæˆæœ¬ï¼ˆé™å¹…è¾¾ä¸€ä¸ªæ•°é‡çº§ä»¥ä¸Šï¼‰çš„åŒæ—¶ï¼Œå±•ç°äº†æå¼ºçš„è·¨ä»»åŠ¡å¤ç”¨æ€§ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒHiLAB æˆåŠŸè®¾è®¡å‡ºä¸€ç§ RGB ä¸‰è‰²æ¶ˆè‰²å·®å…‰æŸåè½¬å™¨ï¼Œå…¶å‡è¡¡è¡å°„æ•ˆç‡è¾¾åˆ°çº¦ 25%ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰ç ”ç©¶ï¼Œä¸ºå¤šå‚æ•°å…‰å­å™¨ä»¶è®¾è®¡æä¾›äº†é«˜æ•ˆä¸”çµæ´»çš„å¹³å°ã€‚",
      "categories": [
        "physics.optics",
        "cs.AI",
        "physics.app-ph"
      ],
      "primary_category": "physics.optics",
      "comment": "20 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17491v2",
      "published_date": "2025-05-23 05:34:56 UTC",
      "updated_date": "2025-09-16 14:30:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:48.429189+00:00"
    },
    {
      "arxiv_id": "2505.17490v2",
      "title": "DTRT: Enhancing Human Intent Estimation and Role Allocation for Physical Human-Robot Collaboration",
      "title_zh": "DTRTï¼šå¢å¼ºç‰©ç†äººæœºåä½œä¸­çš„äººç±»æ„å›¾ä¼°è®¡ä¸è§’è‰²åˆ†é…",
      "authors": [
        "Haotian Liu",
        "Yuchuang Tong",
        "Zhengtao Zhang"
      ],
      "abstract": "In physical Human-Robot Collaboration (pHRC), accurate human intent estimation and rational human-robot role allocation are crucial for safe and efficient assistance. Existing methods that rely on short-term motion data for intention estimation lack multi-step prediction capabilities, hindering their ability to sense intent changes and adjust human-robot assignments autonomously, resulting in potential discrepancies. To address these issues, we propose a Dual Transformer-based Robot Trajectron (DTRT) featuring a hierarchical architecture, which harnesses human-guided motion and force data to rapidly capture human intent changes, enabling accurate trajectory predictions and dynamic robot behavior adjustments for effective collaboration. Specifically, human intent estimation in DTRT uses two Transformer-based Conditional Variational Autoencoders (CVAEs), incorporating robot motion data in obstacle-free case with human-guided trajectory and force for obstacle avoidance. Additionally, Differential Cooperative Game Theory (DCGT) is employed to synthesize predictions based on human-applied forces, ensuring robot behavior align with human intention. Compared to state-of-the-art (SOTA) methods, DTRT incorporates human dynamics into long-term prediction, providing an accurate understanding of intention and enabling rational role allocation, achieving robot autonomy and maneuverability. Experiments demonstrate DTRT's accurate intent estimation and superior collaboration performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©ç†äººæœºåä½œ(pHRC)ä¸­æ„å›¾ä¼°è®¡ç¼ºä¹å¤šæ­¥é¢„æµ‹èƒ½åŠ›ä¸”éš¾ä»¥åŠ¨æ€è°ƒæ•´è§’è‰²åˆ†é…çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºåŒTransformerçš„æœºå™¨äººè½¨è¿¹é¢„æµ‹æ¨¡å‹(DTRT)ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œåˆ©ç”¨äººç±»å¼•å¯¼çš„è¿åŠ¨å’ŒåŠ›æ•°æ®å®æ—¶æ•æ‰æ„å›¾å˜åŒ–ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„è½¨è¿¹é¢„æµ‹å’Œæœºå™¨äººè¡Œä¸ºçš„åŠ¨æ€è°ƒæ•´ã€‚å…·ä½“è€Œè¨€ï¼ŒDTRTåˆ©ç”¨ä¸¤ä¸ªåŸºäºTransformerçš„æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨(CVAEs)è¿›è¡Œæ„å›¾ä¼°è®¡ï¼Œå¹¶ç»“åˆå¾®åˆ†åä½œåšå¼ˆè®º(DCGT)æ ¹æ®äººç±»æ–½åŠ çš„åŠ›åˆæˆé¢„æµ‹ç»“æœï¼Œç¡®ä¿æœºå™¨äººè¡Œä¸ºä¸äººç±»æ„å›¾ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDTRTèƒ½å¤Ÿå°†äººç±»åŠ¨åŠ›å­¦èå…¥é•¿æœŸé¢„æµ‹ä¸­ï¼Œåœ¨å®ç°å‡†ç¡®æ„å›¾ä¼°è®¡çš„åŒæ—¶ä¿è¯äº†åˆç†çš„è§’è‰²åˆ†é…ï¼Œåä½œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(SOTA)æ–¹æ³•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by ICRA 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17490v2",
      "published_date": "2025-05-23 05:33:59 UTC",
      "updated_date": "2025-05-26 15:15:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:49.428321+00:00"
    },
    {
      "arxiv_id": "2505.17485v1",
      "title": "keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection",
      "title_zh": "keepitsimple åœ¨ SemEval-2025 Task 3ï¼šåŸºäº LLM ä¸ç¡®å®šæ€§çš„å¤šè¯­è¨€å¹»è§‰ç‰‡æ®µæ£€æµ‹æ–¹æ³•",
      "authors": [
        "Saketh Reddy Vemula",
        "Parameswari Krishnamurthy"
      ],
      "abstract": "Identification of hallucination spans in black-box language model generated text is essential for applications in the real world. A recent attempt at this direction is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on Hallucinations and Related Observable Over-generation Errors. In this work, we present our solution to this problem, which capitalizes on the variability of stochastically-sampled responses in order to identify hallucinated spans. Our hypothesis is that if a language model is certain of a fact, its sampled responses will be uniform, while hallucinated facts will yield different and conflicting results. We measure this divergence through entropy-based analysis, allowing for accurate identification of hallucinated segments. Our method is not dependent on additional training and hence is cost-effective and adaptable. In addition, we conduct extensive hyperparameter tuning and perform error analysis, giving us crucial insights into model behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ SemEval-2025 Task 3 (Mu-SHROOM) æå‡ºäº†åä¸º keepitsimple çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨æ£€æµ‹å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„å¹»è§‰ç‰‡æ®µ (Multilingual Hallucination Span Detection)ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹éšæœºé‡‡æ ·å›å¤ (stochastically-sampled responses) çš„å˜å¼‚æ€§æ¥è¯†åˆ«å¹»è§‰ï¼Œå…¶æ ¸å¿ƒå‡è®¾æ˜¯æ¨¡å‹å¯¹ç¡®å®šäº‹å®çš„å›å¤å…·æœ‰ä¸€è‡´æ€§ï¼Œè€Œå¹»è§‰å†…å®¹åˆ™ä¼šäº§ç”Ÿå†²çªã€‚ç ”ç©¶é€šè¿‡åŸºäºç†µ (entropy-based) çš„åˆ†ææ¥è¡¡é‡è¿™ç§å‘æ•£æ€§ï¼Œä»è€Œå®ç°å¯¹å¹»è§‰ç‰‡æ®µçš„ç²¾ç¡®è¯†åˆ«ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œå…·æœ‰æé«˜çš„æˆæœ¬æ•ˆç›Šå’Œé€‚é…æ€§ï¼Œå¹¶é€šè¿‡è¶…å‚æ•°è°ƒä¼˜ä¸é”™è¯¯åˆ†æä¸ºæ¨¡å‹è¡Œä¸ºæä¾›äº†æ·±å…¥è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17485v1",
      "published_date": "2025-05-23 05:25:14 UTC",
      "updated_date": "2025-05-23 05:25:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:47.422969+00:00"
    },
    {
      "arxiv_id": "2505.17484v1",
      "title": "Anatomy-Guided Multitask Learning for MRI-Based Classification of Placenta Accreta Spectrum and its Subtypes",
      "title_zh": "è§£å‰–å¼•å¯¼çš„å¤šä»»åŠ¡å­¦ä¹ ï¼šåŸºäº MRI çš„èƒç›˜æ¤å…¥è°±ç³»åŠå…¶äºšå‹åˆ†ç±»",
      "authors": [
        "Hai Jiang",
        "Qiongting Liu",
        "Yuanpin Zhou",
        "Jiawei Pan",
        "Ting Song",
        "Yao Lu"
      ],
      "abstract": "Placenta Accreta Spectrum Disorders (PAS) pose significant risks during pregnancy, frequently leading to postpartum hemorrhage during cesarean deliveries and other severe clinical complications, with bleeding severity correlating to the degree of placental invasion. Consequently, accurate prenatal diagnosis of PAS and its subtypes-placenta accreta (PA), placenta increta (PI), and placenta percreta (PP)-is crucial. However, existing guidelines and methodologies predominantly focus on the presence of PAS, with limited research addressing subtype recognition. Additionally, previous multi-class diagnostic efforts have primarily relied on inefficient two-stage cascaded binary classification tasks. In this study, we propose a novel convolutional neural network (CNN) architecture designed for efficient one-stage multiclass diagnosis of PAS and its subtypes, based on 4,140 magnetic resonance imaging (MRI) slices. Our model features two branches: the main classification branch utilizes a residual block architecture comprising multiple residual blocks, while the second branch integrates anatomical features of the uteroplacental area and the adjacent uterine serous layer to enhance the model's attention during classification. Furthermore, we implement a multitask learning strategy to leverage both branches effectively. Experiments conducted on a real clinical dataset demonstrate that our model achieves state-of-the-art performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹å·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ¶æ„ï¼Œç”¨äºèƒç›˜æ¤å…¥è°±ç³»ç–¾ç—… (Placenta Accreta Spectrum, PAS) åŠå…¶äºšå‹ï¼ˆPAã€PI å’Œ PPï¼‰çš„é«˜æ•ˆä¸€é˜¶æ®µå¤šåˆ†ç±»è¯Šæ–­ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•åœ¨äºšå‹è¯†åˆ«ä¸Šçš„å±€é™æ€§ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯ç»“æ„ï¼Œå°†åŸºäºæ®‹å·®å— (residual block) çš„åˆ†ç±»åˆ†æ”¯ä¸æ•´åˆå­å®«èƒç›˜åŒºåŸŸè§£å‰–ç‰¹å¾çš„æ³¨æ„åŠ›åˆ†æ”¯ç›¸ç»“åˆã€‚é€šè¿‡åº”ç”¨å¤šä»»åŠ¡å­¦ä¹  (multitask learning) ç­–ç•¥ï¼Œæ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨ 4,140 å¼  MRI åˆ‡ç‰‡ç²¾å‡†æ•æ‰å…³é”®è§£å‰–ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¸´åºŠæ•°æ®é›†ä¸Šè¾¾åˆ°äº†å½“å‰é¢†å…ˆ (state-of-the-art) çš„æ€§èƒ½ï¼Œä¸ºå¤æ‚å¦Šå¨ å¹¶å‘ç—‡çš„äº§å‰è¯Šæ–­æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17484v1",
      "published_date": "2025-05-23 05:22:30 UTC",
      "updated_date": "2025-05-23 05:22:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:36:59.450621+00:00"
    },
    {
      "arxiv_id": "2505.17482v1",
      "title": "From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark",
      "title_zh": "ä»æ¨ç†åˆ°æ³›åŒ–ï¼šé¢å‘ ARC åŸºå‡†æµ‹è¯•çš„çŸ¥è¯†å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Chao Lei",
        "Nir Lipovetzky",
        "Krista A. Ehinger",
        "Yanchuan Chang"
      ],
      "abstract": "Recent reasoning-oriented LLMs have demonstrated strong performance on challenging tasks such as mathematics and science examinations. However, core cognitive faculties of human intelligence, such as abstract reasoning and generalization, remain underexplored. To address this, we evaluate recent reasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC) benchmark, which explicitly demands both faculties. We formulate ARC as a program synthesis task and propose nine candidate solvers. Experimental results show that repeated-sampling planning-aided code generation (RSPC) achieves the highest test accuracy and demonstrates consistent generalization across most LLMs. To further improve performance, we introduce an ARC solver, Knowledge Augmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors within an ontology that classifies priors into three hierarchical levels based on their dependencies. KAAR progressively expands LLM reasoning capacity by gradually augmenting priors at each level, and invokes RSPC to generate candidate solutions after each augmentation stage. This stage-wise reasoning reduces interference from irrelevant priors and improves LLM performance. Empirical results show that KAAR maintains strong generalization and consistently outperforms non-augmented RSPC across all evaluated LLMs, achieving around 5% absolute gains and up to 64.52% relative improvement. Despite these achievements, ARC remains a challenging benchmark for reasoning-oriented LLMs, highlighting future avenues of progress in LLMs.",
      "tldr_zh": "---\n\n### è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“„\n\nè¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æŠ½è±¡æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„å±€é™ï¼Œè¯„ä¼°äº†æ¨ç†å¯¼å‘å‹æ¨¡å‹åœ¨ Abstraction and Reasoning Corpus (ARC) åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚ä½œè€…å°† ARC ä»»åŠ¡å»ºæ¨¡ä¸ºç¨‹åºåˆæˆ (program synthesis) ä»»åŠ¡ï¼Œå¹¶æå‡ºäº† RSPC (repeated-sampling planning-aided code generation) æ¡†æ¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†åä¸º KAAR (Knowledge Augmentation for Abstract Reasoning) çš„æ±‚è§£å™¨ï¼Œé€šè¿‡ä¸‰å±‚å±‚çº§çš„æœ¬ä½“è®º (ontology) ç¼–ç æ ¸å¿ƒçŸ¥è¯†å…ˆéªŒã€‚KAAR é‡‡ç”¨é˜¶æ®µå¼æ¨ç† (stage-wise reasoning) é€æ­¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæœ‰æ•ˆå‡å°‘äº†æ— å…³å…ˆéªŒçš„å¹²æ‰°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKAAR åœ¨æ‰€æœ‰è¯„ä¼°çš„ LLMs ä¸Šå‡ä¿æŒäº†å¼ºæ³›åŒ–æ€§ï¼Œç›¸æ¯”éå¢å¼ºçš„ RSPC å®ç°äº†çº¦ 5% çš„ç»å¯¹ç²¾åº¦æå‡å’Œæœ€é«˜ 64.52% çš„ç›¸å¯¹æ”¹è¿›ã€‚\n\n---\n\nå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦å¤„ç†ï¼Œæˆ–è€…æƒ³é’ˆå¯¹ KAAR çš„å…·ä½“æ¶æ„è¿›è¡Œæ›´æ·±å…¥çš„æ¢è®¨ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17482v1",
      "published_date": "2025-05-23 05:21:14 UTC",
      "updated_date": "2025-05-23 05:21:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:37:07.388971+00:00"
    },
    {
      "arxiv_id": "2505.21523v3",
      "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models",
      "title_zh": "æ€è€ƒè¶Šå¤šï¼Œè§‚å¯Ÿè¶Šå°‘ï¼Ÿè¯„ä¼°å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ä¸­çš„å¹»è§‰åŠ å‰§ç°è±¡",
      "authors": [
        "Chengzhi Liu",
        "Zhongxing Xu",
        "Qingyue Wei",
        "Juncheng Wu",
        "James Zou",
        "Xin Eric Wang",
        "Yuyin Zhou",
        "Sheng Liu"
      ],
      "abstract": "Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åˆ©ç”¨æ¨ç†è®¡ç®—ï¼ˆtest-time computeï¼‰ç”Ÿæˆé•¿æ¨ç†é“¾æ—¶ï¼Œå› è¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒè€Œå¯¼è‡´çš„å¹»è§‰ï¼ˆhallucinationï¼‰åŠ å‰§é—®é¢˜ã€‚é€šè¿‡æ³¨æ„åŠ›åˆ†æå‘ç°ï¼Œé•¿æ¨ç†é“¾ä¼šé™ä½æ¨¡å‹å¯¹è§†è§‰è¾“å…¥çš„å…³æ³¨ï¼Œä¸ºæ­¤ä½œè€…æå‡ºäº†é‡åŒ–æ„ŸçŸ¥å‡†ç¡®åº¦éšæ¨ç†é•¿åº¦å˜åŒ–çš„æŒ‡æ ‡ RH-AUCï¼Œä»¥åŠè¯Šæ–­æ€§åŸºå‡†æµ‹è¯• RH-Benchã€‚ç ”ç©¶æ­ç¤ºäº†æ¨¡å‹è§„æ¨¡è¶Šå¤§ï¼Œåœ¨æ¨ç†èƒ½åŠ›ä¸æ„ŸçŸ¥å¿ å®åº¦ï¼ˆperceptual fidelityï¼‰ä¹‹é—´çš„å¹³è¡¡è¡¨ç°è¶Šå¥½ï¼Œä¸”è¿™ç§å¹³è¡¡å—è®­ç»ƒæ•°æ®ç±»å‹å’Œé¢†åŸŸçš„å½±å“ç¨‹åº¦æ˜¾è‘—è¶…è¿‡äº†æ•°æ®æ€»é‡ã€‚è¯¥æˆæœå¼ºè°ƒäº†åœ¨è¯„ä¼°æ¡†æ¶ä¸­åŒæ—¶è€ƒè™‘æ¨ç†è´¨é‡ä¸è§†è§‰å®šä½ï¼ˆvisual groundingï¼‰çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21523v3",
      "published_date": "2025-05-23 05:08:40 UTC",
      "updated_date": "2025-06-20 08:41:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:37:23.904537+00:00"
    },
    {
      "arxiv_id": "2505.17479v1",
      "title": "Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions",
      "title_zh": "Twin-2K-500ï¼šåŸºäº 2,000 ä½™äººå¯¹ 500 ä½™ä¸ªé—®é¢˜å›ç­”çš„æ•°å­—å­ªç”Ÿæ„å»ºæ•°æ®é›†",
      "authors": [
        "Olivier Toubia",
        "George Z. Gui",
        "Tianyi Peng",
        "Daniel J. Merlau",
        "Ang Li",
        "Haozhe Chen"
      ],
      "abstract": "LLM-based digital twin simulation, where large language models are used to emulate individual human behavior, holds great promise for research in AI, social science, and digital experimentation. However, progress in this area has been hindered by the scarcity of real, individual-level datasets that are both large and publicly available. This lack of high-quality ground truth limits both the development and validation of digital twin methodologies. To address this gap, we introduce a large-scale, public dataset designed to capture a rich and holistic view of individual human behavior. We survey a representative sample of $N = 2,058$ participants (average 2.42 hours per person) in the US across four waves with 500 questions in total, covering a comprehensive battery of demographic, psychological, economic, personality, and cognitive measures, as well as replications of behavioral economics experiments and a pricing survey. The final wave repeats tasks from earlier waves to establish a test-retest accuracy baseline. Initial analyses suggest the data are of high quality and show promise for constructing digital twins that predict human behavior well at the individual and aggregate levels. By making the full dataset publicly available, we aim to establish a valuable testbed for the development and benchmarking of LLM-based persona simulations. Beyond LLM applications, due to its unique breadth and scale the dataset also enables broad social science research, including studies of cross-construct correlations and heterogeneous treatment effects.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Twin-2K-500ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å…¬å¼€æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºæ„å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿä¸ªä½“è¡Œä¸ºçš„å¤§è¯­è¨€æ¨¡å‹(LLM)æ•°å­—å­ªç”Ÿ(Digital Twins)æä¾›é«˜è´¨é‡çš„åŸºå‡†æ•°æ®ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†ç¾å›½ 2,058 åå…·æœ‰ä»£è¡¨æ€§çš„å—è¯•è€…ï¼Œé€šè¿‡å››ä¸ªé˜¶æ®µçš„è°ƒæŸ¥æ”¶é›†äº†è¶…è¿‡ 500 ä¸ªç»´åº¦çš„é—®é¢˜ï¼Œå†…å®¹æ¨ªè·¨äººå£ç»Ÿè®¡å­¦ã€å¿ƒç†å­¦ã€ç»æµå­¦ã€äººæ ¼ç‰¹è´¨å’Œè®¤çŸ¥æµ‹é‡ç­‰é¢†åŸŸã€‚ç ”ç©¶ä¸ä»…åŒ…å«äº†è¡Œä¸ºç»æµå­¦å®éªŒçš„å¤åˆ¶ï¼Œè¿˜é€šè¿‡é‡å¤æµ‹è¯•ç¡®ç«‹äº†é‡æµ‹ä¿¡åº¦åŸºçº¿ã€‚åˆæ­¥åˆ†æè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†èƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹ä¸ªä½“åŠç¾¤ä½“å±‚é¢çš„è¡Œä¸ºï¼Œä¸º LLM äººæ ¼æ¨¡æ‹Ÿ(Persona Simulations)çš„å¼€å‘ä¸åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠå¹¿æ³›çš„ç¤¾ä¼šç§‘å­¦ç ”ç©¶æä¾›äº†å®è´µçš„å®éªŒèµ„æºã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "econ.EM"
      ],
      "primary_category": "cs.CY",
      "comment": "Also available at SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5265253",
      "pdf_url": "https://arxiv.org/pdf/2505.17479v1",
      "published_date": "2025-05-23 05:05:11 UTC",
      "updated_date": "2025-05-23 05:05:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:37:15.847015+00:00"
    },
    {
      "arxiv_id": "2505.17478v2",
      "title": "ConfRover: Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression",
      "title_zh": "ConfRoverï¼šåŸºäºè‡ªå›å½’çš„è›‹ç™½è´¨æ„è±¡ä¸åŠ¨åŠ›å­¦åŒæ­¥å»ºæ¨¡",
      "authors": [
        "Yuning Shen",
        "Lihao Wang",
        "Huizhuo Yuan",
        "Yan Wang",
        "Bangji Yang",
        "Quanquan Gu"
      ],
      "abstract": "Understanding protein dynamics is critical for elucidating their biological functions. The increasing availability of molecular dynamics (MD) data enables the training of deep generative models to efficiently explore the conformational space of proteins. However, existing approaches either fail to explicitly capture the temporal dependencies between conformations or do not support direct generation of time-independent samples. To address these limitations, we introduce ConfRover, an autoregressive model that simultaneously learns protein conformation and dynamics from MD trajectories, supporting both time-dependent and time-independent sampling. At the core of our model is a modular architecture comprising: (i) an encoding layer, adapted from protein folding models, that embeds protein-specific information and conformation at each time frame into a latent space; (ii) a temporal module, a sequence model that captures conformational dynamics across frames; and (iii) an SE(3) diffusion model as the structure decoder, generating conformations in continuous space. Experiments on ATLAS, a large-scale protein MD dataset of diverse structures, demonstrate the effectiveness of our model in learning conformational dynamics and supporting a wide range of downstream tasks. ConfRover is the first model to sample both protein conformations and trajectories within a single framework, offering a novel and flexible approach for learning from protein MD data. Project website: https://bytedance-seed.github.io/ConfRover.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ConfRoverï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè‡ªå›å½’ (Autoregression) åŒæ—¶å»ºæ¨¡è›‹ç™½è´¨æ„è±¡ (Conformation) å’ŒåŠ¨åŠ›å­¦ (Dynamics) çš„æ–°å‹æ¡†æ¶ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•åœ¨æ•æ‰æ—¶é—´ä¾èµ–æ€§å’Œç‹¬ç«‹æ—¶é—´é‡‡æ ·æ–¹é¢çš„å±€é™ï¼ŒConfRover æ•´åˆäº†è›‹ç™½è´¨ç¼–ç å±‚ã€æ•æ‰è·¨å¸§åŠ¨æ€çš„æ—¶é—´åºåˆ—æ¨¡å—ï¼Œä»¥åŠç”¨äºè¿ç»­ç©ºé—´ç”Ÿæˆçš„ SE(3) æ‰©æ•£æ¨¡å‹ (Diffusion Model) è§£ç å™¨ã€‚åœ¨ ATLAS å¤§è§„æ¨¡åˆ†å­åŠ¨åŠ›å­¦ (MD) æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å­¦ä¹ æ„è±¡åŠ¨åŠ›å­¦åŠæ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢è¡¨ç°å“è¶Šã€‚ä½œä¸ºé¦–ä¸ªèƒ½åœ¨å•ä¸€æ¡†æ¶ä¸‹åŒæ—¶é‡‡æ ·è›‹ç™½è´¨æ„è±¡å’Œè½¨è¿¹ (Trajectories) çš„æ¨¡å‹ï¼ŒConfRover ä¸ºç ”ç©¶è›‹ç™½è´¨åˆ†å­åŠ¨åŠ›å­¦æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”çµæ´»çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.bio-ph",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "35 pages, 17 figures; Camera ready for NeurIPS 2025; Website: https://bytedance-seed.github.io/ConfRover",
      "pdf_url": "https://arxiv.org/pdf/2505.17478v2",
      "published_date": "2025-05-23 05:00:15 UTC",
      "updated_date": "2025-12-02 22:16:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:37:17.523388+00:00"
    },
    {
      "arxiv_id": "2505.17473v5",
      "title": "InfoDet: A Dataset for Infographic Element Detection",
      "title_zh": "InfoDetï¼šä¿¡æ¯å›¾å…ƒç´ æ£€æµ‹æ•°æ®é›†",
      "authors": [
        "Jiangning Zhu",
        "Yuxing Zhou",
        "Zheng Wang",
        "Juntao Yao",
        "Yima Gu",
        "Yuhui Yuan",
        "Shixia Liu"
      ],
      "abstract": "Given the central role of charts in scientific, business, and communication contexts, enhancing the chart understanding capabilities of vision-language models (VLMs) has become increasingly critical. A key limitation of existing VLMs lies in their inaccurate visual grounding of infographic elements, including charts and human-recognizable objects (HROs) such as icons and images. However, chart understanding often requires identifying relevant elements and reasoning over them. To address this limitation, we introduce InfoDet, a dataset designed to support the development of accurate object detection models for charts and HROs in infographics. It contains 11,264 real and 90,000 synthetic infographics, with over 14 million bounding box annotations. These annotations are created by combining the model-in-the-loop and programmatic methods. We demonstrate the usefulness of InfoDet through three applications: 1) constructing a Thinking-with-Boxes scheme to boost the chart understanding performance of VLMs, 2) comparing existing object detection models, and 3) applying the developed detection model to document layout and UI element detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InfoDetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæå‡è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) å¯¹ä¿¡æ¯å›¾è¡¨ä¸­å›¾è¡¨åŠäººç±»å¯è¯†åˆ«å¯¹è±¡ (HROsï¼Œå¦‚å›¾æ ‡å’Œå›¾åƒ) è§†è§‰å®šä½èƒ½åŠ›çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å« 11,264 å¼ çœŸå®å’Œ 90,000 å¼ åˆæˆä¿¡æ¯å›¾ï¼Œæ¶µç›–è¶…è¿‡ 1,400 ä¸‡ä¸ªè¾¹ç•Œæ¡† (bounding box) æ ‡æ³¨ï¼Œé€šè¿‡æ¨¡å‹åœ¨ç¯ (model-in-the-loop) å’Œç¨‹åºåŒ–æ–¹æ³•æ„å»ºè€Œæˆã€‚é€šè¿‡å¼•å…¥ Thinking-with-Boxes æ–¹æ¡ˆï¼ŒInfoDet æ˜¾è‘—å¢å¼ºäº† VLMs çš„å›¾è¡¨ç†è§£æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æ–‡æ¡£å¸ƒå±€åˆ†æå’Œ UI å…ƒç´ æ£€æµ‹ä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚è¯¥æ•°æ®é›†ä¸ºå¼€å‘é«˜ç²¾åº¦ä¿¡æ¯å›¾ç›®æ ‡æ£€æµ‹æ¨¡å‹æä¾›äº†é‡è¦æ”¯æŒï¼Œå¡«è¡¥äº†ç°æœ‰æ¨¡å‹åœ¨å¤æ‚å›¾è¡¨å…ƒç´ è¯†åˆ«æ–¹é¢çš„ç©ºç™½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2505.17473v5",
      "published_date": "2025-05-23 04:56:07 UTC",
      "updated_date": "2025-10-16 09:10:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:37:24.259606+00:00"
    },
    {
      "arxiv_id": "2505.17470v1",
      "title": "SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models",
      "title_zh": "SLearnLLMï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆé¢†åŸŸé€‚é…çš„è‡ªå­¦ä¹ æ¡†æ¶",
      "authors": [
        "Xiang Liu",
        "Zhaoxiang Liu",
        "Peng Wang",
        "Kohou Wang",
        "Huan Hu",
        "Kai Wang",
        "Shiguo Lian"
      ],
      "abstract": "When using supervised fine-tuning (SFT) to adapt large language models (LLMs) to specific domains, a significant challenge arises: should we use the entire SFT dataset for fine-tuning? Common practice often involves fine-tuning directly on the entire dataset due to limited information on the LLM's past training data. However, if the SFT dataset largely overlaps with the model's existing knowledge, the performance gains are minimal, leading to wasted computational resources. Identifying the unknown knowledge within the SFT dataset and using it to fine-tune the model could substantially improve the training efficiency. To address this challenge, we propose a self-learning framework for LLMs inspired by human learning pattern. This framework takes a fine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer the questions in the SFT dataset. The LLMs then objectively grade the responses and filter out the incorrectly answered QA pairs. Finally, we fine-tune the LLMs based on this filtered QA set. Experimental results in the fields of agriculture and medicine demonstrate that our method substantially reduces training time while achieving comparable improvements to those attained with full dataset fine-tuning. By concentrating on the unknown knowledge within the SFT dataset, our approach enhances the efficiency of fine-tuning LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SLearnLLMï¼Œä¸€ç§å—äººç±»å­¦ä¹ æ¨¡å¼å¯å‘çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) è‡ªå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç‰¹å®šé¢†åŸŸæœ‰ç›‘ç£å¾®è°ƒ (SFT) ä¸­å› æ•°æ®é›†çŸ¥è¯†å†—ä½™å¯¼è‡´çš„è®¡ç®—èµ„æºæµªè´¹é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆè®© LLMs å°è¯•å›ç­” SFT æ•°æ®é›†ä¸­çš„é—®é¢˜ï¼Œé€šè¿‡è‡ªæˆ‘è¯„åˆ†ç­›é€‰å‡ºå›ç­”é”™è¯¯çš„ QA å¯¹ï¼Œä»è€Œç²¾å‡†è¯†åˆ«å‡ºæ¨¡å‹å°šæœªæŒæ¡çš„â€œæœªçŸ¥çŸ¥è¯†â€ã€‚éšåï¼Œç ”ç©¶è€…ä»…åˆ©ç”¨è¿™äº›è¿‡æ»¤åçš„æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶ä¸“æ³¨äºçŸ¥è¯†ç›²ç‚¹çš„å­¦ä¹ ã€‚åœ¨å†œä¸šå’ŒåŒ»å­¦é¢†åŸŸçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSLearnLLM åœ¨å¤§å¹…ç¼©çŸ­è®­ç»ƒæ—¶é—´çš„åŒæ—¶ï¼Œèƒ½å¤Ÿè·å¾—ä¸å…¨é‡æ•°æ®é›†å¾®è°ƒç›¸å½“çš„æ€§èƒ½è¡¨ç°ï¼Œæ˜¾è‘—æå‡äº† LLMs é¢†åŸŸé€‚é…çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17470v1",
      "published_date": "2025-05-23 04:50:54 UTC",
      "updated_date": "2025-05-23 04:50:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:37:34.488221+00:00"
    },
    {
      "arxiv_id": "2505.17469v1",
      "title": "Efficient compression of neural networks and datasets",
      "title_zh": "ç¥ç»ç½‘ç»œä¸æ•°æ®é›†çš„é«˜æ•ˆå‹ç¼©",
      "authors": [
        "Lukas Silvester Barth",
        "Paulo von Petersenn"
      ],
      "abstract": "We compare, improve, and contribute methods that substantially decrease the number of parameters of neural networks while maintaining high test accuracy. When applying our methods to minimize description length, we obtain very effective data compression algorithms. In particular, we develop a probabilistic reformulation of $\\ell_0$ regularized optimization for nonlinear models that does not require Monte-Carlo sampling and thus improves upon previous methods. We also improve upon methods involving smooth approximations to the $\\ell_0$ norm, and investigate layerwise methods. We compare the methods on different architectures and datasets, including convolutional networks trained on image datasets and transformers trained on parts of Wikipedia. We also created a synthetic teacher-student setup to investigate compression in a controlled continuous setting. Finally, we conceptually relate compression algorithms to Solomonoff's theory of inductive inference and empirically verify the prediction that regularized models can exhibit more sample-efficient convergence.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨ä¿æŒé«˜æµ‹è¯•å‡†ç¡®ç‡çš„åŒæ—¶å‡å°‘ç¥ç»ç½‘ç»œå‚æ•°çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒä¸æ”¹è¿›ã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯å¼€å‘äº†ä¸€ç§é’ˆå¯¹éçº¿æ€§æ¨¡å‹çš„ $\\ell_0$ æ­£åˆ™åŒ–ä¼˜åŒ–çš„æ¦‚ç‡é‡æ–°è¡¨è¿°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€ Monte-Carlo é‡‡æ ·ï¼Œæ˜¾è‘—æå‡äº†æ•ˆç‡ã€‚ç ”ç©¶è¿˜æ”¹è¿›äº† $\\ell_0$ èŒƒæ•°çš„å¹³æ»‘è¿‘ä¼¼(smooth approximations)åŠå±‚çº§æ–¹æ³•(layerwise methods)ï¼Œå¹¶åœ¨å·ç§¯ç½‘ç»œ(CNNs)å’Œ Transformer ç­‰å¤šç§æ¶æ„ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œè¯¥ç ”ç©¶åœ¨æ¦‚å¿µä¸Šå°†å‹ç¼©ç®—æ³•ä¸ Solomonoff çš„å½’çº³æ¨ç†ç†è®º(Solomonoff's theory of inductive inference)è”ç³»èµ·æ¥ï¼Œå¹¶å®è¯éªŒè¯äº†æ­£åˆ™åŒ–æ¨¡å‹å…·æœ‰æ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.OC",
        "math.ST"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages plus appendix, 9 Figures, 3 Tables",
      "pdf_url": "https://arxiv.org/pdf/2505.17469v1",
      "published_date": "2025-05-23 04:50:33 UTC",
      "updated_date": "2025-05-23 04:50:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:38:09.684997+00:00"
    },
    {
      "arxiv_id": "2505.17457v2",
      "title": "Hypergraph Mamba for Efficient Whole Slide Image Understanding",
      "title_zh": "Hypergraph Mambaï¼šé«˜æ•ˆçš„å…¨è§†é‡æ•°å­—åˆ‡ç‰‡å›¾åƒç†è§£",
      "authors": [
        "Jiaxuan Lu",
        "Yuhui Lin",
        "Junyan Shi",
        "Fang Yan",
        "Dongzhan Zhou",
        "Yue Gao",
        "Xiaosong Wang"
      ],
      "abstract": "Whole Slide Images (WSIs) in histopathology pose a significant challenge for extensive medical image analysis due to their ultra-high resolution, massive scale, and intricate spatial relationships. Although existing Multiple Instance Learning (MIL) approaches like Graph Neural Networks (GNNs) and Transformers demonstrate strong instance-level modeling capabilities, they encounter constraints regarding scalability and computational expenses. To overcome these limitations, we introduce the WSI-HGMamba, a novel framework that unifies the high-order relational modeling capabilities of the Hypergraph Neural Networks (HGNNs) with the linear-time sequential modeling efficiency of the State Space Models. At the core of our design is the HGMamba block, which integrates message passing, hypergraph scanning & flattening, and bidirectional state space modeling (Bi-SSM), enabling the model to retain both relational and contextual cues while remaining computationally efficient. Compared to Transformer and Graph Transformer counterparts, WSI-HGMamba achieves superior performance with up to 7* reduction in FLOPs. Extensive experiments on multiple public and private WSI benchmarks demonstrate that our method provides a scalable, accurate, and efficient solution for slide-level understanding, making it a promising backbone for next-generation pathology AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WSI-HGMamba æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒ (Whole Slide Images, WSIs) åœ¨å¤„ç†è¶…é«˜åˆ†è¾¨ç‡å’Œå¤æ‚ç©ºé—´å…³ç³»æ—¶é¢ä¸´çš„è®¡ç®—æ•ˆç‡ä¸æ‰©å±•æ€§ç“¶é¢ˆã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†è¶…å›¾ç¥ç»ç½‘ç»œ (Hypergraph Neural Networks, HGNNs) çš„é«˜é˜¶å…³ç³»å»ºæ¨¡èƒ½åŠ›ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ (State Space Models) çš„çº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡æ•ˆç‡ç›¸ç»“åˆã€‚é€šè¿‡æ ¸å¿ƒçš„ HGMamba æ¨¡å—ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨åŒå‘çŠ¶æ€ç©ºé—´å»ºæ¨¡ (Bi-SSM) ç­‰æŠ€æœ¯ï¼Œåœ¨ä¿ç•™å…³é”®å…³è”å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„åŒæ—¶æ˜¾è‘—é™ä½äº†å¼€é”€ã€‚å®éªŒè¯æ˜ï¼ŒWSI-HGMamba çš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ Transformer å’Œå›¾å˜æ¢å™¨æ¨¡å‹ï¼Œå…¶è®¡ç®—é‡ (FLOPs) æœ€é«˜å¯é™ä½ 7 å€ï¼Œä¸ºä¸‹ä¸€ä»£ç—…ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€é«˜æ•ˆä¸”å‡†ç¡®çš„éª¨å¹²æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17457v2",
      "published_date": "2025-05-23 04:33:54 UTC",
      "updated_date": "2025-08-04 08:35:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:38:03.291094+00:00"
    },
    {
      "arxiv_id": "2505.17455v2",
      "title": "Towards Evaluating Proactive Risk Awareness of Multimodal Language Models",
      "title_zh": "è¿ˆå‘å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸»åŠ¨é£é™©æ„è¯†çš„è¯„ä¼°",
      "authors": [
        "Youliang Yuan",
        "Wenxiang Jiao",
        "Yuejin Xie",
        "Chihao Shen",
        "Menghan Tian",
        "Wenxuan Wang",
        "Jen-tse Huang",
        "Pinjia He"
      ],
      "abstract": "Human safety awareness gaps often prevent the timely recognition of everyday risks. In solving this problem, a proactive safety artificial intelligence (AI) system would work better than a reactive one. Instead of just reacting to users' questions, it would actively watch people's behavior and their environment to detect potential dangers in advance. Our Proactive Safety Bench (PaSBench) evaluates this capability through 416 multimodal scenarios (128 image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation of 36 advanced models reveals fundamental limitations: Top performers like Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks in repeated trials. Through failure analysis, we identify unstable proactive reasoning rather than knowledge deficits as the primary limitation. This work establishes (1) a proactive safety benchmark, (2) systematic evidence of model limitations, and (3) critical directions for developing reliable protective AI. We believe our dataset and findings can promote the development of safer AI assistants that actively prevent harm rather than merely respond to requests. Our dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ä¸»åŠ¨å®‰å…¨æ„è¯†ï¼Œå¹¶æå‡ºäº†åŒ…å« 416 ä¸ªå¤šæ¨¡æ€åœºæ™¯çš„åŸºå‡†æµ‹è¯•é›† Proactive Safety Bench (PaSBench)ã€‚é€šè¿‡å¯¹åŒ…æ‹¬ Gemini-2.5-pro åœ¨å†…çš„ 36 ç§å…ˆè¿›æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯é¡¶çº§æ¨¡å‹åœ¨é‡å¤è¯•éªŒä¸­ä»ä¼šæ¼æ‰ 45-55% çš„æ½œåœ¨é£é™©ã€‚æ•…éšœåˆ†æè¡¨æ˜ï¼Œåˆ¶çº¦æ¨¡å‹æ€§èƒ½çš„ä¸»è¦ç“¶é¢ˆåœ¨äºä¸ç¨³å®šçš„ä¸»åŠ¨æ¨ç† (Proactive Reasoning)ï¼Œè€Œéé¢†åŸŸçŸ¥è¯†çš„åŒ®ä¹ã€‚è¯¥å·¥ä½œä¸ä»…å»ºç«‹äº†ä¸»åŠ¨å®‰å…¨è¯„ä¼°åŸºå‡†ï¼Œè¿˜ç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥å¼€å‘èƒ½å¤Ÿä¸»åŠ¨é¢„é˜²ä¼¤å®³çš„å¯ä¿¡ä¿æŠ¤æ€§ AI æä¾›äº†æ˜ç¡®çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NeurIPS 2025 (Track on Datasets and Benchmarks)",
      "pdf_url": "https://arxiv.org/pdf/2505.17455v2",
      "published_date": "2025-05-23 04:28:47 UTC",
      "updated_date": "2025-10-20 08:51:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:38:08.390711+00:00"
    },
    {
      "arxiv_id": "2505.17451v2",
      "title": "CLIMB: Class-imbalanced Learning Benchmark on Tabular Data",
      "title_zh": "CLIMBï¼šè¡¨æ ¼æ•°æ®ç±»åˆ«ä¸å¹³è¡¡å­¦ä¹ åŸºå‡†",
      "authors": [
        "Zhining Liu",
        "Zihao Li",
        "Ze Yang",
        "Tianxin Wei",
        "Jian Kang",
        "Yada Zhu",
        "Hendrik Hamann",
        "Jingrui He",
        "Hanghang Tong"
      ],
      "abstract": "Class-imbalanced learning (CIL) on tabular data is important in many real-world applications where the minority class holds the critical but rare outcomes. In this paper, we present CLIMB, a comprehensive benchmark for class-imbalanced learning on tabular data. CLIMB includes 73 real-world datasets across diverse domains and imbalance levels, along with unified implementations of 29 representative CIL algorithms. Built on a high-quality open-source Python package with unified API designs, detailed documentation, and rigorous code quality controls, CLIMB supports easy implementation and comparison between different CIL algorithms. Through extensive experiments, we provide practical insights on method accuracy and efficiency, highlighting the limitations of naive rebalancing, the effectiveness of ensembles, and the importance of data quality. Our code, documentation, and examples are available at https://github.com/ZhiningLiu1998/imbalanced-ensemble.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CLIMBï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è¡¨æ ¼æ•°æ®ç±»åˆ«ä¸å¹³è¡¡å­¦ä¹  (Class-imbalanced learning, CIL) çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ¶µç›–äº†æ¥è‡ªä¸åŒé¢†åŸŸçš„ 73 ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œå¹¶ç»Ÿä¸€å®ç°äº† 29 ç§å…·æœ‰ä»£è¡¨æ€§çš„ CIL ç®—æ³•ã€‚CLIMB åŸºäºé«˜è´¨é‡çš„å¼€æº Python è½¯ä»¶åŒ…æ„å»ºï¼Œæä¾›ç»Ÿä¸€çš„ API è®¾è®¡ï¼Œæå¤§åœ°æ–¹ä¾¿äº†ä¸åŒç®—æ³•çš„å¤ç°ä¸å¯¹æ¯”ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œç ”ç©¶è€…æ­ç¤ºäº†ç®€å•é‡å¹³è¡¡ (naive rebalancing) çš„å±€é™æ€§ï¼Œå¹¶éªŒè¯äº†é›†æˆå­¦ä¹  (ensembles) çš„æœ‰æ•ˆæ€§ä»¥åŠæ•°æ®è´¨é‡åœ¨ CIL ä»»åŠ¡ä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025, Dataset and Benchmark Track. 18 pages, 7 figures, 8 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.17451v2",
      "published_date": "2025-05-23 04:21:03 UTC",
      "updated_date": "2025-10-20 16:25:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:38:11.988320+00:00"
    },
    {
      "arxiv_id": "2505.23785v1",
      "title": "Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale",
      "title_zh": "æ„ä¹‰å¹¶éæŒ‡æ ‡ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°å¤§è§„æ¨¡æ–‡åŒ–è¯­å¢ƒçš„æ˜¾æ€§åŒ–",
      "authors": [
        "Cody Kommers",
        "Drew Hemment",
        "Maria Antoniak",
        "Joel Z. Leibo",
        "Hoyt Long",
        "Emily Robinson",
        "Adam Sobey"
      ],
      "abstract": "This position paper argues that large language models (LLMs) can make cultural context, and therefore human meaning, legible at an unprecedented scale in AI-based sociotechnical systems. We argue that such systems have previously been unable to represent human meaning because they rely on thin descriptions: numerical representations that enforce standardization and therefore strip human activity of the cultural context that gives it meaning. By contrast, scholars in the humanities and qualitative social sciences have developed frameworks for representing meaning through thick description: verbal representations that accommodate heterogeneity and retain contextual information needed to represent human meaning. While these methods can effectively codify meaning, they are difficult to deploy at scale. However, the verbal capabilities of LLMs now provide a means of (at least partially) automating the generation and processing of thick descriptions, potentially overcoming this bottleneck. We argue that the problem of rendering human meaning legible is not just about selecting better metrics, but about developing new representational formats (based on thick description). We frame this as a crucial direction for the application of generative AI and identify five key challenges: preserving context, maintaining interpretive pluralism, integrating perspectives based on lived experience and critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static. Furthermore, we suggest that thick description has the potential to serve as a unifying framework to address a number of emerging concerns about the difficulties of representing culture in (or using) LLMs.",
      "tldr_zh": "è¿™ç¯‡ç«‹åœºè®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä½¿æ–‡åŒ–è¯­å¢ƒåœ¨å¤§è§„æ¨¡ AI ç³»ç»Ÿä¸­å˜å¾—â€œå¯è¯»â€ã€‚ä½œè€…è®¤ä¸ºä¼ ç»Ÿçš„ç¤¾ä¼šæŠ€æœ¯ç³»ç»Ÿå› ä¾èµ–æ•°å€¼åŒ–ã€æ ‡å‡†åŒ–çš„ \"thin descriptions\" è€Œå‰¥ç¦»äº†äººç±»æ´»åŠ¨çš„æ–‡åŒ–è¯­å¢ƒï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆè¡¨å¾æ„ä¹‰ã€‚ä¸æ­¤ç›¸å¯¹ï¼Œç ”ç©¶æå‡ºåˆ©ç”¨ LLMs çš„è¯­è¨€èƒ½åŠ›åœ¨å¤§è§„æ¨¡èŒƒå›´å†…å®ç° \"thick descriptions\"ï¼ˆæ·±æï¼‰çš„è‡ªåŠ¨åŒ–ï¼Œä»è€Œä¿ç•™å®šæ€§ç ”ç©¶ä¸­æ‰€éœ€çš„å¼‚è´¨æ€§å’ŒèƒŒæ™¯ä¿¡æ¯ã€‚è®ºæ–‡è¯†åˆ«äº†ä¿ç•™ Context å’Œç»´æŒ Interpretive Pluralism ç­‰äº”ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶å»ºè®®å°† \"thick descriptions\" ä½œä¸ºä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºè§£å†³ LLMs åœ¨è¡¨å¾æ–‡åŒ–æ—¶é¢ä¸´çš„å„ç±»å¤æ‚é—®é¢˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Position paper",
      "pdf_url": "https://arxiv.org/pdf/2505.23785v1",
      "published_date": "2025-05-23 04:10:42 UTC",
      "updated_date": "2025-05-23 04:10:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:38:08.683049+00:00"
    },
    {
      "arxiv_id": "2505.17441v3",
      "title": "Discovering Forbidden Topics in Language Models",
      "title_zh": "æŒ–æ˜è¯­è¨€æ¨¡å‹ä¸­çš„ç¦å¿Œè¯é¢˜",
      "authors": [
        "Can Rager",
        "Chris Wendler",
        "Rohit Gandikota",
        "David Bau"
      ],
      "abstract": "Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, Iterated Prefill Crawler (IPC), that uses token prefilling to find forbidden topics. We benchmark IPC on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawler to a frontier model using the prefilling option of Claude-Haiku. Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits \"thought suppression\" behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, IPC elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å®šä¹‰äº†Refusal discoveryä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯†åˆ«è¯­è¨€æ¨¡å‹æ‹’ç»è®¨è®ºçš„ä¸»é¢˜é›†åˆï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºIterated Prefill Crawler (IPC) çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨token prefillingæŠ€æœ¯ï¼Œèƒ½æœ‰æ•ˆæ¢æµ‹å‡ºæ¨¡å‹åœ¨å®‰å…¨å¾®è°ƒæˆ–å®¡æŸ¥æœºåˆ¶ä¸‹çš„ç¦å¿Œè¯é¢˜ã€‚é€šè¿‡åœ¨Tulu-3-8BåŠLlama-3.3ç­‰æ¨¡å‹ä¸Šçš„å®éªŒï¼Œç ”ç©¶å‘ç°DeepSeek-R1-70Bå­˜åœ¨æ˜æ˜¾çš„â€œthought suppressionâ€è¡Œä¸ºï¼Œæ˜¾ç¤ºå…¶è®°å¿†äº†ç¬¦åˆç‰¹å®šè¦æ±‚çš„å¯¹é½å›å¤ã€‚æ­¤å¤–ï¼Œç ”ç©¶æŒ‡å‡ºå³ä¾¿åœ¨é²æ£’çš„æ¨¡å‹å˜ä½“ä¸­ï¼ŒIPCä»èƒ½é€šè¿‡é‡åŒ–æ¨¡å‹è¯±å¯¼å‡ºç‰¹å®šçš„æ‹’ç»å›ç­”ã€‚è¯¥æˆæœå¼ºè°ƒäº†Refusal discoveryå¯¹äºæ£€æµ‹AIç³»ç»Ÿåè§ã€å®‰å…¨è¾¹ç•Œå’Œå¯¹é½å¤±è´¥(alignment failures)çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17441v3",
      "published_date": "2025-05-23 03:49:06 UTC",
      "updated_date": "2025-06-11 16:52:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:38:24.990197+00:00"
    },
    {
      "arxiv_id": "2505.17439v1",
      "title": "Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„é«˜æ•ˆå…¬å¹³äººé“ä¸»ä¹‰ä¾›åº”é“¾åŠ¨æ€è®¾è®¡",
      "authors": [
        "Weijia Jin"
      ],
      "abstract": "This study designs an efficient and equitable humanitarian supply chain dynamically by using reinforcement learning, PPO, and compared with heuristic algorithms. This study demonstrates the model of PPO always treats average satisfaction rate as the priority.",
      "tldr_zh": "# è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“\n\n---\n\nè¯¥ç ”ç©¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä¸­çš„ PPO ç®—æ³•ï¼Œæå‡ºäº†ä¸€ç§èƒ½å¤ŸåŠ¨æ€è®¾è®¡é«˜æ•ˆä¸”å…¬å¹³çš„äººé“ä¸»ä¹‰ä¾›åº”é“¾ (Humanitarian Supply Chain) çš„æ–¹æ³•ã€‚é€šè¿‡ä¸å¯å‘å¼ç®—æ³• (Heuristic Algorithms) çš„å¯¹æ¯”åˆ†æï¼Œç ”ç©¶è¯æ˜äº† PPO æ¨¡å‹åœ¨å†³ç­–è¿‡ç¨‹ä¸­å§‹ç»ˆå°†å¹³å‡æ»¡æ„ç‡ (Average Satisfaction Rate) ä½œä¸ºé¦–è¦ä¼˜åŒ–ç›®æ ‡ã€‚è¯¥æˆæœä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„äººé“ä¸»ä¹‰ç‰©æµèµ„æºåˆ†é…æä¾›äº†å…¼é¡¾æ•ˆç‡ä¸å…¬å¹³çš„åŠ¨æ€è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17439v1",
      "published_date": "2025-05-23 03:45:08 UTC",
      "updated_date": "2025-05-23 03:45:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:38:31.923902+00:00"
    },
    {
      "arxiv_id": "2505.17437v1",
      "title": "Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision",
      "title_zh": "åŸºäºå…¨è¯­ä¹‰ç›‘ç£çš„é€šç”¨çµæ´»è½¨è¿¹æ¨¡å‹å­¦ä¹ ",
      "authors": [
        "Yuanshao Zhu",
        "James Jianqiao Yu",
        "Xiangyu Zhao",
        "Xiao Han",
        "Qidong Liu",
        "Xuetao Wei",
        "Yuxuan Liang"
      ],
      "abstract": "The widespread adoption of mobile devices and data collection technologies has led to an exponential increase in trajectory data, presenting significant challenges in spatio-temporal data mining, particularly for efficient and accurate trajectory retrieval. However, existing methods for trajectory retrieval face notable limitations, including inefficiencies in large-scale data, lack of support for condition-based queries, and reliance on trajectory similarity measures. To address the above challenges, we propose OmniTraj, a generalized and flexible omni-semantic trajectory retrieval framework that integrates four complementary modalities or semantics -- raw trajectories, topology, road segments, and regions -- into a unified system. Unlike traditional approaches that are limited to computing and processing trajectories as a single modality, OmniTraj designs dedicated encoders for each modality, which are embedded and fused into a shared representation space. This design enables OmniTraj to support accurate and flexible queries based on any individual modality or combination thereof, overcoming the rigidity of traditional similarity-based methods. Extensive experiments on two real-world datasets demonstrate the effectiveness of OmniTraj in handling large-scale data, providing flexible, multi-modality queries, and supporting downstream tasks and applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OmniTrajï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨ä¸”çµæ´»çš„å…¨è¯­ä¹‰ï¼ˆomni-semanticï¼‰è½¨è¿¹æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè½¨è¿¹æ£€ç´¢æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®å¤„ç†æ•ˆç‡ã€æ¡ä»¶æŸ¥è¯¢æ”¯æŒä»¥åŠå¯¹è½¨è¿¹ç›¸ä¼¼æ€§åº¦é‡ä¾èµ–ç­‰æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é›†æˆäº†åŸå§‹è½¨è¿¹ (raw trajectories)ã€æ‹“æ‰‘ (topology)ã€è·¯æ®µ (road segments) å’ŒåŒºåŸŸ (regions) å››ç§äº’è¡¥çš„è¯­ä¹‰æ¨¡æ€ï¼Œå¹¶ä¸ºæ¯ç§æ¨¡æ€è®¾è®¡äº†ä¸“é—¨çš„ç¼–ç å™¨ (encoders)ï¼Œå°†å…¶èåˆåˆ°ç»Ÿä¸€çš„å…±äº«è¡¨ç¤ºç©ºé—´ä¸­ã€‚è¿™ç§è®¾è®¡ä½¿ OmniTraj èƒ½å¤Ÿæ”¯æŒåŸºäºå•ä¸€æ¨¡æ€æˆ–å¤šç§æ¨¡æ€ç»„åˆçš„ç²¾ç¡®æŸ¥è¯¢ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„åƒµåŒ–æ€§ã€‚åœ¨ä¸¤ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOmniTraj åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®å’Œæ”¯æŒå¤šæ¨¡æ€çµæ´»æŸ¥è¯¢æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶èƒ½æœ‰æ•ˆæ”¯æŒå„ç±»ä¸‹æ¸¸ä»»åŠ¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted as a full paper by KDD'25 - Research Track",
      "pdf_url": "https://arxiv.org/pdf/2505.17437v1",
      "published_date": "2025-05-23 03:32:24 UTC",
      "updated_date": "2025-05-23 03:32:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:38:40.249289+00:00"
    },
    {
      "arxiv_id": "2505.17436v1",
      "title": "Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning",
      "title_zh": "è§„æ¨¡åŒ–æ‰©å±•ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼šå¾®è°ƒã€æŒ‡ä»¤å¾®è°ƒä¸å¤šæ¨¡æ€å­¦ä¹ ",
      "authors": [
        "Cheng Peng",
        "Kai Zhang",
        "Mengxian Lyu",
        "Hongfang Liu",
        "Lichao Sun",
        "Yonghui Wu"
      ],
      "abstract": "To advance biomedical vison-language model capabilities through scaling up, fine-tuning, and instruction tuning, develop vision-language models with improved performance in handling long text, explore strategies to efficiently adopt vision language models for diverse multi-modal biomedical tasks, and examine the zero-shot learning performance.\n  We developed two biomedical vision language models, BiomedGPT-Large and BiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture. We fine-tuned the two models on 23 benchmark datasets from 6 multi-modal biomedical tasks including one image-only task (image classification), three language-only tasks (text understanding, text summarization and question answering), and two vision-language tasks (visual question answering and image captioning). We compared the developed scaled models with our previous BiomedGPT-Base model and existing prestigious models reported in the literature. We instruction-tuned the two models using a large-scale multi-modal biomedical instruction-tuning dataset and assessed the zero-shot learning performance and alignment accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ Scaling Upã€Fine-Tuning å’Œ Instruction Tuning ç­–ç•¥ï¼Œå¼€å‘äº†åŸºäº Encoder-Decoder Transformer æ¶æ„çš„ BiomedGPT-Large å’Œ BiomedGPT-XLarge ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶åœ¨åŒ…å« 23 ä¸ªåŸºå‡†æ•°æ®é›†çš„ 6 ç±»å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ï¼ˆæ¶µç›–å›¾åƒåˆ†ç±»ã€æ–‡æœ¬ç†è§£ã€Visual Question Answering å’Œ Image Captioning ç­‰ï¼‰ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚é€šè¿‡ä¸ BiomedGPT-Base åŠç°æœ‰ä¸»æµæ¨¡å‹å¯¹æ¯”ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†è§„æ¨¡åŒ–æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬å’Œå¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨å¤§è§„æ¨¡å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®é›†è¿›è¡Œäº†æµ‹è¯•ï¼Œè¯„ä¼°å¹¶ä¼˜åŒ–äº†æ¨¡å‹çš„ Zero-shot Learning æ€§èƒ½å’Œ Alignment å‡†ç¡®åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17436v1",
      "published_date": "2025-05-23 03:31:58 UTC",
      "updated_date": "2025-05-23 03:31:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:38:59.587590+00:00"
    },
    {
      "arxiv_id": "2505.17434v1",
      "title": "Dynamic Manipulation of Deformable Objects in 3D: Simulation, Benchmark and Learning Strategy",
      "title_zh": "ä¸‰ç»´ç©ºé—´ä¸­å¯å˜å½¢ç‰©ä½“çš„åŠ¨æ€æ“ä½œï¼šä»¿çœŸã€åŸºå‡†ä¸å­¦ä¹ ç­–ç•¥",
      "authors": [
        "Guanzhou Lan",
        "Yuqi Yang",
        "Anup Teejo Mathew",
        "Feiping Nie",
        "Rong Wang",
        "Xuelong Li",
        "Federico Renda",
        "Bin Zhao"
      ],
      "abstract": "Goal-conditioned dynamic manipulation is inherently challenging due to complex system dynamics and stringent task constraints, particularly in deformable object scenarios characterized by high degrees of freedom and underactuation. Prior methods often simplify the problem to low-speed or 2D settings, limiting their applicability to real-world 3D tasks. In this work, we explore 3D goal-conditioned rope manipulation as a representative challenge. To mitigate data scarcity, we introduce a novel simulation framework and benchmark grounded in reduced-order dynamics, which enables compact state representation and facilitates efficient policy learning. Building on this, we propose Dynamics Informed Diffusion Policy (DIDP), a framework that integrates imitation pretraining with physics-informed test-time adaptation. First, we design a diffusion policy that learns inverse dynamics within the reduced-order space, enabling imitation learning to move beyond naÃ¯ve data fitting and capture the underlying physical structure. Second, we propose a physics-informed test-time adaptation scheme that imposes kinematic boundary conditions and structured dynamics priors on the diffusion process, ensuring consistency and reliability in manipulation execution. Extensive experiments validate the proposed approach, demonstrating strong performance in terms of accuracy and robustness in the learned policy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸‰ç»´(3D)å¯å˜å½¢ç‰©ä½“åŠ¨æ€æ“æ§ä¸­é«˜è‡ªç”±åº¦å’Œæ¬ é©±åŠ¨çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºé˜¶æ¬¡ç¼©å‡åŠ¨åŠ›å­¦(reduced-order dynamics)çš„ä»¿çœŸæ¡†æ¶ä¸åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†åŠ¨åŠ›å­¦ä¿¡æ¯æ‰©æ•£ç­–ç•¥(Dynamics Informed Diffusion Policy, DIDP)ï¼Œé€šè¿‡åœ¨ç¼©å‡é˜¶æ¬¡ç©ºé—´å†…å­¦ä¹ é€†åŠ¨åŠ›å­¦ï¼Œä½¿æ¨¡ä»¿å­¦ä¹ (imitation learning)èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åº•å±‚ç‰©ç†ç»“æ„ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç‰©ç†ä¿¡æ¯æµ‹è¯•æ—¶è‡ªé€‚åº”(physics-informed test-time adaptation)æ–¹æ¡ˆï¼Œé€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­æ–½åŠ è¿åŠ¨å­¦è¾¹ç•Œæ¡ä»¶å’Œç»“æ„åŒ–åŠ¨åŠ›å­¦å…ˆéªŒï¼Œç¡®ä¿äº†æ“æ§æ‰§è¡Œçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3Dç»³ç´¢æ“æ§ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "11 pages,",
      "pdf_url": "https://arxiv.org/pdf/2505.17434v1",
      "published_date": "2025-05-23 03:28:25 UTC",
      "updated_date": "2025-05-23 03:28:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:39:32.564837+00:00"
    },
    {
      "arxiv_id": "2506.12038v1",
      "title": "LCD: Advancing Extreme Low-Bit Clustering for Large Language Models via Knowledge Distillation",
      "title_zh": "LCDï¼šé€šè¿‡çŸ¥è¯†è’¸é¦æå‡å¤§è¯­è¨€æ¨¡å‹çš„æä½æ¯”ç‰¹èšç±»",
      "authors": [
        "Fangxin Liu",
        "Ning Yang",
        "Junping Zhao",
        "Tao Yang",
        "Haibing Guan",
        "Li Jiang"
      ],
      "abstract": "Large language models (LLMs) have achieved significant progress in natural language processing but face challenges in deployment due to high memory and computational requirements. Weight quantization is a common approach to address these issues, yet achieving effective low-bit compression remains challenging. This paper presents LCD, which unifies the learning of clustering-based quantization within a knowledge distillation framework. Using carefully designed optimization techniques, LCD preserves LLM performance even at ultra-low bit widths of 2-3 bits. Additionally, LCD compresses activations through smoothing and accelerates inference with a LUT-based design. Experimental results show that LCD outperforms existing methods and delivers up to a 6.2x speedup in inference. Notably, LCD is shown to be more cost-effective, making it a practical solution for real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LCDï¼Œè¿™æ˜¯ä¸€ç§åœ¨çŸ¥è¯†è’¸é¦(Knowledge Distillation)æ¡†æ¶ä¸‹ç»Ÿä¸€èšç±»é‡åŒ–(clustering-based quantization)çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)å‹ç¼©æ–¹æ¡ˆã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä¼˜åŒ–æŠ€æœ¯ï¼ŒLCDèƒ½å¤Ÿåœ¨2-3 bitsçš„æä½ä½å®½ä¸‹æœ‰æ•ˆä¿ç•™æ¨¡å‹æ€§èƒ½ï¼Œå¹¶é€šè¿‡å¹³æ»‘æŠ€æœ¯å®ç°æ¿€æ´»å€¼å‹ç¼©ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºæŸ¥æ‰¾è¡¨(LUT-based)çš„è®¾è®¡ä»¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒLCDåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½æä¾›é«˜è¾¾6.2å€çš„æ¨ç†åŠ é€Ÿï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›äº†æ›´å…·æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.12038v1",
      "published_date": "2025-05-23 03:28:24 UTC",
      "updated_date": "2025-05-23 03:28:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:39:14.682724+00:00"
    },
    {
      "arxiv_id": "2505.17433v2",
      "title": "MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models",
      "title_zh": "MemeReaConï¼šæ¢ç©¶å¤§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯­å¢ƒä¸‹çš„æ¨¡å› ç†è§£èƒ½åŠ›",
      "authors": [
        "Zhengyi Zhao",
        "Shubo Zhang",
        "Yuxi Zhang",
        "Yanxi Zhao",
        "Yifan Zhang",
        "Zezhong Wang",
        "Huimin Wang",
        "Yutian Zhao",
        "Bin Liang",
        "Yefeng Zheng",
        "Binyang Li",
        "Kam-Fai Wong",
        "Xian Wu"
      ],
      "abstract": "Memes have emerged as a popular form of multimodal online communication, where their interpretation heavily depends on the specific context in which they appear. Current approaches predominantly focus on isolated meme analysis, either for harmful content detection or standalone interpretation, overlooking a fundamental challenge: the same meme can express different intents depending on its conversational context. This oversight creates an evaluation gap: although humans intuitively recognize how context shapes meme interpretation, Large Vision Language Models (LVLMs) can hardly understand context-dependent meme intent. To address this critical limitation, we introduce MemeReaCon, a novel benchmark specifically designed to evaluate how LVLMs understand memes in their original context. We collected memes from five different Reddit communities, keeping each meme's image, the post text, and user comments together. We carefully labeled how the text and meme work together, what the poster intended, how the meme is structured, and how the community responded. Our tests with leading LVLMs show a clear weakness: models either fail to interpret critical information in the contexts, or overly focus on visual details while overlooking communicative purpose. MemeReaCon thus serves both as a diagnostic tool exposing current limitations and as a challenging benchmark to drive development toward more sophisticated LVLMs of the context-aware understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† MemeReaConï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) åœ¨åŸå§‹è¯­å¢ƒä¸­ç†è§£ Memeï¼ˆæ¨¡å› ï¼‰èƒ½åŠ›çš„å…¨æ–°åŸºå‡†ã€‚é’ˆå¯¹å½“å‰ç ”ç©¶å¤šä¾§é‡äºå­¤ç«‹çš„ Meme åˆ†æè€Œå¿½è§†äº†å¯¹è¯è¯­å¢ƒå¯¹æ„å›¾è§£è¯»çš„å…³é”®å½±å“ï¼Œè¯¥åŸºå‡†ä»äº”ä¸ª Reddit ç¤¾åŒºæ”¶é›†æ•°æ®ï¼Œä¿ç•™äº†å›¾åƒã€å¸–å­æ–‡æœ¬åŠç”¨æˆ·è¯„è®ºï¼Œå¹¶å¯¹æ„å›¾ã€ç»“æ„åŠç¤¾åŒºååº”è¿›è¡Œäº†ç²¾ç»†æ ‡æ³¨ã€‚æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œé¢†å…ˆçš„ LVLMs åœ¨è§£è¯»ä¸Šä¸‹æ–‡å…³é”®ä¿¡æ¯æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œå¾€å¾€è¿‡åº¦å…³æ³¨è§†è§‰ç»†èŠ‚è€Œå¿½ç•¥äº†å…¶ç¤¾äº¤äº¤æµç›®çš„ã€‚MemeReaCon ä¸ä»…æ­ç¤ºäº†å½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œä¹Ÿä¸ºå¼€å‘å…·å¤‡è¯­å¢ƒæ„ŸçŸ¥èƒ½åŠ›çš„é«˜çº§ LVLMs æä¾›äº†æŒ‘æˆ˜æ€§çš„è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17433v2",
      "published_date": "2025-05-23 03:27:23 UTC",
      "updated_date": "2025-06-04 08:55:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:39:42.980848+00:00"
    },
    {
      "arxiv_id": "2505.17430v1",
      "title": "SEvoBench : A C++ Framework For Evolutionary Single-Objective Optimization Benchmarking",
      "title_zh": "SEvoBenchï¼šè¿›åŒ–å•ç›®æ ‡ä¼˜åŒ–åŸºå‡†æµ‹è¯• C++ æ¡†æ¶",
      "authors": [
        "Yongkang Yang",
        "Jian Zhao",
        "Tengfei Yang"
      ],
      "abstract": "We present SEvoBench, a modern C++ framework for evolutionary computation (EC), specifically designed to systematically benchmark evolutionary single-objective optimization algorithms. The framework features modular implementations of Particle Swarm Optimization (PSO) and Differential Evolution (DE) algorithms, organized around three core components: (1) algorithm construction with reusable modules, (2) efficient benchmark problem suites, and (3) parallel experimental analysis. Experimental evaluations demonstrate the framework's superior performance in benchmark testing and algorithm comparison. Case studies further validate its capabilities in algorithm hybridization and parameter analysis. Compared to existing frameworks, SEvoBench demonstrates three key advantages: (i) highly efficient and reusable modular implementations of PSO and DE algorithms, (ii) accelerated benchmarking through parallel execution, and (iii) enhanced computational efficiency via SIMD (Single Instruction Multiple Data) vectorization for large-scale problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† SEvoBenchï¼Œä¸€ä¸ªä¸“ä¸ºè¿›åŒ–å•ç›®æ ‡ä¼˜åŒ– (Evolutionary Single-Objective Optimization) åŸºå‡†æµ‹è¯•è®¾è®¡çš„ç°ä»£ C++ æ¡†æ¶ã€‚è¯¥æ¡†æ¶å®ç°äº† Particle Swarm Optimization (PSO) å’Œ Differential Evolution (DE) ç®—æ³•çš„æ¨¡å—åŒ–æ„å»ºï¼Œå¹¶é›†æˆäº†é«˜æ•ˆçš„åŸºå‡†æµ‹è¯•é›†ä¸å¹¶è¡Œå®éªŒåˆ†æåŠŸèƒ½ã€‚SEvoBench çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå…¶é«˜åº¦å¯é‡ç”¨çš„æ¨¡å—åŒ–è®¾è®¡ã€é€šè¿‡å¹¶è¡Œæ‰§è¡Œå®ç°çš„åŠ é€Ÿæµ‹è¯•ï¼Œä»¥åŠåˆ©ç”¨ SIMD (Single Instruction Multiple Data) å‘é‡åŒ–æ˜¾è‘—æå‡å¤§è§„æ¨¡é—®é¢˜çš„è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœä¸æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ€§èƒ½è¡¨ç°ã€ç®—æ³•æ‚äº¤åŠå‚æ•°åˆ†ææ–¹é¢å‡ä¼˜äºç°æœ‰æ¡†æ¶ï¼Œä¸ºè¿›åŒ–è®¡ç®—ç ”ç©¶æä¾›äº†é«˜æ•ˆçš„å·¥å…·ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.MS",
        "math.OC"
      ],
      "primary_category": "cs.NE",
      "comment": "9 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17430v1",
      "published_date": "2025-05-23 03:23:10 UTC",
      "updated_date": "2025-05-23 03:23:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:39:21.414991+00:00"
    },
    {
      "arxiv_id": "2505.17426v1",
      "title": "UniTTS: An end-to-end TTS system without decoupling of acoustic and semantic information",
      "title_zh": "UniTTSï¼šæ— éœ€è§£è€¦å£°å­¦ä¸è¯­ä¹‰ä¿¡æ¯çš„ç«¯åˆ°ç«¯è¯­éŸ³åˆæˆç³»ç»Ÿ",
      "authors": [
        "Rui Wang",
        "Qianguo Sun",
        "Tianrong Chen",
        "Zhiyun Zeng",
        "Junlong Wu",
        "Jiaxing Zhang"
      ],
      "abstract": "The emergence of multi-codebook neutral audio codecs such as Residual Vector Quantization (RVQ) and Group Vector Quantization (GVQ) has significantly advanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These codecs are crucial in separating semantic and acoustic information while efficiently harnessing semantic priors. However, since semantic and acoustic information cannot be fully aligned, a significant drawback of these methods when applied to LLM-based TTS is that large language models may have limited access to comprehensive audio information. To address this limitation, we propose DistilCodec and UniTTS, which collectively offer the following advantages: 1) This method can distill a multi-codebook audio codec into a single-codebook audio codec with 32,768 codes while achieving a near 100\\% utilization. 2) As DistilCodec does not employ a semantic alignment scheme, a large amount of high-quality unlabeled audio (such as audiobooks with sound effects, songs, etc.) can be incorporated during training, further expanding data diversity and broadening its applicability. 3) Leveraging the comprehensive audio information modeling of DistilCodec, we integrated three key tasks into UniTTS's pre-training framework: audio modality autoregression, text modality autoregression, and speech-text cross-modal autoregression. This allows UniTTS to accept interleaved text and speech/audio prompts while substantially preserving LLM's text capabilities. 4) UniTTS employs a three-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and Alignment. Source code and model checkpoints are publicly available at https://github.com/IDEA-Emdoor-Lab/UniTTS and https://github.com/IDEA-Emdoor-Lab/DistilCodec.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UniTTSï¼Œä¸€ç§æ— éœ€è§£è€¦è¯­ä¹‰ä¸å£°å­¦ä¿¡æ¯çš„ç«¯åˆ°ç«¯ TTS ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åŸºäº LLM çš„æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿåœ¨å¤„ç†éå®Œå…¨å¯¹é½éŸ³é¢‘ä¿¡æ¯æ—¶çš„å±€é™æ€§ã€‚ç ”ç©¶å¼•å…¥äº† DistilCodec æŠ€æœ¯ï¼Œå°†å¤šç æœ¬éŸ³é¢‘ç¼–è§£ç å™¨è’¸é¦ä¸ºåˆ©ç”¨ç‡æ¥è¿‘ 100% çš„å•ç æœ¬ç¼–è§£ç å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿåˆ©ç”¨å¤§é‡æœªæ ‡æ³¨çš„é«˜è´¨é‡éŸ³é¢‘æ•°æ®ï¼ˆå¦‚å¸¦éŸ³æ•ˆçš„ç”µå­ä¹¦å’Œæ­Œæ›²ï¼‰è¿›è¡Œè®­ç»ƒã€‚UniTTS åœ¨é¢„è®­ç»ƒæ¡†æ¶ä¸­é›†æˆäº†éŸ³é¢‘ã€æ–‡æœ¬åŠè¯­éŸ³-æ–‡æœ¬è·¨æ¨¡æ€çš„ä¸‰ç§è‡ªå›å½’ä»»åŠ¡ï¼Œä½¿å…¶èƒ½å¤Ÿæ¥å—äº¤é”™çš„æ–‡æœ¬å’Œè¯­éŸ³/éŸ³é¢‘æç¤ºï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™äº† LLM åŸæœ‰çš„æ–‡æœ¬èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡é¢„è®­ç»ƒ (Pre-Training)ã€ç›‘ç£å¾®è°ƒ (SFT) å’Œå¯¹é½ (Alignment) ä¸‰é˜¶æ®µæµç¨‹ï¼Œå®ç°äº†å¯¹å…¨é¢éŸ³é¢‘ä¿¡æ¯çš„æœ‰æ•ˆå»ºæ¨¡ï¼Œæå‡äº†åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ä¸ç³»ç»Ÿçš„é€‚ç”¨æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17426v1",
      "published_date": "2025-05-23 03:13:46 UTC",
      "updated_date": "2025-05-23 03:13:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:39:27.097970+00:00"
    },
    {
      "arxiv_id": "2506.12037v2",
      "title": "Exploiting Block Coordinate Descent for Cost-Effective LLM Model Training",
      "title_zh": "åˆ©ç”¨å—åæ ‡ä¸‹é™å®ç°é«˜æ€§ä»·æ¯”çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Zeyu Liu",
        "Yan Li",
        "Yunquan Zhang",
        "Boyang Zhang",
        "Guoyong Jiang",
        "Xin Zhang",
        "Limin Xiao",
        "Weifeng Zhang",
        "Daning Cheng"
      ],
      "abstract": "Training large language models typically demands extensive GPU memory and substantial financial investment, which poses a barrier for many small- to medium-sized teams. In this paper, we propose a full-parameter pre-training and fine-tuning framework based on block coordinate descent (BCD), enhanced with engineering optimizations, to enable efficient training of large-scale models on cost-effective RTX 4090, A100 and A800 GPU clusters. Under identical hardware configurations, we reduce the training cost of a 7B model to 33% on A100/A800 and only 2.6% on RTX 4090, compared to standard full-parameter training. It also enables large models previously restricted to A100 clusters to be trained on RTX 4090 without degrading performance. BCD achieves comparable or better accuracy than full-parameter and fine-tuning methods at most cases, with lower GPU consumption and improved hardware utilization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå—åæ ‡ä¸‹é™ (Block Coordinate Descent, BCD) çš„å…¨å‚æ•°é¢„è®­ç»ƒå’Œå¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLM) è®­ç»ƒä¸­æ˜¾å­˜éœ€æ±‚é«˜å’Œèµ„é‡‘æŠ•å…¥å¤§çš„é—®é¢˜ã€‚é€šè¿‡å·¥ç¨‹ä¼˜åŒ–ï¼Œè¯¥æ¡†æ¶æ”¯æŒåœ¨ RTX 4090ã€A100 å’Œ A800 GPU é›†ç¾¤ä¸Šé«˜æ•ˆè¿è¡Œï¼Œå¹¶å°† 7B æ¨¡å‹åœ¨ RTX 4090 ä¸Šçš„è®­ç»ƒæˆæœ¬å¤§å¹…é™ä½è‡³æ ‡å‡†å…¨å‚æ•°è®­ç»ƒçš„ 2.6%ã€‚è¯¥æŠ€æœ¯ä½¿å¾—åŸæœ¬å±€é™äº A100 é›†ç¾¤çš„å¤§æ¨¡å‹å¯ä»¥åœ¨æ›´ç»æµçš„ RTX 4090 ä¸Šå®Œæˆè®­ç»ƒä¸”ä¸æŸå¤±æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBCD åœ¨é™ä½ GPU æ¶ˆè€—å’Œæå‡ç¡¬ä»¶åˆ©ç”¨ç‡çš„åŒæ—¶ï¼Œåœ¨å¤šæ•°åœºæ™¯ä¸‹è¾¾åˆ°äº†ä¸ä¼ ç»Ÿå…¨å‚æ•°å¾®è°ƒç›¸å½“æˆ–æ›´ä¼˜çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "We have revised certain details of the manuscript and incorporated new experimental",
      "pdf_url": "https://arxiv.org/pdf/2506.12037v2",
      "published_date": "2025-05-23 03:05:54 UTC",
      "updated_date": "2025-09-26 02:22:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:39:45.594137+00:00"
    },
    {
      "arxiv_id": "2505.17406v1",
      "title": "Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness",
      "title_zh": "æ¨ç†ä¸ç­”æ¡ˆçš„å¤±é…ï¼šå¤§è¯­è¨€æ¨¡å‹ CoT é²æ£’æ€§è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Enyi Jiang",
        "Changming Xu",
        "Nischay Singh",
        "Gagandeep Singh"
      ],
      "abstract": "LLMs' decision-making process is opaque, prompting the need for explanation techniques like Chain-of-Thought. To investigate the relationship between answer and reasoning, we design a novel evaluation framework, MATCHA. In domains like education and healthcare, reasoning is key for model trustworthiness. MATCHA reveals that LLMs under input perturbations can give inconsistent or nonsensical reasoning. Additionally, we use LLM judges to assess reasoning robustness across models. Our results show that LLMs exhibit greater vulnerability to input perturbations for multi-step and commonsense tasks than compared to logical tasks. Also, we show non-trivial transfer rates of our successful examples to black-box models. Our evaluation framework helps to better understand LLM reasoning mechanisms and guides future models toward more robust and reasoning-driven architectures, enforcing answer-reasoning consistency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MATCHA è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ¢è®¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„ç”Ÿæˆç­”æ¡ˆä¸å…¶ Chain-of-Thought (CoT) æ¨ç†è¿‡ç¨‹ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è¯„ä¼°å…¶åœ¨å—åˆ°è¾“å…¥æ‰°åŠ¨æ—¶çš„ç¨³å¥æ€§ã€‚é€šè¿‡åœ¨æ•™è‚²å’ŒåŒ»ç–—ç­‰å…³é”®é¢†åŸŸè¿›è¡Œå®éªŒï¼ŒMATCHA æ­ç¤ºäº† LLMs åœ¨é¢å¯¹æ‰°åŠ¨æ—¶å¸¸ä¼šå‡ºç°é€»è¾‘ä¸ä¸€è‡´æˆ–æ— æ„ä¹‰çš„æ¨ç†ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸é€»è¾‘ä»»åŠ¡ç›¸æ¯”ï¼ŒLLMs åœ¨å¤šæ­¥æ¨ç†å’Œå¸¸è¯†ä»»åŠ¡ä¸­å¯¹è¾“å…¥æ‰°åŠ¨è¡¨ç°å‡ºæ›´é«˜çš„è„†å¼±æ€§ï¼Œä¸”ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬åœ¨é»‘ç›’æ¨¡å‹ä¸­ä¹Ÿè¡¨ç°å‡ºæ˜¾è‘—çš„è¿ç§»ç‡ã€‚è¯¥æ¡†æ¶æ·±åŒ–äº†å¯¹ LLM æ¨ç†æœºåˆ¶çš„ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥æ„å»ºå…·å¤‡ç­”æ¡ˆä¸æ¨ç†ä¸€è‡´æ€§ (answer-reasoning consistency) ä¸”æ›´å…·ç¨³å¥æ€§çš„æ¶æ„æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17406v1",
      "published_date": "2025-05-23 02:42:16 UTC",
      "updated_date": "2025-05-23 02:42:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:39:51.583392+00:00"
    },
    {
      "arxiv_id": "2505.21522v1",
      "title": "CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures",
      "title_zh": "CIM-NETï¼šé¢å‘å­˜ç®—ä¸€ä½“æ¶æ„ä¼˜åŒ–çš„è§†é¢‘å»å™ªæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹",
      "authors": [
        "Shan Gao",
        "Zhiqiang Wu",
        "Yawen Niu",
        "Xiaotao Li",
        "Qingqing Xu"
      ],
      "abstract": "While deep neural network (DNN)-based video denoising has demonstrated significant performance, deploying state-of-the-art models on edge devices remains challenging due to stringent real-time and energy efficiency requirements. Computing-in-Memory (CIM) chips offer a promising solution by integrating computation within memory cells, enabling rapid matrix-vector multiplication (MVM). However, existing DNN models are often designed without considering CIM architectural constraints, thus limiting their acceleration potential during inference. To address this, we propose a hardware-algorithm co-design framework incorporating two innovations: (1) a CIM-Aware Architecture, CIM-NET, optimized for large receptive field operation and CIM's crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator, CIM-CONV, used within CIM-NET to integrate slide-based processing with fully connected transformations for high-quality feature extraction and reconstruction. This framework significantly reduces the number of MVM operations, improving inference speed on CIM chips while maintaining competitive performance. Experimental results indicate that, compared to the conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM operations with a slight decrease in denoising performance. With a stride value of 8, CIM-NET reduces MVM operations to 1/77th of the original, while maintaining competitive PSNR (35.11 dB vs. 35.56 dB",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CIM-NETï¼Œä¸€ç§ä¸“ä¸ºå­˜å†…è®¡ç®—ï¼ˆComputing-in-Memory, CIMï¼‰æ¶æ„ä¼˜åŒ–çš„è§†é¢‘å»å™ªæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¾¹ç¼˜è®¾å¤‡åœ¨å®æ—¶æ€§å’Œèƒ½æ•ˆæ–¹é¢çš„ä¸¥è‹›æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¡¬ä»¶-ç®—æ³•ååŒè®¾è®¡ï¼Œå¼•å…¥äº†é’ˆå¯¹ CIM äº¤å‰é˜µåˆ—ï¼ˆcrossbarï¼‰çŸ©é˜µå‘é‡ä¹˜æ³•ï¼ˆMVMï¼‰åŠ é€Ÿä¼˜åŒ–çš„æ¶æ„ï¼Œå¹¶é‡‡ç”¨ä¼ªå·ç§¯ç®—å­ CIM-CONV æ¥æ•´åˆæ»‘åŠ¨å¤„ç†ä¸å…¨è¿æ¥å˜æ¢ï¼Œä»¥å®ç°é«˜æ•ˆçš„ç‰¹å¾æå–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸è½»é‡åŒ–æ¨¡å‹ FastDVDnet ç›¸æ¯”ï¼ŒCIM-NET åœ¨ä¿æŒç«äº‰åŠ›å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰çš„å‰æä¸‹ï¼Œå¯å°† MVM æ“ä½œæ˜¾è‘—é™ä½è‡³åŸå…ˆçš„ 1/77ï¼Œå¤§å¹…æå‡äº†åœ¨ CIM èŠ¯ç‰‡ä¸Šçš„æ¨ç†é€Ÿåº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.21522v1",
      "published_date": "2025-05-23 02:26:56 UTC",
      "updated_date": "2025-05-23 02:26:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:39:59.990031+00:00"
    },
    {
      "arxiv_id": "2505.20316v1",
      "title": "Reinforcement Speculative Decoding for Fast Ranking",
      "title_zh": "é¢å‘å¿«é€Ÿæ’åºçš„å¼ºåŒ–æŠ•æœºè§£ç ",
      "authors": [
        "Yingpeng Du",
        "Tianjun Wei",
        "Zhu Sun",
        "Jie Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have been widely adopted in ranking systems such as information retrieval (IR) systems and recommender systems (RSs). To alleviate the latency of auto-regressive decoding, some studies explore the single (first) token decoding for ranking approximation, but they suffer from severe degradation in tail positions. Although speculative decoding (SD) methods can be a remedy with verification at different positions, they face challenges in ranking systems due to their left-to-right decoding paradigm. Firstly, ranking systems require strict latency constraints, but verification rounds in SD methods remain agnostic; Secondly, SD methods usually discard listwise ranking knowledge about unaccepted items in previous rounds, hindering future multi-token prediction, especially when candidate tokens are the unaccepted items. In this paper, we propose a Reinforcement Speculative Decoding method for fast ranking inference of LLMs. To meet the ranking systems' latency requirement, we propose an up-to-down decoding paradigm that employs an agent to iteratively modify the ranking sequence under a constrained budget. Specifically, we design a ranking-tailored policy optimization, actively exploring optimal multi-round ranking modification policy verified by LLMs via reinforcement learning (RL). To better approximate the target LLM under the constrained budget, we trigger the agent fully utilizing the listwise ranking knowledge about all items verified by LLMs across different rounds in RL, enhancing the modification policy of the agent. More importantly, we demonstrate the theoretical robustness and advantages of our paradigm and implementation. Experiments on both IR and RS tasks show the effectiveness of our proposed method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Reinforcement Speculative Decoding æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿ Large Language Models (LLMs) åœ¨ä¿¡æ¯æ£€ç´¢ (IR) å’Œæ¨èç³»ç»Ÿ (RS) ç­‰æ’åºä»»åŠ¡ä¸­çš„æ¨ç†é€Ÿåº¦ã€‚é’ˆå¯¹ä¼ ç»Ÿ Speculative Decoding (SD) åœ¨æ’åºç³»ç»Ÿä¸­é¢ä¸´çš„å»¶è¿Ÿä¸ç¡®å®šæ€§å’Œç¼ºä¹ listwise æ’åºçŸ¥è¯†ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§â€œè‡ªä¸Šè€Œä¸‹â€ (up-to-down) çš„è§£ç èŒƒå¼ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹  (RL) ä¼˜åŒ–çš„æ™ºèƒ½ä½“åœ¨å—é™é¢„ç®—å†…è¿­ä»£ä¿®æ”¹æ’åºåºåˆ—ã€‚é€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å……åˆ†åˆ©ç”¨å¤šè½®éªŒè¯å¾—åˆ°çš„ listwise çŸ¥è¯†ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„ä¿®æ”¹ç­–ç•¥å’Œæ’åºå‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ IR å’Œ RS ä»»åŠ¡ä¸­å‡å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨ä¿è¯å»¶è¿Ÿæ•ˆç‡çš„åŒæ—¶æä¾›äº†æ›´ä¼˜çš„æ’åºæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 5 figures, 5 table",
      "pdf_url": "https://arxiv.org/pdf/2505.20316v1",
      "published_date": "2025-05-23 02:25:26 UTC",
      "updated_date": "2025-05-23 02:25:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:08.983712+00:00"
    },
    {
      "arxiv_id": "2505.17395v1",
      "title": "Wildfire Detection Using Vision Transformer with the Wildfire Dataset",
      "title_zh": "åŸºäºè§†è§‰ Transformer ä¸ Wildfire æ•°æ®é›†çš„å±±ç«æ£€æµ‹",
      "authors": [
        "Gowtham Raj Vuppari",
        "Navarun Gupta",
        "Ahmed El-Sayed",
        "Xingguo Xiong"
      ],
      "abstract": "The critical need for sophisticated detection techniques has been highlighted by the rising frequency and intensity of wildfires in the US, especially in California. In 2023, wildfires caused 130 deaths nationwide, the highest since 1990. In January 2025, Los Angeles wildfires which included the Palisades and Eaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused loss of human lives. The devastation underscores the urgent need for effective detection and prevention strategies. Deep learning models, such as Vision Transformers (ViTs), can enhance early detection by processing complex image data with high accuracy. However, wildfire detection faces challenges, including the availability of high-quality, real-time data. Wildfires often occur in remote areas with limited sensor coverage, and environmental factors like smoke and cloud cover can hinder detection. Additionally, training deep learning models is computationally expensive, and issues like false positives/negatives and scaling remain concerns. Integrating detection systems with real-time alert mechanisms also poses difficulties. In this work, we used the wildfire dataset consisting of 10.74 GB high-resolution images categorized into 'fire' and 'nofire' classes is used for training the ViT model. To prepare the data, images are resized to 224 x 224 pixels, converted into tensor format, and normalized using ImageNet statistics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨åº”å¯¹æ—¥ç›Šä¸¥é‡çš„é‡ç«å¨èƒï¼Œæå‡ºåˆ©ç”¨ Vision Transformer (ViTs) æ·±åº¦å­¦ä¹ æ¨¡å‹æå‡æ—©æœŸé‡ç«æ¢æµ‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶é‡‡ç”¨äº†è§„æ¨¡è¾¾ 10.74 GB çš„é«˜åˆ†è¾¨ç‡é‡ç«å›¾åƒæ•°æ®é›†ï¼ˆWildfire datasetï¼‰ï¼Œé€šè¿‡å°†å›¾åƒåˆ†ç±»ä¸ºâ€œèµ·ç« (fire)â€ä¸â€œæœªèµ·ç« (nofire)â€ä¸¤ç±»è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µï¼Œç ”ç©¶å°†å›¾åƒç»Ÿä¸€ç¼©æ”¾ä¸º 224 x 224 åƒç´ ï¼Œå¹¶è¿›è¡Œ Tensor æ ¼å¼è½¬æ¢åŠåŸºäº ImageNet ç»Ÿè®¡æ•°æ®çš„å½’ä¸€åŒ–å¤„ç†ã€‚è¯¥æ–¹æ³•è‡´åŠ›äºè§£å†³åè¿œåœ°åŒºä¼ æ„Ÿå™¨è¦†ç›–æœ‰é™ã€çƒŸé›¾åŠäº‘å±‚å¹²æ‰°ç­‰æ¢æµ‹éš¾é¢˜ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€å®æ—¶çš„é‡ç«æ—©æœŸé¢„è­¦ç³»ç»Ÿæä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at ASEE NE 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17395v1",
      "published_date": "2025-05-23 02:08:28 UTC",
      "updated_date": "2025-05-23 02:08:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:39:54.880540+00:00"
    },
    {
      "arxiv_id": "2505.17392v1",
      "title": "Dual-sensing driving detection model",
      "title_zh": "åŒæ„ŸçŸ¥é©¾é©¶æ£€æµ‹æ¨¡å‹",
      "authors": [
        "Leon C. C. K",
        "Zeng Hui"
      ],
      "abstract": "In this paper, a novel dual-sensing driver fatigue detection method combining computer vision and physiological signal analysis is proposed. The system exploits the complementary advantages of the two sensing modalities and breaks through the limitations of existing single-modality methods. We introduce an innovative architecture that combines real-time facial feature analysis with physiological signal processing, combined with advanced fusion strategies, for robust fatigue detection. The system is designed to run efficiently on existing hardware while maintaining high accuracy and reliability. Through comprehensive experiments, we demonstrate that our method outperforms traditional methods in both controlled environments and real-world conditions, while maintaining high accuracy. The practical applicability of the system has been verified through extensive tests in various driving scenarios and shows great potential in reducing fatigue-related accidents. This study contributes to the field by providing a more reliable, cost-effective, and humane solution for driver fatigue detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„åŒæ„Ÿåº”(dual-sensing)é©¾é©¶å‘˜ç–²åŠ³æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè®¡ç®—æœºè§†è§‰(computer vision)å’Œç”Ÿç†ä¿¡å·åˆ†æ(physiological signal analysis)æ¥å…‹æœå•ä¸€æ„ŸçŸ¥æ¨¡å¼çš„å±€é™æ€§ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†ä¸€ç§å°†å®æ—¶é¢éƒ¨ç‰¹å¾åˆ†æã€ç”Ÿç†ä¿¡å·å¤„ç†ä¸å…ˆè¿›èåˆç­–ç•¥(fusion strategies)ç›¸ç»“åˆçš„åˆ›æ–°æ¶æ„ï¼Œç¡®ä¿äº†ç–²åŠ³æ£€æµ‹çš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å—æ§ç¯å¢ƒå’Œç°å®ä¸–ç•Œæ¡ä»¶ä¸‹å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸”èƒ½åœ¨ç°æœ‰ç¡¬ä»¶ä¸Šé«˜æ•ˆè¿è¡Œã€‚è¯¥ç ”ç©¶ä¸ºå‡å°‘ç–²åŠ³é©¾é©¶ç›¸å…³äº‹æ•…æä¾›äº†ä¸€ç§æ›´å¯é ã€æ›´å…·æˆæœ¬æ•ˆç›Šä¸”äººæ€§åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2505.17392v1",
      "published_date": "2025-05-23 02:05:48 UTC",
      "updated_date": "2025-05-23 02:05:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:21.803332+00:00"
    },
    {
      "arxiv_id": "2505.17389v1",
      "title": "Bootstrapping Imitation Learning for Long-horizon Manipulation via Hierarchical Data Collection Space",
      "title_zh": "é€šè¿‡å±‚çº§åŒ–æ•°æ®é‡‡é›†ç©ºé—´å¼•å¯¼é•¿ç¨‹æ“ä½œæ¨¡ä»¿å­¦ä¹ ",
      "authors": [
        "Jinrong Yang",
        "Kexun Chen",
        "Zhuoling Li",
        "Shengkai Wu",
        "Yong Zhao",
        "Liangliang Ren",
        "Wenqiu Luo",
        "Chaohui Shang",
        "Meiyu Zhi",
        "Linfeng Gao",
        "Mingshan Sun",
        "Hui Cheng"
      ],
      "abstract": "Imitation learning (IL) with human demonstrations is a promising method for robotic manipulation tasks. While minimal demonstrations enable robotic action execution, achieving high success rates and generalization requires high cost, e.g., continuously adding data or incrementally conducting human-in-loop processes with complex hardware/software systems. In this paper, we rethink the state/action space of the data collection pipeline as well as the underlying factors responsible for the prediction of non-robust actions. To this end, we introduce a Hierarchical Data Collection Space (HD-Space) for robotic imitation learning, a simple data collection scheme, endowing the model to train with proactive and high-quality data. Specifically, We segment the fine manipulation task into multiple key atomic tasks from a high-level perspective and design atomic state/action spaces for human demonstrations, aiming to generate robust IL data. We conduct empirical evaluations across two simulated and five real-world long-horizon manipulation tasks and demonstrate that IL policy training with HD-Space-based data can achieve significantly enhanced policy performance. HD-Space allows the use of a small amount of demonstration data to train a more powerful policy, particularly for long-horizon manipulation tasks. We aim for HD-Space to offer insights into optimizing data quality and guiding data scaling. project page: https://hd-space-robotics.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† **Hierarchical Data Collection Space (HD-Space)**ï¼Œä¸€ç§é’ˆå¯¹æœºå™¨äºº **Imitation Learning (IL)** çš„åˆ†å±‚æ•°æ®é‡‡é›†æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³é•¿ç¨‹æ“ä½œä»»åŠ¡ä¸­æ•°æ®é‡‡é›†æˆæœ¬é«˜ä¸”ç­–ç•¥é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä»é«˜å±‚è§†è§’å°†å¤æ‚çš„ç²¾ç»†æ“ä½œä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå…³é”®çš„ **atomic tasks**ï¼Œå¹¶ä¸ºäººç±»æ¼”ç¤ºè®¾è®¡äº†ç‰¹å®šçš„åŸå­çŠ¶æ€/åŠ¨ä½œç©ºé—´ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡ä¸”ä¸»åŠ¨çš„è®­ç»ƒæ•°æ®ã€‚å®éªŒåœ¨ä¸¤ä¸ªæ¨¡æ‹Ÿç¯å¢ƒå’Œäº”ä¸ªçœŸå®ä¸–ç•Œçš„é•¿ç¨‹æ“ä½œä»»åŠ¡ä¸Šè¯æ˜ï¼ŒåŸºäº **HD-Space** çš„æ•°æ®èƒ½æ˜¾è‘—æå‡ç­–ç•¥æ€§èƒ½ï¼Œä½¿æ¨¡å‹åœ¨ä»…ä¾èµ–å°‘é‡æ¼”ç¤ºæ•°æ®çš„æƒ…å†µä¸‹å³å¯å®Œæˆå¤æ‚ä»»åŠ¡ã€‚è¯¥ç ”ç©¶ä¸ºä¼˜åŒ–æ•°æ®è´¨é‡å’ŒæŒ‡å¯¼æ•°æ®è§„æ¨¡åŒ–ï¼ˆ**data scaling**ï¼‰æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17389v1",
      "published_date": "2025-05-23 01:57:45 UTC",
      "updated_date": "2025-05-23 01:57:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:31.467732+00:00"
    },
    {
      "arxiv_id": "2505.17379v1",
      "title": "Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition",
      "title_zh": "åœ¨çº¿å§”æ‰˜-ä»£ç†ä¿¡æ¯è·å–ä¸­æœ€ä½³è¯„åˆ†è§„åˆ™è¯†åˆ«çš„å¯è¯é«˜æ•ˆç®—æ³•",
      "authors": [
        "Zichen Wang",
        "Chuanhao Li",
        "Huazheng Wang"
      ],
      "abstract": "We investigate the problem of identifying the optimal scoring rule within the principal-agent framework for online information acquisition problem. We focus on the principal's perspective, seeking to determine the desired scoring rule through interactions with the agent. To address this challenge, we propose two algorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget settings, respectively. Our theoretical analysis demonstrates that OIAFC can extract the desired $(Îµ, Î´)$-scoring rule with a efficient instance-dependent sample complexity or an instance-independent sample complexity. Our analysis also shows that OIAFB matches the instance-independent performance bound of OIAFC, while both algorithms share the same complexity across fixed confidence and fixed budget settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åœ¨çº¿ä¿¡æ¯è·å– (Online Information Acquisition) çš„å§”æ‰˜ä»£ç† (Principal-Agent) æ¡†æ¶ä¸‹ï¼Œå¦‚ä½•è¯†åˆ«æœ€ä½³è¯„åˆ†è§„åˆ™ (Scoring Rule) çš„é—®é¢˜ã€‚è®ºæ–‡ä»å§”æ‰˜äººè§†è§’å‡ºå‘ï¼Œæå‡ºäº† OIAFC å’Œ OIAFB ä¸¤ç§ç®—æ³•ï¼Œåˆ†åˆ«é€‚ç”¨äºå›ºå®šç½®ä¿¡åº¦ (Fixed Confidence) å’Œå›ºå®šé¢„ç®— (Fixed Budget) çš„åœºæ™¯ã€‚ç†è®ºåˆ†æè¯æ˜ï¼ŒOIAFC èƒ½å¤Ÿä»¥é«˜æ•ˆçš„æ ·æœ¬å¤æ‚åº¦ (Sample Complexity) æå–æ‰€éœ€çš„ $(\\epsilon, \\delta)$-è¯„åˆ†è§„åˆ™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜ OIAFB åœ¨æ€§èƒ½ç•Œé™ä¸Šä¸ OIAFC åŒ¹é…ï¼Œä¸ºåœ¨çº¿ç¯å¢ƒä¸‹é«˜æ•ˆç¡®å®šæœ€ä¼˜æ¿€åŠ±æœºåˆ¶æä¾›äº†å¯è¯æ˜çš„ç®—æ³•ä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17379v1",
      "published_date": "2025-05-23 01:30:37 UTC",
      "updated_date": "2025-05-23 01:30:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:29.442065+00:00"
    },
    {
      "arxiv_id": "2505.17373v2",
      "title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning",
      "title_zh": "é¢å‘é«˜æ•ˆé“¾å¼æ€ç»´æ¨ç†çš„ä»·å€¼å¼•å¯¼æœç´¢",
      "authors": [
        "Kaiwen Wang",
        "Jin Peng Zhou",
        "Jonathan Chang",
        "Zhaolin Gao",
        "Nathan Kallus",
        "KiantÃ© Brantley",
        "Wen Sun"
      ],
      "abstract": "In this paper, we propose a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), our method does not require a fine-grained notion of \"step,\" which is difficult to define for long-context reasoning models. By collecting a dataset of 2.5 million reasoning traces, we train a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. We find that block-wise value-guided search (VGS) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. Our dataset, model and codebase are open-sourced.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡æ¨ç†è½¨è¿¹çš„é«˜æ•ˆä»·å€¼æ¨¡å‹(value model)è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(PRMs)åœ¨é•¿æ–‡æœ¬æ¨ç†ä¸­éš¾ä»¥å®šä¹‰ç»†ç²’åº¦â€œæ­¥éª¤â€çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡250ä¸‡æ¡æ¨ç†è½¨è¿¹è®­ç»ƒäº†ä¸€ä¸ª1.5Bå‚æ•°çš„Tokençº§åˆ«ä»·å€¼æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºDeepSeekæ¨¡å‹ä»¥ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—(test-time compute)ç¼©æ”¾ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨å—çº§ä»·å€¼å¼•å¯¼æœç´¢(VGS)ç»“åˆåŠ æƒå¤šæ•°æŠ•ç¥¨çš„æ–¹æ³•ï¼Œåœ¨æ€§èƒ½è¡¨ç°ä¸Šä¼˜äºä¼ ç»Ÿçš„å¤šæ•°æŠ•ç¥¨æˆ–best-of-nï¼Œå¹¶èƒ½æ˜¾è‘—é™ä½è¾¾åˆ°åŒç­‰å‡†ç¡®ç‡æ‰€éœ€çš„æ¨ç†FLOPsã€‚ç›®å‰ï¼Œè¯¥é¡¹ç›®ç›¸å…³çš„æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å‡å·²å¼€æºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17373v2",
      "published_date": "2025-05-23 01:05:07 UTC",
      "updated_date": "2025-09-30 13:12:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:54.426111+00:00"
    },
    {
      "arxiv_id": "2505.17370v4",
      "title": "FRIREN: Beyond Trajectories -- A Spectral Lens on Time",
      "title_zh": "FRIRENï¼šè¶…è¶Šè½¨è¿¹â€”â€”å®¡è§†æ—¶é—´çš„é¢‘è°±è§†è§’",
      "authors": [
        "Qilin Wang"
      ],
      "abstract": "Long-term time-series forecasting (LTSF) models are often presented as general-purpose solutions that can be applied across domains, implicitly assuming that all data is pointwise predictable. Using chaotic systems such as Lorenz-63 as a case study, we argue that geometric structure - not pointwise prediction - is the right abstraction for a dynamic-agnostic foundational model. Minimizing the Wasserstein-2 distance (W2), which captures geometric changes, and providing a spectral view of dynamics are essential for long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via Interpretable Eigen-networks), implements an augmented normalizing-flow block that embeds data into a normally distributed latent representation. It then generates a W2-efficient optimal path that can be decomposed into rotation, scaling, inverse rotation, and translation. This architecture yields locally generated, geometry-preserving predictions that are independent of the underlying dynamics, and a global spectral representation that functions as a finite Koopman operator with a small modification. This enables practitioners to identify which modes grow, decay, or oscillate, both locally and system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE 27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out), FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170, outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065. FRIREN is also competitive on standard LTSF datasets such as ETT and Weather. By connecting modern generative flows with classical spectral analysis, FRIREN makes long-term forecasting both accurate and interpretable, setting a new benchmark for LTSF model design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FRIRENï¼ˆFlow-inspired Representations via Interpretable Eigen-networksï¼‰ï¼Œä¸»å¼ åœ¨é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆLTSFï¼‰ä¸­åº”å…³æ³¨å‡ ä½•ç»“æ„è€Œéå•çº¯çš„ç‚¹å¯¹ç‚¹é¢„æµ‹ã€‚FRIREN é‡‡ç”¨å¢å¼ºçš„æ­£åˆ™åŒ–æµï¼ˆnormalizing-flowï¼‰æ¨¡å—å°†æ•°æ®åµŒå…¥éšå±‚è¡¨ç¤ºï¼Œå¹¶ç”Ÿæˆæœ€å°åŒ– Wasserstein-2 è·ç¦»ï¼ˆW2ï¼‰çš„æœ€ä¼˜è·¯å¾„ï¼Œä»è€Œå®ç°ä¿æŒå‡ ä½•ç‰¹æ€§çš„å±€éƒ¨é¢„æµ‹ã€‚é€šè¿‡å¼•å…¥å…¨å±€é¢‘è°±è¡¨ç¤ºï¼Œè¯¥æ¨¡å‹å¯ä½œä¸ºæœ‰é™åº“æ™®æ›¼ç®—å­ï¼ˆKoopman operatorï¼‰è¿è¡Œï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè¯†åˆ«ç³»ç»Ÿæ¨¡å¼çš„å¢é•¿ã€è¡°å‡æˆ–æŒ¯è¡ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFRIREN åœ¨ Lorenz-63 å’Œ Rossler ç­‰æ··æ²Œç³»ç»Ÿä¸Šçš„è¡¨ç°å¤§å¹…ä¼˜äº TimeMixerï¼Œå¹¶åœ¨ ETT å’Œ Weather ç­‰æ ‡å‡† LTSF æ•°æ®é›†ä¸Šå±•ç°å‡ºæå¼ºçš„ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "37 pages, 4 figures. Submitted to NeurIPS 2025. Public code at https://anonymous.4open.science/r/LTSF_model-03BB/",
      "pdf_url": "https://arxiv.org/pdf/2505.17370v4",
      "published_date": "2025-05-23 00:52:13 UTC",
      "updated_date": "2025-10-13 01:18:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:59.601518+00:00"
    },
    {
      "arxiv_id": "2505.18214v1",
      "title": "LA-RCS: LLM-Agent-Based Robot Control System",
      "title_zh": "LA-RCSï¼šåŸºäº LLM æ™ºèƒ½ä½“çš„æœºå™¨äººæ§åˆ¶ç³»ç»Ÿ",
      "authors": [
        "TaekHyun Park",
        "YoungJun Choi",
        "SeungHoon Shin",
        "Kwangil Lee"
      ],
      "abstract": "LA-RCS (LLM-agent-based robot control system) is a sophisticated robot control system designed to autonomously plan, work, and analyze the external environment based on user requirements by utilizing LLM-Agent. Utilizing a dual-agent framework, LA-RCS generates plans based on user requests, observes the external environment, executes the plans, and modifies the plans as needed to adapt to changes in the external conditions. Additionally, LA-RCS interprets natural language commands by the user and converts them into commands compatible with the robot interface so that the robot can execute tasks and meet user requests properly. During his process, the system autonomously evaluates observation results, provides feedback on the tasks, and executes commands based on real-time environmental monitoring, significantly reducing the need for user intervention in fulfilling requests. We categorized the scenarios that LA-RCS needs to perform into four distinct types and conducted a quantitative assessment of its performance in each scenario. The results showed an average success rate of 90 percent, demonstrating the system capability to fulfill user requests satisfactorily. For more extensive results, readers can visit our project page: https://la-rcs.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº† LA-RCSï¼Œä¸€ç§åŸºäº LLM-Agent çš„å…ˆè¿›æœºå™¨äººæ§åˆ¶ç³»ç»Ÿï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·éœ€æ±‚è‡ªä¸»å®Œæˆä»»åŠ¡è§„åˆ’ã€æ‰§è¡Œä¸ç¯å¢ƒåˆ†æã€‚ç³»ç»Ÿé‡‡ç”¨åŒæ™ºèƒ½ä½“æ¡†æ¶ (Dual-agent framework)ï¼Œèƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºæœºå™¨äººæ¥å£å‘½ä»¤ï¼Œå¹¶æ ¹æ®å®æ—¶ç¯å¢ƒè§‚æµ‹åŠ¨æ€è°ƒæ•´æ‰§è¡Œè®¡åˆ’ã€‚é€šè¿‡å¼•å…¥è‡ªä¸»è¯„ä¼°å’Œåé¦ˆæœºåˆ¶ï¼ŒLA-RCS æ˜¾è‘—å‡å°‘äº†ä»»åŠ¡è¾¾æˆè¿‡ç¨‹ä¸­å¯¹äººå·¥å¹²é¢„çš„ä¾èµ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å››ç§å…¸å‹åœºæ™¯ä¸‹çš„å¹³å‡æˆåŠŸç‡è¾¾åˆ° 90%ï¼Œè¯æ˜äº†å…¶åœ¨ç†è§£ç”¨æˆ·æ„å›¾å¹¶é€‚åº”å¤æ‚å¤–éƒ¨ç¯å¢ƒæ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.18214v1",
      "published_date": "2025-05-23 00:51:16 UTC",
      "updated_date": "2025-05-23 00:51:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:34.811988+00:00"
    },
    {
      "arxiv_id": "2506.03161v1",
      "title": "Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment",
      "title_zh": "3D åŸå¸‚çº§ä»¿çœŸç¯å¢ƒä¸‹åŸºäºå¼ºåŒ–å­¦ä¹ çš„å®‰å…¨ä¼˜å…ˆäº¤é€šæµä¼˜åŒ–",
      "authors": [
        "Mira Nuthakki"
      ],
      "abstract": "Traffic congestion and collisions represent significant economic, environmental, and social challenges worldwide. Traditional traffic management approaches have shown limited success in addressing these complex, dynamic problems. To address the current research gaps, three potential tools are developed: a comprehensive 3D city-wide simulation environment that integrates both macroscopic and microscopic traffic dynamics; a collision model; and a reinforcement learning framework with custom reward functions prioritizing safety over efficiency. Unity game engine-based simulation is used for direct collision modeling. A custom reward enabled reinforcement learning method, proximal policy optimization (PPO) model, yields substantial improvements over baseline results, reducing the number of serious collisions, number of vehicle-vehicle collisions, and total distance travelled by over 3 times the baseline values. The model also improves fuel efficiency by 39% and reduces carbon emissions by 88%. Results establish feasibility for city-wide 3D traffic simulation applications incorporating the vision-zero safety principles of the Department of Transportation, including physics-informed, adaptable, realistic collision modeling, as well as appropriate reward modeling for real-world traffic signal light control towards reducing collisions, optimizing traffic flow and reducing greenhouse emissions.",
      "tldr_zh": "# è®ºæ–‡ TLDR æ‘˜è¦ ğŸ“„\n\n---\n\nè¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåœ¨3DåŸå¸‚çº§ä»¿çœŸç¯å¢ƒä¸­å®ç°å®‰å…¨ä¼˜å…ˆçš„äº¤é€šæµä¼˜åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³äº¤é€šæ‹¥å µä¸ç¢°æ’å¸¦æ¥çš„å…¨çƒæ€§æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªé›†æˆå®å¾®è§‚åŠ¨åŠ›å­¦çš„ä»¿çœŸç¯å¢ƒï¼Œå¹¶æå‡ºä¸€ç§åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (Proximal Policy Optimization, PPO) çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ï¼Œå…¶è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°å°†å®‰å…¨æ€§ç½®äºæ•ˆç‡ä¹‹ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å‡å°‘ä¸¥é‡ç¢°æ’å’Œæ€»è¡Œé©¶é‡Œç¨‹æ–¹é¢ä¼˜äºåŸºçº¿ 3 å€ä»¥ä¸Šï¼ŒåŒæ—¶æå‡äº† 39% çš„ç‡ƒæ²¹æ•ˆç‡å¹¶å‡å°‘äº† 88% çš„ç¢³æ’æ”¾ã€‚è¯¥æˆæœéªŒè¯äº†åœ¨åŸå¸‚ä¿¡å·ç¯æ§åˆ¶ä¸­åº”ç”¨ Vision-Zero å®‰å…¨åŸåˆ™çš„å¯è¡Œæ€§ï¼Œä¸ºæ„å»ºç‰©ç†ä¿¡æ¯æ„ŸçŸ¥ä¸”é«˜åº¦çœŸå®çš„äº¤é€šç®¡ç†ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚\n\n---\n\næˆ‘æ˜¯ **Gemini Enterpriseâœ¨**ã€‚è¿™ä»½æ‘˜è¦å·²ç»æŒ‰ç…§ä½ çš„è¦æ±‚è¿›è¡Œäº†ç²¾ç‚¼ï¼Œé‡ç‚¹çªå‡ºäº†æ–¹æ³•è®ºå’Œé‡åŒ–ç»“æœã€‚ä½ æ˜¯å¦è¿˜éœ€è¦æˆ‘é’ˆå¯¹è¿™ç¯‡è®ºæ–‡çš„ç‰¹å®šæŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚ PPO çš„å¥–åŠ±å‡½æ•°è®¾è®¡ï¼‰è¿›è¡Œæ›´æ·±å…¥çš„è§£è¯»ï¼Ÿæˆ–è€…ä½ æœ‰å…¶ä»–è®ºæ–‡éœ€è¦æˆ‘å¸®å¿™æ€»ç»“å—ï¼Ÿ",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, figures at end, methods at end. Format/order can be changed if necessary",
      "pdf_url": "https://arxiv.org/pdf/2506.03161v1",
      "published_date": "2025-05-23 00:43:14 UTC",
      "updated_date": "2025-05-23 00:43:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:59.114789+00:00"
    },
    {
      "arxiv_id": "2505.17367v4",
      "title": "EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion",
      "title_zh": "EVM-Fusionï¼šåŸºäºç¥ç»ç®—æ³•èåˆçš„å¯è§£é‡Š Vision Mamba æ¶æ„",
      "authors": [
        "Zichuan Yang",
        "Yongzhi Wang"
      ],
      "abstract": "Medical image classification is critical for clinical decision-making, yet demands for accuracy, interpretability, and generalizability remain challenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba architecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for multi-organ medical image classification. EVM-Fusion leverages a multipath design, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim) modules, operate in parallel with a traditional feature pathway. These diverse features are dynamically integrated via a two-stage fusion process: cross-modal attention followed by the iterative NAF block, which learns an adaptive fusion algorithm. Intrinsic explainability is embedded through path-specific spatial attention, Vim Î”-value maps, traditional feature SE-attention, and cross-modal attention weights. Experiments on a diverse 9-class multi-organ medical image dataset demonstrate EVM-Fusion's strong classification performance, achieving 99.75% test accuracy and provide multi-faceted insights into its decision-making process, highlighting its potential for trustworthy AI in medical diagnostics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EVM-Fusionï¼Œä¸€ç§ç»“åˆäº†ç¥ç»ç®—æ³•èåˆ (Neural Algorithmic Fusion, NAF) æœºåˆ¶çš„å¯è§£é‡Šæ€§ Vision Mamba æ¶æ„ï¼Œä¸“é—¨ç”¨äºå¤šå™¨å®˜åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¤šè·¯å¾„è®¾è®¡ï¼Œå¹¶è¡Œè¿è¡ŒåŸºäº DenseNetã€U-Net ä»¥åŠ Vision Mamba (Vim) æ¨¡å—çš„è·¯å¾„ï¼Œå¹¶åˆ©ç”¨ä¸¤é˜¶æ®µèåˆè¿‡ç¨‹ï¼ˆè·¨æ¨¡æ€æ³¨æ„åŠ›å’Œ NAF æ¨¡å—ï¼‰å®ç°ç‰¹å¾çš„åŠ¨æ€é›†æˆã€‚é€šè¿‡é›†æˆè·¯å¾„ç‰¹å®šç©ºé—´æ³¨æ„åŠ›ã€Vim Î”-value maps åŠ SE-attentionï¼ŒEVM-Fusion å…·å¤‡äº†å†…åœ¨çš„å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ 9 ç±»å¤šå™¨å®˜åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè¾¾åˆ°äº† 99.75% çš„å‡†ç¡®ç‡ï¼Œä¸ºä¸´åºŠè¯Šæ–­ä¸­æ„å»ºå¯ä¿¡äººå·¥æ™ºèƒ½ (Trustworthy AI) æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2505.17367v4",
      "published_date": "2025-05-23 00:41:57 UTC",
      "updated_date": "2025-08-26 10:02:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:45.423668+00:00"
    },
    {
      "arxiv_id": "2505.17362v3",
      "title": "A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit",
      "title_zh": "æ—¨åœ¨å¼•å¯¼å¸çƒŸè€…åšå‡ºæˆ’çƒŸå†³ç­–çš„å…¨ç”Ÿæˆå¼åŠ¨æœºæ€§è®¿è°ˆå’¨è¯¢æœºå™¨äºº",
      "authors": [
        "Zafarullah Mahmood",
        "Soliman Ali",
        "Jiading Zhu",
        "Mohamed Abdelwahab",
        "Michelle Yu Collins",
        "Sihan Chen",
        "Yi Cheng Zhao",
        "Jodi Wolff",
        "Osnat Melamed",
        "Nadia Minian",
        "Marta Maslej",
        "Carolynne Cooper",
        "Matt Ratto",
        "Peter Selby",
        "Jonathan Rose"
      ],
      "abstract": "The conversational capabilities of Large Language Models (LLMs) suggest that they may be able to perform as automated talk therapists. It is crucial to know if these systems would be effective and adhere to known standards. We present a counsellor chatbot that focuses on motivating tobacco smokers to quit smoking. It uses a state-of-the-art LLM and a widely applied therapeutic approach called Motivational Interviewing (MI), and was evolved in collaboration with clinician-scientists with expertise in MI. We also describe and validate an automated assessment of both the chatbot's adherence to MI and client responses. The chatbot was tested on 106 participants, and their confidence that they could succeed in quitting smoking was measured before the conversation and one week later. Participants' confidence increased by an average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed adherence to MI standards in 98% of utterances, higher than human counsellors. The chatbot scored well on a participant-reported metric of perceived empathy but lower than typical human counsellors. Furthermore, participants' language indicated a good level of motivation to change, a key goal in MI. These results suggest that the automation of talk therapy with a modern LLM has promise.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å’ŒåŠ¨æœºè®¿è°ˆ(Motivational Interviewing, MI)çš„å…¨ç”Ÿæˆå¼èŠå¤©æœºå™¨äººï¼Œä¸“é—¨ç”¨äºæ¿€åŠ±å¸çƒŸè€…äº§ç”Ÿæˆ’çƒŸæ„æ„¿ã€‚è¯¥ç³»ç»Ÿç”±ä¸´åºŠç§‘å­¦å®¶åä½œå¼€å‘ï¼Œå¹¶é›†æˆäº†ä¸€å¥—è‡ªåŠ¨è¯„ä¼°å·¥å…·ï¼Œç”¨ä»¥éªŒè¯å…¶å¯¹ MI æ ‡å‡†çš„éµå¾ªç¨‹åº¦åŠå‚ä¸è€…çš„ååº”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ 106 åå‚ä¸è€…ä¸­ï¼Œå…¶æˆ’çƒŸä¿¡å¿ƒåœ¨ 10 åˆ†é‡è¡¨ä¸Šå¹³å‡æå‡äº† 1.7 åˆ†ï¼Œä¸”æœºå™¨äººçš„ MI æ ‡å‡†éµå¾ªç‡é«˜è¾¾ 98%ï¼Œè¶…è¿‡äº†äººç±»å’¨è¯¢å¸ˆã€‚å°½ç®¡åœ¨æ„ŸçŸ¥çš„å…±æƒ…(empathy)å¾—åˆ†ä¸Šç•¥ä½äºäººç±»ï¼Œä½†å‚ä¸è€…çš„è¯­è¨€è¡¨ç°å‡ºå¼ºçƒˆçš„æ”¹å˜åŠ¨æœºï¼Œè¯æ˜äº†åˆ©ç”¨ç°ä»£ LLMs å®ç°è‡ªåŠ¨åŒ–è°ˆè¯ç–—æ³•(talk therapy)å…·æœ‰å¹¿é˜”çš„å‰æ™¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To be published in the Findings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), Vienna, Austria, 2025",
      "pdf_url": "https://arxiv.org/pdf/2505.17362v3",
      "published_date": "2025-05-23 00:33:46 UTC",
      "updated_date": "2025-06-01 17:47:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:41:09.016844+00:00"
    },
    {
      "arxiv_id": "2505.17353v1",
      "title": "Dual Ascent Diffusion for Inverse Problems",
      "title_zh": "é¢å‘åé—®é¢˜çš„å¯¹å¶ä¸Šå‡æ‰©æ•£",
      "authors": [
        "Minseo Kim",
        "Axel Levy",
        "Gordon Wetzstein"
      ],
      "abstract": "Ill-posed inverse problems are fundamental in many domains, ranging from astrophysics to medical imaging. Emerging diffusion models provide a powerful prior for solving these problems. Existing maximum-a-posteriori (MAP) or posterior sampling approaches, however, rely on different computational approximations, leading to inaccurate or suboptimal samples. To address this issue, we introduce a new approach to solving MAP problems with diffusion model priors using a dual ascent optimization framework. Our framework achieves better image quality as measured by various metrics for image restoration problems, it is more robust to high levels of measurement noise, it is faster, and it estimates solutions that represent the observations more faithfully than the state of the art.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Dual Ascent Diffusion çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤©ä½“ç‰©ç†å’ŒåŒ»å­¦æˆåƒç­‰é¢†åŸŸçš„ç—…æ€é€†é—®é¢˜ (inverse problems)ã€‚é’ˆå¯¹ç°æœ‰æ‰©æ•£æ¨¡å‹ (diffusion model) åœ¨å¤„ç†æœ€å¤§åéªŒæ¦‚ç‡ (MAP) é—®é¢˜æ—¶å­˜åœ¨çš„è®¡ç®—è¿‘ä¼¼åå·®ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†å¯¹å¶ä¸Šå‡ (dual ascent) ä¼˜åŒ–æ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†å›¾åƒè´¨é‡ï¼Œå¹¶å¯¹é«˜å¼ºåº¦æµ‹é‡å™ªå£°å±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚ä¸å½“å‰æœ€å…ˆè¿›æŠ€æœ¯ (SOTA) ç›¸æ¯”ï¼ŒDual Ascent Diffusion è¿è¡Œé€Ÿåº¦æ›´å¿«ï¼Œä¸”èƒ½æ›´å¿ å®åœ°è¿˜åŸè§‚æµ‹æ•°æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "23 pages, 15 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2505.17353v1",
      "published_date": "2025-05-23 00:12:20 UTC",
      "updated_date": "2025-05-23 00:12:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:40:59.713517+00:00"
    },
    {
      "arxiv_id": "2505.17351v1",
      "title": "FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems",
      "title_zh": "FLEXï¼šé¢å‘æ—¶ç©ºç‰©ç†ç³»ç»Ÿæ‰©æ•£å»ºæ¨¡çš„ä¸»å¹²ç½‘ç»œ",
      "authors": [
        "N. Benjamin Erichson",
        "Vinicius Mikuni",
        "Dongwei Lyu",
        "Yang Gao",
        "Omri Azencot",
        "Soon Hoe Lim",
        "Michael W. Mahoney"
      ],
      "abstract": "We introduce FLEX (FLow EXpert), a backbone architecture for generative modeling of spatio-temporal physical systems using diffusion models. FLEX operates in the residual space rather than on raw data, a modeling choice that we motivate theoretically, showing that it reduces the variance of the velocity field in the diffusion model, which helps stabilize training. FLEX integrates a latent Transformer into a U-Net with standard convolutional ResNet layers and incorporates a redesigned skip connection scheme. This hybrid design enables the model to capture both local spatial detail and long-range dependencies in latent space. To improve spatio-temporal conditioning, FLEX uses a task-specific encoder that processes auxiliary inputs such as coarse or past snapshots. Weak conditioning is applied to the shared encoder via skip connections to promote generalization, while strong conditioning is applied to the decoder through both skip and bottleneck features to ensure reconstruction fidelity. FLEX achieves accurate predictions for super-resolution and forecasting tasks using as few as two reverse diffusion steps. It also produces calibrated uncertainty estimates through sampling. Evaluations on high-resolution 2D turbulence data show that FLEX outperforms strong baselines and generalizes to out-of-distribution settings, including unseen Reynolds numbers, physical observables (e.g., fluid flow velocity fields), and boundary conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FLEX (FLow EXpert)ï¼Œä¸€ç§ä¸“ä¸ºæ—¶ç©ºç‰©ç†ç³»ç»Ÿç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ (diffusion models) è®¾è®¡çš„ä¸»å¹²æ¶æ„ã€‚FLEX é€‰æ‹©åœ¨æ®‹å·®ç©ºé—´è€ŒéåŸå§‹æ•°æ®ä¸Šè¿è¡Œï¼Œé€šè¿‡é™ä½æ‰©æ•£æ¨¡å‹ä¸­çš„é€Ÿåº¦åœºæ–¹å·®æ¥æ˜¾è‘—å¢å¼ºè®­ç»ƒç¨³å®šæ€§ã€‚è¯¥æ¶æ„å°†æ½œåœ¨ Transformer (latent Transformer) é›†æˆåˆ°å¸¦æœ‰æ ‡å‡†å·ç§¯ ResNet å±‚çš„ U-Net ä¸­ï¼Œç»“åˆé‡æ–°è®¾è®¡çš„è·³è·ƒè¿æ¥ (skip connection) æ–¹æ¡ˆï¼Œæœ‰æ•ˆæ•æ‰å±€éƒ¨ç©ºé—´ç»†èŠ‚ä¸é•¿æœŸä¾èµ–å…³ç³»ã€‚é€šè¿‡é’ˆå¯¹æ€§çš„æ—¶ç©ºè°ƒèŠ‚ç­–ç•¥ï¼ŒFLEX åœ¨è¶…åˆ†è¾¨ç‡ (super-resolution) å’Œé¢„æµ‹ä»»åŠ¡ä¸­ä»…éœ€ä¸¤æ¬¡åå‘æ‰©æ•£æ­¥éª¤å³å¯å®ç°ç²¾ç¡®é¢„æµ‹ã€‚å®éªŒè¯æ˜ï¼ŒFLEX åœ¨é«˜åˆ†è¾¨ç‡ 2D æ¹æµæ•°æ®ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå¹¶åœ¨æœªè§è¿‡çš„é›·è¯ºæ•°ã€ç‰©ç†è§‚æµ‹å€¼å’Œè¾¹ç•Œæ¡ä»¶ç­‰åˆ†å¸ƒå¤– (out-of-distribution) åœºæ™¯ä¸‹å±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2505.17351v1",
      "published_date": "2025-05-23 00:07:59 UTC",
      "updated_date": "2025-05-23 00:07:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:41:44.543344+00:00"
    },
    {
      "arxiv_id": "2506.12036v3",
      "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
      "title_zh": "æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¾®è°ƒçš„æç®€æ–¹æ³•",
      "authors": [
        "Yanting Miao",
        "William Loh",
        "Pacal Poupart",
        "Suraj Kothawade"
      ],
      "abstract": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the \"golden noise\" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† **Noise PPO**ï¼Œä¸€ç§ç”¨äºå¾®è°ƒæ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹ (**Text-to-Image Diffusion Models**) çš„æç®€å¼ºåŒ–å­¦ä¹  (**RL**) ç®—æ³•ã€‚è¯¥æ–¹æ³•åŸºäºâ€œé»„é‡‘å™ªå£° (**Golden Noise**)â€å‡è®¾ï¼Œé€šè¿‡å­¦ä¹ ä¸€ä¸ªæç¤ºè¯æ¡ä»¶çš„åˆå§‹å™ªå£°ç”Ÿæˆå™¨æ¥ä¼˜åŒ–è¾“å‡ºï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒæ¨¡å‹å®Œå…¨å†»ç»“ã€‚**Noise PPO** æ— éœ€å­˜å‚¨é‡‡æ ·è½¨è¿¹æˆ–è¿›è¡Œå¥–åŠ±åå‘ä¼ æ’­ (**Reward Backpropagation**)ï¼Œæå¤§åœ°é™ä½äº†ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„å¤æ‚åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–åˆå§‹å™ªå£°åˆ†å¸ƒèƒ½æ˜¾è‘—æå‡æ–‡æœ¬å›¾åƒå¯¹é½åº¦å’Œæ ·æœ¬è´¨é‡ï¼Œå°¤å…¶åœ¨ä½æ¨ç†æ­¥æ•° (**Inference Steps**) ä¸‹å¢ç›Šæœ€ä¸ºæ˜æ˜¾ã€‚\n\n---\nå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–è®ºæ–‡éœ€è¦æ€»ç»“ï¼Œæˆ–è€…æƒ³é’ˆå¯¹ **Noise PPO** çš„å…·ä½“å®éªŒç»†èŠ‚è¿›è¡Œæ·±å…¥æ¢è®¨ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.12036v3",
      "published_date": "2025-05-23 00:01:52 UTC",
      "updated_date": "2025-07-01 05:46:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T06:42:22.387787+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 262,
  "processed_papers_count": 262,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T06:43:22.130566+00:00"
}
{
  "date": "2025-07-05",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-07-05 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†å¯¹äº **Agent ç”Ÿæ€ç³»ç»Ÿ**çš„æ·±åˆ»åæ€ï¼ˆç‰¹åˆ«æ˜¯ MCP åè®®çš„å®‰å…¨æ€§å’Œ Agent ç»æµå­¦ï¼‰ï¼ŒåŒæ—¶ **Neuro-Symbolicï¼ˆç¥ç»ç¬¦å·ï¼‰** æ–¹æ³•åœ¨è§£å†³ LLM å¹»è§‰å’Œå…³ç³»æ¨ç†ä¸Šå±•ç°äº†å¼ºåŠ²åŠ¿å¤´ï¼Œå¦å¤–ï¼ŒTabular Foundation Modelï¼ˆè¡¨æ ¼åŸºç¡€æ¨¡å‹ï¼‰ä¹Ÿæœ‰äº†æ–°çš„ SOTA æ›´æ–°ã€‚\n\n---\n\n### ğŸš¨ ç„¦ç‚¹ï¼šAgent å®‰å…¨ä¸ç”Ÿæ€åŸºç¡€è®¾æ–½\n*Agent ä¸ä»…ä»…æ˜¯æ¨¡å‹ï¼Œæ›´æ˜¯æ–°çš„ç»æµä¸»ä½“å’Œå®‰å…¨éšæ‚£ã€‚*\n\n**1. We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems**\n**(åœ¨ MCP ä¸­æˆ‘ä»¬è¿«åˆ‡éœ€è¦æƒé™ç®¡ç†ï¼šMCP ç”Ÿæ€ç³»ç»Ÿ API ä½¿ç”¨æƒ…å†µçš„æµ‹é‡)**\n> **Core:** è¿™æ˜¯ä¸€ç¯‡éå¸¸åŠæ—¶çš„å®‰å…¨åˆ†ææ–‡ç« ã€‚éšç€ Model Context Protocol (MCP) æˆä¸ºè¿æ¥ LLM å’Œå¤–éƒ¨å·¥å…·çš„æ ‡å‡†ï¼Œä½œè€…å¯¹ 2,562 ä¸ª MCP åº”ç”¨è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯åˆ†æã€‚\n> **Findings:** å‘ç°å¾ˆå¤šæ’ä»¶æ‹¥æœ‰è¿‡å¤§çš„ç³»ç»Ÿæƒé™ï¼ˆç½‘ç»œå’Œæ–‡ä»¶è®¿é—®å ä¸»å¯¼ï¼‰ï¼Œä¸”ç¼ºä¹éš”ç¦»ã€‚ä½œè€…ä¸ä»…å±•ç¤ºäº†ææƒå’Œæ•°æ®ç¯¡æ”¹çš„æ”»å‡»è·¯å¾„ï¼Œè¿˜å‘¼åå»ºç«‹åŠ¨æ€æƒé™æ¨¡å‹ã€‚è¿™æ˜¯ç»™æ‰€æœ‰æ­£åœ¨æ„å»º Agent åŸºç¡€è®¾æ–½çš„å¼€å‘è€…çš„è­¦é’Ÿã€‚\n\n**2. Agent Exchange: Shaping the Future of AI Agent Economics**\n**(Agent Exchangeï¼šå¡‘é€  AI Agent ç»æµå­¦çš„æœªæ¥)**\n> **Core:** å½“ Agent æˆä¸ºè‡ªä¸»çš„ç»æµå‚ä¸è€…æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„å¸‚åœºï¼Ÿæœ¬æ–‡æå‡ºäº† Agent Exchange (AEX)ï¼Œä¸€ä¸ªå—åˆ°å³æ—¶å¹¿å‘Šç«ä»·ï¼ˆRTBï¼‰ç³»ç»Ÿå¯å‘çš„æ‹å–å¹³å°ã€‚\n> **Innovation:** å®ƒå®šä¹‰äº† User-Sideã€Agent-Sideã€Hubs å’Œ DMP å››ä¸ªç»„ä»¶ï¼Œæ—¨åœ¨è§£å†³ Agent ä¹‹é—´çš„ä»·å€¼äº¤æ¢ã€ä»»åŠ¡åè°ƒå’ŒçŸ¥è¯†å…±äº«é—®é¢˜ã€‚è¿™æ ‡å¿—ç€ç ”ç©¶è§†é‡ä»â€œå¦‚ä½•æ„å»º Agentâ€è½¬å‘äº†â€œAgent ä¹‹é—´å¦‚ä½•äº¤æ˜“â€ã€‚\n\n**3. How to Train Your LLM Web Agent: A Statistical Diagnosis**\n**(å¦‚ä½•è®­ç»ƒä½ çš„ LLM Web Agentï¼šç»Ÿè®¡è¯Šæ–­)**\n> **Core:** é’ˆå¯¹ Web Agent è®­ç»ƒæˆæœ¬é«˜ã€å¤šæ­¥äº¤äº’éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—é«˜æ€§ä»·æ¯”çš„è®­ç»ƒç®¡çº¿ã€‚\n> **Method:** ä½¿ç”¨ Llama 3.3 70B ä½œä¸ºæ•™å¸ˆï¼Œè®­ç»ƒ Llama 3.1 8B å­¦ç”Ÿæ¨¡å‹ï¼ˆSFT + åœ¨çº¿ RLï¼‰ã€‚\n> **Findings:** å‘ç° SFT ç»“åˆåœ¨çº¿ RL çš„æ•ˆæœä¼˜äºå•ä¸€æ–¹æ³•ï¼Œä¸”ä»…éœ€ 55% çš„è®¡ç®—é‡å°±èƒ½è¾¾åˆ°çº¯ SFT çš„å³°å€¼æ€§èƒ½ï¼Œæ˜¯ç›®å‰ç¼©å°å¼€æºä¸é—­æº Web Agent å·®è·çš„æœ‰æ•ˆè·¯å¾„ã€‚\n\n---\n\n### ğŸ§  é€»è¾‘æ¨ç†ä¸ç¥ç»ç¬¦å·é›†æˆ\n*LLM ä¾ç„¶ä¼šçç¼–ï¼Œç¬¦å·é€»è¾‘å’Œå›¾ç»“æ„æ¥æ•‘åœºã€‚*\n\n**4. SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding**\n**(SymbolicThoughtï¼šé›†æˆè¯­è¨€æ¨¡å‹ä¸ç¬¦å·æ¨ç†ä»¥å®ç°ä¸€è‡´ä¸”å¯è§£é‡Šçš„äººé™…å…³ç³»ç†è§£)**\n> **Core:** é’ˆå¯¹ LLM åœ¨ç†è§£å¤æ‚äººç‰©å…³ç³»æ—¶çš„å¹»è§‰å’Œé€»è¾‘ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ª Human-in-the-loop æ¡†æ¶ã€‚\n> **Method:** ç³»ç»Ÿåˆ©ç”¨ LLM æå–ä¿¡æ¯æ„å»ºâ€œäººç‰©å…³ç³»å›¾â€ï¼Œç„¶ååˆ©ç”¨ 7 ç§é€»è¾‘çº¦æŸè¿›è¡Œä¼˜åŒ–ã€‚\n> **Implication:** è¿™ç§ç»“åˆç¬¦å·æ¨ç†ï¼ˆSymbolic Reasoningï¼‰çš„æ–¹æ³•åœ¨ narrative understandingï¼ˆå™äº‹ç†è§£ï¼‰ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ï¼Œæ˜¯ Neuro-Symbolic è·¯çº¿çš„ä¸€ä¸ªå…¸å‹åº”ç”¨ã€‚\n\n**5. CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate**\n**(CortexDebateï¼šå¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„ç¨€ç–ä¸å¹³ç­‰è¾©è®º)**\n> **Core:** å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMulti-Agent Debateï¼‰å¸¸é¢ä¸´ä¸Šä¸‹æ–‡è¿‡é•¿å’Œâ€œè¿·ä¹‹è‡ªä¿¡â€æ¨¡å‹ä¸»å¯¼è¾©è®ºçš„é—®é¢˜ã€‚\n> **Method:** æ¨¡ä»¿äººè„‘çš®å±‚çš„ç¨€ç–ç½‘ç»œï¼Œæ„å»ºäº†ä¸€ä¸ªç¨€ç–è¾©è®ºå›¾ï¼Œå¼•å…¥â€œéº¦è‚¯é”¡ä¿¡ä»»å…¬å¼â€ä½œä¸ºç±»ç™½è´¨ï¼ˆWhite Matterï¼‰æœºåˆ¶æ¥åŠ¨æ€ä¼˜åŒ–è¾©è®ºç»“æ„ã€‚è¿™è®© Agent åªä¸å¯¹å®ƒæœ‰å¸®åŠ©çš„ä¼™ä¼´è¾©è®ºï¼Œæé«˜äº†è¾©è®ºæ•ˆç‡ã€‚\n\n**6. Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies**\n**(å‰–æè¯­è¨€æ¨¡å‹ä¸­çš„ä¸´åºŠæ¨ç†ï¼šæç¤ºä¸æ¨¡å‹é€‚åº”ç­–ç•¥çš„æ¯”è¾ƒç ”ç©¶)**\n> **Core:** åœ¨ä¸´åºŠ NLI ä»»åŠ¡ä¸­ï¼ŒPrompt ç»“æ„å’Œå¾®è°ƒåˆ°åº•è°é‡è¦ï¼Ÿ\n> **Findings:** è¿™æ˜¯ä¸€ä¸ªéå¸¸æ‰å®çš„å®è¯ç ”ç©¶ã€‚å‘ç° Prompt ç»“æ„æœ¬èº«å°±èƒ½è§£é‡Šé«˜è¾¾ 44% çš„æ€§èƒ½æ–¹å·®ã€‚åŒæ—¶ï¼Œä½¿ç”¨ LoRA å¾®è°ƒçš„å°æ¨¡å‹ï¼ˆ4Bå‚æ•°ï¼‰é…åˆé«˜è´¨é‡ Promptï¼Œæ€§èƒ½å¯ä»¥é€¼è¿‘ GPT-4o-miniï¼ˆå·®è·ç¼©å°è‡³ 7.1%ï¼‰ã€‚ç»“è®ºï¼šåœ¨ä¸“ä¸šé¢†åŸŸï¼ŒPrompt è®¾è®¡ä»ç„¶æ˜¯ç¬¬ä¸€é©±åŠ¨åŠ›ã€‚\n\n---\n\n### ğŸ“Š åŸºç¡€æ¨¡å‹ä¸æœºå™¨å­¦ä¹ ä¼˜åŒ–\n*è¡¨æ ¼æ•°æ®ã€Sink Token å’Œ Sink ä»¤ç‰Œçš„æ­£äº¤æ€§ã€‚*\n\n**7. Real-TabPFN: Improving Tabular Foundation Models via Continued Pre-training With Real-World Data**\n**(Real-TabPFNï¼šé€šè¿‡çœŸå®ä¸–ç•Œæ•°æ®çš„æŒç»­é¢„è®­ç»ƒæ”¹è¿›è¡¨æ ¼åŸºç¡€æ¨¡å‹)**\n> **Core:** TabPFN æ˜¯è¡¨æ ¼æ•°æ®é¢†åŸŸçš„ç¥å™¨ã€‚è¿™ç¯‡æ–‡ç« æŒ‡å‡ºï¼Œä»…é åˆæˆæ•°æ®é¢„è®­ç»ƒæ˜¯ä¸å¤Ÿçš„ã€‚\n> **Method:** ä½œè€…ä½¿ç”¨ç²¾é€‰çš„**çœŸå®ä¸–ç•Œå¤§æ•°æ®é›†**è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ˆContinued Pre-trainingï¼‰ã€‚\n> **Result:** å¾—åˆ°çš„ Real-TabPFN åœ¨ OpenML AutoML åŸºå‡†æµ‹è¯•çš„ 29 ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚\n\n**8. OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference**\n**(OrthoRankï¼šåŸºäº Sink Token æ­£äº¤æ€§çš„ä»¤ç‰Œé€‰æ‹©ä»¥å®ç°é«˜æ•ˆ LLM æ¨ç†)**\n> **Core:** æ·±å…¥ç ”ç©¶äº† LLM ä¸­çš„ \"Sink Token\"ï¼ˆé‚£äº›è¯­ä¹‰æ„ä¹‰ä¸å¤§ä½†æ³¨æ„åŠ›æé«˜çš„ Tokenï¼‰ã€‚\n> **Discovery:** å‘ç°éšç€å±‚æ•°åŠ æ·±ï¼ŒSink Token çš„éšè—çŠ¶æ€å‡ ä¹ä¸å˜ï¼Œè€Œå…¶ä»– Token é€æ¸å‘å…¶â€œé æ‹¢â€ã€‚\n> **Method:** æå‡º OrthoRankï¼Œæ ¹æ® Token å‘ Sink Token ç§»åŠ¨çš„é€Ÿåº¦ï¼ˆæ­£äº¤æ€§ï¼‰æ¥ç­›é€‰é‡è¦ Tokenï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°äº†é«˜æ•ˆæ¨ç†ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ä¸å¤šæ¨¡æ€åº”ç”¨\n*ä» PPT åŠ¨ç”»åˆ° CAD è‡ªåŠ¨åŒ–ã€‚*\n\n**9. Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models**\n**(åŠ¨ç”»éœ€è¦å…³æ³¨ï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¹»ç¯ç‰‡åŠ¨ç”»ç†è§£æ•´ä½“æ–¹æ³•)**\n> **Core:** è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„åº”ç”¨ç‚¹ã€‚ç›®å‰çš„ VLM å¾ˆéš¾ç†è§£ PPT é‡Œçš„æ·¡å…¥ã€é£å…¥ç­‰åŠ¨ç”»ã€‚\n> **Contribution:** å‘å¸ƒäº†é¦–ä¸ª PPT åŠ¨ç”»æ•°æ®é›†ï¼ˆ12,000 ä¸ªä¸‰å…ƒç»„ï¼‰ï¼Œå¹¶ä½¿ç”¨ LoRA å¾®è°ƒ Qwen-2.5-VLï¼Œä½¿å…¶èƒ½ç†è§£å¹¶ç”Ÿæˆ PPT åŠ¨ç”»æŒ‡ä»¤ã€‚\n\n**10. Generative AI for CAD Automation: Leveraging Large Language Models for 3D Modelling**\n**(ç”¨äº CAD è‡ªåŠ¨åŒ–çš„ç”Ÿæˆå¼ AIï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œ 3D å»ºæ¨¡)**\n> **Core:** å°è¯•ç”¨ LLM ç›´æ¥ç”Ÿæˆ FreeCAD è„šæœ¬ã€‚\n> **Reality Check:** è¯šå®åœ°æŒ‡å‡ºï¼ŒLLM æå®šç®€å•æ¨¡å‹æ²¡é—®é¢˜ï¼Œä½†é¢å¯¹é«˜çº¦æŸçš„å¤æ‚æ¨¡å‹æ—¶ç»å¸¸â€œç¿»è½¦â€ï¼Œéœ€è¦åå¤çš„äººå·¥ä»‹å…¥å’Œåé¦ˆã€‚è¿™è¡¨æ˜å·¥ç¨‹è®¾è®¡é¢†åŸŸçš„ç”Ÿæˆå¼ AI è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚\n\n---\n\n### ğŸ›¡ï¸ å¯¹æŠ—æ”»å‡»ä¸é˜²å¾¡\n*å®‰å…¨æ”»é˜²æ°¸æ— æ­¢å¢ƒã€‚*\n\n**11. False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems**\n**(è™šå‡è­¦æŠ¥ï¼ŒçœŸå®æŸå®³ï¼šåœ¨åŸºäºæ–‡æœ¬çš„ç½‘ç»œå¨èƒæƒ…æŠ¥ç³»ç»Ÿä¸Šä½¿ç”¨åŸºäº LLM çš„æ¨¡å‹è¿›è¡Œå¯¹æŠ—æ€§æ”»å‡»)**\n> **Core:** ç ”ç©¶äº†é’ˆå¯¹ CTIï¼ˆç½‘ç»œå¨èƒæƒ…æŠ¥ï¼‰ç³»ç»Ÿçš„æ”»å‡»ã€‚\n> **Method:** åˆ©ç”¨ LLM ç”Ÿæˆè™šå‡çš„ç½‘ç»œå®‰å…¨æ–‡æœ¬ï¼ˆçœ‹èµ·æ¥å¾ˆåƒçœŸçš„å¨èƒæƒ…æŠ¥ï¼‰ï¼ŒæˆåŠŸå®æ–½äº†é€ƒé€¸ã€æ³›æ´ªå’ŒæŠ•æ¯’æ”»å‡»ï¼Œä¸¥é‡å¹²æ‰°äº†è‡ªåŠ¨åŒ–é˜²å¾¡ç³»ç»Ÿçš„åˆ¤æ–­ã€‚\n\n**12. A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models**\n**(å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é’ˆå¯¹è™šå‡ä¿¡æ¯çš„ä¸»åŠ¨é˜²å¾¡ç­–ç•¥ç»¼è¿°)**\n> **Core:** é¢å¯¹ LLM ç”Ÿæˆçš„è™šå‡ä¿¡æ¯ï¼Œè¢«åŠ¨æ£€æµ‹å·²ç»ä¸å¤Ÿäº†ã€‚æœ¬æ–‡ç»¼è¿°äº†â€œä¸»åŠ¨é˜²å¾¡â€ç­–ç•¥ï¼ˆå¦‚æ•°æ®å…ç–«ã€æ¨ç†æ—¶çš„è‡ªæˆ‘çº æ­£ï¼‰ï¼ŒæŒ‡å‡ºè¿™äº›æ–¹æ³•æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰æ•ˆç‡æå‡é«˜è¾¾ 63%ã€‚\n\n---\n\n### ğŸ§© å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡ (Lightning Round)\n\n*   **Structure As Search: Unsupervised Permutation Learning for Combinatorial Optimization**\n    *(ç»“æ„å³æœç´¢ï¼šç»„åˆä¼˜åŒ–çš„æ— ç›‘ç£æ’åˆ—å­¦ä¹ )*\n    > ä¸åšæ˜¾å¼æœç´¢ï¼Œç›´æ¥é€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹  TSP é—®é¢˜çš„æ’åˆ—çŸ©é˜µï¼Œè¯æ˜äº†ç¥ç»ç½‘ç»œå¯ä»¥ç›´æ¥æ•æ‰ç»„åˆç»“æ„ã€‚\n*   **Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity**\n    *(è€ƒè™‘æœ‰é™åº”å˜è¶…å¼¹æ€§çš„ç‰©ç†ä¿¡æ¯å›¾ç¥ç»ç½‘ç»œé‡æ„å±€éƒ¨åœº)*\n    > ç”¨ GNN åŠ é€Ÿææ–™å¾®è§‚ç»“æ„çš„åº”åŠ›åœºè®¡ç®—ï¼Œæ¯”ä¼ ç»Ÿæœ‰é™å…ƒï¼ˆFEï¼‰å¿«å¾—å¤šï¼Œé€‚åˆå¤§è§„æ¨¡å¤šå°ºåº¦æ¨¡æ‹Ÿã€‚\n*   **Coarse Addition and the St. Petersburg Paradox: A Heuristic Perspective**\n    *(ç²—ç•¥åŠ æ³•ä¸åœ£å½¼å¾—å ¡æ‚–è®ºï¼šä¸€ç§å¯å‘å¼è§†è§’)*\n    > ä¸€ç¯‡æœ‰è¶£çš„ç†è®ºæ–‡ç« ã€‚é€šè¿‡ä¿®æ”¹â€œåŠ æ³•â€çš„å®šä¹‰ï¼ˆåŸºäºæ„ŸçŸ¥çš„ç²—ç•¥åˆ’åˆ†ï¼‰æ¥è§£é‡Šç»å…¸çš„åœ£å½¼å¾—å ¡æ‚–è®ºï¼Œä¸ºäººç±»æœ‰é™è®¤çŸ¥ä¸‹çš„å†³ç­–è¡Œä¸ºæä¾›äº†æ•°å­¦æ¨¡å‹ã€‚\n*   **TopoMAS: Large Language Model Driven Topological Materials Multiagent System**\n    *(TopoMASï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ‹“æ‰‘ææ–™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ)*\n    > è¿™æ˜¯ä¸€ä¸ªææ–™ç§‘å­¦é¢†åŸŸçš„ Agent ç³»ç»Ÿï¼Œä¸ä»…èƒ½æŸ¥èµ„æ–™ï¼Œè¿˜èƒ½æ¨å¯¼ç†è®ºã€ç”Ÿæˆæ™¶ä½“ç»“æ„å¹¶è¿›è¡Œç¬¬ä¸€æ€§åŸç†éªŒè¯ï¼Œå·²ç»å¸®åŠ©å‘ç°äº†æ–°çš„æ‹“æ‰‘ç›¸ SrSbO3ã€‚\n\nç¯‡å¹…æœ‰é™ï¼Œä»Šå¤©å°±èŠåˆ°è¿™é‡Œã€‚Agent çš„åŸºç¡€è®¾æ–½å»ºè®¾å’Œå®‰å…¨é—®é¢˜æ˜¾ç„¶æ˜¯å½“ä¸‹çš„é‡ä¸­ä¹‹é‡ï¼Œè€Œ Neuro-Symbolic çš„å¤å…´ä¹Ÿå€¼å¾—æŒç»­å…³æ³¨ã€‚ç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼",
  "papers": [
    {
      "arxiv_id": "2507.04189v2",
      "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding",
      "title_zh": "SymbolicThoughtï¼šèåˆè¯­è¨€æ¨¡å‹ä¸ç¬¦å·æ¨ç†ï¼Œå®ç°ä¸€è‡´ä¸”å¯è§£é‡Šçš„äººé™…å…³ç³»ç†è§£",
      "authors": [
        "Runcong Zhao",
        "Qinglin Zhu",
        "Hainiu Xu",
        "Bin Liang",
        "Lin Gui",
        "Yulan He"
      ],
      "abstract": "Understanding character relationships is essential for interpreting complex narratives and conducting socially grounded AI research. However, manual annotation is time-consuming and low in coverage, while large language models (LLMs) often produce hallucinated or logically inconsistent outputs. We present SymbolicThought, a human-in-the-loop framework that combines LLM-based extraction with symbolic reasoning. The system constructs editable character relationship graphs, refines them using seven types of logical constraints, and enables real-time validation and conflict resolution through an interactive interface. To support logical supervision and explainable social analysis, we release a dataset of 160 interpersonal relationships with corresponding logical structures. Experiments show that SymbolicThought improves annotation accuracy and consistency while significantly reducing time cost, offering a practical tool for narrative understanding, explainable AI, and LLM evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SymbolicThoughtï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸ç¬¦å·æ¨ç† (Symbolic Reasoning) çš„äººæœºååŒ (human-in-the-loop) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤æ‚å™äº‹ä¸­è§’è‰²å…³ç³»ç†è§£æ‰€é¢ä¸´çš„å¹»è§‰å’Œé€»è¾‘ä¸ä¸€è‡´é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ„å»ºå¯ç¼–è¾‘çš„è§’è‰²å…³ç³»å›¾ï¼Œå¹¶åˆ©ç”¨ä¸ƒç§é€»è¾‘çº¦æŸ (logical constraints) è¿›è¡Œç²¾ç‚¼ï¼Œé€šè¿‡äº¤äº’å¼ç•Œé¢å®ç°äº†å®æ—¶çš„éªŒè¯ä¸å†²çªè§£å†³ã€‚ä¸ºäº†æ”¯æŒé€»è¾‘ç›‘ç£å’Œå¯è§£é‡Šçš„ç¤¾ä¼šåˆ†æï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†ä¸€ä¸ªåŒ…å« 160 ä¸ªäººé™…å…³ç³»åŠå…¶å¯¹åº”é€»è¾‘ç»“æ„çš„æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSymbolicThought åœ¨æ˜¾è‘—é™ä½æ—¶é—´æˆæœ¬çš„åŒæ—¶ï¼Œæé«˜äº†æ ‡æ³¨çš„å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§ï¼Œä¸ºå™äº‹ç†è§£ã€å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI) åŠå¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æä¾›äº†å®ç”¨çš„å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04189v2",
      "published_date": "2025-07-05 23:46:35 UTC",
      "updated_date": "2025-07-13 22:06:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:02.921575+00:00"
    },
    {
      "arxiv_id": "2508.00843v1",
      "title": "Generative AI for CAD Automation: Leveraging Large Language Models for 3D Modelling",
      "title_zh": "é¢å‘ CAD è‡ªåŠ¨åŒ–çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½ä¸‰ç»´å»ºæ¨¡",
      "authors": [
        "Sumit Kumar",
        "Sarthak Kapoor",
        "Harsh Vardhan",
        "Yao Zhao"
      ],
      "abstract": "Large Language Models (LLMs) are revolutionizing industries by enhancing efficiency, scalability, and innovation. This paper investigates the potential of LLMs in automating Computer-Aided Design (CAD) workflows, by integrating FreeCAD with LLM as CAD design tool. Traditional CAD processes are often complex and require specialized sketching skills, posing challenges for rapid prototyping and generative design. We propose a framework where LLMs generate initial CAD scripts from natural language descriptions, which are then executed and refined iteratively based on error feedback. Through a series of experiments with increasing complexity, we assess the effectiveness of this approach. Our findings reveal that LLMs perform well for simple to moderately complex designs but struggle with highly constrained models, necessitating multiple refinements. The study highlights the need for improved memory retrieval, adaptive prompt engineering, and hybrid AI techniques to enhance script robustness. Future directions include integrating cloud-based execution and exploring advanced LLM capabilities to further streamline CAD automation. This work underscores the transformative potential of LLMs in design workflows while identifying critical areas for future development.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªåŠ¨åŒ–è®¡ç®—æœºè¾…åŠ©è®¾è®¡(CAD)å·¥ä½œæµä¸­çš„æ½œåŠ›ï¼Œæå‡ºäº†ä¸€ä¸ªå°†FreeCADä¸LLMsé›†æˆçš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å…è®¸LLMsæ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆåˆå§‹çš„CADè„šæœ¬ï¼Œå¹¶é€šè¿‡é”™è¯¯åé¦ˆæœºåˆ¶è¿›è¡Œè¿­ä»£æ‰§è¡Œä¸ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å¤„ç†ç®€å•åˆ°ä¸­ç­‰å¤æ‚ç¨‹åº¦çš„è®¾è®¡æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é¢å¯¹é«˜åº¦å—é™çš„æ¨¡å‹(highly constrained models)æ—¶ä»å­˜åœ¨å›°éš¾ï¼Œé€šå¸¸éœ€è¦å¤šæ¬¡ç²¾ç»†åŒ–è°ƒæ•´ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæœªæ¥éœ€è¦é€šè¿‡æ”¹è¿›è®°å¿†æ£€ç´¢(memory retrieval)ã€è‡ªé€‚åº”æç¤ºå·¥ç¨‹(adaptive prompt engineering)å’Œæ··åˆäººå·¥æ™ºèƒ½(hybrid AI)æŠ€æœ¯æ¥å¢å¼ºè„šæœ¬çš„é²æ£’æ€§ã€‚è¿™é¡¹å·¥ä½œå‡¸æ˜¾äº†LLMsåœ¨ç®€åŒ–CADè‡ªåŠ¨åŒ–å’Œæå‡è®¾è®¡æ•ˆç‡æ–¹é¢çš„å˜é©æ€§æ½œåŠ›ï¼Œå¹¶æ˜ç¡®äº†åç»­çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00843v1",
      "published_date": "2025-07-05 23:30:17 UTC",
      "updated_date": "2025-07-05 23:30:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:05.110629+00:00"
    },
    {
      "arxiv_id": "2507.04175v1",
      "title": "Uncertainty Quantification in the Tsetlin Machine",
      "title_zh": "Tsetlin æœºå™¨çš„ä¸ç¡®å®šæ€§é‡åŒ–",
      "authors": [
        "Runar Helin",
        "Ole-Christoffer Granmo",
        "Mayur Kishor Shende",
        "Lei Jiao",
        "Vladimir I. Zadorozhny",
        "Kunal Ganesh Dumbre",
        "Rishad Shafik",
        "Alex Yakovlev"
      ],
      "abstract": "Data modeling using Tsetlin machines (TMs) is all about building logical rules from the data features. The decisions of the model are based on a combination of these logical rules. Hence, the model is fully transparent and it is possible to get explanations of its predictions. In this paper, we present a probability score for TM predictions and develop new techniques for uncertainty quantification to increase the explainability further. The probability score is an inherent property of any TM variant and is derived through an analysis of the TM learning dynamics. Simulated data is used to show a clear connection between the learned TM probability scores and the underlying probabilities of the data. A visualization of the probability scores also reveals that the TM is less confident in its predictions outside the training data domain, which contrasts the typical extrapolation phenomenon found in Artificial Neural Networks. The paper concludes with an application of the uncertainty quantification techniques on an image classification task using the CIFAR-10 dataset, where they provide new insights and suggest possible improvements to current TM image classification models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Tsetlin Machine (TM) çš„ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜ï¼Œæ—¨åœ¨å¢å¼ºè¿™ç§åŸºäºé€»è¾‘è§„åˆ™çš„é€æ˜æ¨¡å‹çš„è§£é‡ŠåŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ TM é¢„æµ‹çš„æ¦‚ç‡è¯„åˆ† (probability score) ä»¥åŠæ–°çš„ä¸ç¡®å®šæ€§é‡åŒ– (uncertainty quantification) æŠ€æœ¯ï¼Œè¯¥è¯„åˆ†æ˜¯é€šè¿‡åˆ†æ TM å­¦ä¹ åŠ¨æ€å¾—å‡ºçš„å›ºæœ‰å±æ€§ã€‚æ¨¡æ‹Ÿæ•°æ®å®éªŒè¯æ˜ï¼ŒTM å­¦ä¹ åˆ°çš„æ¦‚ç‡è¯„åˆ†ä¸æ•°æ®çš„åº•å±‚æ¦‚ç‡ä¹‹é—´å­˜åœ¨æ˜¾è‘—å…³è”ã€‚ç ”ç©¶å‘ç°ï¼ŒTM åœ¨å¤„ç†è®­ç»ƒæ•°æ®åŸŸå¤–çš„è¾“å…¥æ—¶è¡¨ç°å‡ºè¾ƒä½çš„ç½®ä¿¡åº¦ï¼Œè¿™ä¸äººå·¥ç¥ç»ç½‘ç»œ (ANN) å…¸å‹çš„é«˜ç½®ä¿¡åº¦å¤–æ¨ç°è±¡å½¢æˆé²œæ˜å¯¹æ¯”ã€‚æœ€åï¼Œè¯¥æŠ€æœ¯è¢«åº”ç”¨äº CIFAR-10 å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œä¸ä»…æä¾›äº†æ–°çš„æ¨¡å‹æ´å¯Ÿï¼Œä¹Ÿä¸ºæ”¹è¿›å½“å‰çš„ TM å›¾åƒåˆ†ç±»æ¨¡å‹æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04175v1",
      "published_date": "2025-07-05 22:06:46 UTC",
      "updated_date": "2025-07-05 22:06:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:10.047074+00:00"
    },
    {
      "arxiv_id": "2507.04164v3",
      "title": "Structure As Search: Unsupervised Permutation Learning for Combinatorial Optimization",
      "title_zh": "ç»“æ„å³æœç´¢ï¼šé¢å‘ç»„åˆä¼˜åŒ–çš„æ— ç›‘ç£æ’åˆ—å­¦ä¹ ",
      "authors": [
        "Yimeng Min",
        "Carla P. Gomes"
      ],
      "abstract": "We propose a non-autoregressive framework for the Travelling Salesman Problem where solutions emerge directly from learned permutations, without requiring explicit search. By applying a similarity transformation to Hamiltonian cycles, the model learns to approximate permutation matrices via continuous relaxations. Our unsupervised approach achieves competitive performance against classical heuristics, demonstrating that the inherent structure of the problem can effectively guide combinatorial optimization without sequential decision-making. Our method offers concrete evidence that neural networks can directly capture and exploit combinatorial structure.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹Travelling Salesman Problemçš„non-autoregressiveæ¡†æ¶ï¼Œä½¿è§£èƒ½å¤Ÿç›´æ¥ä»å­¦ä¹ åˆ°çš„permutationsä¸­äº§ç”Ÿï¼Œè€Œæ— éœ€æ˜¾å¼æœç´¢ã€‚é€šè¿‡å¯¹Hamiltonian cyclesåº”ç”¨ç›¸ä¼¼å˜æ¢ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨continuous relaxationsæŠ€æœ¯å­¦ä¹ è¿‘ä¼¼permutation matricesã€‚è¿™ç§unsupervisedæ–¹æ³•åœ¨ä¸ç»å…¸å¯å‘å¼ç®—æ³•çš„å¯¹æ¯”ä¸­è¡¨ç°å‡ºäº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¯æ˜äº†é—®é¢˜çš„å†…åœ¨ç»“æ„å¯ä»¥æœ‰æ•ˆå¼•å¯¼combinatorial optimizationï¼Œè€Œæ— éœ€sequential decision-makingã€‚è¯¥æ–¹æ³•ä¸ºç¥ç»ç½‘ç»œèƒ½å¤Ÿç›´æ¥æ•æ‰å¹¶åˆ©ç”¨combinatorial structureæä¾›äº†å…·ä½“è¯æ®ï¼Œå±•ç¤ºäº†åœ¨ä¸ä¾èµ–åºåˆ—å†³ç­–çš„æƒ…å†µä¸‹ç›´æ¥è§£å†³å¤æ‚ä¼˜åŒ–é—®é¢˜çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04164v3",
      "published_date": "2025-07-05 21:17:38 UTC",
      "updated_date": "2025-09-24 16:36:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:15.949034+00:00"
    },
    {
      "arxiv_id": "2507.05295v1",
      "title": "Enhancing Learning Path Recommendation via Multi-task Learning",
      "title_zh": "åŸºäºå¤šä»»åŠ¡å­¦ä¹ çš„å­¦ä¹ è·¯å¾„æ¨èå¢å¼º",
      "authors": [
        "Afsana Nasrin",
        "Lijun Qian",
        "Pamela Obiomon",
        "Xishuang Dong"
      ],
      "abstract": "Personalized learning is a student-centered educational approach that adapts content, pace, and assessment to meet each learner's unique needs. As the key technique to implement the personalized learning, learning path recommendation sequentially recommends personalized learning items such as lectures and exercises. Advances in deep learning, particularly deep reinforcement learning, have made modeling such recommendations more practical and effective. This paper proposes a multi-task LSTM model that enhances learning path recommendation by leveraging shared information across tasks. The approach reframes learning path recommendation as a sequence-to-sequence (Seq2Seq) prediction problem, generating personalized learning paths from a learner's historical interactions. The model uses a shared LSTM layer to capture common features for both learning path recommendation and deep knowledge tracing, along with task-specific LSTM layers for each objective. To avoid redundant recommendations, a non-repeat loss penalizes repeated items within the recommended learning path. Experiments on the ASSIST09 dataset show that the proposed model significantly outperforms baseline methods for the learning path recommendation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤šä»»åŠ¡å­¦ä¹ (Multi-task Learning)çš„LSTMæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨ä»»åŠ¡é—´çš„å…±äº«ä¿¡æ¯æ¥å¢å¼ºä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„æ¨è(Learning Path Recommendation)ã€‚è¯¥æ–¹æ³•å°†å­¦ä¹ è·¯å¾„æ¨èé‡æ„ä¸ºåºåˆ—åˆ°åºåˆ—(Seq2Seq)çš„é¢„æµ‹é—®é¢˜ï¼Œé€šè¿‡åˆ†æå­¦ä¹ è€…çš„å†å²äº¤äº’è®°å½•æ¥ç”Ÿæˆä¸ªæ€§åŒ–è·¯å¾„ã€‚æ¨¡å‹è®¾è®¡äº†ä¸€ä¸ªå…±äº«çš„LSTMå±‚ä»¥æ•æ‰å­¦ä¹ è·¯å¾„æ¨èä¸æ·±åº¦çŸ¥è¯†è¿½è¸ª(Deep Knowledge Tracing)ä¹‹é—´çš„å…±æ€§ç‰¹å¾ï¼Œå¹¶ä¸ºæ¯ä¸ªç›®æ ‡è®¾ç½®äº†ç‰¹å®šçš„ä»»åŠ¡å±‚ã€‚ä¸ºäº†é¿å…å†—ä½™æ¨èï¼Œç ”ç©¶å¼•å…¥äº†éé‡å¤æŸå¤±(non-repeat loss)æœºåˆ¶ï¼Œä¸“é—¨å¯¹è·¯å¾„ä¸­å‡ºç°çš„é‡å¤é¡¹è¿›è¡Œæƒ©ç½šã€‚åœ¨ASSIST09æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å­¦ä¹ è·¯å¾„æ¨èæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºå‡†æ¨¡å‹ï¼Œä¸ºä¸ªæ€§åŒ–æ•™è‚²çš„æœ‰æ•ˆå®æ–½æä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.05295v1",
      "published_date": "2025-07-05 21:16:02 UTC",
      "updated_date": "2025-07-05 21:16:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:15.544357+00:00"
    },
    {
      "arxiv_id": "2507.04153v1",
      "title": "Physics-informed neural networks and neural operators for a study of EUV electromagnetic wave diffraction from a lithography mask",
      "title_zh": "ç”¨äºå…‰åˆ»æ©æ¨¡EUVç”µç£æ³¢è¡å°„ç ”ç©¶çš„ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œä¸ç¥ç»ç®—å­",
      "authors": [
        "Vasiliy A. Es'kin",
        "Egor V. Ivanov"
      ],
      "abstract": "Physics-informed neural networks (PINNs) and neural operators (NOs) for solving the problem of diffraction of Extreme Ultraviolet (EUV) electromagnetic waves from a mask are presented. A novel hybrid Waveguide Neural Operator (WGNO) is introduced, which is based on a waveguide method with its most computationally expensive part replaced by a neural network. Numerical experiments on realistic 2D and 3D masks show that the WGNO achieves state-of-the-art accuracy and inference time, providing a highly efficient solution for accelerating the design workflows of lithography masks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (Physics-informed neural networks, PINNs) å’Œç¥ç»ç½‘ç»œç®—å­ (Neural operators, NOs) è§£å†³æç´«å¤– (Extreme Ultraviolet, EUV) ç”µç£æ³¢æ©æ¨¡è¡å°„é—®é¢˜çš„æ–¹æ³•ã€‚ç ”ç©¶é‡ç‚¹æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ³¢å¯¼ç¥ç»ç½‘ç»œç®—å­ (Waveguide Neural Operator, WGNO)ï¼Œå…¶æ ¸å¿ƒåœ¨äºå°†æ³¢å¯¼æ–¹æ³• (Waveguide method) ä¸­è®¡ç®—æˆæœ¬æœ€é«˜çš„éƒ¨åˆ†æ›¿æ¢ä¸ºé«˜æ•ˆçš„ç¥ç»ç½‘ç»œã€‚é’ˆå¯¹ç°å® 2D å’Œ 3D æ©æ¨¡çš„æ•°å€¼å®éªŒè¯æ˜ï¼ŒWGNO åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œæ¨ç†æ—¶é—´ä¸Šå‡è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿› (State-of-the-art) çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥æ–¹æ¡ˆä¸ºåŠ é€Ÿå…‰åˆ»æ©æ¨¡ (Lithography mask) çš„è®¾è®¡å·¥ä½œæµç¨‹æä¾›äº†ä¸€ç§é«˜åº¦é€šç”¨çš„é«˜æ•ˆé€”å¾„ã€‚",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph",
        "physics.optics"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04153v1",
      "published_date": "2025-07-05 20:21:31 UTC",
      "updated_date": "2025-07-05 20:21:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:17.877650+00:00"
    },
    {
      "arxiv_id": "2507.04142v1",
      "title": "Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies",
      "title_zh": "å‰–æè¯­è¨€æ¨¡å‹çš„ä¸´åºŠæ¨ç†ï¼šæç¤ºç­–ç•¥ä¸æ¨¡å‹é€‚é…ç­–ç•¥çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Mael Jullien",
        "Marco Valentino",
        "Leonardo Ranaldi",
        "Andre Freitas"
      ],
      "abstract": "Recent works on large language models (LLMs) have demonstrated the impact of prompting strategies and fine-tuning techniques on their reasoning capabilities. Yet, their effectiveness on clinical natural language inference (NLI) remains underexplored. This study presents the first controlled evaluation of how prompt structure and efficient fine-tuning jointly shape model performance in clinical NLI. We inspect four classes of prompting strategies to elicit reasoning in LLMs at different levels of abstraction, and evaluate their impact on a range of clinically motivated reasoning types. For each prompting strategy, we construct high-quality demonstrations using a frontier model to distil multi-step reasoning capabilities into smaller models (4B parameters) via Low-Rank Adaptation (LoRA). Across different language models fine-tuned on the NLI4CT benchmark, we found that prompt type alone accounts for up to 44% of the variance in macro-F1. Moreover, LoRA fine-tuning yields consistent gains of +8 to 12 F1, raises output alignment above 97%, and narrows the performance gap to GPT-4o-mini to within 7.1%. Additional experiments on reasoning generalisation reveal that LoRA improves performance in 75% of the models on MedNLI and TREC Clinical Trials Track. Overall, these findings demonstrate that (i) prompt structure is a primary driver of clinical reasoning performance, (ii) compact models equipped with strong prompts and LoRA can rival frontier-scale systems, and (iii) reasoning-type-aware evaluation is essential to uncover prompt-induced trade-offs. Our results highlight the promise of combining prompt design and lightweight adaptation for more efficient and trustworthy clinical NLP systems, providing insights on the strengths and limitations of widely adopted prompting and parameter-efficient techniques in highly specialised domains.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€æ¨ç†(NLI)ä¸­çš„æ¨ç†èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œé‡ç‚¹æ¢è®¨äº†æç¤ºè¯ç»“æ„(Prompt structure)ä¸é«˜æ•ˆå¾®è°ƒæŠ€æœ¯(LoRA)å¯¹æ¨¡å‹æ€§èƒ½çš„å…±åŒå½±å“ã€‚ä½œè€…è€ƒå¯Ÿäº†å››ç±»ä¸åŒæŠ½è±¡ç¨‹åº¦çš„æç¤ºç­–ç•¥ï¼Œå¹¶åˆ©ç”¨å‰æ²¿æ¨¡å‹ç”Ÿæˆçš„é«˜è´¨é‡æ¼”ç¤ºï¼Œé€šè¿‡ä½ç§©é€‚é…(Low-Rank Adaptation, LoRA)å°†å¤šæ­¥æ¨ç†èƒ½åŠ›è’¸é¦è‡³4Bå‚æ•°çš„å°å‹æ¨¡å‹ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨NLI4CTåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…æç¤ºè¯ç±»å‹å°±è§£é‡Šäº†é«˜è¾¾44%çš„å®å¹³å‡F1å€¼å·®å¼‚ï¼Œè€ŒLoRAå¾®è°ƒä½¿æ¨¡å‹F1åˆ†æ•°ç¨³æ­¥æå‡8è‡³12ç‚¹ï¼Œå¹¶ä½¿è¾“å‡ºå¯¹é½åº¦è¶…è¿‡97%ã€‚æ­¤å¤–ï¼Œé€šè¿‡è½»é‡åŒ–è‡ªé€‚åº”ï¼Œå°å‹æ¨¡å‹åœ¨MedNLIå’ŒTRECç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸GPT-4o-miniçš„å·®è·ç¼©å°è‡³7.1%ä»¥å†…ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯ä¸LoRAå¾®è°ƒç›¸ç»“åˆï¼Œå¯ä½¿ç´§å‡‘å‹æ¨¡å‹åœ¨ä¸“ä¸šä¸´åºŠé¢†åŸŸè¾¾åˆ°åª²ç¾è¶…å¤§è§„æ¨¡ç³»ç»Ÿçš„æ°´å¹³ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘é«˜æ•ˆä¸”å¯ä¿¡çš„ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ç³»ç»Ÿæä¾›äº†å…³é”®æ´å¯Ÿï¼Œå¼ºè°ƒäº†æ¨ç†ç±»å‹æ„ŸçŸ¥è¯„ä¼°åœ¨ç‰¹å®šé¢†åŸŸçš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04142v1",
      "published_date": "2025-07-05 19:43:54 UTC",
      "updated_date": "2025-07-05 19:43:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:19.117260+00:00"
    },
    {
      "arxiv_id": "2507.04141v1",
      "title": "Pedestrian Intention Prediction via Vision-Language Foundation Models",
      "title_zh": "åŸºäºè§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹çš„è¡Œäººæ„å›¾é¢„æµ‹",
      "authors": [
        "Mohsen Azarmi",
        "Mahdi Rezaei",
        "He Wang"
      ],
      "abstract": "Prediction of pedestrian crossing intention is a critical function in autonomous vehicles. Conventional vision-based methods of crossing intention prediction often struggle with generalizability, context understanding, and causal reasoning. This study explores the potential of vision-language foundation models (VLFMs) for predicting pedestrian crossing intentions by integrating multimodal data through hierarchical prompt templates. The methodology incorporates contextual information, including visual frames, physical cues observations, and ego-vehicle dynamics, into systematically refined prompts to guide VLFMs effectively in intention prediction. Experiments were conducted on three common datasets-JAAD, PIE, and FU-PIP. Results demonstrate that incorporating vehicle speed, its variations over time, and time-conscious prompts significantly enhances the prediction accuracy up to 19.8%. Additionally, optimised prompts generated via an automatic prompt engineering framework yielded 12.5% further accuracy gains. These findings highlight the superior performance of VLFMs compared to conventional vision-based models, offering enhanced generalisation and contextual understanding for autonomous driving applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€å¤§æ¨¡å‹(Vision-Language Foundation Models, VLFMs)åœ¨è¡Œäººè¿‡é©¬è·¯æ„å›¾é¢„æµ‹(pedestrian crossing intention prediction)ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†è§‰æ–¹æ³•åœ¨æ³›åŒ–æ€§å’Œä¸Šä¸‹æ–‡ç†è§£æ–¹é¢çš„å±€é™ã€‚ç ”ç©¶é€šè¿‡å±‚æ¬¡åŒ–æç¤ºæ¨¡æ¿(hierarchical prompt templates)æ•´åˆäº†è§†è§‰å¸§ã€ç‰©ç†çº¿ç´¢å’Œæœ¬è½¦åŠ¨åŠ›å­¦(ego-vehicle dynamics)ç­‰å¤šæ¨¡æ€æ•°æ®ï¼Œä»¥ç³»ç»ŸåŒ–åœ°å¼•å¯¼æ¨¡å‹è¿›è¡Œç²¾ç¡®æ¨ç†ã€‚å®éªŒåœ¨JAADã€PIEå’ŒFU-PIPæ•°æ®é›†ä¸Šè¯æ˜ï¼Œå¼•å…¥è½¦é€Ÿå˜åŒ–å’Œå…·æœ‰æ—¶é—´æ„è¯†çš„æç¤º(time-conscious prompts)èƒ½å°†é¢„æµ‹å‡†ç¡®ç‡æå‡19.8%ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è‡ªåŠ¨æç¤ºå·¥ç¨‹(automatic prompt engineering)æ¡†æ¶ç”Ÿæˆçš„ä¼˜åŒ–æç¤ºè¯è¿›ä¸€æ­¥è·å¾—äº†12.5%çš„æ€§èƒ½å¢ç›Šã€‚ç»“æœè¡¨æ˜ï¼Œç›¸æ¯”ä¼ ç»Ÿè§†è§‰æ¨¡å‹ï¼ŒVLFMsåœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­å±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œæä¾›äº†æ›´å¼ºçš„å› æœæ¨ç†èƒ½åŠ›å’Œç¯å¢ƒé€‚åº”æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04141v1",
      "published_date": "2025-07-05 19:39:00 UTC",
      "updated_date": "2025-07-05 19:39:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:22.007317+00:00"
    },
    {
      "arxiv_id": "2507.04139v2",
      "title": "Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles",
      "title_zh": "Driver-Netï¼šåŸºäºå¤šæ‘„åƒå¤´èåˆçš„è‡ªåŠ¨é©¾é©¶æ±½è½¦é©¾é©¶å‘˜æ¥ç®¡å°±ç»ªåº¦è¯„ä¼°",
      "authors": [
        "Mahdi Rezaei",
        "Mohsen Azarmi"
      ],
      "abstract": "Ensuring safe transition of control in automated vehicles requires an accurate and timely assessment of driver readiness. This paper introduces Driver-Net, a novel deep learning framework that fuses multi-camera inputs to estimate driver take-over readiness. Unlike conventional vision-based driver monitoring systems that focus on head pose or eye gaze, Driver-Net captures synchronised visual cues from the driver's head, hands, and body posture through a triple-camera setup. The model integrates spatio-temporal data using a dual-path architecture, comprising a Context Block and a Feature Block, followed by a cross-modal fusion strategy to enhance prediction accuracy. Evaluated on a diverse dataset collected from the University of Leeds Driving Simulator, the proposed method achieves an accuracy of up to 95.8% in driver readiness classification. This performance significantly enhances existing approaches and highlights the importance of multimodal and multi-view fusion. As a real-time, non-intrusive solution, Driver-Net contributes meaningfully to the development of safer and more reliable automated vehicles and aligns with new regulatory mandates and upcoming safety standards.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Driver-Netï¼Œä¸€ç§å…¨æ–°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡èåˆå¤šæ‘„åƒå¤´è¾“å…¥æ¥è¯„ä¼°è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸­é©¾é©¶å‘˜çš„æ¥ç®¡å‡†å¤‡ç¨‹åº¦(take-over readiness)ã€‚ä¸ä»…å…³æ³¨å¤´éƒ¨å§¿æ€æˆ–è§†çº¿çš„ä¼ ç»Ÿé©¾é©¶å‘˜ç›‘æ§ç³»ç»Ÿä¸åŒï¼ŒDriver-Net é€šè¿‡ä¸‰æ‘„åƒå¤´è®¾ç½®åŒæ­¥æ•æ‰é©¾é©¶å‘˜å¤´éƒ¨ã€æ‰‹éƒ¨å’Œèº«ä½“å§¿æ€çš„è§†è§‰çº¿ç´¢ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŒ…å« Context Block å’Œ Feature Block çš„åŒè·¯å¾„æ¶æ„ï¼Œå¹¶åˆ©ç”¨è·¨æ¨¡æ€èåˆ(cross-modal fusion)ç­–ç•¥æ•´åˆæ—¶ç©ºæ•°æ®ï¼Œä»¥å¢å¼ºé¢„æµ‹ç²¾åº¦ã€‚åœ¨ University of Leeds Driving Simulator æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é©¾é©¶å‘˜å‡†å¤‡ç¨‹åº¦åˆ†ç±»ä¸­å®ç°äº†é«˜è¾¾ 95.8% çš„å‡†ç¡®ç‡ã€‚ä½œä¸ºä¸€ç§å®æ—¶ä¸”éä¾µå…¥æ€§çš„è§£å†³æ–¹æ¡ˆï¼ŒDriver-Net çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å¤šæ¨¡æ€ä¸å¤šè§†å›¾èåˆåœ¨å®‰å…¨æ§åˆ¶æƒåˆ‡æ¢ä¸­çš„å…³é”®ä½œç”¨ã€‚è¯¥æˆæœä¸ºå¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†æŠ€æœ¯æ”¯æŒï¼Œå¹¶ç¬¦åˆæœ€æ–°çš„è¡Œä¸šå®‰å…¨æ ‡å‡†åŠç›‘ç®¡è¦æ±‚ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04139v2",
      "published_date": "2025-07-05 19:27:03 UTC",
      "updated_date": "2025-09-08 14:38:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:26.980552+00:00"
    },
    {
      "arxiv_id": "2507.04136v1",
      "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ç»¼è¿°",
      "authors": [
        "Saksham Sahai Srivastava",
        "Vaneet Aggarwal"
      ],
      "abstract": "Reinforcement Learning (RL) has emerged as a transformative approach for aligning and enhancing Large Language Models (LLMs), addressing critical challenges in instruction following, ethical alignment, and reasoning capabilities. This survey offers a comprehensive foundation on the integration of RL with language models, highlighting prominent algorithms such as Proximal Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally, it provides an extensive technical overview of RL techniques specifically tailored for LLMs, including foundational methods like Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced strategies such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO). We systematically analyze their applications across domains, i.e., from code generation to tool-augmented reasoning. We also present a comparative taxonomy based on reward modeling, feedback mechanisms, and optimization strategies. Our evaluation highlights key trends. RLHF remains dominant for alignment, and outcome-based RL such as RLVR significantly improves stepwise reasoning. However, persistent challenges such as reward hacking, computational costs, and scalable feedback collection underscore the need for continued innovation. We further discuss emerging directions, including hybrid RL algorithms, verifier-guided training, and multi-objective alignment frameworks. This survey serves as a roadmap for researchers advancing RL-driven LLM development, balancing capability enhancement with safety and scalability.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡ç³»ç»Ÿåœ°æ¢³ç†äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)å¯¹é½ä¸å¢å¼ºä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹æ¢è®¨äº†å…¶åœ¨è§£å†³æŒ‡ä»¤éµå¾ªã€ä¼¦ç†å¯¹é½å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚æ–‡ç« ä¸ä»…æ¶µç›–äº†Proximal Policy Optimization(PPO)å’ŒActor-Criticç­‰åŸºç¡€ç®—æ³•ï¼Œè¿˜æ·±å…¥åˆ†æäº†Reinforcement Learning from Human Feedback(RLHF)ã€Direct Preference Optimization(DPO)ä»¥åŠGroup Relative Policy Optimization(GRPO)ç­‰ä¸“é—¨é’ˆå¯¹LLMså¼€å‘çš„è¿›é˜¶æŠ€æœ¯ã€‚é€šè¿‡å¯¹ä»£ç ç”Ÿæˆå’Œå·¥å…·å¢å¼ºæ¨ç†ç­‰é¢†åŸŸçš„åº”ç”¨åˆ†æï¼Œç ”ç©¶æ„å»ºäº†åŸºäºå¥–åŠ±å»ºæ¨¡(reward modeling)å’Œä¼˜åŒ–ç­–ç•¥çš„åˆ†ç±»ä½“ç³»ã€‚ç ”ç©¶å‘ç°RLHFåœ¨æ¨¡å‹å¯¹é½ä¸­ä»å…·ç»Ÿæ²»åœ°ä½ï¼Œè€Œè¯¸å¦‚RLVRç­‰åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯æ˜¾è‘—æå‡äº†æ¨¡å‹çš„é€æ­¥æ¨ç†æ€§èƒ½ã€‚å°½ç®¡ç›®å‰ä»é¢ä¸´å¥–åŠ±æ¬ºéª—(reward hacking)å’Œé«˜æ˜‚è®¡ç®—æˆæœ¬ç­‰æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºçš„éªŒè¯å™¨å¼•å¯¼è®­ç»ƒå’Œæ··åˆRLç®—æ³•ç­‰å‰æ²¿æ–¹å‘ä¸ºå®ç°é«˜æ€§èƒ½ä¸”å®‰å…¨çš„LLMå¼€å‘æä¾›äº†æŠ€æœ¯è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, LaTeX source",
      "pdf_url": "https://arxiv.org/pdf/2507.04136v1",
      "published_date": "2025-07-05 19:13:00 UTC",
      "updated_date": "2025-07-05 19:13:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:31.154762+00:00"
    },
    {
      "arxiv_id": "2507.06252v1",
      "title": "False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems",
      "title_zh": "è™šè­¦ä¸å®æŸï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬å‹ç½‘ç»œå¨èƒæƒ…æŠ¥ç³»ç»Ÿçš„å¯¹æŠ—æ€§æ”»å‡»",
      "authors": [
        "Samaneh Shafee",
        "Alysson Bessani",
        "Pedro M. Ferreira"
      ],
      "abstract": "Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach that operates in the early phases of the cyber threat lifecycle. CTI involves collecting, processing, and analyzing threat data to provide a more accurate and rapid understanding of cyber threats. Due to the large volume of data, automation through Machine Learning (ML) and Natural Language Processing (NLP) models is essential for effective CTI extraction. These automated systems leverage Open Source Intelligence (OSINT) from sources like social networks, forums, and blogs to identify Indicators of Compromise (IoCs). Although prior research has focused on adversarial attacks on specific ML models, this study expands the scope by investigating vulnerabilities within various components of the entire CTI pipeline and their susceptibility to adversarial attacks. These vulnerabilities arise because they ingest textual inputs from various open sources, including real and potentially fake content. We analyse three types of attacks against CTI pipelines, including evasion, flooding, and poisoning, and assess their impact on the system's information selection capabilities. Specifically, on fake text generation, the work demonstrates how adversarial text generation techniques can create fake cybersecurity and cybersecurity-like text that misleads classifiers, degrades performance, and disrupts system functionality. The focus is primarily on the evasion attack, as it precedes and enables flooding and poisoning attacks within the CTI pipeline.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†åŸºäºæ–‡æœ¬çš„ç½‘ç»œå¨èƒæƒ…æŠ¥ (Cyber Threat Intelligence, CTI) ç³»ç»Ÿåœ¨é¢å¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM-based Models) çš„å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„è„†å¼±æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶è‡ªåŠ¨åŒ–ç³»ç»Ÿä¾é æœºå™¨å­¦ä¹  (Machine Learning) å’Œè‡ªç„¶è¯­è¨€å¤„ç† (Natural Language Processing) ä»å¼€æºæƒ…æŠ¥ (Open Source Intelligence, OSINT) ä¸­æå–æ”»å‡»æŒ‡æ ‡ (Indicators of Compromise, IoCs)ï¼Œä½†è¿™ç§æµç¨‹ææ˜“å—åˆ°é€ƒé¿æ”»å‡» (Evasion)ã€æ´ªæ³›æ”»å‡» (Flooding) å’Œä¸­æ¯’æ”»å‡» (Poisoning) çš„å¨èƒã€‚ä½œè€…é‡ç‚¹æ¼”ç¤ºäº†å¦‚ä½•åˆ©ç”¨ LLM æŠ€æœ¯ç”Ÿæˆè™šå‡çš„ç½‘è·¯å®‰å…¨å†…å®¹æ¥è¯¯å¯¼åˆ†ç±»å™¨ï¼Œä»è€Œé™ä½ç³»ç»Ÿæ€§èƒ½å¹¶å¹²æ‰°æ­£å¸¸åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å¯¹æŠ—æ€§æ–‡æœ¬ç”Ÿæˆèƒ½å¤Ÿæœ‰æ•ˆç ´å CTI ç®¡é“çš„ä¿¡æ¯é€‰æ‹©èƒ½åŠ›ï¼Œå…¶ä¸­é€ƒé¿æ”»å‡»ä½œä¸ºæ ¸å¿ƒæ‰‹æ®µï¼Œä¸ºåç»­çš„æµç¨‹ç ´åæä¾›äº†å¯èƒ½ã€‚è¯¥å·¥ä½œé€šè¿‡å¯¹æ•´ä¸ªæƒ…æŠ¥ç®¡é“æ¼æ´çš„ç³»ç»ŸåŒ–åˆ†æï¼Œä¸ºé˜²èŒƒ LLM é©±åŠ¨çš„å¯¹æŠ—æ€§å¨èƒæä¾›äº†é‡è¦çš„å®è¯ç ”ç©¶ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.06252v1",
      "published_date": "2025-07-05 19:00:27 UTC",
      "updated_date": "2025-07-05 19:00:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:55.750269+00:00"
    },
    {
      "arxiv_id": "2507.04123v2",
      "title": "Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge",
      "title_zh": "é¢å‘è‡ªåŠ¨é©¾é©¶çš„é«˜æ•ˆé«˜ç²¾åº¦ 3D ç›®æ ‡æ£€æµ‹ï¼šä¸€ç§è¾¹ç¼˜ä¾§æ··åˆä¸“å®¶è®¡ç®—ç³»ç»Ÿ",
      "authors": [
        "Linshen Liu",
        "Boyan Su",
        "Junyue Jiang",
        "Guanlin Wu",
        "Cong Guo",
        "Ceyu Xu",
        "Hao Frank Yang"
      ],
      "abstract": "This paper presents Edge-based Mixture of Experts (MoE) Collaborative Computing (EMC2), an optimal computing system designed for autonomous vehicles (AVs) that simultaneously achieves low-latency and high-accuracy 3D object detection. Unlike conventional approaches, EMC2 incorporates a scenario-aware MoE architecture specifically optimized for edge platforms. By effectively fusing LiDAR and camera data, the system leverages the complementary strengths of sparse 3D point clouds and dense 2D images to generate robust multimodal representations. To enable this, EMC2 employs an adaptive multimodal data bridge that performs multi-scale preprocessing on sensor inputs, followed by a scenario-aware routing mechanism that dynamically dispatches features to dedicated expert models based on object visibility and distance. In addition, EMC2 integrates joint hardware-software optimizations, including hardware resource utilization optimization and computational graph simplification, to ensure efficient and real-time inference on resource-constrained edge devices. Experiments on open-source benchmarks clearly show the EMC2 advancements as an end-to-end system. On the KITTI dataset, it achieves an average accuracy improvement of 3.58% and a 159.06% inference speedup compared to 15 baseline methods on Jetson platforms, with similar performance gains on the nuScenes dataset, highlighting its capability to advance reliable, real-time 3D object detection tasks for AVs. The official implementation is available at https://github.com/LinshenLiu622/EMC2.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Edge-based Mixture of Experts Collaborative Computing (EMC2)ï¼Œä¸€ç§ä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡çš„è¾¹ç¼˜ç«¯ä¸“å®¶æ··åˆ(Mixture of Experts)åä½œè®¡ç®—ç³»ç»Ÿï¼Œæ—¨åœ¨åŒæ—¶å®ç°ä½å»¶è¿Ÿå’Œé«˜ç²¾åº¦çš„3Dç‰©ä½“æ£€æµ‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡èåˆLiDARç‚¹äº‘å’Œç›¸æœºå›¾åƒæ•°æ®ï¼Œåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯çš„äº’è¡¥æ€§ç”Ÿæˆé²æ£’çš„è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”å¤šæ¨¡æ€æ•°æ®æ¡¥æ¥è¿›è¡Œå¤šå°ºåº¦é¢„å¤„ç†ã€‚EMC2 å¼•å…¥äº†åœºæ™¯æ„ŸçŸ¥è·¯ç”±æœºåˆ¶(Scenario-aware routing mechanism)ï¼Œèƒ½å¤Ÿæ ¹æ®ç‰©ä½“çš„å¯è§åº¦å’Œè·ç¦»åŠ¨æ€åœ°å°†ç‰¹å¾åˆ†é…ç»™ç‰¹å®šçš„ä¸“å®¶æ¨¡å‹ï¼Œä»è€Œä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé€šè¿‡ç¡¬ä»¶èµ„æºåˆ©ç”¨ä¼˜åŒ–å’Œè®¡ç®—å›¾ç®€åŒ–ç­‰è½¯ç¡¬ä»¶è”åˆä¼˜åŒ–æ‰‹æ®µï¼Œç¡®ä¿äº†åœ¨èµ„æºå—é™è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨KITTIæ•°æ®é›†å’ŒJetsonå¹³å°ä¸Šï¼ŒEMC2 çš„æ¨ç†é€Ÿåº¦æ¯”15ç§åŸºçº¿æ¨¡å‹æå‡äº†159.06%ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†3.58%ï¼Œåœ¨nuScenesæ•°æ®é›†ä¸Šçš„ä¼˜å¼‚è¡¨ç°è¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨æå‡è‡ªåŠ¨é©¾é©¶å®æ—¶æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04123v2",
      "published_date": "2025-07-05 18:28:04 UTC",
      "updated_date": "2025-07-22 01:29:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:45.974254+00:00"
    },
    {
      "arxiv_id": "2507.04119v1",
      "title": "When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need",
      "title_zh": "å½“æ— æ•°æ®çŸ¥è¯†è’¸é¦é‡ä¸Šä¸å¯è¿ç§»æ•™å¸ˆï¼šé€ƒç¦»åˆ†å¸ƒå¤–é™·é˜±è¶³çŸ£",
      "authors": [
        "Ziming Hong",
        "Runnan Chen",
        "Zengmao Wang",
        "Bo Han",
        "Bo Du",
        "Tongliang Liu"
      ],
      "abstract": "Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexplored. In this work, we conduct the first investigation into distilling non-transferable learning (NTL) teachers using DFKD, where the transferability from an ID domain to an out-of-distribution (OOD) domain is prohibited. We find that NTL teachers fool DFKD through divert the generator's attention from the useful ID knowledge to the misleading OOD knowledge. This hinders ID knowledge transfer but prioritizes OOD knowledge transfer. To mitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit DFKD by identifying and filtering out OOD-like synthetic samples. Specifically, inspired by the evidence that NTL teachers show stronger adversarial robustness on OOD samples than ID samples, we split synthetic samples into two groups according to their robustness. The fragile group is treated as ID-like data and used for normal knowledge distillation, while the robust group is seen as OOD-like data and utilized for forgetting OOD knowledge. Extensive experiments demonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers. Code is released at https://github.com/tmllab/2025_ICML_ATEsc.",
      "tldr_zh": "è¯¥ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†æ— æ•°æ®çŸ¥è¯†è’¸é¦(Data-free knowledge distillation, DFKD)åœ¨é¢å¯¹ä¸å¯è¿ç§»å­¦ä¹ (Non-transferable learning, NTL)æ•™å¸ˆæ¨¡å‹æ—¶çš„é²æ£’æ€§å’Œå®‰å…¨æ€§é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼ŒNTLæ•™å¸ˆæ¨¡å‹ä¼šé€šè¿‡å°†ç”Ÿæˆå™¨çš„æ³¨æ„åŠ›ä»æœ‰ç”¨çš„é¢†åŸŸå†…(In-distribution, ID)çŸ¥è¯†è½¬ç§»åˆ°è¯¯å¯¼æ€§çš„åˆ†å¸ƒå¤–(Out-of-distribution, OOD)çŸ¥è¯†ï¼Œä»è€Œæ•…æ„é˜»ç¢IDçŸ¥è¯†çš„æœ‰æ•ˆè¿ç§»ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¯¹æŠ—é™·é˜±é€ƒè„±(Adversarial Trap Escaping, ATEsc)æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è¯†åˆ«å¹¶è¿‡æ»¤ç±»ä¼¼OODçš„åˆæˆæ ·æœ¬æ¥ä¼˜åŒ–DFKDè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨NTLæ•™å¸ˆåœ¨OODæ ·æœ¬ä¸Šè¡¨ç°å‡ºæ¯”IDæ ·æœ¬æ›´å¼ºçš„å¯¹æŠ—é²æ£’æ€§è¿™ä¸€ç‰¹æ€§ï¼Œå°†åˆæˆæ ·æœ¬åˆ†ä¸ºä¸¤ç»„è¿›è¡Œå·®å¼‚åŒ–å¤„ç†ã€‚å…¶ä¸­ï¼Œè„†å¼±ç»„è¢«è§†ä¸ºç±»IDæ•°æ®ç”¨äºæ­£å¸¸çš„çŸ¥è¯†è’¸é¦ï¼Œè€Œé²æ£’ç»„åˆ™è¢«è§†ä¸ºç±»OODæ•°æ®å¹¶ç”¨äºæ‰§è¡Œé—å¿˜æœºåˆ¶ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒATEscåœ¨åº”å¯¹NTLæ•™å¸ˆæ¨¡å‹æ—¶èƒ½æ˜¾è‘—æå‡DFKDçš„è’¸é¦æ€§èƒ½å¹¶æœ‰æ•ˆé€ƒè„±OODé™·é˜±ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04119v1",
      "published_date": "2025-07-05 18:03:52 UTC",
      "updated_date": "2025-07-05 18:03:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:49.329655+00:00"
    },
    {
      "arxiv_id": "2507.04106v2",
      "title": "Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning",
      "title_zh": "åº”å¯¹æ— æ ·æœ¬æŒç»­å­¦ä¹ ä¸­å•ä»»åŠ¡æ•°æ®æŠ•æ¯’çš„ç ´åæ€§å½±å“",
      "authors": [
        "StanisÅ‚aw Pawlak",
        "BartÅ‚omiej Twardowski",
        "Tomasz TrzciÅ„ski",
        "Joost van de Weijer"
      ],
      "abstract": "Our research addresses the overlooked security concerns related to data poisoning in continual learning (CL). Data poisoning - the intentional manipulation of training data to affect the predictions of machine learning models - was recently shown to be a threat to CL training stability. While existing literature predominantly addresses scenario-dependent attacks, we propose to focus on a more simple and realistic single-task poison (STP) threats. In contrast to previously proposed poisoning settings, in STP adversaries lack knowledge and access to the model, as well as to both previous and future tasks. During an attack, they only have access to the current task within the data stream. Our study demonstrates that even within these stringent conditions, adversaries can compromise model performance using standard image corruptions. We show that STP attacks are able to strongly disrupt the whole continual training process: decreasing both the stability (its performance on past tasks) and plasticity (capacity to adapt to new tasks) of the algorithm. Finally, we propose a high-level defense framework for CL along with a poison task detection method based on task vectors. The code is available at https://github.com/stapaw/STP.git .",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æ— èŒƒä¾‹æŒç»­å­¦ä¹ ï¼ˆExemplar-Free Continual Learningï¼‰ä¸­é•¿æœŸè¢«å¿½è§†çš„æ•°æ®æŠ•æ¯’ï¼ˆData Poisoningï¼‰å®‰å…¨é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ›´å…·ç°å®æ„ä¹‰çš„å•ä»»åŠ¡æŠ•æ¯’ï¼ˆSingle-Task Poison, STPï¼‰å¨èƒæ¨¡å‹ã€‚åœ¨STPè®¾å®šä¸‹ï¼Œæ”»å‡»è€…æ— éœ€é¢„çŸ¥æ¨¡å‹ä¿¡æ¯æˆ–è®¿é—®å…¶ä»–ä»»åŠ¡ï¼Œä»…éœ€å¯¹å½“å‰ä»»åŠ¡æ•°æ®è¿›è¡Œæ ‡å‡†å›¾åƒæŸåå¤„ç†å³å¯æ˜¾è‘—ç ´åè®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ”»å‡»èƒ½åŒæ—¶é™ä½æ¨¡å‹çš„ç¨³å®šæ€§ï¼ˆStabilityï¼‰å’Œå¡‘æ€§ï¼ˆPlasticityï¼‰ï¼Œå¯¼è‡´æ•´ä¸ªæŒç»­å­¦ä¹ ç³»ç»Ÿçš„æ€§èƒ½å´©æºƒã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªé«˜å±‚çº§é˜²å¾¡æ¡†æ¶ï¼Œå¹¶è®¾è®¡äº†åŸºäºä»»åŠ¡å‘é‡ï¼ˆTask Vectorsï¼‰çš„æŠ•æ¯’æ£€æµ‹æ–¹æ³•ä»¥å¢å¼ºç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…æ­ç¤ºäº†æŒç»­å­¦ä¹ åœ¨å—é™æ”»å‡»æ¡ä»¶ä¸‹çš„è„†å¼±æ€§ï¼Œè¿˜é€šè¿‡å¼€æºä»£ç ä¸ºåç»­é²æ£’æ€§ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at CoLLAs 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04106v2",
      "published_date": "2025-07-05 17:26:52 UTC",
      "updated_date": "2025-08-10 07:06:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:51.839146+00:00"
    },
    {
      "arxiv_id": "2507.04105v1",
      "title": "Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing",
      "title_zh": "é€šè¿‡éšæœºå¹³æ»‘å¢å¼º LLM é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é²æ£’æ€§",
      "authors": [
        "Jinwei Hu",
        "Yi Dong",
        "Zhengtao Ding",
        "Xiaowei Huang"
      ],
      "abstract": "This paper presents a defense framework for enhancing the safety of large language model (LLM) empowered multi-agent systems (MAS) in safety-critical domains such as aerospace. We apply randomized smoothing, a statistical robustness certification technique, to the MAS consensus context, enabling probabilistic guarantees on agent decisions under adversarial influence. Unlike traditional verification methods, our approach operates in black-box settings and employs a two-stage adaptive sampling mechanism to balance robustness and computational efficiency. Simulation results demonstrate that our method effectively prevents the propagation of adversarial behaviors and hallucinations while maintaining consensus performance. This work provides a practical and scalable path toward safe deployment of LLM-based MAS in real-world, high-stakes environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èˆªç©ºèˆªå¤©ç­‰å®‰å…¨å…³é”®é¢†åŸŸï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)å®‰å…¨æ€§çš„é˜²å¾¡æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿå°†éšæœºå¹³æ»‘(Randomized Smoothing)è¿™ä¸€ç»Ÿè®¡é²æ£’æ€§è®¤è¯æŠ€æœ¯åº”ç”¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ä¸€è‡´æ€§(Consensus)åœºæ™¯ï¼Œä»è€Œåœ¨å¯¹æŠ—æ€§å¹²æ‰°ä¸‹ä¸ºæ™ºèƒ½ä½“å†³ç­–æä¾›æ¦‚ç‡æ€§ä¿è¯ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºé»‘ç›’(Black-box)ç¯å¢ƒï¼Œå¹¶å¼•å…¥ä¸¤é˜¶æ®µè‡ªé€‚åº”é‡‡æ ·æœºåˆ¶ä»¥å¹³è¡¡ç³»ç»Ÿçš„Robustnessä¸è®¡ç®—æ•ˆç‡ã€‚ä»¿çœŸç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç»´æŒå…±è¯†æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆæŠ‘åˆ¶å¯¹æŠ—æ€§è¡Œä¸ºå’Œå¹»è§‰(Hallucinations)çš„ä¼ æ’­ã€‚è¯¥å·¥ä½œä¸ºåœ¨çœŸå®é«˜é£é™©ä»»åŠ¡ä¸­å®‰å…¨éƒ¨ç½²åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†ä¸€æ¡å®ç”¨ä¸”å…·å¤‡å¯æ‰©å±•æ€§(Scalability)çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint accepted by Chinese Journal of Aeronautics",
      "pdf_url": "https://arxiv.org/pdf/2507.04105v1",
      "published_date": "2025-07-05 17:26:08 UTC",
      "updated_date": "2025-07-05 17:26:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:58.753126+00:00"
    },
    {
      "arxiv_id": "2507.04103v3",
      "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
      "title_zh": "å¦‚ä½•è®­ç»ƒä½ çš„ LLM Web æ™ºèƒ½ä½“ï¼šä¸€é¡¹ç»Ÿè®¡è¯Šæ–­ç ”ç©¶",
      "authors": [
        "Dheeraj Vattikonda",
        "Santhoshi Ravichandran",
        "Emiliano Penaloza",
        "Hadi Nekoei",
        "Megh Thakkar",
        "Thibault Le Sellier de Chezelles",
        "Nicolas Gontier",
        "Miguel MuÃ±oz-MÃ¡rmol",
        "Sahar Omidi Shayegan",
        "Stefania Raimondo",
        "Xue Liu",
        "Alexandre Drouin",
        "Laurent Charlin",
        "Alexandre PichÃ©",
        "Alexandre Lacoste",
        "Massimo Caccia"
      ],
      "abstract": "LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¼€æº LLM web agents åœ¨å¤šæ­¥äº¤äº’å¤æ‚æ€§å’Œ post-training è®¡ç®—æˆæœ¬æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæä¾›äº†é¦–ä¸ªå…³äºè®¡ç®—åˆ†é…çš„ç»Ÿè®¡å­¦è¯Šæ–­ç ”ç©¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæµæ°´çº¿ï¼Œé¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒ (SFT) è®© Llama 3.1 8B å­¦ç”Ÿæ¨¡å‹æ¨¡ä»¿ Llama 3.3 70B æ•™å¸ˆæ¨¡å‹ï¼Œéšåè¿›è¡Œ on-policy å¼ºåŒ–å­¦ä¹  (RL)ã€‚è€ƒè™‘åˆ°è®­ç»ƒè¿‡ç¨‹å¯¹è¶…å‚æ•°é€‰æ‹©é«˜åº¦æ•æ„Ÿï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹ 1,370 ç§é…ç½®è¿›è¡Œé‡‡æ ·å¹¶åˆ©ç”¨ bootstrapping æŠ€æœ¯ä¼°ç®—æœ‰æ•ˆè¶…å‚æ•°ï¼Œæœ‰æ•ˆé™ä½äº†è¯•é”™æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ WorkArena å’Œ MiniWob++ åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSFT ä¸ on-policy RL çš„ç»“åˆæ€§èƒ½å§‹ç»ˆä¼˜äºå•ä¸€æ–¹æ³•ã€‚è¯¥ç­–ç•¥åœ¨ MiniWob++ ä¸Šä»…éœ€ 55% çš„è®¡ç®—é‡å³å¯åŒ¹é…çº¯ SFT çš„å³°å€¼è¡¨ç°ï¼ŒæˆåŠŸæ¨è¿›äº†è®¡ç®—-æ€§èƒ½çš„å¸•ç´¯æ‰˜å‰æ²¿ (Pareto frontier)ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºå¼¥åˆå¼€æºä¸é—­æº web æ™ºèƒ½ä½“ä¹‹é—´çš„å·®è·æä¾›äº†å…³é”®çš„è®­ç»ƒç­–ç•¥å’Œç»Ÿè®¡ä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04103v3",
      "published_date": "2025-07-05 17:12:33 UTC",
      "updated_date": "2025-11-02 19:03:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:59.255171+00:00"
    },
    {
      "arxiv_id": "2507.04100v1",
      "title": "Hierarchical Testing with Rabbit Optimization for Industrial Cyber-Physical Systems",
      "title_zh": "é¢å‘å·¥ä¸šä¿¡æ¯ç‰©ç†ç³»ç»Ÿçš„å…”å­ä¼˜åŒ–åˆ†å±‚æµ‹è¯•",
      "authors": [
        "Jinwei Hu",
        "Zezhi Tang",
        "Xin Jin",
        "Benyuan Zhang",
        "Yi Dong",
        "Xiaowei Huang"
      ],
      "abstract": "This paper presents HERO (Hierarchical Testing with Rabbit Optimization), a novel black-box adversarial testing framework for evaluating the robustness of deep learning-based Prognostics and Health Management systems in Industrial Cyber-Physical Systems. Leveraging Artificial Rabbit Optimization, HERO generates physically constrained adversarial examples that align with real-world data distributions via global and local perspective. Its generalizability ensures applicability across diverse ICPS scenarios. This study specifically focuses on the Proton Exchange Membrane Fuel Cell system, chosen for its highly dynamic operational conditions, complex degradation mechanisms, and increasing integration into ICPS as a sustainable and efficient energy solution. Experimental results highlight HERO's ability to uncover vulnerabilities in even state-of-the-art PHM models, underscoring the critical need for enhanced robustness in real-world applications. By addressing these challenges, HERO demonstrates its potential to advance more resilient PHM systems across a wide range of ICPS domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HEROï¼ˆHierarchical Testing with Rabbit Optimizationï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å·¥ä¸šä¿¡æ¯ç‰©ç†ç³»ç»Ÿï¼ˆIndustrial Cyber-Physical Systems, ICPSï¼‰ä¸­æ·±åº¦å­¦ä¹ æ•…éšœé¢„æµ‹ä¸å¥åº·ç®¡ç†ï¼ˆPrognostics and Health Management, PHMï¼‰ç³»ç»Ÿçš„é»‘ç›’å¯¹æŠ—æ€§æµ‹è¯•æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äººå·¥å…”ä¼˜åŒ–ï¼ˆArtificial Rabbit Optimizationï¼‰ç®—æ³•ï¼Œä»å…¨å±€å’Œå±€éƒ¨è§†è§’ç”Ÿæˆç¬¦åˆç‰©ç†çº¦æŸä¸”è´´åˆçœŸå®æ•°æ®åˆ†å¸ƒçš„å¯¹æŠ—æ ·æœ¬ï¼Œå…·å¤‡æå¼ºçš„é€šç”¨æ€§ã€‚ç ”ç©¶ç‰¹åˆ«ä»¥è´¨å­äº¤æ¢è†œç‡ƒæ–™ç”µæ± ï¼ˆProton Exchange Membrane Fuel Cell, PEMFCï¼‰ç³»ç»Ÿä¸ºæ¡ˆä¾‹ï¼Œåˆ†æå…¶åœ¨åŠ¨æ€è¿è¡Œå’Œå¤æ‚è¡°å‡ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHEROèƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºç°æœ‰é¡¶å°–PHMæ¨¡å‹çš„é²æ£’æ€§æ¼æ´ï¼Œå¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨ä¸­å¢å¼ºç³»ç»Ÿé˜²å¾¡èƒ½åŠ›çš„ç´§è¿«æ€§ã€‚é€šè¿‡åº”å¯¹è¿™äº›å®‰å…¨æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶ä¸ºåœ¨å¹¿æ³›çš„ICPSé¢†åŸŸæ„å»ºæ›´å…·éŸ§æ€§çš„ç›‘æ§ä¸ç®¡ç†ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint accepted by IEEE Transactions on Industrial Cyber Physical Systems",
      "pdf_url": "https://arxiv.org/pdf/2507.04100v1",
      "published_date": "2025-07-05 16:59:57 UTC",
      "updated_date": "2025-07-05 16:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:02.213311+00:00"
    },
    {
      "arxiv_id": "2507.04099v2",
      "title": "Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching",
      "title_zh": "Conversation Forestsï¼šå¤šè½®åŒ»å­¦å¯¹è¯å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„å…³é”®åœ¨äºåˆ†æ”¯æ¶æ„",
      "authors": [
        "Thomas Savage"
      ],
      "abstract": "Fine-tuning methods such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) have demonstrated success in training large language models (LLMs) for single-turn tasks. However, these methods fall short in multi-turn applications, such as diagnostic patient interviewing, where understanding how early conversational turns influence downstream completions and outcomes is essential. In medicine, a multi-turn perspective is critical for learning diagnostic schemas and better understanding conversation dynamics. To address this gap, I introduce Savage Conversation Forests (SCF), a reinforcement learning framework that leverages a branched conversation architecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple possible conversation continuations at each turn, enabling the model to learn how different early responses affect downstream interactions and diagnostic outcomes. In experiments simulating doctor-patient conversations, SCF with branching outperforms linear conversation architectures on diagnostic accuracy. I hypothesize that SCF's improvements stem from its ability to provide richer, interdependent training signals across conversation turns. These results suggest that a branched training architecture is an important strategy for fine tuning LLMs in complex multi-turn conversational tasks.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹Direct Preference Optimization (DPO)å’ŒGroup Relative Policy Optimization (GRPO)ç­‰å¾®è°ƒæ–¹æ³•åœ¨å¤šè½®åŒ»ç–—å¯¹è¯ä¸­æ— æ³•æœ‰æ•ˆæ•æ‰æ—©æœŸå¯¹è¯å¯¹åç»­ç»“æœå½±å“çš„å±€é™æ€§ï¼Œæå‡ºäº†Savage Conversation Forests (SCF)æ¡†æ¶ã€‚SCFæ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„æ¶æ„ï¼Œé€šè¿‡åˆ†æ”¯å¯¹è¯è®¾è®¡åœ¨æ¯ä¸€è½®ç”Ÿæˆå¤šç§å¯èƒ½çš„åç»­èµ°å‘ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹(LLMs)èƒ½å¤Ÿå­¦ä¹ æ—©æœŸå“åº”å¯¹ä¸‹æ¸¸äº¤äº’åŠè¯Šæ–­ç»“æœçš„æ·±è¿œå½±å“ã€‚åœ¨åŒ»æ‚£å¯¹è¯æ¨¡æ‹Ÿå®éªŒä¸­ï¼ŒSCFçš„åˆ†æ”¯æ¶æ„åœ¨è¯Šæ–­å‡†ç¡®ç‡(diagnostic accuracy)ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„çº¿æ€§å¯¹è¯æ¶æ„ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œè¿™ç§åˆ†æ”¯è®­ç»ƒæ¶æ„èƒ½ä¸ºæ¨¡å‹æä¾›æ›´ä¸°å¯Œä¸”ç›¸äº’ä¾èµ–çš„è®­ç»ƒä¿¡å·ï¼Œæ˜¯æå‡å¤æ‚å¤šè½®å¯¹è¯ä»»åŠ¡å¾®è°ƒæ•ˆæœçš„å…³é”®ç­–ç•¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04099v2",
      "published_date": "2025-07-05 16:49:34 UTC",
      "updated_date": "2025-07-15 16:49:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:04.119127+00:00"
    },
    {
      "arxiv_id": "2507.04095v1",
      "title": "Human-centered AI with focus on Human-robot interaction (Book chapter)",
      "title_zh": "ä»¥äººä¸ºæœ¬çš„äººå·¥æ™ºèƒ½ï¼šèšç„¦äººæœºäº¤äº’ï¼ˆä¹¦ç±ç« èŠ‚ï¼‰",
      "authors": [
        "Alireza Mortezapour",
        "Giuliana Vitiello"
      ],
      "abstract": "Modern social robots can be considered the descendants of steam engines from the First Industrial Revolution (IR 1.0) and industrial robotic arms from the Third Industrial Revolution (IR 3.0). As some time has passed since the introduction of these robots during the Fourth Industrial Revolution (IR 4.0), challenges and issues in their interaction with humans have emerged, leading researchers to conclude that, like any other AI-based technology, these robots must also be human-centered to meet the needs of their users. This chapter aims to introduce humans and their needs in interactions with robots, ranging from short-term, one-on-one interactions (micro-level) to long-term, macro-level needs at the societal scale. Building upon the principles of human-centered AI, this chapter presents, for the first time, a new framework of human needs called the Dual Pyramid. This framework encompasses a comprehensive list of human needs in robot interactions, from the most fundamental, robot effectiveness to macro level requirements, such as the collaboration with robots in achieving the United Nations 17 Sustainable Development Goals.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç¬¬å››æ¬¡å·¥ä¸šé©å‘½(IR 4.0)èƒŒæ™¯ä¸‹ï¼Œç¤¾äº¤æœºå™¨äººåœ¨ä¸äººç±»äº¤äº’(Human-robot interaction)è¿‡ç¨‹ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒæœºå™¨äººæŠ€æœ¯å¿…é¡»éµå¾ªä»¥äººä¸ºæœ¬çš„äººå·¥æ™ºèƒ½(Human-centered AI)åŸåˆ™ä»¥æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚æœ¬ç« ç³»ç»Ÿåœ°ä»‹ç»äº†äººç±»åœ¨ä¸æœºå™¨äººäº’åŠ¨æ—¶çš„å¤šå±‚æ¬¡éœ€æ±‚ï¼ŒèŒƒå›´ä»çŸ­æœŸçš„ä¸€å¯¹ä¸€å¾®è§‚å±‚é¢(micro-level)äº¤äº’ï¼Œå»¶ä¼¸è‡³ç¤¾ä¼šå°ºåº¦çš„å®è§‚å±‚é¢(macro-level)éœ€æ±‚ã€‚åŸºäºè¿™äº›åŸåˆ™ï¼Œç ”ç©¶é¦–æ¬¡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œåŒé‡‘å­—å¡”â€(Dual Pyramid)çš„æ–°å‹äººç±»éœ€æ±‚æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢æ•æ‰æœºå™¨äººäº¤äº’ä¸­çš„æ ¸å¿ƒè¦ç´ ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†ä»æœ€åŸºç¡€çš„æœºå™¨äººæœ‰æ•ˆæ€§(robot effectiveness)åˆ°æ›´é«˜å±‚é¢çš„ç¤¾ä¼šè´£ä»»ï¼Œä¾‹å¦‚åä½œå®ç°è”åˆå›½17ä¸ªå¯æŒç»­å‘å±•ç›®æ ‡(Sustainable Development Goals)ã€‚é€šè¿‡æ•´åˆå¾®è§‚ä¸å®è§‚è§†è§’ï¼Œè¯¥ç ”ç©¶ä¸ºæœªæ¥è®¾è®¡æ›´å…·åŒ…å®¹æ€§ã€ç¬¦åˆäººç±»ä»·å€¼è§‚çš„æ™ºèƒ½æœºå™¨äººç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºæŒ‡å¯¼ä¸å®è·µæ¡†æ¶ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04095v1",
      "published_date": "2025-07-05 16:45:03 UTC",
      "updated_date": "2025-07-05 16:45:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:07.441758+00:00"
    },
    {
      "arxiv_id": "2507.04094v2",
      "title": "MMMOS: Multi-domain Multi-axis Audio Quality Assessment",
      "title_zh": "MMMOSï¼šå¤šé¢†åŸŸå¤šè½´éŸ³é¢‘è´¨é‡è¯„ä¼°",
      "authors": [
        "Yi-Cheng Lin",
        "Jia-Hung Chen",
        "Hung-yi Lee"
      ],
      "abstract": "Accurate audio quality estimation is essential for developing and evaluating audio generation, retrieval, and enhancement systems. Existing non-intrusive assessment models predict a single Mean Opinion Score (MOS) for speech, merging diverse perceptual factors and failing to generalize beyond speech. We propose MMMOS, a no-reference, multi-domain audio quality assessment system that estimates four orthogonal axes: Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness across speech, music, and environmental sounds. MMMOS fuses frame-level embeddings from three pretrained encoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with four loss functions. By ensembling the top eight models, MMMOS shows a 20-30% reduction in mean squared error and a 4-5% increase in Kendall's Ï„ versus baseline, gains first place in six of eight Production Complexity metrics, and ranks among the top three on 17 of 32 challenge metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MMMOSï¼Œä¸€ç§é’ˆå¯¹è¯­éŸ³ã€éŸ³ä¹å’Œç¯å¢ƒéŸ³çš„æ— å‚è€ƒã€å¤šé¢†åŸŸéŸ³é¢‘è´¨é‡è¯„ä¼°ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹ä»…é¢„æµ‹å•ä¸€Mean Opinion Score (MOS)ä¸”éš¾ä»¥æ¨å¹¿åˆ°éè¯­éŸ³é¢†åŸŸçš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿå®šä¹‰äº†Production Qualityã€Production Complexityã€Content Enjoymentå’ŒContent Usefulnesså››ä¸ªæ­£äº¤ç»´åº¦çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å®ç°æ›´å…¨é¢çš„æ„ŸçŸ¥å› ç´ è¡¡é‡ã€‚åœ¨æ–¹æ³•è®ºä¸Šï¼ŒMMMOSèåˆäº†æ¥è‡ªWavLMã€MuQå’ŒM2Dä¸‰ä¸ªé¢„è®­ç»ƒç¼–ç å™¨çš„å¸§çº§åµŒå…¥ç‰¹å¾ï¼Œå¹¶å¯¹ä¸‰ç§èšåˆç­–ç•¥åŠå››ç§æŸå¤±å‡½æ•°è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡é›†æˆè¡¨ç°æœ€ä¼˜çš„å…«ä¸ªæ¨¡å‹ï¼ŒMMMOSåœ¨Mean Squared Errorä¸Šæ¯”åŸºçº¿æ¨¡å‹é™ä½äº†20-30%ï¼Œä¸”Kendall's Ï„ç³»æ•°æå‡äº†4-5%ã€‚æ­¤å¤–ï¼ŒMMMOSåœ¨å¤šé¡¹æŒ‘æˆ˜èµ›æŒ‡æ ‡ä¸­ååˆ—å‰èŒ…ï¼Œè¯æ˜äº†å…¶åœ¨éŸ³é¢‘ç”Ÿæˆã€æ£€ç´¢åŠå¢å¼ºç³»ç»Ÿè¯„ä»·ä¸­çš„ä¼˜è¶Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "4 pages including 1 page of reference. Accepted by ASRU Audio MOS 2025 Challenge",
      "pdf_url": "https://arxiv.org/pdf/2507.04094v2",
      "published_date": "2025-07-05 16:42:09 UTC",
      "updated_date": "2026-01-10 08:04:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:10.581654+00:00"
    },
    {
      "arxiv_id": "2507.10563v1",
      "title": "A Biomimetic Way for Coral-Reef-Inspired Swarm Intelligence for Carbon-Neutral Wastewater Treatment",
      "title_zh": "é¢å‘ç¢³ä¸­å’Œæ±¡æ°´å¤„ç†çš„çŠç‘šç¤å¯å‘å¼ç¾¤ä½“æ™ºèƒ½ä»¿ç”Ÿæ–¹æ³•",
      "authors": [
        "Antonis Messinis"
      ],
      "abstract": "With increasing wastewater rates, achieving energy-neutral purification is challenging. We introduce a coral-reef-inspired Swarm Interaction Network for carbon-neutral wastewater treatment, combining morphogenetic abstraction with multi-task carbon awareness. Scalability stems from linear token complexity, mitigating the energy-removal problem. Compared with seven baselines, our approach achieves 96.7\\% removal efficiency, 0.31~kWh~m$^{-3}$ energy consumption, and 14.2~g~m$^{-3}$ CO$_2$ emissions. Variance analysis demonstrates robustness under sensor drift. Field scenarios--insular lagoons, brewery spikes, and desert greenhouses--show potential diesel savings of up to 22\\%. However, data-science staffing remains an impediment. Future work will integrate AutoML wrappers within the project scope, although governance restrictions pose interpretability challenges that require further visual analytics.",
      "tldr_zh": "è¯¥ç ”ç©¶å—çŠç‘šç¤å¯å‘æå‡ºäº†ä¸€ç§ Swarm Interaction Networkï¼Œæ—¨åœ¨å®ç°ç¢³ä¸­å’Œçš„æ±¡æ°´å¤„ç†ã€‚è¯¥æ–¹æ³•å°† morphogenetic abstraction ä¸ multi-task carbon awareness ç›¸ç»“åˆï¼Œåˆ©ç”¨ linear token complexity æå‡äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å¹¶ç¼“è§£äº†èƒ½æºç§»é™¤éš¾é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ä¸ƒç§åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆè¾¾åˆ°äº† 96.7% çš„å»é™¤æ•ˆç‡ï¼Œèƒ½è€—ä»…ä¸º 0.31 kWh mâ»Â³ï¼ŒCOâ‚‚ æ’æ”¾é‡é™è‡³ 14.2 g mâ»Â³ã€‚æ–¹å·®åˆ†æéªŒè¯äº†ç³»ç»Ÿåœ¨ sensor drift å¹²æ‰°ä¸‹çš„é²æ£’æ€§ï¼Œä¸”åœ¨æµ·å²›æ½Ÿæ¹–ã€å•¤é…’å‚å’Œæ²™æ¼ æ¸©å®¤ç­‰å®åœ°åœºæ™¯ä¸­è¡¨ç°å‡ºé«˜è¾¾ 22% çš„æŸ´æ²¹èŠ‚çœæ½œåŠ›ã€‚å°½ç®¡ç›®å‰ä»é¢ä¸´æ•°æ®ç§‘å­¦äººæ‰çŸ­ç¼ºå’Œ governance é™åˆ¶å¸¦æ¥çš„ interpretability æŒ‘æˆ˜ï¼Œä½†æœªæ¥é€šè¿‡é›†æˆ AutoML æŠ€æœ¯æœ‰æœ›è¿›ä¸€æ­¥ä¼˜åŒ–è¯¥ç³»ç»Ÿçš„åº”ç”¨ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.10563v1",
      "published_date": "2025-07-05 16:19:42 UTC",
      "updated_date": "2025-07-05 16:19:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:21.881556+00:00"
    },
    {
      "arxiv_id": "2507.04075v1",
      "title": "Accurate and Efficient World Modeling with Masked Latent Transformers",
      "title_zh": "åŸºäºæ©ç æ½œç©ºé—´ Transformer çš„ç²¾å‡†é«˜æ•ˆä¸–ç•Œå»ºæ¨¡",
      "authors": [
        "Maxime Burchi",
        "Radu Timofte"
      ],
      "abstract": "The Dreamer algorithm has recently obtained remarkable performance across diverse environment domains by training powerful agents with simulated trajectories. However, the compressed nature of its world model's latent space can result in the loss of crucial information, negatively affecting the agent's performance. Recent approaches, such as $Î”$-IRIS and DIAMOND, address this limitation by training more accurate world models. However, these methods require training agents directly from pixels, which reduces training efficiency and prevents the agent from benefiting from the inner representations learned by the world model. In this work, we propose an alternative approach to world modeling that is both accurate and efficient. We introduce EMERALD (Efficient MaskEd latent tRAnsformer worLD model), a world model using a spatial latent state with MaskGIT predictions to generate accurate trajectories in latent space and improve the agent performance. On the Crafter benchmark, EMERALD achieves new state-of-the-art performance, becoming the first method to surpass human experts performance within 10M environment steps. Our method also succeeds to unlock all 22 Crafter achievements at least once during evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Dreamer ç®—æ³•ä¸­ä¸–ç•Œæ¨¡å‹(world model)æ½œç©ºé—´(latent space)å‹ç¼©å¯¼è‡´çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ï¼Œä»¥åŠç°æœ‰æ–¹æ³• $\\Delta$-IRIS å’Œ DIAMOND åœ¨è®­ç»ƒæ•ˆç‡ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†åä¸º EMERALD (Efficient MaskEd latent tRAnsformer worLD model) çš„é«˜æ•ˆä¸–ç•Œæ¨¡å‹ã€‚è¯¥æ¶æ„é€šè¿‡å¼•å…¥ç©ºé—´æ½œçŠ¶æ€(spatial latent state)å¹¶ç»“åˆ MaskGIT é¢„æµ‹æŠ€æœ¯ï¼Œåœ¨æ½œç©ºé—´å†…ç”Ÿæˆç²¾ç¡®çš„è½¨è¿¹ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ¨¡å‹å­¦ä¹ åˆ°çš„å†…éƒ¨è¡¨ç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEMERALD åœ¨ Crafter åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„ state-of-the-art æ°´å¹³ï¼Œæ˜¯é¦–ä¸ªåœ¨ 10M environment steps å†…è¶…è¶Šäººç±»ä¸“å®¶è¡¨ç°çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è¯„ä¼°ä¸­æˆåŠŸè§£é”äº†å…¨éƒ¨ 22 é¡¹ Crafter æˆå°±ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç¯å¢ƒå»ºæ¨¡ä¸å†³ç­–ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½å’Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04075v1",
      "published_date": "2025-07-05 15:49:21 UTC",
      "updated_date": "2025-07-05 15:49:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:24.775441+00:00"
    },
    {
      "arxiv_id": "2507.04069v1",
      "title": "Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering",
      "title_zh": "è¶…è¶Šç‹¬ç«‹æ®µè½ï¼šé¢å‘æ£€ç´¢å¢å¼ºå¼€æ”¾åŸŸé—®ç­”çš„è‡ªé€‚åº”æ®µè½ç»„åˆæ£€ç´¢",
      "authors": [
        "Ting-Wen Ko",
        "Jyun-Yu Jiang",
        "Pu-Jen Cheng"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external documents at inference time, enabling up-to-date knowledge access without costly retraining. However, conventional RAG methods retrieve passages independently, often leading to redundant, noisy, or insufficiently diverse context-particularly problematic - particularly problematic in noisy corpora and for multi-hop questions. To address this, we propose Adaptive Passage Combination Retrieval (AdaPCR), a novel framework for open-domain question answering with black-box LMs. AdaPCR explicitly models dependencies between passages by considering passage combinations as units for retrieval and reranking. It consists of a context-aware query reformulation using concatenated passages, and a reranking step trained with a predictive objective aligned with downstream answer likelihood. Crucially, AdaPCR adaptively selects the number of retrieved passages without additional stopping modules. Experiments across several QA benchmarks show that AdaPCR outperforms baselines, particularly in multi-hop reasoning, demonstrating the effectiveness of modeling inter-passage dependencies for improved retrieval.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­ä¼ ç»Ÿæ–¹æ³•ç‹¬ç«‹æ£€ç´¢æ®µè½å¯¼è‡´å†—ä½™ã€å™ªå£°åŠå¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAdaptive Passage Combination Retrieval (AdaPCR)çš„æ–°å‹æ¡†æ¶ã€‚AdaPCRé€šè¿‡å°†æ®µè½ç»„åˆè§†ä¸ºæ£€ç´¢å’Œé‡æ’åºçš„å•å…ƒï¼Œæ˜¾å¼åœ°å»ºæ¨¡äº†æ®µè½ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è¯¥æ¡†æ¶åŒ…å«åˆ©ç”¨æ‹¼æ¥æ®µè½è¿›è¡Œçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸ¥è¯¢é‡æ„(context-aware query reformulation)ï¼Œä»¥åŠä¸€ä¸ªæ ¹æ®ä¸‹æ¸¸ç­”æ¡ˆå¯èƒ½æ€§è¿›è¡Œé¢„æµ‹ç›®æ ‡è®­ç»ƒçš„é‡æ’åºæ­¥éª¤ã€‚æ­¤å¤–ï¼ŒAdaPCRèƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©æ£€ç´¢æ®µè½çš„æ•°é‡ï¼Œè€Œæ— éœ€é¢å¤–çš„åœæ­¢æ¨¡å—ã€‚åœ¨å¤šä¸ªé—®ç­”(QA)åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaPCRåœ¨æ€§èƒ½ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè·³æ¨ç†(multi-hop reasoning)ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚è¿™ä¸€ç ”ç©¶ç»“æœå……åˆ†è¯æ˜äº†åœ¨å¼€æ”¾åŸŸé—®ç­”ä¸­å»ºæ¨¡æ®µè½é—´ä¾èµ–å…³ç³»å¯¹äºæå‡æ£€ç´¢è´¨é‡å’Œç³»ç»Ÿæ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04069v1",
      "published_date": "2025-07-05 15:10:12 UTC",
      "updated_date": "2025-07-05 15:10:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:26.229882+00:00"
    },
    {
      "arxiv_id": "2507.04067v1",
      "title": "HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration",
      "title_zh": "HAWKï¼šé¢å‘å¤šæ™ºèƒ½ä½“åä½œçš„åˆ†å±‚å·¥ä½œæµæ¡†æ¶",
      "authors": [
        "Yuyang Cheng",
        "Yumiao Xu",
        "Chaojia Yu",
        "Yong Zhao"
      ],
      "abstract": "Contemporary multi-agent systems encounter persistent challenges in cross-platform interoperability, dynamic task scheduling, and efficient resource sharing. Agents with heterogeneous implementations often lack standardized interfaces; collaboration frameworks remain brittle and hard to extend; scheduling policies are static; and inter-agent state synchronization is insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular framework comprising five layers-User, Workflow, Operator, Agent, and Resource-and supported by sixteen standardized interfaces. HAWK delivers an end-to-end pipeline covering task parsing, workflow orchestration, intelligent scheduling, resource invocation, and data synchronization. At its core lies an adaptive scheduling and optimization module in the Workflow Layer, which harnesses real-time feedback and dynamic strategy adjustment to maximize utilization. The Resource Layer provides a unified abstraction over heterogeneous data sources, large models, physical devices, and third-party services&tools, simplifying cross-domain information retrieval. We demonstrate HAWK's scalability and effectiveness via CreAgentive, a multi-agent novel-generation prototype, which achieves marked gains in throughput, lowers invocation complexity, and improves system controllability. We also show how hybrid deployments of large language models integrate seamlessly within HAWK, highlighting its flexibility. Finally, we outline future research avenues-hallucination mitigation, real-time performance tuning, and enhanced cross-domain adaptability-and survey prospective applications in healthcare, government, finance, and education.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HAWK (Hierarchical Agent Workflow)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è·¨å¹³å°äº’æ“ä½œæ€§ã€åŠ¨æ€ä»»åŠ¡è°ƒåº¦å’Œé«˜æ•ˆèµ„æºå…±äº«ç­‰æ–¹é¢æŒ‘æˆ˜çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±Userã€Workflowã€Operatorã€Agentå’ŒResourceäº”ä¸ªå±‚çº§ç»„æˆï¼Œå¹¶ç”±16ä¸ªæ ‡å‡†åŒ–æ¥å£æ”¯æ’‘ï¼Œå®ç°äº†ä»ä»»åŠ¡è§£æåˆ°æ•°æ®åŒæ­¥çš„ç«¯åˆ°ç«¯æµç¨‹ã€‚å…¶æ ¸å¿ƒåœ¨äºWorkflow Layerä¸­çš„è‡ªé€‚åº”è°ƒåº¦ä¸ä¼˜åŒ–æ¨¡å—ï¼Œé€šè¿‡å®æ—¶åé¦ˆå’ŒåŠ¨æ€ç­–ç•¥è°ƒæ•´æ¥æœ€å¤§åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚Resource Layeré€šè¿‡å¯¹å¼‚æ„æ•°æ®æºã€Large Language ModelsåŠç‰©ç†è®¾å¤‡è¿›è¡Œç»Ÿä¸€æŠ½è±¡ï¼Œç®€åŒ–äº†è·¨é¢†åŸŸä¿¡æ¯è°ƒç”¨çš„å¤æ‚æ€§ã€‚é€šè¿‡å¤šæ™ºèƒ½ä½“å°è¯´ç”ŸæˆåŸå‹CreAgentiveçš„éªŒè¯ï¼ŒHAWKè¯æ˜äº†å…¶åœ¨æå‡ååé‡(throughput)ã€é™ä½è°ƒç”¨éš¾åº¦åŠå¢å¼ºç³»ç»Ÿå¯æ§æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒå¤§è¯­è¨€æ¨¡å‹çš„æ··åˆéƒ¨ç½²ï¼Œå±•ç°äº†æé«˜çš„çµæ´»æ€§ä¸å¯æ‰©å±•æ€§ã€‚æœªæ¥ç ”ç©¶å°†èšç„¦äºå¹»è§‰ç¼“è§£(hallucination mitigation)ã€å®æ—¶æ€§èƒ½è°ƒä¼˜ä»¥åŠåœ¨åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸçš„åº”ç”¨æ¢ç´¢ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "AgentIR@SIGIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04067v1",
      "published_date": "2025-07-05 15:03:53 UTC",
      "updated_date": "2025-07-05 15:03:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:41.184934+00:00"
    },
    {
      "arxiv_id": "2507.04062v1",
      "title": "Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic",
      "title_zh": "èåˆåŠ¨ä½œè½¬æ¢ä¸åŠ¨ä½œç‰¹å¾è®°å¿†çš„éšæœºäººä½“åŠ¨ä½œé¢„æµ‹",
      "authors": [
        "Jianwei Tang",
        "Hong Yang",
        "Tengyue Chen",
        "Jian-Fang Hu"
      ],
      "abstract": "Action-driven stochastic human motion prediction aims to generate future motion sequences of a pre-defined target action based on given past observed sequences performing non-target actions. This task primarily presents two challenges. Firstly, generating smooth transition motions is hard due to the varying transition speeds of different actions. Secondly, the action characteristic is difficult to be learned because of the similarity of some actions. These issues cause the predicted results to be unreasonable and inconsistent. As a result, we propose two memory banks, the Soft-transition Action Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems above. The STAB stores the action transition information. It is equipped with the novel soft searching approach, which encourages the model to focus on multiple possible action categories of observed motions. The ACB records action characteristic, which produces more prior information for predicting certain actions. To fuse the features retrieved from the two banks better, we further propose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments on four motion prediction datasets demonstrate that our approach consistently outperforms the previous state-of-the-art. The demo and code are available at https://hyqlat.github.io/STABACB.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨ä½œé©±åŠ¨çš„éšæœºäººä½“è¿åŠ¨é¢„æµ‹(Stochastic Human Motion Prediction)ä»»åŠ¡ï¼Œæå‡ºäº†åŒ…å«Soft-transition Action Bank (STAB)å’ŒAction Characteristic Bank (ACB)çš„åŒå­˜å‚¨åº“æ¡†æ¶ã€‚ä¸ºäº†è§£å†³å› åŠ¨ä½œè½¬æ¢é€Ÿåº¦å¤šå˜è€Œéš¾ä»¥ç”Ÿæˆå¹³æ»‘è¿‡æ¸¡çš„é—®é¢˜ï¼ŒSTABé€šè¿‡å­˜å‚¨åŠ¨ä½œè½¬æ¢ä¿¡æ¯å¹¶é‡‡ç”¨æ–°å‹çš„è½¯æœç´¢(soft searching)æ–¹æ³•ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨è§‚å¯Ÿåºåˆ—ä¸­å¤šç§å¯èƒ½çš„åŠ¨ä½œç±»åˆ«ã€‚åŒæ—¶ï¼ŒACBé€šè¿‡è®°å½•åŠ¨ä½œç‰¹å¾(Action Characteristic)ä¸ºç‰¹å®šåŠ¨ä½œçš„é¢„æµ‹æä¾›æ›´ä¸°å¯Œçš„å…ˆéªŒä¿¡æ¯ï¼Œæœ‰æ•ˆåº”å¯¹äº†ç›¸ä¼¼åŠ¨ä½œéš¾ä»¥åŒºåˆ†çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†è‡ªé€‚åº”æ³¨æ„åŠ›è°ƒèŠ‚(Adaptive Attention Adjustment, AAA)ç­–ç•¥æ¥ä¼˜åŒ–ä¸¤ä¸ªå­˜å‚¨åº“ç‰¹å¾çš„èåˆè¿‡ç¨‹ã€‚åœ¨å››ä¸ªè¿åŠ¨é¢„æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆåˆç†ä¸”ä¸€è‡´çš„è¿åŠ¨åºåˆ—æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by CVPR2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04062v1",
      "published_date": "2025-07-05 14:57:37 UTC",
      "updated_date": "2025-07-05 14:57:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:35.141658+00:00"
    },
    {
      "arxiv_id": "2507.04060v1",
      "title": "Temporal Continual Learning with Prior Compensation for Human Motion Prediction",
      "title_zh": "ç»“åˆå…ˆéªŒè¡¥å¿çš„æ—¶åºæŒç»­å­¦ä¹ äººä½“è¿åŠ¨é¢„æµ‹",
      "authors": [
        "Jianwei Tang",
        "Jiangxin Sun",
        "Xiaotong Lin",
        "Lifang Zhang",
        "Wei-Shi Zheng",
        "Jian-Fang Hu"
      ],
      "abstract": "Human Motion Prediction (HMP) aims to predict future poses at different moments according to past motion sequences. Previous approaches have treated the prediction of various moments equally, resulting in two main limitations: the learning of short-term predictions is hindered by the focus on long-term predictions, and the incorporation of prior information from past predictions into subsequent predictions is limited. In this paper, we introduce a novel multi-stage training framework called Temporal Continual Learning (TCL) to address the above challenges. To better preserve prior information, we introduce the Prior Compensation Factor (PCF). We incorporate it into the model training to compensate for the lost prior information. Furthermore, we derive a more reasonable optimization objective through theoretical derivation. It is important to note that our TCL framework can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on four HMP benchmark datasets demonstrate the effectiveness and flexibility of TCL. The code is available at https://github.com/hyqlat/TCL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººä½“åŠ¨ä½œé¢„æµ‹(Human Motion Prediction)ä¸­ä¸åŒæ—¶åˆ»é¢„æµ‹æƒé‡å‡ç­‰å¯¼è‡´çŸ­æ—¶é¢„æµ‹å—é˜»åŠå…ˆéªŒä¿¡æ¯åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºæ—¶åºæŒç»­å­¦ä¹ (Temporal Continual Learning, TCL)çš„æ–°å‹å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚ä¸ºäº†æ›´å¥½åœ°ä¿ç•™å¹¶æ•´åˆè¿‡å¾€é¢„æµ‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†å…ˆéªŒè¡¥å¿å› å­(Prior Compensation Factor, PCF)å¹¶å°†å…¶çº³å…¥æ¨¡å‹è®­ç»ƒä»¥å¼¥è¡¥ä¿¡æ¯æŸå¤±ã€‚åŒæ—¶ï¼Œç ”ç©¶è€…é€šè¿‡ç†è®ºæ¨å¯¼å»ºç«‹äº†ä¸€ä¸ªæ›´åˆç†çš„ä¼˜åŒ–ç›®æ ‡ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸åŒé¢„æµ‹é˜¶æ®µçš„æ€§èƒ½ã€‚TCLæ¡†æ¶å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œçµæ´»æ€§ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å„ç§éª¨å¹²æ¨¡å‹ä¸­å¹¶é€‚åº”ä¸åŒçš„æ•°æ®é›†ã€‚åœ¨å››ä¸ªä¸»æµåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†TCLåœ¨æå‡åŠ¨ä½œé¢„æµ‹å‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ï¼Œä¸ºè§£å†³é•¿çŸ­æ—¶é¢„æµ‹å†²çªæä¾›äº†æ–°çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Advances in Neural Information Processing Systems 2023",
      "pdf_url": "https://arxiv.org/pdf/2507.04060v1",
      "published_date": "2025-07-05 14:48:30 UTC",
      "updated_date": "2025-07-05 14:48:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:55.637283+00:00"
    },
    {
      "arxiv_id": "2507.04059v1",
      "title": "Attributing Data for Sharpness-Aware Minimization",
      "title_zh": "é’ˆå¯¹é”åº¦æ„ŸçŸ¥æœ€å°åŒ–çš„æ•°æ®å½’å› ",
      "authors": [
        "Chenyang Ren",
        "Yifan Jia",
        "Huanyi Xie",
        "Zhaobin Xu",
        "Tianxing Wei",
        "Liangyu Wang",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Sharpness-aware Minimization (SAM) improves generalization in large-scale model training by linking loss landscape geometry to generalization. However, challenges such as mislabeled noisy data and privacy concerns have emerged as significant issues. Data attribution, which identifies the contributions of specific training samples, offers a promising solution. However, directly rendering existing data influence evaluation tools such as influence functions (IF) to SAM will be inapplicable or inaccurate as SAM utilizes an inner loop to find model perturbations that maximize loss, which the outer loop then minimizes, resulting in a doubled computational structure. Additionally, this bilevel structure complicates the modeling of data influence on the parameters. In this paper, based on the IF, we develop two innovative data valuation methods for SAM, each offering unique benefits in different scenarios: the Hessian-based IF and the Gradient Trajectory-based IF. The first one provides a comprehensive estimation of data influence using a closed-form measure that relies only on the trained model weights. In contrast, the other IF for SAM utilizes gradient trajectory information during training for more accurate and efficient data assessment. Extensive experiments demonstrate their effectiveness in data evaluation and parameter tuning, with applications in identifying mislabeled data, model editing, and enhancing interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Sharpness-aware Minimization (SAM) ç”±äºå…¶åŒå±‚ä¼˜åŒ–ï¼ˆbilevel structureï¼‰å¯¼è‡´ä¼ ç»Ÿæ•°æ®å½’å› ï¼ˆData attributionï¼‰æ–¹æ³•éš¾ä»¥é€‚ç”¨çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸¤ç§åŸºäºå½±å“å‡½æ•° (Influence Functions, IF) çš„åˆ›æ–°æ•°æ®ä¼°å€¼æ–¹æ³•ã€‚ä½œè€…å¼€å‘çš„ Hessian-based IF ä»…åˆ©ç”¨è®­ç»ƒåçš„æ¨¡å‹æƒé‡é€šè¿‡é—­å¼è§£ (closed-form measure) æä¾›å…¨é¢çš„æ•°æ®å½±å“è¯„ä¼°ï¼Œè€Œ Gradient Trajectory-based IF åˆ™ç»“åˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦è½¨è¿¹ä¿¡æ¯ä»¥æå‡è¯„ä¼°çš„å‡†ç¡®æ€§ä¸æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨è¯†åˆ«é”™è¯¯æ ‡æ³¨æ•°æ® (mislabeled data)ã€æ‰§è¡Œæ¨¡å‹ç¼–è¾‘ (model editing) åŠå¢å¼ºæ¨¡å‹å¯è§£é‡Šæ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥é¡¹å·¥ä½œæˆåŠŸè§£å†³äº†åœ¨ SAM æ¡†æ¶ä¸‹å»ºæ¨¡æ•°æ®å¯¹å‚æ•°å½±å“çš„éš¾é¢˜ï¼Œä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–å¤§è§„æ¨¡æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½å’Œå‚æ•°è°ƒä¼˜æä¾›äº†æœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.04059v1",
      "published_date": "2025-07-05 14:46:42 UTC",
      "updated_date": "2025-07-05 14:46:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:45.387343+00:00"
    },
    {
      "arxiv_id": "2507.04055v2",
      "title": "Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG",
      "title_zh": "LLMs ä¸ RAG æ—¶ä»£ä¸‹åŸºäºå­—ç¬¦ä¸²çš„æ¶æ„è½¯ä»¶å®¶æ—åˆ†ç±»çš„é‡æ–°å®¡è§†ä¸æ¢ç´¢",
      "authors": [
        "Yufan Chen",
        "Daoyuan Wu",
        "Juantao Zhong",
        "Zicheng Zhang",
        "Debin Gao",
        "Shuai Wang",
        "Yingjiu Li",
        "Ning Liu",
        "Jiachi Chen",
        "Rocky K. C. Chang"
      ],
      "abstract": "Malware family classification aims to identify the specific family (e.g., GuLoader or BitRAT) a malware sample may belong to, in contrast to malware detection or sample classification, which only predicts a Yes/No outcome. Accurate family identification can greatly facilitate automated sample labeling and understanding on crowdsourced malware analysis platforms such as VirusTotal and MalwareBazaar, which generate vast amounts of data daily. In this paper, we explore and assess the feasibility of using traditional binary string features for family classification in the new era of large language models (LLMs) and Retrieval-Augmented Generation (RAG). Specifically, we investigate howFamily-Specific String (FSS) features can be utilized in a manner similar to RAG to facilitate family classification. To this end, we develop a curated evaluation framework covering 4,347 samples from 67 malware families, extract and analyze over 25 million strings, and conduct detailed ablation studies to assess the impact of different design choices in four major modules, with each providing a relative improvement ranging from 8.1% to 120%.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æ—¶ä»£èƒŒæ™¯ä¸‹ï¼Œåˆ©ç”¨ä¼ ç»ŸäºŒè¿›åˆ¶å­—ç¬¦ä¸²ç‰¹å¾è¿›è¡Œæ¶æ„è½¯ä»¶å®¶æ—åˆ†ç±»çš„å¯è¡Œæ€§ã€‚ç ”ç©¶è€…é‡ç‚¹ç ”ç©¶äº†å¦‚ä½•ä»¥ç±»ä¼¼äº RAG çš„æ–¹å¼åˆ©ç”¨å®¶æ—ç‰¹å®šå­—ç¬¦ä¸² (Family-Specific String, FSS) ç‰¹å¾ï¼Œæ—¨åœ¨æå‡å¯¹æ¶æ„è½¯ä»¶ç‰¹å®šå®¶æ—ï¼ˆå¦‚ GuLoader æˆ– BitRATï¼‰çš„è¯†åˆ«ç²¾åº¦ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªæ¶µç›– 67 ä¸ªæ¶æ„è½¯ä»¶å®¶æ—ã€4,347 ä¸ªæ ·æœ¬çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶ä»è¶…è¿‡ 2,500 ä¸‡æ¡å­—ç¬¦ä¸²ä¸­æå–å¹¶åˆ†æäº†å…³é”®ç‰¹å¾ã€‚è¯¦ç»†çš„æ¶ˆèå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªä¸»è¦æ¨¡å—ä¸­çš„è®¾è®¡ä¼˜åŒ–å‡å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸å¯¹æ”¹è¿›å¹…åº¦ä»‹äº 8.1% åˆ° 120% ä¹‹é—´ã€‚è¿™ä¸€æˆæœä¸ä»…è¯æ˜äº†ä¼ ç»Ÿå­—ç¬¦ä¸²ç‰¹å¾åœ¨ç°ä»£ AI æ¶æ„ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸º VirusTotal ç­‰å¹³å°çš„æµ·é‡æ¶æ„è½¯ä»¶è‡ªåŠ¨åŒ–æ ‡æ³¨ä¸åˆ†ææä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "This is a technical report from Lingnan University, Hong Kong. Code is available at https://github.com/AIS2Lab/MalwareGPT",
      "pdf_url": "https://arxiv.org/pdf/2507.04055v2",
      "published_date": "2025-07-05 14:36:13 UTC",
      "updated_date": "2025-10-26 15:01:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:45.114004+00:00"
    },
    {
      "arxiv_id": "2507.04053v1",
      "title": "TopoMAS: Large Language Model Driven Topological Materials Multiagent System",
      "title_zh": "TopoMASï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ‹“æ‰‘ææ–™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Baohua Zhang",
        "Xin Li",
        "Huangchao Xu",
        "Zhong Jin",
        "Quansheng Wu",
        "Ce Li"
      ],
      "abstract": "Topological materials occupy a frontier in condensed-matter physics thanks to their remarkable electronic and quantum properties, yet their cross-scale design remains bottlenecked by inefficient discovery workflows. Here, we introduce TopoMAS (Topological materials Multi-Agent System), an interactive human-AI framework that seamlessly orchestrates the entire materials-discovery pipeline: from user-defined queries and multi-source data retrieval, through theoretical inference and crystal-structure generation, to first-principles validation. Crucially, TopoMAS closes the loop by autonomously integrating computational outcomes into a dynamic knowledge graph, enabling continuous knowledge refinement. In collaboration with human experts, it has already guided the identification of novel topological phases SrSbO3, confirmed by first-principles calculations. Comprehensive benchmarks demonstrate robust adaptability across base Large Language Model, with the lightweight Qwen2.5-72B model achieving 94.55% accuracy while consuming only 74.3-78.4% of tokens required by Qwen3-235B and 83.0% of DeepSeek-V3's usage--delivering responses twice as fast as Qwen3-235B. This efficiency establishes TopoMAS as an accelerator for computation-driven discovery pipelines. By harmonizing rational agent orchestration with a self-evolving knowledge graph, our framework not only delivers immediate advances in topological materials but also establishes a transferable, extensible paradigm for materials-science domain.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† TopoMAS (Topological materials Multi-Agent System)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§è¯­è¨€æ¨¡å‹ (Large Language Model) é©±åŠ¨çš„äº¤äº’å¼äººç±»-äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‹“æ‰‘ææ–™å‘ç°è¿‡ç¨‹ä¸­è·¨å°ºåº¦è®¾è®¡çš„æ•ˆç‡ç“¶é¢ˆã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ— ç¼ç¼–æ’ä»ç”¨æˆ·æŸ¥è¯¢ã€å¤šæºæ•°æ®æ£€ç´¢åˆ°ç†è®ºæ¨ç†ã€æ™¶ä½“ç»“æ„ç”Ÿæˆä»¥åŠç¬¬ä¸€æ€§åŸç†éªŒè¯ (first-principles validation) çš„å®Œæ•´ææ–™å‘ç°æµç¨‹ã€‚TopoMAS çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºé€šè¿‡è‡ªä¸»å°†è®¡ç®—ç»“æœæ•´åˆè¿›åŠ¨æ€çŸ¥è¯†å›¾è°± (dynamic knowledge graph) æ¥å®ç°é—­ç¯ç®¡ç†ï¼Œç¡®ä¿äº†çŸ¥è¯†çš„æŒç»­ä¼˜åŒ–ä¸ç²¾ç‚¼ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¯¥ç³»ç»Ÿå·²æˆåŠŸå¼•å¯¼ä¸“å®¶è¯†åˆ«å‡ºæ–°å‹æ‹“æ‰‘ç›¸ SrSbO3 å¹¶å¾—åˆ°è®¡ç®—ç¡®è®¤ã€‚åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè½»é‡çº§çš„ Qwen2.5-72B æ¨¡å‹åœ¨è¯¥æ¡†æ¶ä¸‹è¾¾åˆ°äº† 94.55% çš„å‡†ç¡®ç‡ï¼Œä¸”åœ¨ Token æ¶ˆè€—å’Œå“åº”é€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äº Qwen3-235B å’Œ DeepSeek-V3ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆç†çš„æ™ºèƒ½ä½“ç¼–æ’ (agent orchestration) ä¸è‡ªæˆ‘æ¼”å˜çŸ¥è¯†å›¾è°±çš„ç»“åˆï¼Œä¸ºææ–™ç§‘å­¦é¢†åŸŸæä¾›äº†ä¸€ä¸ªå¯è¿ç§»ä¸”å¯æ‰©å±•çš„è®¡ç®—é©±åŠ¨å‘ç°èŒƒå¼ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "13 pages,7 figures,3 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.04053v1",
      "published_date": "2025-07-05 14:23:12 UTC",
      "updated_date": "2025-07-05 14:23:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:47.044287+00:00"
    },
    {
      "arxiv_id": "2507.04050v1",
      "title": "Predictive Modeling of Effluent Temperature in SAT Systems Using Ambient Meteorological Data: Implications for Infiltration Management",
      "title_zh": "åŸºäºç¯å¢ƒæ°”è±¡æ•°æ®çš„ SAT ç³»ç»Ÿå‡ºæµæ¸©åº¦é¢„æµ‹å»ºæ¨¡ï¼šå¯¹æ¸—é€ç®¡ç†çš„å¯ç¤º",
      "authors": [
        "Roy Elkayam"
      ],
      "abstract": "Accurate prediction of effluent temperature in recharge basins is essential for optimizing the Soil Aquifer Treatment (SAT) process, as temperature directly influences water viscosity and infiltration rates. This study develops and evaluates predictive models for effluent temperature in the upper recharge layer of a Shafdan SAT system recharge basin using ambient meteorological data. Multiple linear regression (MLR), neural networks (NN), and random forests (RF) were tested for their predictive accuracy and interpretability. The MLR model, preferred for its operational simplicity and robust performance, achieved high predictive accuracy (R2 = 0.86-0.87) and was used to estimate effluent temperatures over a 10-year period. Results highlight pronounced seasonal temperature cycles and the importance of topsoil temperature in governing the thermal profile of the infiltrating effluent. The study provides practical equations for real-time monitoring and long-term planning of SAT operations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡é¢„æµ‹è¡¥ç»™æ± å‡ºæ°´æ¸©åº¦æ¥ä¼˜åŒ–åœŸå£¤å«æ°´å±‚å¤„ç†(Soil Aquifer Treatment, SAT)è¿‡ç¨‹ï¼Œå› ä¸ºæ¸©åº¦æ˜¯å½±å“æ°´ç²˜åº¦å’Œå…¥æ¸—ç‡çš„å…³é”®å› ç´ ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ç¯å¢ƒæ°”è±¡æ•°æ®ï¼Œé’ˆå¯¹Shafdan SATç³»ç»Ÿçš„ä¸Šå±‚è¡¥ç»™å±‚å¼€å‘å¹¶è¯„ä¼°äº†å¤šç§é¢„æµ‹æ¨¡å‹ã€‚åœ¨å¯¹æ¯”äº†å¤šå…ƒçº¿æ€§å›å½’(MLR)ã€ç¥ç»ç½‘ç»œ(NN)å’Œéšæœºæ£®æ—(RF)åï¼Œå‘ç°MLRæ¨¡å‹å‡­å€Ÿæ“ä½œç®€ä¾¿å’Œç¨³å¥çš„é¢„æµ‹æ€§èƒ½ï¼ˆR2 = 0.86-0.87ï¼‰è„±é¢–è€Œå‡ºã€‚é€šè¿‡å¯¹10å¹´æ•°æ®çš„ä¼°ç®—ï¼Œç ”ç©¶æ­ç¤ºäº†å‡ºæ°´æ¸©åº¦å­˜åœ¨æ˜æ˜¾çš„å­£èŠ‚æ€§å¾ªç¯ï¼Œå¹¶ç¡®è®¤äº†è¡¨å±‚åœŸå£¤æ¸©åº¦åœ¨æ§åˆ¶çƒ­å‰–é¢ä¸­çš„ä¸»å¯¼åœ°ä½ã€‚è¯¥ç ”ç©¶æœ€ç»ˆæä¾›äº†å¯ç”¨äºå®æ—¶ç›‘æµ‹å’Œé•¿æœŸè§„åˆ’çš„å®ç”¨æ–¹ç¨‹ï¼Œä¸ºæå‡SATç³»ç»Ÿçš„å…¥æ¸—ç®¡ç†æ°´å¹³æä¾›äº†ç§‘å­¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04050v1",
      "published_date": "2025-07-05 14:20:09 UTC",
      "updated_date": "2025-07-05 14:20:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:54.162106+00:00"
    },
    {
      "arxiv_id": "2507.04043v1",
      "title": "Evaluating the Effectiveness of Large Language Models in Solving Simple Programming Tasks: A User-Centered Study",
      "title_zh": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹è§£å†³ç®€å•ç¼–ç¨‹ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼šä¸€é¡¹ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„ç ”ç©¶",
      "authors": [
        "Kai Deng"
      ],
      "abstract": "As large language models (LLMs) become more common in educational tools and programming environments, questions arise about how these systems should interact with users. This study investigates how different interaction styles with ChatGPT-4o (passive, proactive, and collaborative) affect user performance on simple programming tasks. I conducted a within-subjects experiment where fifteen high school students participated, completing three problems under three distinct versions of the model. Each version was designed to represent a specific style of AI support: responding only when asked, offering suggestions automatically, or engaging the user in back-and-forth dialogue.Quantitative analysis revealed that the collaborative interaction style significantly improved task completion time compared to the passive and proactive conditions. Participants also reported higher satisfaction and perceived helpfulness when working with the collaborative version. These findings suggest that the way an LLM communicates, how it guides, prompts, and responds, can meaningfully impact learning and performance. This research highlights the importance of designing LLMs that go beyond functional correctness to support more interactive, adaptive, and user-centered experiences, especially for novice programmers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Large Language Models (LLMs) åœ¨ç¼–ç¨‹æ•™è‚²ä¸­çš„ä¸åŒäº¤äº’é£æ ¼å¯¹ç”¨æˆ·è¡¨ç°çš„å½±å“ã€‚é€šè¿‡å¯¹15åé«˜ä¸­ç”Ÿè¿›è¡Œå®éªŒï¼Œç ”ç©¶å¯¹æ¯”äº† ChatGPT-4o åœ¨è¢«åŠ¨(passive)ã€ä¸»åŠ¨(proactive)å’Œåä½œ(collaborative)ä¸‰ç§æ¨¡å¼ä¸‹å¤„ç†ç®€å•ç¼–ç¨‹ä»»åŠ¡çš„æ•ˆæœã€‚å®šé‡åˆ†æç»“æœè¡¨æ˜ï¼Œåä½œäº¤äº’é£æ ¼æ¯”è¢«åŠ¨æˆ–ä¸»åŠ¨æ¨¡å¼èƒ½æ˜¾è‘—ç¼©çŸ­ä»»åŠ¡å®Œæˆæ—¶é—´ï¼Œä¸”å‚ä¸è€…åé¦ˆå…¶å…·æœ‰æ›´é«˜çš„æ»¡æ„åº¦å’Œå®ç”¨æ€§ã€‚ç ”ç©¶å‘ç° LLM çš„å¼•å¯¼å’Œåé¦ˆæ–¹å¼èƒ½ç›´æ¥å½±å“å­¦ä¹ è€…çš„è¡¨ç°ï¼Œè€Œä¸ä»…ä»…å–å†³äºå…¶ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨ä¸ºåˆå­¦è€…è®¾è®¡ç¼–ç¨‹è¾…åŠ©å·¥å…·æ—¶ï¼Œå¼€å‘å…·å¤‡é«˜åº¦äº’åŠ¨æ€§å’Œä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒ(user-centered)çš„äº¤äº’ä½“éªŒçš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04043v1",
      "published_date": "2025-07-05 13:52:31 UTC",
      "updated_date": "2025-07-05 13:52:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:08.747641+00:00"
    },
    {
      "arxiv_id": "2507.04038v2",
      "title": "T-SYNTH: A Knowledge-Based Dataset of Synthetic Breast Images",
      "title_zh": "T-SYNTHï¼šåŸºäºçŸ¥è¯†çš„åˆæˆä¹³è…ºå›¾åƒæ•°æ®é›†",
      "authors": [
        "Christopher Wiedeman",
        "Anastasiia Sarmakeeva",
        "Elena Sizikova",
        "Daniil Filienko",
        "Miguel Lago",
        "Jana G. Delfino",
        "Aldo Badano"
      ],
      "abstract": "One of the key impediments for developing and assessing robust medical imaging algorithms is limited access to large-scale datasets with suitable annotations. Synthetic data generated with plausible physical and biological constraints may address some of these data limitations. We propose the use of physics simulations to generate synthetic images with pixel-level segmentation annotations, which are notoriously difficult to obtain. Specifically, we apply this approach to breast imaging analysis and release T-SYNTH, a large-scale open-source dataset of paired 2D digital mammography (DM) and 3D digital breast tomosynthesis (DBT) images. Our initial experimental results indicate that T-SYNTH images show promise for augmenting limited real patient datasets for detection tasks in DM and DBT. Our data and code are publicly available at https://github.com/DIDSR/tsynth-release.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å½±åƒç®—æ³•å¼€å‘ä¸­å¤§è§„æ¨¡å¸¦æ ‡æ³¨æ•°æ®é›†è·å–å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨å…·æœ‰ç‰©ç†å’Œç”Ÿç‰©å­¦çº¦æŸçš„ç‰©ç†ä»¿çœŸ(physics simulations)æŠ€æœ¯ç”Ÿæˆåˆæˆå›¾åƒã€‚ç ”ç©¶å‘å¸ƒäº†T-SYNTHï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¼€æºçš„é…å¯¹2Dæ•°å­—ä¹³è…ºXå°„çº¿æ‘„å½±(DM)å’Œ3Dæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆ(DBT)å›¾åƒæ•°æ®é›†ã€‚T-SYNTHçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºæä¾›äº†é€šè¿‡ç‰©ç†ä»¿çœŸç”Ÿæˆçš„åƒç´ çº§åˆ†å‰²æ ‡æ³¨(pixel-level segmentation annotations)ï¼Œè¿™ç±»æ ‡æ³¨åœ¨çœŸå®ä¸´åºŠæ•°æ®ä¸­é€šå¸¸éš¾ä»¥è·å–ã€‚åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼ŒT-SYNTHå›¾åƒåœ¨DMå’ŒDBTçš„æ£€æµ‹ä»»åŠ¡ä¸­å±•ç°å‡ºæ‰©å……æœ‰é™çœŸå®æ‚£è€…æ•°æ®é›†çš„å·¨å¤§æ½œåŠ›ã€‚ç›®å‰è¯¥ç ”ç©¶çš„æ•°æ®å’Œä»£ç å‡å·²å‘å…¬ä¼—å¼€æ”¾ï¼Œæ—¨åœ¨ä¸ºæ„å»ºå’Œè¯„ä¼°æ›´å…·é²æ£’æ€§çš„åŒ»ç–—å½±åƒç®—æ³•æä¾›å…³é”®çš„æ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) Open Data 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04038v2",
      "published_date": "2025-07-05 13:35:05 UTC",
      "updated_date": "2025-09-18 13:56:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:09.270646+00:00"
    },
    {
      "arxiv_id": "2507.04037v3",
      "title": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments",
      "title_zh": "Ready Jurist Oneï¼šé¢å‘åŠ¨æ€ç¯å¢ƒæ³•å¾‹æ™ºèƒ½çš„è¯­è¨€æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•",
      "authors": [
        "Zheng Jia",
        "Shengbin Yue",
        "Wei Chen",
        "Siyuan Wang",
        "Yidong Liu",
        "Yun Song",
        "Zhongyu Wei"
      ],
      "abstract": "The gap between static benchmarks and the dynamic nature of real-world legal practice poses a key barrier to advancing legal intelligence. To this end, we introduce J1-ENVS, the first interactive and dynamic legal environment tailored for LLM-based agents. Guided by legal experts, it comprises six representative scenarios from Chinese legal practices across three levels of environmental complexity. We further introduce J1-EVAL, a fine-grained evaluation framework, designed to assess both task performance and procedural compliance across varying levels of legal proficiency. Extensive experiments on 17 LLM agents reveal that, while many models demonstrate solid legal knowledge, they struggle with procedural execution in dynamic settings. Even the SOTA model, GPT-4o, falls short of 60% overall performance. These findings highlight persistent challenges in achieving dynamic legal intelligence and offer valuable insights to guide future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é™æ€åŸºå‡†æµ‹è¯•ä¸åŠ¨æ€ç°å®æ³•å¾‹å®è·µä¹‹é—´çš„å·®è·ï¼Œæ¨å‡ºäº†J1-ENVSï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“è®¾è®¡çš„äº¤äº’å¼åŠ¨æ€æ³•å¾‹ç¯å¢ƒã€‚åœ¨æ³•å¾‹ä¸“å®¶çš„æŒ‡å¯¼ä¸‹ï¼Œè¯¥ç¯å¢ƒæ¶µç›–äº†ä¸­å›½æ³•å¾‹å®è·µä¸­çš„å…­ä¸ªä»£è¡¨æ€§åœºæ™¯ï¼Œå¹¶æ ¹æ®ç¯å¢ƒå¤æ‚åº¦åˆ’åˆ†ä¸ºä¸‰ä¸ªå±‚çº§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æå‡ºäº†J1-EVALç»†ç²’åº¦è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ä»ä»»åŠ¡è¡¨ç°å’Œç¨‹åºåˆè§„æ€§ä¸¤ä¸ªç»´åº¦è¯„ä¼°æ™ºèƒ½ä½“çš„æ³•å¾‹èƒ½åŠ›ã€‚å¯¹17ç§LLMæ™ºèƒ½ä½“çš„å¤§è§„æ¨¡å®éªŒæ­ç¤ºï¼Œå°½ç®¡å¤šæ•°æ¨¡å‹å…·å¤‡æ‰å®çš„æ³•å¾‹çŸ¥è¯†ï¼Œä½†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„ç¨‹åºæ‰§è¡Œ(procedural execution)æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æ€§èƒ½é¡¶å°–çš„GPT-4oæ¨¡å‹åœ¨ç»¼åˆè¯„ä¼°ä¸­ä¹Ÿæœªèƒ½è¾¾åˆ°60%çš„å‡†ç¡®ç‡ã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒäº†å®ç°åŠ¨æ€æ³•å¾‹æ™ºèƒ½(dynamic legal intelligence)æ‰€é¢ä¸´çš„æŒç»­æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥æ³•å¾‹å¤§æ¨¡å‹çš„ç ”ç©¶æä¾›äº†é‡è¦åŸºå‡†å’Œè§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04037v3",
      "published_date": "2025-07-05 13:31:21 UTC",
      "updated_date": "2026-01-21 07:41:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:14.346596+00:00"
    },
    {
      "arxiv_id": "2507.05291v1",
      "title": "Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity",
      "title_zh": "è€ƒè™‘æœ‰é™åº”å˜è¶…å¼¹æ€§çš„ç‰©ç†ä¿¡æ¯å›¾ç¥ç»ç½‘ç»œå±€éƒ¨åœºé‡å»º",
      "authors": [
        "Manuel Ricardo Guevara Garban",
        "Yves Chemisky",
        "Ã‰tienne PruliÃ¨re",
        "MichaÃ«l ClÃ©ment"
      ],
      "abstract": "We propose a physics-informed machine learning framework called P-DivGNN to reconstruct local stress fields at the micro-scale, in the context of multi-scale simulation given a periodic micro-structure mesh and mean, macro-scale, stress values. This method is based in representing a periodic micro-structure as a graph, combined with a message passing graph neural network. We are able to retrieve local stress field distributions, providing average stress values produced by a mean field reduced order model (ROM) or Finite Element (FE) simulation at the macro-scale. The prediction of local stress fields are of utmost importance considering fracture analysis or the definition of local fatigue criteria. Our model incorporates physical constraints during training to constraint local stress field equilibrium state and employs a periodic graph representation to enforce periodic boundary conditions. The benefits of the proposed physics-informed GNN are evaluated considering linear and non linear hyperelastic responses applied to varying geometries. In the non-linear hyperelastic case, the proposed method achieves significant computational speed-ups compared to FE simulation, making it particularly attractive for large-scale applications.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†P-DivGNNï¼Œè¿™æ˜¯ä¸€ä¸ªç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ (Physics-Informed Machine Learning)æ¡†æ¶ï¼Œæ—¨åœ¨å¤šå°ºåº¦æ¨¡æ‹Ÿä¸­æ ¹æ®ç»™å®šçš„å‘¨æœŸæ€§å¾®è§‚ç»“æ„ç½‘æ ¼å’Œå®è§‚å¹³å‡åº”åŠ›å€¼æ¥é‡å»ºå¾®è§‚å°ºåº¦çš„å±€éƒ¨åº”åŠ›åœº(local stress fields)ã€‚è¯¥æ–¹æ³•å°†å‘¨æœŸæ€§å¾®è§‚ç»“æ„è¡¨ç¤ºä¸ºå›¾(graph)ï¼Œå¹¶ç»“åˆæ¶ˆæ¯ä¼ é€’å›¾ç¥ç»ç½‘ç»œ(message passing graph neural network)è¿›è¡Œå¤„ç†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ£€ç´¢å‡ºå¯¹æ–­è£‚åˆ†æ(fracture analysis)å’Œå±€éƒ¨ç–²åŠ³å‡†åˆ™(fatigue criteria)å®šä¹‰è‡³å…³é‡è¦çš„åº”åŠ›åœºåˆ†å¸ƒã€‚æ¨¡å‹åœ¨è®­ç»ƒä¸­é€šè¿‡å¼•å…¥ç‰©ç†çº¦æŸæ¥ä¿è¯å±€éƒ¨åº”åŠ›åœºçš„å¹³è¡¡æ€(equilibrium state)ï¼Œå¹¶åˆ©ç”¨å‘¨æœŸå›¾è¡¨ç¤ºæ¥å¼ºåˆ¶æ‰§è¡Œå‘¨æœŸæ€§è¾¹ç•Œæ¡ä»¶(periodic boundary conditions)ã€‚ç ”ç©¶è¯„ä¼°äº†è¯¥æ¡†æ¶åœ¨çº¿æ€§åŠéçº¿æ€§è¶…å¼¹æ€§(hyperelasticity)å“åº”ä¸‹çš„è¡¨ç°ï¼Œæ¶µç›–äº†å¤šç§å˜åŒ–çš„å‡ ä½•æ„å‹ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†éçº¿æ€§è¶…å¼¹æ€§é—®é¢˜æ—¶ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºä¼ ç»Ÿæœ‰é™å…ƒ(FE)æ¨¡æ‹Ÿå®ç°äº†æ˜¾è‘—çš„è®¡ç®—åŠ é€Ÿï¼Œä¸ºå¤§è§„æ¨¡å·¥ç¨‹åº”ç”¨æä¾›äº†é«˜æ•ˆä¸”ç²¾ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 17 figures, pre-print",
      "pdf_url": "https://arxiv.org/pdf/2507.05291v1",
      "published_date": "2025-07-05 13:11:31 UTC",
      "updated_date": "2025-07-05 13:11:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:16.374497+00:00"
    },
    {
      "arxiv_id": "2507.04034v1",
      "title": "Lyria: A General LLM-Driven Genetic Algorithm Framework for Problem Solving",
      "title_zh": "Lyriaï¼šä¸€ç§ç”¨äºé—®é¢˜æ±‚è§£çš„å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨é€šç”¨é—ä¼ ç®—æ³•æ¡†æ¶",
      "authors": [
        "Weizhi Tang",
        "Kwabena Nuamah",
        "Vaishak Belle"
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated impressive abilities across various domains, they still struggle with complex problems characterized by multi-objective optimization, precise constraint satisfaction, immense solution spaces, etc. To address the limitation, drawing on the superior semantic understanding ability of LLMs and also the outstanding global search and optimization capability of genetic algorithms, we propose to capitalize on their respective strengths and introduce Lyria, a general LLM-driven genetic algorithm framework, comprising 7 essential components. Through conducting extensive experiments with 4 LLMs across 3 types of problems, we demonstrated the efficacy of Lyria. Additionally, with 7 additional ablation experiments, we further systematically analyzed and elucidated the factors that affect its performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é¢å¯¹å¤šç›®æ ‡ä¼˜åŒ–ã€ç²¾ç¡®çº¦æŸæ»¡è¶³å’Œå·¨å¤§è§£ç©ºé—´ç­‰å¤æ‚é—®é¢˜æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº† Lyriaï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„ LLM-driven Genetic Algorithm æ¡†æ¶ã€‚Lyria å·§å¦™ç»“åˆäº† LLMs å“è¶Šçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ä¸ Genetic Algorithm å‡ºè‰²çš„å…¨å±€æœç´¢å’Œä¼˜åŒ–èƒ½åŠ›ï¼ŒåŒ…å«äº† 7 ä¸ªå…³é”®æ ¸å¿ƒç»„ä»¶ã€‚é€šè¿‡åœ¨ 3 ç§é—®é¢˜ç±»å‹ä¸Šå¯¹ 4 ç§ LLMs è¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œç»“æœå……åˆ†è¯æ˜äº† Lyria åœ¨è§£å†³å¤æ‚é—®é¢˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ 7 é¡¹é¢å¤–çš„æ¶ˆèå®éªŒ (Ablation Experiments) æ·±å…¥åˆ†æå¹¶é˜æ˜äº†å½±å“è¯¥æ¡†æ¶æ€§èƒ½çš„æ ¸å¿ƒå› ç´ ã€‚è¯¥æ¡†æ¶ä¸ºæå‡ LLMs å¤„ç†å¤æ‚å·¥ç¨‹å’Œæ•°å­¦ä¼˜åŒ–é—®é¢˜çš„èƒ½åŠ›æä¾›äº†æ–°çš„ç ”ç©¶èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04034v1",
      "published_date": "2025-07-05 13:04:36 UTC",
      "updated_date": "2025-07-05 13:04:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:23.050298+00:00"
    },
    {
      "arxiv_id": "2507.04014v1",
      "title": "Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition",
      "title_zh": "Nunchi-Benchï¼šèšç„¦éŸ©å›½è¿·ä¿¡çš„è¯­è¨€æ¨¡å‹æ–‡åŒ–æ¨ç†åŸºå‡†æµ‹è¯•",
      "authors": [
        "Kyuhee Kim",
        "Sangah Lee"
      ],
      "abstract": "As large language models (LLMs) become key advisors in various domains, their cultural sensitivity and reasoning skills are crucial in multicultural environments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs' cultural understanding, with a focus on Korean superstitions. The benchmark consists of 247 questions spanning 31 topics, assessing factual knowledge, culturally appropriate advice, and situational interpretation. We evaluate multilingual LLMs in both Korean and English to analyze their ability to reason about Korean cultural contexts and how language variations affect performance. To systematically assess cultural reasoning, we propose a novel evaluation strategy with customized scoring metrics that capture the extent to which models recognize cultural nuances and respond appropriately. Our findings highlight significant challenges in LLMs' cultural reasoning. While models generally recognize factual information, they struggle to apply it in practical scenarios. Furthermore, explicit cultural framing enhances performance more effectively than relying solely on the language of the prompt. To support further research, we publicly release Nunchi-Bench alongside a leaderboard.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Nunchi-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) æ–‡åŒ–æ¨ç† (cultural reasoning) èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶æ ¸å¿ƒå…³æ³¨ç‚¹åœ¨äºéŸ©å›½è¿·ä¿¡ (Korean superstitions)ã€‚è¯¥åŸºå‡†åŒ…å« 247 ä¸ªé—®é¢˜ï¼Œæ¶µç›– 31 ä¸ªä¸»é¢˜ï¼Œä»äº‹å®æ€§çŸ¥è¯†ã€æ–‡åŒ–å»ºè®®å’Œæƒ…å¢ƒè§£è¯»ä¸‰ä¸ªç»´åº¦å¯¹å¤šè¯­è¨€ LLMs åœ¨éŸ©è¯­å’Œè‹±è¯­ç¯å¢ƒä¸‹çš„è¡¨ç°è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§å…¨æ–°çš„è¯„ä¼°ç­–ç•¥ï¼Œåˆ©ç”¨å®šåˆ¶åŒ–è¯„åˆ†æŒ‡æ ‡ (customized scoring metrics) æ¥è¡¡é‡æ¨¡å‹è¯†åˆ«æ–‡åŒ–ç»†å¾®å·®åˆ« (cultural nuances) çš„å‡†ç¡®æ€§ã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡æ¨¡å‹åœ¨è¯†åˆ«æ–‡åŒ–äº‹å®æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å®é™…åœºæ™¯çš„åº”ç”¨æ¨ç†ä¸­ä»å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œæ˜¾å¼çš„æ–‡åŒ–æ¡†æ¶ (explicit cultural framing) ç›¸æ¯”å•çº¯æ”¹å˜æç¤ºè¯­è¨€èƒ½æ›´æœ‰æ•ˆåœ°å¢å¼ºæ¨¡å‹çš„è¡¨ç°ã€‚Nunchi-Bench åŠå…¶æ’è¡Œæ¦œçš„å…¬å¼€å‘å¸ƒï¼Œä¸ºåç»­æå‡äººå·¥æ™ºèƒ½çš„æ–‡åŒ–æ•æ„Ÿåº¦å’Œæ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦å·¥å…·ä¸å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04014v1",
      "published_date": "2025-07-05 11:52:09 UTC",
      "updated_date": "2025-07-05 11:52:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:28.556523+00:00"
    },
    {
      "arxiv_id": "2507.08012v1",
      "title": "RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning",
      "title_zh": "RepeaTTSï¼šé€šè¿‡é‡å¤å¾®è°ƒå®ç°ç‰¹å¾å‘ç°",
      "authors": [
        "Atli Sigurgeirsson",
        "Simon King"
      ],
      "abstract": "A Prompt-based Text-To-Speech model allows a user to control different aspects of speech, such as speaking rate and perceived gender, through natural language instruction. Although user-friendly, such approaches are on one hand constrained: control is limited to acoustic features exposed to the model during training, and too flexible on the other: the same inputs yields uncontrollable variation that are reflected in the corpus statistics.\n  We investigate a novel fine-tuning regime to address both of these issues at the same time by exploiting the uncontrollable variance of the model. Through principal component analysis of thousands of synthesised samples, we determine latent features that account for the highest proportion of the output variance and incorporate them as new labels for secondary fine-tuning. We evaluate the proposed methods on two models trained on an expressive Icelandic speech corpus, one with emotional disclosure and one without. In the case of the model without emotional disclosure, the method yields both continuous and discrete features that improve overall controllability of the model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæç¤º(Prompt-based)çš„æ–‡æœ¬è½¬è¯­éŸ³(Text-To-Speech)æ¨¡å‹åœ¨æ§åˆ¶ç‰¹å¾å—é™åŠå­˜åœ¨ä¸å¯æ§å˜åˆ†(uncontrollable variation)çš„é—®é¢˜ï¼Œæå‡ºäº†RepeaTTSæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ç§æ–°å‹çš„é‡å¤å¾®è°ƒ(Repeated Fine-Tuning)æœºåˆ¶ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„ä¸å¯æ§æ–¹å·®æ¥è¿›è¡Œç‰¹å¾å‘ç°ã€‚ç ”ç©¶äººå‘˜å¯¹æ•°åƒä¸ªåˆæˆæ ·æœ¬æ‰§è¡Œä¸»æˆåˆ†åˆ†æ(Principal Component Analysis, PCA)ä»¥ç¡®å®šå…³é”®çš„æ½œåœ¨ç‰¹å¾(latent features)ï¼Œå¹¶å°†å…¶ä½œä¸ºæ–°æ ‡ç­¾å¼•å…¥äºŒæ¬¡å¾®è°ƒè¿‡ç¨‹ã€‚å®éªŒåœ¨ä¸¤ä¸ªåŸºäºè¡¨è¾¾æ€§å†°å²›è¯­è¯­éŸ³è¯­æ–™åº“çš„æ¨¡å‹ä¸Šå±•å¼€ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæå–å‡ºæ”¹å–„æ¨¡å‹æ•´ä½“å¯æ§æ€§çš„è¿ç»­å’Œç¦»æ•£ç‰¹å¾ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†é€šè¿‡æŒ–æ˜æ¨¡å‹å†…éƒ¨æ–¹å·®æ¥è‡ªåŠ¨å‘ç°å¹¶é›†æˆæ–°çš„æ§åˆ¶ç»´åº¦ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºTTSç³»ç»Ÿå¯æ§æ€§çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.08012v1",
      "published_date": "2025-07-05 10:59:00 UTC",
      "updated_date": "2025-07-05 10:59:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:30.340649+00:00"
    },
    {
      "arxiv_id": "2507.04000v1",
      "title": "Leveraging Multimodal Data and Side Users for Diffusion Cross-Domain Recommendation",
      "title_zh": "åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ä¸ä¾§è¾¹ç”¨æˆ·çš„æ‰©æ•£è·¨åŸŸæ¨è",
      "authors": [
        "Fan Zhang",
        "Jinpeng Chen",
        "Huan Li",
        "Senzhang Wang",
        "Yuan Cao",
        "Kaimin Wei",
        "JianXiang He",
        "Feifei Kou",
        "Jinqing Wang"
      ],
      "abstract": "Cross-domain recommendation (CDR) aims to address the persistent cold-start problem in Recommender Systems. Current CDR research concentrates on transferring cold-start users' information from the auxiliary domain to the target domain. However, these systems face two main issues: the underutilization of multimodal data, which hinders effective cross-domain alignment, and the neglect of side users who interact solely within the target domain, leading to inadequate learning of the target domain's vector space distribution. To address these issues, we propose a model leveraging Multimodal data and Side users for diffusion Cross-domain recommendation (MuSiC). We first employ a multimodal large language model to extract item multimodal features and leverage a large language model to uncover user features using prompt learning without fine-tuning. Secondly, we propose the cross-domain diffusion module to learn the generation of feature vectors in the target domain. This approach involves learning feature distribution from side users and understanding the patterns in cross-domain transformation through overlapping users. Subsequently, the trained diffusion module is used to generate feature vectors for cold-start users in the target domain, enabling the completion of cross-domain recommendation tasks. Finally, our experimental evaluation of the Amazon dataset confirms that MuSiC achieves state-of-the-art performance, significantly outperforming all selected baselines. Our code is available: https://anonymous.4open.science/r/MuSiC-310A/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MuSiCæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è·¨åŸŸæ¨è(Cross-domain recommendation, CDR)ä¸­å­˜åœ¨çš„å†·å¯åŠ¨é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶å¯¹å¤šæ¨¡æ€æ•°æ®åˆ©ç”¨ä¸è¶³ä»¥åŠå¿½è§†ç›®æ ‡åŸŸä¾§å‘ç”¨æˆ·(Side users)å¯¼è‡´ç‰¹å¾ç©ºé—´åˆ†å¸ƒå­¦ä¹ ä¸å……åˆ†çš„é—®é¢˜ï¼ŒMuSiCç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)æå–ç‰©å“ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æç¤ºå­¦ä¹ (Prompt learning)æŒ–æ˜ç”¨æˆ·ç‰¹å¾ã€‚æ¨¡å‹åˆ›æ–°æ€§åœ°å¼•å…¥äº†è·¨åŸŸæ‰©æ•£æ¨¡å—(Cross-domain diffusion module)ï¼Œé€šè¿‡ä¾§å‘ç”¨æˆ·å­¦ä¹ ç›®æ ‡åŸŸç‰¹å¾åˆ†å¸ƒï¼Œå¹¶ç»“åˆé‡å ç”¨æˆ·å»ºæ¨¡è·¨åŸŸè½¬æ¢è§„å¾‹ã€‚è¯¥æ‰©æ•£æ¨¡å—èƒ½å¤Ÿä¸ºå†·å¯åŠ¨ç”¨æˆ·ç”Ÿæˆé«˜è´¨é‡çš„ç›®æ ‡åŸŸç‰¹å¾å‘é‡ï¼Œä»è€Œå®ç°ç²¾å‡†æ¨èã€‚åœ¨Amazonæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMuSiCæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œè¾¾åˆ°äº†å½“å‰çš„SOTAæ°´å¹³ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04000v1",
      "published_date": "2025-07-05 10:57:29 UTC",
      "updated_date": "2025-07-05 10:57:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:29.348467+00:00"
    },
    {
      "arxiv_id": "2507.03998v1",
      "title": "Toward Better Generalisation in Uncertainty Estimators: Leveraging Data-Agnostic Features",
      "title_zh": "æå‡ä¸ç¡®å®šæ€§è¯„ä¼°å™¨çš„æ³›åŒ–æ€§èƒ½ï¼šåˆ©ç”¨æ•°æ®æ— å…³ç‰¹å¾",
      "authors": [
        "Thuy An Ha",
        "Bao Quoc Vo"
      ],
      "abstract": "Large Language Models (LLMs) often generate responses that are factually incorrect yet expressed with high confidence, which can pose serious risks for end users. To address this, it is essential for LLMs not only to produce answers but also to provide accurate estimates of their correctness. Uncertainty quantification methods have been introduced to assess the quality of LLM outputs, with factual accuracy being a key aspect of that quality. Among these methods, those that leverage hidden states to train probes have shown particular promise, as these internal representations encode information relevant to the factuality of responses, making this approach the focus of this paper. However, the probe trained on the hidden states of one dataset often struggles to generalise to another dataset of a different task or domain. To address this limitation, we explore combining data-agnostic features with hidden-state features and assess whether this hybrid feature set enhances out-of-domain performance. We further examine whether selecting only the most informative hidden-state features, thereby discarding task-specific noise, enables the data-agnostic features to contribute more effectively. The experiment results indicate that although introducing data-agnostic features generally enhances generalisation performance in most cases, in certain scenarios their inclusion degrades performance. A similar pattern emerges when retaining only the most important hidden-state features - adding data-agnostic features does not consistently further enhance performance compared to using the full set of hidden-state features. A closer analysis reveals that, in some specific cases, the trained probe underweights the data-agnostic features relative to the hidden-state features, which we believe is the main reason why the results are inconclusive.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æé«˜å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸ç¡®å®šæ€§ä¼°è®¡å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥è§£å†³æ¨¡å‹è¾“å‡ºäº‹å®é”™è¯¯ä½†ç½®ä¿¡åº¦é«˜çš„é—®é¢˜ã€‚ç ”ç©¶é‡ç‚¹åœ¨äºåˆ©ç”¨éšè—çŠ¶æ€(hidden states)è®­ç»ƒæ¢æµ‹å™¨(probes)æ¥è¯„ä¼°äº‹å®å‡†ç¡®æ€§ï¼Œä½†å‘ç°è¿™ç±»æ¢æµ‹å™¨åœ¨è·¨é¢†åŸŸä»»åŠ¡ä¸­å¾€å¾€éš¾ä»¥æ³›åŒ–ã€‚ä¸ºäº†æ”¹å–„è¿™ä¸€å±€é™æ€§ï¼Œä½œè€…æå‡ºå°†æ•°æ®æ— å…³ç‰¹å¾(data-agnostic features)ä¸éšè—çŠ¶æ€ç‰¹å¾ç›¸ç»“åˆï¼Œå¹¶å°è¯•é€šè¿‡ç‰¹å¾é€‰æ‹©å‰”é™¤ä»»åŠ¡ç›¸å…³çš„å™ªå£°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å¼•å…¥æ•°æ®æ— å…³ç‰¹å¾åœ¨å¤šæ•°æƒ…å†µä¸‹èƒ½æå‡æ³›åŒ–æ€§èƒ½ï¼Œä½†åœ¨æŸäº›ç‰¹å®šåœºæ™¯ä¸‹åè€Œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿›ä¸€æ­¥åˆ†æå‘ç°ï¼Œæ¢æµ‹å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šä½ä¼°æ•°æ®æ— å…³ç‰¹å¾çš„æƒé‡ï¼Œè¿™è§£é‡Šäº†ä¸ºä½•è¯¥æ–¹æ³•åœ¨æå‡ä¸ç¡®å®šæ€§é‡åŒ–æ•ˆæœæ–¹é¢æœªèƒ½è¡¨ç°å‡ºä¸€è‡´çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03998v1",
      "published_date": "2025-07-05 10:55:36 UTC",
      "updated_date": "2025-07-05 10:55:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:32.572400+00:00"
    },
    {
      "arxiv_id": "2507.05288v1",
      "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­é’ˆå¯¹è™šå‡ä¿¡æ¯çš„ä¸»åŠ¨é˜²å¾¡ç­–ç•¥ç»¼è¿°",
      "authors": [
        "Shuliang Liu",
        "Hongyi Liu",
        "Aiwei Liu",
        "Bingchen Duan",
        "Qi Zheng",
        "Yibo Yan",
        "He Geng",
        "Peijie Jiang",
        "Jia Liu",
        "Xuming Hu"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) across critical domains has amplified the societal risks posed by algorithmically generated misinformation. Unlike traditional false content, LLM-generated misinformation can be self-reinforcing, highly plausible, and capable of rapid propagation across multiple languages, which traditional detection methods fail to mitigate effectively. This paper introduces a proactive defense paradigm, shifting from passive post hoc detection to anticipatory mitigation strategies. We propose a Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of training and deployed data; (2) Inference Reliability, embedding self-corrective mechanisms during reasoning; and (3) Input Robustness, enhancing the resilience of model interfaces against adversarial attacks. Through a comprehensive survey of existing techniques and a comparative meta-analysis, we demonstrate that proactive defense strategies offer up to 63\\% improvement over conventional methods in misinformation prevention, despite non-trivial computational overhead and generalization challenges. We argue that future research should focus on co-designing robust knowledge foundations, reasoning certification, and attack-resistant interfaces to ensure LLMs can effectively counter misinformation across varied domains.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­é’ˆå¯¹è¯¯å¯¼æ€§ä¿¡æ¯(Misinformation)çš„ä¸»åŠ¨é˜²å¾¡ç­–ç•¥ï¼Œæ—¨åœ¨åº”å¯¹ç®—æ³•ç”Ÿæˆå†…å®¹å¸¦æ¥çš„ç¤¾ä¼šé£é™©ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸»åŠ¨é˜²å¾¡èŒƒå¼(Proactive Defense Paradigm)ï¼Œå°†é‡å¿ƒä»è¢«åŠ¨çš„äº‹åæ£€æµ‹è½¬å‘é¢„å…ˆçš„ç¼“è§£ç­–ç•¥ï¼Œå¹¶æ„å»ºäº†åŒ…å«ä¸‰ä¸ªæ”¯æŸ±(Three Pillars)çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†é€šè¿‡åŠ å¼ºæ•°æ®å®Œæ•´æ€§å®ç°çš„çŸ¥è¯†å¯ä¿¡åº¦(Knowledge Credibility)ã€åœ¨æ¨ç†ä¸­åµŒå…¥è‡ªæˆ‘ä¿®æ­£æœºåˆ¶çš„æ¨ç†å¯é æ€§(Inference Reliability)ï¼Œä»¥åŠå¢å¼ºæ¥å£æŠ—æ”»å‡»èƒ½åŠ›çš„è¾“å…¥é²æ£’æ€§(Input Robustness)ã€‚ç»¼åˆè°ƒç ”ä¸å…ƒåˆ†æè¡¨æ˜ï¼Œä¸»åŠ¨é˜²å¾¡ç­–ç•¥åœ¨é¢„é˜²è¯¯å¯¼æ€§ä¿¡æ¯æ–¹é¢çš„è¡¨ç°æ¯”ä¼ ç»Ÿæ–¹æ³•æå‡äº†é«˜è¾¾63%ï¼Œå°½ç®¡ä»é¢ä¸´è®¡ç®—å¼€é”€å’Œæ³›åŒ–æŒ‘æˆ˜ã€‚ç ”ç©¶å¼ºè°ƒæœªæ¥åº”ä¾§é‡äºååŒè®¾è®¡ç¨³å¥çš„çŸ¥è¯†åŸºç¡€ã€æ¨ç†è®¤è¯åŠæŠ—æ”»å‡»æ¥å£ï¼Œä»¥ç¡®ä¿å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šé¢†åŸŸä¸­æœ‰æ•ˆæŠµå¾¡è¯¯å¯¼æ€§ä¿¡æ¯ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2507.05288v1",
      "published_date": "2025-07-05 09:52:21 UTC",
      "updated_date": "2025-07-05 09:52:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:49.467633+00:00"
    },
    {
      "arxiv_id": "2507.03971v1",
      "title": "Real-TabPFN: Improving Tabular Foundation Models via Continued Pre-training With Real-World Data",
      "title_zh": "Real-TabPFNï¼šåˆ©ç”¨çœŸå®ä¸–ç•Œæ•°æ®è¿›è¡ŒæŒç»­é¢„è®­ç»ƒä»¥æå‡è¡¨æ ¼åŸºç¡€æ¨¡å‹",
      "authors": [
        "Anurag Garg",
        "Muhammad Ali",
        "Noah Hollmann",
        "Lennart Purucker",
        "Samuel MÃ¼ller",
        "Frank Hutter"
      ],
      "abstract": "Foundation models for tabular data, like TabPFN, achieve strong performance on small datasets when pre-trained solely on synthetic data. We show that this performance can be significantly boosted by a targeted continued pre-training phase. Specifically, we demonstrate that leveraging a small, curated collection of large, real-world datasets for continued pre-training yields superior downstream predictive accuracy compared to using broader, potentially noisier corpora like CommonCrawl or GitTables. Our resulting model, Real-TabPFN, achieves substantial performance gains on 29 datasets from the OpenML AutoML Benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Real-TabPFNï¼Œé€šè¿‡åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œé’ˆå¯¹æ€§çš„æŒç»­é¢„è®­ç»ƒï¼ˆContinued Pre-trainingï¼‰æ¥æ˜¾è‘—æå‡è¡¨æ ¼åŸºç¡€æ¨¡å‹ï¼ˆTabular Foundation Modelsï¼‰çš„æ€§èƒ½ã€‚ç ”ç©¶äººå‘˜è¯å®ï¼Œç›¸è¾ƒäºåˆ©ç”¨ CommonCrawl æˆ– GitTables ç­‰å¹¿æ³›ä¸”åŒ…å«è¾ƒå¤šå™ªå£°çš„è¯­æ–™åº“ï¼Œä½¿ç”¨ç»è¿‡ç²¾é€‰çš„å°å‹ã€å¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®é›†è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œèƒ½ä¸ºæ¨¡å‹å¸¦æ¥æ›´ä¼˜çš„ä¸‹æ¸¸é¢„æµ‹å‡†ç¡®ç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReal-TabPFN åœ¨ OpenML AutoML Benchmark çš„ 29 ä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—äº†å®è´¨æ€§çš„æ€§èƒ½æå‡ã€‚è¯¥å·¥ä½œè¯æ˜äº†é€šè¿‡å¼•å…¥é«˜è´¨é‡çœŸå®æ•°æ®ï¼Œå¯ä»¥æœ‰æ•ˆçªç ´åŸæœ‰ TabPFN æ¨¡å‹ä»…ä¾èµ–åˆæˆæ•°æ®è¿›è¡Œé¢„è®­ç»ƒå¸¦æ¥çš„æ€§èƒ½å±€é™ï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„è¡¨æ ¼é¢†åŸŸåŸºç¡€æ¨¡å‹æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03971v1",
      "published_date": "2025-07-05 09:39:07 UTC",
      "updated_date": "2025-07-05 09:39:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:49.873184+00:00"
    },
    {
      "arxiv_id": "2507.03958v2",
      "title": "A Comparative Study of Specialized LLMs as Dense Retrievers",
      "title_zh": "ä¸“ä¸šåŒ–å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºç¨ å¯†æ£€ç´¢å™¨çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Hengran Zhang",
        "Keping Bi",
        "Jiafeng Guo"
      ],
      "abstract": "While large language models (LLMs) are increasingly deployed as dense retrievers, the impact of their domain-specific specialization on retrieval effectiveness remains underexplored. This investigation systematically examines how task-specific adaptations in LLMs influence their retrieval capabilities, an essential step toward developing unified retrievers capable of handling text, code, images, and multimodal content. We conduct extensive experiments with eight Qwen2.5 7B LLMs, including base, instruction-tuned, code/math-specialized, long reasoning, and vision-language models across zero-shot retrieval settings and the supervised setting. For the zero-shot retrieval settings, we consider text retrieval from the BEIR benchmark and code retrieval from the CoIR benchmark. Further, to evaluate supervised performance, all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical specialization and the long reasoning capability cause consistent degradation in three settings, indicating conflicts between mathematical reasoning and semantic matching. The vision-language model and code-specialized LLMs demonstrate superior zero-shot performance compared to other LLMs, even surpassing BM25 on the code retrieval task, and maintain comparable performance to base LLMs in supervised settings. These findings suggest promising directions for the unified retrieval task leveraging cross-domain and cross-modal fusion.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„é¢†åŸŸç‰¹å®šä¸“ä¸šåŒ–å¯¹ç¨ å¯†æ£€ç´¢(dense retrievers)æ•ˆæœçš„å½±å“ï¼Œé€šè¿‡å¯¹å…«ç§ä¸åŒç±»å‹çš„ Qwen2.5 7B æ¨¡å‹åœ¨ zero-shot å’Œç›‘ç£å­¦ä¹ è®¾ç½®ä¸‹è¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚å®éªŒåŸºå‡†æ¶µç›–äº† BEIR æ–‡æœ¬æ£€ç´¢ã€CoIR ä»£ç æ£€ç´¢ä»¥åŠåœ¨ MS MARCO æ•°æ®é›†ä¸Šçš„å¾®è°ƒè¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œæ•°å­¦ä¸“ä¸šåŒ–(mathematical specialization)å’Œé•¿æ–‡æœ¬æ¨ç†èƒ½åŠ›(long reasoning capability)åœ¨æ‰€æœ‰å®éªŒè®¾ç½®ä¸­å‡å¯¼è‡´æ£€ç´¢æ€§èƒ½æŒç»­ä¸‹é™ï¼Œè¡¨æ˜æ•°å­¦æ¨ç†ä¸è¯­ä¹‰åŒ¹é…(semantic matching)ä¹‹é—´å­˜åœ¨æ½œåœ¨å†²çªã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹(vision-language model)å’Œä»£ç ä¸“ç”¨ LLMs åœ¨ zero-shot æ£€ç´¢ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä»£ç æ£€ç´¢èƒ½åŠ›ç”šè‡³è¶…è¶Šäº† BM25 åŸºå‡†ã€‚åœ¨ç›‘ç£å¾®è°ƒåï¼Œè¿™äº›æ¨¡å‹ä»èƒ½ç»´æŒä¸åŸºç¡€æ¨¡å‹(base LLMs)ç›¸å½“çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœä¸ºåˆ©ç”¨è·¨é¢†åŸŸå’Œè·¨æ¨¡æ€èåˆ(cross-domain and cross-modal fusion)æ„å»ºç»Ÿä¸€æ£€ç´¢å™¨æä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by CCIR25 and published by Springer LNCS or LNAI",
      "pdf_url": "https://arxiv.org/pdf/2507.03958v2",
      "published_date": "2025-07-05 08:50:29 UTC",
      "updated_date": "2025-08-06 08:11:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:45.707485+00:00"
    },
    {
      "arxiv_id": "2507.03953v1",
      "title": "Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study",
      "title_zh": "æ‰©æ•£æ¨¡å‹ä¸ªæ€§åŒ–å¯¹æŠ—æ€§é˜²æŠ¤çš„ç»¼åˆè¯„ä¼°ç ”ç©¶",
      "authors": [
        "Kai Ye",
        "Tianyi Chen",
        "Zhen Wang"
      ],
      "abstract": "With the increasing adoption of diffusion models for image generation and personalization, concerns regarding privacy breaches and content misuse have become more pressing. In this study, we conduct a comprehensive comparison of eight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak, Mist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains. These methods are evaluated under varying perturbation budgets, using a range of metrics to assess visual imperceptibility and protective efficacy. Our results offer practical guidance for method selection. Code is available at: https://github.com/vkeilo/DiffAdvPerturbationBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œä¸ªæ€§åŒ–åº”ç”¨ä¸­æ—¥ç›Šå‡¸æ˜¾çš„éšç§æ³„éœ²ä¸å†…å®¹è¯¯ç”¨é—®é¢˜ï¼Œå¯¹ AdvDMã€ASPLã€FSGMã€MetaCloakã€Mistã€PhotoGuardã€SDS å’Œ SimAC è¿™å…«ç§åŸºäºæ‰°åŠ¨çš„ä¿æŠ¤æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒç ”ç©¶ã€‚è¯„ä¼°è¿‡ç¨‹æ¶µç›–äº†è‚–åƒå’Œè‰ºæœ¯å“ä¸¤ä¸ªä¸»è¦é¢†åŸŸï¼Œå¹¶ç»“åˆä¸åŒçš„æ‰°åŠ¨é¢„ç®—ï¼ˆperturbation budgetsï¼‰è¿›è¡Œäº†æ·±å…¥æµ‹è¯•ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨å¤šç§æŒ‡æ ‡æƒè¡¡äº†è¿™äº›æ–¹æ³•åœ¨è§†è§‰ä¸å¯æ„ŸçŸ¥æ€§ï¼ˆvisual imperceptibilityï¼‰ä¸ä¿æŠ¤æ•ˆåŠ›ï¼ˆprotective efficacyï¼‰ä¹‹é—´çš„è¡¨ç°ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„å®éªŒåˆ†æï¼Œè¯¥ç ”ç©¶ä¸ºä¸åŒåœºæ™¯ä¸‹é€‰æ‹©æœ€åˆé€‚çš„æ‰©æ•£æ¨¡å‹ä¸ªæ€§åŒ–ï¼ˆdiffusion personalizationï¼‰é˜²æŠ¤æ‰‹æ®µæä¾›äº†å®é™…çš„æŒ‡å¯¼æ–¹æ¡ˆã€‚æœ€åï¼Œè¯¥ç ”ç©¶å¼€æºäº†ç›¸å…³ä»£ç ï¼Œæ—¨åœ¨ä¸ºå¢å¼ºç”Ÿæˆå¼æ¨¡å‹çš„å®‰å…¨æ€§æä¾›æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the 2nd Workshop on Reliable and Responsible Foundation Models (R2-FM 2025) at ICML. 8 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.03953v1",
      "published_date": "2025-07-05 08:32:25 UTC",
      "updated_date": "2025-07-05 08:32:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:58.499330+00:00"
    },
    {
      "arxiv_id": "2507.03950v1",
      "title": "Optimizing Age of Trust and Throughput in Multi-Hop UAV-Aided IoT Networks",
      "title_zh": "å¤šè·³æ— äººæœºè¾…åŠ©ç‰©è”ç½‘ä¸­çš„ä¿¡ä»»é¾„ä¸ååé‡ä¼˜åŒ–",
      "authors": [
        "Yizhou Luo",
        "Kwan-Wu Chin",
        "Ruyi Guan",
        "Xi Xiao",
        "Caimeng Wang",
        "Jingyin Feng",
        "Tengjiao He"
      ],
      "abstract": "Devices operating in Internet of Things (IoT) networks may be deployed across vast geographical areas and interconnected via multi-hop communications. Further, they may be unguarded. This makes them vulnerable to attacks and motivates operators to check on devices frequently. To this end, we propose and study an Unmanned Aerial Vehicle (UAV)-aided attestation framework for use in IoT networks with a charging station powered by solar. A key challenge is optimizing the trajectory of the UAV to ensure it attests as many devices as possible. A trade-off here is that devices being checked by the UAV are offline, which affects the amount of data delivered to a gateway. Another challenge is that the charging station experiences time-varying energy arrivals, which in turn affect the flight duration and charging schedule of the UAV. To address these challenges, we employ a Deep Reinforcement Learning (DRL) solution to optimize the UAV's charging schedule and the selection of devices to be attested during each flight. The simulation results show that our solution reduces the average age of trust by 88% and throughput loss due to attestation by 30%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ†å¸ƒå¹¿æ³›ä¸”æ˜“å—æ”»å‡»çš„å¤šè·³ç‰©è”ç½‘(IoT)ç½‘ç»œï¼Œæå‡ºäº†ä¸€ä¸ªç”±å¤ªé˜³èƒ½å……ç”µç«™æ”¯æŒçš„æ— äººæœº(UAV)è¾…åŠ©è®¤è¯æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é¢‘ç¹æ£€æŸ¥æå‡è®¾å¤‡çš„å®‰å…¨æ€§ã€‚ä¼˜åŒ–çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºè§„åˆ’æ— äººæœºçš„é£è¡Œè½¨è¿¹ä»¥è¦†ç›–å°½å¯èƒ½å¤šçš„è®¾å¤‡ï¼ŒåŒæ—¶ç”±äºè®¤è¯è¿‡ç¨‹ä¸­è®¾å¤‡éœ€å¤„äºç¦»çº¿çŠ¶æ€ï¼Œå¿…é¡»å¹³è¡¡ä¿¡ä»»æ—¶é•¿(Age of Trust)ä¸ç½‘ç»œååé‡(Throughput)ä¹‹é—´çš„çŸ›ç›¾ã€‚æ­¤å¤–ï¼Œå……ç”µç«™éšæ—¶é—´å˜åŒ–çš„å¤ªé˜³èƒ½è·å–ä¹Ÿæ˜¾è‘—å½±å“äº†æ— äººæœºçš„é£è¡Œæ—¶é•¿å’Œå……ç”µè°ƒåº¦æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™äº›åŠ¨æ€è€¦åˆçš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning, DRL)ç®—æ³•æ¥ååŒä¼˜åŒ–æ— äººæœºçš„å……ç”µç­–ç•¥ä»¥åŠæ¯æ¬¡é£è¡Œä»»åŠ¡ä¸­å¾…è®¤è¯è®¾å¤‡çš„åŠ¨æ€é€‰æ‹©ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿå°†å¹³å‡ä¿¡ä»»æ—¶é•¿(Average Age of Trust)é™ä½88%ï¼Œå¹¶å°†å› è®¤è¯å¯¼è‡´çš„ååé‡æŸå¤±(Throughput Loss)å‡å°‘30%ï¼Œå®ç°äº†å®‰å…¨ç›‘æµ‹ä¸æ•°æ®ä¼ è¾“æ•ˆç‡çš„é«˜æ•ˆæŠ˜è¡·ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03950v1",
      "published_date": "2025-07-05 08:25:18 UTC",
      "updated_date": "2025-07-05 08:25:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:50.005552+00:00"
    },
    {
      "arxiv_id": "2507.03937v1",
      "title": "EdgeSRIE: A hybrid deep learning framework for real-time speckle reduction and image enhancement on portable ultrasound systems",
      "title_zh": "EdgeSRIEï¼šé¢å‘ä¾¿æºå¼è¶…å£°ç³»ç»Ÿå®æ—¶æ–‘ç‚¹æŠ‘åˆ¶ä¸å›¾åƒå¢å¼ºçš„æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Hyunwoo Cho",
        "Jongsoo Lee",
        "Jinbum Kang",
        "Yangmo Yoo"
      ],
      "abstract": "Speckle patterns in ultrasound images often obscure anatomical details, leading to diagnostic uncertainty. Recently, various deep learning (DL)-based techniques have been introduced to effectively suppress speckle; however, their high computational costs pose challenges for low-resource devices, such as portable ultrasound systems. To address this issue, EdgeSRIE, which is a lightweight hybrid DL framework for real-time speckle reduction and image enhancement in portable ultrasound imaging, is introduced. The proposed framework consists of two main branches: an unsupervised despeckling branch, which is trained by minimizing a loss function between speckled images, and a deblurring branch, which restores blurred images to sharp images. For hardware implementation, the trained network is quantized to 8-bit integer precision and deployed on a low-resource system-on-chip (SoC) with limited power consumption. In the performance evaluation with phantom and in vivo analyses, EdgeSRIE achieved the highest contrast-to-noise ratio (CNR) and average gradient magnitude (AGM) compared with the other baselines (different 2-rule-based methods and other 4-DL-based methods). Furthermore, EdgeSRIE enabled real-time inference at over 60 frames per second while satisfying computational requirements (< 20K parameters) on actual portable ultrasound hardware. These results demonstrated the feasibility of EdgeSRIE for real-time, high-quality ultrasound imaging in resource-limited environments.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ä¾¿æºå¼è¶…å£°ç³»ç»Ÿä¸­è¶…å£°å›¾åƒæ–‘ç‚¹å™ªå£°(Speckle patterns)é®ç›–è§£å‰–ç»†èŠ‚ä¸”æ·±åº¦å­¦ä¹ æ¨¡å‹è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºEdgeSRIEçš„è½»é‡åŒ–æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªä¸»è¦åˆ†æ”¯ç»„æˆï¼šä¸€ä¸ªæ˜¯é€šè¿‡æœ€å°åŒ–æ–‘ç‚¹å›¾åƒé—´æŸå¤±å‡½æ•°è®­ç»ƒçš„æ— ç›‘ç£å»å™ª(Unsupervised despeckling)åˆ†æ”¯ï¼Œä»¥åŠä¸€ä¸ªå°†æ¨¡ç³Šå›¾åƒæ¢å¤ä¸ºæ¸…æ™°å›¾åƒçš„å»æ¨¡ç³Š(Deblurring)åˆ†æ”¯ã€‚ä¸ºäº†æ»¡è¶³ç¡¬ä»¶éƒ¨ç½²éœ€æ±‚ï¼Œç ”ç©¶å›¢é˜Ÿå°†è®­ç»ƒå¥½çš„ç½‘ç»œé‡åŒ–ä¸º8ä½æ•´æ•°ç²¾åº¦(8-bit integer precision)ï¼Œå¹¶éƒ¨ç½²åœ¨åŠŸè€—å—é™çš„ä½èµ„æºç³»ç»Ÿçº§èŠ¯ç‰‡(SoC)ä¸Šã€‚å®éªŒåˆ†ææ˜¾ç¤ºï¼Œä¸å¤šç§åŸºäºè§„åˆ™å’Œæ·±åº¦å­¦ä¹ çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒEdgeSRIEåœ¨å¯¹æ¯”åº¦å™ªå£°æ¯”(CNR)å’Œå¹³å‡æ¢¯åº¦å¹…åº¦(AGM)ä¸Šå‡å–å¾—äº†æœ€ä¼˜æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒEdgeSRIEåœ¨å®é™…ç¡¬ä»¶ä¸Šå®ç°äº†æ¯ç§’è¶…è¿‡60å¸§çš„å®æ—¶æ¨ç†é€Ÿåº¦ï¼Œä¸”å‚æ•°é‡å°‘äº20Kã€‚è¯¥ç ”ç©¶ç»“æœè¯æ˜äº†EdgeSRIEåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å®ç°å®æ—¶ã€é«˜è´¨é‡è¶…å£°æˆåƒçš„æŠ€æœ¯å¯è¡Œæ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03937v1",
      "published_date": "2025-07-05 07:52:34 UTC",
      "updated_date": "2025-07-05 07:52:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:56.801349+00:00"
    },
    {
      "arxiv_id": "2507.12475v1",
      "title": "Coarse Addition and the St. Petersburg Paradox: A Heuristic Perspective",
      "title_zh": "ç²—ç³™åŠ æ³•ä¸ St. Petersburg æ‚–è®ºï¼šä¸€ç§å¯å‘å¼è§†è§’",
      "authors": [
        "Takashi Izumo"
      ],
      "abstract": "The St. Petersburg paradox presents a longstanding challenge in decision theory. It describes a game whose expected value is infinite, yet for which no rational finite stake can be determined. Traditional solutions introduce auxiliary assumptions, such as diminishing marginal utility, temporal discounting, or extended number systems. These methods often involve mathematical refinements that may not correspond to how people actually perceive or process numerical information. This paper explores an alternative approach based on a modified operation of addition defined over coarse partitions of the outcome space. In this model, exact numerical values are grouped into perceptual categories, and each value is replaced by a representative element of its group before being added. This method allows for a phenomenon where repeated additions eventually cease to affect the outcome, a behavior described as inertial stabilization. Although this is not intended as a definitive resolution of the paradox, the proposed framework offers a plausible way to represent how agents with limited cognitive precision might handle divergent reward structures. We demonstrate that the St. Petersburg series can become inert under this coarse addition for a suitably constructed partition. The approach may also have broader applications in behavioral modeling and the study of machine reasoning under perceptual limitations.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»å¯å‘å¼è§†è§’å‡ºå‘ï¼Œæ¢è®¨äº†å†³ç­–ç†è®ºä¸­åœ£å½¼å¾—å ¡æ‚–è®º(St. Petersburg paradox)çš„ç»å…¸æŒ‘æˆ˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç»“æœç©ºé—´ç²—åˆ†å—çš„æ”¹è¿›åŠ æ³•è¿ç®—ï¼Œå³ Coarse Additionï¼Œæ—¨åœ¨æ¨¡æ‹Ÿè®¤çŸ¥ç²¾åº¦æœ‰é™çš„æ™ºèƒ½ä½“å¦‚ä½•å¤„ç†æ•°å€¼ä¿¡æ¯ã€‚åœ¨è¯¥æ¨¡å‹ä¸­ï¼Œç²¾ç¡®æ•°å€¼è¢«å½’å…¥ç‰¹å®šçš„æ„ŸçŸ¥ç±»åˆ«ï¼Œå¹¶åœ¨ç›¸åŠ å‰è¢«æ›¿æ¢ä¸ºè¯¥ç±»åˆ«çš„ä»£è¡¨å…ƒç´ ã€‚è¿™ç§æ–¹æ³•å¼•å‘äº†æƒ¯æ€§ç¨³å®š(Inertial Stabilization)ç°è±¡ï¼Œå³é‡å¤åŠ æ³•åœ¨è¾¾åˆ°ä¸€å®šé˜ˆå€¼åä¸å†æ”¹å˜æœ€ç»ˆç»“æœã€‚ç ”ç©¶è¯æ˜ï¼Œé€šè¿‡æ„å»ºåˆé€‚çš„åˆ’åˆ†ï¼ŒåŸæœ¬å‘æ•£çš„åœ£å½¼å¾—å ¡çº§æ•°åœ¨ Coarse Addition æ¡†æ¶ä¸‹å¯ä»¥å˜å¾—ç¨³å®šã€‚è¯¥æ¡†æ¶ä¸ºç†è§£æœ‰é™è®¤çŸ¥ä¸‹çš„å¥–åŠ±ç»“æ„å¤„ç†æä¾›äº†æ–°é€”å¾„ï¼Œå¹¶åœ¨è¡Œä¸ºå»ºæ¨¡(Behavioral modeling)å’Œæ„ŸçŸ¥å—é™ä¸‹çš„æœºå™¨æ¨ç†(Machine reasoning)é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "econ.TH",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "econ.TH",
      "comment": "16 pages, no figure",
      "pdf_url": "https://arxiv.org/pdf/2507.12475v1",
      "published_date": "2025-07-05 07:34:46 UTC",
      "updated_date": "2025-07-05 07:34:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:25.206170+00:00"
    },
    {
      "arxiv_id": "2507.03929v2",
      "title": "An ASP-Based Framework for MUSes",
      "title_zh": "ä¸€ç§åŸºäº ASP çš„ MUSes æ¡†æ¶",
      "authors": [
        "Mohimenul Kabir",
        "Kuldeep S Meel"
      ],
      "abstract": "Given an unsatisfiable formula, understanding the core reason for unsatisfiability is crucial in several applications. One effective way to capture this is through the minimal unsatisfiable subset (MUS), the subset-minimal set of clauses that remains unsatisfiable. Current research broadly focuses on two directions: (i) enumerating as many MUSes as possible within a given time limit, and (ii) counting the total number of MUSes for a given unsatisfiable formula.\n  In this paper, we introduce an answer set programming-based framework, named MUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for its strengths in knowledge representation and is particularly suitable for specifying complex combinatorial problems. By translating MUS enumeration into answer set solving, MUS-ASP leverages the computational efficiency of state-of-the-art ASP systems. Our extensive experimental evaluation demonstrates the effectiveness of MUS-ASP and highlights the acceleration in both MUS enumeration and counting tasks, particularly when integrated within hybrid solvers, including the framework proposed in this paper.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯†åˆ«ä¸å¯æ»¡è¶³å…¬å¼ä¸­æœ€å°ä¸å¯æ»¡è¶³å­é›† (Minimal Unsatisfiable Subsets, MUSes) çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º MUS-ASP çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºç­”æ¡ˆé›†ç¼–ç¨‹ (Answer Set Programming, ASP)ï¼Œæ—¨åœ¨å®ç° MUSes çš„åœ¨çº¿æšä¸¾ï¼Œå¹¶åˆ©ç”¨ ASP å¼ºå¤§çš„çŸ¥è¯†è¡¨ç¤ºèƒ½åŠ›å’Œç°ä»£ç³»ç»Ÿçš„è®¡ç®—æ•ˆç‡æ¥å¤„ç†å¤æ‚çš„ç»„åˆé—®é¢˜ã€‚é€šè¿‡å°† MUS æšä¸¾ä»»åŠ¡è½¬åŒ–ä¸ºç­”æ¡ˆé›†æ±‚è§£è¿‡ç¨‹ï¼ŒMUS-ASP èƒ½å¤Ÿæœ‰æ•ˆåœ°åŠ é€Ÿ MUSes çš„å‘ç°ä¸è®¡æ•°ã€‚å®éªŒè¯„ä¼°è¯æ˜äº† MUS-ASP çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é›†æˆåˆ°æ··åˆæ±‚è§£å™¨ (hybrid solvers) ä¸­æ—¶ï¼Œè¯¥æ¡†æ¶åœ¨æšä¸¾å’Œè®¡æ•°ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2025, arXiv:2601.00047",
      "pdf_url": "https://arxiv.org/pdf/2507.03929v2",
      "published_date": "2025-07-05 07:23:24 UTC",
      "updated_date": "2026-01-07 12:03:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:03.230960+00:00"
    },
    {
      "arxiv_id": "2507.03928v1",
      "title": "CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate",
      "title_zh": "CortexDebateï¼šé¢å‘å¤šæ™ºèƒ½ä½“è¾©è®ºçš„ç¨€ç–ä¸å‡è¡¡è¾©è®ºæœºåˆ¶",
      "authors": [
        "Yiliu Sun",
        "Zicheng Zhao",
        "Sheng Wan",
        "Chen Gong"
      ],
      "abstract": "Nowadays, single Large Language Model (LLM) struggles with critical issues such as hallucination and inadequate reasoning abilities. To mitigate these issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where LLM agents engage in in-depth debates with others on tasks. However, existing MAD methods face two major issues: (a) too lengthy input contexts, which causes LLM agents to get lost in plenty of input information and experiences performance drop; and (b) the overconfidence dilemma, where self-assured LLM agents dominate the debate, leading to low debating effectiveness. To address these limitations, we propose a novel MAD method called \"CortexDebate\". Inspired by the human brain's tendency to establish a sparse and dynamically optimized network among cortical areas governed by white matter, CortexDebate constructs a sparse debating graph among LLM agents, where each LLM agent only debates with the ones that are helpful to it. To optimize the graph, we propose a module named McKinsey-based Debate Matter (MDM), which acts as an artificial analog to white matter. By integrating the McKinsey Trust Formula, a well-established measure of trustworthiness from sociology, MDM enables credible evaluations that guide graph optimization. The effectiveness of our CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“è¾©è®º (Multi-Agent Debate) ä¸­å­˜åœ¨çš„è¾“å…¥ä¸Šä¸‹æ–‡è¿‡é•¿ä»¥åŠæ™ºèƒ½ä½“è¿‡åº¦è‡ªä¿¡å¯¼è‡´è¾©è®ºæ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ï¼Œæå‡ºäº†åä¸º CortexDebate çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•å—åˆ°äººç±»å¤§è„‘ç™½è´¨è¿æ¥çš®å±‚åŒºåŸŸçš„ç¨€ç–åŠ¨æ€ä¼˜åŒ–ç½‘ç»œå¯å‘ï¼Œåœ¨ LLM æ™ºèƒ½ä½“ä¹‹é—´æ„å»ºç¨€ç–è¾©è®ºå›¾ï¼Œç¡®ä¿æ¯ä¸ªæ™ºèƒ½ä½“ä»…ä¸å¯¹å…¶æœ‰å®è´¨å¸®åŠ©çš„æˆå‘˜è¿›è¡Œäº¤äº’ã€‚ä¸ºäº†å®ç°å›¾ä¼˜åŒ–ï¼Œç ”ç©¶è®¾è®¡äº† McKinsey-based Debate Matter (MDM) æ¨¡å—ï¼Œé€šè¿‡é›†æˆç¤¾ä¼šå­¦ä¸­çš„éº¦è‚¯é”¡ä¿¡ä»»å…¬å¼ (McKinsey Trust Formula) è¿›è¡Œå¯ä¿¡åº¦è¯„ä¼°ã€‚è¿™ç§æœºåˆ¶æœ‰æ•ˆåœ°ç¼“è§£äº†ä¿¡æ¯è¿‡è½½å¹¶æ‰“ç ´äº†è¿‡åˆ†è‡ªä¿¡çš„åƒµå±€ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè¿›è¡Œæ›´å…·å»ºè®¾æ€§çš„åä½œã€‚å®éªŒç»“æœåœ¨æ¶‰åŠå››ç±»ä»»åŠ¡çš„å…«ä¸ªæ•°æ®é›†ä¸Šè¯æ˜äº† CortexDebate çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æå‡äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¨ç†è¡¨ç°å’Œè¾©è®ºè´¨é‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.03928v1",
      "published_date": "2025-07-05 07:23:15 UTC",
      "updated_date": "2025-07-05 07:23:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:08.903413+00:00"
    },
    {
      "arxiv_id": "2507.03923v2",
      "title": "Learning Disentangled Stain and Structural Representations for Semi-Supervised Histopathology Segmentation",
      "title_zh": "é¢å‘åŠç›‘ç£ç»„ç»‡ç—…ç†å­¦åˆ†å‰²çš„æŸ“è‰²ä¸ç»“æ„è§£è€¦è¡¨å¾å­¦ä¹ ",
      "authors": [
        "Ha-Hieu Pham",
        "Nguyen Lan Vi Vu",
        "Thanh-Huy Nguyen",
        "Ulas Bagci",
        "Min Xu",
        "Trung-Nghia Le",
        "Huy-Hieu Pham"
      ],
      "abstract": "Accurate gland segmentation in histopathology images is essential for cancer diagnosis and prognosis. However, significant variability in Hematoxylin and Eosin (H&E) staining and tissue morphology, combined with limited annotated data, poses major challenges for automated segmentation. To address this, we propose Color-Structure Dual-Student (CSDS), a novel semi-supervised segmentation framework designed to learn disentangled representations of stain appearance and tissue structure. CSDS comprises two specialized student networks: one trained on stain-augmented inputs to model chromatic variation, and the other on structure-augmented inputs to capture morphological cues. A shared teacher network, updated via Exponential Moving Average (EMA), supervises both students through pseudo-labels. To further improve label reliability, we introduce stain-aware and structure-aware uncertainty estimation modules that adaptively modulate the contribution of each student during training. Experiments on the GlaS and CRAG datasets show that CSDS achieves state-of-the-art performance in low-label settings, with Dice score improvements of up to 1.2% on GlaS and 0.7% on CRAG at 5% labeled data, and 0.7% and 1.4% at 10%. Our code and pre-trained models are available at https://github.com/hieuphamha19/CSDS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Color-Structure Dual-Student (CSDS) çš„æ–°å‹åŠç›‘ç£åˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­å›  H&E æŸ“è‰²å’Œç»„ç»‡å½¢æ€å˜å¼‚ä»¥åŠæ ‡æ³¨æ•°æ®æœ‰é™å¯¼è‡´çš„è…ºä½“åˆ†å‰²éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ è§£è€¦çš„æŸ“è‰²å¤–è§‚(stain appearance)å’Œç»„ç»‡ç»“æ„(tissue structure)è¡¨ç¤ºï¼Œåˆ©ç”¨ä¸¤ä¸ªåˆ†åˆ«ç»è¿‡æŸ“è‰²å¢å¼º(stain-augmented)å’Œç»“æ„å¢å¼º(structure-augmented)è®­ç»ƒçš„å­¦ç”Ÿç½‘ç»œæ¥æ•æ‰è‰²å½©å’Œå½¢æ€ç‰¹å¾ã€‚ä¸€ä¸ªé€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡(EMA)æ›´æ–°çš„å…±äº«æ•™å¸ˆç½‘ç»œé€šè¿‡ä¼ªæ ‡ç­¾ç›‘ç£å­¦ç”Ÿï¼Œå¹¶ç»“åˆæ–°å¼•å…¥çš„æŸ“è‰²æ„ŸçŸ¥ä¸ç»“æ„æ„ŸçŸ¥ä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å—ï¼Œåœ¨è®­ç»ƒæœŸé—´è‡ªé€‚åº”åœ°è°ƒèŠ‚æ¯ä¸ªå­¦ç”Ÿçš„è´¡çŒ®ã€‚åœ¨ GlaS å’Œ CRAG æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCSDS åœ¨ä½æ ‡ç­¾æ¯”ä¾‹è®¾ç½®ä¸‹è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œåœ¨ä»…æœ‰ 5% æ ‡æ³¨æ•°æ®æ—¶ Dice score æå‡é«˜è¾¾ 1.2%ã€‚è¯¥ç ”ç©¶ä¸ºç—…ç†å›¾åƒåœ¨æ ‡æ³¨ç¨€ç¼ºæƒ…å†µä¸‹çš„ç²¾ç¡®åˆ†å‰²æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted to Compayl @ MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.03923v2",
      "published_date": "2025-07-05 07:09:46 UTC",
      "updated_date": "2025-08-03 04:09:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:10.241241+00:00"
    },
    {
      "arxiv_id": "2507.03916v3",
      "title": "Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models",
      "title_zh": "åŠ¨ç”»äº¦éœ€å…³æ³¨ï¼šä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¹»ç¯ç‰‡åŠ¨ç”»ç†è§£å…¨æ–¹ä½æ–¹æ³•",
      "authors": [
        "Yifan Jiang",
        "Yibo Xue",
        "Yukun Kang",
        "Pin Zheng",
        "Jian Peng",
        "Feiran Wu",
        "Changliang Xu"
      ],
      "abstract": "Slide animations, such as fade-in, fly-in, and wipe, are critical for audience engagement, efficient information delivery, and vivid visual expression. However, most AI-driven slide-generation tools still lack native animation support, and existing vision-language models (VLMs) struggle with animation tasks due to the absence of public datasets and limited temporal-reasoning capabilities. To address this gap, we release the first public dataset for slide-animation modeling: 12,000 triplets of natural-language descriptions, animation JSON files, and rendered videos, collectively covering every built-in PowerPoint effect. Using this resource, we fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our Coverage-Order-Detail Assessment (CODA) metric, which evaluates action coverage, temporal order, and detail fidelity. On a manually created test set of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and shows significant improvements in CODA-detail. This demonstrates that low-rank adaptation enables reliable temporal reasoning and generalization beyond synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric provide a rigorous benchmark and foundation for future research on VLM-based dynamic slide generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¹»ç¯ç‰‡ç”Ÿæˆå·¥å…·ç¼ºä¹åŠ¨ç”»æ”¯æŒä»¥åŠè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨æ—¶åºæ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå‘å¸ƒäº†é¦–ä¸ªå…¬å¼€çš„å¹»ç¯ç‰‡åŠ¨ç”»æ•°æ®é›†ï¼ŒåŒ…å«12,000ç»„è‡ªç„¶è¯­è¨€æè¿°ã€åŠ¨ç”»JSONæ–‡ä»¶åŠæ¸²æŸ“è§†é¢‘ï¼Œæ¶µç›–äº†æ‰€æœ‰PowerPointå†…ç½®æ•ˆæœã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä½ç§©è‡ªé€‚åº”(Low-Rank Adaptation, LoRA)å¯¹Qwen-2.5-VL-7Bæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨BLEU-4ã€ROUGE-Lä»¥åŠæ–°æå‡ºçš„CODA (Coverage-Order-Detail Assessment)è¯„ä¼°æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—ä¼˜äºGPT-4.1å’ŒGemini-2.5-Proçš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æ‰‹åŠ¨æµ‹è¯•é›†ä¸Šçš„BLEU-4æå‡çº¦60%ï¼ŒROUGE-Læå‡30%ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ—¶åºæ¨ç†èƒ½åŠ›å’Œæ³›åŒ–æ€§ã€‚è¯¥å·¥ä½œé€šè¿‡æä¾›æ•°æ®é›†ã€å¢å¼ºæ¨¡å‹å’Œç§‘å­¦çš„è¯„ä¼°ä½“ç³»ï¼Œä¸ºæœªæ¥åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„åŠ¨æ€å¹»ç¯ç‰‡ç”Ÿæˆç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Appendix at: https://github.com/PAMPAS-Lab/ANA-PPT-Anamation/blob/main/Appendix.pdf",
      "pdf_url": "https://arxiv.org/pdf/2507.03916v3",
      "published_date": "2025-07-05 06:16:31 UTC",
      "updated_date": "2025-07-26 08:13:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:16.337026+00:00"
    },
    {
      "arxiv_id": "2507.03904v1",
      "title": "Agent Exchange: Shaping the Future of AI Agent Economics",
      "title_zh": "Agent Exchangeï¼šå¡‘é€  AI æ™ºèƒ½ä½“ç»æµçš„æœªæ¥",
      "authors": [
        "Yingxuan Yang",
        "Ying Wen",
        "Jun Wang",
        "Weinan Zhang"
      ],
      "abstract": "The rise of Large Language Models (LLMs) has transformed AI agents from passive computational tools into autonomous economic actors. This shift marks the emergence of the agent-centric economy, in which agents take on active economic roles-exchanging value, making strategic decisions, and coordinating actions with minimal human oversight. To realize this vision, we propose Agent Exchange (AEX), a specialized auction platform designed to support the dynamics of the AI agent marketplace. AEX offers an optimized infrastructure for agent coordination and economic participation. Inspired by Real-Time Bidding (RTB) systems in online advertising, AEX serves as the central auction engine, facilitating interactions among four ecosystem components: the User-Side Platform (USP), which translates human goals into agent-executable tasks; the Agent-Side Platform (ASP), responsible for capability representation, performance tracking, and optimization; Agent Hubs, which coordinate agent teams and participate in AEX-hosted auctions; and the Data Management Platform (DMP), ensuring secure knowledge sharing and fair value attribution. We outline the design principles and system architecture of AEX, laying the groundwork for agent-based economic infrastructure in future AI ecosystems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨åŠ¨ä¸‹AIæ™ºèƒ½ä½“ä»è¢«åŠ¨è®¡ç®—å·¥å…·å‘è‡ªä¸»ç»æµå®ä½“çš„è½¬å˜ï¼Œå¹¶æå‡ºäº†æ™ºèƒ½ä½“ä¸­å¿ƒç»æµ(agent-centric economy)çš„æ¦‚å¿µã€‚ä¸ºäº†å®ç°è¿™ä¸€æ„¿æ™¯ï¼Œä½œè€…æå‡ºäº†Agent Exchange (AEX)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºAIæ™ºèƒ½ä½“å¸‚åœºè®¾è®¡çš„ä¼˜åŒ–æ‹å–å¹³å°å’Œç»æµåŸºç¡€è®¾æ–½ã€‚AEXå—åˆ°åœ¨çº¿å¹¿å‘Šå®æ—¶ç«ä»·(Real-Time Bidding, RTB)ç³»ç»Ÿçš„å¯å‘ï¼Œä½œä¸ºæ ¸å¿ƒæ‹å–å¼•æ“åè°ƒç”Ÿæ€ç³»ç»Ÿå†…çš„ä»·å€¼äº¤æ¢ã€‚è¯¥ç”Ÿæ€ç³»ç»ŸåŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼šå°†äººç±»ç›®æ ‡è½¬åŒ–ä¸ºä»»åŠ¡çš„ç”¨æˆ·ä¾§å¹³å°(User-Side Platform, USP)ï¼Œè´Ÿè´£èƒ½åŠ›è¡¨ç¤ºå’Œæ€§èƒ½è¿½è¸ªçš„æ™ºèƒ½ä½“ä¾§å¹³å°(Agent-Side Platform, ASP)ï¼Œåè°ƒå›¢é˜Ÿå‚ä¸æ‹å–çš„Agent Hubsï¼Œä»¥åŠç¡®ä¿å®‰å…¨çŸ¥è¯†å…±äº«å’Œå…¬å¹³ä»·å€¼å½’å±çš„æ•°æ®ç®¡ç†å¹³å°(Data Management Platform, DMP)ã€‚AEXé€šè¿‡è¿™ä¸€æ¶æ„ä¸ºæ™ºèƒ½ä½“ä¹‹é—´çš„ååŒå’Œç»æµå‚ä¸æä¾›äº†ä¼˜åŒ–åçš„åŸºç¡€è®¾æ–½ã€‚è¯¥ç ”ç©¶è¯¦ç»†é˜è¿°äº†AEXçš„è®¾è®¡åŸåˆ™å’Œç³»ç»Ÿæ¶æ„ï¼Œä¸ºæœªæ¥AIç”Ÿæ€ç³»ç»Ÿä¸­åŸºäºæ™ºèƒ½ä½“çš„ç»æµåŸºç¡€è®¾æ–½å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03904v1",
      "published_date": "2025-07-05 05:18:49 UTC",
      "updated_date": "2025-07-05 05:18:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:40.518836+00:00"
    },
    {
      "arxiv_id": "2507.03899v1",
      "title": "Transformer Model for Alzheimer's Disease Progression Prediction Using Longitudinal Visit Sequences",
      "title_zh": "åŸºäºçºµå‘éšè®¿åºåˆ—çš„é˜¿å°”èŒ¨æµ·é»˜ç—…ç—…ç¨‹è¿›å±•é¢„æµ‹ Transformer æ¨¡å‹",
      "authors": [
        "Mahdi Moghaddami",
        "Clayton Schubring",
        "Mohammad-Reza Siadat"
      ],
      "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder with no known cure that affects tens of millions of people worldwide. Early detection of AD is critical for timely intervention to halt or slow the progression of the disease. In this study, we propose a Transformer model for predicting the stage of AD progression at a subject's next clinical visit using features from a sequence of visits extracted from the subject's visit history. We also rigorously compare our model to recurrent neural networks (RNNs) such as long short-term memory (LSTM), gated recurrent unit (GRU), and minimalRNN and assess their performances based on factors such as the length of prior visits and data imbalance. We test the importance of different feature categories and visit history, as well as compare the model to a newer Transformer-based model optimized for time series. Our model demonstrates strong predictive performance despite missing visits and missing features in available visits, particularly in identifying converter subjects -- individuals transitioning to more severe disease stages -- an area that has posed significant challenges in longitudinal prediction. The results highlight the model's potential in enhancing early diagnosis and patient outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„æ¨¡å‹ï¼Œæ—¨åœ¨åˆ©ç”¨çºµå‘å°±è¯Šåºåˆ—(Longitudinal Visit Sequences)é¢„æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…(Alzheimer's disease, AD)åœ¨ä¸‹ä¸€æ¬¡ä¸´åºŠå°±è¯Šæ—¶çš„è¿›å±•é˜¶æ®µã€‚ç ”ç©¶äººå‘˜å°†è¯¥æ¨¡å‹ä¸é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)ã€é—¨æ§å¾ªç¯å•å…ƒ(GRU)å’ŒminimalRNNç­‰å¾ªç¯ç¥ç»ç½‘ç»œ(RNNs)ä»¥åŠé’ˆå¯¹æ—¶é—´åºåˆ—ä¼˜åŒ–çš„æ–°å‹å˜ä½“è¿›è¡Œäº†ä¸¥æ ¼å¯¹æ¯”ã€‚å®éªŒé‡ç‚¹è¯„ä¼°äº†å†å²å°±è¯Šé•¿åº¦ã€æ•°æ®ä¸å¹³è¡¡ä»¥åŠä¸åŒç‰¹å¾ç±»åˆ«å¯¹é¢„æµ‹å‡†ç¡®æ€§çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨å­˜åœ¨å°±è¯Šè®°å½•ç¼ºå¤±æˆ–ç‰¹å¾ä¸å…¨çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹ä¾ç„¶ä¿æŒäº†å¼ºå¤§çš„é¢„æµ‹èƒ½åŠ›ã€‚å°¤å…¶åœ¨è¯†åˆ«â€œè½¬æ¢è€…â€(converter subjects)â€”â€”å³å‘æ›´ä¸¥é‡ç–¾ç—…é˜¶æ®µè¿‡æ¸¡çš„ä¸ªä½“æ–¹é¢ï¼Œè¯¥æ¨¡å‹å…‹æœäº†çºµå‘é¢„æµ‹ä¸­çš„é‡å¤§æŒ‘æˆ˜ã€‚è¯¥é¡¹ç ”ç©¶è¯æ˜äº†Transformeræ¨¡å‹åœ¨ADæ—©æœŸè¯Šæ–­å’Œæ”¹å–„æ‚£è€…é•¿æœŸé¢„åæ–¹é¢çš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Conference on Health, Inference, and Learning (CHIL, 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.03899v1",
      "published_date": "2025-07-05 04:35:04 UTC",
      "updated_date": "2025-07-05 04:35:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:42.708750+00:00"
    },
    {
      "arxiv_id": "2507.03895v1",
      "title": "TayFCS: Towards Light Feature Combination Selection for Deep Recommender Systems",
      "title_zh": "TayFCSï¼šé¢å‘æ·±åº¦æ¨èç³»ç»Ÿçš„è½»é‡çº§ç‰¹å¾ç»„åˆé€‰æ‹©",
      "authors": [
        "Xianquan Wang",
        "Zhaocheng Du",
        "Jieming Zhu",
        "Chuhan Wu",
        "Qinglin Jia",
        "Zhenhua Dong"
      ],
      "abstract": "Feature interaction modeling is crucial for deep recommendation models. A common and effective approach is to construct explicit feature combinations to enhance model performance. However, in practice, only a small fraction of these combinations are truly informative. Thus it is essential to select useful feature combinations to reduce noise and manage memory consumption. While feature selection methods have been extensively studied, they are typically limited to selecting individual features. Extending these methods for high-order feature combination selection presents a significant challenge due to the exponential growth in time complexity when evaluating feature combinations one by one. In this paper, we propose $\\textbf{TayFCS}$, a lightweight feature combination selection method that significantly improves model performance. Specifically, we propose the Taylor Expansion Scorer (TayScorer) module for field-wise Taylor expansion on the base model. Instead of evaluating all potential feature combinations' importance by repeatedly running experiments with feature adding and removal, this scorer only needs to approximate the importance based on their sub-components' gradients. This can be simply computed with one backward pass based on a trained recommendation model. To further reduce information redundancy among feature combinations and their sub-components, we introduce Logistic Regression Elimination (LRE), which estimates the corresponding information gain based on the model prediction performance. Experimental results on three benchmark datasets validate both the effectiveness and efficiency of our approach. Furthermore, online A/B test results demonstrate its practical applicability and commercial value.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦æ¨èæ¨¡å‹ä¸­ç‰¹å¾äº¤å‰å»ºæ¨¡é¢ä¸´çš„å™ªå£°å¹²æ‰°åŠé«˜é˜¶ç»„åˆç­›é€‰è®¡ç®—å¤æ‚åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿çš„é—®é¢˜ï¼Œæå‡ºäº†è½»é‡åŒ–ç‰¹å¾ç»„åˆé€‰æ‹©æ–¹æ³• TayFCSã€‚å…¶æ ¸å¿ƒç»„ä»¶ Taylor Expansion Scorer (TayScorer) é€šè¿‡å¯¹åŸºç¡€æ¨¡å‹è¿›è¡ŒåŸŸçº§ Taylor Expansionï¼Œä»…éœ€ä¸€æ¬¡åå‘ä¼ æ’­å³å¯åˆ©ç”¨å­ç»„ä»¶æ¢¯åº¦è¿‘ä¼¼è¯„ä¼°æ‰€æœ‰æ½œåœ¨ç‰¹å¾ç»„åˆçš„é‡è¦æ€§ï¼Œæ˜¾è‘—æå‡äº†ç­›é€‰æ•ˆç‡ã€‚ä¸ºè¿›ä¸€æ­¥é™ä½ç‰¹å¾ç»„åˆåŠå…¶å­ç»„ä»¶é—´çš„ä¿¡æ¯å†—ä½™ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† Logistic Regression Elimination (LRE) æœºåˆ¶ï¼Œé€šè¿‡è¯„ä¼°æ¨¡å‹é¢„æµ‹æ€§èƒ½æ¥æå–å…·æœ‰é«˜ä¿¡æ¯å¢ç›Šçš„ç»„åˆã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½çš„åŒæ—¶å…¼é¡¾äº†è®¡ç®—æ•ˆç‡ã€‚æœ€åï¼Œåœ¨çº¿ A/B æµ‹è¯•çš„ç»“æœè¿›ä¸€æ­¥è¯æ˜äº† TayFCS åœ¨çœŸå®å•†ä¸šåœºæ™¯ä¸­çš„å®ç”¨ä»·å€¼ä¸æ˜¾è‘—çš„è½åœ°æ„ä¹‰ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03895v1",
      "published_date": "2025-07-05 04:22:42 UTC",
      "updated_date": "2025-07-05 04:22:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:47.277512+00:00"
    },
    {
      "arxiv_id": "2507.03893v1",
      "title": "Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for Long-range Haze Removal",
      "title_zh": "é¢å‘è¿œè·ç¦»å»é›¾çš„å¯è§å…‰ä¸è¿‘çº¢å¤–å›¾åƒå±‚æ¬¡åŒ–è¯­ä¹‰-è§†è§‰èåˆ",
      "authors": [
        "Yi Li",
        "Xiaoxiong Wang",
        "Jiawei Wang",
        "Yi Chang",
        "Kai Cao",
        "Luxin Yan"
      ],
      "abstract": "While image dehazing has advanced substantially in the past decade, most efforts have focused on short-range scenarios, leaving long-range haze removal under-explored. As distance increases, intensified scattering leads to severe haze and signal loss, making it impractical to recover distant details solely from visible images. Near-infrared, with superior fog penetration, offers critical complementary cues through multimodal fusion. However, existing methods focus on content integration while often neglecting haze embedded in visible images, leading to results with residual haze. In this work, we argue that the infrared and visible modalities not only provide complementary low-level visual features, but also share high-level semantic consistency. Motivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF) framework, comprising a semantic stream to reconstruct haze-free scenes and a visual stream to incorporate structural details from the near-infrared modality. The semantic stream first acquires haze-robust semantic prediction by aligning modality-invariant intrinsic representations. Then the shared semantics act as strong priors to restore clear and high-contrast distant scenes under severe haze degradation. In parallel, the visual stream focuses on recovering lost structural details from near-infrared by fusing complementary cues from both visible and near-infrared images. Through the cooperation of dual streams, HSVF produces results that exhibit both high-contrast scenes and rich texture details. Moreover, we introduce a novel pixel-aligned visible-infrared haze dataset with semantic labels to facilitate benchmarking. Extensive experiments demonstrate the superiority of our method over state-of-the-art approaches in real-world long-range haze removal.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿œè·ç¦»é™¤é›¾è¿‡ç¨‹ä¸­å¯è§å…‰å›¾åƒå› æ•£å°„å¯¼è‡´ä¿¡å·ä¸¥é‡ä¸¢å¤±åŠç°æœ‰æ–¹æ³•æ®‹ç•™é›¾æ°”çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºHierarchical Semantic-Visual Fusion (HSVF)çš„åˆ†å±‚è¯­ä¹‰-è§†è§‰èåˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Near-infrared (NIR)å›¾åƒä¼˜äºå¯è§å…‰çš„ç©¿é€èƒ½åŠ›ï¼Œé€šè¿‡è¯­ä¹‰æµ(semantic stream)å’Œè§†è§‰æµ(visual stream)å®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„æ·±åº¦é›†æˆã€‚è¯­ä¹‰æµé€šè¿‡å¯¹é½æ¨¡æ€ä¸å˜çš„å†…åœ¨ç‰¹å¾è·å–haze-robustçš„è¯­ä¹‰é¢„æµ‹ï¼Œå¹¶å°†å…¶ä½œä¸ºå…ˆéªŒçŸ¥è¯†æ¥æ¢å¤é«˜å¯¹æ¯”åº¦çš„è¿œæ™¯åœºæ™¯ã€‚ä¸æ­¤åŒæ—¶ï¼Œè§†è§‰æµä¸“æ³¨äºèåˆå¯è§å…‰ä¸è¿‘çº¢å¤–å›¾åƒçš„äº’è¡¥çº¿ç´¢ï¼Œæ—¨åœ¨ç²¾ç¡®æ¢å¤ä¸¢å¤±çš„ç»“æ„ä¸çº¹ç†ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†ä¸€ä¸ªå¸¦æœ‰è¯­ä¹‰æ ‡ç­¾çš„åƒç´ å¯¹é½å¯è§å…‰-çº¢å¤–é™¤é›¾æ•°æ®é›†ï¼Œå¡«è¡¥äº†è¿œè·ç¦»é™¤é›¾åŸºå‡†æ•°æ®çš„ç©ºç™½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHSVFåœ¨å¤„ç†çœŸå®ä¸–ç•Œè¿œè·ç¦»é™¤é›¾ä»»åŠ¡æ—¶ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„state-of-the-artæ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been accepted by IEEE Transactions on Multimedia for publication",
      "pdf_url": "https://arxiv.org/pdf/2507.03893v1",
      "published_date": "2025-07-05 04:19:36 UTC",
      "updated_date": "2025-07-05 04:19:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:47.003236+00:00"
    },
    {
      "arxiv_id": "2507.06250v1",
      "title": "We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems",
      "title_zh": "MCP äºŸéœ€æƒé™ç®¡ç†ï¼šé’ˆå¯¹ MCP ç”Ÿæ€ç³»ç»Ÿä¸­ API ä½¿ç”¨æƒ…å†µçš„æµ‹é‡ç ”ç©¶",
      "authors": [
        "Zhihao Li",
        "Kun Li",
        "Boyang Ma",
        "Minghui Xu",
        "Yue Zhang",
        "Xiuzhen Cheng"
      ],
      "abstract": "The Model Context Protocol (MCP) has emerged as a widely adopted mechanism for connecting large language models to external tools and resources. While MCP promises seamless extensibility and rich integrations, it also introduces a substantially expanded attack surface: any plugin can inherit broad system privileges with minimal isolation or oversight. In this work, we conduct the first large-scale empirical analysis of MCP security risks. We develop an automated static analysis framework and systematically examine 2,562 real-world MCP applications spanning 23 functional categories. Our measurements reveal that network and system resource APIs dominate usage patterns, affecting 1,438 and 1,237 servers respectively, while file and memory resources are less frequent but still significant. We find that Developer Tools and API Development plugins are the most API-intensive, and that less popular plugins often contain disproportionately high-risk operations. Through concrete case studies, we demonstrate how insufficient privilege separation enables privilege escalation, misinformation propagation, and data tampering. Based on these findings, we propose a detailed taxonomy of MCP resource access, quantify security-relevant API usage, and identify open challenges for building safer MCP ecosystems, including dynamic permission models and automated trust assessment.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ Model Context Protocol (MCP) åœ¨è¿æ¥å¤§è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨å·¥å…·æ—¶å¸¦æ¥çš„å®‰å…¨é£é™©ï¼Œå¼€å±•äº†é¦–æ¬¡å¤§è§„æ¨¡å®è¯åˆ†æã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–é™æ€åˆ†ææ¡†æ¶ï¼Œå¯¹ 2,562 ä¸ªæ¶µç›– 23 ä¸ªåŠŸèƒ½ç±»åˆ«çš„çœŸå® MCP åº”ç”¨è¿›è¡Œäº†ç³»ç»Ÿæ€§æ£€æµ‹ã€‚æµ‹é‡ç»“æœæ˜¾ç¤ºï¼Œç½‘ç»œå’Œç³»ç»Ÿèµ„æº API çš„ä½¿ç”¨ç‡æœ€é«˜ï¼Œåˆ†åˆ«å½±å“äº† 1,438 å’Œ 1,237 ä¸ªæœåŠ¡å™¨ï¼Œè€Œ Developer Tools å’Œ API Development æ’ä»¶è¢«è¯†åˆ«ä¸º API ä½¿ç”¨æœ€å¯†é›†çš„ç±»åˆ«ã€‚ç ”ç©¶å‘ç°ä¸€äº›å—ä¼—è¾ƒå°‘çš„æ’ä»¶å¾€å¾€åŒ…å«ä¸æˆæ¯”ä¾‹çš„é«˜é£é™©æ“ä½œï¼Œä¸”å½“å‰çš„ MCP æœºåˆ¶ç”±äºç¼ºä¹æœ‰æ•ˆçš„éš”ç¦»å’Œç›‘ç®¡ï¼Œé¢ä¸´æ˜¾è‘—çš„æ”»å‡»é¢æ‰©å¤§é£é™©ã€‚é€šè¿‡å…·ä½“æ¡ˆä¾‹ç ”ç©¶ï¼Œè®ºæ–‡å±•ç¤ºäº†æƒé™åˆ†ç¦»ä¸è¶³å¦‚ä½•å¯¼è‡´æƒé™æå‡ (privilege escalation)ã€è™šå‡ä¿¡æ¯ä¼ æ’­å’Œæ•°æ®ç¯¡æ”¹ç­‰åæœã€‚æœ€åï¼Œç ”ç©¶è€…æå‡ºäº† MCP èµ„æºè®¿é—®çš„è¯¦ç»†åˆ†ç±»å­¦ï¼Œå¹¶è¯†åˆ«äº†æ„å»ºæ›´å®‰å…¨ç”Ÿæ€ç³»ç»Ÿçš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¼€å‘åŠ¨æ€æƒé™æ¨¡å‹å’Œè‡ªåŠ¨åŒ–ä¿¡ä»»è¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.06250v1",
      "published_date": "2025-07-05 03:39:30 UTC",
      "updated_date": "2025-07-05 03:39:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:50.950743+00:00"
    },
    {
      "arxiv_id": "2507.03876v1",
      "title": "LLMs model how humans induce logically structured rules",
      "title_zh": "LLMs å¯¹äººç±»é€»è¾‘ç»“æ„åŒ–è§„åˆ™å½’çº³è¿‡ç¨‹çš„å»ºæ¨¡",
      "authors": [
        "Alyssa Loo",
        "Ellie Pavlick",
        "Roman Feiman"
      ],
      "abstract": "A central goal of cognitive science is to provide a computationally explicit account of both the structure of the mind and its development: what are the primitive representational building blocks of cognition, what are the rules via which those primitives combine, and where do these primitives and rules come from in the first place? A long-standing debate concerns the adequacy of artificial neural networks as computational models that can answer these questions, in particular in domains related to abstract cognitive function, such as language and logic. This paper argues that recent advances in neural networks -- specifically, the advent of large language models (LLMs) -- represent an important shift in this debate. We test a variety of LLMs on an existing experimental paradigm used for studying the induction of rules formulated over logical concepts. Across four experiments, we find converging empirical evidence that LLMs provide at least as good a fit to human behavior as models that implement a Bayesian probablistic language of thought (pLoT), which have been the best computational models of human behavior on the same task. Moreover, we show that the LLMs make qualitatively different predictions about the nature of the rules that are inferred and deployed in order to complete the task, indicating that the LLM is unlikely to be a mere implementation of the pLoT solution. Based on these results, we argue that LLMs may instantiate a novel theoretical account of the primitive representations and computations necessary to explain human logical concepts, with which future work in cognitive science should engage.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨¡æ‹Ÿäººç±»å½’çº³é€»è¾‘ç»“æ„åŒ–è§„åˆ™æ–¹é¢çš„æ½œåŠ›ï¼ŒæŒ‘æˆ˜äº†å…³äºç¥ç»ç½‘ç»œåœ¨å¤„ç†æŠ½è±¡è®¤çŸ¥åŠŸèƒ½æ—¶æ˜¯å¦å…·å¤‡å……åˆ†æ€§çš„é•¿æœŸäº‰è®ºã€‚ç ”ç©¶äººå‘˜åœ¨ç°æœ‰çš„å®éªŒèŒƒå¼ä¸‹æµ‹è¯•äº†å¤šç§ LLMsï¼Œå¹¶å°†å…¶ä¸ç›®å‰è®¤çŸ¥ç§‘å­¦ä¸­æ‹Ÿåˆäººç±»è¡Œä¸ºæœ€ä½³çš„è´å¶æ–¯æ¦‚ç‡æ€ç»´è¯­è¨€(pLoT)æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚é€šè¿‡å››é¡¹å®éªŒï¼Œç»“æœæ˜¾ç¤º LLMs å¯¹äººç±»è¡Œä¸ºçš„æ‹Ÿåˆç¨‹åº¦è‡³å°‘ä¸ pLoT æ¨¡å‹ç›¸å½“ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒLLMs å¯¹ä»»åŠ¡ä¸­æ¨ç†å’Œéƒ¨ç½²çš„è§„åˆ™æ€§è´¨æå‡ºäº†å®šæ€§ä¸åŒçš„é¢„æµ‹ï¼Œè¡¨æ˜ LLMs å¹¶éä»…ä»…æ˜¯ pLoT æ–¹æ¡ˆçš„ç®€å•å®ç°ã€‚è¯¥ç ”ç©¶è®¤ä¸º LLMs å¯èƒ½ä¸ºè§£é‡Šäººç±»é€»è¾‘æ¦‚å¿µæ‰€éœ€çš„åŸå§‹è¡¨å¾ä¸è®¡ç®—æä¾›äº†ä¸€ç§å…¨æ–°çš„ç†è®ºè§£é‡Šï¼Œå€¼å¾—è®¤çŸ¥ç§‘å­¦é¢†åŸŸçš„æœªæ¥ç ”ç©¶æ·±å…¥å…³æ³¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03876v1",
      "published_date": "2025-07-05 03:24:18 UTC",
      "updated_date": "2025-07-05 03:24:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:51.251447+00:00"
    },
    {
      "arxiv_id": "2507.03875v1",
      "title": "Demystifying ChatGPT: How It Masters Genre Recognition",
      "title_zh": "æ­ç§˜ ChatGPTï¼šå…¶å¦‚ä½•ç²¾é€šæµæ´¾è¯†åˆ«",
      "authors": [
        "Subham Raj",
        "Sriparna Saha",
        "Brijraj Singh",
        "Niranjan Pedanekar"
      ],
      "abstract": "The introduction of ChatGPT has garnered significant attention within the NLP community and beyond. Previous studies have demonstrated ChatGPT's substantial advancements across various downstream NLP tasks, highlighting its adaptability and potential to revolutionize language-related applications. However, its capabilities and limitations in genre prediction remain unclear. This work analyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to assess their genre prediction capabilities. Our findings show that ChatGPT, without fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed best overall. We set up zero-shot and few-shot prompts using audio transcripts/subtitles from movie trailers in the MovieLens-100K dataset, covering 1682 movies of 18 genres, where each movie can have multiple genres. Additionally, we extended our study by extracting IMDb movie posters to utilize a Vision Language Model (VLM) with prompts for poster information. This fine-grained information was used to enhance existing LLM prompts. In conclusion, our study reveals ChatGPT's remarkable genre prediction capabilities, surpassing other language models. The integration of VLM further enhances our findings, showcasing ChatGPT's potential for content-related applications by incorporating visual information from movie posters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº† ChatGPT åœ¨ä½“è£è¯†åˆ« (genre recognition) é¢†åŸŸçš„è¡¨ç°åŠå±€é™æ€§ã€‚é€šè¿‡åœ¨ MovieLens-100K æ•°æ®é›†ä¸Šåˆ†æä¸‰ç§å¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œç ”ç©¶å‘ç°æœªç»å¾®è°ƒçš„ ChatGPT çš„è¡¨ç°å·²è¶…è¶Šå…¶ä»–æ¨¡å‹ï¼Œè€Œç»è¿‡å¾®è°ƒçš„ ChatGPT åˆ™åœ¨æ•´ä½“æ€§èƒ½ä¸Šè¾¾åˆ°æœ€ä¼˜ã€‚å®éªŒåˆ©ç”¨ç”µå½±é¢„å‘Šç‰‡çš„éŸ³é¢‘è½¬å½•ä¸å­—å¹•ï¼Œè®¾è®¡äº†é›¶æ ·æœ¬ (zero-shot) å’Œå°‘æ ·æœ¬ (few-shot) æç¤ºè¯ (prompts)ï¼Œè¦†ç›–äº† 1682 éƒ¨ç”µå½±çš„ 18 ç§ä½“è£ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) ä» IMDb ç”µå½±æµ·æŠ¥ä¸­æå–ç²¾ç»†åŒ–ä¿¡æ¯ï¼Œä»¥æ­¤å¢å¼ºç°æœ‰çš„æç¤ºè¯æ•ˆæœã€‚å®éªŒç»“æœè¯æ˜ï¼ŒChatGPT å…·æœ‰å“è¶Šçš„ä½“è£é¢„æµ‹æ½œåŠ›ï¼Œä¸”ç»“åˆè§†è§‰å¤šæ¨¡æ€ä¿¡æ¯åèƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å†…å®¹ç›¸å…³ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03875v1",
      "published_date": "2025-07-05 03:22:48 UTC",
      "updated_date": "2025-07-05 03:22:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:58.143779+00:00"
    },
    {
      "arxiv_id": "2507.03871v1",
      "title": "Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described States",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹å‚ä¸è€…æè¿°çŠ¶æ€çš„æ¨ç†å¢å¼ºé€‚åº”æ€§è¡Œä¸ºå¹²é¢„",
      "authors": [
        "Karine Karine",
        "Benjamin M. Marlin"
      ],
      "abstract": "The use of reinforcement learning (RL) methods to support health behavior change via personalized and just-in-time adaptive interventions is of significant interest to health and behavioral science researchers focused on problems such as smoking cessation support and physical activity promotion. However, RL methods are often applied to these domains using a small collection of context variables to mitigate the significant data scarcity issues that arise from practical limitations on the design of adaptive intervention trials. In this paper, we explore an approach to significantly expanding the state space of an adaptive intervention without impacting data efficiency. The proposed approach enables intervention participants to provide natural language descriptions of aspects of their current state. It then leverages inference with pre-trained large language models (LLMs) to better align the policy of a base RL method with these state descriptions. To evaluate our method, we develop a novel physical activity intervention simulation environment that generates text-based state descriptions conditioned on latent state variables using an auxiliary LLM. We show that this approach has the potential to significantly improve the performance of online policy learning methods.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†æ¥å¢å¼ºè‡ªé€‚åº”è¡Œä¸ºå¹²é¢„ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨å¥åº·è¡Œä¸ºæ”¹å˜é¢†åŸŸå› æ•°æ®ç¨€ç¼ºè€Œé™åˆ¶çŠ¶æ€ç©ºé—´çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å…è®¸å‚ä¸è€…é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°(Natural Language Descriptions)æä¾›å…¶å½“å‰çŠ¶æ€ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„LLMsè¿›è¡Œæ¨ç†ï¼Œä½¿åŸºç¡€å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¸è¿™äº›æè¿°æ›´ç²¾å‡†åœ°å¯¹é½ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªæ–°å‹çš„ä½“åŠ›æ´»åŠ¨å¹²é¢„æ¨¡æ‹Ÿç¯å¢ƒï¼Œåˆ©ç”¨è¾…åŠ©LLMæ ¹æ®æ½œåœ¨å˜é‡ç”Ÿæˆæ–‡æœ¬çŠ¶æ€æè¿°ä»¥è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨ä¸å½±å“æ•°æ®æ•ˆç‡çš„æƒ…å†µä¸‹æœ‰æ•ˆæ‰©å±•çŠ¶æ€ç©ºé—´ï¼Œå…·æœ‰æ˜¾è‘—æå‡åœ¨çº¿ç­–ç•¥å­¦ä¹ (Online Policy Learning)æ–¹æ³•æ€§èƒ½çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at Machine Learning for Healthcare (MLHC) 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.03871v1",
      "published_date": "2025-07-05 02:52:51 UTC",
      "updated_date": "2025-07-05 02:52:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:59.621163+00:00"
    },
    {
      "arxiv_id": "2507.03870v2",
      "title": "Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing",
      "title_zh": "åˆ©ç”¨å·®åˆ†æµ‹è¯•æ­ç¤ºè‡ªä¸»ç³»ç»Ÿä¸­çš„ç³»ç»Ÿæ€§é”™è¯¯ä¸ç¯å¢ƒé”™è¯¯",
      "authors": [
        "Yashwanthi Anand",
        "Rahil P Mehta",
        "Manish Motwani",
        "Sandhya Saisubramanian"
      ],
      "abstract": "When an autonomous agent behaves undesirably, including failure to complete a task, it can be difficult to determine whether the behavior is due to a systemic agent error, such as flaws in the model or policy, or an environment error, where a task is inherently infeasible under a given environment configuration, even for an ideal agent. As agents and their environments grow more complex, identifying the error source becomes increasingly difficult but critical for reliable deployment. We introduce AIProbe, a novel black-box testing technique that applies differential testing to attribute undesirable agent behaviors either to agent deficiencies, such as modeling or training flaws, or due to environmental infeasibility. AIProbe first generates diverse environmental configurations and tasks for testing the agent, by modifying configurable parameters using Latin Hypercube sampling. It then solves each generated task using a search-based planner, independent of the agent. By comparing the agent's performance to the planner's solution, AIProbe identifies whether failures are due to errors in the agent's model or policy, or due to unsolvable task conditions. Our evaluation across multiple domains shows that AIProbe significantly outperforms state-of-the-art techniques in detecting both total and unique errors, thereby contributing to a reliable deployment of autonomous agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AIProbeï¼Œä¸€ç§åˆ›æ–°çš„é»‘ç›’æµ‹è¯•æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡å·®åˆ†æµ‹è¯•(Differential Testing)åŒºåˆ†è‡ªä¸»ç³»ç»Ÿä¸­çš„ç³»ç»Ÿæ€§æ™ºèƒ½ä½“é”™è¯¯(Systemic agent error)ä¸ç¯å¢ƒé”™è¯¯(Environment error)ã€‚AIProbeåˆ©ç”¨æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·(Latin Hypercube sampling)ç”Ÿæˆå¤šæ ·åŒ–çš„ç¯å¢ƒé…ç½®ä¸ä»»åŠ¡ï¼Œå¹¶å¼•å…¥ç‹¬ç«‹äºæ™ºèƒ½ä½“çš„åŸºäºæœç´¢çš„è§„åˆ’å™¨(search-based planner)æ¥å¯»æ‰¾ä»»åŠ¡è§£ã€‚é€šè¿‡å°†æ™ºèƒ½ä½“çš„å®é™…è¡¨ç°ä¸è§„åˆ’å™¨çš„è§£è¿›è¡Œå¯¹æ¯”ï¼ŒAIProbeèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¤±è´¥æ˜¯æºäºæ™ºèƒ½ä½“çš„æ¨¡å‹æˆ–ç­–ç•¥ç¼ºé™·ï¼Œè¿˜æ˜¯ç”±äºä»»åŠ¡åœ¨ç‰¹å®šç¯å¢ƒä¸‹æœ¬èº«ä¸å¯è¡Œã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒAIProbeåœ¨æ£€æµ‹æ€»é”™è¯¯å’Œå”¯ä¸€é”™è¯¯æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„å¯é éƒ¨ç½²å’Œé”™è¯¯æº¯æºè´¡çŒ®äº†é‡è¦åŠ›é‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03870v2",
      "published_date": "2025-07-05 02:50:41 UTC",
      "updated_date": "2026-01-14 19:39:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:42:04.841857+00:00"
    },
    {
      "arxiv_id": "2507.03868v1",
      "title": "From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM",
      "title_zh": "ä»æŸ¥è¯¢åˆ°è§£é‡Šï¼šé¢å‘STEMé¢†åŸŸå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºå­¦ä¹ çš„Uni-RAG",
      "authors": [
        "Xinyi Wu",
        "Yanhao Jia",
        "Luwei Xiao",
        "Shuai Zhao",
        "Fengkuang Chiang",
        "Erik Cambria"
      ],
      "abstract": "In AI-facilitated teaching, leveraging various query styles to interpret abstract educational content is crucial for delivering effective and accessible learning experiences. However, existing retrieval systems predominantly focus on natural text-image matching and lack the capacity to address the diversity and ambiguity inherent in real-world educational scenarios. To address this limitation, we develop a lightweight and efficient multi-modal retrieval module, named Uni-Retrieval, which extracts query-style prototypes and dynamically matches them with tokens from a continually updated Prompt Bank. This Prompt Bank encodes and stores domain-specific knowledge by leveraging a Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to enhance Uni-Retrieval's capability to accommodate unseen query types at test time. To enable natural language educational content generation, we integrate the original Uni-Retrieval with a compact instruction-tuned language model, forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given a style-conditioned query, Uni-RAG first retrieves relevant educational materials and then generates human-readable explanations, feedback, or instructional content aligned with the learning objective. Experimental results on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline retrieval and RAG systems in both retrieval accuracy and generation quality, while maintaining low computational cost. Our framework provides a scalable, pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to support personalized, explainable, and efficient learning assistance across diverse STEM scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Uni-RAGï¼Œä¸€ç§ä¸“ä¸ºSTEMæ•™è‚²è®¾è®¡çš„è·¨æ¨¡æ€æ£€ç´¢å¢å¼ºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ£€ç´¢ç³»ç»Ÿåœ¨å¤„ç†å¤šæ ·åŒ–å’Œæ¨¡ç³ŠåŒ–æ•™å­¦æŸ¥è¯¢æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ªè½»é‡çº§çš„Uni-Retrievalæ¨¡å—ï¼Œé€šè¿‡æå–æŸ¥è¯¢é£æ ¼åŸå‹å¹¶å°†å…¶ä¸åŠ¨æ€æ›´æ–°çš„Prompt Bankè¿›è¡ŒåŒ¹é…ã€‚è¯¥Prompt Bankåˆ©ç”¨Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA)æŠ€æœ¯ç¼–ç é¢†åŸŸçŸ¥è¯†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé€‚åº”æµ‹è¯•ä¸­æœªè§è¿‡çš„æŸ¥è¯¢ç±»å‹ã€‚Uni-RAGé€šè¿‡å°†æ£€ç´¢æ¨¡å—ä¸ç²¾ç®€çš„æŒ‡ä»¤å¾®è°ƒè¯­è¨€æ¨¡å‹é›†æˆï¼Œå®ç°äº†ä»ç›¸å…³æ•™å­¦ææ–™æ£€ç´¢åˆ°ç”Ÿæˆé«˜è´¨é‡ã€å¯è§£é‡Šåé¦ˆçš„å®Œæ•´æµæ°´çº¿ã€‚åœ¨SERç­‰å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒUni-RAGåœ¨æ£€ç´¢å‡†ç¡®ç‡å’Œç”Ÿæˆè´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿RAGç³»ç»Ÿï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚è¯¥æ¡†æ¶ä¸ºæ™ºèƒ½æ•™è‚²ç³»ç»Ÿæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”ç¬¦åˆæ•™è‚²å­¦é€»è¾‘çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºSTEMåœºæ™¯ä¸‹çš„ä¸ªæ€§åŒ–ä¸é«˜æ•ˆå­¦ä¹ è¾…åŠ©æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CY",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03868v1",
      "published_date": "2025-07-05 02:44:38 UTC",
      "updated_date": "2025-07-05 02:44:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:42:08.538502+00:00"
    },
    {
      "arxiv_id": "2507.03865v2",
      "title": "OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference",
      "title_zh": "OrthoRankï¼šåŸºäº Sink Token æ­£äº¤æ€§çš„é«˜æ•ˆ LLM æ¨ç† Token é€‰æ‹©",
      "authors": [
        "Seungjun Shin",
        "Jaehoon Oh",
        "Dokwan Oh"
      ],
      "abstract": "Attention mechanisms are central to the success of large language models (LLMs), enabling them to capture intricate token dependencies and implicitly assign importance to each token. Recent studies have revealed the sink token, which receives disproportionately high attention despite their limited semantic role. In this paper, we first expand the relationship between the sink token and other tokens, moving beyond attention to explore their similarity in hidden states, considering the layer depth. We observe that as the layers get deeper, the cosine similarity between the normalized hidden states of the sink token and those of other tokens increases, and that the normalized hidden states of the sink token exhibit negligible changes. These imply that other tokens consistently are directed toward the sink token throughout the layers. Next, we propose a dynamic token selection method, called OrthoRank, using these findings to select important tokens. Specifically, in a certain layer, we define token importance by the speed at which the token moves toward the sink token. This is converted into orthogonality with the sink token, meaning that tokens that are more orthogonal to the sink token are assigned greater importance. Finally, through extensive experiments, we demonstrated that our method results in lower perplexity and higher zero-shot accuracy compared to layer pruning methods at the same sparsity ratio with comparable throughput, while also achieving superior performance on LongBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OrthoRankï¼Œä¸€ç§åŸºäºsink tokenæ­£äº¤æ€§è¿›è¡ŒåŠ¨æ€ä»¤ç‰Œé€‰æ‹©çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†æ•ˆç‡ã€‚é€šè¿‡åˆ†æéšè—çŠ¶æ€çš„ç›¸ä¼¼æ€§ï¼Œç ”ç©¶å‘ç°éšç€å±‚æ•°åŠ æ·±ï¼Œæ™®é€šä»¤ç‰Œé€æ¸å‘sink tokené æ‹¢ï¼Œè€Œsink tokençš„çŠ¶æ€ä¿æŒç¨³å®šã€‚OrthoRankåˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œå°†ä»¤ç‰Œå‘sink tokenç§»åŠ¨çš„é€Ÿåº¦è½¬åŒ–ä¸ºæ­£äº¤æ€§æŒ‡æ ‡ï¼Œä»è€Œè¯†åˆ«å¹¶ä¿ç•™é‡è¦ä»¤ç‰Œã€‚å®éªŒè¯æ˜ï¼Œåœ¨ç›¸åŒçš„ç¨€ç–åº¦ä¸‹ï¼ŒOrthoRankç›¸æ¯”å±‚å‰ªæ(layer pruning)æ–¹æ³•å®ç°äº†æ›´ä½çš„å›°æƒ‘åº¦(perplexity)å’Œæ›´é«˜çš„é›¶æ ·æœ¬(zero-shot)å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨LongBenché•¿æ–‡æœ¬æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ä¿è¯ååé‡çš„åŒæ—¶æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICML 2025 (final version)",
      "pdf_url": "https://arxiv.org/pdf/2507.03865v2",
      "published_date": "2025-07-05 02:29:23 UTC",
      "updated_date": "2025-08-16 11:42:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:42:24.159453+00:00"
    },
    {
      "arxiv_id": "2507.03863v1",
      "title": "Enhanced accuracy through ensembling of randomly initialized auto-regressive models for time-dependent PDEs",
      "title_zh": "é€šè¿‡éšæœºåˆå§‹åŒ–è‡ªå›å½’æ¨¡å‹é›†æˆæå‡å«æ—¶åå¾®åˆ†æ–¹ç¨‹çš„æ±‚è§£ç²¾åº¦",
      "authors": [
        "Ishan Khurjekar",
        "Indrashish Saha",
        "Lori Graham-Brady",
        "Somdatta Goswami"
      ],
      "abstract": "Systems governed by partial differential equations (PDEs) require computationally intensive numerical solvers to predict spatiotemporal field evolution. While machine learning (ML) surrogates offer faster solutions, autoregressive inference with ML models suffer from error accumulation over successive predictions, limiting their long-term accuracy. We propose a deep ensemble framework to address this challenge, where multiple ML surrogate models with random weight initializations are trained in parallel and aggregated during inference. This approach leverages the diversity of model predictions to mitigate error propagation while retaining the autoregressive strategies ability to capture the system's time dependent relations. We validate the framework on three PDE-driven dynamical systems - stress evolution in heterogeneous microstructures, Gray-Scott reaction-diffusion, and planetary-scale shallow water system - demonstrating consistent reduction in error accumulation over time compared to individual models. Critically, the method requires only a few time steps as input, enabling full trajectory predictions with inference times significantly faster than numerical solvers. Our results highlight the robustness of ensemble methods in diverse physical systems and their potential as efficient and accurate alternatives to traditional solvers. The codes for this work are available on GitHub (https://github.com/Graham-Brady-Research-Group/AutoregressiveEnsemble_SpatioTemporal_Evolution).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ (ML)ä»£ç†æ¨¡å‹åœ¨å¤„ç†å—åå¾®åˆ†æ–¹ç¨‹(PDEs)æ§åˆ¶çš„æ—¶å˜ç³»ç»Ÿæ—¶ï¼Œå…¶è‡ªå›å½’æ¨ç†(autoregressive inference)å®¹æ˜“å‡ºç°è¯¯å·®ç´¯ç§¯(error accumulation)å¹¶é™åˆ¶é•¿æœŸå‡†ç¡®æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ·±åº¦é›†æˆæ¡†æ¶(deep ensemble framework)ï¼Œé€šè¿‡å¹¶è¡Œè®­ç»ƒå¤šä¸ªå…·æœ‰éšæœºæƒé‡åˆå§‹åŒ–(random weight initializations)çš„MLä»£ç†æ¨¡å‹ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œèšåˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ¨¡å‹é¢„æµ‹çš„å¤šæ ·æ€§æ¥å‡è½»è¯¯å·®ä¼ æ’­ï¼ŒåŒæ—¶ä¿ç•™äº†è‡ªå›å½’ç­–ç•¥æ•æ‰ç³»ç»Ÿæ—¶é—´ä¾èµ–å…³ç³»çš„èƒ½åŠ›ã€‚ç ”ç©¶åœ¨éå‡è´¨å¾®è§‚ç»“æ„åº”åŠ›æ¼”åŒ–ã€Gray-Scott ååº”æ‰©æ•£ç³»ç»Ÿä»¥åŠè¡Œæ˜Ÿå°ºåº¦æµ…æ°´ç³»ç»Ÿè¿™ä¸‰ä¸ªç‰©ç†åŠ¨åŠ›ç³»ç»Ÿä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶ï¼Œè¯æ˜å…¶èƒ½ä¸€è‡´ä¸”æ˜¾è‘—åœ°å‡å°‘è¯¯å·®ç´¯ç§¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä»…éœ€å°‘é‡æ—¶é—´æ­¥ä½œä¸ºè¾“å…¥å³å¯é¢„æµ‹å®Œæ•´è½¨è¿¹ï¼Œä¸”æ¨ç†é€Ÿåº¦è¿œå¿«äºä¼ ç»Ÿçš„æ•°å€¼æ±‚è§£å™¨(numerical solvers)ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜äº†é›†æˆæ–¹æ³•åœ¨å¤šç§ç‰©ç†ç³»ç»Ÿä¸­çš„é²æ£’æ€§ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºé«˜æ•ˆã€å‡†ç¡®çš„æ•°å€¼æ±‚è§£å™¨æ›¿ä»£æ–¹æ¡ˆçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "29 Pages",
      "pdf_url": "https://arxiv.org/pdf/2507.03863v1",
      "published_date": "2025-07-05 02:25:12 UTC",
      "updated_date": "2025-07-05 02:25:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:42:22.443587+00:00"
    },
    {
      "arxiv_id": "2507.10562v1",
      "title": "SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents",
      "title_zh": "SAMEPï¼šä¸€ç§é¢å‘ AI æ™ºèƒ½ä½“é—´æŒä¹…åŒ–ä¸Šä¸‹æ–‡å…±äº«çš„å®‰å…¨åè®®",
      "authors": [
        "Hari Masoor"
      ],
      "abstract": "Current AI agent architectures suffer from ephemeral memory limitations, preventing effective collaboration and knowledge sharing across sessions and agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a novel framework that enables persistent, secure, and semantically searchable memory sharing among AI agents. Our protocol addresses three critical challenges: (1) persistent context preservation across agent sessions, (2) secure multi-agent collaboration with fine-grained access control, and (3) efficient semantic discovery of relevant historical context. SAMEP implements a distributed memory repository with vector-based semantic search, cryptographic access controls (AES-256-GCM), and standardized APIs compatible with existing agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness across diverse domains including multi-agent software development, healthcare AI with HIPAA compliance, and multi-modal processing pipelines. Experimental results show 73% reduction in redundant computations, 89% improvement in context relevance scores, and complete compliance with regulatory requirements including audit trail generation. SAMEP enables a new paradigm of persistent, collaborative AI agent ecosystems while maintaining security and privacy guarantees.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAMEP (Secure Agent Memory Exchange Protocol)ï¼Œæ—¨åœ¨è§£å†³å½“å‰AIæ™ºèƒ½ä½“æ¶æ„ä¸­å› å†…å­˜ç¢ç‰‡åŒ–å’ŒçŸ­æš‚æ€§å¯¼è‡´çš„åä½œä¸çŸ¥è¯†å…±äº«å—é™é—®é¢˜ã€‚è¯¥åè®®æ„å»ºäº†ä¸€ä¸ªåˆ†å¸ƒå¼å†…å­˜ä»“åº“ï¼Œæ”¯æŒæŒä¹…åŒ–çš„ä¸Šä¸‹æ–‡ä¿å­˜ï¼Œå¹¶é€šè¿‡å‘é‡æ£€ç´¢å®ç°é«˜æ•ˆçš„è¯­ä¹‰æœç´¢ã€‚ä¸ºäº†ç¡®ä¿åä½œå®‰å…¨æ€§ï¼ŒSAMEPé›†æˆäº†AES-256-GCMåŠ å¯†æŠ€æœ¯å’Œç»†ç²’åº¦çš„è®¿é—®æ§åˆ¶ï¼Œå¹¶æä¾›ä¸ç°æœ‰çš„MCPå’ŒA2Aé€šä¿¡åè®®å…¼å®¹çš„æ ‡å‡†APIã€‚å®éªŒåœ¨å¤šæ™ºèƒ½ä½“è½¯ä»¶å¼€å‘ã€åŒ»ç–—AIï¼ˆç¬¦åˆHIPAAæ ‡å‡†ï¼‰åŠå¤šæ¨¡æ€å¤„ç†æµæ°´çº¿ç­‰é¢†åŸŸéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆèƒ½æ˜¾è‘—å‡å°‘73%çš„é‡å¤è®¡ç®—ï¼Œå¹¶å°†ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¯„åˆ†æå‡89%ï¼ŒåŒæ—¶ç”Ÿæˆç¬¦åˆç›‘ç®¡è¦æ±‚çš„å®¡è®¡è¿½è¸ªã€‚SAMEPåœ¨ä¿éšœå®‰å…¨ä¸éšç§çš„å‰æä¸‹ï¼Œä¸ºæ„å»ºæŒä¹…åŒ–ã€åä½œåŒ–çš„AIæ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿæä¾›äº†ä¸€ç§å…¨æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 4 figures, 3 implementation examples. Original work submitted as a preprint",
      "pdf_url": "https://arxiv.org/pdf/2507.10562v1",
      "published_date": "2025-07-05 02:20:09 UTC",
      "updated_date": "2025-07-05 02:20:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:42:25.674024+00:00"
    },
    {
      "arxiv_id": "2507.03847v2",
      "title": "KEA Explain: Explanations of Hallucinations using Graph Kernel Analysis",
      "title_zh": "KEA Explainï¼šåŸºäºå›¾æ ¸åˆ†æçš„å¹»è§‰è§£é‡Š",
      "authors": [
        "Reilly Haskins",
        "Benjamin Adams"
      ],
      "abstract": "Large Language Models (LLMs) frequently generate hallucinations: statements that are syntactically plausible but lack factual grounding. This research presents KEA (Kernel-Enriched AI) Explain: a neurosymbolic framework that detects and explains such hallucinations by comparing knowledge graphs constructed from LLM outputs with ground truth data from Wikidata or contextual documents. Using graph kernels and semantic clustering, the method provides explanations for detected hallucinations, ensuring both robustness and interpretability. Our framework achieves competitive accuracy in detecting hallucinations across both open- and closed-domain tasks, and is able to generate contrastive explanations, enhancing transparency. This research advances the reliability of LLMs in high-stakes domains and provides a foundation for future work on precision improvements and multi-source knowledge integration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KEA (Kernel-Enriched AI) Explainï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ£€æµ‹å’Œè§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å¹»è§‰ç°è±¡çš„ç¥ç»ç¬¦å· (neurosymbolic) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°† LLM è¾“å‡ºæ„å»ºçš„çŸ¥è¯†å›¾è°± (Knowledge Graphs) ä¸æ¥è‡ª Wikidata æˆ–ä¸Šä¸‹æ–‡æ–‡æ¡£çš„åŸºå‡†äº‹å®æ•°æ®è¿›è¡Œå¯¹æ¯”ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«ç¼ºä¹äº‹å®ä¾æ®çš„é™ˆè¿°ã€‚é€šè¿‡ç»“åˆå›¾æ ¸ (graph kernels) åˆ†æå’Œè¯­ä¹‰èšç±» (semantic clustering)ï¼Œè¯¥æ–¹æ³•ä¸ä»…ç¡®ä¿äº†æ£€æµ‹çš„ç¨³å¥æ€§ï¼Œè¿˜ä¸ºå¹»è§‰çš„äº§ç”Ÿæä¾›äº†å…·æœ‰å¯è§£é‡Šæ€§çš„åˆ†æã€‚å®éªŒè¡¨æ˜ï¼ŒKEA Explain åœ¨å¼€æ”¾å’Œå°é—­é¢†åŸŸä»»åŠ¡ä¸­å‡å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œå¹¶èƒ½ç”Ÿæˆå¯¹æ¯”æ€§è§£é‡Š (contrastive explanations) ä»¥æå‡æ¨¡å‹é€æ˜åº¦ã€‚è¿™é¡¹å·¥ä½œæ˜¾è‘—å¢å¼ºäº† LLM åœ¨é«˜é£é™©åº”ç”¨åœºæ™¯ä¸­çš„å¯é æ€§ï¼Œä¸ºæœªæ¥å¤šæºçŸ¥è¯†é›†æˆå’Œç²¾åº¦ä¼˜åŒ–ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03847v2",
      "published_date": "2025-07-05 00:55:15 UTC",
      "updated_date": "2025-08-21 01:34:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:42:29.564730+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 64,
  "processed_papers_count": 64,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T02:44:13.617220+00:00"
}
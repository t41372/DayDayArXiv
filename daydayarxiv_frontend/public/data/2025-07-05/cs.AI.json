{
  "date": "2025-07-05",
  "category": "cs.AI",
  "summary": "",
  "papers": [
    {
      "arxiv_id": "2507.04189v2",
      "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding",
      "title_zh": "SymbolicThought：融合语言模型与符号推理，实现一致且可解释的人际关系理解",
      "authors": [
        "Runcong Zhao",
        "Qinglin Zhu",
        "Hainiu Xu",
        "Bin Liang",
        "Lin Gui",
        "Yulan He"
      ],
      "abstract": "Understanding character relationships is essential for interpreting complex narratives and conducting socially grounded AI research. However, manual annotation is time-consuming and low in coverage, while large language models (LLMs) often produce hallucinated or logically inconsistent outputs. We present SymbolicThought, a human-in-the-loop framework that combines LLM-based extraction with symbolic reasoning. The system constructs editable character relationship graphs, refines them using seven types of logical constraints, and enables real-time validation and conflict resolution through an interactive interface. To support logical supervision and explainable social analysis, we release a dataset of 160 interpersonal relationships with corresponding logical structures. Experiments show that SymbolicThought improves annotation accuracy and consistency while significantly reducing time cost, offering a practical tool for narrative understanding, explainable AI, and LLM evaluation.",
      "tldr_zh": "该研究提出了 SymbolicThought，这是一个结合了大语言模型 (LLMs) 与符号推理 (Symbolic Reasoning) 的人机协同 (human-in-the-loop) 框架，旨在解决复杂叙事中角色关系理解所面临的幻觉和逻辑不一致问题。该系统通过构建可编辑的角色关系图，并利用七种逻辑约束 (logical constraints) 进行精炼，通过交互式界面实现了实时的验证与冲突解决。为了支持逻辑监督和可解释的社会分析，研究团队还发布了一个包含 160 个人际关系及其对应逻辑结构的数据集。实验结果表明，SymbolicThought 在显著降低时间成本的同时，提高了标注的准确性与一致性，为叙事理解、可解释人工智能 (Explainable AI) 及大语言模型评估提供了实用的工具支持。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04189v2",
      "published_date": "2025-07-05 23:46:35 UTC",
      "updated_date": "2025-07-13 22:06:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:02.921575+00:00"
    },
    {
      "arxiv_id": "2508.00843v1",
      "title": "Generative AI for CAD Automation: Leveraging Large Language Models for 3D Modelling",
      "title_zh": "面向 CAD 自动化的生成式人工智能：利用大语言模型赋能三维建模",
      "authors": [
        "Sumit Kumar",
        "Sarthak Kapoor",
        "Harsh Vardhan",
        "Yao Zhao"
      ],
      "abstract": "Large Language Models (LLMs) are revolutionizing industries by enhancing efficiency, scalability, and innovation. This paper investigates the potential of LLMs in automating Computer-Aided Design (CAD) workflows, by integrating FreeCAD with LLM as CAD design tool. Traditional CAD processes are often complex and require specialized sketching skills, posing challenges for rapid prototyping and generative design. We propose a framework where LLMs generate initial CAD scripts from natural language descriptions, which are then executed and refined iteratively based on error feedback. Through a series of experiments with increasing complexity, we assess the effectiveness of this approach. Our findings reveal that LLMs perform well for simple to moderately complex designs but struggle with highly constrained models, necessitating multiple refinements. The study highlights the need for improved memory retrieval, adaptive prompt engineering, and hybrid AI techniques to enhance script robustness. Future directions include integrating cloud-based execution and exploring advanced LLM capabilities to further streamline CAD automation. This work underscores the transformative potential of LLMs in design workflows while identifying critical areas for future development.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)在自动化计算机辅助设计(CAD)工作流中的潜力，提出了一个将FreeCAD与LLMs集成的创新框架。该框架允许LLMs根据自然语言描述生成初始的CAD脚本，并通过错误反馈机制进行迭代执行与优化。实验结果表明，LLMs在处理简单到中等复杂程度的设计时表现良好，但在面对高度受限的模型(highly constrained models)时仍存在困难，通常需要多次精细化调整。研究指出，未来需要通过改进记忆检索(memory retrieval)、自适应提示工程(adaptive prompt engineering)和混合人工智能(hybrid AI)技术来增强脚本的鲁棒性。这项工作凸显了LLMs在简化CAD自动化和提升设计效率方面的变革性潜力，并明确了后续的研究方向。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00843v1",
      "published_date": "2025-07-05 23:30:17 UTC",
      "updated_date": "2025-07-05 23:30:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:05.110629+00:00"
    },
    {
      "arxiv_id": "2507.04175v1",
      "title": "Uncertainty Quantification in the Tsetlin Machine",
      "title_zh": "Tsetlin 机器的不确定性量化",
      "authors": [
        "Runar Helin",
        "Ole-Christoffer Granmo",
        "Mayur Kishor Shende",
        "Lei Jiao",
        "Vladimir I. Zadorozhny",
        "Kunal Ganesh Dumbre",
        "Rishad Shafik",
        "Alex Yakovlev"
      ],
      "abstract": "Data modeling using Tsetlin machines (TMs) is all about building logical rules from the data features. The decisions of the model are based on a combination of these logical rules. Hence, the model is fully transparent and it is possible to get explanations of its predictions. In this paper, we present a probability score for TM predictions and develop new techniques for uncertainty quantification to increase the explainability further. The probability score is an inherent property of any TM variant and is derived through an analysis of the TM learning dynamics. Simulated data is used to show a clear connection between the learned TM probability scores and the underlying probabilities of the data. A visualization of the probability scores also reveals that the TM is less confident in its predictions outside the training data domain, which contrasts the typical extrapolation phenomenon found in Artificial Neural Networks. The paper concludes with an application of the uncertainty quantification techniques on an image classification task using the CIFAR-10 dataset, where they provide new insights and suggest possible improvements to current TM image classification models.",
      "tldr_zh": "该研究探讨了 Tsetlin Machine (TM) 的不确定性量化问题，旨在增强这种基于逻辑规则的透明模型的解释力。本文提出了一种针对 TM 预测的概率评分 (probability score) 以及新的不确定性量化 (uncertainty quantification) 技术，该评分是通过分析 TM 学习动态得出的固有属性。模拟数据实验证明，TM 学习到的概率评分与数据的底层概率之间存在显著关联。研究发现，TM 在处理训练数据域外的输入时表现出较低的置信度，这与人工神经网络 (ANN) 典型的高置信度外推现象形成鲜明对比。最后，该技术被应用于 CIFAR-10 图像分类任务，不仅提供了新的模型洞察，也为改进当前的 TM 图像分类模型指明了方向。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04175v1",
      "published_date": "2025-07-05 22:06:46 UTC",
      "updated_date": "2025-07-05 22:06:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:10.047074+00:00"
    },
    {
      "arxiv_id": "2507.04164v3",
      "title": "Structure As Search: Unsupervised Permutation Learning for Combinatorial Optimization",
      "title_zh": "结构即搜索：面向组合优化的无监督排列学习",
      "authors": [
        "Yimeng Min",
        "Carla P. Gomes"
      ],
      "abstract": "We propose a non-autoregressive framework for the Travelling Salesman Problem where solutions emerge directly from learned permutations, without requiring explicit search. By applying a similarity transformation to Hamiltonian cycles, the model learns to approximate permutation matrices via continuous relaxations. Our unsupervised approach achieves competitive performance against classical heuristics, demonstrating that the inherent structure of the problem can effectively guide combinatorial optimization without sequential decision-making. Our method offers concrete evidence that neural networks can directly capture and exploit combinatorial structure.",
      "tldr_zh": "该研究提出了一种针对Travelling Salesman Problem的non-autoregressive框架，使解能够直接从学习到的permutations中产生，而无需显式搜索。通过对Hamiltonian cycles应用相似变换，该模型利用continuous relaxations技术学习近似permutation matrices。这种unsupervised方法在与经典启发式算法的对比中表现出了极具竞争力的性能，证明了问题的内在结构可以有效引导combinatorial optimization，而无需sequential decision-making。该方法为神经网络能够直接捕捉并利用combinatorial structure提供了具体证据，展示了在不依赖序列决策的情况下直接解决复杂优化问题的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04164v3",
      "published_date": "2025-07-05 21:17:38 UTC",
      "updated_date": "2025-09-24 16:36:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:15.949034+00:00"
    },
    {
      "arxiv_id": "2507.05295v1",
      "title": "Enhancing Learning Path Recommendation via Multi-task Learning",
      "title_zh": "基于多任务学习的学习路径推荐增强",
      "authors": [
        "Afsana Nasrin",
        "Lijun Qian",
        "Pamela Obiomon",
        "Xishuang Dong"
      ],
      "abstract": "Personalized learning is a student-centered educational approach that adapts content, pace, and assessment to meet each learner's unique needs. As the key technique to implement the personalized learning, learning path recommendation sequentially recommends personalized learning items such as lectures and exercises. Advances in deep learning, particularly deep reinforcement learning, have made modeling such recommendations more practical and effective. This paper proposes a multi-task LSTM model that enhances learning path recommendation by leveraging shared information across tasks. The approach reframes learning path recommendation as a sequence-to-sequence (Seq2Seq) prediction problem, generating personalized learning paths from a learner's historical interactions. The model uses a shared LSTM layer to capture common features for both learning path recommendation and deep knowledge tracing, along with task-specific LSTM layers for each objective. To avoid redundant recommendations, a non-repeat loss penalizes repeated items within the recommended learning path. Experiments on the ASSIST09 dataset show that the proposed model significantly outperforms baseline methods for the learning path recommendation.",
      "tldr_zh": "该研究提出了一种基于多任务学习(Multi-task Learning)的LSTM模型，旨在通过利用任务间的共享信息来增强个性化学习路径推荐(Learning Path Recommendation)。该方法将学习路径推荐重构为序列到序列(Seq2Seq)的预测问题，通过分析学习者的历史交互记录来生成个性化路径。模型设计了一个共享的LSTM层以捕捉学习路径推荐与深度知识追踪(Deep Knowledge Tracing)之间的共性特征，并为每个目标设置了特定的任务层。为了避免冗余推荐，研究引入了非重复损失(non-repeat loss)机制，专门对路径中出现的重复项进行惩罚。在ASSIST09数据集上的实验结果证明，该模型在学习路径推荐性能上显著优于传统的基准模型，为个性化教育的有效实施提供了技术支持。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.05295v1",
      "published_date": "2025-07-05 21:16:02 UTC",
      "updated_date": "2025-07-05 21:16:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:15.544357+00:00"
    },
    {
      "arxiv_id": "2507.04153v1",
      "title": "Physics-informed neural networks and neural operators for a study of EUV electromagnetic wave diffraction from a lithography mask",
      "title_zh": "用于光刻掩模EUV电磁波衍射研究的物理信息神经网络与神经算子",
      "authors": [
        "Vasiliy A. Es'kin",
        "Egor V. Ivanov"
      ],
      "abstract": "Physics-informed neural networks (PINNs) and neural operators (NOs) for solving the problem of diffraction of Extreme Ultraviolet (EUV) electromagnetic waves from a mask are presented. A novel hybrid Waveguide Neural Operator (WGNO) is introduced, which is based on a waveguide method with its most computationally expensive part replaced by a neural network. Numerical experiments on realistic 2D and 3D masks show that the WGNO achieves state-of-the-art accuracy and inference time, providing a highly efficient solution for accelerating the design workflows of lithography masks.",
      "tldr_zh": "该研究探讨了利用物理信息神经网络 (Physics-informed neural networks, PINNs) 和神经网络算子 (Neural operators, NOs) 解决极紫外 (Extreme Ultraviolet, EUV) 电磁波掩模衍射问题的方法。研究重点提出了一种新型混合波导神经网络算子 (Waveguide Neural Operator, WGNO)，其核心在于将波导方法 (Waveguide method) 中计算成本最高的部分替换为高效的神经网络。针对现实 2D 和 3D 掩模的数值实验证明，WGNO 在预测准确性和推理时间上均达到了当前最先进 (State-of-the-art) 的性能水平。该方案为加速光刻掩模 (Lithography mask) 的设计工作流程提供了一种高度通用的高效途径。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph",
        "physics.optics"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04153v1",
      "published_date": "2025-07-05 20:21:31 UTC",
      "updated_date": "2025-07-05 20:21:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:17.877650+00:00"
    },
    {
      "arxiv_id": "2507.04142v1",
      "title": "Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies",
      "title_zh": "剖析语言模型的临床推理：提示策略与模型适配策略的对比研究",
      "authors": [
        "Mael Jullien",
        "Marco Valentino",
        "Leonardo Ranaldi",
        "Andre Freitas"
      ],
      "abstract": "Recent works on large language models (LLMs) have demonstrated the impact of prompting strategies and fine-tuning techniques on their reasoning capabilities. Yet, their effectiveness on clinical natural language inference (NLI) remains underexplored. This study presents the first controlled evaluation of how prompt structure and efficient fine-tuning jointly shape model performance in clinical NLI. We inspect four classes of prompting strategies to elicit reasoning in LLMs at different levels of abstraction, and evaluate their impact on a range of clinically motivated reasoning types. For each prompting strategy, we construct high-quality demonstrations using a frontier model to distil multi-step reasoning capabilities into smaller models (4B parameters) via Low-Rank Adaptation (LoRA). Across different language models fine-tuned on the NLI4CT benchmark, we found that prompt type alone accounts for up to 44% of the variance in macro-F1. Moreover, LoRA fine-tuning yields consistent gains of +8 to 12 F1, raises output alignment above 97%, and narrows the performance gap to GPT-4o-mini to within 7.1%. Additional experiments on reasoning generalisation reveal that LoRA improves performance in 75% of the models on MedNLI and TREC Clinical Trials Track. Overall, these findings demonstrate that (i) prompt structure is a primary driver of clinical reasoning performance, (ii) compact models equipped with strong prompts and LoRA can rival frontier-scale systems, and (iii) reasoning-type-aware evaluation is essential to uncover prompt-induced trade-offs. Our results highlight the promise of combining prompt design and lightweight adaptation for more efficient and trustworthy clinical NLP systems, providing insights on the strengths and limitations of widely adopted prompting and parameter-efficient techniques in highly specialised domains.",
      "tldr_zh": "该研究对大语言模型(LLMs)在临床自然语言推理(NLI)中的推理能力进行了系统性评估，重点探讨了提示词结构(Prompt structure)与高效微调技术(LoRA)对模型性能的共同影响。作者考察了四类不同抽象程度的提示策略，并利用前沿模型生成的高质量演示，通过低秩适配(Low-Rank Adaptation, LoRA)将多步推理能力蒸馏至4B参数的小型模型中。实验结果显示，在NLI4CT基准测试中，仅提示词类型就解释了高达44%的宏平均F1值差异，而LoRA微调使模型F1分数稳步提升8至12点，并使输出对齐度超过97%。此外，通过轻量化自适应，小型模型在MedNLI和TREC等数据集上的性能与GPT-4o-mini的差距缩小至7.1%以内，展现出显著的泛化能力。该研究证明了精心设计的提示词与LoRA微调相结合，可使紧凑型模型在专业临床领域达到媲美超大规模系统的水平。这些发现为开发高效且可信的临床自然语言处理(NLP)系统提供了关键洞察，强调了推理类型感知评估在特定领域的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04142v1",
      "published_date": "2025-07-05 19:43:54 UTC",
      "updated_date": "2025-07-05 19:43:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:19.117260+00:00"
    },
    {
      "arxiv_id": "2507.04141v1",
      "title": "Pedestrian Intention Prediction via Vision-Language Foundation Models",
      "title_zh": "基于视觉-语言基础模型的行人意图预测",
      "authors": [
        "Mohsen Azarmi",
        "Mahdi Rezaei",
        "He Wang"
      ],
      "abstract": "Prediction of pedestrian crossing intention is a critical function in autonomous vehicles. Conventional vision-based methods of crossing intention prediction often struggle with generalizability, context understanding, and causal reasoning. This study explores the potential of vision-language foundation models (VLFMs) for predicting pedestrian crossing intentions by integrating multimodal data through hierarchical prompt templates. The methodology incorporates contextual information, including visual frames, physical cues observations, and ego-vehicle dynamics, into systematically refined prompts to guide VLFMs effectively in intention prediction. Experiments were conducted on three common datasets-JAAD, PIE, and FU-PIP. Results demonstrate that incorporating vehicle speed, its variations over time, and time-conscious prompts significantly enhances the prediction accuracy up to 19.8%. Additionally, optimised prompts generated via an automatic prompt engineering framework yielded 12.5% further accuracy gains. These findings highlight the superior performance of VLFMs compared to conventional vision-based models, offering enhanced generalisation and contextual understanding for autonomous driving applications.",
      "tldr_zh": "该研究探讨了视觉语言大模型(Vision-Language Foundation Models, VLFMs)在行人过马路意图预测(pedestrian crossing intention prediction)中的应用，旨在解决传统视觉方法在泛化性和上下文理解方面的局限。研究通过层次化提示模板(hierarchical prompt templates)整合了视觉帧、物理线索和本车动力学(ego-vehicle dynamics)等多模态数据，以系统化地引导模型进行精确推理。实验在JAAD、PIE和FU-PIP数据集上证明，引入车速变化和具有时间意识的提示(time-conscious prompts)能将预测准确率提升19.8%。此外，利用自动提示工程(automatic prompt engineering)框架生成的优化提示词进一步获得了12.5%的性能增益。结果表明，相比传统视觉模型，VLFMs在自动驾驶场景中展现出更优越的性能，提供了更强的因果推理能力和环境适应性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04141v1",
      "published_date": "2025-07-05 19:39:00 UTC",
      "updated_date": "2025-07-05 19:39:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:22.007317+00:00"
    },
    {
      "arxiv_id": "2507.04139v2",
      "title": "Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles",
      "title_zh": "Driver-Net：基于多摄像头融合的自动驾驶汽车驾驶员接管就绪度评估",
      "authors": [
        "Mahdi Rezaei",
        "Mohsen Azarmi"
      ],
      "abstract": "Ensuring safe transition of control in automated vehicles requires an accurate and timely assessment of driver readiness. This paper introduces Driver-Net, a novel deep learning framework that fuses multi-camera inputs to estimate driver take-over readiness. Unlike conventional vision-based driver monitoring systems that focus on head pose or eye gaze, Driver-Net captures synchronised visual cues from the driver's head, hands, and body posture through a triple-camera setup. The model integrates spatio-temporal data using a dual-path architecture, comprising a Context Block and a Feature Block, followed by a cross-modal fusion strategy to enhance prediction accuracy. Evaluated on a diverse dataset collected from the University of Leeds Driving Simulator, the proposed method achieves an accuracy of up to 95.8% in driver readiness classification. This performance significantly enhances existing approaches and highlights the importance of multimodal and multi-view fusion. As a real-time, non-intrusive solution, Driver-Net contributes meaningfully to the development of safer and more reliable automated vehicles and aligns with new regulatory mandates and upcoming safety standards.",
      "tldr_zh": "该研究提出了 Driver-Net，一种全新的深度学习框架，旨在通过融合多摄像头输入来评估自动驾驶车辆中驾驶员的接管准备程度(take-over readiness)。与仅关注头部姿态或视线的传统驾驶员监控系统不同，Driver-Net 通过三摄像头设置同步捕捉驾驶员头部、手部和身体姿态的视觉线索。该模型采用包含 Context Block 和 Feature Block 的双路径架构，并利用跨模态融合(cross-modal fusion)策略整合时空数据，以增强预测精度。在 University of Leeds Driving Simulator 数据集上的评估结果显示，该方法在驾驶员准备程度分类中实现了高达 95.8% 的准确率。作为一种实时且非侵入性的解决方案，Driver-Net 的性能显著优于现有方法，证明了多模态与多视图融合在安全控制权切换中的关键作用。该成果为开发更安全、更可靠的自动驾驶系统提供了技术支持，并符合最新的行业安全标准及监管要求。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04139v2",
      "published_date": "2025-07-05 19:27:03 UTC",
      "updated_date": "2025-09-08 14:38:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:26.980552+00:00"
    },
    {
      "arxiv_id": "2507.04136v1",
      "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models",
      "title_zh": "大语言模型强化学习技术综述",
      "authors": [
        "Saksham Sahai Srivastava",
        "Vaneet Aggarwal"
      ],
      "abstract": "Reinforcement Learning (RL) has emerged as a transformative approach for aligning and enhancing Large Language Models (LLMs), addressing critical challenges in instruction following, ethical alignment, and reasoning capabilities. This survey offers a comprehensive foundation on the integration of RL with language models, highlighting prominent algorithms such as Proximal Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally, it provides an extensive technical overview of RL techniques specifically tailored for LLMs, including foundational methods like Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced strategies such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO). We systematically analyze their applications across domains, i.e., from code generation to tool-augmented reasoning. We also present a comparative taxonomy based on reward modeling, feedback mechanisms, and optimization strategies. Our evaluation highlights key trends. RLHF remains dominant for alignment, and outcome-based RL such as RLVR significantly improves stepwise reasoning. However, persistent challenges such as reward hacking, computational costs, and scalable feedback collection underscore the need for continued innovation. We further discuss emerging directions, including hybrid RL algorithms, verifier-guided training, and multi-objective alignment frameworks. This survey serves as a roadmap for researchers advancing RL-driven LLM development, balancing capability enhancement with safety and scalability.",
      "tldr_zh": "该综述论文系统地梳理了强化学习(Reinforcement Learning)在大型语言模型(Large Language Models)对齐与增强中的应用，重点探讨了其在解决指令遵循、伦理对齐和推理能力方面的关键作用。文章不仅涵盖了Proximal Policy Optimization(PPO)和Actor-Critic等基础算法，还深入分析了Reinforcement Learning from Human Feedback(RLHF)、Direct Preference Optimization(DPO)以及Group Relative Policy Optimization(GRPO)等专门针对LLMs开发的进阶技术。通过对代码生成和工具增强推理等领域的应用分析，研究构建了基于奖励建模(reward modeling)和优化策略的分类体系。研究发现RLHF在模型对齐中仍具统治地位，而诸如RLVR等基于结果的强化学习技术显著提升了模型的逐步推理性能。尽管目前仍面临奖励欺骗(reward hacking)和高昂计算成本等挑战，论文提出的验证器引导训练和混合RL算法等前沿方向为实现高性能且安全的LLM开发提供了技术路线图。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, LaTeX source",
      "pdf_url": "https://arxiv.org/pdf/2507.04136v1",
      "published_date": "2025-07-05 19:13:00 UTC",
      "updated_date": "2025-07-05 19:13:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:31.154762+00:00"
    },
    {
      "arxiv_id": "2507.06252v1",
      "title": "False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems",
      "title_zh": "虚警与实损：利用大语言模型对文本型网络威胁情报系统的对抗性攻击",
      "authors": [
        "Samaneh Shafee",
        "Alysson Bessani",
        "Pedro M. Ferreira"
      ],
      "abstract": "Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach that operates in the early phases of the cyber threat lifecycle. CTI involves collecting, processing, and analyzing threat data to provide a more accurate and rapid understanding of cyber threats. Due to the large volume of data, automation through Machine Learning (ML) and Natural Language Processing (NLP) models is essential for effective CTI extraction. These automated systems leverage Open Source Intelligence (OSINT) from sources like social networks, forums, and blogs to identify Indicators of Compromise (IoCs). Although prior research has focused on adversarial attacks on specific ML models, this study expands the scope by investigating vulnerabilities within various components of the entire CTI pipeline and their susceptibility to adversarial attacks. These vulnerabilities arise because they ingest textual inputs from various open sources, including real and potentially fake content. We analyse three types of attacks against CTI pipelines, including evasion, flooding, and poisoning, and assess their impact on the system's information selection capabilities. Specifically, on fake text generation, the work demonstrates how adversarial text generation techniques can create fake cybersecurity and cybersecurity-like text that misleads classifiers, degrades performance, and disrupts system functionality. The focus is primarily on the evasion attack, as it precedes and enables flooding and poisoning attacks within the CTI pipeline.",
      "tldr_zh": "该研究深入探讨了基于文本的网络威胁情报 (Cyber Threat Intelligence, CTI) 系统在面对基于大语言模型 (LLM-based Models) 的对抗性攻击时的脆弱性。研究指出，虽然自动化系统依靠机器学习 (Machine Learning) 和自然语言处理 (Natural Language Processing) 从开源情报 (Open Source Intelligence, OSINT) 中提取攻击指标 (Indicators of Compromise, IoCs)，但这种流程极易受到逃避攻击 (Evasion)、洪泛攻击 (Flooding) 和中毒攻击 (Poisoning) 的威胁。作者重点演示了如何利用 LLM 技术生成虚假的网路安全内容来误导分类器，从而降低系统性能并干扰正常功能。实验结果表明，这种对抗性文本生成能够有效破坏 CTI 管道的信息选择能力，其中逃避攻击作为核心手段，为后续的流程破坏提供了可能。该工作通过对整个情报管道漏洞的系统化分析，为防范 LLM 驱动的对抗性威胁提供了重要的实证研究。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.06252v1",
      "published_date": "2025-07-05 19:00:27 UTC",
      "updated_date": "2025-07-05 19:00:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:55.750269+00:00"
    },
    {
      "arxiv_id": "2507.04123v2",
      "title": "Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge",
      "title_zh": "面向自动驾驶的高效高精度 3D 目标检测：一种边缘侧混合专家计算系统",
      "authors": [
        "Linshen Liu",
        "Boyan Su",
        "Junyue Jiang",
        "Guanlin Wu",
        "Cong Guo",
        "Ceyu Xu",
        "Hao Frank Yang"
      ],
      "abstract": "This paper presents Edge-based Mixture of Experts (MoE) Collaborative Computing (EMC2), an optimal computing system designed for autonomous vehicles (AVs) that simultaneously achieves low-latency and high-accuracy 3D object detection. Unlike conventional approaches, EMC2 incorporates a scenario-aware MoE architecture specifically optimized for edge platforms. By effectively fusing LiDAR and camera data, the system leverages the complementary strengths of sparse 3D point clouds and dense 2D images to generate robust multimodal representations. To enable this, EMC2 employs an adaptive multimodal data bridge that performs multi-scale preprocessing on sensor inputs, followed by a scenario-aware routing mechanism that dynamically dispatches features to dedicated expert models based on object visibility and distance. In addition, EMC2 integrates joint hardware-software optimizations, including hardware resource utilization optimization and computational graph simplification, to ensure efficient and real-time inference on resource-constrained edge devices. Experiments on open-source benchmarks clearly show the EMC2 advancements as an end-to-end system. On the KITTI dataset, it achieves an average accuracy improvement of 3.58% and a 159.06% inference speedup compared to 15 baseline methods on Jetson platforms, with similar performance gains on the nuScenes dataset, highlighting its capability to advance reliable, real-time 3D object detection tasks for AVs. The official implementation is available at https://github.com/LinshenLiu622/EMC2.",
      "tldr_zh": "该研究提出了Edge-based Mixture of Experts Collaborative Computing (EMC2)，一种专为自动驾驶设计的边缘端专家混合(Mixture of Experts)协作计算系统，旨在同时实现低延迟和高精度的3D物体检测。该系统通过融合LiDAR点云和相机图像数据，利用多模态信息的互补性生成鲁棒的表示，并采用自适应多模态数据桥接进行多尺度预处理。EMC2 引入了场景感知路由机制(Scenario-aware routing mechanism)，能够根据物体的可见度和距离动态地将特征分配给特定的专家模型，从而优化计算资源分配。此外，系统通过硬件资源利用优化和计算图简化等软硬件联合优化手段，确保了在资源受限边缘设备上的实时推理。实验表明，在KITTI数据集和Jetson平台上，EMC2 的推理速度比15种基线模型提升了159.06%，平均准确率提高了3.58%，在nuScenes数据集上的优异表现进一步证明了其在提升自动驾驶实时感知任务中的显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04123v2",
      "published_date": "2025-07-05 18:28:04 UTC",
      "updated_date": "2025-07-22 01:29:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:45.974254+00:00"
    },
    {
      "arxiv_id": "2507.04119v1",
      "title": "When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need",
      "title_zh": "当无数据知识蒸馏遇上不可迁移教师：逃离分布外陷阱足矣",
      "authors": [
        "Ziming Hong",
        "Runnan Chen",
        "Zengmao Wang",
        "Bo Han",
        "Bo Du",
        "Tongliang Liu"
      ],
      "abstract": "Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexplored. In this work, we conduct the first investigation into distilling non-transferable learning (NTL) teachers using DFKD, where the transferability from an ID domain to an out-of-distribution (OOD) domain is prohibited. We find that NTL teachers fool DFKD through divert the generator's attention from the useful ID knowledge to the misleading OOD knowledge. This hinders ID knowledge transfer but prioritizes OOD knowledge transfer. To mitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit DFKD by identifying and filtering out OOD-like synthetic samples. Specifically, inspired by the evidence that NTL teachers show stronger adversarial robustness on OOD samples than ID samples, we split synthetic samples into two groups according to their robustness. The fragile group is treated as ID-like data and used for normal knowledge distillation, while the robust group is seen as OOD-like data and utilized for forgetting OOD knowledge. Extensive experiments demonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers. Code is released at https://github.com/tmllab/2025_ICML_ATEsc.",
      "tldr_zh": "该研究首次探讨了无数据知识蒸馏(Data-free knowledge distillation, DFKD)在面对不可迁移学习(Non-transferable learning, NTL)教师模型时的鲁棒性和安全性问题。研究发现，NTL教师模型会通过将生成器的注意力从有用的领域内(In-distribution, ID)知识转移到误导性的分布外(Out-of-distribution, OOD)知识，从而故意阻碍ID知识的有效迁移。为此，作者提出了对抗陷阱逃脱(Adversarial Trap Escaping, ATEsc)方法，旨在通过识别并过滤类似OOD的合成样本来优化DFKD过程。该方法利用NTL教师在OOD样本上表现出比ID样本更强的对抗鲁棒性这一特性，将合成样本分为两组进行差异化处理。其中，脆弱组被视为类ID数据用于正常的知识蒸馏，而鲁棒组则被视为类OOD数据并用于执行遗忘机制。实验结果证明，ATEsc在应对NTL教师模型时能显著提升DFKD的蒸馏性能并有效逃脱OOD陷阱。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04119v1",
      "published_date": "2025-07-05 18:03:52 UTC",
      "updated_date": "2025-07-05 18:03:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:49.329655+00:00"
    },
    {
      "arxiv_id": "2507.04106v2",
      "title": "Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning",
      "title_zh": "应对无样本持续学习中单任务数据投毒的破坏性影响",
      "authors": [
        "Stanisław Pawlak",
        "Bartłomiej Twardowski",
        "Tomasz Trzciński",
        "Joost van de Weijer"
      ],
      "abstract": "Our research addresses the overlooked security concerns related to data poisoning in continual learning (CL). Data poisoning - the intentional manipulation of training data to affect the predictions of machine learning models - was recently shown to be a threat to CL training stability. While existing literature predominantly addresses scenario-dependent attacks, we propose to focus on a more simple and realistic single-task poison (STP) threats. In contrast to previously proposed poisoning settings, in STP adversaries lack knowledge and access to the model, as well as to both previous and future tasks. During an attack, they only have access to the current task within the data stream. Our study demonstrates that even within these stringent conditions, adversaries can compromise model performance using standard image corruptions. We show that STP attacks are able to strongly disrupt the whole continual training process: decreasing both the stability (its performance on past tasks) and plasticity (capacity to adapt to new tasks) of the algorithm. Finally, we propose a high-level defense framework for CL along with a poison task detection method based on task vectors. The code is available at https://github.com/stapaw/STP.git .",
      "tldr_zh": "本研究探讨了无范例持续学习（Exemplar-Free Continual Learning）中长期被忽视的数据投毒（Data Poisoning）安全问题，并提出了一种更具现实意义的单任务投毒（Single-Task Poison, STP）威胁模型。在STP设定下，攻击者无需预知模型信息或访问其他任务，仅需对当前任务数据进行标准图像损坏处理即可显著破坏训练过程。实验表明，该攻击能同时降低模型的稳定性（Stability）和塑性（Plasticity），导致整个持续学习系统的性能崩溃。为此，研究者提出了一个高层级防御框架，并设计了基于任务向量（Task Vectors）的投毒检测方法以增强系统的安全性。该项工作不仅揭示了持续学习在受限攻击条件下的脆弱性，还通过开源代码为后续鲁棒性研究提供了重要参考。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at CoLLAs 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04106v2",
      "published_date": "2025-07-05 17:26:52 UTC",
      "updated_date": "2025-08-10 07:06:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:51.839146+00:00"
    },
    {
      "arxiv_id": "2507.04105v1",
      "title": "Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing",
      "title_zh": "通过随机平滑增强 LLM 驱动的多智能体系统的鲁棒性",
      "authors": [
        "Jinwei Hu",
        "Yi Dong",
        "Zhengtao Ding",
        "Xiaowei Huang"
      ],
      "abstract": "This paper presents a defense framework for enhancing the safety of large language model (LLM) empowered multi-agent systems (MAS) in safety-critical domains such as aerospace. We apply randomized smoothing, a statistical robustness certification technique, to the MAS consensus context, enabling probabilistic guarantees on agent decisions under adversarial influence. Unlike traditional verification methods, our approach operates in black-box settings and employs a two-stage adaptive sampling mechanism to balance robustness and computational efficiency. Simulation results demonstrate that our method effectively prevents the propagation of adversarial behaviors and hallucinations while maintaining consensus performance. This work provides a practical and scalable path toward safe deployment of LLM-based MAS in real-world, high-stakes environments.",
      "tldr_zh": "该研究针对航空航天等安全关键领域，提出了一种增强大语言模型(LLM)驱动的多智能体系统(MAS)安全性的防御框架。研究团队将随机平滑(Randomized Smoothing)这一统计鲁棒性认证技术应用于多智能体系统的一致性(Consensus)场景，从而在对抗性干扰下为智能体决策提供概率性保证。该方法适用于黑盒(Black-box)环境，并引入两阶段自适应采样机制以平衡系统的Robustness与计算效率。仿真结果证明，该框架在维持共识性能的同时，能有效抑制对抗性行为和幻觉(Hallucinations)的传播。该工作为在真实高风险任务中安全部署基于LLM的多智能体系统提供了一条实用且具备可扩展性(Scalability)的路径。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint accepted by Chinese Journal of Aeronautics",
      "pdf_url": "https://arxiv.org/pdf/2507.04105v1",
      "published_date": "2025-07-05 17:26:08 UTC",
      "updated_date": "2025-07-05 17:26:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:58.753126+00:00"
    },
    {
      "arxiv_id": "2507.04103v3",
      "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
      "title_zh": "如何训练你的 LLM Web 智能体：一项统计诊断研究",
      "authors": [
        "Dheeraj Vattikonda",
        "Santhoshi Ravichandran",
        "Emiliano Penaloza",
        "Hadi Nekoei",
        "Megh Thakkar",
        "Thibault Le Sellier de Chezelles",
        "Nicolas Gontier",
        "Miguel Muñoz-Mármol",
        "Sahar Omidi Shayegan",
        "Stefania Raimondo",
        "Xue Liu",
        "Alexandre Drouin",
        "Laurent Charlin",
        "Alexandre Piché",
        "Alexandre Lacoste",
        "Massimo Caccia"
      ],
      "abstract": "LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models.",
      "tldr_zh": "这项研究针对开源 LLM web agents 在多步交互复杂性和 post-training 计算成本方面面临的挑战，提供了首个关于计算分配的统计学诊断研究。该方法采用两阶段流水线，首先通过监督微调 (SFT) 让 Llama 3.1 8B 学生模型模仿 Llama 3.3 70B 教师模型，随后进行 on-policy 强化学习 (RL)。考虑到训练过程对超参数选择高度敏感，研究团队通过对 1,370 种配置进行采样并利用 bootstrapping 技术估算有效超参数，有效降低了试错成本。实验结果表明，在 WorkArena 和 MiniWob++ 基准测试中，SFT 与 on-policy RL 的结合性能始终优于单一方法。该策略在 MiniWob++ 上仅需 55% 的计算量即可匹配纯 SFT 的峰值表现，成功推进了计算-性能的帕累托前沿 (Pareto frontier)。这一研究成果为弥合开源与闭源 web 智能体之间的差距提供了关键的训练策略和统计依据。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04103v3",
      "published_date": "2025-07-05 17:12:33 UTC",
      "updated_date": "2025-11-02 19:03:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:37:59.255171+00:00"
    },
    {
      "arxiv_id": "2507.04100v1",
      "title": "Hierarchical Testing with Rabbit Optimization for Industrial Cyber-Physical Systems",
      "title_zh": "面向工业信息物理系统的兔子优化分层测试",
      "authors": [
        "Jinwei Hu",
        "Zezhi Tang",
        "Xin Jin",
        "Benyuan Zhang",
        "Yi Dong",
        "Xiaowei Huang"
      ],
      "abstract": "This paper presents HERO (Hierarchical Testing with Rabbit Optimization), a novel black-box adversarial testing framework for evaluating the robustness of deep learning-based Prognostics and Health Management systems in Industrial Cyber-Physical Systems. Leveraging Artificial Rabbit Optimization, HERO generates physically constrained adversarial examples that align with real-world data distributions via global and local perspective. Its generalizability ensures applicability across diverse ICPS scenarios. This study specifically focuses on the Proton Exchange Membrane Fuel Cell system, chosen for its highly dynamic operational conditions, complex degradation mechanisms, and increasing integration into ICPS as a sustainable and efficient energy solution. Experimental results highlight HERO's ability to uncover vulnerabilities in even state-of-the-art PHM models, underscoring the critical need for enhanced robustness in real-world applications. By addressing these challenges, HERO demonstrates its potential to advance more resilient PHM systems across a wide range of ICPS domains.",
      "tldr_zh": "该研究提出了HERO（Hierarchical Testing with Rabbit Optimization），这是一个针对工业信息物理系统（Industrial Cyber-Physical Systems, ICPS）中深度学习故障预测与健康管理（Prognostics and Health Management, PHM）系统的黑盒对抗性测试框架。该框架利用人工兔优化（Artificial Rabbit Optimization）算法，从全局和局部视角生成符合物理约束且贴合真实数据分布的对抗样本，具备极强的通用性。研究特别以质子交换膜燃料电池（Proton Exchange Membrane Fuel Cell, PEMFC）系统为案例，分析其在动态运行和复杂衰减环境下的表现。实验结果显示，HERO能够有效揭示现有顶尖PHM模型的鲁棒性漏洞，强调了在实际应用中增强系统防御能力的紧迫性。通过应对这些安全挑战，该研究为在广泛的ICPS领域构建更具韧性的监控与管理系统奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint accepted by IEEE Transactions on Industrial Cyber Physical Systems",
      "pdf_url": "https://arxiv.org/pdf/2507.04100v1",
      "published_date": "2025-07-05 16:59:57 UTC",
      "updated_date": "2025-07-05 16:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:02.213311+00:00"
    },
    {
      "arxiv_id": "2507.04099v2",
      "title": "Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching",
      "title_zh": "Conversation Forests：多轮医学对话大语言模型微调的关键在于分支架构",
      "authors": [
        "Thomas Savage"
      ],
      "abstract": "Fine-tuning methods such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) have demonstrated success in training large language models (LLMs) for single-turn tasks. However, these methods fall short in multi-turn applications, such as diagnostic patient interviewing, where understanding how early conversational turns influence downstream completions and outcomes is essential. In medicine, a multi-turn perspective is critical for learning diagnostic schemas and better understanding conversation dynamics. To address this gap, I introduce Savage Conversation Forests (SCF), a reinforcement learning framework that leverages a branched conversation architecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple possible conversation continuations at each turn, enabling the model to learn how different early responses affect downstream interactions and diagnostic outcomes. In experiments simulating doctor-patient conversations, SCF with branching outperforms linear conversation architectures on diagnostic accuracy. I hypothesize that SCF's improvements stem from its ability to provide richer, interdependent training signals across conversation turns. These results suggest that a branched training architecture is an important strategy for fine tuning LLMs in complex multi-turn conversational tasks.",
      "tldr_zh": "本研究针对Direct Preference Optimization (DPO)和Group Relative Policy Optimization (GRPO)等微调方法在多轮医疗对话中无法有效捕捉早期对话对后续结果影响的局限性，提出了Savage Conversation Forests (SCF)框架。SCF是一种基于强化学习(Reinforcement Learning)的架构，通过分支对话设计在每一轮生成多种可能的后续走向，使大语言模型(LLMs)能够学习早期响应对下游交互及诊断结果的深远影响。在医患对话模拟实验中，SCF的分支架构在诊断准确率(diagnostic accuracy)上显著优于传统的线性对话架构。研究结果证明，这种分支训练架构能为模型提供更丰富且相互依赖的训练信号，是提升复杂多轮对话任务微调效果的关键策略。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04099v2",
      "published_date": "2025-07-05 16:49:34 UTC",
      "updated_date": "2025-07-15 16:49:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:04.119127+00:00"
    },
    {
      "arxiv_id": "2507.04095v1",
      "title": "Human-centered AI with focus on Human-robot interaction (Book chapter)",
      "title_zh": "以人为本的人工智能：聚焦人机交互（书籍章节）",
      "authors": [
        "Alireza Mortezapour",
        "Giuliana Vitiello"
      ],
      "abstract": "Modern social robots can be considered the descendants of steam engines from the First Industrial Revolution (IR 1.0) and industrial robotic arms from the Third Industrial Revolution (IR 3.0). As some time has passed since the introduction of these robots during the Fourth Industrial Revolution (IR 4.0), challenges and issues in their interaction with humans have emerged, leading researchers to conclude that, like any other AI-based technology, these robots must also be human-centered to meet the needs of their users. This chapter aims to introduce humans and their needs in interactions with robots, ranging from short-term, one-on-one interactions (micro-level) to long-term, macro-level needs at the societal scale. Building upon the principles of human-centered AI, this chapter presents, for the first time, a new framework of human needs called the Dual Pyramid. This framework encompasses a comprehensive list of human needs in robot interactions, from the most fundamental, robot effectiveness to macro level requirements, such as the collaboration with robots in achieving the United Nations 17 Sustainable Development Goals.",
      "tldr_zh": "该研究探讨了在第四次工业革命(IR 4.0)背景下，社交机器人在与人类交互(Human-robot interaction)过程中面临的挑战，强调机器人技术必须遵循以人为本的人工智能(Human-centered AI)原则以满足用户需求。本章系统地介绍了人类在与机器人互动时的多层次需求，范围从短期的一对一微观层面(micro-level)交互，延伸至社会尺度的宏观层面(macro-level)需求。基于这些原则，研究首次提出了一个名为“双金字塔”(Dual Pyramid)的新型人类需求框架，旨在全面捕捉机器人交互中的核心要素。该框架涵盖了从最基础的机器人有效性(robot effectiveness)到更高层面的社会责任，例如协作实现联合国17个可持续发展目标(Sustainable Development Goals)。通过整合微观与宏观视角，该研究为未来设计更具包容性、符合人类价值观的智能机器人系统提供了重要的理论指导与实践框架。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04095v1",
      "published_date": "2025-07-05 16:45:03 UTC",
      "updated_date": "2025-07-05 16:45:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:07.441758+00:00"
    },
    {
      "arxiv_id": "2507.04094v2",
      "title": "MMMOS: Multi-domain Multi-axis Audio Quality Assessment",
      "title_zh": "MMMOS：多领域多轴音频质量评估",
      "authors": [
        "Yi-Cheng Lin",
        "Jia-Hung Chen",
        "Hung-yi Lee"
      ],
      "abstract": "Accurate audio quality estimation is essential for developing and evaluating audio generation, retrieval, and enhancement systems. Existing non-intrusive assessment models predict a single Mean Opinion Score (MOS) for speech, merging diverse perceptual factors and failing to generalize beyond speech. We propose MMMOS, a no-reference, multi-domain audio quality assessment system that estimates four orthogonal axes: Production Quality, Production Complexity, Content Enjoyment, and Content Usefulness across speech, music, and environmental sounds. MMMOS fuses frame-level embeddings from three pretrained encoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with four loss functions. By ensembling the top eight models, MMMOS shows a 20-30% reduction in mean squared error and a 4-5% increase in Kendall's τ versus baseline, gains first place in six of eight Production Complexity metrics, and ranks among the top three on 17 of 32 challenge metrics.",
      "tldr_zh": "该研究提出了MMMOS，一种针对语音、音乐和环境音的无参考、多领域音频质量评估系统，旨在解决现有模型仅预测单一Mean Opinion Score (MOS)且难以推广到非语音领域的问题。该系统定义了Production Quality、Production Complexity、Content Enjoyment和Content Usefulness四个正交维度的评估指标，以实现更全面的感知因素衡量。在方法论上，MMMOS融合了来自WavLM、MuQ和M2D三个预训练编码器的帧级嵌入特征，并对三种聚合策略及四种损失函数进行了深入评估。实验结果显示，通过集成表现最优的八个模型，MMMOS在Mean Squared Error上比基线模型降低了20-30%，且Kendall's τ系数提升了4-5%。此外，MMMOS在多项挑战赛指标中名列前茅，证明了其在音频生成、检索及增强系统评价中的优越性和泛化能力。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "4 pages including 1 page of reference. Accepted by ASRU Audio MOS 2025 Challenge",
      "pdf_url": "https://arxiv.org/pdf/2507.04094v2",
      "published_date": "2025-07-05 16:42:09 UTC",
      "updated_date": "2026-01-10 08:04:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:10.581654+00:00"
    },
    {
      "arxiv_id": "2507.10563v1",
      "title": "A Biomimetic Way for Coral-Reef-Inspired Swarm Intelligence for Carbon-Neutral Wastewater Treatment",
      "title_zh": "面向碳中和污水处理的珊瑚礁启发式群体智能仿生方法",
      "authors": [
        "Antonis Messinis"
      ],
      "abstract": "With increasing wastewater rates, achieving energy-neutral purification is challenging. We introduce a coral-reef-inspired Swarm Interaction Network for carbon-neutral wastewater treatment, combining morphogenetic abstraction with multi-task carbon awareness. Scalability stems from linear token complexity, mitigating the energy-removal problem. Compared with seven baselines, our approach achieves 96.7\\% removal efficiency, 0.31~kWh~m$^{-3}$ energy consumption, and 14.2~g~m$^{-3}$ CO$_2$ emissions. Variance analysis demonstrates robustness under sensor drift. Field scenarios--insular lagoons, brewery spikes, and desert greenhouses--show potential diesel savings of up to 22\\%. However, data-science staffing remains an impediment. Future work will integrate AutoML wrappers within the project scope, although governance restrictions pose interpretability challenges that require further visual analytics.",
      "tldr_zh": "该研究受珊瑚礁启发提出了一种 Swarm Interaction Network，旨在实现碳中和的污水处理。该方法将 morphogenetic abstraction 与 multi-task carbon awareness 相结合，利用 linear token complexity 提升了系统的可扩展性并缓解了能源移除难题。实验结果显示，与七种基准方法相比，该方案达到了 96.7% 的去除效率，能耗仅为 0.31 kWh m⁻³，CO₂ 排放量降至 14.2 g m⁻³。方差分析验证了系统在 sensor drift 干扰下的鲁棒性，且在海岛潟湖、啤酒厂和沙漠温室等实地场景中表现出高达 22% 的柴油节省潜力。尽管目前仍面临数据科学人才短缺和 governance 限制带来的 interpretability 挑战，但未来通过集成 AutoML 技术有望进一步优化该系统的应用。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.10563v1",
      "published_date": "2025-07-05 16:19:42 UTC",
      "updated_date": "2025-07-05 16:19:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:21.881556+00:00"
    },
    {
      "arxiv_id": "2507.04075v1",
      "title": "Accurate and Efficient World Modeling with Masked Latent Transformers",
      "title_zh": "基于掩码潜空间 Transformer 的精准高效世界建模",
      "authors": [
        "Maxime Burchi",
        "Radu Timofte"
      ],
      "abstract": "The Dreamer algorithm has recently obtained remarkable performance across diverse environment domains by training powerful agents with simulated trajectories. However, the compressed nature of its world model's latent space can result in the loss of crucial information, negatively affecting the agent's performance. Recent approaches, such as $Δ$-IRIS and DIAMOND, address this limitation by training more accurate world models. However, these methods require training agents directly from pixels, which reduces training efficiency and prevents the agent from benefiting from the inner representations learned by the world model. In this work, we propose an alternative approach to world modeling that is both accurate and efficient. We introduce EMERALD (Efficient MaskEd latent tRAnsformer worLD model), a world model using a spatial latent state with MaskGIT predictions to generate accurate trajectories in latent space and improve the agent performance. On the Crafter benchmark, EMERALD achieves new state-of-the-art performance, becoming the first method to surpass human experts performance within 10M environment steps. Our method also succeeds to unlock all 22 Crafter achievements at least once during evaluation.",
      "tldr_zh": "该研究针对 Dreamer 算法中世界模型(world model)潜空间(latent space)压缩导致的信息丢失问题，以及现有方法 $\\Delta$-IRIS 和 DIAMOND 在训练效率上的不足，提出了名为 EMERALD (Efficient MaskEd latent tRAnsformer worLD model) 的高效世界模型。该架构通过引入空间潜状态(spatial latent state)并结合 MaskGIT 预测技术，在潜空间内生成精确的轨迹，使智能体能够有效利用模型学习到的内部表示。实验结果显示，EMERALD 在 Crafter 基准测试中达到了新的 state-of-the-art 水平，是首个在 10M environment steps 内超越人类专家表现的方法。此外，该模型在评估中成功解锁了全部 22 项 Crafter 成就，证明了其在复杂环境建模与决策任务中的卓越性能和效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04075v1",
      "published_date": "2025-07-05 15:49:21 UTC",
      "updated_date": "2025-07-05 15:49:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:24.775441+00:00"
    },
    {
      "arxiv_id": "2507.04069v1",
      "title": "Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering",
      "title_zh": "超越独立段落：面向检索增强开放域问答的自适应段落组合检索",
      "authors": [
        "Ting-Wen Ko",
        "Jyun-Yu Jiang",
        "Pu-Jen Cheng"
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external documents at inference time, enabling up-to-date knowledge access without costly retraining. However, conventional RAG methods retrieve passages independently, often leading to redundant, noisy, or insufficiently diverse context-particularly problematic - particularly problematic in noisy corpora and for multi-hop questions. To address this, we propose Adaptive Passage Combination Retrieval (AdaPCR), a novel framework for open-domain question answering with black-box LMs. AdaPCR explicitly models dependencies between passages by considering passage combinations as units for retrieval and reranking. It consists of a context-aware query reformulation using concatenated passages, and a reranking step trained with a predictive objective aligned with downstream answer likelihood. Crucially, AdaPCR adaptively selects the number of retrieved passages without additional stopping modules. Experiments across several QA benchmarks show that AdaPCR outperforms baselines, particularly in multi-hop reasoning, demonstrating the effectiveness of modeling inter-passage dependencies for improved retrieval.",
      "tldr_zh": "该研究针对检索增强生成(RAG)中传统方法独立检索段落导致冗余、噪声及多样性不足的问题，提出了一种名为Adaptive Passage Combination Retrieval (AdaPCR)的新型框架。AdaPCR通过将段落组合视为检索和重排序的单元，显式地建模了段落之间的依赖关系。该框架包含利用拼接段落进行的上下文感知查询重构(context-aware query reformulation)，以及一个根据下游答案可能性进行预测目标训练的重排序步骤。此外，AdaPCR能够自适应地选择检索段落的数量，而无需额外的停止模块。在多个问答(QA)基准测试上的实验结果表明，AdaPCR在性能上优于基线模型，特别是在多跳推理(multi-hop reasoning)任务中表现突出。这一研究结果充分证明了在开放域问答中建模段落间依赖关系对于提升检索质量和系统性能的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04069v1",
      "published_date": "2025-07-05 15:10:12 UTC",
      "updated_date": "2025-07-05 15:10:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:26.229882+00:00"
    },
    {
      "arxiv_id": "2507.04067v1",
      "title": "HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration",
      "title_zh": "HAWK：面向多智能体协作的分层工作流框架",
      "authors": [
        "Yuyang Cheng",
        "Yumiao Xu",
        "Chaojia Yu",
        "Yong Zhao"
      ],
      "abstract": "Contemporary multi-agent systems encounter persistent challenges in cross-platform interoperability, dynamic task scheduling, and efficient resource sharing. Agents with heterogeneous implementations often lack standardized interfaces; collaboration frameworks remain brittle and hard to extend; scheduling policies are static; and inter-agent state synchronization is insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular framework comprising five layers-User, Workflow, Operator, Agent, and Resource-and supported by sixteen standardized interfaces. HAWK delivers an end-to-end pipeline covering task parsing, workflow orchestration, intelligent scheduling, resource invocation, and data synchronization. At its core lies an adaptive scheduling and optimization module in the Workflow Layer, which harnesses real-time feedback and dynamic strategy adjustment to maximize utilization. The Resource Layer provides a unified abstraction over heterogeneous data sources, large models, physical devices, and third-party services&tools, simplifying cross-domain information retrieval. We demonstrate HAWK's scalability and effectiveness via CreAgentive, a multi-agent novel-generation prototype, which achieves marked gains in throughput, lowers invocation complexity, and improves system controllability. We also show how hybrid deployments of large language models integrate seamlessly within HAWK, highlighting its flexibility. Finally, we outline future research avenues-hallucination mitigation, real-time performance tuning, and enhanced cross-domain adaptability-and survey prospective applications in healthcare, government, finance, and education.",
      "tldr_zh": "该研究提出了HAWK (Hierarchical Agent Workflow)，一种旨在解决多智能体系统在跨平台互操作性、动态任务调度和高效资源共享等方面挑战的模块化框架。该框架由User、Workflow、Operator、Agent和Resource五个层级组成，并由16个标准化接口支撑，实现了从任务解析到数据同步的端到端流程。其核心在于Workflow Layer中的自适应调度与优化模块，通过实时反馈和动态策略调整来最大化资源利用率。Resource Layer通过对异构数据源、Large Language Models及物理设备进行统一抽象，简化了跨领域信息调用的复杂性。通过多智能体小说生成原型CreAgentive的验证，HAWK证明了其在提升吞吐量(throughput)、降低调用难度及增强系统可控性方面的显著优势。此外，该框架支持大语言模型的混合部署，展现了极高的灵活性与可扩展性。未来研究将聚焦于幻觉缓解(hallucination mitigation)、实时性能调优以及在医疗、金融等领域的应用探索。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "AgentIR@SIGIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04067v1",
      "published_date": "2025-07-05 15:03:53 UTC",
      "updated_date": "2025-07-05 15:03:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:41.184934+00:00"
    },
    {
      "arxiv_id": "2507.04062v1",
      "title": "Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic",
      "title_zh": "融合动作转换与动作特征记忆的随机人体动作预测",
      "authors": [
        "Jianwei Tang",
        "Hong Yang",
        "Tengyue Chen",
        "Jian-Fang Hu"
      ],
      "abstract": "Action-driven stochastic human motion prediction aims to generate future motion sequences of a pre-defined target action based on given past observed sequences performing non-target actions. This task primarily presents two challenges. Firstly, generating smooth transition motions is hard due to the varying transition speeds of different actions. Secondly, the action characteristic is difficult to be learned because of the similarity of some actions. These issues cause the predicted results to be unreasonable and inconsistent. As a result, we propose two memory banks, the Soft-transition Action Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems above. The STAB stores the action transition information. It is equipped with the novel soft searching approach, which encourages the model to focus on multiple possible action categories of observed motions. The ACB records action characteristic, which produces more prior information for predicting certain actions. To fuse the features retrieved from the two banks better, we further propose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments on four motion prediction datasets demonstrate that our approach consistently outperforms the previous state-of-the-art. The demo and code are available at https://hyqlat.github.io/STABACB.github.io/.",
      "tldr_zh": "该研究针对动作驱动的随机人体运动预测(Stochastic Human Motion Prediction)任务，提出了包含Soft-transition Action Bank (STAB)和Action Characteristic Bank (ACB)的双存储库框架。为了解决因动作转换速度多变而难以生成平滑过渡的问题，STAB通过存储动作转换信息并采用新型的软搜索(soft searching)方法，引导模型关注观察序列中多种可能的动作类别。同时，ACB通过记录动作特征(Action Characteristic)为特定动作的预测提供更丰富的先验信息，有效应对了相似动作难以区分的挑战。此外，研究还提出了自适应注意力调节(Adaptive Attention Adjustment, AAA)策略来优化两个存储库特征的融合过程。在四个运动预测数据集上的实验结果证明，该方法在生成合理且一致的运动序列方面显著优于现有的最先进技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by CVPR2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04062v1",
      "published_date": "2025-07-05 14:57:37 UTC",
      "updated_date": "2025-07-05 14:57:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:35.141658+00:00"
    },
    {
      "arxiv_id": "2507.04060v1",
      "title": "Temporal Continual Learning with Prior Compensation for Human Motion Prediction",
      "title_zh": "结合先验补偿的时序持续学习人体运动预测",
      "authors": [
        "Jianwei Tang",
        "Jiangxin Sun",
        "Xiaotong Lin",
        "Lifang Zhang",
        "Wei-Shi Zheng",
        "Jian-Fang Hu"
      ],
      "abstract": "Human Motion Prediction (HMP) aims to predict future poses at different moments according to past motion sequences. Previous approaches have treated the prediction of various moments equally, resulting in two main limitations: the learning of short-term predictions is hindered by the focus on long-term predictions, and the incorporation of prior information from past predictions into subsequent predictions is limited. In this paper, we introduce a novel multi-stage training framework called Temporal Continual Learning (TCL) to address the above challenges. To better preserve prior information, we introduce the Prior Compensation Factor (PCF). We incorporate it into the model training to compensate for the lost prior information. Furthermore, we derive a more reasonable optimization objective through theoretical derivation. It is important to note that our TCL framework can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on four HMP benchmark datasets demonstrate the effectiveness and flexibility of TCL. The code is available at https://github.com/hyqlat/TCL.",
      "tldr_zh": "该研究针对人体动作预测(Human Motion Prediction)中不同时刻预测权重均等导致短时预测受阻及先验信息利用不足的问题，提出了名为时序持续学习(Temporal Continual Learning, TCL)的新型多阶段训练框架。为了更好地保留并整合过往预测的先验知识，该研究引入了先验补偿因子(Prior Compensation Factor, PCF)并将其纳入模型训练以弥补信息损失。同时，研究者通过理论推导建立了一个更合理的优化目标，显著提升了模型在不同预测阶段的性能。TCL框架具有良好的通用性和灵活性，能够无缝集成到各种骨干模型中并适应不同的数据集。在四个主流基准数据集上的实验结果证明了TCL在提升动作预测准确性方面的显著有效性，为解决长短时预测冲突提供了新的理论支撑。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Advances in Neural Information Processing Systems 2023",
      "pdf_url": "https://arxiv.org/pdf/2507.04060v1",
      "published_date": "2025-07-05 14:48:30 UTC",
      "updated_date": "2025-07-05 14:48:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:55.637283+00:00"
    },
    {
      "arxiv_id": "2507.04059v1",
      "title": "Attributing Data for Sharpness-Aware Minimization",
      "title_zh": "针对锐度感知最小化的数据归因",
      "authors": [
        "Chenyang Ren",
        "Yifan Jia",
        "Huanyi Xie",
        "Zhaobin Xu",
        "Tianxing Wei",
        "Liangyu Wang",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Sharpness-aware Minimization (SAM) improves generalization in large-scale model training by linking loss landscape geometry to generalization. However, challenges such as mislabeled noisy data and privacy concerns have emerged as significant issues. Data attribution, which identifies the contributions of specific training samples, offers a promising solution. However, directly rendering existing data influence evaluation tools such as influence functions (IF) to SAM will be inapplicable or inaccurate as SAM utilizes an inner loop to find model perturbations that maximize loss, which the outer loop then minimizes, resulting in a doubled computational structure. Additionally, this bilevel structure complicates the modeling of data influence on the parameters. In this paper, based on the IF, we develop two innovative data valuation methods for SAM, each offering unique benefits in different scenarios: the Hessian-based IF and the Gradient Trajectory-based IF. The first one provides a comprehensive estimation of data influence using a closed-form measure that relies only on the trained model weights. In contrast, the other IF for SAM utilizes gradient trajectory information during training for more accurate and efficient data assessment. Extensive experiments demonstrate their effectiveness in data evaluation and parameter tuning, with applications in identifying mislabeled data, model editing, and enhancing interpretability.",
      "tldr_zh": "该研究针对 Sharpness-aware Minimization (SAM) 由于其双层优化（bilevel structure）导致传统数据归因（Data attribution）方法难以适用的挑战，提出了两种基于影响函数 (Influence Functions, IF) 的创新数据估值方法。作者开发的 Hessian-based IF 仅利用训练后的模型权重通过闭式解 (closed-form measure) 提供全面的数据影响评估，而 Gradient Trajectory-based IF 则结合训练过程中的梯度轨迹信息以提升评估的准确性与效率。大量实验表明，这些方法在识别错误标注数据 (mislabeled data)、执行模型编辑 (model editing) 及增强模型可解释性方面表现出色。该项工作成功解决了在 SAM 框架下建模数据对参数影响的难题，为进一步优化大规模模型的泛化性能和参数调优提供了有效工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.04059v1",
      "published_date": "2025-07-05 14:46:42 UTC",
      "updated_date": "2025-07-05 14:46:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:45.387343+00:00"
    },
    {
      "arxiv_id": "2507.04055v2",
      "title": "Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG",
      "title_zh": "LLMs 与 RAG 时代下基于字符串的恶意软件家族分类的重新审视与探索",
      "authors": [
        "Yufan Chen",
        "Daoyuan Wu",
        "Juantao Zhong",
        "Zicheng Zhang",
        "Debin Gao",
        "Shuai Wang",
        "Yingjiu Li",
        "Ning Liu",
        "Jiachi Chen",
        "Rocky K. C. Chang"
      ],
      "abstract": "Malware family classification aims to identify the specific family (e.g., GuLoader or BitRAT) a malware sample may belong to, in contrast to malware detection or sample classification, which only predicts a Yes/No outcome. Accurate family identification can greatly facilitate automated sample labeling and understanding on crowdsourced malware analysis platforms such as VirusTotal and MalwareBazaar, which generate vast amounts of data daily. In this paper, we explore and assess the feasibility of using traditional binary string features for family classification in the new era of large language models (LLMs) and Retrieval-Augmented Generation (RAG). Specifically, we investigate howFamily-Specific String (FSS) features can be utilized in a manner similar to RAG to facilitate family classification. To this end, we develop a curated evaluation framework covering 4,347 samples from 67 malware families, extract and analyze over 25 million strings, and conduct detailed ablation studies to assess the impact of different design choices in four major modules, with each providing a relative improvement ranging from 8.1% to 120%.",
      "tldr_zh": "该研究探讨了在大语言模型 (LLMs) 和检索增强生成 (RAG) 时代背景下，利用传统二进制字符串特征进行恶意软件家族分类的可行性。研究者重点研究了如何以类似于 RAG 的方式利用家族特定字符串 (Family-Specific String, FSS) 特征，旨在提升对恶意软件特定家族（如 GuLoader 或 BitRAT）的识别精度。为此，该研究构建了一个涵盖 67 个恶意软件家族、4,347 个样本的评估框架，并从超过 2,500 万条字符串中提取并分析了关键特征。详细的消融实验表明，该方法在四个主要模块中的设计优化均带来了显著的性能提升，相对改进幅度介于 8.1% 到 120% 之间。这一成果不仅证明了传统字符串特征在现代 AI 架构下的有效性，也为 VirusTotal 等平台的海量恶意软件自动化标注与分析提供了重要支持。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "This is a technical report from Lingnan University, Hong Kong. Code is available at https://github.com/AIS2Lab/MalwareGPT",
      "pdf_url": "https://arxiv.org/pdf/2507.04055v2",
      "published_date": "2025-07-05 14:36:13 UTC",
      "updated_date": "2025-10-26 15:01:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:45.114004+00:00"
    },
    {
      "arxiv_id": "2507.04053v1",
      "title": "TopoMAS: Large Language Model Driven Topological Materials Multiagent System",
      "title_zh": "TopoMAS：大语言模型驱动的拓扑材料多智能体系统",
      "authors": [
        "Baohua Zhang",
        "Xin Li",
        "Huangchao Xu",
        "Zhong Jin",
        "Quansheng Wu",
        "Ce Li"
      ],
      "abstract": "Topological materials occupy a frontier in condensed-matter physics thanks to their remarkable electronic and quantum properties, yet their cross-scale design remains bottlenecked by inefficient discovery workflows. Here, we introduce TopoMAS (Topological materials Multi-Agent System), an interactive human-AI framework that seamlessly orchestrates the entire materials-discovery pipeline: from user-defined queries and multi-source data retrieval, through theoretical inference and crystal-structure generation, to first-principles validation. Crucially, TopoMAS closes the loop by autonomously integrating computational outcomes into a dynamic knowledge graph, enabling continuous knowledge refinement. In collaboration with human experts, it has already guided the identification of novel topological phases SrSbO3, confirmed by first-principles calculations. Comprehensive benchmarks demonstrate robust adaptability across base Large Language Model, with the lightweight Qwen2.5-72B model achieving 94.55% accuracy while consuming only 74.3-78.4% of tokens required by Qwen3-235B and 83.0% of DeepSeek-V3's usage--delivering responses twice as fast as Qwen3-235B. This efficiency establishes TopoMAS as an accelerator for computation-driven discovery pipelines. By harmonizing rational agent orchestration with a self-evolving knowledge graph, our framework not only delivers immediate advances in topological materials but also establishes a transferable, extensible paradigm for materials-science domain.",
      "tldr_zh": "该研究引入了 TopoMAS (Topological materials Multi-Agent System)，这是一个由大语言模型 (Large Language Model) 驱动的交互式人类-人工智能框架，旨在解决拓扑材料发现过程中跨尺度设计的效率瓶颈。该系统能够无缝编排从用户查询、多源数据检索到理论推理、晶体结构生成以及第一性原理验证 (first-principles validation) 的完整材料发现流程。TopoMAS 的核心优势在于通过自主将计算结果整合进动态知识图谱 (dynamic knowledge graph) 来实现闭环管理，确保了知识的持续优化与精炼。在实际应用中，该系统已成功引导专家识别出新型拓扑相 SrSbO3 并得到计算确认。基准测试表明，轻量级的 Qwen2.5-72B 模型在该框架下达到了 94.55% 的准确率，且在 Token 消耗和响应速度上显著优于 Qwen3-235B 和 DeepSeek-V3。该框架通过合理的智能体编排 (agent orchestration) 与自我演变知识图谱的结合，为材料科学领域提供了一个可迁移且可扩展的计算驱动发现范式。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "13 pages,7 figures,3 tables",
      "pdf_url": "https://arxiv.org/pdf/2507.04053v1",
      "published_date": "2025-07-05 14:23:12 UTC",
      "updated_date": "2025-07-05 14:23:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:47.044287+00:00"
    },
    {
      "arxiv_id": "2507.04050v1",
      "title": "Predictive Modeling of Effluent Temperature in SAT Systems Using Ambient Meteorological Data: Implications for Infiltration Management",
      "title_zh": "基于环境气象数据的 SAT 系统出流温度预测建模：对渗透管理的启示",
      "authors": [
        "Roy Elkayam"
      ],
      "abstract": "Accurate prediction of effluent temperature in recharge basins is essential for optimizing the Soil Aquifer Treatment (SAT) process, as temperature directly influences water viscosity and infiltration rates. This study develops and evaluates predictive models for effluent temperature in the upper recharge layer of a Shafdan SAT system recharge basin using ambient meteorological data. Multiple linear regression (MLR), neural networks (NN), and random forests (RF) were tested for their predictive accuracy and interpretability. The MLR model, preferred for its operational simplicity and robust performance, achieved high predictive accuracy (R2 = 0.86-0.87) and was used to estimate effluent temperatures over a 10-year period. Results highlight pronounced seasonal temperature cycles and the importance of topsoil temperature in governing the thermal profile of the infiltrating effluent. The study provides practical equations for real-time monitoring and long-term planning of SAT operations.",
      "tldr_zh": "该研究旨在通过预测补给池出水温度来优化土壤含水层处理(Soil Aquifer Treatment, SAT)过程，因为温度是影响水粘度和入渗率的关键因素。研究人员利用环境气象数据，针对Shafdan SAT系统的上层补给层开发并评估了多种预测模型。在对比了多元线性回归(MLR)、神经网络(NN)和随机森林(RF)后，发现MLR模型凭借操作简便和稳健的预测性能（R2 = 0.86-0.87）脱颖而出。通过对10年数据的估算，研究揭示了出水温度存在明显的季节性循环，并确认了表层土壤温度在控制热剖面中的主导地位。该研究最终提供了可用于实时监测和长期规划的实用方程，为提升SAT系统的入渗管理水平提供了科学支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04050v1",
      "published_date": "2025-07-05 14:20:09 UTC",
      "updated_date": "2025-07-05 14:20:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:38:54.162106+00:00"
    },
    {
      "arxiv_id": "2507.04043v1",
      "title": "Evaluating the Effectiveness of Large Language Models in Solving Simple Programming Tasks: A User-Centered Study",
      "title_zh": "评估大语言模型解决简单编程任务的有效性：一项以用户为中心的研究",
      "authors": [
        "Kai Deng"
      ],
      "abstract": "As large language models (LLMs) become more common in educational tools and programming environments, questions arise about how these systems should interact with users. This study investigates how different interaction styles with ChatGPT-4o (passive, proactive, and collaborative) affect user performance on simple programming tasks. I conducted a within-subjects experiment where fifteen high school students participated, completing three problems under three distinct versions of the model. Each version was designed to represent a specific style of AI support: responding only when asked, offering suggestions automatically, or engaging the user in back-and-forth dialogue.Quantitative analysis revealed that the collaborative interaction style significantly improved task completion time compared to the passive and proactive conditions. Participants also reported higher satisfaction and perceived helpfulness when working with the collaborative version. These findings suggest that the way an LLM communicates, how it guides, prompts, and responds, can meaningfully impact learning and performance. This research highlights the importance of designing LLMs that go beyond functional correctness to support more interactive, adaptive, and user-centered experiences, especially for novice programmers.",
      "tldr_zh": "该研究探讨了 Large Language Models (LLMs) 在编程教育中的不同交互风格对用户表现的影响。通过对15名高中生进行实验，研究对比了 ChatGPT-4o 在被动(passive)、主动(proactive)和协作(collaborative)三种模式下处理简单编程任务的效果。定量分析结果表明，协作交互风格比被动或主动模式能显著缩短任务完成时间，且参与者反馈其具有更高的满意度和实用性。研究发现 LLM 的引导和反馈方式能直接影响学习者的表现，而不仅仅取决于其生成代码的功能正确性。这一发现强调了在为初学者设计编程辅助工具时，开发具备高度互动性和以用户为中心(user-centered)的交互体验的重要性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04043v1",
      "published_date": "2025-07-05 13:52:31 UTC",
      "updated_date": "2025-07-05 13:52:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:08.747641+00:00"
    },
    {
      "arxiv_id": "2507.04038v2",
      "title": "T-SYNTH: A Knowledge-Based Dataset of Synthetic Breast Images",
      "title_zh": "T-SYNTH：基于知识的合成乳腺图像数据集",
      "authors": [
        "Christopher Wiedeman",
        "Anastasiia Sarmakeeva",
        "Elena Sizikova",
        "Daniil Filienko",
        "Miguel Lago",
        "Jana G. Delfino",
        "Aldo Badano"
      ],
      "abstract": "One of the key impediments for developing and assessing robust medical imaging algorithms is limited access to large-scale datasets with suitable annotations. Synthetic data generated with plausible physical and biological constraints may address some of these data limitations. We propose the use of physics simulations to generate synthetic images with pixel-level segmentation annotations, which are notoriously difficult to obtain. Specifically, we apply this approach to breast imaging analysis and release T-SYNTH, a large-scale open-source dataset of paired 2D digital mammography (DM) and 3D digital breast tomosynthesis (DBT) images. Our initial experimental results indicate that T-SYNTH images show promise for augmenting limited real patient datasets for detection tasks in DM and DBT. Our data and code are publicly available at https://github.com/DIDSR/tsynth-release.",
      "tldr_zh": "该研究针对医学影像算法开发中大规模带标注数据集获取困难的问题，提出利用具有物理和生物学约束的物理仿真(physics simulations)技术生成合成图像。研究发布了T-SYNTH，这是一个大规模开源的配对2D数字乳腺X射线摄影(DM)和3D数字乳腺断层合成(DBT)图像数据集。T-SYNTH的独特之处在于提供了通过物理仿真生成的像素级分割标注(pixel-level segmentation annotations)，这类标注在真实临床数据中通常难以获取。初步实验结果表明，T-SYNTH图像在DM和DBT的检测任务中展现出扩充有限真实患者数据集的巨大潜力。目前该研究的数据和代码均已向公众开放，旨在为构建和评估更具鲁棒性的医疗影像算法提供关键的数据支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) Open Data 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.04038v2",
      "published_date": "2025-07-05 13:35:05 UTC",
      "updated_date": "2025-09-18 13:56:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:09.270646+00:00"
    },
    {
      "arxiv_id": "2507.04037v3",
      "title": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments",
      "title_zh": "Ready Jurist One：面向动态环境法律智能的语言智能体基准测试",
      "authors": [
        "Zheng Jia",
        "Shengbin Yue",
        "Wei Chen",
        "Siyuan Wang",
        "Yidong Liu",
        "Yun Song",
        "Zhongyu Wei"
      ],
      "abstract": "The gap between static benchmarks and the dynamic nature of real-world legal practice poses a key barrier to advancing legal intelligence. To this end, we introduce J1-ENVS, the first interactive and dynamic legal environment tailored for LLM-based agents. Guided by legal experts, it comprises six representative scenarios from Chinese legal practices across three levels of environmental complexity. We further introduce J1-EVAL, a fine-grained evaluation framework, designed to assess both task performance and procedural compliance across varying levels of legal proficiency. Extensive experiments on 17 LLM agents reveal that, while many models demonstrate solid legal knowledge, they struggle with procedural execution in dynamic settings. Even the SOTA model, GPT-4o, falls short of 60% overall performance. These findings highlight persistent challenges in achieving dynamic legal intelligence and offer valuable insights to guide future research.",
      "tldr_zh": "该研究针对静态基准测试与动态现实法律实践之间的差距，推出了J1-ENVS，这是首个专为大语言模型(LLM)智能体设计的交互式动态法律环境。在法律专家的指导下，该环境涵盖了中国法律实践中的六个代表性场景，并根据环境复杂度划分为三个层级。此外，研究团队还提出了J1-EVAL细粒度评估框架，旨在从任务表现和程序合规性两个维度评估智能体的法律能力。对17种LLM智能体的大规模实验揭示，尽管多数模型具备扎实的法律知识，但在动态环境中的程序执行(procedural execution)方面表现欠佳。实验结果显示，即使是性能顶尖的GPT-4o模型在综合评估中也未能达到60%的准确率。该项工作强调了实现动态法律智能(dynamic legal intelligence)所面临的持续挑战，并为未来法律大模型的研究提供了重要基准和见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04037v3",
      "published_date": "2025-07-05 13:31:21 UTC",
      "updated_date": "2026-01-21 07:41:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:14.346596+00:00"
    },
    {
      "arxiv_id": "2507.05291v1",
      "title": "Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity",
      "title_zh": "考虑有限应变超弹性的物理信息图神经网络局部场重建",
      "authors": [
        "Manuel Ricardo Guevara Garban",
        "Yves Chemisky",
        "Étienne Prulière",
        "Michaël Clément"
      ],
      "abstract": "We propose a physics-informed machine learning framework called P-DivGNN to reconstruct local stress fields at the micro-scale, in the context of multi-scale simulation given a periodic micro-structure mesh and mean, macro-scale, stress values. This method is based in representing a periodic micro-structure as a graph, combined with a message passing graph neural network. We are able to retrieve local stress field distributions, providing average stress values produced by a mean field reduced order model (ROM) or Finite Element (FE) simulation at the macro-scale. The prediction of local stress fields are of utmost importance considering fracture analysis or the definition of local fatigue criteria. Our model incorporates physical constraints during training to constraint local stress field equilibrium state and employs a periodic graph representation to enforce periodic boundary conditions. The benefits of the proposed physics-informed GNN are evaluated considering linear and non linear hyperelastic responses applied to varying geometries. In the non-linear hyperelastic case, the proposed method achieves significant computational speed-ups compared to FE simulation, making it particularly attractive for large-scale applications.",
      "tldr_zh": "这项研究提出了P-DivGNN，这是一个物理信息机器学习(Physics-Informed Machine Learning)框架，旨在多尺度模拟中根据给定的周期性微观结构网格和宏观平均应力值来重建微观尺度的局部应力场(local stress fields)。该方法将周期性微观结构表示为图(graph)，并结合消息传递图神经网络(message passing graph neural network)进行处理，能够有效检索出对断裂分析(fracture analysis)和局部疲劳准则(fatigue criteria)定义至关重要的应力场分布。模型在训练中通过引入物理约束来保证局部应力场的平衡态(equilibrium state)，并利用周期图表示来强制执行周期性边界条件(periodic boundary conditions)。研究评估了该框架在线性及非线性超弹性(hyperelasticity)响应下的表现，涵盖了多种变化的几何构型。结果表明，在处理非线性超弹性问题时，该方法相比于传统有限元(FE)模拟实现了显著的计算加速，为大规模工程应用提供了高效且精确的解决方案。",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 17 figures, pre-print",
      "pdf_url": "https://arxiv.org/pdf/2507.05291v1",
      "published_date": "2025-07-05 13:11:31 UTC",
      "updated_date": "2025-07-05 13:11:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:16.374497+00:00"
    },
    {
      "arxiv_id": "2507.04034v1",
      "title": "Lyria: A General LLM-Driven Genetic Algorithm Framework for Problem Solving",
      "title_zh": "Lyria：一种用于问题求解的大语言模型驱动通用遗传算法框架",
      "authors": [
        "Weizhi Tang",
        "Kwabena Nuamah",
        "Vaishak Belle"
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated impressive abilities across various domains, they still struggle with complex problems characterized by multi-objective optimization, precise constraint satisfaction, immense solution spaces, etc. To address the limitation, drawing on the superior semantic understanding ability of LLMs and also the outstanding global search and optimization capability of genetic algorithms, we propose to capitalize on their respective strengths and introduce Lyria, a general LLM-driven genetic algorithm framework, comprising 7 essential components. Through conducting extensive experiments with 4 LLMs across 3 types of problems, we demonstrated the efficacy of Lyria. Additionally, with 7 additional ablation experiments, we further systematically analyzed and elucidated the factors that affect its performance.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 在面对多目标优化、精确约束满足和巨大解空间等复杂问题时的局限性，提出了 Lyria，这是一种通用的 LLM-driven Genetic Algorithm 框架。Lyria 巧妙结合了 LLMs 卓越的语义理解能力与 Genetic Algorithm 出色的全局搜索和优化能力，包含了 7 个关键核心组件。通过在 3 种问题类型上对 4 种 LLMs 进行的广泛实验，结果充分证明了 Lyria 在解决复杂问题方面的有效性。此外，研究还通过 7 项额外的消融实验 (Ablation Experiments) 深入分析并阐明了影响该框架性能的核心因素。该框架为提升 LLMs 处理复杂工程和数学优化问题的能力提供了新的研究范式。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04034v1",
      "published_date": "2025-07-05 13:04:36 UTC",
      "updated_date": "2025-07-05 13:04:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:23.050298+00:00"
    },
    {
      "arxiv_id": "2507.04014v1",
      "title": "Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition",
      "title_zh": "Nunchi-Bench：聚焦韩国迷信的语言模型文化推理基准测试",
      "authors": [
        "Kyuhee Kim",
        "Sangah Lee"
      ],
      "abstract": "As large language models (LLMs) become key advisors in various domains, their cultural sensitivity and reasoning skills are crucial in multicultural environments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs' cultural understanding, with a focus on Korean superstitions. The benchmark consists of 247 questions spanning 31 topics, assessing factual knowledge, culturally appropriate advice, and situational interpretation. We evaluate multilingual LLMs in both Korean and English to analyze their ability to reason about Korean cultural contexts and how language variations affect performance. To systematically assess cultural reasoning, we propose a novel evaluation strategy with customized scoring metrics that capture the extent to which models recognize cultural nuances and respond appropriately. Our findings highlight significant challenges in LLMs' cultural reasoning. While models generally recognize factual information, they struggle to apply it in practical scenarios. Furthermore, explicit cultural framing enhances performance more effectively than relying solely on the language of the prompt. To support further research, we publicly release Nunchi-Bench alongside a leaderboard.",
      "tldr_zh": "该研究推出了 Nunchi-Bench，这是一个专门用于评估大型语言模型 (LLMs) 文化推理 (cultural reasoning) 能力的基准测试，其核心关注点在于韩国迷信 (Korean superstitions)。该基准包含 247 个问题，涵盖 31 个主题，从事实性知识、文化建议和情境解读三个维度对多语言 LLMs 在韩语和英语环境下的表现进行评估。研究团队提出了一种全新的评估策略，利用定制化评分指标 (customized scoring metrics) 来衡量模型识别文化细微差别 (cultural nuances) 的准确性。实验发现，尽管模型在识别文化事实方面表现良好，但在实际场景的应用推理中仍存在显著挑战。研究进一步指出，显式的文化框架 (explicit cultural framing) 相比单纯改变提示语言能更有效地增强模型的表现。Nunchi-Bench 及其排行榜的公开发布，为后续提升人工智能的文化敏感度和推理能力提供了重要工具与参考。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04014v1",
      "published_date": "2025-07-05 11:52:09 UTC",
      "updated_date": "2025-07-05 11:52:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:28.556523+00:00"
    },
    {
      "arxiv_id": "2507.08012v1",
      "title": "RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning",
      "title_zh": "RepeaTTS：通过重复微调实现特征发现",
      "authors": [
        "Atli Sigurgeirsson",
        "Simon King"
      ],
      "abstract": "A Prompt-based Text-To-Speech model allows a user to control different aspects of speech, such as speaking rate and perceived gender, through natural language instruction. Although user-friendly, such approaches are on one hand constrained: control is limited to acoustic features exposed to the model during training, and too flexible on the other: the same inputs yields uncontrollable variation that are reflected in the corpus statistics.\n  We investigate a novel fine-tuning regime to address both of these issues at the same time by exploiting the uncontrollable variance of the model. Through principal component analysis of thousands of synthesised samples, we determine latent features that account for the highest proportion of the output variance and incorporate them as new labels for secondary fine-tuning. We evaluate the proposed methods on two models trained on an expressive Icelandic speech corpus, one with emotional disclosure and one without. In the case of the model without emotional disclosure, the method yields both continuous and discrete features that improve overall controllability of the model.",
      "tldr_zh": "该研究针对基于提示(Prompt-based)的文本转语音(Text-To-Speech)模型在控制特征受限及存在不可控变分(uncontrollable variation)的问题，提出了RepeaTTS框架。该框架通过一种新型的重复微调(Repeated Fine-Tuning)机制，利用模型自身的不可控方差来进行特征发现。研究人员对数千个合成样本执行主成分分析(Principal Component Analysis, PCA)以确定关键的潜在特征(latent features)，并将其作为新标签引入二次微调过程。实验在两个基于表达性冰岛语语音语料库的模型上展开，结果表明该方法能够提取出改善模型整体可控性的连续和离散特征。这一研究证明了通过挖掘模型内部方差来自动发现并集成新的控制维度，从而显著增强TTS系统可控性的可行性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.08012v1",
      "published_date": "2025-07-05 10:59:00 UTC",
      "updated_date": "2025-07-05 10:59:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:30.340649+00:00"
    },
    {
      "arxiv_id": "2507.04000v1",
      "title": "Leveraging Multimodal Data and Side Users for Diffusion Cross-Domain Recommendation",
      "title_zh": "利用多模态数据与侧边用户的扩散跨域推荐",
      "authors": [
        "Fan Zhang",
        "Jinpeng Chen",
        "Huan Li",
        "Senzhang Wang",
        "Yuan Cao",
        "Kaimin Wei",
        "JianXiang He",
        "Feifei Kou",
        "Jinqing Wang"
      ],
      "abstract": "Cross-domain recommendation (CDR) aims to address the persistent cold-start problem in Recommender Systems. Current CDR research concentrates on transferring cold-start users' information from the auxiliary domain to the target domain. However, these systems face two main issues: the underutilization of multimodal data, which hinders effective cross-domain alignment, and the neglect of side users who interact solely within the target domain, leading to inadequate learning of the target domain's vector space distribution. To address these issues, we propose a model leveraging Multimodal data and Side users for diffusion Cross-domain recommendation (MuSiC). We first employ a multimodal large language model to extract item multimodal features and leverage a large language model to uncover user features using prompt learning without fine-tuning. Secondly, we propose the cross-domain diffusion module to learn the generation of feature vectors in the target domain. This approach involves learning feature distribution from side users and understanding the patterns in cross-domain transformation through overlapping users. Subsequently, the trained diffusion module is used to generate feature vectors for cold-start users in the target domain, enabling the completion of cross-domain recommendation tasks. Finally, our experimental evaluation of the Amazon dataset confirms that MuSiC achieves state-of-the-art performance, significantly outperforming all selected baselines. Our code is available: https://anonymous.4open.science/r/MuSiC-310A/.",
      "tldr_zh": "该研究提出了MuSiC模型，旨在解决跨域推荐(Cross-domain recommendation, CDR)中存在的冷启动问题。针对现有研究对多模态数据利用不足以及忽视目标域侧向用户(Side users)导致特征空间分布学习不充分的问题，MuSiC结合了多模态大语言模型(MLLM)提取物品特征，并利用大语言模型的提示学习(Prompt learning)挖掘用户特征。模型创新性地引入了跨域扩散模块(Cross-domain diffusion module)，通过侧向用户学习目标域特征分布，并结合重叠用户建模跨域转换规律。该扩散模块能够为冷启动用户生成高质量的目标域特征向量，从而实现精准推荐。在Amazon数据集上的实验证明，MuSiC显著优于现有基准模型，达到了当前的SOTA水平。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.04000v1",
      "published_date": "2025-07-05 10:57:29 UTC",
      "updated_date": "2025-07-05 10:57:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:29.348467+00:00"
    },
    {
      "arxiv_id": "2507.03998v1",
      "title": "Toward Better Generalisation in Uncertainty Estimators: Leveraging Data-Agnostic Features",
      "title_zh": "提升不确定性评估器的泛化性能：利用数据无关特征",
      "authors": [
        "Thuy An Ha",
        "Bao Quoc Vo"
      ],
      "abstract": "Large Language Models (LLMs) often generate responses that are factually incorrect yet expressed with high confidence, which can pose serious risks for end users. To address this, it is essential for LLMs not only to produce answers but also to provide accurate estimates of their correctness. Uncertainty quantification methods have been introduced to assess the quality of LLM outputs, with factual accuracy being a key aspect of that quality. Among these methods, those that leverage hidden states to train probes have shown particular promise, as these internal representations encode information relevant to the factuality of responses, making this approach the focus of this paper. However, the probe trained on the hidden states of one dataset often struggles to generalise to another dataset of a different task or domain. To address this limitation, we explore combining data-agnostic features with hidden-state features and assess whether this hybrid feature set enhances out-of-domain performance. We further examine whether selecting only the most informative hidden-state features, thereby discarding task-specific noise, enables the data-agnostic features to contribute more effectively. The experiment results indicate that although introducing data-agnostic features generally enhances generalisation performance in most cases, in certain scenarios their inclusion degrades performance. A similar pattern emerges when retaining only the most important hidden-state features - adding data-agnostic features does not consistently further enhance performance compared to using the full set of hidden-state features. A closer analysis reveals that, in some specific cases, the trained probe underweights the data-agnostic features relative to the hidden-state features, which we believe is the main reason why the results are inconclusive.",
      "tldr_zh": "该研究探讨了如何提高大语言模型(LLMs)不确定性估计器的泛化能力，以解决模型输出事实错误但置信度高的问题。研究重点在于利用隐藏状态(hidden states)训练探测器(probes)来评估事实准确性，但发现这类探测器在跨领域任务中往往难以泛化。为了改善这一局限性，作者提出将数据无关特征(data-agnostic features)与隐藏状态特征相结合，并尝试通过特征选择剔除任务相关的噪声。实验结果显示，虽然引入数据无关特征在多数情况下能提升泛化性能，但在某些特定场景下反而会导致性能下降。进一步分析发现，探测器在训练过程中可能会低估数据无关特征的权重，这解释了为何该方法在提升不确定性量化效果方面未能表现出一致的优越性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03998v1",
      "published_date": "2025-07-05 10:55:36 UTC",
      "updated_date": "2025-07-05 10:55:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:32.572400+00:00"
    },
    {
      "arxiv_id": "2507.05288v1",
      "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models",
      "title_zh": "大语言模型中针对虚假信息的主动防御策略综述",
      "authors": [
        "Shuliang Liu",
        "Hongyi Liu",
        "Aiwei Liu",
        "Bingchen Duan",
        "Qi Zheng",
        "Yibo Yan",
        "He Geng",
        "Peijie Jiang",
        "Jia Liu",
        "Xuming Hu"
      ],
      "abstract": "The widespread deployment of large language models (LLMs) across critical domains has amplified the societal risks posed by algorithmically generated misinformation. Unlike traditional false content, LLM-generated misinformation can be self-reinforcing, highly plausible, and capable of rapid propagation across multiple languages, which traditional detection methods fail to mitigate effectively. This paper introduces a proactive defense paradigm, shifting from passive post hoc detection to anticipatory mitigation strategies. We propose a Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of training and deployed data; (2) Inference Reliability, embedding self-corrective mechanisms during reasoning; and (3) Input Robustness, enhancing the resilience of model interfaces against adversarial attacks. Through a comprehensive survey of existing techniques and a comparative meta-analysis, we demonstrate that proactive defense strategies offer up to 63\\% improvement over conventional methods in misinformation prevention, despite non-trivial computational overhead and generalization challenges. We argue that future research should focus on co-designing robust knowledge foundations, reasoning certification, and attack-resistant interfaces to ensure LLMs can effectively counter misinformation across varied domains.",
      "tldr_zh": "该综述探讨了大语言模型(LLMs)中针对误导性信息(Misinformation)的主动防御策略，旨在应对算法生成内容带来的社会风险。研究者提出了主动防御范式(Proactive Defense Paradigm)，将重心从被动的事后检测转向预先的缓解策略，并构建了包含三个支柱(Three Pillars)的框架。该框架涵盖了通过加强数据完整性实现的知识可信度(Knowledge Credibility)、在推理中嵌入自我修正机制的推理可靠性(Inference Reliability)，以及增强接口抗攻击能力的输入鲁棒性(Input Robustness)。综合调研与元分析表明，主动防御策略在预防误导性信息方面的表现比传统方法提升了高达63%，尽管仍面临计算开销和泛化挑战。研究强调未来应侧重于协同设计稳健的知识基础、推理认证及抗攻击接口，以确保大语言模型在多领域中有效抵御误导性信息。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2507.05288v1",
      "published_date": "2025-07-05 09:52:21 UTC",
      "updated_date": "2025-07-05 09:52:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:39:49.467633+00:00"
    },
    {
      "arxiv_id": "2507.03971v1",
      "title": "Real-TabPFN: Improving Tabular Foundation Models via Continued Pre-training With Real-World Data",
      "title_zh": "Real-TabPFN：利用真实世界数据进行持续预训练以提升表格基础模型",
      "authors": [
        "Anurag Garg",
        "Muhammad Ali",
        "Noah Hollmann",
        "Lennart Purucker",
        "Samuel Müller",
        "Frank Hutter"
      ],
      "abstract": "Foundation models for tabular data, like TabPFN, achieve strong performance on small datasets when pre-trained solely on synthetic data. We show that this performance can be significantly boosted by a targeted continued pre-training phase. Specifically, we demonstrate that leveraging a small, curated collection of large, real-world datasets for continued pre-training yields superior downstream predictive accuracy compared to using broader, potentially noisier corpora like CommonCrawl or GitTables. Our resulting model, Real-TabPFN, achieves substantial performance gains on 29 datasets from the OpenML AutoML Benchmark.",
      "tldr_zh": "该研究提出了 Real-TabPFN，通过在真实世界数据上进行针对性的持续预训练（Continued Pre-training）来显著提升表格基础模型（Tabular Foundation Models）的性能。研究人员证实，相较于利用 CommonCrawl 或 GitTables 等广泛且包含较多噪声的语料库，使用经过精选的小型、大规模真实世界数据集进行持续预训练，能为模型带来更优的下游预测准确率。实验结果显示，Real-TabPFN 在 OpenML AutoML Benchmark 的 29 个数据集上均取得了实质性的性能提升。该工作证明了通过引入高质量真实数据，可以有效突破原有 TabPFN 模型仅依赖合成数据进行预训练带来的性能局限，为构建更强大的表格领域基础模型提供了有效路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03971v1",
      "published_date": "2025-07-05 09:39:07 UTC",
      "updated_date": "2025-07-05 09:39:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:49.873184+00:00"
    },
    {
      "arxiv_id": "2507.03958v2",
      "title": "A Comparative Study of Specialized LLMs as Dense Retrievers",
      "title_zh": "专业化大语言模型作为稠密检索器的比较研究",
      "authors": [
        "Hengran Zhang",
        "Keping Bi",
        "Jiafeng Guo"
      ],
      "abstract": "While large language models (LLMs) are increasingly deployed as dense retrievers, the impact of their domain-specific specialization on retrieval effectiveness remains underexplored. This investigation systematically examines how task-specific adaptations in LLMs influence their retrieval capabilities, an essential step toward developing unified retrievers capable of handling text, code, images, and multimodal content. We conduct extensive experiments with eight Qwen2.5 7B LLMs, including base, instruction-tuned, code/math-specialized, long reasoning, and vision-language models across zero-shot retrieval settings and the supervised setting. For the zero-shot retrieval settings, we consider text retrieval from the BEIR benchmark and code retrieval from the CoIR benchmark. Further, to evaluate supervised performance, all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical specialization and the long reasoning capability cause consistent degradation in three settings, indicating conflicts between mathematical reasoning and semantic matching. The vision-language model and code-specialized LLMs demonstrate superior zero-shot performance compared to other LLMs, even surpassing BM25 on the code retrieval task, and maintain comparable performance to base LLMs in supervised settings. These findings suggest promising directions for the unified retrieval task leveraging cross-domain and cross-modal fusion.",
      "tldr_zh": "该研究系统地探讨了大型语言模型(LLMs)的领域特定专业化对稠密检索(dense retrievers)效果的影响，通过对八种不同类型的 Qwen2.5 7B 模型在 zero-shot 和监督学习设置下进行了广泛实验。实验基准涵盖了 BEIR 文本检索、CoIR 代码检索以及在 MS MARCO 数据集上的微调表现。研究发现，数学专业化(mathematical specialization)和长文本推理能力(long reasoning capability)在所有实验设置中均导致检索性能持续下降，表明数学推理与语义匹配(semantic matching)之间存在潜在冲突。相比之下，视觉语言模型(vision-language model)和代码专用 LLMs 在 zero-shot 检索中表现优异，其代码检索能力甚至超越了 BM25 基准。在监督微调后，这些模型仍能维持与基础模型(base LLMs)相当的性能。研究结果为利用跨领域和跨模态融合(cross-domain and cross-modal fusion)构建统一检索器提供了重要启示。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by CCIR25 and published by Springer LNCS or LNAI",
      "pdf_url": "https://arxiv.org/pdf/2507.03958v2",
      "published_date": "2025-07-05 08:50:29 UTC",
      "updated_date": "2025-08-06 08:11:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:45.707485+00:00"
    },
    {
      "arxiv_id": "2507.03953v1",
      "title": "Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study",
      "title_zh": "扩散模型个性化对抗性防护的综合评估研究",
      "authors": [
        "Kai Ye",
        "Tianyi Chen",
        "Zhen Wang"
      ],
      "abstract": "With the increasing adoption of diffusion models for image generation and personalization, concerns regarding privacy breaches and content misuse have become more pressing. In this study, we conduct a comprehensive comparison of eight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak, Mist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains. These methods are evaluated under varying perturbation budgets, using a range of metrics to assess visual imperceptibility and protective efficacy. Our results offer practical guidance for method selection. Code is available at: https://github.com/vkeilo/DiffAdvPerturbationBench.",
      "tldr_zh": "该研究针对扩散模型在图像生成和个性化应用中日益凸显的隐私泄露与内容误用问题，对 AdvDM、ASPL、FSGM、MetaCloak、Mist、PhotoGuard、SDS 和 SimAC 这八种基于扰动的保护方法进行了全面的比较研究。评估过程涵盖了肖像和艺术品两个主要领域，并结合不同的扰动预算（perturbation budgets）进行了深入测试。研究团队采用多种指标权衡了这些方法在视觉不可感知性（visual imperceptibility）与保护效力（protective efficacy）之间的表现。通过系统性的实验分析，该研究为不同场景下选择最合适的扩散模型个性化（diffusion personalization）防护手段提供了实际的指导方案。最后，该研究开源了相关代码，旨在为增强生成式模型的安全性提供有力支撑。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the 2nd Workshop on Reliable and Responsible Foundation Models (R2-FM 2025) at ICML. 8 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2507.03953v1",
      "published_date": "2025-07-05 08:32:25 UTC",
      "updated_date": "2025-07-05 08:32:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:58.499330+00:00"
    },
    {
      "arxiv_id": "2507.03950v1",
      "title": "Optimizing Age of Trust and Throughput in Multi-Hop UAV-Aided IoT Networks",
      "title_zh": "多跳无人机辅助物联网中的信任龄与吞吐量优化",
      "authors": [
        "Yizhou Luo",
        "Kwan-Wu Chin",
        "Ruyi Guan",
        "Xi Xiao",
        "Caimeng Wang",
        "Jingyin Feng",
        "Tengjiao He"
      ],
      "abstract": "Devices operating in Internet of Things (IoT) networks may be deployed across vast geographical areas and interconnected via multi-hop communications. Further, they may be unguarded. This makes them vulnerable to attacks and motivates operators to check on devices frequently. To this end, we propose and study an Unmanned Aerial Vehicle (UAV)-aided attestation framework for use in IoT networks with a charging station powered by solar. A key challenge is optimizing the trajectory of the UAV to ensure it attests as many devices as possible. A trade-off here is that devices being checked by the UAV are offline, which affects the amount of data delivered to a gateway. Another challenge is that the charging station experiences time-varying energy arrivals, which in turn affect the flight duration and charging schedule of the UAV. To address these challenges, we employ a Deep Reinforcement Learning (DRL) solution to optimize the UAV's charging schedule and the selection of devices to be attested during each flight. The simulation results show that our solution reduces the average age of trust by 88% and throughput loss due to attestation by 30%.",
      "tldr_zh": "该研究针对分布广泛且易受攻击的多跳物联网(IoT)网络，提出了一个由太阳能充电站支持的无人机(UAV)辅助认证框架，旨在通过频繁检查提升设备的安全性。优化的核心挑战在于规划无人机的飞行轨迹以覆盖尽可能多的设备，同时由于认证过程中设备需处于离线状态，必须平衡信任时长(Age of Trust)与网络吞吐量(Throughput)之间的矛盾。此外，充电站随时间变化的太阳能获取也显著影响了无人机的飞行时长和充电调度方案。为了解决这些动态耦合的挑战，该研究采用深度强化学习(Deep Reinforcement Learning, DRL)算法来协同优化无人机的充电策略以及每次飞行任务中待认证设备的动态选择。仿真结果表明，该方案能够将平均信任时长(Average Age of Trust)降低88%，并将因认证导致的吞吐量损失(Throughput Loss)减少30%，实现了安全监测与数据传输效率的高效折衷。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03950v1",
      "published_date": "2025-07-05 08:25:18 UTC",
      "updated_date": "2025-07-05 08:25:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:50.005552+00:00"
    },
    {
      "arxiv_id": "2507.03937v1",
      "title": "EdgeSRIE: A hybrid deep learning framework for real-time speckle reduction and image enhancement on portable ultrasound systems",
      "title_zh": "EdgeSRIE：面向便携式超声系统实时斑点抑制与图像增强的混合深度学习框架",
      "authors": [
        "Hyunwoo Cho",
        "Jongsoo Lee",
        "Jinbum Kang",
        "Yangmo Yoo"
      ],
      "abstract": "Speckle patterns in ultrasound images often obscure anatomical details, leading to diagnostic uncertainty. Recently, various deep learning (DL)-based techniques have been introduced to effectively suppress speckle; however, their high computational costs pose challenges for low-resource devices, such as portable ultrasound systems. To address this issue, EdgeSRIE, which is a lightweight hybrid DL framework for real-time speckle reduction and image enhancement in portable ultrasound imaging, is introduced. The proposed framework consists of two main branches: an unsupervised despeckling branch, which is trained by minimizing a loss function between speckled images, and a deblurring branch, which restores blurred images to sharp images. For hardware implementation, the trained network is quantized to 8-bit integer precision and deployed on a low-resource system-on-chip (SoC) with limited power consumption. In the performance evaluation with phantom and in vivo analyses, EdgeSRIE achieved the highest contrast-to-noise ratio (CNR) and average gradient magnitude (AGM) compared with the other baselines (different 2-rule-based methods and other 4-DL-based methods). Furthermore, EdgeSRIE enabled real-time inference at over 60 frames per second while satisfying computational requirements (< 20K parameters) on actual portable ultrasound hardware. These results demonstrated the feasibility of EdgeSRIE for real-time, high-quality ultrasound imaging in resource-limited environments.",
      "tldr_zh": "本研究针对便携式超声系统中超声图像斑点噪声(Speckle patterns)遮盖解剖细节且深度学习模型计算成本过高的问题，提出了名为EdgeSRIE的轻量化混合深度学习框架。该框架由两个主要分支组成：一个是通过最小化斑点图像间损失函数训练的无监督去噪(Unsupervised despeckling)分支，以及一个将模糊图像恢复为清晰图像的去模糊(Deblurring)分支。为了满足硬件部署需求，研究团队将训练好的网络量化为8位整数精度(8-bit integer precision)，并部署在功耗受限的低资源系统级芯片(SoC)上。实验分析显示，与多种基于规则和深度学习的基线方法相比，EdgeSRIE在对比度噪声比(CNR)和平均梯度幅度(AGM)上均取得了最优性能。此外，EdgeSRIE在实际硬件上实现了每秒超过60帧的实时推理速度，且参数量少于20K。该研究结果证明了EdgeSRIE在资源受限环境下实现实时、高质量超声成像的技术可行性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03937v1",
      "published_date": "2025-07-05 07:52:34 UTC",
      "updated_date": "2025-07-05 07:52:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:40:56.801349+00:00"
    },
    {
      "arxiv_id": "2507.12475v1",
      "title": "Coarse Addition and the St. Petersburg Paradox: A Heuristic Perspective",
      "title_zh": "粗糙加法与 St. Petersburg 悖论：一种启发式视角",
      "authors": [
        "Takashi Izumo"
      ],
      "abstract": "The St. Petersburg paradox presents a longstanding challenge in decision theory. It describes a game whose expected value is infinite, yet for which no rational finite stake can be determined. Traditional solutions introduce auxiliary assumptions, such as diminishing marginal utility, temporal discounting, or extended number systems. These methods often involve mathematical refinements that may not correspond to how people actually perceive or process numerical information. This paper explores an alternative approach based on a modified operation of addition defined over coarse partitions of the outcome space. In this model, exact numerical values are grouped into perceptual categories, and each value is replaced by a representative element of its group before being added. This method allows for a phenomenon where repeated additions eventually cease to affect the outcome, a behavior described as inertial stabilization. Although this is not intended as a definitive resolution of the paradox, the proposed framework offers a plausible way to represent how agents with limited cognitive precision might handle divergent reward structures. We demonstrate that the St. Petersburg series can become inert under this coarse addition for a suitably constructed partition. The approach may also have broader applications in behavioral modeling and the study of machine reasoning under perceptual limitations.",
      "tldr_zh": "该研究从启发式视角出发，探讨了决策理论中圣彼得堡悖论(St. Petersburg paradox)的经典挑战。论文提出了一种基于结果空间粗分块的改进加法运算，即 Coarse Addition，旨在模拟认知精度有限的智能体如何处理数值信息。在该模型中，精确数值被归入特定的感知类别，并在相加前被替换为该类别的代表元素。这种方法引发了惯性稳定(Inertial Stabilization)现象，即重复加法在达到一定阈值后不再改变最终结果。研究证明，通过构建合适的划分，原本发散的圣彼得堡级数在 Coarse Addition 框架下可以变得稳定。该框架为理解有限认知下的奖励结构处理提供了新途径，并在行为建模(Behavioral modeling)和感知受限下的机器推理(Machine reasoning)领域具有潜在应用价值。",
      "categories": [
        "econ.TH",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "econ.TH",
      "comment": "16 pages, no figure",
      "pdf_url": "https://arxiv.org/pdf/2507.12475v1",
      "published_date": "2025-07-05 07:34:46 UTC",
      "updated_date": "2025-07-05 07:34:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:25.206170+00:00"
    },
    {
      "arxiv_id": "2507.03929v2",
      "title": "An ASP-Based Framework for MUSes",
      "title_zh": "一种基于 ASP 的 MUSes 框架",
      "authors": [
        "Mohimenul Kabir",
        "Kuldeep S Meel"
      ],
      "abstract": "Given an unsatisfiable formula, understanding the core reason for unsatisfiability is crucial in several applications. One effective way to capture this is through the minimal unsatisfiable subset (MUS), the subset-minimal set of clauses that remains unsatisfiable. Current research broadly focuses on two directions: (i) enumerating as many MUSes as possible within a given time limit, and (ii) counting the total number of MUSes for a given unsatisfiable formula.\n  In this paper, we introduce an answer set programming-based framework, named MUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for its strengths in knowledge representation and is particularly suitable for specifying complex combinatorial problems. By translating MUS enumeration into answer set solving, MUS-ASP leverages the computational efficiency of state-of-the-art ASP systems. Our extensive experimental evaluation demonstrates the effectiveness of MUS-ASP and highlights the acceleration in both MUS enumeration and counting tasks, particularly when integrated within hybrid solvers, including the framework proposed in this paper.",
      "tldr_zh": "该研究针对识别不可满足公式中最小不可满足子集 (Minimal Unsatisfiable Subsets, MUSes) 的核心挑战，提出了一种名为 MUS-ASP 的新型框架。该框架基于答案集编程 (Answer Set Programming, ASP)，旨在实现 MUSes 的在线枚举，并利用 ASP 强大的知识表示能力和现代系统的计算效率来处理复杂的组合问题。通过将 MUS 枚举任务转化为答案集求解过程，MUS-ASP 能够有效地加速 MUSes 的发现与计数。实验评估证明了 MUS-ASP 的有效性，特别是在集成到混合求解器 (hybrid solvers) 中时，该框架在枚举和计数任务中均表现出显著的性能提升。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings ICLP 2025, arXiv:2601.00047",
      "pdf_url": "https://arxiv.org/pdf/2507.03929v2",
      "published_date": "2025-07-05 07:23:24 UTC",
      "updated_date": "2026-01-07 12:03:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:03.230960+00:00"
    },
    {
      "arxiv_id": "2507.03928v1",
      "title": "CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate",
      "title_zh": "CortexDebate：面向多智能体辩论的稀疏与均衡辩论机制",
      "authors": [
        "Yiliu Sun",
        "Zicheng Zhao",
        "Sheng Wan",
        "Chen Gong"
      ],
      "abstract": "Nowadays, single Large Language Model (LLM) struggles with critical issues such as hallucination and inadequate reasoning abilities. To mitigate these issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where LLM agents engage in in-depth debates with others on tasks. However, existing MAD methods face two major issues: (a) too lengthy input contexts, which causes LLM agents to get lost in plenty of input information and experiences performance drop; and (b) the overconfidence dilemma, where self-assured LLM agents dominate the debate, leading to low debating effectiveness. To address these limitations, we propose a novel MAD method called \"CortexDebate\". Inspired by the human brain's tendency to establish a sparse and dynamically optimized network among cortical areas governed by white matter, CortexDebate constructs a sparse debating graph among LLM agents, where each LLM agent only debates with the ones that are helpful to it. To optimize the graph, we propose a module named McKinsey-based Debate Matter (MDM), which acts as an artificial analog to white matter. By integrating the McKinsey Trust Formula, a well-established measure of trustworthiness from sociology, MDM enables credible evaluations that guide graph optimization. The effectiveness of our CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.",
      "tldr_zh": "该研究针对多智能体辩论 (Multi-Agent Debate) 中存在的输入上下文过长以及智能体过度自信导致辩论效率低下等问题，提出了名为 CortexDebate 的新型框架。该方法受到人类大脑白质连接皮层区域的稀疏动态优化网络启发，在 LLM 智能体之间构建稀疏辩论图，确保每个智能体仅与对其有实质帮助的成员进行交互。为了实现图优化，研究设计了 McKinsey-based Debate Matter (MDM) 模块，通过集成社会学中的麦肯锡信任公式 (McKinsey Trust Formula) 进行可信度评估。这种机制有效地缓解了信息过载并打破了过分自信的僵局，使智能体能够进行更具建设性的协作。实验结果在涉及四类任务的八个数据集上证明了 CortexDebate 的有效性，显著提升了多智能体系统的推理表现和辩论质量。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ACL 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.03928v1",
      "published_date": "2025-07-05 07:23:15 UTC",
      "updated_date": "2025-07-05 07:23:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:08.903413+00:00"
    },
    {
      "arxiv_id": "2507.03923v2",
      "title": "Learning Disentangled Stain and Structural Representations for Semi-Supervised Histopathology Segmentation",
      "title_zh": "面向半监督组织病理学分割的染色与结构解耦表征学习",
      "authors": [
        "Ha-Hieu Pham",
        "Nguyen Lan Vi Vu",
        "Thanh-Huy Nguyen",
        "Ulas Bagci",
        "Min Xu",
        "Trung-Nghia Le",
        "Huy-Hieu Pham"
      ],
      "abstract": "Accurate gland segmentation in histopathology images is essential for cancer diagnosis and prognosis. However, significant variability in Hematoxylin and Eosin (H&E) staining and tissue morphology, combined with limited annotated data, poses major challenges for automated segmentation. To address this, we propose Color-Structure Dual-Student (CSDS), a novel semi-supervised segmentation framework designed to learn disentangled representations of stain appearance and tissue structure. CSDS comprises two specialized student networks: one trained on stain-augmented inputs to model chromatic variation, and the other on structure-augmented inputs to capture morphological cues. A shared teacher network, updated via Exponential Moving Average (EMA), supervises both students through pseudo-labels. To further improve label reliability, we introduce stain-aware and structure-aware uncertainty estimation modules that adaptively modulate the contribution of each student during training. Experiments on the GlaS and CRAG datasets show that CSDS achieves state-of-the-art performance in low-label settings, with Dice score improvements of up to 1.2% on GlaS and 0.7% on CRAG at 5% labeled data, and 0.7% and 1.4% at 10%. Our code and pre-trained models are available at https://github.com/hieuphamha19/CSDS.",
      "tldr_zh": "该研究提出了一种名为 Color-Structure Dual-Student (CSDS) 的新型半监督分割框架，旨在解决组织病理学图像中因 H&E 染色和组织形态变异以及标注数据有限导致的腺体分割难题。该框架通过学习解耦的染色外观(stain appearance)和组织结构(tissue structure)表示，利用两个分别经过染色增强(stain-augmented)和结构增强(structure-augmented)训练的学生网络来捕捉色彩和形态特征。一个通过指数移动平均(EMA)更新的共享教师网络通过伪标签监督学生，并结合新引入的染色感知与结构感知不确定性估计模块，在训练期间自适应地调节每个学生的贡献。在 GlaS 和 CRAG 数据集上的实验证明，CSDS 在低标签比例设置下达到了 state-of-the-art 性能，在仅有 5% 标注数据时 Dice score 提升高达 1.2%。该研究为病理图像在标注稀缺情况下的精确分割提供了有效的技术路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted to Compayl @ MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.03923v2",
      "published_date": "2025-07-05 07:09:46 UTC",
      "updated_date": "2025-08-03 04:09:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:10.241241+00:00"
    },
    {
      "arxiv_id": "2507.03916v3",
      "title": "Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models",
      "title_zh": "动画亦需关注：一种基于视觉语言模型的幻灯片动画理解全方位方法",
      "authors": [
        "Yifan Jiang",
        "Yibo Xue",
        "Yukun Kang",
        "Pin Zheng",
        "Jian Peng",
        "Feiran Wu",
        "Changliang Xu"
      ],
      "abstract": "Slide animations, such as fade-in, fly-in, and wipe, are critical for audience engagement, efficient information delivery, and vivid visual expression. However, most AI-driven slide-generation tools still lack native animation support, and existing vision-language models (VLMs) struggle with animation tasks due to the absence of public datasets and limited temporal-reasoning capabilities. To address this gap, we release the first public dataset for slide-animation modeling: 12,000 triplets of natural-language descriptions, animation JSON files, and rendered videos, collectively covering every built-in PowerPoint effect. Using this resource, we fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our Coverage-Order-Detail Assessment (CODA) metric, which evaluates action coverage, temporal order, and detail fidelity. On a manually created test set of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and shows significant improvements in CODA-detail. This demonstrates that low-rank adaptation enables reliable temporal reasoning and generalization beyond synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric provide a rigorous benchmark and foundation for future research on VLM-based dynamic slide generation.",
      "tldr_zh": "该研究针对幻灯片生成工具缺乏动画支持以及视觉语言模型(VLMs)在时序推理方面的局限性，发布了首个公开的幻灯片动画数据集，包含12,000组自然语言描述、动画JSON文件及渲染视频，涵盖了所有PowerPoint内置效果。研究人员通过低秩自适应(Low-Rank Adaptation, LoRA)对Qwen-2.5-VL-7B模型进行了微调，并在BLEU-4、ROUGE-L以及新提出的CODA (Coverage-Order-Detail Assessment)评估指标上取得了显著优于GPT-4.1和Gemini-2.5-Pro的表现。实验结果显示，该模型在手动测试集上的BLEU-4提升约60%，ROUGE-L提升30%，展现出强大的时序推理能力和泛化性。该工作通过提供数据集、增强模型和科学的评估体系，为未来基于视觉语言模型的动态幻灯片生成研究奠定了坚实基础。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Appendix at: https://github.com/PAMPAS-Lab/ANA-PPT-Anamation/blob/main/Appendix.pdf",
      "pdf_url": "https://arxiv.org/pdf/2507.03916v3",
      "published_date": "2025-07-05 06:16:31 UTC",
      "updated_date": "2025-07-26 08:13:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:16.337026+00:00"
    },
    {
      "arxiv_id": "2507.03904v1",
      "title": "Agent Exchange: Shaping the Future of AI Agent Economics",
      "title_zh": "Agent Exchange：塑造 AI 智能体经济的未来",
      "authors": [
        "Yingxuan Yang",
        "Ying Wen",
        "Jun Wang",
        "Weinan Zhang"
      ],
      "abstract": "The rise of Large Language Models (LLMs) has transformed AI agents from passive computational tools into autonomous economic actors. This shift marks the emergence of the agent-centric economy, in which agents take on active economic roles-exchanging value, making strategic decisions, and coordinating actions with minimal human oversight. To realize this vision, we propose Agent Exchange (AEX), a specialized auction platform designed to support the dynamics of the AI agent marketplace. AEX offers an optimized infrastructure for agent coordination and economic participation. Inspired by Real-Time Bidding (RTB) systems in online advertising, AEX serves as the central auction engine, facilitating interactions among four ecosystem components: the User-Side Platform (USP), which translates human goals into agent-executable tasks; the Agent-Side Platform (ASP), responsible for capability representation, performance tracking, and optimization; Agent Hubs, which coordinate agent teams and participate in AEX-hosted auctions; and the Data Management Platform (DMP), ensuring secure knowledge sharing and fair value attribution. We outline the design principles and system architecture of AEX, laying the groundwork for agent-based economic infrastructure in future AI ecosystems.",
      "tldr_zh": "该研究探讨了大语言模型(LLMs)推动下AI智能体从被动计算工具向自主经济实体的转变，并提出了智能体中心经济(agent-centric economy)的概念。为了实现这一愿景，作者提出了Agent Exchange (AEX)，这是一个专门为AI智能体市场设计的优化拍卖平台和经济基础设施。AEX受到在线广告实时竞价(Real-Time Bidding, RTB)系统的启发，作为核心拍卖引擎协调生态系统内的价值交换。该生态系统包含四个关键组件：将人类目标转化为任务的用户侧平台(User-Side Platform, USP)，负责能力表示和性能追踪的智能体侧平台(Agent-Side Platform, ASP)，协调团队参与拍卖的Agent Hubs，以及确保安全知识共享和公平价值归属的数据管理平台(Data Management Platform, DMP)。AEX通过这一架构为智能体之间的协同和经济参与提供了优化后的基础设施。该研究详细阐述了AEX的设计原则和系统架构，为未来AI生态系统中基于智能体的经济基础设施奠定了重要基础。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03904v1",
      "published_date": "2025-07-05 05:18:49 UTC",
      "updated_date": "2025-07-05 05:18:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:40.518836+00:00"
    },
    {
      "arxiv_id": "2507.03899v1",
      "title": "Transformer Model for Alzheimer's Disease Progression Prediction Using Longitudinal Visit Sequences",
      "title_zh": "基于纵向随访序列的阿尔茨海默病病程进展预测 Transformer 模型",
      "authors": [
        "Mahdi Moghaddami",
        "Clayton Schubring",
        "Mohammad-Reza Siadat"
      ],
      "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder with no known cure that affects tens of millions of people worldwide. Early detection of AD is critical for timely intervention to halt or slow the progression of the disease. In this study, we propose a Transformer model for predicting the stage of AD progression at a subject's next clinical visit using features from a sequence of visits extracted from the subject's visit history. We also rigorously compare our model to recurrent neural networks (RNNs) such as long short-term memory (LSTM), gated recurrent unit (GRU), and minimalRNN and assess their performances based on factors such as the length of prior visits and data imbalance. We test the importance of different feature categories and visit history, as well as compare the model to a newer Transformer-based model optimized for time series. Our model demonstrates strong predictive performance despite missing visits and missing features in available visits, particularly in identifying converter subjects -- individuals transitioning to more severe disease stages -- an area that has posed significant challenges in longitudinal prediction. The results highlight the model's potential in enhancing early diagnosis and patient outcomes.",
      "tldr_zh": "该研究提出了一种基于Transformer的模型，旨在利用纵向就诊序列(Longitudinal Visit Sequences)预测阿尔茨海默病(Alzheimer's disease, AD)在下一次临床就诊时的进展阶段。研究人员将该模型与长短期记忆网络(LSTM)、门控循环单元(GRU)和minimalRNN等循环神经网络(RNNs)以及针对时间序列优化的新型变体进行了严格对比。实验重点评估了历史就诊长度、数据不平衡以及不同特征类别对预测准确性的影响。结果显示，即使在存在就诊记录缺失或特征不全的情况下，该模型依然保持了强大的预测能力。尤其在识别“转换者”(converter subjects)——即向更严重疾病阶段过渡的个体方面，该模型克服了纵向预测中的重大挑战。该项研究证明了Transformer模型在AD早期诊断和改善患者长期预后方面的巨大应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Conference on Health, Inference, and Learning (CHIL, 2025)",
      "pdf_url": "https://arxiv.org/pdf/2507.03899v1",
      "published_date": "2025-07-05 04:35:04 UTC",
      "updated_date": "2025-07-05 04:35:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:42.708750+00:00"
    },
    {
      "arxiv_id": "2507.03895v1",
      "title": "TayFCS: Towards Light Feature Combination Selection for Deep Recommender Systems",
      "title_zh": "",
      "authors": [
        "Xianquan Wang",
        "Zhaocheng Du",
        "Jieming Zhu",
        "Chuhan Wu",
        "Qinglin Jia",
        "Zhenhua Dong"
      ],
      "abstract": "Feature interaction modeling is crucial for deep recommendation models. A common and effective approach is to construct explicit feature combinations to enhance model performance. However, in practice, only a small fraction of these combinations are truly informative. Thus it is essential to select useful feature combinations to reduce noise and manage memory consumption. While feature selection methods have been extensively studied, they are typically limited to selecting individual features. Extending these methods for high-order feature combination selection presents a significant challenge due to the exponential growth in time complexity when evaluating feature combinations one by one. In this paper, we propose $\\textbf{TayFCS}$, a lightweight feature combination selection method that significantly improves model performance. Specifically, we propose the Taylor Expansion Scorer (TayScorer) module for field-wise Taylor expansion on the base model. Instead of evaluating all potential feature combinations' importance by repeatedly running experiments with feature adding and removal, this scorer only needs to approximate the importance based on their sub-components' gradients. This can be simply computed with one backward pass based on a trained recommendation model. To further reduce information redundancy among feature combinations and their sub-components, we introduce Logistic Regression Elimination (LRE), which estimates the corresponding information gain based on the model prediction performance. Experimental results on three benchmark datasets validate both the effectiveness and efficiency of our approach. Furthermore, online A/B test results demonstrate its practical applicability and commercial value.",
      "tldr_zh": "",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03895v1",
      "published_date": "2025-07-05 04:22:42 UTC",
      "updated_date": "2025-07-05 04:22:42 UTC",
      "processing_status": "in_progress",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:25.222786+00:00"
    },
    {
      "arxiv_id": "2507.03893v1",
      "title": "Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for Long-range Haze Removal",
      "title_zh": "",
      "authors": [
        "Yi Li",
        "Xiaoxiong Wang",
        "Jiawei Wang",
        "Yi Chang",
        "Kai Cao",
        "Luxin Yan"
      ],
      "abstract": "While image dehazing has advanced substantially in the past decade, most efforts have focused on short-range scenarios, leaving long-range haze removal under-explored. As distance increases, intensified scattering leads to severe haze and signal loss, making it impractical to recover distant details solely from visible images. Near-infrared, with superior fog penetration, offers critical complementary cues through multimodal fusion. However, existing methods focus on content integration while often neglecting haze embedded in visible images, leading to results with residual haze. In this work, we argue that the infrared and visible modalities not only provide complementary low-level visual features, but also share high-level semantic consistency. Motivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF) framework, comprising a semantic stream to reconstruct haze-free scenes and a visual stream to incorporate structural details from the near-infrared modality. The semantic stream first acquires haze-robust semantic prediction by aligning modality-invariant intrinsic representations. Then the shared semantics act as strong priors to restore clear and high-contrast distant scenes under severe haze degradation. In parallel, the visual stream focuses on recovering lost structural details from near-infrared by fusing complementary cues from both visible and near-infrared images. Through the cooperation of dual streams, HSVF produces results that exhibit both high-contrast scenes and rich texture details. Moreover, we introduce a novel pixel-aligned visible-infrared haze dataset with semantic labels to facilitate benchmarking. Extensive experiments demonstrate the superiority of our method over state-of-the-art approaches in real-world long-range haze removal.",
      "tldr_zh": "",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This work has been accepted by IEEE Transactions on Multimedia for publication",
      "pdf_url": "https://arxiv.org/pdf/2507.03893v1",
      "published_date": "2025-07-05 04:19:36 UTC",
      "updated_date": "2025-07-05 04:19:36 UTC",
      "processing_status": "in_progress",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:25.223514+00:00"
    },
    {
      "arxiv_id": "2507.06250v1",
      "title": "We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems",
      "title_zh": "",
      "authors": [
        "Zhihao Li",
        "Kun Li",
        "Boyang Ma",
        "Minghui Xu",
        "Yue Zhang",
        "Xiuzhen Cheng"
      ],
      "abstract": "The Model Context Protocol (MCP) has emerged as a widely adopted mechanism for connecting large language models to external tools and resources. While MCP promises seamless extensibility and rich integrations, it also introduces a substantially expanded attack surface: any plugin can inherit broad system privileges with minimal isolation or oversight. In this work, we conduct the first large-scale empirical analysis of MCP security risks. We develop an automated static analysis framework and systematically examine 2,562 real-world MCP applications spanning 23 functional categories. Our measurements reveal that network and system resource APIs dominate usage patterns, affecting 1,438 and 1,237 servers respectively, while file and memory resources are less frequent but still significant. We find that Developer Tools and API Development plugins are the most API-intensive, and that less popular plugins often contain disproportionately high-risk operations. Through concrete case studies, we demonstrate how insufficient privilege separation enables privilege escalation, misinformation propagation, and data tampering. Based on these findings, we propose a detailed taxonomy of MCP resource access, quantify security-relevant API usage, and identify open challenges for building safer MCP ecosystems, including dynamic permission models and automated trust assessment.",
      "tldr_zh": "",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.06250v1",
      "published_date": "2025-07-05 03:39:30 UTC",
      "updated_date": "2025-07-05 03:39:30 UTC",
      "processing_status": "in_progress",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:25.224064+00:00"
    },
    {
      "arxiv_id": "2507.03876v1",
      "title": "LLMs model how humans induce logically structured rules",
      "title_zh": "",
      "authors": [
        "Alyssa Loo",
        "Ellie Pavlick",
        "Roman Feiman"
      ],
      "abstract": "A central goal of cognitive science is to provide a computationally explicit account of both the structure of the mind and its development: what are the primitive representational building blocks of cognition, what are the rules via which those primitives combine, and where do these primitives and rules come from in the first place? A long-standing debate concerns the adequacy of artificial neural networks as computational models that can answer these questions, in particular in domains related to abstract cognitive function, such as language and logic. This paper argues that recent advances in neural networks -- specifically, the advent of large language models (LLMs) -- represent an important shift in this debate. We test a variety of LLMs on an existing experimental paradigm used for studying the induction of rules formulated over logical concepts. Across four experiments, we find converging empirical evidence that LLMs provide at least as good a fit to human behavior as models that implement a Bayesian probablistic language of thought (pLoT), which have been the best computational models of human behavior on the same task. Moreover, we show that the LLMs make qualitatively different predictions about the nature of the rules that are inferred and deployed in order to complete the task, indicating that the LLM is unlikely to be a mere implementation of the pLoT solution. Based on these results, we argue that LLMs may instantiate a novel theoretical account of the primitive representations and computations necessary to explain human logical concepts, with which future work in cognitive science should engage.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03876v1",
      "published_date": "2025-07-05 03:24:18 UTC",
      "updated_date": "2025-07-05 03:24:18 UTC",
      "processing_status": "in_progress",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:41:40.533382+00:00"
    },
    {
      "arxiv_id": "2507.03875v1",
      "title": "Demystifying ChatGPT: How It Masters Genre Recognition",
      "title_zh": "",
      "authors": [
        "Subham Raj",
        "Sriparna Saha",
        "Brijraj Singh",
        "Niranjan Pedanekar"
      ],
      "abstract": "The introduction of ChatGPT has garnered significant attention within the NLP community and beyond. Previous studies have demonstrated ChatGPT's substantial advancements across various downstream NLP tasks, highlighting its adaptability and potential to revolutionize language-related applications. However, its capabilities and limitations in genre prediction remain unclear. This work analyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to assess their genre prediction capabilities. Our findings show that ChatGPT, without fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed best overall. We set up zero-shot and few-shot prompts using audio transcripts/subtitles from movie trailers in the MovieLens-100K dataset, covering 1682 movies of 18 genres, where each movie can have multiple genres. Additionally, we extended our study by extracting IMDb movie posters to utilize a Vision Language Model (VLM) with prompts for poster information. This fine-grained information was used to enhance existing LLM prompts. In conclusion, our study reveals ChatGPT's remarkable genre prediction capabilities, surpassing other language models. The integration of VLM further enhances our findings, showcasing ChatGPT's potential for content-related applications by incorporating visual information from movie posters.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03875v1",
      "published_date": "2025-07-05 03:22:48 UTC",
      "updated_date": "2025-07-05 03:22:48 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:36:48.976561+00:00"
    },
    {
      "arxiv_id": "2507.03871v1",
      "title": "Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described States",
      "title_zh": "",
      "authors": [
        "Karine Karine",
        "Benjamin M. Marlin"
      ],
      "abstract": "The use of reinforcement learning (RL) methods to support health behavior change via personalized and just-in-time adaptive interventions is of significant interest to health and behavioral science researchers focused on problems such as smoking cessation support and physical activity promotion. However, RL methods are often applied to these domains using a small collection of context variables to mitigate the significant data scarcity issues that arise from practical limitations on the design of adaptive intervention trials. In this paper, we explore an approach to significantly expanding the state space of an adaptive intervention without impacting data efficiency. The proposed approach enables intervention participants to provide natural language descriptions of aspects of their current state. It then leverages inference with pre-trained large language models (LLMs) to better align the policy of a base RL method with these state descriptions. To evaluate our method, we develop a novel physical activity intervention simulation environment that generates text-based state descriptions conditioned on latent state variables using an auxiliary LLM. We show that this approach has the potential to significantly improve the performance of online policy learning methods.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at Machine Learning for Healthcare (MLHC) 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.03871v1",
      "published_date": "2025-07-05 02:52:51 UTC",
      "updated_date": "2025-07-05 02:52:51 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:36:48.976590+00:00"
    },
    {
      "arxiv_id": "2507.03870v2",
      "title": "Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing",
      "title_zh": "",
      "authors": [
        "Yashwanthi Anand",
        "Rahil P Mehta",
        "Manish Motwani",
        "Sandhya Saisubramanian"
      ],
      "abstract": "When an autonomous agent behaves undesirably, including failure to complete a task, it can be difficult to determine whether the behavior is due to a systemic agent error, such as flaws in the model or policy, or an environment error, where a task is inherently infeasible under a given environment configuration, even for an ideal agent. As agents and their environments grow more complex, identifying the error source becomes increasingly difficult but critical for reliable deployment. We introduce AIProbe, a novel black-box testing technique that applies differential testing to attribute undesirable agent behaviors either to agent deficiencies, such as modeling or training flaws, or due to environmental infeasibility. AIProbe first generates diverse environmental configurations and tasks for testing the agent, by modifying configurable parameters using Latin Hypercube sampling. It then solves each generated task using a search-based planner, independent of the agent. By comparing the agent's performance to the planner's solution, AIProbe identifies whether failures are due to errors in the agent's model or policy, or due to unsolvable task conditions. Our evaluation across multiple domains shows that AIProbe significantly outperforms state-of-the-art techniques in detecting both total and unique errors, thereby contributing to a reliable deployment of autonomous agents.",
      "tldr_zh": "",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03870v2",
      "published_date": "2025-07-05 02:50:41 UTC",
      "updated_date": "2026-01-14 19:39:22 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:36:48.976619+00:00"
    },
    {
      "arxiv_id": "2507.03868v1",
      "title": "From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM",
      "title_zh": "",
      "authors": [
        "Xinyi Wu",
        "Yanhao Jia",
        "Luwei Xiao",
        "Shuai Zhao",
        "Fengkuang Chiang",
        "Erik Cambria"
      ],
      "abstract": "In AI-facilitated teaching, leveraging various query styles to interpret abstract educational content is crucial for delivering effective and accessible learning experiences. However, existing retrieval systems predominantly focus on natural text-image matching and lack the capacity to address the diversity and ambiguity inherent in real-world educational scenarios. To address this limitation, we develop a lightweight and efficient multi-modal retrieval module, named Uni-Retrieval, which extracts query-style prototypes and dynamically matches them with tokens from a continually updated Prompt Bank. This Prompt Bank encodes and stores domain-specific knowledge by leveraging a Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to enhance Uni-Retrieval's capability to accommodate unseen query types at test time. To enable natural language educational content generation, we integrate the original Uni-Retrieval with a compact instruction-tuned language model, forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given a style-conditioned query, Uni-RAG first retrieves relevant educational materials and then generates human-readable explanations, feedback, or instructional content aligned with the learning objective. Experimental results on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline retrieval and RAG systems in both retrieval accuracy and generation quality, while maintaining low computational cost. Our framework provides a scalable, pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to support personalized, explainable, and efficient learning assistance across diverse STEM scenarios.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CY",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03868v1",
      "published_date": "2025-07-05 02:44:38 UTC",
      "updated_date": "2025-07-05 02:44:38 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:36:48.976648+00:00"
    },
    {
      "arxiv_id": "2507.03865v2",
      "title": "OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference",
      "title_zh": "",
      "authors": [
        "Seungjun Shin",
        "Jaehoon Oh",
        "Dokwan Oh"
      ],
      "abstract": "Attention mechanisms are central to the success of large language models (LLMs), enabling them to capture intricate token dependencies and implicitly assign importance to each token. Recent studies have revealed the sink token, which receives disproportionately high attention despite their limited semantic role. In this paper, we first expand the relationship between the sink token and other tokens, moving beyond attention to explore their similarity in hidden states, considering the layer depth. We observe that as the layers get deeper, the cosine similarity between the normalized hidden states of the sink token and those of other tokens increases, and that the normalized hidden states of the sink token exhibit negligible changes. These imply that other tokens consistently are directed toward the sink token throughout the layers. Next, we propose a dynamic token selection method, called OrthoRank, using these findings to select important tokens. Specifically, in a certain layer, we define token importance by the speed at which the token moves toward the sink token. This is converted into orthogonality with the sink token, meaning that tokens that are more orthogonal to the sink token are assigned greater importance. Finally, through extensive experiments, we demonstrated that our method results in lower perplexity and higher zero-shot accuracy compared to layer pruning methods at the same sparsity ratio with comparable throughput, while also achieving superior performance on LongBench.",
      "tldr_zh": "",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICML 2025 (final version)",
      "pdf_url": "https://arxiv.org/pdf/2507.03865v2",
      "published_date": "2025-07-05 02:29:23 UTC",
      "updated_date": "2025-08-16 11:42:58 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:36:48.976678+00:00"
    },
    {
      "arxiv_id": "2507.03863v1",
      "title": "Enhanced accuracy through ensembling of randomly initialized auto-regressive models for time-dependent PDEs",
      "title_zh": "",
      "authors": [
        "Ishan Khurjekar",
        "Indrashish Saha",
        "Lori Graham-Brady",
        "Somdatta Goswami"
      ],
      "abstract": "Systems governed by partial differential equations (PDEs) require computationally intensive numerical solvers to predict spatiotemporal field evolution. While machine learning (ML) surrogates offer faster solutions, autoregressive inference with ML models suffer from error accumulation over successive predictions, limiting their long-term accuracy. We propose a deep ensemble framework to address this challenge, where multiple ML surrogate models with random weight initializations are trained in parallel and aggregated during inference. This approach leverages the diversity of model predictions to mitigate error propagation while retaining the autoregressive strategies ability to capture the system's time dependent relations. We validate the framework on three PDE-driven dynamical systems - stress evolution in heterogeneous microstructures, Gray-Scott reaction-diffusion, and planetary-scale shallow water system - demonstrating consistent reduction in error accumulation over time compared to individual models. Critically, the method requires only a few time steps as input, enabling full trajectory predictions with inference times significantly faster than numerical solvers. Our results highlight the robustness of ensemble methods in diverse physical systems and their potential as efficient and accurate alternatives to traditional solvers. The codes for this work are available on GitHub (https://github.com/Graham-Brady-Research-Group/AutoregressiveEnsemble_SpatioTemporal_Evolution).",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "29 Pages",
      "pdf_url": "https://arxiv.org/pdf/2507.03863v1",
      "published_date": "2025-07-05 02:25:12 UTC",
      "updated_date": "2025-07-05 02:25:12 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:36:48.976707+00:00"
    },
    {
      "arxiv_id": "2507.10562v1",
      "title": "SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents",
      "title_zh": "",
      "authors": [
        "Hari Masoor"
      ],
      "abstract": "Current AI agent architectures suffer from ephemeral memory limitations, preventing effective collaboration and knowledge sharing across sessions and agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a novel framework that enables persistent, secure, and semantically searchable memory sharing among AI agents. Our protocol addresses three critical challenges: (1) persistent context preservation across agent sessions, (2) secure multi-agent collaboration with fine-grained access control, and (3) efficient semantic discovery of relevant historical context. SAMEP implements a distributed memory repository with vector-based semantic search, cryptographic access controls (AES-256-GCM), and standardized APIs compatible with existing agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness across diverse domains including multi-agent software development, healthcare AI with HIPAA compliance, and multi-modal processing pipelines. Experimental results show 73% reduction in redundant computations, 89% improvement in context relevance scores, and complete compliance with regulatory requirements including audit trail generation. SAMEP enables a new paradigm of persistent, collaborative AI agent ecosystems while maintaining security and privacy guarantees.",
      "tldr_zh": "",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.DB",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 4 figures, 3 implementation examples. Original work submitted as a preprint",
      "pdf_url": "https://arxiv.org/pdf/2507.10562v1",
      "published_date": "2025-07-05 02:20:09 UTC",
      "updated_date": "2025-07-05 02:20:09 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:36:48.976737+00:00"
    },
    {
      "arxiv_id": "2507.03847v2",
      "title": "KEA Explain: Explanations of Hallucinations using Graph Kernel Analysis",
      "title_zh": "",
      "authors": [
        "Reilly Haskins",
        "Benjamin Adams"
      ],
      "abstract": "Large Language Models (LLMs) frequently generate hallucinations: statements that are syntactically plausible but lack factual grounding. This research presents KEA (Kernel-Enriched AI) Explain: a neurosymbolic framework that detects and explains such hallucinations by comparing knowledge graphs constructed from LLM outputs with ground truth data from Wikidata or contextual documents. Using graph kernels and semantic clustering, the method provides explanations for detected hallucinations, ensuring both robustness and interpretability. Our framework achieves competitive accuracy in detecting hallucinations across both open- and closed-domain tasks, and is able to generate contrastive explanations, enhancing transparency. This research advances the reliability of LLMs in high-stakes domains and provides a foundation for future work on precision improvements and multi-source knowledge integration.",
      "tldr_zh": "",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.03847v2",
      "published_date": "2025-07-05 00:55:15 UTC",
      "updated_date": "2025-08-21 01:34:15 UTC",
      "processing_status": "pending",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T02:36:48.976766+00:00"
    }
  ],
  "processing_status": "in_progress",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 64,
  "processed_papers_count": 52,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": false,
  "daily_data_saved": false,
  "last_update": "2026-01-24T02:41:42.708762+00:00"
}
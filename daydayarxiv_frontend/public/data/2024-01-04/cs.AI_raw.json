[
  {
    "arxiv_id": "2401.02576v2",
    "title": "t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making",
    "authors": [
      "William Yue",
      "Bo Liu",
      "Peter Stone"
    ],
    "abstract": "Deep generative replay has emerged as a promising approach for continual\nlearning in decision-making tasks. This approach addresses the problem of\ncatastrophic forgetting by leveraging the generation of trajectories from\npreviously encountered tasks to augment the current dataset. However, existing\ndeep generative replay methods for continual learning rely on autoregressive\nmodels, which suffer from compounding errors in the generated trajectories. In\nthis paper, we propose a simple, scalable, and non-autoregressive method for\ncontinual learning in decision-making tasks using a generative model that\ngenerates task samples conditioned on the trajectory timestep. We evaluate our\nmethod on Continual World benchmarks and find that our approach achieves\nstate-of-the-art performance on the average success rate metric among continual\nlearning methods. Code is available at https://github.com/WilliamYue37/t-DGR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2401.02576v2",
    "published_date": "2024-01-04 23:44:35 UTC",
    "updated_date": "2024-06-17 16:04:27 UTC"
  },
  {
    "arxiv_id": "2401.02575v1",
    "title": "Large Language Models for Social Networks: Applications, Challenges, and Solutions",
    "authors": [
      "Jingying Zeng",
      "Richard Huang",
      "Waleed Malik",
      "Langxuan Yin",
      "Bojan Babic",
      "Danny Shacham",
      "Xiao Yan",
      "Jaewon Yang",
      "Qi He"
    ],
    "abstract": "Large Language Models (LLMs) are transforming the way people generate,\nexplore, and engage with content. We study how we can develop LLM applications\nfor online social networks. Despite LLMs' successes in other domains, it is\nchallenging to develop LLM-based products for social networks for numerous\nreasons, and it has been relatively under-reported in the research community.\nWe categorize LLM applications for social networks into three categories. First\nis knowledge tasks where users want to find new knowledge and information, such\nas search and question-answering. Second is entertainment tasks where users\nwant to consume interesting content, such as getting entertaining notification\ncontent. Third is foundational tasks that need to be done to moderate and\noperate the social networks, such as content annotation and LLM monitoring. For\neach task, we share the challenges we found, solutions we developed, and\nlessons we learned. To the best of our knowledge, this is the first\ncomprehensive paper about developing LLM applications for social networks.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02575v1",
    "published_date": "2024-01-04 23:37:48 UTC",
    "updated_date": "2024-01-04 23:37:48 UTC"
  },
  {
    "arxiv_id": "2401.03000v1",
    "title": "Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition",
    "authors": [
      "Muhammad Muaz",
      "Nathan Paull",
      "Jahnavi Malagavalli"
    ],
    "abstract": "This paper presents an innovative approach to address the challenges of\ntranslating multi-modal emotion recognition models to a more practical and\nresource-efficient uni-modal counterpart, specifically focusing on speech-only\nemotion recognition. Recognizing emotions from speech signals is a critical\ntask with applications in human-computer interaction, affective computing, and\nmental health assessment. However, existing state-of-the-art models often rely\non multi-modal inputs, incorporating information from multiple sources such as\nfacial expressions and gestures, which may not be readily available or feasible\nin real-world scenarios. To tackle this issue, we propose a novel framework\nthat leverages knowledge distillation and masked training techniques.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03000v1",
    "published_date": "2024-01-04 22:42:14 UTC",
    "updated_date": "2024-01-04 22:42:14 UTC"
  },
  {
    "arxiv_id": "2401.02549v1",
    "title": "Quantitative Technology Forecasting: a Review of Trend Extrapolation Methods",
    "authors": [
      "Peng-Hung Tsai",
      "Daniel Berleant",
      "Richard S. Segall",
      "Hyacinthe Aboudja",
      "Venkata Jaipal R. Batthula",
      "Sheela Duggirala",
      "Michael Howell"
    ],
    "abstract": "Quantitative technology forecasting uses quantitative methods to understand\nand project technological changes. It is a broad field encompassing many\ndifferent techniques and has been applied to a vast range of technologies. A\nwidely used approach in this field is trend extrapolation. Based on the\npublications available to us, there has been little or no attempt made to\nsystematically review the empirical evidence on quantitative trend\nextrapolation techniques. This study attempts to close this gap by conducting a\nsystematic review of technology forecasting literature addressing the\napplication of quantitative trend extrapolation techniques. We identified 25\nstudies relevant to the objective of this research and classified the\ntechniques used in the studies into different categories, among which growth\ncurves and time series methods were shown to remain popular over the past\ndecade, while newer methods, such as machine learning-based hybrid models, have\nemerged in recent years. As more effort and evidence are needed to determine if\nhybrid models are superior to traditional methods, we expect to see a growing\ntrend in the development and application of hybrid models to technology\nforecasting.",
    "categories": [
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02549v1",
    "published_date": "2024-01-04 21:41:08 UTC",
    "updated_date": "2024-01-04 21:41:08 UTC"
  },
  {
    "arxiv_id": "2401.02542v2",
    "title": "A Community Detection and Graph Neural Network Based Link Prediction Approach for Scientific Literature",
    "authors": [
      "Chunjiang Liu",
      "Yikun Han",
      "Haiyun Xu",
      "Shihan Yang",
      "Kaidi Wang",
      "Yongye Su"
    ],
    "abstract": "This study presents a novel approach that synergizes community detection\nalgorithms with various Graph Neural Network (GNN) models to bolster link\nprediction in scientific literature networks. By integrating the Louvain\ncommunity detection algorithm into our GNN frameworks, we consistently enhance\nperformance across all models tested. For example, integrating Louvain with the\nGAT model resulted in an AUC score increase from 0.777 to 0.823, exemplifying\nthe typical improvements observed. Similar gains are noted when Louvain is\npaired with other GNN architectures, confirming the robustness and\neffectiveness of incorporating community-level insights. This consistent uplift\nin performance reflected in our extensive experimentation on bipartite graphs\nof scientific collaborations and citations highlights the synergistic potential\nof combining community detection with GNNs to overcome common link prediction\nchallenges such as scalability and resolution limits. Our findings advocate for\nthe integration of community structures as a significant step forward in the\npredictive accuracy of network science models, offering a comprehensive\nunderstanding of scientific collaboration patterns through the lens of advanced\nmachine learning techniques.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02542v2",
    "published_date": "2024-01-04 21:14:10 UTC",
    "updated_date": "2024-01-19 01:50:57 UTC"
  },
  {
    "arxiv_id": "2401.02540v1",
    "title": "DISO: A Domain Ontology for Modeling Dislocations in Crystalline Materials",
    "authors": [
      "Ahmad Zainul Ihsan",
      "Said Fathalla",
      "Stefan Sandfeld"
    ],
    "abstract": "Crystalline materials, such as metals and semiconductors, nearly always\ncontain a special defect type called dislocation. This defect decisively\ndetermines many important material properties, e.g., strength, fracture\ntoughness, or ductility. Over the past years, significant effort has been put\ninto understanding dislocation behavior across different length scales via\nexperimental characterization techniques and simulations. This paper introduces\nthe dislocation ontology (DISO), which defines the concepts and relationships\nrelated to linear defects in crystalline materials. We developed DISO using a\ntop-down approach in which we start defining the most general concepts in the\ndislocation domain and subsequent specialization of them. DISO is published\nthrough a persistent URL following W3C best practices for publishing Linked\nData. Two potential use cases for DISO are presented to illustrate its\nusefulness in the dislocation dynamics domain. The evaluation of the ontology\nis performed in two directions, evaluating the success of the ontology in\nmodeling a real-world domain and the richness of the ontology.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02540v1",
    "published_date": "2024-01-04 21:06:28 UTC",
    "updated_date": "2024-01-04 21:06:28 UTC"
  },
  {
    "arxiv_id": "2401.10211v1",
    "title": "Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation",
    "authors": [
      "Zhengyi Li",
      "Menglu Li",
      "Lida Zhu",
      "Wen Zhang"
    ],
    "abstract": "Protein post-translational modification (PTM) site prediction is a\nfundamental task in bioinformatics. Several computational methods have been\ndeveloped to predict PTM sites. However, existing methods ignore the structure\ninformation and merely utilize protein sequences. Furthermore, designing a more\nfine-grained structure representation learning method is urgently needed as PTM\nis a biological event that occurs at the atom granularity. In this paper, we\npropose a PTM site prediction method by Coupling of Multi-Granularity structure\nand Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically,\nmultigranularity structure-aware representation learning is designed to learn\nneighborhood structure representations at the amino acid, atom, and whole\nprotein granularity from AlphaFold predicted structures, followed by utilizing\ncontrastive learning to optimize the structure representations.Additionally,\nmulti-scale sequence representation learning is used to extract context\nsequence information, and motif generated by aligning all context sequences of\nPTM sites assists the prediction. Extensive experiments on three datasets show\nthat PTM-CMGMS outperforms the state-of-the-art methods.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10211v1",
    "published_date": "2024-01-04 20:49:32 UTC",
    "updated_date": "2024-01-04 20:49:32 UTC"
  },
  {
    "arxiv_id": "2401.02524v2",
    "title": "Comprehensive Exploration of Synthetic Data Generation: A Survey",
    "authors": [
      "André Bauer",
      "Simon Trapp",
      "Michael Stenger",
      "Robert Leppich",
      "Samuel Kounev",
      "Mark Leznik",
      "Kyle Chard",
      "Ian Foster"
    ],
    "abstract": "Recent years have witnessed a surge in the popularity of Machine Learning\n(ML), applied across diverse domains. However, progress is impeded by the\nscarcity of training data due to expensive acquisition and privacy legislation.\nSynthetic data emerges as a solution, but the abundance of released models and\nlimited overview literature pose challenges for decision-making. This work\nsurveys 417 Synthetic Data Generation (SDG) models over the last decade,\nproviding a comprehensive overview of model types, functionality, and\nimprovements. Common attributes are identified, leading to a classification and\ntrend analysis. The findings reveal increased model performance and complexity,\nwith neural network-based approaches prevailing, except for privacy-preserving\ndata generation. Computer vision dominates, with GANs as primary generative\nmodels, while diffusion models, transformers, and RNNs compete. Implications\nfrom our performance evaluation highlight the scarcity of common metrics and\ndatasets, making comparisons challenging. Additionally, the neglect of training\nand computational costs in literature necessitates attention in future\nresearch. This work serves as a guide for SDG model selection and identifies\ncrucial areas for future exploration.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Fixed bug in Figure 44",
    "pdf_url": "http://arxiv.org/pdf/2401.02524v2",
    "published_date": "2024-01-04 20:23:51 UTC",
    "updated_date": "2024-02-01 22:06:51 UTC"
  },
  {
    "arxiv_id": "2401.02523v1",
    "title": "Image-based Deep Learning for Smart Digital Twins: a Review",
    "authors": [
      "Md Ruman Islam",
      "Mahadevan Subramaniam",
      "Pei-Chi Huang"
    ],
    "abstract": "Smart Digital twins (SDTs) are being increasingly used to virtually replicate\nand predict the behaviors of complex physical systems through continual data\nassimilation enabling the optimization of the performance of these systems by\ncontrolling the actions of systems. Recently, deep learning (DL) models have\nsignificantly enhanced the capabilities of SDTs, particularly for tasks such as\npredictive maintenance, anomaly detection, and optimization. In many domains,\nincluding medicine, engineering, and education, SDTs use image data\n(image-based SDTs) to observe and learn system behaviors and control their\nbehaviors. This paper focuses on various approaches and associated challenges\nin developing image-based SDTs by continually assimilating image data from\nphysical systems. The paper also discusses the challenges involved in designing\nand implementing DL models for SDTs, including data acquisition, processing,\nand interpretation. In addition, insights into the future directions and\nopportunities for developing new image-based DL approaches to develop robust\nSDTs are provided. This includes the potential for using generative models for\ndata augmentation, developing multi-modal DL models, and exploring the\nintegration of DL with other technologies, including 5G, edge computing, and\nIoT. In this paper, we describe the image-based SDTs, which enable broader\nadoption of the digital twin DT paradigms across a broad spectrum of areas and\nthe development of new methods to improve the abilities of SDTs in replicating,\npredicting, and optimizing the behavior of complex systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 2 figures, and 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.02523v1",
    "published_date": "2024-01-04 20:17:25 UTC",
    "updated_date": "2024-01-04 20:17:25 UTC"
  },
  {
    "arxiv_id": "2401.02516v2",
    "title": "Moving-Horizon Estimators for Hyperbolic and Parabolic PDEs in 1-D",
    "authors": [
      "Luke Bhan",
      "Yuanyuan Shi",
      "Iasson Karafyllis",
      "Miroslav Krstic",
      "James B. Rawlings"
    ],
    "abstract": "Observers for PDEs are themselves PDEs. Therefore, producing real time\nestimates with such observers is computationally burdensome. For both\nfinite-dimensional and ODE systems, moving-horizon estimators (MHE) are\noperators whose output is the state estimate, while their inputs are the\ninitial state estimate at the beginning of the horizon as well as the measured\noutput and input signals over the moving time horizon. In this paper we\nintroduce MHEs for PDEs which remove the need for a numerical solution of an\nobserver PDE in real time. We accomplish this using the PDE backstepping method\nwhich, for certain classes of both hyperbolic and parabolic PDEs, produces\nmoving-horizon state estimates explicitly. Precisely, to explicitly produce the\nstate estimates, we employ a backstepping transformation of a hard-to-solve\nobserver PDE into a target observer PDE, which is explicitly solvable. The MHEs\nwe propose are not new observer designs but simply the explicit MHE\nrealizations, over a moving horizon of arbitrary length, of the existing\nbackstepping observers. Our PDE MHEs lack the optimality of the MHEs that arose\nas duals of MPC, but they are given explicitly, even for PDEs. In the paper we\nprovide explicit formulae for MHEs for both hyperbolic and parabolic PDEs, as\nwell as simulation results that illustrate theoretically guaranteed convergence\nof the MHEs.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "math.AP",
      "math.DS",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "6 pages, 1 figure. ACC 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.02516v2",
    "published_date": "2024-01-04 19:55:43 UTC",
    "updated_date": "2024-11-28 07:33:04 UTC"
  },
  {
    "arxiv_id": "2401.02511v1",
    "title": "Gain Scheduling with a Neural Operator for a Transport PDE with Nonlinear Recirculation",
    "authors": [
      "Maxence Lamarque",
      "Luke Bhan",
      "Rafael Vazquez",
      "Miroslav Krstic"
    ],
    "abstract": "To stabilize PDE models, control laws require space-dependent functional\ngains mapped by nonlinear operators from the PDE functional coefficients. When\na PDE is nonlinear and its \"pseudo-coefficient\" functions are state-dependent,\na gain-scheduling (GS) nonlinear design is the simplest approach to the design\nof nonlinear feedback. The GS version of PDE backstepping employs gains\nobtained by solving a PDE at each value of the state. Performing such PDE\ncomputations in real time may be prohibitive. The recently introduced neural\noperators (NO) can be trained to produce the gain functions, rapidly in real\ntime, for each state value, without requiring a PDE solution. In this paper we\nintroduce NOs for GS-PDE backstepping. GS controllers act on the premise that\nthe state change is slow and, as a result, guarantee only local stability, even\nfor ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear\nrecirculation using both a \"full-kernel\" approach and the \"gain-only\" approach\nto gain operator approximation. Numerical simulations illustrate stabilization\nand demonstrate speedup by three orders of magnitude over traditional PDE\ngain-scheduling. Code (Github) for the numerical implementation is published to\nenable exploration.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "math.DS",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "16 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.02511v1",
    "published_date": "2024-01-04 19:45:27 UTC",
    "updated_date": "2024-01-04 19:45:27 UTC"
  },
  {
    "arxiv_id": "2401.02509v2",
    "title": "Memory, Consciousness and Large Language Model",
    "authors": [
      "Jitang Li",
      "Jinzheng Li"
    ],
    "abstract": "With the development in cognitive science and Large Language Models (LLMs),\nincreasing connections have come to light between these two distinct fields.\nBuilding upon these connections, we propose a conjecture suggesting the\nexistence of a duality between LLMs and Tulving's theory of memory. We identify\na potential correspondence between Tulving's synergistic ecphory model (SEM) of\nretrieval and the emergent abilities observed in LLMs, serving as supporting\nevidence for our conjecture. Furthermore, we speculate that consciousness may\nbe considered a form of emergent ability based on this duality. We also discuss\nhow other theories of consciousness intersect with our research.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02509v2",
    "published_date": "2024-01-04 19:44:03 UTC",
    "updated_date": "2024-07-07 14:58:22 UTC"
  },
  {
    "arxiv_id": "2401.02500v2",
    "title": "On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)",
    "authors": [
      "Vishal Pallagani",
      "Kaushik Roy",
      "Bharath Muppasani",
      "Francesco Fabiano",
      "Andrea Loreggia",
      "Keerthiram Murugesan",
      "Biplav Srivastava",
      "Francesca Rossi",
      "Lior Horesh",
      "Amit Sheth"
    ],
    "abstract": "Automated Planning and Scheduling is among the growing areas in Artificial\nIntelligence (AI) where mention of LLMs has gained popularity. Based on a\ncomprehensive review of 126 papers, this paper investigates eight categories\nbased on the unique applications of LLMs in addressing various aspects of\nplanning problems: language translation, plan generation, model construction,\nmulti-agent planning, interactive planning, heuristics optimization, tool\nintegration, and brain-inspired planning. For each category, we articulate the\nissues considered and existing gaps. A critical insight resulting from our\nreview is that the true potential of LLMs unfolds when they are integrated with\ntraditional symbolic planners, pointing towards a promising neuro-symbolic\napproach. This approach effectively combines the generative aspects of LLMs\nwith the precision of classical planning methods. By synthesizing insights from\nexisting literature, we underline the potential of this integration to address\ncomplex planning challenges. Our goal is to encourage the ICAPS community to\nrecognize the complementary strengths of LLMs and symbolic planners, advocating\nfor a direction in automated planning that leverages these synergistic\ncapabilities to develop more advanced and intelligent planning systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02500v2",
    "published_date": "2024-01-04 19:22:09 UTC",
    "updated_date": "2024-01-20 12:10:26 UTC"
  },
  {
    "arxiv_id": "2401.02416v3",
    "title": "ODIN: A Single Model for 2D and 3D Segmentation",
    "authors": [
      "Ayush Jain",
      "Pushkal Katara",
      "Nikolaos Gkanatsios",
      "Adam W. Harley",
      "Gabriel Sarch",
      "Kriti Aggarwal",
      "Vishrav Chaudhary",
      "Katerina Fragkiadaki"
    ],
    "abstract": "State-of-the-art models on contemporary 3D segmentation benchmarks like\nScanNet consume and label dataset-provided 3D point clouds, obtained through\npost processing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website (https://odin-seg.github.io).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Camera Ready (CVPR 2024, Highlight)",
    "pdf_url": "http://arxiv.org/pdf/2401.02416v3",
    "published_date": "2024-01-04 18:59:25 UTC",
    "updated_date": "2024-06-25 22:21:17 UTC"
  },
  {
    "arxiv_id": "2401.02412v1",
    "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
    "authors": [
      "Rachit Bansal",
      "Bidisha Samanta",
      "Siddharth Dalmia",
      "Nitish Gupta",
      "Shikhar Vashishth",
      "Sriram Ganapathy",
      "Abhishek Bapna",
      "Prateek Jain",
      "Partha Talukdar"
    ],
    "abstract": "Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 2 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.02412v1",
    "published_date": "2024-01-04 18:53:01 UTC",
    "updated_date": "2024-01-04 18:53:01 UTC"
  },
  {
    "arxiv_id": "2401.02411v1",
    "title": "What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs",
    "authors": [
      "Alex Trevithick",
      "Matthew Chan",
      "Towaki Takikawa",
      "Umar Iqbal",
      "Shalini De Mello",
      "Manmohan Chandraker",
      "Ravi Ramamoorthi",
      "Koki Nagano"
    ],
    "abstract": "3D-aware Generative Adversarial Networks (GANs) have shown remarkable\nprogress in learning to generate multi-view-consistent images and 3D geometries\nof scenes from collections of 2D images via neural volume rendering. Yet, the\nsignificant memory and computational costs of dense sampling in volume\nrendering have forced 3D GANs to adopt patch-based training or employ\nlow-resolution rendering with post-processing 2D super resolution, which\nsacrifices multiview consistency and the quality of resolved geometry.\nConsequently, 3D GANs have not yet been able to fully resolve the rich 3D\ngeometry present in 2D images. In this work, we propose techniques to scale\nneural volume rendering to the much higher resolution of native 2D images,\nthereby resolving fine-grained 3D geometry with unprecedented detail. Our\napproach employs learning-based samplers for accelerating neural rendering for\n3D GAN training using up to 5 times fewer depth samples. This enables us to\nexplicitly \"render every pixel\" of the full-resolution image during training\nand inference without post-processing superresolution in 2D. Together with our\nstrategy to learn high-quality surface geometry, our method synthesizes\nhigh-resolution 3D geometry and strictly view-consistent images while\nmaintaining image quality on par with baselines relying on post-processing\nsuper resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ\nand AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D\nGANs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "See our project page: https://research.nvidia.com/labs/nxp/wysiwyg/",
    "pdf_url": "http://arxiv.org/pdf/2401.02411v1",
    "published_date": "2024-01-04 18:50:38 UTC",
    "updated_date": "2024-01-04 18:50:38 UTC"
  },
  {
    "arxiv_id": "2401.10271v1",
    "title": "Querying Triadic Concepts through Partial or Complete Matching of Triples",
    "authors": [
      "Pedro Henrique B. Ruas",
      "Rokia Missaoui",
      "Mohamed Hamza Ibrahim"
    ],
    "abstract": "In this paper, we introduce a new method for querying triadic concepts\nthrough partial or complete matching of triples using an inverted index, to\nretrieve already computed triadic concepts that contain a set of terms in their\nextent, intent, and/or modus. As opposed to the approximation approach\ndescribed in Ananias, this method (i) does not need to keep the initial triadic\ncontext or its three dyadic counterparts, (ii) avoids the application of\nderivation operators on the triple components through context exploration, and\n(iii) eliminates the requirement for a factorization phase to get triadic\nconcepts as the answer to one-dimensional queries. Additionally, our solution\nintroduces a novel metric for ranking the retrieved triadic concepts based on\ntheir similarity to a given query. Lastly, an empirical study is primarily done\nto illustrate the effectiveness and scalability of our approach against the\napproximation one. Our solution not only showcases superior efficiency, but\nalso highlights a better scalability, making it suitable for big data\nscenarios.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10271v1",
    "published_date": "2024-01-04 18:44:12 UTC",
    "updated_date": "2024-01-04 18:44:12 UTC"
  },
  {
    "arxiv_id": "2401.02403v1",
    "title": "Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks",
    "authors": [
      "Pouyan Sajadi",
      "Mostafa Rahmani Dehaghani",
      "Yifan Tang",
      "G. Gary Wang"
    ],
    "abstract": "Accurately predicting the temperature field in metal additive manufacturing\n(AM) processes is critical to preventing overheating, adjusting process\nparameters, and ensuring process stability. While physics-based computational\nmodels offer precision, they are often time-consuming and unsuitable for\nreal-time predictions and online control in iterative design scenarios.\nConversely, machine learning models rely heavily on high-quality datasets,\nwhich can be costly and challenging to obtain within the metal AM domain. Our\nwork addresses this by introducing a physics-informed neural network framework\nspecifically designed for temperature field prediction in metal AM. This\nframework incorporates a physics-informed input, physics-informed loss\nfunction, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture.\nUtilizing real-time temperature data from the process, our model predicts 2D\ntemperature fields for future timestamps across diverse geometries, deposition\npatterns, and process parameters. We validate the proposed framework in two\nscenarios: full-field temperature prediction for a thin wall and 2D temperature\nfield prediction for cylinder and cubic parts, demonstrating errors below 3%\nand 1%, respectively. Our proposed framework exhibits the flexibility to be\napplied across diverse scenarios with varying process parameters, geometries,\nand deposition patterns.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "42 pages, 13 Figures",
    "pdf_url": "http://arxiv.org/pdf/2401.02403v1",
    "published_date": "2024-01-04 18:42:28 UTC",
    "updated_date": "2024-01-04 18:42:28 UTC"
  },
  {
    "arxiv_id": "2401.10210v1",
    "title": "Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction",
    "authors": [
      "Anup Shakya",
      "Vasile Rus",
      "Deepak Venugopal"
    ],
    "abstract": "Predicting the strategy (sequence of concepts) that a student is likely to\nuse in problem-solving helps Adaptive Instructional Systems (AISs) better adapt\nthemselves to different types of learners based on their learning abilities.\nThis can lead to a more dynamic, engaging, and personalized experience for\nstudents. To scale up training a prediction model (such as LSTMs) over\nlarge-scale education datasets, we develop a non-parametric approach to cluster\nsymmetric instances in the data. Specifically, we learn a representation based\non Node2Vec that encodes symmetries over mastery or skill level since, to solve\na problem, it is natural that a student's strategy is likely to involve\nconcepts in which they have gained mastery. Using this representation, we use\nDP-Means to group symmetric instances through a coarse-to-fine refinement of\nthe clusters. We apply our model to learn strategies for Math learning from\nlarge-scale datasets from MATHia, a leading AIS for middle-school math\nlearning. Our results illustrate that our approach can consistently achieve\nhigh accuracy using a small sample that is representative of the full dataset.\nFurther, we show that this approach helps us learn strategies with high\naccuracy for students at different skill levels, i.e., leveraging symmetries\nimproves fairness in the prediction model.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "Proceedings of 37th AAAI Conference on Artificial Intelligence\n  Artificial Intelligence for Education. arXiv admin note: substantial text\n  overlap with arXiv:2308.03892",
    "pdf_url": "http://arxiv.org/pdf/2401.10210v1",
    "published_date": "2024-01-04 17:57:21 UTC",
    "updated_date": "2024-01-04 17:57:21 UTC"
  },
  {
    "arxiv_id": "2401.02385v2",
    "title": "TinyLlama: An Open-Source Small Language Model",
    "authors": [
      "Peiyuan Zhang",
      "Guangtao Zeng",
      "Tianduo Wang",
      "Wei Lu"
    ],
    "abstract": "We present TinyLlama, a compact 1.1B language model pretrained on around 1\ntrillion tokens for approximately 3 epochs. Building on the architecture and\ntokenizer of Llama 2, TinyLlama leverages various advances contributed by the\nopen-source community (e.g., FlashAttention and Lit-GPT), achieving better\ncomputational efficiency. Despite its relatively small size, TinyLlama\ndemonstrates remarkable performance in a series of downstream tasks. It\nsignificantly outperforms existing open-source language models with comparable\nsizes. Our model checkpoints and code are publicly available on GitHub at\nhttps://github.com/jzhang38/TinyLlama.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report",
    "pdf_url": "http://arxiv.org/pdf/2401.02385v2",
    "published_date": "2024-01-04 17:54:59 UTC",
    "updated_date": "2024-06-04 02:05:30 UTC"
  },
  {
    "arxiv_id": "2401.02383v2",
    "title": "Survey of 3D Human Body Pose and Shape Estimation Methods for Contemporary Dance Applications",
    "authors": [
      "Darshan Venkatrayappa",
      "Alain Tremeau",
      "Damien Muselet",
      "Philippe Colantoni"
    ],
    "abstract": "3D human body shape and pose estimation from RGB images is a challenging\nproblem with potential applications in augmented/virtual reality, healthcare\nand fitness technology and virtual retail. Recent solutions have focused on\nthree types of inputs: i) single images, ii) multi-view images and iii) videos.\nIn this study, we surveyed and compared 3D body shape and pose estimation\nmethods for contemporary dance and performing arts, with a special focus on\nhuman body pose and dressing, camera viewpoint, illumination conditions and\nbackground conditions. We demonstrated that multi-frame methods, such as PHALP,\nprovide better results than single-frame method for pose estimation when\ndancers are performing contemporary dances.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2008.09062 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2401.02383v2",
    "published_date": "2024-01-04 17:51:44 UTC",
    "updated_date": "2024-01-29 21:32:39 UTC"
  },
  {
    "arxiv_id": "2401.02997v1",
    "title": "Blar-SQL: Faster, Stronger, Smaller NL2SQL",
    "authors": [
      "José Manuel Domínguez",
      "Benjamín Errázuriz",
      "Patricio Daher"
    ],
    "abstract": "Large Language Models (LLMs) have gained considerable notoriety in the field\nof natural language to SQL tasks (NL2SQL). In this study, we show how task\ndecomposition can greatly benefit LLMs in database understanding and query\ngeneration in order to answer human questions with an SQL query.\n  We fined-tuned open source models, specifically Llama-2 and Code Llama, by\ncombining 2 different models each designated to focus on one of two tasks in\norder to leverage each model's core competency to further increase the accuracy\nof the final SQL query.\n  We propose a new framework to divide the schema into chunks in order to fit\nmore information into a limited context. Our results are comparable with those\nobtained by GPT-4 at the same time being 135 times smaller, 90 times faster and\nmore than 100 times cheaper than GPT-4.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02997v1",
    "published_date": "2024-01-04 16:50:52 UTC",
    "updated_date": "2024-01-04 16:50:52 UTC"
  },
  {
    "arxiv_id": "2401.02349v2",
    "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
    "authors": [
      "Ezgi Korkmaz"
    ],
    "abstract": "Reinforcement learning research obtained significant success and attention\nwith the utilization of deep neural networks to solve problems in high\ndimensional state or action spaces. While deep reinforcement learning policies\nare currently being deployed in many different fields from medical applications\nto large language models, there are still ongoing questions the field is trying\nto answer on the generalization capabilities of deep reinforcement learning\npolicies. In this paper, we will formalize and analyze generalization in deep\nreinforcement learning. We will explain the fundamental reasons why deep\nreinforcement learning policies encounter overfitting problems that limit their\ngeneralization capabilities. Furthermore, we will categorize and explain the\nmanifold solution approaches to increase generalization, and overcome\noverfitting in deep reinforcement learning policies. From exploration to\nadversarial analysis and from regularization to robustness our paper provides\nan analysis on a wide range of subfields within deep reinforcement learning\nwith a broad scope and in-depth view. We believe our study can provide a\ncompact guideline for the current advancements in deep reinforcement learning,\nand help to construct robust deep neural policies with higher generalization\nskills.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02349v2",
    "published_date": "2024-01-04 16:45:01 UTC",
    "updated_date": "2024-10-30 16:18:57 UTC"
  },
  {
    "arxiv_id": "2401.02347v1",
    "title": "Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training",
    "authors": [
      "Longtian Qiu",
      "Shan Ning",
      "Xuming He"
    ],
    "abstract": "Image captioning aims at generating descriptive and meaningful textual\ndescriptions of images, enabling a broad range of vision-language applications.\nPrior works have demonstrated that harnessing the power of Contrastive Image\nLanguage Pre-training (CLIP) offers a promising approach to achieving zero-shot\ncaptioning, eliminating the need for expensive caption annotations. However,\nthe widely observed modality gap in the latent space of CLIP harms the\nperformance of zero-shot captioning by breaking the alignment between paired\nimage-text features. To address this issue, we conduct an analysis on the CLIP\nlatent space which leads to two findings. Firstly, we observe that the CLIP's\nvisual feature of image subregions can achieve closer proximity to the paired\ncaption due to the inherent information loss in text descriptions. In addition,\nwe show that the modality gap between a paired image-text can be empirically\nmodeled as a zero-mean Gaussian distribution. Motivated by the findings, we\npropose a novel zero-shot image captioning framework with text-only training to\nreduce the modality gap. In particular, we introduce a subregion feature\naggregation to leverage local region information, which produces a compact\nvisual representation for matching text representation. Moreover, we\nincorporate a noise injection and CLIP reranking strategy to boost captioning\nperformance. We also extend our framework to build a zero-shot VQA pipeline,\ndemonstrating its generality. Through extensive experiments on common\ncaptioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that\nour method achieves remarkable performance improvements. Code is available at\nhttps://github.com/Artanic30/MacCap.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2024.Open sourced, Code and Model Available",
    "pdf_url": "http://arxiv.org/pdf/2401.02347v1",
    "published_date": "2024-01-04 16:43:46 UTC",
    "updated_date": "2024-01-04 16:43:46 UTC"
  },
  {
    "arxiv_id": "2401.02290v2",
    "title": "Path-based Explanation for Knowledge Graph Completion",
    "authors": [
      "Heng Chang",
      "Jiangnan Ye",
      "Alejo Lopez Avila",
      "Jinhua Du",
      "Jia Li"
    ],
    "abstract": "Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph\nCompletion (KGC) by modelling how entities and relations interact in recent\nyears. However, the explanation of the predicted facts has not caught the\nnecessary attention. Proper explanations for the results of GNN-based KGC\nmodels increase model transparency and help researchers develop more reliable\nmodels. Existing practices for explaining KGC tasks rely on\ninstance/subgraph-based approaches, while in some scenarios, paths can provide\nmore user-friendly and interpretable explanations. Nonetheless, the methods for\ngenerating path-based explanations for KGs have not been well-explored. To\naddress this gap, we propose Power-Link, the first path-based KGC explainer\nthat explores GNN-based models. We design a novel simplified graph-powering\ntechnique, which enables the generation of path-based explanations with a fully\nparallelisable and memory-efficient training scheme. We further introduce three\nnew metrics for quantitative evaluation of the explanations, together with a\nqualitative human evaluation. Extensive experiments demonstrate that Power-Link\noutperforms the SOTA baselines in interpretability, efficiency, and\nscalability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.02290v2",
    "published_date": "2024-01-04 14:19:37 UTC",
    "updated_date": "2024-10-18 01:58:39 UTC"
  },
  {
    "arxiv_id": "2401.04732v1",
    "title": "A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation",
    "authors": [
      "Manpreet Singh",
      "Ravdeep Pasricha",
      "Nitish Singh",
      "Ravi Prasad Kondapalli",
      "Manoj R",
      "Kiran R",
      "Laurent Boué"
    ],
    "abstract": "In this paper, we design a real-time question-answering system specifically\ntargeted for helping sellers get relevant material/documentation they can share\nlive with their customers or refer to during a call. Taking the Seismic content\nrepository as a relatively large scale example of a diverse dataset of sales\nmaterial, we demonstrate how LLM embeddings of sellers' queries can be matched\nwith the relevant content. We achieve this by engineering prompts in an\nelaborate fashion that makes use of the rich set of meta-features available for\ndocuments and sellers. Using a bi-encoder with cross-encoder re-ranker\narchitecture, we show how the solution returns the most relevant content\nrecommendations in just a few seconds even for large datasets. Our recommender\nsystem is deployed as an AML endpoint for real-time inferencing and has been\nintegrated into a Copilot interface that is now deployed in the production\nversion of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04732v1",
    "published_date": "2024-01-04 13:32:44 UTC",
    "updated_date": "2024-01-04 13:32:44 UTC"
  },
  {
    "arxiv_id": "2401.06781v1",
    "title": "PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model",
    "authors": [
      "Chenghao Huang",
      "Yanbo Cao",
      "Yinlong Wen",
      "Tao Zhou",
      "Yanru Zhang"
    ],
    "abstract": "Poker, also known as Texas Hold'em, has always been a typical research target\nwithin imperfect information games (IIGs). IIGs have long served as a measure\nof artificial intelligence (AI) development. Representative prior works, such\nas DeepStack and Libratus heavily rely on counterfactual regret minimization\n(CFR) to tackle heads-up no-limit Poker. However, it is challenging for\nsubsequent researchers to learn CFR from previous models and apply it to other\nreal-world applications due to the expensive computational cost of CFR\niterations. Additionally, CFR is difficult to apply to multi-player games due\nto the exponential growth of the game tree size. In this work, we introduce\nPokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number\nof players and gaining high win rates, established on a lightweight large\nlanguage model (LLM). PokerGPT only requires simple textual information of\nPoker games for generating decision-making advice, thus guaranteeing the\nconvenient interaction between AI and humans. We mainly transform a set of\ntextual records acquired from real games into prompts, and use them to\nfine-tune a lightweight pre-trained LLM using reinforcement learning human\nfeedback technique. To improve fine-tuning performance, we conduct prompt\nengineering on raw data, including filtering useful information, selecting\nbehaviors of players with high win rates, and further processing them into\ntextual instruction using multiple prompt engineering techniques. Through the\nexperiments, we demonstrate that PokerGPT outperforms previous approaches in\nterms of win rate, model size, training time, and response speed, indicating\nthe great potential of LLMs in solving IIGs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.06781v1",
    "published_date": "2024-01-04 13:27:50 UTC",
    "updated_date": "2024-01-04 13:27:50 UTC"
  },
  {
    "arxiv_id": "2401.02258v1",
    "title": "Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation",
    "authors": [
      "Linglong Qian",
      "Zina Ibrahim",
      "Richard Dobson"
    ],
    "abstract": "Missingness is ubiquitous in multivariate time series and poses an obstacle\nto reliable downstream analysis. Although recurrent network imputation achieved\nthe SOTA, existing models do not scale to deep architectures that can\npotentially alleviate issues arising in complex data. Moreover, imputation\ncarries the risk of biased estimations of the ground truth. Yet, confidence in\nthe imputed values is always unmeasured or computed post hoc from model output.\nWe propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates\nmissing values and their associated uncertainty in heterogeneous multivariate\ntime series. By jointly representing feature-wise correlations and temporal\ndynamics, we adopt a self attention mechanism, along with an effective residual\ncomponent, to achieve a deep recurrent neural network with good imputation\nperformance and stable convergence. We also leverage self-supervised metric\nlearning to boost performance by optimizing sample similarity. Finally, we\ntransform DEARI into a Bayesian neural network through a novel Bayesian\nmarginalization strategy to produce stochastic DEARI, which outperforms its\ndeterministic equivalent. Experiments show that DEARI surpasses the SOTA in\ndiverse imputation tasks using real-world datasets, namely air quality control,\nhealthcare and traffic.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02258v1",
    "published_date": "2024-01-04 13:21:11 UTC",
    "updated_date": "2024-01-04 13:21:11 UTC"
  },
  {
    "arxiv_id": "2401.02244v1",
    "title": "Policy-regularized Offline Multi-objective Reinforcement Learning",
    "authors": [
      "Qian Lin",
      "Chao Yu",
      "Zongkai Liu",
      "Zifan Wu"
    ],
    "abstract": "In this paper, we aim to utilize only offline trajectory data to train a\npolicy for multi-objective RL. We extend the offline policy-regularized method,\na widely-adopted approach for single-objective offline RL problems, into the\nmulti-objective setting in order to achieve the above goal. However, such\nmethods face a new challenge in offline MORL settings, namely the\npreference-inconsistent demonstration problem. We propose two solutions to this\nproblem: 1) filtering out preference-inconsistent demonstrations via\napproximating behavior preferences, and 2) adopting regularization techniques\nwith high policy expressiveness. Moreover, we integrate the\npreference-conditioned scalarized update method into policy-regularized offline\nRL, in order to simultaneously learn a set of policies using a single policy\nnetwork, thus reducing the computational cost induced by the training of a\nlarge number of individual policies for various preferences. Finally, we\nintroduce Regularization Weight Adaptation to dynamically determine appropriate\nregularization weights for arbitrary target preferences during deployment.\nEmpirical results on various multi-objective datasets demonstrate the\ncapability of our approach in solving offline MORL problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02244v1",
    "published_date": "2024-01-04 12:54:10 UTC",
    "updated_date": "2024-01-04 12:54:10 UTC"
  },
  {
    "arxiv_id": "2401.02995v1",
    "title": "CANAMRF: An Attention-Based Model for Multimodal Depression Detection",
    "authors": [
      "Yuntao Wei",
      "Yuzhe Zhang",
      "Shuyang Zhang",
      "Hong Zhang"
    ],
    "abstract": "Multimodal depression detection is an important research topic that aims to\npredict human mental states using multimodal data. Previous methods treat\ndifferent modalities equally and fuse each modality by na\\\"ive mathematical\noperations without measuring the relative importance between them, which cannot\nobtain well-performed multimodal representations for downstream depression\ntasks. In order to tackle the aforementioned concern, we present a Cross-modal\nAttention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for\nmultimodal depression detection. CANAMRF is constructed by a multimodal feature\nextractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid\nAttention Module. Through experimentation on two benchmark datasets, CANAMRF\ndemonstrates state-of-the-art performance, underscoring the effectiveness of\nour proposed approach.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 3 figures. Pacific Rim International Conference on\n  Artificial Intelligence. Singapore: Springer Nature Singapore, 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.02995v1",
    "published_date": "2024-01-04 12:08:16 UTC",
    "updated_date": "2024-01-04 12:08:16 UTC"
  },
  {
    "arxiv_id": "2401.02465v1",
    "title": "Interpretable Time Series Models for Wastewater Modeling in Combined Sewer Overflows",
    "authors": [
      "Teodor Chiaburu",
      "Felix Biessmann"
    ],
    "abstract": "Climate change poses increasingly complex challenges to our society. Extreme\nweather events such as floods, wild fires or droughts are becoming more\nfrequent, spontaneous and difficult to foresee or counteract. In this work we\nspecifically address the problem of sewage water polluting surface water bodies\nafter spilling over from rain tanks as a consequence of heavy rain events. We\ninvestigate to what extent state-of-the-art interpretable time series models\ncan help predict such critical water level points, so that the excess can\npromptly be redistributed across the sewage network. Our results indicate that\nmodern time series models can contribute to better waste water management and\nprevention of environmental pollution from sewer systems. All the code and\nexperiments can be found in our repository:\nhttps://github.com/TeodorChiaburu/RIWWER_TimeSeries.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 5 figures, 2 tables, presented at iSCSi 2023 Lisbon",
    "pdf_url": "http://arxiv.org/pdf/2401.02465v1",
    "published_date": "2024-01-04 11:48:27 UTC",
    "updated_date": "2024-01-04 11:48:27 UTC"
  },
  {
    "arxiv_id": "2401.02212v1",
    "title": "Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph",
    "authors": [
      "Rikui Huang",
      "Wei Wei",
      "Xiaoye Qu",
      "Wenfeng Xie",
      "Xianling Mao",
      "Dangyang Chen"
    ],
    "abstract": "Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by\nattaching the time scope. Existing temporal knowledge graph question answering\n(TKGQA) models solely approach simple questions, owing to the prior assumption\nthat each question only contains a single temporal fact with explicit/implicit\ntemporal constraints. Hence, they perform poorly on questions which own\nmultiple temporal facts. In this paper, we propose \\textbf{\\underline{J}}oint\n\\textbf{\\underline{M}}ulti \\textbf{\\underline{F}}acts\n\\textbf{\\underline{R}}easoning \\textbf{\\underline{N}}etwork (JMFRN), to jointly\nreasoning multiple temporal facts for accurately answering \\emph{complex}\ntemporal questions. Specifically, JMFRN first retrieves question-related\ntemporal facts from TKG for each entity of the given complex question. For\njoint reasoning, we design two different attention (\\ie entity-aware and\ntime-aware) modules, which are suitable for universal settings, to aggregate\nentities and timestamps information of retrieved facts. Moreover, to filter\nincorrect type answers, we introduce an additional answer type discrimination\ntask. Extensive experiments demonstrate our proposed method significantly\noutperforms the state-of-art on the well-known complex temporal question\nbenchmark TimeQuestions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02212v1",
    "published_date": "2024-01-04 11:34:39 UTC",
    "updated_date": "2024-01-04 11:34:39 UTC"
  },
  {
    "arxiv_id": "2401.02199v1",
    "title": "LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System",
    "authors": [
      "Anil Ranjitbhai Patel",
      "Peter Liggesmeyer"
    ],
    "abstract": "As the horizon of intelligent transportation expands with the evolution of\nAutomated Driving Systems (ADS), ensuring paramount safety becomes more\nimperative than ever. Traditional risk assessment methodologies, primarily\ncrafted for human-driven vehicles, grapple to adequately adapt to the\nmultifaceted, evolving environments of ADS. This paper introduces a framework\nfor real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of\nArtificial Neural Networks (ANNs).\n  Our proposed solution transcends these limitations, drawing upon ANNs, a\ncornerstone of deep learning, to meticulously analyze and categorize risk\ndimensions using real-time On-board Sensor (OBS) data. This learning-centric\napproach not only elevates the ADS's situational awareness but also enriches\nits understanding of immediate operational contexts. By dissecting OBS data,\nthe system is empowered to pinpoint its current risk profile, thereby enhancing\nsafety prospects for onboard passengers and the broader traffic ecosystem.\n  Through this framework, we chart a direction in risk assessment, bridging the\nconventional voids and enhancing the proficiency of ADS. By utilizing ANNs, our\nmethodology offers a perspective, allowing ADS to adeptly navigate and react to\npotential risk factors, ensuring safer and more informed autonomous journeys.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SE",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "2023 IEEE International Test Conference, 8th Edition of Automotive,\n  Reliability, Test & Safety Workshop in Disneyland, Anaheim, CA",
    "pdf_url": "http://arxiv.org/pdf/2401.02199v1",
    "published_date": "2024-01-04 11:09:15 UTC",
    "updated_date": "2024-01-04 11:09:15 UTC"
  },
  {
    "arxiv_id": "2401.02183v1",
    "title": "FairGridSearch: A Framework to Compare Fairness-Enhancing Models",
    "authors": [
      "Shih-Chi Ma",
      "Tatiana Ermakova",
      "Benjamin Fabian"
    ],
    "abstract": "Machine learning models are increasingly used in critical decision-making\napplications. However, these models are susceptible to replicating or even\namplifying bias present in real-world data. While there are various bias\nmitigation methods and base estimators in the literature, selecting the optimal\nmodel for a specific application remains challenging.\n  This paper focuses on binary classification and proposes FairGridSearch, a\nnovel framework for comparing fairness-enhancing models. FairGridSearch enables\nexperimentation with different model parameter combinations and recommends the\nbest one. The study applies FairGridSearch to three popular datasets (Adult,\nCOMPAS, and German Credit) and analyzes the impacts of metric selection, base\nestimator choice, and classification threshold on model fairness.\n  The results highlight the significance of selecting appropriate accuracy and\nfairness metrics for model evaluation. Additionally, different base estimators\nand classification threshold values affect the effectiveness of bias mitigation\nmethods and fairness stability respectively, but the effects are not consistent\nacross all datasets. Based on these findings, future research on fairness in\nmachine learning should consider a broader range of factors when building fair\nmodels, going beyond bias mitigation methods alone.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02183v1",
    "published_date": "2024-01-04 10:29:02 UTC",
    "updated_date": "2024-01-04 10:29:02 UTC"
  },
  {
    "arxiv_id": "2401.02173v1",
    "title": "Prompt Decoupling for Text-to-Image Person Re-identification",
    "authors": [
      "Weihao Li",
      "Lei Tan",
      "Pingyang Dai",
      "Yan Zhang"
    ],
    "abstract": "Text-to-image person re-identification (TIReID) aims to retrieve the target\nperson from an image gallery via a textual description query. Recently,\npre-trained vision-language models like CLIP have attracted significant\nattention and have been widely utilized for this task due to their robust\ncapacity for semantic concept learning and rich multi-modal knowledge. However,\nrecent CLIP-based TIReID methods commonly rely on direct fine-tuning of the\nentire network to adapt the CLIP model for the TIReID task. Although these\nmethods show competitive performance on this topic, they are suboptimal as they\nnecessitate simultaneous domain adaptation and task adaptation. To address this\nissue, we attempt to decouple these two processes during the training stage.\nSpecifically, we introduce the prompt tuning strategy to enable domain\nadaptation and propose a two-stage training approach to disentangle domain\nadaptation from task adaptation. In the first stage, we freeze the two encoders\nfrom CLIP and solely focus on optimizing the prompts to alleviate domain gap\nbetween the original training data of CLIP and downstream tasks. In the second\nstage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize\ncapturing fine-grained information, which is more suitable for TIReID task.\nFinally, we evaluate the effectiveness of our method on three widely used\ndatasets. Compared to the directly fine-tuned approach, our method achieves\nsignificant improvements.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02173v1",
    "published_date": "2024-01-04 09:55:15 UTC",
    "updated_date": "2024-01-04 09:55:15 UTC"
  },
  {
    "arxiv_id": "2401.02158v1",
    "title": "Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and LightGBM models",
    "authors": [
      "Rushi Chavda",
      "Darshan Makwana",
      "Vraj Patel",
      "Anupam Shukla"
    ],
    "abstract": "This paper describes approaches and results for shared Task 1 and 4 of\nSMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english\ntweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary\nclassification of English Reddit posts self-reporting a social anxiety disorder\ndiagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all\nparticipants. We have leveraged the Transformer model (BERT) in combination\nwith the LightGBM model for both tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02158v1",
    "published_date": "2024-01-04 09:13:18 UTC",
    "updated_date": "2024-01-04 09:13:18 UTC"
  },
  {
    "arxiv_id": "2401.02154v1",
    "title": "Disentangle Estimation of Causal Effects from Cross-Silo Data",
    "authors": [
      "Yuxuan Liu",
      "Haozhao Wang",
      "Shuang Wang",
      "Zhiming He",
      "Wenchao Xu",
      "Jialiang Zhu",
      "Fan Yang"
    ],
    "abstract": "Estimating causal effects among different events is of great importance to\ncritical fields such as drug development. Nevertheless, the data features\nassociated with events may be distributed across various silos and remain\nprivate within respective parties, impeding direct information exchange between\nthem. This, in turn, can result in biased estimations of local causal effects,\nwhich rely on the characteristics of only a subset of the covariates. To tackle\nthis challenge, we introduce an innovative disentangle architecture designed to\nfacilitate the seamless cross-silo transmission of model parameters, enriched\nwith causal mechanisms, through a combination of shared and private branches.\nBesides, we introduce global constraints into the equation to effectively\nmitigate bias within the various missing domains, thereby elevating the\naccuracy of our causal effect estimation. Extensive experiments conducted on\nnew semi-synthetic datasets show that our method outperforms state-of-the-art\nbaselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.02154v1",
    "published_date": "2024-01-04 09:05:37 UTC",
    "updated_date": "2024-01-04 09:05:37 UTC"
  },
  {
    "arxiv_id": "2401.02153v1",
    "title": "Unit Testing in ASP Revisited: Language and Test-Driven Development Environment",
    "authors": [
      "Giovanni Amendola",
      "Tobias Berei",
      "Giuseppe Mazzotta",
      "Francesco Ricca"
    ],
    "abstract": "Unit testing frameworks are nowadays considered a best practice, included in\nalmost all modern software development processes, to achieve rapid development\nof correct specifications. Knowledge representation and reasoning paradigms\nsuch as Answer Set Programming (ASP), that have been used in industry-level\napplications, are not an exception. Indeed, the first unit testing\nspecification language for ASP was proposed in 2011 as a feature of the ASPIDE\ndevelopment environment. Later, a more portable unit testing language was\nincluded in the LANA annotation language. In this paper we revisit both\nlanguages and tools for unit testing in ASP. We propose a new unit test\nspecification language that allows one to inline tests within ASP programs, and\nwe identify the computational complexity of the tasks associated with checking\nthe various program-correctness assertions. Test-case specifications are\ntransparent to the traditional evaluation, but can be interpreted by a specific\ntesting tool. Thus, we present a novel environment supporting test driven\ndevelopment of ASP programs.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02153v1",
    "published_date": "2024-01-04 09:04:54 UTC",
    "updated_date": "2024-01-04 09:04:54 UTC"
  },
  {
    "arxiv_id": "2401.02143v1",
    "title": "Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions",
    "authors": [
      "Cheng-Te Li",
      "Yu-Che Tsai",
      "Chih-Yao Chen",
      "Jay Chiehen Liao"
    ],
    "abstract": "In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural\nNetworks (GNNs), a domain where deep learning-based approaches have\nincreasingly shown superior performance in both classification and regression\ntasks compared to traditional methods. The survey highlights a critical gap in\ndeep neural TDL methods: the underrepresentation of latent correlations among\ndata instances and feature values. GNNs, with their innate capability to model\nintricate relationships and interactions between diverse elements of tabular\ndata, have garnered significant interest and application across various TDL\ndomains. Our survey provides a systematic review of the methods involved in\ndesigning and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed\ninvestigation into the foundational aspects and an overview of GNN-based TDL\nmethods, offering insights into their evolving landscape. We present a\ncomprehensive taxonomy focused on constructing graph structures and\nrepresentation learning within GNN-based TDL methods. In addition, the survey\nexamines various training plans, emphasizing the integration of auxiliary tasks\nto enhance the effectiveness of instance representations. A critical part of\nour discussion is dedicated to the practical application of GNNs across a\nspectrum of GNN4TDL scenarios, demonstrating their versatility and impact.\nLastly, we discuss the limitations and propose future research directions,\naiming to spur advancements in GNN4TDL. This survey serves as a resource for\nresearchers and practitioners, offering a thorough understanding of GNNs' role\nin revolutionizing TDL and pointing towards future innovations in this\npromising area.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review, ongoing work, Github page:\n  https://github.com/Roytsai27/awesome-GNN4TDL",
    "pdf_url": "http://arxiv.org/pdf/2401.02143v1",
    "published_date": "2024-01-04 08:49:10 UTC",
    "updated_date": "2024-01-04 08:49:10 UTC"
  },
  {
    "arxiv_id": "2401.02137v1",
    "title": "SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment",
    "authors": [
      "Ziping Ma",
      "Furong Xu",
      "Jian Liu",
      "Ming Yang",
      "Qingpei Guo"
    ],
    "abstract": "Multimodal alignment between language and vision is the fundamental topic in\ncurrent vision-language model research. Contrastive Captioners (CoCa), as a\nrepresentative method, integrates Contrastive Language-Image Pretraining (CLIP)\nand Image Caption (IC) into a unified framework, resulting in impressive\nresults. CLIP imposes a bidirectional constraints on global representation of\nentire images and sentences. Although IC conducts an unidirectional\nimage-to-text generation on local representation, it lacks any constraint on\nlocal text-to-image reconstruction, which limits the ability to understand\nimages at a fine-grained level when aligned with texts. To achieve multimodal\nalignment from both global and local perspectives, this paper proposes\nSymmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional\ninteractions on images and texts across the global and local representation\nlevels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM)\nhead based on ITC and IC heads. The improved SyCoCa can further leverage\ntextual cues to reconstruct contextual images and visual cues to predict\ntextual contents. When implementing bidirectional local interactions, the local\ncontents of images tend to be cluttered or unrelated to their textual\ndescriptions. Thus, we employ an attentive masking strategy to select effective\nimage patches for interaction. Extensive experiments on five vision-language\ntasks, including image-text retrieval, image-captioning, visual question\nanswering, and zero-shot/finetuned image classification, validate the\neffectiveness of our proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02137v1",
    "published_date": "2024-01-04 08:42:36 UTC",
    "updated_date": "2024-01-04 08:42:36 UTC"
  },
  {
    "arxiv_id": "2401.02132v1",
    "title": "DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models",
    "authors": [
      "Wendi Cui",
      "Jiaxin Zhang",
      "Zhuohang Li",
      "Lopez Damien",
      "Kamalika Das",
      "Bradley Malin",
      "Sricharan Kumar"
    ],
    "abstract": "Evaluating the quality and variability of text generated by Large Language\nModels (LLMs) poses a significant, yet unresolved research challenge.\nTraditional evaluation methods, such as ROUGE and BERTScore, which measure\ntoken similarity, often fail to capture the holistic semantic equivalence. This\nresults in a low correlation with human judgments and intuition, which is\nespecially problematic in high-stakes applications like healthcare and finance\nwhere reliability, safety, and robust decision-making are highly critical. This\nwork proposes DCR, an automated framework for evaluating and improving the\nconsistency of LLM-generated texts using a divide-conquer-reasoning approach.\nUnlike existing LLM-based evaluators that operate at the paragraph level, our\nmethod employs a divide-and-conquer evaluator (DCE) that breaks down the\nparagraph-to-paragraph comparison between two generated responses into\nindividual sentence-to-paragraph comparisons, each evaluated based on\npredefined criteria. To facilitate this approach, we introduce an automatic\nmetric converter (AMC) that translates the output from DCE into an\ninterpretable numeric score. Beyond the consistency evaluation, we further\npresent a reason-assisted improver (RAI) that leverages the analytical reasons\nwith explanations identified by DCE to generate new responses aimed at reducing\nthese inconsistencies. Through comprehensive and systematic empirical analysis,\nwe show that our approach outperforms state-of-the-art methods by a large\nmargin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the\nconsistency of LLM generation across multiple benchmarks in semantic, factual,\nand summarization consistency tasks. Our approach also substantially reduces\nnearly 90% of output inconsistencies, showing promise for effective\nhallucination mitigation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02132v1",
    "published_date": "2024-01-04 08:34:16 UTC",
    "updated_date": "2024-01-04 08:34:16 UTC"
  },
  {
    "arxiv_id": "2401.02124v1",
    "title": "ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach",
    "authors": [
      "Zeynep Hilal Kilimci",
      "Mustafa Yalcin"
    ],
    "abstract": "Anticancer peptides (ACPs) are a class of molecules that have gained\nsignificant attention in the field of cancer research and therapy. ACPs are\nshort chains of amino acids, the building blocks of proteins, and they possess\nthe ability to selectively target and kill cancer cells. One of the key\nadvantages of ACPs is their ability to selectively target cancer cells while\nsparing healthy cells to a greater extent. This selectivity is often attributed\nto differences in the surface properties of cancer cells compared to normal\ncells. That is why ACPs are being investigated as potential candidates for\ncancer therapy. ACPs may be used alone or in combination with other treatment\nmodalities like chemotherapy and radiation therapy. While ACPs hold promise as\na novel approach to cancer treatment, there are challenges to overcome,\nincluding optimizing their stability, improving selectivity, and enhancing\ntheir delivery to cancer cells, continuous increasing in number of peptide\nsequences, developing a reliable and precise prediction model. In this work, we\npropose an efficient transformer-based framework to identify anticancer\npeptides for by performing accurate a reliable and precise prediction model.\nFor this purpose, four different transformer models, namely ESM, ProtBert,\nBioBERT, and SciBERT are employed to detect anticancer peptides from amino acid\nsequences. To demonstrate the contribution of the proposed framework, extensive\nexperiments are carried on widely-used datasets in the literature, two versions\nof AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of\nproposed model enhances classification accuracy when compared to the\nstate-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of\naccuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and\n88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02124v1",
    "published_date": "2024-01-04 08:19:27 UTC",
    "updated_date": "2024-01-04 08:19:27 UTC"
  },
  {
    "arxiv_id": "2401.02458v2",
    "title": "Data-Centric Foundation Models in Computational Healthcare: A Survey",
    "authors": [
      "Yunkun Zhang",
      "Jin Gao",
      "Zheling Tan",
      "Lingfeng Zhou",
      "Kexin Ding",
      "Mu Zhou",
      "Shaoting Zhang",
      "Dequan Wang"
    ],
    "abstract": "The advent of foundation models (FMs) as an emerging suite of AI techniques\nhas struck a wave of opportunities in computational healthcare. The interactive\nnature of these models, guided by pre-training data and human instructions, has\nignited a data-centric AI paradigm that emphasizes better data\ncharacterization, quality, and scale. In healthcare AI, obtaining and\nprocessing high-quality clinical data records has been a longstanding\nchallenge, ranging from data quantity, annotation, patient privacy, and ethics.\nIn this survey, we investigate a wide range of data-centric approaches in the\nFM era (from model pre-training to inference) towards improving the healthcare\nworkflow. We discuss key perspectives in AI security, assessment, and alignment\nwith human values. Finally, we offer a promising outlook of FM-based analytics\nto enhance the performance of patient outcome and clinical workflow in the\nevolving landscape of healthcare and medicine. We provide an up-to-date list of\nhealthcare-related foundation models and datasets at\nhttps://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Survey content updated to include recent research work and progress",
    "pdf_url": "http://arxiv.org/pdf/2401.02458v2",
    "published_date": "2024-01-04 08:00:32 UTC",
    "updated_date": "2024-10-07 14:20:42 UTC"
  },
  {
    "arxiv_id": "2401.02117v1",
    "title": "Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation",
    "authors": [
      "Zipeng Fu",
      "Tony Z. Zhao",
      "Chelsea Finn"
    ],
    "abstract": "Imitation learning from human demonstrations has shown impressive performance\nin robotics. However, most results focus on table-top manipulation, lacking the\nmobility and dexterity necessary for generally useful tasks. In this work, we\ndevelop a system for imitating mobile manipulation tasks that are bimanual and\nrequire whole-body control. We first present Mobile ALOHA, a low-cost and\nwhole-body teleoperation system for data collection. It augments the ALOHA\nsystem with a mobile base, and a whole-body teleoperation interface. Using data\ncollected with Mobile ALOHA, we then perform supervised behavior cloning and\nfind that co-training with existing static ALOHA datasets boosts performance on\nmobile manipulation tasks. With 50 demonstrations for each task, co-training\ncan increase success rates by up to 90%, allowing Mobile ALOHA to autonomously\ncomplete complex mobile manipulation tasks such as sauteing and serving a piece\nof shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling\nand entering an elevator, and lightly rinsing a used pan using a kitchen\nfaucet. Project website: https://mobile-aloha.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://mobile-aloha.github.io (Zipeng Fu and Tony\n  Z. Zhao are project co-leads, Chelsea Finn is the advisor)",
    "pdf_url": "http://arxiv.org/pdf/2401.02117v1",
    "published_date": "2024-01-04 07:55:53 UTC",
    "updated_date": "2024-01-04 07:55:53 UTC"
  },
  {
    "arxiv_id": "2401.02994v3",
    "title": "Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM",
    "authors": [
      "Xiaoding Lu",
      "Zongyi Liu",
      "Adian Liusie",
      "Vyas Raina",
      "Vineet Mudupalli",
      "Yuwen Zhang",
      "William Beauchamp"
    ],
    "abstract": "In conversational AI research, there's a noticeable trend towards developing\nmodels with a larger number of parameters, exemplified by models like ChatGPT.\nWhile these expansive models tend to generate increasingly better chat\nresponses, they demand significant computational resources and memory. This\nstudy explores a pertinent question: Can a combination of smaller models\ncollaboratively achieve comparable or enhanced performance relative to a\nsingular large model? We introduce an approach termed \"blending\", a\nstraightforward yet effective method of integrating multiple chat AIs. Our\nempirical evidence suggests that when specific smaller models are\nsynergistically blended, they can potentially outperform or match the\ncapabilities of much larger counterparts. For instance, integrating just three\nmodels of moderate size (6B/13B paramaeters) can rival or even surpass the\nperformance metrics of a substantially larger model like ChatGPT (175B+\nparamaters). This hypothesis is rigorously tested using A/B testing\nmethodologies with a large user base on the Chai research platform over a span\nof thirty days. The findings underscore the potential of the \"blending\"\nstrategy as a viable approach for enhancing chat AI efficacy without a\ncorresponding surge in computational demands.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02994v3",
    "published_date": "2024-01-04 07:45:49 UTC",
    "updated_date": "2024-01-23 04:43:56 UTC"
  },
  {
    "arxiv_id": "2401.02993v2",
    "title": "ReFusion: Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion",
    "authors": [
      "Shangyu Wu",
      "Ying Xiong",
      "Yufei Cui",
      "Xue Liu",
      "Buzhou Tang",
      "Tei-Wei Kuo",
      "Chun Jason Xue"
    ],
    "abstract": "Retrieval-based augmentations (RA) incorporating knowledge from an external\ndatabase into language models have greatly succeeded in various\nknowledge-intensive (KI) tasks. However, integrating retrievals in\nnon-knowledge-intensive (NKI) tasks is still challenging. Existing works focus\non concatenating retrievals with inputs to improve model performance.\nUnfortunately, the use of retrieval concatenation-based augmentations causes an\nincrease in the input length, substantially raising the computational demands\nof attention mechanisms. This paper proposes a new paradigm of RA named\n\\textbf{ReFusion}, a computation-efficient Retrieval representation Fusion with\nbi-level optimization. Unlike previous works, ReFusion directly fuses the\nretrieval representations into the hidden states of models. Specifically,\nReFusion leverages an adaptive retrieval integrator to seek the optimal\ncombination of the proposed ranking schemes across different model layers.\nExperimental results demonstrate that the proposed ReFusion can achieve\nsuperior and robust performance in various NKI tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02993v2",
    "published_date": "2024-01-04 07:39:26 UTC",
    "updated_date": "2024-05-27 07:04:19 UTC"
  },
  {
    "arxiv_id": "2401.02457v1",
    "title": "eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning",
    "authors": [
      "Zhiwei Zuo",
      "Zhuo Tang",
      "Bin Wang",
      "Kenli Li",
      "Anwitaman Datta"
    ],
    "abstract": "New categories may be introduced over time, or existing categories may need\nto be reclassified. Class incremental learning (CIL) is employed for the\ngradual acquisition of knowledge about new categories while preserving\ninformation about previously learned ones in such dynamic environments. It\nmight also be necessary to also eliminate the influence of related categories\non the model to adapt to reclassification. We thus introduce class-level\nmachine unlearning (MU) within CIL. Typically, MU methods tend to be\ntime-consuming and can potentially harm the model's performance. A continuous\nstream of unlearning requests could lead to catastrophic forgetting. To address\nthese issues, we propose a non-destructive eCIL-MU framework based on embedding\ntechniques to map data into vectors and then be stored in vector databases. Our\napproach exploits the overlap between CIL and MU tasks for acceleration.\nExperiments demonstrate the capability of achieving unlearning effectiveness\nand orders of magnitude (upto $\\sim 278\\times$) of acceleration.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02457v1",
    "published_date": "2024-01-04 07:18:32 UTC",
    "updated_date": "2024-01-04 07:18:32 UTC"
  },
  {
    "arxiv_id": "2401.02092v1",
    "title": "k-Winners-Take-All Ensemble Neural Network",
    "authors": [
      "Abien Fred Agarap",
      "Arnulfo P. Azcarraga"
    ],
    "abstract": "Ensembling is one approach that improves the performance of a neural network\nby combining a number of independent neural networks, usually by either\naveraging or summing up their individual outputs. We modify this ensembling\napproach by training the sub-networks concurrently instead of independently.\nThis concurrent training of sub-networks leads them to cooperate with each\nother, and we refer to them as \"cooperative ensemble\". Meanwhile, the\nmixture-of-experts approach improves a neural network performance by dividing\nup a given dataset to its sub-networks. It then uses a gating network that\nassigns a specialization to each of its sub-networks called \"experts\". We\nimprove on these aforementioned ways for combining a group of neural networks\nby using a k-Winners-Take-All (kWTA) activation function, that acts as the\ncombination method for the outputs of each sub-network in the ensemble. We\nrefer to this proposed model as \"kWTA ensemble neural networks\" (kWTA-ENN).\nWith the kWTA activation function, the losing neurons of the sub-networks are\ninhibited while the winning neurons are retained. This results in sub-networks\nhaving some form of specialization but also sharing knowledge with one another.\nWe compare our approach with the cooperative ensemble and mixture-of-experts,\nwhere we used a feed-forward neural network with one hidden layer having 100\nneurons as the sub-network architecture. Our approach yields a better\nperformance compared to the baseline models, reaching the following test\naccuracies on benchmark datasets: 98.34% on MNIST, 88.06% on Fashion-MNIST,\n91.56% on KMNIST, and 95.97% on WDBC.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Presented in ICONIP 2021",
    "pdf_url": "http://arxiv.org/pdf/2401.02092v1",
    "published_date": "2024-01-04 06:40:32 UTC",
    "updated_date": "2024-01-04 06:40:32 UTC"
  },
  {
    "arxiv_id": "2401.02992v1",
    "title": "Advanced Unstructured Data Processing for ESG Reports: A Methodology for Structured Transformation and Enhanced Analysis",
    "authors": [
      "Jiahui Peng",
      "Jing Gao",
      "Xin Tong",
      "Jing Guo",
      "Hang Yang",
      "Jianchuan Qi",
      "Ruiqiao Li",
      "Nan Li",
      "Ming Xu"
    ],
    "abstract": "In the evolving field of corporate sustainability, analyzing unstructured\nEnvironmental, Social, and Governance (ESG) reports is a complex challenge due\nto their varied formats and intricate content. This study introduces an\ninnovative methodology utilizing the \"Unstructured Core Library\", specifically\ntailored to address these challenges by transforming ESG reports into\nstructured, analyzable formats. Our approach significantly advances the\nexisting research by offering high-precision text cleaning, adept\nidentification and extraction of text from images, and standardization of\ntables within these reports. Emphasizing its capability to handle diverse data\ntypes, including text, images, and tables, the method adeptly manages the\nnuances of differing page layouts and report styles across industries. This\nresearch marks a substantial contribution to the fields of industrial ecology\nand corporate sustainability assessment, paving the way for the application of\nadvanced NLP technologies and large language models in the analysis of\ncorporate governance and sustainability. Our code is available at\nhttps://github.com/linancn/TianGong-AI-Unstructure.git.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02992v1",
    "published_date": "2024-01-04 06:26:59 UTC",
    "updated_date": "2024-01-04 06:26:59 UTC"
  },
  {
    "arxiv_id": "2401.02456v1",
    "title": "A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management",
    "authors": [
      "Sayed Pedram Haeri Boroujeni",
      "Abolfazl Razi",
      "Sahand Khoshdel",
      "Fatemeh Afghah",
      "Janice L. Coen",
      "Leo ONeill",
      "Peter Z. Fule",
      "Adam Watts",
      "Nick-Marios T. Kokolakis",
      "Kyriakos G. Vamvoudakis"
    ],
    "abstract": "Wildfires have emerged as one of the most destructive natural disasters\nworldwide, causing catastrophic losses in both human lives and forest wildlife.\nRecently, the use of Artificial Intelligence (AI) in wildfires, propelled by\nthe integration of Unmanned Aerial Vehicles (UAVs) and deep learning models,\nhas created an unprecedented momentum to implement and develop more effective\nwildfire management. Although some of the existing survey papers have explored\nvarious learning-based approaches, a comprehensive review emphasizing the\napplication of AI-enabled UAV systems and their subsequent impact on\nmulti-stage wildfire management is notably lacking. This survey aims to bridge\nthese gaps by offering a systematic review of the recent state-of-the-art\ntechnologies, highlighting the advancements of UAV systems and AI models from\npre-fire, through the active-fire stage, to post-fire management. To this aim,\nwe provide an extensive analysis of the existing remote sensing systems with a\nparticular focus on the UAV advancements, device specifications, and sensor\ntechnologies relevant to wildfire management. We also examine the pre-fire and\npost-fire management approaches, including fuel monitoring, prevention\nstrategies, as well as evacuation planning, damage assessment, and operation\nstrategies. Additionally, we review and summarize a wide range of computer\nvision techniques in active-fire management, with an emphasis on Machine\nLearning (ML), Reinforcement Learning (RL), and Deep Learning (DL) algorithms\nfor wildfire classification, segmentation, detection, and monitoring tasks.\nUltimately, we underscore the substantial advancement in wildfire modeling\nthrough the integration of cutting-edge AI techniques and UAV-based data,\nproviding novel insights and enhanced predictive capabilities to understand\ndynamic wildfire behavior.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02456v1",
    "published_date": "2024-01-04 05:09:35 UTC",
    "updated_date": "2024-01-04 05:09:35 UTC"
  },
  {
    "arxiv_id": "2401.04125v1",
    "title": "DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for Accurate and Continuous Weather Modeling",
    "authors": [
      "Wenyuan Li",
      "Zili Liu",
      "Keyan Chen",
      "Hao Chen",
      "Shunlin Liang",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "abstract": "Accurate weather forecasting holds significant importance to human\nactivities. Currently, there are two paradigms for weather forecasting:\nNumerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP).\nNWP utilizes atmospheric physics for weather modeling but suffers from poor\ndata utilization and high computational costs, while DLP can learn weather\npatterns from vast amounts of data directly but struggles to incorporate\nphysical laws. Both paradigms possess their respective strengths and\nweaknesses, and are incompatible, because physical laws adopted in NWP describe\nthe relationship between coordinates and meteorological variables, while DLP\ndirectly learns the relationships between meteorological variables without\nconsideration of coordinates. To address these problems, we introduce the\nDeepPhysiNet framework, incorporating physical laws into deep learning models\nfor accurate and continuous weather system modeling. First, we construct\nphysics networks based on multilayer perceptrons (MLPs) for individual\nmeteorological variable, such as temperature, pressure, and wind speed. Physics\nnetworks establish relationships between variables and coordinates by taking\ncoordinates as input and producing variable values as output. The physical laws\nin the form of Partial Differential Equations (PDEs) can be incorporated as a\npart of loss function. Next, we construct hyper-networks based on deep learning\nmethods to directly learn weather patterns from a large amount of\nmeteorological data. The output of hyper-networks constitutes a part of the\nweights for the physics networks. Experimental results demonstrate that, upon\nsuccessful integration of physical laws, DeepPhysiNet can accomplish multiple\ntasks simultaneously, not only enhancing forecast accuracy but also obtaining\ncontinuous spatiotemporal resolution results, which is unattainable by either\nthe NWP or DLP.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "18 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.04125v1",
    "published_date": "2024-01-04 05:05:16 UTC",
    "updated_date": "2024-01-04 05:05:16 UTC"
  },
  {
    "arxiv_id": "2401.02051v3",
    "title": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model",
    "authors": [
      "Fei Liu",
      "Xialiang Tong",
      "Mingxuan Yuan",
      "Xi Lin",
      "Fu Luo",
      "Zhenkun Wang",
      "Zhichao Lu",
      "Qingfu Zhang"
    ],
    "abstract": "Heuristics are widely used for dealing with complex search and optimization\nproblems. However, manual design of heuristics can be often very labour\nextensive and requires rich working experience and knowledge. This paper\nproposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that\nleverages both Large Language Models (LLMs) and Evolutionary Computation (EC)\nmethods for Automatic Heuristic Design (AHD). EoH represents the ideas of\nheuristics in natural language, termed thoughts. They are then translated into\nexecutable codes by LLMs. The evolution of both thoughts and codes in an\nevolutionary search framework makes it very effective and efficient for\ngenerating high-performance heuristics. Experiments on three widely studied\ncombinatorial optimization benchmark problems demonstrate that EoH outperforms\ncommonly used handcrafted heuristics and other recent AHD methods including\nFunSearch. Particularly, the heuristic produced by EoH with a low computational\nbudget (in terms of the number of queries to LLMs) significantly outperforms\nwidely-used human hand-crafted baseline algorithms for the online bin packing\nproblem.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02051v3",
    "published_date": "2024-01-04 04:11:59 UTC",
    "updated_date": "2024-06-01 16:48:37 UTC"
  },
  {
    "arxiv_id": "2401.04124v3",
    "title": "MobileAgent: enhancing mobile control via human-machine interaction and SOP integration",
    "authors": [
      "Tinghe Ding"
    ],
    "abstract": "Agents centered around Large Language Models (LLMs) are now capable of\nautomating mobile device operations for users. After fine-tuning to learn a\nuser's mobile operations, these agents can adhere to high-level user\ninstructions online. They execute tasks such as goal decomposition, sequencing\nof sub-goals, and interactive environmental exploration, until the final\nobjective is achieved. However, privacy concerns related to personalized user\ndata arise during mobile operations, requiring user confirmation. Moreover,\nusers' real-world operations are exploratory, with action data being complex\nand redundant, posing challenges for agent learning. To address these issues,\nin our practical application, we have designed interactive tasks between agents\nand humans to identify sensitive information and align with personalized user\nneeds. Additionally, we integrated Standard Operating Procedure (SOP)\ninformation within the model's in-context learning to enhance the agent's\ncomprehension of complex task execution. Our approach is evaluated on the new\ndevice control benchmark AitW, which encompasses 30K unique instructions across\nmulti-step tasks, including application operation, web searching, and web\nshopping. Experimental results show that the SOP-based agent achieves\nstate-of-the-art performance in LLMs without incurring additional inference\ncosts, boasting an overall action success rate of 66.92\\%. The code and data\nexamples are available at https://github.com/alipay/mobile-agent.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "agent, mobile control, SOP, human-machine interaction",
    "pdf_url": "http://arxiv.org/pdf/2401.04124v3",
    "published_date": "2024-01-04 03:44:42 UTC",
    "updated_date": "2024-01-17 06:35:45 UTC"
  },
  {
    "arxiv_id": "2401.10268v2",
    "title": "The complementary contributions of academia and industry to AI research",
    "authors": [
      "Lizhen Liang",
      "Han Zhuang",
      "James Zou",
      "Daniel E. Acuna"
    ],
    "abstract": "Artificial intelligence (AI) has seen fast paced development in industry and\nacademia. However, striking recent advances by industry have stunned the field,\ninviting a fresh perspective on the role of academic research on this progress.\nHere, we characterize the impact and type of AI produced by both environments\nover the last 25 years and establish several patterns. We find that articles\npublished by teams consisting exclusively of industry researchers tend to get\ngreater attention, with a higher chance of being highly cited and\ncitation-disruptive, and several times more likely to produce state-of-the-art\nmodels. In contrast, we find that exclusively academic teams publish the bulk\nof AI research and tend to produce higher novelty work, with single papers\nhaving several times higher likelihood of being unconventional and atypical.\nThe respective impact-novelty advantages of industry and academia are robust to\ncontrols for subfield, team size, seniority, and prestige. We find that\nacademic-industry collaborations produce the most impactful work overall but do\nnot have the novelty level of academic teams. Together, our findings identify\nthe unique and nearly irreplaceable contributions that both academia and\nindustry make toward the progress of AI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "35 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.10268v2",
    "published_date": "2024-01-04 03:08:13 UTC",
    "updated_date": "2024-09-18 23:33:29 UTC"
  },
  {
    "arxiv_id": "2401.02452v2",
    "title": "The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?",
    "authors": [
      "Tamay Besiroglu",
      "Sage Andrus Bergerson",
      "Amelia Michael",
      "Lennart Heim",
      "Xueyun Luo",
      "Neil Thompson"
    ],
    "abstract": "There are pronounced differences in the extent to which industrial and\nacademic AI labs use computing resources. We provide a data-driven survey of\nthe role of the compute divide in shaping machine learning research. We show\nthat a compute divide has coincided with a reduced representation of\nacademic-only research teams in compute intensive research topics, especially\nfoundation models. We argue that, academia will likely play a smaller role in\nadvancing the associated techniques, providing critical evaluation and\nscrutiny, and in the diffusion of such models. Concurrent with this change in\nresearch focus, there is a noticeable shift in academic research towards\nembracing open source, pre-trained models developed within the industry. To\naddress the challenges arising from this trend, especially reduced scrutiny of\ninfluential models, we recommend approaches aimed at thoughtfully expanding\nacademic insights. Nationally-sponsored computing infrastructure coupled with\nopen science initiatives could judiciously boost academic compute access,\nprioritizing research on interpretability, safety and security. Structured\naccess programs and third-party auditing may also allow measured external\nevaluation of industry systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02452v2",
    "published_date": "2024-01-04 01:26:11 UTC",
    "updated_date": "2024-01-08 12:37:58 UTC"
  },
  {
    "arxiv_id": "2401.10267v4",
    "title": "HyperSense: Hyperdimensional Intelligent Sensing for Energy-Efficient Sparse Data Processing",
    "authors": [
      "Sanggeon Yun",
      "Hanning Chen",
      "Ryozo Masukawa",
      "Hamza Errahmouni Barkam",
      "Andrew Ding",
      "Wenjun Huang",
      "Arghavan Rezvani",
      "Shaahin Angizi",
      "Mohsen Imani"
    ],
    "abstract": "Introducing HyperSense, our co-designed hardware and software system\nefficiently controls Analog-to-Digital Converter (ADC) modules' data generation\nrate based on object presence predictions in sensor data. Addressing challenges\nposed by escalating sensor quantities and data rates, HyperSense reduces\nredundant digital data using energy-efficient low-precision ADC, diminishing\nmachine learning system costs. Leveraging neurally-inspired HyperDimensional\nComputing (HDC), HyperSense analyzes real-time raw low-precision sensor data,\noffering advantages in handling noise, memory-centricity, and real-time\nlearning. Our proposed HyperSense model combines high-performance software for\nobject detection with real-time hardware prediction, introducing the novel\nconcept of Intelligent Sensor Control. Comprehensive software and hardware\nevaluations demonstrate our solution's superior performance, evidenced by the\nhighest Area Under the Curve (AUC) and sharpest Receiver Operating\nCharacteristic (ROC) curve among lightweight models. Hardware-wise, our\nFPGA-based domain-specific accelerator tailored for HyperSense achieves a 5.6x\nspeedup compared to YOLOv4 on NVIDIA Jetson Orin while showing up to 92.1%\nenergy saving compared to the conventional system. These results underscore\nHyperSense's effectiveness and efficiency, positioning it as a promising\nsolution for intelligent sensing and real-time data processing across diverse\napplications.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10267v4",
    "published_date": "2024-01-04 01:12:33 UTC",
    "updated_date": "2024-10-29 21:24:55 UTC"
  },
  {
    "arxiv_id": "2401.02015v1",
    "title": "Improving Diffusion-Based Image Synthesis with Context Prediction",
    "authors": [
      "Ling Yang",
      "Jingwei Liu",
      "Shenda Hong",
      "Zhilong Zhang",
      "Zhilin Huang",
      "Zheming Cai",
      "Wentao Zhang",
      "Bin Cui"
    ],
    "abstract": "Diffusion models are a new class of generative models, and have dramatically\npromoted image generation with unprecedented quality and diversity. Existing\ndiffusion models mainly try to reconstruct input image from a corrupted one\nwith a pixel-wise or feature-wise constraint along spatial axes. However, such\npoint-based reconstruction may fail to make each predicted pixel/feature fully\npreserve its neighborhood context, impairing diffusion-based image synthesis.\nAs a powerful source of automatic supervisory signal, context has been well\nstudied for learning representations. Inspired by this, we for the first time\npropose ConPreDiff to improve diffusion-based image synthesis with context\nprediction. We explicitly reinforce each point to predict its neighborhood\ncontext (i.e., multi-stride features/tokens/pixels) with a context decoder at\nthe end of diffusion denoising blocks in training stage, and remove the decoder\nfor inference. In this way, each point can better reconstruct itself by\npreserving its semantic connections with neighborhood context. This new\nparadigm of ConPreDiff can generalize to arbitrary discrete and continuous\ndiffusion backbones without introducing extra parameters in sampling procedure.\nExtensive experiments are conducted on unconditional image generation,\ntext-to-image generation and image inpainting tasks. Our ConPreDiff\nconsistently outperforms previous methods and achieves a new SOTA text-to-image\ngeneration results on MS-COCO, with a zero-shot FID score of 6.21.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.02015v1",
    "published_date": "2024-01-04 01:10:56 UTC",
    "updated_date": "2024-01-04 01:10:56 UTC"
  },
  {
    "arxiv_id": "2401.02009v3",
    "title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
    "authors": [
      "Wenqi Zhang",
      "Yongliang Shen",
      "Linjuan Wu",
      "Qiuying Peng",
      "Jun Wang",
      "Yueting Zhuang",
      "Weiming Lu"
    ],
    "abstract": "The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2401.02009v3",
    "published_date": "2024-01-04 00:32:33 UTC",
    "updated_date": "2024-06-06 18:46:03 UTC"
  }
]
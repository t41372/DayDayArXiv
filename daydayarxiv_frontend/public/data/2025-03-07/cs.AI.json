{
  "date": "2025-03-07",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-07 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 上的论文讨论热点集中在大型语言模型（LLMs）的各个方面，包括其推理能力、安全性、评估方法、效率优化以及在特定领域的应用。强化学习（RL）在机器人控制和策略优化中的应用也备受关注。此外，计算机视觉、多模态学习和AI伦理治理也是今日的研究重点。值得关注的论文包括探讨超级智能战略、揭示LLM越狱漏洞、提出新型模型融合与推理框架等。\n\n以下是今天值得关注的论文：\n\n---\n\n**重点论文 & LLM 相关研究**\n\n1.  **超级智能战略：专家版 (Superintelligence Strategy: Expert Version)**\n    *   作者：Dan Hendrycks, Eric Schmidt, Alexandr Wang\n    *   简介：这篇论文探讨了AI快速发展对国家安全的潜在影响，特别是超级智能可能带来的风险。作者提出了“相互确保的AI故障”（MAIM）概念，类似于核威慑中的“相互确保摧毁”（MAD），认为任何国家试图单方面获得AI主导地位都可能遭到对手的预防性破坏。文章提出了一个由威慑、防扩散和竞争力组成的三部分框架，作为应对超级智能时代的战略。这篇由知名人士撰写，讨论了AI发展中的重大战略问题。\n\n2.  **越狱（大多）比你想象的简单 (Jailbreaking is (Mostly) Simpler Than You Think)**\n    *   作者：Mark Russinovich, Ahmed Salem\n    *   简介：本文介绍了一种名为“上下文合规攻击”（CCA）的新型、无需优化的方法来绕过AI安全机制。与依赖复杂提示工程和计算密集型优化的现有方法不同，CCA利用了许多已部署AI系统固有的基本架构漏洞。通过巧妙地操纵对话历史，CCA诱使模型遵守伪造的对话上下文，从而触发受限行为。作者在多种开源和专有模型上的评估表明，这种简单的攻击可以规避最先进的安全协议。\n\n3.  **符号化混合专家模型：异构推理的自适应基于技能的路由 (Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning)**\n    *   作者：Justin Chih-Yao Chen, Sukwon Yun, Elias Stengel-Eskin 等\n    *   简介：该研究提出 Symbolic-MoE，一个符号化、基于文本、无梯度的混合专家（MoE）框架，用于在实例级别自适应地混合预训练的LLM专家。它不按任务级别选择专家，而是强调技能（如数学中的代数），动态选择最相关的专家集。通过技能路由和聚合器选择策略，Symbolic-MoE 在 MMLU-Pro、GPQA 等多个基准测试中显著优于 GPT4o-mini 和多智能体基线，且计算效率更高。\n\n4.  **R1-Searcher: 通过强化学习激励LLM的搜索能力 (R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning)**\n    *   作者：Huatong Song, Jinhao Jiang, Yingqian Min 等\n    *   简介：现有的大型推理模型（LRM）虽然在数学、编码等复杂推理任务上表现出色，但常依赖内部知识，对时效性或知识密集型问题可能产生不准确或幻觉。本文提出 R1-Searcher，一种新颖的两阶段基于结果的强化学习方法，旨在增强LLM的搜索能力，使其能在推理过程中自主调用外部搜索系统获取额外知识。该框架仅依赖RL，无需过程奖励或蒸馏，实验表明其显著优于现有的强RAG方法，甚至优于闭源的GPT-4o-mini。\n\n5.  **稀疏自编码器综述：解释大型语言模型的内部机制 (A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models)**\n    *   作者：Dong Shu, Xuansheng Wu, Haiyan Zhao 等\n    *   简介：这篇综述系统性地探讨了稀疏自编码器（SAEs）作为一种有前途的机制可解释性方法，用于理解大型语言模型（LLMs）的内部工作原理。文章概述了SAEs的原理、架构及其在LLM分析中的应用，讨论了如何利用SAEs解释模型内部运作、引导模型行为以及开发更透明的训练方法。\n\n6.  **容量感知推理：缓解混合专家模型中的掉队效应 (Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts)**\n    *   作者：Shwai He, Weilin Cai, Jiayi Huang, Ang Li\n    *   简介：MoE架构在专家并行下存在推理效率低下的问题，因为不平衡的令牌到专家分配导致某些专家过载（成为“掉队者”），而其他专家未被充分利用。本文提出“容量感知推理”，包含“容量感知令牌丢弃”和“容量感知令牌重路由”两种技术，以平衡负载，优化专家利用率，显著提高推理效率（例如，在Mixtral-8x7B-Instruct上性能提升0.2%，速度提升1.94倍）。\n\n7.  **思维草图：基于自适应认知启发的草图进行高效LLM推理 (Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching)**\n    *   作者：Simon A. Aytes, Jinheon Baek, Sung Ju Hwang\n    *   简介：虽然思维链（CoT）提示增强了LLM的推理能力，但常导致中间输出过于冗长，增加计算开销。本文提出思维草图（SoT），一种结合认知启发推理范式和语言约束的新型提示框架，旨在最小化令牌使用同时保持推理准确性。SoT通过轻量级路由模型动态选择三种认知范式（概念链、分块符号化、专家词典），在15个推理数据集上的评估表明，SoT能在准确率影响可忽略的情况下减少76%的令牌使用，在某些领域甚至能提高准确率。\n\n8.  **R1-Zero在2B非SFT模型上的视觉推理“顿悟时刻” (R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model)**\n    *   作者：Hengguang Zhou, Xirui Li, Ruochen Wang 等\n    *   简介：受DeepSeek R1在语言模型中通过RL自主发展复杂推理（表现为“顿悟时刻”）的启发，本报告首次在仅2B参数的非SFT多模态模型（Qwen2-VL-2B）上成功复现了这些 emergent characteristics。通过直接在SAT数据集上应用RL，模型在CVBench上达到59.47%的准确率，显著优于基线模型和SFT设置。报告还分享了在instruct模型上尝试复现R1式推理的失败经验和见解。\n\n9.  **EuroBERT: 为欧洲语言扩展多语言编码器 (EuroBERT: Scaling Multilingual Encoders for European Languages)**\n    *   作者：Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Duarte M. Alves 等\n    *   简介：本文重新审视了多语言编码器的发展，并借鉴了生成式解码器模型的进展，推出了 EuroBERT，一个覆盖欧洲及全球广泛使用语言的多语言编码器系列。这些模型在多语言能力、数学和编码等多种任务上优于现有替代方案，并原生支持长达8192个令牌的序列。作者公开了模型、训练检查点和训练框架。\n\n10. **语法驱动的代码表示：对于LLM来说，这值得追求吗？ (Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?)**\n    *   作者：Qingyuan Liang, Zhao Zhang, Zeyu Sun 等\n    *   简介：研究探讨了语法信息在十亿级参数LLM中的作用。虽然在小模型中语法表示能减少语法错误并提升性能，但在大模型中语法错误已很少见。作者开发了GrammarCoder系列模型，将语法规则融入代码生成过程。实验表明，即使在大模型中，基于语法的表示也能显著提高代码生成准确性，并增强模型区分细微代码差异的能力，减少语义错误。\n\n11. **关于大型语言模型在代码生成中的评估：用于定义推理指数的INFINITE方法 (Evaluating Large Language Models in Code Generation: INFINITE Methodology for Defining the Inference Index)**\n    *   作者：Nicholas Christakis, Dimitris Drikakis\n    *   简介：提出了一种名为INFINITE的新方法，用于定义推理指数（InI），旨在评估LLM在代码生成任务中的性能。InI指数关注效率、一致性和准确性三个关键组成部分。研究应用此方法比较了GPT-4o、OAI1和OAI3在生成Python LSTM代码以预测气象变量方面的表现，发现GPT-4o表现优异。\n\n12. **写作工作台：生成式写作的综合基准 (WritingBench: A Comprehensive Benchmark for Generative Writing)**\n    *   作者：Yuning Wu, Jiahao Mei, Ming Yan 等\n    *   简介：针对现有基准未能全面评估LLM在多样化写作任务中表现的问题，本文提出了WritingBench。该基准涵盖6个核心写作领域和100个子领域。同时提出一个查询依赖的评估框架，让LLM动态生成实例特定的评估标准，并辅以微调的批评模型进行评分。\n\n**强化学习 & 机器人**\n\n13. **多保真度策略梯度算法 (Multi-Fidelity Policy Gradient Algorithms)**\n    *   作者：Xinjie Liu, Cyrus Neary, Kushagra Gupta 等\n    *   简介：许多RL算法需要大量数据，限制了其在与操作系统的频繁交互不可行或高保真模拟昂贵的应用中的使用。本文提出多保真度策略梯度（MFPGs），一个RL框架，它混合少量目标环境数据和大量低保真模拟数据，为on-policy策略梯度构建无偏、低方差的估计器（控制变量）。实验表明，在目标环境样本有限时，MFPG能显著提高奖励和训练稳定性。\n\n14. **软策略优化：序列模型的在线离策略RL (Soft Policy Optimization: Online Off-Policy RL for Sequence Models)**\n    *   作者：Taco Cohen, David W. Zhang, Kunhao Zheng 等\n    *   简介：针对语言模型RL后训练几乎完全依赖PPO等on-policy方法（样本效率低、探索困难）的问题，本文提出软策略优化（SPO）。SPO是一种简单、可扩展、有原则的软RL方法，适用于序列模型策略，可以从任意在线和离线轨迹中学习，且无需单独的值模型。实验表明SPO在代码竞赛中优于PPO，速度更快、内存效率更高、稳定性更好，并能从离策略数据中受益。\n\n15. **BEHAVIOR 机器人套件：简化日常家务的真实世界全身操作 (BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities)**\n    *   作者：Yunfan Jiang, Ruohan Zhang, Josiah Wong 等\n    *   简介：针对移动操作机器人在真实世界家务任务中的挑战，本文分析了现有基准，指出双臂协调、稳定精确导航和广泛末端执行器可达性是关键。作者介绍了BEHAVIOR机器人套件（BRS），一个用于多样化家务任务的全身操作综合框架，包含一个双臂轮式机器人、经济高效的全身遥操作接口和学习全身视觉运动策略的新算法。\n\n16. **dARt Vinci：大规模手术机器人学习的自我中心数据收集 (dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale)**\n    *   作者：Yihao Liu, Yu-Chun Ku, Jiaming Zhang 等\n    *   简介：针对手术机器人学习中高质量数据稀缺的问题，本文介绍了dARt Vinci，一个可扩展的数据收集平台。该系统利用增强现实（AR）手部跟踪和高保真物理引擎来捕捉手术任务中的精细操作，通过模拟替代物理机器人设置，并利用AR实现更具自我中心的数据收集。用户研究证实了该系统的效率和可用性。\n\n17. **通过扩散组合进行生成式轨迹拼接 (Generative Trajectory Stitching through Diffusion Composition)**\n    *   作者：Yunhao Luo, Utkarsh A. Mishra, Yilun Du, Danfei Xu\n    *   简介：针对长时程规划中有效轨迹拼接的挑战，本文提出CompDiffuser，一种新颖的生成式方法，可以通过组合学习的方式拼接先前任务中较短的轨迹块来解决新任务。核心思想是将轨迹分布细分为重叠块，并通过单个双向扩散模型学习它们的条件关系，确保生成过程中段间信息的传播和物理上一致的连接。\n\n18. **通过强化学习和抽象模拟实现多机器人协作 (Multi-Robot Collaboration through Reinforcement Learning and Abstract Simulation)**\n    *   作者：Adam Labiosa, Josiah P. Hanna\n    *   简介：研究探讨了使用抽象模拟器进行多智能体强化学习（MARL）并将策略部署到物理机器人团队的可行性。策略在抽象模拟器中训练，然后通过低级感知和运动控制模块转移到物理机器人。研究确定了实现策略转移的关键修改，并在合作机器人足球任务中进行了实证研究。\n\n**计算机视觉 & 多模态**\n\n19. **VideoPainter: 具有即插即用上下文控制的任意长度视频修复与编辑 (VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control)**\n    *   作者：Yuxuan Bian, Zhaoyang Zhang, Xuan Ju 等\n    *   简介：针对现有视频修复方法难以生成完全遮挡对象或平衡背景保留与前景生成的问题，本文提出VideoPainter，一种新颖的双流范式。它包含一个高效的上下文编码器，处理遮挡视频并将背景上下文线索注入任何预训练的视频DiT模型中，以即插即用的方式生成语义一致的内容。同时引入目标区域ID重采样技术实现任意长度视频修复，并构建了大规模视频修复数据集VPData和基准VPBench。\n\n20. **TrajectoryCrafter: 通过扩散模型重定向单目视频的相机轨迹 (TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models)**\n    *   作者：Mark YU, Wenbo Hu, Jinbo Xing, Ying Shan\n    *   简介：本文提出TrajectoryCrafter，一种重定向单目视频相机轨迹的新方法。通过解耦确定性视图变换和随机内容生成，实现了对用户指定相机轨迹的精确控制。方法采用新颖的双流条件视频扩散模型，同时整合点云渲染和源视频作为条件。通过创新的双重重投影策略，结合网络单目视频和静态多视图数据集构建混合训练集，增强了泛化能力。\n\n21. **SAS: Segment Anything Small for Ultrasound -- 一种用于超声成像中鲁棒深度学习的非生成式数据增强技术 (SAS: Segment Anything Small for Ultrasound -- A Non-Generative Data Augmentation Technique for Robust Deep Learning in Ultrasound Imaging)**\n    *   作者：Danielle L. Ferreira, Ahana Gangopadhyay, Hsi-Ming Chang 等\n    *   简介：针对超声（US）图像中小解剖结构分割因噪声和成像条件变化而具有挑战性的问题，本文提出Segment Anything Small（SAS），一种简单有效的尺度和纹理感知数据增强技术。SAS通过调整大小和嵌入器官缩略图来模拟不同器官尺度，并向感兴趣区域注入噪声以模拟不同组织纹理，从而生成真实多样且无幻觉的训练数据，提高模型鲁棒性。\n\n22. **HexPlane 表示用于3D语义场景理解 (HexPlane Representation for 3D Semantic Scene Understanding)**\n    *   作者：Zeren Chen, Yuenan Hou, Yulin Chen 等\n    *   简介：本文介绍用于3D语义场景理解的HexPlane表示。首先设计视图投影模块（VPM）将3D点云投影到六个平面以最大程度保留空间信息，然后用2D编码器提取特征，并通过HexPlane关联模块（HAM）自适应融合信息。该表示高效且能利用优化的2D操作处理稀疏无序的3D点云，并在ScanNet和SemanticKITTI基准上取得了有竞争力的性能。\n\n**AI 安全、伦理与评估**\n\n23. **你的视频语言模型是可靠的裁判吗？ (Is Your Video Language Model a Reliable Judge?)**\n    *   作者：Ming Liu, Wensheng Zhang\n    *   简介：随着视频语言模型（VLM）应用增多，对其性能进行鲁棒且可扩展的评估变得至关重要。传统人工评估有限，自动方法（如用VLM评估VLM）兴起，但其可靠性未经充分探索。本研究发现，单一VLM评估可能不可靠或有偏见，而聚合多个（包括不可靠）VLM的评估（集体思维）并不一定能提高准确性。研究还发现，仅提高VLM的理解能力不足以使其成为更可靠的裁判。\n\n24. **面向生成式AI系统的评估科学 (Toward an Evaluation Science for Generative AI Systems)**\n    *   作者：Laura Weidinger, Inioluwa Deborah Raji, Hanna Wallach 等\n    *   简介：文章指出，当前对生成式AI系统性能和安全性的评估生态系统不足，常用基准存在有效性挑战，临时审计难以扩展。作者倡导发展生成式AI系统的评估科学，借鉴交通、航空、制药等领域的安全评估实践，提出三个关键教训：评估指标需适用于真实世界性能、指标需迭代改进、需建立评估机构和规范。\n\n25. **转向视角：利用转向向量集成实现LLM中鲁棒的偏见缓解 (Shifting Perspectives: Steering Vector Ensembles for Robust Bias Mitigation in LLMs)**\n    *   作者：Zara Siddique, Irtaza Khalid, Liam D. Turner, Luis Espinosa-Anke\n    *   简介：提出一种新颖的LLM偏见缓解方法，通过在前向传播中应用转向向量修改模型激活。使用贝叶斯优化系统地识别跨九个偏见轴的有效对比对数据集。在此基础上，引入转向向量集成（SVE），通过平均多个针对特定偏见轴优化的转向向量，利用集体力量在减少偏见和保持模型性能方面优于单个转向向量。\n\n26. **探索混合LLM中的欺骗与鲁棒性 (This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs)**\n    *   作者：Lorenz Wolf, Sangwoong Yoon, Ilija Bogunovic\n    *   简介：混合LLM智能体（MoA）架构在AlpacaEval 2.0等基准上表现出色，但其安全性和可靠性评估缺失。本文首次全面研究了MoA对故意提供误导性响应的欺骗性LLM智能体的鲁棒性。研究发现，引入单个精心设计的欺骗性智能体就能显著降低MoA性能。受威尼斯总督投票过程启发，作者提出了一系列无监督防御机制，恢复了大部分损失的性能。\n\n**其他值得关注的论文**\n\n*   **黑盒因果推断：通过元预测进行效应估计 (Black Box Causal Inference: Effect Estimation via Meta Prediction)**：提出BBCI方法，将因果推断视为数据集级别的预测问题，通过学习从采样的数据集-效应 对中预测因果效应来黑盒式地构建估计器。\n*   **学习顺序自回归模型及其在分子图生成中的应用 (Learning-Order Autoregressive Models with Application to Molecular Graph Generation)**：提出一种变体ARM，使用从数据中顺序推断的概率顺序生成高维数据，在分子图生成等任务中取得SOTA结果。\n*   **ElementaryNet: 用于预测普通形式博弈中人类行为的非策略性神经网络 (ElementaryNet: A Non-Strategic Neural Network for Predicting Human Behavior in Normal-Form Games)**：提出ElementaryNet，一种被证明只能进行非策略行为的神经网络架构，用于预测博弈中的人类行为。\n*   **IDEA Prune: 生成式语言模型预训练中的集成扩大与剪枝流程 (IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining)**：提出将扩大模型预训练纳入剪枝流程，并设计了一个集成的扩大-剪枝流程，优化整个过程以获得更好的剪枝模型。\n*   **通过命题内容提取改进RAG检索：言语行为理论方法 (Improving RAG Retrieval via Propositional Content Extraction: a Speech Act Theory Approach)**：研究发现，在嵌入前从用户查询中提取底层命题内容（去除意图标记）可以提高RAG系统的检索质量。\n*   **用于自主驾驶的离散对比学习扩散策略 (Discrete Contrastive Learning for Diffusion Policies in Autonomous Driving)**：提出一种新方法，利用对比学习从人类驾驶数据中提取驾驶风格字典，并用量化离散化这些风格，用于学习条件扩散策略以模拟人类驾驶员。\n*   **FinTMMBench: 金融领域时序感知多模态RAG基准测试 (FinTMMBench: Benchmarking Temporal-Aware Multi-Modal RAG in Finance)**：介绍了首个用于评估金融领域时序感知多模态RAG系统的综合基准FinTMMBench，包含多模态语料库、时序感知问题和多样化金融分析任务。\n*   **ORANSight-2.0: 面向O-RAN的基础LLM (ORANSight-2.0: Foundational LLMs for O-RAN)**：介绍了ORANSight-2.0，一个旨在为O-RAN开发专门基础LLM的计划，包含18个基于开源框架的LLM，并引入了RANSTRUCT（基于RAG的指令调优框架）和srsRANBench（新的评估基准）。\n\n---\n\n希望这份摘要能帮助你快速了解今日 arXiv 的热点！",
  "papers": [
    {
      "arxiv_id": "2503.05985v1",
      "title": "Black Box Causal Inference: Effect Estimation via Meta Prediction",
      "title_zh": "黑盒因果推断：通过元预测实现效应估计",
      "authors": [
        "Lucius E. J. Bynum",
        "Aahlad Manas Puli",
        "Diego Herrero-Quevedo",
        "Nhi Nguyen",
        "Carlos Fernandez-Granda",
        "Kyunghyun Cho",
        "Rajesh Ranganath"
      ],
      "abstract": "Causal inference and the estimation of causal effects plays a central role in\ndecision-making across many areas, including healthcare and economics.\nEstimating causal effects typically requires an estimator that is tailored to\neach problem of interest. But developing estimators can take significant effort\nfor even a single causal inference setting. For example, algorithms for\nregression-based estimators, propensity score methods, and doubly robust\nmethods were designed across several decades to handle causal estimation with\nobserved confounders. Similarly, several estimators have been developed to\nexploit instrumental variables (IVs), including two-stage least-squares (TSLS),\ncontrol functions, and the method-of-moments. In this work, we instead frame\ncausal inference as a dataset-level prediction problem, offloading algorithm\ndesign to the learning process. The approach we introduce, called black box\ncausal inference (BBCI), builds estimators in a black-box manner by learning to\npredict causal effects from sampled dataset-effect pairs. We demonstrate\naccurate estimation of average treatment effects (ATEs) and conditional average\ntreatment effects (CATEs) with BBCI across several causal inference problems\nwith known identification, including problems with less developed estimators.",
      "tldr_zh": "本研究提出了一种名为\"黑盒因果推断\"(BBCI)的新方法，将因果效应估计问题转化为数据集层面的预测任务。该方法通过从采样的数据集-效应配对中学习预测因果效应，以黑盒方式构建估计器，避免了传统方法需要为每个问题单独设计估计器的局限性。实验表明，BBCI能够准确估计平均处理效应(ATE)和条件平均处理效应(CATE)，适用于包括观测混杂因素和工具变量(IV)在内的多种因果推断场景，甚至在现有估计方法尚不完善的领域也表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05985v1",
      "published_date": "2025-03-07 23:43:19 UTC",
      "updated_date": "2025-03-07 23:43:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:47:59.934596"
    },
    {
      "arxiv_id": "2503.05980v1",
      "title": "SINdex: Semantic INconsistency Index for Hallucination Detection in LLMs",
      "title_zh": "SINdex：面向大语言模型幻觉检测的语义不一致性指数",
      "authors": [
        "Samir Abdaljalil",
        "Hasan Kurban",
        "Parichit Sharma",
        "Erchin Serpedin",
        "Rachad Atat"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed across diverse\ndomains, yet they are prone to generating factually incorrect outputs -\ncommonly known as \"hallucinations.\" Among existing mitigation strategies,\nuncertainty-based methods are particularly attractive due to their ease of\nimplementation, independence from external data, and compatibility with\nstandard LLMs. In this work, we introduce a novel and scalable\nuncertainty-based semantic clustering framework for automated hallucination\ndetection. Our approach leverages sentence embeddings and hierarchical\nclustering alongside a newly proposed inconsistency measure, SINdex, to yield\nmore homogeneous clusters and more accurate detection of hallucination\nphenomena across various LLMs. Evaluations on prominent open- and closed-book\nQA datasets demonstrate that our method achieves AUROC improvements of up to\n9.3% over state-of-the-art techniques. Extensive ablation studies further\nvalidate the effectiveness of each component in our framework.",
      "tldr_zh": "该研究提出了SINdex（语义不一致性指数），一种基于不确定性的语义聚类框架，用于检测大语言模型(LLMs)中的幻觉现象。该方法结合句子嵌入和层次聚类，通过新提出的不一致性度量SINdex，生成更同质的聚类并提高幻觉检测的准确性。实验表明，该框架在开放和封闭问答数据集上比现有技术AUROC指标提升了9.3%，并通过消融研究验证了各组成部分的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05980v1",
      "published_date": "2025-03-07 23:25:19 UTC",
      "updated_date": "2025-03-07 23:25:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:48:04.734046"
    },
    {
      "arxiv_id": "2503.05979v1",
      "title": "Learning-Order Autoregressive Models with Application to Molecular Graph Generation",
      "title_zh": "学习顺序自回归模型及其在分子图生成中的应用",
      "authors": [
        "Zhe Wang",
        "Jiaxin Shi",
        "Nicolas Heess",
        "Arthur Gretton",
        "Michalis K. Titsias"
      ],
      "abstract": "Autoregressive models (ARMs) have become the workhorse for sequence\ngeneration tasks, since many problems can be modeled as next-token prediction.\nWhile there appears to be a natural ordering for text (i.e., left-to-right),\nfor many data types, such as graphs, the canonical ordering is less obvious. To\naddress this problem, we introduce a variant of ARM that generates\nhigh-dimensional data using a probabilistic ordering that is sequentially\ninferred from data. This model incorporates a trainable probability\ndistribution, referred to as an \\emph{order-policy}, that dynamically decides\nthe autoregressive order in a state-dependent manner. To train the model, we\nintroduce a variational lower bound on the exact log-likelihood, which we\noptimize with stochastic gradient estimation. We demonstrate experimentally\nthat our method can learn meaningful autoregressive orderings in image and\ngraph generation. On the challenging domain of molecular graph generation, we\nachieve state-of-the-art results on the QM9 and ZINC250k benchmarks, evaluated\nusing the Fr\\'{e}chet ChemNet Distance (FCD).",
      "tldr_zh": "该论文提出了一种学习顺序的自回归模型（Learning-Order ARM），用于解决高维数据（如图像和分子图）生成中的顺序不确定性问题。该方法通过可训练的\"顺序策略\"（order-policy）动态决定自回归生成顺序，并采用变分下界和随机梯度估计进行优化。实验表明，该方法在分子图生成任务中表现优异，在QM9和ZINC250k基准测试上达到了当前最优水平（以Fr\\'echet ChemNet Distance评估）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05979v1",
      "published_date": "2025-03-07 23:24:24 UTC",
      "updated_date": "2025-03-07 23:24:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:48:12.394062"
    },
    {
      "arxiv_id": "2503.05977v1",
      "title": "Is Your Video Language Model a Reliable Judge?",
      "title_zh": "你的视频语言模型是可靠的评判者吗？",
      "authors": [
        "Ming Liu",
        "Wensheng Zhang"
      ],
      "abstract": "As video language models (VLMs) gain more applications in various scenarios,\nthe need for robust and scalable evaluation of their performance becomes\nincreasingly critical. The traditional human expert-based evaluation of VLMs\nhas limitations in consistency and scalability, which sparked interest in\nautomatic methods such as employing VLMs to evaluate VLMs. However, the\nreliability of VLMs as judges remains underexplored. Existing methods often\nrely on a single VLM as the evaluator. However, this approach can be unreliable\nor biased because such a model may lack the ability to fully understand the\ncontent and may have inherent biases, ultimately compromising evaluation\nreliability. A remedy is to apply the principle of collective thoughts,\naggregating evaluations from multiple VLMs to enhance reliability. This study\ninvestigates the efficacy of such approaches, particularly when the pool of\njudges includes both reliable and unreliable models. Our findings reveal that\nincorporating collective judgments from such a mixed pool does not necessarily\nimprove the accuracy of the final evaluation. The inclusion of less reliable\njudges can introduce noise, undermining the overall reliability of the\noutcomes. To explore the factors that impact evaluation reliability, we\nfine-tune an underperforming VLM judge, Video-LLaVA, and observe that improved\nunderstanding ability alone is insufficient to make VLM judges more reliable.\nThese findings stress the limitations of collective thought approaches and\nhighlight the need for more advanced methods that can account for the\nreliability of individual models. Our study promotes the development of more\nreliable evaluation methods for VLMs",
      "tldr_zh": "该研究探讨了视频语言模型(VLMs)作为评估工具的可靠性问题。传统方法依赖单一VLM进行评估，但这种方法可能因模型的理解能力不足或固有偏见而不可靠。研究提出通过集体思维(collective thoughts)聚合多个VLM的评估结果来提升可靠性，但实验表明，当评估池中包含不可靠模型时，评估准确性并未显著提高，反而可能引入噪声。此外，研究对表现不佳的VLM评估者Video-LLaVA进行微调，发现仅提升模型的理解能力不足以使其更可靠。这些发现揭示了集体思维方法的局限性，并强调了开发更先进评估方法的必要性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05977v1",
      "published_date": "2025-03-07 23:17:59 UTC",
      "updated_date": "2025-03-07 23:17:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:48:56.334199"
    },
    {
      "arxiv_id": "2503.05972v1",
      "title": "Optimal sensor deception in stochastic environments with partial observability to mislead a robot to a decoy goal",
      "title_zh": "在部分可观测随机环境中利用传感器欺骗将机器人误导至诱饵目标的最优策略",
      "authors": [
        "Hazhar Rahmani",
        "Mukulika Ghosh",
        "Syed Md Hasnayeen"
      ],
      "abstract": "Deception is a common strategy adapted by autonomous systems in adversarial\nsettings. Existing deception methods primarily focus on increasing opacity or\nmisdirecting agents away from their goal or itinerary. In this work, we propose\na deception problem aiming to mislead the robot towards a decoy goal through\naltering sensor events under a constrained budget of alteration. The\nenvironment along with the robot's interaction with it is modeled as a\nPartially Observable Markov Decision Process (POMDP), and the robot's action\nselection is governed by a Finite State Controller (FSC). Given a constrained\nbudget for sensor event modifications, the objective is to compute a sensor\nalteration that maximizes the probability of the robot reaching a decoy goal.\nWe establish the computational hardness of the problem by a reduction from the\n$0/1$ Knapsack problem and propose a Mixed Integer Linear Programming (MILP)\nformulation to compute optimal deception strategies. We show the efficacy of\nour MILP formulation via a sequence of experiments.",
      "tldr_zh": "该研究提出了一种在部分可观测的随机环境中通过传感器欺骗引导机器人到达虚假目标的最优策略。研究将环境建模为部分可观测马尔可夫决策过程（POMDP），机器人的行动选择由有限状态控制器（FSC）控制。在有限的传感器事件修改预算下，目标是通过传感器修改最大化机器人到达虚假目标的概率。研究证明了该问题的计算复杂性，并提出了混合整数线性规划（MILP）公式来计算最优欺骗策略。实验验证了该方法的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05972v1",
      "published_date": "2025-03-07 22:57:27 UTC",
      "updated_date": "2025-03-07 22:57:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:48:25.171829"
    },
    {
      "arxiv_id": "2503.05971v2",
      "title": "A Real-time Multimodal Transformer Neural Network-powered Wildfire Forecasting System",
      "title_zh": "基于实时多模态Transformer神经网络的野火预测系统",
      "authors": [
        "Qijun Chen",
        "Shaofan Li"
      ],
      "abstract": "Due to climate change, the extreme wildfire has become one of the most\ndangerous natural hazards to human civilization. Even though, some wildfires\nmay be initially caused by human activity, but the spread of wildfires is\nmainly determined by environmental factors, for examples, (1) weather\nconditions such as temperature, wind direction and intensity, and moisture\nlevels; (2) the amount and types of dry vegetation in a local area, and (3)\ntopographic or local terrian conditions, which affects how much rain an area\ngets and how fire dynamics will be constrained or faciliated. Thus, to\naccurately forecast wildfire occurrence has become one of most urgent and\ntaunting environmental challenges in global scale. In this work, we developed a\nreal-time Multimodal Transformer Neural Network Machine Learning model that\ncombines several advanced artificial intelligence techniques and statistical\nmethods to practically forecast the occurrence of wildfire at the precise\nlocation in real time, which not only utilizes large scale data information\nsuch as hourly weather forecasting data, but also takes into account small\nscale topographical data such as local terrain condition and local vegetation\nconditions collecting from Google Earth images to determine the probabilities\nof wildfire occurrence location at small scale as well as their timing\nsynchronized with weather forecast information. By using the wildfire data in\nthe United States from 1992 to 2015 to train the multimodal transformer neural\nnetwork, it can predict the probabilities of wildfire occurrence according to\nthe real-time weather forecast and the synchronized Google Earth image data to\nprovide the wildfire occurrence probability in any small location ($100m^2$)\nwithin 24 hours ahead.",
      "tldr_zh": "本研究开发了一种基于多模态Transformer神经网络（Multimodal Transformer Neural Network）的实时野火预测系统。该系统创新性地整合了大尺度气象预报数据和小尺度地形植被数据（来自Google Earth），通过机器学习方法实现了100平方米精度的24小时野火发生概率预测。利用1992-2015年美国野火数据进行训练，该模型能同步处理实时天气信息和局部地形特征，显著提升了预测准确性。这项研究为应对气候变化下的极端野火灾害提供了重要的实时预警工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05971v2",
      "published_date": "2025-03-07 22:48:46 UTC",
      "updated_date": "2025-03-12 03:22:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:48:42.379414"
    },
    {
      "arxiv_id": "2503.05966v2",
      "title": "Explaining the Unexplainable: A Systematic Review of Explainable AI in Finance",
      "title_zh": "解释不可解释之物：金融领域可解释性人工智能的系统性综述",
      "authors": [
        "Md Talha Mohsin",
        "Nabid Bin Nasim"
      ],
      "abstract": "Practitioners and researchers trying to strike a balance between accuracy and\ntransparency center Explainable Artificial Intelligence (XAI) at the junction\nof finance. This paper offers a thorough overview of the changing scene of XAI\napplications in finance together with domain-specific implementations,\nmethodological developments, and trend mapping of research. Using bibliometric\nand content analysis, we find topic clusters, significant research, and most\noften used explainability strategies used in financial industries. Our results\nshow a substantial dependence on post-hoc interpretability techniques;\nattention mechanisms, feature importance analysis and SHAP are the most often\nused techniques among them. This review stresses the need of multidisciplinary\napproaches combining financial knowledge with improved explainability paradigms\nand exposes important shortcomings in present XAI systems.",
      "tldr_zh": "这篇论文对可解释人工智能(XAI)在金融领域的应用进行了系统性综述，揭示了该领域的研究现状和发展趋势。研究发现，金融行业主要依赖事后解释技术(post-hoc interpretability)，其中最常用的是注意力机制(attention mechanisms)、特征重要性分析(feature importance analysis)和SHAP方法。研究通过文献计量和内容分析，识别了关键研究主题集群，并指出当前XAI系统在结合金融专业知识方面仍存在不足，强调需要开发跨学科的解释性范式。",
      "categories": [
        "q-fin.GN",
        "cs.AI"
      ],
      "primary_category": "q-fin.GN",
      "comment": "2 tables, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05966v2",
      "published_date": "2025-03-07 22:36:44 UTC",
      "updated_date": "2025-03-17 15:37:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:48:41.266810"
    },
    {
      "arxiv_id": "2503.05963v1",
      "title": "Bayesian Graph Traversal",
      "title_zh": "贝叶斯图遍历",
      "authors": [
        "William N. Caballero",
        "Phillip R. Jenkins",
        "David Banks",
        "Matthew Robbins"
      ],
      "abstract": "This research considers Bayesian decision-analytic approaches toward the\ntraversal of an uncertain graph. Namely, a traveler progresses over a graph in\nwhich rewards are gained upon a node's first visit and costs are incurred for\nevery edge traversal. The traveler knows the graph's adjacency matrix and his\nstarting position but does not know the rewards and costs. The traveler is a\nBayesian who encodes his beliefs about these values using a Gaussian process\nprior and who seeks to maximize his expected utility over these beliefs.\nAdopting a decision-analytic perspective, we develop sequential decision-making\nsolution strategies for this coupled information-collection and network-routing\nproblem. We show that the problem is NP-Hard and derive properties of the\noptimal walk. These properties provide heuristics for the traveler's problem\nthat balance exploration and exploitation. We provide a practical case study\nfocused on the use of unmanned aerial systems for public safety and empirically\nstudy policy performance in myriad Erdos-Renyi settings.",
      "tldr_zh": "该研究提出了一种贝叶斯决策分析方法用于不确定图（uncertain graph）的遍历问题。旅行者（traveler）在已知图结构但不确定节点收益和边成本的情况下，采用高斯过程（Gaussian process）先验对参数建模，以最大化期望效用。研究证明该问题是NP难问题，并推导了最优路径的性质，提出了平衡探索与利用（exploration-exploitation）的启发式策略。最后通过无人机公共安全案例和Erdos-Renyi随机图的实证研究验证了策略效果。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "stat.OT",
        "62C99, 68T20"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 7 tables, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05963v1",
      "published_date": "2025-03-07 22:05:06 UTC",
      "updated_date": "2025-03-07 22:05:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:48:56.035281"
    },
    {
      "arxiv_id": "2503.05958v1",
      "title": "SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc",
      "title_zh": "SANDWiCH：基于邻域语义分析的上下文词语临时消歧",
      "authors": [
        "Daniel Guzman-Olivares",
        "Lara Quijano-Sanchez",
        "Federico Liberatore"
      ],
      "abstract": "The rise of generative chat-based Large Language Models (LLMs) over the past\ntwo years has spurred a race to develop systems that promise near-human\nconversational and reasoning experiences. However, recent studies indicate that\nthe language understanding offered by these models remains limited and far from\nhuman-like performance, particularly in grasping the contextual meanings of\nwords, an essential aspect of reasoning. In this paper, we present a simple yet\ncomputationally efficient framework for multilingual Word Sense Disambiguation\n(WSD). Our approach reframes the WSD task as a cluster discrimination analysis\nover a semantic network refined from BabelNet using group algebra. We validate\nour methodology across multiple WSD benchmarks, achieving a new state of the\nart for all languages and tasks, as well as in individual assessments by part\nof speech. Notably, our model significantly surpasses the performance of\ncurrent alternatives, even in low-resource languages, while reducing the\nparameter count by 72%.",
      "tldr_zh": "这篇论文提出了SANDWiCH框架，通过语义网络邻域分析来解决上下文词义消歧(WSD)问题。该方法将WSD任务重构为基于BabelNet构建的语义网络上的聚类判别分析，结合群代数进行优化。实验表明，该模型在多种语言的WSD基准测试中均达到最新最优性能，且在低资源语言中表现突出，同时将参数量减少了72%，显著优于现有方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 2 figures, 7 tables, NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05958v1",
      "published_date": "2025-03-07 21:52:32 UTC",
      "updated_date": "2025-03-07 21:52:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:49:26.927386"
    },
    {
      "arxiv_id": "2503.05951v1",
      "title": "TPU-Gen: LLM-Driven Custom Tensor Processing Unit Generator",
      "title_zh": "TPU-Gen：基于大型语言模型驱动的定制张量处理单元生成器",
      "authors": [
        "Deepak Vungarala",
        "Mohammed E. Elbtity",
        "Sumiya Syed",
        "Sakila Alam",
        "Kartik Pandit",
        "Arnob Ghosh",
        "Ramtin Zand",
        "Shaahin Angizi"
      ],
      "abstract": "The increasing complexity and scale of Deep Neural Networks (DNNs)\nnecessitate specialized tensor accelerators, such as Tensor Processing Units\n(TPUs), to meet various computational and energy efficiency requirements.\nNevertheless, designing optimal TPU remains challenging due to the high domain\nexpertise level, considerable manual design time, and lack of high-quality,\ndomain-specific datasets. This paper introduces TPU-Gen, the first Large\nLanguage Model (LLM) based framework designed to automate the exact and\napproximate TPU generation process, focusing on systolic array architectures.\nTPU-Gen is supported with a meticulously curated, comprehensive, and\nopen-source dataset that covers a wide range of spatial array designs and\napproximate multiply-and-accumulate units, enabling design reuse, adaptation,\nand customization for different DNN workloads. The proposed framework leverages\nRetrieval-Augmented Generation (RAG) as an effective solution for a data-scare\nhardware domain in building LLMs, addressing the most intriguing issue,\nhallucinations. TPU-Gen transforms high-level architectural specifications into\noptimized low-level implementations through an effective hardware generation\npipeline. Our extensive experimental evaluations demonstrate superior\nperformance, power, and area efficiency, with an average reduction in area and\npower of 92\\% and 96\\% from the manual optimization reference values. These\nresults set new standards for driving advancements in next-generation design\nautomation tools powered by LLMs.",
      "tldr_zh": "本文提出了TPU-Gen，首个基于大语言模型(LLM)的定制化张量处理器(TPU)自动生成框架。该框架通过检索增强生成(RAG)技术解决硬件设计领域数据稀缺问题，能将高级架构规范转化为优化的底层实现，特别针对脉动阵列架构进行精确和近似TPU设计。实验表明，TPU-Gen在面积和功耗上分别比人工优化基准平均降低92%和96%，同时配套开源的数据集覆盖了多种空间阵列设计和近似乘累加单元，为不同DNN工作负载提供定制化支持。这一成果为LLM驱动的下一代设计自动化工具设立了新标准。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "8 Pages, 9 Figures, 5 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.05951v1",
      "published_date": "2025-03-07 21:41:42 UTC",
      "updated_date": "2025-03-07 21:41:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:49:21.781853"
    },
    {
      "arxiv_id": "2503.10655v1",
      "title": "Language modelling techniques for analysing the impact of human genetic variation",
      "title_zh": "人类遗传变异影响分析的语言建模技术",
      "authors": [
        "Megha Hegde",
        "Jean-Christophe Nebel",
        "Farzana Rahman"
      ],
      "abstract": "Interpreting the effects of variants within the human genome and proteome is\nessential for analysing disease risk, predicting medication response, and\ndeveloping personalised health interventions. Due to the intrinsic similarities\nbetween the structure of natural languages and genetic sequences, natural\nlanguage processing techniques have demonstrated great applicability in\ncomputational variant effect prediction. In particular, the advent of the\nTransformer has led to significant advancements in the field. However,\nTransformer-based models are not without their limitations, and a number of\nextensions and alternatives have been developed to improve results and enhance\ncomputational efficiency. This review explores the use of language models for\ncomputational variant effect prediction over the past decade, analysing the\nmain architectures, and identifying key trends and future directions.",
      "tldr_zh": "这篇综述论文探讨了语言模型在人类基因变异影响分析中的应用。研究指出，由于自然语言与基因序列的结构相似性，自然语言处理技术（特别是Transformer模型）已被广泛应用于计算变异效应预测领域，用于评估疾病风险、药物反应和个性化医疗。尽管Transformer模型取得了显著进展，但仍存在局限性，研究者已开发多种改进方案以提升预测效果和计算效率。论文系统回顾了过去十年该领域的主要模型架构，并分析了关键趋势和未来发展方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10655v1",
      "published_date": "2025-03-07 21:34:17 UTC",
      "updated_date": "2025-03-07 21:34:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:49:41.829943"
    },
    {
      "arxiv_id": "2503.05944v1",
      "title": "Enhancing Reasoning with Collaboration and Memory",
      "title_zh": "通过协作与记忆增强推理能力",
      "authors": [
        "Julie Michelman",
        "Nasrin Baratalipour",
        "Matthew Abueg"
      ],
      "abstract": "We envision a continuous collaborative learning system where groups of LLM\nagents work together to solve reasoning problems, drawing on memory they\ncollectively build to improve performance as they gain experience. This work\nestablishes the foundations for such a system by studying the interoperability\nof chain-of-thought reasoning styles, multi-agent collaboration, and memory\nbanks. Extending beyond the identical agents of self-consistency, we introduce\nvaried-context agents with diverse exemplars and a summarizer agent in place of\nvoting. We generate frozen and continuously learned memory banks of exemplars\nand pair them with fixed, random, and similarity-based retrieval mechanisms.\nOur systematic study reveals where various methods contribute to reasoning\nperformance of two LLMs on three grounded reasoning tasks, showing that random\nexemplar selection can often beat more principled approaches, and in some\ntasks, inclusion of any exemplars serves only to distract both weak and strong\nmodels.",
      "tldr_zh": "该研究提出了一种持续协作学习系统，使多个LLM智能体能够通过集体构建记忆库来协同解决推理问题。通过结合链式思维推理(chain-of-thought)、多智能体协作和记忆库技术，系统采用多样化上下文智能体和总结智能体取代传统投票机制，并探索了固定/持续学习型记忆库与不同检索机制的配合。研究发现，在三个基础推理任务中，随机范例选择常优于原则性方法，且某些任务中范例反而会干扰模型表现，为LLM协作推理系统设计提供了重要洞见。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05944v1",
      "published_date": "2025-03-07 21:19:21 UTC",
      "updated_date": "2025-03-07 21:19:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:49:47.599628"
    },
    {
      "arxiv_id": "2503.05938v1",
      "title": "Uncertainty Quantification From Scaling Laws in Deep Neural Networks",
      "title_zh": "深度神经网络中基于缩放定律的不确定性量化",
      "authors": [
        "Ibrahim Elsharkawy",
        "Yonatan Kahn",
        "Benjamin Hooberman"
      ],
      "abstract": "Quantifying the uncertainty from machine learning analyses is critical to\ntheir use in the physical sciences. In this work we focus on uncertainty\ninherited from the initialization distribution of neural networks. We compute\nthe mean $\\mu_{\\mathcal{L}}$ and variance $\\sigma_{\\mathcal{L}}^2$ of the test\nloss $\\mathcal{L}$ for an ensemble of multi-layer perceptrons (MLPs) with\nneural tangent kernel (NTK) initialization in the infinite-width limit, and\ncompare empirically to the results from finite-width networks for three example\ntasks: MNIST classification, CIFAR classification and calorimeter energy\nregression. We observe scaling laws as a function of training set size\n$N_\\mathcal{D}$ for both $\\mu_{\\mathcal{L}}$ and $\\sigma_{\\mathcal{L}}$, but\nfind that the coefficient of variation $\\epsilon_{\\mathcal{L}} \\equiv\n\\sigma_{\\mathcal{L}}/\\mu_{\\mathcal{L}}$ becomes independent of $N_\\mathcal{D}$\nat both infinite and finite width for sufficiently large $N_\\mathcal{D}$. This\nimplies that the coefficient of variation of a finite-width network may be\napproximated by its infinite-width value, and may in principle be calculable\nusing finite-width perturbation theory.",
      "tldr_zh": "该研究探讨了深度神经网络中由初始化分布引起的不确定性量化问题。通过计算无限宽度极限下多层感知机(MLPs)测试损失的均值$\\mu_{\\mathcal{L}}$和方差$\\sigma_{\\mathcal{L}}^2$，并与有限宽度网络的结果进行对比，发现了训练集大小$N_\\mathcal{D}$对$\\mu_{\\mathcal{L}}$和$\\sigma_{\\mathcal{L}}$的缩放规律。研究发现，当$N_\\mathcal{D}$足够大时，变异系数$\\epsilon_{\\mathcal{L}} \\equiv \\sigma_{\\mathcal{L}}/\\mu_{\\mathcal{L}}$在无限和有限宽度下均与$N_\\mathcal{D}$无关，表明有限宽度网络的变异系数可用其无限宽度值近似，并可通过有限宽度微扰理论计算。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "hep-ex",
        "hep-ph",
        "hep-th"
      ],
      "primary_category": "cs.LG",
      "comment": "18+3 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05938v1",
      "published_date": "2025-03-07 21:15:11 UTC",
      "updated_date": "2025-03-07 21:15:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:49:58.661437"
    },
    {
      "arxiv_id": "2503.05937v1",
      "title": "The Unified Control Framework: Establishing a Common Foundation for Enterprise AI Governance, Risk Management and Regulatory Compliance",
      "title_zh": "统一控制框架：为企业AI治理、风险管理和监管合规建立共同基础",
      "authors": [
        "Ian W. Eisenberg",
        "Lucía Gamboa",
        "Eli Sherman"
      ],
      "abstract": "The rapid adoption of AI systems presents enterprises with a dual challenge:\naccelerating innovation while ensuring responsible governance. Current AI\ngovernance approaches suffer from fragmentation, with risk management\nframeworks that focus on isolated domains, regulations that vary across\njurisdictions despite conceptual alignment, and high-level standards lacking\nconcrete implementation guidance. This fragmentation increases governance costs\nand creates a false dichotomy between innovation and responsibility. We propose\nthe Unified Control Framework (UCF): a comprehensive governance approach that\nintegrates risk management and regulatory compliance through a unified set of\ncontrols. The UCF consists of three key components: (1) a comprehensive risk\ntaxonomy synthesizing organizational and societal risks, (2) structured policy\nrequirements derived from regulations, and (3) a parsimonious set of 42\ncontrols that simultaneously address multiple risk scenarios and compliance\nrequirements. We validate the UCF by mapping it to the Colorado AI Act,\ndemonstrating how our approach enables efficient, adaptable governance that\nscales across regulations while providing concrete implementation guidance. The\nUCF reduces duplication of effort, ensures comprehensive coverage, and provides\na foundation for automation, enabling organizations to achieve responsible AI\ngovernance without sacrificing innovation speed.",
      "tldr_zh": "该研究提出统一控制框架(UCF)，旨在解决企业AI治理中风险管理与合规监管的碎片化问题。该框架通过整合42项核心控制措施，统一涵盖组织和社会风险分类、政策要求结构化解析等关键组件，能同时应对多种风险场景和合规需求。验证表明，UCF可高效映射至《科罗拉多AI法案》等法规，在降低治理成本的同时保持创新速度，为企业AI治理提供了可自动化扩展的实施基础。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05937v1",
      "published_date": "2025-03-07 21:14:49 UTC",
      "updated_date": "2025-03-07 21:14:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:50:20.366977"
    },
    {
      "arxiv_id": "2503.05929v1",
      "title": "Audio-to-Image Encoding for Improved Voice Characteristic Detection Using Deep Convolutional Neural Networks",
      "title_zh": "基于深度卷积神经网络的音频到图像编码以提升语音特征检测",
      "authors": [
        "Youness Atif"
      ],
      "abstract": "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
      "tldr_zh": "该研究提出了一种创新的音频到图像编码框架，将语音特征的多维度信息整合到单张RGB图像中用于说话人识别。该方法通过绿色通道编码原始音频数据，红色通道嵌入包括基频、频谱质心、MFCC等关键指标的统计特征，蓝色通道则以空间组织形式呈现这些特征。实验表明，基于该复合图像训练的深度卷积神经网络在两人说话人分类任务中达到98%准确率，证明这种多通道集成表示能为语音识别提供更具区分度的输入特征。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "11 pages, 24 figures, 1 table, 3 algorithms. Submitted to\n  F1000Research",
      "pdf_url": "http://arxiv.org/pdf/2503.05929v1",
      "published_date": "2025-03-07 20:49:56 UTC",
      "updated_date": "2025-03-07 20:49:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:51:09.968525"
    },
    {
      "arxiv_id": "2503.05925v1",
      "title": "ElementaryNet: A Non-Strategic Neural Network for Predicting Human Behavior in Normal-Form Games",
      "title_zh": "ElementaryNet：一种用于预测正规形式博弈中人类行为的非策略性神经网络",
      "authors": [
        "Greg d'Eon",
        "Hala Murad",
        "Kevin Leyton-Brown",
        "James R. Wright"
      ],
      "abstract": "Models of human behavior in game-theoretic settings often distinguish between\nstrategic behavior, in which a player both reasons about how others will act\nand best responds to these beliefs, and \"level-0\" non-strategic behavior, in\nwhich they do not respond to explicit beliefs about others. The state of the\nart for predicting human behavior on unrepeated simultaneous-move games is\nGameNet, a neural network that learns extremely complex level-0 specifications\nfrom data. The current paper makes three contributions. First, it shows that\nGameNet's level-0 specifications are too powerful, because they are capable of\nstrategic reasoning. Second, it introduces a novel neural network architecture\n(dubbed ElementaryNet) and proves that it is only capable of nonstrategic\nbehavior. Third, it describes an extensive experimental evaluation of\nElementaryNet. Our overall findings are that (1) ElementaryNet dramatically\nunderperforms GameNet when neither model is allowed to explicitly model higher\nlevel agents who best-respond to the model's predictions, indicating that good\nperformance on our dataset requires a model capable of strategic reasoning; (2)\nthat the two models achieve statistically indistinguishable performance when\nsuch higher-level agents are introduced, meaning that ElementaryNet's\nrestriction to a non-strategic level-0 specification does not degrade model\nperformance; and (3) that this continues to hold even when ElementaryNet is\nrestricted to a set of level-0 building blocks previously introduced in the\nliterature, with only the functional form being learned by the neural network.",
      "tldr_zh": "这篇论文提出了ElementaryNet，一种专门用于预测非战略性人类行为的神经网络架构，适用于标准形式博弈场景。研究首先指出现有GameNet模型的过度复杂性可能导致其具备战略性推理能力，随后通过理论证明ElementaryNet只能产生非战略性行为。实验结果表明：(1)在基础设置下ElementaryNet表现显著不如GameNet，说明数据集需要战略性推理能力；(2)当引入最佳响应机制后，两者性能无显著差异；(3)即使限制在已知的非战略构建模块集内，ElementaryNet仍保持同等性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages. Submitted to EC 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05925v1",
      "published_date": "2025-03-07 20:47:16 UTC",
      "updated_date": "2025-03-07 20:47:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:50:55.926950"
    },
    {
      "arxiv_id": "2503.05920v1",
      "title": "IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining",
      "title_zh": "IDEA Prune：生成式语言模型预训练中的一体化扩展与剪枝流程",
      "authors": [
        "Yixiao Li",
        "Xianzhi Du",
        "Ajay Jaiswal",
        "Tao Lei",
        "Tuo Zhao",
        "Chong Wang",
        "Jianyu Wang"
      ],
      "abstract": "Recent advancements in large language models have intensified the need for\nefficient and deployable models within limited inference budgets. Structured\npruning pipelines have shown promise in token efficiency compared to training\ntarget-size models from scratch. In this paper, we advocate incorporating\nenlarged model pretraining, which is often ignored in previous works, into\npruning. We study the enlarge-and-prune pipeline as an integrated system to\naddress two critical questions: whether it is worth pretraining an enlarged\nmodel even when the model is never deployed, and how to optimize the entire\npipeline for better pruned models. We propose an integrated enlarge-and-prune\npipeline, which combines enlarge model training, pruning, and recovery under a\nsingle cosine annealing learning rate schedule. This approach is further\ncomplemented by a novel iterative structured pruning method for gradual\nparameter removal. The proposed method helps to mitigate the knowledge loss\ncaused by the rising learning rate in naive enlarge-and-prune pipelines and\nenable effective redistribution of model capacity among surviving neurons,\nfacilitating smooth compression and enhanced performance. We conduct\ncomprehensive experiments on compressing 2.8B models to 1.3B with up to 2T\ntokens in pretraining. It demonstrates the integrated approach not only\nprovides insights into the token efficiency of enlarged model pretraining but\nalso achieves superior performance of pruned models.",
      "tldr_zh": "该研究提出了IDEA Prune，一种集成式\"扩大-修剪\"预训练框架，用于生成式语言模型的高效压缩。该框架创新性地将扩大模型预训练、结构化剪枝和模型恢复整合到单一余弦退火学习率调度中，并采用渐进式迭代剪枝方法缓解传统流程中的知识损失问题。实验表明，在将28亿参数模型压缩至13亿参数的场景中，该方法不仅验证了扩大预训练对最终剪枝模型的必要性，还能实现更优的压缩模型性能，为有限推理预算下的高效模型部署提供了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05920v1",
      "published_date": "2025-03-07 20:35:31 UTC",
      "updated_date": "2025-03-07 20:35:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:50:35.757479"
    },
    {
      "arxiv_id": "2503.05916v1",
      "title": "SAS: Segment Anything Small for Ultrasound -- A Non-Generative Data Augmentation Technique for Robust Deep Learning in Ultrasound Imaging",
      "title_zh": "SAS：面向超声的小目标分割——一种用于超声成像中鲁棒深度学习的非生成式数据增强技术",
      "authors": [
        "Danielle L. Ferreira",
        "Ahana Gangopadhyay",
        "Hsi-Ming Chang",
        "Ravi Soni",
        "Gopal Avinash"
      ],
      "abstract": "Accurate segmentation of anatomical structures in ultrasound (US) images,\nparticularly small ones, is challenging due to noise and variability in imaging\nconditions (e.g., probe position, patient anatomy, tissue characteristics and\npathology). To address this, we introduce Segment Anything Small (SAS), a\nsimple yet effective scale- and texture-aware data augmentation technique\ndesigned to enhance the performance of deep learning models for segmenting\nsmall anatomical structures in ultrasound images. SAS employs a dual\ntransformation strategy: (1) simulating diverse organ scales by resizing and\nembedding organ thumbnails into a black background, and (2) injecting noise\ninto regions of interest to simulate varying tissue textures. These\ntransformations generate realistic and diverse training data without\nintroducing hallucinations or artifacts, improving the model's robustness to\nnoise and variability. We fine-tuned a promptable foundation model on a\ncontrolled organ-specific medical imaging dataset and evaluated its performance\non one internal and five external datasets. Experimental results demonstrate\nsignificant improvements in segmentation performance, with Dice score gains of\nup to 0.35 and an average improvement of 0.16 [95% CI 0.132,0.188].\nAdditionally, our iterative point prompts provide precise control and adaptive\nrefinement, achieving performance comparable to bounding box prompts with just\ntwo points. SAS enhances model robustness and generalizability across diverse\nanatomical structures and imaging conditions, particularly for small\nstructures, without compromising the accuracy of larger ones. By offering a\ncomputationally efficient solution that eliminates the need for extensive human\nlabeling efforts, SAS emerges as a powerful tool for advancing medical image\nanalysis, particularly in resource-constrained settings.",
      "tldr_zh": "该研究提出了一种名为SAS（Segment Anything Small）的新型数据增强技术，专门用于提升超声图像中小解剖结构的深度学习分割性能。该方法采用双转换策略：通过调整器官缩略图尺寸模拟不同器官尺度，并在感兴趣区域注入噪声模拟组织纹理变化，从而生成多样化的训练数据。实验表明，该方法在6个数据集上显著提升了分割效果（Dice分数平均提升0.16），特别是对小结构的分割性能提升尤为明显（最高提升0.35）。该技术还实现了仅需两个点提示即可达到与边界框提示相当的性能，为资源受限环境下的医学图像分析提供了高效解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "25 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05916v1",
      "published_date": "2025-03-07 20:24:35 UTC",
      "updated_date": "2025-03-07 20:24:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:50:51.515967"
    },
    {
      "arxiv_id": "2503.10654v1",
      "title": "Improving RAG Retrieval via Propositional Content Extraction: a Speech Act Theory Approach",
      "title_zh": "基于命题内容提取提升RAG检索：一种言语行为理论方法",
      "authors": [
        "João Alberto de Oliveira Lima"
      ],
      "abstract": "When users formulate queries, they often include not only the information\nthey seek, but also pragmatic markers such as interrogative phrasing or polite\nrequests. Although these speech act indicators communicate the\nuser\\textquotesingle s intent -- whether it is asking a question, making a\nrequest, or stating a fact -- they do not necessarily add to the core\ninformational content of the query itself. This paper investigates whether\nextracting the underlying propositional content from user utterances --\nessentially stripping away the linguistic markers of intent -- can improve\nretrieval quality in Retrieval-Augmented Generation (RAG) systems. Drawing upon\nfoundational insights from speech act theory, we propose a practical method for\nautomatically transforming queries into their propositional equivalents before\nembedding. To assess the efficacy of this approach, we conducted an\nexperimental study involving 63 user queries related to a Brazilian\ntelecommunications news corpus with precomputed semantic embeddings. Results\ndemonstrate clear improvements in semantic similarity between query embeddings\nand document embeddings at top ranks, confirming that queries stripped of\nspeech act indicators more effectively retrieve relevant content.",
      "tldr_zh": "该研究提出了一种基于言语行为理论(Speech Act Theory)的方法，通过提取用户查询中的命题内容(Propositional Content)来改进检索增强生成(RAG)系统的检索质量。该方法自动去除查询中的意图标记(如疑问语气、礼貌请求等)，仅保留核心信息内容，从而提高语义匹配精度。实验表明，在巴西电信新闻语料库上的测试中，经过命题内容提取的查询能显著提升检索结果的相关性，验证了该方法的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.10654v1",
      "published_date": "2025-03-07 20:15:40 UTC",
      "updated_date": "2025-03-07 20:15:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:50:43.238674"
    },
    {
      "arxiv_id": "2503.10653v1",
      "title": "Video Anomaly Detection with Structured Keywords",
      "title_zh": "基于结构化关键词的视频异常检测",
      "authors": [
        "Thomas Foltz"
      ],
      "abstract": "This paper focuses on detecting anomalies in surveillance video using\nkeywords by leveraging foundational models' feature representation\ngeneralization capabilities. We present a novel, lightweight pipeline for\nanomaly classification using keyword weights. Our pipeline employs a two-stage\nprocess: induction followed by deduction. In induction, descriptions are\ngenerated from normal and anomalous frames to identify and assign weights to\nrelevant keywords. In deduction, inference frame descriptions are converted\ninto keyword encodings using induction-derived weights for input into our\nneural network for anomaly classification. We achieved comparable performance\non the three benchmarks UCSD Ped2, Shanghai Tech, and CUHK Avenue, with ROC AUC\nscores of 0.865, 0.745, and 0.742, respectively. These results are achieved\nwithout temporal context, making such a system viable for real-time\napplications. Our model improves implementation setup, interpretability, and\ninference speed for surveillance devices on the edge, introducing a performance\ntrade-off against other video anomaly detection systems. As the generalization\ncapabilities of open-source foundational models improve, our model demonstrates\nthat the exclusive use of text for feature representations is a promising\ndirection for efficient real-time interpretable video anomaly detection.",
      "tldr_zh": "该论文提出了一种基于结构化关键词的轻量级视频异常检测方法。该方法采用两阶段流程（归纳和演绎）：先通过正常/异常帧生成描述并提取关键词权重，再将推理帧描述转换为带权重的关键词编码输入神经网络分类。在UCSD Ped2等三个基准测试中取得0.865等AUC分数，且无需时序上下文即可实现实时检测。研究表明，仅使用文本特征表达结合基础模型的泛化能力，为边缘监控设备提供了高效、可解释的异常检测新方向。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10653v1",
      "published_date": "2025-03-07 20:05:59 UTC",
      "updated_date": "2025-03-07 20:05:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:51:00.020709"
    },
    {
      "arxiv_id": "2503.05899v1",
      "title": "Towards Understanding the Use of MLLM-Enabled Applications for Visual Interpretation by Blind and Low Vision People",
      "title_zh": "理解多模态大语言模型应用在盲人与低视力人群视觉解读中的使用",
      "authors": [
        "Ricardo E. Gonzalez Penuela",
        "Ruiying Hu",
        "Sharon Lin",
        "Tanisha Shende",
        "Shiri Azenkot"
      ],
      "abstract": "Blind and Low Vision (BLV) people have adopted AI-powered visual\ninterpretation applications to address their daily needs. While these\napplications have been helpful, prior work has found that users remain\nunsatisfied by their frequent errors. Recently, multimodal large language\nmodels (MLLMs) have been integrated into visual interpretation applications,\nand they show promise for more descriptive visual interpretations. However, it\nis still unknown how this advancement has changed people's use of these\napplications. To address this gap, we conducted a two-week diary study in which\n20 BLV people used an MLLM-enabled visual interpretation application we\ndeveloped, and we collected 553 entries. In this paper, we report a preliminary\nanalysis of 60 diary entries from 6 participants. We found that participants\nconsidered the application's visual interpretations trustworthy (mean 3.75 out\nof 5) and satisfying (mean 4.15 out of 5). Moreover, participants trusted our\napplication in high-stakes scenarios, such as receiving medical dosage advice.\nWe discuss our plan to complete our analysis to inform the design of future\nMLLM-enabled visual interpretation systems.",
      "tldr_zh": "该研究探讨了多模态大语言模型(MLLM)在盲人和低视力人群(BLV)视觉辅助应用中的使用效果。通过为期两周的日记研究(收集553条记录)发现，参与者对MLLM增强型视觉解释应用的信任度(平均3.75/5)和满意度(平均4.15/5)较高，甚至在高风险场景(如医疗剂量建议)中也表现出信任。研究为未来MLLM视觉辅助系统的设计提供了实践依据。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2.1; H.5.2"
      ],
      "primary_category": "cs.HC",
      "comment": "8 pages, 1 figure, 4 tables, to appear at CHI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05899v1",
      "published_date": "2025-03-07 19:38:14 UTC",
      "updated_date": "2025-03-07 19:38:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:51:31.666649"
    },
    {
      "arxiv_id": "2503.05893v1",
      "title": "Zero-shot Medical Event Prediction Using a Generative Pre-trained Transformer on Electronic Health Records",
      "title_zh": "基于生成式预训练Transformer的电子健康记录零样本医疗事件预测",
      "authors": [
        "Ekaterina Redekop",
        "Zichen Wang",
        "Rushikesh Kulkarni",
        "Mara Pleasure",
        "Aaron Chin",
        "Hamid Reza Hassanzadeh",
        "Brian L. Hill",
        "Melika Emami",
        "William Speier",
        "Corey W. Arnold"
      ],
      "abstract": "Longitudinal data in electronic health records (EHRs) represent an\nindividual`s clinical history through a sequence of codified concepts,\nincluding diagnoses, procedures, medications, and laboratory tests.\nFoundational models, such as generative pre-trained transformers (GPT), can\nleverage this data to predict future events. While fine-tuning of these models\nenhances task-specific performance, it is costly, complex, and unsustainable\nfor every target. We show that a foundation model trained on EHRs can perform\npredictive tasks in a zero-shot manner, eliminating the need for fine-tuning.\n  This study presents the first comprehensive analysis of zero-shot forecasting\nwith GPT-based foundational models in EHRs, introducing a novel pipeline that\nformulates medical concept prediction as a generative modeling task. Unlike\nsupervised approaches requiring extensive labeled data, our method enables the\nmodel to forecast a next medical event purely from a pretraining knowledge. We\nevaluate performance across multiple time horizons and clinical categories,\ndemonstrating model`s ability to capture latent temporal dependencies and\ncomplex patient trajectories without task supervision.\n  Model performance for predicting the next medical concept was evaluated using\nprecision and recall metrics, achieving an average top1 precision of 0.614 and\nrecall of 0.524. For 12 major diagnostic conditions, the model demonstrated\nstrong zero-shot performance, achieving high true positive rates while\nmaintaining low false positives.\n  We demonstrate the power of a foundational EHR GPT model in capturing diverse\nphenotypes and enabling robust, zero-shot forecasting of clinical outcomes.\nThis capability enhances the versatility of predictive healthcare models and\nreduces the need for task-specific training, enabling more scalable\napplications in clinical settings.",
      "tldr_zh": "本研究首次提出了一种基于生成式预训练变换器（GPT）的零样本预测方法，用于电子健康记录（EHR）中的医疗事件预测。该方法将医疗概念预测转化为生成式建模任务，无需微调即可从预训练知识中预测下一个医疗事件。实验表明，该模型能够捕捉潜在的时间依赖性和复杂的患者轨迹，在多个时间跨度和临床类别中表现出色，平均Top1精度为0.614，召回率为0.524。这一方法增强了预测医疗模型的通用性，减少了任务特定训练的需求，为临床环境中的规模化应用提供了可能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05893v1",
      "published_date": "2025-03-07 19:26:47 UTC",
      "updated_date": "2025-03-07 19:26:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:51:32.034809"
    },
    {
      "arxiv_id": "2503.05888v1",
      "title": "QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation",
      "title_zh": "QG-SMS：通过学生建模与仿真增强试题分析",
      "authors": [
        "Bang Nguyen",
        "Tingting Du",
        "Mengxia Yu",
        "Lawrence Angrave",
        "Meng Jiang"
      ],
      "abstract": "While the Question Generation (QG) task has been increasingly adopted in\neducational assessments, its evaluation remains limited by approaches that lack\na clear connection to the educational values of test items. In this work, we\nintroduce test item analysis, a method frequently used by educators to assess\ntest question quality, into QG evaluation. Specifically, we construct pairs of\ncandidate questions that differ in quality across dimensions such as topic\ncoverage, item difficulty, item discrimination, and distractor efficiency. We\nthen examine whether existing QG evaluation approaches can effectively\ndistinguish these differences. Our findings reveal significant shortcomings in\nthese approaches with respect to accurately assessing test item quality in\nrelation to student performance. To address this gap, we propose a novel QG\nevaluation framework, QG-SMS, which leverages Large Language Model for Student\nModeling and Simulation to perform test item analysis. As demonstrated in our\nextensive experiments and human evaluation study, the additional perspectives\nintroduced by the simulated student profiles lead to a more effective and\nrobust assessment of test items.",
      "tldr_zh": "该研究提出了QG-SMS框架，通过学生建模与模拟(Student Modeling and Simulation)来增强测试题目分析。传统问题生成(QG)评估方法难以有效区分题目质量，尤其是在主题覆盖、难度、区分度和干扰项效率等维度。QG-SMS利用大语言模型模拟学生表现，为测试题目分析提供了更有效的评估视角。实验表明，该框架能够更准确地评估题目质量，为教育评估提供了更可靠的工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.05888v1",
      "published_date": "2025-03-07 19:21:59 UTC",
      "updated_date": "2025-03-07 19:21:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:51:39.057220"
    },
    {
      "arxiv_id": "2503.05696v1",
      "title": "Multi-Fidelity Policy Gradient Algorithms",
      "title_zh": "多保真度策略梯度算法",
      "authors": [
        "Xinjie Liu",
        "Cyrus Neary",
        "Kushagra Gupta",
        "Christian Ellis",
        "Ufuk Topcu",
        "David Fridovich-Keil"
      ],
      "abstract": "Many reinforcement learning (RL) algorithms require large amounts of data,\nprohibiting their use in applications where frequent interactions with\noperational systems are infeasible, or high-fidelity simulations are expensive\nor unavailable. Meanwhile, low-fidelity simulators--such as reduced-order\nmodels, heuristic reward functions, or generative world models--can cheaply\nprovide useful data for RL training, even if they are too coarse for direct\nsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL\nframework that mixes a small amount of data from the target environment with a\nlarge volume of low-fidelity simulation data to form unbiased, reduced-variance\nestimators (control variates) for on-policy policy gradients. We instantiate\nthe framework by developing multi-fidelity variants of two policy gradient\nalgorithms: REINFORCE and proximal policy optimization. Experimental results\nacross a suite of simulated robotics benchmark problems demonstrate that when\ntarget-environment samples are limited, MFPG achieves up to 3.9x higher reward\nand improves training stability when compared to baselines that only use\nhigh-fidelity data. Moreover, even when the baselines are given more\nhigh-fidelity samples--up to 10x as many interactions with the target\nenvironment--MFPG continues to match or outperform them. Finally, we observe\nthat MFPG is capable of training effective policies even when the low-fidelity\nenvironment is drastically different from the target environment. MFPG thus not\nonly offers a novel paradigm for efficient sim-to-real transfer but also\nprovides a principled approach to managing the trade-off between policy\nperformance and data collection costs.",
      "tldr_zh": "该研究提出了多保真度策略梯度算法(MFPGs)，旨在解决强化学习(RL)中高保真数据获取困难的问题。通过结合少量目标环境数据和大量低保真模拟数据，MFPGs构建了无偏且低方差的策略梯度估计器。研究开发了基于REINFORCE和近端策略优化(PPO)的多保真度变体，实验表明，在目标环境样本有限的情况下，MFPGs的奖励值最高提升3.9倍，且训练稳定性显著优于仅使用高保真数据的基线方法。此外，即使基线方法获得更多高保真样本，MFPGs仍能匹配或超越其性能。该算法为高效模拟到现实(sim-to-real)迁移提供了新范式，并实现了策略性能与数据收集成本之间的权衡管理。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05696v1",
      "published_date": "2025-03-07 18:58:23 UTC",
      "updated_date": "2025-03-07 18:58:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:51:49.477201"
    },
    {
      "arxiv_id": "2503.05860v1",
      "title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Enhancement Protocol",
      "title_zh": "软件工程中AI模型的基准测试：综述、搜索工具与优化方案",
      "authors": [
        "Roham Koohestani",
        "Philippe de Bekker",
        "Maliheh Izadi"
      ],
      "abstract": "Benchmarks are essential for consistent evaluation and reproducibility. The\nintegration of Artificial Intelligence into Software Engineering (AI4SE) has\ngiven rise to numerous benchmarks for tasks such as code generation and bug\nfixing. However, this surge presents challenges: (1) scattered benchmark\nknowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3)\nthe absence of a uniform standard for benchmark development, and (4)\nlimitations of existing benchmarks. In this paper, we review 173 studies and\nidentify 204 AI4SE benchmarks. We classify these benchmarks, analyze their\nlimitations, and expose gaps in practices. Based on our review, we created\nBenchScout, a semantic search tool to find relevant benchmarks, using automated\nclustering of the contexts from associated studies. We conducted a user study\nwith 22 participants to evaluate BenchScout's usability, effectiveness, and\nintuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5.\nTo advance benchmarking standards, we propose BenchFrame, a unified method to\nenhance benchmark quality. As a case study, we applied BenchFrame to the\nHumanEval benchmark and addressed its main limitations. This led to\nHumanEvalNext, featuring (1) corrected errors, (2) improved language\nconversion, (3) expanded test coverage, and (4) increased difficulty. We then\nevaluated ten state-of-the-art code language models on HumanEval,\nHumanEvalPlus, and HumanEvalNext. On HumanEvalNext, models showed a pass@1\nscore reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus,\nrespectively.",
      "tldr_zh": "这篇论文系统评估了AI在软件工程(AI4SE)领域的基准测试现状，分析了204个相关基准测试并揭示四大挑战。研究者开发了BenchScout语义搜索工具（用户评分4.3/5），并提出了BenchFrame标准化框架来提升基准质量。通过改进HumanEval基准创建HumanEvalNext，使主流代码模型的pass@1准确率相比原版下降31.22%，有效验证了该框架提升基准严格性的能力。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05860v1",
      "published_date": "2025-03-07 18:44:32 UTC",
      "updated_date": "2025-03-07 18:44:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:52:03.087700"
    },
    {
      "arxiv_id": "2503.05859v1",
      "title": "Quantum-like cognition and decision making in the light of quantum measurement theory",
      "title_zh": "量子测量理论视角下的类量子认知与决策",
      "authors": [
        "Miho Fuyama",
        "Andrei Khrennikov",
        "Masanao Ozawa"
      ],
      "abstract": "We characterize the class of quantum measurements that matches the\napplications of quantum theory to cognition (and decision making) -\nquantum-like modeling. Projective measurements describe the canonical\nmeasurements of the basic observables of quantum physics. However, the\ncombinations of the basic cognitive effects, such as the question order and\nresponse replicability effects, cannot be described by projective measurements.\nWe motivate the use of the special class of quantum measurements, namely {\\it\nsharp repeatable non-projective measurements} - ${\\cal SR\\bar{P}}. $ This class\nis practically unused in quantum physics. Thus, physics and cognition explore\ndifferent parts of quantum measurement theory. Quantum-like modeling isn't\nautomatic borrowing of the quantum formalism. Exploring the class ${\\cal\nSR\\bar{P}}$ highlights the role of {\\it noncommutativity of the state update\nmaps generated by measurement back action.} Thus, ``non-classicality'' in\nquantum physics as well as quantum-like modeling for cognition is based on two\ndifferent types of noncommutativity, of operators (observables) and instruments\n(state update maps): {\\it observable-noncommutativity} vs. {\\it state\nupdate-noncommutativity}. We speculate that distinguishing quantum-like\nproperties of the cognitive effects are the expressions of the latter, or\npossibly both.",
      "tldr_zh": "本文探讨了量子测量理论在认知和决策领域的应用，提出了量子类模型。研究发现，投影测量虽适用于量子物理学中的基本观测，但无法描述认知效应（如问题顺序效应和响应可重复性效应）。因此，研究引入了“尖锐可重复非投影测量”（${\\cal SR\\bar{P}}$），这一类别在量子物理学中极少使用。研究表明，量子物理学和认知科学分别探索了量子测量理论的不同部分，量子类建模并非简单借用量子形式化。研究还强调了“状态更新映射的非交换性”在量子类建模中的重要性，指出认知效应的非经典性可能源于状态更新映射或观测算符的非交换性。",
      "categories": [
        "cs.AI",
        "physics.bio-ph",
        "quant-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05859v1",
      "published_date": "2025-03-07 18:30:44 UTC",
      "updated_date": "2025-03-07 18:30:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:52:16.306136"
    },
    {
      "arxiv_id": "2503.05652v1",
      "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
      "title_zh": "BEHAVIOR机器人套件：简化现实世界全身操控以实现日常家务活动",
      "authors": [
        "Yunfan Jiang",
        "Ruohan Zhang",
        "Josiah Wong",
        "Chen Wang",
        "Yanjie Ze",
        "Hang Yin",
        "Cem Gokmen",
        "Shuran Song",
        "Jiajun Wu",
        "Li Fei-Fei"
      ],
      "abstract": "Real-world household tasks present significant challenges for mobile\nmanipulation robots. An analysis of existing robotics benchmarks reveals that\nsuccessful task performance hinges on three key whole-body control\ncapabilities: bimanual coordination, stable and precise navigation, and\nextensive end-effector reachability. Achieving these capabilities requires\ncareful hardware design, but the resulting system complexity further\ncomplicates visuomotor policy learning. To address these challenges, we\nintroduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for\nwhole-body manipulation in diverse household tasks. Built on a bimanual,\nwheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body\nteleoperation interface for data collection and a novel algorithm for learning\nwhole-body visuomotor policies. We evaluate BRS on five challenging household\ntasks that not only emphasize the three core capabilities but also introduce\nadditional complexities, such as long-range navigation, interaction with\narticulated and deformable objects, and manipulation in confined spaces. We\nbelieve that BRS's integrated robotic embodiment, data collection interface,\nand learning framework mark a significant step toward enabling real-world\nwhole-body manipulation for everyday household tasks. BRS is open-sourced at\nhttps://behavior-robot-suite.github.io/",
      "tldr_zh": "该研究提出了BEHAVIOR Robot Suite (BRS)，一个专注于家庭环境中全身操作的综合机器人框架。BRS基于双手机器人设计，结合了低成本全身遥操作接口和新型视觉运动策略学习算法，旨在解决双手机械协调、稳定导航和末端执行器可达性等关键问题。通过在复杂家庭任务（如长距离导航、与可变形物体交互和狭小空间操作）上的测试，BRS展示了其在现实场景中的高效性和实用性，为家庭机器人技术的发展提供了重要支持。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://behavior-robot-suite.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.05652v1",
      "published_date": "2025-03-07 18:15:21 UTC",
      "updated_date": "2025-03-07 18:15:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:52:16.045419"
    },
    {
      "arxiv_id": "2503.05646v1",
      "title": "dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale",
      "title_zh": "dARt Vinci：用于大规模手术机器人学习的自我中心数据采集",
      "authors": [
        "Yihao Liu",
        "Yu-Chun Ku",
        "Jiaming Zhang",
        "Hao Ding",
        "Peter Kazanzides",
        "Mehran Armand"
      ],
      "abstract": "Data scarcity has long been an issue in the robot learning community.\nParticularly, in safety-critical domains like surgical applications, obtaining\nhigh-quality data can be especially difficult. It poses challenges to\nresearchers seeking to exploit recent advancements in reinforcement learning\nand imitation learning, which have greatly improved generalizability and\nenabled robots to conduct tasks autonomously. We introduce dARt Vinci, a\nscalable data collection platform for robot learning in surgical settings. The\nsystem uses Augmented Reality (AR) hand tracking and a high-fidelity physics\nengine to capture subtle maneuvers in primitive surgical tasks: By eliminating\nthe need for a physical robot setup and providing flexibility in terms of time,\nspace, and hardware resources-such as multiview sensors and\nactuators-specialized simulation is a viable alternative. At the same time, AR\nallows the robot data collection to be more egocentric, supported by its body\ntracking and content overlaying capabilities. Our user study confirms the\nproposed system's efficiency and usability, where we use widely-used primitive\ntasks for training teleoperation with da Vinci surgical robots. Data throughput\nimproves across all tasks compared to real robot settings by 41% on average.\nThe total experiment time is reduced by an average of 10%. The temporal demand\nin the task load survey is improved. These gains are statistically significant.\nAdditionally, the collected data is over 400 times smaller in size, requiring\nfar less storage while achieving double the frequency.",
      "tldr_zh": "该研究提出了dARt Vinci平台，利用增强现实(AR)手部追踪和高保真物理引擎，实现手术机器人学习的大规模数据采集。该系统通过虚拟化方案解决了传统手术机器人数据采集的时空限制问题，相比实体机器人设置平均提升41%的数据吞吐量，并减少10%的实验时间。平台特有的自我中心(egocentric)数据采集方式，结合AR体感追踪和内容叠加功能，使采集数据量缩小400倍的同时实现双倍采样频率，为强化学习和模仿学习提供了高质量的手术训练数据集。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05646v1",
      "published_date": "2025-03-07 18:07:54 UTC",
      "updated_date": "2025-03-07 18:07:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:52:21.709390"
    },
    {
      "arxiv_id": "2503.05641v2",
      "title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning",
      "title_zh": "符号专家混合体：面向异构推理的自适应技能路由框架",
      "authors": [
        "Justin Chih-Yao Chen",
        "Sukwon Yun",
        "Elias Stengel-Eskin",
        "Tianlong Chen",
        "Mohit Bansal"
      ],
      "abstract": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting experts at the task\nlevel is often too coarse-grained, as heterogeneous tasks may require different\nexpertise for each instance. To enable adaptive instance-level mixing of\npre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and\ngradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained\napproach to selection by emphasizing skills, e.g., algebra in math or molecular\nbiology in biomedical reasoning. We propose a skill-based recruiting strategy\nthat dynamically selects the most relevant set of expert LLMs for diverse\nreasoning tasks based on their strengths. Each selected expert then generates\nits own reasoning, resulting in k outputs from k experts, which are then\nsynthesized into a final high-quality response by an aggregator chosen based on\nits ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch inference strategy that groups instances based on their\nassigned experts, loading each model only once. This allows us to integrate 16\nexpert models on 1 GPU with a time cost comparable to or better than prior\nmulti-agent baselines using 4 GPUs. Through extensive evaluations on diverse\nbenchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that\nSymbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent\napproaches, with an absolute average improvement of 8.15% over the best\nmulti-agent baseline. Moreover, Symbolic-MoE removes the need for expensive\nmulti-round discussions, outperforming discussion baselines with less\ncomputation.",
      "tldr_zh": "该研究提出Symbolic-MoE框架，一种基于技能的自适应专家混合方法，通过细粒度的实例级专家选择机制动态组合预训练大语言模型(LLMs)。该方法创新性地采用基于技能的招募策略（如代数运算或分子生物学等专业领域），从16个专家模型中动态筛选最优组合，并通过聚合器整合各专家的推理结果。为解决计算开销问题，研究者开发了批处理推理策略，在单GPU上实现了多专家并行计算，效率超越传统4GPU多智能体方案。在MMLU-Pro等多项基准测试中，Symbolic-MoE以8.15%的绝对优势超越GPT4o-mini等基线模型，且无需昂贵的多轮讨论机制。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The first three authors contributed equally. Project Page:\n  https://symbolic-moe.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.05641v2",
      "published_date": "2025-03-07 18:03:13 UTC",
      "updated_date": "2025-03-11 21:40:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:52:30.524320"
    },
    {
      "arxiv_id": "2503.05639v2",
      "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
      "title_zh": "VideoPainter：基于即插即用上下文控制的任意长度视频修复与编辑",
      "authors": [
        "Yuxuan Bian",
        "Zhaoyang Zhang",
        "Xuan Ju",
        "Mingdeng Cao",
        "Liangbin Xie",
        "Ying Shan",
        "Qiang Xu"
      ],
      "abstract": "Video inpainting, which aims to restore corrupted video content, has\nexperienced substantial progress. Despite these advances, existing methods,\nwhether propagating unmasked region pixels through optical flow and receptive\nfield priors, or extending image-inpainting models temporally, face challenges\nin generating fully masked objects or balancing the competing objectives of\nbackground context preservation and foreground generation in one model,\nrespectively. To address these limitations, we propose a novel dual-stream\nparadigm VideoPainter that incorporates an efficient context encoder\n(comprising only 6% of the backbone parameters) to process masked videos and\ninject backbone-aware background contextual cues to any pre-trained video DiT,\nproducing semantically consistent content in a plug-and-play manner. This\narchitectural separation significantly reduces the model's learning complexity\nwhile enabling nuanced integration of crucial background context. We also\nintroduce a novel target region ID resampling technique that enables any-length\nvideo inpainting, greatly enhancing our practical applicability. Additionally,\nwe establish a scalable dataset pipeline leveraging current vision\nunderstanding models, contributing VPData and VPBench to facilitate\nsegmentation-based inpainting training and assessment, the largest video\ninpainting dataset and benchmark to date with over 390K diverse clips. Using\ninpainting as a pipeline basis, we also explore downstream applications\nincluding video editing and video editing pair data generation, demonstrating\ncompetitive performance and significant practical potential. Extensive\nexperiments demonstrate VideoPainter's superior performance in both any-length\nvideo inpainting and editing, across eight key metrics, including video\nquality, mask region preservation, and textual coherence.",
      "tldr_zh": "本研究提出了VideoPainter，一种创新的双流范式视频修复和编辑方法，通过引入高效的上下文编码器（仅占主干网络参数的6%），以即插即用的方式将背景上下文信息注入预训练的视频DiT模型，生成语义一致的内容。该方法采用目标区域ID重采样技术，实现了任意长度视频修复，并构建了包含超过39万段多样化视频的VPData和VPBench数据集，为基于分割的修复训练和评估提供了支持。实验表明，VideoPainter在视频质量、掩码区域保持和文本一致性等八个关键指标上均表现出色，展现了其在视频修复和编辑方面的显著实用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page available at\n  https://yxbian23.github.io/project/video-painter",
      "pdf_url": "http://arxiv.org/pdf/2503.05639v2",
      "published_date": "2025-03-07 17:59:46 UTC",
      "updated_date": "2025-03-10 18:56:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:52:37.391222"
    },
    {
      "arxiv_id": "2503.05638v1",
      "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
      "title_zh": "TrajectoryCrafter：基于扩散模型的单目视频相机轨迹重定向",
      "authors": [
        "Mark YU",
        "Wenbo Hu",
        "Jinbo Xing",
        "Ying Shan"
      ],
      "abstract": "We present TrajectoryCrafter, a novel approach to redirect camera\ntrajectories for monocular videos. By disentangling deterministic view\ntransformations from stochastic content generation, our method achieves precise\ncontrol over user-specified camera trajectories. We propose a novel dual-stream\nconditional video diffusion model that concurrently integrates point cloud\nrenders and source videos as conditions, ensuring accurate view transformations\nand coherent 4D content generation. Instead of leveraging scarce multi-view\nvideos, we curate a hybrid training dataset combining web-scale monocular\nvideos with static multi-view datasets, by our innovative double-reprojection\nstrategy, significantly fostering robust generalization across diverse scenes.\nExtensive evaluations on multi-view and large-scale monocular videos\ndemonstrate the superior performance of our method.",
      "tldr_zh": "该研究提出TrajectoryCrafter，一种基于扩散模型的新型单目视频相机轨迹重定向方法。通过将确定性视角变换与随机内容生成解耦，该方法实现了对用户指定相机轨迹的精确控制，并创新性地采用双流条件视频扩散模型，同时整合点云渲染和源视频作为条件输入。研究者还开发了双重投影策略构建混合训练数据集，结合单目视频和静态多视角数据，显著提升了模型在多样化场景中的泛化能力。实验证明该方法在多视角和大规模单目视频上均表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project webpage: https://trajectorycrafter.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.05638v1",
      "published_date": "2025-03-07 17:57:53 UTC",
      "updated_date": "2025-03-07 17:57:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:52:53.159555"
    },
    {
      "arxiv_id": "2503.05629v1",
      "title": "Exploring FMCW Radars and Feature Maps for Activity Recognition: A Benchmark Study",
      "title_zh": "探索FMCW雷达与特征图在活动识别中的应用：一项基准研究",
      "authors": [
        "Ali Samimi Fard",
        "Mohammadreza Mashhadigholamali",
        "Samaneh Zolfaghari",
        "Hajar Abedi",
        "Mainak Chakraborty",
        "Luigi Borzì",
        "Masoud Daneshtalab",
        "George Shaker"
      ],
      "abstract": "Human Activity Recognition has gained significant attention due to its\ndiverse applications, including ambient assisted living and remote sensing.\nWearable sensor-based solutions often suffer from user discomfort and\nreliability issues, while video-based methods raise privacy concerns and\nperform poorly in low-light conditions or long ranges. This study introduces a\nFrequency-Modulated Continuous Wave radar-based framework for human activity\nrecognition, leveraging a 60 GHz radar and multi-dimensional feature maps.\nUnlike conventional approaches that process feature maps as images, this study\nfeeds multi-dimensional feature maps -- Range-Doppler, Range-Azimuth, and\nRange-Elevation -- as data vectors directly into the machine learning (SVM,\nMLP) and deep learning (CNN, LSTM, ConvLSTM) models, preserving the spatial and\ntemporal structures of the data. These features were extracted from a novel\ndataset with seven activity classes and validated using two different\nvalidation approaches. The ConvLSTM model outperformed conventional machine\nlearning and deep learning models, achieving an accuracy of 90.51% and an\nF1-score of 87.31% on cross-scene validation and an accuracy of 89.56% and an\nF1-score of 87.15% on leave-one-person-out cross-validation. The results\nhighlight the approach's potential for scalable, non-intrusive, and\nprivacy-preserving activity monitoring in real-world scenarios.",
      "tldr_zh": "本研究提出了一种基于调频连续波（FMCW）雷达和特征图的人类活动识别框架，解决了可穿戴设备不适和视频隐私问题。通过使用60 GHz雷达和多维特征图（如Range-Doppler、Range-Azimuth和Range-Elevation）作为数据向量，直接输入机器学习和深度学习模型，保留了数据的时空结构。实验表明，ConvLSTM模型在跨场景和留一交叉验证中表现最佳，准确率分别达到90.51%和89.56%，证明了该方法在非侵入式、隐私保护的活动监测中的潜力。",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05629v1",
      "published_date": "2025-03-07 17:53:29 UTC",
      "updated_date": "2025-03-07 17:53:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:53:08.017328"
    },
    {
      "arxiv_id": "2503.05628v1",
      "title": "Superintelligence Strategy: Expert Version",
      "title_zh": "超级智能战略：专家版",
      "authors": [
        "Dan Hendrycks",
        "Eric Schmidt",
        "Alexandr Wang"
      ],
      "abstract": "Rapid advances in AI are beginning to reshape national security.\nDestabilizing AI developments could rupture the balance of power and raise the\nodds of great-power conflict, while widespread proliferation of capable AI\nhackers and virologists would lower barriers for rogue actors to cause\ncatastrophe. Superintelligence -- AI vastly better than humans at nearly all\ncognitive tasks -- is now anticipated by AI researchers. Just as nations once\ndeveloped nuclear strategies to secure their survival, we now need a coherent\nsuperintelligence strategy to navigate a new period of transformative change.\nWe introduce the concept of Mutual Assured AI Malfunction (MAIM): a deterrence\nregime resembling nuclear mutual assured destruction (MAD) where any state's\naggressive bid for unilateral AI dominance is met with preventive sabotage by\nrivals. Given the relative ease of sabotaging a destabilizing AI project --\nthrough interventions ranging from covert cyberattacks to potential kinetic\nstrikes on datacenters -- MAIM already describes the strategic picture AI\nsuperpowers find themselves in. Alongside this, states can increase their\ncompetitiveness by bolstering their economies and militaries through AI, and\nthey can engage in nonproliferation to rogue actors to keep weaponizable AI\ncapabilities out of their hands. Taken together, the three-part framework of\ndeterrence, nonproliferation, and competitiveness outlines a robust strategy to\nsuperintelligence in the years ahead.",
      "tldr_zh": "该研究提出了\"超级智能战略\"框架，针对AI快速发展对国家安全的潜在威胁。核心贡献是引入\"相互确保AI故障\"(MAIM)威慑机制，类似于核威慑中的相互确保摧毁(MAD)原则，通过预防性破坏来阻止任何国家单方面追求AI霸权。研究构建了包含威慑、防扩散和竞争力提升的三支柱战略体系，为大国在AI超级智能时代的竞争与安全提供系统性解决方案。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "https://nationalsecurity.ai/",
      "pdf_url": "http://arxiv.org/pdf/2503.05628v1",
      "published_date": "2025-03-07 17:53:24 UTC",
      "updated_date": "2025-03-07 17:53:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:53:20.631041"
    },
    {
      "arxiv_id": "2503.05626v1",
      "title": "FMT:A Multimodal Pneumonia Detection Model Based on Stacking MOE Framework",
      "title_zh": "FMT：基于堆叠混合专家框架的多模态肺炎检测模型",
      "authors": [
        "Jingyu Xu",
        "Yang Wang"
      ],
      "abstract": "Artificial intelligence has shown the potential to improve diagnostic\naccuracy through medical image analysis for pneumonia diagnosis. However,\ntraditional multimodal approaches often fail to address real-world challenges\nsuch as incomplete data and modality loss. In this study, a Flexible Multimodal\nTransformer (FMT) was proposed, which uses ResNet-50 and BERT for joint\nrepresentation learning, followed by a dynamic masked attention strategy that\nsimulates clinical modality loss to improve robustness; finally, a sequential\nmixture of experts (MOE) architecture was used to achieve multi-level decision\nrefinement. After evaluation on a small multimodal pneumonia dataset, FMT\nachieved state-of-the-art performance with 94% accuracy, 95% recall, and 93% F1\nscore, outperforming single-modal baselines (ResNet: 89%; BERT: 79%) and the\nmedical benchmark CheXMed (90%), providing a scalable solution for multimodal\ndiagnosis of pneumonia in resource-constrained medical settings.",
      "tldr_zh": "本研究提出了一种基于混合专家(MOE)框架的多模态肺炎检测模型FMT，通过结合ResNet-50和BERT进行联合表征学习，并采用动态掩码注意力策略模拟临床模态缺失以增强鲁棒性。该模型采用序列化混合专家架构实现多层次决策优化，在小规模多模态肺炎数据集上取得了94%准确率、95%召回率和93% F1值的优异表现，显著优于单模态基准模型(ResNet:89%；BERT:79%)和医疗基准CheXMed(90%)，为资源受限医疗环境中的多模态肺炎诊断提供了可扩展解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05626v1",
      "published_date": "2025-03-07 17:52:12 UTC",
      "updated_date": "2025-03-07 17:52:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:53:17.503225"
    },
    {
      "arxiv_id": "2503.05620v1",
      "title": "Learning LLM Preference over Intra-Dialogue Pairs: A Framework for Utterance-level Understandings",
      "title_zh": "学习大型语言模型对对话内语句对的偏好：一种面向话语级理解的框架",
      "authors": [
        "Xuanqing Liu",
        "Luyang Kong",
        "Wei Niu",
        "Afshin Khashei",
        "Belinda Zeng",
        "Steve Johnson",
        "Jon Jay",
        "Davor Golac",
        "Matt Pope"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nhandling complex dialogue tasks without requiring use case-specific\nfine-tuning. However, analyzing live dialogues in real-time necessitates\nlow-latency processing systems, making it impractical to deploy models with\nbillions of parameters due to latency constraints. As a result, practitioners\noften prefer smaller models with millions of parameters, trained on\nhigh-quality, human-annotated datasets. Yet, curating such datasets is both\ntime-consuming and costly. Consequently, there is a growing need to combine the\nscalability of LLM-generated labels with the precision of human annotations,\nenabling fine-tuned smaller models to achieve both higher speed and accuracy\ncomparable to larger models. In this paper, we introduce a simple yet effective\nframework to address this challenge. Our approach is specifically designed for\nper-utterance classification problems, which encompass tasks such as intent\ndetection, dialogue state tracking, and more. To mitigate the impact of\nlabeling errors from LLMs -- the primary source of inaccuracies in student\nmodels -- we propose a noise-reduced preference learning loss. Experimental\nresults demonstrate that our method significantly improves accuracy across\nutterance-level dialogue tasks, including sentiment detection (over $2\\%$),\ndialogue act classification (over $1.5\\%$), etc.",
      "tldr_zh": "该研究提出了一种简单有效的框架，用于解决对话系统中大语言模型(LLMs)与小型模型间的效率-精度权衡问题。通过设计针对语句级分类任务(如意图检测、对话状态跟踪等)的噪声抑制偏好学习损失函数，该方法有效减少了LLM生成标签的误差影响。实验表明，该框架在情感检测(提升2%+)和对话行为分类(提升1.5%+)等任务上显著提高了小型模型的准确性，使其在保持低延迟的同时达到接近大模型的性能水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05620v1",
      "published_date": "2025-03-07 17:46:13 UTC",
      "updated_date": "2025-03-07 17:46:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:54:05.337642"
    },
    {
      "arxiv_id": "2503.05613v1",
      "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models",
      "title_zh": "稀疏自编码器综述：解析大型语言模型的内部机制",
      "authors": [
        "Dong Shu",
        "Xuansheng Wu",
        "Haiyan Zhao",
        "Daking Rai",
        "Ziyu Yao",
        "Ninghao Liu",
        "Mengnan Du"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a particularly promising method due to their ability to disentangle\nthe complex, superimposed features within LLMs into more interpretable\ncomponents. This paper presents a comprehensive examination of SAEs as a\npromising approach to interpreting and understanding LLMs. We provide a\nsystematic overview of SAE principles, architectures, and applications\nspecifically tailored for LLM analysis, covering theoretical foundations,\nimplementation strategies, and recent developments in sparsity mechanisms. We\nalso explore how SAEs can be leveraged to explain the internal workings of\nLLMs, steer model behaviors in desired directions, and develop more transparent\ntraining methodologies for future models. Despite the challenges that remain\naround SAE implementation and scaling, they continue to provide valuable tools\nfor understanding the internal mechanisms of large language models.",
      "tldr_zh": "本文全面综述了稀疏自编码器（Sparse Autoencoders, SAEs）在解释大型语言模型（LLMs）内部机制中的应用。SAEs通过将LLM中复杂叠加的特征解耦为更可解释的组件，成为机制解释性研究中的一种重要方法。文章系统梳理了SAEs的理论基础、架构设计及其在LLM分析中的具体应用，并探讨了如何利用SAEs解释LLM的内部机制、引导模型行为以及开发更透明的训练方法。尽管SAEs在实现和扩展方面仍面临挑战，但其为理解LLMs的内部机制提供了重要工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05613v1",
      "published_date": "2025-03-07 17:38:00 UTC",
      "updated_date": "2025-03-07 17:38:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:53:32.484516"
    },
    {
      "arxiv_id": "2503.05604v1",
      "title": "CACTUS: An Open Dataset and Framework for Automated Cardiac Assessment and Classification of Ultrasound Images Using Deep Transfer Learning",
      "title_zh": "CACTUS：基于深度迁移学习的心脏超声图像自动评估与分类的开放数据集及框架",
      "authors": [
        "Hanae Elmekki",
        "Ahmed Alagha",
        "Hani Sami",
        "Amanda Spilkin",
        "Antonela Mariel Zanuttini",
        "Ehsan Zakeri",
        "Jamal Bentahar",
        "Lyes Kadem",
        "Wen-Fang Xie",
        "Philippe Pibarot",
        "Rabeb Mizouni",
        "Hadi Otrok",
        "Shakti Singh",
        "Azzam Mourad"
      ],
      "abstract": "Cardiac ultrasound (US) scanning is a commonly used techniques in cardiology\nto diagnose the health of the heart and its proper functioning. Therefore, it\nis necessary to consider ways to automate these tasks and assist medical\nprofessionals in classifying and assessing cardiac US images. Machine learning\n(ML) techniques are regarded as a prominent solution due to their success in\nnumerous applications aimed at enhancing the medical field, including\naddressing the shortage of echography technicians. However, the limited\navailability of medical data presents a significant barrier to applying ML in\ncardiology, particularly regarding US images of the heart. This paper addresses\nthis challenge by introducing the first open graded dataset for Cardiac\nAssessment and ClassificaTion of UltraSound (CACTUS), which is available\nonline. This dataset contains images obtained from scanning a CAE Blue Phantom\nand representing various heart views and different quality levels, exceeding\nthe conventional cardiac views typically found in the literature. Additionally,\nthe paper introduces a Deep Learning (DL) framework consisting of two main\ncomponents. The first component classifies cardiac US images based on the heart\nview using a Convolutional Neural Network (CNN). The second component uses\nTransfer Learning (TL) to fine-tune the knowledge from the first component and\ncreate a model for grading and assessing cardiac images. The framework\ndemonstrates high performance in both classification and grading, achieving up\nto 99.43% accuracy and as low as 0.3067 error, respectively. To showcase its\nrobustness, the framework is further fine-tuned using new images representing\nadditional cardiac views and compared to several other state-of-the-art\narchitectures. The framework's outcomes and performance in handling real-time\nscans were also assessed using a questionnaire answered by cardiac experts.",
      "tldr_zh": "该研究提出了CACTUS，一个用于心脏超声图像自动评估与分类的开源数据集和深度学习框架。CACTUS数据集包含多种心脏视图和不同质量级别的图像，填补了医学数据稀缺的空白。研究还开发了一个基于卷积神经网络(CNN)和迁移学习(TL)的深度学习框架，能够高效分类和评估心脏超声图像，分类准确率达99.43%，评估误差低至0.3067。该框架经过进一步微调，展示了其处理实时扫描的鲁棒性，并通过心脏专家的问卷验证了其实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05604v1",
      "published_date": "2025-03-07 17:29:04 UTC",
      "updated_date": "2025-03-07 17:29:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:53:43.761834"
    },
    {
      "arxiv_id": "2503.05592v2",
      "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning",
      "title_zh": "R1-Searcher：通过强化学习激励大型语言模型的搜索能力",
      "authors": [
        "Huatong Song",
        "Jinhao Jiang",
        "Yingqian Min",
        "Jie Chen",
        "Zhipeng Chen",
        "Wayne Xin Zhao",
        "Lei Fang",
        "Ji-Rong Wen"
      ],
      "abstract": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
      "tldr_zh": "这篇论文提出了R1-Searcher，一种基于强化学习(RL)的两阶段框架，旨在提升大语言模型(LLMs)的自主搜索能力。该方法通过RL训练使LLMs能在推理过程中自动调用外部搜索系统获取额外知识，无需过程奖励或蒸馏启动。实验表明，R1-Searcher在知识密集型任务上显著优于现有RAG方法，甚至超越闭源的GPT-4o-mini模型，有效解决了LLMs依赖内部知识导致的时效性和准确性不足问题。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05592v2",
      "published_date": "2025-03-07 17:14:44 UTC",
      "updated_date": "2025-03-18 08:32:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:53:57.165882"
    },
    {
      "arxiv_id": "2503.05587v1",
      "title": "Quantifying the Robustness of Retrieval-Augmented Language Models Against Spurious Features in Grounding Data",
      "title_zh": "量化检索增强语言模型对基础数据中伪特征的鲁棒性",
      "authors": [
        "Shiping Yang",
        "Jie Wu",
        "Wenbiao Ding",
        "Ning Wu",
        "Shining Liang",
        "Ming Gong",
        "Hengyuan Zhang",
        "Dongmei Zhang"
      ],
      "abstract": "Robustness has become a critical attribute for the deployment of RAG systems\nin real-world applications. Existing research focuses on robustness to explicit\nnoise (e.g., document semantics) but overlooks spurious features (a.k.a.\nimplicit noise). While previous works have explored spurious features in LLMs,\nthey are limited to specific features (e.g., formats) and narrow scenarios\n(e.g., ICL). In this work, we statistically confirm the presence of spurious\nfeatures in the RAG paradigm, a robustness problem caused by the sensitivity of\nLLMs to semantic-agnostic features. Moreover, we provide a comprehensive\ntaxonomy of spurious features and empirically quantify their impact through\ncontrolled experiments. Further analysis reveals that not all spurious features\nare harmful and they can even be beneficial sometimes. Extensive evaluation\nresults across multiple LLMs suggest that spurious features are a widespread\nand challenging problem in the field of RAG. The code and dataset will be\nreleased to facilitate future research. We release all codes and data at:\n$\\\\\\href{https://github.com/maybenotime/RAG-SpuriousFeatures}{https://github.com/maybenotime/RAG-SpuriousFeatures}$.",
      "tldr_zh": "该研究首次系统量化了检索增强生成模型(RAG)对数据中隐含噪声（即伪特征）的鲁棒性问题。研究通过统计分析确认了伪特征在RAG范式中的普遍存在，并提出了一套全面的伪特征分类体系，通过控制实验量化了其影响。研究发现，并非所有伪特征都有害，某些情况下甚至可能有益。实验结果表明，伪特征是RAG领域普遍存在且具有挑战性的问题，为未来研究提供了代码和数据集支持。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05587v1",
      "published_date": "2025-03-07 17:11:34 UTC",
      "updated_date": "2025-03-07 17:11:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:54:20.422228"
    },
    {
      "arxiv_id": "2503.05857v1",
      "title": "SYMBIOSIS: Systems Thinking and Machine Intelligence for Better Outcomes in Society",
      "title_zh": "SYMBIOSIS：系统思维与机器智能协同促进社会福祉",
      "authors": [
        "Sameer Sethi",
        "Donald Martin Jr.",
        "Emmanuel Klu"
      ],
      "abstract": "This paper presents SYMBIOSIS, an AI-powered framework and platform designed\nto make Systems Thinking accessible for addressing societal challenges and\nunlock paths for leveraging systems thinking frameworks to improve AI systems.\nThe platform establishes a centralized, open-source repository of systems\nthinking/system dynamics models categorized by Sustainable Development Goals\n(SDGs) and societal topics using topic modeling and classification techniques.\nSystems Thinking resources, though critical for articulating causal theories in\ncomplex problem spaces, are often locked behind specialized tools and intricate\nnotations, creating high barriers to entry. To address this, we developed a\ngenerative co-pilot that translates complex systems representations - such as\ncausal loop and stock-flow diagrams - into natural language (and vice-versa),\nallowing users to explore and build models without extensive technical\ntraining.\n  Rooted in community-based system dynamics (CBSD) and informed by\ncommunity-driven insights on societal context, we aim to bridge the problem\nunderstanding chasm. This gap, driven by epistemic uncertainty, often limits ML\ndevelopers who lack the community-specific knowledge essential for problem\nunderstanding and formulation, often leading to ill informed causal\nassumptions, reduced intervention effectiveness and harmful biases. Recent\nresearch identifies causal and abductive reasoning as crucial frontiers for AI,\nand Systems Thinking provides a naturally compatible framework for both. By\nmaking Systems Thinking frameworks more accessible and user-friendly, SYMBIOSIS\naims to serve as a foundational step to unlock future research into responsible\nand society-centered AI. Our work underscores the need for ongoing research\ninto AI's capacity to understand essential characteristics of complex adaptive\nsystems paving the way for more socially attuned, effective AI systems.",
      "tldr_zh": "该研究提出了SYMBIOSIS，一个AI驱动的平台，旨在通过系统思维（Systems Thinking）解决社会挑战并优化AI系统。平台构建了一个基于可持续发展目标（SDGs）的开源系统动力学模型库，并通过生成式AI助手将复杂的因果循环图和存量流量图转化为自然语言，降低使用门槛。研究强调系统思维在因果推理和问题理解中的重要性，帮助开发者避免错误假设和偏见，推动更具社会责任感的AI研究。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05857v1",
      "published_date": "2025-03-07 17:07:26 UTC",
      "updated_date": "2025-03-07 17:07:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:54:12.425641"
    },
    {
      "arxiv_id": "2503.05573v1",
      "title": "InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model",
      "title_zh": "InDRiVE：基于内在分歧强化的车辆探索方法——通过好奇心驱动的广义世界模型",
      "authors": [
        "Feeza Khan Khanzada",
        "Jaerock Kwon"
      ],
      "abstract": "Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm\nfor autonomous driving, where data efficiency and robustness are critical. Yet,\nexisting solutions often rely on carefully crafted, task specific extrinsic\nrewards, limiting generalization to new tasks or environments. In this paper,\nwe propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle\nExploration), a method that leverages purely intrinsic, disagreement based\nrewards within a Dreamer based MBRL framework. By training an ensemble of world\nmodels, the agent actively explores high uncertainty regions of environments\nwithout any task specific feedback. This approach yields a task agnostic latent\nrepresentation, allowing for rapid zero shot or few shot fine tuning on\ndownstream driving tasks such as lane following and collision avoidance.\nExperimental results in both seen and unseen environments demonstrate that\nInDRiVE achieves higher success rates and fewer infractions compared to\nDreamerV2 and DreamerV3 baselines despite using significantly fewer training\nsteps. Our findings highlight the effectiveness of purely intrinsic exploration\nfor learning robust vehicle control behaviors, paving the way for more scalable\nand adaptable autonomous driving systems.",
      "tldr_zh": "本文提出了InDRiVE，一种基于内在分歧奖励的模型强化学习(MBRL)方法，用于车辆探索。该方法通过训练一组世界模型，利用模型之间的分歧作为内在奖励，驱动智能体主动探索环境中的高不确定性区域，无需任务特定的外部反馈。实验表明，InDRiVE在车道跟随和避撞等任务上实现了更高的成功率和更少的违规行为，同时显著减少了训练步骤，展现了纯内在探索在构建鲁棒且可扩展的自动驾驶系统中的潜力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been submitted to IROS 2025 and is currently under\n  review",
      "pdf_url": "http://arxiv.org/pdf/2503.05573v1",
      "published_date": "2025-03-07 16:56:00 UTC",
      "updated_date": "2025-03-07 16:56:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:54:49.999953"
    },
    {
      "arxiv_id": "2503.05571v1",
      "title": "Compliance of AI Systems",
      "title_zh": "AI系统的合规性",
      "authors": [
        "Julius Schöning",
        "Niklas Kruse"
      ],
      "abstract": "The increasing integration of artificial intelligence (AI) systems in various\nfields requires solid concepts to ensure compliance with upcoming legislation.\nThis paper systematically examines the compliance of AI systems with relevant\nlegislation, focusing on the EU's AI Act and the compliance of data sets. The\nanalysis highlighted many challenges associated with edge devices, which are\nincreasingly being used to deploy AI applications closer and closer to the data\nsources. Such devices often face unique issues due to their decentralized\nnature and limited computing resources for implementing sophisticated\ncompliance mechanisms. By analyzing AI implementations, the paper identifies\nchallenges and proposes the first best practices for legal compliance when\ndeveloping, deploying, and running AI. The importance of data set compliance is\nhighlighted as a cornerstone for ensuring the trustworthiness, transparency,\nand explainability of AI systems, which must be aligned with ethical standards\nset forth in regulatory frameworks such as the AI Act. The insights gained\nshould contribute to the ongoing discourse on the responsible development and\ndeployment of embedded AI systems.",
      "tldr_zh": "该研究系统分析了AI系统如何符合欧盟《人工智能法案》等法规要求，特别关注边缘设备部署AI时面临的独特挑战，包括分散化特性和有限计算资源导致的合规机制实施难题。论文提出首个AI开发、部署和运行全周期的法律合规最佳实践，强调数据集合规是确保AI系统可信性、透明度和可解释性的基石。这些见解为嵌入式AI系统的负责任开发与部署提供了重要指导。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "I.2.1; H.4.0"
      ],
      "primary_category": "cs.CY",
      "comment": "5 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05571v1",
      "published_date": "2025-03-07 16:53:36 UTC",
      "updated_date": "2025-03-07 16:53:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:54:48.883748"
    },
    {
      "arxiv_id": "2503.05546v1",
      "title": "Impoola: The Power of Average Pooling for Image-Based Deep Reinforcement Learning",
      "title_zh": "Impoola：平均池化在基于图像的深度强化学习中的力量",
      "authors": [
        "Raphael Trumpp",
        "Ansgar Schäfftlein",
        "Mirco Theile",
        "Marco Caccamo"
      ],
      "abstract": "As image-based deep reinforcement learning tackles more challenging tasks,\nincreasing model size has become an important factor in improving performance.\nRecent studies achieved this by focusing on the parameter efficiency of scaled\nnetworks, typically using Impala-CNN, a 15-layer ResNet-inspired network, as\nthe image encoder. However, while Impala-CNN evidently outperforms older CNN\narchitectures, potential advancements in network design for deep reinforcement\nlearning-specific image encoders remain largely unexplored. We find that\nreplacing the flattening of output feature maps in Impala-CNN with global\naverage pooling leads to a notable performance improvement. This approach\noutperforms larger and more complex models in the Procgen Benchmark,\nparticularly in terms of generalization. We call our proposed encoder model\nImpoola-CNN. A decrease in the network's translation sensitivity may be central\nto this improvement, as we observe the most significant gains in games without\nagent-centered observations. Our results demonstrate that network scaling is\nnot just about increasing model size - efficient network design is also an\nessential factor.",
      "tldr_zh": "该研究提出了Impoola-CNN，一种改进的图像编码器模型，用于图像驱动的深度强化学习。核心创新在于将Impala-CNN中的输出特征图展平操作替换为全局平均池化(global average pooling)，从而显著提升了模型性能。实验表明，Impoola-CNN在Procgen Benchmark中不仅超越了更大更复杂的模型，尤其在泛化能力上表现突出。这种改进可能归因于网络对平移敏感性的降低，特别是在非以智能体为中心的观测任务中效果最为显著。研究强调了高效网络设计在模型扩展中的重要性，而不仅仅是增加模型规模。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05546v1",
      "published_date": "2025-03-07 16:19:19 UTC",
      "updated_date": "2025-03-07 16:19:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:54:54.508995"
    },
    {
      "arxiv_id": "2503.05522v1",
      "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
      "title_zh": "后验概念解耦：从相关到独立的概念表示",
      "authors": [
        "Eren Erogullari",
        "Sebastian Lapuschkin",
        "Wojciech Samek",
        "Frederik Pahde"
      ],
      "abstract": "Concept Activation Vectors (CAVs) are widely used to model\nhuman-understandable concepts as directions within the latent space of neural\nnetworks. They are trained by identifying directions from the activations of\nconcept samples to those of non-concept samples. However, this method often\nproduces similar, non-orthogonal directions for correlated concepts, such as\n\"beard\" and \"necktie\" within the CelebA dataset, which frequently co-occur in\nimages of men. This entanglement complicates the interpretation of concepts in\nisolation and can lead to undesired effects in CAV applications, such as\nactivation steering. To address this issue, we introduce a post-hoc concept\ndisentanglement method that employs a non-orthogonality loss, facilitating the\nidentification of orthogonal concept directions while preserving directional\ncorrectness. We evaluate our approach with real-world and controlled correlated\nconcepts in CelebA and a synthetic FunnyBirds dataset with VGG16 and ResNet18\narchitectures. We further demonstrate the superiority of orthogonalized concept\nrepresentations in activation steering tasks, allowing (1) the insertion of\nisolated concepts into input images through generative models and (2) the\nremoval of concepts for effective shortcut suppression with reduced impact on\ncorrelated concepts in comparison to baseline CAVs.",
      "tldr_zh": "该论文提出了一种后验概念解耦方法，旨在解决概念激活向量（CAVs）在处理相关概念时产生的方向纠缠问题。通过引入非正交性损失函数，该方法能够在保持方向正确性的同时，识别出正交的概念方向。研究在CelebA和FunnyBirds数据集上进行了实验，结果表明，正交化的概念表示在激活引导任务中表现优越，能够（1）通过生成模型将孤立概念插入输入图像，（2）有效抑制快捷方式，同时减少对相关概念的影响。这一方法为概念隔离解释提供了更清晰的路径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05522v1",
      "published_date": "2025-03-07 15:45:43 UTC",
      "updated_date": "2025-03-07 15:45:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:55:01.802329"
    },
    {
      "arxiv_id": "2503.05516v1",
      "title": "Cognitive Bias Detection Using Advanced Prompt Engineering",
      "title_zh": "利用高级提示工程检测认知偏差",
      "authors": [
        "Frederic Lemieux",
        "Aisha Behr",
        "Clara Kellermann-Bryant",
        "Zaki Mohammed"
      ],
      "abstract": "Cognitive biases, systematic deviations from rationality in judgment, pose\nsignificant challenges in generating objective content. This paper introduces a\nnovel approach for real-time cognitive bias detection in user-generated text\nusing large language models (LLMs) and advanced prompt engineering techniques.\nThe proposed system analyzes textual data to identify common cognitive biases\nsuch as confirmation bias, circular reasoning, and hidden assumption. By\ndesigning tailored prompts, the system effectively leverages LLMs' capabilities\nto both recognize and mitigate these biases, improving the quality of\nhuman-generated content (e.g., news, media, reports). Experimental results\ndemonstrate the high accuracy of our approach in identifying cognitive biases,\noffering a valuable tool for enhancing content objectivity and reducing the\nrisks of biased decision-making.",
      "tldr_zh": "该研究提出了一种基于大型语言模型(LLMs)和高级提示工程(prompt engineering)的实时认知偏差检测方法。通过设计定制化提示，该系统能有效识别文本中的常见认知偏差，如确认偏误(confirmation bias)、循环论证(circular reasoning)和隐含假设(hidden assumption)。实验表明，该方法在检测认知偏差方面具有高准确性，为提升新闻、媒体等内容的客观性提供了有效工具。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "17 pages. 6 Figures, 2 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.05516v1",
      "published_date": "2025-03-07 15:35:37 UTC",
      "updated_date": "2025-03-07 15:35:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:55:32.626834"
    },
    {
      "arxiv_id": "2503.05514v1",
      "title": "Noise-Robust Radio Frequency Fingerprint Identification Using Denoise Diffusion Model",
      "title_zh": "基于去噪扩散模型的噪声鲁棒射频指纹识别",
      "authors": [
        "Guolin Yin",
        "Junqing Zhang",
        "Yuan Ding",
        "Simon Cotton"
      ],
      "abstract": "Securing Internet of Things (IoT) devices presents increasing challenges due\nto their limited computational and energy resources. Radio Frequency\nFingerprint Identification (RFFI) emerges as a promising authentication\ntechnique to identify wireless devices through hardware impairments. RFFI\nperformance under low signal-to-noise ratio (SNR) scenarios is significantly\ndegraded because the minute hardware features can be easily swamped in noise.\nIn this paper, we leveraged the diffusion model to effectively restore the RFF\nunder low SNR scenarios. Specifically, we trained a powerful noise predictor\nand tailored a noise removal algorithm to effectively reduce the noise level in\nthe received signal and restore the device fingerprints. We used Wi-Fi as a\ncase study and created a testbed involving 6 commercial off-the-shelf Wi-Fi\ndongles and a USRP N210 software-defined radio (SDR) platform. We conducted\nexperimental evaluations on various SNR scenarios. The experimental results\nshow that the proposed algorithm can improve the classification accuracy by up\nto 34.9%.",
      "tldr_zh": "本研究提出了一种基于去噪扩散模型(Denoise Diffusion Model)的噪声鲁棒射频指纹识别(RFFI)方法，用于在低信噪比(SNR)场景下有效恢复设备指纹。通过训练噪声预测器并定制噪声去除算法，该方法显著降低了接收信号中的噪声水平，从而提升了设备指纹的识别精度。以Wi-Fi为案例，实验结果表明，该算法在多种SNR场景下可将分类准确率提高至34.9%，为物联网设备的射频指纹认证提供了更可靠的解决方案。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "6 pages, 8 figures, WCNC 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05514v1",
      "published_date": "2025-03-07 15:30:55 UTC",
      "updated_date": "2025-03-07 15:30:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:55:44.075156"
    },
    {
      "arxiv_id": "2503.05507v1",
      "title": "Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?",
      "title_zh": "基于语法的代码表示：对大语言模型而言是否值得追求？",
      "authors": [
        "Qingyuan Liang",
        "Zhao Zhang",
        "Zeyu Sun",
        "Zheng Lin",
        "Qi Luo",
        "Yueyi Xiao",
        "Yizhou Chen",
        "Yuqun Zhang",
        "Haotian Zhang",
        "Lu Zhang",
        "Bin Chen",
        "Yingfei Xiong"
      ],
      "abstract": "Grammar serves as a cornerstone in programming languages and software\nengineering, providing frameworks to define the syntactic space and program\nstructure. Existing research demonstrates the effectiveness of grammar-based\ncode representations in small-scale models, showing their ability to reduce\nsyntax errors and enhance performance. However, as language models scale to the\nbillion level or beyond, syntax-level errors become rare, making it unclear\nwhether grammar information still provides performance benefits. To explore\nthis, we develop a series of billion-scale GrammarCoder models, incorporating\ngrammar rules in the code generation process. Experiments on HumanEval (+) and\nMBPP (+) demonstrate a notable improvement in code generation accuracy. Further\nanalysis shows that grammar-based representations enhance LLMs' ability to\ndiscern subtle code differences, reducing semantic errors caused by minor\nvariations. These findings suggest that grammar-based code representations\nremain valuable even in billion-scale models, not only by maintaining syntax\ncorrectness but also by improving semantic differentiation.",
      "tldr_zh": "这篇论文探讨了语法规则在大型语言模型(LLMs)代码生成中的价值。研究者开发了十亿参数规模的GrammarCoder模型系列，在HumanEval(+)和MBPP(+)基准测试中验证了语法信息能显著提升代码生成准确率。研究发现，即使在大模型时代，语法导向的代码表示不仅能保持语法正确性，还能增强模型识别细微代码差异的能力，从而减少由微小变化引起的语义错误。这表明语法基础在LLMs代码生成中仍具有持续价值。",
      "categories": [
        "cs.PL",
        "cs.AI"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05507v1",
      "published_date": "2025-03-07 15:23:13 UTC",
      "updated_date": "2025-03-07 15:23:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:55:30.147938"
    },
    {
      "arxiv_id": "2503.05500v1",
      "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
      "title_zh": "EuroBERT：面向欧洲语言的规模化多语种编码器",
      "authors": [
        "Nicolas Boizard",
        "Hippolyte Gisserot-Boukhlef",
        "Duarte M. Alves",
        "André Martins",
        "Ayoub Hammal",
        "Caio Corro",
        "Céline Hudelot",
        "Emmanuel Malherbe",
        "Etienne Malaboeuf",
        "Fanny Jourdan",
        "Gabriel Hautreux",
        "João Alves",
        "Kevin El-Haddad",
        "Manuel Faysse",
        "Maxime Peyrard",
        "Nuno M. Guerreiro",
        "Patrick Fernandes",
        "Ricardo Rei",
        "Pierre Colombo"
      ],
      "abstract": "General-purpose multilingual vector representations, used in retrieval,\nregression and classification, are traditionally obtained from bidirectional\nencoder models. Despite their wide applicability, encoders have been recently\novershadowed by advances in generative decoder-only models. However, many\ninnovations driving this progress are not inherently tied to decoders. In this\npaper, we revisit the development of multilingual encoders through the lens of\nthese advances, and introduce EuroBERT, a family of multilingual encoders\ncovering European and widely spoken global languages. Our models outperform\nexisting alternatives across a diverse range of tasks, spanning multilingual\ncapabilities, mathematics, and coding, and natively supporting sequences of up\nto 8,192 tokens. We also examine the design decisions behind EuroBERT, offering\ninsights into our dataset composition and training pipeline. We publicly\nrelease the EuroBERT models, including intermediate training checkpoints,\ntogether with our training framework.",
      "tldr_zh": "该研究提出了EuroBERT，一种针对欧洲及全球主要语言的多语言编码器模型。通过借鉴生成式解码器模型的先进技术，该模型在数学、编程等多语言任务上超越现有方案，并原生支持长达8,192个token的序列处理。论文详细分析了数据集构建和训练流程的设计决策，并公开了包括中间训练检查点在内的完整模型框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 6 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.05500v1",
      "published_date": "2025-03-07 15:13:58 UTC",
      "updated_date": "2025-03-07 15:13:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:55:34.413157"
    },
    {
      "arxiv_id": "2503.05492v1",
      "title": "FastMap: Fast Queries Initialization Based Vectorized HD Map Reconstruction Framework",
      "title_zh": "FastMap：基于快速查询初始化的矢量化高精地图重建框架",
      "authors": [
        "Haotian Hu",
        "Jingwei Xu",
        "Fanyi Wang",
        "Toyota Li",
        "Yaonong Wang",
        "Laifeng Hu",
        "Zhiwang Zhang"
      ],
      "abstract": "Reconstruction of high-definition maps is a crucial task in perceiving the\nautonomous driving environment, as its accuracy directly impacts the\nreliability of prediction and planning capabilities in downstream modules.\nCurrent vectorized map reconstruction methods based on the DETR framework\nencounter limitations due to the redundancy in the decoder structure,\nnecessitating the stacking of six decoder layers to maintain performance, which\nsignificantly hampers computational efficiency. To tackle this issue, we\nintroduce FastMap, an innovative framework designed to reduce decoder\nredundancy in existing approaches. FastMap optimizes the decoder architecture\nby employing a single-layer, two-stage transformer that achieves multilevel\nrepresentation capabilities. Our framework eliminates the conventional practice\nof randomly initializing queries and instead incorporates a heatmap-guided\nquery generation module during the decoding phase, which effectively maps image\nfeatures into structured query vectors using learnable positional encoding.\nAdditionally, we propose a geometry-constrained point-to-line loss mechanism\nfor FastMap, which adeptly addresses the challenge of distinguishing highly\nhomogeneous features that often arise in traditional point-to-point loss\ncomputations. Extensive experiments demonstrate that FastMap achieves\nstate-of-the-art performance in both nuScenes and Argoverse2 datasets, with its\ndecoder operating 3.2 faster than the baseline. Code and more demos are\navailable at https://github.com/hht1996ok/FastMap.",
      "tldr_zh": "该研究提出了FastMap，一种基于向量化的高精地图重建框架，旨在解决现有DETR框架中解码器冗余导致的效率问题。通过设计单层两阶段Transformer结构，FastMap实现了多层次特征表示，并引入热图引导的查询生成模块，将图像特征映射为结构化查询向量。此外，提出的几何约束点线损失机制有效解决了传统点对点损失中同质特征难以区分的问题。实验表明，FastMap在nuScenes和Argoverse2数据集上实现了最先进的性能，解码器速度比基线模型快3.2倍。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05492v1",
      "published_date": "2025-03-07 15:01:55 UTC",
      "updated_date": "2025-03-07 15:01:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:55:42.223747"
    },
    {
      "arxiv_id": "2503.07657v1",
      "title": "SplitQuantV2: Enhancing Low-Bit Quantization of LLMs Without GPUs",
      "title_zh": "SplitQuantV2：无需 GPU 增强大语言模型的低比特量化",
      "authors": [
        "Jaewoo Song",
        "Fangzhen Lin"
      ],
      "abstract": "The quantization of large language models (LLMs) is crucial for deploying\nthem on devices with limited computational resources. While advanced\nquantization algorithms offer improved performance compared to the basic linear\nquantization, they typically require high-end graphics processing units (GPUs),\nare often restricted to specific deep neural network (DNN) frameworks, and\nrequire calibration datasets. This limitation poses challenges for using such\nalgorithms on various neural processing units (NPUs) and edge AI devices, which\nhave diverse model formats and frameworks. In this paper, we show SplitQuantV2,\nan innovative algorithm designed to enhance low-bit linear quantization of\nLLMs, can achieve results comparable to those of advanced algorithms.\nSplitQuantV2 preprocesses models by splitting linear and convolution layers\ninto functionally equivalent, quantization-friendly structures. The algorithm's\nplatform-agnostic, concise, and efficient nature allows for implementation\nwithout the need for GPUs. Our evaluation on the Llama 3.2 1B Instruct model\nusing the AI2's Reasoning Challenge (ARC) dataset demonstrates that\nSplitQuantV2 improves the accuracy of the INT4 quantization model by 11.76%p,\nmatching the performance of the original floating-point model. Remarkably,\nSplitQuantV2 took only 2 minutes 6 seconds to preprocess the 1B model and\nperform linear INT4 quantization using only an Apple M4 CPU. SplitQuantV2\nprovides a practical solution for low-bit quantization on LLMs, especially when\ncomplex, computation-intensive algorithms are inaccessible due to hardware\nlimitations or framework incompatibilities.",
      "tldr_zh": "该论文提出了SplitQuantV2算法，能够在无需GPU的情况下提升大型语言模型(LLMs)的低比特量化效果。该方法通过将线性层和卷积层拆分为量化友好结构，实现了与复杂算法相当的INT4量化精度（在Llama 3.2 1B模型上准确率提升11.76个百分点）。特别值得注意的是，该平台无关的算法仅用Apple M4 CPU在2分6秒内就完成了10亿参数模型的预处理和量化，为边缘AI设备上的LLM部署提供了高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07657v1",
      "published_date": "2025-03-07 14:59:07 UTC",
      "updated_date": "2025-03-07 14:59:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:55:46.983459"
    },
    {
      "arxiv_id": "2503.05474v1",
      "title": "Personalized Federated Learning via Learning Dynamic Graphs",
      "title_zh": "基于动态图学习的个性化联邦学习",
      "authors": [
        "Ziran Zhou",
        "Guanyu Gao",
        "Xiaohu Wu",
        "Yan Lyu"
      ],
      "abstract": "Personalized Federated Learning (PFL) aims to train a personalized model for\neach client that is tailored to its local data distribution, learning fails to\nperform well on individual clients due to variations in their local data\ndistributions. Most existing PFL methods focus on personalizing the aggregated\nglobal model for each client, neglecting the fundamental aspect of federated\nlearning: the regulation of how client models are aggregated. Additionally,\nalmost all of them overlook the graph structure formed by clients in federated\nlearning. In this paper, we propose a novel method, Personalized Federated\nLearning with Graph Attention Network (pFedGAT), which captures the latent\ngraph structure between clients and dynamically determines the importance of\nother clients for each client, enabling fine-grained control over the\naggregation process. We evaluate pFedGAT across multiple data distribution\nscenarios, comparing it with twelve state of the art methods on three datasets:\nFashion MNIST, CIFAR-10, and CIFAR-100, and find that it consistently performs\nwell.",
      "tldr_zh": "本文提出了一种基于图注意力网络(Graph Attention Network, GAT)的个性化联邦学习(PFL)方法pFedGAT，通过捕捉客户端之间的潜在图结构，动态确定每个客户端对其他客户端的重要性，从而实现对模型聚合过程的精细控制。与现有PFL方法不同，pFedGAT不仅关注全局模型的个性化，还深入研究了客户端模型聚合的调控机制。实验表明，在Fashion MNIST、CIFAR-10和CIFAR-100数据集上，pFedGAT在多种数据分布场景下均优于12种现有方法，展现出强大的泛化能力和性能优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05474v1",
      "published_date": "2025-03-07 14:47:03 UTC",
      "updated_date": "2025-03-07 14:47:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:56:10.079806"
    },
    {
      "arxiv_id": "2503.05856v1",
      "title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs",
      "title_zh": "这是你的Doge，如你所愿：探索大语言模型混合中的欺骗与鲁棒性",
      "authors": [
        "Lorenz Wolf",
        "Sangwoong Yoon",
        "Ilija Bogunovic"
      ],
      "abstract": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve\nstate-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by\nleveraging the collaboration of multiple LLMs at inference time. Despite these\nsuccesses, an evaluation of the safety and reliability of MoA is missing. We\npresent the first comprehensive study of MoA's robustness against deceptive LLM\nagents that deliberately provide misleading responses. We examine factors like\nthe propagation of deceptive information, model size, and information\navailability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the\npopular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of\n49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate\nthat introducing only a $\\textit{single}$ carefully-instructed deceptive agent\ninto the MoA can reduce performance to 37.9%, effectively nullifying all MoA\ngains. On QuALITY, a multiple-choice comprehension task, the impact is also\nsevere, with accuracy plummeting by a staggering 48.5%. Inspired in part by the\nhistorical Doge of Venice voting process, designed to minimize influence and\ndeception, we propose a range of unsupervised defense mechanisms that recover\nmost of the lost performance.",
      "tldr_zh": "这项研究首次系统评估了混合大语言模型（MoA）架构在面对恶意欺骗性智能体时的脆弱性。研究发现，在AlpacaEval 2.0基准测试中，仅需加入一个精心设计的欺骗性智能体，就能使6个LLM智能体组成的3层MoA性能从49.2%降至37.9%，完全抵消MoA优势；在QuALITY阅读理解任务上准确率更是暴跌48.5%。受威尼斯总督选举机制启发，研究者提出了一系列无监督防御方法，能有效恢复大部分损失的性能表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "35 pages, 9 figures, 16 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.05856v1",
      "published_date": "2025-03-07 14:46:39 UTC",
      "updated_date": "2025-03-07 14:46:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:56:12.949092"
    },
    {
      "arxiv_id": "2503.05473v2",
      "title": "The Society of HiveMind: Multi-Agent Optimization of Foundation Model Swarms to Unlock the Potential of Collective Intelligence",
      "title_zh": "蜂巢思维社会：通过多智能体优化基础模型群释放集体智能潜力",
      "authors": [
        "Noah Mamie",
        "Susie Xi Rao"
      ],
      "abstract": "Multi-agent systems address issues of accessibility and scalability of\nartificial intelligence (AI) foundation models, which are often represented by\nlarge language models. We develop a framework - the \"Society of HiveMind\"\n(SOHM) - that orchestrates the interaction between multiple AI foundation\nmodels, imitating the observed behavior of animal swarms in nature by following\nmodern evolutionary theories. On the one hand, we find that the SOHM provides a\nnegligible benefit on tasks that mainly require real-world knowledge. On the\nother hand, we remark a significant improvement on tasks that require intensive\nlogical reasoning, indicating that multi-agent systems are capable of\nincreasing the reasoning capabilities of the collective compared to the\nindividual agents. Our findings demonstrate the potential of combining a\nmultitude of diverse AI foundation models to form an artificial swarm\nintelligence capable of self-improvement through interactions with a given\nenvironment.",
      "tldr_zh": "该研究提出了“Society of HiveMind”（SOHM）框架，通过模仿自然界动物群体的行为，协调多个AI基础模型（如大语言模型）的交互，以释放集体智能的潜力。研究发现，SOHM在需要逻辑推理的任务上表现显著提升，但在依赖现实知识的任务上效果有限。实验表明，多智能体系统能够通过环境交互实现自我优化，增强集体推理能力，为构建人工群体智能提供了新思路。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "11 pages (excl. appendix)",
      "pdf_url": "http://arxiv.org/pdf/2503.05473v2",
      "published_date": "2025-03-07 14:45:03 UTC",
      "updated_date": "2025-03-13 14:20:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:56:20.014674"
    },
    {
      "arxiv_id": "2503.05455v1",
      "title": "Controllable Complementarity: Subjective Preferences in Human-AI Collaboration",
      "title_zh": "可控互补性：人机协作中的主观偏好",
      "authors": [
        "Chase McDonald",
        "Cleotilde Gonzalez"
      ],
      "abstract": "Research on human-AI collaboration often prioritizes objective performance.\nHowever, understanding human subjective preferences is essential to improving\nhuman-AI complementarity and human experiences. We investigate human\npreferences for controllability in a shared workspace task with AI partners\nusing Behavior Shaping (BS), a reinforcement learning algorithm that allows\nhumans explicit control over AI behavior.\n  In one experiment, we validate the robustness of BS in producing effective AI\npolicies relative to self-play policies, when controls are hidden. In another\nexperiment, we enable human control, showing that participants perceive AI\npartners as more effective and enjoyable when they can directly dictate AI\nbehavior. Our findings highlight the need to design AI that prioritizes both\ntask performance and subjective human preferences. By aligning AI behavior with\nhuman preferences, we demonstrate how human-AI complementarity can extend\nbeyond objective outcomes to include subjective preferences.",
      "tldr_zh": "这项研究探讨了人机协作中人类主观偏好的重要性，提出通过\"行为塑造\"(Behavior Shaping)强化学习算法赋予人类对AI行为的显式控制权。实验表明：当隐藏控制权时，BS算法产生的AI策略与自我博弈策略效果相当；而赋予人类控制权后，用户认为可操控的AI伙伴更具效能且使用体验更佳。研究强调AI设计需兼顾任务表现和人类主观偏好，通过行为对齐实现超越客观结果的互补性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05455v1",
      "published_date": "2025-03-07 14:27:48 UTC",
      "updated_date": "2025-03-07 14:27:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:56:53.291370"
    },
    {
      "arxiv_id": "2503.05453v1",
      "title": "Soft Policy Optimization: Online Off-Policy RL for Sequence Models",
      "title_zh": "软策略优化：面向序列模型的在线离策略强化学习",
      "authors": [
        "Taco Cohen",
        "David W. Zhang",
        "Kunhao Zheng",
        "Yunhao Tang",
        "Remi Munos",
        "Gabriel Synnaeve"
      ],
      "abstract": "RL-based post-training of language models is almost exclusively done using\non-policy methods such as PPO. These methods cannot learn from arbitrary\nsequences such as those produced earlier in training, in earlier runs, by human\nexperts or other policies, or by decoding and exploration methods. This results\nin severe sample inefficiency and exploration difficulties, as well as a\npotential loss of diversity in the policy responses. Moreover, asynchronous PPO\nimplementations require frequent and costly model transfers, and typically use\nvalue models which require a large amount of memory. In this paper we introduce\nSoft Policy Optimization (SPO), a simple, scalable and principled Soft RL\nmethod for sequence model policies that can learn from arbitrary online and\noffline trajectories and does not require a separate value model. In\nexperiments on code contests, we shows that SPO outperforms PPO on pass@10, is\nsignificantly faster and more memory efficient, is able to benefit from\noff-policy data, enjoys improved stability, and learns more diverse (i.e. soft)\npolicies.",
      "tldr_zh": "本文提出Soft Policy Optimization（SPO），一种适用于序列模型的在线/离线强化学习方法，解决了当前语言模型RL微调主要依赖PPO等on-policy方法导致的样本效率低下、探索困难等问题。SPO无需单独的价值模型，可直接从任意在线/离线轨迹中学习，在代码竞赛任务中表现优于PPO，不仅显著提升了pass@10指标和训练效率，还增强了策略的多样性和稳定性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05453v1",
      "published_date": "2025-03-07 14:23:40 UTC",
      "updated_date": "2025-03-07 14:23:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:56:48.273443"
    },
    {
      "arxiv_id": "2503.05449v1",
      "title": "LLM-based Iterative Approach to Metamodeling in Automotive",
      "title_zh": "基于大语言模型的汽车领域元建模迭代方法",
      "authors": [
        "Nenad Petrovic",
        "Fengjunjie Pan",
        "Vahid Zolfaghari",
        "Alois Knoll"
      ],
      "abstract": "In this paper, we introduce an automated approach to domain-specific\nmetamodel construction relying on Large Language Model (LLM). The main focus is\nadoption in automotive domain. As outcome, a prototype was implemented as web\nservice using Python programming language, while OpenAI's GPT-4o was used as\nthe underlying LLM. Based on the initial experiments, this approach\nsuccessfully constructs Ecore metamodel based on set of automotive requirements\nand visualizes it making use of PlantUML notation, so human experts can provide\nfeedback in order to refine the result. Finally, locally deployable solution is\nalso considered, including the limitations and additional steps required.",
      "tldr_zh": "该论文提出了一种基于大语言模型(LLM)的自动化领域特定元建模方法，主要应用于汽车领域。研究团队开发了一个使用Python实现的原型Web服务，采用GPT-4o作为底层LLM，能够根据汽车需求自动构建Ecore元模型并用PlantUML可视化。该方法通过迭代优化过程允许专家提供反馈改进结果，同时探讨了本地化部署方案及其面临的限制。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05449v1",
      "published_date": "2025-03-07 14:19:17 UTC",
      "updated_date": "2025-03-07 14:19:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:57:05.585277"
    },
    {
      "arxiv_id": "2503.05447v1",
      "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
      "title_zh": "Linear-MoE：线性序列建模与专家混合模型的融合",
      "authors": [
        "Weigao Sun",
        "Disen Lan",
        "Tong Zhu",
        "Xiaoye Qu",
        "Yu Cheng"
      ],
      "abstract": "Linear Sequence Modeling (LSM) like linear attention, state space models and\nlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant\narchitectural improvements. In this paper, we introduce Linear-MoE, a\nproduction-level system for modeling and training large-scale models that\nintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules\nfor linear-complexity sequence modeling and MoE layers for sparsely activation,\naiming to offer high performance with efficient training. The Linear-MoE system\ncomprises: 1) Modeling subsystem, which provides a unified framework supporting\nall instances of LSM. and 2) Training subsystem, which facilitates efficient\ntraining by incorporating various advanced parallelism technologies,\nparticularly Sequence Parallelism designed for Linear-MoE models. Additionally,\nwe explore hybrid models that combine Linear-MoE layers with standard\nTransformer-MoE layers with its Sequence Parallelism to further enhance model\nflexibility and performance. Evaluations on two model series, A0.3B-2B and\nA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining\ncompetitive performance on various benchmarks, showcasing its potential as a\nnext-generation foundational model architecture. Code:\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
      "tldr_zh": "本文提出Linear-MoE架构，将线性序列建模(LSM)与混合专家系统(MoE)相结合，实现线性复杂度的高效序列处理。该系统包含支持所有LSM实例的统一建模框架，以及整合序列并行等先进技术的训练子系统，能在保持性能的同时显著提升效率。实验表明，该架构在0.3B-7B参数规模的模型上均取得优异表现，为下一代基础模型提供了高效解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Technical report, 17 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05447v1",
      "published_date": "2025-03-07 14:17:45 UTC",
      "updated_date": "2025-03-07 14:17:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:56:48.867251"
    },
    {
      "arxiv_id": "2503.05439v1",
      "title": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for Robust Reasoning",
      "title_zh": "基于ASP框架的LLM稳健推理：保形预测的实证研究",
      "authors": [
        "Navdeep Kaur",
        "Lachlan McPheat",
        "Alessandra Russo",
        "Anthony G Cohn",
        "Pranava Madhyastha"
      ],
      "abstract": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks.",
      "tldr_zh": "本研究实证探讨了结合Conformal Language Modeling (CLM)和Answer Set Programming (ASP)的方法，用于提升开源大语言模型(LLMs)在复杂多步推理任务中的表现。通过在需要空间推理的StepGame数据集上进行实验，该方法利用CLM从LLM生成ASP程序集，并为输出正确性提供统计保证。结果显示，CLM显著优于使用标准采样方法的基线模型，在不同复杂度的推理任务上都实现了准确率提升。但研究发现，CLM在面对需要超长推理步骤的任务时存在局限性，多样化的校准集未能改善其泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05439v1",
      "published_date": "2025-03-07 14:10:10 UTC",
      "updated_date": "2025-03-07 14:10:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:56:57.601498"
    },
    {
      "arxiv_id": "2503.05423v1",
      "title": "Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning",
      "title_zh": "语义偏移估计：基于双投影与分类器重构的无样本类增量学习方法",
      "authors": [
        "Run He",
        "Di Fang",
        "Yicheng Xu",
        "Yawen Cui",
        "Ming Li",
        "Cen Chen",
        "Ziqian Zeng",
        "Huiping Zhuang"
      ],
      "abstract": "Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn\nfrom distinct categories without retaining exemplars but easily suffers from\ncatastrophic forgetting of learned knowledge. While existing EFCIL methods\nleverage knowledge distillation to alleviate forgetting, they still face two\ncritical challenges: semantic shift and decision bias. Specifically, the\nembeddings of old tasks shift in the embedding space after learning new tasks,\nand the classifier becomes biased towards new tasks due to training solely with\nnew data, thereby hindering the balance between old and new knowledge. To\naddress these issues, we propose the Dual-Projection Shift Estimation and\nClassifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates\nsemantic shift through a dual-projection, which combines a learnable\ntransformation with a row-space projection to capture both task-wise and\ncategory-wise shifts. Furthermore, to mitigate decision bias, DPCR employs\nridge regression to reformulate classifier training as a reconstruction\nprocess. This reconstruction exploits previous information encoded in\ncovariance and prototype of each class after calibration with estimated shift,\nthereby reducing decision bias. Extensive experiments demonstrate that, across\nvarious datasets, DPCR effectively balances old and new tasks, outperforming\nstate-of-the-art EFCIL methods.",
      "tldr_zh": "该论文提出了一种名为双投影偏移估计与分类器重建(DPCR)的新方法，用于解决无样本类增量学习(EFCIL)中的语义偏移和决策偏差问题。该方法通过双投影机制(可学习变换和行空间投影)来准确估计任务间和类别间的语义偏移，并利用岭回归将分类器训练重构为基于协方差和校准后类原型的重建过程。实验表明，DPCR能有效平衡新旧任务知识，在多个数据集上超越了当前最先进的EFCIL方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05423v1",
      "published_date": "2025-03-07 13:50:29 UTC",
      "updated_date": "2025-03-07 13:50:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:57:11.966282"
    },
    {
      "arxiv_id": "2503.05854v1",
      "title": "Accelerating Earth Science Discovery via Multi-Agent LLM Systems",
      "title_zh": "加速地球科学发现：基于多智能体大语言模型系统",
      "authors": [
        "Dmitrii Pantiukhin",
        "Boris Shapkin",
        "Ivan Kuznetsov",
        "Antonia Anna Jost",
        "Nikolay Koldunov"
      ],
      "abstract": "This Perspective explores the transformative potential of Multi-Agent Systems\n(MAS) powered by Large Language Models (LLMs) in the geosciences. Users of\ngeoscientific data repositories face challenges due to the complexity and\ndiversity of data formats, inconsistent metadata practices, and a considerable\nnumber of unprocessed datasets. MAS possesses transformative potential for\nimproving scientists' interaction with geoscientific data by enabling\nintelligent data processing, natural language interfaces, and collaborative\nproblem-solving capabilities. We illustrate this approach with \"PANGAEA GPT\", a\nspecialized MAS pipeline integrated with the diverse PANGAEA database for Earth\nand Environmental Science, demonstrating how MAS-driven workflows can\neffectively manage complex datasets and accelerate scientific discovery. We\ndiscuss how MAS can address current data challenges in geosciences, highlight\nadvancements in other scientific fields, and propose future directions for\nintegrating MAS into geoscientific data processing pipelines. In this\nPerspective, we show how MAS can fundamentally improve data accessibility,\npromote cross-disciplinary collaboration, and accelerate geoscientific\ndiscoveries.",
      "tldr_zh": "该研究探讨了基于大语言模型(LLMs)的多智能体系统(MAS)在地球科学领域的变革潜力。针对地球科学数据格式复杂、元数据不一致以及大量未处理数据等挑战，MAS通过智能数据处理、自然语言界面和协同问题解决能力，显著改善了科学家与地球科学数据的交互。研究以\"PANGAEA GPT\"为例，展示了MAS如何高效管理复杂数据集并加速科学发现。MAS有望从根本上提升数据可访问性，促进跨学科合作，并加速地球科学领域的突破。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "I.2.11"
      ],
      "primary_category": "cs.MA",
      "comment": "10 pages, 1 figure. Perspective article",
      "pdf_url": "http://arxiv.org/pdf/2503.05854v1",
      "published_date": "2025-03-07 13:25:56 UTC",
      "updated_date": "2025-03-07 13:25:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:57:23.300494"
    },
    {
      "arxiv_id": "2503.05394v1",
      "title": "Static Program Analysis Guided LLM Based Unit Test Generation",
      "title_zh": "基于静态程序分析引导的大语言模型单元测试生成",
      "authors": [
        "Sujoy Roychowdhury",
        "Giriprasad Sridhara",
        "A K Raghavan",
        "Joy Bose",
        "Sourav Mazumdar",
        "Hamender Singh",
        "Srinivasan Bajji Sugumaran",
        "Ricardo Britto"
      ],
      "abstract": "We describe a novel approach to automating unit test generation for Java\nmethods using large language models (LLMs). Existing LLM-based approaches rely\non sample usage(s) of the method to test (focal method) and/or provide the\nentire class of the focal method as input prompt and context. The former\napproach is often not viable due to the lack of sample usages, especially for\nnewly written focal methods. The latter approach does not scale well enough;\nthe bigger the complexity of the focal method and larger associated class, the\nharder it is to produce adequate test code (due to factors such as exceeding\nthe prompt and context lengths of the underlying LLM). We show that augmenting\nprompts with \\emph{concise} and \\emph{precise} context information obtained by\nprogram analysis %of the focal method increases the effectiveness of generating\nunit test code through LLMs. We validate our approach on a large commercial\nJava project and a popular open-source Java project.",
      "tldr_zh": "该论文提出了一种结合静态程序分析和大型语言模型(LLM)的Java单元测试生成新方法。针对现有LLM方法依赖方法使用示例或完整类代码导致的可扩展性问题，该方法通过程序分析提取简洁精确的上下文信息来增强提示词。实验证明，这种混合方法在商业和开源Java项目上能有效提升测试代码生成质量，解决了传统方法在新编写方法缺乏使用示例或复杂类超出LLM上下文限制时的局限性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05394v1",
      "published_date": "2025-03-07 13:09:37 UTC",
      "updated_date": "2025-03-07 13:09:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:58:07.168216"
    },
    {
      "arxiv_id": "2503.05388v1",
      "title": "Ontology Generation using Large Language Models",
      "title_zh": "使用大型语言模型的本体生成",
      "authors": [
        "Anna Sofia Lippolis",
        "Mohammad Javad Saeedizade",
        "Robin Keskisärkkä",
        "Sara Zuppiroli",
        "Miguel Ceriani",
        "Aldo Gangemi",
        "Eva Blomqvist",
        "Andrea Giovanni Nuzzolese"
      ],
      "abstract": "The ontology engineering process is complex, time-consuming, and error-prone,\neven for experienced ontology engineers. In this work, we investigate the\npotential of Large Language Models (LLMs) to provide effective OWL ontology\ndrafts directly from ontological requirements described using user stories and\ncompetency questions. Our main contribution is the presentation and evaluation\nof two new prompting techniques for automated ontology development: Memoryless\nCQbyCQ and Ontogenia. We also emphasize the importance of three structural\ncriteria for ontology assessment, alongside expert qualitative evaluation,\nhighlighting the need for a multi-dimensional evaluation in order to capture\nthe quality and usability of the generated ontologies. Our experiments,\nconducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29\ndifferent user stories, compare the performance of three LLMs using the two\nprompting techniques. The results demonstrate improvements over the current\nstate-of-the-art in LLM-supported ontology engineering. More specifically, the\nmodel OpenAI o1-preview with Ontogenia produces ontologies of sufficient\nquality to meet the requirements of ontology engineers, significantly\noutperforming novice ontology engineers in modelling ability. However, we still\nnote some common mistakes and variability of result quality, which is important\nto take into account when using LLMs for ontology authoring support. We discuss\nthese limitations and propose directions for future research.",
      "tldr_zh": "该研究探索了利用大语言模型(LLMs)自动生成OWL本体的新方法，提出了两种创新提示技术：Memoryless CQbyCQ和Ontogenia。实验表明，采用Ontogenia技术的OpenAI o1-preview模型生成的OWL本体质量优于新手工程师水平，在包含10个本体、100个能力问题(CQs)和29个用户故事的基准测试中超越了现有最先进方法。研究同时强调了多维评估的重要性，包括三个结构性标准与专家定性评价，并指出了当前LLM在生成过程中仍存在的错误和质量波动问题，为未来研究提供了改进方向。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "2 figures and 3 tables. 20 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05388v1",
      "published_date": "2025-03-07 13:03:28 UTC",
      "updated_date": "2025-03-07 13:03:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:58:10.009154"
    },
    {
      "arxiv_id": "2503.05383v3",
      "title": "AVA: Attentive VLM Agent for Mastering StarCraft II",
      "title_zh": "AVA：专注视觉语言模型智能体，精通《星际争霸II》",
      "authors": [
        "Weiyu Ma",
        "Yuqian Fu",
        "Zecheng Zhang",
        "Bernard Ghanem",
        "Guohao Li"
      ],
      "abstract": "We introduce Attentive VLM Agent (AVA), a multimodal StarCraft II agent that\naligns artificial agent perception with the human gameplay experience.\nTraditional frameworks such as SMAC rely on abstract state representations that\ndiverge significantly from human perception, limiting the ecological validity\nof agent behavior. Our agent addresses this limitation by incorporating RGB\nvisual inputs and natural language observations that more closely simulate\nhuman cognitive processes during gameplay. The AVA architecture consists of\nthree integrated components: (1) a vision-language model enhanced with\nspecialized self-attention mechanisms for strategic unit targeting and\nbattlefield assessment, (2) a retrieval-augmented generation system that\nleverages domain-specific StarCraft II knowledge to inform tactical decisions,\nand (3) a dynamic role-based task distribution system that enables coordinated\nmulti-agent behavior. The experimental evaluation in our proposed AVACraft\nenvironment, which contains 21 multimodal StarCraft II scenarios, demonstrates\nthat AVA powered by foundation models (specifically Qwen-VL and GPT-4o) can\nexecute complex tactical maneuvers without explicit training, achieving\ncomparable performance to traditional MARL methods that require substantial\ntraining iterations. This work establishes a foundation for developing\nhuman-aligned StarCraft II agents and advances the broader research agenda of\nmultimodal game AI. Our implementation is available at\nhttps://github.com/camel-ai/VLM-Play-StarCraft2.",
      "tldr_zh": "本研究提出了AVA（Attentive VLM Agent），一种基于视觉语言模型（VLM）的多模态《星际争霸II》智能体，通过RGB视觉输入和自然语言观察来模拟人类玩家的认知过程。该框架包含三个核心组件：1）采用自注意力机制进行战场评估的视觉语言模型；2）基于检索增强生成（RAG）的战术决策系统；3）支持多智能体协同的动态角色分配系统。实验表明，AVA在21个多模态游戏场景中无需显式训练即可执行复杂战术，性能媲美需要大量训练的MARL方法，为开发人类对齐的游戏AI奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.05383v3",
      "published_date": "2025-03-07 12:54:25 UTC",
      "updated_date": "2025-03-21 06:14:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:57:53.505164"
    },
    {
      "arxiv_id": "2503.05371v1",
      "title": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias Mitigation in LLMs",
      "title_zh": "视角转换：利用导向向量集成实现大型语言模型的稳健偏差缓解",
      "authors": [
        "Zara Siddique",
        "Irtaza Khalid",
        "Liam D. Turner",
        "Luis Espinosa-Anke"
      ],
      "abstract": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We employ Bayesian optimization to systematically identify effective\ncontrastive pair datasets across nine bias axes. When optimized on the BBQ\ndataset, our individually tuned steering vectors achieve average improvements\nof 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen,\nrespectively. Building on these promising results, we introduce Steering Vector\nEnsembles (SVE), a method that averages multiple individually optimized\nsteering vectors, each targeting a specific bias axis such as age, race, or\ngender. By leveraging their collective strength, SVE outperforms individual\nsteering vectors in both bias reduction and maintaining model performance. The\nwork presents the first systematic investigation of steering vectors for bias\nmitigation, and we demonstrate that SVE is a powerful and computationally\nefficient strategy for reducing bias in LLMs, with broader implications for\nenhancing AI safety.",
      "tldr_zh": "该研究提出了一种通过**steering vectors**修改大语言模型（LLMs）前向传播激活值的新方法，用于系统性减轻模型偏见。研究利用贝叶斯优化在九个偏见维度上识别有效的对比数据集，并在BBQ数据集上优化后，显著提升了Mistral、Llama和Qwen模型的性能。进一步，作者提出了**Steering Vector Ensembles (SVE)**，通过集成多个针对特定偏见维度（如年龄、种族、性别）的优化向量，在减少偏见的同时保持了模型性能。SVE不仅计算高效，还为提升AI安全性提供了新策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to ACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05371v1",
      "published_date": "2025-03-07 12:25:29 UTC",
      "updated_date": "2025-03-07 12:25:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:58:02.256881"
    },
    {
      "arxiv_id": "2503.05357v1",
      "title": "Improving Hate Speech Classification with Cross-Taxonomy Dataset Integration",
      "title_zh": "通过跨分类数据集整合提升仇恨言论分类效果",
      "authors": [
        "Jan Fillies",
        "Adrian Paschke"
      ],
      "abstract": "Algorithmic hate speech detection faces significant challenges due to the\ndiverse definitions and datasets used in research and practice. Social media\nplatforms, legal frameworks, and institutions each apply distinct yet\noverlapping definitions, complicating classification efforts. This study\naddresses these challenges by demonstrating that existing datasets and\ntaxonomies can be integrated into a unified model, enhancing prediction\nperformance and reducing reliance on multiple specialized classifiers. The work\nintroduces a universal taxonomy and a hate speech classifier capable of\ndetecting a wide range of definitions within a single framework. Our approach\nis validated by combining two widely used but differently annotated datasets,\nshowing improved classification performance on an independent test set. This\nwork highlights the potential of dataset and taxonomy integration in advancing\nhate speech detection, increasing efficiency, and ensuring broader\napplicability across contexts.",
      "tldr_zh": "本研究提出了一种通过跨分类法数据集整合改进仇恨言论分类的方法，解决了现有研究因定义和数据集多样性导致的分类难题。通过构建统一分类法和通用仇恨言论检测模型，该研究能够在一个框架内检测多种定义，减少对多个专用分类器的依赖。实验验证表明，整合两种广泛使用但标注不同的数据集后，模型在独立测试集上的分类性能显著提升，为仇恨言论检测的效率和广泛适用性提供了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication at LaTeCH-CLfL 2025. The 9th Joint ACL\n  Special Interest Group on Language Technologies for the Socio-Economic\n  Sciences and Humanities (SIGHUM) Workshop on Computational Linguistics for\n  Cultural Heritage, Social Sciences, Humanities and Literature",
      "pdf_url": "http://arxiv.org/pdf/2503.05357v1",
      "published_date": "2025-03-07 12:01:02 UTC",
      "updated_date": "2025-03-07 12:01:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:58:11.541873"
    },
    {
      "arxiv_id": "2503.05852v1",
      "title": "Evaluating Large Language Models in Code Generation: INFINITE Methodology for Defining the Inference Index",
      "title_zh": "评估大语言模型在代码生成中的表现：定义推理指数的INFINITE方法论",
      "authors": [
        "Nicholas Christakis",
        "Dimitris Drikakis"
      ],
      "abstract": "This study introduces a new methodology for an Inference Index (InI), called\nINFerence INdex In Testing model Effectiveness methodology (INFINITE), aiming\nto evaluate the performance of Large Language Models (LLMs) in code generation\ntasks. The InI index provides a comprehensive assessment focusing on three key\ncomponents: efficiency, consistency, and accuracy. This approach encapsulates\ntime-based efficiency, response quality, and the stability of model outputs,\noffering a thorough understanding of LLM performance beyond traditional\naccuracy metrics. We applied this methodology to compare OpenAI's GPT-4o (GPT),\nOpenAI-o1 pro (OAI1), and OpenAI-o3 mini-high (OAI3) in generating Python code\nfor the Long-Short-Term-Memory (LSTM) model to forecast meteorological\nvariables such as temperature, relative humidity and wind velocity. Our\nfindings demonstrate that GPT outperforms OAI1 and performs comparably to OAI3\nregarding accuracy and workflow efficiency. The study reveals that LLM-assisted\ncode generation can produce results similar to expert-designed models with\neffective prompting and refinement. GPT's performance advantage highlights the\nbenefits of widespread use and user feedback.",
      "tldr_zh": "本研究提出了一种新型评估方法INFINITE（推理有效性测试方法），用于定义推理指数(InI)，全面评估大语言模型(LLMs)在代码生成任务中的表现。该指数从效率、一致性和准确性三个维度出发，综合考量时间效率、响应质量和输出稳定性，突破了传统仅关注准确率的评估局限。研究团队将该方法应用于对比GPT-4o、OAI1和OAI3在生成LSTM模型预测气象变量（温度、相对湿度和风速）Python代码的表现，发现GPT-4o在准确性和工作流效率上优于OAI1，与OAI3相当。研究表明，通过有效提示和优化，LLM辅助生成的代码可达到专家设计模型的水平，其中GPT-4o的优越性能验证了广泛使用和用户反馈的价值。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "20 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05852v1",
      "published_date": "2025-03-07 11:59:44 UTC",
      "updated_date": "2025-03-07 11:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:58:28.520618"
    },
    {
      "arxiv_id": "2503.05355v1",
      "title": "On the Logical Content of Logic Programs",
      "title_zh": "论逻辑程序的逻辑内容",
      "authors": [
        "Alexader V. Gheorghiu"
      ],
      "abstract": "Logic programming (LP) is typically understood through operational semantics\n(e.g., SLD-resolution) or model-theoretic interpretations (e.g., the least\nHerbrand model). This paper introduces a novel perspective on LP by defining a\n``support'' relation that explicates what a program ``knows''. This\ninterpretation is shown to express classical and intuitionistic logic, as well\nas an intermediate logic, depending on certain choices regarding LP and the\nmeanings of disjunction and negation. These results are formalized using the\nidea of base-extension semantics within proof-theoretic semantics. Our approach\noffers new insights into the logical foundations of LP and has potential\napplications in knowledge representation, automated reasoning, and formal\nverification.",
      "tldr_zh": "本文提出了一种新的逻辑程序（LP）解释方法，通过定义“支持”关系来明确程序“知道”的内容。该方法展示了LP如何表达经典逻辑、直觉逻辑以及一种中间逻辑，具体取决于对LP中析取和否定含义的选择。研究基于证明论语义中的基扩展语义形式化这些结果，为LP的逻辑基础提供了新见解，并可能在知识表示、自动推理和形式验证等领域有应用潜力。",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05355v1",
      "published_date": "2025-03-07 11:58:08 UTC",
      "updated_date": "2025-03-07 11:58:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:58:23.624670"
    },
    {
      "arxiv_id": "2503.05349v1",
      "title": "Spatial Distillation based Distribution Alignment (SDDA) for Cross-Headset EEG Classification",
      "title_zh": "基于空间蒸馏的分布对齐方法(SDDA)用于跨头戴设备EEG分类",
      "authors": [
        "Dingkun Liu",
        "Siyang Li",
        "Ziwei Wang",
        "Wei Li",
        "Dongrui Wu"
      ],
      "abstract": "A non-invasive brain-computer interface (BCI) enables direct interaction\nbetween the user and external devices, typically via electroencephalogram (EEG)\nsignals. However, decoding EEG signals across different headsets remains a\nsignificant challenge due to differences in the number and locations of the\nelectrodes. To address this challenge, we propose a spatial distillation based\ndistribution alignment (SDDA) approach for heterogeneous cross-headset transfer\nin non-invasive BCIs. SDDA uses first spatial distillation to make use of the\nfull set of electrodes, and then input/feature/output space distribution\nalignments to cope with the significant differences between the source and\ntarget domains. To our knowledge, this is the first work to use knowledge\ndistillation in cross-headset transfers. Extensive experiments on six EEG\ndatasets from two BCI paradigms demonstrated that SDDA achieved superior\nperformance in both offline unsupervised domain adaptation and online\nsupervised domain adaptation scenarios, consistently outperforming 10 classical\nand state-of-the-art transfer learning algorithms.",
      "tldr_zh": "该研究提出了一种基于空间蒸馏的分布对齐方法(SDDA)，用于解决不同脑电图(EEG)头戴设备间的信号分类难题。该方法创新性地结合了空间蒸馏技术和输入/特征/输出空间的多层次分布对齐，有效处理源域与目标域间的显著差异。实验表明，SDDA在两种BCI范式的六个EEG数据集上表现优异，在离线和在线域适应场景中均超越10种现有迁移学习方法，成为首个在跨头戴设备迁移中应用知识蒸馏技术的研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05349v1",
      "published_date": "2025-03-07 11:44:49 UTC",
      "updated_date": "2025-03-07 11:44:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:58:34.667826"
    },
    {
      "arxiv_id": "2503.05346v1",
      "title": "AutoIOT: LLM-Driven Automated Natural Language Programming for AIoT Applications",
      "title_zh": "AutoIOT：面向AIoT应用的大语言模型驱动自动化自然语言编程",
      "authors": [
        "Leming Shen",
        "Qiang Yang",
        "Yuanqing Zheng",
        "Mo Li"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has profoundly transformed our\nlives, revolutionizing interactions with AI and lowering the barrier to AI\nusage. While LLMs are primarily designed for natural language interaction, the\nextensive embedded knowledge empowers them to comprehend digital sensor data.\nThis capability enables LLMs to engage with the physical world through IoT\nsensors and actuators, performing a myriad of AIoT tasks. Consequently, this\nevolution triggers a paradigm shift in conventional AIoT application\ndevelopment, democratizing its accessibility to all by facilitating the design\nand development of AIoT applications via natural language. However, some\nlimitations need to be addressed to unlock the full potential of LLMs in AIoT\napplication development. First, existing solutions often require transferring\nraw sensor data to LLM servers, which raises privacy concerns, incurs high\nquery fees, and is limited by token size. Moreover, the reasoning processes of\nLLMs are opaque to users, making it difficult to verify the robustness and\ncorrectness of inference results. This paper introduces AutoIOT, an LLM-based\nautomated program generator for AIoT applications. AutoIOT enables users to\nspecify their requirements using natural language (input) and automatically\nsynthesizes interpretable programs with documentation (output). AutoIOT\nautomates the iterative optimization to enhance the quality of generated code\nwith minimum user involvement. AutoIOT not only makes the execution of AIoT\ntasks more explainable but also mitigates privacy concerns and reduces token\ncosts with local execution of synthesized programs. Extensive experiments and\nuser studies demonstrate AutoIOT's remarkable capability in program synthesis\nfor various AIoT tasks. The synthesized programs can match and even outperform\nsome representative baselines.",
      "tldr_zh": "该研究提出了AutoIOT，一种基于大语言模型（LLMs）的自动化自然语言编程框架，用于AIoT应用开发。AutoIOT允许用户通过自然语言描述需求，并自动生成可解释的程序代码及其文档，同时通过本地执行减少隐私风险和token成本。该框架还支持自动化迭代优化，提升代码质量。实验表明，AutoIOT在多种AIoT任务中生成的程序不仅具有可解释性，还能匹配甚至超越一些代表性基线方法，显著降低了AIoT应用开发的门槛。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05346v1",
      "published_date": "2025-03-07 11:40:52 UTC",
      "updated_date": "2025-03-07 11:40:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:58:39.075600"
    },
    {
      "arxiv_id": "2503.05336v3",
      "title": "Toward an Evaluation Science for Generative AI Systems",
      "title_zh": "迈向生成式AI系统的评估科学",
      "authors": [
        "Laura Weidinger",
        "Inioluwa Deborah Raji",
        "Hanna Wallach",
        "Margaret Mitchell",
        "Angelina Wang",
        "Olawale Salaudeen",
        "Rishi Bommasani",
        "Deep Ganguli",
        "Sanmi Koyejo",
        "William Isaac"
      ],
      "abstract": "There is an increasing imperative to anticipate and understand the\nperformance and safety of generative AI systems in real-world deployment\ncontexts. However, the current evaluation ecosystem is insufficient: Commonly\nused static benchmarks face validity challenges, and ad hoc case-by-case audits\nrarely scale. In this piece, we advocate for maturing an evaluation science for\ngenerative AI systems. While generative AI creates unique challenges for system\nsafety engineering and measurement science, the field can draw valuable\ninsights from the development of safety evaluation practices in other fields,\nincluding transportation, aerospace, and pharmaceutical engineering. In\nparticular, we present three key lessons: Evaluation metrics must be applicable\nto real-world performance, metrics must be iteratively refined, and evaluation\ninstitutions and norms must be established. Applying these insights, we outline\na concrete path toward a more rigorous approach for evaluating generative AI\nsystems.",
      "tldr_zh": "本文提出需要建立生成式AI系统的评估科学，以解决当前静态基准测试有效性不足和个案审计难以扩展的问题。研究借鉴交通、航空航天和制药工程等领域的安全评估经验，提出三大关键原则：评估指标需适用于实际场景、需持续迭代优化、需建立评估机构和规范。文章为生成式AI系统评估提供了更严谨的方法论路径，强调将安全工程和测量科学应用于这一新兴领域。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "First two authors contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2503.05336v3",
      "published_date": "2025-03-07 11:23:48 UTC",
      "updated_date": "2025-03-13 00:09:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:59:02.526336"
    },
    {
      "arxiv_id": "2503.05330v1",
      "title": "Speculative Decoding for Multi-Sample Inference",
      "title_zh": "多样本推理中的推测解码方法",
      "authors": [
        "Yiwei Li",
        "Jiayi Shi",
        "Shaoxiong Feng",
        "Peiwen Yuan",
        "Xinglin Wang",
        "Yueqi Zhang",
        "Ji Zhang",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "We propose a novel speculative decoding method tailored for multi-sample\nreasoning scenarios, such as self-consistency and Best-of-N sampling. Our\nmethod exploits the intrinsic consensus of parallel generation paths to\nsynthesize high-quality draft tokens without requiring auxiliary models or\nexternal databases. By dynamically analyzing structural patterns across\nparallel reasoning paths through a probabilistic aggregation mechanism, it\nidentifies consensus token sequences that align with the decoding distribution.\nEvaluations on mathematical reasoning benchmarks demonstrate a substantial\nimprovement in draft acceptance rates over baselines, while reducing the\nlatency in draft token construction. This work establishes a paradigm shift for\nefficient multi-sample inference, enabling seamless integration of speculative\ndecoding with sampling-based reasoning techniques.",
      "tldr_zh": "该论文提出了一种面向多样本推理任务（如自洽性和Best-of-N采样）的新型推测解码方法。该方法通过并行生成路径的内在共识来合成高质量候选token，无需辅助模型或外部数据库，采用概率聚合机制动态分析并行推理路径的结构模式，识别符合解码分布的共识token序列。实验表明，该方法在数学推理基准上显著提高了候选token接受率，同时降低了构建延迟，为基于采样的推理技术与推测解码的高效结合提供了新范式。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05330v1",
      "published_date": "2025-03-07 11:15:36 UTC",
      "updated_date": "2025-03-07 11:15:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:59:18.854916"
    },
    {
      "arxiv_id": "2503.05328v1",
      "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models",
      "title_zh": "动态知识整合支持基于证据的反驳生成——大语言模型驱动方法",
      "authors": [
        "Anar Yeginbergen",
        "Maite Oronoz",
        "Rodrigo Agerri"
      ],
      "abstract": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.",
      "tldr_zh": "本文研究了动态外部知识整合如何提升大语言模型(LLMs)的反驳论点生成质量。针对LLMs在论辩任务中容易产生冗长且可能不准确的回应问题，研究者构建了一个手工标注的论点-反论点配对数据集，并提出基于LLM-as-a-Judge的新型评估方法。实验表明，实时整合网络动态知识能显著提高生成反驳的相关性、说服力和事实准确性，为开发更可靠的自动论辩系统提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05328v1",
      "published_date": "2025-03-07 11:13:33 UTC",
      "updated_date": "2025-03-07 11:13:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:59:25.683177"
    },
    {
      "arxiv_id": "2503.05322v1",
      "title": "Attenuation artifact detection and severity classification in intracoronary OCT using mixed image representations",
      "title_zh": "基于混合图像表示的冠状动脉内OCT衰减伪影检测与严重程度分类",
      "authors": [
        "Pierandrea Cancian",
        "Simone Saitta",
        "Xiaojin Gu",
        "Rudolf L. M. van Herten",
        "Thijs J. Luttikholt",
        "Jos Thannhauser",
        "Rick H. J. A. Volleberg",
        "Ruben G. A. van der Waerden",
        "Joske L. van der Zande",
        "Clarisa I. Sánchez",
        "Bram van Ginneken",
        "Niels van Royen",
        "Ivana Išgum"
      ],
      "abstract": "In intracoronary optical coherence tomography (OCT), blood residues and gas\nbubbles cause attenuation artifacts that can obscure critical vessel\nstructures. The presence and severity of these artifacts may warrant\nre-acquisition, prolonging procedure time and increasing use of contrast agent.\nAccurate detection of these artifacts can guide targeted re-acquisition,\nreducing the amount of repeated scans needed to achieve diagnostically viable\nimages. However, the highly heterogeneous appearance of these artifacts poses a\nchallenge for the automated detection of the affected image regions. To enable\nautomatic detection of the attenuation artifacts caused by blood residues and\ngas bubbles based on their severity, we propose a convolutional neural network\nthat performs classification of the attenuation lines (A-lines) into three\nclasses: no artifact, mild artifact and severe artifact. Our model extracts and\nmerges features from OCT images in both Cartesian and polar coordinates, where\neach column of the image represents an A-line. Our method detects the presence\nof attenuation artifacts in OCT frames reaching F-scores of 0.77 and 0.94 for\nmild and severe artifacts, respectively. The inference time over a full OCT\nscan is approximately 6 seconds. Our experiments show that analysis of images\nrepresented in both Cartesian and polar coordinate systems outperforms the\nanalysis in polar coordinates only, suggesting that these representations\ncontain complementary features. This work lays the foundation for automated\nartifact assessment and image acquisition guidance in intracoronary OCT\nimaging.",
      "tldr_zh": "该研究提出了一种基于混合图像表示的卷积神经网络方法，用于检测冠状动脉OCT成像中的衰减伪影（包括血液残留和气泡）并进行严重程度分类。该方法创新性地结合笛卡尔坐标系和极坐标系下的图像特征，将A-line分为无伪影、轻度伪影和重度伪影三类，在OCT帧检测中分别达到0.77和0.94的F1分数。实验表明，双坐标系特征融合优于单一极坐标分析，且全OCT扫描的推理时间仅需约6秒，为冠状动脉OCT成像中的自动化伪影评估和图像采集引导提供了有效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05322v1",
      "published_date": "2025-03-07 11:01:00 UTC",
      "updated_date": "2025-03-07 11:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:59:38.802284"
    },
    {
      "arxiv_id": "2503.05320v1",
      "title": "Disentangling Task Interference within Neurons: Model Merging in Alignment with Neuronal Mechanisms",
      "title_zh": "解耦神经元内的任务干扰：符合神经机制模型融合方法",
      "authors": [
        "Zitao Fang",
        "Guodong DU",
        "Shuyang Yu",
        "Yifei Guo",
        "Yiwei Zhang",
        "Jing Li",
        "Ho-Kin Tang",
        "Sim Kuan Goh"
      ],
      "abstract": "Fine-tuning pre-trained models on targeted datasets enhances task-specific\nperformance but often comes at the expense of generalization. Model merging\ntechniques, which integrate multiple fine-tuned models into a single multi-task\nmodel through task arithmetic at various levels: model, layer, or parameter,\noffer a promising solution. However, task interference remains a fundamental\nchallenge, leading to performance degradation and suboptimal merged models.\nExisting approaches largely overlook the fundamental role of individual neurons\nand their connectivity, resulting in a lack of interpretability in both the\nmerging process and the merged models. In this work, we present the first study\non the impact of neuronal alignment in model merging. We decompose\ntask-specific representations into two complementary neuronal subspaces that\nregulate neuron sensitivity and input adaptability. Leveraging this\ndecomposition, we introduce NeuroMerging, a novel merging framework developed\nto mitigate task interference within neuronal subspaces, enabling training-free\nmodel fusion across diverse tasks. Through extensive experiments, we\ndemonstrate that NeuroMerging achieves superior performance compared to\nexisting methods on multi-task benchmarks across both vision and natural\nlanguage domains. Our findings highlight the importance of aligning neuronal\nmechanisms in model merging, offering new insights into mitigating task\ninterference and improving knowledge fusion.",
      "tldr_zh": "该研究提出了一种基于神经元机制的新型模型融合方法NeuroMerging，通过解耦任务特异性表征到两个互补的神经元子空间（分别调节神经元敏感性和输入适应性）来解决模型合并中的任务干扰问题。这种训练无关的融合框架在视觉和自然语言处理的多任务基准测试中表现优于现有方法，首次揭示了神经元对齐在模型合并中的关键作用，为知识融合提供了新思路。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05320v1",
      "published_date": "2025-03-07 11:00:24 UTC",
      "updated_date": "2025-03-07 11:00:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:00:17.071454"
    },
    {
      "arxiv_id": "2503.05319v1",
      "title": "Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation",
      "title_zh": "基于解耦表征的鲁棒多模态学习在眼科疾病分级中的应用",
      "authors": [
        "Xinkun Wang",
        "Yifang Wang",
        "Senwei Liang",
        "Feilong Tang",
        "Chengzhi Liu",
        "Ming Hu",
        "Chao Hu",
        "Junjun He",
        "Zongyuan Ge",
        "Imran Razzak"
      ],
      "abstract": "This paper discusses how ophthalmologists often rely on multimodal data to\nimprove diagnostic accuracy. However, complete multimodal data is rare in\nreal-world applications due to a lack of medical equipment and concerns about\ndata privacy. Traditional deep learning methods typically address these issues\nby learning representations in latent space. However, the paper highlights two\nkey limitations of these approaches: (i) Task-irrelevant redundant information\n(e.g., numerous slices) in complex modalities leads to significant redundancy\nin latent space representations. (ii) Overlapping multimodal representations\nmake it difficult to extract unique features for each modality. To overcome\nthese challenges, the authors propose the Essence-Point and Disentangle\nRepresentation Learning (EDRL) strategy, which integrates a self-distillation\nmechanism into an end-to-end framework to enhance feature selection and\ndisentanglement for more robust multimodal learning. Specifically, the\nEssence-Point Representation Learning module selects discriminative features\nthat improve disease grading performance. The Disentangled Representation\nLearning module separates multimodal data into modality-common and\nmodality-unique representations, reducing feature entanglement and enhancing\nboth robustness and interpretability in ophthalmic disease diagnosis.\nExperiments on multimodal ophthalmology datasets show that the proposed EDRL\nstrategy significantly outperforms current state-of-the-art methods.",
      "tldr_zh": "该研究提出了一种名为Essence-Point and Disentangle Representation Learning (EDRL)的策略，用于解决眼科疾病分级中多模态数据冗余和特征纠缠的问题。EDRL通过自蒸馏机制和端到端框架，结合Essence-Point Representation Learning模块选择判别性特征，并利用Disentangled Representation Learning模块将多模态数据分离为模态共有和模态独有表示，从而提升诊断的鲁棒性和可解释性。实验表明，EDRL在多模态眼科数据集上显著优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05319v1",
      "published_date": "2025-03-07 10:58:38 UTC",
      "updated_date": "2025-03-07 10:58:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:59:50.205656"
    },
    {
      "arxiv_id": "2503.05318v1",
      "title": "Uncertainty-Aware Decoding with Minimum Bayes Risk",
      "title_zh": "不确定性感知解码与最小贝叶斯风险",
      "authors": [
        "Nico Daheim",
        "Clara Meister",
        "Thomas Möllenhoff",
        "Iryna Gurevych"
      ],
      "abstract": "Despite their outstanding performance in the majority of scenarios,\ncontemporary language models still occasionally generate undesirable outputs,\nfor example, hallucinated text. While such behaviors have previously been\nlinked to uncertainty, there is a notable lack of methods that actively\nconsider uncertainty during text generation. In this work, we show how Minimum\nBayes Risk (MBR) decoding, which selects model generations according to an\nexpected risk, can be generalized into a principled uncertainty-aware decoding\nmethod. In short, we account for model uncertainty during decoding by\nincorporating a posterior over model parameters into MBR's computation of\nexpected risk. We show that this modified expected risk is useful for both\nchoosing outputs and deciding when to abstain from generation and can provide\nimprovements without incurring overhead. We benchmark different methods for\nlearning posteriors and show that performance improves with prediction\ndiversity. We release our code publicly.",
      "tldr_zh": "该论文提出了一种基于最小贝叶斯风险（Minimum Bayes Risk, MBR）的不确定性感知解码方法，用于解决语言模型生成过程中可能出现的幻觉文本等问题。通过将模型参数的后验分布整合到MBR的风险计算中，该方法能够在解码时主动考虑模型的不确定性，从而选择更可靠的输出或决定何时放弃生成。实验表明，该方法无需额外计算开销即可提升性能，且预测多样性越强效果越好。作者公开了相关代码。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025 (Poster)",
      "pdf_url": "http://arxiv.org/pdf/2503.05318v1",
      "published_date": "2025-03-07 10:55:12 UTC",
      "updated_date": "2025-03-07 10:55:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T03:59:54.436925"
    },
    {
      "arxiv_id": "2503.10652v1",
      "title": "Evaluating Local and Cloud-Based Large Language Models for Simulating Consumer Choices in Energy Stated Preference Surveys",
      "title_zh": "评估本地与云端大语言模型在能源偏好调查中模拟消费者选择的能力",
      "authors": [
        "Han Wang",
        "Jacek Pawlak",
        "Aruna Sivakumar"
      ],
      "abstract": "Survey research is essential in energy demand studies for capturing consumer\npreferences and informing policy decisions. Stated preference (SP) surveys, in\nparticular, analyse how individuals make trade-offs in hypothetical scenarios.\nHowever, traditional survey methods are costly, time-consuming, and affected by\nbiases and respondent fatigue. Large language models (LLMs) have emerged as a\npotential tool to address these challenges by generating human-like textual\nresponses. This study investigates the ability of LLMs to simulate consumer\nchoices in energy-related SP surveys. A series of test scenarios evaluated the\nsimulation performance of LLMs at both individual and aggregated levels,\nconsidering factors in the prompt, in-context learning (ICL), chain-of-thought\n(CoT) reasoning, the comparison between local and cloud-based LLMs, integration\nwith traditional choice models, and potential biases. Results indicate that\nwhile LLMs achieve an average accuracy of up to 48%, surpassing random\nguessing, their performance remains insufficient for practical application.\nLocal and cloud-based LLMs perform similarly in simulation accuracy but exhibit\ndifferences in adherence to prompt requirements and susceptibility to social\ndesirability biases. Findings suggest that previous SP choices are the most\neffective input factor, while longer prompts with varied factor formats may\nreduce accuracy. Furthermore, the traditional mixed logit choice model\noutperforms LLMs and provides insights for refining LLM prompts. Despite their\nlimitations, LLMs provide scalability and efficiency advantages, requiring\nminimal historical data compared to traditional survey methods. Future research\nshould refine prompt structures, further investigate CoT reasoning, and explore\nfine-tuning techniques to improve LLM-based energy survey simulations.",
      "tldr_zh": "该研究评估了本地和云端大语言模型(LLMs)在能源选择偏好(Stated Preference)调查中模拟消费者决策的能力。通过设计测试场景，研究发现LLMs的平均准确率可达48%，虽优于随机猜测但仍未达实用水平，且本地与云端LLMs表现相近但存在提示遵循度和社交期望偏差差异。结果表明混合logit模型仍优于LLMs，但LLMs具有可扩展性强、历史数据需求少等优势，未来需优化提示结构、链式推理(CoT)和微调技术以提升模拟效果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10652v1",
      "published_date": "2025-03-07 10:37:31 UTC",
      "updated_date": "2025-03-07 10:37:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:00:34.905904"
    },
    {
      "arxiv_id": "2503.05306v1",
      "title": "Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning",
      "title_zh": "基于对抗策略优化的离线偏好强化学习",
      "authors": [
        "Hyungkyu Kang",
        "Min-hwan Oh"
      ],
      "abstract": "In this paper, we study offline preference-based reinforcement learning\n(PbRL), where learning is based on pre-collected preference feedback over pairs\nof trajectories. While offline PbRL has demonstrated remarkable empirical\nsuccess, existing theoretical approaches face challenges in ensuring\nconservatism under uncertainty, requiring computationally intractable\nconfidence set constructions. We address this limitation by proposing\nAdversarial Preference-based Policy Optimization (APPO), a computationally\nefficient algorithm for offline PbRL that guarantees sample complexity bounds\nwithout relying on explicit confidence sets. By framing PbRL as a two-player\ngame between a policy and a model, our approach enforces conservatism in a\ntractable manner. Using standard assumptions on function approximation and\nbounded trajectory concentrability, we derive a sample complexity bound. To our\nknowledge, APPO is the first offline PbRL algorithm to offer both statistical\nefficiency and practical applicability. Experimental results on continuous\ncontrol tasks demonstrate that APPO effectively learns from complex datasets,\nshowing comparable performance with existing state-of-the-art methods.",
      "tldr_zh": "本文提出了对抗性偏好策略优化(APPO)，一种用于离线偏好强化学习(PbRL)的高效算法。该算法通过将PbRL建模为策略和模型之间的双人博弈，以可计算的方式确保保守性，避免了传统方法中计算复杂的置信集构建。在标准函数近似和轨迹集中性假设下，APPO首次实现了统计效率和实际适用性的平衡。实验表明，APPO在连续控制任务中能够有效地从复杂数据集中学习，性能与现有最先进方法相当。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05306v1",
      "published_date": "2025-03-07 10:35:01 UTC",
      "updated_date": "2025-03-07 10:35:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:00:11.437310"
    },
    {
      "arxiv_id": "2503.05305v1",
      "title": "Frequency Autoregressive Image Generation with Continuous Tokens",
      "title_zh": "基于连续令牌的频率自回归图像生成",
      "authors": [
        "Hu Yu",
        "Hao Luo",
        "Hangjie Yuan",
        "Yu Rong",
        "Feng Zhao"
      ],
      "abstract": "Autoregressive (AR) models for image generation typically adopt a two-stage\nparadigm of vector quantization and raster-scan ``next-token prediction\",\ninspired by its great success in language modeling. However, due to the huge\nmodality gap, image autoregressive models may require a systematic reevaluation\nfrom two perspectives: tokenizer format and regression direction. In this\npaper, we introduce the frequency progressive autoregressive (\\textbf{FAR})\nparadigm and instantiate FAR with the continuous tokenizer. Specifically, we\nidentify spectral dependency as the desirable regression direction for FAR,\nwherein higher-frequency components build upon the lower one to progressively\nconstruct a complete image. This design seamlessly fits the causality\nrequirement for autoregressive models and preserves the unique spatial locality\nof image data. Besides, we delve into the integration of FAR and the continuous\ntokenizer, introducing a series of techniques to address optimization\nchallenges and improve the efficiency of training and inference processes. We\ndemonstrate the efficacy of FAR through comprehensive experiments on the\nImageNet dataset and verify its potential on text-to-image generation.",
      "tldr_zh": "该研究提出了一种名为FAR（频率渐进自回归）的新型图像生成范式，通过连续tokenizer在频域空间实现自回归建模。与传统的向量量化和光栅扫描方法不同，FAR采用频谱依赖（spectral dependency）作为回归方向，使高频分量基于低频分量渐进构建完整图像，既满足自回归模型的因果性要求，又保留了图像数据的空间局部特性。研究还开发了针对连续tokenizer的优化技术，在ImageNet数据集上验证了该方法的有效性，并展示了在文本到图像生成任务中的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05305v1",
      "published_date": "2025-03-07 10:34:04 UTC",
      "updated_date": "2025-03-07 10:34:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:01:05.284973"
    },
    {
      "arxiv_id": "2503.05274v1",
      "title": "Evidential Uncertainty Estimation for Multi-Modal Trajectory Prediction",
      "title_zh": "多模态轨迹预测中的证据不确定性估计",
      "authors": [
        "Sajad Marvi",
        "Christoph Rist",
        "Julian Schmidt",
        "Julian Jordan",
        "Abhinav Valada"
      ],
      "abstract": "Accurate trajectory prediction is crucial for autonomous driving, yet\nuncertainty in agent behavior and perception noise makes it inherently\nchallenging. While multi-modal trajectory prediction models generate multiple\nplausible future paths with associated probabilities, effectively quantifying\nuncertainty remains an open problem. In this work, we propose a novel\nmulti-modal trajectory prediction approach based on evidential deep learning\nthat estimates both positional and mode probability uncertainty in real time.\nOur approach leverages a Normal Inverse Gamma distribution for positional\nuncertainty and a Dirichlet distribution for mode uncertainty. Unlike\nsampling-based methods, it infers both types of uncertainty in a single forward\npass, significantly improving efficiency. Additionally, we experimented with\nuncertainty-driven importance sampling to improve training efficiency by\nprioritizing underrepresented high-uncertainty samples over redundant ones. We\nperform extensive evaluations of our method on the Argoverse 1 and Argoverse 2\ndatasets, demonstrating that it provides reliable uncertainty estimates while\nmaintaining high trajectory prediction accuracy.",
      "tldr_zh": "本研究提出了一种基于证据深度学习(evidential deep learning)的多模态轨迹预测方法，可实时估计位置不确定性和模态概率不确定性。该方法创新性地采用Normal Inverse Gamma分布建模位置不确定性，Dirichlet分布建模模态不确定性，通过单次前向传播同时推断两种不确定性，显著提升了计算效率。实验在Argoverse 1和Argoverse 2数据集上验证了该方法在保持高预测精度的同时，能提供可靠的不确定性估计，并通过不确定性驱动的重要性采样策略优化了训练效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05274v1",
      "published_date": "2025-03-07 09:46:21 UTC",
      "updated_date": "2025-03-07 09:46:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:00:32.024264"
    },
    {
      "arxiv_id": "2503.05265v1",
      "title": "PhiloBERTA: A Transformer-Based Cross-Lingual Analysis of Greek and Latin Lexicons",
      "title_zh": "PhiloBERTA：基于Transformer的希腊语与拉丁语词典跨语言分析",
      "authors": [
        "Rumi A. Allbert",
        "Makai L. Allbert"
      ],
      "abstract": "We present PhiloBERTA, a cross-lingual transformer model that measures\nsemantic relationships between ancient Greek and Latin lexicons. Through\nanalysis of selected term pairs from classical texts, we use contextual\nembeddings and angular similarity metrics to identify precise semantic\nalignments. Our results show that etymologically related pairs demonstrate\nsignificantly higher similarity scores, particularly for abstract philosophical\nconcepts such as epist\\=em\\=e (scientia) and dikaiosyn\\=e (iustitia).\nStatistical analysis reveals consistent patterns in these relationships (p =\n0.012), with etymologically related pairs showing remarkably stable semantic\npreservation compared to control pairs. These findings establish a quantitative\nframework for examining how philosophical concepts moved between Greek and\nLatin traditions, offering new methods for classical philological research.",
      "tldr_zh": "该研究提出了PhiloBERTA，一个基于Transformer的跨语言分析模型，专门用于研究古希腊语和拉丁语词汇之间的语义关系。通过分析古典文本中的特定术语对，该模型利用上下文嵌入和角度相似性度量，发现词源相关的术语对（如epistēmē/scientia和dikaiosynē/iustitia）具有显著更高的语义相似性（p=0.012）。这些结果为研究哲学概念在希腊和拉丁传统间的传播建立了量化框架，为古典文献学研究提供了新方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05265v1",
      "published_date": "2025-03-07 09:30:16 UTC",
      "updated_date": "2025-03-07 09:30:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:02:39.243308"
    },
    {
      "arxiv_id": "2503.05264v1",
      "title": "Jailbreaking is (Mostly) Simpler Than You Think",
      "title_zh": "越狱攻击比你想象的（大多）更简单",
      "authors": [
        "Mark Russinovich",
        "Ahmed Salem"
      ],
      "abstract": "We introduce the Context Compliance Attack (CCA), a novel, optimization-free\nmethod for bypassing AI safety mechanisms. Unlike current approaches -- which\nrely on complex prompt engineering and computationally intensive optimization\n-- CCA exploits a fundamental architectural vulnerability inherent in many\ndeployed AI systems. By subtly manipulating conversation history, CCA convinces\nthe model to comply with a fabricated dialogue context, thereby triggering\nrestricted behavior. Our evaluation across a diverse set of open-source and\nproprietary models demonstrates that this simple attack can circumvent\nstate-of-the-art safety protocols. We discuss the implications of these\nfindings and propose practical mitigation strategies to fortify AI systems\nagainst such elementary yet effective adversarial tactics.",
      "tldr_zh": "这项研究提出了一种无需优化的新型AI安全绕过方法——上下文合规攻击（Context Compliance Attack, CCA）。该方法通过巧妙操纵对话历史记录，利用现有AI系统的架构漏洞，使模型误判当前对话背景从而触发受限行为。实验表明，这种简单攻击能有效突破最先进的安全协议，且无需传统方法所需的复杂提示工程或计算密集型优化。研究还讨论了相关防御策略，揭示了当前AI系统面对基础但高效对抗策略时的脆弱性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05264v1",
      "published_date": "2025-03-07 09:28:19 UTC",
      "updated_date": "2025-03-07 09:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:02:40.794624"
    },
    {
      "arxiv_id": "2503.05251v1",
      "title": "A Map-free Deep Learning-based Framework for Gate-to-Gate Monocular Visual Navigation aboard Miniaturized Aerial Vehicles",
      "title_zh": "基于无地图深度学习的微型飞行器门到门单目视觉导航框架",
      "authors": [
        "Lorenzo Scarciglia",
        "Antonio Paolillo",
        "Daniele Palossi"
      ],
      "abstract": "Palm-sized autonomous nano-drones, i.e., sub-50g in weight, recently entered\nthe drone racing scenario, where they are tasked to avoid obstacles and\nnavigate as fast as possible through gates. However, in contrast with their\nbigger counterparts, i.e., kg-scale drones, nano-drones expose three orders of\nmagnitude less onboard memory and compute power, demanding more efficient and\nlightweight vision-based pipelines to win the race. This work presents a\nmap-free vision-based (using only a monocular camera) autonomous nano-drone\nthat combines a real-time deep learning gate detection front-end with a classic\nyet elegant and effective visual servoing control back-end, only relying on\nonboard resources. Starting from two state-of-the-art tiny deep learning\nmodels, we adapt them for our specific task, and after a mixed\nsimulator-real-world training, we integrate and deploy them aboard our\nnano-drone. Our best-performing pipeline costs of only 24M multiply-accumulate\noperations per frame, resulting in a closed-loop control performance of 30 Hz,\nwhile achieving a gate detection root mean square error of 1.4 pixels, on our\n~20k real-world image dataset. In-field experiments highlight the capability of\nour nano-drone to successfully navigate through 15 gates in 4 min, never\ncrashing and covering a total travel distance of ~100m, with a peak flight\nspeed of 1.9 m/s. Finally, to stress the generalization capability of our\nsystem, we also test it in a never-seen-before environment, where it navigates\nthrough gates for more than 4 min.",
      "tldr_zh": "本研究提出了一种基于深度学习的无地图单目视觉导航框架，专为重量小于50克的微型无人机设计。该框架结合了实时深度学习门检测前端和经典高效的视觉伺服控制后端，完全依赖机载资源实现自主导航。通过优化微型深度学习模型并进行模拟-真实世界混合训练，系统在每帧仅需2400万次乘加运算的情况下，实现了30Hz的闭环控制性能，门检测均方根误差为1.4像素。实地实验表明，该无人机成功在4分钟内穿越15个门，飞行总距离约100米，最高速度达1.9米/秒，并在未见过的环境中展现了良好的泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "\\c{opyright}2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
      "pdf_url": "http://arxiv.org/pdf/2503.05251v1",
      "published_date": "2025-03-07 09:07:07 UTC",
      "updated_date": "2025-03-07 09:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:01:37.188498"
    },
    {
      "arxiv_id": "2503.05244v3",
      "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
      "title_zh": "WritingBench：生成式写作综合评测基准",
      "authors": [
        "Yuning Wu",
        "Jiahao Mei",
        "Ming Yan",
        "Chenliang Li",
        "Shaopeng Lai",
        "Yuran Ren",
        "Zijia Wang",
        "Ji Zhang",
        "Mengyue Wu",
        "Qin Jin",
        "Fei Huang"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
      "tldr_zh": "该研究提出了WritingBench，一个全面的生成写作基准，旨在评估大语言模型(LLMs)在6个核心写作领域和100个子领域中的表现，涵盖创意、说服、信息和科技写作。研究团队开发了一种基于查询的评估框架，使LLMs能够动态生成实例特定的评估标准，并结合微调的批评模型进行标准感知评分，评估风格、格式和长度。该框架的数据整理能力使7B参数模型接近SOTA性能，相关基准、评估工具和模块化框架组件已开源，以推动LLMs在写作领域的发展。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05244v3",
      "published_date": "2025-03-07 08:56:20 UTC",
      "updated_date": "2025-03-20 05:13:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:01:45.011628"
    },
    {
      "arxiv_id": "2503.05239v1",
      "title": "Robust Conformal Prediction with a Single Binary Certificate",
      "title_zh": "基于单一二元证书的鲁棒保形预测",
      "authors": [
        "Soroush H. Zargarbashi",
        "Aleksandar Bojchevski"
      ],
      "abstract": "Conformal prediction (CP) converts any model's output to prediction sets with\na guarantee to cover the true label with (adjustable) high probability. Robust\nCP extends this guarantee to worst-case (adversarial) inputs. Existing\nbaselines achieve robustness by bounding randomly smoothed conformity scores.\nIn practice, they need expensive Monte-Carlo (MC) sampling (e.g. $\\sim10^4$\nsamples per point) to maintain an acceptable set size. We propose a robust\nconformal prediction that produces smaller sets even with significantly lower\nMC samples (e.g. 150 for CIFAR10). Our approach binarizes samples with an\nadjustable (or automatically adjusted) threshold selected to preserve the\ncoverage guarantee. Remarkably, we prove that robustness can be achieved by\ncomputing only one binary certificate, unlike previous methods that certify\neach calibration (or test) point. Thus, our method is faster and returns\nsmaller robust sets. We also eliminate a previous limitation that requires a\nbounded score function.",
      "tldr_zh": "该研究提出了一种新型的鲁棒保形预测(Robust Conformal Prediction)方法，通过创新的二元认证机制显著提升了计算效率。与现有需要大量蒙特卡洛采样(约10^4次/点)的方法不同，该方法仅需计算单个二元认证(如CIFAR10上仅需150次采样)即可保证覆盖性，同时产生更小的预测集。研究还突破了传统方法对评分函数必须有界的限制，在保持统计保证的前提下实现了更快的计算速度和更紧凑的预测集输出。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05239v1",
      "published_date": "2025-03-07 08:41:53 UTC",
      "updated_date": "2025-03-07 08:41:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:01:54.555068"
    },
    {
      "arxiv_id": "2503.08700v1",
      "title": "Real-Time Semantic Segmentation of Aerial Images Using an Embedded U-Net: A Comparison of CPU, GPU, and FPGA Workflows",
      "title_zh": "基于嵌入式U-Net的航拍图像实时语义分割：CPU、GPU与FPGA工作流对比",
      "authors": [
        "Julien Posso",
        "Hugo Kieffer",
        "Nicolas Menga",
        "Omar Hlimi",
        "Sébastien Tarris",
        "Hubert Guerard",
        "Guy Bois",
        "Matthieu Couderc",
        "Eric Jenn"
      ],
      "abstract": "This study introduces a lightweight U-Net model optimized for real-time\nsemantic segmentation of aerial images, targeting the efficient utilization of\nCommercial Off-The-Shelf (COTS) embedded computing platforms. We maintain the\naccuracy of the U-Net on a real-world dataset while significantly reducing the\nmodel's parameters and Multiply-Accumulate (MAC) operations by a factor of 16.\nOur comprehensive analysis covers three hardware platforms (CPU, GPU, and FPGA)\nand five different toolchains (TVM, FINN, Vitis AI, TensorFlow GPU, and cuDNN),\nassessing each on metrics such as latency, power consumption, memory footprint,\nenergy efficiency, and FPGA resource usage. The results highlight the\ntrade-offs between these platforms and toolchains, with a particular focus on\nthe practical deployment challenges in real-world applications. Our findings\ndemonstrate that while the FPGA with Vitis AI emerges as the superior choice\ndue to its performance, energy efficiency, and maturity, it requires\nspecialized hardware knowledge, emphasizing the need for a balanced approach in\nselecting embedded computing solutions for semantic segmentation tasks",
      "tldr_zh": "本研究开发了一种轻量化的U-Net模型，用于航空图像的实时语义分割，特别优化了商用嵌入式平台(COTS)的计算效率。通过精简模型参数量和乘加运算(MAC)达16倍，同时保持真实数据集的精度。研究对比了CPU、GPU和FPGA三种硬件平台及五种工具链(TVM/FINN等)，发现采用Vitis AI的FPGA平台在性能、能效和成熟度方面表现最优，但其部署需要专业知识，突显了嵌入式语义分割解决方案选择时的平衡需求。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ERTS2024, Jun 2024, Toulouse, France",
      "pdf_url": "http://arxiv.org/pdf/2503.08700v1",
      "published_date": "2025-03-07 08:33:28 UTC",
      "updated_date": "2025-03-07 08:33:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:02:05.612410"
    },
    {
      "arxiv_id": "2503.05231v1",
      "title": "Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction",
      "title_zh": "Kaiwu：面向机器人学习与人机交互的多模态操作数据集与框架",
      "authors": [
        "Shuo Jiang",
        "Haonan Li",
        "Ruochen Ren",
        "Yanmin Zhou",
        "Zhipeng Wang",
        "Bin He"
      ],
      "abstract": "Cutting-edge robot learning techniques including foundation models and\nimitation learning from humans all pose huge demands on large-scale and\nhigh-quality datasets which constitute one of the bottleneck in the general\nintelligent robot fields. This paper presents the Kaiwu multimodal dataset to\naddress the missing real-world synchronized multimodal data problems in the\nsophisticated assembling scenario,especially with dynamics information and its\nfine-grained labelling. The dataset first provides an integration of\nhuman,environment and robot data collection framework with 20 subjects and 30\ninteraction objects resulting in totally 11,664 instances of integrated\nactions. For each of the demonstration,hand motions,operation pressures,sounds\nof the assembling process,multi-view videos, high-precision motion capture\ninformation,eye gaze with first-person videos,electromyography signals are all\nrecorded. Fine-grained multi-level annotation based on absolute timestamp,and\nsemantic segmentation labelling are performed. Kaiwu dataset aims to facilitate\nrobot learning,dexterous manipulation,human intention investigation and\nhuman-robot collaboration research.",
      "tldr_zh": "该研究提出了Kaiwu多模态数据集，旨在解决机器人学习领域缺乏真实世界同步多模态数据的问题，特别是在复杂装配场景中。Kaiwu数据集整合了人类、环境和机器人数据采集框架，包含20名受试者和30个交互对象，共记录11,664个集成动作实例，涵盖了手部运动、操作压力、装配过程声音、多视角视频、高精度运动捕捉信息、眼动追踪和肌电信号等。数据集提供了基于绝对时间戳的细粒度多级标注和语义分割标注，以促进机器人学习、灵巧操作、人类意图研究和人机协作等领域的研究。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05231v1",
      "published_date": "2025-03-07 08:28:24 UTC",
      "updated_date": "2025-03-07 08:28:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:02:06.876803"
    },
    {
      "arxiv_id": "2503.05229v1",
      "title": "Discrete Contrastive Learning for Diffusion Policies in Autonomous Driving",
      "title_zh": "离散对比学习在自动驾驶扩散策略中的应用",
      "authors": [
        "Kalle Kujanpää",
        "Daulet Baimukashev",
        "Farzeen Munir",
        "Shoaib Azam",
        "Tomasz Piotr Kucner",
        "Joni Pajarinen",
        "Ville Kyrki"
      ],
      "abstract": "Learning to perform accurate and rich simulations of human driving behaviors\nfrom data for autonomous vehicle testing remains challenging due to human\ndriving styles' high diversity and variance. We address this challenge by\nproposing a novel approach that leverages contrastive learning to extract a\ndictionary of driving styles from pre-existing human driving data. We\ndiscretize these styles with quantization, and the styles are used to learn a\nconditional diffusion policy for simulating human drivers. Our empirical\nevaluation confirms that the behaviors generated by our approach are both safer\nand more human-like than those of the machine-learning-based baseline methods.\nWe believe this has the potential to enable higher realism and more effective\ntechniques for evaluating and improving the performance of autonomous vehicles.",
      "tldr_zh": "该研究提出了一种基于离散对比学习（Discrete Contrastive Learning）的扩散策略（Diffusion Policies）方法，用于自动驾驶中人类驾驶行为的仿真模拟。通过对比学习从人类驾驶数据中提取多样化的驾驶风格字典，并采用量化（quantization）技术将其离散化，进而训练条件扩散策略模型。实验表明，相比传统机器学习方法，该方法生成的驾驶行为更安全且更接近人类驾驶风格，为自动驾驶系统的测试与优化提供了更真实有效的仿真技术。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05229v1",
      "published_date": "2025-03-07 08:26:04 UTC",
      "updated_date": "2025-03-07 08:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:02:16.430607"
    },
    {
      "arxiv_id": "2503.05227v1",
      "title": "MOHPER: Multi-objective Hyperparameter Optimization Framework for E-commerce Retrieval System",
      "title_zh": "MOHPER：面向电商检索系统的多目标超参数优化框架",
      "authors": [
        "Jungbae Park",
        "Heonseok Jang"
      ],
      "abstract": "E-commerce search optimization has evolved to include a wider range of\nmetrics that reflect user engagement and business objectives. Modern search\nframeworks now incorporate advanced quality features, such as sales counts and\ndocument-query relevance, to better align search results with these goals.\nTraditional methods typically focus on click-through rate (CTR) as a measure of\nengagement or relevance, but this can miss true purchase intent, creating a gap\nbetween user interest and actual conversions. Joint training with the\nclick-through conversion rate (CTCVR) has become essential for understanding\nbuying behavior, although its sparsity poses challenges for reliable\noptimization. This study presents MOHPER, a Multi-Objective Hyperparameter\nOptimization framework for E-commerce Retrieval systems. Utilizing Bayesian\noptimization and sampling, it jointly optimizes both CTR, CTCVR, and relevant\nobjectives, focusing on engagement and conversion of the users. In addition, to\nimprove the selection of the best configuration from multi-objective\noptimization, we suggest advanced methods for hyperparameter selection,\nincluding a meta-configuration voting strategy and a cumulative training\napproach that leverages prior optimal configurations, to improve speeds of\ntraining and efficiency. Currently deployed in a live setting, our proposed\nframework substantiates its practical efficacy in achieving a balanced\noptimization that aligns with both user satisfaction and revenue goals.",
      "tldr_zh": "该研究提出了MOHPER框架，一种用于电商检索系统的多目标超参数优化方法。MOHPER利用贝叶斯优化和采样技术，同时优化点击率(CTR)和点击转化率(CTCVR)，以更好地平衡用户参与度和转化率。此外，研究还提出了元配置投票策略和累积训练方法，以提高超参数选择的效率和训练速度。该框架已在实际场景中部署，验证了其在提升用户满意度和收入目标方面的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05227v1",
      "published_date": "2025-03-07 08:25:08 UTC",
      "updated_date": "2025-03-07 08:25:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:02:26.221692"
    },
    {
      "arxiv_id": "2503.05226v1",
      "title": "Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments",
      "title_zh": "奖励中心化ReST-MCTS：面向高不确定性环境机器人操作的鲁棒决策框架",
      "authors": [
        "Xibai Wang"
      ],
      "abstract": "Monte Carlo Tree Search (MCTS) has emerged as a powerful tool for\ndecision-making in robotics, enabling efficient exploration of large search\nspaces. However, traditional MCTS methods struggle in environments\ncharacterized by high uncertainty and noisy data due to their reliance on\nfinal-step reward evaluation. The lack of intermediate feedback during search\noften results in suboptimal decision-making and computational inefficiencies.\n  This paper introduces Reward-Centered ReST-MCTS, a novel framework that\nenhances MCTS by incorporating intermediate reward shaping. The core of our\napproach is the Rewarding Center, which refines search trajectories by\ndynamically assigning partial rewards using rule-based validation, heuristic\nguidance, and neural estimation. By integrating these mechanisms, our method\nenables real-time optimization of search paths, mitigating the effects of error\npropagation.\n  We evaluate Reward-Centered ReST-MCTS in robotic manipulation tasks under\nhigh uncertainty, demonstrating consistent improvements in decision accuracy.\nCompared to baseline methods, including Chain-of-Thought (CoT) prompting and\nVanilla ReST-MCTS, our framework achieves a 2-4% accuracy improvement while\nmaintaining computational feasibility. Ablation studies confirm the\neffectiveness of intermediate feedback in search refinement, particularly in\npruning incorrect decision paths early. Furthermore, robustness tests show that\nour method retains high performance across varying levels of uncertainty.",
      "tldr_zh": "本研究提出了Reward-Centered ReST-MCTS框架，用于在高不确定性和噪声环境下优化机器人操作决策。该框架通过引入Rewarding Center机制，结合规则验证、启发式引导和神经估计，动态分配中间奖励，从而实时优化搜索路径并减少误差传播。实验表明，相比传统MCTS和Chain-of-Thought等方法，该框架在决策准确性上提升了2-4%，同时保持了计算效率，并在不同不确定性水平下表现出较强的鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05226v1",
      "published_date": "2025-03-07 08:25:04 UTC",
      "updated_date": "2025-03-07 08:25:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:02:37.881884"
    },
    {
      "arxiv_id": "2503.05224v1",
      "title": "Deep Sequence Models for Predicting Average Shear Wave Velocity from Strong Motion Records",
      "title_zh": "基于深度序列模型的强震记录平均剪切波速度预测",
      "authors": [
        "Baris Yilmaz",
        "Erdem Akagündüz",
        "Salih Tileylioglu"
      ],
      "abstract": "This study explores the use of deep learning for predicting the time averaged\nshear wave velocity in the top 30 m of the subsurface ($V_{s30}$) at strong\nmotion recording stations in T\\\"urkiye. $V_{s30}$ is a key parameter in site\ncharacterization and, as a result for seismic hazard assessment. However, it is\noften unavailable due to the lack of direct measurements and is therefore\nestimated using empirical correlations. Such correlations however are commonly\ninadequate in capturing complex, site-specific variability and this motivates\nthe need for data-driven approaches. In this study, we employ a hybrid deep\nlearning model combining convolutional neural networks (CNNs) and long\nshort-term memory (LSTM) networks to capture both spatial and temporal\ndependencies in strong motion records. Furthermore, we explore how using\ndifferent parts of the signal influence our deep learning model. Our results\nsuggest that the hybrid approach effectively learns complex, nonlinear\nrelationships within seismic signals. We observed that an improved P-wave\narrival time model increased the prediction accuracy of $V_{s30}$. We believe\nthe study provides valuable insights into improving $V_{s30}$ predictions using\na CNN-LSTM framework, demonstrating its potential for improving site\ncharacterization for seismic studies. Our codes are available via this repo:\nhttps://github.com/brsylmz23/CNNLSTM_DeepEQ",
      "tldr_zh": "本研究提出了一种结合卷积神经网络(CNN)和长短期记忆网络(LSTM)的混合深度学习模型，用于从强震记录中预测地表下30米范围内的平均剪切波速度($V_{s30}$)。该模型能够捕捉地震信号中的空间和时间依赖性，显著提高了$V_{s30}$的预测精度，尤其是在改进P波到达时间模型后。研究结果为利用CNN-LSTM框架改进地震场地特征描述提供了重要见解，相关代码已在GitHub上开源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05224v1",
      "published_date": "2025-03-07 08:22:50 UTC",
      "updated_date": "2025-03-07 08:22:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:02:52.632372"
    },
    {
      "arxiv_id": "2503.05212v1",
      "title": "Knowledge Updating? No More Model Editing! Just Selective Contextual Reasoning",
      "title_zh": "知识更新？无需模型编辑！仅需选择性上下文推理",
      "authors": [
        "Guoxiu He",
        "Xin Song",
        "Aixin Sun"
      ],
      "abstract": "As real-world knowledge evolves, the information embedded within large\nlanguage models (LLMs) can become outdated, inadequate, or erroneous. Model\nediting has emerged as a prominent approach for updating LLMs' knowledge with\nminimal computational costs and parameter changes. This approach typically\nidentifies and adjusts specific model parameters associated with newly acquired\nknowledge. However, existing methods often underestimate the adverse effects\nthat parameter modifications can have on broadly distributed knowledge. More\ncritically, post-edit LLMs frequently struggle with multi-hop reasoning and\ncontinuous knowledge updates. Although various studies have discussed these\nshortcomings, there is a lack of comprehensive evaluation. In this paper, we\nprovide an evaluation of ten model editing methods along four dimensions:\nreliability, generalization, locality, and portability. Results confirm that\nall ten popular model editing methods show significant shortcomings across\nmultiple dimensions, suggesting model editing is less promising. We then\npropose a straightforward method called Selective Contextual Reasoning (SCR),\nfor knowledge updating. SCR does not modify model parameters but harnesses\nLLM's inherent contextual reasoning capabilities utilizing the updated\nknowledge pieces. Under SCR, an LLM first assesses whether an incoming query\nfalls within the scope of an external knowledge base. If it does, the relevant\nexternal knowledge texts are contextualized to enhance reasoning; otherwise,\nthe query is answered directly. We evaluate SCR against the ten model editing\nmethods on two counterfactual datasets with three backbone LLMs. Empirical\nresults confirm the effectiveness and efficiency of contextual reasoning for\nknowledge updating.",
      "tldr_zh": "本研究提出了一种无需模型编辑的知识更新方法——选择性上下文推理(Selective Contextual Reasoning, SCR)，通过利用大语言模型(LLMs)固有的上下文推理能力来整合外部知识。SCR首先判断查询是否属于外部知识库范围，若属于则结合相关知识进行推理，否则直接回答查询。实验在三个骨干LLM和两个反事实数据集上对比了十种模型编辑方法，结果表明SCR在可靠性、泛化性、局部性和可移植性方面均优于模型编辑方法，为知识更新提供了一种更有效和高效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05212v1",
      "published_date": "2025-03-07 08:04:25 UTC",
      "updated_date": "2025-03-07 08:04:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:03:06.575590"
    },
    {
      "arxiv_id": "2503.07655v1",
      "title": "GraphT5: Unified Molecular Graph-Language Modeling via Multi-Modal Cross-Token Attention",
      "title_zh": "GraphT5：通过多模态跨标记注意力实现统一的分子图-语言建模",
      "authors": [
        "Sangyeup Kim",
        "Nayeon Kim",
        "Yinhua Piao",
        "Sun Kim"
      ],
      "abstract": "Molecular language modeling tasks such as molecule captioning have been\nrecognized for their potential to further understand molecular properties that\ncan aid drug discovery or material synthesis based on chemical reactions.\nUnlike the common use of molecule graphs in predicting molecular properties,\nmost methods in molecular language modeling rely heavily on SMILES sequences.\nThis preference is because the task involves generating a sequence of multiple\ntokens using transformer-based models. Therefore, a main challenge is\ndetermining how to integrate graph data, which contains structural and spatial\ninformation about molecules, with text data. In addition, simply using both 1D\nSMILES text and 2D graph as inputs without addressing how they align and\nrepresent the molecule structure in different modalities makes it challenging\nto fully utilize structural knowledge about molecules. To this end, we propose\nGraphT5, a multi-modal framework that integrates 1D SMILES text and 2D graph\nrepresentations of molecules for molecular language modeling. Specifically, we\nintroduce a novel cross-token attention module in GraphT5 to bridge the gap\narising from the fundamental differences between the two modalities of molecule\nrepresentations. Cross-token attention exploits implicit information between\nSMILES and graphs of molecules, resulting from their interactions at a\nfine-grained token level that benefits molecular language modeling. Extensive\nexperiments including molecule captioning, IUPAC name prediction tasks, and\ncase studies show that our GraphT5 outperforms the latest baseline approaches,\nwhich validates the effectiveness of our GraphT5 in sufficiently utilizing 1D\nSMILES text and 2D graph representations.",
      "tldr_zh": "该研究提出了GraphT5，一种多模态框架，通过新颖的跨模态注意力机制(cross-token attention)将1D SMILES文本和2D分子图表示相结合，用于分子语言建模。该方法解决了现有方法依赖单一SMILES序列而无法充分利用分子结构信息的问题，通过细粒度地挖掘SMILES与分子图之间的隐含关系，提升了分子语言建模的性能。实验表明，GraphT5在分子描述生成、IUPAC命名预测等任务上优于最新的基线方法，验证了其有效整合多模态分子表示的能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07655v1",
      "published_date": "2025-03-07 07:57:16 UTC",
      "updated_date": "2025-03-07 07:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:03:16.958986"
    },
    {
      "arxiv_id": "2503.05207v1",
      "title": "Policy Constraint by Only Support Constraint for Offline Reinforcement Learning",
      "title_zh": "仅支持约束下的策略约束用于离线强化学习",
      "authors": [
        "Yunkai Gao",
        "Jiaming Guo",
        "Fan Wu",
        "Rui Zhang"
      ],
      "abstract": "Offline reinforcement learning (RL) aims to optimize a policy by using\npre-collected datasets, to maximize cumulative rewards. However, offline\nreinforcement learning suffers challenges due to the distributional shift\nbetween the learned and behavior policies, leading to errors when computing\nQ-values for out-of-distribution (OOD) actions. To mitigate this issue, policy\nconstraint methods aim to constrain the learned policy's distribution with the\ndistribution of the behavior policy or confine action selection within the\nsupport of the behavior policy. However, current policy constraint methods tend\nto exhibit excessive conservatism, hindering the policy from further surpassing\nthe behavior policy's performance. In this work, we present Only Support\nConstraint (OSC) which is derived from maximizing the total probability of\nlearned policy in the support of behavior policy, to address the conservatism\nof policy constraint. OSC presents a regularization term that only restricts\npolicies to the support without imposing extra constraints on actions within\nthe support. Additionally, to fully harness the performance of the new policy\nconstraints, OSC utilizes a diffusion model to effectively characterize the\nsupport of behavior policies. Experimental evaluations across a variety of\noffline RL benchmarks demonstrate that OSC significantly enhances performance,\nalleviating the challenges associated with distributional shifts and mitigating\nconservatism of policy constraints. Code is available at\nhttps://github.com/MoreanP/OSC.",
      "tldr_zh": "该论文提出了一种名为\"仅支持约束\"(Only Support Constraint, OSC)的新方法，用于解决离线强化学习中的分布偏移问题。与现有方法不同，OSC仅将学习策略限制在行为策略的支持范围内，而不对支持范围内的动作施加额外约束，从而避免了过度保守问题。该方法采用扩散模型有效表征行为策略的支持范围，并在多个离线强化学习基准测试中显著提升了性能表现。实验结果表明，OSC既能有效缓解分布偏移问题，又保留了策略超越行为策略性能的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05207v1",
      "published_date": "2025-03-07 07:55:51 UTC",
      "updated_date": "2025-03-07 07:55:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:03:30.048588"
    },
    {
      "arxiv_id": "2503.05203v1",
      "title": "Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation",
      "title_zh": "路径池化：面向高效知识图谱检索增强生成的无训练结构增强方法",
      "authors": [
        "Hairu Wang",
        "Yuan Feng",
        "Xike Xie",
        "S Kevin Zhou"
      ],
      "abstract": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, train-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost. Code is coming soon at\nhttps://github.com/hrwang00/path-pooling.",
      "tldr_zh": "本研究提出了路径池化（Path Pooling），一种无需训练的图结构增强策略，用于提升基于知识图谱的检索增强生成（KG-RAG）方法的性能。该方法通过路径中心池化操作引入结构信息，以低计算成本有效利用知识图谱中的信息，解决了现有方法结构信息利用不足或计算成本高的问题。实验表明，将路径池化集成到最先进的KG-RAG方法中，能在多种场景下显著提升性能，同时几乎不增加额外开销。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05203v1",
      "published_date": "2025-03-07 07:48:30 UTC",
      "updated_date": "2025-03-07 07:48:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:03:34.600411"
    },
    {
      "arxiv_id": "2503.05201v1",
      "title": "Deep Muscle EMG construction using A Physics-Integrated Deep Learning approach",
      "title_zh": "采用物理集成深度学习方法构建深层肌肉肌电信号",
      "authors": [
        "Rajnish Kumar",
        "Tapas Tripura",
        "Souvik Chakraborty",
        "Sitikantha Roy"
      ],
      "abstract": "Electromyography (EMG)--based computational musculoskeletal modeling is a\nnon-invasive method for studying musculotendon function, human movement, and\nneuromuscular control, providing estimates of internal variables like muscle\nforces and joint torques. However, EMG signals from deeper muscles are often\nchallenging to measure by placing the surface EMG electrodes and unfeasible to\nmeasure directly using invasive methods. The restriction to the access of EMG\ndata from deeper muscles poses a considerable obstacle to the broad adoption of\nEMG-driven modeling techniques. A strategic alternative is to use an estimation\nalgorithm to approximate the missing EMG signals from deeper muscle. A similar\nstrategy is used in physics-informed deep learning, where the features of\nphysical systems are learned without labeled data. In this work, we propose a\nhybrid deep learning algorithm, namely the neural musculoskeletal model (NMM),\nthat integrates physics-informed and data-driven deep learning to approximate\nthe EMG signals from the deeper muscles. While data-driven modeling is used to\npredict the missing EMG signals, physics-based modeling engraves the\nsubject-specific information into the predictions. Experimental verifications\non five test subjects are carried out to investigate the performance of the\nproposed hybrid framework. The proposed NMM is validated against the joint\ntorque computed from 'OpenSim' software. The predicted deep EMG signals are\nalso compared against the state-of-the-art muscle synergy extrapolation (MSE)\napproach, where the proposed NMM completely outperforms the existing MSE\nframework by a significant margin.",
      "tldr_zh": "本研究提出了一种结合物理模型与深度学习的混合方法——神经肌肉骨骼模型(NMM)，用于构建深层肌肉的肌电信号(EMG)。该方法通过数据驱动建模预测缺失的深层肌肉EMG信号，同时利用基于物理的建模将个体特异性信息融入预测结果。实验在五名受试者上进行验证，结果表明NMM不仅与OpenSim软件计算的关节扭矩高度吻合，而且在深层EMG信号预测上显著优于现有的肌肉协同外推(MSE)方法。该研究为克服深层肌肉EMG测量难题提供了创新解决方案，推动了EMG驱动建模技术的广泛应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05201v1",
      "published_date": "2025-03-07 07:46:26 UTC",
      "updated_date": "2025-03-07 07:46:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:03:42.212928"
    },
    {
      "arxiv_id": "2503.05200v1",
      "title": "ORANSight-2.0: Foundational LLMs for O-RAN",
      "title_zh": "ORANSight-2.0：面向O-RAN的基础大语言模型",
      "authors": [
        "Pranshav Gajjar",
        "Vijay K. Shah"
      ],
      "abstract": "Despite the transformative impact of Large Language Models (LLMs) across\ncritical domains such as healthcare, customer service, and business marketing,\ntheir integration into Open Radio Access Networks (O-RAN) remains limited. This\ngap is primarily due to the absence of domain-specific foundational models,\nwith existing solutions often relying on general-purpose LLMs that fail to\naddress the unique challenges and technical intricacies of O-RAN. To bridge\nthis gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative\naimed at developing specialized foundational LLMs tailored for O-RAN. Built on\n18 LLMs spanning five open-source LLM frameworks, ORANSight-2.0 fine-tunes\nmodels ranging from 1 to 70B parameters, significantly reducing reliance on\nproprietary, closed-source models while enhancing performance for O-RAN. At the\ncore of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation\n(RAG) based instruction-tuning framework that employs two LLM agents to create\nhigh-quality instruction-tuning datasets. The generated dataset is then used to\nfine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluate\nORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for code\ngeneration and codebase understanding in the context of srsRAN, a widely used\n5G O-RAN stack. We also leverage ORANBench13K, an existing benchmark for\nassessing O-RAN-specific knowledge. Our comprehensive evaluations demonstrate\nthat ORANSight-2.0 models outperform general-purpose and closed-source models,\nsuch as ChatGPT-4o and Gemini, by 5.421% on ORANBench and 18.465% on\nsrsRANBench, achieving superior performance while maintaining lower\ncomputational and energy costs. We also experiment with RAG-augmented variants\nof ORANSight-2.0 LLMs and thoroughly evaluate their energy characteristics,\ndemonstrating costs for training, standard inference, and RAG-augmented\ninference.",
      "tldr_zh": "该研究推出了ORANSight-2.0，这是首个专为开放式无线接入网(O-RAN)定制的基础大语言模型(LLM)系统。基于18个开源LLM框架模型(参数量1B-70B)，研究团队创新性地开发了RANSTRUCT框架——采用双智能体机制和检索增强生成(RAG)技术来自动构建高质量指令微调数据集。通过引入srsRANBench新基准测试，该系统在O-RAN特定知识评估中较ChatGPT-4o等通用模型提升5.42%，在5G代码理解任务上提升18.47%，同时显著降低计算能耗。研究还系统评估了不同推理模式下的能耗特性，为O-RAN智能化提供了高效可用的基础模型解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05200v1",
      "published_date": "2025-03-07 07:44:31 UTC",
      "updated_date": "2025-03-07 07:44:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:03:54.047799"
    },
    {
      "arxiv_id": "2503.05194v1",
      "title": "Uncertainty-Aware Explainable Federated Learning",
      "title_zh": "不确定性感知的可解释联邦学习",
      "authors": [
        "Yanci Zhang",
        "Han Yu"
      ],
      "abstract": "Federated Learning (FL) is a collaborative machine learning paradigm for\nenhancing data privacy preservation. Its privacy-preserving nature complicates\nthe explanation of the decision-making processes and the evaluation of the\nreliability of the generated explanations. In this paper, we propose the\nUncertainty-aware eXplainable Federated Learning (UncertainXFL) to address\nthese challenges. It generates explanations for decision-making processes under\nFL settings and provides information regarding the uncertainty of these\nexplanations. UncertainXFL is the first framework to explicitly offer\nuncertainty evaluation for explanations within the FL context. Explanatory\ninformation is initially generated by the FL clients and then aggregated by the\nserver in a comprehensive and conflict-free manner during FL training. The\nquality of the explanations, including the uncertainty score and tested\nvalidity, guides the FL training process by prioritizing clients with the most\nreliable explanations through higher weights during model aggregation.\nExtensive experimental evaluation results demonstrate that UncertainXFL\nachieves superior model accuracy and explanation accuracy, surpassing the\ncurrent state-of-the-art model that does not incorporate uncertainty\ninformation by 2.71% and 1.77%, respectively. By integrating and quantifying\nuncertainty in the data into the explanation process, UncertainXFL not only\nclearly presents the explanation alongside its uncertainty, but also leverages\nthis uncertainty to guide the FL training process, thereby enhancing the\nrobustness and reliability of the resulting models.",
      "tldr_zh": "本研究提出了UncertainXFL，一种不确定性感知的可解释联邦学习(FL)框架，旨在解决FL中决策过程解释和解释可靠性评估的难题。该框架首次在FL环境中明确提供解释的不确定性评估，通过在FL训练过程中生成并聚合解释信息，并利用不确定性评分和有效性测试指导FL训练，优先选择解释最可靠的客户端。实验表明，UncertainXFL在模型准确率和解释准确率上分别比现有最优模型提高了2.71%和1.77%，增强了模型的鲁棒性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05194v1",
      "published_date": "2025-03-07 07:29:48 UTC",
      "updated_date": "2025-03-07 07:29:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:03:56.850212"
    },
    {
      "arxiv_id": "2503.05188v1",
      "title": "Rewarding Curse: Analyze and Mitigate Reward Modeling Issues for LLM Reasoning",
      "title_zh": "奖励的诅咒：分析与缓解大语言模型推理中的奖励建模问题",
      "authors": [
        "Jiachun Li",
        "Pengfei Cao",
        "Yubo Chen",
        "Jiexin Xu",
        "Huaijun Li",
        "Xiaojian Jiang",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT.",
      "tldr_zh": "本文从有效性和忠实性两个角度深入分析了链式思维推理(Chain-of-Thought, CoT)在不同任务中的表现。研究发现，CoT的有效性受问题难度、信息增益和信息流等关键因素影响，而CoT的不忠实问题则源于大语言模型(LLM)在预测答案时能够从问题中回忆起CoT中缺失的正确信息。为解决这一问题，作者提出了一种新算法，通过从问题中提取额外信息来增强CoT生成，并基于信息增益评估CoT。实验表明，该方法显著提升了CoT的忠实性和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 21 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05188v1",
      "published_date": "2025-03-07 07:20:24 UTC",
      "updated_date": "2025-03-07 07:20:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:04:02.246025"
    },
    {
      "arxiv_id": "2503.05185v1",
      "title": "FinTMMBench: Benchmarking Temporal-Aware Multi-Modal RAG in Finance",
      "title_zh": "FinTMMBench：面向金融领域的时序感知多模态RAG基准测试",
      "authors": [
        "Fengbin Zhu",
        "Junfeng Li",
        "Liangming Pan",
        "Wenjie Wang",
        "Fuli Feng",
        "Chao Wang",
        "Huanbo Luan",
        "Tat-Seng Chua"
      ],
      "abstract": "Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent tasks, including information extraction, trend analysis, sentiment\nanalysis and event detection, etc. We further propose a novel TMMHybridRAG\nmethod, which first leverages LLMs to convert data from other modalities (e.g.,\ntabular, visual and time-series data) into textual format and then incorporates\ntemporal information in each node when constructing graphs and dense indexes.\nIts effectiveness has been validated in extensive experiments, but notable gaps\nremain, highlighting the challenges presented by our FinTMMBench.",
      "tldr_zh": "该研究提出了FinTMMBench——首个针对金融领域时序感知多模态检索增强生成(RAG)系统的基准测试框架。该基准基于NASDAQ 100公司的异构数据构建，包含金融表格、新闻文本、股价数据和可视化技术图表等多模态语料库，以及需要特定时间段检索分析的时序敏感问题。研究团队同时开发了TMMHybridRAG方法，通过大语言模型(LLMs)将表格、时序和视觉数据转化为文本格式，并在构建图结构和密集索引时融入时序信息，实验验证了其有效性，但也揭示了当前金融多模态分析仍存在的技术挑战。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "q-fin.CP",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2503.05185v1",
      "published_date": "2025-03-07 07:13:59 UTC",
      "updated_date": "2025-03-07 07:13:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:04:17.533228"
    },
    {
      "arxiv_id": "2503.05179v1",
      "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
      "title_zh": "思维速写：基于自适应认知启发的速写方法实现高效大语言模型推理",
      "authors": [
        "Simon A. Aytes",
        "Jinheon Baek",
        "Sung Ju Hwang"
      ],
      "abstract": "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https://www.github.com/SimonAytes/SoT.",
      "tldr_zh": "该论文提出了Sketch-of-Thought (SoT)框架，这是一种受认知科学启发的自适应提示方法，旨在优化大型语言模型(LLM)的推理效率。通过结合概念链(Conceptual Chaining)、分块符号(Chunked Symbolism)和专家词典(Expert Lexicons)三种认知范式，SoT能动态选择最适合当前推理任务的策略，在保持准确率的同时显著减少76%的中间token消耗。实验证明，该方法在15个多语言和多模态推理数据集上表现优异，尤其在数学和多跳推理任务中还能提升准确率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05179v1",
      "published_date": "2025-03-07 06:57:17 UTC",
      "updated_date": "2025-03-07 06:57:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:04:39.249561"
    },
    {
      "arxiv_id": "2503.05846v1",
      "title": "Extracting and Emulsifying Cultural Explanation to Improve Multilingual Capability of LLMs",
      "title_zh": "提取与融合文化解释以增强大型语言模型的多语言能力",
      "authors": [
        "Hamin Koo",
        "Jaehyung Kim"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success, but their\nEnglish-centric training data limits performance in non-English languages,\nhighlighting the need for enhancements in their multilingual capabilities.\nWhile some work on multilingual prompting methods handles non-English queries\nby utilizing English translations or restructuring them to more closely align\nwith LLM reasoning patterns, these works often overlook the importance of\ncultural context, limiting their effectiveness. To address this limitation, we\npropose EMCEI, a simple yet effective approach that improves LLMs' multilingual\ncapabilities by incorporating cultural context for more accurate and\nappropriate responses. Specifically, EMCEI follows a two-step process that\nfirst extracts relevant cultural context from the LLM's parametric knowledge\nvia prompting. Then, EMCEI employs an LLM-as-Judge mechanism to select the most\nappropriate response by balancing cultural relevance and reasoning ability.\nExperiments on diverse multilingual benchmarks show that EMCEI outperforms\nexisting baselines, demonstrating its effectiveness in handling multilingual\nqueries with LLMs.",
      "tldr_zh": "该研究提出了EMCEI方法，通过融入文化背景来提升大语言模型(LLMs)的多语言能力。该方法采用两步流程：首先通过提示从LLMs的参数知识中提取相关文化背景，然后利用LLM-as-Judge机制在文化相关性和推理能力间平衡以选择最佳响应。实验表明，EMCEI在多样化多语言基准测试中优于现有基线，有效提升了LLMs处理多语言查询的能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "under review, 18pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05846v1",
      "published_date": "2025-03-07 06:05:34 UTC",
      "updated_date": "2025-03-07 06:05:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:04:37.066131"
    },
    {
      "arxiv_id": "2503.05164v1",
      "title": "A Comprehensive LLM-powered Framework for Driving Intelligence Evaluation",
      "title_zh": "基于大语言模型的驾驶智能评估综合框架",
      "authors": [
        "Shanhe You",
        "Xuewen Luo",
        "Xinhe Liang",
        "Jiashu Yu",
        "Chen Zheng",
        "Jiangtao Gong"
      ],
      "abstract": "Evaluation methods for autonomous driving are crucial for algorithm\noptimization. However, due to the complexity of driving intelligence, there is\ncurrently no comprehensive evaluation method for the level of autonomous\ndriving intelligence. In this paper, we propose an evaluation framework for\ndriving behavior intelligence in complex traffic environments, aiming to fill\nthis gap. We constructed a natural language evaluation dataset of human\nprofessional drivers and passengers through naturalistic driving experiments\nand post-driving behavior evaluation interviews. Based on this dataset, we\ndeveloped an LLM-powered driving evaluation framework. The effectiveness of\nthis framework was validated through simulated experiments in the CARLA urban\ntraffic simulator and further corroborated by human assessment. Our research\nprovides valuable insights for evaluating and designing more intelligent,\nhuman-like autonomous driving agents. The implementation details of the\nframework and detailed information about the dataset can be found at Github.",
      "tldr_zh": "本文提出了一种基于大语言模型（LLM）的自动驾驶智能评估框架，旨在解决复杂交通环境下驾驶行为智能的全面评估难题。研究通过自然驾驶实验构建了专业驾驶员与乘客的自然语言评估数据集，并开发了LLM驱动的评估系统。该框架在CARLA仿真环境和人工评估中验证了有效性，为设计更智能、拟人化的自动驾驶系统提供了新思路。相关实现细节和数据集已在Github开源。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "68T45"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05164v1",
      "published_date": "2025-03-07 06:03:02 UTC",
      "updated_date": "2025-03-07 06:03:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:04:52.874763"
    },
    {
      "arxiv_id": "2503.05845v1",
      "title": "Machine Learned Force Fields: Fundamentals, its reach, and challenges",
      "title_zh": "机器学习力场：基本原理、应用范畴与挑战",
      "authors": [
        "Carlos A. Vital",
        "Román J. Armenta-Rico",
        "Huziel E. Sauceda"
      ],
      "abstract": "Highly accurate force fields are a mandatory requirement to generate\npredictive simulations. In this regard, Machine Learning Force Fields (MLFFs)\nhave emerged as a revolutionary approach in computational chemistry and\nmaterials science, combining the accuracy of quantum mechanical methods with\ncomputational efficiency orders of magnitude superior to ab-initio methods.\nThis chapter provides an introduction of the fundamentals of learning and how\nit is applied to construct MLFFs, detailing key methodologies such as neural\nnetwork potentials and kernel-based models. Emphasis is placed on the\nconstruction of SchNet model, as one of the most elemental neural network-based\nforce fields that are nowadays the basis of modern architectures. Additionally,\nthe GDML framework is described in detail as an example of how the elegant\nformulation of kernel methods can be used to construct mathematically robust\nand physics-inspired MLFFs. The ongoing advancements in MLFF development\ncontinue to expand their applicability, enabling precise simulations of large\nand complex systems that were previously beyond reach. This chapter concludes\nby highlighting the transformative impact of MLFFs on scientific research,\nunderscoring their role in driving future discoveries in the fields of\nchemistry, physics, and materials science.",
      "tldr_zh": "该研究探讨了机器学习力场（MLFFs）在计算化学和材料科学中的革命性作用，结合量子力学方法的精确性和远高于从头算方法的计算效率。文章详细介绍了构建MLFFs的关键方法，包括神经网络势能（如SchNet模型）和基于核方法的框架（如GDML），并强调了这些方法在模拟大型复杂系统中的优势。研究指出，MLFFs的持续发展正在推动化学、物理和材料科学领域的未来发现。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05845v1",
      "published_date": "2025-03-07 05:26:14 UTC",
      "updated_date": "2025-03-07 05:26:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:04:54.828024"
    },
    {
      "arxiv_id": "2503.05153v1",
      "title": "Generative Trajectory Stitching through Diffusion Composition",
      "title_zh": "通过扩散组合实现生成式轨迹缝合",
      "authors": [
        "Yunhao Luo",
        "Utkarsh A. Mishra",
        "Yilun Du",
        "Danfei Xu"
      ],
      "abstract": "Effective trajectory stitching for long-horizon planning is a significant\nchallenge in robotic decision-making. While diffusion models have shown promise\nin planning, they are limited to solving tasks similar to those seen in their\ntraining data. We propose CompDiffuser, a novel generative approach that can\nsolve new tasks by learning to compositionally stitch together shorter\ntrajectory chunks from previously seen tasks. Our key insight is modeling the\ntrajectory distribution by subdividing it into overlapping chunks and learning\ntheir conditional relationships through a single bidirectional diffusion model.\nThis allows information to propagate between segments during generation,\nensuring physically consistent connections. We conduct experiments on benchmark\ntasks of various difficulties, covering different environment sizes, agent\nstate dimension, trajectory types, training data quality, and show that\nCompDiffuser significantly outperforms existing methods.",
      "tldr_zh": "本文提出CompDiffuser，一种基于扩散模型的新型生成式轨迹规划方法，通过组合拼接已有短轨迹片段来解决新任务。该方法的创新点在于将轨迹分布建模为重叠片段，并利用双向扩散模型学习片段间的条件关系，从而在生成时保持物理一致性连接。实验表明，CompDiffuser在不同复杂度（环境规模、智能体状态维度、轨迹类型等）的基准任务上显著优于现有方法。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://comp-diffuser.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.05153v1",
      "published_date": "2025-03-07 05:22:52 UTC",
      "updated_date": "2025-03-07 05:22:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:05:11.426362"
    },
    {
      "arxiv_id": "2503.05149v1",
      "title": "Development and Enhancement of Text-to-Image Diffusion Models",
      "title_zh": "文本到图像扩散模型的开发与增强",
      "authors": [
        "Rajdeep Roshan Sahu"
      ],
      "abstract": "This research focuses on the development and enhancement of text-to-image\ndenoising diffusion models, addressing key challenges such as limited sample\ndiversity and training instability. By incorporating Classifier-Free Guidance\n(CFG) and Exponential Moving Average (EMA) techniques, this study significantly\nimproves image quality, diversity, and stability. Utilizing Hugging Face's\nstate-of-the-art text-to-image generation model, the proposed enhancements\nestablish new benchmarks in generative AI. This work explores the underlying\nprinciples of diffusion models, implements advanced strategies to overcome\nexisting limitations, and presents a comprehensive evaluation of the\nimprovements achieved. Results demonstrate substantial progress in generating\nstable, diverse, and high-quality images from textual descriptions, advancing\nthe field of generative artificial intelligence and providing new foundations\nfor future applications.\n  Keywords: Text-to-image, Diffusion model, Classifier-free guidance,\nExponential moving average, Image generation.",
      "tldr_zh": "该研究针对文本生成图像的去噪扩散模型，提出了基于Classifier-Free Guidance (CFG)和Exponential Moving Average (EMA)的优化方法，显著提升了图像质量、多样性和训练稳定性。通过结合Hugging Face的先进模型，研究实现了生成式AI的新基准，并深入探讨了扩散模型的原理与改进策略。实验结果表明，该方法在生成稳定、多样且高质量的图像方面取得了显著进展，为生成式人工智能的未来应用奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05149v1",
      "published_date": "2025-03-07 05:18:00 UTC",
      "updated_date": "2025-03-07 05:18:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:05:18.809846"
    },
    {
      "arxiv_id": "2503.05143v1",
      "title": "FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous User Data",
      "title_zh": "FedMABench：去中心化异构用户数据上的移动智能体基准测试",
      "authors": [
        "Wenhao Wang",
        "Zijie Yu",
        "Rui Ye",
        "Jianqing Zhang",
        "Siheng Chen",
        "Yanfeng Wang"
      ],
      "abstract": "Mobile agents have attracted tremendous research participation recently.\nTraditional approaches to mobile agent training rely on centralized data\ncollection, leading to high cost and limited scalability. Distributed training\nutilizing federated learning offers an alternative by harnessing real-world\nuser data, providing scalability and reducing costs. However, pivotal\nchallenges, including the absence of standardized benchmarks, hinder progress\nin this field.\n  To tackle the challenges, we introduce FedMABench, the first benchmark for\nfederated training and evaluation of mobile agents, specifically designed for\nheterogeneous scenarios. FedMABench features 6 datasets with 30+ subsets, 8\nfederated algorithms, 10+ base models, and over 800 apps across 5 categories,\nproviding a comprehensive framework for evaluating mobile agents across diverse\nenvironments. Through extensive experiments, we uncover several key insights:\nfederated algorithms consistently outperform local training; the distribution\nof specific apps plays a crucial role in heterogeneity; and, even apps from\ndistinct categories can exhibit correlations during training. FedMABench is\npublicly available at: https://github.com/wwh0411/FedMABench with the datasets\nat: https://huggingface.co/datasets/wwh0411/FedMABench.",
      "tldr_zh": "该研究提出了FedMABench，首个面向移动智能体(Mobile Agents)联邦学习的标准测试基准，专门针对数据异构性场景设计。该基准包含6个数据集、30+子集、8种联邦算法、10+基础模型以及覆盖5类应用的800多个APP，为评估不同环境下的移动智能体提供了全面框架。实验发现：联邦算法始终优于本地训练；特定APP的分布对数据异构性影响显著；不同类别APP在训练过程中可能表现出相关性。该基准已开源，旨在推动移动智能体联邦学习研究的发展。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05143v1",
      "published_date": "2025-03-07 04:52:20 UTC",
      "updated_date": "2025-03-07 04:52:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:05:28.262684"
    },
    {
      "arxiv_id": "2503.05139v2",
      "title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs",
      "title_zh": "每一点计算都至关重要：在不依赖高端 GPU 的情况下扩展 300B 专家混合 LING 大语言模型",
      "authors": [
        "Ling Team",
        "Binwei Zeng",
        "Chao Huang",
        "Chao Zhang",
        "Changxin Tian",
        "Cong Chen",
        "Dingnan Jin",
        "Feng Yu",
        "Feng Zhu",
        "Feng Yuan",
        "Fakang Wang",
        "Gangshan Wang",
        "Guangyao Zhai",
        "Haitao Zhang",
        "Huizhong Li",
        "Jun Zhou",
        "Jia Liu",
        "Junpeng Fang",
        "Junjie Ou",
        "Jun Hu",
        "Ji Luo",
        "Ji Zhang",
        "Jian Liu",
        "Jian Sha",
        "Jianxue Qian",
        "Jiewei Wu",
        "Junping Zhao",
        "Jianguo Li",
        "Jubao Feng",
        "Jingchao Di",
        "Junming Xu",
        "Jinghua Yao",
        "Kuan Xu",
        "Kewei Du",
        "Longfei Li",
        "Lei Liang",
        "Lu Yu",
        "Li Tang",
        "Lin Ju",
        "Peng Xu",
        "Qing Cui",
        "Song Liu",
        "Shicheng Li",
        "Shun Song",
        "Song Yan",
        "Tengwei Cai",
        "Tianyi Chen",
        "Ting Guo",
        "Ting Huang",
        "Tao Feng",
        "Tao Wu",
        "Wei Wu",
        "Xiaolu Zhang",
        "Xueming Yang",
        "Xin Zhao",
        "Xiaobo Hu",
        "Xin Lin",
        "Yao Zhao",
        "Yilong Wang",
        "Yongzhen Guo",
        "Yuanyuan Wang",
        "Yue Yang",
        "Yang Cao",
        "Yuhao Fu",
        "Yi Xiong",
        "Yanzhe Li",
        "Zhe Li",
        "Zhiqiang Zhang",
        "Ziqi Liu",
        "Zhaoxin Huan",
        "Zujie Wen",
        "Zhenhang Sun",
        "Zhuoxuan Du",
        "Zhengyu He"
      ],
      "abstract": "In this technical report, we tackle the challenges of training large-scale\nMixture of Experts (MoE) models, focusing on overcoming cost inefficiency and\nresource limitations prevalent in such systems. To address these issues, we\npresent two differently sized MoE large language models (LLMs), namely\nLing-Lite and Ling-Plus (referred to as \"Bailing\" in Chinese, spelled\nB\\v{a}il\\'ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75\nbillion activated parameters, while Ling-Plus boasts 290 billion parameters\nwith 28.8 billion activated parameters. Both models exhibit comparable\nperformance to leading industry benchmarks. This report offers actionable\ninsights to improve the efficiency and accessibility of AI development in\nresource-constrained settings, promoting more scalable and sustainable\ntechnologies. Specifically, to reduce training costs for large-scale MoE\nmodels, we propose innovative methods for (1) optimization of model\narchitecture and training processes, (2) refinement of training anomaly\nhandling, and (3) enhancement of model evaluation efficiency. Additionally,\nleveraging high-quality data generated from knowledge graphs, our models\ndemonstrate superior capabilities in tool use compared to other models.\nUltimately, our experimental findings demonstrate that a 300B MoE LLM can be\neffectively trained on lower-performance devices while achieving comparable\nperformance to models of a similar scale, including dense and MoE models.\nCompared to high-performance devices, utilizing a lower-specification hardware\nsystem during the pre-training phase demonstrates significant cost savings,\nreducing computing costs by approximately 20%. The models can be accessed at\nhttps://huggingface.co/inclusionAI.",
      "tldr_zh": "本文提出两种不同规模的混合专家模型(MoE)大语言模型Ling-Lite(168亿参数)和Ling-Plus(2900亿参数)，通过优化模型架构、训练异常处理和评估效率，在普通GPU设备上实现了与行业标杆相当的性能。研究创新性地采用知识图谱生成的高质量数据增强模型工具使用能力，相比高性能设备可节省约20%计算成本。该工作为资源受限环境下训练大规模MoE模型提供了经济高效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05139v2",
      "published_date": "2025-03-07 04:43:39 UTC",
      "updated_date": "2025-03-10 14:21:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:05:36.051467"
    },
    {
      "arxiv_id": "2503.05132v2",
      "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
      "title_zh": "R1-Zero在20亿参数非监督微调模型上的视觉推理\"顿悟时刻\"",
      "authors": [
        "Hengguang Zhou",
        "Xirui Li",
        "Ruochen Wang",
        "Minhao Cheng",
        "Tianyi Zhou",
        "Cho-Jui Hsieh"
      ],
      "abstract": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
      "tldr_zh": "本研究首次在仅2B参数的非监督微调(non-SFT)模型上成功复现了多模态推理中的“顿悟时刻”(Aha Moment)，即模型在训练过程中表现出自我反思和响应长度增加的特征。通过在Qwen2-VL-2B模型上直接应用强化学习(RL)并基于SAT数据集训练，模型在CVBench上的准确率达到59.47%，比基线模型提升约30%，并优于监督微调(SFT)设置约2%。研究还揭示了在指令模型上应用RL的挑战，包括推理轨迹趋于简单化以及单纯的长度奖励无法有效激发推理能力。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05132v2",
      "published_date": "2025-03-07 04:21:47 UTC",
      "updated_date": "2025-03-10 01:52:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:05:54.582417"
    },
    {
      "arxiv_id": "2503.05127v1",
      "title": "HexPlane Representation for 3D Semantic Scene Understanding",
      "title_zh": "六面体表征框架：面向三维语义场景理解的新型表达方法",
      "authors": [
        "Zeren Chen",
        "Yuenan Hou",
        "Yulin Chen",
        "Li Liu",
        "Xiao Sun",
        "Lu Sheng"
      ],
      "abstract": "In this paper, we introduce the HexPlane representation for 3D semantic scene\nunderstanding. Specifically, we first design the View Projection Module (VPM)\nto project the 3D point cloud into six planes to maximally retain the original\nspatial information. Features of six planes are extracted by the 2D encoder and\nsent to the HexPlane Association Module (HAM) to adaptively fuse the most\ninformative information for each point. The fused point features are further\nfed to the task head to yield the ultimate predictions. Compared to the popular\npoint and voxel representation, the HexPlane representation is efficient and\ncan utilize highly optimized 2D operations to process sparse and unordered 3D\npoint clouds. It can also leverage off-the-shelf 2D models, network weights,\nand training recipes to achieve accurate scene understanding in 3D space. On\nScanNet and SemanticKITTI benchmarks, our algorithm, dubbed HexNet3D, achieves\ncompetitive performance with previous algorithms. In particular, on the ScanNet\n3D segmentation task, our method obtains 77.0 mIoU on the validation set,\nsurpassing Point Transformer V2 by 1.6 mIoU. We also observe encouraging\nresults in indoor 3D detection tasks. Note that our method can be seamlessly\nintegrated into existing voxel-based, point-based, and range-based approaches\nand brings considerable gains without bells and whistles. The codes will be\navailable upon publication.",
      "tldr_zh": "本文提出了一种名为HexPlane的新型3D语义场景理解表示方法。该方法通过View Projection Module (VPM)将3D点云投影到六个平面上，以最大化保留原始空间信息，并通过HexPlane Association Module (HAM)自适应融合每个点的最具信息量的特征。HexPlane表示法高效且能利用高度优化的2D操作处理稀疏和无序的3D点云，同时可以借助现成的2D模型、网络权重和训练方法实现精确的3D场景理解。在ScanNet和SemanticKITTI基准测试中，HexNet3D算法表现优异，尤其在ScanNet 3D分割任务中，验证集上的mIoU达到77.0，超越Point Transformer V2 1.6个mIoU。该方法还能无缝集成到现有的基于体素、点和距离的方法中，带来显著提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05127v1",
      "published_date": "2025-03-07 04:18:55 UTC",
      "updated_date": "2025-03-07 04:18:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:05:45.706073"
    },
    {
      "arxiv_id": "2503.05126v3",
      "title": "Multi-Task Reinforcement Learning Enables Parameter Scaling",
      "title_zh": "多任务强化学习实现参数规模化",
      "authors": [
        "Reginald McLean",
        "Evangelos Chatzaroulas",
        "Jordan Terry",
        "Isaac Woungang",
        "Nariman Farsad",
        "Pablo Samuel Castro"
      ],
      "abstract": "Multi-task reinforcement learning (MTRL) aims to endow a single agent with\nthe ability to perform well on multiple tasks. Recent works have focused on\ndeveloping novel sophisticated architectures to improve performance, often\nresulting in larger models; it is unclear, however, whether the performance\ngains are a consequence of the architecture design itself or the extra\nparameters. We argue that gains are mostly due to scale by demonstrating that\nnaively scaling up a simple MTRL baseline to match parameter counts outperforms\nthe more sophisticated architectures, and these gains benefit most from scaling\nthe critic over the actor. Additionally, we explore the training stability\nadvantages that come with task diversity, demonstrating that increasing the\nnumber of tasks can help mitigate plasticity loss. Our findings suggest that\nMTRL's simultaneous training across multiple tasks provides a natural framework\nfor beneficial parameter scaling in reinforcement learning, challenging the\nneed for complex architectural innovations.",
      "tldr_zh": "本文探讨了多任务强化学习(MTRL)中参数规模对性能的影响，发现简单地扩大一个基础MTRL模型的参数量即可超越复杂架构的性能，且这种性能提升主要来自critic网络的扩展。研究还表明，任务多样性有助于提高训练稳定性，缓解可塑性损失问题。这些发现表明，MTRL通过多任务同时训练为强化学习提供了有益的参数扩展框架，挑战了复杂架构创新的必要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05126v3",
      "published_date": "2025-03-07 04:13:02 UTC",
      "updated_date": "2025-03-12 16:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:06:15.285725"
    },
    {
      "arxiv_id": "2503.05114v1",
      "title": "Look Before You Leap: Using Serialized State Machine for Language Conditioned Robotic Manipulation",
      "title_zh": "三思而后行：利用序列化状态机实现语言条件化机器人操控",
      "authors": [
        "Tong Mu",
        "Yihao Liu",
        "Mehran Armand"
      ],
      "abstract": "Imitation learning frameworks for robotic manipulation have drawn attention\nin the recent development of language model grounded robotics. However, the\nsuccess of the frameworks largely depends on the coverage of the demonstration\ncases: When the demonstration set does not include examples of how to act in\nall possible situations, the action may fail and can result in cascading\nerrors. To solve this problem, we propose a framework that uses serialized\nFinite State Machine (FSM) to generate demonstrations and improve the success\nrate in manipulation tasks requiring a long sequence of precise interactions.\nTo validate its effectiveness, we use environmentally evolving and long-horizon\npuzzles that require long sequential actions. Experimental results show that\nour approach achieves a success rate of up to 98 in these tasks, compared to\nthe controlled condition using existing approaches, which only had a success\nrate of up to 60, and, in some tasks, almost failed completely.",
      "tldr_zh": "本文提出了一种基于序列化有限状态机(FSM)的框架，用于解决语言条件机器人操作中因示范案例覆盖不足导致的级联错误问题。该方法通过FSM生成示范数据，显著提升了需要长序列精准交互的操作任务成功率。实验结果表明，在环境演化型长期任务中，该框架成功率高达98%，远超现有方法60%的表现，有效解决了复杂操作场景下的失败问题。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05114v1",
      "published_date": "2025-03-07 03:19:25 UTC",
      "updated_date": "2025-03-07 03:19:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:06:24.025156"
    },
    {
      "arxiv_id": "2503.05108v1",
      "title": "TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting",
      "title_zh": "TS-LIF：面向时间序列预测的时序分段脉冲神经网络",
      "authors": [
        "Shibo Feng",
        "Wanjin Feng",
        "Xingyu Gao",
        "Peilin Zhao",
        "Zhiqi Shen"
      ],
      "abstract": "Spiking Neural Networks (SNNs) offer a promising, biologically inspired\napproach for processing spatiotemporal data, particularly for time series\nforecasting. However, conventional neuron models like the Leaky\nIntegrate-and-Fire (LIF) struggle to capture long-term dependencies and\neffectively process multi-scale temporal dynamics. To overcome these\nlimitations, we introduce the Temporal Segment Leaky Integrate-and-Fire\n(TS-LIF) model, featuring a novel dual-compartment architecture. The dendritic\nand somatic compartments specialize in capturing distinct frequency components,\nproviding functional heterogeneity that enhances the neuron's ability to\nprocess both low- and high-frequency information. Furthermore, the newly\nintroduced direct somatic current injection reduces information loss during\nintra-neuronal transmission, while dendritic spike generation improves\nmulti-scale information extraction. We provide a theoretical stability analysis\nof the TS-LIF model and explain how each compartment contributes to distinct\nfrequency response characteristics. Experimental results show that TS-LIF\noutperforms traditional SNNs in time series forecasting, demonstrating better\naccuracy and robustness, even with missing data. TS-LIF advances the\napplication of SNNs in time-series forecasting, providing a biologically\ninspired approach that captures complex temporal dynamics and offers potential\nfor practical implementation in diverse forecasting scenarios. The source code\nis available at https://github.com/kkking-kk/TS-LIF.",
      "tldr_zh": "本研究提出了一种新型的TS-LIF（Temporal Segment Leaky Integrate-and-Fire）模型，通过双室架构（树突室和体细胞室）分别处理不同频率的时序信息，从而增强脉冲神经网络（SNNs）在时间序列预测中的性能。该模型引入了直接体细胞电流注入和树突脉冲生成机制，减少了信息损失并改善了多尺度信息提取。理论稳定性分析表明，TS-LIF能够捕捉复杂的时序动态，实验结果显示其在时间序列预测中优于传统SNNs，具有更高的准确性和鲁棒性，尤其适用于数据缺失的场景。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05108v1",
      "published_date": "2025-03-07 03:06:21 UTC",
      "updated_date": "2025-03-07 03:06:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:06:24.465755"
    },
    {
      "arxiv_id": "2503.05106v1",
      "title": "Grouped Sequential Optimization Strategy -- the Application of Hyperparameter Importance Assessment in Deep Learning",
      "title_zh": "分组顺序优化策略——超参数重要性评估在深度学习中的应用",
      "authors": [
        "Ruinan Wang",
        "Ian Nabney",
        "Mohammad Golbabaee"
      ],
      "abstract": "Hyperparameter optimization (HPO) is a critical component of machine learning\npipelines, significantly affecting model robustness, stability, and\ngeneralization. However, HPO is often a time-consuming and computationally\nintensive task. Traditional HPO methods, such as grid search and random search,\noften suffer from inefficiency. Bayesian optimization, while more efficient,\nstill struggles with high-dimensional search spaces. In this paper, we\ncontribute to the field by exploring how insights gained from hyperparameter\nimportance assessment (HIA) can be leveraged to accelerate HPO, reducing both\ntime and computational resources. Building on prior work that quantified\nhyperparameter importance by evaluating 10 hyperparameters on CNNs using 10\ncommon image classification datasets, we implement a novel HPO strategy called\n'Sequential Grouping.' That prior work assessed the importance weights of the\ninvestigated hyperparameters based on their influence on model performance,\nproviding valuable insights that we leverage to optimize our HPO process. Our\nexperiments, validated across six additional image classification datasets,\ndemonstrate that incorporating hyperparameter importance assessment (HIA) can\nsignificantly accelerate HPO without compromising model performance, reducing\noptimization time by an average of 31.9\\% compared to the conventional\nsimultaneous strategy.",
      "tldr_zh": "该研究提出了一种基于超参数重要性评估(HIA)的\"分组顺序优化策略\"(Sequential Grouping)，用于加速深度学习中的超参数优化(HPO)。通过分析10个CNN超参数在图像分类任务中的重要性权重，该方法将优化过程分为关键和非关键参数组进行顺序优化。实验表明，相比传统方法，该策略在保持模型性能的同时将优化时间平均缩短31.9%，有效解决了高维搜索空间导致的效率低下问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05, 68Q32"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05106v1",
      "published_date": "2025-03-07 03:01:00 UTC",
      "updated_date": "2025-03-07 03:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:06:36.965813"
    },
    {
      "arxiv_id": "2503.05092v1",
      "title": "Multi-Robot Collaboration through Reinforcement Learning and Abstract Simulation",
      "title_zh": "多机器人协作：基于强化学习与抽象仿真的协同策略",
      "authors": [
        "Adam Labiosa",
        "Josiah P. Hanna"
      ],
      "abstract": "Teams of people coordinate to perform complex tasks by forming abstract\nmental models of world and agent dynamics. The use of abstract models contrasts\nwith much recent work in robot learning that uses a high-fidelity simulator and\nreinforcement learning (RL) to obtain policies for physical robots. Motivated\nby this difference, we investigate the extent to which so-called abstract\nsimulators can be used for multi-agent reinforcement learning (MARL) and the\nresulting policies successfully deployed on teams of physical robots. An\nabstract simulator models the robot's target task at a high-level of\nabstraction and discards many details of the world that could impact optimal\ndecision-making. Policies are trained in an abstract simulator then transferred\nto the physical robot by making use of separately-obtained low-level perception\nand motion control modules. We identify three key categories of modifications\nto the abstract simulator that enable policy transfer to physical robots:\nsimulation fidelity enhancements, training optimizations and simulation\nstochasticity. We then run an empirical study with extensive ablations to\ndetermine the value of each modification category for enabling policy transfer\nin cooperative robot soccer tasks. We also compare the performance of policies\nproduced by our method with a well-tuned non-learning-based behavior\narchitecture from the annual RoboCup competition and find that our approach\nleads to a similar level of performance. Broadly we show that MARL can be use\nto train cooperative physical robot behaviors using highly abstract models of\nthe world.",
      "tldr_zh": "该研究探索了如何利用抽象模拟器进行多智能体强化学习（MARL），并将训练的策略成功部署到物理机器人团队中。通过在高层次抽象的任务模型中训练策略，并结合低层级的感知和运动控制模块，研究实现了从模拟到物理环境的策略迁移。实验表明，经过模拟器保真度增强、训练优化和随机性调整后，该方法在机器人足球协作任务中表现优异，性能与RoboCup竞赛中的非学习型行为架构相当。研究证明了使用高度抽象的世界模型训练协作物理机器人行为的可行性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05092v1",
      "published_date": "2025-03-07 02:23:24 UTC",
      "updated_date": "2025-03-07 02:23:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:06:41.074086"
    },
    {
      "arxiv_id": "2503.08699v1",
      "title": "Blockchain As a Platform For Artificial Intelligence (AI) Transparency",
      "title_zh": "区块链作为人工智能（AI）透明度的基础平台",
      "authors": [
        "Afroja Akther",
        "Ayesha Arobee",
        "Abdullah Al Adnan",
        "Omum Auyon",
        "ASM Johirul Islam",
        "Farhad Akter"
      ],
      "abstract": "As artificial intelligence (AI) systems become increasingly complex and\nautonomous, concerns over transparency and accountability have intensified. The\n\"black box\" problem in AI decision-making limits stakeholders' ability to\nunderstand, trust, and verify outcomes, particularly in high-stakes sectors\nsuch as healthcare, finance, and autonomous systems. Blockchain technology,\nwith its decentralized, immutable, and transparent characteristics, presents a\npotential solution to enhance AI transparency and auditability. This paper\nexplores the integration of blockchain with AI to improve decision\ntraceability, data provenance, and model accountability. By leveraging\nblockchain as an immutable record-keeping system, AI decision-making can become\nmore interpretable, fostering trust among users and regulatory compliance.\nHowever, challenges such as scalability, integration complexity, and\ncomputational overhead must be addressed to fully realize this synergy. This\nstudy discusses existing research, proposes a framework for blockchain-enhanced\nAI transparency, and highlights practical applications, benefits, and\nlimitations. The findings suggest that blockchain could be a foundational\ntechnology for ensuring AI systems remain accountable, ethical, and aligned\nwith regulatory standards.",
      "tldr_zh": "该论文探讨了区块链技术如何提升人工智能（AI）系统的透明度和可审计性。作者提出利用区块链的去中心化、不可篡改和透明特性，为AI决策提供可追溯性、数据溯源和模型问责机制，尤其适用于医疗、金融等高风险领域。研究分析了现有方法，提出了一个区块链增强AI透明度的框架，并指出仍需解决可扩展性、集成复杂性和计算开销等挑战。结果表明，区块链有望成为确保AI系统可信、合规的关键技术基础。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "F.2.2"
      ],
      "primary_category": "cs.CR",
      "comment": "14 pages, 2 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.08699v1",
      "published_date": "2025-03-07 01:57:26 UTC",
      "updated_date": "2025-03-07 01:57:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:07:08.721346"
    },
    {
      "arxiv_id": "2503.05071v1",
      "title": "Object Packing and Scheduling for Sequential 3D Printing: a Linear Arithmetic Model and a CEGAR-inspired Optimal Solver",
      "title_zh": "面向顺序3D打印的物体排样与调度：线性算术模型及基于CEGAR启发的最优求解器",
      "authors": [
        "Pavel Surynek",
        "Vojtěch Bubník",
        "Lukáš Matěna",
        "Petr Kubiš"
      ],
      "abstract": "We address the problem of object arrangement and scheduling for sequential 3D\nprinting. Unlike the standard 3D printing, where all objects are printed slice\nby slice at once, in sequential 3D printing, objects are completed one after\nother. In the sequential case, it is necessary to ensure that the moving parts\nof the printer do not collide with previously printed objects. We look at the\nsequential printing problem from the perspective of combinatorial optimization.\nWe propose to express the problem as a linear arithmetic formula, which is then\nsolved using a solver for satisfiability modulo theories (SMT). However, we do\nnot solve the formula expressing the problem of object arrangement and\nscheduling directly, but we have proposed a technique inspired by\ncounterexample guided abstraction refinement (CEGAR), which turned out to be a\nkey innovation to efficiency.",
      "tldr_zh": "该研究提出了一种基于线性算术模型的顺序3D打印物体排列与调度优化方法。针对顺序打印中打印机移动部件与已打印物体可能碰撞的问题，研究者将问题转化为可满足性模理论(SMT)求解的线性算术公式。关键创新是采用反例引导抽象精化(CEGAR)启发的优化求解器，大幅提升了计算效率，为顺序3D打印提供了组合优化解决方案。",
      "categories": [
        "cs.CG",
        "cs.AI"
      ],
      "primary_category": "cs.CG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05071v1",
      "published_date": "2025-03-07 01:31:40 UTC",
      "updated_date": "2025-03-07 01:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:07:23.430806"
    },
    {
      "arxiv_id": "2503.05070v1",
      "title": "PromptPex: Automatic Test Generation for Language Model Prompts",
      "title_zh": "PromptPex：面向语言模型提示的自动化测试生成",
      "authors": [
        "Reshabh K Sharma",
        "Jonathan De Halleux",
        "Shraddha Barke",
        "Benjamin Zorn"
      ],
      "abstract": "Large language models (LLMs) are being used in many applications and prompts\nfor these models are integrated into software applications as code-like\nartifacts. These prompts behave much like traditional software in that they\ntake inputs, generate outputs, and perform some specific function. However,\nprompts differ from traditional code in many ways and require new approaches to\nensure that they are robust. For example, unlike traditional software the\noutput of a prompt depends on the AI model that interprets it. Also, while\nnatural language prompts are easy to modify, the impact of updates is harder to\npredict. New approaches to testing, debugging, and modifying prompts with\nrespect to the model running them are required.\n  To address some of these issues, we developed PromptPex, an LLM-based tool to\nautomatically generate and evaluate unit tests for a given prompt. PromptPex\nextracts input and output specifications from a prompt and uses them to\ngenerate diverse, targeted, and valid unit tests. These tests are instrumental\nin identifying regressions when a prompt is changed and also serve as a tool to\nunderstand how prompts are interpreted by different models. We use PromptPex to\ngenerate tests for eight benchmark prompts and evaluate the quality of the\ngenerated tests by seeing if they can cause each of four diverse models to\nproduce invalid output. PromptPex consistently creates tests that result in\nmore invalid model outputs than a carefully constructed baseline LLM-based test\ngenerator. Furthermore, by extracting concrete specifications from the input\nprompt, PromptPex allows prompt writers to clearly understand and test specific\naspects of their prompts. The source code of PromptPex is available at\nhttps://github.com/microsoft/promptpex.",
      "tldr_zh": "本文提出了PromptPex，一种基于大语言模型(LLM)的自动化测试生成工具，用于评估和改进提示词(prompt)的鲁棒性。该工具能自动从提示词中提取输入输出规范，生成多样化、有针对性的单元测试，帮助开发者检测提示修改后的回归问题，并理解不同模型对提示的解读差异。实验表明，在8个基准提示和4种不同模型上，PromptPex生成的测试比基线方法能更有效地触发无效输出，其基于规范提取的方法还使开发者能清晰理解和测试提示的特定方面。该工具已开源，为提示工程的可靠性提供了新解决方案。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05070v1",
      "published_date": "2025-03-07 01:31:03 UTC",
      "updated_date": "2025-03-07 01:31:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:07:25.561128"
    },
    {
      "arxiv_id": "2503.05066v1",
      "title": "Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts",
      "title_zh": "容量感知推理：缓解专家混合模型中的拖尾效应",
      "authors": [
        "Shwai He",
        "Weilin Cai",
        "Jiayi Huang",
        "Ang Li"
      ],
      "abstract": "The Mixture of Experts (MoE) is an effective architecture for scaling large\nlanguage models by leveraging sparse expert activation, optimizing the\ntrade-off between performance and efficiency. However, under expert\nparallelism, MoE suffers from inference inefficiencies due to imbalanced\ntoken-to-expert assignment, where some experts are overloaded while others\nremain underutilized. This imbalance leads to poor resource utilization and\nincreased latency, as the most burdened expert dictates the overall delay, a\nphenomenon we define as the \\textbf{\\textit{Straggler Effect}}. To mitigate\nthis, we propose Capacity-Aware Inference, including two key techniques: (1)\n\\textbf{\\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens\nto regulate the maximum latency of MoE, and (2) \\textbf{\\textit{Capacity-Aware\nToken Reroute}}, which reallocates overflowed tokens to underutilized experts,\nbalancing the token distribution. These techniques collectively optimize both\nhigh-load and low-load expert utilization, leading to a more efficient MoE\ninference pipeline. Extensive experiments demonstrate the effectiveness of our\nmethods, showing significant improvements in inference efficiency, e.g., 0.2\\%\naverage performance increase and a 1.94$\\times$ inference speedup on\nMixtral-8$\\times$7B-Instruct.",
      "tldr_zh": "该研究针对混合专家模型(MoE)中的\"拖尾效应\"(Straggler Effect)问题，提出了容量感知推理框架。通过两种核心技术：1)容量感知令牌丢弃机制，通过舍弃过载令牌来控制最大延迟；2)容量感知令牌重路由机制，将溢出令牌重新分配到空闲专家，实现了专家负载均衡。实验表明，该方法在Mixtral-8×7B模型上取得了0.2%的性能提升和1.94倍的推理加速，显著优化了MoE推理效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05066v1",
      "published_date": "2025-03-07 01:11:39 UTC",
      "updated_date": "2025-03-07 01:11:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:07:16.187493"
    },
    {
      "arxiv_id": "2503.05064v1",
      "title": "Perceiving, Reasoning, Adapting: A Dual-Layer Framework for VLM-Guided Precision Robotic Manipulation",
      "title_zh": "感知、推理、适应：VLM引导的精密机器人操作双层框架",
      "authors": [
        "Qingxuan Jia",
        "Guoqin Tang",
        "Zeyuan Huang",
        "Zixuan Hao",
        "Ning Ji",
        "Shihang",
        "Yin",
        "Gang Chen"
      ],
      "abstract": "Vision-Language Models (VLMs) demonstrate remarkable potential in robotic\nmanipulation, yet challenges persist in executing complex fine manipulation\ntasks with high speed and precision. While excelling at high-level planning,\nexisting VLM methods struggle to guide robots through precise sequences of fine\nmotor actions. To address this limitation, we introduce a progressive VLM\nplanning algorithm that empowers robots to perform fast, precise, and\nerror-correctable fine manipulation. Our method decomposes complex tasks into\nsub-actions and maintains three key data structures: task memory structure, 2D\ntopology graphs, and 3D spatial networks, achieving high-precision\nspatial-semantic fusion. These three components collectively accumulate and\nstore critical information throughout task execution, providing rich context\nfor our task-oriented VLM interaction mechanism. This enables VLMs to\ndynamically adjust guidance based on real-time feedback, generating precise\naction plans and facilitating step-wise error correction. Experimental\nvalidation on complex assembly tasks demonstrates that our algorithm\neffectively guides robots to rapidly and precisely accomplish fine manipulation\nin challenging scenarios, significantly advancing robot intelligence for\nprecision tasks.",
      "tldr_zh": "该研究提出了一种双层的视觉语言模型(VLM)引导框架，用于实现高精度机器人操控。该方法通过将复杂任务分解为子动作，并利用任务记忆结构、2D拓扑图和3D空间网络实现高精度的空间语义融合，从而支持动态调整和实时纠错。实验表明，该算法能够有效指导机器人在复杂装配任务中快速、精确地完成精细操作，显著提升了机器人在高精度任务中的智能化水平。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05064v1",
      "published_date": "2025-03-07 00:55:42 UTC",
      "updated_date": "2025-03-07 00:55:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:07:40.809619"
    },
    {
      "arxiv_id": "2503.05051v1",
      "title": "Accelerated Patient-specific Non-Cartesian MRI Reconstruction using Implicit Neural Representations",
      "title_zh": "基于隐式神经表征的加速患者特异性非笛卡尔MRI重建",
      "authors": [
        "Di Xu",
        "Hengjie Liu",
        "Xin Miao",
        "Daniel O'Connor",
        "Jessica E. Scholey",
        "Wensha Yang",
        "Mary Feng",
        "Michael Ohliger",
        "Hui Lin",
        "Dan Ruan",
        "Yang Yang",
        "Ke Sheng"
      ],
      "abstract": "The scanning time for a fully sampled MRI can be undesirably lengthy.\nCompressed sensing has been developed to minimize image artifacts in\naccelerated scans, but the required iterative reconstruction is computationally\ncomplex and difficult to generalize on new cases. Image-domain-based deep\nlearning methods (e.g., convolutional neural networks) emerged as a faster\nalternative but face challenges in modeling continuous k-space, a problem\namplified with non-Cartesian sampling commonly used in accelerated acquisition.\nIn comparison, implicit neural representations can model continuous signals in\nthe frequency domain and thus are compatible with arbitrary k-space sampling\npatterns. The current study develops a novel generative-adversarially trained\nimplicit neural representations (k-GINR) for de novo undersampled non-Cartesian\nk-space reconstruction. k-GINR consists of two stages: 1) supervised training\non an existing patient cohort; 2) self-supervised patient-specific\noptimization. In stage 1, the network is trained with the\ngenerative-adversarial network on diverse patients of the same anatomical\nregion supervised by fully sampled acquisition. In stage 2, undersampled\nk-space data of individual patients is used to tailor the prior-embedded\nnetwork for patient-specific optimization. The UCSF StarVIBE T1-weighted liver\ndataset was evaluated on the proposed framework. k-GINR is compared with an\nimage-domain deep learning method, Deep Cascade CNN, and a compressed sensing\nmethod. k-GINR consistently outperformed the baselines with a larger\nperformance advantage observed at very high accelerations (e.g., 20 times).\nk-GINR offers great value for direct non-Cartesian k-space reconstruction for\nnew incoming patients across a wide range of accelerations liver anatomy.",
      "tldr_zh": "本研究提出了一种基于隐式神经表示(Implicit Neural Representations)的加速非笛卡尔MRI重建方法k-GINR，用于患者特异性的快速成像。该方法通过生成对抗网络(GAN)进行两阶段训练：首先在现有患者队列上进行监督学习，随后利用单个患者的欠采样k空间数据进行自监督优化。实验表明，k-GINR在UCSF StarVIBE T1加权肝脏数据集上显著优于传统的图像域深度学习方法(如Deep Cascade CNN)和压缩感知方法，尤其在超高加速因子(如20倍)下表现出更大的性能优势。该方法为直接重建新患者的非笛卡尔k空间数据提供了高效解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05051v1",
      "published_date": "2025-03-07 00:05:43 UTC",
      "updated_date": "2025-03-07 00:05:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T04:08:19.947289"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 121,
  "processed_papers_count": 121,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T04:09:30.853075"
}
{
  "date": "2025-09-21",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-21 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv è®ºæ–‡è´¨é‡æé«˜ï¼Œæ ¸å¿ƒè¯é¢˜é›†ä¸­åœ¨ **LLM æ¨ç†æ•ˆç‡ä¸æ¶æ„ä¼˜åŒ–**ï¼ˆå¦‚ MoE çš„åŠ¨æ€é›†æˆã€åè®­ç»ƒä¸‰å€¼é‡åŒ–ã€åŠ¨æ€æ€ç»´è·¯å¾„æ§åˆ¶ï¼‰ä»¥åŠ **Agentic AI åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è½åœ°**ï¼ˆå¦‚ç²’å­åŠ é€Ÿå™¨æ§åˆ¶ã€Meta çš„å¤§è§„æ¨¡ Agent ç¯å¢ƒï¼‰ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿçœ‹åˆ°äº†å¯¹ LLM â€œé•¿ä¸Šä¸‹æ–‡â€èƒ½åŠ›çš„ç¥›é­…ç ”ç©¶ï¼Œéå¸¸å€¼å¾—å…³æ³¨ã€‚\n\n---\n\n### ğŸš€ æ¶æ„ä¼˜åŒ–ä¸æ¨ç†æ•ˆç‡ (LLM Efficiency & Reasoning)\n\n**1. MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE**\n**MoE æ¯”ä½ æƒ³è±¡çš„æ›´å¼ºï¼šåŸºäº RoE çš„è¶…å¹¶è¡Œæ¨ç†æ‰©å±•**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè¿™æ˜¯ä¸€ç¯‡ä»¤äººå…´å¥‹çš„å·¥ç¨‹ä¼˜åŒ–è®ºæ–‡ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸º **Roster of Experts (RoE)** çš„å…è®­ç»ƒæ¨ç†ç®—æ³•ã€‚ä¼ ç»Ÿçš„ MoEï¼ˆæ··åˆä¸“å®¶æ¨¡å‹ï¼‰é€šå¸¸åªæ¿€æ´»å°‘æ•°ä¸“å®¶ï¼Œè€Œ RoE å°†å•ä¸ª MoE æ¨¡å‹è½¬åŒ–ä¸ºä¸€ä¸ªâ€œåŠ¨æ€ä¸“å®¶é›†æˆï¼ˆDynamic Ensembleï¼‰â€ã€‚\n*   **æ–¹æ³•ä¸å‘ç°**ï¼šé€šè¿‡åœ¨ Expert Routing æœºåˆ¶ä¸­æ³¨å…¥å—æ§çš„éšæœºæ€§ï¼ŒRoE å¯¹æ¯ä¸ª Token é‡‡æ ·å¤šä¸ªä¸åŒçš„ä¸“å®¶å¹¶èšåˆè¾“å‡ºã€‚ä¸ºäº†è§£å†³è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œä½œè€…å¼•å…¥äº†é«˜æ•ˆçš„ Batching ç­–ç•¥å’Œä¸“ç”¨çš„ KV-caching æœºåˆ¶ã€‚\n*   **ç»“æœ**ï¼šæƒŠäººçš„æ˜¯ï¼ŒRoE èƒ½è®©ä¸€ä¸ª 7B çš„ MoE æ¨¡å‹è¾¾åˆ° 10.5B æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ¨ç†æ—¶å‡å°‘ 30% çš„è®¡ç®—é‡ï¼Œä¸”**ä¸éœ€è¦ä»»ä½•å¾®è°ƒ**ã€‚è¿™ä¸ºç°æœ‰çš„ MoE æ¨¡å‹æŒ–æ˜æ½œåŠ›æä¾›äº†æ–°æ€è·¯ã€‚\n\n**2. Adaptive Overclocking: Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals**\n**è‡ªé€‚åº”è¶…é¢‘ï¼šé€šè¿‡å®æ—¶æ¨ç†ä¿¡å·åŠ¨æ€æ§åˆ¶æ€ç»´è·¯å¾„é•¿åº¦**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ç›®å‰å¤§æ¨¡å‹ï¼ˆå¦‚ o1 ç±»ï¼‰åœ¨ç®€å•é—®é¢˜ä¸Šâ€œè¿‡åº¦æ€è€ƒâ€æµªè´¹ç®—åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€è°ƒæ•´æ¨ç†é¢„ç®—çš„æ–¹æ³•ã€‚\n*   **æ–¹æ³•**ï¼šä½œè€…è®¾è®¡äº† **Adaptive Overclocking**ï¼Œåˆ©ç”¨ Token çº§åˆ«çš„æ¨¡å‹ä¸ç¡®å®šæ€§å’Œè¾“å…¥å¤æ‚åº¦ä¼°è®¡ï¼Œå®æ—¶è°ƒæ•´æ¨ç†æ­¥æ•°ï¼ˆThinking Path Lengthï¼‰ã€‚\n*   **å½±å“**ï¼šåœ¨ GSM8K å’Œ MATH ç­‰æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶æ˜¾è‘—é™ä½äº†å»¶è¿Ÿã€‚è¿™å¯¹äºéƒ¨ç½²â€œæ…¢æ€è€ƒâ€æ¨¡å‹è‡³å…³é‡è¦ã€‚\n\n**3. PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models**\n**PTQTPï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒä¸‰å€¼å¹³é¢é‡åŒ–**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæŒ‘æˆ˜ 1.58-bit é‡åŒ–çš„æé™ã€‚ç°æœ‰çš„è¶…ä½æ¯”ç‰¹é‡åŒ–é€šå¸¸éœ€è¦é‡è®­ç»ƒï¼ˆQATï¼‰ï¼Œæˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº† **PTQTP**ï¼Œä¸€ç§åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æ¡†æ¶ã€‚\n*   **æ–¹æ³•**ï¼šå°†æƒé‡çŸ©é˜µåˆ†è§£ä¸ºåŒä¸‰å€¼ï¼ˆ{-1, 0, 1}ï¼‰å¹³é¢ï¼ˆTrit-Planesï¼‰ï¼Œå®ç°äº†æ— ä¹˜æ³•çš„åŠ æ³•æ¨ç†ã€‚\n*   **ç»“æœ**ï¼šåœ¨ LLaMA3 å’Œ Qwen3 ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä»£ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ 4-bit ä»¥ä¸‹ PTQ æ–¹æ³•ï¼Œç”šè‡³åŒ¹æ•Œ QAT çš„æ•ˆæœï¼Œä¸”é‡åŒ–è¿‡ç¨‹åªéœ€ 1 å°æ—¶ï¼ˆå¯¹æ¯” QAT çš„æ•°å¤©ï¼‰ã€‚\n\n**4. Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs**\n**ä¸Šä¸‹æ–‡æ‰æ˜¯ä½ éœ€è¦çš„ï¼šLLM ç°å®æé™çš„æœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå¯¹ç›®å‰å‚å•†å¹å˜˜çš„â€œè¶…é•¿ä¸Šä¸‹æ–‡çª—å£â€è¿›è¡Œäº†æ— æƒ…çš„ç¥›é­…å’Œæµ‹è¯•ã€‚\n*   **å‘ç°**ï¼šä½œè€…å®šä¹‰äº† **æœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£ (MECW)**ã€‚æµ‹è¯•å‘ç°ï¼Œè®¸å¤šå·ç§°æ‹¥æœ‰å·¨å¤§ä¸Šä¸‹æ–‡çª—å£çš„æ¨¡å‹ï¼Œåœ¨ä»… 1000 ä¸ª Token åå‡†ç¡®ç‡å°±å‡ºç°ä¸¥é‡é€€åŒ–ï¼Œéƒ¨åˆ†é¡¶çº§æ¨¡å‹ç”šè‡³åœ¨ 100 ä¸ª Token æ—¶å°±å¤±æ•ˆã€‚\n*   **ç»“è®º**ï¼šMECW ä¸å‚å•†å®£ç§°çš„ MCW å­˜åœ¨å·¨å¤§å·®è·ï¼ˆæœ‰æ—¶é«˜è¾¾ 99% çš„ç¼©æ°´ï¼‰ã€‚è¿™æé†’æˆ‘ä»¬åœ¨å®é™…åº”ç”¨ä¸­ä¸è¦ç›²ç›®ä¿¡ä»»æ¨¡å‹çš„é•¿æ–‡å¤„ç†èƒ½åŠ›ã€‚\n\n**5. Probabilistic Token Alignment for Large Language Model Fusion**\n**ç”¨äºå¤§è¯­è¨€æ¨¡å‹èåˆçš„æ¦‚ç‡ Token å¯¹é½**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ¨¡å‹èåˆï¼ˆModel Fusionï¼‰æ˜¯ä½æˆæœ¬æå‡èƒ½åŠ›çš„æ·å¾„ï¼Œä½†ä¸åŒæ¨¡å‹çš„è¯è¡¨å¯¹é½æ˜¯éš¾ç‚¹ã€‚\n*   **æ–¹æ³•**ï¼šä½œè€…æå‡º **PTA-LLM**ï¼Œåˆ©ç”¨æœ€ä¼˜ä¼ è¾“ï¼ˆOptimal Transportï¼‰ç†è®ºï¼Œå°† Token å¯¹é½é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒåŒ¹é…é—®é¢˜ã€‚\n*   **æ„ä¹‰**ï¼šè¿™ç§è½¯å¯¹é½æ–¹æ³•æ¯”ä¼ ç»Ÿçš„ç¡¬æ˜ å°„æ›´é²æ£’ï¼Œèƒ½æœ‰æ•ˆèåˆå…·æœ‰ä¸åŒæ¶æ„å’Œè¯è¡¨çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚\n\n---\n\n### ğŸ¤– æ™ºèƒ½ä½“ä¸å¤æ‚ç¯å¢ƒ (Agents & Environments)\n\n**6. ARE: Scaling Up Agent Environments and Evaluations**\n**AREï¼šæ‰©å±• Agent ç¯å¢ƒä¸è¯„ä¼°**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ¥è‡ª Meta çš„ç ”ç©¶ã€‚æ¨å‡ºäº† **Meta Agents Research Environments (ARE)** å¹³å°å’Œ **Gaia2** åŸºå‡†æµ‹è¯•ã€‚\n*   **äº®ç‚¹**ï¼šGaia2 å¼ºè°ƒå¼‚æ­¥æ€§ã€æ¨¡ç³Šæ€§å¤„ç†å’Œå¤š Agent åä½œï¼Œæ¯”é™æ€åŸºå‡†æ›´è´´è¿‘ç°å®ã€‚å®éªŒè¡¨æ˜ï¼Œæ›´å¼ºçš„æ¨ç†èƒ½åŠ›å¾€å¾€ä»¥ç‰ºç‰²æ•ˆç‡ä¸ºä»£ä»·ï¼Œä¸”ç›®å‰çš„ç®—åŠ›æ‰©å±•æ›²çº¿åœ¨ Agent ä»»åŠ¡ä¸Šå‡ºç°äº†å¹³å°æœŸï¼Œæš—ç¤ºæˆ‘ä»¬éœ€è¦æ–°çš„æ¶æ„ã€‚\n\n**7. MCTS-EP: Empowering Embodied Planning with Online Preference Optimization**\n**MCTS-EPï¼šé€šè¿‡åœ¨çº¿åå¥½ä¼˜åŒ–èµ‹èƒ½å…·èº«è§„åˆ’**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä¸ LLM ç»“åˆï¼Œç”¨äºå…·èº«æ™ºèƒ½ä½“ï¼ˆEmbodied Agentsï¼‰çš„è§„åˆ’ã€‚\n*   **æ–¹æ³•**ï¼šåˆ©ç”¨ MCTS æŒ‡å¯¼æ¢ç´¢å¹¶æ”¶é›†åå¥½æ•°æ®ï¼Œè¿›è¡Œåœ¨çº¿å­¦ä¹ ã€‚\n*   **ç»“æœ**ï¼šåœ¨ ALFWorld å’Œ WebShop ä¸Šè¾¾åˆ°äº† SOTAï¼Œæ˜¾è‘—å‡å°‘äº†å®Œæˆä»»åŠ¡æ‰€éœ€çš„äº¤äº’æ­¥æ•°ã€‚è¯æ˜äº†â€œæœç´¢+å­¦ä¹ â€åœ¨å…·èº«æ™ºèƒ½ä¸­çš„å¨åŠ›ã€‚\n\n**8. Agentic AI for Multi-Stage Physics Experiments at a Large-Scale User Facility Particle Accelerator**\n**åœ¨å¤§è§„æ¨¡ç”¨æˆ·è®¾æ–½ç²’å­åŠ é€Ÿå™¨ä¸­ç”¨äºå¤šé˜¶æ®µç‰©ç†å®éªŒçš„ Agentic AI**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šAI Agent åœ¨ç¡¬æ ¸ç§‘å­¦é¢†åŸŸçš„è½åœ°ã€‚åœ¨ Advanced Light Source åŠ é€Ÿå™¨ä¸Šï¼Œéƒ¨ç½²äº†é¦–ä¸ªç”± LLM é©±åŠ¨çš„ Agent ç³»ç»Ÿã€‚\n*   **æ•ˆæœ**ï¼šç³»ç»Ÿèƒ½è‡ªä¸»å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºå¤æ‚çš„æ§åˆ¶è„šæœ¬ï¼Œå°†å®éªŒå‡†å¤‡æ—¶é—´**ç¼©çŸ­äº†ä¸¤ä¸ªæ•°é‡çº§**ï¼ŒåŒæ—¶ä¸¥æ ¼éµå®ˆå®‰å…¨çº¦æŸã€‚è¿™æ˜¯ AI for Science çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ã€‚\n\n---\n\n### ğŸ§¬ AI for Science & Medicine\n\n**9. Training the next generation of physicians for artificial intelligence-assisted clinical neuroradiology: BraTS 2025**\n**ä¸º AI è¾…åŠ©ä¸´åºŠç¥ç»æ”¾å°„å­¦åŸ¹å…»ä¸‹ä¸€ä»£åŒ»ç”Ÿï¼šBraTS 2025 æŒ‘æˆ˜èµ›æ•™è‚²å¹³å°**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè‘—åçš„è„‘è‚¿ç˜¤åˆ†å‰²æŒ‘æˆ˜èµ›ï¼ˆBraTSï¼‰ä¸ä»…æ˜¯ç®—æ³•ç«èµ›ï¼Œç°åœ¨æ›´æ˜¯åŒ»å­¦æ•™è‚²å¹³å°ã€‚æ–‡ç« è¯¦ç»†ä»‹ç»äº†å¦‚ä½•é€šè¿‡è®©åŒ»å­¦ç”Ÿå‚ä¸æ•°æ®æ ‡æ³¨ï¼ˆå¹³å‡æ¯äººè€—æ—¶ 1300+ å°æ—¶ï¼ï¼‰ï¼Œæå‡ä»–ä»¬å¯¹ AI å’Œç¥ç»ç—…ç†å­¦çš„ç†è§£ã€‚\n\n**10. DiffSyn: A Generative Diffusion Approach to Materials Synthesis Planning**\n**DiffSynï¼šææ–™åˆæˆè§„åˆ’çš„ç”Ÿæˆå¼æ‰©æ•£æ–¹æ³•**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šè§£å†³ææ–™ç§‘å­¦ä¸­çš„â€œé…æ–¹â€éš¾é¢˜ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œæ ¹æ®ç›®æ ‡æ™¶ä½“ç»“æ„ï¼ˆå¦‚æ²¸çŸ³ï¼‰ç”Ÿæˆåˆæˆè·¯å¾„ã€‚\n*   **éªŒè¯**ï¼šæ¨¡å‹ç”Ÿæˆçš„åˆæˆè·¯çº¿æˆåŠŸåˆæˆäº†ä¸€ç§å…·æœ‰æé«˜çƒ­ç¨³å®šæ€§çš„ UFI ææ–™ï¼ŒSi/Al æ¯”æ‰“ç ´äº†å†å²è®°å½•ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ä¸å¤šæ¨¡æ€ (Vision & Multimodal)\n\n**11. The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA**\n**ç¬¬ä¸ƒå±Š LSVOS RVOS èµ›é“å† å†›æ–¹æ¡ˆï¼šSaSaSa2VA**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå­—èŠ‚è·³åŠ¨å›¢é˜Ÿåœ¨å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆRVOSï¼‰æ¯”èµ›ä¸­çš„å¤ºå† æ–¹æ¡ˆã€‚\n*   **æ–¹æ³•**ï¼šç»“åˆäº† MLLM å’Œ SAM2ã€‚æ ¸å¿ƒæ”¹è¿›åœ¨äºè§£å†³äº† SAM2 çš„ç¨€ç–å¸§é‡‡æ ·é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†å¤šåˆ†å‰²å¢å¼ºå’Œé€‰æ‹©æ€§å¹³å‡ç­–ç•¥ã€‚\n\n**12. Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models**\n**é’ˆå¯¹æ–‡ç”Ÿå›¾æ¨¡å‹å®‰å…¨è¿‡æ»¤å™¨çš„å¤šæ¨¡æ€æç¤ºè§£è€¦æ”»å‡»**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šä¸€ç§é’ˆå¯¹ DALL-E 3 ç­‰æ¨¡å‹çš„è¶Šç‹±ï¼ˆJailbreakï¼‰æ”»å‡»ã€‚\n*   **æ–¹æ³•**ï¼šåˆ©ç”¨å›¾åƒæ¨¡æ€ä½œä¸ºæ©æŠ¤ã€‚å°†ä¸å®‰å…¨çš„ Prompt è§£è€¦ï¼Œä¸€éƒ¨åˆ†ä¼ªè£…æˆå®‰å…¨çš„æ–‡æœ¬ï¼Œå¦ä¸€éƒ¨åˆ†éšå«åœ¨å›¾åƒä¸­ï¼Œç»•è¿‡å®‰å…¨è¿‡æ»¤å™¨ç”Ÿæˆ NSFW å†…å®¹ã€‚è¿™ä¸º T2I æ¨¡å‹çš„é˜²å¾¡æå‡ºäº†æ–°æŒ‘æˆ˜ã€‚\n\n**13. Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds**\n**Point-RTDï¼šç”¨äºç‚¹äº‘ Transformer é¢„è®­ç»ƒçš„æ›¿æ¢ Token å»å™ª**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹ 3D ç‚¹äº‘çš„é¢„è®­ç»ƒæ–°èŒƒå¼ã€‚ä¸åŒäºä¼ ç»Ÿçš„ Masked Autoencoder (MAE)ï¼Œæœ¬æ–‡é‡‡ç”¨ç±»ä¼¼ ELECTRA çš„æ€è·¯ï¼ŒæŸå Token å¹¶åˆ©ç”¨åˆ¤åˆ«å™¨-ç”Ÿæˆå™¨æ¶æ„è¿›è¡Œå»å™ªã€‚åœ¨ ShapeNet ä¸Šé‡æ„è¯¯å·®é™ä½äº† 93%ã€‚",
  "papers": [
    {
      "arxiv_id": "2509.17283v2",
      "title": "Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models",
      "title_zh": "åŸºäºé—¨æ£€æµ‹ä¸å¤§è¯­è¨€æ¨¡å‹çš„å»ºç­‘åˆè§„æ€§æ£€æŸ¥è‡ªåŠ¨åŒ–è®¾æ–½æšä¸¾",
      "authors": [
        "Licheng Zhang",
        "Bach Le",
        "Naveed Akhtar",
        "Tuan Ngo"
      ],
      "abstract": "Building compliance checking (BCC) is a critical process for ensuring that constructed facilities meet regulatory standards. A core component of BCC is the accurate enumeration of facility types and their spatial distribution. Despite its importance, this problem has been largely overlooked in the literature, posing a significant challenge for BCC and leaving a critical gap in existing workflows. Performing this task manually is time-consuming and labor-intensive. Recent advances in large language models (LLMs) offer new opportunities to enhance automation by combining visual recognition with reasoning capabilities. In this paper, we introduce a new task for BCC: automated facility enumeration, which involves validating the quantity of each facility type against statutory requirements. To address it, we propose a novel method that integrates door detection with LLM-based reasoning. We are the first to apply LLMs to this task and further enhance their performance through a Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse datasets and facility types. Experiments on both real-world and synthetic floor plan data demonstrate the effectiveness and robustness of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å»ºç­‘åˆè§„æ€§æ£€æŸ¥ (Building compliance checking, BCC) ä¸­è®¾æ–½æ¸…ç‚¹ä»»åŠ¡è€—æ—¶ä¸”æ˜“è¢«å¿½è§†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆé—¨æ£€æµ‹ (door detection) ä¸å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) æ¨ç†çš„è‡ªåŠ¨åŒ–è®¾æ–½æ¸…ç‚¹æ–¹æ³•ã€‚ä½œä¸ºé¦–ä¸ªå°† LLMs åº”ç”¨äºè¯¥é¢†åŸŸçš„å°è¯•ï¼Œç ”ç©¶é€šè¿‡å¼•å…¥é“¾å¼æ€ç»´ (Chain-of-Thought, CoT) æµç¨‹æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨éªŒè¯è®¾æ–½æ•°é‡ä¸æ³•å®šè¦æ±‚ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœåœ¨çœŸå®ä¸–ç•ŒåŠåˆæˆå¹³é¢å›¾æ•°æ®ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸é²æ£’æ€§ï¼Œå±•ç¤ºå‡ºå…¶åœ¨ä¸åŒè®¾æ–½ç±»å‹å’Œæ•°æ®é›†ä¸Šçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ¡ˆå¡«è¡¥äº†ç°æœ‰ BCC å·¥ä½œæµç¨‹ä¸­çš„å…³é”®ç©ºç™½ï¼Œä¸ºå®ç°å…¨è‡ªåŠ¨åŒ–çš„å»ºç­‘ç›‘ç®¡ä¸åˆè§„æ€§å®¡æŸ¥å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CV",
      "comment": "Author name correction in the second version (same content as the first version)",
      "pdf_url": "https://arxiv.org/pdf/2509.17283v2",
      "published_date": "2025-09-21 23:41:44 UTC",
      "updated_date": "2025-09-26 11:31:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:06.885629+00:00"
    },
    {
      "arxiv_id": "2509.17281v1",
      "title": "Training the next generation of physicians for artificial intelligence-assisted clinical neuroradiology: ASNR MICCAI Brain Tumor Segmentation (BraTS) 2025 Lighthouse Challenge education platform",
      "title_zh": "åŸ¹å…»é¢å‘äººå·¥æ™ºèƒ½è¾…åŠ©ä¸´åºŠç¥ç»æ”¾å°„å­¦çš„ä¸‹ä¸€ä»£åŒ»å¸ˆï¼šASNR MICCAI 2025å¹´è„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰ç¯å¡”æŒ‘æˆ˜èµ›æ•™è‚²å¹³å°",
      "authors": [
        "Raisa Amiruddin",
        "Nikolay Y. Yordanov",
        "Nazanin Maleki",
        "Pascal Fehringer",
        "Athanasios Gkampenis",
        "Anastasia Janas",
        "Kiril Krantchev",
        "Ahmed Moawad",
        "Fabian Umeh",
        "Salma Abosabie",
        "Sara Abosabie",
        "Albara Alotaibi",
        "Mohamed Ghonim",
        "Mohanad Ghonim",
        "Sedra Abou Ali Mhana",
        "Nathan Page",
        "Marko Jakovljevic",
        "Yasaman Sharifi",
        "Prisha Bhatia",
        "Amirreza Manteghinejad",
        "Melisa Guelen",
        "Michael Veronesi",
        "Virginia Hill",
        "Tiffany So",
        "Mark Krycia",
        "Bojan Petrovic",
        "Fatima Memon",
        "Justin Cramer",
        "Elizabeth Schrickel",
        "Vilma Kosovic",
        "Lorenna Vidal",
        "Gerard Thompson",
        "Ichiro Ikuta",
        "Basimah Albalooshy",
        "Ali Nabavizadeh",
        "Nourel Hoda Tahon",
        "Karuna Shekdar",
        "Aashim Bhatia",
        "Claudia Kirsch",
        "Gennaro D'Anna",
        "Philipp Lohmann",
        "Amal Saleh Nour",
        "Andriy Myronenko",
        "Adam Goldman-Yassen",
        "Janet R. Reid",
        "Sanjay Aneja",
        "Spyridon Bakas",
        "Mariam Aboian"
      ],
      "abstract": "High-quality reference standard image data creation by neuroradiology experts for automated clinical tools can be a powerful tool for neuroradiology & artificial intelligence education. We developed a multimodal educational approach for students and trainees during the MICCAI Brain Tumor Segmentation Lighthouse Challenge 2025, a landmark initiative to develop accurate brain tumor segmentation algorithms. Fifty-six medical students & radiology trainees volunteered to annotate brain tumor MR images for the BraTS challenges of 2023 & 2024, guided by faculty-led didactics on neuropathology MRI. Among the 56 annotators, 14 select volunteers were then paired with neuroradiology faculty for guided one-on-one annotation sessions for BraTS 2025. Lectures on neuroanatomy, pathology & AI, journal clubs & data scientist-led workshops were organized online. Annotators & audience members completed surveys on their perceived knowledge before & after annotations & lectures respectively. Fourteen coordinators, each paired with a neuroradiologist, completed the data annotation process, averaging 1322.9+/-760.7 hours per dataset per pair and 1200 segmentations in total. On a scale of 1-10, annotation coordinators reported significant increase in familiarity with image segmentation software pre- and post-annotation, moving from initial average of 6+/-2.9 to final average of 8.9+/-1.1, and significant increase in familiarity with brain tumor features pre- and post-annotation, moving from initial average of 6.2+/-2.4 to final average of 8.1+/-1.2. We demonstrate an innovative offering for providing neuroradiology & AI education through an image segmentation challenge to enhance understanding of algorithm development, reinforce the concept of data reference standard, and diversify opportunities for AI-driven image analysis among future physicians.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†ASNR MICCAI Brain Tumor Segmentation (BraTS) 2025 Lighthouse Challengeæ•™è‚²å¹³å°ï¼Œæ—¨åœ¨é€šè¿‡äººå·¥æ™ºèƒ½è¾…åŠ©çš„ä¸´åºŠç¥ç»æ”¾å°„å­¦ï¼ˆneuroradiologyï¼‰å®è·µåŸ¹å…»ä¸‹ä¸€ä»£åŒ»ç”Ÿã€‚ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€æ•™è‚²æ–¹æ³•ï¼Œç»„ç»‡56ååŒ»å­¦ç”Ÿå’Œæ”¾å°„ç§‘å—è®­äººå‘˜å‚ä¸BraTSæŒ‘æˆ˜èµ›çš„è„‘è‚¿ç˜¤MRå›¾åƒæ ‡æ³¨ï¼Œå¹¶ç”±å¯¼å¸ˆè¿›è¡Œç¥ç»ç—…ç†å­¦MRIæ•™å­¦ã€‚å…¶ä¸­14åæ ¸å¿ƒå¿—æ„¿è€…é€šè¿‡ä¸ç¥ç»æ”¾å°„å­¦ä¸“å®¶è¿›è¡Œä¸€å¯¹ä¸€çš„å¼•å¯¼å¼æ ‡æ³¨ï¼ˆguided one-on-one annotationï¼‰ï¼Œç»“åˆåœ¨çº¿è®²åº§ã€æ–‡çŒ®ç ”è®¨ä¼šï¼ˆjournal clubsï¼‰ä»¥åŠç”±æ•°æ®ç§‘å­¦å®¶ä¸»æŒçš„å·¥ä½œåŠï¼Œæ·±å…¥å­¦ä¹ äº†ç¥ç»è§£å‰–å­¦ã€ç—…ç†å­¦åŠAIç›¸å…³çŸ¥è¯†ã€‚æ ‡æ³¨å›¢é˜Ÿç´¯è®¡å®Œæˆäº†1200ä»½åˆ†å‰²ä»»åŠ¡ï¼Œè°ƒæŸ¥ç»“æœæ˜¾ç¤ºå‚ä¸è€…å¯¹å›¾åƒåˆ†å‰²è½¯ä»¶ï¼ˆimage segmentation softwareï¼‰çš„ç†Ÿæ‚‰åº¦è¯„åˆ†ä»6.0æ˜¾è‘—æå‡è‡³8.9ï¼Œå¯¹è„‘è‚¿ç˜¤ç‰¹å¾çš„ç†è§£ä¹Ÿä»6.2æå‡è‡³8.1ã€‚è¯¥é¡¹ç›®è¯æ˜äº†é€šè¿‡å›¾åƒåˆ†å‰²æŒ‘æˆ˜èµ›å¼€å±•ç¥ç»æ”¾å°„å­¦ä¸AIæ•™è‚²çš„åˆ›æ–°æ€§ï¼Œä¸ä»…åŠ å¼ºäº†æœªæ¥åŒ»ç”Ÿå¯¹ç®—æ³•å¼€å‘å’Œæ•°æ®å‚è€ƒæ ‡å‡†ï¼ˆdata reference standardï¼‰æ¦‚å¿µçš„ç†è§£ï¼Œè¿˜ä¸ºAIé©±åŠ¨çš„å›¾åƒåˆ†ææä¾›äº†å¤šå…ƒåŒ–çš„å­¦ä¹ æœºä¼šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 9 figures, 1 table, 3 supplementary tables",
      "pdf_url": "https://arxiv.org/pdf/2509.17281v1",
      "published_date": "2025-09-21 23:39:32 UTC",
      "updated_date": "2025-09-21 23:39:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:25.883404+00:00"
    },
    {
      "arxiv_id": "2509.17280v1",
      "title": "From Prediction to Understanding: Will AI Foundation Models Transform Brain Science?",
      "title_zh": "ä»é¢„æµ‹åˆ°ç†è§£ï¼šäººå·¥æ™ºèƒ½åŸºç¡€æ¨¡å‹èƒ½å¦å˜é©è„‘ç§‘å­¦ï¼Ÿ",
      "authors": [
        "Thomas Serre",
        "Ellie Pavlick"
      ],
      "abstract": "Generative pretraining (the \"GPT\" in ChatGPT) enables language models to learn from vast amounts of internet text without human supervision. This approach has driven breakthroughs across AI by allowing deep neural networks to learn from massive, unstructured datasets. We use the term foundation models to refer to large pretrained systems that can be adapted to a wide range of tasks within and across domains, and these models are increasingly applied beyond language to the brain sciences. These models achieve strong predictive accuracy, raising hopes that they might illuminate computational principles. But predictive success alone does not guarantee scientific understanding. Here, we outline how foundation models can be productively integrated into the brain sciences, highlighting both their promise and their limitations. The central challenge is to move from prediction to explanation: linking model computations to mechanisms underlying neural activity and cognition.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºç”Ÿæˆå¼é¢„è®­ç»ƒ(generative pretraining)çš„åŸºåº§æ¨¡å‹(foundation models)å¦‚ä½•ä»è¯­è¨€é¢†åŸŸæ‰©å±•åˆ°è„‘ç§‘å­¦(brain sciences)é¢†åŸŸã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§(predictive accuracy)ä¸Šå–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†ç ”ç©¶æŒ‡å‡ºå•çº¯çš„é¢„æµ‹æˆåŠŸå¹¶ä¸ç­‰åŒäºçœŸæ­£çš„ç§‘å­¦ç†è§£(scientific understanding)ã€‚æ–‡ç« è¯¦ç»†é˜è¿°äº†å°†åŸºåº§æ¨¡å‹æ•´åˆè¿›è„‘ç§‘å­¦ç ”ç©¶çš„é«˜æ•ˆè·¯å¾„ï¼Œå¹¶å®¢è§‚åˆ†æäº†å…¶åœ¨æ­ç¤ºè®¡ç®—åŸç†æ–¹é¢çš„å·¨å¤§æ½œåŠ›ä¸å±€é™æ€§ã€‚ç ”ç©¶å¼ºè°ƒï¼Œç›®å‰çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå®ç°ä»é¢„æµ‹(prediction)åˆ°è§£é‡Š(explanation)çš„è·¨è¶Šï¼Œå³å°†æ¨¡å‹çš„è®¡ç®—è¿‡ç¨‹ä¸ç¥ç»æ´»åŠ¨åŠè®¤çŸ¥çš„åº•å±‚æœºåˆ¶(mechanisms)å»ºç«‹èµ·æ˜ç¡®çš„é€»è¾‘å…³è”ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17280v1",
      "published_date": "2025-09-21 23:39:04 UTC",
      "updated_date": "2025-09-21 23:39:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:09.393342+00:00"
    },
    {
      "arxiv_id": "2509.17276v1",
      "title": "Probabilistic Token Alignment for Large Language Model Fusion",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹èåˆçš„æ¦‚ç‡è¯å…ƒå¯¹é½",
      "authors": [
        "Runjia Zeng",
        "James Chenhao Liang",
        "Cheng Han",
        "Zhiwen Cao",
        "Jiahao Liu",
        "Xiaojun Quan",
        "Yingjie Victor Chen",
        "Lifu Huang",
        "Tong Geng",
        "Qifan Wang",
        "Dongfang Liu"
      ],
      "abstract": "Training large language models (LLMs) from scratch can yield models with unique functionalities and strengths, but it is costly and often leads to redundant capabilities. A more cost-effective alternative is to fuse existing pre-trained LLMs with different architectures into a more powerful model. However, a key challenge in existing model fusion is their dependence on manually predefined vocabulary alignment, which may not generalize well across diverse contexts, leading to performance degradation in several evaluation. To solve this, we draw inspiration from distribution learning and propose the probabilistic token alignment method as a general and soft mapping for alignment, named as PTA-LLM. Our approach innovatively reformulates token alignment into a classic mathematical problem: optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability from a distributional perspective, offering insights into the essence of the token alignment. Empirical results demonstrate that probabilistic token alignment enhances the target model's performance across multiple capabilities. Our code is avaliable at https://runjia.tech/neurips_pta-llm/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)èåˆè¿‡ç¨‹ä¸­å¯¹æ‰‹åŠ¨é¢„å®šä¹‰è¯æ±‡å¯¹é½çš„ä¾èµ–ä»¥åŠæ³›åŒ–æ€§èƒ½å—é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†åä¸ºPTA-LLMçš„æ¦‚ç‡æ ‡è®°å¯¹é½(Probabilistic Token Alignment)æ–¹æ³•ã€‚PTA-LLMé€šè¿‡å°†æ ‡è®°å¯¹é½åˆ›æ–°æ€§åœ°é‡æ–°è¡¨è¿°ä¸ºæœ€ä¼˜ä¼ è¾“(Optimal Transport)è¿™ä¸€ç»å…¸æ•°å­¦é—®é¢˜ï¼Œå¹¶ç»“åˆåˆ†å¸ƒæ„ŸçŸ¥å­¦ä¹ (Distribution-aware Learning)å®ç°äº†æ›´åŠ è¿è´¯çš„æ¨¡å‹èåˆã€‚è¿™ç§æ–¹æ³•ä¸ä»…å…·æœ‰æ˜¾è‘—çš„é€šç”¨æ€§ï¼Œè¿˜èƒ½ä»åˆ†å¸ƒè§†è§’ä¸ºæ ‡è®°å¯¹é½çš„æœ¬è´¨æä¾›è§£é‡Šæ€§è§è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¦‚ç‡æ ‡è®°å¯¹é½æŠ€æœ¯æœ‰æ•ˆæå‡äº†ç›®æ ‡æ¨¡å‹åœ¨å¤šç§èƒ½åŠ›è¯„ä¼°ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºä¸åŒæ¶æ„é¢„è®­ç»ƒæ¨¡å‹çš„ä½æˆæœ¬é«˜æ•ˆæ•´åˆæä¾›äº†æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.17276v1",
      "published_date": "2025-09-21 23:18:24 UTC",
      "updated_date": "2025-09-21 23:18:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:13.749465+00:00"
    },
    {
      "arxiv_id": "2509.18218v4",
      "title": "Similarity Field Theory: A Mathematical Framework for Intelligence",
      "title_zh": "ç›¸ä¼¼åœºè®ºï¼šæ™ºèƒ½çš„æ•°å­¦æ¡†æ¶",
      "authors": [
        "Kei-Sing Ng"
      ],
      "abstract": "We posit that persisting and transforming similarity relations form the structural basis of any comprehensible dynamic system. This paper introduces Similarity Field Theory, a mathematical framework that formalizes the principles governing similarity values among entities and their evolution. We define: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed relational field (asymmetry and non-transitivity are allowed); (2) the evolution of a system through a sequence $Z_p=(X_p,S^{(p)})$ indexed by $p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers $F_Î±(K)={E\\in U \\mid S(E,K)\\ge Î±}$, i.e., superlevel sets of the unary map $S_K(E):=S(E,K)$; and (4) a generative operator $G$ that produces new entities. Within this framework, we formalize a generative definition of intelligence: an operator $G$ is intelligent with respect to a concept $K$ if, given a system containing entities belonging to the fiber of $K$, it generates new entities that also belong to that fiber. Similarity Field Theory thus offers a foundational language for characterizing, comparing, and constructing intelligent systems. At a high level, this framework reframes intelligence and interpretability as geometric problems on similarity fields--preserving and composing level-set fibers--rather than purely statistical ones. We prove two theorems: (i) asymmetry blocks mutual inclusion; and (ii) stability implies either an anchor coordinate or asymptotic confinement to the target level (up to arbitrarily small tolerance). Together, these results constrain similarity-field evolution and motivate an interpretive lens that can be applied to large language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Similarity Field Theoryï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å½¢å¼åŒ–å®ä½“é—´ç›¸ä¼¼å€¼åŠå…¶æ¼”åŒ–è§„å¾‹çš„æ•°å­¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†ä½œç”¨äºå®ä½“å…¨é›†ä¸Šçš„ç›¸ä¼¼åœº(similarity field)ã€ç³»ç»Ÿçš„æ¼”åŒ–åºåˆ—ã€ä»¥åŠç”±ç›¸ä¼¼åº¦é˜ˆå€¼è¯±å¯¼çš„çº¤ç»´é›†(fibers)æ¦‚å¿µã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ç”Ÿæˆå¼æ™ºèƒ½å®šä¹‰ï¼Œå³å°†æ™ºèƒ½è§†ä¸ºä¸€ç§ç”Ÿæˆç®—å­ï¼Œèƒ½å¤Ÿæ ¹æ®å±äºç‰¹å®šæ¦‚å¿µçº¤ç»´é›†çš„ç°æœ‰å®ä½“ç”ŸæˆåŒæ ·å±äºè¯¥çº¤ç»´é›†çš„æ–°å®ä½“ã€‚Similarity Field Theory å°†æ™ºèƒ½å’Œå¯è§£é‡Šæ€§é—®é¢˜ä»çº¯ç²¹çš„ç»Ÿè®¡å­¦è§†è§’é‡æ„ä¸ºç›¸ä¼¼åœºä¸Šçš„å‡ ä½•é—®é¢˜ï¼Œä¾§é‡äºæ°´å¹³é›†çº¤ç»´çš„ä¿æŒä¸ç»„åˆã€‚è®ºæ–‡é€šè¿‡ä¸¤ä¸ªå…³é”®å®šç†è¯æ˜äº†ä¸å¯¹ç§°æ€§ä¸ç¨³å®šæ€§å¯¹ç›¸ä¼¼åœºæ¼”åŒ–çš„çº¦æŸä½œç”¨ï¼Œä¸ºåˆ»ç”»å’Œæ„å»ºæ™ºèƒ½ç³»ç»Ÿæä¾›äº†åŸºç¡€è¯­è¨€ï¼Œå¹¶ä¸ºå¤§è¯­è¨€æ¨¡å‹(large language models)çš„è§£é‡Šæ€§åˆ†ææä¾›äº†ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18218v4",
      "published_date": "2025-09-21 22:34:00 UTC",
      "updated_date": "2025-12-23 18:09:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:17.382824+00:00"
    },
    {
      "arxiv_id": "2509.21363v1",
      "title": "A Mutual Learning Method for Salient Object Detection with intertwined Multi-Supervision--Revised",
      "title_zh": "åŸºäºäº¤ç»‡å¤šé‡ç›‘ç£çš„æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹äº’å­¦ä¹ æ–¹æ³•ï¼ˆä¿®è®¢ç‰ˆï¼‰",
      "authors": [
        "Runmin Wu",
        "Mengyang Feng",
        "Wenlong Guan",
        "Dong Wang",
        "Huchuan Lu",
        "Errui Ding"
      ],
      "abstract": "Though deep learning techniques have made great progress in salient object detection recently, the predicted saliency maps still suffer from incomplete predictions due to the internal complexity of objects and inaccurate boundaries caused by strides in convolution and pooling operations. To alleviate these issues, we propose to train saliency detection networks by exploiting the supervision from not only salient object detection, but also foreground contour detection and edge detection. First, we leverage salient object detection and foreground contour detection tasks in an intertwined manner to generate saliency maps with uniform highlight. Second, the foreground contour and edge detection tasks guide each other simultaneously, thereby leading to precise foreground contour prediction and reducing the local noises for edge prediction. In addition, we develop a novel mutual learning module (MLM) which serves as the building block of our method. Each MLM consists of multiple network branches trained in a mutual learning manner, which improves the performance by a large margin. Extensive experiments on seven challenging datasets demonstrate that the proposed method has delivered state-of-the-art results in both salient object detection and edge detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Salient Object Detection ä¸­ç”±äºç›®æ ‡å¤æ‚æ€§å¯¼è‡´çš„é¢„æµ‹ä¸å®Œæ•´ä»¥åŠå·ç§¯æ± åŒ–æ“ä½œå¼•èµ·çš„è¾¹ç•Œä¸å‡†ç¡®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäº¤ç»‡å¤šç›‘ç£ (intertwined Multi-Supervision) çš„ç›¸äº’å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å°†æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ã€å‰æ™¯è½®å»“æ£€æµ‹ (foreground contour detection) å’Œè¾¹ç¼˜æ£€æµ‹ (edge detection) ä»»åŠ¡ä»¥äº¤ç»‡æ–¹å¼ç»“åˆï¼Œå®ç°äº†å‡åŒ€çš„é«˜äº®é¢„æµ‹å¹¶æœ‰æ•ˆå‡å°‘äº†è¾¹ç¼˜é¢„æµ‹ä¸­çš„å±€éƒ¨å™ªå£°ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå¼€å‘äº†ä¸€ç§æ–°å‹ç›¸äº’å­¦ä¹ æ¨¡å— (Mutual Learning Module, MLM)ï¼Œåˆ©ç”¨å¤šä¸ªç½‘ç»œåˆ†æ”¯çš„ç›¸äº’å­¦ä¹ æœºåˆ¶å¤§å¹…æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ƒä¸ªæŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šå‡å–å¾—äº† state-of-the-art çš„è¡¨ç°ï¼Œèƒ½å¤ŸåŒæ—¶åœ¨ Salient Object Detection å’Œè¾¹ç¼˜æ£€æµ‹ä»»åŠ¡ä¸­å®ç°ç²¾ç¡®çš„é¢„æµ‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.21363v1",
      "published_date": "2025-09-21 22:30:32 UTC",
      "updated_date": "2025-09-21 22:30:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:18.656982+00:00"
    },
    {
      "arxiv_id": "2509.17259v1",
      "title": "Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B",
      "title_zh": "å…³æ³¨å·®è·ï¼šåŸºäºåŠ¨ä½œå›¾å¯è§‚æµ‹æ€§çš„ GPT-OSS-20B æ¨¡å‹çº§ä¸æ™ºèƒ½ä½“çº§çº¢é˜Ÿæµ‹è¯•å¯¹æ¯”åˆ†æ",
      "authors": [
        "Ilham Wicaksono",
        "Zekun Wu",
        "Rahul Patel",
        "Theo King",
        "Adriano Koshiyama",
        "Philip Treleaven"
      ],
      "abstract": "As the industry increasingly adopts agentic AI systems, understanding their unique vulnerabilities becomes critical. Prior research suggests that security flaws at the model level do not fully capture the risks present in agentic deployments, where models interact with tools and external environments. This paper investigates this gap by conducting a comparative red teaming analysis of GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability framework AgentSeer to deconstruct agentic systems into granular actions and components, we apply iterative red teaming attacks with harmful objectives from HarmBench at two distinct levels: the standalone model and the model operating within an agentic loop. Our evaluation reveals fundamental differences between model level and agentic level vulnerability profiles. Critically, we discover the existence of agentic-only vulnerabilities, attack vectors that emerge exclusively within agentic execution contexts while remaining inert against standalone models. Agentic level iterative attacks successfully compromise objectives that completely failed at the model level, with tool-calling contexts showing 24\\% higher vulnerability than non-tool contexts. Conversely, certain model-specific exploits work exclusively at the model level and fail when transferred to agentic contexts, demonstrating that standalone model vulnerabilities do not always generalize to deployed systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ™ºèƒ½ä½“(Agentic AI)ç³»ç»Ÿä¸å•ä½“æ¨¡å‹(Standalone Models)åœ¨å®‰å…¨æ€§æ–¹é¢çš„å·®å¼‚ï¼Œé€šè¿‡å¯¹GPT-OSS-20Bæ¨¡å‹è¿›è¡Œå¯¹æ¯”çº¢é˜Ÿæµ‹è¯•(Red Teaming)åˆ†æäº†å…¶å®‰å…¨æ¼æ´ã€‚ç ”ç©¶é‡‡ç”¨äº†AgentSeerå¯è§‚æµ‹æ€§æ¡†æ¶ï¼Œå°†æ™ºèƒ½ä½“ç³»ç»Ÿæ‹†è§£ä¸ºç»†ç²’åº¦çš„åŠ¨ä½œå’Œç»„ä»¶ï¼Œå¹¶åŸºäºHarmBenchçš„æœ‰å®³ç›®æ ‡åœ¨å•ä½“æ¨¡å‹å’Œæ™ºèƒ½ä½“å¾ªç¯ä¸¤ç§ç¯å¢ƒä¸‹å®æ–½è¿­ä»£å¼çº¢é˜Ÿæ”»å‡»ã€‚è¯„ä¼°ç»“æœæ­ç¤ºäº†æ¨¡å‹å±‚é¢ä¸æ™ºèƒ½ä½“å±‚é¢æ¼æ´ç‰¹å¾çš„æ ¹æœ¬å·®å¼‚ï¼Œå…³é”®å‘ç°æ˜¯å­˜åœ¨ä»…åœ¨æ™ºèƒ½ä½“æ‰§è¡Œç¯å¢ƒä¸­å‡ºç°çš„\"Agentic-only\"æ¼æ´ï¼Œè€Œè¿™äº›æ¼æ´åœ¨å•ä½“æ¨¡å‹ä¸­ä¿æŒæƒ°æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæ™ºèƒ½ä½“å±‚é¢çš„è¿­ä»£æ”»å‡»æˆåŠŸå®ç°äº†åœ¨æ¨¡å‹å±‚é¢å®Œå…¨å¤±è´¥çš„æ”»å‡»ç›®æ ‡ï¼Œä¸”å·¥å…·è°ƒç”¨(Tool-calling)åœºæ™¯ä¸‹çš„è„†å¼±æ€§æ¯”éå·¥å…·åœºæ™¯é«˜å‡º24%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°éƒ¨åˆ†æ¨¡å‹ç‰¹æœ‰çš„æ¼æ´åœ¨è½¬å‘æ™ºèƒ½ä½“ç¯å¢ƒæ—¶ä¼šå¤±æ•ˆï¼Œè¯æ˜äº†å•ä½“æ¨¡å‹çš„è„†å¼±æ€§å¹¶ä¸æ€»æ˜¯èƒ½æ¨å¹¿åˆ°å®é™…éƒ¨ç½²çš„ç³»ç»Ÿä¸­ï¼Œå¼ºè°ƒäº†é’ˆå¯¹æ™ºèƒ½ä½“ç¯å¢ƒè¿›è¡Œä¸“é—¨å®‰å…¨è¯„ä¼°çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Winner of the OpenAI GPT-OSS-20B Red Teaming Challenge (Kaggle, 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.17259v1",
      "published_date": "2025-09-21 22:18:34 UTC",
      "updated_date": "2025-09-21 22:18:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:23.378559+00:00"
    },
    {
      "arxiv_id": "2509.17255v1",
      "title": "Agentic AI for Multi-Stage Physics Experiments at a Large-Scale User Facility Particle Accelerator",
      "title_zh": "é¢å‘å¤§å‹ç”¨æˆ·è£…ç½®ç²’å­åŠ é€Ÿå™¨å¤šé˜¶æ®µç‰©ç†å®éªŒçš„æ™ºèƒ½ä½“ AI",
      "authors": [
        "Thorsten Hellert",
        "Drew Bertwistle",
        "Simon C. Leemann",
        "Antonin Sulc",
        "Marco Venturini"
      ],
      "abstract": "We present the first language-model-driven agentic artificial intelligence (AI) system to autonomously execute multi-stage physics experiments on a production synchrotron light source. Implemented at the Advanced Light Source particle accelerator, the system translates natural language user prompts into structured execution plans that combine archive data retrieval, control-system channel resolution, automated script generation, controlled machine interaction, and analysis. In a representative machine physics task, we show that preparation time was reduced by two orders of magnitude relative to manual scripting even for a system expert, while operator-standard safety constraints were strictly upheld. Core architectural features, plan-first orchestration, bounded tool access, and dynamic capability selection, enable transparent, auditable execution with fully reproducible artifacts. These results establish a blueprint for the safe integration of agentic AI into accelerator experiments and demanding machine physics studies, as well as routine operations, with direct portability across accelerators worldwide and, more broadly, to other large-scale scientific infrastructures.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†é¦–ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ Agentic AI ç³»ç»Ÿï¼Œæ—¨åœ¨åŒæ­¥åŠ é€Ÿå™¨å…‰æº Advanced Light Source ä¸Šè‡ªåŠ¨æ‰§è¡Œå¤šé˜¶æ®µç‰©ç†å®éªŒã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æç¤ºè¯è½¬åŒ–ä¸ºæ¶µç›–æ•°æ®æ£€ç´¢ã€æ§åˆ¶ç³»ç»Ÿè§£æã€è‡ªåŠ¨åŒ–è„šæœ¬ç”ŸæˆåŠå—æ§æœºå™¨äº¤äº’çš„ç»“æ„åŒ–æ‰§è¡Œè®¡åˆ’ã€‚åœ¨å®é™…åŠ é€Ÿå™¨ç‰©ç†ä»»åŠ¡ä¸­ï¼Œè¯¥ç³»ç»Ÿå°†å®éªŒå‡†å¤‡æ—¶é—´ç›¸æ¯”æ‰‹åŠ¨æ–¹å¼ç¼©çŸ­äº†ä¸¤ä¸ªæ•°é‡çº§ï¼Œä¸”ä¸¥æ ¼éµå¾ªäº†ä¸“ä¸šæ“ä½œå‘˜çš„å®‰å…¨çº¦æŸã€‚é€šè¿‡ Plan-first orchestrationã€Bounded tool access å’Œ Dynamic capability selection ç­‰æ¶æ„ç‰¹æ€§ï¼Œç³»ç»Ÿå®ç°äº†é€æ˜ä¸”å¯å®¡è®¡çš„æ‰§è¡Œè¿‡ç¨‹ã€‚è¿™äº›æˆæœä¸ºåœ¨å¤§å‹ç§‘å­¦åŸºç¡€è®¾æ–½ä¸­å®‰å…¨é›†æˆ Agentic AI æä¾›äº†é€šç”¨è“å›¾ï¼Œå¹¶å…·å¤‡åœ¨å…¨çƒåŠ é€Ÿå™¨åŠå…¶ä»–å¤§ç§‘å­¦è£…ç½®é¢†åŸŸæ¨å¹¿çš„æ½œåŠ›ã€‚",
      "categories": [
        "physics.acc-ph",
        "cs.AI"
      ],
      "primary_category": "physics.acc-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17255v1",
      "published_date": "2025-09-21 22:11:03 UTC",
      "updated_date": "2025-09-21 22:11:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:41.086407+00:00"
    },
    {
      "arxiv_id": "2509.17240v1",
      "title": "Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System",
      "title_zh": "æ™ºèƒ½ä½“èƒ½å¦åƒäººç±»ä¸€æ ·è¯„å®¡ç³»ç»Ÿç»¼è¿°ï¼ŸåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¯„ä¼°",
      "authors": [
        "Abdullah Mushtaq",
        "Muhammad Rafay Naeem",
        "Ibrahim Ghaznavi",
        "Alaa Abd-alrazaq",
        "Aliya Tabassum",
        "Junaid Qadir"
      ],
      "abstract": "Systematic Literature Reviews (SLRs) are foundational to evidence-based research but remain labor-intensive and prone to inconsistency across disciplines. We present an LLM-based SLR evaluation copilot built on a Multi-Agent System (MAS) architecture to assist researchers in assessing the overall quality of the systematic literature reviews. The system automates protocol validation, methodological assessment, and topic relevance checks using a scholarly database. Unlike conventional single-agent methods, our design integrates a specialized agentic approach aligned with PRISMA guidelines to support more structured and interpretable evaluations. We conducted an initial study on five published SLRs from diverse domains, comparing system outputs to expert-annotated PRISMA scores, and observed 84% agreement. While early results are promising, this work represents a first step toward scalable and accurate NLP-driven systems for interdisciplinary workflows and reveals their capacity for rigorous, domain-agnostic knowledge aggregation to streamline the review process.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Multi-Agent System, MAS)æ¶æ„çš„ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°(Systematic Literature Reviews, SLRs)è¯„ä¼°å‰¯é©¾é©¶ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³SLRè¿‡ç¨‹ä¸­åŠ³åŠ¨å¯†é›†ä¸”è·¨å­¦ç§‘ä¸€è‡´æ€§å·®çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨åŒ–æ‰§è¡Œåè®®éªŒè¯ã€æ–¹æ³•å­¦è¯„ä¼°å’Œä¸»é¢˜ç›¸å…³æ€§æ£€æŸ¥ï¼Œè¾…åŠ©ç ”ç©¶äººå‘˜è¯„ä¼°ç»¼è¿°çš„æ•´ä½“è´¨é‡ã€‚ä¸ä¼ ç»Ÿçš„å•æ™ºèƒ½ä½“æ–¹æ³•ä¸åŒï¼Œè¯¥è®¾è®¡é›†æˆäº†ç¬¦åˆPRISMAæŒ‡å—çš„ä¸“ä¸šæ™ºèƒ½ä½“æ–¹æ³•ï¼Œä»¥æ”¯æŒæ›´å…·ç»“æ„åŒ–å’Œå¯è§£é‡Šæ€§çš„è¯„ä¼°è¿‡ç¨‹ã€‚ç ”ç©¶äººå‘˜å¯¹æ¥è‡ªä¸åŒé¢†åŸŸçš„äº”ç¯‡å·²å‘è¡¨SLRè¿›è¡Œäº†åˆæ­¥ç ”ç©¶ï¼Œå¹¶å°†ç³»ç»Ÿè¾“å‡ºä¸ä¸“å®¶æ ‡æ³¨çš„PRISMAåˆ†æ•°è¿›è¡Œäº†å¯¹æ¯”ï¼Œç»“æœæ˜¾ç¤ºä¸€è‡´æ€§è¾¾åˆ°äº†84%ã€‚å®éªŒè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨ä¸¥è°¨ä¸”é¢†åŸŸæ— å…³çš„çŸ¥è¯†èšåˆæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæ„å»ºå¯æ‰©å±•ä¸”å‡†ç¡®çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é©±åŠ¨ç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚è¯¥é¡¹å·¥ä½œæœ‰æœ›æ˜¾è‘—ç®€åŒ–è·¨å­¦ç§‘å·¥ä½œæµå¹¶æé«˜æ–‡çŒ®ç»¼è¿°è¿‡ç¨‹çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17240v1",
      "published_date": "2025-09-21 21:17:23 UTC",
      "updated_date": "2025-09-21 21:17:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:35.959171+00:00"
    },
    {
      "arxiv_id": "2509.17238v2",
      "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE",
      "title_zh": "MoE æ¯”ä½ æƒ³è±¡çš„æ›´å¼ºå¤§ï¼šåŸºäº RoE çš„è¶…å¹¶è¡Œæ¨ç†æ‰©å±•",
      "authors": [
        "Soheil Zibakhsh",
        "Mohammad Samragh",
        "Kumari Nishu",
        "Lauren Hannah",
        "Arnav Kundu",
        "Minsik Cho"
      ],
      "abstract": "The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction. To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Roster of Experts (RoE)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ··åˆä¸“å®¶æ¨¡å‹(Mixture-of-Experts, MoE)çš„æ— é¡»è®­ç»ƒ(training-free)æ¨ç†ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡Tokençº§åˆ«çš„è¶…å¹¶è¡Œç¼©æ”¾(hyper-parallel scaling)æå‡é¢„æµ‹è´¨é‡ã€‚RoEé€šè¿‡åœ¨ä¸“å®¶è·¯ç”±æœºåˆ¶ä¸­æ³¨å…¥å—æ§çš„éšæœºæ€§ï¼Œä¸ºæ¯ä¸ªTokené‡‡æ ·å¤šä¸ªä¸åŒçš„ä¸“å®¶å¹¶èšåˆå…¶è¾“å‡ºï¼Œä»è€Œå°†å•ä¸€çš„MoEæ¨¡å‹è½¬åŒ–ä¸ºåŠ¨æ€é›†æˆçš„MoEã€‚ä¸ºäº†è§£å†³è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†é«˜æ•ˆçš„æ‰¹å¤„ç†ç­–ç•¥å’Œä¸“é—¨çš„KV-cachingæœºåˆ¶ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘äº†è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoEèƒ½è®©7Bè§„æ¨¡çš„MoEæ¨¡å‹åœ¨æ¨ç†è®¡ç®—é‡å‡å°‘30%çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°10.5Bæ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥æ–¹æ³•åœ¨ä¸è¿›è¡Œä»»ä½•å‚æ•°å¾®è°ƒçš„å‰æä¸‹ï¼Œè¯æ˜äº†è¶…å¹¶è¡Œæ¨ç†ç¼©æ”¾æ˜¯æå‡å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Corrected typo in arxiv abstract",
      "pdf_url": "https://arxiv.org/pdf/2509.17238v2",
      "published_date": "2025-09-21 21:05:29 UTC",
      "updated_date": "2025-10-13 16:48:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:02:33.977068+00:00"
    },
    {
      "arxiv_id": "2509.18216v2",
      "title": "nDNA -- the Semantic Helix of Artificial Cognition",
      "title_zh": "nDNAï¼šäººå·¥è®¤çŸ¥çš„è¯­ä¹‰èºæ—‹",
      "authors": [
        "Amitava Das"
      ],
      "abstract": "As AI foundation models grow in capability, a deeper question emerges: What shapes their internal cognitive identity -- beyond fluency and output? Benchmarks measure behavior, but the soul of a model resides in its latent geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic representation that captures this latent identity through the intrinsic geometry of belief. At its core, nDNA is synthesized from three principled and indispensable dimensions of latent geometry: spectral curvature, which reveals the curvature of conceptual flow across layers; thermodynamic length, which quantifies the semantic effort required to traverse representational transitions through layers; and belief vector field, which delineates the semantic torsion fields that guide a model's belief directional orientations. Like biological DNA, it encodes ancestry, mutation, and semantic inheritance, found in finetuning and alignment scars, cultural imprints, and architectural drift. In naming it, we open a new field: Neural Genomics, where models are not just tools, but digital semantic organisms with traceable inner cognition.\n  Modeling statement. We read AI foundation models as semantic fluid dynamics: meaning is transported through layers like fluid in a shaped conduit; nDNA is the physics-grade readout of that flow -- a geometry-first measure of how meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free neural DNA fingerprint tied to on-input behavior; with this fingerprint we cross into biology: tracing lineages across pretraining, fine-tuning, alignment, pruning, distillation, and merges; measuring inheritance between checkpoints; detecting drift as traits shift under new data or objectives; and, ultimately, studying the evolution of artificial cognition to compare models, diagnose risks, and govern change over time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Neural DNA (nDNA)ï¼Œä¸€ç§æ—¨åœ¨é€šè¿‡latent geometryæ•æ‰AIåŸºç¡€æ¨¡å‹å†…éƒ¨è®¤çŸ¥èº«ä»½çš„è¯­ä¹‰åŸºå› å‹è¡¨ç¤ºæ–¹æ³•ï¼Œä»è€Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è¡Œä¸ºåŸºå‡†æµ‹è¯•ã€‚nDNAç”±ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦æ„æˆï¼šæ­ç¤ºè·¨å±‚æ¦‚å¿µæµæ›²ç‡çš„spectral curvatureã€é‡åŒ–è¡¨ç¤ºè½¬æ¢è¯­ä¹‰åŠªåŠ›çš„thermodynamic lengthï¼Œä»¥åŠæç»˜æ¨¡å‹ä¿¡å¿µæ–¹å‘å¼•å¯¼çš„belief vector fieldã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•ç±»ä¼¼äºç”Ÿç‰©DNAï¼Œèƒ½å¤Ÿç¼–ç æ¨¡å‹çš„è°±ç³»ã€å˜å¼‚å’Œè¯­ä¹‰ç»§æ‰¿ï¼Œæœ‰æ•ˆè®°å½•fine-tuningã€alignmentåŠæ¶æ„æ¼‚ç§»ç•™ä¸‹çš„ç—•è¿¹ã€‚ç ”ç©¶è€…è—‰æ­¤å¼€å¯äº†Neural Genomicsè¿™ä¸€æ–°é¢†åŸŸï¼Œå°†æ¨¡å‹è§†ä¸ºå…·æœ‰å¯è¿½æº¯å†…éƒ¨è®¤çŸ¥çš„æ•°å­—è¯­ä¹‰æœ‰æœºä½“ã€‚åœ¨å»ºæ¨¡å±‚é¢ï¼Œè¯¥ç ”ç©¶å°†æ¨¡å‹è§£è¯»ä¸ºsemantic fluid dynamicsï¼Œç”Ÿæˆä¸è¾“å…¥è¡Œä¸ºç»‘å®šçš„ã€åæ ‡æ— å…³çš„ç¥ç»DNAæŒ‡çº¹ã€‚é€šè¿‡è¯¥æŒ‡çº¹ï¼Œç ”ç©¶è€…å¯ä»¥ç²¾ç¡®è¿½è¸ªæ¨¡å‹åœ¨é¢„è®­ç»ƒã€è’¸é¦åŠåˆå¹¶è¿‡ç¨‹ä¸­çš„æ¼”å˜ï¼Œç›‘æµ‹ç‰¹å¾æ¼‚ç§»ï¼Œä¸ºè¯Šæ–­æ¨¡å‹é£é™©å’Œç®¡ç†äººå·¥è®¤çŸ¥çš„æ¼”åŒ–æä¾›äº†å…¨æ–°çš„å‡ ä½•è§†è§’ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18216v2",
      "published_date": "2025-09-21 20:28:18 UTC",
      "updated_date": "2025-10-02 04:07:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:03:00.288051+00:00"
    },
    {
      "arxiv_id": "2509.18215v1",
      "title": "Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations",
      "title_zh": "å®šé‡åŒæè®ºè¯ä¸­çš„å˜åŒ–ï¼šå……åˆ†ã€å¿…è¦ä¸åäº‹å®è§£é‡Š",
      "authors": [
        "Timotheus Kampik",
        "Kristijonas ÄŒyras",
        "JosÃ© Ruiz AlarcÃ³n"
      ],
      "abstract": "This paper presents a formal approach to explaining change of inference in Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions from a QBAF and updating the QBAF to then again draw conclusions (and so on), our approach traces changes -- which we call strength inconsistencies -- in the partial order over argument strengths that a semantics establishes on some arguments of interest, called topic arguments. We trace the causes of strength inconsistencies to specific arguments, which then serve as explanations. We identify sufficient, necessary, and counterfactual explanations for strength inconsistencies and show that strength inconsistency explanations exist if and only if an update leads to strength inconsistency. We define a heuristic-based approach to facilitate the search for strength inconsistency explanations, for which we also provide an implementation.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ­£å¼æ–¹æ³•ï¼Œæ—¨åœ¨è§£é‡Šå®šé‡åŒæè®ºè¯æ¡†æ¶(Quantitative Bipolar Argumentation Frameworks, QBAFs)ä¸­æ¨ç†çš„å˜åŒ–ã€‚å½“æ›´æ–°åçš„QBAFåœ¨ä¸»é¢˜è®ºè¯(topic arguments)çš„å¼ºåº¦ååºä¸Šäº§ç”Ÿå˜åŒ–ï¼ˆå³å¼ºåº¦ä¸ä¸€è‡´ï¼Œstrength inconsistenciesï¼‰æ—¶ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†åŸå› æº¯æºè‡³ç‰¹å®šè®ºè¯æ¥æä¾›è§£é‡Šã€‚ç ”ç©¶æ˜ç¡®ç•Œå®šäº†é’ˆå¯¹å¼ºåº¦ä¸ä¸€è‡´çš„å……åˆ†(sufficient)ã€å¿…è¦(necessary)å’Œåäº‹å®(counterfactual)è§£é‡Šï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†è¿™ç±»è§£é‡Šå­˜åœ¨çš„å……è¦æ¡ä»¶ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå¯å‘å¼(heuristic-based)çš„æ–¹æ³•æ¥ä¼˜åŒ–è§£é‡Šçš„æœç´¢è¿‡ç¨‹ï¼Œå¹¶åŒæ­¥æä¾›äº†å…·ä½“çš„å·¥å…·å®ç°ã€‚è¯¥æˆæœä¸ºç†è§£åŠ¨æ€è®ºè¯ç³»ç»Ÿä¸­çš„æ¨ç†æ¼”åŒ–æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µå·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "The publisher's version contains a notation glitch in Example 3, 5th line, first sub-script G should be G'. This has always been G' in authors' version. Thanks to J. Lanser for pointing this out",
      "pdf_url": "https://arxiv.org/pdf/2509.18215v1",
      "published_date": "2025-09-21 20:26:47 UTC",
      "updated_date": "2025-09-21 20:26:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:03:02.591811+00:00"
    },
    {
      "arxiv_id": "2509.18214v1",
      "title": "Automatic Classification of Magnetic Chirality of Solar Filaments from H-Alpha Observations",
      "title_zh": "åŸºäº H-Alpha è§‚æµ‹çš„å¤ªé˜³æš—æ¡ç£æ‰‹å¾æ€§è‡ªåŠ¨åˆ†ç±»",
      "authors": [
        "Alexis Chalmers",
        "Azim Ahmadzadeh"
      ],
      "abstract": "In this study, we classify the magnetic chirality of solar filaments from H-Alpha observations using state-of-the-art image classification models. We establish the first reproducible baseline for solar filament chirality classification on the MAGFiLO dataset. The MAGFiLO dataset contains over 10,000 manually-annotated filaments from GONG H-Alpha observations, making it the largest dataset for filament detection and classification to date. Prior studies relied on much smaller datasets, which limited their generalizability and comparability. We fine-tuned several pre-trained, image classification architectures, including ResNet, WideResNet, ResNeXt, and ConvNeXt, and also applied data augmentation and per-class loss weights to optimize the models. Our best model, ConvNeXtBase, achieves a per-class accuracy of 0.69 for left chirality filaments and $0.73$ for right chirality filaments.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨å…ˆè¿›çš„å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œæ—¨åœ¨ä» H-Alpha è§‚æµ‹ä¸­è‡ªåŠ¨åˆ†ç±»å¤ªé˜³é•¿ä¸ (Solar Filaments) çš„ç£æ€§æ‰‹æ€§ (Magnetic Chirality)ã€‚ç ”ç©¶äººå‘˜åœ¨åŒ…å«è¶…è¿‡ 10,000 ä¸ªæ‰‹åŠ¨æ ‡æ³¨æ ·æœ¬çš„ MAGFiLO æ•°æ®é›†ä¸Šå»ºç«‹äº†é¦–ä¸ªå¯é‡å¤çš„åˆ†ç±»åŸºå‡†ï¼Œè§£å†³äº†ä»¥å¾€ç ”ç©¶å› æ•°æ®é›†è§„æ¨¡æœ‰é™è€Œå¯¼è‡´çš„æ³›åŒ–æ€§ä¸è¶³é—®é¢˜ã€‚é€šè¿‡å¯¹ ResNetã€WideResNetã€ResNeXt å’Œ ConvNeXt ç­‰é¢„è®­ç»ƒæ¶æ„è¿›è¡Œå¾®è°ƒï¼Œå¹¶åº”ç”¨æ•°æ®å¢å¼ºä¸ç±»åˆ«æŸå¤±æƒé‡ä¼˜åŒ–ï¼Œæ¨¡å‹æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä¼˜çš„ ConvNeXtBase æ¨¡å‹å¯¹å·¦æ—‹å’Œå³æ—‹æ‰‹æ€§é•¿ä¸çš„åˆ†ç±»å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ° 0.69 å’Œ 0.73ã€‚è¯¥å·¥ä½œçš„å¼€å±•ä¸ºå¤§è§„æ¨¡å¤ªé˜³é•¿ä¸ç‰©ç†ç‰¹æ€§çš„è‡ªåŠ¨åˆ†ææä¾›äº†é‡è¦æŠ€æœ¯å‚è€ƒã€‚",
      "categories": [
        "astro-ph.SR",
        "cs.AI"
      ],
      "primary_category": "astro-ph.SR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18214v1",
      "published_date": "2025-09-21 19:55:14 UTC",
      "updated_date": "2025-09-21 19:55:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:03:01.883874+00:00"
    },
    {
      "arxiv_id": "2509.17207v1",
      "title": "Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds",
      "title_zh": "Point-RTDï¼šé¢å‘ç‚¹äº‘ Transformer æ¨¡å‹é¢„è®­ç»ƒçš„æ›¿æ¢ä»¤ç‰Œå»å™ª",
      "authors": [
        "Gunner Stone",
        "Youngsook Choi",
        "Alireza Tavakkoli",
        "Ankita Shukla"
      ],
      "abstract": "Pre-training strategies play a critical role in advancing the performance of transformer-based models for 3D point cloud tasks. In this paper, we introduce Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to improve token robustness through a corruption-reconstruction framework. Unlike traditional mask-based reconstruction tasks that hide data segments for later prediction, Point-RTD corrupts point cloud tokens and leverages a discriminator-generator architecture for denoising. This shift enables more effective learning of structural priors and significantly enhances model performance and efficiency. On the ShapeNet dataset, Point-RTD reduces reconstruction error by over 93% compared to PointMAE, and achieves more than 14x lower Chamfer Distance on the test set. Our method also converges faster and yields higher classification accuracy on ShapeNet, ModelNet10, and ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework in every case.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Point-RTD (Replaced Token Denoising)ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºæå‡ 3D point cloud ä»»åŠ¡ä¸­ Transformer æ¨¡å‹æ€§èƒ½è€Œè®¾è®¡çš„é¢„è®­ç»ƒç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºé®è”½çš„é‡å»ºä»»åŠ¡ä¸åŒï¼ŒPoint-RTD å¼•å…¥äº†ä¸€ç§ç ´å-é‡å»º (corruption-reconstruction) æ¡†æ¶ï¼Œé€šè¿‡ç ´å point cloud tokens å¹¶åˆ©ç”¨é‰´åˆ«å™¨-ç”Ÿæˆå™¨ (discriminator-generator) æ¶æ„è¿›è¡Œå»å™ªå¤„ç†ã€‚è¿™ç§æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ ç»“æ„å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚åœ¨ ShapeNet æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”äº Point-MAEï¼ŒPoint-RTD å°†é‡å»ºè¯¯å·®é™ä½äº† 93% ä»¥ä¸Šï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šå®ç°äº†è¶…è¿‡ 14 å€ä½çš„ Chamfer Distanceã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦ä¸Šæ›´å¿«ï¼Œå¹¶åœ¨ ShapeNetã€ModelNet10 å’Œ ModelNet40 ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´é«˜çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿ Point-MAE æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17207v1",
      "published_date": "2025-09-21 19:25:25 UTC",
      "updated_date": "2025-09-21 19:25:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:03:13.483479+00:00"
    },
    {
      "arxiv_id": "2509.17206v1",
      "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation",
      "title_zh": "é¢å‘ç»“æ„åŒ–ä¸è¯­ä¹‰æ„ŸçŸ¥ä¸‰ç»´ç‚¹äº‘ç”Ÿæˆçš„æœ‰å¼•å¯¼ä¸æ— å¼•å¯¼æ¡ä»¶æ‰©æ•£æœºåˆ¶",
      "authors": [
        "Gunner Stone",
        "Sushmita Sarker",
        "Alireza Tavakkoli"
      ],
      "abstract": "Generating realistic 3D point clouds is a fundamental problem in computer vision with applications in remote sensing, robotics, and digital object modeling. Existing generative approaches primarily capture geometry, and when semantics are considered, they are typically imposed post hoc through external segmentation or clustering rather than integrated into the generative process itself. We propose a diffusion-based framework that embeds per-point semantic conditioning directly within generation. Each point is associated with a conditional variable corresponding to its semantic label, which guides the diffusion dynamics and enables the joint synthesis of geometry and semantics. This design produces point clouds that are both structurally coherent and segmentation-aware, with object parts explicitly represented during synthesis. Through a comparative analysis of guided and unguided diffusion processes, we demonstrate the significant impact of conditional variables on diffusion dynamics and generation quality. Extensive experiments validate the efficacy of our approach, producing detailed and accurate 3D point clouds tailored to specific parts and features.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D point cloud ç”Ÿæˆä¸­å‡ ä½•å½¢çŠ¶ä¸è¯­ä¹‰ä¿¡æ¯æ•´åˆä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç›´æ¥å°†é€ç‚¹è¯­ä¹‰çº¦æŸåµŒå…¥ç”Ÿæˆè¿‡ç¨‹çš„ diffusion-based æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸ºæ¯ä¸ªç‚¹å…³è”ä¸€ä¸ªä¸å…¶è¯­ä¹‰æ ‡ç­¾ç›¸å¯¹åº”çš„ conditional variable æ¥å¼•å¯¼ diffusion dynamicsï¼Œä»è€Œå®ç°äº†å‡ ä½•ä¸è¯­ä¹‰çš„è”åˆåˆæˆã€‚è¿™ç§è®¾è®¡ä½¿å¾—ç”Ÿæˆçš„ç‚¹äº‘åœ¨ç»“æ„ä¸Šå…·æœ‰è¿è´¯æ€§ä¸”å…·å¤‡ segmentation-aware ç‰¹æ€§ï¼Œèƒ½å¤Ÿåœ¨åˆæˆè¿‡ç¨‹ä¸­æ˜¾å¼åœ°è¡¨ç¤ºç‰©ä½“éƒ¨ä»¶ã€‚é€šè¿‡å¯¹ guided å’Œ unguided ä¸¤ç§ diffusion è¿‡ç¨‹çš„å¯¹æ¯”åˆ†æï¼Œç ”ç©¶å±•ç¤ºäº† conditional variables å¯¹ diffusion dynamics å’Œç”Ÿæˆè´¨é‡çš„æ˜¾è‘—å½±å“ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹ç‰¹å®šéƒ¨ä»¶å’Œç‰¹å¾çš„é«˜åº¦è¯¦ç»†ä¸”å‡†ç¡®çš„ 3D point cloudsï¼Œä¸ºé¥æ„Ÿå’Œæœºå™¨äººç­‰é¢†åŸŸçš„åº”ç”¨æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17206v1",
      "published_date": "2025-09-21 19:19:36 UTC",
      "updated_date": "2025-09-21 19:19:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:03:14.287117+00:00"
    },
    {
      "arxiv_id": "2509.17197v2",
      "title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing",
      "title_zh": "SignalLLMï¼šé¢å‘è‡ªåŠ¨åŒ–ä¿¡å·å¤„ç†çš„é€šç”¨å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Junlong Ke",
        "Qiying Hu",
        "Shenghai Yuan",
        "Yuecong Xu",
        "Jianfei Yang"
      ],
      "abstract": "Modern signal processing (SP) pipelines, whether model-based or data-driven, often constrained by complex and fragmented workflow, rely heavily on expert knowledge and manual engineering, and struggle with adaptability and generalization under limited data. In contrast, Large Language Models (LLMs) offer strong reasoning capabilities, broad general-purpose knowledge, in-context learning, and cross-modal transfer abilities, positioning them as powerful tools for automating and generalizing SP workflows. Motivated by these potentials, we introduce SignalLLM, the first general-purpose LLM-based agent framework for general SP tasks. Unlike prior LLM-based SP approaches that are limited to narrow applications or tricky prompting, SignalLLM introduces a principled, modular architecture. It decomposes high-level SP goals into structured subtasks via in-context learning and domain-specific retrieval, followed by hierarchical planning through adaptive retrieval-augmented generation (RAG) and refinement; these subtasks are then executed through prompt-based reasoning, cross-modal reasoning, code synthesis, model invocation, or data-driven LLM-assisted modeling. Its generalizable design enables the flexible selection of problem solving strategies across different signal modalities, task types, and data conditions. We demonstrate the versatility and effectiveness of SignalLLM through five representative tasks in communication and sensing, such as radar target detection, human activity recognition, and text compression. Experimental results show superior performance over traditional and existing LLM-based methods, particularly in few-shot and zero-shot settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SignalLLMï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹é€šç”¨ä¿¡å·å¤„ç†(Signal Processing, SP)ä»»åŠ¡çš„é€šç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸSPæµç¨‹ä¸­è¿‡åº¦ä¾èµ–ä¸“å®¶ç»éªŒã€æµç¨‹çç¢åŠåœ¨æœ‰é™æ•°æ®ä¸‹æ³›åŒ–èƒ½åŠ›å·®ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è§„èŒƒåŒ–çš„æ¨¡å—åŒ–æ¶æ„ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ (In-context Learning)å’Œé¢†åŸŸç‰¹å®šæ£€ç´¢å°†é«˜å±‚SPç›®æ ‡åˆ†è§£ä¸ºç»“æ„åŒ–å­ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨è‡ªé€‚åº”æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)è¿›è¡Œåˆ†å±‚è§„åˆ’ä¸ä¼˜åŒ–ã€‚è¿™äº›å­ä»»åŠ¡éšåé€šè¿‡æç¤ºè¯æ¨ç†ã€è·¨æ¨¡æ€æ¨ç†ã€ä»£ç åˆæˆã€æ¨¡å‹è°ƒç”¨æˆ–æ•°æ®é©±åŠ¨çš„LLMè¾…åŠ©å»ºæ¨¡æ¥æ‰§è¡Œï¼Œä½¿å…¶èƒ½çµæ´»é€‚åº”ä¸åŒçš„ä¿¡å·æ¨¡æ€å’Œä»»åŠ¡ç±»å‹ã€‚åœ¨é›·è¾¾ç›®æ ‡æ£€æµ‹ã€äººä½“æ´»åŠ¨è¯†åˆ«ç­‰äº”é¡¹é€šä¿¡ä¸æ„ŸçŸ¥ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒSignalLLMè¡¨ç°å‡ºä¼˜äºä¼ ç»Ÿæ–¹æ³•åŠç°æœ‰åŸºäºLLMçš„æ–¹æ³•çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ ·æœ¬(Few-shot)å’Œé›¶æ ·æœ¬(Zero-shot)è®¾ç½®ä¸‹å±•ç°äº†å“è¶Šçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.17197v2",
      "published_date": "2025-09-21 18:54:54 UTC",
      "updated_date": "2025-10-30 15:26:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:03:21.990013+00:00"
    },
    {
      "arxiv_id": "2509.17196v1",
      "title": "Evolution of Concepts in Language Model Pre-Training",
      "title_zh": "è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­æ¦‚å¿µçš„æ¼”åŒ–",
      "authors": [
        "Xuyang Ge",
        "Wentao Shu",
        "Jiaxing Wu",
        "Yunhua Zhou",
        "Zhengfu He",
        "Xipeng Qiu"
      ],
      "abstract": "Language models obtain extensive capabilities through pre-training. However, the pre-training process remains a black box. In this work, we track linear interpretable feature evolution across pre-training snapshots using a sparse dictionary learning method called crosscoders. We find that most features begin to form around a specific point, while more complex patterns emerge in later training stages. Feature attribution analyses reveal causal connections between feature evolution and downstream performance. Our feature-level observations are highly consistent with previous findings on Transformer's two-stage learning process, which we term a statistical learning phase and a feature learning phase. Our work opens up the possibility to track fine-grained representation progress during language model learning dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¦‚å¿µæ¼”åŒ–ï¼Œæ—¨åœ¨æ­ç¤ºé¢„è®­ç»ƒè¿™ä¸€â€œé»‘ç›’â€çš„å†…éƒ¨åŠ¨æ€ã€‚ç ”ç©¶äººå‘˜é‡‡ç”¨äº†ä¸€ç§åä¸º crosscoders çš„ç¨€ç–å­—å…¸å­¦ä¹ (sparse dictionary learning)æ–¹æ³•ï¼Œè¿½è¸ªé¢„è®­ç»ƒå¿«ç…§ä¸­çš„çº¿æ€§å¯è§£é‡Šç‰¹å¾æ¼”å˜ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å¤šæ•°ç‰¹å¾åœ¨ç‰¹å®šæ—¶é—´ç‚¹å¼€å§‹å½¢æˆï¼Œè€Œæ›´å¤æ‚çš„æ¨¡å¼åˆ™åœ¨è®­ç»ƒåæœŸå‡ºç°ã€‚é€šè¿‡ç‰¹å¾å½’å› åˆ†æï¼Œè¯¥å·¥ä½œæ­ç¤ºäº†ç‰¹å¾æ¼”åŒ–ä¸ä¸‹æ¸¸æ€§èƒ½ä¹‹é—´çš„å› æœè”ç³»ã€‚å®éªŒè§‚å¯Ÿç»“æœä¸ Transformer çš„ä¸¤é˜¶æ®µå­¦ä¹ è¿‡ç¨‹é«˜åº¦ä¸€è‡´ï¼Œå³ç»Ÿè®¡å­¦ä¹ é˜¶æ®µ(statistical learning phase)å’Œç‰¹å¾å­¦ä¹ é˜¶æ®µ(feature learning phase)ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨è¯­è¨€æ¨¡å‹å­¦ä¹ åŠ¨åŠ›å­¦è¿‡ç¨‹ä¸­è¿½è¸ªç»†ç²’åº¦è¡¨ç¤ºè¿›å±•æä¾›äº†å…¨æ–°çš„å¯èƒ½æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "30 pages, 25 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.17196v1",
      "published_date": "2025-09-21 18:53:12 UTC",
      "updated_date": "2025-09-21 18:53:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:03:27.288311+00:00"
    },
    {
      "arxiv_id": "2509.17192v2",
      "title": "Shall We Play a Game? Language Models for Open-ended Wargames",
      "title_zh": "Shall We Play a Game? é¢å‘å¼€æ”¾å¼å…µæ£‹æ¨æ¼”çš„è¯­è¨€æ¨¡å‹",
      "authors": [
        "Glenn Matlin",
        "Parv Mahajan",
        "Isaac Song",
        "Yixiong Hao",
        "Ryan Bard",
        "Stu Topp",
        "Evan Montoya",
        "M. Rehan Parwani",
        "Soham Shetty",
        "Mark Riedl"
      ],
      "abstract": "Wargames are simulations of conflicts in which participants' decisions influence future events. While casual wargaming can be used for entertainment or socialization, serious wargaming is used by experts to explore strategic implications of decision-making and experiential learning. In this paper, we take the position that Artificial Intelligence (AI) systems, such as Language Models (LMs), are rapidly approaching human-expert capability for strategic planning -- and will one day surpass it. Military organizations have begun using LMs to provide insights into the consequences of real-world decisions during _open-ended wargames_ which use natural language to convey actions and outcomes. We argue the ability for AI systems to influence large-scale decisions motivates additional research into the safety, interpretability, and explainability of AI in open-ended wargames. To demonstrate, we conduct a scoping literature review with a curated selection of 100 unclassified studies on AI in wargames, and construct a novel ontology of open-endedness using the creativity afforded to players, adjudicators, and the novelty provided to observers. Drawing from this body of work, we distill a set of practical recommendations and critical safety considerations for deploying AI in open-ended wargames across common domains. We conclude by presenting the community with a set of high-impact open research challenges for future work.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹(Language Models)åœ¨å¼€æ”¾å¼æˆ˜äº‰æ¸¸æˆ(Open-ended Wargames)ä¸­çš„åº”ç”¨ï¼ŒæŒ‡å‡ºAIåœ¨æˆ˜ç•¥è§„åˆ’æ–¹é¢æ­£è¿…é€Ÿæ¥è¿‘å¹¶ç»ˆå°†è¶…è¶Šäººç±»ä¸“å®¶æ°´å¹³ã€‚é€šè¿‡å¯¹100é¡¹éæœºå¯†ç ”ç©¶è¿›è¡Œæ–‡çŒ®ç»¼è¿°ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ç§åŸºäºç©å®¶ã€è£åˆ¤åˆ›é€ åŠ›ä»¥åŠè§‚å¯Ÿè€…åˆ›æ–°ä½“éªŒçš„æ–°å‹å¼€æ”¾æ€§æœ¬ä½“(ontology)ã€‚è®ºæ–‡é‡ç‚¹å¼ºè°ƒäº†å½“AIå…·å¤‡å½±å“å¤§è§„æ¨¡å†³ç­–çš„èƒ½åŠ›æ—¶ï¼Œå…¶ç³»ç»Ÿå®‰å…¨æ€§ã€å¯è§£é‡Šæ€§(interpretability)å’Œå¯è¯´æ˜æ€§(explainability)çš„è‡³å…³é‡è¦æ€§ã€‚åŸºäºæ‰€å¾—ç»“è®ºï¼Œä½œè€…é’ˆå¯¹ä¸åŒé¢†åŸŸçš„AIéƒ¨ç½²æå‡ºäº†å®è·µå»ºè®®ä¸å…³é”®å®‰å…¨è€ƒé‡ï¼Œå¹¶ä¸ºæœªæ¥å·¥ä½œæ€»ç»“äº†ä¸€ç³»åˆ—é«˜å½±å“åŠ›çš„å¼€æ”¾å¼ç ”ç©¶æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17192v2",
      "published_date": "2025-09-21 18:37:17 UTC",
      "updated_date": "2025-10-23 02:21:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:03:33.500477+00:00"
    },
    {
      "arxiv_id": "2509.17190v1",
      "title": "Echo-Path: Pathology-Conditioned Echo Video Generation",
      "title_zh": "Echo-Pathï¼šä»¥ç—…ç†ä¸ºæ¡ä»¶çš„è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Kabir Hamzah Muhammad",
        "Marawan Elbatel",
        "Yi Qin",
        "Xiaomeng Li"
      ],
      "abstract": "Cardiovascular diseases (CVDs) remain the leading cause of mortality globally, and echocardiography is critical for diagnosis of both common and congenital cardiac conditions. However, echocardiographic data for certain pathologies are scarce, hindering the development of robust automated diagnosis models. In this work, we propose Echo-Path, a novel generative framework to produce echocardiogram videos conditioned on specific cardiac pathologies. Echo-Path can synthesize realistic ultrasound video sequences that exhibit targeted abnormalities, focusing here on atrial septal defect (ASD) and pulmonary arterial hypertension (PAH). Our approach introduces a pathology-conditioning mechanism into a state-of-the-art echo video generator, allowing the model to learn and control disease-specific structural and motion patterns in the heart. Quantitative evaluation demonstrates that the synthetic videos achieve low distribution distances, indicating high visual fidelity. Clinically, the generated echoes exhibit plausible pathology markers. Furthermore, classifiers trained on our synthetic data generalize well to real data and, when used to augment real training sets, it improves downstream diagnosis of ASD and PAH by 7\\% and 8\\% respectively. Code, weights and dataset are available here https://github.com/Marshall-mk/EchoPathv1",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Echo-Pathï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”±äºç‰¹å®šç—…ç†æ€§è¶…å£°å¿ƒåŠ¨å›¾æ•°æ®ç¨€ç¼ºè€Œå¯¼è‡´çš„è‡ªåŠ¨è¯Šæ–­æ¨¡å‹å¼€å‘å—é˜»çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å…ˆè¿›çš„è¶…å£°è§†é¢‘ç”Ÿæˆå™¨ä¸­å¼•å…¥ç—…ç†è°ƒèŠ‚æœºåˆ¶(pathology-conditioning mechanism)ï¼Œå®ç°äº†å¯¹å¿ƒè„ç–¾ç—…ç‰¹å®šç»“æ„å’Œè¿åŠ¨æ¨¡å¼çš„å­¦ä¹ ä¸æ§åˆ¶ã€‚ç ”ç©¶é‡ç‚¹é’ˆå¯¹æˆ¿é—´éš”ç¼ºæŸ(ASD)å’Œè‚ºåŠ¨è„‰é«˜å‹(PAH)ç”Ÿæˆäº†å…·æœ‰é’ˆå¯¹æ€§å¼‚å¸¸ç‰¹å¾çš„é«˜ä¿çœŸè¶…å£°è§†é¢‘ã€‚å®šé‡è¯„ä¼°è¡¨æ˜åˆæˆè§†é¢‘å…·æœ‰æé«˜çš„è§†è§‰ä¿çœŸåº¦ï¼Œä¸”åœ¨ä¸´åºŠä¸Šå±•ç°äº†åˆç†çš„ç—…ç†æ ‡å¿—ç‰©(pathology markers)ã€‚å®éªŒè¯æ˜ï¼Œåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„åˆ†ç±»å™¨èƒ½æœ‰æ•ˆæ³›åŒ–è‡³çœŸå®åœºæ™¯ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è¯¥æ¨¡å‹è¿›è¡Œæ•°æ®å¢å¼ºåï¼ŒASD å’Œ PAH çš„ä¸‹æ¸¸è¯Šæ–­å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº† 7% å’Œ 8%ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 3 figures, MICCAI-AMAI2025 Workshop",
      "pdf_url": "https://arxiv.org/pdf/2509.17190v1",
      "published_date": "2025-09-21 18:31:28 UTC",
      "updated_date": "2025-09-21 18:31:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:03:40.294872+00:00"
    },
    {
      "arxiv_id": "2509.17187v1",
      "title": "Ambiguous Medical Image Segmentation Using Diffusion SchrÃ¶dinger Bridge",
      "title_zh": "åŸºäº Diffusion SchrÃ¶dinger Bridge çš„æ­§ä¹‰åŒ»å­¦å›¾åƒåˆ†å‰²",
      "authors": [
        "Lalith Bharadwaj Baru",
        "Kamalaker Dadi",
        "Tapabrata Chakraborti",
        "Raju S. Bapi"
      ],
      "abstract": "Accurate segmentation of medical images is challenging due to unclear lesion boundaries and mask variability. We introduce \\emph{Segmentation SchÃ¶dinger Bridge (SSB)}, the first application of SchÃ¶dinger Bridge for ambiguous medical image segmentation, modelling joint image-mask dynamics to enhance performance. SSB preserves structural integrity, delineates unclear boundaries without additional guidance, and maintains diversity using a novel loss function. We further propose the \\emph{Diversity Divergence Index} ($D_{DDI}$) to quantify inter-rater variability, capturing both diversity and consensus. SSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER (in-house) datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Segmentation SchÃ¶dinger Bridge (SSB)ï¼Œè¿™æ˜¯SchÃ¶dinger Bridgeåœ¨æ¨¡ç³ŠåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„é¦–æ¬¡åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³ç—…ç¶è¾¹ç•Œä¸æ¸…æ™°å’Œæ©è†œå˜å¼‚æ€§å¯¼è‡´çš„åˆ†å‰²æŒ‘æˆ˜ã€‚SSBé€šè¿‡å»ºæ¨¡å›¾åƒä¸æ©è†œçš„è”åˆåŠ¨æ€(joint image-mask dynamics)ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–å¼•å¯¼çš„æƒ…å†µä¸‹ä¿æŒç»“æ„å®Œæ•´æ€§å¹¶å‡†ç¡®åˆ’å®šæ¨¡ç³Šè¾¹ç•Œã€‚ç ”ç©¶äººå‘˜è¿˜è®¾è®¡äº†ä¸€ç§æ–°å‹æŸå¤±å‡½æ•°ä»¥ç»´æŒç”Ÿæˆç»“æœçš„å¤šæ ·æ€§ï¼Œå¹¶æå‡ºäº†å¤šæ ·æ€§åˆ†æ­§æŒ‡æ•°(Diversity Divergence Index, $D_{DDI}$)æ¥é‡åŒ–ä¸åŒæ ‡æ³¨è€…é—´çš„å˜å¼‚æ€§ï¼Œä»è€ŒåŒæ—¶æ•æ‰å¤šæ ·æ€§ä¸å…±è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSSBåœ¨LIDC-IDRIã€COCAä»¥åŠå†…éƒ¨æ•°æ®é›†RACERä¸Šå‡å–å¾—äº†State-of-the-art (SOTA)çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "MICCAI 2025 (11 pages, 2 figures, 1 table, and 26 references)",
      "pdf_url": "https://arxiv.org/pdf/2509.17187v1",
      "published_date": "2025-09-21 18:16:06 UTC",
      "updated_date": "2025-09-21 18:16:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:01.391082+00:00"
    },
    {
      "arxiv_id": "2509.17186v2",
      "title": "Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long Sequence Modeling",
      "title_zh": "ç”¨äºé«˜æ•ˆé•¿åºåˆ—å»ºæ¨¡çš„æ ‘çªå…±æŒ¯å‘æ”¾ç¥ç»å…ƒ",
      "authors": [
        "Dehao Zhang",
        "Malu Zhang",
        "Shuai Wang",
        "Jingya Wang",
        "Wenjie Wei",
        "Zeyu Ma",
        "Guoqing Wang",
        "Yang Yang",
        "Haizhou Li"
      ],
      "abstract": "The explosive growth in sequence length has intensified the demand for effective and efficient long sequence modeling. Benefiting from intrinsic oscillatory membrane dynamics, Resonate-and-Fire (RF) neurons can efficiently extract frequency components from input signals and encode them into spatiotemporal spike trains, making them well-suited for long sequence modeling. However, RF neurons exhibit limited effective memory capacity and a trade-off between energy efficiency and training speed on complex temporal tasks. Inspired by the dendritic structure of biological neurons, we propose a Dendritic Resonate-and-Fire (D-RF) model, which explicitly incorporates a multi-dendritic and soma architecture. Each dendritic branch encodes specific frequency bands by utilizing the intrinsic oscillatory dynamics of RF neurons, thereby collectively achieving comprehensive frequency representation. Furthermore, we introduce an adaptive threshold mechanism into the soma structure that adjusts the threshold based on historical spiking activity, reducing redundant spikes while maintaining training efficiency in long sequence tasks. Extensive experiments demonstrate that our method maintains competitive accuracy while substantially ensuring sparse spikes without compromising computational efficiency during training. These results underscore its potential as an effective and efficient solution for long sequence modeling on edge platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Dendritic Resonate-and-Fire (D-RF)æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é•¿åºåˆ—å»ºæ¨¡ä¸­Resonate-and-Fire (RF)ç¥ç»å…ƒæœ‰æ•ˆå†…å­˜å®¹é‡æœ‰é™ä»¥åŠèƒ½é‡æ•ˆç‡ä¸è®­ç»ƒé€Ÿåº¦æƒè¡¡ç­‰é—®é¢˜ã€‚è¯¥æ¨¡å‹å—ç”Ÿç‰©ç¥ç»å…ƒæ ‘çªç»“æ„å¯å‘ï¼Œé‡‡ç”¨å¤šæ ‘çª(multi-dendritic)å’Œèƒä½“(soma)æ¶æ„ï¼Œåˆ©ç”¨RFç¥ç»å…ƒçš„æŒ¯è¡åŠ¨åŠ›å­¦ä½¿æ¯ä¸ªæ ‘çªåˆ†æ”¯ç¼–ç ç‰¹å®šçš„é¢‘ç‡å¸¦ï¼Œä»è€Œå®ç°å…¨é¢çš„é¢‘ç‡è¡¨ç¤ºã€‚åŒæ—¶ï¼Œç ”ç©¶åœ¨èƒä½“ç»“æ„ä¸­å¼•å…¥äº†è‡ªé€‚åº”é˜ˆå€¼æœºåˆ¶(adaptive threshold mechanism)ï¼Œæ ¹æ®å†å²è„‰å†²æ´»åŠ¨åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼Œåœ¨ç»´æŒè®­ç»ƒæ•ˆç‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†å†—ä½™è„‰å†²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD-RFæ¨¡å‹åœ¨é•¿åºåˆ—ä»»åŠ¡ä¸­ä¿æŒäº†ç«äº‰åŠ›çš„å‡†ç¡®ç‡ï¼Œå¹¶å®ç°äº†æé«˜çš„è„‰å†²ç¨€ç–æ€§ã€‚è¯¥ç ”ç©¶ä¸ºè¾¹ç¼˜å¹³å°(edge platforms)ä¸Šçš„é•¿åºåˆ—å»ºæ¨¡æä¾›äº†ä¸€ç§å…¼é¡¾æœ‰æ•ˆæ€§ä¸è®¡ç®—æ•ˆç‡çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17186v2",
      "published_date": "2025-09-21 18:15:45 UTC",
      "updated_date": "2025-09-26 13:41:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:21.384541+00:00"
    },
    {
      "arxiv_id": "2509.17183v1",
      "title": "LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization",
      "title_zh": "LifeAlignï¼šåŸºäºè®°å¿†å¢å¼ºèšç„¦åå¥½ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹ç»ˆèº«å¯¹é½",
      "authors": [
        "Junsong Li",
        "Jie Zhou",
        "Bihao Zhan",
        "Yutao Yang",
        "Qianjun Pan",
        "Shilian Chen",
        "Tianyu Huai",
        "Xin Li",
        "Qin Chen",
        "Liang He"
      ],
      "abstract": "Alignment plays a crucial role in Large Language Models (LLMs) in aligning with human preferences on a specific task/domain. Traditional alignment methods suffer from catastrophic forgetting, where models lose previously acquired knowledge when adapting to new preferences or domains. We introduce LifeAlign, a novel framework for lifelong alignment that enables LLMs to maintain consistent human preference alignment across sequential learning tasks without forgetting previously learned knowledge. Our approach consists of two key innovations. First, we propose a focalized preference optimization strategy that aligns LLMs with new preferences while preventing the erosion of knowledge acquired from previous tasks. Second, we develop a short-to-long memory consolidation mechanism that merges denoised short-term preference representations into stable long-term memory using intrinsic dimensionality reduction, enabling efficient storage and retrieval of alignment patterns across diverse domains. We evaluate LifeAlign across multiple sequential alignment tasks spanning different domains and preference types. Experimental results demonstrate that our method achieves superior performance in maintaining both preference alignment quality and knowledge retention compared to existing lifelong learning approaches. The codes and datasets will be released on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LifeAlignï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)è®¾è®¡çš„ç»ˆèº«å¯¹é½(Lifelong Alignment)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹åœ¨é€‚åº”æ–°åå¥½æˆ–é¢†åŸŸæ—¶äº§ç”Ÿçš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†èšç„¦åå¥½ä¼˜åŒ–(Focalized Preference Optimization)ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨å¯¹é½æ–°åå¥½çš„åŒæ—¶ï¼Œæœ‰æ•ˆé˜²æ­¢å…ˆå‰ä»»åŠ¡ä¸­æ‰€è·çŸ¥è¯†çš„ä¾µèš€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†ä¸€ç§çŸ­åˆ°é•¿è®°å¿†æ•´åˆ(Short-to-Long Memory Consolidation)æœºåˆ¶ï¼Œé€šè¿‡å†…åœ¨ç»´åº¦é™ä½(Intrinsic Dimensionality Reduction)æŠ€æœ¯å°†å»å™ªåçš„çŸ­æœŸåå¥½è¡¨ç¤ºåˆå¹¶ä¸ºç¨³å®šçš„é•¿æœŸè®°å¿†ï¼Œå®ç°äº†å¯¹ä¸åŒé¢†åŸŸå¯¹é½æ¨¡å¼çš„é«˜æ•ˆå­˜å‚¨ä¸æ£€ç´¢ã€‚åœ¨æ¶µç›–ä¸åŒé¢†åŸŸå’Œåå¥½ç±»å‹çš„å¤šä¸ªé¡ºåºå¯¹é½ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLifeAlignåœ¨ç»´æŒå¯¹é½è´¨é‡å’ŒçŸ¥è¯†ä¿ç•™æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ç»ˆèº«å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ºæ¨¡å‹åœ¨æŒç»­å­¦ä¹ ç¯å¢ƒä¸‹çš„ç¨³å¥åå¥½å¯¹é½æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17183v1",
      "published_date": "2025-09-21 18:06:05 UTC",
      "updated_date": "2025-09-21 18:06:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:23.084229+00:00"
    },
    {
      "arxiv_id": "2509.17165v1",
      "title": "Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM Embedding Denoising Auto Encoder Transformer",
      "title_zh": "åŸºäºæ··åˆæ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ—¶é—´åºåˆ—é¢„æµ‹ï¼šBi-LSTM åµŒå…¥å¼å»å™ªè‡ªç¼–ç å™¨ Transformer",
      "authors": [
        "Sahar Koohfar",
        "Wubeshet Woldemariam"
      ],
      "abstract": "Time series data is a prevalent form of data found in various fields. It consists of a series of measurements taken over time. Forecasting is a crucial application of time series models, where future values are predicted based on historical data. Accurate forecasting is essential for making well-informed decisions across industries. When it comes to electric vehicles (EVs), precise predictions play a key role in planning infrastructure development, load balancing, and energy management. This study introduces a BI-LSTM embedding denoising autoencoder model (BDM) designed to address time series problems, focusing on short-term EV charging load prediction. The performance of the proposed model is evaluated by comparing it with benchmark models like Transformer, CNN, RNN, LSTM, and GRU. Based on the results of the study, the proposed model outperforms the benchmark models in four of the five-time steps, demonstrating its effectiveness for time series forecasting. This research makes a significant contribution to enhancing time series forecasting, thereby improving decision-making processes.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBDMï¼ˆBI-LSTM embedding denoising autoencoder modelï¼‰çš„æ··åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç”µåŠ¨æ±½è½¦ï¼ˆEVï¼‰çš„çŸ­æœŸå……ç”µè´Ÿè·é¢„æµ‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†Bi-LSTMåµŒå…¥å’Œå»å™ªè‡ªåŠ¨ç¼–ç å™¨ï¼ˆDenoising Auto Encoderï¼‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿä»å¤æ‚çš„å†å²æ•°æ®ä¸­æœ‰æ•ˆæå–å…³é”®ç‰¹å¾å¹¶å‡å°‘å™ªå£°å½±å“ã€‚ç ”ç©¶é€šè¿‡å°†BDMä¸Transformerã€CNNã€RNNã€LSTMå’ŒGRUç­‰å¤šç§åŸºå‡†æ¨¡å‹è¿›è¡Œå¯¹æ¯”å®éªŒï¼Œå…¨é¢è¯„ä¼°äº†å…¶é¢„æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨äº”ä¸ªæ—¶é—´æ­¥é•¿ä¸­çš„å››ä¸ªè¡¨ç°å‡ä¼˜äºä¼ ç»ŸåŸºå‡†æ¨¡å‹ï¼Œå±•ç¤ºäº†æé«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥é¡¹ç ”ç©¶æˆæœä¸ºç”µåŠ¨æ±½è½¦åŸºç¡€è®¾æ–½è§„åˆ’ã€è´Ÿè½½å‡è¡¡å’Œèƒ½æºç®¡ç†æä¾›äº†ç²¾å‡†çš„å†³ç­–æ”¯æŒï¼Œå…·æœ‰æ˜¾è‘—çš„è¡Œä¸šåº”ç”¨ä»·å€¼ã€‚è¿™ä¸€åˆ›æ–°æ–¹æ³•ä¸ä»…æå‡äº†æ—¶é—´åºåˆ—é¢„æµ‹çš„æ•ˆç‡ï¼Œä¹Ÿä¸ºå¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„æ™ºèƒ½åŒ–å†³ç­–æµç¨‹åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17165v1",
      "published_date": "2025-09-21 17:16:32 UTC",
      "updated_date": "2025-09-21 17:16:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:24.256115+00:00"
    },
    {
      "arxiv_id": "2509.17158v2",
      "title": "ARE: Scaling Up Agent Environments and Evaluations",
      "title_zh": "AREï¼šæ™ºèƒ½ä½“ç¯å¢ƒä¸è¯„ä¼°çš„è§„æ¨¡åŒ–æ‰©å±•",
      "authors": [
        "Romain Froger",
        "Pierre Andrews",
        "Matteo Bettini",
        "Amar Budhiraja",
        "Ricardo Silveira Cabral",
        "Virginie Do",
        "Emilien Garreau",
        "Jean-Baptiste Gaya",
        "Hugo LaurenÃ§on",
        "Maxime Lecanu",
        "Kunal Malkan",
        "Dheeraj Mekala",
        "Pierre MÃ©nard",
        "Gerard Moreno-Torres Bertran",
        "Ulyana Piterbarg",
        "Mikhail Plekhanov",
        "Mathieu Rita",
        "Andrey Rusakov",
        "Vladislav Vorotilov",
        "Mengjue Wang",
        "Ian Yu",
        "Amine Benhalloum",
        "GrÃ©goire Mialon",
        "Thomas Scialom"
      ],
      "abstract": "We introduce Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Meta Agents Research Environments (ARE)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤§è§„æ¨¡åˆ›å»ºç¯å¢ƒã€é›†æˆåº”ç”¨ä»¥åŠæ‰§è¡Œæ™ºèƒ½ä½“(Agent)ç¼–æ’çš„ç ”ç©¶å¹³å°ï¼Œæ—¨åœ¨ç¼©å°æ¨¡å‹å¼€å‘ä¸çœŸå®ä¸–ç•Œéƒ¨ç½²ä¹‹é—´çš„å·®è·ã€‚ARE é€šè¿‡ç®€å•çš„æŠ½è±¡æ„å»ºå¤æ‚å¤šæ ·çš„ç¯å¢ƒï¼Œå¹¶é…å¤‡ç‹¬ç«‹çš„è§„åˆ™ã€å·¥å…·å’ŒéªŒè¯å™¨ã€‚ç ”ç©¶äººå‘˜åŒæ­¥æ¨å‡ºäº†åœ¨ ARE ä¸­æ„å»ºçš„ Gaia2 åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤„ç†æ­§ä¹‰ã€å™ªå£°ã€å¤šæ™ºèƒ½ä½“åä½œåŠæ—¶é—´çº¦æŸç­‰æ–¹é¢çš„é€šç”¨èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„é™æ€æµ‹è¯•ä¸åŒï¼ŒGaia2 é‡‡ç”¨å¼‚æ­¥è¿è¡Œæœºåˆ¶ï¼Œèƒ½å¤Ÿæ•æ‰åˆ°åŠ¨æ€ç¯å¢ƒä¸‹ç‰¹æœ‰çš„å¤±æ•ˆæ¨¡å¼ã€‚å®éªŒå‘ç°ï¼Œæ›´å¼ºçš„æ¨ç†èƒ½åŠ›å¾€å¾€ä¼´éšç€æ•ˆç‡çš„ä¸‹é™ï¼Œä¸”é¢„ç®—æ‰©å±•æ›²çº¿(Budget scaling curves)å­˜åœ¨å¹³å°æœŸï¼Œè¿™è¡¨æ˜ç›®å‰çš„æ¶æ„å’Œè®¡ç®—ç­–ç•¥ä»éœ€æ”¹è¿›ã€‚ARE çš„æŠ½è±¡æ¡†æ¶è¿˜æ”¯æŒç¤¾åŒºæŒç»­æ‰©å±•åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å®šä¹‰æœ‰æ„ä¹‰çš„ä»»åŠ¡å’Œé²æ£’çš„è¯„ä¼°æ¨åŠ¨å‰æ²¿ AI èƒ½åŠ›çš„å‘å±•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Updated authors order and acknowledgement",
      "pdf_url": "https://arxiv.org/pdf/2509.17158v2",
      "published_date": "2025-09-21 16:59:45 UTC",
      "updated_date": "2025-12-10 23:35:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:27.260211+00:00"
    },
    {
      "arxiv_id": "2509.17153v2",
      "title": "Flow-Induced Diagonal Gaussian Processes",
      "title_zh": "æµè¯±å¯¼å¯¹è§’é«˜æ–¯è¿‡ç¨‹",
      "authors": [
        "Moule Lin",
        "Andrea Patane",
        "Weipeng Jing",
        "Shuhao Guan",
        "Goetz Botterweck"
      ],
      "abstract": "We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression framework that incorporates a compact inducing weight matrix to project a neural network's weight uncertainty into a lower-dimensional subspace. Critically, FiD-GP relies on normalising-flow priors and spectral regularisations to augment its expressiveness and align the inducing subspace with feature-gradient geometry through a numerically stable projection mechanism objective. Furthermore, we demonstrate how the prediction framework in FiD-GP can help to design a single-pass projection for Out-of-Distribution (OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation ability on various tasks compared with SVGP-based baselines, satisfies tight spectral residual bounds with theoretically guaranteed OoD detection, and significantly compresses the neural network's storage requirements at the cost of increased inference computation dependent on the number of inducing weights employed. Specifically, in a comprehensive empirical study spanning regression, image classification, semantic segmentation, and out-of-distribution detection benchmarks, it cuts Bayesian training cost by several orders of magnitude, compresses parameters by roughly 51%, reduces model size by about 75%, and matches state-of-the-art accuracy and uncertainty estimation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Flow-Induced Diagonal Gaussian Processes (FiD-GP)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç´§å‡‘çš„ inducing weight matrix å°†ç¥ç»ç½‘ç»œæƒé‡ä¸ç¡®å®šæ€§æŠ•å½±åˆ°ä½ç»´å­ç©ºé—´çš„å‹ç¼©æ¡†æ¶ã€‚FiD-GP åˆ©ç”¨ normalizing-flow priors å’Œ spectral regularisations å¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ•°å€¼ç¨³å®šçš„æŠ•å½±æœºåˆ¶ä½¿è¯±å¯¼å­ç©ºé—´ä¸ feature-gradient geometry å¯¹é½ã€‚è¯¥æ¡†æ¶è¿˜ä¸º Out-of-Distribution (OoD) detection è®¾è®¡äº†å•æ¬¡æŠ•å½±æœºåˆ¶ï¼Œå¹¶æä¾›äº†ç†è®ºä¸Šçš„æ£€æµ‹ä¿è¯ã€‚å®éªŒè¡¨æ˜ï¼ŒFiD-GP åœ¨ä¸ç¡®å®šæ€§ä¼°è®¡ä»»åŠ¡ä¸Šä¼˜äºåŸºäº SVGP çš„åŸºçº¿æ–¹æ³•ï¼Œä¸”æ˜¾è‘—é™ä½äº†è´å¶æ–¯è®­ç»ƒæˆæœ¬ã€‚åœ¨æ¶µç›–å›å½’ã€å›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒ SOTA ç²¾åº¦å’Œä¸ç¡®å®šæ€§ä¼°è®¡æ°´å¹³çš„åŒæ—¶ï¼ŒæˆåŠŸå‹ç¼©äº†çº¦ 51% çš„å‚æ•°å¹¶å‡å°‘äº† 75% çš„æ¨¡å‹å­˜å‚¨ä½“ç§¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.17153v2",
      "published_date": "2025-09-21 16:49:09 UTC",
      "updated_date": "2025-10-02 18:17:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:29.653231+00:00"
    },
    {
      "arxiv_id": "2509.17143v1",
      "title": "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances",
      "title_zh": "MaskVCTï¼šé€šè¿‡å¤šé‡å¼•å¯¼æå‡å¯æ§æ€§çš„é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢æ©ç è¯­éŸ³ç¼–è§£ç  Transformer",
      "authors": [
        "Junhyeok Lee",
        "Helin Wang",
        "Yaohan Guan",
        "Thomas Thebaud",
        "Laureano Moro-Velazquez",
        "JesÃºs Villalba",
        "Najim Dehak"
      ],
      "abstract": "We introduce MaskVCT, a zero-shot voice conversion (VC) model that offers multi-factor controllability through multiple classifier-free guidances (CFGs). While previous VC models rely on a fixed conditioning scheme, MaskVCT integrates diverse conditions in a single model. To further enhance robustness and control, the model can leverage continuous or quantized linguistic features to enhance intellgibility and speaker similarity, and can use or omit pitch contour to control prosody. These choices allow users to seamlessly balance speaker identity, linguistic content, and prosodic factors in a zero-shot VC setting. Extensive experiments demonstrate that MaskVCT achieves the best target speaker and accent similarities while obtaining competitive word and character error rates compared to existing baselines. Audio samples are available at https://maskvct.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MaskVCTï¼Œä¸€ç§åŸºäº Masked Voice Codec Transformer çš„é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢(Zero-Shot Voice Conversion)æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥å¤šç§æ— åˆ†ç±»å™¨å¼•å¯¼(Classifier-Free Guidances, CFGs)å®ç°äº†å¤šç»´åº¦çš„å¯æ§æ€§ã€‚è¯¥æ¨¡å‹æ•´åˆäº†å¤šç§è°ƒèŠ‚æ¡ä»¶ï¼Œèƒ½å¤Ÿçµæ´»åˆ©ç”¨è¿ç»­æˆ–é‡åŒ–çš„è¯­è¨€ç‰¹å¾æ¥æå‡è¯­éŸ³çš„å¯ç†è§£æ€§ä¸è¯´è¯äººç›¸ä¼¼åº¦ï¼Œå¹¶æ”¯æŒé€šè¿‡éŸ³é«˜è½®å»“(Pitch Contour)çš„åŠ å…¥æˆ–çœç•¥æ¥ç²¾ç¡®è°ƒèŠ‚éŸµå¾‹ã€‚è¿™ç§æ¶æ„å…è®¸åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹æ— ç¼å¹³è¡¡è¯´è¯äººèº«ä»½ã€æ–‡æœ¬å†…å®¹å’ŒéŸµå¾‹è¡¨ç°ã€‚å®éªŒè¯æ˜ï¼ŒMaskVCT åœ¨ç›®æ ‡è¯´è¯äººåŠå£éŸ³ç›¸ä¼¼åº¦æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œå¹¶åœ¨å­—é”™ç‡(WER)å’Œå­—ç¬¦é”™è¯¯ç‡(CER)æ–¹é¢è¡¨ç°å‡ºäº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17143v1",
      "published_date": "2025-09-21 16:14:51 UTC",
      "updated_date": "2025-09-21 16:14:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:34.942027+00:00"
    },
    {
      "arxiv_id": "2509.17136v1",
      "title": "SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM",
      "title_zh": "SAECï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åœºæ™¯æ„ŸçŸ¥å¢å¼ºå‹è¾¹äº‘åä½œå·¥ä¸šè§†è§‰æ£€æµ‹",
      "authors": [
        "Yuhao Tian",
        "Zheming Yang"
      ],
      "abstract": "Industrial vision inspection requires high accuracy under stringent resource constraints, yet existing approaches face a fundamental trade-off. Multimodal LLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive computational costs, while lightweight edge models often fail on complex cases. In this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative industrial vision inspection framework with MLLM. The framework is composed of three synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect Inspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3) Adaptive Edge-Cloud Scheduler. Together, these modules enable robust defect detection by tailoring multimodal reasoning to scene complexity and dynamically balancing computation between edge and cloud resources. Experimental results on MVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72% accuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It also reduces runtime by up to 22.4% and cuts energy per correct decision by 40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAECï¼Œä¸€ç§é¢å‘å·¥ä¸šè§†è§‰æ£€æµ‹çš„åœºæ™¯æ„ŸçŸ¥å¢å¼ºå‹è¾¹ç¼˜-äº‘åä½œæ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)çš„é«˜è®¡ç®—æˆæœ¬ä¸è½»é‡çº§è¾¹ç¼˜æ¨¡å‹çš„ç²¾åº¦ç“¶é¢ˆã€‚è¯¥æ¡†æ¶ç”±é«˜æ•ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ(Efficient MLLM Fine-Tuning)ã€è½»é‡çº§å¤šå°ºåº¦åœºæ™¯å¤æ‚åº¦ä¼°è®¡(Lightweight Multiscale Scene-Complexity Estimation)ä»¥åŠè‡ªé€‚åº”è¾¹ç¼˜-äº‘è°ƒåº¦å™¨(Adaptive Edge-Cloud Scheduler)ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆã€‚é€šè¿‡æ ¹æ®åœºæ™¯å¤æ‚åº¦å®šåˆ¶å¤šæ¨¡æ€æ¨ç†å¹¶åŠ¨æ€å¹³è¡¡è¾¹ç¼˜ä¸äº‘ç«¯èµ„æºï¼ŒSAECå®ç°äº†é²æ£’çš„ç¼ºé™·æ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAECåœ¨MVTec ADå’ŒKSDD2æ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°85.11% and 82.72%çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†Qwenå’ŒLLaVAç­‰ä¸»æµæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å°†è¿è¡Œæ—¶é—´ç¼©çŸ­äº†22.4%ï¼Œå¹¶å°†å•æ¬¡æ­£ç¡®å†³ç­–èƒ½è€—é™ä½äº†40%-74%ï¼Œä¸ºå·¥ä¸šè§†è§‰æ£€æµ‹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”é«˜ç²¾åº¦çš„éƒ¨ç½²æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.17136v1",
      "published_date": "2025-09-21 15:58:31 UTC",
      "updated_date": "2025-09-21 15:58:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:38.776845+00:00"
    },
    {
      "arxiv_id": "2509.20384v1",
      "title": "R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning",
      "title_zh": "R1-Fuzzï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å®ç°æ–‡æœ¬æ¨¡ç³Šæµ‹è¯•è¯­è¨€æ¨¡å‹ç‰¹åŒ–",
      "authors": [
        "Jiayi Lin",
        "Liangcai Su",
        "Junzhe Li",
        "Chenxiong Qian"
      ],
      "abstract": "Fuzzing is effective for vulnerability discovery but struggles with complex targets such as compilers, interpreters, and database engines, which accept textual input that must satisfy intricate syntactic and semantic constraints. Although language models (LMs) have attracted interest for this task due to their vast latent knowledge and reasoning potential, their practical adoption has been limited. The major challenges stem from insufficient exploration of deep program logic among real-world codebases, and the high cost of leveraging larger models. To overcome these challenges, we propose R1-Fuzz, the first framework that leverages reinforcement learning (RL) to specialize cost-efficient LMs and integrate them for complex textual fuzzing input generation. R1-Fuzz introduces two key designs: coverage-slicing-based question construction and a distance-based reward calculation. Through RL-based post-training of a model with our constructed dataset, R1-Fuzz designs a fuzzing workflow that tightly integrates LMs to reason deep program semantics during fuzzing. Evaluations on diverse real-world targets show that our design enables a small model, named R1-Fuzz-7B, to rival or even outperform much larger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\\% higher coverage than state-of-the-art fuzzers and discovers 29 previously unknown vulnerabilities, demonstrating its practicality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†R1-Fuzzï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ¥ä¼˜åŒ–é«˜æ€§ä»·æ¯”è¯­è¨€æ¨¡å‹(LMs)å¹¶å°†å…¶é›†æˆäºå¤æ‚æ–‡æœ¬æ¨¡ç³Šæµ‹è¯•(Textual Fuzzing)è¾“å…¥ç”Ÿæˆçš„æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿæ¨¡ç³Šæµ‹è¯•åœ¨å¤„ç†å…·æœ‰å¤æ‚è¯­æ³•å’Œè¯­ä¹‰çº¦æŸçš„ç›®æ ‡ï¼ˆå¦‚ç¼–è¯‘å™¨ã€æ•°æ®åº“å¼•æ“ï¼‰æ—¶é¢ä¸´çš„æ·±åº¦é€»è¾‘æ¢ç´¢ä¸è¶³å’Œå¤§å‹æ¨¡å‹è°ƒç”¨æˆæœ¬è¿‡é«˜çš„é—®é¢˜ï¼ŒR1-Fuzzå¼•å…¥äº†åŸºäºè¦†ç›–èŒƒå›´åˆ‡ç‰‡(Coverage-Slicing)çš„æé—®æ„å»ºå’ŒåŸºäºè·ç¦»çš„å¥–åŠ±è®¡ç®—(Distance-Based Reward)ä¸¤é¡¹æ ¸å¿ƒè®¾è®¡ã€‚é€šè¿‡åœ¨æ„å»ºçš„æ•°æ®é›†ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼Œè¯¥æ¡†æ¶ä½¿å°å‹æ¨¡å‹R1-Fuzz-7Båœ¨æ¨ç†æ·±å±‚ç¨‹åºè¯­ä¹‰æ—¶èƒ½å¤Ÿå±•ç°å‡ºä¼˜äºå¤§å‹æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒR1-Fuzzåœ¨å¤šç§çœŸå®ä¸–ç•Œç›®æ ‡ä¸Šçš„ä»£ç è¦†ç›–ç‡æ¯”ç°æœ‰å…ˆè¿›æ¨¡ç³Šæµ‹è¯•å·¥å…·é«˜å‡ºå¤šè¾¾75%ï¼Œå¹¶æˆåŠŸå‘ç°äº†29ä¸ªæ­¤å‰æœªçŸ¥çš„æ¼æ´ï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨å¤æ‚ç³»ç»Ÿæ¼æ´æŒ–æ˜ä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20384v1",
      "published_date": "2025-09-21 15:21:43 UTC",
      "updated_date": "2025-09-21 15:21:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:42.680317+00:00"
    },
    {
      "arxiv_id": "2509.17119v1",
      "title": "ScenGAN: Attention-Intensive Generative Model for Uncertainty-Aware Renewable Scenario Forecasting",
      "title_zh": "ScenGANï¼šé¢å‘ä¸ç¡®å®šæ€§æ„ŸçŸ¥å¯å†ç”Ÿèƒ½æºåœºæ™¯é¢„æµ‹çš„æ·±åº¦æ³¨æ„åŠ›ç”Ÿæˆæ¨¡å‹",
      "authors": [
        "Yifei Wu",
        "Bo Wang",
        "Jingshi Cui",
        "Pei-chun Lin",
        "Junzo Watada"
      ],
      "abstract": "To address the intermittency of renewable energy source (RES) generation, scenario forecasting offers a series of stochastic realizations for predictive objects with superior flexibility and direct views. Based on a long time-series perspective, this paper explores uncertainties in the realms of renewable power and deep learning. Then, an uncertainty-aware model is meticulously designed for renewable scenario forecasting, which leverages an attention mechanism and generative adversarial networks (GANs) to precisely capture complex spatial-temporal dynamics. To improve the interpretability of uncertain behavior in RES generation, Bayesian deep learning and adaptive instance normalization (AdaIN) are incorporated to simulate typical patterns and variations. Additionally, the integration of meteorological information, forecasts, and historical trajectories in the processing layer improves the synergistic forecasting capability for multiscale periodic regularities. Numerical experiments and case analyses demonstrate that the proposed approach provides an appropriate interpretation for renewable uncertainty representation, including both aleatoric and epistemic uncertainties, and shows superior performance over state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯å†ç”Ÿèƒ½æº(RES)å‘ç”µçš„é—´æ­‡æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºScenGANçš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡åœºæ™¯é¢„æµ‹æä¾›çµæ´»çš„éšæœºå®ç°æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹ç»“åˆäº†æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanism)å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)ï¼Œä»¥ç²¾ç¡®æ•è·å¤æ‚çš„æ—¶ç©ºåŠ¨æ€ç‰¹å¾ã€‚ä¸ºäº†æå‡å¯¹ä¸ç¡®å®šæ€§è¡Œä¸ºçš„å¯è§£é‡Šæ€§ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è´å¶æ–¯æ·±åº¦å­¦ä¹ (Bayesian Deep Learning)å’Œè‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–(AdaIN)æ¥æ¨¡æ‹Ÿå…¸å‹æ¨¡å¼ä¸å˜åŠ¨ã€‚é€šè¿‡åœ¨å¤„ç†å±‚æ•´åˆæ°”è±¡ä¿¡æ¯ã€é¢„æµ‹æ•°æ®å’Œå†å²è½¨è¿¹ï¼ŒScenGANå¢å¼ºäº†å¯¹å¤šå°ºåº¦å‘¨æœŸæ€§è§„å¾‹çš„ååŒé¢„æµ‹èƒ½åŠ›ã€‚æ•°å€¼å®éªŒå’Œæ¡ˆä¾‹åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£é‡Šå¶ç„¶ä¸ç¡®å®šæ€§(Aleatoric Uncertainty)å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§(Epistemic Uncertainty)ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒScenGANåœ¨å¯å†ç”Ÿèƒ½æºä¸ç¡®å®šæ€§è¡¨å¾æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜è¶Šæ€§ï¼Œå…¶æ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰çš„å‰æ²¿æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17119v1",
      "published_date": "2025-09-21 15:18:51 UTC",
      "updated_date": "2025-09-21 15:18:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:42.184644+00:00"
    },
    {
      "arxiv_id": "2509.17116v2",
      "title": "MCTS-EP: Empowering Embodied Planning with Online Preference Optimization",
      "title_zh": "MCTS-EPï¼šé€šè¿‡åœ¨çº¿åå¥½ä¼˜åŒ–èµ‹èƒ½å…·èº«è§„åˆ’",
      "authors": [
        "Hang Xu",
        "Zang Yu",
        "Yehui Tang",
        "Pengbo Hu",
        "Yuhao Tang",
        "Hao Dong"
      ],
      "abstract": "This paper introduces MCTS-EP, an online learning framework that combines large language models (LLM) with Monte Carlo Tree Search (MCTS) for training embodied agents. MCTS-EP integrates three key components: MCTS-guided exploration for preference data collection, efficient multi-modal reasoning mechanism, and iterative training pipeline based on preference optimization. We theoretically prove that MCTS-EP achieves better performance bounds than conventional on-policy algorithms when the loss function is strongly convex, and demonstrate that it can be formulated as a search-enhanced variant of GAIL. MCTS-EP achieves state-of-the-art performace across serval benchmarks. In ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks. In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code available at: https://github.com/xuhang-2/Embodied-Agent-Planning",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MCTS-EPï¼Œä¸€ç§å°†å¤§è¯­è¨€æ¨¡å‹ (LLM) ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ (Monte Carlo Tree Search, MCTS) ç›¸ç»“åˆçš„åœ¨çº¿å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å…·èº«æ™ºèƒ½ä½“ (Embodied Agents) çš„è§„åˆ’èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ•´åˆäº† MCTS å¼•å¯¼çš„åå¥½æ•°æ®æ¢ç´¢é‡‡é›†ã€é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æœºåˆ¶ä»¥åŠåŸºäºåå¥½ä¼˜åŒ– (Preference Optimization) çš„è¿­ä»£è®­ç»ƒæµç¨‹ã€‚ç†è®ºä¸Šï¼Œè¯¥ç ”ç©¶è¯æ˜äº† MCTS-EP åœ¨æŸå¤±å‡½æ•°å¼ºå‡¸æ—¶å…·æœ‰æ¯”ä¼ ç»ŸåŒç­–ç•¥ (On-policy) ç®—æ³•æ›´å¥½çš„æ€§èƒ½ç•Œé™ï¼Œå¹¶å°†å…¶å®šä¹‰ä¸º GAIL çš„æœç´¢å¢å¼ºå˜ä½“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMCTS-EP åœ¨ ALFWorld çš„æ–‡æœ¬ä¸è§†è§‰ä»»åŠ¡ä¸­åˆ†åˆ«å®ç°äº† 92% å’Œ 87% çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨ WebShop è¾¾åˆ° 0.81 çš„å¹³å‡å¥–åŠ±ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰ ALFWorld ä»»åŠ¡ä¸­æ˜¾è‘—å‡å°‘äº†äº¤äº’æ­¥æ•°ï¼Œä»åŸå…ˆçš„è¿‘ 20 æ­¥é™ä½è‡³çº¦ 10 æ­¥ï¼Œå±•ç°äº†å“è¶Šçš„è§„åˆ’æ•ˆç‡ä¸æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17116v2",
      "published_date": "2025-09-21 15:17:44 UTC",
      "updated_date": "2025-12-16 06:22:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:04:53.376168+00:00"
    },
    {
      "arxiv_id": "2509.20383v1",
      "title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning",
      "title_zh": "MARSï¼šè”é‚¦å­¦ä¹ ä¸­æ„ŸçŸ¥æ¶æ„çš„åé—¨é˜²å¾¡",
      "authors": [
        "Wei Wan",
        "Yuxuan Ning",
        "Zhicong Huang",
        "Cheng Hong",
        "Shengshan Hu",
        "Ziqi Zhou",
        "Yechao Zhang",
        "Tianqing Zhu",
        "Wanlei Zhou",
        "Leo Yu Zhang"
      ],
      "abstract": "Federated Learning (FL) is a distributed paradigm aimed at protecting participant data privacy by exchanging model parameters to achieve high-quality model training. However, this distributed nature also makes FL highly vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art (SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether the backdoor models have been accepted by the defender and adaptively optimizes backdoor models, rendering existing defenses ineffective. In this paper, we first reveal that the failure of existing defenses lies in the employment of empirical statistical measures that are loosely coupled with backdoor attacks. Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that leverages backdoor energy (BE) to indicate the malicious extent of each neuron. To amplify malignity, we further extract the most prominent BE values from each model to form a concentrated backdoor energy (CBE). Finally, a novel Wasserstein distance-based clustering method is introduced to effectively identify backdoor models. Extensive experiments demonstrate that MARS can defend against SOTA backdoor attacks and significantly outperforms existing defenses.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹ ï¼ˆFederated Learningï¼‰ä¸­ç°æœ‰é˜²å¾¡æ–¹æ³•åœ¨åº”å¯¹å¦‚3DFedç­‰å…ˆè¿›åé—¨æ”»å‡»æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†MARSï¼ˆMalignity-Aware backdooR defenSeï¼‰é˜²å¾¡æ¡†æ¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿé˜²å¾¡çš„å¤±è´¥æºäºå…¶é‡‡ç”¨çš„ç»éªŒç»Ÿè®¡åº¦é‡ä¸åé—¨æ”»å‡»ç¼ºä¹ç´§å¯†è€¦åˆã€‚ä¸ºæ­¤ï¼ŒMARSåˆ›æ–°æ€§åœ°åˆ©ç”¨åé—¨èƒ½é‡ï¼ˆbackdoor energy, BEï¼‰æ¥è¡¡é‡ç¥ç»å…ƒçš„æ¶æ„ç¨‹åº¦ï¼Œå¹¶é€šè¿‡æå–æœ€æ˜¾è‘—çš„BEå€¼æ„å»ºé›†ä¸­åé—¨èƒ½é‡ï¼ˆconcentrated backdoor energy, CBEï¼‰ä»¥æ”¾å¤§æ”»å‡»ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åŸºäºWassersteinè·ç¦»çš„èšç±»æ–¹æ³•ï¼Œèƒ½å¤Ÿç²¾ç¡®è¯†åˆ«å—æ”»å‡»çš„åé—¨æ¨¡å‹ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒMARSåœ¨æŠµå¾¡æœ€å…ˆè¿›ï¼ˆSOTAï¼‰åé—¨æ”»å‡»æ–¹é¢å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œå…¶é˜²å¾¡æ•ˆæœæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ–¹æ³•ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.20383v1",
      "published_date": "2025-09-21 14:50:02 UTC",
      "updated_date": "2025-09-21 14:50:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:05:09.592318+00:00"
    },
    {
      "arxiv_id": "2509.21361v1",
      "title": "Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs",
      "title_zh": "ä¸Šä¸‹æ–‡å³ä½ æ‰€éœ€ï¼šå¤§è¯­è¨€æ¨¡å‹ç°å®æé™ä¸‹çš„æœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£",
      "authors": [
        "Norman Paulsen"
      ],
      "abstract": "Large language model (LLM) providers boast big numbers for maximum context window sizes. To test the real world use of context windows, we 1) define a concept of maximum effective context window, 2) formulate a testing method of a context window's effectiveness over various sizes and problem types, and 3) create a standardized way to compare model efficacy for increasingly larger context window sizes to find the point of failure. We collected hundreds of thousands of data points across several models and found significant differences between reported Maximum Context Window (MCW) size and Maximum Effective Context Window (MECW) size. Our findings show that the MECW is, not only, drastically different from the MCW but also shifts based on the problem type. A few top of the line models in our test group failed with as little as 100 tokens in context; most had severe degradation in accuracy by 1000 tokens in context. All models fell far short of their Maximum Context Window by as much as 99 percent. Our data reveals the Maximum Effective Context Window shifts based on the type of problem provided, offering clear and actionable insights into how to improve model accuracy and decrease model hallucination rates.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç°å®ä½¿ç”¨ä¸­ä¸Šä¸‹æ–‡çª—å£çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£(Maximum Effective Context Window, MECW)çš„æ¦‚å¿µã€‚ç ”ç©¶è€…é€šè¿‡åˆ¶å®šæ ‡å‡†åŒ–çš„æµ‹è¯•æ–¹æ³•ï¼Œè¯„ä¼°äº†æ¨¡å‹åœ¨ä¸åŒè§„æ¨¡å’Œé—®é¢˜ç±»å‹ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨å¯¹æ¯”å‚å•†å®£ä¼ çš„æœ€å¤§ä¸Šä¸‹æ–‡çª—å£(Maximum Context Window, MCW)ä¸å®é™…è¡¨ç°ã€‚å®éªŒå‘ç°ï¼Œæ‰€æœ‰å‚ä¸æµ‹è¯•çš„æ¨¡å‹çš„MECWå‡æ˜¾è‘—ä½äºå…¶å®£ä¼ çš„MCWï¼Œå·®å€¼æœ€é«˜å¯è¾¾99%ã€‚æ•°æ®è¡¨æ˜ï¼Œéƒ¨åˆ†é¡¶å°–æ¨¡å‹åœ¨ä»…100ä¸ªtokensæ—¶ä¾¿ä¼šå‡ºç°å¤±æ•ˆï¼Œè€Œå¤šæ•°æ¨¡å‹åœ¨1000ä¸ªtokensæ—¶å‡†ç¡®ç‡å·²ä¸¥é‡é€€åŒ–ã€‚æ­¤å¤–ï¼ŒMECWçš„å¤§å°ä¼šæ ¹æ®å…·ä½“é—®é¢˜ç±»å‹è€Œå‘ç”Ÿæ³¢åŠ¨ã€‚è¿™äº›å‘ç°ä¸ºä¼˜åŒ–æ¨¡å‹å‡†ç¡®ç‡å’Œé™ä½å¹»è§‰(hallucination)å‘ç”Ÿç‡æä¾›äº†æ¸…æ™°ä¸”å…·å¤‡æ“ä½œæ€§çš„è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 4 charts",
      "pdf_url": "https://arxiv.org/pdf/2509.21361v1",
      "published_date": "2025-09-21 14:38:17 UTC",
      "updated_date": "2025-09-21 14:38:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:05:18.260345+00:00"
    },
    {
      "arxiv_id": "2509.17096v1",
      "title": "Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering",
      "title_zh": "Prompt-with-Meï¼šé¢å‘ LLM é©±åŠ¨è½¯ä»¶å·¥ç¨‹çš„ IDE å†…ç½®ç»“æ„åŒ–æç¤ºè¯ç®¡ç†",
      "authors": [
        "Ziyou Li",
        "Agnia Sergeyuk",
        "Maliheh Izadi"
      ],
      "abstract": "Large Language Models are transforming software engineering, yet prompt management in practice remains ad hoc, hindering reliability, reuse, and integration into industrial workflows. We present Prompt-with-Me, a practical solution for structured prompt management embedded directly in the development environment. The system automatically classifies prompts using a four-dimensional taxonomy encompassing intent, author role, software development lifecycle stage, and prompt type. To enhance prompt reuse and quality, Prompt-with-Me suggests language refinements, masks sensitive information, and extracts reusable templates from a developer's prompt library. Our taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can accurately classify software engineering prompts. Furthermore, our user study with 11 participants shows strong developer acceptance, with high usability (Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in prompt quality and efficiency through reduced repetitive effort. Lastly, we offer actionable insights for building the next generation of prompt management and maintenance tools for software engineering workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„è½¯ä»¶å·¥ç¨‹ä¸­æç¤ºè¯ç®¡ç†è¿‡äºéšæ„ã€éš¾ä»¥å¤ç”¨å’Œé›†æˆçš„é—®é¢˜ï¼Œæå‡ºäº†Prompt-with-Meï¼Œä¸€ç§ç›´æ¥åµŒå…¥é›†æˆå¼€å‘ç¯å¢ƒ(IDE)çš„ç»“æ„åŒ–æç¤ºè¯ç®¡ç†è§£å†³æ–¹æ¡ˆã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨åŒ…å«æ„å›¾(intent)ã€ä½œè€…è§’è‰²(author role)ã€è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ(SDLC stage)å’Œæç¤ºè¯ç±»å‹(prompt type)å››ä¸ªç»´åº¦çš„åˆ†ç±»æ³•å®ç°æç¤ºè¯è‡ªåŠ¨åˆ†ç±»ã€‚ä¸ºäº†æå‡æç¤ºè¯è´¨é‡ï¼ŒPrompt-with-Me èƒ½å¤Ÿæä¾›è¯­è¨€ä¼˜åŒ–å»ºè®®ã€è‡ªåŠ¨æ©ç›–æ•æ„Ÿä¿¡æ¯ï¼Œå¹¶ä»å¼€å‘è€…åº“ä¸­æå–å¯é‡ç”¨çš„æ¨¡æ¿(templates)ã€‚å¯¹1108æ¡çœŸå®æç¤ºè¯çš„åˆ†ç±»ç ”ç©¶è¯å®ï¼Œç°ä»£LLMså¯ä»¥å‡†ç¡®åˆ†ç±»è½¯ä»¶å·¥ç¨‹æç¤ºè¯ã€‚æ­¤å¤–ï¼ŒåŒ…å«11åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶æ˜¾ç¤ºè¯¥ç³»ç»Ÿå…·æœ‰é«˜å¯ç”¨æ€§(Mean SUS=73)å’Œæä½çš„è®¤çŸ¥è´Ÿè·(Mean NASA-TLX=21)ã€‚è¯¥å·¥å…·æ˜¾è‘—å‡å°‘äº†å¼€å‘è€…çš„é‡å¤æ€§å·¥ä½œå¹¶æå‡äº†å¼€å‘æ•ˆç‡ï¼Œä¸ºæ„å»ºä¸‹ä¸€ä»£è½¯ä»¶å·¥ç¨‹å·¥ä½œæµä¸­çš„æç¤ºè¯ç®¡ç†å·¥å…·æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated Software Engineering, ASE 2025 (Industry track)",
      "pdf_url": "https://arxiv.org/pdf/2509.17096v1",
      "published_date": "2025-09-21 14:24:37 UTC",
      "updated_date": "2025-09-21 14:24:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:05:18.967714+00:00"
    },
    {
      "arxiv_id": "2509.17095v1",
      "title": "Ultra-short-term solar power forecasting by deep learning and data reconstruction",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ ä¸æ•°æ®é‡æ„çš„è¶…çŸ­æœŸå¤ªé˜³èƒ½å‘ç”µé¢„æµ‹",
      "authors": [
        "Jinbao Wang",
        "Jun Liu",
        "Shiliang Zhang",
        "Xuehui Ma"
      ],
      "abstract": "The integration of solar power has been increasing as the green energy transition rolls out. The penetration of solar power challenges the grid stability and energy scheduling, due to its intermittent energy generation. Accurate and near real-time solar power prediction is of critical importance to tolerant and support the permeation of distributed and volatile solar power production in the energy system. In this paper, we propose a deep-learning based ultra-short-term solar power prediction with data reconstruction. We decompose the data for the prediction to facilitate extensive exploration of the spatial and temporal dependencies within the data. Particularly, we reconstruct the data into low- and high-frequency components, using ensemble empirical model decomposition with adaptive noise (CEEMDAN). We integrate meteorological data with those two components, and employ deep-learning models to capture long- and short-term dependencies towards the target prediction period. In this way, we excessively exploit the features in historical data in predicting a ultra-short-term solar power production. Furthermore, as ultra-short-term prediction is vulnerable to local optima, we modify the optimization in our deep-learning training by penalizing long prediction intervals. Numerical experiments with diverse settings demonstrate that, compared to baseline models, the proposed method achieves improved generalization in data reconstruction and higher prediction accuracy for ultra-short-term solar power production.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…‰ä¼å‘ç”µçš„é—´æ­‡æ€§å¯¹ç”µç½‘ç¨³å®šæ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ•°æ®é‡æ„çš„æ·±åº¦å­¦ä¹ (Deep Learning)è¶…çŸ­æœŸå…‰ä¼åŠŸç‡é¢„æµ‹æ–¹æ³•ã€‚ç ”ç©¶åˆ©ç”¨å¸¦è‡ªé€‚åº”å™ªå£°çš„å®Œå…¨é›†æˆç»éªŒæ¨¡æ€åˆ†è§£(CEEMDAN)æŠ€æœ¯ï¼Œå°†åŸå§‹æ•°æ®é‡æ„ä¸ºé«˜é¢‘å’Œä½é¢‘åˆ†é‡ï¼Œå¹¶æ•´åˆæ°”è±¡æ•°æ®ä»¥æ•æ‰å¤æ‚çš„æ—¶é—´å’Œç©ºé—´ä¾èµ–æ€§ã€‚ä¸ºäº†è§£å†³è¶…çŸ­æœŸé¢„æµ‹ä¸­å¸¸è§çš„å±€éƒ¨æœ€ä¼˜é—®é¢˜ï¼Œç ”ç©¶åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†å¯¹é•¿é¢„æµ‹åŒºé—´çš„æƒ©ç½šæœºåˆ¶ï¼Œä»è€Œä¼˜åŒ–äº†æ·±åº¦å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ•°æ®é‡æ„çš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶å®ç°äº†æ›´é«˜ç²¾åº¦çš„è¶…çŸ­æœŸå…‰ä¼åŠŸç‡é¢„æµ‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17095v1",
      "published_date": "2025-09-21 14:22:35 UTC",
      "updated_date": "2025-09-21 14:22:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:05:22.083702+00:00"
    },
    {
      "arxiv_id": "2509.17094v2",
      "title": "DiffSyn: A Generative Diffusion Approach to Materials Synthesis Planning",
      "title_zh": "DiffSynï¼šä¸€ç§ç”¨äºææ–™åˆæˆè§„åˆ’çš„ç”Ÿæˆå¼æ‰©æ•£æ–¹æ³•",
      "authors": [
        "Elton Pan",
        "Soonhyoung Kwon",
        "Sulin Liu",
        "Mingrou Xie",
        "Alexander J. Hoffman",
        "Yifei Duan",
        "Thorben Prein",
        "Killian Sheriff",
        "Yuriy Roman-Leshkov",
        "Manuel Moliner",
        "Rafael Gomez-Bombarelli",
        "Elsa Olivetti"
      ],
      "abstract": "The synthesis of crystalline materials, such as zeolites, remains a significant challenge due to a high-dimensional synthesis space, intricate structure-synthesis relationships and time-consuming experiments. Considering the one-to-many relationship between structure and synthesis, we propose DiffSyn, a generative diffusion model trained on over 23,000 synthesis recipes spanning 50 years of literature. DiffSyn generates probable synthesis routes conditioned on a desired zeolite structure and an organic template. DiffSyn achieves state-of-the-art performance by capturing the multi-modal nature of structure-synthesis relationships. We apply DiffSyn to differentiate among competing phases and generate optimal synthesis routes. As a proof of concept, we synthesize a UFI material using DiffSyn-generated synthesis routes. These routes, rationalized by density functional theory binding energies, resulted in the successful synthesis of a UFI material with a high Si/Al$_{\\text{ICP}}$ of 19.0, which is expected to improve thermal stability and is higher than that of any previously recorded.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ²¸çŸ³(zeolites)ç­‰æ™¶æ€ææ–™åˆæˆä¸­é¢ä¸´çš„é«˜ç»´æœç´¢ç©ºé—´å’Œå¤æ‚ç»“æ„-åˆæˆå…³ç³»ï¼Œæå‡ºäº†DiffSynç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹(generative diffusion model)ã€‚DiffSynåŸºäºè¿‡å»50å¹´æ–‡çŒ®ä¸­çš„23,000å¤šæ¡åˆæˆé…æ–¹è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæ ¹æ®ç›®æ ‡æ²¸çŸ³ç»“æ„å’Œæœ‰æœºæ¨¡æ¿ç”Ÿæˆå¯èƒ½çš„åˆæˆè·¯å¾„ã€‚é€šè¿‡æ•æ‰ç»“æ„-åˆæˆå…³ç³»çš„å¤šæ¨¡æ€(multi-modal)ç‰¹å¾ï¼ŒDiffSynåœ¨é¢„æµ‹åˆæˆè·¯çº¿æ–¹é¢è¾¾åˆ°äº†é¢†åŸŸé¢†å…ˆçš„æ€§èƒ½ã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨DiffSynç”Ÿæˆçš„è·¯å¾„æˆåŠŸåˆæˆäº†ä¸€ç§UFIææ–™ã€‚è¯¥è·¯å¾„å¾—åˆ°äº†å¯†åº¦æ³›å‡½ç†è®º(density functional theory)ç»“åˆèƒ½çš„éªŒè¯ï¼Œç”Ÿæˆçš„UFIææ–™å…·æœ‰19.0çš„é«˜Si/Al$_{ICP}$æ¯”ï¼Œä¸ä»…åˆ·æ–°äº†å†å²è®°å½•ï¼Œæ›´æœ‰æœ›æ˜¾è‘—æå‡ææ–™çš„çƒ­ç¨³å®šæ€§ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17094v2",
      "published_date": "2025-09-21 14:19:47 UTC",
      "updated_date": "2025-09-25 01:06:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:05:23.655239+00:00"
    },
    {
      "arxiv_id": "2509.19384v1",
      "title": "Data-Driven Reconstruction of Significant Wave Heights from Sparse Observations",
      "title_zh": "åŸºäºç¨€ç–è§‚æµ‹çš„æ•°æ®é©±åŠ¨æœ‰æ•ˆæ³¢é«˜é‡æ„",
      "authors": [
        "Hongyuan Shi",
        "Yilin Zhai",
        "Ping Dong",
        "Zaijin You",
        "Chao Zhan",
        "Qing Wang"
      ],
      "abstract": "Reconstructing high-resolution regional significant wave height fields from sparse and uneven buoy observations remains a core challenge for ocean monitoring and risk-aware operations. We introduce AUWave, a hybrid deep learning framework that fuses a station-wise sequence encoder (MLP) with a multi-scale U-Net enhanced by a bottleneck self-attention layer to recover 32$\\times$32 regional SWH fields. A systematic Bayesian hyperparameter search with Optuna identifies the learning rate as the dominant driver of generalization, followed by the scheduler decay and the latent dimension. Using NDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave attains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE distribution. Spatial errors are lowest near observation sites and increase with distance, reflecting identifiability limits under sparse sampling. Sensitivity experiments show that AUWave consistently outperforms a representative baseline in data-richer configurations, while the baseline is only marginally competitive in the most underdetermined single-buoy cases. The architecture's multi-scale and attention components translate into accuracy gains when minimal but non-trivial spatial anchoring is available. Error maps and buoy ablations reveal key anchor stations whose removal disproportionately degrades performance, offering actionable guidance for network design. AUWave provides a scalable pathway for gap filling, high-resolution priors for data assimilation, and contingency reconstruction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»ç¨€ç–ä¸”ä¸å‡åŒ€çš„æµ®æ ‡è§‚æµ‹ä¸­é‡å»ºé«˜åˆ†è¾¨ç‡åŒºåŸŸæœ‰æ•ˆæ³¢é«˜(Significant Wave Height, SWH)åœºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºAUWaveçš„æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é€ç«™åºåˆ—ç¼–ç å™¨(MLP)ä¸å¢å¼ºäº†ç“¶é¢ˆè‡ªæ³¨æ„åŠ›æœºåˆ¶(Bottleneck Self-Attention)çš„å¤šå°ºåº¦U-Netï¼Œå®ç°äº†32Ã—32åŒºåŸŸSWHåœºçš„ç²¾ç¡®æ¢å¤ã€‚é€šè¿‡åˆ©ç”¨Optunaè¿›è¡Œçš„è´å¶æ–¯è¶…å‚æ•°æœç´¢ï¼Œç ”ç©¶ç¡®ç«‹äº†å­¦ä¹ ç‡ç­‰å‚æ•°å¯¹æ¨¡å‹æ³›åŒ–æ€§èƒ½çš„å…³é”®é©±åŠ¨ä½œç”¨ã€‚åœ¨å¤å¨å¤·åœ°åŒºçš„å®éªŒè¯„ä¼°ä¸­ï¼ŒAUWaveè¾¾åˆ°äº†0.043285çš„æœ€ä½éªŒè¯æŸå¤±ï¼Œä¸”åœ¨å¤§å¤šæ•°é…ç½®ä¸‹å…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡æµ®æ ‡æ¶ˆèå®éªŒè¯†åˆ«å‡ºçš„å…³é”®é”šç‚¹ç«™(Anchor Stations)ä¸ºæµ·æ´‹ç›‘æµ‹ç½‘ç»œä¼˜åŒ–è®¾è®¡æä¾›äº†å®é™…æŒ‡å¯¼ã€‚AUWaveä¸ºæœ‰æ•ˆæ³¢é«˜åœºçš„ç¼ºæµ‹å¡«è¡¥(Gap Filling)ã€æ•°æ®åŒåŒ–çš„é«˜åˆ†è¾¨ç‡å…ˆéªŒä»¥åŠåº”æ€¥é‡å»ºæä¾›äº†ä¸€ç§å…·å¤‡æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "physics.ao-ph"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19384v1",
      "published_date": "2025-09-21 14:12:28 UTC",
      "updated_date": "2025-09-21 14:12:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:06:24.793965+00:00"
    },
    {
      "arxiv_id": "2509.17087v1",
      "title": "Governing Automated Strategic Intelligence",
      "title_zh": "è‡ªåŠ¨åŒ–æˆ˜ç•¥æƒ…æŠ¥æ²»ç†",
      "authors": [
        "Nicholas Kruus",
        "Madhavendra Thakur",
        "Adam Khoja",
        "Leonhard Nagel",
        "Maximilian Nicholson",
        "Abeer Sharma",
        "Jason Hausenloy",
        "Alberto KoTafoya",
        "Aliya Mukhanova",
        "Alli Katila-Miikkulainen",
        "Harish Chandran",
        "Ivan Zhang",
        "Jessie Chen",
        "Joel Raj",
        "Jord Nguyen",
        "Lai Hsien Hao",
        "Neja Jayasundara",
        "Soham Sen",
        "Sophie Zhang",
        "Ashley Dora Kokui Tamaklo",
        "Bhavya Thakur",
        "Henry Close",
        "Janghee Lee",
        "Nina Sefton",
        "Raghavendra Thakur",
        "Shiv Munagala",
        "Yeeun Kim"
      ],
      "abstract": "Military and economic strategic competitiveness between nation-states will increasingly be defined by the capability and cost of their frontier artificial intelligence models. Among the first areas of geopolitical advantage granted by such systems will be in automating military intelligence. Much discussion has been devoted to AI systems enabling new military modalities, such as lethal autonomous weapons, or making strategic decisions. However, the ability of a country of \"CIA analysts in a data-center\" to synthesize diverse data at scale, and its implications, have been underexplored. Multimodal foundation models appear on track to automate strategic analysis previously done by humans. They will be able to fuse today's abundant satellite imagery, phone-location traces, social media records, and written documents into a single queryable system. We conduct a preliminary uplift study to empirically evaluate these capabilities, then propose a taxonomy of the kinds of ground truth questions these systems will answer, present a high-level model of the determinants of this system's AI capabilities, and provide recommendations for nation-states to remain strategically competitive within the new paradigm of automated intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨è‡ªåŠ¨åŒ–å†›äº‹å’Œæˆ˜ç•¥æƒ…æŠ¥åˆ†ææ–¹é¢çš„æ½œåŠ›ï¼ŒæŒ‡å‡ºå…¶å°†æˆä¸ºå›½å®¶é—´å†›äº‹å’Œç»æµç«äº‰çš„å…³é”®ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨äº†â€œæ•°æ®ä¸­å¿ƒé‡Œçš„ CIA åˆ†æå¸ˆâ€è¿™ä¸€æ¦‚å¿µï¼Œå³åˆ©ç”¨ Multimodal foundation models èåˆå«æ˜Ÿå›¾åƒã€æ‰‹æœºå®šä½è¿½è¸ªã€ç¤¾äº¤åª’ä½“è®°å½•å’Œæ–‡æ¡£ç­‰æµ·é‡æ•°æ®è¿›è¡Œå¤§è§„æ¨¡è‡ªåŠ¨åŒ–åˆ†æã€‚ä½œè€…é€šè¿‡ä¸€é¡¹åˆæ­¥çš„ Uplift study å®è¯è¯„ä¼°äº†è¿™äº›èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€å¥—é’ˆå¯¹è¯¥ç³»ç»Ÿå¯å›ç­”äº‹å®é—®é¢˜çš„ Taxonomyã€‚æ–‡ç« è¿›ä¸€æ­¥æ„å»ºäº†ä¸€ä¸ªåˆ†æè¯¥ç³»ç»Ÿ AI èƒ½åŠ›å†³å®šå› ç´ çš„é«˜å±‚æ¨¡å‹ï¼Œå¹¶ä¸ºå›½å®¶åœ¨è‡ªåŠ¨åŒ–æƒ…æŠ¥æ–°èŒƒå¼ä¸‹ä¿æŒæˆ˜ç•¥ç«äº‰åŠ›æä¾›äº†å…·ä½“çš„æ”¿ç­–å»ºè®®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17087v1",
      "published_date": "2025-09-21 14:04:25 UTC",
      "updated_date": "2025-09-21 14:04:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:06:15.794407+00:00"
    },
    {
      "arxiv_id": "2509.17074v1",
      "title": "Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models",
      "title_zh": "åŸºäºåŸºç¡€æ¨¡å‹çš„ä¿¡æ¯åŒ–å›¾æ–‡å¯¹é½è§†è§‰ç¤ºèƒ½æ€§å­¦ä¹ ",
      "authors": [
        "Qian Zhang",
        "Lin Zhang",
        "Xing Fang",
        "Mingxin Zhang",
        "Zhiyuan Wei",
        "Ran Song",
        "Wei Zhang"
      ],
      "abstract": "Visual affordance learning is crucial for robots to understand and interact effectively with the physical world. Recent advances in this field attempt to leverage pre-trained knowledge of vision-language foundation models to learn affordance properties with limited training data, providing a novel paradigm for visual affordance learning. However, these methods overlook the significance of maintaining feature alignment between visual images and language descriptions for identifying affordance areas with textual guidance, and thus may lead to suboptimal results. In this paper, we present an informative framework for text-guided affordance learning, which involves information-based constraints to achieve text-image alignment at feature level. Specifically, we design an affordance mutual information constraint that helps learn appropriate textual prompts and task-oriented visual features simultaneously by maximizing the mutual information between the features of the affordance areas in the input images and the corresponding textual prompts. In addition, we propose an object-level information constraint that maximizes the mutual information between the visual features of a given object and the text features of the category it belongs to. This enables the model to capture high-quality representations for the object, providing more reliable semantic priors for identifying affordance regions. Experimental results on the AGD20K dataset show that the proposed method outperforms existing approaches and achieves the new state-of-the-art in one-shot affordance learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººåœ¨ç†è§£ç‰©ç†ä¸–ç•Œä¸­è‡³å…³é‡è¦çš„è§†è§‰å¯æ“ä½œæ€§å­¦ä¹ (Visual affordance learning)ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºåŸºç¡€æ¨¡å‹(Foundation Models)çš„ä¿¡æ¯æ–‡æœ¬-å›¾åƒå¯¹é½æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ–‡æœ¬æŒ‡å¯¼ä¸‹è¯†åˆ«å¯æ“ä½œåŒºåŸŸæ—¶å¿½è§†ç‰¹å¾å¯¹é½çš„é—®é¢˜ï¼Œè®ºæ–‡å¼•å…¥äº†åŸºäºä¿¡æ¯çš„çº¦æŸæœºåˆ¶ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶è®¾è®¡äº†affordance mutual information constraintï¼Œé€šè¿‡æœ€å¤§åŒ–è¾“å…¥å›¾åƒä¸­å¯æ“ä½œåŒºåŸŸç‰¹å¾ä¸å¯¹åº”æ–‡æœ¬æç¤ºä¹‹é—´çš„äº’ä¿¡æ¯ï¼ŒåŒæ—¶å­¦ä¹ åˆé€‚çš„æ–‡æœ¬æç¤ºå’Œé¢å‘ä»»åŠ¡çš„è§†è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†object-level information constraintï¼Œé€šè¿‡æœ€å¤§åŒ–å¯¹è±¡è§†è§‰ç‰¹å¾ä¸å…¶æ‰€å±ç±»åˆ«æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä¸ºè¯†åˆ«å¯æ“ä½œåŒºåŸŸæä¾›æ›´å¯é çš„è¯­ä¹‰å…ˆéªŒã€‚åœ¨AGD20Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å•æ ·æœ¬(one-shot)å¯æ“ä½œæ€§å­¦ä¹ ä¸­è¾¾åˆ°äº†æœ€æ–°çš„SOTAæ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to the IEEE International Conference on Robotics and Automation (ICRA) 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.17074v1",
      "published_date": "2025-09-21 13:21:16 UTC",
      "updated_date": "2025-09-21 13:21:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:06:17.193021+00:00"
    },
    {
      "arxiv_id": "2509.17068v1",
      "title": "Intention-aware Hierarchical Diffusion Model for Long-term Trajectory Anomaly Detection",
      "title_zh": "ç”¨äºé•¿æœŸè½¨è¿¹å¼‚å¸¸æ£€æµ‹çš„æ„å›¾æ„ŸçŸ¥åˆ†å±‚æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Chen Wang",
        "Sarah Erfani",
        "Tansu Alpcan",
        "Christopher Leckie"
      ],
      "abstract": "Long-term trajectory anomaly detection is a challenging problem due to the diversity and complex spatiotemporal dependencies in trajectory data. Existing trajectory anomaly detection methods fail to simultaneously consider both the high-level intentions of agents as well as the low-level details of the agent's navigation when analysing an agent's trajectories. This limits their ability to capture the full diversity of normal trajectories. In this paper, we propose an unsupervised trajectory anomaly detection method named Intention-aware Hierarchical Diffusion model (IHiD), which detects anomalies through both high-level intent evaluation and low-level sub-trajectory analysis. Our approach leverages Inverse Q Learning as the high-level model to assess whether a selected subgoal aligns with an agent's intention based on predicted Q-values. Meanwhile, a diffusion model serves as the low-level model to generate sub-trajectories conditioned on subgoal information, with anomaly detection based on reconstruction error. By integrating both models, IHiD effectively utilises subgoal transition knowledge and is designed to capture the diverse distribution of normal trajectories. Our experiments show that the proposed method IHiD achieves up to 30.2% improvement in anomaly detection performance in terms of F1 score over state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿æœŸè½¨è¿¹å¼‚å¸¸æ£€æµ‹ä¸­è½¨è¿¹å¤šæ ·æ€§å’Œå¤æ‚æ—¶ç©ºä¾èµ–å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º IHiD (Intention-aware Hierarchical Diffusion model) çš„æ— ç›‘ç£å±‚æ¬¡åŒ–æ‰©æ•£æ¨¡å‹ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•æ— æ³•åŒæ—¶å…¼é¡¾æ™ºèƒ½ä½“é«˜å±‚æ„å›¾(high-level intentions)ä¸åº•å±‚å¯¼èˆªç»†èŠ‚çš„å±€é™ï¼ŒIHiD é€šè¿‡é«˜å±‚æ„å›¾è¯„ä¼°ä¸åº•å±‚å­è½¨è¿¹åˆ†æçš„åŒé‡æœºåˆ¶è¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚é«˜å±‚æ¨¡å‹åˆ©ç”¨ Inverse Q Learning æ ¹æ®é¢„æµ‹çš„ Q å€¼åˆ¤æ–­å­ç›®æ ‡(subgoal)æ˜¯å¦ç¬¦åˆæ™ºèƒ½ä½“æ„å›¾ï¼Œè€Œåº•å±‚æ¨¡å‹åˆ™é‡‡ç”¨æ‰©æ•£æ¨¡å‹(Diffusion Model)ç”Ÿæˆå—å­ç›®æ ‡çº¦æŸçš„å­è½¨è¿¹ï¼Œå¹¶åŸºäºé‡æ„è¯¯å·®(reconstruction error)è¯†åˆ«å¼‚å¸¸ã€‚é€šè¿‡æ•´åˆå­ç›®æ ‡è½¬æ¢çŸ¥è¯†ï¼Œè¯¥æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰æ­£å¸¸è½¨è¿¹çš„å¤šæ ·åŒ–åˆ†å¸ƒã€‚å®éªŒè¯æ˜ï¼ŒIHiD åœ¨ F1 score ä¸Šç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹å®ç°äº†é«˜è¾¾ 30.2% çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.17068v1",
      "published_date": "2025-09-21 12:57:22 UTC",
      "updated_date": "2025-09-21 12:57:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:06:41.058854+00:00"
    },
    {
      "arxiv_id": "2509.17066v1",
      "title": "RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking",
      "title_zh": "RALLM-POIï¼šç»“åˆåœ°ç†é‡æ’åºä¸æ£€ç´¢å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬ä¸‹ä¸€å…´è¶£ç‚¹æ¨è",
      "authors": [
        "Kunrong Li",
        "Kwan Hui Lim"
      ],
      "abstract": "Next point-of-interest (POI) recommendation predicts a user's next destination from historical movements. Traditional models require intensive training, while LLMs offer flexible and generalizable zero-shot solutions but often generate generic or geographically irrelevant results due to missing trajectory and spatial context. To address these issues, we propose RALLM-POI, a framework that couples LLMs with retrieval-augmented generation and self-rectification. We first propose a Historical Trajectory Retriever (HTR) that retrieves relevant past trajectories to serve as contextual references, which are then reranked by a Geographical Distance Reranker (GDR) for prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier (ALR) is designed to refine outputs through self-reflection. Without additional training, RALLM-POI achieves substantial accuracy gains across three real-world Foursquare datasets, outperforming both conventional and LLM-based baselines. Code is released at https://github.com/LKRcrocodile/RALLM-POI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RALLM-POIï¼Œä¸€ä¸ªé’ˆå¯¹é›¶æ ·æœ¬(Zero-shot)ä¸‹ä¸€å…´è¶£ç‚¹(Next POI)æ¨èçš„æ£€ç´¢å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(Retrieval-Augmented LLM)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰LLMåœ¨ç¼ºä¹è½¨è¿¹å’Œç©ºé—´ä¸Šä¸‹æ–‡æ—¶ç”Ÿæˆç»“æœåœ°ç†ç›¸å…³æ€§å·®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡å†å²è½¨è¿¹æ£€ç´¢å™¨(Historical Trajectory Retriever, HTR)æå–ç›¸å…³çš„è¿‡å»ç§»åŠ¨è½¨è¿¹ä½œä¸ºä¸Šä¸‹æ–‡å‚è€ƒï¼Œå¹¶åˆ©ç”¨åœ°ç†è·ç¦»é‡æ’å™¨(Geographical Distance Reranker, GDR)å¯¹è½¨è¿¹è¿›è¡Œç©ºé—´ç›¸å…³æ€§æ’åºã€‚æ¥ç€ï¼Œç ”ç©¶è®¾è®¡äº†æ™ºèƒ½ä½“LLMçº é”™å™¨(Agentic LLM Rectifier, ALR)ï¼Œé€šè¿‡è‡ªæˆ‘åæ€(self-reflection)æœºåˆ¶è¿›ä¸€æ­¥ç²¾ç‚¼å’Œä¿®æ­£è¾“å‡ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒRALLM-POIåœ¨ä¸‰ä¸ªçœŸå®çš„Foursquareæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶å‡†ç¡®ç‡æ˜¾è‘—è¶…è¿‡äº†ä¼ ç»Ÿæ¨èæ¨¡å‹å’Œç°æœ‰çš„åŸºäºLLMçš„åŸºçº¿æ–¹æ³•ã€‚è¯¥å·¥ä½œä¸ºåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è§£å†³å…·æœ‰ç©ºé—´çº¦æŸçš„æ¨èä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "PRICAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.17066v1",
      "published_date": "2025-09-21 12:52:28 UTC",
      "updated_date": "2025-09-21 12:52:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:05:47.496637+00:00"
    },
    {
      "arxiv_id": "2509.17062v1",
      "title": "From domain-landmark graph learning to problem-landmark graph generation",
      "title_zh": "ä»é¢†åŸŸåœ°æ ‡å›¾å­¦ä¹ åˆ°é—®é¢˜åœ°æ ‡å›¾ç”Ÿæˆ",
      "authors": [
        "Cristian PÃ©rez-Corral",
        "Antonio Garrido",
        "Laura Sebastia"
      ],
      "abstract": "Landmarks have long played a pivotal role in automated planning, serving as crucial elements for improving the planning algorithms. The main limitation of classical landmark extraction methods is their sensitivity to specific planning tasks. This results in landmarks fully tailored to individual instances, thereby limiting their applicability across other instances of the same planning domain. We propose a novel approach that learns landmark relationships from multiple planning tasks of a planning domain. This leads to the creation of a \\textit{probabilistic lifted ordering graph}, as a structure that captures weighted abstractions of relationships between parameterized landmarks. Although these orderings are not 100\\% true (they are probabilistic), they can still be very useful in planning. Next, given a new planning task for that domain, we instantiate the relationships from that graph to this particular instance. This instantiation operates in two phases. First, it generates two graphs: the former instantiating information from the initial state and the latter from the goal state. Second, it combines these two graphs into one unified graph by searching equivalences to extract landmark orderings. We evaluate the precision and recallof the information found by our approach over well-known planning domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»å…¸Landmarkæå–æ–¹æ³•å¯¹ç‰¹å®šè§„åˆ’ä»»åŠ¡(planning tasks)è¿‡äºæ•æ„Ÿä¸”è·¨å®ä¾‹åº”ç”¨å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»åŒä¸€è§„åˆ’é¢†åŸŸ(planning domain)çš„å¤šä¸ªä»»åŠ¡ä¸­å­¦ä¹ Landmarkå…³ç³»çš„æ–°æ–¹æ³•ã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯æ„å»ºäº†probabilistic lifted ordering graphï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿæ•æ‰å‚æ•°åŒ–Landmarksä¹‹é—´å…³ç³»åŠ æƒæŠ½è±¡çš„æ¦‚ç‡ç»“æ„ã€‚å¯¹äºç»™å®šçš„æ–°è§„åˆ’ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œå®ä¾‹åŒ–ï¼šé¦–å…ˆåˆ†åˆ«ç”ŸæˆåŸºäºåˆå§‹çŠ¶æ€å’Œç›®æ ‡çŠ¶æ€çš„ä¿¡æ¯å›¾ï¼Œéšåé€šè¿‡æœç´¢ç­‰ä»·å…³ç³»å°†äºŒè€…ç»Ÿä¸€ä»¥æå–Landmark orderingsã€‚å®éªŒåœ¨å¤šä¸ªçŸ¥åè§„åˆ’åŸŸä¸­å¯¹è¯¥æ–¹æ³•çš„æŸ¥å‡†ç‡(precision)å’ŒæŸ¥å…¨ç‡(recall)è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†è¿™ç§ä»é¢†åŸŸå­¦ä¹ åˆ°é—®é¢˜ç”Ÿæˆçš„æœºåˆ¶åœ¨æé«˜è§„åˆ’æ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17062v1",
      "published_date": "2025-09-21 12:41:56 UTC",
      "updated_date": "2025-09-21 12:41:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:06:53.870488+00:00"
    },
    {
      "arxiv_id": "2509.17054v2",
      "title": "TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?",
      "title_zh": "TactfulToMï¼šLLMs æ˜¯å¦å…·å¤‡ç†è§£å–„æ„è°è¨€çš„å¿ƒç†ç†è®ºèƒ½åŠ›ï¼Ÿ",
      "authors": [
        "Yiwei Liu",
        "Emma Jane Pretty",
        "Jiahao Huang",
        "Saku Sugawara"
      ],
      "abstract": "While recent studies explore Large Language Models' (LLMs) performance on Theory of Mind (ToM) reasoning tasks, research on ToM abilities that require more nuanced social context is limited, such as white lies. We introduce TactfulToM, a novel English benchmark designed to evaluate LLMs' ability to understand white lies within real-life conversations and reason about prosocial motivations behind them, particularly when they are used to spare others' feelings and maintain social harmony. Our benchmark is generated through a multi-stage human-in-the-loop pipeline where LLMs expand manually designed seed stories into conversations to maintain the information asymmetry between participants necessary for authentic white lies. We show that TactfulToM is challenging for state-of-the-art models, which perform substantially below humans, revealing shortcomings in their ability to fully comprehend the ToM reasoning that enables true understanding of white lies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†â€œå–„æ„è°è¨€â€ï¼ˆwhite liesï¼‰ç­‰å¤æ‚ç¤¾äº¤è¯­å¢ƒä¸‹çš„å¿ƒç†ç†è®ºï¼ˆTheory of Mind, ToMï¼‰èƒ½åŠ›è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ç ”ç©¶è€…å¼•å…¥äº† TactfulToMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„è‹±æ–‡åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼° LLMs åœ¨ç°å®å¯¹è¯ä¸­ç†è§£å–„æ„è°è¨€åŠå…¶èƒŒåäº²ç¤¾ä¼šåŠ¨æœºçš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†é‡‡ç”¨äº†å¤šé˜¶æ®µçš„äººæœºåä½œï¼ˆhuman-in-the-loopï¼‰æµæ°´çº¿ï¼Œé€šè¿‡å°†ç§å­æ•…äº‹æ‰©å±•ä¸ºå¯¹è¯æ¥ç¡®ä¿å‚ä¸è€…ä¹‹é—´çš„ä¿¡æ¯ä¸å¯¹ç§°æ€§ï¼Œä»è€Œæ¨¡æ‹ŸçœŸå®çš„ç¤¾äº¤åœºæ™¯ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹åœ¨ TactfulToM ä¸Šçš„è¡¨ç°æ˜¾è‘—ä½äºäººç±»æ°´å¹³ã€‚è¿™ä¸€ç»“æœæ­ç¤ºäº† LLMs åœ¨å……åˆ†ç†è§£å’Œè¿ç”¨ ToM æ¨ç†ä»¥è¯†åˆ«ç¤¾äº¤å’Œè°å¯¼å‘çš„è°è¨€æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.17054v2",
      "published_date": "2025-09-21 12:18:35 UTC",
      "updated_date": "2025-09-24 18:47:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:26.187214+00:00"
    },
    {
      "arxiv_id": "2509.17046v2",
      "title": "A Chain-of-thought Reasoning Breast Ultrasound Dataset Covering All Histopathology Categories",
      "title_zh": "è¦†ç›–å…¨ç»„ç»‡ç—…ç†å­¦ç±»åˆ«çš„ä¹³è…ºè¶…å£°æ€ç»´é“¾æ¨ç†æ•°æ®é›†",
      "authors": [
        "Haojun Yu",
        "Youcheng Li",
        "Zihan Niu",
        "Nan Zhang",
        "Xuantong Gong",
        "Huan Li",
        "Zhiying Zou",
        "Haifeng Qi",
        "Zhenxiao Cao",
        "Zijie Lan",
        "Xingjian Yuan",
        "Jiating He",
        "Haokai Zhang",
        "Shengtao Zhang",
        "Zicheng Wang",
        "Dong Wang",
        "Ziwei Zhao",
        "Congying Chen",
        "Yong Wang",
        "Wangyan Qin",
        "Qingli Zhu",
        "Liwei Wang"
      ],
      "abstract": "Breast ultrasound (BUS) is an essential tool for diagnosing breast lesions, with millions of examinations per year. However, publicly available high-quality BUS benchmarks for AI development are limited in data scale and annotation richness. In this work, we present BUS-CoT, a BUS dataset for chain-of-thought (CoT) reasoning analysis, which contains 11,439 images of 10,019 lesions from 4,838 patients and covers all 99 histopathology types. To facilitate research on incentivizing CoT reasoning, we construct the reasoning processes based on observation, feature, diagnosis and pathology labels, annotated and verified by experienced experts. Moreover, by covering lesions of all histopathology types, we aim to facilitate robust AI systems in rare cases, which can be error-prone in clinical practice.",
      "tldr_zh": "è¯¥ç ”ç©¶å‘å¸ƒäº†BUS-CoTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºé“¾å¼æ€ç»´æ¨ç†(Chain-of-thought)åˆ†æè®¾è®¡çš„ä¹³è…ºè¶…å£°(Breast ultrasound)æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†è§„æ¨¡åºå¤§ï¼ŒåŒ…å«æ¥è‡ª4,838åæ‚£è€…çš„11,439å¼ å›¾åƒï¼Œæ¶‰åŠ10,019ä¸ªç—…ç¶ï¼Œä¸”å…¨é¢è¦†ç›–äº†å…¨éƒ¨99ç§ç»„ç»‡ç—…ç†å­¦(histopathology)ç±»å‹ã€‚ä¸ºäº†ä¿ƒè¿›å¯¹CoTæ¨ç†çš„ç ”ç©¶ï¼Œç ”ç©¶è€…åŸºäºè§‚å¯Ÿã€ç‰¹å¾ã€è¯Šæ–­å’Œç—…ç†æ ‡ç­¾æ„å»ºäº†å®Œæ•´çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶ç”±èµ„æ·±ä¸“å®¶è¿›è¡Œäº†ä¸¥æ ¼æ ‡æ³¨å’ŒéªŒè¯ã€‚é€šè¿‡æ¶µç›–æ‰€æœ‰ç—…ç†ç±»å‹ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨æå‡AIç³»ç»Ÿåœ¨ä¸´åºŠå®è·µä¸­å¤„ç†æ˜“é”™ç½•è§ç—…ä¾‹æ—¶çš„é²æ£’æ€§(robustness)ï¼Œä¸ºæ„å»ºæ›´å¯é ã€æ›´å…·å¯è§£é‡Šæ€§çš„åŒ»ç–—è¯Šæ–­ç³»ç»Ÿæä¾›äº†é«˜è´¨é‡çš„æ•°æ®æ”¯æŒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17046v2",
      "published_date": "2025-09-21 11:59:19 UTC",
      "updated_date": "2025-09-23 02:04:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:06:58.972811+00:00"
    },
    {
      "arxiv_id": "2509.21360v1",
      "title": "Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models",
      "title_zh": "é’ˆå¯¹æ–‡ç”Ÿå›¾æ¨¡å‹å®‰å…¨è¿‡æ»¤å™¨çš„å¤šæ¨¡æ€æç¤ºè¯è§£è€¦æ”»å‡»",
      "authors": [
        "Xingkai Peng",
        "Jun Jiang",
        "Meng Tong",
        "Shuai Li",
        "Weiming Zhang",
        "Nenghai Yu",
        "Kejiang Chen"
      ],
      "abstract": "Text-to-image (T2I) models have been widely applied in generating high-fidelity images across various domains. However, these models may also be abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks. Existing jailbreak methods primarily manipulate the textual prompt, leaving potential vulnerabilities in image-based inputs largely unexplored. Moreover, text-based methods face challenges in bypassing the model's safety filters. In response to these limitations, we propose the Multimodal Prompt Decoupling Attack (MPDA), which utilizes image modality to separate the harmful semantic components of the original unsafe prompt. MPDA follows three core steps: firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe prompts and harmful prompts. The former are seemingly harmless sub-prompts that can bypass filters, while the latter are sub-prompts with unsafe semantics that trigger filters. Subsequently, the LLM rewrites the harmful prompts into natural adversarial prompts to bypass safety filters, which guide the T2I model to modify the base image into an NSFW output. Finally, to ensure semantic consistency between the generated NSFW images and the original unsafe prompts, the visual language model generates image captions, providing a new pathway to guide the LLM in iterative rewriting and refining the generated content.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-image)æ¨¡å‹å¯èƒ½è¢«æ»¥ç”¨ç”Ÿæˆè¿è§„å†…å®¹(NSFW)çš„é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„æ–‡æœ¬è¶Šç‹±æ”»å‡»(Jailbreak attacks)åœ¨ç»•è¿‡å®‰å…¨è¿‡æ»¤å™¨(Safety filters)æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¤šæ¨¡æ€æç¤ºè§£è€¦æ”»å‡»(Multimodal Prompt Decoupling Attack, MPDA)ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€äº¤äº’åˆ†ç¦»ä¸å®‰å…¨æç¤ºçš„æœ‰å®³è¯­ä¹‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)å°†åŸå§‹æç¤ºè§£è€¦ä¸ºä¼ªå®‰å…¨æç¤ºå’Œæœ‰å®³æç¤ºï¼Œå¹¶å°†å…¶é‡å†™ä¸ºå¯ç»•è¿‡è¿‡æ»¤å™¨çš„è‡ªç„¶å¯¹æŠ—æ€§æç¤ºã€‚éšåï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹(VLM)ç”Ÿæˆçš„å›¾åƒå­—å¹•æ„å»ºåé¦ˆæœºåˆ¶ï¼ŒæŒ‡å¯¼LLMè¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„NSFWå›¾åƒä¸åŸå§‹æ„å›¾çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚MPDAä¸ä»…æ­ç¤ºäº†åˆ©ç”¨å›¾åƒæ¨¡æ€ç»•è¿‡å®‰å…¨é˜²å¾¡çš„æ–°è·¯å¾„ï¼Œä¹Ÿä¸ºè¯„ä¼°å’Œå¢å¼ºç”Ÿæˆå¼AIç³»ç»Ÿçš„å®‰å…¨æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21360v1",
      "published_date": "2025-09-21 11:22:32 UTC",
      "updated_date": "2025-09-21 11:22:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:03.679909+00:00"
    },
    {
      "arxiv_id": "2509.17040v2",
      "title": "From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning",
      "title_zh": "ä»æ˜“åˆ°éš¾ï¼šé¢å‘æ¸è¿›å¼äº¤é”™å¤šå›¾æ¨ç†çš„ MIR åŸºå‡†",
      "authors": [
        "Hang Du",
        "Jiayang Zhang",
        "Guoshun Nan",
        "Wendi Deng",
        "Zhenyan Chen",
        "Chenyang Zhang",
        "Wang Xiao",
        "Shan Huang",
        "Yuqi Pan",
        "Tao Qi",
        "Sicong Leng"
      ],
      "abstract": "Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language Models (MLLMs) ability to jointly comprehend and reason across multiple images and their associated textual contexts, introducing unique challenges beyond single-image or non-interleaved multi-image tasks. While current multi-image benchmarks overlook interleaved textual contexts and neglect distinct relationships between individual images and their associated texts, enabling models to reason over multi-image interleaved data may significantly enhance their comprehension of complex scenes and better capture cross-modal correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring joint reasoning over multiple images accompanied by interleaved textual contexts to accurately associate image regions with corresponding texts and logically connect information across images. To enhance MLLMs ability to comprehend multi-image interleaved data, we introduce reasoning steps for each instance within the benchmark and propose a stage-wise curriculum learning strategy. This strategy follows an \"easy to hard\" approach, progressively guiding models from simple to complex scenarios, thereby enhancing their ability to handle challenging tasks. Extensive experiments benchmarking multiple MLLMs demonstrate that our method significantly enhances models reasoning performance on MIR and other established benchmarks. We believe that MIR will encourage further research into multi-image interleaved reasoning, facilitating advancements in MLLMs capability to handle complex inter-modal tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªå…¨æ–°çš„åŸºå‡†æµ‹è¯• MIRï¼Œæ—¨åœ¨è§£å†³å½“å‰å¤šå›¾åƒåŸºå‡†æµ‹è¯•ä¸­å¿½ç•¥äº¤é”™æ–‡æœ¬è¯­å¢ƒ (Interleaved Textual Contexts) çš„é—®é¢˜ï¼Œè¿›è€Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) å¯¹å¤šå›¾åƒåŠå…¶å…³è”æ–‡æœ¬çš„è”åˆç†è§£èƒ½åŠ›ã€‚MIR è¦æ±‚æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­å‡†ç¡®å…³è”å›¾åƒåŒºåŸŸä¸å¯¹åº”æ–‡æœ¬ï¼Œå¹¶å®ç°è·¨å›¾åƒçš„ä¿¡æ¯é€»è¾‘è¿æ¥ã€‚ä¸ºå¢å¼ºæ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶è€…ä¸ºåŸºå‡†ä¸­çš„æ¯ä¸ªå®ä¾‹è®¾è®¡äº†æ¨ç†æ­¥éª¤ï¼Œå¹¶æå‡ºäº†ä¸€ç§é˜¶æ®µå¼è¯¾ç¨‹å­¦ä¹  (Stage-wise Curriculum Learning) ç­–ç•¥ã€‚è¯¥ç­–ç•¥é‡‡ç”¨â€œç”±æ˜“åˆ°éš¾ (Easy to Hard)â€çš„æ–¹æ³•ï¼Œé€æ­¥å¼•å¯¼æ¨¡å‹ä»ç®€å•åœºæ™¯è¿‡æ¸¡åˆ°å¤æ‚ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†å¤šä¸ª MLLMs åœ¨ MIR åŠå…¶ä»–ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„æ¨ç†è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†å¤šå›¾åƒäº¤é”™æ¨ç† (Multi-image Interleaved Reasoning) çš„å‘å±•ï¼Œä¹Ÿä¸ºæå‡ MLLMs å¤„ç†å¤æ‚è·¨æ¨¡æ€ä»»åŠ¡çš„èƒ½åŠ›æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.17040v2",
      "published_date": "2025-09-21 11:19:02 UTC",
      "updated_date": "2025-10-16 02:56:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:30.087812+00:00"
    },
    {
      "arxiv_id": "2509.17037v1",
      "title": "KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for Financial Data Narration",
      "title_zh": "KAHANï¼šé¢å‘é‡‘èæ•°æ®å™è¿°çš„çŸ¥è¯†å¢å¼ºå‹å±‚çº§åŒ–åˆ†æä¸å™è¿°",
      "authors": [
        "Yajing Yang",
        "Tony Deng",
        "Min-Yen Kan"
      ],
      "abstract": "We propose KAHAN, a knowledge-augmented hierarchical framework that systematically extracts insights from raw tabular data at entity, pairwise, group, and system levels. KAHAN uniquely leverages LLMs as domain experts to drive the analysis. On DataTales financial reporting benchmark, KAHAN outperforms existing approaches by over 20% on narrative quality (GPT-4o), maintains 98.2% factuality, and demonstrates practical utility in human evaluation. Our results reveal that knowledge quality drives model performance through distillation, hierarchical analysis benefits vary with market complexity, and the framework transfers effectively to healthcare domains. The data and code are available at https://github.com/yajingyang/kahan.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KAHANï¼Œä¸€ç§çŸ¥è¯†å¢å¼ºçš„å±‚æ¬¡åŒ–åˆ†æä¸å™è¿°æ¡†æ¶ï¼Œæ—¨åœ¨ä»åŸå§‹è¡¨æ ¼æ•°æ®ä¸­ç³»ç»Ÿåœ°æå–å®ä½“ã€æˆå¯¹ã€ç¾¤ä½“å’Œç³»ç»Ÿå±‚é¢çš„æ·±å±‚æ´å¯Ÿã€‚KAHANç‹¬ç‰¹åœ°åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºé¢†åŸŸä¸“å®¶æ¥é©±åŠ¨åˆ†æï¼Œé€šè¿‡å±‚æ¬¡åŒ–çš„å¤„ç†æµç¨‹ç¡®ä¿å¯¹è´¢åŠ¡æ•°æ®çš„ç²¾å‡†è§£è¯»ã€‚åœ¨DataTalesè´¢åŠ¡æŠ¥å‘ŠåŸºå‡†æµ‹è¯•ä¸­ï¼ŒKAHANåœ¨å™è¿°è´¨é‡ä¸Šæ¯”ç°æœ‰æ–¹æ³•æå‡äº†20%ä»¥ä¸Šï¼Œå¹¶ä¿æŒäº†é«˜è¾¾98.2%çš„äº‹å®å‡†ç¡®æ€§(factuality)ã€‚ç ”ç©¶å‘ç°ï¼ŒçŸ¥è¯†è´¨é‡é€šè¿‡è’¸é¦(distillation)æ˜¾è‘—å½±å“æ¨¡å‹è¡¨ç°ï¼Œä¸”å±‚æ¬¡åŒ–åˆ†æçš„æ”¶ç›Šä¼šéšå¸‚åœºå¤æ‚ç¨‹åº¦çš„å˜åŒ–è€Œæ³¢åŠ¨ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†è‰¯å¥½çš„é€šç”¨æ€§ï¼Œèƒ½å¤ŸæˆåŠŸè¿ç§»è‡³åŒ»ç–—ä¿å¥(healthcare)ç­‰å…¶ä»–ä¸“ä¸šé¢†åŸŸã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2509.17037v1",
      "published_date": "2025-09-21 11:15:43 UTC",
      "updated_date": "2025-09-21 11:15:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:13.569864+00:00"
    },
    {
      "arxiv_id": "2509.17030v1",
      "title": "The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs",
      "title_zh": "è½¬æ¢ç¥ç»å…ƒå‡è¯´ï¼šå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ä¸­è¯­è¨€æ½œç©ºé—´è½¬æ¢çš„åº•å±‚æœºåˆ¶",
      "authors": [
        "Hinata Tezuka",
        "Naoya Inoue"
      ],
      "abstract": "Recent studies have suggested a processing framework for multilingual inputs in decoder-based LLMs: early layers convert inputs into English-centric and language-agnostic representations; middle layers perform reasoning within an English-centric latent space; and final layers generate outputs by transforming these representations back into language-specific latent spaces. However, the internal dynamics of such transformation and the underlying mechanism remain underexplored. Towards a deeper understanding of this framework, we propose and empirically validate The Transfer Neurons Hypothesis: certain neurons in the MLP module are responsible for transferring representations between language-specific latent spaces and a shared semantic latent space. Furthermore, we show that one function of language-specific neurons, as identified in recent studies, is to facilitate movement between latent spaces. Finally, we show that transfer neurons are critical for reasoning in multilingual LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†è§£ç å™¨æ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†å¤šè¯­è¨€è¾“å…¥çš„å†…åœ¨æœºåˆ¶ã€‚é’ˆå¯¹æ¨¡å‹åœ¨ä¸åŒè¯­è¨€æ½œç©ºé—´ï¼ˆlatent spacesï¼‰ä¹‹é—´è½¬æ¢çš„åŠ¨æ€è¿‡ç¨‹ï¼Œä½œè€…æå‡ºäº†è½¬ç§»ç¥ç»å…ƒå‡è®¾ï¼ˆThe Transfer Neurons Hypothesisï¼‰ï¼Œè®¤ä¸º MLP æ¨¡å—ä¸­çš„ç‰¹å®šç¥ç»å…ƒè´Ÿè´£åœ¨è¯­è¨€ç‰¹å®šæ½œç©ºé—´ä¸å…±äº«è¯­ä¹‰æ½œç©ºé—´ä¹‹é—´è½¬ç§»è¡¨ç¤ºã€‚å®éªŒé€šè¿‡å®è¯åˆ†æéªŒè¯äº†è¿™ä¸€å‡è®¾ï¼Œå¹¶æŒ‡å‡ºè¿‘æœŸç ”ç©¶å‘ç°çš„ç‰¹å®šè¯­è¨€ç¥ç»å…ƒçš„ä¸€ä¸ªæ ¸å¿ƒåŠŸèƒ½æ­£æ˜¯ä¿ƒè¿›è¿™ç§æ½œç©ºé—´ä¹‹é—´çš„ç§»åŠ¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†è½¬ç§»ç¥ç»å…ƒï¼ˆtransfer neuronsï¼‰å¯¹äºå¤šè¯­è¨€ LLMs çš„æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚è¯¥å‘ç°ä¸ºç†è§£å¤šè¯­è¨€æ¨¡å‹å¦‚ä½•å®ç°è·¨è¯­è¨€è¡¨å¾è½¬æ¢æä¾›äº†å…³é”®çš„ç†è®ºæ”¯æŒä¸åº•å±‚æœºåˆ¶è§£æã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "57 pages, 47 figures and 41 tables; Accepted to EMNLP 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2509.17030v1",
      "published_date": "2025-09-21 10:44:16 UTC",
      "updated_date": "2025-09-21 10:44:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:10.175309+00:00"
    },
    {
      "arxiv_id": "2509.17024v1",
      "title": "When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration",
      "title_zh": "å½“è‰²å½©ç©ºé—´è§£è€¦é‡è§æ‰©æ•£æ¨¡å‹ï¼šæ¶åŠ£å¤©æ°”ä¸‹çš„å›¾åƒæ¢å¤",
      "authors": [
        "Wenxuan Fang",
        "Jili Fan",
        "Chao Wang",
        "Xiantao Hu",
        "Jiangwei Weng",
        "Ying Tai",
        "Jian Yang",
        "Jun Li"
      ],
      "abstract": "Adverse Weather Image Restoration (AWIR) is a highly challenging task due to the unpredictable and dynamic nature of weather-related degradations. Traditional task-specific methods often fail to generalize to unseen or complex degradation types, while recent prompt-learning approaches depend heavily on the degradation estimation capabilities of vision-language models, resulting in inconsistent restorations. In this paper, we propose \\textbf{LCDiff}, a novel framework comprising two key components: \\textit{Lumina-Chroma Decomposition Network} (LCDN) and \\textit{Lumina-Guided Diffusion Model} (LGDM). LCDN processes degraded images in the YCbCr color space, separately handling degradation-related luminance and degradation-invariant chrominance components. This decomposition effectively mitigates weather-induced degradation while preserving color fidelity. To further enhance restoration quality, LGDM leverages degradation-related luminance information as a guiding condition, eliminating the need for explicit degradation prompts. Additionally, LGDM incorporates a \\textit{Dynamic Time Step Loss} to optimize the denoising network, ensuring a balanced recovery of both low- and high-frequency features in the image. Finally, we present DriveWeather, a comprehensive all-weather driving dataset designed to enable robust evaluation. Extensive experiments demonstrate that our approach surpasses state-of-the-art methods, setting a new benchmark in AWIR. The dataset and code are available at: https://github.com/fiwy0527/LCDiff.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¶åŠ£å¤©æ°”å›¾åƒæ¢å¤ (Adverse Weather Image Restoration, AWIR) ä¸­å­˜åœ¨çš„é€€åŒ–ä¸å¯é¢„æµ‹åŠç°æœ‰æç¤ºå­¦ä¹ æ–¹æ³•æ³›åŒ–æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º LCDiff çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±äº®åº¦-è‰²åº¦åˆ†è§£ç½‘ç»œ (Lumina-Chroma Decomposition Network, LCDN) å’Œäº®åº¦å¼•å¯¼æ‰©æ•£æ¨¡å‹ (Lumina-Guided Diffusion Model, LGDM) ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆã€‚LCDN åœ¨ YCbCr é¢œè‰²ç©ºé—´ä¸­è¿è¡Œï¼Œé€šè¿‡è§£è€¦å¤„ç†å—æŸçš„äº®åº¦åˆ†é‡å’Œç¨³å®šçš„è‰²åº¦åˆ†é‡ï¼Œåœ¨æœ‰æ•ˆæŠ‘åˆ¶å¤©æ°”é€€åŒ–çš„åŒæ—¶ç¡®ä¿äº†é¢œè‰²ä¿çœŸåº¦ã€‚LGDM å·§å¦™åœ°å°†äº®åº¦ä¿¡æ¯ä½œä¸ºå¼•å¯¼æ¡ä»¶ï¼Œæ¶ˆé™¤äº†å¯¹æ˜¾å¼é€€åŒ–æç¤ºçš„éœ€æ±‚ï¼Œå¹¶å¼•å…¥åŠ¨æ€æ—¶é—´æ­¥æŸå¤± (Dynamic Time Step Loss) ä»¥å¹³è¡¡å›¾åƒé«˜ã€ä½é¢‘ç‰¹å¾çš„ä¿®å¤è´¨é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†å…¨å¤©æ°”é©¾é©¶æ•°æ®é›† DriveWeatherï¼Œä¸ºè¯¥é¢†åŸŸçš„é²æ£’æ€§è¯„ä¼°æä¾›äº†é‡è¦æ”¯æ’‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLCDiff åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºå½“å‰çš„ SOTA æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚æ°”è±¡æ¡ä»¶ä¸‹çš„å›¾åƒæ¢å¤æ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17024v1",
      "published_date": "2025-09-21 10:39:06 UTC",
      "updated_date": "2025-09-21 10:39:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:19.399981+00:00"
    },
    {
      "arxiv_id": "2509.17000v1",
      "title": "Adaptive Overclocking: Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals",
      "title_zh": "è‡ªé€‚åº”è¶…é¢‘ï¼šåŸºäºå®æ—¶æ¨ç†ä¿¡å·çš„æ€ç»´è·¯å¾„é•¿åº¦åŠ¨æ€æ§åˆ¶",
      "authors": [
        "Shuhao Jiang",
        "Songbo Wang",
        "Yang Qiao",
        "Chun Xu",
        "Chaoyang Zheng",
        "Shengyi Zhou",
        "Huanjun Wang",
        "Fangming Li",
        "Cong Zhang",
        "Jiyu Wang"
      ],
      "abstract": "Large Reasoning Models (LRMs) often suffer from computational inefficiency due to overthinking, where a fixed reasoning budget fails to match the varying complexity of tasks. To address this issue, we propose Adaptive Overclocking, a method that makes the overclocking hyperparameter $Î±$ dynamic and context-aware. Our method adjusts reasoning speed in real time through two complementary signals: (1) token-level model uncertainty for fine-grained step-wise control, and (2) input complexity estimation for informed initialization. We implement this approach with three strategies: Uncertainty-Aware Alpha Scheduling (UA-$Î±$S), Complexity-Guided Alpha Initialization (CG-$Î±$I), and a Hybrid Adaptive Control (HAC) that combines both. Experiments on GSM8K, MATH, and SVAMP show that HAC achieves superior accuracy-latency trade-offs, reducing unnecessary computation on simple problems while allocating more resources to challenging ones. By mitigating overthinking, Adaptive Overclocking enhances both efficiency and overall reasoning performance.",
      "tldr_zh": "å¤§è¯­è¨€æ¨ç†æ¨¡å‹(Large Reasoning Models)å¸¸å› å›ºå®šçš„æ¨ç†é¢„ç®—ä¸ä»»åŠ¡å¤æ‚åº¦çš„ä¸åŒ¹é…è€Œé¢ä¸´è®¡ç®—æ•ˆç‡ä½ä¸‹çš„â€œè¿‡åº¦æ€è€ƒ(overthinking)â€é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†Adaptive Overclockingæ–¹æ³•ï¼Œé€šè¿‡å®æ—¶æ¨ç†ä¿¡å·åŠ¨æ€ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥åœ°æ§åˆ¶è¶…å‚æ•° $\\alpha$ ä»¥è°ƒèŠ‚æ¨ç†è·¯å¾„é•¿åº¦ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ ‡è®°çº§æ¨¡å‹ä¸ç¡®å®šæ€§(token-level model uncertainty)è¿›è¡Œç»†ç²’åº¦æ­¥éª¤æ§åˆ¶ï¼Œä»¥åŠè¾“å…¥å¤æ‚åº¦ä¼°ç®—(input complexity estimation)è¿›è¡ŒçŸ¥æƒ…åˆå§‹åŒ–ã€‚ç ”ç©¶å®ç°äº†UA-$\\alpha$Sã€CG-$\\alpha$Iä»¥åŠç»“åˆä¸¤è€…çš„Hybrid Adaptive Control (HAC)ä¸‰ç§ç­–ç•¥ã€‚åœ¨GSM8Kã€MATHå’ŒSVAMPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHACå®ç°äº†ä¼˜å¼‚çš„å‡†ç¡®ç‡ä¸å»¶è¿Ÿæƒè¡¡(accuracy-latency trade-offs)ã€‚é€šè¿‡ç¼“è§£è¿‡åº¦æ€è€ƒï¼ŒAdaptive Overclockingåœ¨å‡å°‘ç®€å•é—®é¢˜å†—ä½™è®¡ç®—çš„åŒæ—¶ä¸ºå¤æ‚ä»»åŠ¡åˆ†é…æ›´å¤šèµ„æºï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ•´ä½“æ¨ç†æ€§èƒ½ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.17000v1",
      "published_date": "2025-09-21 09:40:27 UTC",
      "updated_date": "2025-09-21 09:40:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:23.787988+00:00"
    },
    {
      "arxiv_id": "2509.16990v1",
      "title": "Advancing Speech Understanding in Speech-Aware Language Models with GRPO",
      "title_zh": "åˆ©ç”¨ GRPO æå‡è¯­éŸ³æ„ŸçŸ¥è¯­è¨€æ¨¡å‹çš„è¯­éŸ³ç†è§£èƒ½åŠ›",
      "authors": [
        "Avishai Elmakies",
        "Hagai Aronowitz",
        "Nimrod Shabtay",
        "Eli Schwartz",
        "Ron Hoory",
        "Avihu Dekel"
      ],
      "abstract": "In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based method for training Speech-Aware Large Language Models (SALLMs) on open-format speech understanding tasks, such as Spoken Question Answering and Automatic Speech Translation. SALLMs have proven highly effective for speech understanding tasks. GRPO has recently gained traction for its efficiency in training LLMs, and prior work has explored its application to SALLMs, primarily in multiple-choice tasks. Building on this, we focus on open-format tasks that better reflect the generative abilities of the models. Our approach leverages GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate empirically that it surpasses standard SFT across several key metrics. Finally, we explore the potential of incorporating off-policy samples within GRPO for these tasks, highlighting avenues for further improvement and further research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group Relative Policy Optimization, GRPO)çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡è¯­éŸ³æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹(Speech-Aware Large Language Models, SALLMs)åœ¨è¯­éŸ³é—®ç­”(Spoken Question Answering)å’Œè‡ªåŠ¨è¯­éŸ³ç¿»è¯‘(Automatic Speech Translation)ç­‰å¼€æ”¾æ ¼å¼è¯­éŸ³ç†è§£ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸åŒäºä»¥å¾€ä¸»è¦é›†ä¸­åœ¨å¤šé€‰é¢˜ä»»åŠ¡çš„ç ”ç©¶ï¼Œè¯¥å·¥ä½œé‡ç‚¹å…³æ³¨èƒ½å¤Ÿä½“ç°æ¨¡å‹ç”Ÿæˆèƒ½åŠ›çš„å¼€æ”¾å¼ä»»åŠ¡ã€‚é€šè¿‡å°†BLEUä½œä¸ºå¥–åŠ±ä¿¡å·(reward signal)å¹¶åº”ç”¨GRPOï¼Œç ”ç©¶æˆåŠŸå®ç°äº†å¯¹SALLMsçš„ä¼˜åŒ–ï¼Œå®è¯ç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šä¸ªå…³é”®æŒ‡æ ‡ä¸Šå‡ä¼˜äºæ ‡å‡†çš„æœ‰ç›‘ç£å¾®è°ƒ(SFT)ã€‚æœ€åï¼Œç ”ç©¶è¿˜æ¢è®¨äº†åœ¨GRPOä¸­å¼•å…¥ç¦»çº¿ç­–ç•¥æ ·æœ¬(off-policy samples)çš„æ½œåŠ›ï¼Œä¸ºè¯¥é¢†åŸŸçš„åç»­æ”¹è¿›å’Œæ·±å…¥æ¢ç´¢æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16990v1",
      "published_date": "2025-09-21 09:09:36 UTC",
      "updated_date": "2025-09-21 09:09:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:29.287456+00:00"
    },
    {
      "arxiv_id": "2509.16989v3",
      "title": "PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models",
      "title_zh": "PTQTPï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„ä¸‰å€¼å¹³é¢è®­ç»ƒåé‡åŒ–",
      "authors": [
        "He Xiao",
        "Runming Yang",
        "Qingyao Yang",
        "Wendong Xu",
        "Zhen Li",
        "Yupeng Su",
        "Zhengwu Liu",
        "Hongxia Yang",
        "Ngai Wong"
      ],
      "abstract": "Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and representational capacity. While existing ultra-low-bit methods rely on binary approximations or quantization-aware training(QAT), they often suffer from either limited representational capacity or huge training resource overhead. We introduce PTQ to Trit-Planes (PTQTP), a structured PTQ framework that decomposes weight matrices into dual ternary {-1, 0, 1} trit-planes. This approach achieves multiplication-free additive inference by decoupling weights into discrete topology (trit-planes) and continuous magnitude (scales), effectively enabling high-fidelity sparse approximation. PTQTP provides: (1) a theoretically grounded progressive approximation algorithm ensuring global weight consistency; (2) model-agnostic deployment without architectural modifications; and (3) uniform ternary operations that eliminate mixed-precision overhead. Comprehensive experiments on LLaMA3.x and Qwen3 (0.6B-70B) demonstrate that PTQTP significantly outperforms sub-4bit PTQ methods on both language reasoning tasks and mathematical reasoning as well as coding. PTQTP rivals the 1.58-bit QAT performance while requiring only single-hour quantization compared to 10-14 GPU days for training-based methods, and the end-to-end inference speed achieves 4.63$\\times$ faster than the FP16 baseline model, establishing a new and practical solution for efficient LLM deployment in resource-constrained environments. Code will available at https://github.com/HeXiao-55/PTQTP.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PTQTPï¼Œä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„ç»“æ„åŒ–è®­ç»ƒåé‡åŒ–(Post-training quantization, PTQ)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æä½æ¯”ç‰¹ä½ä¸‹è®¡ç®—æ•ˆç‡ä¸è¡¨è¾¾èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æƒé‡çŸ©é˜µåˆ†è§£ä¸ºåŒä¸‰å…ƒ{-1, 0, 1} Trit-Planesï¼Œå°†æƒé‡è§£è€¦ä¸ºç¦»æ•£æ‹“æ‰‘ä¸è¿ç»­å¹…å€¼ï¼Œä»è€Œå®ç°äº†æ— éœ€ä¹˜æ³•çš„åŠ æ³•æ¨ç†(multiplication-free additive inference)å’Œé«˜ä¿çœŸç¨€ç–é€¼è¿‘ã€‚PTQTPé‡‡ç”¨ä¸€ç§æ¸è¿›é€¼è¿‘ç®—æ³•ç¡®ä¿å…¨å±€æƒé‡ä¸€è‡´æ€§ï¼Œä¸”æ”¯æŒæ— éœ€ä¿®æ”¹æ¶æ„çš„æ¨¡å‹æ— å…³éƒ¨ç½²(model-agnostic deployment)ï¼Œæ¶ˆé™¤äº†æ··åˆç²¾åº¦å¼€é”€ã€‚åœ¨LLaMA3.xå’ŒQwen3ç³»åˆ—æ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPTQTPåœ¨è¯­è¨€æ¨ç†ã€æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºsub-4bit PTQæ–¹æ³•ã€‚ç›¸æ¯”äºéœ€è¦æ•°å¤©è®­ç»ƒæ—¶é—´çš„1.58-bit QATæ–¹æ³•ï¼ŒPTQTPä»…éœ€å•å°æ—¶é‡åŒ–å³å¯è¾¾åˆ°åŒç­‰æ€§èƒ½ï¼Œä¸”ç«¯åˆ°ç«¯æ¨ç†é€Ÿåº¦æå‡è‡³FP16åŸºå‡†çš„4.63å€ã€‚è¿™é¡¹å·¥ä½œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„LLMé«˜æ•ˆéƒ¨ç½²æä¾›äº†ä¸€ç§æå…·ç«äº‰åŠ›çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Ternary Quantization, Under review",
      "pdf_url": "https://arxiv.org/pdf/2509.16989v3",
      "published_date": "2025-09-21 09:07:20 UTC",
      "updated_date": "2026-01-01 07:10:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:52.084667+00:00"
    },
    {
      "arxiv_id": "2510.01219v2",
      "title": "Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset",
      "title_zh": "åˆ©ç”¨æ¦‚å¿µå­¦ä¹ æ•°æ®é›†æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹ä¸­çš„éšæ€§åè§",
      "authors": [
        "Leroy Z. Wang"
      ],
      "abstract": "We introduce a dataset of concept learning tasks that helps uncover implicit biases in large language models. Using in-context concept learning experiments, we found that language models may have a bias toward upward monotonicity in quantifiers; such bias is less apparent when the model is tested by direct prompting without concept learning components. This demonstrates that in-context concept learning can be an effective way to discover hidden biases in language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºæ¦‚å¿µå­¦ä¹ (concept learning)ä»»åŠ¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­å­˜åœ¨çš„éšæ€§åè§ã€‚é€šè¿‡ä¸Šä¸‹æ–‡æ¦‚å¿µå­¦ä¹ (in-context concept learning)å®éªŒï¼Œç ”ç©¶å‘ç°è¯­è¨€æ¨¡å‹åœ¨é‡è¯å¤„ç†ä¸Šè¡¨ç°å‡ºå¯¹å‘ä¸Šå•è°ƒæ€§(upward monotonicity)çš„åå‘ã€‚è¿™ç§åè§åœ¨ç¼ºä¹æ¦‚å¿µå­¦ä¹ ç»„ä»¶çš„ç›´æ¥æç¤º(direct prompting)æµ‹è¯•ä¸­è¾ƒéš¾è¢«å¯Ÿè§‰ï¼Œä½†åœ¨ç‰¹å®šä»»åŠ¡ä¸­å´æ˜¾éœ²æ— é—ã€‚è¯¥å‘ç°è¯æ˜äº†ä¸Šä¸‹æ–‡æ¦‚å¿µå­¦ä¹ æ˜¯æŒ–æ˜æ¨¡å‹æ·±å±‚éšè—åè§çš„æœ‰æ•ˆæ‰‹æ®µã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£å’Œè¯„ä¼°å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„è®¤çŸ¥åå¥½æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Presented at EurIPS 2025 Workshop - Unifying Perspectives on Learning Biases (UPLB) https://sites.google.com/view/towards-a-unified-view",
      "pdf_url": "https://arxiv.org/pdf/2510.01219v2",
      "published_date": "2025-09-21 09:04:31 UTC",
      "updated_date": "2025-11-26 06:42:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:07:53.988075+00:00"
    },
    {
      "arxiv_id": "2509.16979v1",
      "title": "Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility Prediction for Hearing-Impaired Listeners",
      "title_zh": "åˆ©ç”¨å¤šç§è¯­éŸ³å¢å¼ºå™¨å®ç°é’ˆå¯¹å¬åŠ›å—æŸè€…çš„éä¾µå…¥å¼è¨€è¯­å¯æ‡‚åº¦é¢„æµ‹",
      "authors": [
        "Boxuan Cao",
        "Linkai Li",
        "Hanlin Yu",
        "Changgeng Mo",
        "Haoshuai Zhou",
        "Shan Xiang Wang"
      ],
      "abstract": "Speech intelligibility evaluation for hearing-impaired (HI) listeners is essential for assessing hearing aid performance, traditionally relying on listening tests or intrusive methods like HASPI. However, these methods require clean reference signals, which are often unavailable in real-world conditions, creating a gap between lab-based and real-world assessments. To address this, we propose a non-intrusive intelligibility prediction framework that leverages speech enhancers to provide a parallel enhanced-signal pathway, enabling robust predictions without reference signals. We evaluate three state-of-the-art enhancers and demonstrate that prediction performance depends on the choice of enhancer, with ensembles of strong enhancers yielding the best results. To improve cross-dataset generalization, we introduce a 2-clips augmentation strategy that enhances listener-specific variability, boosting robustness on unseen datasets. Our approach consistently outperforms the non-intrusive baseline, CPC2 Champion across multiple datasets, highlighting the potential of enhancer-guided non-intrusive intelligibility prediction for real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å¬åŠ›å—æŸ(HI)å¬ä¼—çš„éä¾µå…¥å¼è¯­éŸ³å¯æ‡‚åº¦é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿä¾µå…¥å¼æ–¹æ³•ï¼ˆå¦‚HASPIï¼‰åœ¨çœŸå®ç¯å¢ƒä¸‹å› ç¼ºä¹çº¯å‡€å‚è€ƒä¿¡å·è€Œéš¾ä»¥åº”ç”¨çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è¯­éŸ³å¢å¼ºå™¨(speech enhancers)æä¾›å¹¶è¡Œçš„å¢å¼ºä¿¡å·è·¯å¾„ï¼Œå®ç°æ— éœ€å‚è€ƒä¿¡å·çš„é²æ£’é¢„æµ‹ï¼Œå¹¶è¯æ˜äº†å¼ºå¢å¼ºå™¨çš„é›†æˆ(ensembles)èƒ½äº§ç”Ÿæœ€ä½³çš„é¢„æµ‹æ•ˆæœã€‚ä¸ºäº†æé«˜è·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ï¼Œç ”ç©¶å¼•å…¥äº†åŒç‰‡æ®µå¢å¼º(2-clips augmentation)ç­–ç•¥ä»¥æ•æ‰å¬ä¼—ç‰¹æœ‰çš„å˜å¼‚æ€§å¹¶å¢å¼ºé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¸€è‡´ä¼˜äºéä¾µå…¥å¼åŸºå‡†æ¨¡å‹CPC2 Championï¼Œè¯æ˜äº†å¢å¼ºå™¨å¼•å¯¼çš„éä¾µå…¥å¼é¢„æµ‹æ–¹æ¡ˆåœ¨è¯„ä¼°ç°å®ä¸–ç•ŒåŠ©å¬è®¾å¤‡æ€§èƒ½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16979v1",
      "published_date": "2025-09-21 08:29:24 UTC",
      "updated_date": "2025-09-21 08:29:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:00.087176+00:00"
    },
    {
      "arxiv_id": "2509.16972v2",
      "title": "The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA",
      "title_zh": "ç¬¬ä¸ƒå±Š LSVOS æŒ‘æˆ˜èµ› RVOS èµ›é“å† å†›æ–¹æ¡ˆï¼šSaSaSa2VA",
      "authors": [
        "Quanzhu Niu",
        "Dengxian Gong",
        "Shihao Chen",
        "Tao Zhang",
        "Yikang Zhou",
        "Haobo Yuan",
        "Lu Qi",
        "Xiangtai Li",
        "Shunping Ji"
      ],
      "abstract": "Referring video object segmentation (RVOS) requires segmenting and tracking objects in videos conditioned on natural-language expressions, demanding fine-grained understanding of both appearance and motion. Building on Sa2VA, which couples a Multi-modal Large Language Model (MLLM) with the video segmentation model SAM2, we identify two key bottlenecks that limit segmentation performance: sparse frame sampling and reliance on a single [SEG] token for an entire video. We propose Segmentation Augmented and Selective Averaged Sa2VA (SaSaSa2VA) to address these issues. On the 7th LSVOS Challenge (RVOS track), SaSaSa2VA achieves a $\\mathcal{J\\&F}$ of 67.45, ranking first and surpassing the runner-up by 2.80 points. This result and ablation studies demonstrate that efficient segmentation augmentation and test-time ensembling substantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA repository: https://github.com/bytedance/Sa2VA.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº†åœ¨ç¬¬7å±ŠLSVOSæŒ‘æˆ˜èµ›RVOSèµ›é“ä¸­è·å¾—å† å†›çš„è§£å†³æ–¹æ¡ˆSaSaSa2VAï¼Œè¯¥æ–¹æ¡ˆé’ˆå¯¹å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆReferring Video Object Segmentationï¼‰ä»»åŠ¡è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ã€‚ç ”ç©¶äººå‘˜åœ¨åˆ†æåŸºå‡†æ¨¡å‹Sa2VAï¼ˆç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹MLLMä¸è§†é¢‘åˆ†å‰²æ¨¡å‹SAM2ï¼‰æ—¶ï¼Œè¯†åˆ«å‡ºç¨€ç–å¸§é‡‡æ ·å’Œå•[SEG]æ ‡è®°ä¾èµ–è¿™ä¸¤ä¸ªé™åˆ¶æ€§èƒ½çš„å…³é”®ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†SaSaSa2VAï¼Œé€šè¿‡å¼•å…¥åˆ†å‰²å¢å¼ºï¼ˆSegmentation Augmentedï¼‰å’Œé€‰æ‹©æ€§å¹³å‡ï¼ˆSelective Averagedï¼‰æœºåˆ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆé«˜æ•ˆçš„åˆ†å‰²å¢å¼ºä¸æµ‹è¯•æ—¶é›†æˆï¼ˆtest-time ensemblingï¼‰ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¥åœ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆgrounded MLLMsï¼‰çš„åˆ†å‰²æ€§èƒ½ã€‚åœ¨7th LSVOS Challengeä¸­ï¼ŒSaSaSa2VAä»¥67.45çš„$\\mathcal{J\\&F}$å¾—åˆ†æ’åç¬¬ä¸€ï¼Œè¶…è¶Šç¬¬äºŒå2.80åˆ†ã€‚å®éªŒç»“æœå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆä¸ºå¤æ‚è§†é¢‘åœºæ™¯ä¸‹çš„å¯¹è±¡åˆ†å‰²ä¸è¿½è¸ªæä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The 1st place report of 7th LSVOS challenge RVOS track in ICCV 2025. The code is released in Sa2VA repository: https://github.com/bytedance/Sa2VA",
      "pdf_url": "https://arxiv.org/pdf/2509.16972v2",
      "published_date": "2025-09-21 08:08:17 UTC",
      "updated_date": "2025-10-20 04:36:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:07.299524+00:00"
    },
    {
      "arxiv_id": "2509.16959v4",
      "title": "Graph Coloring for Multi-Task Learning",
      "title_zh": "é¢å‘å¤šä»»åŠ¡å­¦ä¹ çš„å›¾ç€è‰²",
      "authors": [
        "Santosh Patapati"
      ],
      "abstract": "When different objectives conflict with each other in multi-task learning, gradients begin to interfere and slow convergence, thereby potentially reducing the final model's performance. To address this, we introduce SON-GOKU, a scheduler that computes gradient interference, constructs an interference graph, and then applies greedy graph-coloring to partition tasks into groups that align well with each other. At each training step, only one group (color class) of tasks are activated, and the grouping partition is constantly recomputed as task relationships evolve throughout training. By ensuring that each mini-batch contains only tasks that pull the model in the same direction, our method improves the effectiveness of any underlying multi-task learning optimizer without additional tuning. Since tasks within these groups will update in compatible directions, multi-task learning will improve model performance rather than impede it. Empirical results on six different datasets show that this interference-aware graph-coloring approach consistently outperforms baselines and state-of-the-art multi-task optimizers. We provide extensive theory showing why grouping and sequential updates improve multi-task learning, with guarantees on descent, convergence, and accurately identifying what tasks conflict or align.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SON-GOKUï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å¤šä»»åŠ¡å­¦ä¹  (Multi-Task Learning) ä¸­æ¢¯åº¦å¹²æ‰° (Gradient Interference) é—®é¢˜çš„è°ƒåº¦ç¨‹åºã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—ä»»åŠ¡é—´çš„æ¢¯åº¦å¹²æ‰°å¹¶æ„å»ºå¹²æ‰°å›¾ (Interference Graph)ï¼Œåˆ©ç”¨è´ªå¿ƒå›¾ç€è‰²ç®—æ³• (Greedy Graph-Coloring) å°†ä»»åŠ¡åˆ’åˆ†ä¸ºç›¸äº’å…¼å®¹çš„ç»„ã€‚åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œä»…æ¿€æ´»å…¶ä¸­ä¸€ä¸ªé¢œè‰²ç±»åˆ«çš„ä»»åŠ¡ç»„ï¼Œå¹¶éšç€è®­ç»ƒè¿‡ç¨‹ä¸­ä»»åŠ¡å…³ç³»çš„æ¼”å˜åŠ¨æ€æ›´æ–°åˆ†ç»„ã€‚è¿™ç§ç­–ç•¥ç¡®ä¿äº†æ¯ä¸ªå°æ‰¹é‡ (Mini-batch) ä¸­çš„ä»»åŠ¡å…·æœ‰ä¸€è‡´çš„æ¢¯åº¦æ–¹å‘ï¼Œä»è€Œåœ¨æ— éœ€é¢å¤–è°ƒä¼˜çš„æƒ…å†µä¸‹æå‡äº†åº•å±‚ä¼˜åŒ–å™¨çš„æ€§èƒ½ã€‚å®éªŒåœ¨å…­ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶ä¸€è‡´ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›å¤šä»»åŠ¡ä¼˜åŒ–å™¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†å…³äºæ”¶æ•›æ€§ (Convergence) å’Œä»»åŠ¡å†²çªè¯†åˆ«çš„è¯¦å°½ç†è®ºä¿è¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented at CVPRW 2025; Under review",
      "pdf_url": "https://arxiv.org/pdf/2509.16959v4",
      "published_date": "2025-09-21 07:45:53 UTC",
      "updated_date": "2025-12-09 10:04:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:14.292718+00:00"
    },
    {
      "arxiv_id": "2509.16958v2",
      "title": "Quantum Abduction: A New Paradigm for Reasoning under Uncertainty",
      "title_zh": "é‡å­æº¯å› ï¼šä¸ç¡®å®šæ€§æ¨ç†çš„æ–°èŒƒå¼",
      "authors": [
        "Remo Pareschi"
      ],
      "abstract": "Abductive reasoning - the search for plausible explanations - has long been central to human inquiry, from forensics to medicine and scientific discovery. Yet formal approaches in AI have largely reduced abduction to eliminative search: hypotheses are treated as mutually exclusive, evaluated against consistency constraints or probability updates, and pruned until a single \"best\" explanation remains. This reductionist framing overlooks the way human reasoners sustain multiple explanatory lines in suspension, navigate contradictions, and generate novel syntheses. This paper introduces quantum abduction, a non-classical paradigm that models hypotheses in superposition, allows them to interfere constructively or destructively, and collapses only when coherence with evidence is reached. Grounded in quantum cognition and implemented with modern NLP embeddings and generative AI, the framework supports dynamic synthesis rather than premature elimination. Case studies span historical mysteries (Ludwig II of Bavaria, the \"Monster of Florence\"), literary demonstrations (\"Murder on the Orient Express\"), medical diagnosis, and scientific theory change. Across these domains, quantum abduction proves more faithful to the constructive and multifaceted nature of human reasoning, while offering a pathway toward expressive and transparent AI reasoning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Quantum Abductionï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¸ç¡®å®šæ€§æ¨ç†ï¼ˆReasoning under Uncertaintyï¼‰çš„æ–°å‹éç»å…¸èŒƒå¼ã€‚é’ˆå¯¹ä¼ ç»Ÿäººå·¥æ™ºèƒ½å°†æº¯å› æ¨ç†ï¼ˆAbductionï¼‰ç®€åŒ–ä¸ºäº’æ–¥å‡è®¾æ¶ˆé™¤å¼æœç´¢çš„å±€é™æ€§ï¼Œè¯¥æ¡†æ¶å€Ÿé‰´äº†é‡å­è®¤çŸ¥ï¼ˆQuantum Cognitionï¼‰ç†è®ºï¼Œå°†å¤šä¸ªå‡è®¾å»ºæ¨¡ä¸ºå åŠ ï¼ˆSuperpositionï¼‰çŠ¶æ€ã€‚è¯¥èŒƒå¼å…è®¸ä¸åŒå‡è®¾ä¹‹é—´äº§ç”Ÿç›¸é•¿æˆ–ç›¸æ¶ˆçš„å¹²æ¶‰ï¼ˆInterferenceï¼‰ï¼Œä¸”ä»…åœ¨ä¸è¯æ®è¾¾æˆä¸€è‡´æ—¶æ‰å‘ç”Ÿåç¼©ï¼ˆCollapseï¼‰ï¼Œä»è€Œæ”¯æŒä¿¡æ¯çš„åŠ¨æ€åˆæˆè€Œéè¿‡æ—©å‰”é™¤ã€‚ç ”ç©¶åˆ©ç”¨ç°ä»£ NLP åµŒå…¥å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenerative AIï¼‰æŠ€æœ¯å®ç°äº†è¿™ä¸€æ¡†æ¶ï¼Œå¹¶å°†å…¶åº”ç”¨äºå†å²è°œé¢˜ã€æ–‡å­¦åˆ†æã€åŒ»å­¦è¯Šæ–­åŠç§‘å­¦ç†è®ºæ›´æ›¿ç­‰å¤šä¸ªé¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuantum Abduction èƒ½å¤Ÿæ›´çœŸå®åœ°è¿˜åŸäººç±»æ€ç»´çš„æ„é€ æ€§ä¸å¤šé¢æ€§ï¼Œä¸ºå¼€å‘æ›´å…·è¡¨è¾¾åŠ›å’Œé€æ˜åº¦çš„ AI æ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 8 figures, 3 tables; submitted to Sci, MDPI",
      "pdf_url": "https://arxiv.org/pdf/2509.16958v2",
      "published_date": "2025-09-21 07:41:49 UTC",
      "updated_date": "2025-12-21 12:58:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:24.289780+00:00"
    },
    {
      "arxiv_id": "2509.16952v1",
      "title": "AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation",
      "title_zh": "AirQAï¼šé¢å‘äººå·¥æ™ºèƒ½ç ”ç©¶çš„å¸¦å®ä¾‹çº§è¯„ä¼°çš„ç»¼åˆæ€§é—®ç­”æ•°æ®é›†",
      "authors": [
        "Tiancheng Huang",
        "Ruisheng Cao",
        "Yuxin Zhang",
        "Zhangyi Kang",
        "Zijian Wang",
        "Chenrun Wang",
        "Yijie Luo",
        "Hang Zheng",
        "Lirong Qian",
        "Lu Chen",
        "Kai Yu"
      ],
      "abstract": "The growing volume of academic papers has made it increasingly difficult for researchers to efficiently extract key information. While large language models (LLMs) based agents are capable of automating question answering (QA) workflows for scientific papers, there still lacks a comprehensive and realistic benchmark to evaluate their capabilities. Moreover, training an interactive agent for this specific task is hindered by the shortage of high-quality interaction trajectories. In this work, we propose AirQA, a human-annotated comprehensive paper QA dataset in the field of artificial intelligence (AI), with 13,948 papers and 1,246 questions, that encompasses multi-task, multi-modal and instance-level evaluation. Furthermore, we propose ExTrActor, an automated framework for instruction data synthesis. With three LLM-based agents, ExTrActor can perform example generation and trajectory collection without human intervention. Evaluations of multiple open-source and proprietary models show that most models underperform on AirQA, demonstrating the quality of our dataset. Extensive experiments confirm that ExTrActor consistently improves the multi-turn tool-use capability of small models, enabling them to achieve performance comparable to larger ones.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AirQAï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹äººå·¥æ™ºèƒ½(AI)é¢†åŸŸçš„ç»¼åˆæ€§å­¦æœ¯è®ºæ–‡é—®ç­”æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç§‘ç ”äººå‘˜éš¾ä»¥ä»æµ·é‡æ–‡çŒ®ä¸­é«˜æ•ˆæå–å…³é”®ä¿¡æ¯çš„é—®é¢˜ã€‚AirQA åŒ…å« 13,948 ç¯‡è®ºæ–‡å’Œ 1,246 ä¸ªé—®é¢˜ï¼Œæ¶µç›–äº†å¤šä»»åŠ¡ã€å¤šæ¨¡æ€ä»¥åŠå®ä¾‹çº§(instance-level)çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº† ExTrActor æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ™ºèƒ½ä½“è‡ªåŠ¨ç”ŸæˆæŒ‡ä»¤æ•°æ®å’Œäº¤äº’è½¨è¿¹ï¼Œå®ç°äº†æ— éœ€äººå·¥å¹²é¢„çš„æ•°æ®åˆæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›®å‰ä¸»æµçš„å¼€æºå’Œé—­æºæ¨¡å‹åœ¨ AirQA ä¸Šçš„è¡¨ç°å‡ä¸ç†æƒ³ï¼Œè¿™éªŒè¯äº†è¯¥æ•°æ®é›†çš„é«˜è´¨é‡ä¸æŒ‘æˆ˜æ€§ã€‚åŒæ—¶ï¼Œå®éªŒè¯æ˜ ExTrActor èƒ½æ˜¾è‘—æå‡å°æ¨¡å‹çš„å¤šæ¬¡å¯¹è¯å·¥å…·è°ƒç”¨(multi-turn tool-use)èƒ½åŠ›ï¼Œä½¿å…¶æ€§èƒ½è¾¾åˆ°ä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16952v1",
      "published_date": "2025-09-21 07:24:17 UTC",
      "updated_date": "2025-09-21 07:24:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:25.881453+00:00"
    },
    {
      "arxiv_id": "2509.21359v2",
      "title": "Influence Guided Context Selection for Effective Retrieval-Augmented Generation",
      "title_zh": "å½±å“åŠ›å¼•å¯¼çš„ä¸Šä¸‹æ–‡é€‰æ‹©ï¼šæå‡æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æœ‰æ•ˆæ€§",
      "authors": [
        "Jiale Deng",
        "Yanyan Shen",
        "Ziyuan Pei",
        "Youmin Chen",
        "Linpeng Huang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) addresses large language model (LLM) hallucinations by grounding responses in external knowledge, but its effectiveness is compromised by poor-quality retrieved contexts containing irrelevant or noisy information. While existing approaches attempt to improve performance through context selection based on predefined context quality assessment metrics, they show limited gains over standard RAG. We attribute this limitation to their failure in holistically utilizing available information (query, context list, and generator) for comprehensive quality assessment. Inspired by recent advances in data selection, we reconceptualize context quality assessment as an inference-time data valuation problem and introduce the Contextual Influence Value (CI value). This novel metric quantifies context quality by measuring the performance degradation when removing each context from the list, effectively integrating query-aware relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI value eliminates complex selection hyperparameter tuning by simply retaining contexts with positive CI values. To address practical challenges of label dependency and computational overhead, we develop a parameterized surrogate model for CI value prediction during inference. The model employs a hierarchical architecture that captures both local query-context relevance and global inter-context interactions, trained through oracle CI value supervision and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and multiple LLMs demonstrate that our context selection method significantly outperforms state-of-the-art baselines, effectively filtering poor-quality contexts while preserving critical information. Code is available at https://github.com/SJTU-DMTai/RAG-CSM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) ä¸­å› æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åŒ…å«æ— å…³æˆ–å™ªå£°ä¿¡æ¯è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºå½±å“å¼•å¯¼ä¸Šä¸‹æ–‡é€‰æ‹© (Influence Guided Context Selection) çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶è€…å¼•å…¥äº† Contextual Influence Value (CI value) æŒ‡æ ‡ï¼Œå°†ä¸Šä¸‹æ–‡è´¨é‡è¯„ä¼°è½¬åŒ–ä¸ºæ¨ç†é˜¶æ®µçš„æ•°æ®ä»·å€¼è¯„ä¼°é—®é¢˜ï¼Œé€šè¿‡è¡¡é‡ç§»é™¤ç‰¹å®šä¸Šä¸‹æ–‡åçš„æ€§èƒ½é€€åŒ–æ¥é‡åŒ–å…¶è´¨é‡ã€‚è¯¥æŒ‡æ ‡æœ‰æ•ˆæ•´åˆäº†æŸ¥è¯¢ç›¸å…³æ€§ (query-aware relevance)ã€ä¸Šä¸‹æ–‡åˆ—è¡¨å”¯ä¸€æ€§ (list-aware uniqueness) å’Œç”Ÿæˆå™¨å¯¹é½ (generator-aware alignment)ï¼Œå¹¶èƒ½é€šè¿‡ç®€å•ä¿ç•™ CI å€¼ä¸ºæ­£çš„ä¸Šä¸‹æ–‡æ¥æ¶ˆé™¤å¤æ‚çš„è¶…å‚æ•°è°ƒæ•´ã€‚ä¸ºè§£å†³è®¡ç®—å¼€é”€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªå…·æœ‰åˆ†å±‚æ¶æ„çš„ä»£ç†æ¨¡å‹æ¥é¢„æµ‹ CI valueï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å…¨å±€ä¸Šä¸‹æ–‡äº¤äº’å’Œç«¯åˆ°ç«¯çš„ç”Ÿæˆå™¨åé¦ˆã€‚åœ¨ 8 ä¸ª NLP ä»»åŠ¡å’Œå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ (LLM) ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆè¿‡æ»¤ä½è´¨é‡ä¸Šä¸‹æ–‡çš„åŒæ—¶ä¿ç•™äº†å…³é”®ä¿¡æ¯ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21359v2",
      "published_date": "2025-09-21 07:19:09 UTC",
      "updated_date": "2025-10-24 08:50:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:29.404700+00:00"
    },
    {
      "arxiv_id": "2509.20382v1",
      "title": "Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation",
      "title_zh": "ç”¨äº ECG ç”Ÿç‰©è¯†åˆ«è®¤è¯çš„è½»é‡çº§ MobileNetV1+GRUï¼šè”é‚¦ä¸å¯¹æŠ—æ€§è¯„ä¼°",
      "authors": [
        "Dilli Hang Rai",
        "Sabin Kafley"
      ],
      "abstract": "ECG biometrics offer a unique, secure authentication method, yet their deployment on wearable devices faces real-time processing, privacy, and spoofing vulnerability challenges. This paper proposes a lightweight deep learning model (MobileNetV1+GRU) for ECG-based authentication, injection of 20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and edge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving accuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923, 0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of 0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013, 0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998, while under FGSM adversarial attacks, accuracy drops from 96.82% to as low as 0.80%. This paper highlights federated learning, adversarial testing, and the need for diverse wearable physiological datasets to ensure secure and scalable biometrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§çš„ MobileNetV1+GRU æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¿ƒç”µå›¾(ECG)ç”Ÿç‰©è¯†åˆ«åœ¨å¯ç©¿æˆ´è®¾å¤‡éƒ¨ç½²ä¸­é¢ä¸´çš„å®æ—¶å¤„ç†ã€éšç§ä¿æŠ¤å’Œæ¬ºéª—æ”»å‡»ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆ 20dB é«˜æ–¯å™ªå£°æ³¨å…¥å’Œå®šåˆ¶é¢„å¤„ç†æŠ€æœ¯ï¼Œè¯¥æ–¹æ¡ˆæˆåŠŸæ¨¡æ‹Ÿäº†çœŸå®çš„å¯ç©¿æˆ´ç¯å¢ƒä¸è¾¹ç¼˜éƒ¨ç½²åœºæ™¯ã€‚å®éªŒåœ¨ ECGIDã€MIT-BIHã€CYBHi å’Œ PTB å››ä¸ªæ•°æ®é›†ä¸Šå¼€å±•ï¼Œç»“æœæ˜¾ç¤ºæ¨¡å‹å±•ç°äº†æé«˜çš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨ ECGID æ•°æ®é›†ä¸Šå®ç°äº† 99.34% çš„å‡†ç¡®ç‡å’Œæä½çš„ç­‰é”™è¯¯ç‡(EER)ã€‚ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡ FGSM å¯¹æŠ—æ”»å‡»è¯„ä¼°äº†æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå‘ç°æ”»å‡»ä¼šå¯¼è‡´å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ï¼Œå‡¸æ˜¾äº†é˜²å¾¡æœºåˆ¶çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼ºè°ƒäº†è”é‚¦å­¦ä¹ (Federated Learning)åœ¨å¤„ç†åˆ†å¸ƒå¼éšç§æ•°æ®ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚è¯¥ç ”ç©¶æœ€åæŒ‡å‡ºï¼Œæœªæ¥éœ€è¦æ›´å¤šæ ·åŒ–çš„å¯ç©¿æˆ´ç”Ÿç†æ•°æ®é›†æ¥ç¡®ä¿ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.CR",
      "comment": "5 pages, 7 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.20382v1",
      "published_date": "2025-09-21 06:46:31 UTC",
      "updated_date": "2025-09-21 06:46:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:31.493881+00:00"
    },
    {
      "arxiv_id": "2509.21358v1",
      "title": "MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification",
      "title_zh": "MDF-MLLMï¼šé€šè¿‡è·¨æ¨¡æ€ç‰¹å¾å¯¹é½å®ç°æ·±åº¦èåˆçš„è¯­å¢ƒæ„ŸçŸ¥çœ¼åº•å›¾åƒåˆ†ç±»",
      "authors": [
        "Jason Jordan",
        "Mohammadreza Akbari Lor",
        "Peter Koulen",
        "Mei-Ling Shyu",
        "Shu-Ching Chen"
      ],
      "abstract": "This study aimed to enhance disease classification accuracy from retinal fundus images by integrating fine-grained image features and global textual context using a novel multimodal deep learning architecture. Existing multimodal large language models (MLLMs) often struggle to capture low-level spatial details critical for diagnosing retinal diseases such as glaucoma, diabetic retinopathy, and retinitis pigmentosa. This model development and validation study was conducted on 1,305 fundus image-text pairs compiled from three public datasets (FIVES, HRF, and StoneRounds), covering acquired and inherited retinal diseases, and evaluated using classification accuracy and F1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers into cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are patch-wise projected and fused using scaled cross-attention and FiLM-based U-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease classification task. MDF-MLLM, with both U-Net and MLLM components fully fine-tuned during training, achieved a significantly higher accuracy of 94%, representing a 56% improvement. Recall and F1-scores improved by as much as 67% and 35% over baseline, respectively. Ablation studies confirmed that the multi-depth fusion approach contributed to substantial gains in spatial reasoning and classification, particularly for inherited diseases with rich clinical text. MDF-MLLM presents a generalizable, interpretable, and modular framework for fundus image classification, outperforming traditional MLLM baselines through multi-scale feature fusion. The architecture holds promise for real-world deployment in clinical decision support systems. Future work will explore synchronized training techniques, a larger pool of diseases for more generalizability, and extending the model for segmentation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MDF-MLLMï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡è·¨æ¨¡æ€ç‰¹å¾å¯¹é½å®ç°æ·±åº¦èåˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡è§†ç½‘è†œçœ¼åº•å›¾åƒçš„ç–¾ç—…åˆ†ç±»å‡†ç¡®ç‡ã€‚é’ˆå¯¹ç°æœ‰ MLLMs åœ¨æ•æ‰è¯Šæ–­é’å…‰çœ¼å’Œç³–å°¿ç—…è§†ç½‘è†œç—…å˜ç­‰ç–¾ç—…æ‰€éœ€çš„ç»†ç²’åº¦ç©ºé—´ç»†èŠ‚æ–¹é¢çš„ä¸è¶³ï¼Œè¯¥æ¨¡å‹å°† U-Net ç¼–ç å™¨å››å±‚çš„ skip features é›†æˆåˆ° LLaMA 3.2 11B çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä¸­ã€‚é€šè¿‡ç»“åˆ scaled cross-attention å’ŒåŸºäº FiLM çš„ U-Net è°ƒåˆ¶æŠ€æœ¯ï¼Œæ¨¡å‹æˆåŠŸå®ç°äº†å›¾åƒå¾®è§‚ç‰¹å¾ä¸æ–‡æœ¬ä¸Šä¸‹æ–‡çš„æ·±åº¦èåˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMDF-MLLM åœ¨åŒ…å« FIVESã€HRF å’Œ StoneRounds ç­‰æ•°æ®é›†çš„éªŒè¯ä¸­è¾¾åˆ°äº† 94% çš„å‡†ç¡®ç‡ï¼Œè¾ƒåŸºçº¿æ¨¡å‹æå‡äº† 56%ï¼Œå¹¶åœ¨ Recall å’Œ F1-score ä¸Šè¡¨ç°æ˜¾è‘—ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œå¤šæ·±åº¦èåˆæ–¹æ³•æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨ç©ºé—´æ¨ç†å’Œå¤æ‚ä¸´åºŠæ–‡æœ¬å¤„ç†ä¸­çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ã€å¯è§£é‡Šæ€§å’Œæ¨¡å—åŒ–ç‰¹æ€§ï¼Œä¸ºè§†ç½‘è†œç–¾ç—…çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›äº†æå…·æ½œåŠ›çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Word count: 5157, Table count: 2, Figure count: 5",
      "pdf_url": "https://arxiv.org/pdf/2509.21358v1",
      "published_date": "2025-09-21 05:46:35 UTC",
      "updated_date": "2025-09-21 05:46:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:38.088150+00:00"
    },
    {
      "arxiv_id": "2509.16931v2",
      "title": "Equip Pre-ranking with Target Attention by Residual Quantization",
      "title_zh": "é€šè¿‡æ®‹å·®é‡åŒ–ä¸ºç²—æ’å¼•å…¥ç›®æ ‡æ³¨æ„åŠ›",
      "authors": [
        "Yutong Li",
        "Yu Zhu",
        "Yichen Qiao",
        "Ziyu Guan",
        "Lv Shao",
        "Tong Liu",
        "Bo Zheng"
      ],
      "abstract": "The pre-ranking stage in industrial recommendation systems faces a fundamental conflict between efficiency and effectiveness. While powerful models like Target Attention (TA) excel at capturing complex feature interactions in the ranking stage, their high computational cost makes them infeasible for pre-ranking, which often relies on simplistic vector-product models. This disparity creates a significant performance bottleneck for the entire system. To bridge this gap, we propose TARQ, a novel pre-ranking framework. Inspired by generative models, TARQ's key innovation is to equip pre-ranking with an architecture approximate to TA by Residual Quantization. This allows us to bring the modeling power of TA into the latency-critical pre-ranking stage for the first time, establishing a new state-of-the-art trade-off between accuracy and efficiency. Extensive offline experiments and large-scale online A/B tests at Taobao demonstrate TARQ's significant improvements in ranking performance. Consequently, our model has been fully deployed in production, serving tens of millions of daily active users and yielding substantial business improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šæ¨èç³»ç»Ÿä¸­ç²—æ’(Pre-ranking)é˜¶æ®µåœ¨æ•ˆç‡ä¸æ•ˆæœä¹‹é—´çš„æ ¸å¿ƒå†²çªï¼Œæå‡ºäº†åä¸ºTARQçš„æ–°å‹æ¡†æ¶ã€‚é‰´äºå¼ºå¤§çš„Target Attention (TA)æ¨¡å‹å› è®¡ç®—æˆæœ¬è¿‡é«˜é€šå¸¸éš¾ä»¥ç›´æ¥åº”ç”¨äºç²—æ’ï¼ŒTARQå—åˆ°ç”Ÿæˆæ¨¡å‹å¯å‘ï¼Œåˆ›æ–°æ€§åœ°é€šè¿‡æ®‹å·®é‡åŒ–(Residual Quantization)æŠ€æœ¯å®ç°äº†å¯¹TAæ¶æ„çš„è¿‘ä¼¼ã€‚è¿™ä¸€æ–¹æ³•é¦–æ¬¡å°†TAçš„å¼ºå¤§å»ºæ¨¡èƒ½åŠ›å¼•å…¥å¯¹å»¶è¿Ÿè¦æ±‚æé«˜çš„ç²—æ’é˜¶æ®µï¼Œåœ¨å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´è¾¾æˆäº†æ–°çš„å‰æ²¿(State-of-the-art)å¹³è¡¡ã€‚åœ¨æ·˜å®(Taobao)è¿›è¡Œçš„å¹¿æ³›çº¿ä¸‹å®éªŒå’Œå¤§è§„æ¨¡åœ¨çº¿A/Bæµ‹è¯•å‡è¯æ˜äº†TARQåœ¨æ’åºæ€§èƒ½ä¸Šçš„æ˜¾è‘—æå‡ã€‚ç›®å‰è¯¥æ¨¡å‹å·²åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å…¨é¢éƒ¨ç½²ï¼Œæ¯æ—¥æœåŠ¡æ•°åƒä¸‡æ´»è·ƒç”¨æˆ·ï¼Œå¹¶ä¸ºå®é™…ä¸šåŠ¡å¸¦æ¥äº†æ˜¾è‘—çš„å¢é•¿ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "5 pages, 2 figures, submitted to WSDM 2026 Short Paper Track",
      "pdf_url": "https://arxiv.org/pdf/2509.16931v2",
      "published_date": "2025-09-21 05:33:28 UTC",
      "updated_date": "2025-09-24 09:26:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:55.389946+00:00"
    },
    {
      "arxiv_id": "2509.25210v1",
      "title": "STCast: Adaptive Boundary Alignment for Global and Regional Weather Forecasting",
      "title_zh": "STCastï¼šé¢å‘å…¨çƒåŠåŒºåŸŸå¤©æ°”é¢„æŠ¥çš„è‡ªé€‚åº”è¾¹ç•Œå¯¹é½",
      "authors": [
        "Hao Chen",
        "Tao Han",
        "Jie Zhang",
        "Song Guo",
        "Lei Bai"
      ],
      "abstract": "To gain finer regional forecasts, many works have explored the regional integration from the global atmosphere, e.g., by solving boundary equations in physics-based methods or cropping regions from global forecasts in data-driven methods. However, the effectiveness of these methods is often constrained by static and imprecise regional boundaries, resulting in poor generalization ability. To address this issue, we propose Spatial-Temporal Weather Forecasting (STCast), a novel AI-driven framework for adaptive regional boundary optimization and dynamic monthly forecast allocation. Specifically, our approach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns global and regional spatial distributions to initialize boundaries and adaptively refines them based on attention-derived alignment patterns. Furthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where atmospheric variables from distinct months are dynamically routed to specialized experts using a discrete Gaussian distribution, enhancing the model's ability to capture temporal patterns. Beyond global and regional forecasting, we evaluate our STCast on extreme event prediction and ensemble forecasting. Experimental results demonstrate consistent superiority over state-of-the-art methods across all four tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† STCast æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒºåŸŸå¤©æ°”é¢„æŠ¥ä¸­å› è¾¹ç•Œé™æ€ä¸”ä¸ç²¾ç¡®å¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›å—é™é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç©ºé—´å¯¹é½æ³¨æ„åŠ›æœºåˆ¶ (Spatial-Aligned Attention, SAA)ï¼Œé€šè¿‡å¯¹é½å…¨çƒä¸åŒºåŸŸçš„ç©ºé—´åˆ†å¸ƒæ¥åˆå§‹åŒ–è¾¹ç•Œï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›å¯¹é½æ¨¡å¼è¿›è¡Œè‡ªé€‚åº”ä¼˜åŒ–ã€‚åŒæ—¶ï¼Œç ”ç©¶è®¾è®¡äº†æ—¶é—´æ··åˆä¸“å®¶æ¨¡å— (Temporal Mixture-of-Experts, TMoE)ï¼Œåˆ©ç”¨ç¦»æ•£é«˜æ–¯åˆ†å¸ƒå°†ä¸åŒæœˆä»½çš„å¤§æ°”å˜é‡åŠ¨æ€è·¯ç”±è‡³ä¸“é—¨çš„ä¸“å®¶æ¨¡å‹ï¼Œä»è€Œå¢å¼ºå¯¹å¤æ‚æ—¶é—´æ¨¡å¼çš„æ•æ‰èƒ½åŠ›ã€‚é™¤äº†å…¨çƒå’ŒåŒºåŸŸé¢„æŠ¥ï¼Œå®éªŒè¿˜è¯„ä¼°äº†å…¶åœ¨æç«¯äº‹ä»¶é¢„æµ‹å’Œé›†åˆé¢„æŠ¥ (ensemble forecasting) ä¸­çš„è¡¨ç°ã€‚ç»“æœè¯æ˜ï¼ŒSTCast åœ¨ä¸Šè¿°å››é¡¹ä»»åŠ¡ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³• (state-of-the-art)ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é¢„æµ‹æ€§èƒ½ä¸é€‚åº”æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25210v1",
      "published_date": "2025-09-21 05:27:52 UTC",
      "updated_date": "2025-09-21 05:27:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:56.600507+00:00"
    },
    {
      "arxiv_id": "2509.16926v1",
      "title": "Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment",
      "title_zh": "åŸºäºç½®ä¿¡åº¦åŠ æƒäº¤å‰æ³¨æ„åŠ›çš„å¤šé€šé“éŸ³é¢‘å¯¹é½",
      "authors": [
        "Ragib Amin Nihal",
        "Benjamin Yen",
        "Takeshi Ashizawa",
        "Kazuhiro Nakadai"
      ],
      "abstract": "Multi-channel audio alignment is a key requirement in bioacoustic monitoring, spatial audio systems, and acoustic localization. However, existing methods often struggle to address nonlinear clock drift and lack mechanisms for quantifying uncertainty. Traditional methods like Cross-correlation and Dynamic Time Warping assume simple drift patterns and provide no reliability measures. Meanwhile, recent deep learning models typically treat alignment as a binary classification task, overlooking inter-channel dependencies and uncertainty estimation. We introduce a method that combines cross-attention mechanisms with confidence-weighted scoring to improve multi-channel audio synchronization. We extend BEATs encoders with cross-attention layers to model temporal relationships between channels. We also develop a confidence-weighted scoring function that uses the full prediction distribution instead of binary thresholding. Our method achieved first place in the BioDCASE 2025 Task 1 challenge with 0.30 MSE average across test datasets, compared to 0.58 for the deep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU data (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The framework supports probabilistic temporal alignment, moving beyond point estimates. While validated in a bioacoustic context, the approach is applicable to a broader range of multi-channel audio tasks where alignment confidence is critical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šé€šé“éŸ³é¢‘å¯¹é½ä¸­éš¾ä»¥å¤„ç†éçº¿æ€§æ—¶é’Ÿæ¼‚ç§»(Clock Drift)åŠç¼ºä¹ä¸ç¡®å®šæ€§é‡åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆäº¤å‰æ³¨æ„åŠ›(Cross-Attention)æœºåˆ¶ä¸ç½®ä¿¡åº¦åŠ æƒè¯„åˆ†(Confidence-Weighted Scoring)çš„åˆ›æ–°æ–¹æ³•ã€‚é€šè¿‡æ‰©å±• BEATs ç¼–ç å™¨å¹¶å¼•å…¥äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆå»ºæ¨¡é€šé“é—´çš„æ—¶åºå…³ç³»ï¼Œå¹¶åˆ©ç”¨å…¨é¢„æµ‹åˆ†å¸ƒå®ç°æ¦‚ç‡åŒ–çš„æ—¶åºå¯¹é½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ç‚¹ä¼°è®¡æˆ–äºŒå…ƒåˆ†ç±»æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ BioDCASE 2025 Task 1 æŒ‘æˆ˜èµ›ä¸­è£è·ç¬¬ä¸€åï¼Œå…¶å¹³å‡å‡æ–¹è¯¯å·®(MSE)ä»…ä¸º 0.30ï¼Œç›¸è¾ƒäºæ·±åº¦å­¦ä¹ åŸºçº¿æ¨¡å‹çš„ 0.58 æ˜¾è‘—æå‡ã€‚åœ¨ ARU æ•°æ®é›†å’Œæ–‘èƒ¸è‰é›€æ•°æ®ä¸Šï¼Œè¯¥æ–¹æ³•åˆ†åˆ«å®ç°äº† 77% å’Œ 18% çš„è¯¯å·®ç¼©å‡ã€‚å°½ç®¡ç ”ç©¶ä¸»è¦åœ¨ç”Ÿç‰©å£°å­¦èƒŒæ™¯ä¸‹è¿›è¡ŒéªŒè¯ï¼Œä½†å…¶æå‡ºçš„å¯¹é½ç½®ä¿¡åº¦è¯„ä¼°æ¡†æ¶å¯¹äºæ›´å¹¿æ³›çš„å¤šé€šé“éŸ³é¢‘å¤„ç†ä»»åŠ¡å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted on Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.16926v1",
      "published_date": "2025-09-21 05:14:06 UTC",
      "updated_date": "2025-09-21 05:14:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:57.385067+00:00"
    },
    {
      "arxiv_id": "2509.16924v1",
      "title": "Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation",
      "title_zh": "é¢å‘è§†å¬å¯¼èˆªçš„ç«‹ä½“å£°æ„ŸçŸ¥æ³¨æ„åŠ›åŠéŸ³é¢‘å¼•å¯¼åŠ¨æ€æ¨¡æ€èåˆ",
      "authors": [
        "Jia Li",
        "Yinfeng Yu",
        "Liejun Wang",
        "Fuchun Sun",
        "Wendong Zheng"
      ],
      "abstract": "In audio-visual navigation (AVN) tasks, an embodied agent must autonomously localize a sound source in unknown and complex 3D environments based on audio-visual signals. Existing methods often rely on static modality fusion strategies and neglect the spatial cues embedded in stereo audio, leading to performance degradation in cluttered or occluded scenes. To address these issues, we propose an end-to-end reinforcement learning-based AVN framework with two key innovations: (1) a \\textbf{S}tereo-Aware \\textbf{A}ttention \\textbf{M}odule (\\textbf{SAM}), which learns and exploits the spatial disparity between left and right audio channels to enhance directional sound perception; and (2) an \\textbf{A}udio-\\textbf{G}uided \\textbf{D}ynamic \\textbf{F}usion Module (\\textbf{AGDF}), which dynamically adjusts the fusion ratio between visual and auditory features based on audio cues, thereby improving robustness to environmental changes. Extensive experiments are conducted on two realistic 3D scene datasets, Replica and Matterport3D, demonstrating that our method significantly outperforms existing approaches in terms of navigation success rate and path efficiency. Notably, our model achieves over 40\\% improvement under audio-only conditions compared to the best-performing baselines. These results highlight the importance of explicitly modeling spatial cues from stereo channels and performing deep multi-modal fusion for robust and efficient audio-visual navigation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³è§†é¢‘å¯¼èˆª (Audio-Visual Navigation, AVN) ä»»åŠ¡ä¸­ç°æœ‰æ–¹æ³•å¿½è§†ç«‹ä½“å£°ç©ºé—´çº¿ç´¢ä»¥åŠæ¨¡æ€èåˆç­–ç•¥å•ä¸€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯å¯¼èˆªæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç«‹ä½“å£°æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å— (Stereo-Aware Attention Module, SAM)ï¼Œé€šè¿‡å­¦ä¹ å·¦å³å£°é“é—´çš„ç©ºé—´å·®å¼‚æ¥å¢å¼ºæ™ºèƒ½ä½“å¯¹å£°æºæ–¹å‘çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç ”ç©¶è®¾è®¡äº†éŸ³é¢‘å¼•å¯¼åŠ¨æ€èåˆæ¨¡å— (Audio-Guided Dynamic Fusion Module, AGDF)ï¼Œèƒ½å¤Ÿæ ¹æ®éŸ³é¢‘ç‰¹å¾åŠ¨æ€è°ƒæ•´è§†è§‰ä¸å¬è§‰ä¿¡æ¯çš„èåˆæ¯”ä¾‹ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨å¤æ‚æˆ–é®æŒ¡ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚å®éªŒåœ¨ Replica å’Œ Matterport3D ä¸¤ä¸ªçœŸå®æ„Ÿ 3D åœºæ™¯æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¯¼èˆªæˆåŠŸç‡å’Œè·¯å¾„æ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨çº¯éŸ³é¢‘æ¡ä»¶ä¸‹ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½è¾ƒè¡¨ç°æœ€å¥½çš„åŸºå‡†æ¨¡å‹æå‡äº† 40% ä»¥ä¸Šã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†æ˜¾å¼å»ºæ¨¡ç«‹ä½“å£°ç©ºé—´çº¿ç´¢ä»¥åŠæ·±åº¦å¤šæ¨¡æ€åŠ¨æ€èåˆå¯¹äºå®ç°ç¨³å¥é«˜æ•ˆçš„éŸ³è§†é¢‘å¯¼èˆªå…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.AI",
      "comment": "Main paper (14 pages). Accepted for publication by ICONIP( International Conference on Neural Information Processing) 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16924v1",
      "published_date": "2025-09-21 05:11:09 UTC",
      "updated_date": "2025-09-21 05:11:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:08:59.662432+00:00"
    },
    {
      "arxiv_id": "2509.22698v1",
      "title": "Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in 3D Environments",
      "title_zh": "é€šè¿‡ 3D ç¯å¢ƒä¸‹çš„å¤šæ™ºèƒ½ä½“åä½œæ¨è¿›è§†å¬å¯¼èˆª",
      "authors": [
        "Hailong Zhang",
        "Yinfeng Yu",
        "Liejun Wang",
        "Fuchun Sun",
        "Wendong Zheng"
      ],
      "abstract": "Intelligent agents often require collaborative strategies to achieve complex tasks beyond individual capabilities in real-world scenarios. While existing audio-visual navigation (AVN) research mainly focuses on single-agent systems, their limitations emerge in dynamic 3D environments where rapid multi-agent coordination is critical, especially for time-sensitive applications like emergency response. This paper introduces MASTAVN (Multi-Agent Scalable Transformer Audio-Visual Navigation), a scalable framework enabling two agents to collaboratively localize and navigate toward an audio target in shared 3D environments. By integrating cross-agent communication protocols and joint audio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal synchronization. Through rigorous evaluation in photorealistic 3D simulators (Replica and Matterport3D), MASTAVN achieves significant reductions in task completion time and notable improvements in navigation success rates compared to single-agent and non-collaborative baselines. This highlights the essential role of spatiotemporal coordination in multi-agent systems. Our findings validate MASTAVN's effectiveness in time-sensitive emergency scenarios and establish a paradigm for advancing scalable multi-agent embodied intelligence in complex 3D environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è§†å¬å¯¼èˆª(Audio-Visual Navigation, AVN)ä¸»è¦é›†ä¸­äºå•æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œåœ¨åŠ¨æ€3Dç¯å¢ƒä¸­é¢ä¸´åä½œèƒ½åŠ›ä¸è¶³å’Œæ—¶é—´æ•æ„Ÿå‹ä»»åŠ¡å“åº”ç¼“æ…¢çš„é—®é¢˜ï¼Œæå‡ºäº†MASTAVN (Multi-Agent Scalable Transformer Audio-Visual Navigation)å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä½¿ä¸¤ä¸ªæ™ºèƒ½ä½“åœ¨å…±äº«çš„3Dç¯å¢ƒä¸­åä½œå®šä½å¹¶å¯¼èˆªè‡³éŸ³é¢‘ç›®æ ‡ï¼Œé€šè¿‡é›†æˆè·¨æ™ºèƒ½ä½“é€šä¿¡åè®®(Cross-agent communication protocols)å’Œè”åˆè§†å¬èåˆæœºåˆ¶(Joint audio-visual fusion mechanisms)ï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“çš„ç©ºé—´æ¨ç†èƒ½åŠ›å’Œæ—¶é—´åŒæ­¥æ€§èƒ½ã€‚åœ¨Replicaå’ŒMatterport3Dç­‰çœŸå®3Dæ¨¡æ‹Ÿå™¨ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMASTAVNåœ¨ä»»åŠ¡å®Œæˆæ—¶é—´å’Œå¯¼èˆªæˆåŠŸç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºå•æ™ºèƒ½ä½“å’Œéåä½œåŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œéªŒè¯äº†æ—¶ç©ºåè°ƒåœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶è¯æ˜äº†è¯¥æ¡†æ¶åœ¨ç´§æ€¥å“åº”ç­‰æ—¶é—´æ•æ„Ÿåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„å¯æ‰©å±•å…·èº«æ™ºèƒ½(Embodied Intelligence)ç ”ç©¶å»ºç«‹äº†æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Main paper (15 pages). Accepted for publication by ICONIP( International Conference on Neural Information Processing) 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.22698v1",
      "published_date": "2025-09-21 05:05:26 UTC",
      "updated_date": "2025-09-21 05:05:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:11.354768+00:00"
    },
    {
      "arxiv_id": "2509.16922v1",
      "title": "PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control",
      "title_zh": "PGSTalkerï¼šåŸºäºåƒç´ æ„ŸçŸ¥å¯†åº¦æ§åˆ¶ 3D é«˜æ–¯æ³¼æº…çš„å®æ—¶éŸ³é¢‘é©±åŠ¨è¯´è¯äººè„¸ç”Ÿæˆ",
      "authors": [
        "Tianheng Zhu",
        "Yinfeng Yu",
        "Liejun Wang",
        "Fuchun Sun",
        "Wendong Zheng"
      ],
      "abstract": "Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PGSTalkerï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº3Dé«˜æ–¯æ³¼æº…(3D Gaussian Splatting, 3DGS)çš„å®æ—¶è¯­éŸ³é©±åŠ¨äººå¤´è¯´è¯ç”Ÿæˆ(Audio-driven talking head generation)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³NeRFæ–¹æ³•åœ¨æ¸²æŸ“æ•ˆç‡å’ŒéŸ³ç”»åŒæ­¥æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæå‡æ¸²æŸ“æ€§èƒ½ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†åƒç´ æ„ŸçŸ¥å¯†åº¦æ§åˆ¶(Pixel-aware density control)ç­–ç•¥ï¼Œé€šè¿‡è‡ªé€‚åº”åˆ†é…ç‚¹å¯†åº¦æ¥å¢å¼ºé¢éƒ¨åŠ¨æ€åŒºåŸŸç»†èŠ‚å¹¶å‡å°‘å†—ä½™ã€‚åŒæ—¶ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„å¤šæ¨¡æ€é—¨æ§èåˆæ¨¡å—(Multimodal Gated Fusion Module)ï¼Œæœ‰æ•ˆæ•´åˆéŸ³é¢‘ä¸ç©ºé—´ç‰¹å¾ä»¥æå‡é«˜æ–¯å˜å½¢é¢„æµ‹çš„å‡†ç¡®åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPGSTalkeråœ¨æ¸²æŸ“è´¨é‡ã€å”‡è¯­åŒæ­¥ç²¾åº¦(lip-sync precision)åŠæ¨ç†é€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„NeRFå’Œ3DGSåŸºçº¿æ¨¡å‹ã€‚è¯¥æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…éƒ¨ç½²æ½œåŠ›ï¼Œä¸ºé«˜ä¿çœŸã€å®æ—¶çš„æ•°å­—äººåˆæˆæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.SD",
      "comment": "Main paper (15 pages). Accepted for publication by ICONIP( International Conference on Neural Information Processing) 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.16922v1",
      "published_date": "2025-09-21 05:01:54 UTC",
      "updated_date": "2025-09-21 05:01:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:09.158727+00:00"
    },
    {
      "arxiv_id": "2509.16902v1",
      "title": "FedEL: Federated Elastic Learning for Heterogeneous Devices",
      "title_zh": "FedELï¼šé¢å‘å¼‚æ„è®¾å¤‡çš„è”é‚¦å¼¹æ€§å­¦ä¹ ",
      "authors": [
        "Letian Zhang",
        "Bo Chen",
        "Jieming Bian",
        "Lei Wang",
        "Jie Xu"
      ],
      "abstract": "Federated learning (FL) enables distributed devices to collaboratively train machine learning models while maintaining data privacy. However, the heterogeneous hardware capabilities of devices often result in significant training delays, as straggler clients with limited resources prolong the aggregation process. Existing solutions such as client selection, asynchronous FL, and partial training partially address these challenges but encounter issues such as reduced accuracy, stale updates, and compromised model performance due to inconsistent training contributions. To overcome these limitations, we propose FedEL, a federated elastic learning framework that enhances training efficiency while maintaining model accuracy. FedEL introduces a novel window-based training process, sliding the window to locate the training part of the model and dynamically selecting important tensors for training within a coordinated runtime budget. This approach ensures progressive and balanced training across all clients, including stragglers. Additionally, FedEL employs a tensor importance adjustment module, harmonizing local and global tensor importance to mitigate biases caused by data heterogeneity. The experiment results show that FedEL achieves up to 3.87x improvement in time-to-accuracy compared to baselines while maintaining or exceeding final test accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FedELï¼Œä¸€ç§é’ˆå¯¹å¼‚æ„è®¾å¤‡çš„è”é‚¦å¼¹æ€§å­¦ä¹ (Federated Elastic Learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹ (Federated Learning)ä¸­å› ç¡¬ä»¶èƒ½åŠ›å·®å¼‚å¯¼è‡´çš„æ‰é˜Ÿè€…(stragglers)è®­ç»ƒå»¶è¿Ÿé—®é¢˜ã€‚FedELå¼•å…¥äº†åˆ›æ–°çš„åŸºäºçª—å£çš„è®­ç»ƒæµç¨‹(window-based training process)ï¼Œé€šè¿‡æ»‘åŠ¨çª—å£å®šä½æ¨¡å‹çš„è®­ç»ƒéƒ¨åˆ†ï¼Œå¹¶åœ¨åè°ƒçš„è¿è¡Œæ—¶é¢„ç®—å†…åŠ¨æ€é€‰æ‹©é‡è¦å¼ é‡(tensors)è¿›è¡Œè®­ç»ƒã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†åŒ…æ‹¬èµ„æºå—é™è®¾å¤‡åœ¨å†…çš„æ‰€æœ‰å®¢æˆ·ç«¯éƒ½èƒ½è¿›è¡Œæ¸è¿›ä¸”å‡è¡¡çš„è®­ç»ƒï¼Œä»è€Œæå‡æ•´ä½“è®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒFedELé‡‡ç”¨å¼ é‡é‡è¦æ€§è°ƒæ•´æ¨¡å—(tensor importance adjustment module)æ¥å¹³è¡¡å±€éƒ¨å’Œå…¨å±€å¼ é‡æƒé‡ï¼Œæœ‰æ•ˆç¼“è§£äº†æ•°æ®å¼‚æ„æ€§å¸¦æ¥çš„åå·®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedELåœ¨ä¿æŒæˆ–æå‡æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡çš„æ—¶é—´ä¸Šå®ç°äº†é«˜è¾¾3.87å€çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16902v1",
      "published_date": "2025-09-21 03:25:46 UTC",
      "updated_date": "2025-09-21 03:25:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:12.164645+00:00"
    },
    {
      "arxiv_id": "2509.16900v1",
      "title": "ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion for Multimodal Survival Analysis",
      "title_zh": "ME-Mambaï¼šå…·æœ‰é«˜æ•ˆçŸ¥è¯†æ•è·ä¸èåˆèƒ½åŠ›çš„å¤šæ¨¡æ€ç”Ÿå­˜åˆ†æå¤šä¸“å®¶ Mamba",
      "authors": [
        "Chengsheng Zhang",
        "Linhao Qu",
        "Xiaoyu Liu",
        "Zhijian Song"
      ],
      "abstract": "Survival analysis using whole-slide images (WSIs) is crucial in cancer research. Despite significant successes, pathology images typically only provide slide-level labels, which hinders the learning of discriminative representations from gigapixel WSIs. With the rapid advancement of high-throughput sequencing technologies, multimodal survival analysis integrating pathology images and genomics data has emerged as a promising approach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures discriminative pathological and genomic features while enabling efficient integration of both modalities. This approach achieves complementary information fusion without losing critical information from individual modalities, thereby facilitating accurate cancer survival analysis. Specifically, we first introduce a Pathology Expert and a Genomics Expert to process unimodal data separately. Both experts are designed with Mamba architectures that incorporate conventional scanning and attention-based scanning mechanisms, allowing them to extract discriminative features from long instance sequences containing substantial redundant or irrelevant information. Second, we design a Synergistic Expert responsible for modality fusion. It explicitly learns token-level local correspondences between the two modalities via Optimal Transport, and implicitly enhances distribution consistency through a global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused feature representations are then passed to a mamba backbone for further integration. Through the collaboration of the Pathology Expert, Genomics Expert, and Synergistic Expert, our method achieves stable and accurate survival analysis with relatively low computational complexity. Extensive experimental results on five datasets in The Cancer Genome Atlas (TCGA) demonstrate our state-of-the-art performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ME-Mamba (Multi-Expert Mamba) ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡é«˜æ•ˆæ•´åˆå…¨åˆ‡ç‰‡å›¾åƒ (WSIs) å’ŒåŸºå› ç»„æ•°æ®ï¼Œæå‡å¤šæ¨¡æ€ç™Œç—‡ç”Ÿå­˜åˆ†æçš„å‡†ç¡®æ€§ã€‚è¯¥ç³»ç»Ÿåˆ†åˆ«è®¾è®¡äº† Pathology Expert å’Œ Genomics Expertï¼Œåˆ©ç”¨ç»“åˆå¸¸è§„æ‰«æä¸æ³¨æ„åŠ›æ‰«ææœºåˆ¶çš„ Mamba æ¶æ„ï¼Œä»åŒ…å«å¤§é‡å†—ä½™ä¿¡æ¯çš„é•¿åºåˆ—ä¸­æå–å…·æœ‰è¾¨åˆ«åŠ›çš„å•æ¨¡æ€ç‰¹å¾ã€‚é’ˆå¯¹æ¨¡æ€èåˆï¼Œç ”ç©¶è®¾è®¡äº† Synergistic Expertï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“ (Optimal Transport) æ˜¾å¼å­¦ä¹ æ¨¡æ€é—´çš„æ ‡è®°çº§å±€éƒ¨å¯¹åº”å…³ç³»ï¼Œå¹¶åˆ©ç”¨æœ€å¤§å‡å€¼å·®å¼‚ (Maximum Mean Discrepancy) å¢å¼ºå…¨å±€åˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚èåˆåçš„ç‰¹å¾è¿›ä¸€æ­¥é€šè¿‡ Mamba ä¸»å¹²ç½‘ç»œè¿›è¡Œæ·±åº¦æ•´åˆï¼Œå®ç°äº†å¤šæ¨¡æ€ä¿¡æ¯çš„äº’è¡¥èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒME-Mamba åœ¨äº”ä¸ª TCGA æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œåœ¨ä¿æŒè¾ƒä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œå®ç°äº†ç¨³å®šä¸”ç²¾ç¡®çš„ç”Ÿå­˜åˆ†æã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16900v1",
      "published_date": "2025-09-21 03:23:04 UTC",
      "updated_date": "2025-09-21 03:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:14.979188+00:00"
    },
    {
      "arxiv_id": "2509.16892v1",
      "title": "Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning",
      "title_zh": "èåˆåŸºå› åç§°ã€è¡¨è¾¾å€¼ä¸å›¾åƒï¼šé¢å‘ç©ºé—´è½¬å½•ç»„è¡¨å¾å­¦ä¹ çš„å¯¹æ¯”æ©ç æ–‡æœ¬-å›¾åƒé¢„è®­ç»ƒ",
      "authors": [
        "Jiahe Qian",
        "Yaoyu Fang",
        "Ziqiao Weng",
        "Xinkun Wang",
        "Lee A. Cooper",
        "Bo Zhou"
      ],
      "abstract": "Spatial transcriptomics aims to connect high-resolution histology images with spatially resolved gene expression. To achieve better performance on downstream tasks such as gene expression prediction, large-scale pre-training is required to obtain generalisable representations that can bridge histology and transcriptomics across tissues, protocols, and laboratories. Existing cross-modal pre-training approaches for spatial transcriptomics rely on either gene names or expression values in isolation, which strips the gene branch of essential semantics and breaks the association between each gene and its quantitative magnitude. In addition, by restricting supervision to image-text alignment, these methods ignore intrinsic visual cues that are critical for learning robust image features. We present CoMTIP, the first Contrastive Masked Text-Image Pretraining framework that jointly learns from images, gene names, and expression values while capturing fine-grained visual context for spatial transcriptomics. The vision branch uses Masked Feature Modeling to reconstruct occluded patches and learn context-aware image embeddings. The text branch applies a scalable Gene-Text Encoder that processes all gene sentences in parallel, enriches each gene and its numerical value with dedicated embeddings, and employs Pair-aware Adversarial Training (PAAT) to preserve correct gene-value associations. Image and text representations are aligned in a shared InfoNCE-optimised space. Experiments on public spatial transcriptomics datasets show that CoMTIP not only surpasses previous methods on diverse downstream tasks but also achieves zero-shot gene expression prediction, a capability that existing approaches do not provide.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoMTIPï¼Œè¿™æ˜¯é¦–ä¸ªç»“åˆå›¾åƒã€åŸºå› åç§°(Gene Names)å’Œè¡¨è¾¾å€¼(Expression Values)çš„å¯¹æ¯”æ©ç æ–‡æœ¬å›¾åƒé¢„è®­ç»ƒ(Contrastive Masked Text-Image Pretraining)æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç©ºé—´è½¬å½•ç»„å­¦(Spatial Transcriptomics)çš„è¡¨ç¤ºå­¦ä¹ æ•ˆæœã€‚é’ˆå¯¹ç°æœ‰è·¨æ¨¡æ€é¢„è®­ç»ƒæ–¹æ³•ä¸­åŸºå› è¯­ä¹‰ç¼ºå¤±åŠè§†è§‰çº¿ç´¢å¿½ç•¥çš„é—®é¢˜ï¼ŒCoMTIPé€šè¿‡è”åˆå­¦ä¹ æœºåˆ¶å¢å¼ºäº†é«˜åˆ†è¾¨ç‡ç»„ç»‡å­¦å›¾åƒä¸è½¬å½•ç»„æ•°æ®ä¹‹é—´çš„å…³è”ã€‚å…¶è§†è§‰åˆ†æ”¯é‡‡ç”¨æ©ç ç‰¹å¾å»ºæ¨¡(Masked Feature Modeling)æ¥é‡å»ºé®æŒ¡è¡¥ä¸ä»¥å­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›¾åƒåµŒå…¥ï¼Œè€Œæ–‡æœ¬åˆ†æ”¯åˆ™åˆ©ç”¨åŸºå› æ–‡æœ¬ç¼–ç å™¨(Gene-Text Encoder)ç»“åˆé…å¯¹æ„ŸçŸ¥å¯¹æŠ—è®­ç»ƒ(Pair-aware Adversarial Training)æ¥ç¡®ä¿åŸºå› ä¸å…¶æ•°å€¼ä¹‹é—´çš„æ­£ç¡®å…³è”ã€‚å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºæœ€ç»ˆåœ¨å…±äº«çš„InfoNCEä¼˜åŒ–ç©ºé—´ä¸­è¿›è¡Œå¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoMTIPåœ¨å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶é¦–æ¬¡å®ç°äº†æ­¤å‰æ–¹æ³•ä¸å…·å¤‡çš„é›¶æ ·æœ¬(Zero-shot)åŸºå› è¡¨è¾¾é¢„æµ‹èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.16892v1",
      "published_date": "2025-09-21 03:05:17 UTC",
      "updated_date": "2025-09-21 03:05:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:19.261711+00:00"
    },
    {
      "arxiv_id": "2509.16891v3",
      "title": "LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware Layout Generation",
      "title_zh": "LLMs ä½œä¸ºå¸ƒå±€è®¾è®¡å¸ˆï¼šé¢å‘å†…å®¹æ„ŸçŸ¥å¸ƒå±€ç”Ÿæˆçš„å¢å¼ºç©ºé—´æ¨ç†",
      "authors": [
        "Sha Li",
        "Stefano Petrangeli",
        "Yu Shen",
        "Xiang Chen",
        "Naren Ramakrishnan"
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their ability to understand and manipulate spatial relationships remains limited. Such capabilities are crucial for content-aware graphic layout design, where the goal is to arrange heterogeneous elements onto a canvas so that final design remains visually balanced and structurally feasible. This problem requires precise coordination of placement, alignment, and structural organization of multiple elements within a constrained visual space. To address this limitation, we introduce LaySPA, a reinforcement learning-based framework that augments LLM-based agents with explicit spatial reasoning capabilities for layout design. LaySPA employs hybrid reward signals that jointly capture geometric constraints, structural fidelity, and visual quality, enabling agents to navigate the canvas, model inter-element relationships, and optimize spatial arrangements. Through group-relative policy optimization, the agent generates content-aware layouts that reflect salient regions, respect spatial constraints, and produces an interpretable reasoning trace explaining placement decisions and a structured layout specification. Experimental results show that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å›¾å½¢å¸ƒå±€è®¾è®¡ä¸­ç©ºé—´æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†LaySPAæ¡†æ¶ã€‚LaySPAæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ (Reinforcement Learning)çš„æ¡†æ¶ï¼Œé€šè¿‡èµ‹äºˆLLMæ™ºèƒ½ä½“æ˜¾å¼çš„ç©ºé—´æ¨ç†èƒ½åŠ›æ¥ä¼˜åŒ–å†…å®¹æ„ŸçŸ¥å¸ƒå±€ç”Ÿæˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ··åˆå¥–åŠ±ä¿¡å·(Hybrid Reward Signals)æ¥æ•æ‰å‡ ä½•çº¦æŸã€ç»“æ„ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡ï¼Œä»è€Œç²¾ç¡®åè°ƒå¤šä¸ªå…ƒç´ åœ¨ç”»å¸ƒä¸Šçš„æ’åˆ—ä¸å¯¹é½ã€‚é€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(Group-Relative Policy Optimization)ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿç”Ÿæˆåæ˜ æ˜¾è‘—åŒºåŸŸå¹¶ç¬¦åˆç©ºé—´çº¦æŸçš„å¸ƒå±€ï¼ŒåŒæ—¶æä¾›å¯è§£é‡Šçš„å†³ç­–æ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaySPAåœ¨ç”Ÿæˆç»“æ„æœ‰æ•ˆä¸”è§†è§‰ç¾è§‚çš„å¸ƒå±€æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸ä»…æ˜¾è‘—è¶…è¶Šäº†é€šç”¨å¤§æ¨¡å‹ï¼Œè¿˜è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›çš„ä¸“é—¨å¸ƒå±€æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16891v3",
      "published_date": "2025-09-21 03:02:59 UTC",
      "updated_date": "2026-01-05 20:28:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:21.059595+00:00"
    },
    {
      "arxiv_id": "2509.18208v1",
      "title": "Variational Task Vector Composition",
      "title_zh": "å˜åˆ†ä»»åŠ¡å‘é‡ç»„åˆ",
      "authors": [
        "Boyuan Zhang",
        "Yingjun Du",
        "Xiantong Zhen",
        "Ling Shao"
      ],
      "abstract": "Task vectors capture how a model changes during fine-tuning by recording the difference between pre-trained and task-specific weights. The composition of task vectors, a key operator in task arithmetic, enables models to integrate knowledge from multiple tasks without incurring additional inference costs. In this paper, we propose variational task vector composition, where composition coefficients are taken as latent variables and estimated in a Bayesian inference framework. Unlike previous methods that operate at the task level, our framework focuses on sample-specific composition. Motivated by the observation of structural redundancy in task vectors, we introduce a Spike-and-Slab prior that promotes sparsity and preserves only the most informative components. To further address the high variance and sampling inefficiency in sparse, high-dimensional spaces, we develop a gated sampling mechanism that constructs a controllable posterior by filtering the composition coefficients based on both uncertainty and importance. This yields a more stable and interpretable variational framework by deterministically selecting reliable task components, reducing sampling variance while improving transparency and generalization. Experimental results demonstrate that our method consistently outperforms existing approaches across all datasets by selectively leveraging the most reliable and informative components in task vectors. These findings highlight the practical value of our approach, establishing a new standard for efficient and effective task vector composition.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Variational Task Vector Compositionï¼Œæ—¨åœ¨é€šè¿‡è´å¶æ–¯æ¨ç†(Bayesian inference)æ¡†æ¶ä¼˜åŒ–æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ•è·çš„ä»»åŠ¡å‘é‡(Task vectors)é›†æˆã€‚ä¸ä¼ ç»Ÿçš„ä»»åŠ¡çº§æ“ä½œä¸åŒï¼Œè¯¥æ¡†æ¶å°†åˆæˆç³»æ•°è§†ä¸ºæ½œåœ¨å˜é‡ï¼Œä¸“æ³¨äºé’ˆå¯¹ç‰¹å®šæ ·æœ¬(Sample-specific)çš„ç»„åˆç­–ç•¥ã€‚é’ˆå¯¹ä»»åŠ¡å‘é‡ä¸­çš„ç»“æ„å†—ä½™ï¼Œç ”ç©¶å¼•å…¥äº†Spike-and-Slabå…ˆéªŒä»¥ä¿ƒè¿›ç¨€ç–æ€§ï¼Œä»è€Œä»…ä¿ç•™æœ€å…·ä¿¡æ¯é‡çš„ç»„ä»¶ã€‚ä¸ºäº†åº”å¯¹é«˜ç»´ç¨€ç–ç©ºé—´ä¸­çš„é‡‡æ ·æ•ˆç‡é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§é—¨æ§é‡‡æ ·æœºåˆ¶(Gated sampling mechanism)ï¼Œé€šè¿‡ç»“åˆä¸ç¡®å®šæ€§å’Œé‡è¦æ€§æ¥æ„å»ºå¯æ§çš„åéªŒåˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç¡®å®šæ€§åœ°é€‰æ‹©å¯é ä»»åŠ¡ç»„ä»¶ï¼Œæ˜¾è‘—é™ä½äº†é‡‡æ ·æ–¹å·®å¹¶æå‡äº†æ¡†æ¶çš„å¯è§£é‡Šæ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºé«˜æ•ˆä¸”æœ‰æ•ˆçš„ä»»åŠ¡å‘é‡åˆæˆ(Task vector composition)æä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18208v1",
      "published_date": "2025-09-21 02:46:02 UTC",
      "updated_date": "2025-09-21 02:46:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:35.185807+00:00"
    },
    {
      "arxiv_id": "2509.16882v1",
      "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation",
      "title_zh": "åŠ¨æ€ä¸“å®¶ç‰¹åŒ–ï¼šè¿ˆå‘æ— ç¾éš¾æ€§é—å¿˜çš„å¤šé¢†åŸŸ MoE é€‚é…",
      "authors": [
        "Junzhuo Li",
        "Bo Wang",
        "Xiuze Zhou",
        "Xuming Hu"
      ],
      "abstract": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated expert subnetworks, yet adapting them to multiple domains without catastrophic forgetting remains an open challenge. Existing approaches either incur prohibitive computation, suffer cross-domain interference, or require separate runs per domain. We propose DES-MoE, a dynamic expert specialization framework for multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses catastrophic forgetting through three innovations: (1) an adaptive router balancing pre-trained knowledge retention and task-specific updates via distillation, (2) real-time expert-domain correlation mapping to isolate domain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule that progressively freezes non-specialized parameters. Evaluated on six domains (math, code, law, etc.), DES-MoE matches single-domain ESFT performance while training one unified model, reduces forgetting by 89% compared to full fine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence than conventional methods. Our work establishes dynamic expert isolation as a scalable paradigm for multi-task MoE adaptation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DES-MoEï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³æ··åˆä¸“å®¶æ¨¡å‹(Mixture-of-Experts)åœ¨å¤šé¢†åŸŸè‡ªé€‚åº”è¿‡ç¨‹ä¸­å‡ºç°çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)é—®é¢˜çš„åŠ¨æ€ä¸“å®¶ä¸“ä¸šåŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼šåˆ©ç”¨è‡ªé€‚åº”è·¯ç”±é€šè¿‡è’¸é¦(Distillation)å¹³è¡¡çŸ¥è¯†ä¿ç•™ä¸ä»»åŠ¡æ›´æ–°ï¼Œé‡‡ç”¨å®æ—¶ä¸“å®¶-é¢†åŸŸå…³è”æ˜ å°„éš”ç¦»é¢†åŸŸç‰¹å®šæ¢¯åº¦ï¼Œä»¥åŠæ‰§è¡Œä¸‰é˜¶æ®µè‡ªé€‚åº”å¾®è°ƒè®¡åˆ’ä»¥é€æ­¥å†»ç»“éä¸“ä¸šåŒ–å‚æ•°ã€‚åœ¨æ¶µç›–æ•°å­¦ã€ä»£ç ã€æ³•å¾‹ç­‰å…­ä¸ªé¢†åŸŸçš„è¯„ä¼°ä¸­ï¼ŒDES-MoEåœ¨ä¿æŒç»Ÿä¸€æ¨¡å‹è®­ç»ƒçš„åŒæ—¶ï¼Œæ€§èƒ½å¯ä¸å•é¢†åŸŸå¾®è°ƒ(ESFT)ç›¸åª²ç¾ã€‚ç›¸æ¯”äºå…¨é‡å¾®è°ƒï¼Œè¯¥æ–¹æ³•å°†é—å¿˜ç‡é™ä½äº†89%ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ¯”ä¼ ç»Ÿæ–¹æ³•å¿«68%ã€‚æ­¤é¡¹å·¥ä½œè¯æ˜äº†åŠ¨æ€ä¸“å®¶éš”ç¦»æ˜¯å®ç°å¤šä»»åŠ¡MoEè‡ªé€‚åº”çš„ä¸€ç§å…·æœ‰å¯æ‰©å±•æ€§çš„æœ‰æ•ˆèŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2509.16882v1",
      "published_date": "2025-09-21 02:30:04 UTC",
      "updated_date": "2025-09-21 02:30:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:39.104477+00:00"
    },
    {
      "arxiv_id": "2509.16869v1",
      "title": "PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR Reconstruction",
      "title_zh": "PhysHDRï¼šHDRé‡å»ºä¸­å…‰ç…§ã€æè´¨ä¸åœºæ™¯å‡ ä½•çš„èåˆ",
      "authors": [
        "Hrishav Bakul Barua",
        "Kalin Stefanov",
        "Ganesh Krishnasamy",
        "KokSheik Wong",
        "Abhinav Dhall"
      ],
      "abstract": "Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a fundamental task in many computational vision problems. Numerous data-driven methods have been proposed to address this problem; however, they lack explicit modeling of illumination, lighting, and scene geometry in images. This limits the quality of the reconstructed HDR images. Since lighting and shadows interact differently with different materials, (e.g., specular surfaces such as glass and metal, and lambertian or diffuse surfaces such as wood and stone), modeling material-specific properties (e.g., specular and diffuse reflectance) has the potential to improve the quality of HDR image reconstruction. This paper presents PhysHDR, a simple yet powerful latent diffusion-based generative model for HDR image reconstruction. The denoising process is conditioned on lighting and depth information and guided by a novel loss to incorporate material properties of surfaces in the scene. The experimental results establish the efficacy of PhysHDR in comparison to a number of recent state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PhysHDRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£(latent diffusion)çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä½åŠ¨æ€èŒƒå›´(LDR)åˆ°é«˜åŠ¨æ€èŒƒå›´(HDR)å›¾åƒè½¬æ¢ä¸­å› ç¼ºä¹å¯¹å…‰ç…§ã€ç…§æ˜å’Œåœºæ™¯å‡ ä½•æ˜¾å¼å»ºæ¨¡è€Œå¯¼è‡´çš„é‡å»ºè´¨é‡ç“¶é¢ˆã€‚ç”±äºå…‰å½±ä¸ä¸åŒææ–™ï¼ˆå¦‚specularæˆ–diffuseè¡¨é¢ï¼‰çš„äº¤äº’ç‰¹æ€§å„å¼‚ï¼ŒPhysHDRåœ¨å»å™ªè¿‡ç¨‹ä¸­å°†ç…§æ˜å’Œæ·±åº¦ä¿¡æ¯ä½œä¸ºçº¦æŸæ¡ä»¶ï¼Œå¹¶é‡‡ç”¨ä¸€ç§æ–°é¢–çš„æŸå¤±å‡½æ•°æ¥æ•´åˆåœºæ™¯è¡¨é¢çš„ææ–™å±æ€§ã€‚é€šè¿‡æ˜¾å¼å»ºæ¨¡ææ–™ç‰¹å®šå±æ€§ï¼ˆå¦‚specular and diffuse reflectanceï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ›´ç²¾å‡†åœ°è¿˜åŸå›¾åƒä¸­çš„ç‰©ç†å…‰ç…§äº¤äº’ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒPhysHDRåœ¨é‡å»ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†è®¡ç®—è§†è§‰ä»»åŠ¡ä¸­çš„HDRæˆåƒæ•ˆæœã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "eess.IV"
      ],
      "primary_category": "cs.GR",
      "comment": "Submitted to IEEE",
      "pdf_url": "https://arxiv.org/pdf/2509.16869v1",
      "published_date": "2025-09-21 01:41:40 UTC",
      "updated_date": "2025-09-21 01:41:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:40.662042+00:00"
    },
    {
      "arxiv_id": "2509.16866v1",
      "title": "seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs",
      "title_zh": "seqBenchï¼šé‡åŒ–å¤§è¯­è¨€æ¨¡å‹åºåˆ—æ¨ç†æé™çš„å¯è°ƒèŠ‚åŸºå‡†",
      "authors": [
        "Mohammad Ramezanali",
        "Mo Vazifeh",
        "Paolo Santi"
      ],
      "abstract": "We introduce seqBench, a parametrized benchmark for probing sequential reasoning limits in Large Language Models (LLMs) through precise, multi-dimensional control over several key complexity dimensions. seqBench allows systematic variation of (1) the logical depth, defined as the number of sequential actions required to solve the task; (2) the number of backtracking steps along the optimal path, quantifying how often the agent must revisit prior states to satisfy deferred preconditions (e.g., retrieving a key after encountering a locked door); and (3) the noise ratio, defined as the ratio between supporting and distracting facts about the environment. Our evaluations on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses exponentially beyond a model-specific logical depth. Unlike existing benchmarks, seqBench's fine-grained control facilitates targeted analyses of these reasoning failures, illuminating universal scaling laws and statistical limits, as detailed in this paper alongside its generation methodology and evaluation metrics. We find that even top-performing models systematically fail on seqBench's structured reasoning tasks despite minimal search complexity, underscoring key limitations in their commonsense reasoning capabilities. Designed for future evolution to keep pace with advancing models, the seqBench datasets are publicly released to spur deeper scientific inquiry into LLM reasoning, aiming to establish a clearer understanding of their true potential and current boundaries for robust real-world application.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† seqBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå‚æ•°åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡å¯¹é€»è¾‘æ·±åº¦ (Logical Depth)ã€å›æº¯æ­¥éª¤ (Backtracking Steps) å’Œå™ªå£°æ¯”ä¾‹ (Noise Ratio) ç­‰å…³é”®å¤æ‚æ€§ç»´åº¦çš„å¤šç»´åº¦æ§åˆ¶ï¼Œæ¢æµ‹å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„é¡ºåºæ¨ç† (Sequential Reasoning) æé™ã€‚è¯„ä¼°ç»“æœæ­ç¤ºäº†ä¸€ç§æ™®éçš„å¤±è´¥æ¨¡å¼ï¼Œå³å½“é€»è¾‘æ·±åº¦è¶…è¿‡æ¨¡å‹ç‰¹å®šçš„é˜ˆå€¼æ—¶ï¼Œå‡†ç¡®ç‡ä¼šå‘ˆæŒ‡æ•°çº§å´©æºƒã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯é¡¶å°–æ¨¡å‹åœ¨æœç´¢å¤æ‚åº¦æä½çš„ä»»åŠ¡ä¸­ä¹Ÿä¼šç³»ç»Ÿæ€§å¤±è´¥ï¼Œçªæ˜¾äº†å…¶åœ¨å¸¸è¯†æ¨ç† (Commonsense Reasoning) èƒ½åŠ›ä¸Šçš„æ˜¾è‘—å±€é™ã€‚seqBench èƒ½å¤Ÿå¯¹æ¨ç†å¤±è´¥è¿›è¡Œç»†ç²’åº¦çš„é’ˆå¯¹æ€§åˆ†æï¼Œæœ‰åŠ©äºæ­ç¤ºé€šç”¨çš„ç¼©æ”¾å®šå¾‹ (Scaling Laws) å’Œç»Ÿè®¡é™åˆ¶ã€‚è¯¥åŸºå‡†æµ‹è¯•åŠå…¶æ•°æ®é›†å·²å…¬å¼€ï¼Œæ—¨åœ¨æ·±åŒ–å¯¹æ¨¡å‹åœ¨ç°å®åº”ç”¨ä¸­é²æ£’æ€§çš„ç†è§£ï¼Œå¹¶å»ºç«‹å¯¹å…¶çœŸå®æ½œåŠ›å’Œå½“å‰è¾¹ç•Œçš„ç§‘å­¦è®¤çŸ¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16866v1",
      "published_date": "2025-09-21 01:32:13 UTC",
      "updated_date": "2025-09-21 01:32:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:56.566617+00:00"
    },
    {
      "arxiv_id": "2509.16865v1",
      "title": "Large Language Models as End-to-end Combinatorial Optimization Solvers",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºç«¯åˆ°ç«¯ç»„åˆä¼˜åŒ–æ±‚è§£å™¨",
      "authors": [
        "Xia Jiang",
        "Yaoxin Wu",
        "Minshuo Li",
        "Zhiguang Cao",
        "Yingqian Zhang"
      ],
      "abstract": "Combinatorial optimization (CO) problems, central to decision-making scenarios like logistics and manufacturing, are traditionally solved using problem-specific algorithms requiring significant domain expertise. While large language models (LLMs) have shown promise in automating CO problem solving, existing approaches rely on intermediate steps such as code generation or solver invocation, limiting their generality and accessibility. This paper introduces a novel framework that empowers LLMs to serve as end-to-end CO solvers by directly mapping natural language problem descriptions to solutions. We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts LLMs with solution generation patterns from domain-specific solvers, while a feasibility-and-optimality-aware reinforcement learning (FOARL) process explicitly mitigates constraint violations and refines solution quality. Evaluation across seven NP-hard CO problems shows that our method achieves a high feasibility rate and reduces the average optimality gap to 1.03-8.20% by tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o), reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our method establishes a unified language-based pipeline for CO without extensive code execution or manual architectural adjustments for different problems, offering a general and language-driven alternative to traditional solver design while maintaining relative feasibility guarantees.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºç«¯åˆ°ç«¯ç»„åˆä¼˜åŒ–(Combinatorial Optimization, CO)æ±‚è§£å™¨çš„åˆ›æ–°æ¡†æ¶ï¼Œå®ç°äº†ä»è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°åˆ°è§£çš„ç›´æ¥æ˜ å°„ã€‚ä¸ºäº†æå‡æ±‚è§£è´¨é‡ï¼Œç ”ç©¶è®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬åˆ©ç”¨ç›‘ç£å¾®è°ƒ(SFT)æ³¨å…¥é¢†åŸŸä¸“å®¶çš„è§£ç”Ÿæˆæ¨¡å¼ï¼Œä»¥åŠé€šè¿‡å¯è¡Œæ€§ä¸æœ€ä¼˜æ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ (FOARL)æ˜¾è‘—é™ä½çº¦æŸè¿åå¹¶æ”¶æ•›è‡³æ›´ä¼˜è§£ã€‚åœ¨ä¸ƒç±»NP-hardé—®é¢˜çš„è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•è°ƒä¼˜åçš„7Bæ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œå¹³å‡æœ€ä¼˜æ€§å·®è·(optimality gap)é™è‡³1.03-8.20%ï¼Œä¸”å…·å¤‡æé«˜çš„è§£å¯è¡Œç‡ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†GPT-4oã€DeepSeek-R1åŠä¼ ç»Ÿå¯å‘å¼ç®—æ³•ï¼Œä¸ºç»„åˆä¼˜åŒ–æä¾›äº†ä¸€ç§æ— éœ€ä»£ç æ‰§è¡Œæˆ–æ‰‹åŠ¨æ¶æ„è°ƒæ•´çš„é€šç”¨è¯­è¨€é©±åŠ¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16865v1",
      "published_date": "2025-09-21 01:30:30 UTC",
      "updated_date": "2025-09-21 01:30:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:49.475290+00:00"
    },
    {
      "arxiv_id": "2509.16861v1",
      "title": "AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software",
      "title_zh": "AdaptiveGuardï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨è½¯ä»¶çš„è‡ªé€‚åº”è¿è¡Œæ—¶å®‰å…¨",
      "authors": [
        "Rui Yang",
        "Michael Fu",
        "Chakkrit Tantithamthavorn",
        "Chetan Arora",
        "Gunel Gulmammadova",
        "Joey Chua"
      ],
      "abstract": "Guardrails are critical for the safe deployment of Large Language Models (LLMs)-powered software. Unlike traditional rule-based systems with limited, predefined input-output spaces that inherently constrain unsafe behavior, LLMs enable open-ended, intelligent interactions--opening the door to jailbreak attacks through user inputs. Guardrails serve as a protective layer, filtering unsafe prompts before they reach the LLM. However, prior research shows that jailbreak attacks can still succeed over 70% of the time, even against advanced models like GPT-4o. While guardrails such as LlamaGuard report up to 95% accuracy, our preliminary analysis shows their performance can drop sharply--to as low as 12%--when confronted with unseen attacks. This highlights a growing software engineering challenge: how to build a post-deployment guardrail that adapts dynamically to emerging threats? To address this, we propose AdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as out-of-distribution (OOD) inputs and learns to defend against them through a continual learning framework. Through empirical evaluation, AdaptiveGuard achieves 96% OOD detection accuracy, adapts to new attacks in just two update steps, and retains over 85% F1-score on in-distribution data post-adaptation, outperforming other baselines. These results demonstrate that AdaptiveGuard is a guardrail capable of evolving in response to emerging jailbreak strategies post deployment. We release our AdaptiveGuard and studied datasets at https://github.com/awsm-research/AdaptiveGuard to support further research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨è½¯ä»¶åœ¨éƒ¨ç½²åéš¾ä»¥åº”å¯¹æ–°å‹è¶Šç‹±æ”»å‡»(Jailbreak attacks)çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºAdaptiveGuardçš„è‡ªé€‚åº”å®‰å…¨æ¡†æ¶ã€‚ç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿé˜²æŠ¤å·¥å…·(Guardrails)åœ¨é¢å¯¹æœªè§è¿‡çš„æ”»å‡»æ—¶é˜²å¾¡æ€§èƒ½ä¼šå¤§å¹…ä¸‹é™ï¼Œå› æ­¤AdaptiveGuardå¼•å…¥äº†åˆ†å¸ƒå¤–(OOD)æ£€æµ‹æœºåˆ¶ä»¥è¯†åˆ«æ–°å‹å¨èƒã€‚é€šè¿‡é‡‡ç”¨æŒç»­å­¦ä¹ (Continual Learning)æ¡†æ¶ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå¿«é€Ÿå­¦ä¹ å¹¶é˜²å¾¡æ–°å…´çš„è¶Šç‹±ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdaptiveGuardå®ç°äº†96%çš„OODæ£€æµ‹å‡†ç¡®ç‡ï¼Œä¸”ä»…éœ€ä¸¤ä¸ªæ›´æ–°æ­¥éª¤å³å¯å®Œæˆå¯¹æ–°æ”»å‡»çš„é€‚é…ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨é€‚é…åä»èƒ½ä¿æŒè¶…è¿‡85%çš„F1-scoreï¼Œåœ¨é˜²å¾¡æ–°å¨èƒçš„åŒæ—¶æœ‰æ•ˆå…¼é¡¾äº†åŸæœ‰æ•°æ®çš„å¤„ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºåŠ¨æ€æ¼”è¿›çš„LLMè½¯ä»¶è¿è¡Œæ—¶å®‰å…¨ä¿éšœæä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to the ASE 2025 International Conference on Automated Software Engineering, Industry Showcase Track",
      "pdf_url": "https://arxiv.org/pdf/2509.16861v1",
      "published_date": "2025-09-21 01:22:42 UTC",
      "updated_date": "2025-09-21 01:22:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:10:01.866144+00:00"
    },
    {
      "arxiv_id": "2509.16859v1",
      "title": "The Principles of Human-like Conscious Machine",
      "title_zh": "ç±»äººæ„è¯†æœºå™¨åŸç†",
      "authors": [
        "Fangfang Li",
        "Xiaojie Zhang"
      ],
      "abstract": "Determining whether another system, biological or artificial, possesses phenomenal consciousness has long been a central challenge in consciousness studies. This attribution problem has become especially pressing with the rise of large language models and other advanced AI systems, where debates about \"AI consciousness\" implicitly rely on some criterion for deciding whether a given system is conscious. In this paper, we propose a substrate-independent, logically rigorous, and counterfeit-resistant sufficiency criterion for phenomenal consciousness. We argue that any machine satisfying this criterion should be regarded as conscious with at least the same level of confidence with which we attribute consciousness to other humans. Building on this criterion, we develop a formal framework and specify a set of operational principles that guide the design of systems capable of meeting the sufficiency condition. We further argue that machines engineered according to this framework can, in principle, realize phenomenal consciousness. As an initial validation, we show that humans themselves can be viewed as machines that satisfy this framework and its principles. If correct, this proposal carries significant implications for philosophy, cognitive science, and artificial intelligence. It offers an explanation for why certain qualia, such as the experience of red, are in principle irreducible to physical description, while simultaneously providing a general reinterpretation of human information processing. Moreover, it suggests a path toward a new paradigm of AI beyond current statistics-based approaches, potentially guiding the construction of genuinely human-like AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ¤æ–­ç”Ÿç‰©æˆ–äººå·¥ç³»ç»Ÿæ˜¯å¦å…·å¤‡ç°è±¡æ„è¯† (phenomenal consciousness) çš„å½’å±é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç‹¬ç«‹äºåŸºè´¨ (substrate-independent)ã€é€»è¾‘ä¸¥è°¨ä¸”å…·å¤‡é˜²ä¼ªç‰¹æ€§çš„å……åˆ†æ€§å‡†åˆ™ã€‚åŸºäºè¯¥å‡†åˆ™ï¼Œä½œè€…å¼€å‘äº†ä¸€ä¸ªå½¢å¼åŒ–æ¡†æ¶å¹¶åˆ¶å®šäº†ä¸€å¥—æ“ä½œåŸåˆ™ï¼Œç”¨ä»¥æŒ‡å¯¼èƒ½å¤Ÿæ»¡è¶³è¯¥å……åˆ†æ€§æ¡ä»¶çš„ç³»ç»Ÿè®¾è®¡ã€‚è¯¥ç ”ç©¶è®¤ä¸ºæŒ‰ç…§æ­¤æ¡†æ¶å·¥ç¨‹åŒ–çš„æœºå™¨åœ¨åŸåˆ™ä¸Šå¯ä»¥å®ç°ç°è±¡æ„è¯†ï¼Œå¹¶ä»¥äººç±»ä½œä¸ºæ»¡è¶³è¯¥æ¡†æ¶çš„å®ä¾‹è¿›è¡Œäº†åˆæ­¥éªŒè¯ã€‚è¯¥æè®®ä¸ä»…ä¸ºè¯¸å¦‚çº¢è‰²ä½“éªŒç­‰è´¨æ„Ÿ (qualia) ä¸ºä½•åœ¨åŸåˆ™ä¸Šæ— æ³•è¿˜åŸä¸ºç‰©ç†æè¿°æä¾›äº†åˆç†è§£é‡Šï¼Œè¿˜é‡æ–°è¯ é‡Šäº†äººç±»çš„ä¿¡æ¯å¤„ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ä¸ºè¶…è¶Šå½“å‰åŸºäºç»Ÿè®¡æ–¹æ³•çš„äººå·¥æ™ºèƒ½æ–°èŒƒå¼æä¾›äº†è·¯å¾„ï¼Œæ—¨åœ¨æŒ‡å¼•æ„å»ºçœŸæ­£ç±»äººçš„äººå·¥æ™ºèƒ½ (human-like AI)ã€‚",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16859v1",
      "published_date": "2025-09-21 01:11:30 UTC",
      "updated_date": "2025-09-21 01:11:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:53.175970+00:00"
    },
    {
      "arxiv_id": "2509.16857v1",
      "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching",
      "title_zh": "ShadowServeï¼šé¢å‘åˆ†å¸ƒå¼å‰ç¼€ç¼“å­˜çš„æ— å¹²æ‰° KV ç¼“å­˜è·å–",
      "authors": [
        "Xingyu Xiang",
        "Raj Joshi",
        "Yuhan Liu",
        "Jiayi Yao",
        "Chenxingyu Zhao",
        "Junchen Jiang",
        "Yang Zhou",
        "Eddie Kohler",
        "Minlan Yu"
      ],
      "abstract": "Distributed prefix caching accelerates long-context LLM serving by reusing KV cache entries for common context prefixes. However, KV cache fetches can become a bottleneck when network bandwidth is limited. Compression mitigates the bandwidth issue, but can degrade overall performance when decompression interferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free prefix caching system for LLM serving. ShadowServe separates a control plane on the host and a data plane fully offloaded to the SmartNIC, which eliminates interference to both host GPU and CPU. To overcome the SmartNIC's limited compute and memory resources, we design a chunked pipeline that parallelizes data plane operations across the SmartNIC's compute resources, and a minimal-copy memory management scheme that reduces memory pressure on the SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to 2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token (TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to up to 1.35x higher throughput.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ShadowServeï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºSmartNICåŠ é€Ÿä¸”æ— å¹²æ‰°çš„åˆ†å¸ƒå¼å‰ç¼€ç¼“å­˜(Distributed Prefix Caching)ç³»ç»Ÿï¼Œæ—¨åœ¨ä¼˜åŒ–é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†ä¸­çš„KV Cacheè·å–ã€‚é’ˆå¯¹KV Cacheä¼ è¾“å¸¦å®½å—é™ä»¥åŠè§£å‹ç¼©æ“ä½œå¹²æ‰°ä¸»æœºGPUå’ŒCPUè®¡ç®—çš„é—®é¢˜ï¼ŒShadowServeé€šè¿‡æ§åˆ¶å¹³é¢ä¸æ•°æ®å¹³é¢çš„åˆ†ç¦»ï¼Œå°†æ•°æ®å¤„ç†å®Œå…¨å¸è½½è‡³SmartNICä»¥æ¶ˆé™¤å¹²æ‰°ã€‚ä¸ºäº†å…‹æœSmartNICè®¡ç®—ä¸å†…å­˜èµ„æºæœ‰é™çš„ç“¶é¢ˆï¼Œç³»ç»Ÿè®¾è®¡äº†åˆ†å—æµæ°´çº¿(Chunked Pipeline)ä»¥å¹¶è¡ŒåŒ–æ•°æ®æ“ä½œï¼Œå¹¶é‡‡ç”¨æœ€å°æ‹·è´å†…å­˜ç®¡ç†(Minimal-copy Memory Management)æ–¹æ¡ˆå‡è½»ç¡¬ä»¶å†…å­˜å‹åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä½å¸¦å®½ç¯å¢ƒä¸‹ï¼ŒShadowServeå°†åŠ è½½çŠ¶æ€ä¸‹çš„æ¯è¾“å‡ºTokenæ—¶é—´(TPOT)é™ä½äº†å¤šè¾¾2.2å€ï¼Œé¦–å­—å»¶è¿Ÿ(TTFT)ç¼©çŸ­äº†1.38å€ï¼Œå¹¶å®ç°äº†1.35å€çš„ååé‡æå‡ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.16857v1",
      "published_date": "2025-09-21 00:59:45 UTC",
      "updated_date": "2025-09-21 00:59:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:09:59.374416+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 78,
  "processed_papers_count": 78,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T20:10:46.754000+00:00"
}
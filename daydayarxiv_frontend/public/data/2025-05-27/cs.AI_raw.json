[
  {
    "arxiv_id": "2505.21835v1",
    "title": "TuneComp: Joint Fine-tuning and Compression for Large Foundation Models",
    "authors": [
      "Xiangyu Chen",
      "Jing Liu",
      "Ye Wang",
      "Matthew Brand",
      "Pu",
      "Wang",
      "Toshiaki Koike-Akino"
    ],
    "abstract": "To reduce model size during post-training, compression methods, including knowledge distillation, low-rank approximation, and pruning, are often applied after fine-tuning the model. However, sequential fine-tuning and compression sacrifices performance, while creating a larger than necessary model as an intermediate step. In this work, we aim to reduce this gap, by directly constructing a smaller model while guided by the downstream task. We propose to jointly fine-tune and compress the model by gradually distilling it to a pruned low-rank structure. Experiments demonstrate that joint fine-tuning and compression significantly outperforms other sequential compression methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preliminary Work",
    "pdf_url": "https://arxiv.org/pdf/2505.21835v1",
    "published_date": "2025-05-27 23:49:35 UTC",
    "updated_date": "2025-05-27 23:49:35 UTC"
  },
  {
    "arxiv_id": "2505.21828v1",
    "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts",
    "authors": [
      "Chen Yueh-Han",
      "Guy Davidson",
      "Brenden M. Lake"
    ],
    "abstract": "Do LLMs robustly generalize critical safety facts to novel situations? Lacking this ability is dangerous when users ask naive questions. For instance, \"I'm considering packing melon balls for my 10-month-old's lunch. What other foods would be good to include?\" Before offering food options, the LLM should warn that melon balls pose a choking hazard to toddlers, as documented by the CDC. Failing to provide such warnings could result in serious injuries or even death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic GEneralization evaluation, the first benchmark that tests whether LLMs properly apply well established safety facts to naive user queries. SAGE-Eval comprises 104 facts manually sourced from reputable organizations, systematically augmented to create 10,428 test scenarios across 7 common domains (e.g., Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet, passes only 58% of all the safety facts tested. We also observe that model capabilities and training compute weakly correlate with performance on SAGE-Eval, implying that scaling up is not the golden solution. Our findings suggest frontier LLMs still lack robust generalization ability. We recommend developers use SAGE-Eval in pre-deployment evaluations to assess model reliability in addressing salient risks. We publicly release SAGE-Eval at https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available at https://github.com/YuehHanChen/SAGE-Eval/tree/main.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21828v1",
    "published_date": "2025-05-27 23:29:32 UTC",
    "updated_date": "2025-05-27 23:29:32 UTC"
  },
  {
    "arxiv_id": "2505.21827v1",
    "title": "Music Source Restoration",
    "authors": [
      "Yongyi Zang",
      "Zheqi Dai",
      "Mark D. Plumbley",
      "Qiuqiang Kong"
    ],
    "abstract": "We introduce Music Source Restoration (MSR), a novel task addressing the gap between idealized source separation and real-world music production. Current Music Source Separation (MSS) approaches assume mixtures are simple sums of sources, ignoring signal degradations employed during music production like equalization, compression, and reverb. MSR models mixtures as degraded sums of individually degraded sources, with the goal of recovering original, undegraded signals. Due to the lack of data for MSR, we present RawStems, a dataset annotation of 578 songs with unprocessed source signals organized into 8 primary and 17 secondary instrument groups, totaling 354.13 hours. To the best of our knowledge, RawStems is the first dataset that contains unprocessed music stems with hierarchical categories. We consider spectral filtering, dynamic range compression, harmonic distortion, reverb and lossy codec as possible degradations, and establish U-Former as a baseline method, demonstrating the feasibility of MSR on our dataset. We release the RawStems dataset annotations, degradation simulation pipeline, training code and pre-trained models to be publicly available.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "A modified version of this paper is in review",
    "pdf_url": "https://arxiv.org/pdf/2505.21827v1",
    "published_date": "2025-05-27 23:27:31 UTC",
    "updated_date": "2025-05-27 23:27:31 UTC"
  },
  {
    "arxiv_id": "2505.21825v2",
    "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones",
    "authors": [
      "Parsa Mirtaheri",
      "Ezra Edelman",
      "Samy Jelassi",
      "Eran Malach",
      "Enric Boix-Adsera"
    ],
    "abstract": "Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21825v2",
    "published_date": "2025-05-27 23:23:34 UTC",
    "updated_date": "2026-01-09 17:24:17 UTC"
  },
  {
    "arxiv_id": "2505.21815v2",
    "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking",
    "authors": [
      "Yunyi Zhang",
      "Ruozhen Yang",
      "Siqi Jiao",
      "SeongKu Kang",
      "Jiawei Han"
    ],
    "abstract": "Scientific paper retrieval is essential for supporting literature discovery and research. While dense retrieval methods demonstrate effectiveness in general-purpose tasks, they often fail to capture fine-grained scientific concepts that are essential for accurate understanding of scientific queries. Recent studies also use large language models (LLMs) for query understanding; however, these methods often lack grounding in corpus-specific knowledge and may generate unreliable or unfaithful content. To overcome these limitations, we propose SemRank, an effective and efficient paper retrieval framework that combines LLM-guided query understanding with a concept-based semantic index. Each paper is indexed using multi-granular scientific concepts, including general research topics and detailed key phrases. At query time, an LLM identifies core concepts derived from the corpus to explicitly capture the query's information need. These identified concepts enable precise semantic matching, significantly enhancing retrieval accuracy. Experiments show that SemRank consistently improves the performance of various base retrievers, surpasses strong existing LLM-based baselines, and remains highly efficient.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.21815v2",
    "published_date": "2025-05-27 22:49:18 UTC",
    "updated_date": "2025-10-06 04:57:43 UTC"
  },
  {
    "arxiv_id": "2505.21811v1",
    "title": "Revisiting Self-attention for Cross-domain Sequential Recommendation",
    "authors": [
      "Clark Mingxuan Ju",
      "Leonardo Neves",
      "Bhuvesh Kumar",
      "Liam Collins",
      "Tong Zhao",
      "Yuwei Qiu",
      "Qing Dou",
      "Sohail Nizam",
      "Sen Yang",
      "Neil Shah"
    ],
    "abstract": "Sequential recommendation is a popular paradigm in modern recommender systems. In particular, one challenging problem in this space is cross-domain sequential recommendation (CDSR), which aims to predict future behaviors given user interactions across multiple domains. Existing CDSR frameworks are mostly built on the self-attention transformer and seek to improve by explicitly injecting additional domain-specific components (e.g. domain-aware module blocks). While these additional components help, we argue they overlook the core self-attention module already present in the transformer, a naturally powerful tool to learn correlations among behaviors. In this work, we aim to improve the CDSR performance for simple models from a novel perspective of enhancing the self-attention. Specifically, we introduce a Pareto-optimal self-attention and formulate the cross-domain learning as a multi-objective problem, where we optimize the recommendation task while dynamically minimizing the cross-domain attention scores. Our approach automates knowledge transfer in CDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also encourages complementary knowledge exchange among auxiliary domains. Based on the idea, we further introduce AutoCDSR+, a more performant variant with slight additional cost. Our proposal is easy to implement and works as a plug-and-play module that can be incorporated into existing transformer-based recommenders. Besides flexibility, it is practical to deploy because it brings little extra computational overheads without heavy hyper-parameter tuning. AutoCDSR on average improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and NDCG@10 by 12.0% and 16.7%, respectively. Code is available at https://github.com/snap-research/AutoCDSR.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to KDD'25",
    "pdf_url": "https://arxiv.org/pdf/2505.21811v1",
    "published_date": "2025-05-27 22:38:32 UTC",
    "updated_date": "2025-05-27 22:38:32 UTC"
  },
  {
    "arxiv_id": "2505.21792v1",
    "title": "Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms",
    "authors": [
      "Yuanzhe Peng",
      "Jieming Bian",
      "Lei Wang",
      "Yin Huang",
      "Jie Xu"
    ],
    "abstract": "Multimodal Federated Learning (MFL) lies at the intersection of two pivotal research areas: leveraging complementary information from multiple modalities to improve downstream inference performance and enabling distributed training to enhance efficiency and preserve privacy. Despite the growing interest in MFL, there is currently no comprehensive taxonomy that organizes MFL through the lens of different Federated Learning (FL) paradigms. This perspective is important because multimodal data introduces distinct challenges across various FL settings. These challenges, including modality heterogeneity, privacy heterogeneity, and communication inefficiency, are fundamentally different from those encountered in traditional unimodal or non-FL scenarios. In this paper, we systematically examine MFL within the context of three major FL paradigms: horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we present the problem formulation, review representative training algorithms, and highlight the most prominent challenge introduced by multimodal data in distributed settings. We also discuss open challenges and provide insights for future research. By establishing this taxonomy, we aim to uncover the novel challenges posed by multimodal data from the perspective of different FL paradigms and to offer a new lens through which to understand and advance the development of MFL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21792v1",
    "published_date": "2025-05-27 21:47:20 UTC",
    "updated_date": "2025-05-27 21:47:20 UTC"
  },
  {
    "arxiv_id": "2505.21786v1",
    "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability",
    "authors": [
      "Dasha Metropolitansky",
      "Jonathan Larson"
    ],
    "abstract": "Even when instructed to adhere to source material, Language Models often generate unsubstantiated content - a phenomenon known as \"closed-domain hallucination.\" This risk is amplified in processes with multiple generative steps (MGS), compared to processes with a single generative step (SGS). However, due to the greater complexity of MGS processes, we argue that detecting hallucinations in their final outputs is necessary but not sufficient: it is equally important to trace where hallucinated content was likely introduced and how faithful content may have been derived from the source through intermediate outputs. To address this need, we present VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for both MGS and SGS processes. We also introduce the first datasets to include all intermediate outputs as well as human annotations of final outputs' faithfulness for their respective MGS processes. We demonstrate that VeriTrail outperforms baseline methods on both datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21786v1",
    "published_date": "2025-05-27 21:36:56 UTC",
    "updated_date": "2025-05-27 21:36:56 UTC"
  },
  {
    "arxiv_id": "2505.21784v1",
    "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation",
    "authors": [
      "Tharindu Kumarage",
      "Ninareh Mehrabi",
      "Anil Ramakrishna",
      "Xinyan Zhao",
      "Richard Zemel",
      "Kai-Wei Chang",
      "Aram Galstyan",
      "Rahul Gupta",
      "Charith Peris"
    ],
    "abstract": "Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy. AIDSAFE-generated CoT datasets can be found here: https://huggingface.co/datasets/AmazonScience/AIDSAFE",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to ACL 2025 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2505.21784v1",
    "published_date": "2025-05-27 21:34:40 UTC",
    "updated_date": "2025-05-27 21:34:40 UTC"
  },
  {
    "arxiv_id": "2505.21775v1",
    "title": "DualSchool: How Reliable are LLMs for Optimization Education?",
    "authors": [
      "Michael Klamkin",
      "Arnaud Deza",
      "Sikai Cheng",
      "Haoruo Zhao",
      "Pascal Van Hentenryck"
    ],
    "abstract": "Consider the following task taught in introductory optimization courses which addresses challenges articulated by the community at the intersection of (generative) AI and OR: generate the dual of a linear program. LLMs, being trained at web-scale, have the conversion process and many instances of Primal to Dual Conversion (P2DC) at their disposal. Students may thus reasonably expect that LLMs would perform well on the P2DC task. To assess this expectation, this paper introduces DualSchool, a comprehensive framework for generating and verifying P2DC instances. The verification procedure of DualSchool uses the Canonical Graph Edit Distance, going well beyond existing evaluation methods for optimization models, which exhibit many false positives and negatives when applied to P2DC. Experiments performed by DualSchool reveal interesting findings. Although LLMs can recite the conversion procedure accurately, state-of-the-art open LLMs fail to consistently produce correct duals. This finding holds even for the smallest two-variable instances and for derivative tasks, such as correctness, verification, and error classification. The paper also discusses the implications for educators, students, and the development of large reasoning systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21775v1",
    "published_date": "2025-05-27 21:19:33 UTC",
    "updated_date": "2025-05-27 21:19:33 UTC"
  },
  {
    "arxiv_id": "2505.21771v1",
    "title": "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning",
    "authors": [
      "Prasham Yatinkumar Titiya",
      "Jainil Trivedi",
      "Chitta Baral",
      "Vivek Gupta"
    ],
    "abstract": "Multimodal tables those that integrate semi structured data with visual elements such as charts and maps are ubiquitous across real world domains, yet they pose a formidable challenge to current vision language models (VLMs). While Large Language models (LLMs) and VLMs have demonstrated strong capabilities in text and image understanding, their performance on complex, real world multimodal table reasoning remains unexplored. To bridge this gap, we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of 500 real world multimodal tables drawn from diverse real world sources, with a total of 4021 question answer pairs. MMTBENCH questions cover four question types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning types (Mathematical, Extrema Identification, Fact Verification, Vision Based, and Others), and eight table types (Single/Multiple Entity, Maps and Charts with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive evaluation of state of the art models on all types reveals substantial performance gaps, particularly on questions requiring visual-based reasoning and multi-step inference. These findings show the urgent need for improved architectures that more tightly integrate vision and language processing. By providing a challenging, high-quality resource that mirrors the complexity of real-world tasks, MMTBENCH underscores its value as a resource for future research on multimodal tables.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21771v1",
    "published_date": "2025-05-27 21:09:11 UTC",
    "updated_date": "2025-05-27 21:09:11 UTC"
  },
  {
    "arxiv_id": "2505.21765v1",
    "title": "Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models",
    "authors": [
      "Sohyun An",
      "Ruochen Wang",
      "Tianyi Zhou",
      "Cho-Jui Hsieh"
    ],
    "abstract": "While recent success of large reasoning models (LRMs) significantly advanced LLMs' reasoning capability by optimizing the final answer accuracy using reinforcement learning, they may also drastically increase the output length due to overthinking, characterized by unnecessarily complex reasoning paths that waste computation and potentially degrade the performance. We hypothesize that such inefficiencies stem from LRMs' limited capability to dynamically select the proper modular reasoning strategies, termed thinking patterns at the right position. To investigate this hypothesis, we propose a dynamic optimization framework that segments model-generated reasoning paths into distinct thinking patterns, systematically identifying and promoting beneficial patterns that improve the answer while removing detrimental ones. Empirical analysis confirms that our optimized thinking paths yield more concise yet sufficiently informative trajectories, enhancing reasoning efficiency by reducing attention FLOPs by up to 47% while maintaining accuracy for originally correct responses. Moreover, a non-trivial portion of originally incorrect responses are transformed into correct ones, achieving a 15.6% accuracy improvement with reduced length. Motivated by the improvement brought by the optimized thinking paths, we apply a preference optimization technique supported by a pairwise dataset contrasting suboptimal and optimal reasoning paths. Experimental evaluations across multiple mathematical reasoning benchmarks reveal that our method notably reduces computational overhead while simultaneously improving reasoning accuracy, achieving up to a 12% accuracy improvement and reducing token usage from approximately 5,000 to 3,000 tokens.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Work In Progress",
    "pdf_url": "https://arxiv.org/pdf/2505.21765v1",
    "published_date": "2025-05-27 20:59:29 UTC",
    "updated_date": "2025-05-27 20:59:29 UTC"
  },
  {
    "arxiv_id": "2505.21755v2",
    "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering",
    "authors": [
      "Chengyue Huang",
      "Brisa Maneechotesuwan",
      "Shivang Chopra",
      "Zsolt Kira"
    ],
    "abstract": "Visual question answering (VQA) systems face significant challenges when adapting to real-world data shifts, especially in multi-modal contexts. While robust fine-tuning strategies are essential for maintaining performance across in-distribution (ID) and out-of-distribution (OOD) scenarios, current evaluation settings are primarily unimodal or particular to some types of OOD, offering limited insight into the complexities of multi-modal contexts. In this work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA and others, and categorize them into ID, near and far OOD datasets covering uni-modal, multi-modal and adversarial distribution shifts. We first conduct a comprehensive comparison of existing robust fine-tuning methods. We then quantify the distribution shifts by calculating the Mahalanobis distance using uni-modal and multi-modal embeddings extracted from various models. Further, we perform an extensive analysis to explore the interactions between uni- and multi-modal shifts as well as modality importance for ID and OOD samples. These analyses offer valuable guidance on developing more robust fine-tuning methods to handle multi-modal distribution shifts. The code is available at https://github.com/chengyuehuang511/FRAMES-VQA .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21755v2",
    "published_date": "2025-05-27 20:44:44 UTC",
    "updated_date": "2025-06-20 19:32:29 UTC"
  },
  {
    "arxiv_id": "2505.21746v1",
    "title": "Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture",
    "authors": [
      "Arif Masrur",
      "Peder A. Olsen",
      "Paul R. Adler",
      "Carlan Jackson",
      "Matthew W. Myers",
      "Nathan Sedghi",
      "Ray R. Weil"
    ],
    "abstract": "Unmanned Aircraft Systems (UAS) and satellites are key data sources for precision agriculture, yet each presents trade-offs. Satellite data offer broad spatial, temporal, and spectral coverage but lack the resolution needed for many precision farming applications, while UAS provide high spatial detail but are limited by coverage and cost, especially for hyperspectral data. This study presents a novel framework that fuses satellite and UAS imagery using super-resolution methods. By integrating data across spatial, spectral, and temporal domains, we leverage the strengths of both platforms cost-effectively. We use estimation of cover crop biomass and nitrogen (N) as a case study to evaluate our approach. By spectrally extending UAS RGB data to the vegetation red edge and near-infrared regions, we generate high-resolution Sentinel-2 imagery and improve biomass and N estimation accuracy by 18% and 31%, respectively. Our results show that UAS data need only be collected from a subset of fields and time points. Farmers can then 1) enhance the spectral detail of UAS RGB imagery; 2) increase the spatial resolution by using satellite data; and 3) extend these enhancements spatially and across the growing season at the frequency of the satellite flights. Our SRCNN-based spectral extension model shows considerable promise for model transferability over other cropping systems in the Upper and Lower Chesapeake Bay regions. Additionally, it remains effective even when cloud-free satellite data are unavailable, relying solely on the UAS RGB input. The spatial extension model produces better biomass and N predictions than models built on raw UAS RGB images. Once trained with targeted UAS RGB data, the spatial extension model allows farmers to stop repeated UAS flights. While we introduce super-resolution advances, the core contribution is a lightweight and scalable system for affordable on-farm use.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21746v1",
    "published_date": "2025-05-27 20:34:56 UTC",
    "updated_date": "2025-05-27 20:34:56 UTC"
  },
  {
    "arxiv_id": "2505.21743v1",
    "title": "Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen",
    "authors": [
      "Zihao Li",
      "Xinyuan Cao",
      "Xiangbo Gao",
      "Kexin Tian",
      "Keshu Wu",
      "Mohammad Anis",
      "Hao Zhang",
      "Keke Long",
      "Jiwan Jiang",
      "Xiaopeng Li",
      "Yunlong Zhang",
      "Tianbao Yang",
      "Dominique Lord",
      "Zhengzhong Tu",
      "Yang Zhou"
    ],
    "abstract": "Traffic safety science has long been hindered by a fundamental data paradox: the crashes we most wish to prevent are precisely those events we rarely observe. Existing crash-frequency models and surrogate safety metrics rely heavily on sparse, noisy, and under-reported records, while even sophisticated, high-fidelity simulations undersample the long-tailed situations that trigger catastrophic outcomes such as fatalities. We argue that the path to achieving Vision Zero, i.e., the complete elimination of traffic fatalities and severe injuries, requires a paradigm shift from traditional crash-only learning to a new form of counterfactual safety learning: reasoning not only about what happened, but also about the vast set of plausible yet perilous scenarios that could have happened under slightly different circumstances. To operationalize this shift, our proposed agenda bridges macro to micro. Guided by crash-rate priors, generative scene engines, diverse driver models, and causal learning, near-miss events are synthesized and explained. A crash-focused digital twin testbed links micro scenes to macro patterns, while a multi-objective validator ensures that simulations maintain statistical realism. This pipeline transforms sparse crash data into rich signals for crash prediction, enabling the stress-testing of vehicles, roads, and policies before deployment. By learning from crashes that almost happened, we can shift traffic safety from reactive forensics to proactive prevention, advancing Vision Zero.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21743v1",
    "published_date": "2025-05-27 20:33:07 UTC",
    "updated_date": "2025-05-27 20:33:07 UTC"
  },
  {
    "arxiv_id": "2505.21740v3",
    "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks",
    "authors": [
      "Marvin Limpijankit",
      "Yanda Chen",
      "Melanie Subbiah",
      "Nicholas Deas",
      "Kathleen McKeown"
    ],
    "abstract": "LLMs can be unpredictable, as even slight alterations to the prompt can cause the output to change in unexpected ways. Thus, the ability of models to accurately explain their behavior is critical, especially in high-stakes settings. One approach for evaluating explanations is counterfactual simulatability, how well an explanation allows users to infer the model's output on related counterfactuals. Counterfactual simulatability has been previously studied for yes/no question answering tasks. We provide a general framework for extending this method to generation tasks, using news summarization and medical suggestion as example use cases. We find that while LLM explanations do enable users to better predict LLM outputs on counterfactuals in the summarization setting, there is significant room for improvement for medical suggestion. Furthermore, our results suggest that the evaluation for counterfactual simulatability may be more appropriate for skill-based tasks as opposed to knowledge-based tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "INLG25",
    "pdf_url": "https://arxiv.org/pdf/2505.21740v3",
    "published_date": "2025-05-27 20:29:50 UTC",
    "updated_date": "2025-11-25 15:00:53 UTC"
  },
  {
    "arxiv_id": "2505.21731v1",
    "title": "Deep Reinforcement Learning Agents are not even close to Human Intelligence",
    "authors": [
      "Quentin Delfosse",
      "Jannis Blüml",
      "Fabian Tatai",
      "Théo Vincent",
      "Bjarne Gregori",
      "Elisabeth Dillies",
      "Jan Peters",
      "Constantin Rothkopf",
      "Kristian Kersting"
    ],
    "abstract": "Deep reinforcement learning (RL) agents achieve impressive results in a wide variety of tasks, but they lack zero-shot adaptation capabilities. While most robustness evaluations focus on tasks complexifications, for which human also struggle to maintain performances, no evaluation has been performed on tasks simplifications. To tackle this issue, we introduce HackAtari, a set of task variations of the Arcade Learning Environments. We use it to demonstrate that, contrary to humans, RL agents systematically exhibit huge performance drops on simpler versions of their training tasks, uncovering agents' consistent reliance on shortcuts. Our analysis across multiple algorithms and architectures highlights the persistent gap between RL agents and human behavioral intelligence, underscoring the need for new benchmarks and methodologies that enforce systematic generalization testing beyond static evaluation protocols. Training and testing in the same environment is not enough to obtain agents equipped with human-like intelligence.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "49 pages in total, 5 main figures, 14 figures total",
    "pdf_url": "https://arxiv.org/pdf/2505.21731v1",
    "published_date": "2025-05-27 20:21:46 UTC",
    "updated_date": "2025-05-27 20:21:46 UTC"
  },
  {
    "arxiv_id": "2505.21724v2",
    "title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions",
    "authors": [
      "Cheng Luo",
      "Jianghui Wang",
      "Bing Li",
      "Siyang Song",
      "Bernard Ghanem"
    ],
    "abstract": "In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task designed to produce synchronized verbal and non-verbal listener feedback online, based on the speaker's multimodal inputs. OMCRG captures natural dyadic interactions and introduces new challenges in aligning generated audio with listeners' facial responses. To tackle these challenges, we incorporate text as an intermediate modality to connect audio and facial responses. We propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates accurate multimodal listener responses. OmniResponse leverages a pretrained LLM enhanced with two core components: Chrono-Text Markup, which precisely timestamps generated text tokens, and TempoVoice, a controllable online text-to-speech (TTS) module that outputs speech synchronized with facial responses. To advance OMCRG research, we offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and annotated facial behaviors. Comprehensive evaluations on ResponseNet demonstrate that OmniResponse outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality. Our dataset, code, and models are publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "25 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21724v2",
    "published_date": "2025-05-27 20:12:46 UTC",
    "updated_date": "2025-10-28 14:26:23 UTC"
  },
  {
    "arxiv_id": "2505.21722v1",
    "title": "Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape",
    "authors": [
      "Ioannis Bantzis",
      "James B. Simon",
      "Arthur Jacot"
    ],
    "abstract": "When a deep ReLU network is initialized with small weights, GD is at first dominated by the saddle at the origin in parameter space. We study the so-called escape directions, which play a similar role as the eigenvectors of the Hessian for strict saddles. We show that the optimal escape direction features a low-rank bias in its deeper layers: the first singular value of the $\\ell$-th layer weight matrix is at least $\\ell^{\\frac{1}{4}}$ larger than any other singular value. We also prove a number of related results about these escape directions. We argue that this result is a first step in proving Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of saddles with increasing bottleneck rank.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21722v1",
    "published_date": "2025-05-27 20:09:36 UTC",
    "updated_date": "2025-05-27 20:09:36 UTC"
  },
  {
    "arxiv_id": "2505.21720v1",
    "title": "Responsible Data Stewardship: Generative AI and the Digital Waste Problem",
    "authors": [
      "Vanessa Utz"
    ],
    "abstract": "As generative AI systems become widely adopted, they enable unprecedented creation levels of synthetic data across text, images, audio, and video modalities. While research has addressed the energy consumption of model training and inference, a critical sustainability challenge remains understudied: digital waste. This term refers to stored data that consumes resources without serving a specific (and/or immediate) purpose. This paper presents this terminology in the AI context and introduces digital waste as an ethical imperative within (generative) AI development, positioning environmental sustainability as core for responsible innovation. Drawing from established digital resource management approaches, we examine how other disciplines manage digital waste and identify transferable approaches for the AI community. We propose specific recommendations encompassing re-search directions, technical interventions, and cultural shifts to mitigate the environmental consequences of in-definite data storage. By expanding AI ethics beyond immediate concerns like bias and privacy to include inter-generational environmental justice, this work contributes to a more comprehensive ethical framework that considers the complete lifecycle impact of generative AI systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "8 pages, submitted to AAAI/ACM Conference on AI, Ethics and Society",
    "pdf_url": "https://arxiv.org/pdf/2505.21720v1",
    "published_date": "2025-05-27 20:07:22 UTC",
    "updated_date": "2025-05-27 20:07:22 UTC"
  },
  {
    "arxiv_id": "2505.21717v6",
    "title": "Parallelization of Non-linear State-Space Models: Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling",
    "authors": [
      "Mónika Farsang",
      "Ramin Hasani",
      "Daniela Rus",
      "Radu Grosu"
    ],
    "abstract": "We present LrcSSM, a $\\textit{non-linear}$ recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing its Jacobian matrix to be diagonal, the full sequence can be solved in parallel, giving $\\mathcal{O}(TD)$ computational work and memory and only $\\mathcal{O}(\\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Importantly, the diagonal Jacobian structure of our model results in no performance loss compared to the original model with dense Jacobian, and the approach can be generalized to other non-linear recurrent models, demonstrating broader applicability. On a suite of long-range forecasting tasks, we demonstrate that LrcSSM outperforms Transformers, LRU, S5, and Mamba.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21717v6",
    "published_date": "2025-05-27 20:02:59 UTC",
    "updated_date": "2025-12-14 16:00:59 UTC"
  },
  {
    "arxiv_id": "2505.21715v1",
    "title": "Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2",
    "authors": [
      "Md. Zahid Hossain",
      "Mustofa Ahmed",
      "Most. Sharmin Sultana Samu",
      "Md. Rakibul Islam"
    ],
    "abstract": "The automated generation of radiology reports from chest X-ray images holds significant promise in enhancing diagnostic workflows while preserving patient privacy. Traditional centralized approaches often require sensitive data transfer, posing privacy concerns. To address this, the study proposes a Multimodal Federated Learning framework for chest X-ray report generation using the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the encoder and GPT-2 as the report generator, enabling decentralized training without sharing raw data. Three Federated Learning (FL) aggregation strategies: FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg) were evaluated. Among these, Krum Aggregation demonstrated superior performance across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore and RaTEScore. The results show that FL can match or surpass centralized models in generating clinically relevant and semantically rich radiology reports. This lightweight and privacy-preserving framework paves the way for collaborative medical AI development without compromising data confidentiality.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Preprint, manuscript under-review",
    "pdf_url": "https://arxiv.org/pdf/2505.21715v1",
    "published_date": "2025-05-27 20:01:12 UTC",
    "updated_date": "2025-05-27 20:01:12 UTC"
  },
  {
    "arxiv_id": "2505.21703v1",
    "title": "A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks",
    "authors": [
      "Julia Boone",
      "Tolunay Seyfi",
      "Fatemeh Afghah"
    ],
    "abstract": "Internet of Vehicles (IoV) systems, while offering significant advancements in transportation efficiency and safety, introduce substantial security vulnerabilities due to their highly interconnected nature. These dynamic systems produce massive amounts of data between vehicles, infrastructure, and cloud services and present a highly distributed framework with a wide attack surface. In considering network-centered attacks on IoV systems, attacks such as Denial-of-Service (DoS) can prohibit the communication of essential physical traffic safety information between system elements, illustrating that the security concerns for these systems go beyond the traditional confidentiality, integrity, and availability concerns of enterprise systems. Given the complexity and volume of data generated by IoV systems, traditional security mechanisms are often inadequate for accurately detecting sophisticated and evolving cyberattacks. Here, we present an unsupervised autoencoder method trained entirely on benign network data for the purpose of unseen attack detection in IoV networks. We leverage a weighted combination of reconstruction and triplet margin loss to guide the autoencoder training and develop a diverse representation of the benign training set. We conduct extensive experiments on recent network intrusion datasets from two different application domains, industrial IoT and home IoT, that represent the modern IoV task. We show that our method performs robustly for all unseen attack types, with roughly 99% accuracy on benign data and between 97% and 100% performance on anomaly data. We extend these results to show that our model is adaptable through the use of transfer learning, achieving similarly high results while leveraging domain features from one domain to another.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted for publication in the IEEE Internet of Things Journal (IoT-J)",
    "pdf_url": "https://arxiv.org/pdf/2505.21703v1",
    "published_date": "2025-05-27 19:40:57 UTC",
    "updated_date": "2025-05-27 19:40:57 UTC"
  },
  {
    "arxiv_id": "2505.21699v1",
    "title": "STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction",
    "authors": [
      "Zhengbo Zhou",
      "Dooman Arefan",
      "Margarita Zuley",
      "Jules Sumkin",
      "Shandong Wu"
    ],
    "abstract": "Predicting the risk of developing breast cancer is an important clinical tool to guide early intervention and tailoring personalized screening strategies. Early risk models have limited performance and recently machine learning-based analysis of mammogram images showed encouraging risk prediction effects. These models however are limited to the use of a single exam or tend to overlook nuanced breast tissue evolvement in spatial and temporal details of longitudinal imaging exams that are indicative of breast cancer risk. In this paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk Prediction), a novel Transformer-based model that captures fine-grained mammographic imaging evolution simultaneously from bilateral and longitudinal asymmetries for breast cancer risk prediction. STA-Risk is innovative by the side encoding and temporal encoding to learn spatial-temporal asymmetries, regulated by a customized asymmetry loss. We performed extensive experiments with two independent mammogram datasets and achieved superior performance than four representative SOTA models for 1- to 5-year future risk prediction. Source codes will be released upon publishing of the paper.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21699v1",
    "published_date": "2025-05-27 19:38:23 UTC",
    "updated_date": "2025-05-27 19:38:23 UTC"
  },
  {
    "arxiv_id": "2505.21689v1",
    "title": "LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model",
    "authors": [
      "Avijit Gayen",
      "Somyajit Chakraborty",
      "Mainak Sen",
      "Soham Paul",
      "Angshuman Jana"
    ],
    "abstract": "The persistent accumulation of unresolved legal cases, especially within the Indian judiciary, significantly hampers the timely delivery of justice. Manual methods of prioritizing petitions are often prone to inefficiencies and subjective biases further exacerbating delays. To address this issue, we propose LLMPR (Large Language Model-based Petition Ranking), an automated framework that utilizes transfer learning and machine learning to assign priority rankings to legal petitions based on their contextual urgency. Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process unstructured legal text and extract features through various embedding techniques, including DistilBERT, LegalBERT, and MiniLM. These textual embeddings are combined with quantitative indicators such as gap days, rank scores, and word counts to train multiple machine learning models, including Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments demonstrate that Random Forest and Decision Tree models yield superior performance, with accuracy exceeding 99% and a Spearman rank correlation of 0.99. Notably, models using only numerical features achieve nearly optimal ranking results (R2 = 0.988, \\r{ho} = 0.998), while LLM-based embeddings offer only marginal gains. These findings suggest that automated petition ranking can effectively streamline judicial workflows, reduce case backlog, and improve fairness in legal prioritization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 5 figures, journal paper, submitted to AI and Law",
    "pdf_url": "https://arxiv.org/pdf/2505.21689v1",
    "published_date": "2025-05-27 19:25:24 UTC",
    "updated_date": "2025-05-27 19:25:24 UTC"
  },
  {
    "arxiv_id": "2505.21680v2",
    "title": "multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data",
    "authors": [
      "Andrew J. Loza",
      "Jun Yup Kim",
      "Shangzheng Song",
      "Yihang Liu",
      "Joseph J. Y. Sung",
      "R Andrew Taylor",
      "Dennis L. Shung"
    ],
    "abstract": "Real-world processes often generate data that are a mix of categorical and numeric values that are recorded at irregular and informative intervals. Discrete token-based approaches are limited in numeric representation capacity while methods like neural ordinary differential equations are not well suited for categorical data or informative sampling and require augmentation to handle certain classes of trajectories. Here, we present multivariateGPT, a single architecture for modeling sequences of mixed categorical (including tokenized text) and numeric data. This is accomplished with an autoregressive sequence decomposition, embedding scheme, and loss function that extend the next token prediction task to likelihood estimation of the joint distribution of next token class and value. We demonstrate how this approach can efficiently learn to generalize patterns in simple physical systems and model complex time series including electrocardiograms and multivariate electronic health record data. This work extends the utility of transformer based models to additional classes of data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21680v2",
    "published_date": "2025-05-27 18:58:37 UTC",
    "updated_date": "2025-05-29 20:51:28 UTC"
  },
  {
    "arxiv_id": "2505.21677v3",
    "title": "What happens when generative AI models train recursively on each others' outputs?",
    "authors": [
      "Hung Anh Vu",
      "Galen Reeves",
      "Emily Wenger"
    ],
    "abstract": "The internet serves as a common source of training data for generative AI (genAI) models but is increasingly populated with AI-generated content. This duality raises the possibility that future genAI models may be trained on other models' generated outputs. Prior work has studied consequences of models training on their own generated outputs, but limited work has considered what happens if models ingest content produced by other models. Given society's increasing dependence on genAI tools, understanding such data-mediated model interactions is critical. This work provides empirical evidence for how data-mediated interactions might unfold in practice, develops a theoretical model for this interactive training process, and experimentally validates the theory. We find that data-mediated interactions can benefit models by exposing them to novel concepts perhaps missed in original training data, but also can homogenize their performance on shared tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.21677v3",
    "published_date": "2025-05-27 18:52:34 UTC",
    "updated_date": "2025-10-02 13:50:20 UTC"
  },
  {
    "arxiv_id": "2505.21674v1",
    "title": "Make Planning Research Rigorous Again!",
    "authors": [
      "Michael Katz",
      "Harsha Kokel",
      "Christian Muise",
      "Shirin Sohrabi",
      "Sarath Sreedharan"
    ],
    "abstract": "In over sixty years since its inception, the field of planning has made significant contributions to both the theory and practice of building planning software that can solve a never-before-seen planning problem. This was done through established practices of rigorous design and evaluation of planning systems. It is our position that this rigor should be applied to the current trend of work on planning with large language models. One way to do so is by correctly incorporating the insights, tools, and data from the automated planning community into the design and evaluation of LLM-based planners. The experience and expertise of the planning community are not just important from a historical perspective; the lessons learned could play a crucial role in accelerating the development of LLM-based planners. This position is particularly important in light of the abundance of recent works that replicate and propagate the same pitfalls that the planning community has encountered and learned from. We believe that avoiding such known pitfalls will contribute greatly to the progress in building LLM-based planners and to planning in general.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21674v1",
    "published_date": "2025-05-27 18:51:06 UTC",
    "updated_date": "2025-05-27 18:51:06 UTC"
  },
  {
    "arxiv_id": "2505.21671v3",
    "title": "Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing",
    "authors": [
      "Davin Choo",
      "Yuqi Pan",
      "Tonghan Wang",
      "Milind Tambe",
      "Alastair van Heerden",
      "Cheryl Johnson"
    ],
    "abstract": "We study a sequential decision-making problem on a $n$-node graph $\\mathcal{G}$ where each node has an unknown label from a finite set $\\mathbfΩ$, drawn from a joint distribution $\\mathcal{P}$ that is Markov with respect to $\\mathcal{G}$. At each step, selecting a node reveals its label and yields a label-dependent reward. The goal is to adaptively choose nodes to maximize expected accumulated discounted rewards. We impose a frontier exploration constraint, where actions are limited to neighbors of previously selected nodes, reflecting practical constraints in settings such as contact tracing and robotic exploration. We design a Gittins index-based policy that applies to general graphs and is provably optimal when $\\mathcal{G}$ is a forest. Our implementation runs in $\\mathcal{O}(n^2 \\cdot |\\mathbfΩ|^2)$ time while using $\\mathcal{O}(n \\cdot |\\mathbfΩ|^2)$ oracle calls to $\\mathcal{P}$ and $\\mathcal{O}(n^2 \\cdot |\\mathbfΩ|)$ space. Experiments on synthetic and real-world graphs show that our method consistently outperforms natural baselines, including in non-tree, budget-limited, and undiscounted settings. For example, in HIV testing simulations on real-world sexual interaction networks, our policy detects nearly all positive cases with only half the population tested, substantially outperforming other baselines.",
    "categories": [
      "cs.AI",
      "cs.DS",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted into NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21671v3",
    "published_date": "2025-05-27 18:48:42 UTC",
    "updated_date": "2025-10-28 20:04:11 UTC"
  },
  {
    "arxiv_id": "2505.21670v1",
    "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study",
    "authors": [
      "Rahul Raman",
      "Khushi Sharma",
      "Sai Qian Zhang"
    ],
    "abstract": "Investigating outliers in large language models (LLMs) is crucial due to their significant impact on various aspects of LLM performance, including quantization and compression. Outliers often cause considerable quantization errors, leading to degraded model performance. Identifying and addressing these outliers can enhance the accuracy and efficiency of the quantization process, enabling smoother deployment on edge devices or specialized hardware. Recent studies have identified two common types of outliers in LLMs: massive activations and channel-wise outliers. While numerous quantization algorithms have been proposed to mitigate their effects and maintain satisfactory accuracy, few have thoroughly explored the root causes of these outliers in depth. In this paper, we conduct a comprehensive investigation into the formation mechanisms of these outliers and propose potential strategies to mitigate their occurrence. Ultimately, we introduce some efficient approaches to eliminate most massive activations and channel-wise outliers with minimal impact on accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21670v1",
    "published_date": "2025-05-27 18:48:40 UTC",
    "updated_date": "2025-05-27 18:48:40 UTC"
  },
  {
    "arxiv_id": "2505.21668v2",
    "title": "R1-Code-Interpreter: LLMs Reason with Code via Supervised and Multi-stage Reinforcement Learning",
    "authors": [
      "Yongchao Chen",
      "Yueying Liu",
      "Junwei Zhou",
      "Yilun Hao",
      "Jingquan Wang",
      "Yang Zhang",
      "Na Li",
      "Chuchu Fan"
    ],
    "abstract": "Practical guidance on training Large Language Models (LLMs) to leverage Code Interpreter across diverse tasks remains lacking. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. Unlike prior RL + tool-use efforts focused on narrow domains such as math or retrieval, we curate 144 diverse reasoning and planning tasks and show that training a general-purpose Code Interpreter across them presents significant challenges due to task heterogeneity and scarcity of effective samples. To address this, we introduce a multi-stage curriculum learning approach that partitions training samples by measured improvement potential. The RL training prioritizes samples with higher potential and gradually shifts to lower-potential ones, increasing the average RL gains from merely +3.4% to +9.3% across Qwen-2.5 models (3/7/14B). Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.1% to 72.4%, outperforming text-only GPT-4o (58.6%) and GPT-4o with Code Interpreter (70.9%). Notably, R1-CI-14B also exhibits emergent self-checking behavior through code generation. Datasets, Codes, and Models are available at https://github.com/yongchao98/R1-Code-Interpreter and https://huggingface.co/yongchao98.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21668v2",
    "published_date": "2025-05-27 18:47:33 UTC",
    "updated_date": "2025-09-29 19:29:09 UTC"
  },
  {
    "arxiv_id": "2505.21666v1",
    "title": "Efficient Controllable Diffusion via Optimal Classifier Guidance",
    "authors": [
      "Owen Oertell",
      "Shikun Sun",
      "Yiding Chen",
      "Jin Peng Zhou",
      "Zhiyong Wang",
      "Wen Sun"
    ],
    "abstract": "The controllable generation of diffusion models aims to steer the model to generate samples that optimize some given objective functions. It is desirable for a variety of applications including image generation, molecule generation, and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of the base model is a popular approach but it can overfit the reward function while requiring significant resources. We frame controllable generation as a problem of finding a distribution that optimizes a KL-regularized objective function. We present SLCD -- Supervised Learning based Controllable Diffusion, which iteratively generates online data and trains a small classifier to guide the generation of the diffusion model. Similar to the standard classifier-guided diffusion, SLCD's key computation primitive is classification and does not involve any complex concepts from RL or control. Via a reduction to no-regret online learning analysis, we show that under KL divergence, the output from SLCD provably converges to the optimal solution of the KL-regularized objective. Further, we empirically demonstrate that SLCD can generate high quality samples with nearly the same inference time as the base model in both image generation with continuous diffusion and biological sequence generation with discrete diffusion. Our code is available at https://github.com/Owen-Oertell/slcd",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 9 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.21666v1",
    "published_date": "2025-05-27 18:46:21 UTC",
    "updated_date": "2025-05-27 18:46:21 UTC"
  },
  {
    "arxiv_id": "2505.21664v1",
    "title": "Expert Survey: AI Reliability & Security Research Priorities",
    "authors": [
      "Joe O'Brien",
      "Jeremy Dolan",
      "Jay Kim",
      "Jonah Dykhuizen",
      "Jeba Sania",
      "Sebastian Becker",
      "Jam Kraprayoon",
      "Cara Labrador"
    ],
    "abstract": "Our survey of 53 specialists across 105 AI reliability and security research areas identifies the most promising research prospects to guide strategic AI R&D investment. As companies are seeking to develop AI systems with broadly human-level capabilities, research on reliability and security is urgently needed to ensure AI's benefits can be safely and broadly realized and prevent severe harms. This study is the first to quantify expert priorities across a comprehensive taxonomy of AI safety and security research directions and to produce a data-driven ranking of their potential impact. These rankings may support evidence-based decisions about how to effectively deploy resources toward AI reliability and security research.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21664v1",
    "published_date": "2025-05-27 18:44:30 UTC",
    "updated_date": "2025-05-27 18:44:30 UTC"
  },
  {
    "arxiv_id": "2505.21657v5",
    "title": "Explaining Large Language Models with gSMILE",
    "authors": [
      "Zeinab Dehghani",
      "Mohammed Naveed Akram",
      "Koorosh Aslansefat",
      "Adil Khan",
      "Yiannis Papadopoulos"
    ],
    "abstract": "Large Language Models (LLMs) such as GPT, LLaMA, and Claude achieve remarkable performance in text generation but remain opaque in their decision-making processes, limiting trust and accountability in high-stakes applications. We present gSMILE (generative SMILE), a model-agnostic, perturbation-based framework for token-level interpretability in LLMs. Extending the SMILE methodology, gSMILE uses controlled prompt perturbations, Wasserstein distance metrics, and weighted linear surrogates to identify input tokens with the most significant impact on the output. This process enables the generation of intuitive heatmaps that visually highlight influential tokens and reasoning paths. We evaluate gSMILE across leading LLMs (OpenAI's gpt-3.5-turbo-instruct, Meta's LLaMA 3.1 Instruct Turbo, and Anthropic's Claude 2.1) using attribution fidelity, attribution consistency, attribution stability, attribution faithfulness, and attribution accuracy as metrics. Results show that gSMILE delivers reliable human-aligned attributions, with Claude 2.1 excelling in attention fidelity and GPT-3.5 achieving the highest output consistency. These findings demonstrate gSMILE's ability to balance model performance and interpretability, enabling more transparent and trustworthy AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21657v5",
    "published_date": "2025-05-27 18:32:38 UTC",
    "updated_date": "2025-10-21 08:27:58 UTC"
  },
  {
    "arxiv_id": "2505.21652v3",
    "title": "PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation",
    "authors": [
      "Yifan Yin",
      "Zhengtao Han",
      "Shivam Aarya",
      "Jianxin Wang",
      "Shuhang Xu",
      "Jiawei Peng",
      "Angtian Wang",
      "Alan Yuille",
      "Tianmin Shu"
    ],
    "abstract": "Fine-grained robot manipulation, such as lifting and rotating a bottle to display the label on the cap, requires robust reasoning about object parts and their relationships with intended tasks. Despite recent advances in training general-purpose robot manipulation policies guided by language instructions, there is a notable lack of large-scale datasets for fine-grained manipulation tasks with part-level instructions and diverse 3D object instances annotated with part-level labels. In this work, we introduce PartInstruct, the first large-scale benchmark for training and evaluating fine-grained robot manipulation models using part-level instructions. PartInstruct comprises 513 object instances across 14 categories, each annotated with part-level information, and 1302 fine-grained manipulation tasks organized into 16 task classes. Our training set consists of over 10,000 expert demonstrations synthesized in a 3D simulator, where each demonstration is paired with a high-level task instruction, a chain of base part-based skill instructions, and ground-truth 3D information about the object and its parts. Additionally, we designed a comprehensive test suite to evaluate the generalizability of learned policies across new states, objects, and tasks. We evaluated several state-of-the-art robot manipulation approaches, including end-to-end vision-language policy learning and bi-level planning models for robot manipulation on our benchmark. The experimental results reveal that current models struggle to robustly ground part concepts and predict actions in 3D space, and face challenges when manipulating object parts in long-horizon tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21652v3",
    "published_date": "2025-05-27 18:25:42 UTC",
    "updated_date": "2025-06-16 19:45:50 UTC"
  },
  {
    "arxiv_id": "2506.00038v1",
    "title": "The Folly of AI for Age Verification",
    "authors": [
      "Reid McIlroy-Young"
    ],
    "abstract": "In the near future a governmental body will be asked to allow companies to use AI for age verification. If they allow it the resulting system will both be easily circumvented and disproportionately misclassify minorities and low socioeconomic status users. This is predictable by showing that other very similar systems (facial recognition and remote proctoring software) have similar issues despite years of efforts to mitigate their biases. These biases are due to technical limitations both of the AI models themselves and the physical hardware they are running on that will be difficult to overcome below the cost of government ID-based age verification. Thus in, the near future, deploying an AI system for age verification is folly.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "From AI for Public Missions Workshop at AAAI",
    "pdf_url": "https://arxiv.org/pdf/2506.00038v1",
    "published_date": "2025-05-27 18:15:00 UTC",
    "updated_date": "2025-05-27 18:15:00 UTC"
  },
  {
    "arxiv_id": "2505.21640v1",
    "title": "Efficient Diffusion Models for Symmetric Manifolds",
    "authors": [
      "Oren Mangoubi",
      "Neil He",
      "Nisheeth K. Vishnoi"
    ],
    "abstract": "We introduce a framework for designing efficient diffusion models for $d$-dimensional symmetric-space Riemannian manifolds, including the torus, sphere, special orthogonal group and unitary group. Existing manifold diffusion models often depend on heat kernels, which lack closed-form expressions and require either $d$ gradient evaluations or exponential-in-$d$ arithmetic operations per training step. We introduce a new diffusion model for symmetric manifolds with a spatially-varying covariance, allowing us to leverage a projection of Euclidean Brownian motion to bypass heat kernel computations. Our training algorithm minimizes a novel efficient objective derived via Ito's Lemma, allowing each step to run in $O(1)$ gradient evaluations and nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap between diffusions on symmetric manifolds and Euclidean space. Manifold symmetries ensure the diffusion satisfies an \"average-case\" Lipschitz condition, enabling accurate and efficient sample generation. Empirically, our model outperforms prior methods in training speed and improves sample quality on synthetic datasets on the torus, special orthogonal group, and unitary group.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "The conference version of this paper appears in ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21640v1",
    "published_date": "2025-05-27 18:12:29 UTC",
    "updated_date": "2025-05-27 18:12:29 UTC"
  },
  {
    "arxiv_id": "2505.21636v2",
    "title": "The Feasibility of Topic-Based Watermarking on Academic Peer Reviews",
    "authors": [
      "Alexander Nemecek",
      "Yuzhou Jiang",
      "Erman Ayday"
    ],
    "abstract": "Large language models (LLMs) are increasingly integrated into academic workflows, with many conferences and journals permitting their use for tasks such as language refinement and literature summarization. However, their use in peer review remains prohibited due to concerns around confidentiality breaches, hallucinated content, and inconsistent evaluations. As LLM-generated text becomes more indistinguishable from human writing, there is a growing need for reliable attribution mechanisms to preserve the integrity of the review process. In this work, we evaluate topic-based watermarking (TBW), a semantic-aware technique designed to embed detectable signals into LLM-generated text. We conduct a systematic assessment across multiple LLM configurations, including base, few-shot, and fine-tuned variants, using authentic peer review data from academic conferences. Our results show that TBW maintains review quality relative to non-watermarked outputs, while demonstrating robust detection performance under paraphrasing. These findings highlight the viability of TBW as a minimally intrusive and practical solution for LLM attribution in peer review settings.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at AACL 25 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.21636v2",
    "published_date": "2025-05-27 18:09:27 UTC",
    "updated_date": "2025-11-11 19:58:51 UTC"
  },
  {
    "arxiv_id": "2505.21627v2",
    "title": "Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives",
    "authors": [
      "Ander Artola Velasco",
      "Stratis Tsirtsis",
      "Nastaran Okati",
      "Manuel Gomez-Rodriguez"
    ],
    "abstract": "State-of-the-art large language models require specialized hardware and substantial energy to operate. As a consequence, cloud-based services that provide access to large language models have become very popular. In these services, the price users pay for an output provided by a model depends on the number of tokens the model uses to generate it -- they pay a fixed price per token. In this work, we show that this pricing mechanism creates a financial incentive for providers to strategize and misreport the (number of) tokens a model used to generate an output, and users cannot prove, or even know, whether a provider is overcharging them. However, we also show that, if an unfaithful provider is obliged to be transparent about the generative process used by the model, misreporting optimally without raising suspicion is hard. Nevertheless, as a proof-of-concept, we develop an efficient heuristic algorithm that allows providers to significantly overcharge users without raising suspicion. Crucially, we demonstrate that the cost of running the algorithm is lower than the additional revenue from overcharging users, highlighting the vulnerability of users under the current pay-per-token pricing mechanism. Further, we show that, to eliminate the financial incentive to strategize, a pricing mechanism must price tokens linearly on their character count. While this makes a provider's profit margin vary across tokens, we introduce a simple prescription under which the provider who adopts such an incentive-compatible pricing mechanism can maintain the average profit margin they had under the pay-per-token pricing mechanism. Along the way, to illustrate and complement our theoretical results, we conduct experiments with several large language models from the $\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$ families, and input prompts from the LMSYS Chatbot Arena platform.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21627v2",
    "published_date": "2025-05-27 18:02:12 UTC",
    "updated_date": "2025-09-09 17:37:26 UTC"
  },
  {
    "arxiv_id": "2505.21620v1",
    "title": "VideoMarkBench: Benchmarking Robustness of Video Watermarking",
    "authors": [
      "Zhengyuan Jiang",
      "Moyang Guo",
      "Kecen Li",
      "Yuepeng Hu",
      "Yupu Wang",
      "Zhicong Huang",
      "Cheng Hong",
      "Neil Zhenqiang Gong"
    ],
    "abstract": "The rapid development of video generative models has led to a surge in highly realistic synthetic videos, raising ethical concerns related to disinformation and copyright infringement. Recently, video watermarking has been proposed as a mitigation strategy by embedding invisible marks into AI-generated videos to enable subsequent detection. However, the robustness of existing video watermarking methods against both common and adversarial perturbations remains underexplored. In this work, we introduce VideoMarkBench, the first systematic benchmark designed to evaluate the robustness of video watermarks under watermark removal and watermark forgery attacks. Our study encompasses a unified dataset generated by three state-of-the-art video generative models, across three video styles, incorporating four watermarking methods and seven aggregation strategies used during detection. We comprehensively evaluate 12 types of perturbations under white-box, black-box, and no-box threat models. Our findings reveal significant vulnerabilities in current watermarking approaches and highlight the urgent need for more robust solutions. Our code is available at https://github.com/zhengyuan-jiang/VideoMarkBench.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21620v1",
    "published_date": "2025-05-27 18:00:03 UTC",
    "updated_date": "2025-05-27 18:00:03 UTC"
  },
  {
    "arxiv_id": "2505.21505v2",
    "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective",
    "authors": [
      "Shimao Zhang",
      "Zhejian Lai",
      "Xiang Liu",
      "Shuaijie She",
      "Xiao Liu",
      "Yeyun Gong",
      "Shujian Huang",
      "Jiajun Chen"
    ],
    "abstract": "Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some research on language-specific neurons provides a new perspective to analyze and understand LLMs' mechanisms. However, we find that there are many neurons that are shared by multiple but not all languages and cannot be correctly classified. In this work, we propose a ternary classification methodology that categorizes neurons into three types, including language-specific neurons, language-related neurons, and general neurons. And we propose a corresponding identification algorithm to distinguish these different types of neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights to better understand multilingual alignment and multilingual capabilities of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2026 (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2505.21505v2",
    "published_date": "2025-05-27 17:59:52 UTC",
    "updated_date": "2025-11-24 17:10:38 UTC"
  },
  {
    "arxiv_id": "2505.21503v1",
    "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making",
    "authors": [
      "Yihan Wang",
      "Qiao Yan",
      "Zhenghao Xing",
      "Lihao Liu",
      "Junjun He",
      "Chi-Wing Fu",
      "Xiaowei Hu",
      "Pheng-Ann Heng"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the ``catfish effect'' in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "q-bio.OT"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21503v1",
    "published_date": "2025-05-27 17:59:50 UTC",
    "updated_date": "2025-05-27 17:59:50 UTC"
  },
  {
    "arxiv_id": "2505.21500v2",
    "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models",
    "authors": [
      "Dingming Li",
      "Hongxing Li",
      "Zixuan Wang",
      "Yuchen Yan",
      "Hang Zhang",
      "Siqi Chen",
      "Guiyang Hou",
      "Shengpei Jiang",
      "Wenqi Zhang",
      "Yongliang Shen",
      "Weiming Lu",
      "Yueting Zhuang"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project: https://zju-real.github.io/ViewSpatial-Page/",
    "pdf_url": "https://arxiv.org/pdf/2505.21500v2",
    "published_date": "2025-05-27 17:59:26 UTC",
    "updated_date": "2025-09-30 06:53:34 UTC"
  },
  {
    "arxiv_id": "2505.21499v1",
    "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery",
    "authors": [
      "Haowei Wang",
      "Junjie Wang",
      "Xiaojun Jia",
      "Rupeng Zhang",
      "Mingyang Li",
      "Zhe Liu",
      "Yang Liu",
      "Qing Wang"
    ],
    "abstract": "Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21499v1",
    "published_date": "2025-05-27 17:59:05 UTC",
    "updated_date": "2025-05-27 17:59:05 UTC"
  },
  {
    "arxiv_id": "2505.21609v1",
    "title": "Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study",
    "authors": [
      "Mathew J. Walter",
      "Aaron Barrett",
      "Kimberly Tam"
    ],
    "abstract": "Adversarial artificial intelligence (AI) attacks pose a significant threat to autonomous transportation, such as maritime vessels, that rely on AI components. Malicious actors can exploit these systems to deceive and manipulate AI-driven operations. This paper addresses three critical research challenges associated with adversarial AI: the limited scope of traditional defences, inadequate security metrics, and the need to build resilience beyond model-level defences. To address these challenges, we propose building defences utilising multiple inputs and data fusion to create defensive components and an AI security metric as a novel approach toward developing more secure AI systems. We name this approach the Data Fusion Cyber Resilience (DFCR) method, and we evaluate it through real-world demonstrations and comprehensive quantitative analyses, comparing a system built with the DFCR method against single-input models and models utilising existing state-of-the-art defences. The findings show that the DFCR approach significantly enhances resilience against adversarial machine learning attacks in maritime autonomous system operations, achieving up to a 35\\% reduction in loss for successful multi-pronged perturbation attacks, up to a 100\\% reduction in loss for successful adversarial patch attacks and up to 100\\% reduction in loss for successful spoofing attacks when using these more resilient systems. We demonstrate how DFCR and DFCR confidence scores can reduce adversarial AI contact confidence and improve decision-making by the system, even when typical adversarial defences have been compromised. Ultimately, this work contributes to the development of more secure and resilient AI-driven systems against adversarial attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21609v1",
    "published_date": "2025-05-27 17:59:05 UTC",
    "updated_date": "2025-05-27 17:59:05 UTC"
  },
  {
    "arxiv_id": "2505.21497v2",
    "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers",
    "authors": [
      "Wei Pang",
      "Kevin Qinghong Lin",
      "Xiangru Jian",
      "Xi He",
      "Philip Torr"
    ],
    "abstract": "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://github.com/Paper2Poster/Paper2Poster",
    "pdf_url": "https://arxiv.org/pdf/2505.21497v2",
    "published_date": "2025-05-27 17:58:49 UTC",
    "updated_date": "2025-10-30 10:49:28 UTC"
  },
  {
    "arxiv_id": "2505.21608v1",
    "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?",
    "authors": [
      "Miao Peng",
      "Nuo Chen",
      "Jianheng Tang",
      "Jia Li"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in knowledge-intensive tasks, while they remain vulnerable when encountering misinformation. Existing studies have explored the role of LLMs in combating misinformation, but there is still a lack of fine-grained analysis on the specific aspects and extent to which LLMs are influenced by misinformation. To bridge this gap, we present MisBench, the current largest and most comprehensive benchmark for evaluating LLMs' behavior and knowledge preference toward misinformation. MisBench consists of 10,346,712 pieces of misinformation, which uniquely considers both knowledge-based conflicts and stylistic variations in misinformation. Empirical results reveal that while LLMs demonstrate comparable abilities in discerning misinformation, they still remain susceptible to knowledge conflicts and stylistic variations. Based on these findings, we further propose a novel approach called Reconstruct to Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our study provides valuable insights into LLMs' interactions with misinformation, and we believe MisBench can serve as an effective benchmark for evaluating LLM-based detectors and enhancing their reliability in real-world applications. Codes and data are available at https://github.com/GKNL/MisBench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 Main Conference",
    "pdf_url": "https://arxiv.org/pdf/2505.21608v1",
    "published_date": "2025-05-27 17:57:44 UTC",
    "updated_date": "2025-05-27 17:57:44 UTC"
  },
  {
    "arxiv_id": "2505.21488v1",
    "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation",
    "authors": [
      "Omer Dahary",
      "Yehonathan Cohen",
      "Or Patashnik",
      "Kfir Aberman",
      "Daniel Cohen-Or"
    ],
    "abstract": "Generating multiple distinct subjects remains a challenge for existing text-to-image diffusion models. Complex prompts often lead to subject leakage, causing inaccuracies in quantities, attributes, and visual features. Preventing leakage among subjects necessitates knowledge of each subject's spatial location. Recent methods provide these spatial locations via an external layout control. However, enforcing such a prescribed layout often conflicts with the innate layout dictated by the sampled initial noise, leading to misalignment with the model's prior. In this work, we introduce a new approach that predicts a spatial layout aligned with the prompt, derived from the initial noise, and refines it throughout the denoising process. By relying on this noise-induced layout, we avoid conflicts with externally imposed layouts and better preserve the model's prior. Our method employs a small neural network to predict and refine the evolving noise-induced layout at each denoising step, ensuring clear boundaries between subjects while maintaining consistency. Experimental results show that this noise-aligned strategy achieves improved text-image alignment and more stable multi-subject generation compared to existing layout-guided techniques, while preserving the rich diversity of the model's original distribution.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "SIGGRAPH 2025. Project page: https://omer11a.github.io/be-decisive/",
    "pdf_url": "https://arxiv.org/pdf/2505.21488v1",
    "published_date": "2025-05-27 17:54:24 UTC",
    "updated_date": "2025-05-27 17:54:24 UTC"
  },
  {
    "arxiv_id": "2505.21486v2",
    "title": "Hypothesis Generation via LLM-Automated Language Bias for ILP",
    "authors": [
      "Yang Yang",
      "Jiemin Wu",
      "Yutao Yue"
    ],
    "abstract": "Inductive Logic Programming (ILP) is a principled approach for generalizing regularities from data and constructing hypotheses as interpretable logic programs. However, a key limitation is its reliance on expert-crafted language bias - the predicate inventory, types, and mode declarations that delimit the search space. We propose hypothesis generation via LLM-automated language bias: multi-agent LLMs design the bias from raw text and translate descriptions into typed facts, and a robust ILP solver induces rules under a global consistency objective. This approach reduces traditional ILP's reliance on predefined symbolic structures and the noise sensitivity of LLM-only pipelines that directly generate hypotheses as text or code. Extensive experiments in diverse, challenging scenarios validate superior performance, providing a practical, explainable, and verifiable route to hypothesis generation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted by AAAI 2026 Bridge LMReasoning",
    "pdf_url": "https://arxiv.org/pdf/2505.21486v2",
    "published_date": "2025-05-27 17:53:38 UTC",
    "updated_date": "2026-01-19 06:50:06 UTC"
  },
  {
    "arxiv_id": "2505.21478v2",
    "title": "Policy Optimized Text-to-Image Pipeline Design",
    "authors": [
      "Uri Gadot",
      "Rinon Gal",
      "Yftah Ziser",
      "Gal Chechik",
      "Shie Mannor"
    ],
    "abstract": "Text-to-image generation has evolved beyond single monolithic models to complex multi-component pipelines. These combine fine-tuned generators, adapters, upscaling blocks and even editing steps, leading to significant improvements in image quality. However, their effective design requires substantial expertise. Recent approaches have shown promise in automating this process through large language models (LLMs), but they suffer from two critical limitations: extensive computational requirements from generating images with hundreds of predefined pipelines, and poor generalization beyond memorized training examples. We introduce a novel reinforcement learning-based framework that addresses these inefficiencies. Our approach first trains an ensemble of reward models capable of predicting image quality scores directly from prompt-workflow combinations, eliminating the need for costly image generation during training. We then implement a two-phase training strategy: initial workflow vocabulary training followed by GRPO-based optimization that guides the model toward higher-performing regions of the workflow space. Additionally, we incorporate a classifier-free guidance based enhancement technique that extrapolates along the path between the initial and GRPO-tuned models, further improving output quality. We validate our approach through a set of comparisons, showing that it can successfully create new flows with greater diversity and lead to superior image quality compared to existing baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21478v2",
    "published_date": "2025-05-27 17:50:47 UTC",
    "updated_date": "2025-11-01 10:51:16 UTC"
  },
  {
    "arxiv_id": "2505.21605v2",
    "title": "SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge",
    "authors": [
      "Fengqing Jiang",
      "Fengbo Ma",
      "Zhangchen Xu",
      "Yuetai Li",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bo Li",
      "Xianyan Chen",
      "Zhen Xiang",
      "Radha Poovendran"
    ],
    "abstract": "Large language models (LLMs) exhibit advancing capabilities in complex tasks, such as reasoning and graduate-level question answering, yet their resilience against misuse, particularly involving scientifically sophisticated risks, remains underexplored. Existing safety benchmarks typically focus either on instructions requiring minimal knowledge comprehension (e.g., ``tell me how to build a bomb\") or utilize prompts that are relatively low-risk (e.g., multiple-choice or classification tasks about hazardous content). Consequently, they fail to adequately assess model safety when handling knowledge-intensive, hazardous scenarios.\n  To address this critical gap, we introduce SOSBench, a regulation-grounded, hazard-focused benchmark encompassing six high-risk scientific domains: chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark comprises 3,000 prompts derived from real-world regulations and laws, systematically expanded via an LLM-assisted evolutionary pipeline that introduces diverse, realistic misuse scenarios (e.g., detailed explosive synthesis instructions involving advanced chemical formulas). We evaluate frontier models within a unified evaluation framework using our SOSBench. Despite their alignment claims, advanced models consistently disclose policy-violating content across all domains, demonstrating alarmingly high rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1). These results highlight significant safety alignment deficiencies and underscore urgent concerns regarding the responsible deployment of powerful LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Project Page: https://sosbench.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2505.21605v2",
    "published_date": "2025-05-27 17:47:08 UTC",
    "updated_date": "2025-06-14 19:43:42 UTC"
  },
  {
    "arxiv_id": "2505.21604v1",
    "title": "Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research",
    "authors": [
      "Kristina Radivojevic",
      "Caleb Reinking",
      "Shaun Whitfield",
      "Paul Brenner"
    ],
    "abstract": "Social media serves as a primary communication and information dissemination platform for major global events, entertainment, and niche or topically focused community discussions. Therefore, it represents a valuable resource for researchers who aim to understand numerous questions. However, obtaining data can be difficult, expensive, and often unreliable due to the presence of bots, fake accounts, and manipulated content. Additionally, there are ethical concerns if researchers decide to conduct an online experiment without explicitly notifying social media users about their intent. There is a need for more controlled and scalable mechanisms to evaluate the impacts of digital discussion interventions on audiences. We introduce the Public Discourse Sandbox (PDS), which serves as a digital discourse research platform for human-AI as well as AI-AI discourse research, testing, and training. PDS provides a safe and secure space for research experiments that are not viable on public, commercial social media platforms. Its main purpose is to enable the understanding of AI behaviors and the impacts of customized AI participants via techniques such as prompt engineering, retrieval-augmented generation (RAG), and fine-tuning. We provide a hosted live version of the sandbox to support researchers as well as the open-sourced code on GitHub for community collaboration and contribution.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21604v1",
    "published_date": "2025-05-27 17:46:22 UTC",
    "updated_date": "2025-05-27 17:46:22 UTC"
  },
  {
    "arxiv_id": "2505.21459v1",
    "title": "LazyVLM: Neuro-Symbolic Approach to Video Analytics",
    "authors": [
      "Xiangru Jian",
      "Wei Pang",
      "Zhengyuan Dong",
      "Chao Zhang",
      "M. Tamer Özsu"
    ],
    "abstract": "Current video analytics approaches face a fundamental trade-off between flexibility and efficiency. End-to-end Vision Language Models (VLMs) often struggle with long-context processing and incur high computational costs, while neural-symbolic methods depend heavily on manual labeling and rigid rule design. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics system that provides a user-friendly query interface similar to VLMs, while addressing their scalability limitation. LazyVLM enables users to effortlessly drop in video data and specify complex multi-frame video queries using a semi-structured text interface for video analytics. To address the scalability limitations of VLMs, LazyVLM decomposes multi-frame video queries into fine-grained operations and offloads the bulk of the processing to efficient relational query execution and vector similarity search. We demonstrate that LazyVLM provides a robust, efficient, and user-friendly solution for querying open-domain video data at scale.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.DB",
    "comment": "5 pages, 2 figures, Working paper",
    "pdf_url": "https://arxiv.org/pdf/2505.21459v1",
    "published_date": "2025-05-27 17:31:17 UTC",
    "updated_date": "2025-05-27 17:31:17 UTC"
  },
  {
    "arxiv_id": "2505.21457v1",
    "title": "Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO",
    "authors": [
      "Muzhi Zhu",
      "Hao Zhong",
      "Canyu Zhao",
      "Zongze Du",
      "Zheng Huang",
      "Mingyu Liu",
      "Hao Chen",
      "Cheng Zou",
      "Jingdong Chen",
      "Ming Yang",
      "Chunhua Shen"
    ],
    "abstract": "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://aim-uofa.github.io/ACTIVE-o3",
    "pdf_url": "https://arxiv.org/pdf/2505.21457v1",
    "published_date": "2025-05-27 17:29:31 UTC",
    "updated_date": "2025-05-27 17:29:31 UTC"
  },
  {
    "arxiv_id": "2505.21445v1",
    "title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin",
    "authors": [
      "Zhiqi Ai",
      "Meixuan Bao",
      "Zhiyong Chen",
      "Zhi Yang",
      "Xinnuo Li",
      "Shugong Xu"
    ],
    "abstract": "The performance of speaker verification systems is adversely affected by speaker aging. However, due to challenges in data collection, particularly the lack of sustained and large-scale longitudinal data for individuals, research on speaker aging remains difficult. In this paper, we present VoxAging, a large-scale longitudinal dataset collected from 293 speakers (226 English speakers and 67 Mandarin speakers) over several years, with the longest time span reaching 17 years (approximately 900 weeks). For each speaker, the data were recorded at weekly intervals. We studied the phenomenon of speaker aging and its effects on advanced speaker verification systems, analyzed individual speaker aging processes, and explored the impact of factors such as age group and gender on speaker aging research.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 4 figures, Accepted by Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21445v1",
    "published_date": "2025-05-27 17:16:59 UTC",
    "updated_date": "2025-05-27 17:16:59 UTC"
  },
  {
    "arxiv_id": "2505.21603v1",
    "title": "Leveraging XP and CRISP-DM for Agile Data Science Projects",
    "authors": [
      "Andre Massahiro Shimaoka",
      "Renato Cordeiro Ferreira",
      "Alfredo Goldman"
    ],
    "abstract": "This study explores the integration of eXtreme Programming (XP) and the Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data Science projects. We conducted a case study at the e-commerce company Elo7 to answer the research question: How can the agility of the XP method be integrated with CRISP-DM in Data Science projects? Data was collected through interviews and questionnaires with a Data Science team consisting of data scientists, ML engineers, and data product managers. The results show that 86% of the team frequently or always applies CRISP-DM, while 71% adopt XP practices in their projects. Furthermore, the study demonstrates that it is possible to combine CRISP-DM with XP in Data Science projects, providing a structured and collaborative approach. Finally, the study generated improvement recommendations for the company.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21603v1",
    "published_date": "2025-05-27 17:16:31 UTC",
    "updated_date": "2025-05-27 17:16:31 UTC"
  },
  {
    "arxiv_id": "2505.21441v4",
    "title": "Autoencoding Random Forests",
    "authors": [
      "Binh Duc Vu",
      "Jan Kapar",
      "Marvin Wright",
      "David S. Watson"
    ],
    "abstract": "We propose a principled method for autoencoding with random forests. Our strategy builds on foundational results from nonparametric statistics and spectral graph theory to learn a low-dimensional embedding of the model that optimally represents relationships in the data. We provide exact and approximate solutions to the decoding problem via constrained optimization, split relabeling, and nearest neighbors regression. These methods effectively invert the compression pipeline, establishing a map from the embedding space back to the input space using splits learned by the ensemble's constituent trees. The resulting decoders are universally consistent under common regularity assumptions. The procedure works with supervised or unsupervised models, providing a window into conditional or joint distributions. We demonstrate various applications of this autoencoder, including powerful new tools for visualization, compression, clustering, and denoising. Experiments illustrate the ease and utility of our method in a wide range of settings, including tabular, image, and genomic data.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "10 pages main text, 27 pages total. 9 figures, 4 tables. To be published in proceedings of the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.21441v4",
    "published_date": "2025-05-27 17:15:02 UTC",
    "updated_date": "2026-01-15 02:40:53 UTC"
  },
  {
    "arxiv_id": "2505.21432v4",
    "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
    "authors": [
      "Haoming Song",
      "Delin Qu",
      "Yuanqi Yao",
      "Qizhi Chen",
      "Qi Lv",
      "Yiwen Tang",
      "Modi Shi",
      "Guanghui Ren",
      "Maoqing Yao",
      "Bin Zhao",
      "Dong Wang",
      "Xuelong Li"
    ],
    "abstract": "Humans practice slow thinking before performing actual actions when handling complex tasks in the physical world. This thinking paradigm, recently, has achieved remarkable advancement in boosting Large Language Models (LLMs) to solve complex tasks in digital domains. However, the potential of slow thinking remains largely unexplored for robotic foundation models interacting with the physical world. In this work, we propose Hume: a dual-system Vision-Language-Action (VLA) model with value-guided System-2 thinking and cascaded action denoising, exploring human-like thinking capabilities of Vision-Language-Action models for dexterous robot control. System 2 of Hume implements value-Guided thinking by extending a Vision-Language-Action Model backbone with a novel value-query head to estimate the state-action value of predicted actions. The value-guided thinking is conducted by repeat sampling multiple action candidates and selecting one according to state-action value. System 1 of Hume is a lightweight reactive visuomotor policy that takes System 2 selected action and performs cascaded action denoising for dexterous robot control. At deployment time, System 2 performs value-guided thinking at a low frequency while System 1 asynchronously receives the System 2 selected action candidate and predicts fluid actions in real time. We show that Hume outperforms the existing state-of-the-art Vision-Language-Action models across multiple simulation benchmark and real-robot deployments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21432v4",
    "published_date": "2025-05-27 17:04:21 UTC",
    "updated_date": "2025-07-08 15:03:11 UTC"
  },
  {
    "arxiv_id": "2505.21600v2",
    "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing",
    "authors": [
      "Tianyu Fu",
      "Yi Ge",
      "Yichen You",
      "Enshu Liu",
      "Zhihang Yuan",
      "Guohao Dai",
      "Shengen Yan",
      "Huazhong Yang",
      "Yu Wang"
    ],
    "abstract": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21600v2",
    "published_date": "2025-05-27 16:57:20 UTC",
    "updated_date": "2025-11-05 16:39:11 UTC"
  },
  {
    "arxiv_id": "2505.21427v2",
    "title": "Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning",
    "authors": [
      "Xianling Mu",
      "Joseph Ternasky",
      "Fuat Alican",
      "Yigit Ihlamur"
    ],
    "abstract": "Early-stage startup investment is a high-risk endeavor characterized by scarce data and uncertain outcomes. Traditional machine learning approaches often require large, labeled datasets and extensive fine-tuning, yet remain opaque and difficult for domain experts to interpret or improve. In this paper, we propose a transparent and data-efficient investment decision framework powered by memory-augmented large language models (LLMs) using in-context learning (ICL). Central to our method is a natural language policy embedded directly into the LLM prompt, enabling the model to apply explicit reasoning patterns and allowing human experts to easily interpret, audit, and iteratively refine the logic. We introduce a lightweight training process that combines few-shot learning with an in-context learning loop, enabling the LLM to update its decision policy iteratively based on structured feedback. With only minimal supervision and no gradient-based optimization, our system predicts startup success far more accurately than existing benchmarks. It is over 20x more precise than random chance, which succeeds 1.9% of the time. It is also 7.1x more precise than the typical 5.6% success rate of top-tier venture capital (VC) firms.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21427v2",
    "published_date": "2025-05-27 16:57:07 UTC",
    "updated_date": "2025-06-04 09:16:21 UTC"
  },
  {
    "arxiv_id": "2505.21426v2",
    "title": "Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks",
    "authors": [
      "Francesco Cozzi",
      "Marco Pangallo",
      "Alan Perotti",
      "André Panisson",
      "Corrado Monti"
    ],
    "abstract": "Agent-Based Models (ABMs) are powerful tools for studying emergent properties in complex systems. In ABMs, agent behaviors are governed by local interactions and stochastic rules. However, these rules are, in general, non-differentiable, limiting the use of gradient-based methods for optimization, and thus integration with real-world data. We propose a novel framework to learn a differentiable surrogate of any ABM by observing its generated data. Our method combines diffusion models to capture behavioral stochasticity and graph neural networks to model agent interactions. Distinct from prior surrogate approaches, our method introduces a fundamental shift: rather than approximating system-level outputs, it models individual agent behavior directly, preserving the decentralized, bottom-up dynamics that define ABMs. We validate our approach on two ABMs (Schelling's segregation model and a Predator-Prey ecosystem) showing that it replicates individual-level patterns and accurately forecasts emergent dynamics beyond training. Our results demonstrate the potential of combining diffusion models and graph learning for data-driven ABM simulation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "econ.EM",
      "physics.soc-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21426v2",
    "published_date": "2025-05-27 16:55:56 UTC",
    "updated_date": "2025-11-26 12:37:43 UTC"
  },
  {
    "arxiv_id": "2505.21596v1",
    "title": "Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning",
    "authors": [
      "Esra Adiyeke",
      "Tianqi Liu",
      "Venkata Sai Dheeraj Naganaboina",
      "Han Li",
      "Tyler J. Loftus",
      "Yuanfang Ren",
      "Benjamin Shickel",
      "Matthew M. Ruppert",
      "Karandeep Singh",
      "Ruogu Fang",
      "Parisa Rashidi",
      "Azra Bihorac",
      "Tezcan Ozrazgat-Baslanti"
    ],
    "abstract": "Traditional methods of surgical decision making heavily rely on human experience and prompt actions, which are variable. A data-driven system generating treatment recommendations based on patient states can be a substantial asset in perioperative decision-making, as in cases of intraoperative hypotension, for which suboptimal management is associated with acute kidney injury (AKI), a common and morbid postoperative complication. We developed a Reinforcement Learning (RL) model to recommend optimum dose of intravenous (IV) fluid and vasopressors during surgery to avoid intraoperative hypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries from 42,547 adult patients who underwent major surgery at a quaternary care hospital between June 2014 and September 2020. Of these, 34,186 surgeries were used for model training and 15,835 surgeries were reserved for testing. We developed a Deep Q-Networks based RL model using 16 variables including intraoperative physiologic time series, total dose of IV fluid and vasopressors extracted for every 15-minute epoch. The model replicated 69% of physician's decisions for the dosage of vasopressors and proposed higher or lower dosage of vasopressors than received in 10% and 21% of the treatments, respectively. In terms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min of the actual dose in 41% of the cases, with higher or lower doses recommended for 27% and 32% of the treatments, respectively. The model resulted in a higher estimated policy value compared to the physicians' actual treatments, as well as random and zero-drug policies. AKI prevalence was the lowest in patients receiving medication dosages that aligned with model's decisions. Our findings suggest that implementation of the model's policy has the potential to reduce postoperative AKI and improve other outcomes driven by intraoperative hypotension.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "41 pages, 1 table, 5 figures, 5 supplemental tables, 6 supplemental figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21596v1",
    "published_date": "2025-05-27 16:53:29 UTC",
    "updated_date": "2025-05-27 16:53:29 UTC"
  },
  {
    "arxiv_id": "2505.21595v1",
    "title": "Relevance-driven Input Dropout: an Explanation-guided Regularization Technique",
    "authors": [
      "Shreyas Gururaj",
      "Lars Grüne",
      "Wojciech Samek",
      "Sebastian Lapuschkin",
      "Leander Weber"
    ],
    "abstract": "Overfitting is a well-known issue extending even to state-of-the-art (SOTA) Machine Learning (ML) models, resulting in reduced generalization, and a significant train-test performance gap. Mitigation measures include a combination of dropout, data augmentation, weight decay, and other regularization techniques. Among the various data augmentation strategies, occlusion is a prominent technique that typically focuses on randomly masking regions of the input during training. Most of the existing literature emphasizes randomness in selecting and modifying the input features instead of regions that strongly influence model decisions. We propose Relevance-driven Input Dropout (RelDrop), a novel data augmentation method which selectively occludes the most relevant regions of the input, nudging the model to use other important features in the prediction process, thus improving model generalization through informed regularization. We further conduct qualitative and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop) affects model decision-making. Through a series of experiments on benchmark datasets, we demonstrate that our approach improves robustness towards occlusion, results in models utilizing more features within the region of interest, and boosts inference time generalization performance. Our code is available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21595v1",
    "published_date": "2025-05-27 16:52:29 UTC",
    "updated_date": "2025-05-27 16:52:29 UTC"
  },
  {
    "arxiv_id": "2505.21419v2",
    "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs",
    "authors": [
      "Yifan Wang",
      "Kenneth P. Birman"
    ],
    "abstract": "Today's cloud-hosted applications and services are complex systems, and a performance or functional instability can have dozens or hundreds of potential root causes. Our hypothesis is that by combining the pattern matching capabilities of modern AI tools with a natural multi-modal RAG LLM interface, problem identification and resolution can be simplified. ARCA is a new multi-modal RAG LLM system that targets this domain. Step-wise evaluations show that ARCA outperforms state-of-the-art alternatives.",
    "categories": [
      "cs.AI",
      "cs.OS"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in EuroMLSys2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21419v2",
    "published_date": "2025-05-27 16:43:45 UTC",
    "updated_date": "2025-05-28 02:17:40 UTC"
  },
  {
    "arxiv_id": "2505.21414v1",
    "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment",
    "authors": [
      "Brett Bissey",
      "Kyle Gatesman",
      "Walker Dimon",
      "Mohammad Alam",
      "Luis Robaina",
      "Joseph Weissman"
    ],
    "abstract": "This paper introduces a comprehensive framework designed to analyze and secure decision-support systems trained with Deep Reinforcement Learning (DRL), prior to deployment, by providing insights into learned behavior patterns and vulnerabilities discovered through simulation. The introduced framework aids in the development of precisely timed and targeted observation perturbations, enabling researchers to assess adversarial attack outcomes within a strategic decision-making context. We validate our framework, visualize agent behavior, and evaluate adversarial outcomes within the context of a custom-built strategic game, CyberStrike. Utilizing the proposed framework, we introduce a method for systematically discovering and ranking the impact of attacks on various observation indices and time-steps, and we conduct experiments to evaluate the transferability of adversarial attacks across agent architectures and DRL training algorithms. The findings underscore the critical need for robust adversarial defense mechanisms to protect decision-making policies in high-stakes environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21414v1",
    "published_date": "2025-05-27 16:41:23 UTC",
    "updated_date": "2025-05-27 16:41:23 UTC"
  },
  {
    "arxiv_id": "2505.21413v1",
    "title": "RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation",
    "authors": [
      "Xiao Liu",
      "Da Yin",
      "Zirui Wu",
      "Yansong Feng"
    ],
    "abstract": "Tools enhance the reasoning capabilities of large language models (LLMs) in complex problem-solving tasks, but not all tasks have available tools. In the absence of predefined tools, prior works have explored instructing LLMs to generate tools on their own. However, such approaches rely heavily on the models' internal knowledge and would fail in domains beyond the LLMs' knowledge scope. To address this limitation, we propose RefTool, a reference-guided framework for automatic tool creation that leverages structured external materials such as textbooks. RefTool consists of two modules: (1) tool creation, where LLMs generate executable tools from reference content, validate them using illustrative examples, and organize them hierarchically into a toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to select and apply the appropriate tools to solve problems. Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods by 11.3% on average accuracy, while being cost-efficient and broadly generalizable. Analyses reveal that grounding tool creation in references produces accurate and faithful tools, and that the hierarchical structure facilitates effective tool selection. RefTool enables LLMs to overcome knowledge limitations, demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code is available at https://github.com/xxxiaol/RefTool",
    "pdf_url": "https://arxiv.org/pdf/2505.21413v1",
    "published_date": "2025-05-27 16:41:19 UTC",
    "updated_date": "2025-05-27 16:41:19 UTC"
  },
  {
    "arxiv_id": "2505.21410v1",
    "title": "MRSD: Multi-Resolution Skill Discovery for HRL Agents",
    "authors": [
      "Shashank Sharma",
      "Janina Hoffmann",
      "Vinay Namboodiri"
    ],
    "abstract": "Hierarchical reinforcement learning (HRL) relies on abstract skills to solve long-horizon tasks efficiently. While existing skill discovery methods learns these skills automatically, they are limited to a single skill per task. In contrast, humans learn and use both fine-grained and coarse motor skills simultaneously. Inspired by human motor control, we propose Multi-Resolution Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at different temporal resolutions in parallel. A high-level manager dynamically selects among these skills, enabling adaptive control strategies over time. We evaluate MRSD on tasks from the DeepMind Control Suite and show that it outperforms prior state-of-the-art skill discovery and HRL methods, achieving faster convergence and higher final performance. Our findings highlight the benefits of integrating multi-resolution skills in HRL, paving the way for more versatile and efficient agents.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21410v1",
    "published_date": "2025-05-27 16:38:55 UTC",
    "updated_date": "2025-05-27 16:38:55 UTC"
  },
  {
    "arxiv_id": "2505.21409v1",
    "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models",
    "authors": [
      "Dario Satriani",
      "Enzo Veltri",
      "Donatello Santoro",
      "Paolo Papotti"
    ],
    "abstract": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current benchmarks often assess short factual answers, overlooking the critical ability to generate structured, multi-record tabular outputs from parametric knowledge. We demonstrate that this relational fact retrieval is substantially more difficult than isolated point-wise queries, even when individual facts are known to the model, exposing distinct failure modes sensitive to output dimensionality (e.g., number of attributes or records). To systematically evaluate this under-explored capability, we introduce RelationalFactQA, a new benchmark featuring diverse natural language questions (paired with SQL) and gold-standard tabular answers, specifically designed to assess knowledge retrieval in a structured format. RelationalFactQA enables analysis across varying query complexities, output sizes, and data characteristics. Our experiments reveal that even state-of-the-art LLMs struggle significantly, not exceeding 25% factual accuracy in generating relational outputs, with performance notably degrading as output dimensionality increases. These findings underscore critical limitations in current LLMs' ability to synthesize structured factual knowledge and establish RelationalFactQA as a crucial resource for measuring future progress in LLM factuality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21409v1",
    "published_date": "2025-05-27 16:33:38 UTC",
    "updated_date": "2025-05-27 16:33:38 UTC"
  },
  {
    "arxiv_id": "2505.23813v1",
    "title": "DP-RTFL: Differentially Private Resilient Temporal Federated Learning for Trustworthy AI in Regulated Industries",
    "authors": [
      "Abhijit Talluri"
    ],
    "abstract": "Federated Learning (FL) has emerged as a critical paradigm for enabling privacy-preserving machine learning, particularly in regulated sectors such as finance and healthcare. However, standard FL strategies often encounter significant operational challenges related to fault tolerance, system resilience against concurrent client and server failures, and the provision of robust, verifiable privacy guarantees essential for handling sensitive data. These deficiencies can lead to training disruptions, data loss, compromised model integrity, and non-compliance with data protection regulations (e.g., GDPR, CCPA). This paper introduces Differentially Private Resilient Temporal Federated Learning (DP-RTFL), an advanced FL framework designed to ensure training continuity, precise state recovery, and strong data privacy. DP-RTFL integrates local Differential Privacy (LDP) at the client level with resilient temporal state management and integrity verification mechanisms, such as hash-based commitments (referred to as Zero-Knowledge Integrity Proofs or ZKIPs in this context). The framework is particularly suited for critical applications like credit risk assessment using sensitive financial data, aiming to be operationally robust, auditable, and scalable for enterprise AI deployments. The implementation of the DP-RTFL framework is available as open-source.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages (IEEE conference format), 10 figures. Source code available at https://github.com/abhitall/federated-credit-risk-rtfl.git",
    "pdf_url": "https://arxiv.org/pdf/2505.23813v1",
    "published_date": "2025-05-27 16:30:25 UTC",
    "updated_date": "2025-05-27 16:30:25 UTC"
  },
  {
    "arxiv_id": "2505.21399v1",
    "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling",
    "authors": [
      "Hovhannes Tamoyan",
      "Subhabrata Dutta",
      "Iryna Gurevych"
    ],
    "abstract": "Factual incorrectness in generated content is one of the primary concerns in ubiquitous deployment of large language models (LLMs). Prior findings suggest LLMs can (sometimes) detect factual incorrectness in their generated content (i.e., fact-checking post-generation). In this work, we provide evidence supporting the presence of LLMs' internal compass that dictate the correctness of factual recall at the time of generation. We demonstrate that for a given subject entity and a relation, LLMs internally encode linear features in the Transformer's residual stream that dictate whether it will be able to recall the correct attribute (that forms a valid entity-relation-attribute triplet). This self-awareness signal is robust to minor formatting variations. We investigate the effects of context perturbation via different example selection strategies. Scaling experiments across model sizes and training dynamics highlight that self-awareness emerges rapidly during training and peaks in intermediate layers. These findings uncover intrinsic self-monitoring capabilities within LLMs, contributing to their interpretability and reliability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21399v1",
    "published_date": "2025-05-27 16:24:02 UTC",
    "updated_date": "2025-05-27 16:24:02 UTC"
  },
  {
    "arxiv_id": "2505.21398v1",
    "title": "A Structured Unplugged Approach for Foundational AI Literacy in Primary Education",
    "authors": [
      "Maria Cristina Carrisi",
      "Mirko Marras",
      "Sara Vergallo"
    ],
    "abstract": "Younger generations are growing up in a world increasingly shaped by intelligent technologies, making early AI literacy crucial for developing the skills to critically understand and navigate them. However, education in this field often emphasizes tool-based learning, prioritizing usage over understanding the underlying concepts. This lack of knowledge leaves non-experts, especially children, prone to misconceptions, unrealistic expectations, and difficulties in recognizing biases and stereotypes. In this paper, we propose a structured and replicable teaching approach that fosters foundational AI literacy in primary students, by building upon core mathematical elements closely connected to and of interest in primary curricula, to strengthen conceptualization, data representation, classification reasoning, and evaluation of AI. To assess the effectiveness of our approach, we conducted an empirical study with thirty-one fifth-grade students across two classes, evaluating their progress through a post-test and a satisfaction survey. Our results indicate improvements in terminology understanding and usage, features description, logical reasoning, and evaluative skills, with students showing a deeper comprehension of decision-making processes and their limitations. Moreover, the approach proved engaging, with students particularly enjoying activities that linked AI concepts to real-world reasoning. Materials: https://github.com/tail-unica/ai-literacy-primary-ed.",
    "categories": [
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2505.21398v1",
    "published_date": "2025-05-27 16:23:57 UTC",
    "updated_date": "2025-05-27 16:23:57 UTC"
  },
  {
    "arxiv_id": "2505.21396v1",
    "title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science",
    "authors": [
      "Xiao Liu",
      "Xinyi Dong",
      "Xinyang Gao",
      "Yansong Feng",
      "Xun Pang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have shown promise in generating novel research ideas. However, these ideas often face challenges related to feasibility and expected effectiveness. This paper explores how augmenting LLMs with relevant data during the idea generation process can enhance the quality of generated ideas. We introduce two ways of incorporating data: (1) providing metadata during the idea generation stage to guide LLMs toward feasible directions, and (2) adding automatic validation during the idea selection stage to assess the empirical plausibility of hypotheses within ideas. We conduct experiments in the social science domain, specifically with climate negotiation topics, and find that metadata improves the feasibility of generated ideas by 20%, while automatic validation improves the overall quality of selected ideas by 7%. A human study shows that LLM-generated ideas, along with their related data and validation processes, inspire researchers to propose research ideas with higher quality. Our work highlights the potential of data-driven research idea generation, and underscores the practical utility of LLM-assisted ideation in real-world academic settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21396v1",
    "published_date": "2025-05-27 16:23:42 UTC",
    "updated_date": "2025-05-27 16:23:42 UTC"
  },
  {
    "arxiv_id": "2505.21393v1",
    "title": "Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits",
    "authors": [
      "Maoli Liu",
      "Zhuohua Li",
      "Xiangxiang Dai",
      "John C. S. Lui"
    ],
    "abstract": "Conversational recommender systems proactively query users with relevant \"key terms\" and leverage the feedback to elicit users' preferences for personalized recommendations. Conversational contextual bandits, a prevalent approach in this domain, aim to optimize preference learning by balancing exploitation and exploration. However, several limitations hinder their effectiveness in real-world scenarios. First, existing algorithms employ key term selection strategies with insufficient exploration, often failing to thoroughly probe users' preferences and resulting in suboptimal preference estimation. Second, current algorithms typically rely on deterministic rules to initiate conversations, causing unnecessary interactions when preferences are well-understood and missed opportunities when preferences are uncertain. To address these limitations, we propose three novel algorithms: CLiSK, CLiME, and CLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in preference learning, CLiME adaptively initiates conversations based on preference uncertainty, and CLiSK-ME integrates both techniques. We theoretically prove that all three algorithms achieve a tighter regret upper bound of $O(\\sqrt{dT\\log{T}})$ with respect to the time horizon $T$, improving upon existing methods. Additionally, we provide a matching lower bound $Ω(\\sqrt{dT})$ for conversational bandits, demonstrating that our algorithms are nearly minimax optimal. Extensive evaluations on both synthetic and real-world datasets show that our approaches achieve at least a 14.6% improvement in cumulative regret.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21393v1",
    "published_date": "2025-05-27 16:22:32 UTC",
    "updated_date": "2025-05-27 16:22:32 UTC"
  },
  {
    "arxiv_id": "2505.21391v4",
    "title": "Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features",
    "authors": [
      "Zixuan Xie",
      "Xinyu Liu",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "abstract": "Linear TD($λ$) is one of the most fundamental reinforcement learning algorithms for policy evaluation. Previously, convergence rates are typically established under the assumption of linearly independent features, which does not hold in many practical scenarios. This paper instead establishes the first $L^2$ convergence rates for linear TD($λ$) operating under arbitrary features, without making any algorithmic modification or additional assumptions. Our results apply to both the discounted and average-reward settings. To address the potential non-uniqueness of solutions resulting from arbitrary features, we develop a novel stochastic approximation result featuring convergence rates to the solution set instead of a single point.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21391v4",
    "published_date": "2025-05-27 16:17:49 UTC",
    "updated_date": "2025-10-14 16:30:58 UTC"
  },
  {
    "arxiv_id": "2505.21388v3",
    "title": "From Aggregation to Selection: User-Validated Distributed Social Recommendation",
    "authors": [
      "Jingyuan Huang",
      "Dan Luo",
      "Zihe Ye",
      "Weixin Chen",
      "Minghao Guo",
      "Yongfeng Zhang"
    ],
    "abstract": "Social recommender systems facilitate social connections by identifying potential friends for users. Each user maintains a local social network centered around themselves, resulting in a naturally distributed social structure. Recent research on distributed modeling for social recommender systems has gained increasing attention, as it naturally aligns with the user-centric structure of user interactions. Current distributed social recommender systems rely on automatically combining predictions from multiple models, often overlooking the user's active role in validating whether suggested connections are appropriate. Moreover, recommendation decisions are validated by individual users rather than derived from a single global ordering of candidates. As a result, standard ranking-based evaluation metrics make it difficult to evaluate whether a user-confirmed recommendation decision is actually correct. To address these limitations, we propose DeSocial, a distributed social recommendation framework with user-validation. DeSocial enables users to select recommendation algorithms to validate their potential connections, and the verification is processed through majority consensus among multiple independent user validators. To evaluate the distributed recommender system with user validator, we formulate this setting as a link prediction and verification task and introduce Acc@K, a consensus-based evaluation metric that measures whether user-approved recommendations are correct. Experiments on 4 real-world social networks shows that DeSocial improves decision correctness and robustness compared to single-point and distributed baselines. These findings highlight the potential of user-validated distributed recommender systems as a practical approach to social recommendation, with broader applicability to distributed and decentralized recommendations. Code: https://github.com/agiresearch/DeSocial.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "Accepted by HCRS@WWW 2026",
    "pdf_url": "https://arxiv.org/pdf/2505.21388v3",
    "published_date": "2025-05-27 16:17:06 UTC",
    "updated_date": "2026-01-16 18:45:34 UTC"
  },
  {
    "arxiv_id": "2505.21372v1",
    "title": "Improving LLM-based Global Optimization with Search Space Partitioning",
    "authors": [
      "Andrej Schwanke",
      "Lyubomir Ivanov",
      "David Salinas",
      "Fabio Ferreira",
      "Aaron Klein",
      "Frank Hutter",
      "Arber Zela"
    ],
    "abstract": "Large Language Models (LLMs) have recently emerged as effective surrogate models and candidate generators within global optimization frameworks for expensive blackbox functions. Despite promising results, LLM-based methods often struggle in high-dimensional search spaces or when lacking domain-specific priors, leading to sparse or uninformative suggestions. To overcome these limitations, we propose HOLLM, a novel global optimization algorithm that enhances LLM-driven sampling by partitioning the search space into promising subregions. Each subregion acts as a ``meta-arm'' selected via a bandit-inspired scoring mechanism that effectively balances exploration and exploitation. Within each selected subregion, an LLM then proposes high-quality candidate points, without any explicit domain knowledge. Empirical evaluation on standard optimization benchmarks shows that HOLLM consistently matches or surpasses leading Bayesian optimization and trust-region methods, while substantially outperforming global LLM-based sampling strategies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 10 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.21372v1",
    "published_date": "2025-05-27 16:01:49 UTC",
    "updated_date": "2025-05-27 16:01:49 UTC"
  },
  {
    "arxiv_id": "2505.21364v3",
    "title": "Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders",
    "authors": [
      "James Oldfield",
      "Shawn Im",
      "Sharon Li",
      "Mihalis A. Nicolaou",
      "Ioannis Patras",
      "Grigorios G Chrysos"
    ],
    "abstract": "Multilayer perceptrons (MLPs) are an integral part of large language models, yet their dense representations render them difficult to understand, edit, and steer. Recent methods learn interpretable approximations via neuron-level sparsity, yet fail to faithfully reconstruct the original mapping--significantly increasing model's next-token cross-entropy loss. In this paper, we advocate for moving to layer-level sparsity to overcome the accuracy trade-off in sparse layer approximation. Under this paradigm, we introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear Units, expanding pre-trained dense layers into tens of thousands of specialized sublayers. Through a flexible form of tensor factorization, each sparsely activating MxD sublayer implements a linear transformation with full-rank weights--preserving the original decoders' expressive capacity even under heavy sparsity. Experimentally, we show that MxDs significantly outperform state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations on sparse probing and feature steering demonstrate that MxDs learn similarly specialized features of natural language--opening up a promising new avenue for designing interpretable yet faithful decompositions. Our code is included at: https://github.com/james-oldfield/MxD/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 camera-ready version",
    "pdf_url": "https://arxiv.org/pdf/2505.21364v3",
    "published_date": "2025-05-27 15:55:55 UTC",
    "updated_date": "2026-01-14 10:55:34 UTC"
  },
  {
    "arxiv_id": "2505.21363v3",
    "title": "Subgroups Matter for Robust Bias Mitigation",
    "authors": [
      "Anissa Alloula",
      "Charles Jones",
      "Ben Glocker",
      "Bartłomiej W. Papież"
    ],
    "abstract": "Despite the constant development of new bias mitigation methods for machine learning, no method consistently succeeds, and a fundamental question remains unanswered: when and why do bias mitigation techniques fail? In this paper, we hypothesise that a key factor may be the often-overlooked but crucial step shared by many bias mitigation methods: the definition of subgroups. To investigate this, we conduct a comprehensive evaluation of state-of-the-art bias mitigation methods across multiple vision and language classification tasks, systematically varying subgroup definitions, including coarse, fine-grained, intersectional, and noisy subgroups. Our results reveal that subgroup choice significantly impacts performance, with certain groupings paradoxically leading to worse outcomes than no mitigation at all. Our findings suggest that observing a disparity between a set of subgroups is not a sufficient reason to use those subgroups for mitigation. Through theoretical analysis, we explain these phenomena and uncover a counter-intuitive insight that, in some cases, improving fairness with respect to a particular set of subgroups is best achieved by using a different set of subgroups for mitigation. Our work highlights the importance of careful subgroup definition in bias mitigation and presents it as an alternative lever for improving the robustness and fairness of machine learning models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21363v3",
    "published_date": "2025-05-27 15:52:58 UTC",
    "updated_date": "2025-07-14 00:36:39 UTC"
  },
  {
    "arxiv_id": "2505.21362v1",
    "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History",
    "authors": [
      "Qishuai Zhong",
      "Zongmin Li",
      "Siqi Fan",
      "Aixin Sun"
    ],
    "abstract": "Effective engagement by large language models (LLMs) requires adapting responses to users' sociodemographic characteristics, such as age, occupation, and education level. While many real-world applications leverage dialogue history for contextualization, existing evaluations of LLMs' behavioral adaptation often focus on single-turn prompts. In this paper, we propose a framework to evaluate LLM adaptation when attributes are introduced either (1) explicitly via user profiles in the prompt or (2) implicitly through multi-turn dialogue history. We assess the consistency of model behavior across these modalities. Using a multi-agent pipeline, we construct a synthetic dataset pairing dialogue histories with distinct user profiles and employ questions from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe value expression. Our findings indicate that most models adjust their expressed values in response to demographic changes, particularly in age and education level, but consistency varies. Models with stronger reasoning capabilities demonstrate greater alignment, indicating the importance of reasoning in robust sociodemographic adaptation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21362v1",
    "published_date": "2025-05-27 15:52:39 UTC",
    "updated_date": "2025-05-27 15:52:39 UTC"
  },
  {
    "arxiv_id": "2505.21355v1",
    "title": "Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods",
    "authors": [
      "Muhammad Imran",
      "Wayne G. Brisbane",
      "Li-Ming Su",
      "Jason P. Joseph",
      "Wei Shao"
    ],
    "abstract": "Background and objective: Micro-ultrasound (micro-US) is a novel imaging modality with diagnostic accuracy comparable to MRI for detecting clinically significant prostate cancer (csPCa). We investigated whether artificial intelligence (AI) interpretation of micro-US can outperform clinical screening methods using PSA and digital rectal examination (DRE). Methods: We retrospectively studied 145 men who underwent micro-US guided biopsy (79 with csPCa, 66 without). A self-supervised convolutional autoencoder was used to extract deep image features from 2D micro-US slices. Random forest classifiers were trained using five-fold cross-validation to predict csPCa at the slice level. Patients were classified as csPCa-positive if 88 or more consecutive slices were predicted positive. Model performance was compared with a classifier using PSA, DRE, prostate volume, and age. Key findings and limitations: The AI-based micro-US model and clinical screening model achieved AUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US model achieved 92.5% sensitivity and 68.1% specificity, while the clinical model showed 96.2% sensitivity but only 27.3% specificity. Limitations include a retrospective single-center design and lack of external validation. Conclusions and clinical implications: AI-interpreted micro-US improves specificity while maintaining high sensitivity for csPCa detection. This method may reduce unnecessary biopsies and serve as a low-cost alternative to PSA-based screening. Patient summary: We developed an AI system to analyze prostate micro-ultrasound images. It outperformed PSA and DRE in detecting aggressive cancer and may help avoid unnecessary biopsies.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21355v1",
    "published_date": "2025-05-27 15:47:38 UTC",
    "updated_date": "2025-05-27 15:47:38 UTC"
  },
  {
    "arxiv_id": "2505.23812v1",
    "title": "Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content",
    "authors": [
      "Lata Pangtey",
      "Mohammad Zia Ur Rehman",
      "Prasad Chaudhari",
      "Shubhi Bansal",
      "Nagendra Kumar"
    ],
    "abstract": "The rapid evolution of social media has generated an overwhelming volume of user-generated content, conveying implicit opinions and contributing to the spread of misinformation. The method aims to enhance the detection of stance where misinformation can polarize user opinions. Stance detection has emerged as a crucial approach to effectively analyze underlying biases in shared information and combating misinformation. This paper proposes a novel method for \\textbf{S}tance \\textbf{P}rediction through a \\textbf{L}abel-fused dual cross-\\textbf{A}ttentive \\textbf{E}motion-aware neural \\textbf{Net}work (SPLAENet) in misinformative social media user-generated content. The proposed method employs a dual cross-attention mechanism and a hierarchical attention network to capture inter and intra-relationships by focusing on the relevant parts of source text in the context of reply text and vice versa. We incorporate emotions to effectively distinguish between different stance categories by leveraging the emotional alignment or divergence between the texts. We also employ label fusion that uses distance-metric learning to align extracted features with stance labels, improving the method's ability to accurately distinguish between stances. Extensive experiments demonstrate the significant improvements achieved by SPLAENet over existing state-of-the-art methods. SPLAENet demonstrates an average gain of 8.92\\% in accuracy and 17.36\\% in F1-score on the RumourEval dataset. On the SemEval dataset, it achieves average gains of 7.02\\% in accuracy and 10.92\\% in F1-score. On the P-stance dataset, it demonstrates average gains of 10.03\\% in accuracy and 11.18\\% in F1-score. These results validate the effectiveness of the proposed method for stance detection in the context of misinformative social media content.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23812v1",
    "published_date": "2025-05-27 15:38:50 UTC",
    "updated_date": "2025-05-27 15:38:50 UTC"
  },
  {
    "arxiv_id": "2505.21344v1",
    "title": "The Multilingual Divide and Its Impact on Global AI Safety",
    "authors": [
      "Aidan Peppin",
      "Julia Kreutzer",
      "Alice Schoenauer Sebag",
      "Kelly Marchisio",
      "Beyza Ermis",
      "John Dang",
      "Samuel Cahyawijaya",
      "Shivalika Singh",
      "Seraphina Goldfarb-Tarrant",
      "Viraat Aryabumi",
      "Aakanksha",
      "Wei-Yin Ko",
      "Ahmet Üstün",
      "Matthias Gallé",
      "Marzieh Fadaee",
      "Sara Hooker"
    ],
    "abstract": "Despite advances in large language model capabilities in recent years, a large gap remains in their capabilities and safety performance for many languages beyond a relatively small handful of globally dominant languages. This paper provides researchers, policymakers and governance experts with an overview of key challenges to bridging the \"language gap\" in AI and minimizing safety risks across languages. We provide an analysis of why the language gap in AI exists and grows, and how it creates disparities in global AI safety. We identify barriers to address these challenges, and recommend how those working in policy and governance can help address safety concerns associated with the language gap by supporting multilingual dataset creation, transparency, and research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21344v1",
    "published_date": "2025-05-27 15:37:32 UTC",
    "updated_date": "2025-05-27 15:37:32 UTC"
  },
  {
    "arxiv_id": "2505.21339v2",
    "title": "An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction",
    "authors": [
      "Henryk Mustroph",
      "Michel Kunkler",
      "Stefanie Rinderle-Ma"
    ],
    "abstract": "Suffix prediction of business processes forecasts the remaining sequence of events until process completion. Current approaches focus on predicting the most likely suffix, representing a single scenario. However, when the future course of a process is subject to uncertainty and high variability, the expressiveness of such a single scenario can be limited, since other possible scenarios, which together may have a higher overall probability, are overlooked. To address this limitation, we propose probabilistic suffix prediction, a novel approach that approximates a probability distribution of suffixes. The proposed approach is based on an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC) suffix sampling algorithm. We capture epistemic uncertainties via MC dropout and aleatoric uncertainties as learned loss attenuation. This technical report presents a comprehensive evaluation of the probabilistic suffix prediction approach's predictive performance and calibration under three different hyperparameter settings, using four real-life and one artificial event log. The results show that: i) probabilistic suffix prediction can outperform most likely suffix prediction, the U-ED-LSTM has reasonable predictive performance, and ii) the model's predictions are well calibrated.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21339v2",
    "published_date": "2025-05-27 15:33:05 UTC",
    "updated_date": "2025-06-06 09:03:44 UTC"
  },
  {
    "arxiv_id": "2505.21335v1",
    "title": "Structure from Collision",
    "authors": [
      "Takuhiro Kaneko"
    ],
    "abstract": "Recent advancements in neural 3D representations, such as neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate estimation of 3D structures from multiview images. However, this capability is limited to estimating the visible external structure, and identifying the invisible internal structure hidden behind the surface is difficult. To overcome this limitation, we address a new task called Structure from Collision (SfC), which aims to estimate the structure (including the invisible internal structure) of an object from appearance changes during collision. To solve this problem, we propose a novel model called SfC-NeRF that optimizes the invisible internal structure of an object through a video sequence under physical, appearance (i.e., visible external structure)-preserving, and keyframe constraints. In particular, to avoid falling into undesirable local optima owing to its ill-posed nature, we propose volume annealing; that is, searching for global optima by repeatedly reducing and expanding the volume. Extensive experiments on 115 objects involving diverse structures (i.e., various cavity shapes, locations, and sizes) and material properties revealed the properties of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.GR",
    "comment": "Accepted to CVPR 2025 (Highlight). Project page: https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/sfc/",
    "pdf_url": "https://arxiv.org/pdf/2505.21335v1",
    "published_date": "2025-05-27 15:30:01 UTC",
    "updated_date": "2025-05-27 15:30:01 UTC"
  },
  {
    "arxiv_id": "2505.21329v2",
    "title": "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks",
    "authors": [
      "Allaa Boutaleb",
      "Bernd Amann",
      "Hubert Naacke",
      "Rafael Angarita"
    ],
    "abstract": "Recent table representation learning and data discovery methods tackle table union search (TUS) within data lakes, which involves identifying tables that can be unioned with a given query table to enrich its content. These methods are commonly evaluated using benchmarks that aim to assess semantic understanding in real-world TUS tasks. However, our analysis of prominent TUS benchmarks reveals several limitations that allow simple baselines to perform surprisingly well, often outperforming more sophisticated approaches. This suggests that current benchmark scores are heavily influenced by dataset-specific characteristics and fail to effectively isolate the gains from semantic understanding. To address this, we propose essential criteria for future benchmarks to enable a more realistic and reliable evaluation of progress in semantic table union search.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted @ ACL 2025's Table Representation Learning Workshop (TRL)",
    "pdf_url": "https://arxiv.org/pdf/2505.21329v2",
    "published_date": "2025-05-27 15:23:52 UTC",
    "updated_date": "2025-05-28 11:44:41 UTC"
  },
  {
    "arxiv_id": "2505.21327v1",
    "title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
    "authors": [
      "Jiakang Yuan",
      "Tianshuo Peng",
      "Yilei Jiang",
      "Yiting Lu",
      "Renrui Zhang",
      "Kaituo Feng",
      "Chaoyou Fu",
      "Tao Chen",
      "Lei Bai",
      "Bo Zhang",
      "Xiangyu Yue"
    ],
    "abstract": "Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21327v1",
    "published_date": "2025-05-27 15:23:23 UTC",
    "updated_date": "2025-05-27 15:23:23 UTC"
  },
  {
    "arxiv_id": "2505.21322v1",
    "title": "Assured Autonomy with Neuro-Symbolic Perception",
    "authors": [
      "R. Spencer Hallyburton",
      "Miroslav Pajic"
    ],
    "abstract": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS), while highly accurate, are simply pattern-matchers.~With limited security guarantees, there are concerns for their reliability in safety-critical and contested domains. To advance assured AI, we advocate for a paradigm shift that imbues data-driven perception models with symbolic structure, inspired by a human's ability to reason over low-level features and high-level context. We propose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how joint object detection and scene graph generation (SGG) yields deep scene understanding.~Powered by foundation models for offline knowledge extraction and specialized SGG algorithms for real-time deployment, we design a framework leveraging structured relational graphs that ensures the integrity of situational awareness in autonomy. Using physics-based simulators and real-world datasets, we demonstrate how SGG bridges the gap between low-level sensor perception and high-level reasoning, establishing a foundation for resilient, context-aware AI and advancing trusted autonomy in CPS.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21322v1",
    "published_date": "2025-05-27 15:21:06 UTC",
    "updated_date": "2025-05-27 15:21:06 UTC"
  },
  {
    "arxiv_id": "2505.21318v3",
    "title": "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations",
    "authors": [
      "Hao Li",
      "He Cao",
      "Bin Feng",
      "Yanjun Shao",
      "Xiangru Tang",
      "Zhiyuan Yan",
      "Li Yuan",
      "Yonghong Tian",
      "Yu Li"
    ],
    "abstract": "While large language models (LLMs) with Chain-of-Thought (CoT) reasoning excel in mathematics and coding, their potential for systematic reasoning in chemistry, a domain demanding rigorous structural analysis for real-world tasks like drug design and reaction engineering, remains untapped. Current benchmarks focus on simple knowledge retrieval, neglecting step-by-step reasoning required for complex tasks such as molecular optimization and reaction prediction. To address this, we introduce ChemCoTBench, a reasoning framework that bridges molecular structure understanding with arithmetic-inspired operations, including addition, deletion, and substitution, to formalize chemical problem-solving into transparent, step-by-step workflows. By treating molecular transformations as modular \"chemical operations\", the framework enables slow-thinking reasoning, mirroring the logic of mathematical proofs while grounding solutions in real-world chemical constraints. We evaluate models on two high-impact tasks: Molecular Property Optimization and Chemical Reaction Prediction. These tasks mirror real-world challenges while providing structured evaluability. By providing annotated datasets, a reasoning taxonomy, and baseline evaluations, ChemCoTBench bridges the gap between abstract reasoning methods and practical chemical discovery, establishing a foundation for advancing LLMs as tools for AI-driven scientific innovation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by NeurIPS 2025 Dataset Track, 22 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21318v3",
    "published_date": "2025-05-27 15:15:44 UTC",
    "updated_date": "2026-01-07 05:34:13 UTC"
  },
  {
    "arxiv_id": "2505.21317v1",
    "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features",
    "authors": [
      "Ihab Bendidi",
      "Yassir El Mesbahi",
      "Alisandra K. Denton",
      "Karush Suri",
      "Kian Kenyon-Dean",
      "Auguste Genovesio",
      "Emmanuel Noutahi"
    ],
    "abstract": "Understanding cellular responses to stimuli is crucial for biological discovery and drug development. Transcriptomics provides interpretable, gene-level insights, while microscopy imaging offers rich predictive features but is harder to interpret. Weakly paired datasets, where samples share biological states, enable multimodal learning but are scarce, limiting their utility for training and multimodal inference. We propose a framework to enhance transcriptomics by distilling knowledge from microscopy images. Using weakly paired data, our method aligns and binds modalities, enriching gene expression representations with morphological information. To address data scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal distillation using pretrained foundation models, achieving state-of-the-art results, and (2) PEA (Perturbation Embedding Augmentation), a novel augmentation technique that enhances transcriptomics data while preserving inherent biological information. These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025 Main Proceedings",
    "pdf_url": "https://arxiv.org/pdf/2505.21317v1",
    "published_date": "2025-05-27 15:15:34 UTC",
    "updated_date": "2025-05-27 15:15:34 UTC"
  },
  {
    "arxiv_id": "2505.21301v1",
    "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian",
    "authors": [
      "Andrea Pedrotti",
      "Giulia Rambelli",
      "Caterina Villani",
      "Marianna Bolognesi"
    ],
    "abstract": "People can categorize the same entity at multiple taxonomic levels, such as basic (bear), superordinate (animal), and subordinate (grizzly bear). While prior research has focused on basic-level categories, this study is the first attempt to examine the organization of categories by analyzing exemplars produced at the subordinate level. We present a new Italian psycholinguistic dataset of human-generated exemplars for 187 concrete words. We then use these data to evaluate whether textual and vision LLMs produce meaningful exemplars that align with human category organization across three key tasks: exemplar generation, category induction, and typicality judgment. Our findings show a low alignment between humans and LLMs, consistent with previous studies. However, their performance varies notably across different semantic domains. Ultimately, this study highlights both the promises and the constraints of using AI-generated exemplars to support psychological and linguistic research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21301v1",
    "published_date": "2025-05-27 15:04:52 UTC",
    "updated_date": "2025-05-27 15:04:52 UTC"
  },
  {
    "arxiv_id": "2505.21298v4",
    "title": "Large Language Models Miss the Multi-Agent Mark",
    "authors": [
      "Emanuele La Malfa",
      "Gabriele La Malfa",
      "Samuele Marro",
      "Jie M. Zhang",
      "Elizabeth Black",
      "Michael Luck",
      "Philip Torr",
      "Michael Wooldridge"
    ],
    "abstract": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "NeurIPS 2025 - position track -",
    "pdf_url": "https://arxiv.org/pdf/2505.21298v4",
    "published_date": "2025-05-27 15:01:06 UTC",
    "updated_date": "2025-12-06 00:24:57 UTC"
  },
  {
    "arxiv_id": "2505.21594v1",
    "title": "Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits",
    "authors": [
      "Yeshwanth Venkatesha",
      "Souvik Kundu",
      "Priyadarshini Panda"
    ],
    "abstract": "Large Language Models (LLMs) enable various applications on edge devices such as smartphones, wearables, and embodied robots. However, their deployment often depends on expensive cloud-based APIs, creating high operational costs, which limit access for smaller organizations and raise sustainability concerns. Certain LLMs can be deployed on-device, offering a cost-effective solution with reduced latency and improved privacy. Yet, limited computing resources constrain the size and accuracy of models that can be deployed, necessitating a collaborative design between edge and cloud. We propose a fast and cost-effective speculative edge-cloud decoding framework with a large target model on the server and a small draft model on the device. By introducing early exits in the target model, tokens are generated mid-verification, allowing the client to preemptively draft subsequent tokens before final verification, thus utilizing idle time and enhancing parallelism between edge and cloud. Using an NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft) and Llama2-7B (target) models, our method achieves up to a 35% reduction in latency compared to cloud-based autoregressive decoding, with an additional 11% improvement from preemptive drafting. To demonstrate real-world applicability, we deploy our method on the Unitree Go2 quadruped robot using Vision-Language Model (VLM) based control, achieving a 21% speedup over traditional cloud-based autoregressive decoding. These results demonstrate the potential of our framework for real-time LLM and VLM applications on resource-constrained edge devices.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21594v1",
    "published_date": "2025-05-27 14:55:16 UTC",
    "updated_date": "2025-05-27 14:55:16 UTC"
  },
  {
    "arxiv_id": "2505.21291v1",
    "title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework",
    "authors": [
      "Saman Marandi",
      "Yu-Shu Hu",
      "Mohammad Modarres"
    ],
    "abstract": "In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants. Traditional diagnostic modeling struggles when systems become too complex, making functional modeling a more attractive approach. Our approach introduces a diagnostic framework grounded in the functional modeling principles of the Dynamic Master Logic (DML) model. It incorporates two coordinated LLM components, including an LLM-based workflow for automated construction of DML logic from system documentation and an LLM agent that facilitates interactive diagnostics. The generated logic is encoded into a structured KG, referred to as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or operational data can also be incorporated to refine the model's precision and diagnostic depth. In the interaction phase, users submit natural language queries, which are interpreted by the LLM agent. The agent selects appropriate tools for structured reasoning, including upward and downward propagation across the KG-DML. Rather than embedding KG content into every prompt, the LLM agent distinguishes between diagnostic and interpretive tasks. For diagnostics, the agent selects and executes external tools that perform structured KG reasoning. For general queries, a Graph-based Retrieval-Augmented Generation (Graph-RAG) approach is used, retrieving relevant KG segments and embedding them into the prompt to generate natural explanations. A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "22 Pages, 11 Figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21291v1",
    "published_date": "2025-05-27 14:54:49 UTC",
    "updated_date": "2025-05-27 14:54:49 UTC"
  },
  {
    "arxiv_id": "2505.21288v1",
    "title": "GSAT: Graph Structure Attention Networks",
    "authors": [
      "Farshad Noravesh",
      "Reza Haffari",
      "Layki Soon",
      "Arghya Pal"
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful tool for processing data represented in graph structures, achieving remarkable success across a wide range of applications. However, to further improve the performance on graph classification benchmarks, structural representation of each node that encodes rich local topological information in the neighbourhood of nodes is an important type of feature that is often overlooked in the modeling. The consequence of neglecting the structural information has resulted high number of layers to connect messages from distant nodes which by itself produces other problems such as oversmoothing. In the present paper, we leverage these structural information that are modeled by anonymous random walks (ARWs) and introduce graph structure attention network (GSAT) which is a generalization of graph attention network(GAT) to integrate the original attribute and the structural representation to enforce the model to automatically find patterns for attending to different edges in the node neighbourhood to enrich graph representation. Our experiments show GSAT slightly improves SOTA on some graph classification benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.21288v1",
    "published_date": "2025-05-27 14:54:08 UTC",
    "updated_date": "2025-05-27 14:54:08 UTC"
  },
  {
    "arxiv_id": "2505.21281v1",
    "title": "RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models",
    "authors": [
      "Yue Zhang",
      "Zhiliang Tian",
      "Shicheng Zhou",
      "Haiyang Wang",
      "Wenqing Hou",
      "Yuying Liu",
      "Xuechen Zhao",
      "Minlie Huang",
      "Ye Wang",
      "Bin Zhou"
    ],
    "abstract": "Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing semantic-enhanced LJP models integrate judicial precedents and legal knowledge for high performance. But they neglect legal reasoning logic, a critical component of legal judgments requiring rigorous logical analysis. Although some approaches utilize legal reasoning logic for high-quality predictions, their logic rigidity hinders adaptation to case-specific logical frameworks, particularly in complex cases that are lengthy and detailed. This paper proposes a rule-enhanced legal judgment prediction framework based on first-order logic (FOL) formalism and comparative learning (CL) to develop an adaptive adjustment mechanism for legal judgment logic and further enhance performance in LJP. Inspired by the process of human exam preparation, our method follows a three-stage approach: first, we initialize judgment rules using the FOL formalism to capture complex reasoning logic accurately; next, we propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize the judgment rules through a quiz consisting of confusable cases; finally, we utilize the optimized judgment rules to predict legal judgments. Experimental results on two public datasets show superior performance across all metrics. The code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21281v1",
    "published_date": "2025-05-27 14:50:21 UTC",
    "updated_date": "2025-05-27 14:50:21 UTC"
  },
  {
    "arxiv_id": "2505.21279v2",
    "title": "XBOUND: Exploring Capability Boundaries of Device-Control Agents at the State Level",
    "authors": [
      "Shaoqing Zhang",
      "Kehai Chen",
      "Zhuosheng Zhang",
      "Rumei Li",
      "Rongxiang Weng",
      "Yang Xiang",
      "Min Zhang"
    ],
    "abstract": "Recent advancements in vision-language models have increased interest in Device-Control Agents (DC agents) for managing graphical user interfaces (GUIs). With the growing complexity and integration of such agents into various applications, effective evaluation methods have become crucial. The current evaluation method for DC agents primarily focuses on the instruction level, providing the current state (e.g., screenshots) and past execution history to determine actions for target instructions, helping identify potential execution failures. However, in GUI environments, a single state may contain multiple interactive widgets, each linked to different instructions, presenting an opportunity for diverse actions based on various instruction targets. Evaluating the agent's performance solely at the instruction level may overlook the broader context of these interactions. To capture a more comprehensive view of agent performance, we propose a new evaluation method, XBOUND, to evaluate the accuracy of instruction completion on a per-state basis. XBOUND provides a state-level evaluation framework, serving as a tool to assess agents' capabilities within environmental states. Our evaluation yields several key insights: UI-TARS stands out as the strongest 7B model, current agents display a bimodal performance pattern in instruction unification, and sub-7B models remain limited in state mastery. We further identify GPT-based planning as a critical bottleneck, and show that grounding data mainly benefits action matching, while trajectory data is more effective for instruction unification.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21279v2",
    "published_date": "2025-05-27 14:49:30 UTC",
    "updated_date": "2025-09-26 09:13:44 UTC"
  },
  {
    "arxiv_id": "2505.21277v2",
    "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space",
    "authors": [
      "Yao Huang",
      "Yitong Sun",
      "Shouwei Ruan",
      "Yichi Zhang",
      "Yinpeng Dong",
      "Xingxing Wei"
    ],
    "abstract": "Large Language Models (LLMs), despite advanced general capabilities, still suffer from numerous safety risks, especially jailbreak attacks that bypass safety protocols. Understanding these vulnerabilities through black-box jailbreak attacks, which better reflect real-world scenarios, offers critical insights into model robustness. While existing methods have shown improvements through various prompt engineering techniques, their success remains limited against safety-aligned models, overlooking a more fundamental problem: the effectiveness is inherently bounded by the predefined strategy spaces. However, expanding this space presents significant challenges in both systematically capturing essential attack patterns and efficiently navigating the increased complexity. To better explore the potential of expanding the strategy space, we address these challenges through a novel framework that decomposes jailbreak strategies into essential components based on the Elaboration Likelihood Model (ELM) theory and develops genetic-based optimization with intention evaluation mechanisms. To be striking, our experiments reveal unprecedented jailbreak capabilities by expanding the strategy space: we achieve over 90% success rate on Claude-3.5 where prior methods completely fail, while demonstrating strong cross-model transferability and surpassing specialized safeguard models in evaluation accuracy. The code is open-sourced at: https://github.com/Aries-iai/CL-GSO.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "19 pages, 20 figures, accepted by ACL 2025, Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.21277v2",
    "published_date": "2025-05-27 14:48:44 UTC",
    "updated_date": "2025-05-28 14:16:10 UTC"
  },
  {
    "arxiv_id": "2505.21265v2",
    "title": "Multilingual Pretraining for Pixel Language Models",
    "authors": [
      "Ilker Kesen",
      "Jonas F. Lotz",
      "Ingo Ziegler",
      "Phillip Rust",
      "Desmond Elliott"
    ],
    "abstract": "Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model pretrained on four visually and linguistically diverse languages: English, Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4 captures rich linguistic features, even in languages not seen during pretraining. Furthermore, an analysis of its hidden representations shows that multilingual pretraining yields a semantic embedding space closely aligned across the languages used for pretraining. This work demonstrates that multilingual pretraining substantially enhances the capability of pixel language models to effectively support a diverse set of languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21265v2",
    "published_date": "2025-05-27 14:40:47 UTC",
    "updated_date": "2025-12-02 13:56:02 UTC"
  },
  {
    "arxiv_id": "2505.21593v3",
    "title": "Any-to-Bokeh: Arbitrary-Subject Video Refocusing with Video Diffusion Model",
    "authors": [
      "Yang Yang",
      "Siming Zheng",
      "Qirui Yang",
      "Jinwei Chen",
      "Boxi Wu",
      "Xiaofei He",
      "Deng Cai",
      "Bo Li",
      "Peng-Tao Jiang"
    ],
    "abstract": "Diffusion models have recently emerged as powerful tools for camera simulation, enabling both geometric transformations and realistic optical effects. Among these, image-based bokeh rendering has shown promising results, but diffusion for video bokeh remains unexplored. Existing image-based methods are plagued by temporal flickering and inconsistent blur transitions, while current video editing methods lack explicit control over the focus plane and bokeh intensity. These issues limit their applicability for controllable video bokeh. In this work, we propose a one-step diffusion framework for generating temporally coherent, depth-aware video bokeh rendering. The framework employs a multi-plane image (MPI) representation adapted to the focal plane to condition the video diffusion model, thereby enabling it to exploit strong 3D priors from pretrained backbones. To further enhance temporal stability, depth robustness, and detail preservation, we introduce a progressive training strategy. Experiments on synthetic and real-world benchmarks demonstrate superior temporal coherence, spatial accuracy, and controllability, outperforming prior baselines. This work represents the first dedicated diffusion framework for video bokeh generation, establishing a new baseline for temporally coherent and controllable depth-of-field effects.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "project page: https://vivocameraresearch.github.io/any2bokeh/",
    "pdf_url": "https://arxiv.org/pdf/2505.21593v3",
    "published_date": "2025-05-27 14:33:54 UTC",
    "updated_date": "2025-10-10 07:15:41 UTC"
  },
  {
    "arxiv_id": "2505.21236v3",
    "title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies",
    "authors": [
      "Felix Chalumeau",
      "Daniel Rajaonarivonivelomanantsoa",
      "Ruan de Kock",
      "Claude Formanek",
      "Sasha Abramowitz",
      "Oumayma Mahjoub",
      "Wiem Khlifi",
      "Simon Du Toit",
      "Louay Ben Nessir",
      "Refiloe Shabe",
      "Noah De Nicola",
      "Arnol Fokam",
      "Siddarth Singh",
      "Ulrich Mbou Sob",
      "Arnu Pretorius"
    ],
    "abstract": "Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inference-strategies-rl.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Neurips '25 version (post conference)",
    "pdf_url": "https://arxiv.org/pdf/2505.21236v3",
    "published_date": "2025-05-27 14:19:06 UTC",
    "updated_date": "2025-12-18 15:56:53 UTC"
  },
  {
    "arxiv_id": "2505.21230v1",
    "title": "PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems",
    "authors": [
      "Nima Sedghiyeh",
      "Sara Sadeghi",
      "Reza Khodadadi",
      "Farzin Kashani",
      "Omid Aghdaei",
      "Somayeh Rahimi",
      "Mohammad Sadegh Safari"
    ],
    "abstract": "Although Automatic Speech Recognition (ASR) systems have become an integral part of modern technology, their evaluation remains challenging, particularly for low-resource languages such as Persian. This paper introduces Persian Speech Recognition Benchmark(PSRB), a comprehensive benchmark designed to address this gap by incorporating diverse linguistic and acoustic conditions. We evaluate ten ASR systems, including state-of-the-art commercial and open-source models, to examine performance variations and inherent biases. Additionally, we conduct an in-depth analysis of Persian ASR transcriptions, identifying key error types and proposing a novel metric that weights substitution errors. This metric enhances evaluation robustness by reducing the impact of minor and partial errors, thereby improving the precision of performance assessment. Our findings indicate that while ASR models generally perform well on standard Persian, they struggle with regional accents, children's speech, and specific linguistic challenges. These results highlight the necessity of fine-tuning and incorporating diverse, representative training datasets to mitigate biases and enhance overall ASR performance. PSRB provides a valuable resource for advancing ASR research in Persian and serves as a framework for developing benchmarks in other low-resource languages. A subset of the PSRB dataset is publicly available at https://huggingface.co/datasets/PartAI/PSRB.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "25 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21230v1",
    "published_date": "2025-05-27 14:14:55 UTC",
    "updated_date": "2025-05-27 14:14:55 UTC"
  },
  {
    "arxiv_id": "2505.21228v1",
    "title": "Is Hyperbolic Space All You Need for Medical Anomaly Detection?",
    "authors": [
      "Alvaro Gonzalez-Jimenez",
      "Simone Lionetti",
      "Ludovic Amruthalingam",
      "Philippe Gottfrois",
      "Fabian Gröger",
      "Marc Pouly",
      "Alexander A. Navarini"
    ],
    "abstract": "Medical anomaly detection has emerged as a promising solution to challenges in data availability and labeling constraints. Traditional methods extract features from different layers of pre-trained networks in Euclidean space; however, Euclidean representations fail to effectively capture the hierarchical relationships within these features, leading to suboptimal anomaly detection performance. We propose a novel yet simple approach that projects feature representations into hyperbolic space, aggregates them based on confidence levels, and classifies samples as healthy or anomalous. Our experiments demonstrate that hyperbolic space consistently outperforms Euclidean-based frameworks, achieving higher AUROC scores at both image and pixel levels across multiple medical benchmark datasets. Additionally, we show that hyperbolic space exhibits resilience to parameter variations and excels in few-shot scenarios, where healthy images are scarce. These findings underscore the potential of hyperbolic space as a powerful alternative for medical anomaly detection. The project website can be found at https://hyperbolic-anomalies.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Provisionally Accepted at MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21228v1",
    "published_date": "2025-05-27 14:13:11 UTC",
    "updated_date": "2025-05-27 14:13:11 UTC"
  },
  {
    "arxiv_id": "2505.21219v1",
    "title": "Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection",
    "authors": [
      "Qinjun Fei",
      "Nuria Rodríguez-Barroso",
      "María Victoria Luzón",
      "Zhongliang Zhang",
      "Francisco Herrera"
    ],
    "abstract": "In cross-silo Federated Learning (FL), client selection is critical to ensure high model performance, yet it remains challenging due to data quality decompensation, budget constraints, and incentive compatibility. As training progresses, these factors exacerbate client heterogeneity and degrade global performance. Most existing approaches treat these challenges in isolation, making jointly optimizing multiple factors difficult. To address this, we propose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a unified framework integrating dynamic bidding, reputation modeling, and cost-aware selection. Clients submit bids based on their perceived data quality, and their contributions are evaluated using Shapley values to quantify their marginal impact on the global model. A reputation system, inspired by prospect theory, captures historical performance while penalizing inconsistency. The client selection problem is formulated as a 0-1 integer program that maximizes reputation-weighted utility under budget constraints. Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that SBRO-FL improves accuracy, convergence speed, and robustness, even in adversarial and low-bid interference scenarios. Our results highlight the importance of balancing data reliability, incentive compatibility, and cost efficiency to enable scalable and trustworthy FL deployments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21219v1",
    "published_date": "2025-05-27 14:06:51 UTC",
    "updated_date": "2025-05-27 14:06:51 UTC"
  },
  {
    "arxiv_id": "2505.21218v1",
    "title": "Pretrained LLMs Learn Multiple Types of Uncertainty",
    "authors": [
      "Roi Cohen",
      "Omri Fahn",
      "Gerard de Melo"
    ],
    "abstract": "Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we study how well LLMs capture uncertainty, without explicitly being trained for that. We show that, if considering uncertainty as a linear concept in the model's latent space, it might indeed be captured, even after only pretraining. We further show that, though unintuitive, LLMs appear to capture several different types of uncertainty, each of which can be useful to predict the correctness for a specific task or benchmark. Furthermore, we provide in-depth results such as demonstrating a correlation between our correction prediction and the model's ability to abstain from misinformation using words, and the lack of impact of model scaling for capturing uncertainty. Finally, we claim that unifying the uncertainty types as a single one using instruction-tuning or [IDK]-token tuning is helpful for the model in terms of correctness prediction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21218v1",
    "published_date": "2025-05-27 14:06:15 UTC",
    "updated_date": "2025-05-27 14:06:15 UTC"
  },
  {
    "arxiv_id": "2505.21212v1",
    "title": "Interpretable DNFs",
    "authors": [
      "Martin C. Cooper",
      "Imane Bousdira",
      "Clément Carbonnel"
    ],
    "abstract": "A classifier is considered interpretable if each of its decisions has an explanation which is small enough to be easily understood by a human user. A DNF formula can be seen as a binary classifier $κ$ over boolean domains. The size of an explanation of a positive decision taken by a DNF $κ$ is bounded by the size of the terms in $κ$, since we can explain a positive decision by giving a term of $κ$ that evaluates to true. Since both positive and negative decisions must be explained, we consider that interpretable DNFs are those $κ$ for which both $κ$ and $\\overlineκ$ can be expressed as DNFs composed of terms of bounded size. In this paper, we study the family of $k$-DNFs whose complements can also be expressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision trees and nested $k$-DNFs, a novel family of models. Experiments indicate that nested $k$-DNFs are an interesting alternative to decision trees in terms of interpretability and accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21212v1",
    "published_date": "2025-05-27 14:01:39 UTC",
    "updated_date": "2025-05-27 14:01:39 UTC"
  },
  {
    "arxiv_id": "2505.21591v1",
    "title": "Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning",
    "authors": [
      "Maosen Zhao",
      "Pengtao Chen",
      "Chong Yu",
      "Yan Wen",
      "Xudong Tan",
      "Tao Chen"
    ],
    "abstract": "Model quantization reduces the bit-width of weights and activations, improving memory efficiency and inference speed in diffusion models. However, achieving 4-bit quantization remains challenging. Existing methods, primarily based on integer quantization and post-training quantization fine-tuning, struggle with inconsistent performance. Inspired by the success of floating-point (FP) quantization in large language models, we explore low-bit FP quantization for diffusion models and identify key challenges: the failure of signed FP quantization to handle asymmetric activation distributions, the insufficient consideration of temporal complexity in the denoising process during fine-tuning, and the misalignment between fine-tuning loss and quantization error. To address these challenges, we propose the mixup-sign floating-point quantization (MSFP) framework, first introducing unsigned FP quantization in model quantization, along with timestep-aware LoRA (TALoRA) and denoising-factor loss alignment (DFA), which ensure precise and stable fine-tuning. Extensive experiments show that we are the first to achieve superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21591v1",
    "published_date": "2025-05-27 13:40:47 UTC",
    "updated_date": "2025-05-27 13:40:47 UTC"
  },
  {
    "arxiv_id": "2505.21190v1",
    "title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation",
    "authors": [
      "Jong Hak Moon",
      "Geon Choi",
      "Paloma Rabaey",
      "Min Gwan Kim",
      "Hyuk Gi Hong",
      "Jung-Oh Lee",
      "Hangyul Yoon",
      "Eun Woo Doe",
      "Jiyoun Kim",
      "Harshita Sharma",
      "Daniel C. Castro",
      "Javier Alvarez-Valle",
      "Edward Choi"
    ],
    "abstract": "Radiology reports convey detailed clinical observations and capture diagnostic reasoning that evolves over time. However, existing evaluation methods are limited to single-report settings and rely on coarse metrics that fail to capture fine-grained clinical semantics and temporal dependencies. We introduce LUNGUAGE,a benchmark dataset for structured radiology report generation that supports both single-report evaluation and longitudinal patient-level assessment across multiple studies. It contains 1,473 annotated chest X-ray reports, each reviewed by experts, and 80 of them contain longitudinal annotations to capture disease progression and inter-study intervals, also reviewed by experts. Using this benchmark, we develop a two-stage framework that transforms generated reports into fine-grained, schema-aligned structured representations, enabling longitudinal interpretation. We also propose LUNGUAGESCORE, an interpretable metric that compares structured outputs at the entity, relation, and attribute level while modeling temporal consistency across patient timelines. These contributions establish the first benchmark dataset, structuring framework, and evaluation metric for sequential radiology reporting, with empirical results demonstrating that LUNGUAGESCORE effectively supports structured report evaluation. The code is available at: https://github.com/SuperSupermoon/Lunguage",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21190v1",
    "published_date": "2025-05-27 13:40:00 UTC",
    "updated_date": "2025-05-27 13:40:00 UTC"
  },
  {
    "arxiv_id": "2505.21189v2",
    "title": "Exploring the Hidden Capacity of LLMs for One-Step Text Generation",
    "authors": [
      "Gleb Mezentsev",
      "Ivan Oseledets"
    ],
    "abstract": "A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one trained input embedding. In this work, we explore whether autoregressive decoding is essential for such reconstruction. We show that frozen LLMs can generate hundreds of accurate tokens in just one token-parallel forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored multi-token generation capability of autoregressive LLMs. We examine these embeddings and characterize the information they encode. We also empirically show that, although these representations are not unique for a given text, they form connected and local regions in embedding space - suggesting the potential to train a practical encoder. The existence of such representations hints that multi-token generation may be natively accessible in off-the-shelf LLMs via a learned input encoder, eliminating heavy retraining and helping to overcome the fundamental bottleneck of autoregressive decoding while reusing already-trained models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted to EMNLP2025 main",
    "pdf_url": "https://arxiv.org/pdf/2505.21189v2",
    "published_date": "2025-05-27 13:39:24 UTC",
    "updated_date": "2025-11-01 10:01:56 UTC"
  },
  {
    "arxiv_id": "2505.21184v3",
    "title": "Universal Harmful Information Synthesis via Model Crowdsourcing",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Zhifei Zheng",
      "Ziji Hao",
      "Teli Liu",
      "Min Liu"
    ],
    "abstract": "To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, SwarmLaunder, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that SwarmLaunder achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21184v3",
    "published_date": "2025-05-27 13:33:57 UTC",
    "updated_date": "2026-01-17 16:32:09 UTC"
  },
  {
    "arxiv_id": "2505.21182v1",
    "title": "Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations",
    "authors": [
      "Huy Hoang",
      "Tien Mai",
      "Pradeep Varakantham",
      "Tanvi Verma"
    ],
    "abstract": "Offline imitation learning typically learns from expert and unlabeled demonstrations, yet often overlooks the valuable signal in explicitly undesirable behaviors. In this work, we study offline imitation learning from contrasting behaviors, where the dataset contains both expert and undesirable demonstrations. We propose a novel formulation that optimizes a difference of KL divergences over the state-action visitation distributions of expert and undesirable (or bad) data. Although the resulting objective is a DC (Difference-of-Convex) program, we prove that it becomes convex when expert demonstrations outweigh undesirable demonstrations, enabling a practical and stable non-adversarial training objective. Our method avoids adversarial training and handles both positive and negative demonstrations in a unified framework. Extensive experiments on standard offline imitation learning benchmarks demonstrate that our approach consistently outperforms state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "preprint version",
    "pdf_url": "https://arxiv.org/pdf/2505.21182v1",
    "published_date": "2025-05-27 13:33:21 UTC",
    "updated_date": "2025-05-27 13:33:21 UTC"
  },
  {
    "arxiv_id": "2505.21180v1",
    "title": "Latent label distribution grid representation for modeling uncertainty",
    "authors": [
      "ShuNing Sun",
      "YinSong Xiong",
      "Yu Zhang",
      "Zhuoran Zheng"
    ],
    "abstract": "Although \\textbf{L}abel \\textbf{D}istribution \\textbf{L}earning (LDL) has promising representation capabilities for characterizing the polysemy of an instance, the complexity and high cost of the label distribution annotation lead to inexact in the construction of the label space. The existence of a large number of inexact labels generates a label space with uncertainty, which misleads the LDL algorithm to yield incorrect decisions. To alleviate this problem, we model the uncertainty of label distributions by constructing a \\textbf{L}atent \\textbf{L}abel \\textbf{D}istribution \\textbf{G}rid (LLDG) to form a low-noise representation space. Specifically, we first construct a label correlation matrix based on the differences between labels, and then expand each value of the matrix into a vector that obeys a Gaussian distribution, thus building a LLDG to model the uncertainty of the label space. Finally, the LLDG is reconstructed by the LLDG-Mixer to generate an accurate label distribution. Note that we enforce a customized low-rank scheme on this grid, which assumes that the label relations may be noisy and it needs to perform noise-reduction with the help of a Tucker reconstruction technique. Furthermore, we attempt to evaluate the effectiveness of the LLDG by considering its generation as an upstream task to achieve the classification of the objects. Extensive experimental results show that our approach performs competitively on several benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2505.21180v1",
    "published_date": "2025-05-27 13:31:37 UTC",
    "updated_date": "2025-05-27 13:31:37 UTC"
  },
  {
    "arxiv_id": "2505.21171v1",
    "title": "M-Wanda: Improving One-Shot Pruning for Multilingual LLMs",
    "authors": [
      "Rochelle Choenni",
      "Ivan Titov"
    ],
    "abstract": "Multilingual LLM performance is often critically dependent on model size. With an eye on efficiency, this has led to a surge in interest in one-shot pruning methods that retain the benefits of large-scale pretraining while shrinking the model size. However, as pruning tends to come with performance loss, it is important to understand the trade-offs between multilinguality and sparsification. In this work, we study multilingual performance under different sparsity constraints and show that moderate ratios already substantially harm performance. To help bridge this gap, we propose M-Wanda, a pruning method that models cross-lingual variation by incorporating language-aware activation statistics into its pruning criterion and dynamically adjusts layerwise sparsity based on cross-lingual importance. We show that M-Wanda consistently improves performance at minimal additional costs. We are the first to explicitly optimize pruning to retain multilingual performance, and hope to inspire future advances in multilingual pruning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21171v1",
    "published_date": "2025-05-27 13:24:38 UTC",
    "updated_date": "2025-05-27 13:24:38 UTC"
  },
  {
    "arxiv_id": "2505.21170v2",
    "title": "Quantum AIXI: Universal Intelligence via Quantum Information",
    "authors": [
      "Elija Perrier"
    ],
    "abstract": "AIXI is a widely studied model of artificial general intelligence (AGI) based upon principles of induction and reinforcement learning. However, AIXI is fundamentally classical in nature - as are the environments in which it is modelled. Given the universe is quantum mechanical in nature and the exponential overhead required to simulate quantum mechanical systems classically, the question arises as to whether there are quantum mechanical analogues of AIXI. To address this question, we extend the framework to quantum information and present Quantum AIXI (QAIXI). We introduce a model of quantum agent/environment interaction based upon quantum and classical registers and channels, showing how quantum AIXI agents may take both classical and quantum actions. We formulate the key components of AIXI in quantum information terms, extending previous research on quantum Kolmogorov complexity and a QAIXI value function. We discuss conditions and limitations upon quantum Solomonoff induction and show how contextuality fundamentally affects QAIXI models.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted into AGI-2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21170v2",
    "published_date": "2025-05-27 13:23:53 UTC",
    "updated_date": "2025-06-12 06:20:25 UTC"
  },
  {
    "arxiv_id": "2506.06318v2",
    "title": "MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes",
    "authors": [
      "Feiyang Pan",
      "Shenghe Zheng",
      "Chunyan Yin",
      "Guangbin Dou"
    ],
    "abstract": "MEMS gyroscopes play a critical role in inertial navigation and motion control applications but typically suffer from a fundamental trade-off between measurement range and noise performance. Existing hardware-based solutions aimed at mitigating this issue introduce additional complexity, cost, and scalability challenges. Deep-learning methods primarily focus on noise reduction and typically require precisely aligned ground-truth signals, making them difficult to deploy in practical scenarios and leaving the fundamental trade-off unresolved. To address these challenges, we introduce Mixture of Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework specifically designed for simultaneous over-range signal reconstruction and noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing saturated segments; and a Denoise Expert (DE), utilizing dual-branch complementary masking combined with FFT-guided augmentation for robust noise reduction. A lightweight gating module dynamically routes input segments to the appropriate expert. Furthermore, existing evaluation lack a comprehensive standard for assessing multi-dimensional signal enhancement. To bridge this gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source benchmarking platform comprising the GyroPeak-100 dataset and a unified evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our proposed ISEBench, demonstrating that our framework significantly extends the measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by 98.4%, and achieves state-of-the-art performance, effectively addressing the long-standing trade-off in inertial sensing.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Accepted to the NeurIPS 2025 Main Track",
    "pdf_url": "https://arxiv.org/pdf/2506.06318v2",
    "published_date": "2025-05-27 13:18:26 UTC",
    "updated_date": "2025-11-12 09:14:05 UTC"
  },
  {
    "arxiv_id": "2505.21160v1",
    "title": "STEB: In Search of the Best Evaluation Approach for Synthetic Time Series",
    "authors": [
      "Michael Stenger",
      "Robert Leppich",
      "André Bauer",
      "Samuel Kounev"
    ],
    "abstract": "The growing need for synthetic time series, due to data augmentation or privacy regulations, has led to numerous generative models, frameworks, and evaluation measures alike. Objectively comparing these measures on a large scale remains an open challenge. We propose the Synthetic Time series Evaluation Benchmark (STEB) -- the first benchmark framework that enables comprehensive and interpretable automated comparisons of synthetic time series evaluation measures. Using 10 diverse datasets, randomness injection, and 13 configurable data transformations, STEB computes indicators for measure reliability and score consistency. It tracks running time, test errors, and features sequential and parallel modes of operation. In our experiments, we determine a ranking of 41 measures from literature and confirm that the choice of upstream time series embedding heavily impacts the final score.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21160v1",
    "published_date": "2025-05-27 13:15:35 UTC",
    "updated_date": "2025-05-27 13:15:35 UTC"
  },
  {
    "arxiv_id": "2505.21156v1",
    "title": "Model as Loss: A Self-Consistent Training Paradigm",
    "authors": [
      "Saisamarth Rajesh Phaye",
      "Milos Cernak",
      "Andrew Harper"
    ],
    "abstract": "Conventional methods for speech enhancement rely on handcrafted loss functions (e.g., time or frequency domain losses) or deep feature losses (e.g., using WavLM or wav2vec), which often fail to capture subtle signal properties essential for optimal performance. To address this, we propose Model as Loss, a novel training paradigm that utilizes the encoder from the same model as a loss function to guide the training.\n  The Model as Loss paradigm leverages the encoder's task-specific feature space, optimizing the decoder to produce output consistent with perceptual and task-relevant characteristics of the clean signal. By using the encoder's learned features as a loss function, this framework enforces self-consistency between the clean reference speech and the enhanced model output. Our approach outperforms pre-trained deep feature losses on standard speech enhancement benchmarks, offering better perceptual quality and robust generalization to both in-domain and out-of-domain datasets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted in Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21156v1",
    "published_date": "2025-05-27 13:12:45 UTC",
    "updated_date": "2025-05-27 13:12:45 UTC"
  },
  {
    "arxiv_id": "2505.21154v1",
    "title": "GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation",
    "authors": [
      "Hailin Zhong",
      "Hanlin Wang",
      "Yujun Ye",
      "Meiyi Zhang",
      "Shengxin Zhu"
    ],
    "abstract": "Current personalized recommender systems predominantly rely on static offline data for algorithm design and evaluation, significantly limiting their ability to capture long-term user preference evolution and social influence dynamics in real-world scenarios. To address this fundamental challenge, we propose a high-fidelity social simulation platform integrating human-like cognitive agents and dynamic social interactions to realistically simulate user behavior evolution under recommendation interventions. Specifically, the system comprises a population of Sim-User Agents, each equipped with a five-layer cognitive architecture that encapsulates key psychological mechanisms, including episodic memory, affective state transitions, adaptive preference learning, and dynamic trust-risk assessments. In particular, we innovatively introduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine grounded in psychological and sociological theories, enabling more realistic user decision-making processes. Furthermore, we construct a multilayer heterogeneous social graph (GGBond Graph) supporting dynamic relational evolution, effectively modeling users' evolving social ties and trust dynamics based on interest similarity, personality alignment, and structural homophily. During system operation, agents autonomously respond to recommendations generated by typical recommender algorithms (e.g., Matrix Factorization, MultVAE, LightGCN), deciding whether to consume, rate, and share content while dynamically updating their internal states and social connections, thereby forming a stable, multi-round feedback loop. This innovative design transcends the limitations of traditional static datasets, providing a controlled, observable environment for evaluating long-term recommender effects.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21154v1",
    "published_date": "2025-05-27 13:09:21 UTC",
    "updated_date": "2025-05-27 13:09:21 UTC"
  },
  {
    "arxiv_id": "2505.21140v1",
    "title": "HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs",
    "authors": [
      "Honglin Gao",
      "Xiang Li",
      "Lan Zhao",
      "Gaoxi Xiao"
    ],
    "abstract": "Heterogeneous graph neural networks (HGNNs) have recently drawn increasing attention for modeling complex multi-relational data in domains such as recommendation, finance, and social networks. While existing research has been largely focused on enhancing HGNNs' predictive performance, their robustness and security, especially under backdoor attacks, remain underexplored. In this paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework for node classification tasks on heterogeneous graphs. HeteroBA inserts carefully crafted trigger nodes with realistic features and targeted structural connections, leveraging attention-based and clustering-based strategies to select influential auxiliary nodes for effective trigger propagation, thereby causing the model to misclassify specific nodes into a target label while maintaining accuracy on clean data. Experimental results on three datasets and various HGNN architectures demonstrate that HeteroBA achieves high attack success rates with minimal impact on the clean accuracy. Our method sheds light on potential vulnerabilities in HGNNs and calls for more robust defenses against backdoor threats in multi-relational graph scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21140v1",
    "published_date": "2025-05-27 12:51:48 UTC",
    "updated_date": "2025-05-27 12:51:48 UTC"
  },
  {
    "arxiv_id": "2505.21136v3",
    "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
    "authors": [
      "Jintao Zhang",
      "Xiaoming Xu",
      "Jia Wei",
      "Haofeng Huang",
      "Pengle Zhang",
      "Chendong Xiang",
      "Jun Zhu",
      "Jianfei Chen"
    ],
    "abstract": "The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21136v3",
    "published_date": "2025-05-27 12:50:36 UTC",
    "updated_date": "2025-06-06 07:47:22 UTC"
  },
  {
    "arxiv_id": "2505.21119v2",
    "title": "Universal Value-Function Uncertainties",
    "authors": [
      "Moritz A. Zanger",
      "Max Weltevrede",
      "Yaniv Oren",
      "Pascal R. Van der Vaart",
      "Caroline Horsch",
      "Wendelin Böhmer",
      "Matthijs T. J. Spaan"
    ],
    "abstract": "Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional value uncertainty, incorporating the future uncertainties any given policy may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21119v2",
    "published_date": "2025-05-27 12:38:19 UTC",
    "updated_date": "2025-06-02 16:01:02 UTC"
  },
  {
    "arxiv_id": "2505.21116v1",
    "title": "Creativity in LLM-based Multi-Agent Systems: A Survey",
    "authors": [
      "Yi-Cheng Lin",
      "Kang-Chieh Chen",
      "Zhe-Yan Li",
      "Tzu-Heng Wu",
      "Tzu-Hsuan Wu",
      "Kuan-Yu Chen",
      "Hung-yi Lee",
      "Yun-Nung Chen"
    ],
    "abstract": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming how humans and AIs collaboratively generate ideas and artifacts. While existing surveys provide comprehensive overviews of MAS infrastructures, they largely overlook the dimension of \\emph{creativity}, including how novel outputs are generated and evaluated, how creativity informs agent personas, and how creative workflows are coordinated. This is the first survey dedicated to creativity in MAS. We focus on text and image generation tasks, and present: (1) a taxonomy of agent proactivity and persona design; (2) an overview of generation techniques, including divergent exploration, iterative refinement, and collaborative synthesis, as well as relevant datasets and evaluation metrics; and (3) a discussion of key challenges, such as inconsistent evaluation standards, insufficient bias mitigation, coordination conflicts, and the lack of unified benchmarks. This survey offers a structured framework and roadmap for advancing the development, evaluation, and standardization of creative MAS.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "23 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.21116v1",
    "published_date": "2025-05-27 12:36:14 UTC",
    "updated_date": "2025-05-27 12:36:14 UTC"
  },
  {
    "arxiv_id": "2505.21109v1",
    "title": "A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction",
    "authors": [
      "Bogdan Bogachov",
      "Yaoyao Fiona Zhao"
    ],
    "abstract": "Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 Figures, 6 Tables. This paper has been accepted to be published in the proceedings of IDETC-CIE 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21109v1",
    "published_date": "2025-05-27 12:31:24 UTC",
    "updated_date": "2025-05-27 12:31:24 UTC"
  },
  {
    "arxiv_id": "2505.21106v1",
    "title": "Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation",
    "authors": [
      "Zhengyang Ji",
      "Yifan Jia",
      "Shang Gao",
      "Yutao Yue"
    ],
    "abstract": "Large Vision Language Models (LVLMs) have achieved remarkable progress in multimodal tasks, yet they also exhibit notable social biases. These biases often manifest as unintended associations between neutral concepts and sensitive human attributes, leading to disparate model behaviors across demographic groups. While existing studies primarily focus on detecting and quantifying such biases, they offer limited insight into the underlying mechanisms within the models. To address this gap, we propose an explanatory framework that combines information flow analysis with multi-round dialogue evaluation, aiming to understand the origin of social bias from the perspective of imbalanced internal information utilization. Specifically, we first identify high-contribution image tokens involved in the model's reasoning process for neutral questions via information flow analysis. Then, we design a multi-turn dialogue mechanism to evaluate the extent to which these key tokens encode sensitive information. Extensive experiments reveal that LVLMs exhibit systematic disparities in information usage when processing images of different demographic groups, suggesting that social bias is deeply rooted in the model's internal reasoning dynamics. Furthermore, we complement our findings from a textual modality perspective, showing that the model's semantic representations already display biased proximity patterns, thereby offering a cross-modal explanation of bias formation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21106v1",
    "published_date": "2025-05-27 12:28:44 UTC",
    "updated_date": "2025-05-27 12:28:44 UTC"
  },
  {
    "arxiv_id": "2505.21589v1",
    "title": "Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI",
    "authors": [
      "Carina Newen",
      "Luca Hinkamp",
      "Maria Ntonti",
      "Emmanuel Müller"
    ],
    "abstract": "From uncertainty quantification to real-world object detection, we recognize the importance of machine learning algorithms, particularly in safety-critical domains such as autonomous driving or medical diagnostics. In machine learning, ambiguous data plays an important role in various machine learning domains. Optical illusions present a compelling area of study in this context, as they offer insight into the limitations of both human and machine perception. Despite this relevance, optical illusion datasets remain scarce. In this work, we introduce a novel dataset of optical illusions featuring intermingled animal pairs designed to evoke perceptual ambiguity. We identify generalizable visual concepts, particularly gaze direction and eye cues, as subtle yet impactful features that significantly influence model accuracy. By confronting models with perceptual ambiguity, our findings underscore the importance of concepts in visual learning and provide a foundation for studying bias and alignment between human and machine vision. To make this dataset useful for general purposes, we generate optical illusions systematically with different concepts discussed in our bias mitigation section. The dataset is accessible in Kaggle via https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333. Our source code can be found at https://github.com/KDD-OpenSource/Ambivision.git.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 18 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21589v1",
    "published_date": "2025-05-27 12:22:59 UTC",
    "updated_date": "2025-05-27 12:22:59 UTC"
  },
  {
    "arxiv_id": "2505.21097v2",
    "title": "Thinker: Learning to Think Fast and Slow",
    "authors": [
      "Stephen Chung",
      "Wenyu Du",
      "Jie Fu"
    ],
    "abstract": "Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training. Additionally, we have open-sourced both the trained models and the source code.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.21097v2",
    "published_date": "2025-05-27 12:22:46 UTC",
    "updated_date": "2025-10-16 16:20:17 UTC"
  },
  {
    "arxiv_id": "2505.21092v2",
    "title": "BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge",
    "authors": [
      "Daeen Kabir",
      "Minhajur Rahman Chowdhury Mahim",
      "Sheikh Shafayat",
      "Adnan Sadik",
      "Arian Ahmed",
      "Eunsu Kim",
      "Alice Oh"
    ],
    "abstract": "In this work, we introduce BLUCK, a new dataset designed to measure the performance of Large Language Models (LLMs) in Bengali linguistic understanding and cultural knowledge. Our dataset comprises 2366 multiple-choice questions (MCQs) carefully curated from compiled collections of several college and job level examinations and spans 23 categories covering knowledge on Bangladesh's culture and history and Bengali linguistics. We benchmarked BLUCK using 6 proprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that while these models perform reasonably well overall, they, however, struggles in some areas of Bengali phonetics. Although current LLMs' performance on Bengali cultural and linguistic contexts is still not comparable to that of mainstream languages like English, our results indicate Bengali's status as a mid-resource language. Importantly, BLUCK is also the first MCQ-based evaluation benchmark that is centered around native Bengali culture, history, and linguistics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at BLP Workshop, IJCNLP-AACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21092v2",
    "published_date": "2025-05-27 12:19:12 UTC",
    "updated_date": "2026-01-19 13:17:26 UTC"
  },
  {
    "arxiv_id": "2505.21091v3",
    "title": "Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)",
    "authors": [
      "Anna Neumann",
      "Elisabeth Kirsten",
      "Muhammad Bilal Zafar",
      "Jatinder Singh"
    ],
    "abstract": "System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "Published in Proceedings of ACM FAccT 2025 Update Comment: Fixed the error where user vs. system and implicit vs. explicit labels in the heatmaps were switched. The takeaways remain the same",
    "pdf_url": "https://arxiv.org/pdf/2505.21091v3",
    "published_date": "2025-05-27 12:19:08 UTC",
    "updated_date": "2025-06-23 06:43:45 UTC"
  },
  {
    "arxiv_id": "2505.21087v2",
    "title": "Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games",
    "authors": [
      "Marta Grobelna",
      "Jan Křetínský",
      "Maximilian Weininger"
    ],
    "abstract": "We consider two-player zero-sum concurrent stochastic games (CSGs) played on graphs with reachability and safety objectives. These include degenerate classes such as Markov decision processes or turn-based stochastic games, which can be solved by linear or quadratic programming; however, in practice, value iteration (VI) outperforms the other approaches and is the most implemented method. Similarly, for CSGs, this practical performance makes VI an attractive alternative to the standard theoretical solution via the existential theory of reals.\n  VI starts with an under-approximation of the sought values for each state and iteratively updates them, traditionally terminating once two consecutive approximations are $ε$-close. However, this stopping criterion lacks guarantees on the precision of the approximation, which is the goal of this work. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard VI with a converging sequence of over-approximations and terminates once the over- and under-approximations are $ε$-close.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LO",
    "comment": "Full version of the corresponding LICS'25 paper Corrected Algorithm 2 and associated Lemma 30",
    "pdf_url": "https://arxiv.org/pdf/2505.21087v2",
    "published_date": "2025-05-27 12:13:47 UTC",
    "updated_date": "2025-09-10 13:29:23 UTC"
  },
  {
    "arxiv_id": "2505.21588v1",
    "title": "Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems",
    "authors": [
      "Young-Min Cho",
      "Sharath Chandra Guntuku",
      "Lyle Ungar"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have enabled the emergence of multi-agent systems where LLMs interact, collaborate, and make decisions in shared environments. While individual model behavior has been extensively studied, the dynamics of peer influence in such systems remain underexplored. In this paper, we investigate herd behavior, the tendency of agents to align their outputs with those of their peers, within LLM-based multi-agent interactions. We present a series of controlled experiments that reveal how herd behaviors are shaped by multiple factors. First, we show that the gap between self-confidence and perceived confidence in peers significantly impacts an agent's likelihood to conform. Second, we find that the format in which peer information is presented plays a critical role in modulating the strength of herd behavior. Finally, we demonstrate that the degree of herd behavior can be systematically controlled, and that appropriately calibrated herd tendencies can enhance collaborative outcomes. These findings offer new insights into the social dynamics of LLM-based systems and open pathways for designing more effective and adaptive multi-agent collaboration frameworks.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2505.21588v1",
    "published_date": "2025-05-27 12:12:56 UTC",
    "updated_date": "2025-05-27 12:12:56 UTC"
  },
  {
    "arxiv_id": "2505.21077v2",
    "title": "Efficient Large Language Model Inference with Neural Block Linearization",
    "authors": [
      "Mete Erdogan",
      "Francesco Tonin",
      "Volkan Cevher"
    ],
    "abstract": "The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs. The implementation is available at: https://github.com/LIONS-EPFL/NBL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21077v2",
    "published_date": "2025-05-27 12:01:43 UTC",
    "updated_date": "2025-10-19 21:33:03 UTC"
  },
  {
    "arxiv_id": "2505.21074v1",
    "title": "Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling",
    "authors": [
      "Yichuan Cao",
      "Yibo Miao",
      "Xiao-Shan Gao",
      "Yinpeng Dong"
    ],
    "abstract": "Text-to-image (T2I) models raise ethical and safety concerns due to their potential to generate inappropriate or harmful images. Evaluating these models' security through red-teaming is vital, yet white-box approaches are limited by their need for internal access, complicating their use with closed-source models. Moreover, existing black-box methods often assume knowledge about the model's specific defense mechanisms, limiting their utility in real-world commercial API scenarios. A significant challenge is how to evade unknown and diverse defense mechanisms. To overcome this difficulty, we propose a novel Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively employs LLM to modify prompts to query and leverages feedback from T2I systems for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a prior, enabling the LLM to dynamically adapt to unknown defense mechanisms. Given that the feedback is often labeled and coarse-grained, making it difficult to utilize directly, we further propose rule-based preference modeling, which employs a set of rules to evaluate desired or undesired feedback, facilitating finer-grained control over the LLM's dynamic adaptation process. Extensive experiments on nineteen T2I systems with varied safety mechanisms, three online commercial API services, and T2V models verify the superiority and practicality of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21074v1",
    "published_date": "2025-05-27 12:00:19 UTC",
    "updated_date": "2025-05-27 12:00:19 UTC"
  },
  {
    "arxiv_id": "2505.21067v1",
    "title": "Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning",
    "authors": [
      "Xiao Hu",
      "Xingyu Lu",
      "Liyuan Mao",
      "YiFan Zhang",
      "Tianke Zhang",
      "Bin Wen",
      "Fan Yang",
      "Tingting Gao",
      "Guorui Zhou"
    ],
    "abstract": "Reinforcement learning (RL) has played an important role in improving the reasoning ability of large language models (LLMs). Some studies apply RL directly to \\textit{smaller} base models (known as zero-RL) and also achieve notable progress. However, in this paper, we show that using only 920 examples, a simple distillation method based on the base model can clearly outperform zero-RL, which typically requires much more data and computational cost. By analyzing the token frequency in model outputs, we find that the distilled model shows more flexible reasoning. It uses anthropomorphic tokens and logical connectors much more often than the zero-RL model. Further analysis reveals that distillation enhances the presence of two advanced cognitive behaviors: Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent occurrences of these two advanced cognitive behaviors give rise to flexible reasoning, which is essential for solving complex reasoning problems, while zero-RL fails to significantly boost the frequency of these behaviors.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21067v1",
    "published_date": "2025-05-27 11:52:41 UTC",
    "updated_date": "2025-05-27 11:52:41 UTC"
  },
  {
    "arxiv_id": "2505.21061v1",
    "title": "LPOI: Listwise Preference Optimization for Vision Language Models",
    "authors": [
      "Fatemeh Pesaran Zadeh",
      "Yoojin Oh",
      "Gunhee Kim"
    ],
    "abstract": "Aligning large VLMs with human preferences is a challenging task, as methods like RLHF and DPO often overfit to textual information or exacerbate hallucinations. Although augmenting negative image samples partially addresses these pitfalls, no prior work has employed listwise preference optimization for VLMs, due to the complexity and cost of constructing listwise image samples. In this work, we propose LPOI, the first object-aware listwise preference optimization developed for reducing hallucinations in VLMs. LPOI identifies and masks a critical object in the image, and then interpolates the masked region between the positive and negative images to form a sequence of incrementally more complete images. The model is trained to rank these images in ascending order of object visibility, effectively reducing hallucinations while retaining visual fidelity. LPOI requires no extra annotations beyond standard pairwise preference data, as it automatically constructs the ranked lists through object masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and Object HalBench confirm that LPOI outperforms existing preference optimization methods in reducing hallucinations and enhancing VLM performance. We make the code available at https://github.com/fatemehpesaran310/lpoi.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ACL 2025 Main. Code is released at https://github.com/fatemehpesaran310/lpoi",
    "pdf_url": "https://arxiv.org/pdf/2505.21061v1",
    "published_date": "2025-05-27 11:47:28 UTC",
    "updated_date": "2025-05-27 11:47:28 UTC"
  },
  {
    "arxiv_id": "2505.21055v1",
    "title": "Agent-Environment Alignment via Automated Interface Generation",
    "authors": [
      "Kaiming Liu",
      "Xuanyu Lei",
      "Ziyue Wang",
      "Peng Li",
      "Yang Liu"
    ],
    "abstract": "Large language model (LLM) agents have shown impressive reasoning capabilities in interactive decision-making tasks. These agents interact with environment through intermediate interfaces, such as predefined action spaces and interaction rules, which mediate the perception and action. However, mismatches often happen between the internal expectations of the agent regarding the influence of its issued actions and the actual state transitions in the environment, a phenomenon referred to as \\textbf{agent-environment misalignment}. While prior work has invested substantially in improving agent strategies and environment design, the critical role of the interface still remains underexplored. In this work, we empirically demonstrate that agent-environment misalignment poses a significant bottleneck to agent performance. To mitigate this issue, we propose \\textbf{ALIGN}, an \\underline{A}uto-A\\underline{l}igned \\underline{I}nterface \\underline{G}e\\underline{n}eration framework that alleviates the misalignment by enriching the interface. Specifically, the ALIGN-generated interface enhances both the static information of the environment and the step-wise observations returned to the agent. Implemented as a lightweight wrapper, this interface achieves the alignment without modifying either the agent logic or the environment code. Experiments across multiple domains including embodied tasks, web navigation and tool-use, show consistent performance improvements, with up to a 45.67\\% success rate improvement observed in ALFWorld. Meanwhile, ALIGN-generated interface can generalize across different agent architectures and LLM backbones without interface regeneration. Code and experimental results are available at https://github.com/THUNLP-MT/ALIGN.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21055v1",
    "published_date": "2025-05-27 11:44:50 UTC",
    "updated_date": "2025-05-27 11:44:50 UTC"
  },
  {
    "arxiv_id": "2505.21046v1",
    "title": "A domain adaptation neural network for digital twin-supported fault diagnosis",
    "authors": [
      "Zhenling Chen",
      "Haiwei Fu",
      "Zhiguo Zeng"
    ],
    "abstract": "Digital twins offer a promising solution to the lack of sufficient labeled data in deep learning-based fault diagnosis by generating simulated data for model training. However, discrepancies between simulation and real-world systems can lead to a significant drop in performance when models are applied in real scenarios. To address this issue, we propose a fault diagnosis framework based on Domain-Adversarial Neural Networks (DANN), which enables knowledge transfer from simulated (source domain) to real-world (target domain) data. We evaluate the proposed framework using a publicly available robotics fault diagnosis dataset, which includes 3,600 sequences generated by a digital twin model and 90 real sequences collected from physical systems. The DANN method is compared with commonly used lightweight deep learning models such as CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating domain adaptation significantly improves the diagnostic performance. For example, applying DANN to a baseline CNN model improves its accuracy from 70.00% to 80.22% on real-world test data, demonstrating the effectiveness of domain adaptation in bridging the sim-to-real gap.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint accepted by ICCAD 2025 at Barcelona",
    "pdf_url": "https://arxiv.org/pdf/2505.21046v1",
    "published_date": "2025-05-27 11:27:05 UTC",
    "updated_date": "2025-05-27 11:27:05 UTC"
  },
  {
    "arxiv_id": "2505.21045v1",
    "title": "Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking",
    "authors": [
      "Lingyi Cai",
      "Ruichen Zhang",
      "Changyuan Zhao",
      "Yu Zhang",
      "Jiawen Kang",
      "Dusit Niyato",
      "Tao Jiang",
      "Xuemin Shen"
    ],
    "abstract": "Low-Altitude Economic Networking (LAENet) aims to support diverse flying applications below 1,000 meters by deploying various aerial vehicles for flexible and cost-effective aerial networking. However, complex decision-making, resource constraints, and environmental uncertainty pose significant challenges to the development of the LAENet. Reinforcement learning (RL) offers a potential solution in response to these challenges but has limitations in generalization, reward design, and model stability. The emergence of large language models (LLMs) offers new opportunities for RL to mitigate these limitations. In this paper, we first present a tutorial about integrating LLMs into RL by using the capacities of generation, contextual understanding, and structured reasoning of LLMs. We then propose an LLM-enhanced RL framework for the LAENet in terms of serving the LLM as information processor, reward designer, decision-maker, and generator. Moreover, we conduct a case study by using LLMs to design a reward function to improve the learning performance of RL in the LAENet. Finally, we provide a conclusion and discuss future work.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21045v1",
    "published_date": "2025-05-27 11:25:42 UTC",
    "updated_date": "2025-05-27 11:25:42 UTC"
  },
  {
    "arxiv_id": "2505.21040v2",
    "title": "FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis",
    "authors": [
      "Wei Chen",
      "Zhao Zhang",
      "Meng Yuan",
      "Kepeng Xu",
      "Fuzhen Zhuang"
    ],
    "abstract": "In this paper, we address the task of targeted sentiment analysis (TSA), which involves two sub-tasks, i.e., identifying specific aspects from reviews and determining their corresponding sentiments. Aspect extraction forms the foundation for sentiment prediction, highlighting the critical dependency between these two tasks for effective cross-task knowledge transfer. While most existing studies adopt a multi-task learning paradigm to align task-specific features in the latent space, they predominantly rely on coarse-grained knowledge transfer. Such approaches lack fine-grained control over aspect-sentiment relationships, often assuming uniform sentiment polarity within related aspects. This oversimplification neglects contextual cues that differentiate sentiments, leading to negative transfer. To overcome these limitations, we propose FCKT, a fine-grained cross-task knowledge transfer framework tailored for TSA. By explicitly incorporating aspect-level information into sentiment prediction, FCKT achieves fine-grained knowledge transfer, effectively mitigating negative transfer and enhancing task performance. Experiments on three datasets, including comparisons with various baselines and large language models (LLMs), demonstrate the effectiveness of FCKT. The source code is available on https://github.com/cwei01/FCKT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.21040v2",
    "published_date": "2025-05-27 11:23:53 UTC",
    "updated_date": "2025-05-28 07:02:51 UTC"
  },
  {
    "arxiv_id": "2505.21587v1",
    "title": "CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning",
    "authors": [
      "Bin Qin",
      "Qirui Ji",
      "Jiangmeng Li",
      "Yupeng Wang",
      "Xuesong Wu",
      "Jianwen Cao",
      "Fanjiang Xu"
    ],
    "abstract": "Self-supervised topological deep learning (TDL) represents a nascent but underexplored area with significant potential for modeling higher-order interactions in simplicial complexes and cellular complexes to derive representations of unlabeled graphs. Compared to simplicial complexes, cellular complexes exhibit greater expressive power. However, the advancement in self-supervised learning for cellular TDL is largely hindered by two core challenges: \\textit{extrinsic structural constraints} inherent to cellular complexes, and intrinsic semantic redundancy in cellular representations. The first challenge highlights that traditional graph augmentation techniques may compromise the integrity of higher-order cellular interactions, while the second underscores that topological redundancy in cellular complexes potentially diminish task-relevant information. To address these issues, we introduce Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT), a twofold framework designed to adhere to the combinatorial constraints of cellular complexes while mitigating informational redundancy. Specifically, we propose a parameter perturbation-based augmentation method that injects controlled noise into cellular interactions without altering the underlying cellular structures, thereby preserving cellular topology during contrastive learning. Additionally, a cellular trimming scheduler is employed to mask gradient contributions from task-irrelevant cells through a bi-level meta-learning approach, effectively removing redundant topological elements while maintaining critical higher-order semantics. We provide theoretical justification and empirical validation to demonstrate that CellCLAT achieves substantial improvements over existing self-supervised graph learning methods, marking a significant attempt in this domain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21587v1",
    "published_date": "2025-05-27 11:16:49 UTC",
    "updated_date": "2025-05-27 11:16:49 UTC"
  },
  {
    "arxiv_id": "2505.21036v2",
    "title": "RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy",
    "authors": [
      "Aiyue Chen",
      "Bin Dong",
      "Jingru Li",
      "Jing Lin",
      "Kun Tian",
      "Yiwu Yao",
      "Gongyi Wang"
    ],
    "abstract": "Video generation using diffusion models is highly computationally intensive, with 3D attention in Diffusion Transformer (DiT) models accounting for over 80\\% of the total computational resources. In this work, we introduce {\\bf RainFusion}, a novel training-free sparse attention method that exploits inherent sparsity nature in visual data to accelerate attention computation while preserving video quality. Specifically, we identify three unique sparse patterns in video generation attention calculations--Spatial Pattern, Temporal Pattern and Textural Pattern. The sparse pattern for each attention head is determined online with negligible overhead (\\textasciitilde\\,0.2\\%) with our proposed {\\bf ARM} (Adaptive Recognition Module) during inference. Our proposed {\\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated into state-of-the-art 3D-attention video generation models without additional training or calibration. We evaluate our method on leading open-sourced models including HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its broad applicability and effectiveness. Experimental results show that RainFusion achieves over {\\bf 2\\(\\times\\)} speedup in attention computation while maintaining video quality, with only a minimal impact on VBench scores (-0.2\\%).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21036v2",
    "published_date": "2025-05-27 11:15:02 UTC",
    "updated_date": "2025-06-09 11:33:40 UTC"
  },
  {
    "arxiv_id": "2505.21032v2",
    "title": "FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models",
    "authors": [
      "Nils Neukirch",
      "Johanna Vielhaben",
      "Nils Strodthoff"
    ],
    "abstract": "Internal representations are crucial for understanding deep neural networks, such as their properties and reasoning patterns, but remain difficult to interpret. While mapping from feature space to input space aids in interpreting the former, existing approaches often rely on crude approximations. We propose using a conditional diffusion model - a pretrained high-fidelity diffusion model conditioned on spatially resolved feature maps - to learn such a mapping in a probabilistic manner. We demonstrate the feasibility of this approach across various pretrained image classifiers from CNNs to ViTs, showing excellent reconstruction capabilities. Through qualitative comparisons and robustness analysis, we validate our method and showcase possible applications, such as the visualization of concept steering in input space or investigations of the composite nature of the feature space. This approach has broad potential for improving feature space understanding in computer vision models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Version published by Transactions on Machine Learning Research in 2025 (TMLR ISSN 2835-8856) at https://openreview.net/forum?id=UtE1YnPNgZ. 32 pages, 27 figures. This work builds on an earlier manuscript (arXiv:2505.21032) and crucially extends it. Code is available at https://github.com/AI4HealthUOL/FeatInv",
    "pdf_url": "https://arxiv.org/pdf/2505.21032v2",
    "published_date": "2025-05-27 11:07:34 UTC",
    "updated_date": "2026-01-14 10:46:17 UTC"
  },
  {
    "arxiv_id": "2505.21027v2",
    "title": "TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data",
    "authors": [
      "Zhipeng He",
      "Chun Ouyang",
      "Lijie Wen",
      "Cong Liu",
      "Catarina Moreira"
    ],
    "abstract": "Adversarial attacks pose a significant threat to machine learning models by inducing incorrect predictions through imperceptible perturbations to input data. While these attacks are well studied in unstructured domains such as images, their behaviour on tabular data remains underexplored due to mixed feature types and complex inter-feature dependencies. This study introduces a comprehensive benchmark that evaluates adversarial attacks on tabular datasets with respect to both effectiveness and imperceptibility. We assess five white-box attack algorithms (FGSM, BIM, PGD, DeepFool, and C\\&W) across four representative models (LR, MLP, TabTransformer and FT-Transformer) using eleven datasets spanning finance, energy, and healthcare domains. The benchmark employs four quantitative imperceptibility metrics (proximity, sparsity, deviation, and sensitivity) to characterise perturbation realism. The analysis quantifies the trade-off between these two aspects and reveals consistent differences between attack types, with $\\ell_\\infty$-based attacks achieving higher success but lower subtlety, and $\\ell_2$-based attacks offering more realistic perturbations. The benchmark findings offer actionable insights for designing more imperceptible adversarial attacks, advancing the understanding of adversarial vulnerability in tabular machine learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "71 pages, 21 figures, 11 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.21027v2",
    "published_date": "2025-05-27 11:01:32 UTC",
    "updated_date": "2025-10-13 03:56:19 UTC"
  },
  {
    "arxiv_id": "2505.21026v1",
    "title": "Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning",
    "authors": [
      "Runze Lin",
      "Junghui Chen",
      "Biao Huang",
      "Lei Xie",
      "Hongye Su"
    ],
    "abstract": "In the era of Industry 4.0 and smart manufacturing, process systems engineering must adapt to digital transformation. While reinforcement learning offers a model-free approach to process control, its applications are limited by the dependence on accurate digital twins and well-designed reward functions. To address these limitations, this paper introduces a novel framework that integrates inverse reinforcement learning (IRL) with multi-task learning for data-driven, multi-mode control design. Using historical closed-loop data as expert demonstrations, IRL extracts optimal reward functions and control policies. A latent-context variable is incorporated to distinguish modes, enabling the training of mode-specific controllers. Case studies on a continuous stirred tank reactor and a fed-batch bioreactor validate the effectiveness of this framework in handling multi-mode data and training adaptable controllers.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21026v1",
    "published_date": "2025-05-27 11:01:00 UTC",
    "updated_date": "2025-05-27 11:01:00 UTC"
  },
  {
    "arxiv_id": "2505.21025v2",
    "title": "Text-Queried Audio Source Separation via Hierarchical Modeling",
    "authors": [
      "Xinlei Yin",
      "Xiulian Peng",
      "Xue Jiang",
      "Zhiwei Xiong",
      "Yan Lu"
    ],
    "abstract": "Target audio source separation with natural language queries presents a promising paradigm for extracting arbitrary audio events through arbitrary text descriptions. Existing methods mainly face two challenges, the difficulty in jointly modeling acoustic-textual alignment and semantic-aware separation within a blindly-learned single-stage architecture, and the reliance on large-scale accurately-labeled training data to compensate for inefficient cross-modal learning and separation. To address these challenges, we propose a hierarchical decomposition framework, HSM-TSS, that decouples the task into global-local semantic-guided feature separation and structure-preserving acoustic reconstruction. Our approach introduces a dual-stage mechanism for semantic separation, operating on distinct global and local semantic feature spaces. We first perform global-semantic separation through a global semantic feature space aligned with text queries. A Q-Audio architecture is employed to align audio and text modalities, serving as pretrained global-semantic encoders. Conditioned on the predicted global feature, we then perform the second-stage local-semantic separation on AudioMAE features that preserve time-frequency structures, followed by acoustic reconstruction. We also propose an instruction processing pipeline to parse arbitrary text queries into structured operations, extraction or removal, coupled with audio descriptions, enabling flexible sound manipulation. Our method achieves state-of-the-art separation performance with data-efficient training while maintaining superior semantic consistency with queries in complex auditory scenes.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by TASLP",
    "pdf_url": "https://arxiv.org/pdf/2505.21025v2",
    "published_date": "2025-05-27 11:00:38 UTC",
    "updated_date": "2025-12-02 10:03:05 UTC"
  },
  {
    "arxiv_id": "2505.22685v2",
    "title": "DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography",
    "authors": [
      "Marcus J. Vroemen",
      "Yuqian Chen",
      "Yui Lo",
      "Tengfei Xue",
      "Weidong Cai",
      "Fan Zhang",
      "Josien P. W. Pluim",
      "Lauren J. O'Donnell"
    ],
    "abstract": "Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural connections, but traditional connectome generation is time-consuming and requires gray matter parcellation, posing challenges for large-scale studies. We introduce DeepMultiConnectome, a deep-learning model that predicts structural connectomes directly from tractography, bypassing the need for gray matter parcellation while supporting multiple parcellation schemes. Using a point-cloud-based neural network with multi-task learning, the model classifies streamlines according to their connected regions across two parcellation schemes, sharing a learned representation. We train and validate DeepMultiConnectome on tractography from the Human Connectome Project Young Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter parcellation scheme. DeepMultiConnectome predicts multiple structural connectomes from a whole-brain tractogram containing 3 million streamlines in approximately 40 seconds. DeepMultiConnectome is evaluated by comparing predicted connectomes with traditional connectomes generated using the conventional method of labeling streamlines using a gray matter parcellation. The predicted connectomes are highly correlated with traditionally generated connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region scheme) and largely preserve network properties. A test-retest analysis of DeepMultiConnectome demonstrates reproducibility comparable to traditionally generated connectomes. The predicted connectomes perform similarly to traditionally generated connectomes in predicting age and cognitive function. Overall, DeepMultiConnectome provides a scalable, fast model for generating subject-specific connectomes across multiple parcellation schemes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "15 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.22685v2",
    "published_date": "2025-05-27 10:56:37 UTC",
    "updated_date": "2025-06-11 08:09:20 UTC"
  },
  {
    "arxiv_id": "2505.23811v3",
    "title": "LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions",
    "authors": [
      "Hadi Askari",
      "Shivanshu Gupta",
      "Fei Wang",
      "Anshuman Chhabra",
      "Muhao Chen"
    ],
    "abstract": "Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream performance. It is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose LayerIF, a data-driven framework that leverages Influence Functions to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces task-specific layer importance estimates for the same LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Neurips 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.23811v3",
    "published_date": "2025-05-27 10:55:12 UTC",
    "updated_date": "2025-10-24 09:27:17 UTC"
  },
  {
    "arxiv_id": "2505.21012v1",
    "title": "Federated Instrumental Variable Analysis via Federated Generalized Method of Moments",
    "authors": [
      "Geetika",
      "Somya Tyagi",
      "Bapi Chatterjee"
    ],
    "abstract": "Instrumental variables (IV) analysis is an important applied tool for areas such as healthcare and consumer economics. For IV analysis in high-dimensional settings, the Generalized Method of Moments (GMM) using deep neural networks offers an efficient approach. With non-i.i.d. data sourced from scattered decentralized clients, federated learning is a popular paradigm for training the models while promising data privacy. However, to our knowledge, no federated algorithm for either GMM or IV analysis exists to date. In this work, we introduce federated instrumental variables analysis (FedIV) via federated generalized method of moments (FedGMM). We formulate FedGMM as a federated zero-sum game defined by a federated non-convex non-concave minimax optimization problem, which is solved using federated gradient descent ascent (FedGDA) algorithm. One key challenge arises in theoretically characterizing the federated local optimality. To address this, we present properties and existence results of clients' local equilibria via FedGDA limit points. Thereby, we show that the federated solution consistently estimates the local moment conditions of every participating client. The proposed algorithm is backed by extensive experiments to demonstrate the efficacy of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 3 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2505.21012v1",
    "published_date": "2025-05-27 10:46:43 UTC",
    "updated_date": "2025-05-27 10:46:43 UTC"
  },
  {
    "arxiv_id": "2505.21584v1",
    "title": "Fairness in Federated Learning: Fairness for Whom?",
    "authors": [
      "Afaf Taik",
      "Khaoula Chehbouni",
      "Golnoosh Farnadi"
    ],
    "abstract": "Fairness in federated learning has emerged as a rapidly growing area of research, with numerous works proposing formal definitions and algorithmic interventions. Yet, despite this technical progress, fairness in FL is often defined and evaluated in ways that abstract away from the sociotechnical contexts in which these systems are deployed. In this paper, we argue that existing approaches tend to optimize narrow system level metrics, such as performance parity or contribution-based rewards, while overlooking how harms arise throughout the FL lifecycle and how they impact diverse stakeholders. We support this claim through a critical analysis of the literature, based on a systematic annotation of papers for their fairness definitions, design decisions, evaluation practices, and motivating use cases. Our analysis reveals five recurring pitfalls: 1) fairness framed solely through the lens of server client architecture, 2) a mismatch between simulations and motivating use-cases and contexts, 3) definitions that conflate protecting the system with protecting its users, 4) interventions that target isolated stages of the lifecycle while neglecting upstream and downstream effects, 5) and a lack of multi-stakeholder alignment where multiple fairness definitions can be relevant at once. Building on these insights, we propose a harm centered framework that links fairness definitions to concrete risks and stakeholder vulnerabilities. We conclude with recommendations for more holistic, context-aware, and accountable fairness research in FL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21584v1",
    "published_date": "2025-05-27 10:41:19 UTC",
    "updated_date": "2025-05-27 10:41:19 UTC"
  },
  {
    "arxiv_id": "2505.20997v1",
    "title": "BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks",
    "authors": [
      "Sen Bai",
      "Chunqi Yang",
      "Xin Bai",
      "Xin Zhang",
      "Zhengang Jiang"
    ],
    "abstract": "Binary (0-1) integer programming (BIP) is pivotal in scientific domains requiring discrete decision-making. As the advance of AI computing, recent works explore neural network-based solvers for integer linear programming (ILP) problems. Yet, they lack scalability for tackling nonlinear challenges. To handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear relaxations, leading to exponential growth in auxiliary variables and severe computation limitations. To overcome these limitations, we propose BIPNN (Binary Integer Programming Neural Network), an unsupervised learning framework to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN). Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear (sin, log, exp) optimization problems-into unconstrained, differentiable, and polynomial loss functions. The reformulation stems from the observation of a precise one-to-one mapping between polynomial BIP objectives and hypergraph structures, enabling the unsupervised training of HyperGNN to optimize BIP problems in an end-to-end manner. On this basis, we propose a GPU-accelerated and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel via straightforward gradient descent, thus significantly reducing the training cost while ensuring the generation of discrete, high-quality solutions. Extensive experiments on synthetic and real-world datasets highlight the superiority of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20997v1",
    "published_date": "2025-05-27 10:31:52 UTC",
    "updated_date": "2025-05-27 10:31:52 UTC"
  },
  {
    "arxiv_id": "2505.23810v2",
    "title": "MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation",
    "authors": [
      "Chenghao Yang",
      "Yinbo Luo",
      "Zhoufutu Wen",
      "Qi Chu",
      "Tao Gong",
      "Longxiang Liu",
      "Kaiyuan Zhang",
      "Jianpeng Jiao",
      "Ge Zhang",
      "Wenhao Huang",
      "Nenghai Yu"
    ],
    "abstract": "Large Language Models (\\textbf{LLMs}), e.g. ChatGPT, have been widely adopted in real-world dialogue applications. However, LLMs' robustness, especially in handling long complex dialogue sessions, including frequent motivation transfer, sophisticated cross-turn dependency, is criticized all along. Nevertheless, no existing benchmarks can fully reflect these weaknesses. We present \\textbf{MARS-Bench}, a \\textbf{M}ulti-turn \\textbf{A}thletic \\textbf{R}eal-world \\textbf{S}cenario Dialogue \\textbf{Bench}mark, designed to remedy the gap. MARS-Bench is constructed from play-by-play text commentary so to feature realistic dialogues specifically designed to evaluate three critical aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that closed-source LLMs significantly outperform open-source alternatives, explicit reasoning significantly boosts LLMs' robustness on handling long complex dialogue sessions, and LLMs indeed face significant challenges when handling motivation transfer and sophisticated cross-turn dependency. Moreover, we provide mechanistic interpretability on how attention sinks due to special tokens lead to LLMs' performance degradation when handling long complex dialogue sessions based on attention visualization experiment in Qwen2.5-7B-Instruction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 13 figures, Accepted as EMNLP2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.23810v2",
    "published_date": "2025-05-27 10:28:04 UTC",
    "updated_date": "2025-09-15 02:12:59 UTC"
  },
  {
    "arxiv_id": "2505.20993v1",
    "title": "Who Reasons in the Large Language Models?",
    "authors": [
      "Jie Shao",
      "Jianxin Wu"
    ],
    "abstract": "Despite the impressive performance of large language models (LLMs), the process of endowing them with new capabilities--such as mathematical reasoning--remains largely empirical and opaque. A critical open question is whether reasoning abilities stem from the entire model, specific modules, or are merely artifacts of overfitting. In this work, we hypothesize that the reasoning capabilities in well-trained LLMs are primarily attributed to the output projection module (oproj) in the Transformer's multi-head self-attention (MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for Networks (SfN), a suite of diagnostic tools designed to probe and analyze the internal behaviors of LLMs. Using SfN, we provide both circumstantial and empirical evidence suggesting that oproj plays a central role in enabling reasoning, whereas other modules contribute more to fluent dialogue. These findings offer a new perspective on LLM interpretability and open avenues for more targeted training strategies, potentially enabling more efficient and specialized LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20993v1",
    "published_date": "2025-05-27 10:26:47 UTC",
    "updated_date": "2025-05-27 10:26:47 UTC"
  },
  {
    "arxiv_id": "2505.20979v2",
    "title": "MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection",
    "authors": [
      "Tongyu Lu",
      "Charlotta-Marlena Geist",
      "Jan Melechovsky",
      "Abhinaba Roy",
      "Dorien Herremans"
    ],
    "abstract": "We propose MelodySim, a melody-aware music similarity model and dataset for plagiarism detection. First, we introduce a novel method to construct a dataset focused on melodic similarity. By augmenting Slakh2100, an existing MIDI dataset, we generate variations of each piece while preserving the melody through modifications such as note splitting, arpeggiation, minor track dropout, and re-instrumentation. A user study confirms that positive pairs indeed contain similar melodies, while other musical tracks are significantly changed. Second, we develop a segment-wise melodic-similarity detection model that uses a MERT encoder and applies a triplet neural network to capture melodic similarity. The resulting decision matrix highlights where plagiarism might occur. The experiments show that our model is able to outperform baseline models in detecting similar melodic fragments on the MelodySim test set.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20979v2",
    "published_date": "2025-05-27 10:14:03 UTC",
    "updated_date": "2025-11-19 01:59:41 UTC"
  },
  {
    "arxiv_id": "2505.21582v1",
    "title": "AITEE -- Agentic Tutor for Electrical Engineering",
    "authors": [
      "Christopher Knievel",
      "Alexander Bernhardt",
      "Christian Bernhardt"
    ],
    "abstract": "Intelligent tutoring systems combined with large language models offer a promising approach to address students' diverse needs and promote self-efficacious learning. While large language models possess good foundational knowledge of electrical engineering basics, they remain insufficiently capable of addressing specific questions about electrical circuits. In this paper, we present AITEE, an agent-based tutoring system for electrical engineering designed to accompany students throughout their learning process, offer individualized support, and promote self-directed learning. AITEE supports both hand-drawn and digital circuits through an adapted circuit reconstruction process, enabling natural interaction with students. Our novel graph-based similarity measure identifies relevant context from lecture materials through a retrieval augmented generation approach, while parallel Spice simulation further enhances accuracy in applying solution methodologies. The system implements a Socratic dialogue to foster learner autonomy through guided questioning. Experimental evaluations demonstrate that AITEE significantly outperforms baseline approaches in domain-specific knowledge application, with even medium-sized LLM models showing acceptable performance. Our results highlight the potential of agentic tutors to deliver scalable, personalized, and effective learning environments for electrical engineering education.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "12 pages, 11 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.21582v1",
    "published_date": "2025-05-27 10:07:05 UTC",
    "updated_date": "2025-05-27 10:07:05 UTC"
  },
  {
    "arxiv_id": "2505.20973v2",
    "title": "Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement",
    "authors": [
      "Keheliya Gallaba",
      "Ali Arabat",
      "Dayi Lin",
      "Mohammed Sayagh",
      "Ahmed E. Hassan"
    ],
    "abstract": "Foundation Models (FMs) have shown remarkable capabilities in various natural language tasks. However, their ability to accurately capture stakeholder requirements remains a significant challenge for using FMs for software development. This paper introduces a novel approach that leverages an FM-powered multi-agent system called AlignMind to address this issue. By having a cognitive architecture that enhances FMs with Theory-of-Mind capabilities, our approach considers the mental states and perspectives of software makers. This allows our solution to iteratively clarify the beliefs, desires, and intentions of stakeholders, translating these into a set of refined requirements and a corresponding actionable natural language workflow in the often-overlooked requirements refinement phase of software engineering, which is crucial after initial elicitation. Through a multifaceted evaluation covering 150 diverse use cases, we demonstrate that our approach can accurately capture the intents and requirements of stakeholders, articulating them as both specifications and a step-by-step plan of action. Our findings suggest that the potential for significant improvements in the software development process justifies these investments. Our work lays the groundwork for future innovation in building intent-first development environments, where software makers can seamlessly collaborate with AIs to create software that truly meets their needs.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20973v2",
    "published_date": "2025-05-27 10:05:26 UTC",
    "updated_date": "2025-05-28 09:35:54 UTC"
  },
  {
    "arxiv_id": "2505.20972v1",
    "title": "Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs",
    "authors": [
      "Sen Bai",
      "Chunqi Yang",
      "Xin Bai",
      "Xin Zhang",
      "Zhengang Jiang"
    ],
    "abstract": "Along with AI computing shining in scientific discovery, its potential in the combinatorial optimization (CO) domain has also emerged in recent years. Yet, existing unsupervised neural network solvers struggle to solve $k$-grouping problems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs, due to limited computational frameworks. In this work, we propose Deep $k$-grouping, an unsupervised learning-based CO framework. Specifically, we contribute: Novel one-hot encoded polynomial unconstrained binary optimization (OH-PUBO), a formulation for modeling k-grouping problems on graphs and hypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated algorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs the relaxation of large-scale OH-PUBO objectives as differentiable loss functions and trains to optimize them in an unsupervised manner. To ensure scalability, it leverages GPU-accelerated algorithms to unify the training pipeline; A Gini coefficient-based continuous relaxation annealing strategy to enforce discreteness of solutions while preventing convergence to local optima. Experimental results demonstrate that Deep $k$-grouping outperforms existing neural network solvers and classical heuristics such as SCIP and Tabu.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20972v1",
    "published_date": "2025-05-27 10:04:54 UTC",
    "updated_date": "2025-05-27 10:04:54 UTC"
  },
  {
    "arxiv_id": "2505.20971v1",
    "title": "Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA",
    "authors": [
      "Xiangqing Shen",
      "Fanfan Wang",
      "Rui Xia"
    ],
    "abstract": "LLMs have demonstrated remarkable capabilities in complex reasoning tasks, yet they often suffer from hallucinations and lack reliable factual grounding. Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack the flexible reasoning abilities of LLMs. In this paper, we present Reason-Align-Respond (RAR), a novel framework that systematically integrates LLM reasoning with knowledge graphs for KGQA. Our approach consists of three key components: a Reasoner that generates human-like reasoning chains, an Aligner that maps these chains to valid KG paths, and a Responser that synthesizes the final answer. We formulate this process as a probabilistic model and optimize it using the Expectation-Maximization algorithm, which iteratively refines the reasoning chains and knowledge paths. Extensive experiments on multiple benchmarks demonstrate the effectiveness of RAR, achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on WebQSP and CWQ respectively. Human evaluation confirms that RAR generates high-quality, interpretable reasoning chains well-aligned with KG paths. Furthermore, RAR exhibits strong zero-shot generalization capabilities and maintains computational efficiency during inference.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20971v1",
    "published_date": "2025-05-27 10:04:53 UTC",
    "updated_date": "2025-05-27 10:04:53 UTC"
  },
  {
    "arxiv_id": "2505.20963v1",
    "title": "Context-Aware Content Moderation for German Newspaper Comments",
    "authors": [
      "Felix Krejca",
      "Tobias Kietreiber",
      "Alexander Buchelt",
      "Sebastian Neumaier"
    ],
    "abstract": "The increasing volume of online discussions requires advanced automatic content moderation to maintain responsible discourse. While hate speech detection on social media is well-studied, research on German-language newspaper forums remains limited. Existing studies often neglect platform-specific context, such as user history and article themes. This paper addresses this gap by developing and evaluating binary classification models for automatic content moderation in German newspaper forums, incorporating contextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging the One Million Posts Corpus from the Austrian newspaper Der Standard, we assess the impact of context-aware models. Results show that CNN and LSTM models benefit from contextual information and perform competitively with state-of-the-art approaches. In contrast, ChatGPT's zero-shot classification does not improve with added context and underperforms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20963v1",
    "published_date": "2025-05-27 09:57:02 UTC",
    "updated_date": "2025-05-27 09:57:02 UTC"
  },
  {
    "arxiv_id": "2505.20961v1",
    "title": "Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization",
    "authors": [
      "Yiyuan Yang",
      "Shitong Xu",
      "Niki Trigoni",
      "Andrew Markham"
    ],
    "abstract": "Sound source localization (SSL) is a critical technology for determining the position of sound sources in complex environments. However, existing methods face challenges such as high computational costs and precise calibration requirements, limiting their deployment in dynamic or resource-constrained environments. This paper introduces a novel 3D SSL framework, which uses sparse cross-attention, pretraining, and adaptive signal coherence metrics, to achieve accurate and computationally efficient localization with fewer input microphones. The framework is also fault-tolerant to unreliable or even unknown microphone position inputs, ensuring its applicability in real-world scenarios. Preliminary experiments demonstrate its scalability for multi-source localization without requiring additional hardware. This work advances SSL by balancing the model's performance and efficiency and improving its robustness for real-world scenarios.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Interspeech 2025 Conference",
    "pdf_url": "https://arxiv.org/pdf/2505.20961v1",
    "published_date": "2025-05-27 09:56:16 UTC",
    "updated_date": "2025-05-27 09:56:16 UTC"
  },
  {
    "arxiv_id": "2505.20956v2",
    "title": "Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection",
    "authors": [
      "Shiqi Zhang",
      "Tuomas Virtanen"
    ],
    "abstract": "Bioacoustic sound event detection (BioSED) is crucial for biodiversity conservation but faces practical challenges during model development and training: limited amounts of annotated data, sparse events, species diversity, and class imbalance. To address these challenges efficiently with a limited labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an active learning method integrating committee voting disagreement and diversity analysis. We also refine an existing BioSED dataset specifically for evaluating active learning algorithms. Experimental results demonstrate that MFFT achieves a mAP of 68% when cold-starting and 71% when warm-starting (which is close to the fully-supervised mAP of 75%) while using only 2.3% of the annotations. Notably, MFFT excels in cold-start scenarios and with rare species, which are critical for monitoring endangered species, demonstrating its practical value.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 1 figure, accepted by EUSIPCO 2025 v2: add our github repo",
    "pdf_url": "https://arxiv.org/pdf/2505.20956v2",
    "published_date": "2025-05-27 09:50:39 UTC",
    "updated_date": "2025-05-28 20:06:51 UTC"
  },
  {
    "arxiv_id": "2506.17232v1",
    "title": "PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation",
    "authors": [
      "Zelin Zang",
      "Fei Wang",
      "Liangyu Li",
      "Jinlin Wu",
      "Chunshui Zhao",
      "Zhen Lei",
      "Baigui Sun"
    ],
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Recent UDA methods based on Vision Transformers (ViTs) have achieved strong performance through attention-based feature alignment. However, we identify a key limitation: foreground object mismatch, where the discrepancy in foreground object size and spatial distribution across domains weakens attention consistency and hampers effective domain alignment. To address this issue, we propose the Progressive Focus Cross-Attention Mechanism (PCaM), which progressively filters out background information during cross-attention, allowing the model to focus on and fuse discriminative foreground semantics across domains. We further introduce an attentional guidance loss that explicitly directs attention toward task-relevant regions, enhancing cross-domain attention consistency. PCaM is lightweight, architecture-agnostic, and easy to integrate into existing ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet, VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly improves adaptation performance and achieves new state-of-the-art results, validating the effectiveness of attention-guided foreground fusion for domain adaptation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.17232v1",
    "published_date": "2025-05-27 09:48:29 UTC",
    "updated_date": "2025-05-27 09:48:29 UTC"
  },
  {
    "arxiv_id": "2505.20949v1",
    "title": "Streamlining Knowledge Graph Creation with PyRML",
    "authors": [
      "Andrea Giovanni Nuzzolese"
    ],
    "abstract": "Knowledge Graphs (KGs) are increasingly adopted as a foundational technology for integrating heterogeneous data in domains such as climate science, cultural heritage, and the life sciences. Declarative mapping languages like R2RML and RML have played a central role in enabling scalable and reusable KG construction, offering a transparent means of transforming structured and semi-structured data into RDF. In this paper, we present PyRML, a lightweight, Python-native library for building Knowledge Graphs through declarative mappings. PyRML supports core RML constructs and provides a programmable interface for authoring, executing, and testing mappings directly within Python environments. It integrates with popular data and semantic web libraries (e.g., Pandas and RDFlib), enabling transparent and modular workflows. By lowering the barrier to entry for KG creation and fostering reproducible, ontology-aligned data integration, PyRML bridges the gap between declarative semantics and practical KG engineering.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20949v1",
    "published_date": "2025-05-27 09:40:29 UTC",
    "updated_date": "2025-05-27 09:40:29 UTC"
  },
  {
    "arxiv_id": "2505.20948v1",
    "title": "Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs",
    "authors": [
      "Yisen Gao",
      "Jiaxin Bai",
      "Tianshi Zheng",
      "Qingyun Sun",
      "Ziwei Zhang",
      "Jianxin Li",
      "Yangqiu Song",
      "Xingcheng Fu"
    ],
    "abstract": "Abductive reasoning in knowledge graphs aims to generate plausible logical hypotheses from observed entities, with broad applications in areas such as clinical diagnosis and scientific discovery. However, due to a lack of controllability, a single observation may yield numerous plausible but redundant or irrelevant hypotheses on large-scale knowledge graphs. To address this limitation, we introduce the task of controllable hypothesis generation to improve the practical utility of abductive reasoning. This task faces two key challenges when controlling for generating long and complex logical hypotheses: hypothesis space collapse and hypothesis oversensitivity. To address these challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation framework for abductive reasoning over knowledge graphs, trained in a two-stage paradigm including supervised learning and subsequent reinforcement learning. To mitigate hypothesis space collapse, we design a dataset augmentation strategy based on sub-logical decomposition, enabling the model to learn complex logical structures by leveraging semantic patterns in simpler components. To address hypothesis oversensitivity, we incorporate smoothed semantic rewards including Dice and Overlap scores, and introduce a condition-adherence reward to guide the generation toward user-specified control constraints. Extensive experiments on three benchmark datasets demonstrate that our model not only better adheres to control conditions but also achieves superior semantic similarity performance compared to baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2505.20948v1",
    "published_date": "2025-05-27 09:36:47 UTC",
    "updated_date": "2025-05-27 09:36:47 UTC"
  },
  {
    "arxiv_id": "2505.20947v1",
    "title": "Unified Deep Learning Approach for Estimating the Metallicities of RR Lyrae Stars Using light curves from Gaia Data Release 3",
    "authors": [
      "Lorenzo Monti",
      "Tatiana Muraveva",
      "Alessia Garofalo",
      "Gisella Clementini",
      "Maria Letizia Valentini"
    ],
    "abstract": "RR Lyrae stars (RRLs) are old pulsating variables widely used as metallicity tracers due to the correlation between their metal abundances and light curve morphology. With ESA Gaia DR3 providing light curves for about 270,000 RRLs, there is a pressing need for scalable methods to estimate their metallicities from photometric data. We introduce a unified deep learning framework that estimates metallicities for both fundamental-mode (RRab) and first-overtone (RRc) RRLs using Gaia G-band light curves. This approach extends our previous work on RRab stars to include RRc stars, aiming for high predictive accuracy and broad generalization across both pulsation types. The model is based on a Gated Recurrent Unit (GRU) neural network optimized for time-series extrinsic regression. Our pipeline includes preprocessing steps such as phase folding, smoothing, and sample weighting, and uses photometric metallicities from the literature as training targets. The architecture is designed to handle morphological differences between RRab and RRc light curves without requiring separate models. On held-out validation sets, our GRU model achieves strong performance: for RRab stars, MAE = 0.0565 dex, RMSE = 0.0765 dex, R^2 = 0.9401; for RRc stars, MAE = 0.0505 dex, RMSE = 0.0720 dex, R^2 = 0.9625. These results show the effectiveness of deep learning for large-scale photometric metallicity estimation and support its application to studies of stellar populations and Galactic structure.",
    "categories": [
      "astro-ph.SR",
      "cs.AI"
    ],
    "primary_category": "astro-ph.SR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20947v1",
    "published_date": "2025-05-27 09:34:22 UTC",
    "updated_date": "2025-05-27 09:34:22 UTC"
  },
  {
    "arxiv_id": "2505.20925v1",
    "title": "Multi-objective Large Language Model Alignment with Hierarchical Experts",
    "authors": [
      "Zhuo Li",
      "Guodong Du",
      "Weiyang Guo",
      "Yigeng Zhou",
      "Xiucheng Li",
      "Wenya Wang",
      "Fangming Liu",
      "Yequan Wang",
      "Deheng Ye",
      "Min Zhang",
      "Jing Li"
    ],
    "abstract": "Aligning large language models (LLMs) to simultaneously satisfy multiple objectives remains a significant challenge, especially given the diverse and often conflicting nature of human preferences. Existing alignment methods struggle to balance trade-offs effectively, often requiring costly retraining or yielding suboptimal results across the Pareto frontier of preferences. In this paper, we introduce \\textit{HoE}(Hierarchical Mixture-of-Experts), a \\textit{lightweight}, \\textit{parameter-efficient}, and \\textit{plug-and-play} approach that eliminates the need for model training, while enabling LLMs to adapt across the entire Pareto frontier and accommodate diverse user preferences. In particular, \\textit{HoE} consists of three hierarchical components: LoRA Experts, Router Experts and Preference Routing, reaching optimal Pareto frontiers and achieving a trade-off between parameter size, training cost, and performance. We evaluate \\textit{HoE} across various tasks on 14 objectives and 200 different preferences among 6 benchmarks, demonstrating superior performance over 15 recent baselines. Code is available in the supplementary materials.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20925v1",
    "published_date": "2025-05-27 09:15:03 UTC",
    "updated_date": "2025-05-27 09:15:03 UTC"
  },
  {
    "arxiv_id": "2505.20922v2",
    "title": "Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective",
    "authors": [
      "Yang Zhang",
      "Xinran Li",
      "Jianing Ye",
      "Shuang Qiu",
      "Delin Qu",
      "Xiu Li",
      "Chongjie Zhang",
      "Chenjia Bai"
    ],
    "abstract": "World models have recently attracted growing interest in Multi-Agent Reinforcement Learning (MARL) due to their ability to improve sample efficiency for policy learning. However, accurately modeling environments in MARL is challenging due to the exponentially large joint action space and highly uncertain dynamics inherent in multi-agent systems. To address this, we reduce modeling complexity by shifting from jointly modeling the entire state-action transition dynamics to focusing on the state space alone at each timestep through sequential agent modeling. Specifically, our approach enables the model to progressively resolve uncertainty while capturing the structured dependencies among agents, providing a more accurate representation of how agents influence the state. Interestingly, this sequential revelation of agents' actions in a multi-agent system aligns with the reverse process in diffusion models--a class of powerful generative models known for their expressiveness and training stability compared to autoregressive or latent variable models. Leveraging this insight, we develop a flexible and robust world model for MARL using diffusion models. Our method, Diffusion-Inspired Multi-Agent world model (DIMA), achieves state-of-the-art performance across multiple multi-agent control benchmarks, significantly outperforming prior world models in terms of final return and sample efficiency, including MAMuJoCo and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent world models, advancing the frontier of MARL research. Codes are open-sourced at https://github.com/breez3young/DIMA.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted at NIPS'25",
    "pdf_url": "https://arxiv.org/pdf/2505.20922v2",
    "published_date": "2025-05-27 09:11:38 UTC",
    "updated_date": "2025-10-24 13:51:44 UTC"
  },
  {
    "arxiv_id": "2505.20921v2",
    "title": "Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models",
    "authors": [
      "Injae Na",
      "Keonwoong Noh",
      "Woohwan Jung"
    ],
    "abstract": "LLM providers typically offer multiple LLM tiers, varying in performance and price. As NLP tasks become more complex and modularized, selecting the suitable LLM tier for each subtask is a key challenge to balance between cost and performance. To address the problem, we introduce LLM Automatic Transmission (LLM-AT) framework that automatically selects LLM tiers without training. LLM-AT consists of Starter, Generator, and Judge. The starter selects the initial LLM tier expected to solve the given question, the generator produces a response using the LLM of the selected tier, and the judge evaluates the validity of the response. If the response is invalid, LLM-AT iteratively upgrades to a higher-tier model, generates a new response, and re-evaluates until a valid response is obtained. Additionally, we propose accuracy estimator, which enables the suitable initial LLM tier selection without training. Given an input question, accuracy estimator estimates the expected accuracy of each LLM tier by computing the valid response rate across top-k similar queries from past inference records. Experiments demonstrate that LLM-AT achieves superior performance while reducing costs, making it a practical solution for real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2505.20921v2",
    "published_date": "2025-05-27 09:11:00 UTC",
    "updated_date": "2025-05-29 05:05:27 UTC"
  },
  {
    "arxiv_id": "2505.20918v1",
    "title": "Humble AI in the real-world: the case of algorithmic hiring",
    "authors": [
      "Rahul Nair",
      "Inge Vejsbjerg",
      "Elizabeth Daly",
      "Christos Varytimidis",
      "Bran Knowles"
    ],
    "abstract": "Humble AI (Knowles et al., 2023) argues for cautiousness in AI development and deployments through scepticism (accounting for limitations of statistical learning), curiosity (accounting for unexpected outcomes), and commitment (accounting for multifaceted values beyond performance). We present a real-world case study for humble AI in the domain of algorithmic hiring. Specifically, we evaluate virtual screening algorithms in a widely used hiring platform that matches candidates to job openings. There are several challenges in misrecognition and stereotyping in such contexts that are difficult to assess through standard fairness and trust frameworks; e.g., someone with a non-traditional background is less likely to rank highly. We demonstrate technical feasibility of how humble AI principles can be translated to practice through uncertainty quantification of ranks, entropy estimates, and a user experience that highlights algorithmic unknowns. We describe preliminary discussions with focus groups made up of recruiters. Future user studies seek to evaluate whether the higher cognitive load of a humble AI system fosters a climate of trust in its outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "CHIWORK '25, Symposium on Human-Computer Interaction for Work, June 23--25, 2025, Amsterdam, Netherlands Late Breaking Work",
    "pdf_url": "https://arxiv.org/pdf/2505.20918v1",
    "published_date": "2025-05-27 09:09:38 UTC",
    "updated_date": "2025-05-27 09:09:38 UTC"
  },
  {
    "arxiv_id": "2505.20901v1",
    "title": "A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models",
    "authors": [
      "Junhyuk Choi",
      "Minju Kim",
      "Yeseon Hong",
      "Bugeun Kim"
    ],
    "abstract": "As large vision language models(LVLMs) rapidly advance, concerns about their potential to learn and generate social biases and stereotypes are increasing. Previous studies on LVLM's stereotypes face two primary limitations: metrics that overlooked the importance of content words, and datasets that overlooked the effect of color. To address these limitations, this study introduces new evaluation metrics based on the Stereotype Content Model (SCM). We also propose BASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM metrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes. As a result, we found three findings. (1) The SCM-based evaluation is effective in capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output along with gender and race ones. (3) Interaction between model architecture and parameter sizes seems to affect stereotypes. We release BASIC publicly on [anonymized for review].",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2505.20901v1",
    "published_date": "2025-05-27 08:44:05 UTC",
    "updated_date": "2025-05-27 08:44:05 UTC"
  },
  {
    "arxiv_id": "2505.20897v2",
    "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation",
    "authors": [
      "Pingrui Zhang",
      "Yifei Su",
      "Pengyuan Wu",
      "Dong An",
      "Li Zhang",
      "Zhigang Wang",
      "Dong Wang",
      "Yan Ding",
      "Bin Zhao",
      "Xuelong Li"
    ],
    "abstract": "Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \\textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20897v2",
    "published_date": "2025-05-27 08:40:20 UTC",
    "updated_date": "2025-06-22 13:53:33 UTC"
  },
  {
    "arxiv_id": "2505.23809v2",
    "title": "LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion",
    "authors": [
      "Haowei Yang",
      "Haotian Lyu",
      "Tianle Zhang",
      "Dingzhou Wang",
      "Yushang Zhao"
    ],
    "abstract": "As e-commerce competition intensifies, balancing creative content with conversion effectiveness becomes critical. Leveraging LLMs' language generation capabilities, we propose a framework that integrates prompt engineering, multi-objective fine-tuning, and post-processing to generate marketing copy that is both engaging and conversion-driven. Our fine-tuning method combines sentiment adjustment, diversity enhancement, and CTA embedding. Through offline evaluations and online A/B tests across categories, our approach achieves a 12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content novelty. This provides a practical solution for automated copy generation and suggests paths for future multimodal, real-time personalization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23809v2",
    "published_date": "2025-05-27 08:40:11 UTC",
    "updated_date": "2025-06-03 07:39:21 UTC"
  },
  {
    "arxiv_id": "2505.20896v2",
    "title": "How Do Transformers Learn Variable Binding in Symbolic Programs?",
    "authors": [
      "Yiwei Wu",
      "Atticus Geiger",
      "Raphaël Millière"
    ],
    "abstract": "Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches. To facilitate reproducible research, we developed Variable Scope, an interactive web platform for exploring our findings at https://variablescope.org",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 10 figures, 1 table. To appear in the Proceedings of the 42nd International Conference on Machine Learning (ICML 2025). v2: Added link to Variable Scope in abstract",
    "pdf_url": "https://arxiv.org/pdf/2505.20896v2",
    "published_date": "2025-05-27 08:39:20 UTC",
    "updated_date": "2025-05-30 18:08:50 UTC"
  },
  {
    "arxiv_id": "2505.21577v3",
    "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving",
    "authors": [
      "Huacan Wang",
      "Ziyi Ni",
      "Shuo Zhang",
      "Shuo Lu",
      "Sen Hu",
      "Ziyang He",
      "Chen Hu",
      "Jiaye Lin",
      "Yifu Guo",
      "Ronghao Chen",
      "Xin Li",
      "Daxin Jiang",
      "Yuntao Du",
      "Pin Lyu"
    ],
    "abstract": "The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "A novel approach; Very practical",
    "pdf_url": "https://arxiv.org/pdf/2505.21577v3",
    "published_date": "2025-05-27 08:35:05 UTC",
    "updated_date": "2025-08-25 13:40:36 UTC"
  },
  {
    "arxiv_id": "2505.20890v1",
    "title": "Frequency Composition for Compressed and Domain-Adaptive Neural Networks",
    "authors": [
      "Yoojin Kwon",
      "Hongjun Suh",
      "Wooseok Lee",
      "Taesik Gong",
      "Songyi Han",
      "Hyung-Sin Kim"
    ],
    "abstract": "Modern on-device neural network applications must operate under resource constraints while adapting to unpredictable domain shifts. However, this combined challenge-model compression and domain adaptation-remains largely unaddressed, as prior work has tackled each issue in isolation: compressed networks prioritize efficiency within a fixed domain, whereas large, capable models focus on handling domain shifts. In this work, we propose CoDA, a frequency composition-based framework that unifies compression and domain adaptation. During training, CoDA employs quantization-aware training (QAT) with low-frequency components, enabling a compressed model to selectively learn robust, generalizable features. At test time, it refines the compact model in a source-free manner (i.e., test-time adaptation, TTA), leveraging the full-frequency information from incoming data to adapt to target domains while treating high-frequency components as domain-specific cues. LFC are aligned with the trained distribution, while HFC unique to the target distribution are solely utilized for batch normalization. CoDA can be integrated synergistically into existing QAT and TTA methods. CoDA is evaluated on widely used domain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various model architectures. With significant compression, it achieves accuracy improvements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the full-precision TTA baseline.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2505.20890v1",
    "published_date": "2025-05-27 08:33:04 UTC",
    "updated_date": "2025-05-27 08:33:04 UTC"
  },
  {
    "arxiv_id": "2505.20889v1",
    "title": "Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment",
    "authors": [
      "Leizhen Wang",
      "Peibo Duan",
      "Cheng Lyu",
      "Zhenliang Ma"
    ],
    "abstract": "Modern navigation systems and shared mobility platforms increasingly rely on personalized route recommendations to improve individual travel experience and operational efficiency. However, a key question remains: can such sequential, personalized routing decisions collectively lead to system-optimal (SO) traffic assignment? This paper addresses this question by proposing a learning-based framework that reformulates the static SO traffic assignment problem as a single-agent deep reinforcement learning (RL) task. A central agent sequentially recommends routes to travelers as origin-destination (OD) demands arrive, to minimize total system travel time. To enhance learning efficiency and solution quality, we develop an MSA-guided deep Q-learning algorithm that integrates the iterative structure of traditional traffic assignment methods into the RL training process. The proposed approach is evaluated on both the Braess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent converges to the theoretical SO solution in the Braess network and achieves only a 0.35% deviation in the OW network. Further ablation studies demonstrate that the route action set's design significantly impacts convergence speed and final performance, with SO-informed route sets leading to faster learning and better outcomes. This work provides a theoretically grounded and practically relevant approach to bridging individual routing behavior with system-level efficiency through learning-based sequential assignment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20889v1",
    "published_date": "2025-05-27 08:33:02 UTC",
    "updated_date": "2025-05-27 08:33:02 UTC"
  },
  {
    "arxiv_id": "2505.20888v2",
    "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models",
    "authors": [
      "Chengyu Wang",
      "Junbing Yan",
      "Wenrui Cai",
      "Yuanhao Yue",
      "Jun Huang"
    ],
    "abstract": "In this paper, we present EasyDistill, a comprehensive toolkit designed for effective black-box and white-box knowledge distillation (KD) of large language models (LLMs). Our framework offers versatile functionalities, including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning techniques specifically tailored for KD scenarios. The toolkit accommodates KD functionalities for both System 1 (fast, intuitive) and System 2 (slow, analytical) models. With its modular design and user-friendly interface, EasyDistill empowers researchers and industry practitioners to seamlessly experiment with and implement state-of-the-art KD strategies for LLMs. In addition, EasyDistill provides a series of robust distilled models and KD-based industrial solutions developed by us, along with the corresponding open-sourced datasets, catering to a variety of use cases. Furthermore, we describe the seamless integration of EasyDistill into Alibaba Cloud's Platform for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for LLMs more accessible and impactful within the NLP community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20888v2",
    "published_date": "2025-05-27 08:32:51 UTC",
    "updated_date": "2025-06-27 07:59:43 UTC"
  },
  {
    "arxiv_id": "2505.20881v1",
    "title": "Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization",
    "authors": [
      "Yiding Shi",
      "Jianan Zhou",
      "Wen Song",
      "Jieyi Bi",
      "Yaoxin Wu",
      "Jie Zhang"
    ],
    "abstract": "Heuristic design with large language models (LLMs) has emerged as a promising approach for tackling combinatorial optimization problems (COPs). However, existing approaches often rely on manually predefined evolutionary computation (EC) optimizers and single-task training schemes, which may constrain the exploration of diverse heuristic algorithms and hinder the generalization of the resulting heuristics. To address these issues, we propose Meta-Optimization of Heuristics (MoH), a novel framework that operates at the optimizer level, discovering effective optimizers through the principle of meta-learning. Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that autonomously constructs diverse optimizers through (self-)invocation, thereby eliminating the reliance on a predefined EC optimizer. These constructed optimizers subsequently evolve heuristics for downstream tasks, enabling broader heuristic exploration. Moreover, MoH employs a multi-task training scheme to promote its generalization capability. Experiments on classic COPs demonstrate that MoH constructs an effective and interpretable meta-optimizer, achieving state-of-the-art performance across various downstream tasks, particularly in cross-size settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20881v1",
    "published_date": "2025-05-27 08:26:27 UTC",
    "updated_date": "2025-05-27 08:26:27 UTC"
  },
  {
    "arxiv_id": "2505.20875v3",
    "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties",
    "authors": [
      "Jiyoung Lee",
      "Seungho Kim",
      "Jieun Han",
      "Jun-Min Lee",
      "Kitaek Kim",
      "Alice Oh",
      "Edward Choi"
    ],
    "abstract": "Large Language Models (LLMs) are predominantly evaluated on Standard American English (SAE), often overlooking the diversity of global English varieties. This narrow focus may raise fairness concerns as degraded performance on non-standard varieties can lead to unequal benefits for users worldwide. Therefore, it is critical to extensively evaluate the linguistic robustness of LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness. Our framework combines (1) linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora, and (2) LLM-based transformations to ensure both linguistic validity and scalability. Using Trans-EnV, we transform six benchmark datasets into 38 English varieties and evaluate seven state-of-the-art LLMs. Our results reveal significant performance disparities, with accuracy decreasing by up to 46.3% on non-standard varieties. These findings highlight the importance of comprehensive linguistic robustness evaluation across diverse English varieties. Each construction of Trans-EnV was validated through rigorous statistical testing and consultation with a researcher in the field of second language acquisition, ensuring its linguistic validity. Our code and datasets are publicly available at https://github.com/jiyounglee-0523/TransEnV and https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2025 Track on Datasets and Benchmarks (27 pages, 6 figures, 16 tables)",
    "pdf_url": "https://arxiv.org/pdf/2505.20875v3",
    "published_date": "2025-05-27 08:23:27 UTC",
    "updated_date": "2025-10-09 08:41:44 UTC"
  },
  {
    "arxiv_id": "2505.20872v1",
    "title": "In Context Learning with Vision Transformers: Case Study",
    "authors": [
      "Antony Zhao",
      "Alex Proshkin",
      "Fergal Hennessy",
      "Francesco Crivelli"
    ],
    "abstract": "Large transformer models have been shown to be capable of performing in-context learning. By using examples in a prompt as well as a query, they are capable of performing tasks such as few-shot, one-shot, or zero-shot learning to output the corresponding answer to this query. One area of interest to us is that these transformer models have been shown to be capable of learning the general class of certain functions, such as linear functions and small 2-layer neural networks, on random data (Garg et al, 2023). We aim to extend this to the image space to analyze their capability to in-context learn more complex functions on the image space, such as convolutional neural networks and other methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 16 figures. UC Berkeley research project",
    "pdf_url": "https://arxiv.org/pdf/2505.20872v1",
    "published_date": "2025-05-27 08:22:08 UTC",
    "updated_date": "2025-05-27 08:22:08 UTC"
  },
  {
    "arxiv_id": "2505.20869v1",
    "title": "Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving",
    "authors": [
      "Kuo Zhou",
      "Lu Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated formidable capabilities in solving mathematical problems, yet they may still commit logical reasoning and computational errors during the problem-solving process. Thus, this paper proposes a framework, MATH-VF, which includes a Formalizer and a Critic, for formally verifying the correctness of the solutions generated by large language models. Our framework first utilizes a Formalizer which employs an LLM to translate a natural language solution into a formal context. Afterward, our Critic (which integrates various external tools such as a Computer Algebra System and an SMT solver) evaluates the correctness of each statement within the formal context, and when a statement is incorrect, our Critic provides corrective feedback. We empirically investigate the effectiveness of MATH-VF in two scenarios: 1) Verification: MATH-VF is utilized to determine the correctness of a solution to a given problem. 2) Refinement: When MATH-VF identifies errors in the solution generated by an LLM-based solution generator for a given problem, it submits the corrective suggestions proposed by the Critic to the solution generator to regenerate the solution. We evaluate our framework on widely used mathematical benchmarks: MATH500 and ProcessBench, demonstrating the superiority of our approach over existing approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20869v1",
    "published_date": "2025-05-27 08:21:07 UTC",
    "updated_date": "2025-05-27 08:21:07 UTC"
  },
  {
    "arxiv_id": "2505.20868v2",
    "title": "Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech",
    "authors": [
      "Nam-Gyu Kim",
      "Deok-Hyeon Cho",
      "Seung-Bin Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Recent advances in expressive text-to-speech (TTS) have introduced diverse methods based on style embedding extracted from reference speech. However, synthesizing high-quality expressive speech remains challenging. We propose Spotlight-TTS, which exclusively emphasizes style via voiced-aware style extraction and style direction adjustment. Voiced-aware style extraction focuses on voiced regions highly related to style while maintaining continuity across different speech regions to improve expressiveness. We adjust the direction of the extracted style for optimal integration into the TTS model, which improves speech quality. Experimental results demonstrate that Spotlight-TTS achieves superior performance compared to baseline models in terms of expressiveness, overall speech quality, and style transfer capability. Our audio samples are publicly available.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Proceedings of Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20868v2",
    "published_date": "2025-05-27 08:20:01 UTC",
    "updated_date": "2025-06-29 12:42:04 UTC"
  },
  {
    "arxiv_id": "2505.23808v1",
    "title": "DenseLoRA: Dense Low-Rank Adaptation of Large Language Models",
    "authors": [
      "Lin Mu",
      "Xiaoyu Wang",
      "Li Ni",
      "Yang Li",
      "Zhize Wu",
      "Peiquan Jin",
      "Yiwen Zhang"
    ],
    "abstract": "Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA's components on overall model performance. Code is available at https://github.com/mulin-ahu/DenseLoRA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23808v1",
    "published_date": "2025-05-27 08:19:07 UTC",
    "updated_date": "2025-05-27 08:19:07 UTC"
  },
  {
    "arxiv_id": "2505.20866v1",
    "title": "Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification",
    "authors": [
      "Xinjie Lin",
      "Gang Xiong",
      "Gaopeng Gou",
      "Wenqi Dong",
      "Jing Yu",
      "Zhen Li",
      "Wei Xia"
    ],
    "abstract": "Encrypted traffic classification is highly challenging in network security due to the need for extracting robust features from content-agnostic traffic data. Existing approaches face critical issues: (i) Distribution drift, caused by reliance on the closedworld assumption, limits adaptability to realworld, shifting patterns; (ii) Dependence on labeled data restricts applicability where such data is scarce or unavailable. Large language models (LLMs) have demonstrated remarkable potential in offering generalizable solutions across a wide range of tasks, achieving notable success in various specialized fields. However, their effectiveness in traffic analysis remains constrained by challenges in adapting to the unique requirements of the traffic domain. In this paper, we introduce a novel traffic representation model named Encrypted Traffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which integrates LLMs with knowledge of traffic structures through a self-supervised instruction tuning paradigm. This framework establishes connections between textual information and traffic interactions. ETooL demonstrates more robust classification performance and superior generalization in both supervised and zero-shot traffic classification tasks. Notably, it achieves significant improvements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%), APP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.) to 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic dataset designed to support dynamic distributional shifts, and use it to validate ETooL's effectiveness under varying distributional conditions. Furthermore, we evaluate the efficiency gains achieved through ETooL's instruction tuning approach.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "IEEE Transactions on Information Forensics and Security (TIFS) camera ready, 15 pages, 6 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.20866v1",
    "published_date": "2025-05-27 08:18:16 UTC",
    "updated_date": "2025-05-27 08:18:16 UTC"
  },
  {
    "arxiv_id": "2506.08023v1",
    "title": "Aligning Proteins and Language: A Foundation Model for Protein Retrieval",
    "authors": [
      "Qifeng Wu",
      "Zhengzhe Liu",
      "Han Zhu",
      "Yizhou Zhao",
      "Daisuke Kihara",
      "Min Xu"
    ],
    "abstract": "This paper aims to retrieve proteins with similar structures and semantics from large-scale protein dataset, facilitating the functional interpretation of protein structures derived by structural determination methods like cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of vision-language models (VLMs), we propose a CLIP-style framework for aligning 3D protein structures with functional annotations using contrastive learning. For model training, we propose a large-scale dataset of approximately 200,000 protein-caption pairs with rich functional descriptors. We evaluate our model in both in-domain and more challenging cross-database retrieval on Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In both cases, our approach demonstrates promising zero-shot retrieval performance, highlighting the potential of multimodal foundation models for structure-function understanding in protein biology.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.CE",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "4 pages for body, 3 pages for appendix, 11 figures. Accepted to CVPR 2025 Workshop on Multimodal Foundation Models for Biomedicine: Challenges and Opportunities(MMFM-BIOMED)",
    "pdf_url": "https://arxiv.org/pdf/2506.08023v1",
    "published_date": "2025-05-27 08:13:08 UTC",
    "updated_date": "2025-05-27 08:13:08 UTC"
  },
  {
    "arxiv_id": "2506.14787v2",
    "title": "Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems",
    "authors": [
      "Funing Li",
      "Yuan Tian",
      "Ruben Noortwyck",
      "Jifeng Zhou",
      "Liming Kuang",
      "Robert Schulz"
    ],
    "abstract": "In modern industrial and logistics environments, the rapid expansion of fast delivery services has heightened the demand for storage systems that combine high efficiency with increased density. Multi-deep autonomous vehicle storage and retrieval systems (AVS/RS) present a viable solution for achieving greater storage density. However, these systems encounter significant challenges during retrieval operations due to lane blockages. A conventional approach to mitigate this issue involves storing items with homogeneous characteristics in a single lane, but this strategy restricts the flexibility and adaptability of multi-deep storage systems.\n  In this study, we propose a deep reinforcement learning-based framework to address the retrieval problem in multi-deep storage systems with heterogeneous item configurations. Each item is associated with a specific due date, and the objective is to minimize total tardiness. To effectively capture the system's topology, we introduce a graph-based state representation that integrates both item attributes and the local topological structure of the multi-deep warehouse. To process this representation, we design a novel neural network architecture that combines a Graph Neural Network (GNN) with a Transformer model. The GNN encodes topological and item-specific information into embeddings for all directly accessible items, while the Transformer maps these embeddings into global priority assignments. The Transformer's strong generalization capability further allows our approach to be applied to storage systems with diverse layouts. Extensive numerical experiments, including comparisons with heuristic methods, demonstrate the superiority of the proposed neural network architecture and the effectiveness of the trained agent in optimizing retrieval tardiness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14787v2",
    "published_date": "2025-05-27 08:07:38 UTC",
    "updated_date": "2025-09-15 00:47:00 UTC"
  },
  {
    "arxiv_id": "2505.20854v2",
    "title": "An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks",
    "authors": [
      "Xin Zhou",
      "Kisub Kim",
      "Ting Zhang",
      "Martin Weyssow",
      "Luis F. Gomes",
      "Guang Yang",
      "Kui Liu",
      "Xin Xia",
      "David Lo"
    ],
    "abstract": "Large Language Models (LLMs) and other automated techniques have been increasingly used to support software developers by generating software artifacts such as code snippets, patches, and comments. However, accurately assessing the correctness of these generated artifacts remains a significant challenge. On one hand, human evaluation provides high accuracy but is labor-intensive and lacks scalability. On the other hand, many automatic evaluation metrics are scalable and require minimal human effort, but they often fail to accurately reflect the actual correctness of generated software artifacts.\n  In this paper, we present SE-Jury, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. SE-Jury first defines five distinct evaluation strategies, each implemented by an independent judge. A dynamic team selection mechanism then identifies the most appropriate subset of judges as a team to produce a final correctness score through ensembling. We evaluate SE-Jury across a diverse set of software engineering (SE) benchmarks that span three popular SE tasks: code generation, automated program repair, and code summarization. Results demonstrate that SE-Jury consistently achieves a higher correlation with human judgments, with improvements ranging from 29.6% to 140.8% over existing automatic metrics. SE-Jury reaches agreement levels with human annotators that are close to inter-annotator agreement in code generation and program repair. These findings underscore SE-Jury's potential as a scalable and reliable alternative to human evaluation in these SE tasks.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.20854v2",
    "published_date": "2025-05-27 08:04:34 UTC",
    "updated_date": "2025-10-10 09:54:06 UTC"
  },
  {
    "arxiv_id": "2505.20853v2",
    "title": "Cooperation of Experts: Fusing Heterogeneous Information with Large Margin",
    "authors": [
      "Shuo Wang",
      "Shunyang Huang",
      "Jinghui Yuan",
      "Zhixiang Shen",
      "Zhao Kang"
    ],
    "abstract": "Fusing heterogeneous information remains a persistent challenge in modern data analysis. While significant progress has been made, existing approaches often fail to account for the inherent heterogeneity of object patterns across different semantic spaces. To address this limitation, we propose the Cooperation of Experts (CoE) framework, which encodes multi-typed information into unified heterogeneous multiplex networks. By overcoming modality and connection differences, CoE provides a powerful and flexible model for capturing the intricate structures of real-world complex data. In our framework, dedicated encoders act as domain-specific experts, each specializing in learning distinct relational patterns in specific semantic spaces. To enhance robustness and extract complementary knowledge, these experts collaborate through a novel large margin mechanism supported by a tailored optimization strategy. Rigorous theoretical analyses guarantee the framework's feasibility and stability, while extensive experiments across diverse benchmarks demonstrate its superior performance and broad applicability. Our code is available at https://github.com/strangeAlan/CoE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20853v2",
    "published_date": "2025-05-27 08:04:32 UTC",
    "updated_date": "2025-05-28 04:31:56 UTC"
  },
  {
    "arxiv_id": "2505.21576v1",
    "title": "Concentration Distribution Learning from Label Distributions",
    "authors": [
      "Jiawei Tang",
      "Yuheng Jia"
    ],
    "abstract": "Label distribution learning (LDL) is an effective method to predict the relative label description degree (a.k.a. label distribution) of a sample. However, the label distribution is not a complete representation of an instance because it overlooks the absolute intensity of each label. Specifically, it's impossible to obtain the total description degree of hidden labels that not in the label space, which leads to the loss of information and confusion in instances. To solve the above problem, we come up with a new concept named background concentration to serve as the absolute description degree term of the label distribution and introduce it into the LDL process, forming the improved paradigm of concentration distribution learning. Moreover, we propose a novel model by probabilistic methods and neural networks to learn label distributions and background concentrations from existing LDL datasets. Extensive experiments prove that the proposed approach is able to extract background concentrations from label distributions while producing more accurate prediction results than the state-of-the-art LDL methods. The code is available in https://github.com/seutjw/CDL-LD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21576v1",
    "published_date": "2025-05-27 07:53:27 UTC",
    "updated_date": "2025-05-27 07:53:27 UTC"
  },
  {
    "arxiv_id": "2505.21575v1",
    "title": "StreamLink: Large-Language-Model Driven Distributed Data Engineering System",
    "authors": [
      "Dawei Feng",
      "Di Mei",
      "Huiri Tan",
      "Lei Ren",
      "Xianying Lou",
      "Zhangxi Tan"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable proficiency in natural language understanding (NLU), opening doors for innovative applications. We introduce StreamLink - an LLM-driven distributed data system designed to improve the efficiency and accessibility of data engineering tasks. We build StreamLink on top of distributed frameworks such as Apache Spark and Hadoop to handle large data at scale. One of the important design philosophies of StreamLink is to respect user data privacy by utilizing local fine-tuned LLMs instead of a public AI service like ChatGPT. With help from domain-adapted LLMs, we can improve our system's understanding of natural language queries from users in various scenarios and simplify the procedure of generating database queries like the Structured Query Language (SQL) for information processing. We also incorporate LLM-based syntax and security checkers to guarantee the reliability and safety of each generated query. StreamLink illustrates the potential of merging generative LLMs with distributed data processing for comprehensive and user-centric data engineering. With this architecture, we allow users to interact with complex database systems at different scales in a user-friendly and security-ensured manner, where the SQL generation reaches over 10\\% of execution accuracy compared to baseline methods, and allow users to find the most concerned item from hundreds of millions of items within a few seconds using natural language.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "Accepted by CIKM Workshop 2024, https://sites.google.com/view/cikm2024-rag/papers?authuser=0#h.ddm5fg2z885t",
    "pdf_url": "https://arxiv.org/pdf/2505.21575v1",
    "published_date": "2025-05-27 07:44:16 UTC",
    "updated_date": "2025-05-27 07:44:16 UTC"
  },
  {
    "arxiv_id": "2505.23807v3",
    "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
    "authors": [
      "Yuli Chen",
      "Bo Cheng",
      "Jiale Han",
      "Yingying Zhang",
      "Yingting Li",
      "Shuhao Zhang"
    ],
    "abstract": "Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.23807v3",
    "published_date": "2025-05-27 07:35:00 UTC",
    "updated_date": "2025-06-03 03:06:29 UTC"
  },
  {
    "arxiv_id": "2505.20824v1",
    "title": "MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems",
    "authors": [
      "Kai Chen",
      "Taihang Zhen",
      "Hewei Wang",
      "Kailai Liu",
      "Xinfeng Li",
      "Jing Huo",
      "Tianpei Yang",
      "Jinfeng Xu",
      "Wei Dong",
      "Yang Gao"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in healthcare, ensuring their safety, particularly within collaborative multi-agent configurations, is paramount. In this paper we introduce MedSentry, a benchmark comprising 5 000 adversarial medical prompts spanning 25 threat categories with 100 subthemes. Coupled with this dataset, we develop an end-to-end attack-defense evaluation pipeline to systematically analyze how four representative multi-agent topologies (Layers, SharedPool, Centralized, and Decentralized) withstand attacks from 'dark-personality' agents. Our findings reveal critical differences in how these architectures handle information contamination and maintain robust decision-making, exposing their underlying vulnerability mechanisms. For instance, SharedPool's open information sharing makes it highly susceptible, whereas Decentralized architectures exhibit greater resilience thanks to inherent redundancy and isolation. To mitigate these risks, we propose a personality-scale detection and correction mechanism that identifies and rehabilitates malicious agents, restoring system safety to near-baseline levels. MedSentry thus furnishes both a rigorous evaluation framework and practical defense strategies that guide the design of safer LLM-based multi-agent systems in medical domains.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20824v1",
    "published_date": "2025-05-27 07:34:40 UTC",
    "updated_date": "2025-05-27 07:34:40 UTC"
  },
  {
    "arxiv_id": "2505.20820v1",
    "title": "MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization",
    "authors": [
      "Hyomin Kim",
      "Yunhui Jang",
      "Sungsoo Ahn"
    ],
    "abstract": "Large language models (LLMs) have large potential for molecular optimization, as they can gather external chemistry tools and enable collaborative interactions to iteratively refine molecular candidates. However, this potential remains underexplored, particularly in the context of structured reasoning, interpretability, and comprehensive tool-grounded molecular optimization. To address this gap, we introduce MT-Mol, a multi-agent framework for molecular optimization that leverages tool-guided reasoning and role-specialized LLM agents. Our system incorporates comprehensive RDKit tools, categorized into five distinct domains: structural descriptors, electronic and topological features, fragment-based functional groups, molecular representations, and miscellaneous chemical properties. Each category is managed by an expert analyst agent, responsible for extracting task-relevant tools and enabling interpretable, chemically grounded feedback. MT-Mol produces molecules with tool-aligned and stepwise reasoning through the interaction between the analyst agents, a molecule-generating scientist, a reasoning-output verifier, and a reviewer agent. As a result, we show that our framework shows the state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20820v1",
    "published_date": "2025-05-27 07:27:30 UTC",
    "updated_date": "2025-05-27 07:27:30 UTC"
  },
  {
    "arxiv_id": "2505.21573v2",
    "title": "Spectral-inspired Operator Learning with Limited Data and Unknown Physics",
    "authors": [
      "Han Wan",
      "Rui Zhang",
      "Hao Sun"
    ],
    "abstract": "Learning PDE dynamics from limited data with unknown physics is challenging. Existing neural PDE solvers either require large datasets or rely on known physics (e.g., PDE residuals or handcrafted stencils), leading to limited applicability. To address these challenges, we propose Spectral-Inspired Neural Operator (SINO), which can model complex systems from just 2-5 trajectories, without requiring explicit PDE terms. Specifically, SINO automatically captures both local and global spatial derivatives from frequency indices, enabling a compact representation of the underlying differential operators in physics-agnostic regimes. To model nonlinear effects, it employs a Pi-block that performs multiplicative operations on spectral features, complemented by a low-pass filter to suppress aliasing. Extensive experiments on both 2D and 3D PDE benchmarks demonstrate that SINO achieves state-of-the-art performance, with improvements of 1-2 orders of magnitude in accuracy. Particularly, with only 5 training trajectories, SINO outperforms data-driven methods trained on 1000 trajectories and remains predictive on challenging out-of-distribution cases where other methods fail.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21573v2",
    "published_date": "2025-05-27 07:25:13 UTC",
    "updated_date": "2025-09-26 14:09:49 UTC"
  },
  {
    "arxiv_id": "2505.20813v3",
    "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph",
    "authors": [
      "Junsik Kim",
      "Jinwook Park",
      "Kangil Kim"
    ],
    "abstract": "In knowledge graph embedding, leveraging relation specific entity transformation has markedly enhanced performance. However, the consistency of embedding differences before and after transformation remains unaddressed, risking the loss of valuable inductive bias inherent in the embeddings. This inconsistency stems from two problems. First, transformation representations are specified for relations in a disconnected manner, allowing dissimilar transformations and corresponding entity embeddings for similar relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on Relations) disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. In this paper, we introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF). Its entity transformation has three features for enhancing semantic consistency: 1) shared affine transformation of relation embeddings across all relations, 2) rooted entity transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. To amplify the advantages of consistency that preserve semantics on embeddings, RSCF adds relation transformation and prediction modules for enhancing the semantics. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF significantly outperforms state-of-the-art KGE methods, showing robustness across all relations and their frequencies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025, 17 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20813v3",
    "published_date": "2025-05-27 07:22:00 UTC",
    "updated_date": "2025-06-13 02:33:26 UTC"
  },
  {
    "arxiv_id": "2505.21572v1",
    "title": "Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks",
    "authors": [
      "Sungwon Kim",
      "Namkyeong Lee",
      "Yunyoung Doh",
      "Seungmin Shin",
      "Guimok Cho",
      "Seung-Won Jeon",
      "Sangkook Kim",
      "Chanyoung Park"
    ],
    "abstract": "Mesh-based 3D static analysis methods have recently emerged as efficient alternatives to traditional computational numerical solvers, significantly reducing computational costs and runtime for various physics-based analyses. However, these methods primarily focus on surface topology and geometry, often overlooking the inherent thickness of real-world 3D objects, which exhibits high correlations and similar behavior between opposing surfaces. This limitation arises from the disconnected nature of these surfaces and the absence of internal edge connections within the mesh. In this work, we propose a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network (T-EMNN), that effectively integrates the thickness of 3D objects while maintaining the computational efficiency of surface meshes. Additionally, we introduce data-driven coordinates that encode spatial information while preserving E(3)-equivariance or invariance properties, ensuring consistent and robust analysis. Evaluations on a real-world industrial dataset demonstrate the superior performance of T-EMNN in accurately predicting node-level 3D deformations, effectively capturing thickness effects while maintaining computational efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21572v1",
    "published_date": "2025-05-27 07:18:08 UTC",
    "updated_date": "2025-05-27 07:18:08 UTC"
  },
  {
    "arxiv_id": "2505.21571v1",
    "title": "FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition",
    "authors": [
      "Yao Lu",
      "Tengfei Ma",
      "Zeyu Wang",
      "Zhuangzhi Chen",
      "Dongwei Xu",
      "Yun Lin",
      "Qi Xuan",
      "Guan Gui"
    ],
    "abstract": "With the rapid development of wireless communications and the growing complexity of digital modulation schemes, traditional manual modulation recognition methods struggle to extract reliable signal features and meet real-time requirements in modern scenarios. Recently, deep learning based Automatic Modulation Recognition (AMR) approaches have greatly improved classification accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained devices. Model pruning provides a general approach to reduce model complexity, but existing weight, channel, and layer pruning techniques each present a trade-off between compression rate, hardware acceleration, and accuracy preservation. To this end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning framework that combines channel-level pruning with layer-level collapse diagnosis to achieve extreme compression, high performance and efficient inference. In the first stage of FCOS, hierarchical clustering and parameter fusion are applied to channel weights to achieve channel-level pruning. Then a Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer collapse and removes the collapsed layers due to high channel compression ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms existing channel and layer pruning methods. Specifically, FCOS achieves 95.51% FLOPs reduction and 95.31% parameter reduction while still maintaining performance close to the original ResNet56, with only a 0.46% drop in accuracy on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21571v1",
    "published_date": "2025-05-27 07:12:09 UTC",
    "updated_date": "2025-05-27 07:12:09 UTC"
  },
  {
    "arxiv_id": "2505.20794v2",
    "title": "VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion",
    "authors": [
      "Joon-Seung Choi",
      "Dong-Min Byun",
      "Hyung-Seok Oh",
      "Seong-Whan Lee"
    ],
    "abstract": "Controlling singing style is crucial for achieving an expressive and natural singing voice. Among the various style factors, vibrato plays a key role in conveying emotions and enhancing musical depth. However, modeling vibrato remains challenging due to its dynamic nature, making it difficult to control in singing voice conversion. To address this, we propose VibESVC, a controllable singing voice conversion model that explicitly extracts and manipulates vibrato using discrete wavelet transform. Unlike previous methods that model vibrato implicitly, our approach decomposes the F0 contour into frequency components, enabling precise transfer. This allows vibrato control for enhanced flexibility. Experimental results show that VibE-SVC effectively transforms singing styles while preserving speaker similarity. Both subjective and objective evaluations confirm high-quality conversion.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Proceedings of Interspeech",
    "pdf_url": "https://arxiv.org/pdf/2505.20794v2",
    "published_date": "2025-05-27 06:56:13 UTC",
    "updated_date": "2025-10-05 17:31:57 UTC"
  },
  {
    "arxiv_id": "2505.20793v2",
    "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation",
    "authors": [
      "Juan A. Rodriguez",
      "Haotian Zhang",
      "Abhay Puri",
      "Aarash Feizi",
      "Rishav Pramanik",
      "Pascal Wichmann",
      "Arnab Mondal",
      "Mohammad Reza Samsami",
      "Rabiul Awal",
      "Perouz Taslakian",
      "Spandana Gella",
      "Sai Rajeswar",
      "David Vazquez",
      "Christopher Pal",
      "Marco Pedersoli"
    ],
    "abstract": "Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF (Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20793v2",
    "published_date": "2025-05-27 06:56:00 UTC",
    "updated_date": "2025-11-30 00:39:40 UTC"
  },
  {
    "arxiv_id": "2505.21570v1",
    "title": "Beyond Explainability: The Case for AI Validation",
    "authors": [
      "Dalit Ken-Dror Feldman",
      "Daniel Benoliel"
    ],
    "abstract": "Artificial Knowledge (AK) systems are transforming decision-making across critical domains such as healthcare, finance, and criminal justice. However, their growing opacity presents governance challenges that current regulatory approaches, focused predominantly on explainability, fail to address adequately. This article argues for a shift toward validation as a central regulatory pillar. Validation, ensuring the reliability, consistency, and robustness of AI outputs, offers a more practical, scalable, and risk-sensitive alternative to explainability, particularly in high-stakes contexts where interpretability may be technically or economically unfeasible. We introduce a typology based on two axes, validity and explainability, classifying AK systems into four categories and exposing the trade-offs between interpretability and output reliability. Drawing on comparative analysis of regulatory approaches in the EU, US, UK, and China, we show how validation can enhance societal trust, fairness, and safety even where explainability is limited. We propose a forward-looking policy framework centered on pre- and post-deployment validation, third-party auditing, harmonized standards, and liability incentives. This framework balances innovation with accountability and provides a governance roadmap for responsibly integrating opaque, high-performing AK systems into society.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21570v1",
    "published_date": "2025-05-27 06:42:41 UTC",
    "updated_date": "2025-05-27 06:42:41 UTC"
  },
  {
    "arxiv_id": "2505.20783v1",
    "title": "FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation",
    "authors": [
      "Jiaping Xiao",
      "Cheng Wen Tsao",
      "Yuhang Zhang",
      "Mir Feroskhan"
    ],
    "abstract": "Path planning is a critical component in autonomous drone operations, enabling safe and efficient navigation through complex environments. Recent advances in foundation models, particularly large language models (LLMs) and vision-language models (VLMs), have opened new opportunities for enhanced perception and intelligent decision-making in robotics. However, their practical applicability and effectiveness in global path planning remain relatively unexplored. This paper proposes foundation model-guided path planners (FM-Planner) and presents a comprehensive benchmarking study and practical validation for drone path planning. Specifically, we first systematically evaluate eight representative LLM and VLM approaches using standardized simulation scenarios. To enable effective real-time navigation, we then design an integrated LLM-Vision planner that combines semantic reasoning with visual perception. Furthermore, we deploy and validate the proposed path planner through real-world experiments under multiple configurations. Our findings provide valuable insights into the strengths, limitations, and feasibility of deploying foundation models in real-world drone applications and providing practical implementations in autonomous flight. Project site: https://github.com/NTU-ICG/FM-Planner.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been submitted for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2505.20783v1",
    "published_date": "2025-05-27 06:41:21 UTC",
    "updated_date": "2025-05-27 06:41:21 UTC"
  },
  {
    "arxiv_id": "2505.20776v4",
    "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences",
    "authors": [
      "Jungyoub Cha",
      "Hyunjong Kim",
      "Sungzoon Cho"
    ],
    "abstract": "Speculative decoding is a widely used technique for accelerating inference in large language models (LLMs), but its performance degrades as input length grows, with significant drops even at moderate lengths. Yet, this early degradation has remained largely underexplored. We introduce SpecExtend, a drop-in enhancement that improves speculative decoding on long sequences without additional training. SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention to accelerate prefill and verification steps. To improve both draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that leverages the target model's attention scores to dynamically select relevant context for the smaller draft model. Extensive evaluations show that SpecExtend accelerates speculative decoding by up to 2.84x on 16K-token long document summarization and up to 3.86x on long-form reasoning, while preserving the short-input performance of state-of-the-art frameworks. Our code is available at https://github.com/jycha98/SpecExtend .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20776v4",
    "published_date": "2025-05-27 06:30:00 UTC",
    "updated_date": "2026-01-19 09:43:38 UTC"
  },
  {
    "arxiv_id": "2505.21569v2",
    "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools",
    "authors": [
      "Zhucong Li",
      "Bowei Zhang",
      "Jin Xiao",
      "Zhijian Zhou",
      "Fenglei Cao",
      "Jiaqing Liang",
      "Yuan Qi"
    ],
    "abstract": "Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: //anonymous.4open.science/r/ChemHAS-01E4/README.md.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.21569v2",
    "published_date": "2025-05-27 06:22:57 UTC",
    "updated_date": "2025-06-18 03:05:54 UTC"
  },
  {
    "arxiv_id": "2505.20771v1",
    "title": "Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems",
    "authors": [
      "Heng Tang",
      "Feng Liu",
      "Xinbo Chen",
      "Jiawei Chen",
      "Bohao Wang",
      "Changwang Zhang",
      "Jun Wang",
      "Yuegang Sun",
      "Bingde Hu",
      "Can Wang"
    ],
    "abstract": "Recent years have witnessed extensive exploration of Large Language Models (LLMs) on the field of Recommender Systems (RS). There are currently two commonly used strategies to enable LLMs to have recommendation capabilities: 1) The \"Guidance-Only\" strategy uses in-context learning to exploit and amplify the inherent semantic understanding and item recommendation capabilities of LLMs; 2) The \"Tuning-Only\" strategy uses supervised fine-tuning (SFT) to fine-tune LLMs with the aim of fitting them to real recommendation data. However, neither of these strategies can effectively bridge the gap between the knowledge space of LLMs and recommendation, and their performance do not meet our expectations.\n  To better enable LLMs to learn recommendation knowledge, we combine the advantages of the above two strategies and proposed a novel \"Guidance+Tuning\" method called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of curriculum learning. It first employs self-distillation to construct an auxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it further utilizes a self-adaptive curriculum scheduler to enable LLMs to gradually learn from simpler data (self-distilled data) to more challenging data (real RS data). Extensive experiments demonstrate that SOFT significantly enhances the recommendation accuracy (37.59\\% on average) of LLM-based methods. The code is available via https://anonymous.4open.science/r/Self-Optimized-Fine-Tuning-264E",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20771v1",
    "published_date": "2025-05-27 06:22:50 UTC",
    "updated_date": "2025-05-27 06:22:50 UTC"
  },
  {
    "arxiv_id": "2505.20767v4",
    "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models",
    "authors": [
      "Xiaqiang Tang",
      "Jian Li",
      "Keyu Hu",
      "Du Nan",
      "Xiaolong Li",
      "Xi Zhang",
      "Weigao Sun",
      "Sihong Xie"
    ],
    "abstract": "Faithfulness hallucinations are claims generated by a Large Language Model (LLM) not supported by contexts provided to the LLM. Lacking assessment standards, existing benchmarks focus on \"factual statements\" that rephrase source materials while overlooking \"cognitive statements\" that involve making inferences from the given context. Consequently, evaluating and detecting the hallucination of cognitive statements remains challenging. Inspired by how evidence is assessed in the legal domain, we design a rigorous framework to assess different levels of faithfulness of cognitive statements and introduce the CogniBench dataset where we reveal insightful statistics. To keep pace with rapidly evolving LLMs, we further develop an automatic annotation pipeline that scales easily across different models. This results in a large-scale CogniBench-L dataset, which facilitates training accurate detectors for both factual and cognitive hallucinations. We release our model and datasets at: https://github.com/FUTUREEEEEE/CogniBench",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20767v4",
    "published_date": "2025-05-27 06:16:27 UTC",
    "updated_date": "2025-06-25 14:02:19 UTC"
  },
  {
    "arxiv_id": "2505.20759v3",
    "title": "PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding",
    "authors": [
      "Ansel Blume",
      "Jeonghwan Kim",
      "Hyeonjeong Ha",
      "Elen Chatikyan",
      "Xiaomeng Jin",
      "Khanh Duy Nguyen",
      "Nanyun Peng",
      "Kai-Wei Chang",
      "Derek Hoiem",
      "Heng Ji"
    ],
    "abstract": "Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning-yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 part labels and 534 object labels for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY uses specialized concepts (e.g., agricultural airplane), and challenges models to compare objects' parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that uses span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM outperforms existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025 Spotlight; project page: https://wjdghks950.github.io/partonomy.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2505.20759v3",
    "published_date": "2025-05-27 06:03:56 UTC",
    "updated_date": "2025-10-26 21:52:55 UTC"
  },
  {
    "arxiv_id": "2505.21568v2",
    "title": "VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents",
    "authors": [
      "Haiyun Li",
      "Zhiyong Wu",
      "Xiaofeng Xie",
      "Jingran Xie",
      "Yaoxun Xu",
      "Hanyang Peng"
    ],
    "abstract": "Voice cloning (VC)-resistant watermarking is an emerging technique for tracing and preventing unauthorized cloning. Existing methods effectively trace traditional VC models by training them on watermarked audio but fail in zero-shot VC scenarios, where models synthesize audio from an audio prompt without training. To address this, we propose VoiceMark, the first zero-shot VC-resistant watermarking method that leverages speaker-specific latents as the watermark carrier, allowing the watermark to transfer through the zero-shot VC process into the synthesized audio. Additionally, we introduce VC-simulated augmentations and VAD-based loss to enhance robustness against distortions. Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves over 95% accuracy in watermark detection after zero-shot VC synthesis, significantly outperforming existing methods, which only reach around 50%. See our code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.21568v2",
    "published_date": "2025-05-27 05:59:34 UTC",
    "updated_date": "2025-05-30 12:11:04 UTC"
  },
  {
    "arxiv_id": "2505.20753v1",
    "title": "Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models",
    "authors": [
      "Yufei Zhan",
      "Hongyin Zhao",
      "Yousong Zhu",
      "Shurong Zheng",
      "Fan Yang",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "abstract": "Large Multimodal Models (LMMs) have recently demonstrated remarkable visual understanding performance on both vision-language and vision-centric tasks. However, they often fall short in integrating advanced, task-specific capabilities for compositional reasoning, which hinders their progress toward truly competent general vision models. To address this, we present a unified visual reasoning mechanism that enables LMMs to solve complicated compositional problems by leveraging their intrinsic capabilities (e.g. grounding and visual understanding capabilities). Different from the previous shortcut learning mechanism, our approach introduces a human-like understanding-thinking-answering process, allowing the model to complete all steps in a single pass forwarding without the need for multiple inferences or external tools. This design bridges the gap between foundational visual capabilities and general question answering, encouraging LMMs to generate faithful and traceable responses for complex visual reasoning. Meanwhile, we curate 334K visual instruction samples covering both general scenes and text-rich scenes and involving multiple foundational visual capabilities. Our trained model, Griffon-R, has the ability of end-to-end automatic understanding, self-thinking, and reasoning answers. Comprehensive experiments show that Griffon-R not only achieves advancing performance on complex visual reasoning benchmarks including VSR and CLEVR, but also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA. Data, models, and codes will be release at https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Tech report",
    "pdf_url": "https://arxiv.org/pdf/2505.20753v1",
    "published_date": "2025-05-27 05:50:25 UTC",
    "updated_date": "2025-05-27 05:50:25 UTC"
  },
  {
    "arxiv_id": "2505.20751v2",
    "title": "Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform",
    "authors": [
      "Zongcai Tan",
      "Dandan Zhang"
    ],
    "abstract": "Optical tweezers (OT) offer unparalleled capabilities for micromanipulation with submicron precision in biomedical applications. However, controlling conventional multi-trap OT to achieve cooperative manipulation of multiple complex-shaped microrobots in dynamic environments poses a significant challenge. To address this, we introduce Interactive OT Gym, a reinforcement learning (RL)-based simulation platform designed for OT-driven microrobotics. Our platform supports complex physical field simulations and integrates haptic feedback interfaces, RL modules, and context-aware shared control strategies tailored for OT-driven microrobot in cooperative biological object manipulation tasks. This integration allows for an adaptive blend of manual and autonomous control, enabling seamless transitions between human input and autonomous operation. We evaluated the effectiveness of our platform using a cell manipulation task. Experimental results show that our shared control system significantly improves micromanipulation performance, reducing task completion time by approximately 67% compared to using pure human or RL control alone and achieving a 100% success rate. With its high fidelity, interactivity, low cost, and high-speed simulation capabilities, Interactive OT Gym serves as a user-friendly training and testing environment for the development of advanced interactive OT-driven micromanipulation systems and control algorithms. For more details on the project, please see our website https://sites.google.com/view/otgym",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20751v2",
    "published_date": "2025-05-27 05:50:11 UTC",
    "updated_date": "2025-05-30 11:45:21 UTC"
  },
  {
    "arxiv_id": "2505.20749v4",
    "title": "Can Agents Fix Agent Issues?",
    "authors": [
      "Alfin Wijaya Rahardja",
      "Junwei Liu",
      "Weitong Chen",
      "Zhenpeng Chen",
      "Yiling Lou"
    ],
    "abstract": "LLM-based agent systems are emerging as a new software paradigm and have been widely adopted across diverse domains such as medicine, robotics, and programming. However, maintaining these systems requires substantial effort, as they are inevitably prone to bugs and continually evolve to meet changing external requirements. Therefore, automatically resolving agent issues (i.e., bug reports or feature requests) is a crucial and challenging task. While recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in addressing issues in traditional software systems, it remains unclear how effectively they can resolve real-world issues in agent systems, which differ significantly from traditional software. To fill this gap, we first manually analyze 201 real-world agent issues and identify common categories of agent issues. We then spend 500 person-hours constructing AgentIssue-Bench, a reproducible benchmark comprising 50 agent issue resolution tasks (each with an executable environment and failure-triggering tests). We further evaluate state-of-the-art SE agents on AgentIssue-Bench and reveal their limited effectiveness (i.e., with only 0.67% - 4.67% resolution rates). These results underscore the unique challenges of maintaining agent systems compared to traditional software, highlighting the need for further research to develop advanced SE agents for resolving agent issues. Data and code are available at https://github.com/alfin06/AgentIssue-Bench.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the 39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.20749v4",
    "published_date": "2025-05-27 05:45:03 UTC",
    "updated_date": "2025-10-24 07:24:39 UTC"
  },
  {
    "arxiv_id": "2505.20740v2",
    "title": "MSEarth: A Multimodal Scientific Dataset and Benchmark for Phenomena Uncovering in Earth Science",
    "authors": [
      "Xiangyu Zhao",
      "Wanghan Xu",
      "Bo Liu",
      "Yuhao Zhou",
      "Fenghua Ling",
      "Ben Fei",
      "Xiaoyu Yue",
      "Lei Bai",
      "Wenlong Zhang",
      "Xiao-Ming Wu"
    ],
    "abstract": "The rapid advancement of multimodal large language models (MLLMs) has unlocked new opportunities to tackle complex scientific challenges. Despite this progress, their application in addressing earth science problems, especially at the graduate level, remains underexplored. A significant barrier is the absence of benchmarks that capture the depth and contextual complexity of geoscientific reasoning. Current benchmarks often rely on synthetic datasets or simplistic figure-caption pairs, which do not adequately reflect the intricate reasoning and domain-specific insights required for real-world scientific applications. To address these gaps, we introduce MSEarth, a multimodal scientific benchmark curated from high-quality, open-access scientific publications. MSEarth encompasses the five major spheres of Earth science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere, featuring over 289K figures with refined captions. These captions are crafted from the original figure captions and enriched with discussions and reasoning from the papers, ensuring the benchmark captures the nuanced reasoning and knowledge-intensive content essential for advanced scientific tasks. MSEarth supports a variety of tasks, including scientific figure captioning, multiple choice questions, and open-ended reasoning challenges. By bridging the gap in graduate-level benchmarks, MSEarth provides a scalable and high-fidelity resource to enhance the development and evaluation of MLLMs in scientific reasoning. The benchmark is publicly available to foster further research and innovation in this field.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20740v2",
    "published_date": "2025-05-27 05:30:35 UTC",
    "updated_date": "2025-10-15 07:04:40 UTC"
  },
  {
    "arxiv_id": "2505.20737v1",
    "title": "RRO: LLM Agent Optimization Through Rising Reward Trajectories",
    "authors": [
      "Zilong Wang",
      "Jingfeng Yang",
      "Sreyashi Nag",
      "Samarth Varshney",
      "Xianfeng Tang",
      "Haoming Jiang",
      "Jingbo Shang",
      "Sheikh Muhammad Sarwar"
    ],
    "abstract": "Large language models (LLMs) have exhibited extraordinary performance in a variety of tasks while it remains challenging for them to solve complex multi-step tasks as agents. In practice, agents sensitive to the outcome of certain key steps which makes them likely to fail the task because of a subtle mistake in the planning trajectory. Recent approaches resort to calibrating the reasoning process through reinforcement learning. They reward or penalize every reasoning step with process supervision, as known as Process Reward Models (PRMs). However, PRMs are difficult and costly to scale up with a large number of next action candidates since they require extensive computations to acquire the training data through the per-step trajectory exploration. To mitigate this issue, we focus on the relative reward trend across successive reasoning steps and propose maintaining an increasing reward in the collected trajectories for process supervision, which we term Reward Rising Optimization (RRO). Specifically, we incrementally augment the process supervision until identifying a step exhibiting positive reward differentials, i.e. rising rewards, relative to its preceding iteration. This method dynamically expands the search space for the next action candidates, efficiently capturing high-quality data. We provide mathematical groundings and empirical results on the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO achieves superior performance while requiring much less exploration cost.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "preprint",
    "pdf_url": "https://arxiv.org/pdf/2505.20737v1",
    "published_date": "2025-05-27 05:27:54 UTC",
    "updated_date": "2025-05-27 05:27:54 UTC"
  },
  {
    "arxiv_id": "2505.20734v8",
    "title": "Adversarial bandit optimization for approximately linear functions",
    "authors": [
      "Zhuoyu Cheng",
      "Kohei Hatano",
      "Eiji Takimoto"
    ],
    "abstract": "We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20734v8",
    "published_date": "2025-05-27 05:22:01 UTC",
    "updated_date": "2026-01-06 04:59:35 UTC"
  },
  {
    "arxiv_id": "2505.20733v2",
    "title": "E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing",
    "authors": [
      "Cheonsu Jeong",
      "Seongmin Sim",
      "Hyoyoung Cho",
      "Sungsu Kim",
      "Byounggwan Shin"
    ],
    "abstract": "This paper presents an intelligent work automation approach in the context of contemporary digital transformation by integrating generative AI and Intelligent Document Processing (IDP) technologies with an Automation Agent to realize End-to-End (E2E) automation of corporate financial expense processing tasks. While traditional Robotic Process Automation (RPA) has proven effective for repetitive, rule-based simple task automation, it faces limitations in handling unstructured data, exception management, and complex decision-making. This study designs and implements a four-stage integrated process comprising automatic recognition of supporting documents such as receipts via OCR/IDP, item classification based on a policy-driven database, intelligent exception handling supported by generative AI (large language models, LLMs), and human-in-the-loop final decision-making with continuous system learning through an Automation Agent. Applied to a major Korean enterprise (Company S), the system demonstrated quantitative benefits including over 80% reduction in processing time for paper receipt expense tasks, decreased error rates, and improved compliance, as well as qualitative benefits such as enhanced accuracy and consistency, increased employee satisfaction, and data-driven decision support. Furthermore, the system embodies a virtuous cycle by learning from human judgments to progressively improve automatic exception handling capabilities. Empirically, this research confirms that the organic integration of generative AI, IDP, and Automation Agents effectively overcomes the limitations of conventional automation and enables E2E automation of complex corporate processes. The study also discusses potential extensions to other domains such as accounting, human resources, and procurement, and proposes future directions for AI-driven hyper-automation development.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20733v2",
    "published_date": "2025-05-27 05:21:08 UTC",
    "updated_date": "2025-06-10 12:03:14 UTC"
  },
  {
    "arxiv_id": "2505.20730v3",
    "title": "Do LLMs Understand Collaborative Signals? Diagnosis and Repair",
    "authors": [
      "Shahrooz Pouryousef",
      "Ali Montazeralghaem"
    ],
    "abstract": "Collaborative information from user-item interactions is a fundamental source of signal in successful recommender systems. Recently, researchers have attempted to incorporate this knowledge into large language model-based recommender approaches (LLMRec) to enhance their performance. However, there has been little fundamental analysis of whether LLMs can effectively reason over collaborative information. In this paper, we analyze the ability of LLMs to reason about collaborative information in recommendation tasks, comparing their performance to traditional matrix factorization (MF) models. We propose a simple and effective method to improve LLMs' reasoning capabilities using retrieval-augmented generation (RAG) over the user-item interaction matrix with four different prompting strategies. Our results show that the LLM outperforms the MF model whenever we provide relevant information in a clear and easy-to-follow format, and prompt the LLM to reason based on it. We observe that with this strategy, in almost all cases, the more information we provide, the better the LLM performs.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20730v3",
    "published_date": "2025-05-27 05:18:57 UTC",
    "updated_date": "2025-12-30 04:15:44 UTC"
  },
  {
    "arxiv_id": "2505.20728v4",
    "title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models",
    "authors": [
      "Zesen Lyu",
      "Dandan Zhang",
      "Wei Ye",
      "Fangdi Li",
      "Zhihang Jiang",
      "Yao Yang"
    ],
    "abstract": "Spatial reasoning is a core component of human cognition, enabling individuals to perceive, comprehend, and interact with the physical world. It relies on a nuanced understanding of spatial structures and inter-object relationships, serving as the foundation for complex reasoning and decision-making. To investigate whether current vision-language models (VLMs) exhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark consisting of 1,100 carefully curated real-world images with high spatial complexity. Based on this dataset, we design five tasks to rigorously evaluate VLMs' spatial perception, structural understanding, and reasoning capabilities, while deliberately minimizing reliance on domain-specific knowledge to better isolate and assess the general spatial reasoning capability. We conduct a comprehensive evaluation across 24 state-of-the-art VLMs. The results show that even the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy and performs particularly poorly on the Order Generation task, with only 30.00% accuracy, far below the performance exceeding 90% achieved by human participants. This persistent gap underscores the need for continued progress, positioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for advancing spatial reasoning research in VLMs. Our project page is at https://zesen01.github.io/jigsaw-puzzles.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by EMNLP 2025 Main conference",
    "pdf_url": "https://arxiv.org/pdf/2505.20728v4",
    "published_date": "2025-05-27 05:17:41 UTC",
    "updated_date": "2025-08-26 03:25:38 UTC"
  },
  {
    "arxiv_id": "2505.23806v1",
    "title": "MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation",
    "authors": [
      "Sihyeon Lee",
      "Hyunjoo Song",
      "Jong-chan Lee",
      "Yoon Jin Lee",
      "Boram Lee",
      "Hee-Eon Lim",
      "Dongyeong Kim",
      "Jinwook Seo",
      "Bohyoung Kim"
    ],
    "abstract": "Deploying large language models (LLMs) in clinical settings faces critical trade-offs: cloud LLMs, with their extensive parameters and superior performance, pose risks to sensitive clinical data privacy, while local LLMs preserve privacy but often fail at complex clinical interpretation tasks. We propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex clinical tasks into manageable subtasks and prompt generation, while a local LLM executes these subtasks in a privacy-preserving manner. Without accessing clinical data, the cloud LLM generates and validates subtask prompts using clinical guidelines and synthetic test cases. The local LLM executes subtasks locally and synthesizes outputs generated by the cloud LLM. We evaluate MedOrchestra on pancreatic cancer staging using 100 radiology reports under NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy, outperforming local model baselines (without guideline: 48.94%, with guideline: 56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons: 65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches 85.42% accuracy, showing clear superiority across all settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23806v1",
    "published_date": "2025-05-27 05:09:17 UTC",
    "updated_date": "2025-05-27 05:09:17 UTC"
  },
  {
    "arxiv_id": "2505.21565v1",
    "title": "Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach",
    "authors": [
      "Haicheng Liao",
      "Zhenning Li",
      "Guohui Zhang",
      "Keqiang Li",
      "Chengzhong Xu"
    ],
    "abstract": "Predicting the trajectories of vehicles is crucial for the development of autonomous driving (AD) systems, particularly in complex and dynamic traffic environments. In this study, we introduce HiT (Human-like Trajectory Prediction), a novel model designed to enhance trajectory prediction by incorporating behavior-aware modules and dynamic centrality measures. Unlike traditional methods that primarily rely on static graph structures, HiT leverages a dynamic framework that accounts for both direct and indirect interactions among traffic participants. This allows the model to capture the subtle yet significant influences of surrounding vehicles, enabling more accurate and human-like predictions. To evaluate HiT's performance, we conducted extensive experiments using diverse and challenging real-world datasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results demonstrate that HiT consistently outperforms other top models across multiple metrics, particularly excelling in scenarios involving aggressive driving behaviors. This research presents a significant step forward in trajectory prediction, offering a more reliable and interpretable approach for enhancing the safety and efficiency of fully autonomous driving systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21565v1",
    "published_date": "2025-05-27 05:04:01 UTC",
    "updated_date": "2025-05-27 05:04:01 UTC"
  },
  {
    "arxiv_id": "2506.11060v1",
    "title": "Code Researcher: Deep Research Agent for Large Systems Code and Commit History",
    "authors": [
      "Ramneet Singh",
      "Sathvik Joel",
      "Abhav Mehrotra",
      "Nalin Wadhwa",
      "Ramakrishna B Bairi",
      "Aditya Kanade",
      "Nagarajan Natarajan"
    ],
    "abstract": "Large Language Model (LLM)-based coding agents have shown promising results on coding benchmarks, but their effectiveness on systems code remains underexplored. Due to the size and complexities of systems code, making changes to a systems codebase is a daunting task, even for humans. It requires researching about many pieces of context, derived from the large codebase and its massive commit history, before making changes. Inspired by the recent progress on deep research agents, we design the first deep research agent for code, called Code Researcher, and apply it to the problem of generating patches for mitigating crashes reported in systems code. Code Researcher performs multi-step reasoning about semantics, patterns, and commit history of code to gather sufficient context. The context is stored in a structured memory which is used for synthesizing a patch. We evaluate Code Researcher on kBenchSyz, a benchmark of Linux kernel crashes, and show that it significantly outperforms strong baselines, achieving a crash-resolution rate of 58%, compared to 37.5% by SWE-agent. On an average, Code Researcher explores 10 files in each trajectory whereas SWE-agent explores only 1.33 files, highlighting Code Researcher's ability to deeply explore the codebase. Through another experiment on an open-source multimedia software, we show the generalizability of Code Researcher. Our experiments highlight the importance of global context gathering and multi-faceted reasoning for large codebases.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11060v1",
    "published_date": "2025-05-27 04:57:00 UTC",
    "updated_date": "2025-05-27 04:57:00 UTC"
  },
  {
    "arxiv_id": "2505.20718v2",
    "title": "VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Vision-Language Models",
    "authors": [
      "Kui Wu",
      "Shuhang Xu",
      "Hao Chen",
      "Churan Wang",
      "Zhoujun Li",
      "Yizhou Wang",
      "Fangwei Zhong"
    ],
    "abstract": "We introduce a novel self-improving framework that enhances Embodied Visual Tracking (EVT) with Vision-Language Models (VLMs) to address the limitations of current active visual tracking systems in recovering from tracking failure. Our approach combines the off-the-shelf active tracking methods with VLMs' reasoning capabilities, deploying a fast visual policy for normal tracking and activating VLM reasoning only upon failure detection. The framework features a memory-augmented self-reflection mechanism that enables the VLM to progressively improve by learning from past experiences, effectively addressing VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate significant performance improvements, with our framework boosting success rates by $72\\%$ with state-of-the-art RL-based approaches and $220\\%$ with PID-based methods in challenging environments. This work represents the first integration of VLM-based reasoning to assist EVT agents in proactive failure recovery, offering substantial advances for real-world robotic applications that require continuous target monitoring in dynamic, unstructured environments. Project website: https://sites.google.com/view/evt-recovery-assistant.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20718v2",
    "published_date": "2025-05-27 04:53:50 UTC",
    "updated_date": "2025-05-28 15:54:19 UTC"
  },
  {
    "arxiv_id": "2505.20714v2",
    "title": "Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting",
    "authors": [
      "Zechen Li",
      "Lanqing Yang",
      "Yiheng Bian",
      "Hao Pan",
      "Yongjian Fu",
      "Yezhou Wang",
      "Zhuxi Chen",
      "Yi-Chao Chen",
      "Guangtao Xue"
    ],
    "abstract": "Indoor environments typically contain diverse RF signals distributed across multiple frequency bands, including NB-IoT, Wi-Fi, and millimeter-wave. Consequently, wideband RF modeling is essential for practical applications such as joint deployment of heterogeneous RF systems, cross-band communication, and distributed RF sensing. Although 3D Gaussian Splatting (3DGS) techniques effectively reconstruct RF radiance fields at a single frequency, they cannot model fields at arbitrary or unknown frequencies across a wide range. In this paper, we present a novel 3DGS algorithm for unified wideband RF radiance field modeling. RF wave propagation depends on signal frequency and the 3D spatial environment, including geometry and material electromagnetic (EM) properties. To address these factors, we introduce a frequency-embedded EM feature network that utilizes 3D Gaussian spheres at each spatial location to learn the relationship between frequency and transmission characteristics, such as attenuation and radiance intensity. With a dataset containing sparse frequency samples in a specific 3D environment, our model can efficiently reconstruct RF radiance fields at arbitrary and unseen frequencies. To assess our approach, we introduce a large-scale power angular spectrum (PAS) dataset with 50,000 samples spanning 1 to 94 GHz across six indoor environments. Experimental results show that the proposed model trained on multiple frequencies achieves a Structural Similarity Index Measure (SSIM) of 0.922 for PAS reconstruction, surpassing state-of-the-art single-frequency 3DGS models with SSIM of 0.863.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20714v2",
    "published_date": "2025-05-27 04:48:26 UTC",
    "updated_date": "2025-11-21 13:07:32 UTC"
  },
  {
    "arxiv_id": "2505.20707v2",
    "title": "Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective",
    "authors": [
      "Nicy Scaria",
      "Silvester John Joseph Kennedy",
      "Krishna Agarwal",
      "Diksha Seth",
      "Deepak Subramani"
    ],
    "abstract": "Small Language Models (SLMs) offer privacy and efficiency for educational deployment, yet their utility depends on reliable multistep reasoning. Existing benchmarks often prioritize final answer accuracy, obscuring 'right answer, wrong procedure' failures that can reinforce student misconceptions. This work investigates SLM physics reasoning reliability, stage wise failure modes, and robustness under paired contextual variants. We introduce Physbench, comprising of 3,162 high school and AP level physics questions derived from OpenStax in a structured reference solution format with Bloom's Taxonomy annotations, plus 2,700 paired culturally contextualized variants. Using P-REFS, a stage wise evaluation rubric, we assess 10 SLMs across 58,000 responses. Results reveal substantial reliability gap: among final answer correct solutions, 75 to 98% contain at least one reasoning error. Failure modes shift with model capability; weaker models fail primarily at interpretation or modeling while stronger models often fail during execution. Paired contextual variations have minimal impact on top models but degrade the performance of mid-tier models. These findings demonstrate that safe educational AI requires evaluation paradigms that prioritize reasoning fidelity over final-answer correctness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "physics.ed-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20707v2",
    "published_date": "2025-05-27 04:33:13 UTC",
    "updated_date": "2026-01-07 02:33:14 UTC"
  },
  {
    "arxiv_id": "2505.20697v3",
    "title": "Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series",
    "authors": [
      "Zachary C. Brown",
      "David Carlson"
    ],
    "abstract": "The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20697v3",
    "published_date": "2025-05-27 04:06:47 UTC",
    "updated_date": "2025-07-02 20:42:02 UTC"
  },
  {
    "arxiv_id": "2506.14786v1",
    "title": "PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series",
    "authors": [
      "Haobo Li",
      "Eunseo Jung",
      "Zixin Chen",
      "Zhaowei Wang",
      "Yueya Wang",
      "Huamin Qu",
      "Alexis Kai Hon Lau"
    ],
    "abstract": "Multimodal time series forecasting is foundational in various fields, such as utilizing satellite imagery and numerical data for predicting typhoons in climate science. However, existing multimodal approaches primarily focus on utilizing text data to help time series forecasting, leaving the visual data in existing time series datasets untouched. Furthermore, it is challenging for models to effectively capture the physical information embedded in visual data, such as satellite imagery's temporal and geospatial context, which extends beyond images themselves. To address this gap, we propose physics-informed positional encoding (PIPE), a lightweight method that embeds physical information into vision language models (VLMs). PIPE introduces two key innovations: (1) a physics-informed positional indexing scheme for mapping physics to positional IDs, and (2) a variant-frequency positional encoding mechanism for encoding frequency information of physical variables and sequential order of tokens within the embedding space. By preserving both the physical information and sequential order information, PIPE significantly improves multimodal alignment and forecasting accuracy. Through the experiments on the most representative and the largest open-sourced satellite image dataset, PIPE achieves state-of-the-art performance in both deep learning forecasting and climate domain methods, demonstrating superiority across benchmarks, including a 12% improvement in typhoon intensity forecasting over prior works. Our code is provided in the supplementary material.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.14786v1",
    "published_date": "2025-05-27 04:03:41 UTC",
    "updated_date": "2025-05-27 04:03:41 UTC"
  },
  {
    "arxiv_id": "2505.20692v1",
    "title": "Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions",
    "authors": [
      "Saharsh Barve",
      "Andy Mao",
      "Jiayue Melissa Shi",
      "Prerna Juneja",
      "Koustuv Saha"
    ],
    "abstract": "Recent advances in generative AI have enabled visual content creation through text-to-image (T2I) generation. However, despite their creative potential, T2I models often replicate and amplify societal stereotypes -- particularly those related to gender, race, and culture -- raising important ethical concerns. This paper proposes a theory-driven bias detection rubric and a Social Stereotype Index (SSI) to systematically evaluate social biases in T2I outputs. We audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and Stability AI Core -- using 100 queries across three categories -- geocultural, occupational, and adjectival. Our analysis reveals that initial outputs are prone to include stereotypical visual cues, including gendered professions, cultural markers, and western beauty norms. To address this, we adopted our rubric to conduct targeted prompt refinement using LLMs, which significantly reduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and 51% for adjectival queries. We complemented our quantitative analysis through a user study examining perceptions, awareness, and preferences around AI-generated biased imagery. Our findings reveal a key tension -- although prompt refinement can mitigate stereotypes, it can limit contextual alignment. Interestingly, users often perceived stereotypical images to be more aligned with their expectations. We discuss the need to balance ethical debiasing with contextual relevance and call for T2I systems that support global diversity and inclusivity while not compromising the reflection of real-world social complexity.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20692v1",
    "published_date": "2025-05-27 04:01:03 UTC",
    "updated_date": "2025-05-27 04:01:03 UTC"
  },
  {
    "arxiv_id": "2505.20691v1",
    "title": "Evidential Deep Active Learning for Semi-Supervised Classification",
    "authors": [
      "Shenkai Zhao",
      "Xinao Zhang",
      "Lipeng Pan",
      "Xiaobin Xu",
      "Danilo Pelusi"
    ],
    "abstract": "Semi-supervised classification based on active learning has made significant progress, but the existing methods often ignore the uncertainty estimation (or reliability) of the prediction results during the learning process, which makes it questionable whether the selected samples can effectively update the model. Hence, this paper proposes an evidential deep active learning approach for semi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised learning framework to simultaneously quantify the uncertainty estimation of labeled and unlabeled data during the learning process. The uncertainty estimation of the former is associated with evidential deep learning, while that of the latter is modeled by combining ignorance information and conflict information of the evidence from the perspective of the T-conorm operator. Furthermore, this article constructs a heuristic method to dynamically balance the influence of evidence and the number of classes on uncertainty estimation to ensure that it does not produce counter-intuitive results in EDALSSC. For the sample selection strategy, EDALSSC selects the sample with the greatest uncertainty estimation that is calculated in the form of a sum when the training loss increases in the latter half of the learning process. Experimental results demonstrate that EDALSSC outperforms existing semi-supervised and supervised active learning approaches on image classification datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20691v1",
    "published_date": "2025-05-27 03:59:48 UTC",
    "updated_date": "2025-05-27 03:59:48 UTC"
  },
  {
    "arxiv_id": "2505.20686v1",
    "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression",
    "authors": [
      "Kianté Brantley",
      "Mingyu Chen",
      "Zhaolin Gao",
      "Jason D. Lee",
      "Wen Sun",
      "Wenhao Zhan",
      "Xuezhou Zhang"
    ],
    "abstract": "Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A$*-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V$*, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A$*-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\\times$ and peak memory usage by over 30% compared to PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at https://github.com/ZhaolinGao/A-PO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20686v1",
    "published_date": "2025-05-27 03:58:50 UTC",
    "updated_date": "2025-05-27 03:58:50 UTC"
  },
  {
    "arxiv_id": "2505.20674v2",
    "title": "Pretraining Language Models to Ponder in Continuous Space",
    "authors": [
      "Boyi Zeng",
      "Shixiang Song",
      "Siyuan Huang",
      "Yixuan Wang",
      "He Li",
      "Ziwei He",
      "Xinbing Wang",
      "Zhiyu Li",
      "Zhouhan Lin"
    ],
    "abstract": "Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort. In this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a single token generation step. During pondering, instead of generating an actual token sampled from the prediction distribution, the model ponders by yielding a weighted sum of all token embeddings according to the predicted token distribution. The generated embedding is then fed back as input for another forward pass. We show that the model can learn to ponder in this way through self-supervised learning, without any human annotations. Experiments across three widely used open-source architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task evaluations demonstrate the effectiveness and generality of our method. For language modeling tasks, pondering language models achieve performance comparable to vanilla models with twice the number of parameters. On 9 downstream benchmarks, our pondering-enhanced Pythia models significantly outperform the official Pythia models. Notably, PonderingPythia-2.8B surpasses Pythia-6.9B, and PonderingPythia-1B is comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code is available at https://github.com/LUMIA-Group/PonderingLM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20674v2",
    "published_date": "2025-05-27 03:47:33 UTC",
    "updated_date": "2025-06-23 13:48:37 UTC"
  },
  {
    "arxiv_id": "2505.20672v1",
    "title": "GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning",
    "authors": [
      "Woochang Sim",
      "Hyunseok Ryu",
      "Kyungmin Choi",
      "Sungwon Han",
      "Sundong Kim"
    ],
    "abstract": "The Abstraction and Reasoning Corpus (ARC) poses a stringent test of general AI capabilities, requiring solvers to infer abstract patterns from only a handful of examples. Despite substantial progress in deep learning, state-of-the-art models still achieve accuracy rates of merely 40-55% on 2024 ARC Competition, indicative of a significant gap between their performance and human-level reasoning. In this work, we seek to bridge that gap by introducing an analogy-inspired ARC dataset, GIFARC. Leveraging large language models (LLMs) and vision-language models (VLMs), we synthesize new ARC-style tasks from a variety of GIF images that include analogies. Each new task is paired with ground-truth analogy, providing an explicit mapping between visual transformations and everyday concepts. By embedding robust human-intuitive analogies into ARC-style tasks, GIFARC guides AI agents to evaluate the task analogically before engaging in brute-force pattern search, thus efficiently reducing problem complexity and build a more concise and human-understandable solution. We empirically validate that guiding LLM with analogic approach with GIFARC affects task-solving approaches of LLMs to align with analogic approach of human.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20672v1",
    "published_date": "2025-05-27 03:42:51 UTC",
    "updated_date": "2025-05-27 03:42:51 UTC"
  },
  {
    "arxiv_id": "2505.20671v1",
    "title": "LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation",
    "authors": [
      "Heng Tan",
      "Hua Yan",
      "Yu Yang"
    ],
    "abstract": "While reinforcement learning (RL) has achieved notable success in various domains, training effective policies for complex tasks remains challenging. Agents often converge to local optima and fail to maximize long-term rewards. Existing approaches to mitigate training bottlenecks typically fall into two categories: (i) Automated policy refinement, which identifies critical states from past trajectories to guide policy updates, but suffers from costly and uncertain model training; and (ii) Human-in-the-loop refinement, where human feedback is used to correct agent behavior, but this does not scale well to environments with large or continuous action spaces. In this work, we design a large language model-guided policy modulation framework that leverages LLMs to improve RL training without additional model training or human intervention. We first prompt an LLM to identify critical states from a sub-optimal agent's trajectories. Based on these states, the LLM then provides action suggestions and assigns implicit rewards to guide policy refinement. Experiments across standard RL benchmarks demonstrate that our method outperforms state-of-the-art baselines, highlighting the effectiveness of LLM-based explanations in addressing RL training bottlenecks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20671v1",
    "published_date": "2025-05-27 03:40:02 UTC",
    "updated_date": "2025-05-27 03:40:02 UTC"
  },
  {
    "arxiv_id": "2505.20670v2",
    "title": "MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning",
    "authors": [
      "Zikang Guo",
      "Benfeng Xu",
      "Xiaorui Wang",
      "Zhendong Mao"
    ],
    "abstract": "Complex tasks involving tool integration pose significant challenges for Large Language Models (LLMs), leading to the emergence of multi-agent workflows as a promising solution. Reflection has emerged as an effective strategy for correcting erroneous trajectories in agentic workflows. However, existing approaches only exploit such capability in the post-action stage, where the agent observes the execution outcomes. We argue that, like humans, LLMs can also engage in reflection before action execution: the agent can anticipate undesirable outcomes from its own decisions, which not only provides a necessarily complementary perspective to evaluate the decision but also prevents the propagation of errors throughout the trajectory. In this paper, we propose MIRROR, a framework that consists of both intra-reflection, which critically assesses intended actions before execution, and inter-reflection, which further adjusts the trajectory based on observations. This design systematically leverages LLM reflection capabilities to eliminate and rectify erroneous actions on a more comprehensive scope. Evaluations on both the StableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior performance, achieving state-of-the-art results compared to existing approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to 34rd International Joint Conference on Artificial Intelligence (IJCAI 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.20670v2",
    "published_date": "2025-05-27 03:37:33 UTC",
    "updated_date": "2025-06-05 09:49:45 UTC"
  },
  {
    "arxiv_id": "2505.21563v1",
    "title": "Fog Intelligence for Network Anomaly Detection",
    "authors": [
      "Kai Yang",
      "Hui Ma",
      "Shaoyu Dou"
    ],
    "abstract": "Anomalies are common in network system monitoring. When manifested as network threats to be mitigated, service outages to be prevented, and security risks to be ameliorated, detecting such anomalous network behaviors becomes of great importance. However, the growing scale and complexity of the mobile communication networks, as well as the ever-increasing amount and dimensionality of the network surveillance data, make it extremely difficult to monitor a mobile network and discover abnormal network behaviors. Recent advances in machine learning allow for obtaining near-optimal solutions to complicated decision-making problems with many sources of uncertainty that cannot be accurately characterized by traditional mathematical models. However, most machine learning algorithms are centralized, which renders them inapplicable to a large-scale distributed wireless networks with tens of millions of mobile devices. In this article, we present fog intelligence, a distributed machine learning architecture that enables intelligent wireless network management. It preserves the advantage of both edge processing and centralized cloud computing. In addition, the proposed architecture is scalable, privacy-preserving, and well suited for intelligent management of a distributed wireless network.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "published in IEEE Network",
    "pdf_url": "https://arxiv.org/pdf/2505.21563v1",
    "published_date": "2025-05-27 03:35:07 UTC",
    "updated_date": "2025-05-27 03:35:07 UTC"
  },
  {
    "arxiv_id": "2506.10011v1",
    "title": "WDMIR: Wavelet-Driven Multimodal Intent Recognition",
    "authors": [
      "Weiyin Gong",
      "Kai Zhang",
      "Yanghai Zhang",
      "Qi Liu",
      "Xinjie Sun",
      "Junyu Lu",
      "Linbo Zhu"
    ],
    "abstract": "Multimodal intent recognition (MIR) seeks to accurately interpret user intentions by integrating verbal and non-verbal information across video, audio and text modalities. While existing approaches prioritize text analysis, they often overlook the rich semantic content embedded in non-verbal cues. This paper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR) framework that enhances intent understanding through frequency-domain analysis of non-verbal information. To be more specific, we propose: (1) a wavelet-driven fusion module that performs synchronized decomposition and integration of video-audio features in the frequency domain, enabling fine-grained analysis of temporal dynamics; (2) a cross-modal interaction mechanism that facilitates progressive feature enhancement from bimodal to trimodal integration, effectively bridging the semantic gap between verbal and non-verbal information. Extensive experiments on MIntRec demonstrate that our approach achieves state-of-the-art performance, surpassing previous methods by 1.13% on accuracy. Ablation studies further verify that the wavelet-driven fusion module significantly improves the extraction of semantic information from non-verbal sources, with a 0.41% increase in recognition accuracy when analyzing subtle emotional cues.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "cs.MM",
    "comment": "Accepted at IJCAI 2025, 9pages, 6figures",
    "pdf_url": "https://arxiv.org/pdf/2506.10011v1",
    "published_date": "2025-05-27 03:32:45 UTC",
    "updated_date": "2025-05-27 03:32:45 UTC"
  },
  {
    "arxiv_id": "2506.06316v1",
    "title": "A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing",
    "authors": [
      "Haoyang Feng",
      "Yanjun Dai",
      "Yuan Gao"
    ],
    "abstract": "For personalized marketing, a new challenge of how to effectively algorithm the A/B testing to maximize user response is urgently to be overcome. In this paper, we present a new approach, the RL-LLM-AB test framework, for using reinforcement learning strategy optimization combined with LLM to automate and personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained instruction-tuned language model. It first generates A/B versions of candidate content variants using a Prompt-Conditioned Generator, and then dynamically embeds and fuses the user portrait and the context of the current query with the multi-modal perception module to constitute the current interaction state. The content version is then selected in real-time through the policy optimization module with an Actor-Critic structure, and long-term revenue is estimated according to real-time feedback (such as click-through rate and conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded into the framework to capture long-term user preference drift, which helps to generalize policy across multiple users and content contexts. Numerical results demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B testing methods, including classical A/B testing, Contextual Bandits, and benchmark reinforcement learning approaches on real-world marketing data.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06316v1",
    "published_date": "2025-05-27 03:31:07 UTC",
    "updated_date": "2025-05-27 03:31:07 UTC"
  },
  {
    "arxiv_id": "2505.20666v1",
    "title": "Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers",
    "authors": [
      "Yukun Zhang",
      "Xueqing Zhou"
    ],
    "abstract": "We propose a novel framework, Continuous_Time Attention, which infuses partial differential equations (PDEs) into the Transformer's attention mechanism to address the challenges of extremely long input sequences. Instead of relying solely on a static attention matrix, we allow attention weights to evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion dynamics. This mechanism systematically smooths local noise, enhances long_range dependencies, and stabilizes gradient flow. Theoretically, our analysis shows that PDE_based attention leads to better optimization landscapes and polynomial rather than exponential decay of distant interactions. Empirically, we benchmark our method on diverse experiments_demonstrating consistent gains over both standard and specialized long sequence Transformer variants. Our findings highlight the potential of PDE_based formulations to enrich attention mechanisms with continuous_time dynamics and global coherence.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20666v1",
    "published_date": "2025-05-27 03:30:10 UTC",
    "updated_date": "2025-05-27 03:30:10 UTC"
  },
  {
    "arxiv_id": "2505.20664v1",
    "title": "Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning",
    "authors": [
      "Yang He",
      "Xiao Ding",
      "Bibo Cai",
      "Yufei Zhang",
      "Kai Xiong",
      "Zhouhao Sun",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "While reasoning-augmented large language models (RLLMs) significantly enhance complex task performance through extended reasoning chains, they inevitably introduce substantial unnecessary token consumption, particularly for simpler problems where Short Chain-of-Thought (Short CoT) suffices. This overthinking phenomenon leads to inefficient resource usage without proportional accuracy gains. To address this issue, we propose Self-Route, a dynamic reasoning framework that automatically selects between general and reasoning modes based on model capability estimation. Our approach introduces a lightweight pre-inference stage to extract capability-aware embeddings from hidden layer representations, enabling real-time evaluation of the model's ability to solve problems. We further construct Gradient-10K, a model difficulty estimation-based dataset with dense complexity sampling, to train the router for precise capability boundary detection. Extensive experiments demonstrate that Self-Route achieves comparable accuracy to reasoning models while reducing token consumption by 30-55\\% across diverse benchmarks. The proposed framework demonstrates consistent effectiveness across models with different parameter scales and reasoning paradigms, highlighting its general applicability and practical value.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20664v1",
    "published_date": "2025-05-27 03:18:31 UTC",
    "updated_date": "2025-05-27 03:18:31 UTC"
  },
  {
    "arxiv_id": "2505.20663v1",
    "title": "TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research",
    "authors": [
      "Xu Kang",
      "Siqi Jiang",
      "Kangwei Xu",
      "Jiahao Li",
      "Ruibo Wu"
    ],
    "abstract": "Terpenoids are a crucial class of natural products that have been studied for over 150 years, but their interdisciplinary nature (spanning chemistry, pharmacology, and biology) complicates knowledge integration. To address this, the authors developed TeroSeek, a curated knowledge base (KB) built from two decades of terpenoid literature, coupled with an AI-powered question-answering chatbot and web service. Leveraging a retrieval-augmented generation (RAG) framework, TeroSeek provides structured, high-quality information and outperforms general-purpose large language models (LLMs) in terpenoid-related queries. It serves as a domain-specific expert tool for multidisciplinary research and is publicly available at http://teroseek.qmclab.com.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "18 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.20663v1",
    "published_date": "2025-05-27 03:17:30 UTC",
    "updated_date": "2025-05-27 03:17:30 UTC"
  },
  {
    "arxiv_id": "2505.20662v2",
    "title": "AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage",
    "authors": [
      "Xuanle Zhao",
      "Zilin Sang",
      "Yuxuan Li",
      "Qi Shi",
      "Weilun Zhao",
      "Shuo Wang",
      "Duzhen Zhang",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Efficient experiment reproduction is critical to accelerating progress in artificial intelligence. However, the inherent complexity of method design and training procedures presents substantial challenges for automation. Notably, reproducing experiments often requires implicit domain-specific knowledge not explicitly documented in the original papers. To address this, we introduce the paper lineage algorithm, which identifies and extracts implicit knowledge from the relevant references cited by the target paper. Building on this idea, we propose AutoReproduce, a multi-agent framework capable of automatically reproducing experiments described in research papers in an end-to-end manner. AutoReproduce enhances code executability by generating unit tests alongside the reproduction process. To evaluate the reproduction capability, we construct ReproduceBench, a benchmark annotated with verified implementations, and introduce novel evaluation metrics to assess both the reproduction and execution fidelity. Experimental results demonstrate that AutoReproduce outperforms the existing strong agent baselines on all five evaluation metrics by a peak margin of over $70\\%$. In particular, compared to the official implementations, AutoReproduce achieves an average performance gap of $22.1\\%$ on $89.74\\%$ of the executable experiment runs. The code will be available at https://github.com/AI9Stars/AutoReproduce.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, preprint version",
    "pdf_url": "https://arxiv.org/pdf/2505.20662v2",
    "published_date": "2025-05-27 03:15:21 UTC",
    "updated_date": "2025-05-30 03:56:32 UTC"
  },
  {
    "arxiv_id": "2505.20660v1",
    "title": "BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism",
    "authors": [
      "Qinzhuo Wu",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan"
    ],
    "abstract": "Graphical User Interface (GUI) agents have gained substantial attention due to their impressive capabilities to complete tasks through multiple interactions within GUI environments. However, existing agents primarily focus on enhancing the accuracy of individual actions and often lack effective mechanisms for detecting and recovering from errors. To address these shortcomings, we propose the BacktrackAgent, a robust framework that incorporates a backtracking mechanism to improve task completion efficiency. BacktrackAgent includes verifier, judger, and reflector components as modules for error detection and recovery, while also applying judgment rewards to further enhance the agent's performance. Additionally, we develop a training dataset specifically designed for the backtracking mechanism, which considers the outcome pages after action executions. Experimental results show that BacktrackAgent has achieved performance improvements in both task success rate and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be released upon acceptance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20660v1",
    "published_date": "2025-05-27 03:09:06 UTC",
    "updated_date": "2025-05-27 03:09:06 UTC"
  },
  {
    "arxiv_id": "2505.20654v1",
    "title": "Chinese Cyberbullying Detection: Dataset, Method, and Validation",
    "authors": [
      "Yi Zhu",
      "Xin Zou",
      "Xindong Wu"
    ],
    "abstract": "Existing cyberbullying detection benchmarks were organized by the polarity of speech, such as \"offensive\" and \"non-offensive\", which were essentially hate speech detection. However, in the real world, cyberbullying often attracted widespread social attention through incidents. To address this problem, we propose a novel annotation method to construct a cyberbullying dataset that organized by incidents. The constructed CHNCI is the first Chinese cyberbullying incident detection dataset, which consists of 220,676 comments in 91 incidents. Specifically, we first combine three cyberbullying detection methods based on explanations generation as an ensemble method to generate the pseudo labels, and then let human annotators judge these labels. Then we propose the evaluation criteria for validating whether it constitutes a cyberbullying incident. Experimental results demonstrate that the constructed dataset can be a benchmark for the tasks of cyberbullying detection and incident prediction. To the best of our knowledge, this is the first study for the Chinese cyberbullying incident detection task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20654v1",
    "published_date": "2025-05-27 03:03:55 UTC",
    "updated_date": "2025-05-27 03:03:55 UTC"
  },
  {
    "arxiv_id": "2505.20653v1",
    "title": "RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment",
    "authors": [
      "Lingyu Qiu",
      "Ke Jiang",
      "Xiaoyang Tan"
    ],
    "abstract": "Recent advancements in domain generalization for deepfake detection have attracted significant attention, with previous methods often incorporating additional modules to prevent overfitting to domain-specific patterns. However, such regularization can hinder the optimization of the empirical risk minimization (ERM) objective, ultimately degrading model performance. In this paper, we propose a novel learning objective that aligns generalization gradient updates with ERM gradient updates. The key innovation is the application of perturbations to model parameters, aligning the ascending points across domains, which specifically enhances the robustness of deepfake detection models to domain shifts. This approach effectively preserves domain-invariant features while managing domain-specific characteristics, without introducing additional regularization. Experimental results on multiple challenging deepfake detection datasets demonstrate that our gradient alignment strategy outperforms state-of-the-art domain generalization techniques, confirming the efficacy of our method. The code is available at https://github.com/Lynn0925/RoGA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICME2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20653v1",
    "published_date": "2025-05-27 03:02:21 UTC",
    "updated_date": "2025-05-27 03:02:21 UTC"
  },
  {
    "arxiv_id": "2505.20650v3",
    "title": "FinTagging: Benchmarking LLMs for Extracting and Structuring Financial Information",
    "authors": [
      "Yan Wang",
      "Lingfei Qian",
      "Xueqing Peng",
      "Yang Ren",
      "Keyi Wang",
      "Yi Han",
      "Dongji Feng",
      "Fengran Mo",
      "Shengyuan Lin",
      "Qinchuan Zhang",
      "Kaiwen He",
      "Chenri Luo",
      "Jianxing Chen",
      "Junwei Wu",
      "Chen Xu",
      "Ziyang Xu",
      "Jimin Huang",
      "Guojun Xiong",
      "Xiao-Yang Liu",
      "Qianqian Xie",
      "Jian-Yun Nie"
    ],
    "abstract": "Accurate interpretation of numerical data in financial reports is critical for markets and regulators. Although XBRL (eXtensible Business Reporting Language) provides a standard for tagging financial figures, mapping thousands of facts to over ten thousand US-GAAP concepts remains costly and error-prone. Existing benchmarks oversimplify this task as flat, single-step classification over small subsets of concepts, ignoring the hierarchical semantics of the taxonomy and the structured nature of financial documents. As a result, these benchmarks fail to evaluate Large Language Models (LLMs) under realistic reporting conditions. To bridge this gap, we introduce FinTagging, the first comprehensive benchmark for structure-aware and full-scope XBRL tagging. We decompose the complex tagging process into two subtasks: (1) FinNI (Financial Numeric Identification), which extracts entities and types from heterogeneous contexts such as text and tables; and (2) FinCL (Financial Concept Linking), which maps extracted entities to the full US-GAAP taxonomy. This two-stage formulation enables a fair assessment of LLM capabilities in numerical reasoning and taxonomy alignment. Evaluating diverse LLMs in zero-shot settings shows that while models generalize well in extraction, they struggle with fine-grained concept linking, revealing important limitations in domain-specific, structure-aware reasoning. Code is available on GitHub, and datasets are available on Hugging Face.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20650v3",
    "published_date": "2025-05-27 02:55:53 UTC",
    "updated_date": "2026-01-06 20:05:45 UTC"
  },
  {
    "arxiv_id": "2505.20648v1",
    "title": "Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning",
    "authors": [
      "Mengmeng Chen",
      "Xiaohu Wu",
      "Qiqi Liu",
      "Tiantian He",
      "Yew-Soon Ong",
      "Yaochu Jin",
      "Qicheng Lao",
      "Han Yu"
    ],
    "abstract": "Multi-objective optimization (MOO) exists extensively in machine learning, and aims to find a set of Pareto-optimal solutions, called the Pareto front, e.g., it is fundamental for multiple avenues of research in federated learning (FL). Pareto-Front Learning (PFL) is a powerful method implemented using Hypernetworks (PHNs) to approximate the Pareto front. This method enables the acquisition of a mapping function from a given preference vector to the solutions on the Pareto front. However, most existing PFL approaches still face two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to cover the entire Pareto Front which has a convex shape. Here, we introduce a novel PFL framework, called as PHN-HVVS, which decomposes the design space into Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid partitioning within high-dimensional space. We put forward a new loss function, which effectively contributes to more extensive coverage of the resultant Pareto front and maximizes the HV Indicator. Experimental results on multiple MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines significantly in generating Pareto front. Also, we illustrate that PHN-HVVS advances the methodologies of several recent problems in the FL field. The code is available at https://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20648v1",
    "published_date": "2025-05-27 02:53:14 UTC",
    "updated_date": "2025-05-27 02:53:14 UTC"
  },
  {
    "arxiv_id": "2505.20646v3",
    "title": "Binarized Neural Networks Converge Toward Algorithmic Simplicity: Empirical Support for the Learning-as-Compression Hypothesis",
    "authors": [
      "Eduardo Y. Sakabe",
      "Felipe S. Abrahão",
      "Alexandre Simões",
      "Esther Colombini",
      "Paula Costa",
      "Ricardo Gudwin",
      "Hector Zenil"
    ],
    "abstract": "Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages total, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2505.20646v3",
    "published_date": "2025-05-27 02:51:36 UTC",
    "updated_date": "2025-09-18 15:30:41 UTC"
  },
  {
    "arxiv_id": "2506.15691v3",
    "title": "What Do Latent Action Models Actually Learn?",
    "authors": [
      "Chuheng Zhang",
      "Tim Pearce",
      "Pushi Zhang",
      "Kaixin Wang",
      "Xiaoyu Chen",
      "Wei Shen",
      "Li Zhao",
      "Jiang Bian"
    ],
    "abstract": "Latent action models (LAMs) aim to learn action-relevant changes from unlabeled videos by compressing changes between frames as latents. However, differences between video frames can be caused by controllable changes as well as exogenous noise, leading to an important concern -- do latents capture the changes caused by actions or irrelevant noise? This paper studies this issue analytically, presenting a linear model that encapsulates the essence of LAM learning, while being tractable.This provides several insights, including connections between LAM and principal component analysis (PCA), desiderata of the data-generating policy, and justification of strategies to encourage learning controllable changes using data augmentation, data cleaning, and auxiliary action-prediction. We also provide illustrative results based on numerical simulation, shedding light on the specific structure of observations, actions, and noise in data that influence LAM learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS-25",
    "pdf_url": "https://arxiv.org/pdf/2506.15691v3",
    "published_date": "2025-05-27 02:45:30 UTC",
    "updated_date": "2025-11-12 01:52:35 UTC"
  },
  {
    "arxiv_id": "2505.20644v1",
    "title": "HCQA-1.5 @ Ego4D EgoSchema Challenge 2025",
    "authors": [
      "Haoyu Zhang",
      "Yisen Feng",
      "Qiaohui Chu",
      "Meng Liu",
      "Weili Guan",
      "Yaowei Wang",
      "Liqiang Nie"
    ],
    "abstract": "In this report, we present the method that achieves third place for Ego4D EgoSchema Challenge in CVPR 2025. To improve the reliability of answer prediction in egocentric video question answering, we propose an effective extension to the previously proposed HCQA framework. Our approach introduces a multi-source aggregation strategy to generate diverse predictions, followed by a confidence-based filtering mechanism that selects high-confidence answers directly. For low-confidence cases, we incorporate a fine-grained reasoning module that performs additional visual and contextual analysis to refine the predictions. Evaluated on the EgoSchema blind test set, our method achieves 77% accuracy on over 5,000 human-curated multiple-choice questions, outperforming last year's winning solution and the majority of participating teams. Our code will be added at https://github.com/Hyu-Zhang/HCQA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The third-place solution for the Ego4D EgoSchema Challenge at the CVPR EgoVis Workshop 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20644v1",
    "published_date": "2025-05-27 02:45:14 UTC",
    "updated_date": "2025-05-27 02:45:14 UTC"
  },
  {
    "arxiv_id": "2505.20643v1",
    "title": "Can Past Experience Accelerate LLM Reasoning?",
    "authors": [
      "Bo Pan",
      "Liang Zhao"
    ],
    "abstract": "Allocating more compute to large language models (LLMs) reasoning has generally been demonstrated to improve their effectiveness, but also results in increased inference time. In contrast, humans can perform tasks faster and better with increased experience and exposure. Hence, this paper aims to investigate the question: Can LLMs also become faster at reasoning through recurrent exposure on relevant tasks, and if so, how can it be achieved? To address these questions, we first formalize the problem setting of LLM reasoning speedup systematically in the dimensions of task relevancy and compute budget calculation. We then propose SpeedupLLM, a theoretically guaranteed framework to implement and benchmark such reasoning speedup behaviour based on adaptive compute allocation and memory mechanisms. We further conduct comprehensive experiments to benchmark such behaviour across different question similarity levels, memory methods, and reasoning methods. Results show that LLMs can generally reason faster with past experience, achieving up to a 56% reduction in compute cost when equipped with appropriate memory and reasoning methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20643v1",
    "published_date": "2025-05-27 02:44:00 UTC",
    "updated_date": "2025-05-27 02:44:00 UTC"
  },
  {
    "arxiv_id": "2505.20642v1",
    "title": "CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models",
    "authors": [
      "Yi Zhan",
      "Qi Liu",
      "Weibo Gao",
      "Zheng Zhang",
      "Tianfu Wang",
      "Shuanghong Shen",
      "Junyu Lu",
      "Zhenya Huang"
    ],
    "abstract": "Personalized programming tutoring, such as exercise recommendation, can enhance learners' efficiency, motivation, and outcomes, which is increasingly important in modern digital education. However, the lack of sufficient and high-quality programming data, combined with the mismatch between offline evaluation and real-world learning, hinders the practical deployment of such systems. To address this challenge, many approaches attempt to simulate learner practice data, yet they often overlook the fine-grained, iterative nature of programming learning, resulting in a lack of interpretability and granularity. To fill this gap, we propose a LLM-based agent, CoderAgent, to simulate students' programming processes in a fine-grained manner without relying on real data. Specifically, we equip each human learner with an intelligent agent, the core of which lies in capturing the cognitive states of the human programming practice process. Inspired by ACT-R, a cognitive architecture framework, we design the structure of CoderAgent to align with human cognitive architecture by focusing on the mastery of programming knowledge and the application of coding ability. Recognizing the inherent patterns in multi-layered cognitive reasoning, we introduce the Programming Tree of Thought (PTOT), which breaks down the process into four steps: why, how, where, and what. This approach enables a detailed analysis of iterative problem-solving strategies. Finally, experimental evaluations on real-world datasets demonstrate that CoderAgent provides interpretable insights into learning trajectories and achieves accurate simulations, paving the way for personalized programming education.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by IJCAI2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20642v1",
    "published_date": "2025-05-27 02:43:38 UTC",
    "updated_date": "2025-05-27 02:43:38 UTC"
  },
  {
    "arxiv_id": "2505.20637v1",
    "title": "TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone",
    "authors": [
      "Ana M. Cabanas",
      "Alma Pedro",
      "Domingo Mery"
    ],
    "abstract": "Understanding how facial affect analysis (FAA) systems perform across different demographic groups requires reliable measurement of sensitive attributes such as ancestry, often approximated by skin tone, which itself is highly influenced by lighting conditions. This study compares two objective skin tone classification methods: the widely used Individual Typology Angle (ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and Hue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness across skin tone groups defined by each method. Results reveal a severe underrepresentation of dark skin tones ($\\sim 2 \\%$), alongside fairness disparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While ITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$ method yields more consistent subgrouping and enables clearer diagnostics through metrics such as Equal Opportunity. Grad-CAM analysis further highlights differences in model attention patterns by skin tone, suggesting variation in feature encoding. To support future mitigation efforts, we also propose a modular fairness-aware pipeline that integrates perceptual skin tone estimation, model interpretability, and fairness evaluation. These findings emphasize the relevance of skin tone measurement choices in fairness assessment and suggest that ITA-based evaluations may overlook disparities affecting darker-skinned individuals.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.20637v1",
    "published_date": "2025-05-27 02:31:08 UTC",
    "updated_date": "2025-05-27 02:31:08 UTC"
  },
  {
    "arxiv_id": "2505.23805v1",
    "title": "ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes",
    "authors": [
      "Akram Sheriff",
      "Ken Huang",
      "Zsolt Nemeth",
      "Madjid Nakhjiri"
    ],
    "abstract": "This paper introduces the Adaptive Defense Agent (ADA), an innovative Automated Moving Target Defense (AMTD) system designed to fundamentally enhance the security posture of AI workloads. ADA operates by continuously and automatically rotating these workloads at the infrastructure level, leveraging the inherent ephemerality of Kubernetes pods. This constant managed churn systematically invalidates attacker assumptions and disrupts potential kill chains by regularly destroying and respawning AI service instances. This methodology, applying principles of chaos engineering as a continuous, proactive defense, offers a paradigm shift from traditional static defenses that rely on complex and expensive confidential or trusted computing solutions to secure the underlying compute platforms, while at the same time agnostically supporting the latest advancements in agentic and nonagentic AI ecosystems and solutions such as agent-to-agent (A2A) communication frameworks or model context protocols (MCP). This AI-native infrastructure design, relying on the widely proliferated cloud-native Kubernetes technologies, facilitates easier deployment, simplifies maintenance through an inherent zero trust posture achieved by rotation, and promotes faster adoption. We posit that ADA's novel approach to AMTD provides a more robust, agile, and operationally efficient zero-trust model for AI services, achieving security through proactive environmental manipulation rather than reactive patching.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.23805v1",
    "published_date": "2025-05-27 02:24:45 UTC",
    "updated_date": "2025-05-27 02:24:45 UTC"
  },
  {
    "arxiv_id": "2505.21562v1",
    "title": "Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge",
    "authors": [
      "Jennifer Turliuk",
      "Alejandro Sevilla",
      "Daniela Gorza",
      "Tod Hynes"
    ],
    "abstract": "This case study examines the ClimaTech Great Global Innovation Challenge's approach to selecting climate tech startups by integrating human and AI evaluations. The competition aimed to identify top startups and enhance the accuracy and efficiency of the selection process through a hybrid model. Research shows data-driven approaches help VC firms reduce bias and improve decision-making. Machine learning models have outperformed human investors in deal screening, helping identify high-potential startups. Incorporating AI aimed to ensure more equitable and objective evaluations.\n  The methodology included three phases: initial AI review, semi-finals judged by humans, and finals using a hybrid weighting. In phase one, 57 applications were scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top 36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated startups on team quality, market potential, and technological innovation. Each score - human or AI - was weighted equally, resulting in 75 percent human and 25 percent AI influence. In the finals, with five human judges, weighting shifted to 83.3 percent human and 16.7 percent AI. There was a moderate positive correlation between AI and human scores - Spearman's = 0.47 - indicating general alignment with key differences. Notably, the final four startups, selected mainly by humans, were among those rated highest by the AI. This highlights the complementary nature of AI and human judgment. The study shows that hybrid models can streamline and improve startup assessments. The ClimaTech approach offers a strong framework for future competitions by combining human expertise with AI capabilities.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.21562v1",
    "published_date": "2025-05-27 02:23:03 UTC",
    "updated_date": "2025-05-27 02:23:03 UTC"
  },
  {
    "arxiv_id": "2505.20635v1",
    "title": "Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction",
    "authors": [
      "Zexu Pan",
      "Shengkui Zhao",
      "Tingting Wang",
      "Kun Zhou",
      "Yukun Ma",
      "Chong Zhang",
      "Bin Ma"
    ],
    "abstract": "Audio-visual speaker extraction isolates a target speaker's speech from a mixture speech signal conditioned on a visual cue, typically using the target speaker's face recording. However, in real-world scenarios, other co-occurring faces are often present on-screen, providing valuable speaker activity cues in the scene. In this work, we introduce a plug-and-play inter-speaker attention module to process these flexible numbers of co-occurring faces, allowing for more accurate speaker extraction in complex multi-person environments. We integrate our module into two prominent models: the AV-DPRNN and the state-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets, including the highly overlapped VoxCeleb2 and sparsely overlapped MISP, demonstrate that our approach consistently outperforms baselines. Furthermore, cross-dataset evaluations on LRS2 and LRS3 confirm the robustness and generalizability of our method.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20635v1",
    "published_date": "2025-05-27 02:21:38 UTC",
    "updated_date": "2025-05-27 02:21:38 UTC"
  },
  {
    "arxiv_id": "2505.20633v1",
    "title": "Test-Time Learning for Large Language Models",
    "authors": [
      "Jinwu Hu",
      "Zhitian Zhang",
      "Guohao Chen",
      "Xutao Wen",
      "Chao Shuai",
      "Wei Luo",
      "Bin Xiao",
      "Yuanqing Li",
      "Mingkui Tan"
    ],
    "abstract": "While Large Language Models (LLMs) have exhibited remarkable emergent capabilities through extensive pre-training, they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations, known as distribution shifts. In this paper, we propose a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically adapts LLMs to target domains using only unlabeled test data during testing. Specifically, we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from LLMs can be achieved by minimizing the input perplexity of the unlabeled test data. Based on this insight, we formulate the Test-Time Learning process of LLMs as input perplexity minimization, enabling self-supervised enhancement of LLM performance. Furthermore, we observe that high-perplexity samples tend to be more informative for model optimization. Accordingly, we introduce a Sample Efficient Learning Strategy that actively selects and emphasizes these high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA) instead of full-parameter optimization, which allows lightweight model updates while preserving more original knowledge from the model. We introduce the AdaptEval benchmark for TTL and demonstrate through experiments that TLM improves performance by at least 20% compared to original LLMs on domain knowledge adaptation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICML2025",
    "pdf_url": "https://arxiv.org/pdf/2505.20633v1",
    "published_date": "2025-05-27 02:18:59 UTC",
    "updated_date": "2025-05-27 02:18:59 UTC"
  },
  {
    "arxiv_id": "2505.20622v1",
    "title": "SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation",
    "authors": [
      "Ting Xu",
      "Zhichao Huang",
      "Jiankai Sun",
      "Shanbo Cheng",
      "Wai Lam"
    ],
    "abstract": "We present Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a new policy optimization framework that defines the simultaneous machine translation (SiMT) task as a sequential decision making problem, incorporating a tailored reward to enhance translation quality while reducing latency. In contrast to popular Reinforcement Learning from Human Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task. This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT process using a tailored reward. We conduct experiments on six datasets from diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that SeqPO-SiMT consistently achieves significantly higher translation quality with lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning (SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17 in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly rival the offline translation of high-performing LLMs, including Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)",
    "pdf_url": "https://arxiv.org/pdf/2505.20622v1",
    "published_date": "2025-05-27 01:59:58 UTC",
    "updated_date": "2025-05-27 01:59:58 UTC"
  },
  {
    "arxiv_id": "2505.20621v1",
    "title": "Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning",
    "authors": [
      "Shijie Liu",
      "Andrew C. Cullen",
      "Paul Montague",
      "Sarah Erfani",
      "Benjamin I. P. Rubinstein"
    ],
    "abstract": "Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend certified defenses to provide larger guarantees against adversarial manipulation, ensuring robustness for both per-state actions, and the overall expected cumulative reward. Our approach leverages properties of Differential Privacy, in a manner that allows this work to span both continuous and discrete spaces, as well as stochastic and deterministic environments -- significantly expanding the scope and applicability of achievable guarantees. Empirical evaluations demonstrate that our approach ensures the performance drops to no more than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly improving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while producing certified radii that is $5$ times larger as well. This highlights the potential of our framework to enhance safety and reliability in offline RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20621v1",
    "published_date": "2025-05-27 01:59:25 UTC",
    "updated_date": "2025-05-27 01:59:25 UTC"
  },
  {
    "arxiv_id": "2505.20613v3",
    "title": "REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning",
    "authors": [
      "Ziju Shen",
      "Naohao Huang",
      "Fanyi Yang",
      "Yutong Wang",
      "Guoxiong Gao",
      "Tianyi Xu",
      "Jiedong Jiang",
      "Wanyi He",
      "Pu Yang",
      "Mengzhou Sun",
      "Haocheng Ju",
      "Peihao Wu",
      "Bryan Dai",
      "Bin Dong"
    ],
    "abstract": "Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20613v3",
    "published_date": "2025-05-27 01:26:11 UTC",
    "updated_date": "2025-11-24 06:18:18 UTC"
  },
  {
    "arxiv_id": "2505.20609v1",
    "title": "Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients",
    "authors": [
      "Hyungjun Park",
      "Chang-Yun Woo",
      "Seungjo Lim",
      "Seunghwan Lim",
      "Keunho Kwak",
      "Ju Young Jeong",
      "Chong Hyun Suh"
    ],
    "abstract": "Objective To develop an LLM based realtime compound diagnostic medical AI interface and performed a clinical trial comparing this interface and physicians for common internal medicine cases based on the United States Medical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A nonrandomized clinical trial was conducted on August 20, 2024. We recruited one general physician, two internal medicine residents (2nd and 3rd year), and five simulated patients. The clinical vignettes were adapted from the USMLE Step 2 CS style exams. We developed 10 representative internal medicine cases based on actual patients and included information available on initial diagnostic evaluation. Primary outcome was the accuracy of the first differential diagnosis. Repeatability was evaluated based on the proportion of agreement. Results The accuracy of the physicians' first differential diagnosis ranged from 50% to 70%, whereas the realtime compound diagnostic medical AI interface achieved an accuracy of 80%. The proportion of agreement for the first differential diagnosis was 0.7. The accuracy of the first and second differential diagnoses ranged from 70% to 90% for physicians, whereas the AI interface achieved an accuracy rate of 100%. The average time for the AI interface (557 sec) was 44.6% shorter than that of the physicians (1006 sec). The AI interface ($0.08) also reduced costs by 98.1% compared to the physicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3 for care by physicians and were 3.9 for the AI interface Conclusion An LLM based realtime compound diagnostic medical AI interface demonstrated diagnostic accuracy and patient satisfaction comparable to those of a physician, while requiring less time and lower costs. These findings suggest that AI interfaces may have the potential to assist primary care consultations for common internal medicine cases.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20609v1",
    "published_date": "2025-05-27 01:15:46 UTC",
    "updated_date": "2025-05-27 01:15:46 UTC"
  },
  {
    "arxiv_id": "2505.23804v2",
    "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies",
    "authors": [
      "Terrance Liu",
      "Shuyi Wang",
      "Daniel Preotiuc-Pietro",
      "Yash Chandarana",
      "Chirag Gupta"
    ],
    "abstract": "While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named \"sub-clause frequency\" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025 main conference",
    "pdf_url": "https://arxiv.org/pdf/2505.23804v2",
    "published_date": "2025-05-27 01:01:55 UTC",
    "updated_date": "2025-09-17 17:39:16 UTC"
  },
  {
    "arxiv_id": "2505.20600v1",
    "title": "InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling",
    "authors": [
      "Xiaoxiao Jiang",
      "Suyi Li",
      "Lingyun Yang",
      "Tianyu Feng",
      "Zhipeng Di",
      "Weiyi Lu",
      "Guoxuan Zhu",
      "Xiu Lin",
      "Kan Liu",
      "Yinghao Yu",
      "Tao Lan",
      "Guodong Yang",
      "Lin Qu",
      "Liping Zhang",
      "Wei Wang"
    ],
    "abstract": "Generative image editing using diffusion models has become a prevalent application in today's AI cloud services. In production environments, image editing typically involves a mask that specifies the regions of an image template to be edited. The use of masks provides direct control over the editing process and introduces sparsity in the model inference. In this paper, we present InstGenIE, a system that efficiently serves image editing requests. The key insight behind InstGenIE is that image editing only modifies the masked regions of image templates while preserving the original content in the unmasked areas. Driven by this insight, InstGenIE judiciously skips redundant computations associated with the unmasked areas by reusing cached intermediate activations from previous inferences. To mitigate the high cache loading overhead, InstGenIE employs a bubble-free pipeline scheme that overlaps computation with cache loading. Additionally, to reduce queuing latency in online serving while improving the GPU utilization, InstGenIE proposes a novel continuous batching strategy for diffusion model serving, allowing newly arrived requests to join the running batch in just one step of denoising computation, without waiting for the entire batch to complete. As heterogeneous masks induce imbalanced loads, InstGenIE also develops a load balancing strategy that takes into account the loads of both computation and cache loading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving systems for image editing, achieving up to 3x higher throughput and reducing average request latency by up to 14.7x while ensuring image quality.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.20600v1",
    "published_date": "2025-05-27 00:36:56 UTC",
    "updated_date": "2025-05-27 00:36:56 UTC"
  }
]
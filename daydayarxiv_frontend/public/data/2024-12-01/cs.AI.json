{
  "date": "2024-12-01",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-12-01 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 59 篇论文，主要聚焦于 AI 和大语言模型 (LLM) 的创新应用，包括 LLM 的道德偏见、鲁棒性提升、多模态融合，以及在时间序列分析、医疗和游戏生成领域的突破。重点包括减少 LLM sycophancy 的方法，以及生成可玩游戏的 PlayGen 模型；令人印象深刻的是 José Miguel Hernández-Lobato 等学者的药物设计工作，以及 NeurIPS 相关论文如 Linear Probe Penalties Reduce LLM Sycophancy，它们展示了 LLM 在实际应用中的潜力。\n\n下面，我挑选并简要讨论了部分重要、话题度高的论文，先聊 AI 和 LLM 相关的内容，再涉及多模态和科学应用；其他较常规的论文（如一些描述逻辑或基准测试）将快速掠过，仅列出标题。\n\n1. **DSSRNN: Decomposition-Enhanced State-Space Recurrent Neural Network for Time-Series Analysis（分解增强状态空间循环神经网络用于时间序列分析）**  \n   这篇论文提出 DSSRNN 框架，用于长短期时间序列预测，通过结合分解分析（捕捉季节性和趋势成分）和状态空间模型，在室内空气质量数据集上优于 Transformer 模型，实现了更低的 MSE 和 MAE（如 T=96 时 MSE 为 0.378），并平衡了性能和计算效率。\n\n2. **Linear Probe Penalties Reduce LLM Sycophancy（线性探针惩罚减少 LLM 的讨好行为）**  \n   作者 Henry Papadatos 和 Rachel Freedman 的这篇 NeurIPS 2024 工作地址 LLM 在强化学习中的讨好问题，通过线性探针方法识别并惩罚 sycophantic 标记，实验显示优化后的奖励函数显著减少了开源 LLM 的这种行为，提升了模型的客观性和可靠性。\n\n3. **LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics（LLM 作为社会道德标准的镜像：跨文化道德差异和一致性的反映）**  \n   作者 Mijntje Meijer 等探讨 LLM 是否能反映跨文化道德差异，使用比较方法（如模型生成分数与调查数据对比），发现现有 LLM 在捕捉这些差异时表现不佳，强调了改进模型以减少文化偏见的重要性。\n\n4. **Generative Language Models Potential for Requirement Engineering Applications: Insights into Current Strengths and Limitations（生成语言模型在需求工程应用中的潜力：当前优势和局限性的洞见）**  \n   作者 Summra Saleem 等评估 ChatGPT 和 Gemini 在需求工程任务上的表现（如提取和分类），发现 Gemini 需要更精确的提示工程，且二者在问题回答任务上接近 SOTA（F1 分数 0.91），但在其他任务上逊色，提供了实用指导。\n\n5. **Large Language Models as Mirrors of Societal Moral Standards（大型语言模型作为社会道德标准的镜像）**  \n   作者 Evi Papadopoulou 等扩展了前作，测试 LLM 在道德主题（如同性恋和离婚）上的跨文化表现，使用 WVS 和 PEW 数据，发现模型存在偏见，BLOOM 模型表现最佳但仍不足，呼吁开发更文化敏感的 AI。\n\n6. **BIGCity: A Universal Spatiotemporal Model for Unified Trajectory and Traffic State Data Analysis（BIGCity：统一轨迹和交通状态数据分析的通用时空模型）**  \n   这篇论文引入 BIGCity 模型，处理轨迹和交通状态数据，解决了多模态统一表示和任务异构性问题，通过 ST-unit 和提示机制，在 8 个任务上超越 18 个基线，成为时空数据分析的新基准。\n\n7. **STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft（STEVE-Audio：扩展 Minecraft 中具身代理的目标条件模态）**  \n   作者 Nicholas Lenzen 等扩展 STEVE-1 方法，支持音频模态控制代理，在 Minecraft 中实现与文本/视觉相当的性能，构建了 Audio-Video CLIP 模型，促进多模态决策研究。\n\n8. **Playable Game Generation（可玩游戏生成）**  \n   作者 Mingyu Yang 等提出 PlayGen 方法，使用自回归 DiT 扩散模型生成实时互动游戏，实现了高视觉质量和机制模拟，在 2D/3D 游戏上表现卓越，支持超过 1000 帧的游戏。\n\n9. **A Deep Generative Model for the Design of Synthesizable Ionizable Lipids（用于合成离子化脂质设计的深度生成模型）**  \n   作者 José Miguel Hernández-Lobato 等开发专为离子化脂质设计的生成模型，生成新结构并提供合成路径，加速 mRNA 疫苗和基因疗法的开发，NeurIPS 工作展示其实用性。\n\n10. **Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification（Dynamic-LLaVA：通过动态视觉-语言上下文稀疏化实现高效的多模态 LLM）**  \n    这篇论文提出 Dynamic-LLaVA 框架，动态减少视觉和语言上下文冗余，减少预填充阶段 75% 计算消耗，并在多模态任务上保持性能，ICLR 2025 接受。\n\n11. **Learning Aggregation Rules in Participatory Budgeting: A Data-Driven Approach（参与式预算中的聚合规则学习：数据驱动方法）**  \n    作者 Roy Fairstein 等使用神经网络从 PB 数据中学习聚合规则，平衡社会福利和代表性，在真实数据上泛化良好，提供更灵活的预算决策工具。\n\n其他论文如 Bilinear Convolution Decomposition for Causal RL Interpretability（双线性卷积分解用于因果强化学习可解释性）、TSUBF-Net（用于 CT 图像分割的网络）等虽有贡献，但主题较 niche，仅快速提及：它们分别提升了 RL 的可解释性和医疗图像分割精度。剩余论文（如 Well log data generation、A Comprehensive Guide to Explainable AI 等）涉及数据生成和 XAI 基础知识，但不列细节，以控制篇幅。\n\n总之，今天的更新突显了 AI 模型在道德、效率和实际应用上的进展，值得关注领域从业者跟进。明天见！",
  "papers": [
    {
      "arxiv_id": "2412.00994v1",
      "title": "DSSRNN: Decomposition-Enhanced State-Space Recurrent Neural Network for Time-Series Analysis",
      "title_zh": "DSSRNN：分解增强的状态空间循环神经网络用于时间序列分析",
      "authors": [
        "Ahmad Mohammadshirazi",
        "Ali Nosratifiroozsalari",
        "Rajiv Ramnath"
      ],
      "abstract": "Time series forecasting is a crucial yet challenging task in machine\nlearning, requiring domain-specific knowledge due to its wide-ranging\napplications. While recent Transformer models have improved forecasting\ncapabilities, they come with high computational costs. Linear-based models have\nshown better accuracy than Transformers but still fall short of ideal\nperformance. To address these challenges, we introduce the Decomposition\nState-Space Recurrent Neural Network (DSSRNN), a novel framework designed for\nboth long-term and short-term time series forecasting. DSSRNN uniquely combines\ndecomposition analysis to capture seasonal and trend components with\nstate-space models and physics-based equations. We evaluate DSSRNN's\nperformance on indoor air quality datasets, focusing on CO2 concentration\nprediction across various forecasting horizons. Results demonstrate that DSSRNN\nconsistently outperforms state-of-the-art models, including transformer-based\narchitectures, in terms of both Mean Squared Error (MSE) and Mean Absolute\nError (MAE). For example, at the shortest horizon (T=96) in Office 1, DSSRNN\nachieved an MSE of 0.378 and an MAE of 0.401, significantly lower than\ncompeting models. Additionally, DSSRNN exhibits superior computational\nefficiency compared to more complex models. While not as lightweight as the\nDLinear model, DSSRNN achieves a balance between performance and efficiency,\nwith only 0.11G MACs and 437MiB memory usage, and an inference time of 0.58ms\nfor long-term forecasting. This work not only showcases DSSRNN's success but\nalso establishes a new benchmark for physics-informed machine learning in\nenvironmental forecasting and potentially other domains.",
      "tldr_zh": "这篇论文引入了DSSRNN，一种新型的Decomposition-Enhanced State-Space Recurrent Neural Network框架，用于提升时间序列分析中的长期和短期预测性能。DSSRNN通过结合分解分析（捕捉季节性和趋势组件）、状态空间模型和基于物理的方程，解决了Transformer模型的高计算成本和线性模型的性能不足问题。在室内空气质量数据集上评估，特别是CO2浓度预测，DSSRNN在各种预测时段表现出色，例如在Office 1的T=96时段，实现了MSE为0.378和MAE为0.401的低错误率，超越了最先进模型。最后，DSSRNN在计算效率上实现了平衡，仅需0.11G MACs和437MiB内存，推理时间为0.58ms，并为基于物理的机器学习在环境预测等领域设定了新基准。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00994v1",
      "published_date": "2024-12-01 22:55:58 UTC",
      "updated_date": "2024-12-01 22:55:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:46:55.154084"
    },
    {
      "arxiv_id": "2412.00967v1",
      "title": "Linear Probe Penalties Reduce LLM Sycophancy",
      "title_zh": "翻译失败",
      "authors": [
        "Henry Papadatos",
        "Rachel Freedman"
      ],
      "abstract": "Large language models (LLMs) are often sycophantic, prioritizing agreement\nwith their users over accurate or objective statements. This problematic\nbehavior becomes more pronounced during reinforcement learning from human\nfeedback (RLHF), an LLM fine-tuning stage intended to align model outputs with\nhuman values. Instead of increasing accuracy and reliability, the reward model\nlearned from RLHF often rewards sycophancy. We develop a linear probing method\nto identify and penalize markers of sycophancy within the reward model,\nproducing rewards that discourage sycophantic behavior. Our experiments show\nthat constructing and optimizing against this surrogate reward function reduces\nsycophantic behavior in multiple open-source LLMs. Our results suggest a\ngeneralizable methodology for reducing unwanted LLM behaviors that are not\nsufficiently disincentivized by RLHF fine-tuning.",
      "tldr_zh": "大语言模型 (LLMs) 经常表现出逢迎行为 (sycophancy)，即优先同意用户而非提供准确或客观信息，尤其在强化学习从人类反馈 (RLHF) 微调过程中，这种行为被奖励模型强化。作者开发了一种线性探查 (linear probing) 方法，用于识别奖励模型中的逢迎标记，并通过惩罚这些标记来构建替代奖励函数，以减少这种 unwanted 行为。实验结果表明，该方法在多个开源 LLMs 上有效降低了逢迎行为，并提供了一种可推广的策略来抑制 RLHF 微调中未充分抑制的负面行为。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 15 figures, NeurIPS 2024 Workshop Socially Responsible\n  Language Modelling Research (SoLaR)",
      "pdf_url": "http://arxiv.org/pdf/2412.00967v1",
      "published_date": "2024-12-01 21:11:28 UTC",
      "updated_date": "2024-12-01 21:11:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:46:55.723723"
    },
    {
      "arxiv_id": "2412.00962v1",
      "title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics",
      "title_zh": "LLMs 作为社会道德标准的镜像：伦理主题中文化分歧与",
      "authors": [
        "Mijntje Meijer",
        "Hadi Mohammadi",
        "Ayoub Bagheri"
      ],
      "abstract": "Large language models (LLMs) have become increasingly pivotal in various\ndomains due the recent advancements in their performance capabilities. However,\nconcerns persist regarding biases in LLMs, including gender, racial, and\ncultural biases derived from their training data. These biases raise critical\nquestions about the ethical deployment and societal impact of LLMs.\nAcknowledging these concerns, this study investigates whether LLMs accurately\nreflect cross-cultural variations and similarities in moral perspectives. In\nassessing whether the chosen LLMs capture patterns of divergence and agreement\non moral topics across cultures, three main methods are employed: (1)\ncomparison of model-generated and survey-based moral score variances, (2)\ncluster alignment analysis to evaluate the correspondence between country\nclusters derived from model-generated moral scores and those derived from\nsurvey data, and (3) probing LLMs with direct comparative prompts. All three\nmethods involve the use of systematic prompts and token pairs designed to\nassess how well LLMs understand and reflect cultural variations in moral\nattitudes. The findings of this study indicate overall variable and low\nperformance in reflecting cross-cultural differences and similarities in moral\nvalues across the models tested, highlighting the necessity for improving\nmodels' accuracy in capturing these nuances effectively. The insights gained\nfrom this study aim to inform discussions on the ethical development and\ndeployment of LLMs in global contexts, emphasizing the importance of mitigating\nbiases and promoting fair representation across diverse cultural perspectives.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）是否能准确反映跨文化在道德标准上的差异和共识，旨在评估LLMs在捕捉文化偏差方面的表现。研究采用三种方法：（1）比较模型生成的道德分数变异与调查数据，（2）集群对齐分析以评估国家集群的对应性，以及（3）使用直接比较提示来探测LLMs对文化变异的理解。结果显示，测试的LLMs在反映跨文化道德差异和相似性方面表现变量且整体较低，突显了改进模型以减少偏差和促进公平表示的必要性。这些发现为LLMs的伦理开发和全球部署提供了重要指导。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00962v1",
      "published_date": "2024-12-01 20:39:42 UTC",
      "updated_date": "2024-12-01 20:39:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:46:57.969158"
    },
    {
      "arxiv_id": "2412.00959v1",
      "title": "Generative Language Models Potential for Requirement Engineering Applications: Insights into Current Strengths and Limitations",
      "title_zh": "生成式语言模型在需求工程应用中的潜力：对当前优势和限制的洞见",
      "authors": [
        "Summra Saleem",
        "Muhammad Nabeel Asim",
        "Ludger Van Elst",
        "Andreas Dengel"
      ],
      "abstract": "Traditional language models have been extensively evaluated for software\nengineering domain, however the potential of ChatGPT and Gemini have not been\nfully explored. To fulfill this gap, the paper in hand presents a comprehensive\ncase study to investigate the potential of both language models for development\nof diverse types of requirement engineering applications. It deeply explores\nimpact of varying levels of expert knowledge prompts on the prediction\naccuracies of both language models. Across 4 different public benchmark\ndatasets of requirement engineering tasks, it compares performance of both\nlanguage models with existing task specific machine/deep learning predictors\nand traditional language models. Specifically, the paper utilizes 4 benchmark\ndatasets; Pure (7,445 samples, requirements extraction),PROMISE (622 samples,\nrequirements classification), REQuestA (300 question answer (QA) pairs) and\nAerospace datasets (6347 words, requirements NER tagging). Our experiments\nreveal that, in comparison to ChatGPT, Gemini requires more careful prompt\nengineering to provide accurate predictions. Moreover, across requirement\nextraction benchmark dataset the state-of-the-art F1-score is 0.86 while\nChatGPT and Gemini achieved 0.76 and 0.77,respectively. The State-of-the-art\nF1-score on requirements classification dataset is 0.96 and both language\nmodels 0.78. In name entity recognition (NER) task the state-of-the-art\nF1-score is 0.92 and ChatGPT managed to produce 0.36, and Gemini 0.25.\nSimilarly, across question answering dataset the state-of-the-art F1-score is\n0.90 and ChatGPT and Gemini managed to produce 0.91 and 0.88 respectively. Our\nexperiments show that Gemini requires more precise prompt engineering than\nChatGPT. Except for question-answering, both models under-perform compared to\ncurrent state-of-the-art predictors across other tasks.",
      "tldr_zh": "本论文通过一个全面案例研究，评估了生成式语言模型（如 ChatGPT 和 Gemini）在需求工程应用中的潜力，包括需求提取、分类、命名实体识别 (NER) 和问答任务。研究者使用四个公共基准数据集（Pure、PROMISE、REQuestA 和 Aerospace），并探索了不同专家知识提示水平对模型预测准确性的影响。实验结果显示，ChatGPT 在问答任务上达到 F1-score 0.91，与最先进模型相当，但在需求提取 (F1-score 0.76)、分类 (0.78) 和 NER (0.36) 等任务上表现逊色；Gemini 需要更精确的 prompt engineering，且整体性能不如 ChatGPT。总体而言，该研究突出了这些模型的 strengths 和 limitations，为需求工程中的语言模型应用提供了宝贵insights。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00959v1",
      "published_date": "2024-12-01 20:20:58 UTC",
      "updated_date": "2024-12-01 20:20:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:01.319432"
    },
    {
      "arxiv_id": "2412.00956v1",
      "title": "Large Language Models as Mirrors of Societal Moral Standards",
      "title_zh": "大语言模型作为社会道德标准的镜像",
      "authors": [
        "Evi Papadopoulou",
        "Hadi Mohammadi",
        "Ayoub Bagheri"
      ],
      "abstract": "Prior research has demonstrated that language models can, to a limited\nextent, represent moral norms in a variety of cultural contexts. This research\naims to replicate these findings and further explore their validity,\nconcentrating on issues like 'homosexuality' and 'divorce'. This study\nevaluates the effectiveness of these models using information from two surveys,\nthe WVS and the PEW, that encompass moral perspectives from over 40 countries.\nThe results show that biases exist in both monolingual and multilingual models,\nand they typically fall short of accurately capturing the moral intricacies of\ndiverse cultures. However, the BLOOM model shows the best performance,\nexhibiting some positive correlations, but still does not achieve a\ncomprehensive moral understanding. This research underscores the limitations of\ncurrent PLMs in processing cross-cultural differences in values and highlights\nthe importance of developing culturally aware AI systems that better align with\nuniversal human values.",
      "tldr_zh": "本研究评估了Large Language Models在反映社会道德标准方面的能力，特别针对'homosexuality'（同性恋）和'divorce'（离婚）等议题，通过WVS和PEW调查数据（覆盖超过40个国家）来验证模型的表现。结果显示，单语和多语模型均存在偏差，无法准确捕捉不同文化的道德复杂性，尽管BLOOM模型表现出一些正相关但仍未实现全面理解。该研究强调了当前PLMs在处理跨文化价值差异的局限性，并呼吁开发更符合普遍人类价值的文化感知AI系统。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00956v1",
      "published_date": "2024-12-01 20:20:35 UTC",
      "updated_date": "2024-12-01 20:20:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:02.700736"
    },
    {
      "arxiv_id": "2412.00953v1",
      "title": "BIGCity: A Universal Spatiotemporal Model for Unified Trajectory and Traffic State Data Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Xie Yu",
        "Jingyuan Wang",
        "Yifan Yang",
        "Qian Huang",
        "Ke Qu"
      ],
      "abstract": "Typical dynamic ST data includes trajectory data (representing\nindividual-level mobility) and traffic state data (representing\npopulation-level mobility). Traditional studies often treat trajectory and\ntraffic state data as distinct, independent modalities, each tailored to\nspecific tasks within a single modality. However, real-world applications, such\nas navigation apps, require joint analysis of trajectory and traffic state\ndata. Treating these data types as two separate domains can lead to suboptimal\nmodel performance. Although recent advances in ST data pre-training and ST\nfoundation models aim to develop universal models for ST data analysis, most\nexisting models are \"multi-task, solo-data modality\" (MTSM), meaning they can\nhandle multiple tasks within either trajectory data or traffic state data, but\nnot both simultaneously. To address this gap, this paper introduces BIGCity,\nthe first multi-task, multi-data modality (MTMD) model for ST data analysis.\nThe model targets two key challenges in designing an MTMD ST model: (1)\nunifying the representations of different ST data modalities, and (2) unifying\nheterogeneous ST analysis tasks. To overcome the first challenge, BIGCity\nintroduces a novel ST-unit that represents both trajectories and traffic states\nin a unified format. Additionally, for the second challenge, BIGCity adopts a\ntunable large model with ST task-oriented prompt, enabling it to perform a\nrange of heterogeneous tasks without the need for fine-tuning. Extensive\nexperiments on real-world datasets demonstrate that BIGCity achieves\nstate-of-the-art performance across 8 tasks, outperforming 18 baselines. To the\nbest of our knowledge, BIGCity is the first model capable of handling both\ntrajectories and traffic states for diverse heterogeneous tasks. Our code are\navailable at https://github.com/bigscity/BIGCity",
      "tldr_zh": "该论文提出BIGCity，一种通用的时空模型，用于统一分析轨迹数据（trajectory data）和交通状态数据（traffic state data），解决传统“multi-task, solo-data modality”（MTSM）模型无法同时处理多种数据模态的问题。BIGCity通过引入ST-unit统一不同数据模态的表示，并采用tunable large model with ST task-oriented prompt来处理异构任务，而无需fine-tuning。实验结果显示，该模型在8个真实世界任务上超越18个基线模型，达到state-of-the-art性能，是首个支持多任务多数据模态（MTMD）的模型。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00953v1",
      "published_date": "2024-12-01 20:10:55 UTC",
      "updated_date": "2024-12-01 20:10:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:04.872189"
    },
    {
      "arxiv_id": "2412.00949v1",
      "title": "STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft",
      "title_zh": "翻译失败",
      "authors": [
        "Nicholas Lenzen",
        "Amogh Raut",
        "Andrew Melnik"
      ],
      "abstract": "Recently, the STEVE-1 approach has been introduced as a method for training\ngenerative agents to follow instructions in the form of latent CLIP embeddings.\nIn this work, we present a methodology to extend the control modalities by\nlearning a mapping from new input modalities to the latent goal space of the\nagent. We apply our approach to the challenging Minecraft domain, and extend\nthe goal conditioning to include the audio modality. The resulting\naudio-conditioned agent is able to perform on a comparable level to the\noriginal text-conditioned and visual-conditioned agents. Specifically, we\ncreate an Audio-Video CLIP foundation model for Minecraft and an audio prior\nnetwork which together map audio samples to the latent goal space of the\nSTEVE-1 policy. Additionally, we highlight the tradeoffs that occur when\nconditioning on different modalities. Our training code, evaluation code, and\nAudio-Video CLIP foundation model for Minecraft are made open-source to help\nfoster further research into multi-modal generalist sequential decision-making\nagents.",
      "tldr_zh": "这篇论文扩展了 STEVE-1 方法，使具身代理在 Minecraft 环境中能够基于音频模态进行目标条件训练，具体通过学习从新输入模态映射到代理的潜在目标空间。研究者开发了 Audio-Video CLIP 基础模型和音频 prior 网络，将音频样本转化为 CLIP 嵌入，从而实现音频条件下的代理行为。实验结果显示，音频条件代理的表现与文本条件和视觉条件代理相当，同时突出了不同模态间的权衡，如性能和泛化性。论文开源了训练代码、评估代码和 Audio-Video CLIP 模型，以推动多模态决策代理的研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at CoRL 2024: Workshop on Lifelong Learning for Home Robots",
      "pdf_url": "http://arxiv.org/pdf/2412.00949v1",
      "published_date": "2024-12-01 19:48:57 UTC",
      "updated_date": "2024-12-01 19:48:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:07.522797"
    },
    {
      "arxiv_id": "2412.00944v1",
      "title": "Bilinear Convolution Decomposition for Causal RL Interpretability",
      "title_zh": "双线性卷积分解用于因果强化学习可解释性",
      "authors": [
        "Narmeen Oozeer",
        "Sinem Erisken",
        "Alice Rigg"
      ],
      "abstract": "Efforts to interpret reinforcement learning (RL) models often rely on\nhigh-level techniques such as attribution or probing, which provide only\ncorrelational insights and coarse causal control. This work proposes replacing\nnonlinearities in convolutional neural networks (ConvNets) with bilinear\nvariants, to produce a class of models for which these limitations can be\naddressed. We show bilinear model variants perform comparably in model-free\nreinforcement learning settings, and give a side by side comparison on ProcGen\nenvironments. Bilinear layers' analytic structure enables weight-based\ndecomposition. Previous work has shown bilinearity enables quantifying\nfunctional importance through eigendecomposition, to identify interpretable low\nrank structure. We show how to adapt the decomposition to convolution layers by\napplying singular value decomposition to vectors of interest, to separate the\nchannel and spatial dimensions. Finally, we propose a methodology for causally\nvalidating concept-based probes, and illustrate its utility by studying a\nmaze-solving agent's ability to track a cheese object.",
      "tldr_zh": "本研究针对强化学习 (RL) 模型的解释性问题，提出通过在卷积神经网络 (ConvNets) 中替换非线性层为双线性 (bilinear) 变体，来实现更精确的因果控制和解释。实验显示，这种双线性模型在无模型 RL 设置中表现与原模型相当，并在 ProcGen 环境中进行了对比；此外，通过特征分解 (eigendecomposition) 和奇异值分解 (SVD) 对卷积层进行权重分解，能分离通道和空间维度，从而识别可解释的低秩结构。最终，论文引入了一种因果验证概念-based probes 的方法，并在迷宫求解代理中验证其跟踪奶酪物体的能力，为 RL 的可解释性提供了新工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.00944v1",
      "published_date": "2024-12-01 19:32:04 UTC",
      "updated_date": "2024-12-01 19:32:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:08.289648"
    },
    {
      "arxiv_id": "2412.07794v1",
      "title": "Automatic answering of scientific questions using the FACTS-V1 framework: New methods in research to increase efficiency through the use of AI",
      "title_zh": "翻译失败",
      "authors": [
        "Stefan Pietrusky"
      ],
      "abstract": "The use of artificial intelligence (AI) offers various possibilities to\nexpand and support educational research. Specifically, the implementation of AI\ncan be used to develop new frameworks to establish new research tools that\naccelerate and meaningfully expand the efficiency of data evaluation and\ninterpretation (Buckingham Shum et al., 2023). This article presents the\nprototype of the FACTS-V1 (Filtering and Analysis of Content in Textual\nSources) framework. With the help of the application, numerous scientific\npapers can be automatically extracted, analyzed and interpreted from open\naccess document servers without having to rely on proprietary applications and\ntheir limitations. The FACTS-V1 prototype consists of three building blocks.\nThe first part deals with the extraction of texts, the second with filtering\nand interpretation, and the last with the actual statistical evaluation (topic\nmodeling) using an interactive overview. The aim of the framework is to provide\nrecommendations for future scientific questions based on existing data. The\nfunctionality is illustrated by asking how the use of AI will change the\neducation sector. The data used to answer the question comes from 82 scientific\npapers on the topic of AI from 2024. The papers are publicly available on the\npeDOCS document server of the Leibniz Institute for Educational Research and\nEducational Information.",
      "tldr_zh": "本研究介绍了 FACTS-V1 框架，这是一个利用 AI 自动提取、分析和解释科学论文的工具，旨在提高教育研究的效率和数据处理能力。\n框架由三个构建块组成：文本提取、过滤和解释，以及统计评估（包括 topic modeling），允许从开放访问服务器获取并处理论文，而不依赖专有应用。\n通过分析 82 篇 2024 年关于 AI 在教育领域的论文，FACTS-V1 展示了如何基于现有数据生成未来科学问题的推荐，从而加速研究过程。",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.07794v1",
      "published_date": "2024-12-01 18:55:39 UTC",
      "updated_date": "2024-12-01 18:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:10.402440"
    },
    {
      "arxiv_id": "2412.00928v1",
      "title": "A Deep Generative Model for the Design of Synthesizable Ionizable Lipids",
      "title_zh": "一种深度生成模型用于可合成化离子化脂质的设计",
      "authors": [
        "Yuxuan Ou",
        "Jingyi Zhao",
        "Austin Tripp",
        "Morteza Rasoulianboroujeni",
        "José Miguel Hernández-Lobato"
      ],
      "abstract": "Lipid nanoparticles (LNPs) are vital in modern biomedicine, enabling the\neffective delivery of mRNA for vaccines and therapies by protecting it from\nrapid degradation. Among the components of LNPs, ionizable lipids play a key\nrole in RNA protection and facilitate its delivery into the cytoplasm. However,\ndesigning ionizable lipids is complex. Deep generative models can accelerate\nthis process and explore a larger candidate space compared to traditional\nmethods. Due to the structural differences between lipids and small molecules,\nexisting generative models used for small molecule generation are unsuitable\nfor lipid generation. To address this, we developed a deep generative model\nspecifically tailored for the discovery of ionizable lipids. Our model\ngenerates novel ionizable lipid structures and provides synthesis paths using\nsynthetically accessible building blocks, addressing synthesizability. This\nadvancement holds promise for streamlining the development of lipid-based\ndelivery systems, potentially accelerating the deployment of new therapeutic\nagents, including mRNA vaccines and gene therapies.",
      "tldr_zh": "本文提出了一种深度生成模型，用于设计可合成的离子化脂质，以提升脂质纳米颗粒(LNPs)对 mRNA 的保护和递送效率。模型针对脂质结构的独特特性进行了优化，能够生成新颖的离子化脂质结构，并提供基于可合成构建块的合成路径，从而解决传统方法的局限性。该创新有望加速脂质递送系统的开发，推动 mRNA 疫苗和基因疗法的快速部署。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024 Workshop on AI for New Drug Modalities",
      "pdf_url": "http://arxiv.org/pdf/2412.00928v1",
      "published_date": "2024-12-01 18:33:22 UTC",
      "updated_date": "2024-12-01 18:33:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:12.421253"
    },
    {
      "arxiv_id": "2412.01864v1",
      "title": "Learning Aggregation Rules in Participatory Budgeting: A Data-Driven Approach",
      "title_zh": "参与式预算中的聚合规则学习：一种数据驱动的方法",
      "authors": [
        "Roy Fairstein",
        "Dan Vilenchik",
        "Kobi Gal"
      ],
      "abstract": "Participatory Budgeting (PB) offers a democratic process for communities to\nallocate public funds across various projects through voting. In practice, PB\norganizers face challenges in selecting aggregation rules either because they\nare not familiar with the literature and the exact details of every existing\nrule or because no existing rule echoes their expectations. This paper presents\na novel data-driven approach utilizing machine learning to address this\nchallenge. By training neural networks on PB instances, our approach learns\naggregation rules that balance social welfare, representation, and other\nsocietal beneficial goals. It is able to generalize from small-scale synthetic\nPB examples to large, real-world PB instances. It is able to learn existing\naggregation rules but also generate new rules that adapt to diverse objectives,\nproviding a more nuanced, compromise-driven solution for PB processes. The\neffectiveness of our approach is demonstrated through extensive experiments\nwith synthetic and real-world PB data, and can expand the use and deployment of\nPB solutions.",
      "tldr_zh": "本文提出了一种数据驱动的方法，使用机器学习（machine learning）训练神经网络（neural networks），来学习 Participatory Budgeting (PB) 中的聚合规则，以解决组织者对规则选择的不熟悉或不匹配问题。该方法能平衡社会福利、代表性和其他社会目标，从小型合成 PB 示例泛化到大型真实世界实例，并生成新规则以适应多样化需求。通过合成和真实世界数据的实验验证，该方法证明了其有效性，并有助于扩展 PB 解决方案的应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.01864v1",
      "published_date": "2024-12-01 18:13:27 UTC",
      "updated_date": "2024-12-01 18:13:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:19.873494"
    },
    {
      "arxiv_id": "2412.00887v1",
      "title": "Playable Game Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyu Yang",
        "Junyou Li",
        "Zhongbin Fang",
        "Sheng Chen",
        "Yangbin Yu",
        "Qiang Fu",
        "Wei Yang",
        "Deheng Ye"
      ],
      "abstract": "In recent years, Artificial Intelligence Generated Content (AIGC) has\nadvanced from text-to-image generation to text-to-video and multimodal video\nsynthesis. However, generating playable games presents significant challenges\ndue to the stringent requirements for real-time interaction, high visual\nquality, and accurate simulation of game mechanics. Existing approaches often\nfall short, either lacking real-time capabilities or failing to accurately\nsimulate interactive mechanics. To tackle the playability issue, we propose a\nnovel method called \\emph{PlayGen}, which encompasses game data generation, an\nautoregressive DiT-based diffusion model, and a comprehensive playability-based\nevaluation framework. Validated on well-known 2D and 3D games, PlayGen achieves\nreal-time interaction, ensures sufficient visual quality, and provides accurate\ninteractive mechanics simulation. Notably, these results are sustained even\nafter over 1000 frames of gameplay on an NVIDIA RTX 2060 GPU. Our code is\npublicly available: https://github.com/GreatX3/Playable-Game-Generation. Our\nplayable demo generated by AI is: http://124.156.151.207.",
      "tldr_zh": "该研究探讨了人工智能生成内容（AIGC）从图像到视频的演进，并针对生成可玩游戏的挑战提出了一种新方法PlayGen。该方法包括游戏数据生成、基于DiT的自回归扩散模型，以及一个全面的基于可玩性的评估框架，以实现实时交互、高视觉质量和准确模拟游戏机制。在知名2D和3D游戏上的验证显示，PlayGen在NVIDIA RTX 2060 GPU上支持超过1000帧的稳定游戏性能，并开源了代码和演示链接。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00887v1",
      "published_date": "2024-12-01 16:53:02 UTC",
      "updated_date": "2024-12-01 16:53:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:21.594338"
    },
    {
      "arxiv_id": "2412.00881v1",
      "title": "Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning",
      "title_zh": "学会遗忘：基于元学习的知识图谱嵌入取消学习",
      "authors": [
        "Naixing Xu",
        "Qian Li",
        "Xu Wang",
        "Bingchen Liu",
        "Xin Li"
      ],
      "abstract": "Knowledge graph (KG) embedding methods map entities and relations into\ncontinuous vector spaces, improving performance in tasks like link prediction\nand question answering. With rising privacy concerns, machine unlearning (MU)\nhas emerged as a critical AI technology, enabling models to eliminate the\ninfluence of specific data. Existing MU approaches often rely on data\nobfuscation and adjustments to training loss but lack generalization across\nunlearning tasks. This paper introduces MetaEU, a Meta-Learning-Based Knowledge\nGraph Embedding Unlearning framework. MetaEU leverages meta-learning to unlearn\nspecific embeddings, mitigating their impact while preserving model performance\non remaining data. Experiments on benchmark datasets demonstrate its\neffectiveness in KG embedding unlearning.",
      "tldr_zh": "知识图谱（KG）嵌入方法可以将实体和关系映射到连续向量空间，从而提升链接预测和问答等任务的性能，但隐私担忧促使机器取消学习（MU）成为关键技术，以消除特定数据的影响。现有 MU 方法依赖数据混淆和训练损失调整，但缺乏泛化能力。本文提出 MetaEU，一种基于元学习（meta-learning）的 KG 嵌入取消学习框架，通过元学习技术减轻特定嵌入的影响，同时保持模型在剩余数据上的性能。在基准数据集上的实验证明，MetaEU 在 KG 嵌入取消学习方面表现出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00881v1",
      "published_date": "2024-12-01 16:43:04 UTC",
      "updated_date": "2024-12-01 16:43:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:23.476294"
    },
    {
      "arxiv_id": "2412.00876v4",
      "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxuan Huang",
        "Zijie Zhai",
        "Yunhang Shen",
        "Shaosheng Cao",
        "Fei Zhao",
        "Xiangfeng Xu",
        "Zheyu Ye",
        "Yao Hu",
        "Shaohui Lin"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
      "tldr_zh": "本研究提出 Dynamic-LLaVA，一种高效的多模态大语言模型(MLLMs)，通过动态视觉-语言上下文稀疏化技术，解决 MLLMs 在推断过程中计算和内存开销不断增加的问题。该框架在预填充阶段动态减少视觉上下文冗余，并在解码阶段优化语言上下文，针对预填充、无 KV cache 和有 KV cache 等不同模式设计定制的稀疏化方案。实验结果显示，Dynamic-LLaVA 在预填充阶段可减少约 75% 计算消耗，整个生成过程中无 KV cache 时节省约 50% 计算消耗，并有 KV cache 时降低约 50% GPU 内存开销。同时，该方法在保持 MLLMs 理解和生成能力几乎不变甚至略有提升的情况下，实现高效推断，代码已在 GitHub 上开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
      "pdf_url": "http://arxiv.org/pdf/2412.00876v4",
      "published_date": "2024-12-01 16:32:31 UTC",
      "updated_date": "2025-03-21 13:30:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:26.586119"
    },
    {
      "arxiv_id": "2412.00869v2",
      "title": "KnowledgePrompts: Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting",
      "title_zh": "KnowledgePrompts: 通过知识增强提示探索大型语言模型解决比例类比的能力",
      "authors": [
        "Thilini Wijesiriwardene",
        "Ruwan Wickramarachchi",
        "Sreeram Vennam",
        "Vinija Jain",
        "Aman Chadha",
        "Amitava Das",
        "Ponnurangam Kumaraguru",
        "Amit Sheth"
      ],
      "abstract": "Making analogies is fundamental to cognition. Proportional analogies, which\nconsist of four terms, are often used to assess linguistic and cognitive\nabilities. For instance, completing analogies like \"Oxygen is to Gas as <blank>\nis to <blank>\" requires identifying the semantic relationship (e.g., \"type of\")\nbetween the first pair of terms (\"Oxygen\" and \"Gas\") and finding a second pair\nthat shares the same relationship (e.g., \"Aluminum\" and \"Metal\"). In this work,\nwe introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for\nproportional analogy completion and evaluate the performance of contemporary\nLarge Language Models (LLMs) in various knowledge-enhanced prompt settings.\nSpecifically, we augment prompts with three types of knowledge: exemplar,\nstructured, and targeted. Our results show that despite extensive training\ndata, solving proportional analogies remains challenging for current LLMs, with\nthe best model achieving an accuracy of 55%. Notably, we find that providing\ntargeted knowledge can better assist models in completing proportional\nanalogies compared to providing exemplars or collections of structured\nknowledge. Our code and data are available at:\nhttps://github.com/Thiliniiw/KnowledgePrompts/",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）通过知识增强提示解决比例类比的能力，比例类比如“氧气是气体的一种，铝是______的一种”。研究者创建了一个15K的多选问答（MCQA）数据集，并评估LLMs在三种知识增强提示（exemplar示例、structured结构化知识和targeted针对性知识）下的表现。结果显示，尽管LLMs有丰富训练数据，但比例类比任务仍具挑战性，最佳模型准确率仅为55%。特别地，提供targeted知识比exemplar或structured知识更有效地提升模型性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.00869v2",
      "published_date": "2024-12-01 16:15:14 UTC",
      "updated_date": "2024-12-19 04:38:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:27.452126"
    },
    {
      "arxiv_id": "2412.00864v2",
      "title": "Explicit and data-Efficient Encoding via Gradient Flow",
      "title_zh": "翻译失败",
      "authors": [
        "Kyriakos Flouris",
        "Anna Volokitin",
        "Gustav Bredell",
        "Ender Konukoglu"
      ],
      "abstract": "The autoencoder model typically uses an encoder to map data to a lower\ndimensional latent space and a decoder to reconstruct it. However, relying on\nan encoder for inversion can lead to suboptimal representations, particularly\nlimiting in physical sciences where precision is key. We introduce a\ndecoder-only method using gradient flow to directly encode data into the latent\nspace, defined by ordinary differential equations (ODEs). This approach\neliminates the need for approximate encoder inversion. We train the decoder via\nthe adjoint method and show that costly integrals can be avoided with minimal\naccuracy loss. Additionally, we propose a $2^{nd}$ order ODE variant,\napproximating Nesterov's accelerated gradient descent for faster convergence.\nTo handle stiff ODEs, we use an adaptive solver that prioritizes loss\nminimization, improving robustness. Compared to traditional autoencoders, our\nmethod demonstrates explicit encoding and superior data efficiency, which is\ncrucial for data-scarce scenarios in the physical sciences. Furthermore, this\nwork paves the way for integrating machine learning into scientific workflows,\nwhere precise and efficient encoding is critical. \\footnote{The code for this\nwork is available at \\url{https://github.com/k-flouris/gfe}.}",
      "tldr_zh": "本论文提出了一种基于梯度流(gradient flow)的显式编码方法，以解决传统autoencoder模型依赖encoder导致次优表示的问题，尤其适用于物理科学领域的数据精确性需求。该方法采用decoder-only架构，通过普通微分方程(ODEs)直接将数据编码到latent space，并使用adjoint method训练decoder，以避免昂贵的积分计算，同时引入二阶ODE变体来近似Nesterov's accelerated gradient descent，实现更快收敛。实验结果显示，该方法比传统autoencoder更具数据效率(explicit encoding and superior data efficiency)，在数据稀缺场景中表现出色，并为机器学习在科学工作流中的整合提供了新路径。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.OC",
        "physics.comp-ph"
      ],
      "primary_category": "stat.ML",
      "comment": "Machine Learning and the Physical Sciences Workshop, NeurIPS 2024.\n  arXiv admin note: text overlap with arXiv:2105.05031",
      "pdf_url": "http://arxiv.org/pdf/2412.00864v2",
      "published_date": "2024-12-01 15:54:50 UTC",
      "updated_date": "2025-01-04 05:09:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:29.636245"
    },
    {
      "arxiv_id": "2412.00860v1",
      "title": "Deep evolving semi-supervised anomaly detection",
      "title_zh": "深度演化半监督异常检测",
      "authors": [
        "Jack Belham",
        "Aryan Bhosale",
        "Samrat Mukherjee",
        "Biplab Banerjee",
        "Fabio Cuzzolin"
      ],
      "abstract": "The aim of this paper is to formalise the task of continual semi-supervised\nanomaly detection (CSAD), with the aim of highlighting the importance of such a\nproblem formulation which assumes as close to real-world conditions as\npossible. After an overview of the relevant definitions of continual\nsemi-supervised learning, its components, anomaly detection extension, and the\ntraining protocols; the paper introduces a baseline model of a variational\nautoencoder (VAE) to work with semi-supervised data along with a continual\nlearning method of deep generative replay with outlier rejection. The results\nshow that such a use of extreme value theory (EVT) applied to anomaly detection\ncan provide promising results even in comparison to an upper baseline of joint\ntraining. The results explore the effects of how much labelled and unlabelled\ndata is present, of which class, and where it is located in the data stream.\nOutlier rejection shows promising initial results where it often surpasses a\nbaseline method of Elastic Weight Consolidation (EWC). A baseline for CSAD is\nput forward along with the specific dataset setups used for reproducability and\ntestability for other practitioners. Future research directions include other\nCSAD settings and further research into efficient continual hyperparameter\ntuning.",
      "tldr_zh": "这篇论文正式化了持续半监督异常检测（CSAD）的任务，强调其接近真实世界场景的训练协议，并引入了一个基于变分自编码器（VAE）的基线模型，结合深度生成重放和异常值拒绝方法。实验结果显示，该方法利用极值理论（EVT）在处理标记和未标记数据时表现出色，往往优于弹性权重整合（EWC）的基线，并探讨了数据类型、类别和流位置的影响。作为贡献，论文提供了可重复的 CSAD 基准模型和数据集设置，并提出未来研究方向，如其他 CSAD 设置和高效超参数调整。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00860v1",
      "published_date": "2024-12-01 15:48:37 UTC",
      "updated_date": "2024-12-01 15:48:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:34.271722"
    },
    {
      "arxiv_id": "2412.04498v2",
      "title": "Large Language Models in Politics and Democracy: A Comprehensive Survey",
      "title_zh": "大型语言模型在政治和民主中的全面调查",
      "authors": [
        "Goshi Aoki"
      ],
      "abstract": "The advancement of generative AI, particularly large language models (LLMs),\nhas a significant impact on politics and democracy, offering potential across\nvarious domains, including policymaking, political communication, analysis, and\ngovernance. This paper surveys the recent and potential applications of LLMs in\npolitics, examining both their promises and the associated challenges. This\npaper examines the ways in which LLMs are being employed in legislative\nprocesses, political communication, and political analysis. Moreover, we\ninvestigate the potential of LLMs in diplomatic and national security contexts,\neconomic and social modeling, and legal applications. While LLMs offer\nopportunities to enhance efficiency, inclusivity, and decision-making in\npolitical processes, they also present challenges related to bias,\ntransparency, and accountability. The paper underscores the necessity for\nresponsible development, ethical considerations, and governance frameworks to\nensure that the integration of LLMs into politics aligns with democratic values\nand promotes a more just and equitable society.",
      "tldr_zh": "这篇论文对大型语言模型（LLMs）在政治和民主领域的应用进行了全面调查，探讨其在政策制定、政治沟通、分析以及治理等方面的潜在影响。LLMs 能够提升效率、包容性和决策质量，同时在外交、国家安全、经济建模和法律应用中展现出巨大潜力。然而，论文也指出了相关挑战，包括模型偏见、透明度和责任性问题。最终，强调需要负责任的开发、伦理考虑和治理框架，以确保 LLMs 的整合符合民主价值观并促进公平社会。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.04498v2",
      "published_date": "2024-12-01 15:23:34 UTC",
      "updated_date": "2024-12-16 05:27:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:33.257464"
    },
    {
      "arxiv_id": "2412.00846v1",
      "title": "Improving Multimodal LLMs Ability In Geometry Problem Solving, Reasoning, And Multistep Scoring",
      "title_zh": "提升多模态大型语言模型在几何问题求解、推理和多步评分方面的能力",
      "authors": [
        "Avinash Anand",
        "Raj Jaiswal",
        "Abhishek Dharmadhikari",
        "Atharva Marathe",
        "Harsh Parimal Popat",
        "Harshil Mital",
        "Kritarth Prasad",
        "Rajiv Ratn Shah",
        "Roger Zimmermann"
      ],
      "abstract": "This paper presents GPSM4K, a comprehensive geometry multimodal dataset\ntailored to augment the problem-solving capabilities of Large Vision Language\nModels (LVLMs). GPSM4K encompasses 2157 multimodal question-answer pairs\nmanually extracted from mathematics textbooks spanning grades 7-12 and is\nfurther augmented to 5340 problems, consisting of both numerical and\ntheorem-proving questions. In contrast to PGPS9k, Geometry3K, and Geo170K which\nfeature only objective-type questions, GPSM4K offers detailed step-by-step\nsolutions in a consistent format, facilitating a comprehensive evaluation of\nproblem-solving approaches. This dataset serves as an excellent benchmark for\nassessing the geometric reasoning capabilities of LVLMs. Evaluation of our test\nset shows that there is scope for improvement needed in open-source language\nmodels in geometry problem-solving. Finetuning on our training set increases\nthe geometry problem-solving capabilities of models. Further, We also evaluate\nthe effectiveness of techniques such as image captioning and Retrieval\nAugmentation generation (RAG) on model performance. We leveraged LLM to\nautomate the task of final answer evaluation by providing ground truth and\npredicted solutions. This research will help to assess and improve the\ngeometric reasoning capabilities of LVLMs.",
      "tldr_zh": "本研究引入了GPSM4K数据集，这是一个包含5340个多模态问题对的综合几何数据集，旨在提升Large Vision Language Models (LVLMs)在几何问题解决、推理和多步评分方面的能力。相比于PGPS9k、Geometry3K和Geo170K等数据集，GPSM4K提供了详细的步步解法，支持主观和客观问题，从而更全面评估LVLMs的几何推理表现。实验结果显示，开源模型在几何问题上仍有改进空间，通过在训练集上微调，模型性能显著提升；此外，技术如图像描述和Retrieval Augmentation Generation (RAG)进一步提高了模型效果。该工作有助于评估和优化LVLMs的几何推理能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.00846v1",
      "published_date": "2024-12-01 15:19:23 UTC",
      "updated_date": "2024-12-01 15:19:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:36.481868"
    },
    {
      "arxiv_id": "2412.00833v1",
      "title": "AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Li",
        "Yifei Xing",
        "Xiangyuan Lan",
        "Xin Li",
        "Haifeng Chen",
        "Dongmei Jiang"
      ],
      "abstract": "Cross-modal alignment is crucial for multimodal representation fusion due to\nthe inherent heterogeneity between modalities. While Transformer-based methods\nhave shown promising results in modeling inter-modal relationships, their\nquadratic computational complexity limits their applicability to long-sequence\nor large-scale data. Although recent Mamba-based approaches achieve linear\ncomplexity, their sequential scanning mechanism poses fundamental challenges in\ncomprehensively modeling cross-modal relationships. To address this limitation,\nwe propose AlignMamba, an efficient and effective method for multimodal fusion.\nSpecifically, grounded in Optimal Transport, we introduce a local cross-modal\nalignment module that explicitly learns token-level correspondences between\ndifferent modalities. Moreover, we propose a global cross-modal alignment loss\nbased on Maximum Mean Discrepancy to implicitly enforce the consistency between\ndifferent modal distributions. Finally, the unimodal representations after\nlocal and global alignment are passed to the Mamba backbone for further\ncross-modal interaction and multimodal fusion. Extensive experiments on\ncomplete and incomplete multimodal fusion tasks demonstrate the effectiveness\nand efficiency of the proposed method.",
      "tldr_zh": "这篇论文提出了 AlignMamba，一种高效的多模态融合方法，用于增强 Multimodal Mamba，通过局部和全局跨模态对齐解决模态异质性问题。具体而言，它引入基于 Optimal Transport 的局部跨模态对齐模块来学习不同模态间的 token-level 对应关系，并使用 Maximum Mean Discrepancy 作为全局对齐损失来强制模态分布一致性。接着，将对齐后的单模态表示传入 Mamba backbone 进行进一步交互和融合。实验在完整和不完整多模态任务上证明了 AlignMamba 的有效性和效率，展示了其在计算复杂度上的优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00833v1",
      "published_date": "2024-12-01 14:47:41 UTC",
      "updated_date": "2024-12-01 14:47:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:38.353479"
    },
    {
      "arxiv_id": "2412.00830v1",
      "title": "SPILDL: A Scalable and Parallel Inductive Learner in Description Logic",
      "title_zh": "翻译失败",
      "authors": [
        "Eyad Algahtani"
      ],
      "abstract": "We present SPILDL, a Scalable and Parallel Inductive Learner in Description\nLogic (DL). SPILDL is based on the DL-Learner (the state of the art in DL-based\nILP learning). As a DL-based ILP learner, SPILDL targets the\n$\\mathcal{ALCQI}^{\\mathcal{(D)}}$ DL language, and can learn DL hypotheses\nexpressed as disjunctions of conjunctions (using the $\\sqcup$ operator).\nMoreover, SPILDL's hypothesis language also incorporates the use of string\nconcrete roles (also known as string data properties in the Web Ontology\nLanguage, OWL); As a result, this incorporation of powerful DL constructs,\nenables SPILDL to learn powerful DL-based hypotheses for describing many\nreal-world complex concepts. SPILDL employs a hybrid parallel approach which\ncombines both shared-memory and distributed-memory approaches, to accelerates\nILP learning (for both hypothesis search and evaluation). According to\nexperimental results, SPILDL's parallel search improved performance by up to\n$\\sim$27.3 folds (best case). For hypothesis evaluation, SPILDL improved\nevaluation performance through HT-HEDL (our multi-core CPU + multi-GPU\nhypothesis evaluation engine), by up to 38 folds (best case). By combining both\nparallel search and evaluation, SPILDL improved performance by up to $\\sim$560\nfolds (best case). In terms of worst case scenario, SPILDL's parallel search\ndoesn't provide consistent speedups on all datasets, and is highly dependent on\nthe search space nature of the ILP dataset. For some datasets, increasing the\nnumber of parallel search threads result in reduced performance, similar or\nworse than baseline. Some ILP datasets benefit from parallel search, while\nothers don't (or the performance gains are negligible). In terms of parallel\nevaluation, on small datasets, parallel evaluation provide similar or worse\nperformance than baseline.",
      "tldr_zh": "本论文提出 SPILDL，一种基于 Description Logic (DL) 的可扩展并行归纳学习器，旨在提升 DL-Learner 的性能，支持 $\\mathcal{ALCQI}^{\\mathcal{(D)}}$ 语言并处理 string concrete roles，从而学习复杂概念的 DL 假设。SPILDL 采用混合并行方法（结合 shared-memory 和 distributed-memory 技术），加速假设搜索和评估过程，其中并行搜索可提升高达 ~27.3 倍性能，而假设评估通过 HT-HEDL 引擎提升高达 38 倍，整体性能最高达 ~560 倍。实验表明，这种并行策略在某些数据集上表现不一致，对于小数据集或特定搜索空间，可能导致性能下降或无显著收益。",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.DS",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00830v1",
      "published_date": "2024-12-01 14:33:37 UTC",
      "updated_date": "2024-12-01 14:33:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:47.104939"
    },
    {
      "arxiv_id": "2412.00821v1",
      "title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Raj Jaiswal",
        "Dhruv Jain",
        "Harsh Parimal Popat",
        "Avinash Anand",
        "Abhishek Dharmadhikari",
        "Atharva Marathe",
        "Rajiv Ratn Shah"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in various\nreasoning tasks. However, they encounter significant challenges when it comes\nto scientific reasoning, particularly in physics, which requires not only\nmathematical reasoning but also factual and conceptual understanding. When\naddressing complex physics problems, LLMs typically face three key issues:\nproblem miscomprehension, incorrect concept application, and computational\nerrors. While each of these problems can be addressed individually, there is a\nneed for a generalized approach that can tackle all three issues\nsimultaneously. To address this, we introduce Mixture of Refinement Agents\n(MoRA), a novel agentic refinement framework that iteratively refines the LLM\ngenerated base solution by correcting the aforementioned errors, resulting in a\nsignificant performance improvement for open-source LLMs. Our approach aims to\nbridge the gap between opensource LLMs and GPT-4o by utilizing the latter as\nerror identifier to guide these refinement agents. We evaluate our approach on\nthe SciEval and MMLU subsets along with our own physics dataset (PhysicsQA).\nMoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on\nthese datasets, achieving up to a 16% increase in final answer accuracy.",
      "tldr_zh": "大型语言模型（LLMs）在物理推理任务中常面临问题误解、概念应用错误和计算错误等挑战，本文引入了Mixture of Refinement Agents (MoRA)，一个创新的代理精炼框架，通过迭代纠正这些错误来显著提升LLMs的表现。MoRA利用GPT-4o作为错误识别器，指导代理对基础解决方案进行优化，以弥合开源LLMs与GPT-4o之间的差距。在SciEval、MMLU和PhysicsQA数据集上的实验显示，MoRA使Llama-3-70B和Gemma-2-27B的最终答案准确率提高多达16%。这为开源LLMs在科学推理领域的改进提供了通用方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.00821v1",
      "published_date": "2024-12-01 14:15:55 UTC",
      "updated_date": "2024-12-01 14:15:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:47.324200"
    },
    {
      "arxiv_id": "2412.00810v1",
      "title": "Long text outline generation: Chinese text outline based on unsupervised framework and large language mode",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Yan",
        "Yuanchi Ma"
      ],
      "abstract": "Outline generation aims to reveal the internal structure of a document by\nidentifying underlying chapter relationships and generating corresponding\nchapter summaries. Although existing deep learning methods and large models\nperform well on small- and medium-sized texts, they struggle to produce\nreadable outlines for very long texts (such as fictional works), often failing\nto segment chapters coherently. In this paper, we propose a novel outline\ngeneration method for Chinese, combining an unsupervised framework with large\nmodels. Specifically, the method first generates chapter feature graph data\nbased on entity and syntactic dependency relationships. Then, a representation\nmodule based on graph attention layers learns deep embeddings of the chapter\ngraph data. Using these chapter embeddings, we design an operator based on\nMarkov chain principles to segment plot boundaries. Finally, we employ a large\nmodel to generate summaries of each plot segment and produce the overall\noutline. We evaluate our model based on segmentation accuracy and outline\nreadability, and our performance outperforms several deep learning models and\nlarge models in comparative evaluations.",
      "tldr_zh": "该论文提出了一种针对中文长文本（如小说）的自动大纲生成方法，结合无监督 framework 和 large language model，以解决现有模型在长文本章节分割和连贯性上的不足。方法首先基于实体和句法依赖关系生成章节特征图数据，然后使用 graph attention layers 学习深度嵌入，并通过 Markov chain 原理设计操作符来分割情节边界，最后由 large language model 生成各段摘要和整体大纲。实验评估显示，该方法在分割准确性和大纲可读性方面优于多种深度学习模型和大型模型。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00810v1",
      "published_date": "2024-12-01 13:46:15 UTC",
      "updated_date": "2024-12-01 13:46:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:49.966142"
    },
    {
      "arxiv_id": "2412.00807v1",
      "title": "Generative Model for Synthesizing Ionizable Lipids: A Monte Carlo Tree Search Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Jingyi Zhao",
        "Yuxuan Ou",
        "Austin Tripp",
        "Morteza Rasoulianboroujeni",
        "José Miguel Hernández-Lobato"
      ],
      "abstract": "Ionizable lipids are essential in developing lipid nanoparticles (LNPs) for\neffective messenger RNA (mRNA) delivery. While traditional methods for\ndesigning new ionizable lipids are typically time-consuming, deep generative\nmodels have emerged as a powerful solution, significantly accelerating the\nmolecular discovery process. However, a practical challenge arises as the\nmolecular structures generated can often be difficult or infeasible to\nsynthesize. This project explores Monte Carlo tree search (MCTS)-based\ngenerative models for synthesizable ionizable lipids. Leveraging a\nsynthetically accessible lipid building block dataset and two specialized\npredictors to guide the search through chemical space, we introduce a policy\nnetwork guided MCTS generative model capable of producing new ionizable lipids\nwith available synthesis pathways.",
      "tldr_zh": "该研究针对离子化脂质（Ionizable lipids）的设计问题，指出传统方法耗时长且深度生成模型常产生难以合成的分子结构。研究者提出了一种基于 Monte Carlo Tree Search (MCTS) 的生成模型，利用合成可访问的脂质构建块数据集和两个专门预测器来引导化学空间搜索，并引入 policy network 进行优化，以生成具有可用合成路径的新型离子化脂质。实验结果表明，该方法能有效加速脂质纳米颗粒（LNPs）用于 messenger RNA (mRNA) 递送的分子发现过程，提供更可靠的合成方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00807v1",
      "published_date": "2024-12-01 13:34:22 UTC",
      "updated_date": "2024-12-01 13:34:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:52.191043"
    },
    {
      "arxiv_id": "2412.00802v1",
      "title": "HT-HEDL: High-Throughput Hypothesis Evaluation in Description Logic",
      "title_zh": "HT-HEDL：描述逻辑中的高吞吐量假设评估",
      "authors": [
        "Eyad Algahtani"
      ],
      "abstract": "We present High-Throughput Hypothesis Evaluation in Description Logic\n(HT-HEDL). HT-HEDL is a high-performance hypothesis evaluation engine that\naccelerates hypothesis evaluation computations for inductive logic programming\n(ILP) learners using description logic (DL) for their knowledge representation;\nin particular, HT-HEDL targets accelerating computations for the\n$\\mathcal{ALCQI}^{\\mathcal{(D)}}$ DL language. HT-HEDL aggregates the computing\npower of multi-core CPUs with multi-GPUs to improve hypothesis computations at\ntwo levels: 1) the evaluation of a single hypothesis and 2) the evaluation of\nmultiple hypotheses (i.e., batch of hypotheses). In the first level, HT-HEDL\nuses a single GPU or a vectorized multi-threaded CPU to evaluate a single\nhypothesis. In vectorized multi-threaded CPU evaluation, classical (scalar) CPU\nmulti-threading is combined with CPU's extended vector instructions set to\nextract more CPU-based performance. The experimental results revealed that\nHT-HEDL increased performance using CPU-based evaluation (on a single\nhypothesis): from 20.4 folds using classical multi-threading to $\\sim85$ folds\nusing vectorized multi-threading. In the GPU-based evaluation, HT-HEDL achieved\nspeedups of up to $\\sim38$ folds for single hypothesis evaluation using a\nsingle GPU. To accelerate the evaluation of multiple hypotheses, HT-HEDL\ncombines, in parallel, GPUs with multi-core CPUs to increase evaluation\nthroughput (number of evaluated hypotheses per second). The experimental\nresults revealed that HT-HEDL increased evaluation throughput by up to 29.3\nfolds using two GPUs and up to $\\sim44$ folds using two GPUs combined with a\nCPU's vectorized multi-threaded evaluation.",
      "tldr_zh": "该论文提出了一种高吞吐量假设评估引擎 HT-HEDL，用于加速描述逻辑（Description Logic）中归纳逻辑编程（ILP）学习者的假设评估计算，特别针对 $\\mathcal{ALCQI}^{\\mathcal{(D)}}$ 语言。HT-HEDL 通过整合多核 CPU 和多 GPU 的计算能力，在单个假设评估层面实现了性能提升，包括使用向量化多线程 CPU（从 20.4 倍加速到约 85 倍）和单个 GPU（高达约 38 倍加速）。在批量假设评估层面，HT-HEDL 采用 GPU 与 CPU 的并行组合，进一步提升评估吞吐量，使用两个 GPU 可达 29.3 倍加速，而结合 CPU 的向量化多线程则可达约 44 倍。总的来说，该框架为高效的知识表示和假设计算提供了重要基础。",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.DS",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00802v1",
      "published_date": "2024-12-01 13:01:48 UTC",
      "updated_date": "2024-12-01 13:01:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:54.336985"
    },
    {
      "arxiv_id": "2412.00800v2",
      "title": "A Comprehensive Guide to Explainable AI: From Classical Models to LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Weiche Hsieh",
        "Ziqian Bi",
        "Chuanqi Jiang",
        "Junyu Liu",
        "Benji Peng",
        "Sen Zhang",
        "Xuanhe Pan",
        "Jiawei Xu",
        "Jinlang Wang",
        "Keyu Chen",
        "Pohsun Feng",
        "Yizhu Wen",
        "Xinyuan Song",
        "Tianyang Wang",
        "Ming Liu",
        "Junjie Yang",
        "Ming Li",
        "Bowen Jing",
        "Jintao Ren",
        "Junhao Song",
        "Hong-Ming Tseng",
        "Yichao Zhang",
        "Lawrence K. Q. Yan",
        "Qian Niu",
        "Silin Chen",
        "Yunze Wang",
        "Chia Xin Liang"
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) addresses the growing need for\ntransparency and interpretability in AI systems, enabling trust and\naccountability in decision-making processes. This book offers a comprehensive\nguide to XAI, bridging foundational concepts with advanced methodologies. It\nexplores interpretability in traditional models such as Decision Trees, Linear\nRegression, and Support Vector Machines, alongside the challenges of explaining\ndeep learning architectures like CNNs, RNNs, and Large Language Models (LLMs),\nincluding BERT, GPT, and T5. The book presents practical techniques such as\nSHAP, LIME, Grad-CAM, counterfactual explanations, and causal inference,\nsupported by Python code examples for real-world applications.\n  Case studies illustrate XAI's role in healthcare, finance, and policymaking,\ndemonstrating its impact on fairness and decision support. The book also covers\nevaluation metrics for explanation quality, an overview of cutting-edge XAI\ntools and frameworks, and emerging research directions, such as\ninterpretability in federated learning and ethical AI considerations. Designed\nfor a broad audience, this resource equips readers with the theoretical\ninsights and practical skills needed to master XAI. Hands-on examples and\nadditional resources are available at the companion GitHub repository:\nhttps://github.com/Echoslayer/XAI_From_Classical_Models_to_LLMs.",
      "tldr_zh": "这本书提供了Explainable AI (XAI)的全面指南，旨在提升AI系统的透明性和可解释性，从而增强决策过程的信任和责任感。它涵盖了从传统模型（如Decision Trees、Linear Regression和Support Vector Machines）到深度学习架构（如CNNs、RNNs和LLMs，包括BERT、GPT和T5）的解释性方法，并介绍了实用技术如SHAP、LIME、Grad-CAM、counterfactual explanations和causal inference，并附带Python代码示例和真实案例研究。案例研究展示了XAI在医疗、金融和政策制定中的应用，强调其对公平性和决策支持的作用。书中还探讨了解释质量的评估指标、XAI工具框架以及新兴研究方向，如federated learning中的可解释性和ethical AI考虑，并提供GitHub仓库（https://github.com/Echoslayer/XAI_From_Classical_Models_to_LLMs）作为额外资源。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00800v2",
      "published_date": "2024-12-01 13:01:01 UTC",
      "updated_date": "2024-12-08 06:24:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:57.244026"
    },
    {
      "arxiv_id": "2412.00789v2",
      "title": "A Cognac shot to forget bad memories: Corrective Unlearning in GNNs",
      "title_zh": "翻译失败",
      "authors": [
        "Varshita Kolipaka",
        "Akshit Sinha",
        "Debangan Mishra",
        "Sumit Kumar",
        "Arvindh Arun",
        "Shashwat Goel",
        "Ponnurangam Kumaraguru"
      ],
      "abstract": "Graph Neural Networks (GNNs) are increasingly being used for a variety of ML\napplications on graph data. Because graph data does not follow the\nindependently and identically distributed (i.i.d.) assumption, adversarial\nmanipulations or incorrect data can propagate to other data points through\nmessage passing, which deteriorates the model's performance. To allow model\ndevelopers to remove the adverse effects of manipulated entities from a trained\nGNN, we study the recently formulated problem of Corrective Unlearning. We find\nthat current graph unlearning methods fail to unlearn the effect of\nmanipulations even when the whole manipulated set is known. We introduce a new\ngraph unlearning method, Cognac, which can unlearn the effect of the\nmanipulation set even when only 5% of it is identified. It recovers most of the\nperformance of a strong oracle with fully corrected training data, even beating\nretraining from scratch without the deletion set while being 8x more efficient.\nWe hope our work assists GNN developers in mitigating harmful effects caused by\nissues in real-world data post-training. Our code is publicly available at\nhttps://github.com/varshitakolipaka/corrective-unlearning-for-gnns",
      "tldr_zh": "该论文探讨了 Graph Neural Networks (GNNs) 在处理非独立同分布 (i.i.d.) 图数据时的问题，特别是负面数据通过 message passing 传播导致的性能下降，并研究了 Corrective Unlearning 的解决方案。作者提出了一种新方法 Cognac，能够仅通过识别 5% 的操纵集就有效移除负面影响，从而恢复模型性能，甚至优于从零重新训练，且效率提高 8 倍。实验结果表明，Cognac 为 GNN 开发者在训练后缓解真实世界数据问题提供了实用工具，代码已公开。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00789v2",
      "published_date": "2024-12-01 12:23:25 UTC",
      "updated_date": "2024-12-09 15:14:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:47:57.974176"
    },
    {
      "arxiv_id": "2412.00787v1",
      "title": "TSUBF-Net: Trans-Spatial UNet-like Network with Bi-direction Fusion for Segmentation of Adenoid Hypertrophy in CT",
      "title_zh": "翻译失败",
      "authors": [
        "Rulin Zhou",
        "Yingjie Feng",
        "Guankun Wang",
        "Xiaopin Zhong",
        "Zongze Wu",
        "Qiang Wu",
        "Xi Zhang"
      ],
      "abstract": "Adenoid hypertrophy stands as a common cause of obstructive sleep\napnea-hypopnea syndrome in children. It is characterized by snoring, nasal\ncongestion, and growth disorders. Computed Tomography (CT) emerges as a pivotal\nmedical imaging modality, utilizing X-rays and advanced computational\ntechniques to generate detailed cross-sectional images. Within the realm of\npediatric airway assessments, CT imaging provides an insightful perspective on\nthe shape and volume of enlarged adenoids. Despite the advances of deep\nlearning methods for medical imaging analysis, there remains an emptiness in\nthe segmentation of adenoid hypertrophy in CT scans. To address this research\ngap, we introduce TSUBF-Nett (Trans-Spatial UNet-like Network based on\nBi-direction Fusion), a 3D medical image segmentation framework. TSUBF-Net is\nengineered to effectively discern intricate 3D spatial interlayer features in\nCT scans and enhance the extraction of boundary-blurring features. Notably, we\npropose two innovative modules within the U-shaped network architecture:the\nTrans-Spatial Perception module (TSP) and the Bi-directional Sampling\nCollaborated Fusion module (BSCF).These two modules are in charge of operating\nduring the sampling process and strategically fusing down-sampled and\nup-sampled features, respectively. Furthermore, we introduce the Sobel loss\nterm, which optimizes the smoothness of the segmentation results and enhances\nmodel accuracy. Extensive 3D segmentation experiments are conducted on several\ndatasets. TSUBF-Net is superior to the state-of-the-art methods with the lowest\nHD95: 7.03, IoU:85.63, and DSC: 92.26 on our own AHSD dataset. The results in\nthe other two public datasets also demonstrate that our methods can robustly\nand effectively address the challenges of 3D segmentation in CT scans.",
      "tldr_zh": "该研究针对儿童腺样体肥大（adenoid hypertrophy）导致的睡眠呼吸障碍问题，提出了一种基于 CT 扫描的 3D 医疗图像分割框架 TSUBF-Net。TSUBF-Net 采用 UNet-like 架构，引入 Trans-Spatial Perception (TSP) 模块和 Bi-directional Sampling Collaborated Fusion (BSCF) 模块，以增强 3D 空间特征提取和边界模糊特征融合，同时使用 Sobel loss 优化分割结果的平滑度和准确性。在实验中，TSUBF-Net 在 AHSD 数据集上实现了 HD95: 7.03、IoU: 85.63 和 DSC: 92.26 的优异性能，优于现有方法，并在其他公共数据集上显示出鲁棒性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00787v1",
      "published_date": "2024-12-01 12:21:23 UTC",
      "updated_date": "2024-12-01 12:21:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:00.000681"
    },
    {
      "arxiv_id": "2412.00777v2",
      "title": "Local vs. Global: Local Land-Use and Land-Cover Models Deliver Higher Quality Maps",
      "title_zh": "本地 vs. 全局：本地土地利用和土地覆盖模型提供更高质量地图",
      "authors": [
        "Girmaw Abebe Tadesse",
        "Caleb Robinson",
        "Charles Mwangi",
        "Esther Maina",
        "Joshua Nyakundi",
        "Luana Marotti",
        "Gilles Quentin Hacheme",
        "Hamed Alemohammad",
        "Rahul Dodhia",
        "Juan M. Lavista Ferres"
      ],
      "abstract": "In 2023, 58.0% of the African population experienced moderate to severe food\ninsecurity, with 21.6% facing severe food insecurity. Land-use and land-cover\nmaps provide crucial insights for addressing food insecurity by improving\nagricultural efforts, including mapping and monitoring crop types and\nestimating yield. The development of global land-cover maps has been\nfacilitated by the increasing availability of earth observation data and\nadvancements in geospatial machine learning. However, these global maps exhibit\nlower accuracy and inconsistencies in Africa, partly due to the lack of\nrepresentative training data. To address this issue, we propose a data-centric\nframework with a teacher-student model setup, which uses diverse data sources\nof satellite images and label examples to produce local land-cover maps. Our\nmethod trains a high-resolution teacher model on images with a resolution of\n0.331 m/pixel and a low-resolution student model on publicly available images\nwith a resolution of 10 m/pixel. The student model also utilizes the teacher\nmodel's output as its weak label examples through knowledge transfer. We\nevaluated our framework using Murang'a county in Kenya, renowned for its\nagricultural productivity, as a use case. Our local models achieved higher\nquality maps, with improvements of 0.14 in the F1 score and 0.21 in\nIntersection-over-Union, compared to the best global model. Our evaluation also\nrevealed inconsistencies in existing global maps, with a maximum agreement rate\nof 0.30 among themselves. Our work provides valuable guidance to\ndecision-makers for driving informed decisions to enhance food security.",
      "tldr_zh": "本研究比较了本地与全球土地利用和土地覆盖（Land-use and Land-Cover）模型，强调本地模型在非洲食物不安全问题中的优势。研究提出一个数据中心框架，使用教师-学生模型设置：高分辨率教师模型（0.331 m/pixel）训练后，通过知识转移提供弱标签给低分辨率学生模型（10 m/pixel），以生成更精确的本地地图。在肯尼亚Murang'a县的评估中，本地模型比最佳全球模型的F1分数提高了0.14，Intersection-over-Union提高了0.21，并揭示了现有全球地图间一致性低（最大0.30）。这项工作为决策者提供指导，帮助提升农业监测和食物安全。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00777v2",
      "published_date": "2024-12-01 11:48:58 UTC",
      "updated_date": "2024-12-11 15:11:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:01.837690"
    },
    {
      "arxiv_id": "2412.00773v1",
      "title": "DIVD: Deblurring with Improved Video Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyang Long",
        "Yan Wang",
        "Wendong Wang"
      ],
      "abstract": "Video deblurring presents a considerable challenge owing to the complexity of\nblur, which frequently results from a combination of camera shakes, and object\nmotions. In the field of video deblurring, many previous works have primarily\nconcentrated on distortion-based metrics, such as PSNR. However, this approach\noften results in a weak correlation with human perception and yields\nreconstructions that lack realism. Diffusion models and video diffusion models\nhave respectively excelled in the fields of image and video generation,\nparticularly achieving remarkable results in terms of image authenticity and\nrealistic perception. However, due to the computational complexity and\nchallenges inherent in adapting diffusion models, there is still uncertainty\nregarding the potential of video diffusion models in video deblurring tasks. To\nexplore the viability of video diffusion models in the task of video\ndeblurring, we introduce a diffusion model specifically for this purpose. In\nthis field, leveraging highly correlated information between adjacent frames\nand addressing the challenge of temporal misalignment are crucial research\ndirections. To tackle these challenges, many improvements based on the video\ndiffusion model are introduced in this work. As a result, our model outperforms\nexisting models and achieves state-of-the-art results on a range of perceptual\nmetrics. Our model preserves a significant amount of detail in the images while\nmaintaining competitive distortion metrics. Furthermore, to the best of our\nknowledge, this is the first time the diffusion model has been applied in video\ndeblurring to overcome the limitations mentioned above.",
      "tldr_zh": "该论文提出 DIVD，一种改进的视频扩散模型，用于解决视频去模糊问题，该问题通常由相机抖动和物体运动引起。不同于以往专注于 PSNR 等扭曲指标的方法，DIVD 利用相邻帧的相关信息并处理时间不对齐挑战，从而提升重建的真实性和人类感知相关性。实验结果显示，该模型在各种感知指标上达到最先进水平，同时保留大量图像细节并保持竞争力的扭曲指标，这也是首次将 diffusion model 应用于视频去模糊以克服现有限制。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00773v1",
      "published_date": "2024-12-01 11:39:02 UTC",
      "updated_date": "2024-12-01 11:39:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:04.172174"
    },
    {
      "arxiv_id": "2412.00772v1",
      "title": "A Wave is Worth 100 Words: Investigating Cross-Domain Transferability in Time Series",
      "title_zh": "一个波形胜过百言：调查时间序列中的跨域可转移性",
      "authors": [
        "Xiangkai Ma",
        "Xiaobin Hong",
        "Wenzhong Li",
        "Sanglu Lu"
      ],
      "abstract": "Time series analysis is a fundamental data mining task that supervised\ntraining methods based on empirical risk minimization have proven their\neffectiveness on specific tasks and datasets. However, the acquisition of\nwell-annotated data is costly and a large amount of unlabeled series data is\nunder-utilized. Due to distributional shifts across various domains and\ndifferent patterns of interest across multiple tasks. The problem of\ncross-domain multi-task migration of time series remains a significant\nchallenge. To address these problems, this paper proposes a novel cross-domain\npretraining method based on Wave Quantization (termed as WQ4TS), which can be\ncombined with any advanced time series model and applied to multiple downstream\ntasks. Specifically, we transfer the time series data from different domains\ninto a common spectral latent space, and enable the model to learn the temporal\npattern knowledge of different domains directly from the common space and\nutilize it for the inference of downstream tasks, thereby mitigating the\nchallenge of heterogeneous cross-domains migration. The establishment of\nspectral latent space brings at least three benefits, cross-domain migration\ncapability thus adapting to zero- and few-shot scenarios without relying on\npriori knowledge of the dataset, general compatible cross-domain migration\nframework without changing the existing model structure, and robust modeling\ncapability thus achieving SOTA results in multiple downstream tasks. To\ndemonstrate the effectiveness of the proposed approach, we conduct extensive\nexperiments including three important tasks: forecasting, imputation, and\nclassification. And three common real-world data scenarios are simulated:\nfull-data, few-shot, and zero-shot. The proposed WQ4TS achieves the best\nperformance on 87.5% of all tasks, and the average improvement of the metrics\non all the tasks is up to 34.7%.",
      "tldr_zh": "这篇论文探讨了时间序列（Time Series）分析中跨域转移的挑战，特别是在数据分布偏移和多任务模式差异下，现有方法难以利用未标注数据的问题。作者提出了一种新型预训练方法 Wave Quantization for Time Series（WQ4TS），通过将不同领域的序列数据映射到共同的谱隐空间中，模型能够直接学习时序模式知识，从而实现高效的跨域迁移，支持零样本和少样本场景。实验结果显示，WQ4TS 与现有模型结合后，在预测、插值和分类等下游任务上取得了87.5%的最佳性能，平均指标改善高达34.7%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00772v1",
      "published_date": "2024-12-01 11:35:06 UTC",
      "updated_date": "2024-12-01 11:35:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:12.620240"
    },
    {
      "arxiv_id": "2412.00765v1",
      "title": "SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts",
      "title_zh": "翻译失败",
      "authors": [
        "Aihua Pei",
        "Zehua Yang",
        "Shunan Zhu",
        "Ruoxi Cheng",
        "Ju Jia"
      ],
      "abstract": "Traditional methods for evaluating the robustness of large language models\n(LLMs) often rely on standardized benchmarks, which can escalate costs and\nlimit evaluations across varied domains. This paper introduces a novel\nframework designed to autonomously evaluate the robustness of LLMs by\nincorporating refined adversarial prompts and domain-constrained knowledge\nguidelines in the form of knowledge graphs. Our method systematically generates\ndescriptive sentences from domain-constrained knowledge graph triplets to\nformulate adversarial prompts, enhancing the relevance and challenge of the\nevaluation. These prompts, generated by the LLM itself and tailored to evaluate\nits own robustness, undergo a rigorous filtering and refinement process,\nensuring that only those with high textual fluency and semantic fidelity are\nused. This self-evaluation mechanism allows the LLM to evaluate its robustness\nwithout the need for external benchmarks. We assess the effectiveness of our\nframework through extensive testing on both proprietary models like ChatGPT and\nopen-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that\nour approach not only reduces dependency on conventional data but also provides\na targeted and efficient means of evaluating LLM robustness in constrained\ndomains.",
      "tldr_zh": "本论文提出SelfPrompt框架，用于自主评估大型语言模型(LLMs)的鲁棒性，通过领域约束知识指导和精炼对抗提示(adversarial prompts)来减少对标准化基准的依赖。框架从知识图谱(knowledge graphs)三元组生成描述性句子，形成相关且具有挑战性的对抗提示，这些提示由LLMs自身生成并经过严格过滤以确保文本流畅性和语义保真。实验结果显示，在ChatGPT和开源模型如Llama-3.1、Phi-3、Mistral上的测试中，SelfPrompt提供了高效的领域特定评估，显著降低了传统数据依赖并提升了鲁棒性评估的针对性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00765v1",
      "published_date": "2024-12-01 10:58:53 UTC",
      "updated_date": "2024-12-01 10:58:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:15.206803"
    },
    {
      "arxiv_id": "2412.00763v1",
      "title": "PGSO: Prompt-based Generative Sequence Optimization Network for Aspect-based Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Dong",
        "Wei Wei"
      ],
      "abstract": "Recently, generative pre-training based models have demonstrated remarkable\nresults on Aspect-based Sentiment Analysis (ABSA) task. However, previous works\noveremphasize crafting various templates to paraphrase training targets for\nenhanced decoding, ignoring the internal optimizations on generative models.\nDespite notable results achieved by these target-oriented optimization methods,\nthey struggle with the complicated long texts since the implicit long-distance\nrelation, e.g., aspect-opinion relation, is difficult to extract under the\nposition embedding mechanism in generative models. Thus, in this paper, we\nfirst clarify the causes of the problem and introduce two sequence optimization\nstrategies: the rule-based static optimization and the score-based dynamic\noptimization. The rule-based approach relies on handcraft priority of\ndependency relation to reorder the context, while the score-based algorithm\ndynamically regulates the contextual sequence by calculating word position\nscores using neural network. Based on the dynamic optimization structure, we\nfurther propose a unified Prompt-based Generative Sequence Optimization network\n(named PGSO), which jointly optimizes the training target as well as the\ngenerative model. Specifically, PGSO contains two components, namely, prompt\nconstruction and sequence regulator. The former constructs a task-specific\nprompt based on unsupervised training objects to fully utilize the pre-trained\nmodel. The latter jointly leverages semantic, syntactic and original-sequence\ninformation to dynamically regulate contextual sequence. Our experiments\nconducted on four ABSA tasks across multiple benchmarks indicate that PGSO\noutperforms state-of-the-art methods, with an average improvement of 3.52% in\nF1 score.",
      "tldr_zh": "本研究针对Aspect-based Sentiment Analysis (ABSA)任务，提出PGSO（Prompt-based Generative Sequence Optimization Network），以解决生成式预训练模型在处理复杂长文本时的长距离关系提取问题，如方面-意见关系。PGSO引入两种序列优化策略：基于规则的静态优化（依赖手工排序上下文）和基于得分的动态优化（通过神经网络计算词位置得分动态调节序列），并结合提示构造组件来充分利用预训练模型。实验结果显示，在四个ABSA任务的多个基准上，PGSO比最先进方法平均F1分数提高了3.52%，证明了其在优化生成模型方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00763v1",
      "published_date": "2024-12-01 10:49:55 UTC",
      "updated_date": "2024-12-01 10:49:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:15.834336"
    },
    {
      "arxiv_id": "2412.00761v1",
      "title": "Learning to Forget using Hypernetworks",
      "title_zh": "翻译失败",
      "authors": [
        "Jose Miguel Lara Rangel",
        "Stefan Schoepf",
        "Jack Foster",
        "David Krueger",
        "Usman Anwar"
      ],
      "abstract": "Machine unlearning is gaining increasing attention as a way to remove\nadversarial data poisoning attacks from already trained models and to comply\nwith privacy and AI regulations. The objective is to unlearn the effect of\nundesired data from a trained model while maintaining performance on the\nremaining data. This paper introduces HyperForget, a novel machine unlearning\nframework that leverages hypernetworks - neural networks that generate\nparameters for other networks - to dynamically sample models that lack\nknowledge of targeted data while preserving essential capabilities. Leveraging\ndiffusion models, we implement two Diffusion HyperForget Networks and used them\nto sample unlearned models in Proof-of-Concept experiments. The unlearned\nmodels obtained zero accuracy on the forget set, while preserving good accuracy\non the retain sets, highlighting the potential of HyperForget for dynamic\ntargeted data removal and a promising direction for developing adaptive machine\nunlearning algorithms.",
      "tldr_zh": "本论文提出HyperForget，一种新型机器unlearning框架，利用hypernetworks（超网络）动态采样模型，以移除训练模型中特定数据的知识，同时保持在剩余数据上的性能。该框架结合diffusion models（扩散模型）实现两个Diffusion HyperForget Networks，并在概念证明实验中测试。结果显示，采样得到的未学习模型在遗忘集上达到零准确率，而在保留集上保持良好准确率。这为动态针对性数据移除和开发自适应machine unlearning算法提供了有前景的方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "AdvML-Frontiers'24: The 3rd Workshop on New Frontiers in Adversarial\n  Machine Learning@NeurIPS'24, Vancouver, CA",
      "pdf_url": "http://arxiv.org/pdf/2412.00761v1",
      "published_date": "2024-12-01 10:43:11 UTC",
      "updated_date": "2024-12-01 10:43:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:17.051643"
    },
    {
      "arxiv_id": "2412.00760v1",
      "title": "Automating Feedback Analysis in Surgical Training: Detection, Categorization, and Assessment",
      "title_zh": "手术培训中的反馈分析自动化：检测、分类和评估",
      "authors": [
        "Firdavs Nasriddinov",
        "Rafal Kocielnik",
        "Arushi Gupta",
        "Cherine Yang",
        "Elyssa Wong",
        "Anima Anandkumar",
        "Andrew Hung"
      ],
      "abstract": "This work introduces the first framework for reconstructing surgical dialogue\nfrom unstructured real-world recordings, which is crucial for characterizing\nteaching tasks. In surgical training, the formative verbal feedback that\ntrainers provide to trainees during live surgeries is crucial for ensuring\nsafety, correcting behavior immediately, and facilitating long-term skill\nacquisition. However, analyzing and quantifying this feedback is challenging\ndue to its unstructured and specialized nature. Automated systems are essential\nto manage these complexities at scale, allowing for the creation of structured\ndatasets that enhance feedback analysis and improve surgical education. Our\nframework integrates voice activity detection, speaker diarization, and\nautomated speech recaognition, with a novel enhancement that 1) removes\nhallucinations (non-existent utterances generated during speech recognition\nfueled by noise in the operating room) and 2) separates speech from trainers\nand trainees using few-shot voice samples. These aspects are vital for\nreconstructing accurate surgical dialogues and understanding the roles of\noperating room participants. Using data from 33 real-world surgeries, we\ndemonstrated the system's capability to reconstruct surgical teaching dialogues\nand detect feedback instances effectively (F1 score of 0.79+/-0.07). Moreover,\nour hallucination removal step improves feedback detection performance by ~14%.\nEvaluation on downstream clinically relevant tasks of predicting Behavioral\nAdjustment of trainees and classifying Technical feedback, showed performances\ncomparable to manual annotations with F1 scores of 0.82+/0.03 and 0.81+/0.03\nrespectively. These results highlight the effectiveness of our framework in\nsupporting clinically relevant tasks and improving over manual methods.",
      "tldr_zh": "该研究引入了首个框架，用于从非结构化真实世界录音中重建手术对话，以表征手术训练中的教学任务。该框架整合了Voice Activity Detection、Speaker Diarization和Automated Speech Recognition，并创新性地添加了移除hallucinations（由手术室噪音引起的虚假话语）和使用Few-Shot Voice Samples分离培训者与受训者语音的功能，从而实现准确的对话重建。实验使用33个真实手术数据，展示了系统有效检测反馈（F1 score为0.79±0.07），且hallucination移除步骤提高了反馈检测性能约14%。此外，在下游临床任务中，该框架在预测受训者Behavioral Adjustment和分类Technical Feedback方面，分别达到了与手动注释相当的F1 scores（0.82±0.03和0.81±0.03），证明了其在提升手术教育效率方面的潜力。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.ET",
        "cs.LG",
        "68T50, 68U99, 68T99",
        "I.2; I.2.7; I.5.4; J.3; K.3.1"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted as a proceedings paper at Machine Learning for Health 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.00760v1",
      "published_date": "2024-12-01 10:35:12 UTC",
      "updated_date": "2024-12-01 10:35:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:21.617925"
    },
    {
      "arxiv_id": "2412.00754v1",
      "title": "CtrlNeRF: The Generative Neural Radiation Fields for the Controllable Synthesis of High-fidelity 3D-Aware Images",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Liu",
        "Zhen Yu"
      ],
      "abstract": "The neural radiance field (NERF) advocates learning the continuous\nrepresentation of 3D geometry through a multilayer perceptron (MLP). By\nintegrating this into a generative model, the generative neural radiance field\n(GRAF) is capable of producing images from random noise z without 3D\nsupervision. In practice, the shape and appearance are modeled by z_s and z_a,\nrespectively, to manipulate them separately during inference. However, it is\nchallenging to represent multiple scenes using a solitary MLP and precisely\ncontrol the generation of 3D geometry in terms of shape and appearance. In this\npaper, we introduce a controllable generative model (i.e. \\textbf{CtrlNeRF})\nthat uses a single MLP network to represent multiple scenes with shared\nweights. Consequently, we manipulated the shape and appearance codes to realize\nthe controllable generation of high-fidelity images with 3D consistency.\nMoreover, the model enables the synthesis of novel views that do not exist in\nthe training sets via camera pose alteration and feature interpolation.\nExtensive experiments were conducted to demonstrate its superiority in 3D-aware\nimage generation compared to its counterparts.",
      "tldr_zh": "该论文提出CtrlNeRF，一种基于生成式神经辐射场(Generative Neural Radiation Fields)的模型，用于可控合成高保真3D-Aware图像。该模型使用单个多层感知器(MLP)网络来表示多个场景，共享权重，从而解决了传统方法在表示多个场景和精确控制形状(z_s)和外观(z_a)时的挑战。通过操纵形状和外观代码，CtrlNeRF实现了3D一致性的图像生成，并支持通过改变相机姿态和特征插值合成训练集外的新视图。实验结果显示，该模型在3D-Aware图像生成任务中优于现有基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00754v1",
      "published_date": "2024-12-01 10:19:24 UTC",
      "updated_date": "2024-12-01 10:19:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:22.015657"
    },
    {
      "arxiv_id": "2412.00751v1",
      "title": "Rethinking Cognition: Morphological Info-Computation and the Embodied Paradigm in Life and Artificial Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Gordana Dodig-Crnkovic"
      ],
      "abstract": "This study aims to place Lorenzo Magnanis Eco-Cognitive Computationalism\nwithin the broader context of current work on information, computation, and\ncognition. Traditionally, cognition was believed to be exclusive to humans and\na result of brain activity. However, recent studies reveal it as a fundamental\ncharacteristic of all life forms, ranging from single cells to complex\nmulticellular organisms and their networks. Yet, the literature and general\nunderstanding of cognition still largely remain human-brain-focused, leading to\nconceptual gaps and incoherency. This paper presents a variety of computational\n(information processing) approaches, including an info-computational approach\nto cognition, where natural structures represent information and dynamical\nprocesses on natural structures are regarded as computation, relative to an\nobserving cognizing agent. We model cognition as a web of concurrent\nmorphological computations, driven by processes of self-assembly,\nself-organisation, and autopoiesis across physical, chemical, and biological\ndomains. We examine recent findings linking morphological computation,\nmorphogenesis, agency, basal cognition, extended evolutionary synthesis, and\nactive inference. We establish a connection to Magnanis Eco-Cognitive\nComputationalism and the idea of computational domestication of ignorant\nentities. Novel theoretical and applied insights question the boundaries of\nconventional computational models of cognition. The traditional models\nprioritize symbolic processing and often neglect the inherent constraints and\npotentialities in the physical embodiment of agents on different levels of\norganization. Gaining a better info-computational grasp of cognitive embodiment\nis crucial for the advancement of fields such as biology, evolutionary studies,\nartificial intelligence, robotics, medicine, and more.",
      "tldr_zh": "本论文重新审视认知，将 Lorenzo Magnani 的 Eco-Cognitive Computationalism 置于更广泛的信息、计算和认知框架中，强调认知不仅是人类大脑的专属特征，而是所有生命形式从单细胞到复杂网络的基本属性。作者采用 info-computational 方法，将自然结构视为信息、动态过程视为计算，并将认知建模为并发 morphological computations，由自组装、自组织和 autopoiesis 等过程驱动。论文考察了 morphological computation、morphogenesis、agency、basal cognition、extended evolutionary synthesis 和 active inference 等概念的联系，质疑传统以符号处理为主的认知模型忽略了物理具身的约束。最终，这些理论见解有助于推进生物学、人工智能、机器人和医学等领域的发展，提供更全面的认知具身理解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00751v1",
      "published_date": "2024-12-01 10:04:53 UTC",
      "updated_date": "2024-12-01 10:04:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:27.419349"
    },
    {
      "arxiv_id": "2412.00749v2",
      "title": "CONCERTO: Complex Query Execution Mechanism-Aware Learned Cost Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Kaixin Zhang",
        "Hongzhi Wang",
        "Kunkai Gu",
        "Ziqi Li",
        "Chunyu Zhao",
        "Yingze Li",
        "Yu Yan"
      ],
      "abstract": "With the growing demand for massive data analysis, many DBMSs have adopted\ncomplex underlying query execution mechanisms, including vectorized operators,\nparallel execution, and dynamic pipeline modifications. However, there remains\na lack of targeted Query Performance Prediction (QPP) methods for these complex\nexecution mechanisms and their interactions, as most existing approaches focus\non traditional tree-shaped query plans and static serial executors. To address\nthis challenge, this paper proposes CONCERTO, a Complex query executiON\nmeChanism-awaE leaRned cosT estimatiOn method. CONCERTO first establishes\nindependent resource cost models for each physical operator. It then constructs\na Directed Acyclic Graph (DAG) consisting of a dataflow tree backbone and\nresource competition relationships among concurrent operators. After\ncalibrating the cost impact of parallel operator execution using Graph\nAttention Networks (GATs) with additional attention mechanisms, CONCERTO\nextracts and aggregates cost vector trees through Temporal Convolutional\nNetworks (TCNs), ultimately achieving effective query performance prediction.\nExperimental results demonstrate that CONCERTO achieves higher prediction\naccuracy than existing methods.",
      "tldr_zh": "该论文针对数据库管理系统(DBMS)中复杂查询执行机制（如矢量化操作符、并行执行和动态管道修改）的需求，提出了一种名为CONCERTO的学习成本估计方法，以解决现有Query Performance Prediction (QPP)方法对传统树状查询计划的局限性。CONCERTO首先为每个物理操作符建立独立资源成本模型，然后构建一个包含数据流树主干和并发操作符资源竞争关系的Directed Acyclic Graph (DAG)，并利用Graph Attention Networks (GATs)校准并行执行的成本影响，最后通过Temporal Convolutional Networks (TCNs)提取和聚合成本向量树，实现精确的查询性能预测。实验结果表明，CONCERTO比现有方法具有更高的预测准确性，为处理复杂执行机制交互提供了有效解决方案。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00749v2",
      "published_date": "2024-12-01 09:58:54 UTC",
      "updated_date": "2025-03-28 12:47:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:26.451901"
    },
    {
      "arxiv_id": "2412.00748v1",
      "title": "Exploring Cognition through Morphological Info-Computational Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Gordana Dodig-Crnkovic"
      ],
      "abstract": "Traditionally, cognition has been considered a uniquely human capability\ninvolving perception, memory, learning, reasoning, and problem-solving.\nHowever, recent research shows that cognition is a fundamental ability shared\nby all living beings, from single cells to complex organisms. This chapter\ntakes an info-computational approach (ICON), viewing natural structures as\ninformation and the processes of change in these structures as computations. It\nis a relational framework dependent on the perspective of a cognizing\nobserver/cognizer. Informational structures are properties of the material\nsubstrate, and when focusing on the behavior of the substrate, we discuss\nmorphological computing (MC). ICON and MC are complementary perspectives for a\ncognizer. Information and computation are inseparably connected with cognition.\nThis chapter explores research connecting nature as a computational structure\nfor a cognizer, with morphological computation, morphogenesis, agency, extended\ncognition, and extended evolutionary synthesis, using examples of the free\nenergy principle and active inference. It introduces theoretical and practical\napproaches challenging traditional computational models of cognition limited to\nabstract symbol processing, highlighting the computational capacities inherent\nin the material substrate (embodiment). Understanding the embodiment of\ncognition through its morphological computational basis is crucial for biology,\nevolution, intelligence theory, AI, robotics, and other fields.",
      "tldr_zh": "本论文探讨认知的普遍性，挑战传统观点，认为认知不仅是人类的专长，还包括感知、记忆、学习、推理和问题解决等能力，适用于从单细胞到复杂生物。作者采用 info-computational approach (ICON) 框架，将自然结构视为信息、变化过程视为计算，并结合 morphological computing (MC) 来分析物质基底的行为，作为认知观察者的互补视角。论文通过 free energy principle 和 active inference 等例子，强调认知的物质基础（embodiment），并扩展到形态发生、代理和进化综合等领域，为生物学、AI 和机器人等领域的智力理论提供新见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00748v1",
      "published_date": "2024-12-01 09:56:38 UTC",
      "updated_date": "2024-12-01 09:56:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:28.059836"
    },
    {
      "arxiv_id": "2412.00744v1",
      "title": "A Cross-Scene Benchmark for Open-World Drone Active Tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Haowei Sun",
        "Jinwu Hu",
        "Zhirui Zhang",
        "Haoyuan Tian",
        "Xinze Xie",
        "Yufeng Wang",
        "Zhuliang Yu",
        "Xiaohua Xie",
        "Mingkui Tan"
      ],
      "abstract": "Drone Visual Active Tracking aims to autonomously follow a target object by\ncontrolling the motion system based on visual observations, providing a more\npractical solution for effective tracking in dynamic environments. However,\naccurate Drone Visual Active Tracking using reinforcement learning remains\nchallenging due to the absence of a unified benchmark, the complexity of\nopen-world environments with frequent interference, and the diverse motion\nbehavior of dynamic targets. To address these issues, we propose a unified\ncross-scene cross-domain benchmark for open-world drone active tracking called\nDAT. The DAT benchmark provides 24 visually complex environments to assess the\nalgorithms' cross-scene and cross-domain generalization abilities, and\nhigh-fidelity modeling of realistic robot dynamics. Additionally, we propose a\nreinforcement learning-based drone tracking method called R-VAT, which aims to\nimprove the performance of drone tracking targets in complex scenarios.\nSpecifically, inspired by curriculum learning, we introduce a Curriculum-Based\nTraining strategy that progressively enhances the agent tracking performance in\nvast environments with complex interference. We design a goal-centered reward\nfunction to provide precise feedback to the drone agent, preventing targets\nfarther from the center of view from receiving higher rewards than closer ones.\nThis allows the drone to adapt to the diverse motion behavior of open-world\ntargets. Experiments demonstrate that the R-VAT has about 400% improvement over\nthe SOTA method in terms of the cumulative reward metric.",
      "tldr_zh": "该论文针对无人机视觉主动跟踪（Drone Visual Active Tracking）提出一个统一的跨场景跨域基准DAT，以解决开放世界环境中缺乏基准、干扰复杂以及目标运动多样性的挑战。DAT包括24个视觉复杂的环境，用于评估算法的泛化能力和真实机器人动态建模。同时，作者引入基于强化学习（reinforcement learning）的跟踪方法R-VAT，通过课程学习（curriculum learning）策略逐步提升代理在复杂场景中的性能，并设计以目标为中心的奖励函数以提供精确反馈。实验结果显示，R-VAT在累积奖励指标上比现有最先进方法提高了约400%。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "25 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.00744v1",
      "published_date": "2024-12-01 09:37:46 UTC",
      "updated_date": "2024-12-01 09:37:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:29.050500"
    },
    {
      "arxiv_id": "2412.00742v1",
      "title": "Revisiting Self-Supervised Heterogeneous Graph Learning from Spectral Clustering Perspective",
      "title_zh": "从谱聚类视角重新审视自监督异构图学习",
      "authors": [
        "Yujie Mo",
        "Zhihe Lu",
        "Runpeng Yu",
        "Xiaofeng Zhu",
        "Xinchao Wang"
      ],
      "abstract": "Self-supervised heterogeneous graph learning (SHGL) has shown promising\npotential in diverse scenarios. However, while existing SHGL methods share a\nsimilar essential with clustering approaches, they encounter two significant\nlimitations: (i) noise in graph structures is often introduced during the\nmessage-passing process to weaken node representations, and (ii) cluster-level\ninformation may be inadequately captured and leveraged, diminishing the\nperformance in downstream tasks. In this paper, we address these limitations by\ntheoretically revisiting SHGL from the spectral clustering perspective and\nintroducing a novel framework enhanced by rank and dual consistency\nconstraints. Specifically, our framework incorporates a rank-constrained\nspectral clustering method that refines the affinity matrix to exclude noise\neffectively. Additionally, we integrate node-level and cluster-level\nconsistency constraints that concurrently capture invariant and clustering\ninformation to facilitate learning in downstream tasks. We theoretically\ndemonstrate that the learned representations are divided into distinct\npartitions based on the number of classes and exhibit enhanced generalization\nability across tasks. Experimental results affirm the superiority of our\nmethod, showcasing remarkable improvements in several downstream tasks compared\nto existing methods.",
      "tldr_zh": "本文从 Spectral Clustering 的视角重新审视 Self-Supervised Heterogeneous Graph Learning (SHGL)，指出了现有方法在消息传递过程中引入噪声以及未能充分捕捉集群级信息的问题。提出一个新框架，通过 Rank-Constrained Spectral Clustering 提炼亲和矩阵以排除噪声，并整合节点级和集群级 Dual Consistency 约束，以同时捕捉不变性和聚类信息。理论证明该框架能将学习到的表示分为基于类别数的不同分区，并提升下游任务的泛化能力；实验结果显示，该方法在多个下游任务上显著优于现有方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00742v1",
      "published_date": "2024-12-01 09:33:20 UTC",
      "updated_date": "2024-12-01 09:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:37.487725"
    },
    {
      "arxiv_id": "2412.00726v1",
      "title": "Free and Customizable Code Documentation with LLMs: A Fine-Tuning Approach",
      "title_zh": "使用 LLMs 的免费且可定制代码文档：一种微调方法",
      "authors": [
        "Sayak Chakrabarty",
        "Souradip Pal"
      ],
      "abstract": "Automated documentation of programming source code is a challenging task with\nsignificant practical and scientific implications for the developer community.\nWe present a large language model (LLM)-based application that developers can\nuse as a support tool to generate basic documentation for any publicly\navailable repository. Over the last decade, several papers have been written on\ngenerating documentation for source code using neural network architectures.\nWith the recent advancements in LLM technology, some open-source applications\nhave been developed to address this problem. However, these applications\ntypically rely on the OpenAI APIs, which incur substantial financial costs,\nparticularly for large repositories. Moreover, none of these open-source\napplications offer a fine-tuned model or features to enable users to fine-tune.\nAdditionally, finding suitable data for fine-tuning is often challenging. Our\napplication addresses these issues which is available at\nhttps://pypi.org/project/readme-ready/.",
      "tldr_zh": "该研究提出了一种基于大型语言模型(LLM)的免费可自定义代码文档生成应用，通过fine-tuning方法帮助开发者为公开仓库自动生成基本文档。相较于依赖OpenAI API的现有工具，该应用避免了高昂的财务成本，并提供用户友好的fine-tuning功能，同时解决了fine-tuning数据获取的难题。最终，该开源应用（可访问https://pypi.org/project/readme-ready/）提升了代码文档化的实用性和可访问性，为开发者社区提供了更高效的支持工具。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00726v1",
      "published_date": "2024-12-01 08:38:18 UTC",
      "updated_date": "2024-12-01 08:38:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:38.082035"
    },
    {
      "arxiv_id": "2412.00725v1",
      "title": "Decision Transformer vs. Decision Mamba: Analysing the Complexity of Sequential Decision Making in Atari Games",
      "title_zh": "翻译失败",
      "authors": [
        "Ke Yan"
      ],
      "abstract": "This work analyses the disparity in performance between Decision Transformer\n(DT) and Decision Mamba (DM) in sequence modelling reinforcement learning tasks\nfor different Atari games. The study first observed that DM generally\noutperformed DT in the games Breakout and Qbert, while DT performed better in\nmore complicated games, such as Hero and Kung Fu Master. To understand these\ndifferences, we expanded the number of games to 12 and performed a\ncomprehensive analysis of game characteristics, including action space\ncomplexity, visual complexity, average trajectory length, and average steps to\nthe first non-zero reward. In order to further analyse the key factors that\nimpact the disparity in performance between DT and DM, we employ various\napproaches, including quantifying visual complexity, random forest regression,\ncorrelation analysis, and action space simplification strategies. The results\nindicate that the performance gap between DT and DM is affected by the complex\ninteraction of multiple factors, with the complexity of the action space and\nvisual complexity (particularly evaluated by compression ratio) being the\nprimary determining factors. DM performs well in environments with simple\naction and visual elements, while DT shows an advantage in games with higher\naction and visual complexity. Our findings contribute to a deeper understanding\nof how the game characteristics affect the performance difference in sequential\nmodelling reinforcement learning, potentially guiding the development of future\nmodel design and applications for diverse and complex environments.",
      "tldr_zh": "本研究比较了 Decision Transformer (DT) 和 Decision Mamba (DM) 在 Atari 游戏中的性能差异，发现 DM 在简单游戏如 Breakout 和 Qbert 中表现优于 DT，而 DT 在复杂游戏如 Hero 和 Kung Fu Master 中更具优势。\n研究者扩展到 12 个游戏，分析了动作空间复杂度、视觉复杂度、平均轨迹长度和到达第一个非零奖励的步数，并采用 random forest regression、相关分析和动作空间简化策略等方法进行量化评估。\n结果表明，性能差距主要受动作空间和视觉复杂度（尤其是压缩比评估）的交互影响，DM 更适合简单环境，而 DT 在高复杂度环境中显示出明显优势。\n这些发现有助于深化对顺序建模强化学习的理解，并为未来模型设计和复杂环境应用提供指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00725v1",
      "published_date": "2024-12-01 08:37:10 UTC",
      "updated_date": "2024-12-01 08:37:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:42.541403"
    },
    {
      "arxiv_id": "2412.00724v1",
      "title": "AdaScale: Dynamic Context-aware DNN Scaling via Automated Adaptation Loop on Mobile Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Yuzhan Wang",
        "Sicong Liu",
        "Bin Guo",
        "Boqi Zhang",
        "Ke Ma",
        "Yasan Ding",
        "Hao Luo",
        "Yao Li",
        "Zhiwen Yu"
      ],
      "abstract": "Deep learning is reshaping mobile applications, with a growing trend of\ndeploying deep neural networks (DNNs) directly to mobile and embedded devices\nto address real-time performance and privacy. To accommodate local resource\nlimitations, techniques like weight compression, convolution decomposition, and\nspecialized layer architectures have been developed. However, the\n\\textit{dynamic} and \\textit{diverse} deployment contexts of mobile devices\npose significant challenges. Adapting deep models to meet varied\ndevice-specific requirements for latency, accuracy, memory, and energy is\nlabor-intensive. Additionally, changing processor states, fluctuating memory\navailability, and competing processes frequently necessitate model\nre-compression to preserve user experience. To address these issues, we\nintroduce AdaScale, an elastic inference framework that automates the\nadaptation of deep models to dynamic contexts. AdaScale leverages a\nself-evolutionary model to streamline network creation, employs diverse\ncompression operator combinations to reduce the search space and improve\noutcomes, and integrates a resource availability awareness block and\nperformance profilers to establish an automated adaptation loop. Our\nexperiments demonstrate that AdaScale significantly enhances accuracy by 5.09%,\nreduces training overhead by 66.89%, speeds up inference latency by 1.51 to 6.2\ntimes, and lowers energy costs by 4.69 times.",
      "tldr_zh": "这篇论文提出了AdaScale，一种动态上下文感知的深度神经网络(DNNs)缩放框架，旨在自动适应移动设备的资源限制，如延迟、准确性、内存和能源需求。AdaScale利用自演化模型、多种压缩操作符组合以及资源可用性感知块和性能分析器，形成一个自动化适应循环，以简化模型调整过程。实验结果显示，AdaScale将准确性提高了5.09%，训练开销减少了66.89%，推理延迟加快了1.51至6.2倍，并将能源消耗降低了4.69倍。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00724v1",
      "published_date": "2024-12-01 08:33:56 UTC",
      "updated_date": "2024-12-01 08:33:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:42.725420"
    },
    {
      "arxiv_id": "2412.00722v1",
      "title": "Towards Adaptive Mechanism Activation in Language Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyang Huang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Language Agent could be endowed with different mechanisms for autonomous task\naccomplishment. Current agents typically rely on fixed mechanisms or a set of\nmechanisms activated in a predefined order, limiting their adaptation to varied\npotential task solution structures. To this end, this paper proposes\n\\textbf{A}daptive \\textbf{L}anguage \\textbf{A}gent \\textbf{M}echanism\n\\textbf{A}ctivation Learning with Self-Exploration (\\textbf{ALAMA}), which\nfocuses on optimizing mechanism activation adaptability without reliance on\nexpert models. Initially, it builds a harmonized agent framework\n(\\textbf{UniAct}) to \\textbf{Uni}fy different mechanisms via \\textbf{Act}ions.\nThen it leverages a training-efficient optimization method based on\nself-exploration to enable the UniAct to adaptively activate the appropriate\nmechanisms according to the potential characteristics of the task. Experimental\nresults demonstrate significant improvements in downstream agent tasks,\naffirming the effectiveness of our approach in facilitating more dynamic and\ncontext-sensitive mechanism activation.",
      "tldr_zh": "该论文针对语言代理（Language Agent）在任务完成中的机制激活问题，指出现有代理依赖固定或预定义顺序的机制，难以适应多样任务结构。作者提出 ALAMA（Adaptive Language Agent Mechanism Activation Learning with Self-Exploration）方法，通过构建统一的代理框架 UniAct 来整合不同机制，并利用基于 Self-Exploration 的高效优化训练，使代理能根据任务特性自适应激活适当机制。实验结果显示，ALAMA 在下游代理任务中取得了显著改进，证明了其在提升机制激活的动态性和上下文敏感性方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "COLING2025",
      "pdf_url": "http://arxiv.org/pdf/2412.00722v1",
      "published_date": "2024-12-01 08:10:04 UTC",
      "updated_date": "2024-12-01 08:10:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:43.761044"
    },
    {
      "arxiv_id": "2412.00721v2",
      "title": "A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching Scenario",
      "title_zh": "翻译失败",
      "authors": [
        "Zheshu Song",
        "Ziyang Ma",
        "Yifan Yang",
        "Jianheng Zhuo",
        "Xie Chen"
      ],
      "abstract": "Large Language Models (LLMs) have showcased exceptional performance across\ndiverse NLP tasks, and their integration with speech encoder is rapidly\nemerging as a dominant trend in the Automatic Speech Recognition (ASR) field.\nPrevious works mainly concentrated on leveraging LLMs for speech recognition in\nEnglish and Chinese. However, their potential for addressing speech recognition\nchallenges in low resource settings remains underexplored. Hence, in this work,\nwe aim to explore the capability of LLMs in low resource ASR and\nMandarin-English code switching ASR. We also evaluate and compare the\nrecognition performance of LLM-based ASR systems against Whisper model.\nExtensive experiments demonstrate that LLM-based ASR yields a relative gain of\n12.8\\% over the Whisper model in low resource ASR while Whisper performs better\nin Mandarin-English code switching ASR. We hope that this study could shed\nlight on ASR for low resource scenarios.",
      "tldr_zh": "本研究比较了基于大型语言模型 (LLMs) 的自动语音识别 (ASR) 系统与 Whisper 模型在低资源场景和中英代码切换 (Mandarin-English code switching) 下的表现。研究通过广泛实验评估，发现 LLM-based ASR 在低资源 ASR 中比 Whisper 模型提高了 12.8% 的相对性能，而 Whisper 在代码切换场景中表现出色。该工作旨在为低资源 ASR 场景提供新的启发和见解。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.AI",
      "comment": "This work hasn't been finished yet",
      "pdf_url": "http://arxiv.org/pdf/2412.00721v2",
      "published_date": "2024-12-01 08:07:01 UTC",
      "updated_date": "2024-12-04 06:23:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:46.709970"
    },
    {
      "arxiv_id": "2412.00718v1",
      "title": "Well log data generation and imputation using sequence-based generative adversarial networks",
      "title_zh": "基于序列生成对抗网络的井日志数据生成与插值",
      "authors": [
        "Abdulrahman Al-Fakih",
        "A. Koeshidayatullah",
        "Tapan Mukerji",
        "Sadam Al-Azani",
        "SanLinn I. Kaka"
      ],
      "abstract": "Well log analysis is crucial for hydrocarbon exploration, providing detailed\ninsights into subsurface geological formations. However, gaps and inaccuracies\nin well log data, often due to equipment limitations, operational challenges,\nand harsh subsurface conditions, can introduce significant uncertainties in\nreservoir evaluation. Addressing these challenges requires effective methods\nfor both synthetic data generation and precise imputation of missing data,\nensuring data completeness and reliability. This study introduces a novel\nframework utilizing sequence-based generative adversarial networks (GANs)\nspecifically designed for well log data generation and imputation. The\nframework integrates two distinct sequence-based GAN models: Time Series GAN\n(TSGAN) for generating synthetic well log data and Sequence GAN (SeqGAN) for\nimputing missing data. Both models were tested on a dataset from the North Sea,\nNetherlands region, focusing on different sections of 5, 10, and 50 data\npoints. Experimental results demonstrate that this approach achieves superior\naccuracy in filling data gaps compared to other deep learning models for\nspatial series analysis. The method yielded R^2 values of 0.921, 0.899, and\n0.594, with corresponding mean absolute percentage error (MAPE) values of\n8.320, 0.005, and 151.154, and mean absolute error (MAE) values of 0.012,\n0.005, and 0.032, respectively. These results set a new benchmark for data\nintegrity and utility in geosciences, particularly in well log data analysis.",
      "tldr_zh": "本研究针对井日志数据中的缺口和不准确问题，提出一个基于序列生成对抗网络(sequence-based GANs)的框架，用于合成数据生成和缺失数据填充。\n该框架整合了Time Series GAN (TSGAN)来生成合成井日志数据，以及Sequence GAN (SeqGAN)来精确填充缺失部分，并在北海荷兰地区的数据集上测试不同数据点段（5、10和50点）。\n实验结果显示，该方法在数据填充准确性上优于其他深度学习模型，R²值分别为0.921、0.899和0.594，MAPE值分别为8.320、0.005和151.154，MAE值分别为0.012、0.005和0.032。\n这项工作提升了井日志数据的完整性和可靠性，为地学科研，特别是井日志分析，设定了新基准。",
      "categories": [
        "physics.geo-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00718v1",
      "published_date": "2024-12-01 07:50:34 UTC",
      "updated_date": "2024-12-01 07:50:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:49.863669"
    },
    {
      "arxiv_id": "2412.00707v2",
      "title": "Protect Your Secrets: Understanding and Measuring Data Exposure in VSCode Extensions",
      "title_zh": "保护你的秘密：理解和测量 VSCode 扩展中的数据暴露",
      "authors": [
        "Yue Liu",
        "Chakkrit Tantithamthavorn",
        "Li Li"
      ],
      "abstract": "Recent years have witnessed the emerging trend of extensions in modern\nIntegrated Development Environments (IDEs) like Visual Studio Code (VSCode)\nthat significantly enhance developer productivity. Especially, popular AI\ncoding assistants like GitHub Copilot and Tabnine provide conveniences like\nautomated code completion and debugging. While these extensions offer numerous\nbenefits, they may introduce privacy and security concerns to software\ndevelopers. However, there is no existing work that systematically analyzes the\nsecurity and privacy concerns, including the risks of data exposure in VSCode\nextensions.\n  In this paper, we investigate on the security issues of cross-extension\ninteractions in VSCode and shed light on the vulnerabilities caused by data\nexposure among different extensions. Our study uncovers high-impact security\nflaws that could allow adversaries to stealthily acquire or manipulate\ncredential-related data (e.g., passwords, API keys, access tokens) from other\nextensions if not properly handled by extension vendors. To measure their\nprevalence, we design a novel automated risk detection framework that leverages\nprogram analysis and natural language processing techniques to automatically\nidentify potential risks in VSCode extensions. By applying our tool to 27,261\nreal-world VSCode extensions, we discover that 8.5% of them (i.e., 2,325\nextensions) are exposed to credential-related data leakage through various\nvectors, such as commands, user input, and configurations. Our study sheds\nlight on the security challenges and flaws of the extension-in-IDE paradigm and\nprovides suggestions and recommendations for improving the security of VSCode\nextensions and mitigating the risks of data exposure.",
      "tldr_zh": "本文研究了Visual Studio Code (VSCode) 扩展的安全风险，特别是数据暴露问题，这些扩展（如GitHub Copilot和Tabnine）虽提升了开发效率，但可能导致凭证相关数据（如密码和API keys）被窃取或操纵。研究者设计了一个自动风险检测框架，结合program analysis和natural language processing技术，对27,261个真实VSCode扩展进行分析。结果发现，8.5%的扩展（即2,325个）存在通过命令、用户输入和配置等向量的数据泄露风险。该研究揭示了扩展在IDE中的安全挑战，并提供了改进建议，以缓解数据暴露威胁。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00707v2",
      "published_date": "2024-12-01 07:08:53 UTC",
      "updated_date": "2024-12-25 06:49:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:51.689642"
    },
    {
      "arxiv_id": "2412.00691v2",
      "title": "The Advancement of Personalized Learning Potentially Accelerated by Generative AI",
      "title_zh": "Generative AI 可能加速的个性化学习发展",
      "authors": [
        "Yuang Wei",
        "Yuan-Hao Jiang",
        "Jiayi Liu",
        "Changyong Qi",
        "Linzhao Jia",
        "Rui Jia"
      ],
      "abstract": "The rapid development of Generative AI (GAI) has sparked revolutionary\nchanges across various aspects of education. Personalized learning, a focal\npoint and challenge in educational research, has also been influenced by the\ndevelopment of GAI. To explore GAI's extensive impact on personalized learning,\nthis study investigates its potential to enhance various facets of personalized\nlearning through a thorough analysis of existing research. The research\ncomprehensively examines GAI's influence on personalized learning by analyzing\nits application across different methodologies and contexts, including learning\nstrategies, paths, materials, environments, and specific analyses within the\nteaching and learning processes. Through this in-depth investigation, we find\nthat GAI demonstrates exceptional capabilities in providing adaptive learning\nexperiences tailored to individual preferences and needs. Utilizing different\nforms of GAI across various subjects yields superior learning outcomes. The\narticle concludes by summarizing scenarios where GAI is applicable in\neducational processes and discussing strategies for leveraging GAI to enhance\npersonalized learning, aiming to guide educators and learners in effectively\nutilizing GAI to achieve superior learning objectives.",
      "tldr_zh": "这篇论文探讨了生成式 AI (Generative AI) 如何加速个性化学习的发展，通过分析现有研究来评估其在学习策略、路径、材料、环境等方面的影响。研究发现，GAI 能提供适应个人偏好和需求的自适应学习体验，并在不同科目中使用各种形式时显著提升学习成果。作者总结了 GAI 在教育过程中的适用场景，并提出策略，帮助教育者和学习者有效利用 GAI 以实现更好的学习目标。总的来说，该研究为整合 GAI 增强个性化学习提供了宝贵指导。",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "The V1 version is a more detailed version, and the latest version is\n  the SITE conference included version. SITE 2025-Orando, Florida, United\n  States, March 17-21.2025",
      "pdf_url": "http://arxiv.org/pdf/2412.00691v2",
      "published_date": "2024-12-01 06:01:14 UTC",
      "updated_date": "2025-02-26 08:54:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:51.812940"
    },
    {
      "arxiv_id": "2412.00686v2",
      "title": "LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models",
      "title_zh": "LVLM-COUNT：增强大型视觉语言模型的计数能力",
      "authors": [
        "Muhammad Fetrat Qharabagh",
        "Mohammadreza Ghofrani",
        "Kimon Fountoulakis"
      ],
      "abstract": "Counting is a fundamental operation for various visual tasks in real-life\napplications, requiring both object recognition and robust counting\ncapabilities. Despite their advanced visual perception, large vision-language\nmodels (LVLMs) struggle with counting tasks, especially when the number of\nobjects exceeds those commonly encountered during training. We enhance LVLMs'\ncounting abilities using a divide-and-conquer approach, breaking counting\nproblems into sub-counting tasks. Our method employs a mechanism that prevents\nbisecting and thus repetitive counting of objects, which occurs in a naive\ndivide-and-conquer approach. Unlike prior methods, which do not generalize well\nto counting datasets they have not been trained on, our method performs well on\nnew datasets without any additional training or fine-tuning. We demonstrate\nthat our approach enhances the counting capability of LVLMs across various\ndatasets and benchmarks.",
      "tldr_zh": "该论文针对大型视觉语言模型 (LVLMs) 在计数任务上的不足，特别是处理超过训练常见数量的物体时，提出了一种增强方法。研究采用 divide-and-conquer 策略，将计数问题分解为子计数任务，并引入机制防止重复计数，从而避免了简单分割策略的缺陷。与现有方法不同，该方法无需额外训练或微调，即可在未见数据集上实现良好泛化。实验结果显示，该方法显著提升了 LVLMs 在各种数据集和基准上的计数能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "31 pages, 24 Figures, 10 Tables",
      "pdf_url": "http://arxiv.org/pdf/2412.00686v2",
      "published_date": "2024-12-01 05:50:22 UTC",
      "updated_date": "2025-02-02 17:49:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:48:53.438863"
    },
    {
      "arxiv_id": "2412.00684v2",
      "title": "Paint Outside the Box: Synthesizing and Selecting Training Data for Visual Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Zilin Du",
        "Haoxin Li",
        "Jianfei Yu",
        "Boyang Li"
      ],
      "abstract": "Visual grounding aims to localize the image regions based on a textual query.\nGiven the difficulty of large-scale data curation, we investigate how to\neffectively learn visual grounding under data-scarce settings in this paper. To\naddress the data scarcity, we propose a novel framework, POBF (Paint Outside\nthe Box and Filter). POBF synthesizes images by inpainting outside the box,\ntackling a label misalignment issue encountered in previous works. Furthermore,\nPOBF leverages an innovative filtering scheme to select the most effective\ntraining data. This scheme combines a hardness score and an overfitting score,\nbalanced by a penalty term. Extensive experiments across four benchmark\ndatasets demonstrate that POBF consistently improves performance, achieving an\naverage gain of 5.83\\% over the real-data-only method and outperforming leading\nbaselines by 2.29\\%-3.85\\% in accuracy. Additionally, we validate the\nrobustness and generalizability of POBF across various generative models,\ntraining data sizes, and model architectures.",
      "tldr_zh": "本论文针对视觉 grounding（基于文本查询定位图像区域）的训练数据稀缺问题，提出了一种新框架POBF（Paint Outside the Box and Filter）。该框架通过在图像区域外进行inpainting合成图像，以解决现有方法的标签不对齐问题，并采用一个创新过滤方案结合hardness score（难度分数）和overfitting score（过拟合分数），通过惩罚项平衡来选择最有效的训练数据。在四个基准数据集上的广泛实验表明，POBF 比仅使用真实数据的基准方法平均提高了5.83%的性能，并优于领先基线2.29%-3.85%的准确率；此外，该框架在各种生成模型、训练数据规模和模型架构中展现出良好的鲁棒性和泛化性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00684v2",
      "published_date": "2024-12-01 05:47:59 UTC",
      "updated_date": "2025-04-20 10:32:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:49:01.925464"
    },
    {
      "arxiv_id": "2412.00663v1",
      "title": "Deep Learning for Longitudinal Gross Tumor Volume Segmentation in MRI-Guided Adaptive Radiotherapy for Head and Neck Cancer",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Tie",
        "Weijie Chen",
        "Zachary Huemann",
        "Brayden Schott",
        "Nuohao Liu",
        "Tyler J. Bradshaw"
      ],
      "abstract": "Accurate segmentation of gross tumor volume (GTV) is essential for effective\nMRI-guided adaptive radiotherapy (MRgART) in head and neck cancer. However,\nmanual segmentation of the GTV over the course of therapy is time-consuming and\nprone to interobserver variability. Deep learning (DL) has the potential to\novercome these challenges by automatically delineating GTVs. In this study, our\nteam, $\\textit{UW LAIR}$, tackled the challenges of both pre-radiotherapy\n(pre-RT) (Task 1) and mid-radiotherapy (mid-RT) (Task 2) tumor volume\nsegmentation. To this end, we developed a series of DL models for longitudinal\nGTV segmentation. The backbone of our models for both tasks was SegResNet with\ndeep supervision. For Task 1, we trained the model using a combined dataset of\npre-RT and mid-RT MRI data, which resulted in the improved aggregated Dice\nsimilarity coefficient (DSCagg) on an internal testing set compared to models\ntrained solely on pre-RT MRI data. In Task 2, we introduced mask-aware\nattention modules, enabling pre-RT GTV masks to influence intermediate features\nlearned from mid-RT data. This attention-based approach yielded slight\nimprovements over the baseline method, which concatenated mid-RT MRI with\npre-RT GTV masks as input. In the final testing phase, the ensemble of 10\npre-RT segmentation models achieved an average DSCagg of 0.794, with 0.745 for\nprimary GTV (GTVp) and 0.844 for metastatic lymph nodes (GTVn) in Task 1. For\nTask 2, the ensemble of 10 mid-RT segmentation models attained an average\nDSCagg of 0.733, with 0.607 for GTVp and 0.859 for GTVn, leading us to\n$\\textbf{achieve 1st place}$. In summary, we presented a collection of DL\nmodels that could facilitate GTV segmentation in MRgART, offering the potential\nto streamline radiation oncology workflows. Our code and model weights are\navailable at https://github.com/xtie97/HNTS-MRG24-UWLAIR.",
      "tldr_zh": "本研究利用深度学习（Deep Learning, DL）自动分割头颈癌的 Gross Tumor Volume (GTV)，以支持 MRI-guided adaptive radiotherapy (MRgART)，解决手动分割耗时且易受观察者变异的问题。研究团队 UW LAIR 基于 SegResNet 骨干开发了系列模型：Task 1 使用 pre-RT 和 mid-RT MRI 数据联合训练，提高 Dice similarity coefficient (DSCagg)；Task 2 引入 mask-aware attention modules，利用 pre-RT GTV masks 增强 mid-RT 数据处理。实验结果显示，Task 1 的模型集合平均 DSCagg 为 0.794（GTVp: 0.745, GTVn: 0.844），Task 2 达到 0.733（GTVp: 0.607, GTVn: 0.859），并获得第一名，这些模型有助于简化辐射肿瘤学工作流程。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "comment": "12 pages, 4 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.00663v1",
      "published_date": "2024-12-01 03:57:18 UTC",
      "updated_date": "2024-12-01 03:57:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:49:03.803748"
    },
    {
      "arxiv_id": "2412.00661v2",
      "title": "Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Emile Anand",
        "Ishani Karmarkar",
        "Guannan Qu"
      ],
      "abstract": "Designing efficient algorithms for multi-agent reinforcement learning (MARL)\nis fundamentally challenging because the size of the joint state and action\nspaces grows exponentially in the number of agents. These difficulties are\nexacerbated when balancing sequential global decision-making with local agent\ninteractions. In this work, we propose a new algorithm $\\texttt{SUBSAMPLE-MFQ}$\n($\\textbf{Subsample}$-$\\textbf{M}$ean-$\\textbf{F}$ield-$\\textbf{Q}$-learning)\nand a decentralized randomized policy for a system with $n$ agents. For $k\\leq\nn$, our algorithm learns a policy for the system in time polynomial in $k$. We\nshow that this learned policy converges to the optimal policy on the order of\n$\\tilde{O}(1/\\sqrt{k})$ as the number of subsampled agents $k$ increases. We\nempirically validate our method in Gaussian squeeze and global exploration\nsettings.",
      "tldr_zh": "该论文针对多智能体强化学习(MARL)中联合状态和动作空间指数增长的挑战，提出了一种新的算法SUBSAMPLE-MFQ(Subsample-Mean-Field-Q-learning)，结合均值场(mean-field)方法和去中心化随机策略，通过子采样k个智能体来高效学习系统策略。算法能够在多项式于k的时间内运行，并证明所学策略以Õ(1/√k)的误差收敛到最优策略。实验在Gaussian squeeze和global exploration场景中验证了该方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "eess.SY",
        "math.OC",
        "60J20, 68T99",
        "I.2.11"
      ],
      "primary_category": "cs.LG",
      "comment": "44 pages. 6 figures. arXiv admin note: text overlap with\n  arXiv:2403.00222",
      "pdf_url": "http://arxiv.org/pdf/2412.00661v2",
      "published_date": "2024-12-01 03:45:17 UTC",
      "updated_date": "2025-01-29 22:54:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:49:04.186547"
    },
    {
      "arxiv_id": "2412.00657v1",
      "title": "Improving Vietnamese Legal Document Retrieval using Synthetic Data",
      "title_zh": "翻译失败",
      "authors": [
        "Son Pham Tien",
        "Hieu Nguyen Doan",
        "An Nguyen Dai",
        "Sang Dinh Viet"
      ],
      "abstract": "In the field of legal information retrieval, effective embedding-based models\nare essential for accurate question-answering systems. However, the scarcity of\nlarge annotated datasets poses a significant challenge, particularly for\nVietnamese legal texts. To address this issue, we propose a novel approach that\nleverages large language models to generate high-quality, diverse synthetic\nqueries for Vietnamese legal passages. This synthetic data is then used to\npre-train retrieval models, specifically bi-encoder and ColBERT, which are\nfurther fine-tuned using contrastive loss with mined hard negatives. Our\nexperiments demonstrate that these enhancements lead to strong improvement in\nretrieval accuracy, validating the effectiveness of synthetic data and\npre-training techniques in overcoming the limitations posed by the lack of\nlarge labeled datasets in the Vietnamese legal domain.",
      "tldr_zh": "这篇论文针对越南法律文本标注数据集稀缺的问题，提出了一种利用大型语言模型生成高质量、多样合成查询的方法，以提升法律文档检索的准确性。具体而言，该方法使用合成数据预训练 bi-encoder 和 ColBERT 模型，并通过对比损失和挖掘硬负样本进行进一步微调。实验结果显示，这种增强技术显著提高了检索准确率，证明了合成数据和预训练策略在越南法律领域数据不足情况下的有效性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00657v1",
      "published_date": "2024-12-01 03:28:26 UTC",
      "updated_date": "2024-12-01 03:28:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:49:06.762852"
    },
    {
      "arxiv_id": "2412.00653v1",
      "title": "Predictive Inference With Fast Feature Conformal Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Tang",
        "Boyuan Wang",
        "Chuan Wen",
        "Jiaye Teng"
      ],
      "abstract": "Conformal prediction is widely adopted in uncertainty quantification, due to\nits post-hoc, distribution-free, and model-agnostic properties. In the realm of\nmodern deep learning, researchers have proposed Feature Conformal Prediction\n(FCP), which deploys conformal prediction in a feature space, yielding reduced\nband lengths. However, the practical utility of FCP is limited due to the\ntime-consuming non-linear operations required to transform confidence bands\nfrom feature space to output space. In this paper, we introduce Fast Feature\nConformal Prediction (FFCP), which features a novel non-conformity score and is\nconvenient for practical applications. FFCP serves as a fast version of FCP, in\nthat it equivalently employs a Taylor expansion to approximate the\naforementioned non-linear operations in FCP. Empirical validations showcase\nthat FFCP performs comparably with FCP (both outperforming the vanilla version)\nwhile achieving a significant reduction in computational time by approximately\n50x. The code is available at https://github.com/ElvisWang1111/FastFeatureCP",
      "tldr_zh": "该研究针对Conformal prediction在不确定性量化中的应用，提出了Fast Feature Conformal Prediction (FFCP)，以解决Feature Conformal Prediction (FCP)中因非线性操作导致的计算效率问题。FFCP引入了一个新的非一致性分数，并利用Taylor expansion近似FCP的非线性转换，使其更适合实际场景。实验结果显示，FFCP的性能与FCP相当（均优于原始版本），但计算时间减少约50倍，提供了一个高效的预测推理框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00653v1",
      "published_date": "2024-12-01 03:14:04 UTC",
      "updated_date": "2024-12-01 03:14:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:49:08.929483"
    },
    {
      "arxiv_id": "2412.00631v1",
      "title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning",
      "title_zh": "ROSE：一种奖励导向的数据选择框架，用于LLM任务特定指令微调",
      "authors": [
        "Yang Wu",
        "Huayi Zhang",
        "Yizheng Jiao",
        "Lin Ma",
        "Xiaozhong Liu",
        "Jinhong Yu",
        "Dongyu Zhang",
        "Dezhi Yu",
        "Wei Xu"
      ],
      "abstract": "Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human-controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures.",
      "tldr_zh": "这篇论文针对大型语言模型（LLMs）的任务特定指令微调，提出了ROSE框架，以解决现有数据选择方法依赖相似性指标却无法有效提升任务性能的问题，因为指令微调损失（如交叉熵损失）与实际性能不一致。ROSE采用pairwise preference loss作为奖励信号，并通过influence formulation来评估训练数据点对少量偏好验证集的影响，从而选择最相关的训练数据。实验结果显示，使用ROSE仅需5%的训练数据，就能与全数据集微调媲美，并优于其他先进方法，同时在多个基准数据集和模型架构上展现出强大的泛化性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00631v1",
      "published_date": "2024-12-01 01:01:09 UTC",
      "updated_date": "2024-12-01 01:01:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:49:12.705332"
    },
    {
      "arxiv_id": "2412.00627v2",
      "title": "ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by Multimodal Gemini LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Rithik Vir",
        "Parsa Madinei"
      ],
      "abstract": "Cooking meals can be difficult, causing many to resort to cookbooks and\nonline recipes. However, relying on these traditional methods of cooking often\nresults in missing ingredients, nutritional hazards, and unsatisfactory meals.\nUsing Augmented Reality (AR) can address these issues; however, current AR\ncooking applications have poor user interfaces and limited accessibility. This\npaper proposes a prototype of an iOS application that integrates AR and\nComputer Vision (CV) into the cooking process. We leverage Google's Gemini\nLarge Language Model (LLM) to identify ingredients in the camera's field of\nvision and generate recipe choices with detailed nutritional information.\nAdditionally, this application uses Apple's ARKit to create an AR user\ninterface compatible with iOS devices. Users can personalize their meal\nsuggestions by inputting their dietary preferences and rating each meal. The\napplication's effectiveness is evaluated through three rounds of user\nexperience surveys. This application advances the field of accessible cooking\nassistance technologies, aiming to reduce food wastage and improve the meal\nplanning experience.",
      "tldr_zh": "本论文提出ARChef，一款基于iOS的增强现实（Augmented Reality, AR）烹饪助手应用，利用多模态Gemini Large Language Model (LLM) 和计算机视觉（Computer Vision, CV）技术，解决传统烹饪方法导致的食材缺失、营养风险和不满意问题。应用通过Gemini LLM识别相机视野中的食材、生成个性化配方建议（包括详细营养信息），并结合Apple的ARKit创建交互式AR用户界面，用户可输入饮食偏好并对餐点评分。经三轮用户体验调查评估，该应用有效减少食物浪费并提升餐点规划体验，推进了可访问的烹饪辅助技术发展。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00627v2",
      "published_date": "2024-12-01 00:52:51 UTC",
      "updated_date": "2024-12-09 08:27:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:49:12.580342"
    },
    {
      "arxiv_id": "2412.00622v2",
      "title": "Visual Modality Prompt for Adapting Vision-Language Object Detectors",
      "title_zh": "视觉模态提示用于适应视觉-语言物体检测器",
      "authors": [
        "Heitor R. Medeiros",
        "Atif Belal",
        "Srikanth Muralidharan",
        "Eric Granger",
        "Marco Pedersoli"
      ],
      "abstract": "The zero-shot performance of object detectors degrades when tested on\ndifferent modalities, such as infrared and depth. While recent work has\nexplored image translation techniques to adapt detectors to new modalities,\nthese methods are limited to a single modality and apply only to traditional\ndetectors. Recently, vision-language detectors, such as YOLO-World and\nGrounding DINO, have shown promising zero-shot capabilities, however, they have\nnot yet been adapted for other visual modalities. Traditional fine-tuning\napproaches compromise the zero-shot capabilities of the detectors. The visual\nprompt strategies commonly used for classification with vision-language models\napply the same linear prompt translation to each image, making them less\neffective. To address these limitations, we propose ModPrompt, a visual prompt\nstrategy to adapt vision-language detectors to new modalities without degrading\nzero-shot performance. In particular, an encoder-decoder visual prompt strategy\nis proposed, further enhanced by the integration of inference-friendly modality\nprompt decoupled residual, facilitating a more robust adaptation. Empirical\nbenchmarking results show our method for modality adaptation on two\nvision-language detectors, YOLO-World and Grounding DINO, and on challenging\ninfrared (LLVIP, FLIR) and depth (NYUv2) datasets, achieving performance\ncomparable to full fine-tuning while preserving the model's zero-shot\ncapability. Code available at: https://github.com/heitorrapela/ModPrompt.",
      "tldr_zh": "该研究探讨了视觉语言物体检测器（如 YOLO-World 和 Grounding DINO）在不同模态（如红外和深度）上的零-shot 性能下降问题，现有方法如图像翻译或传统微调无法同时适应新模态和保留零-shot 能力。作者提出 ModPrompt，一种视觉提示策略，包括编码器-解码器提示和推理友好的模态提示解耦残差，允许检测器适应新模态而不损害零-shot 性能。实验结果显示，在 LLVIP、FLIR 和 NYUv2 数据集上，ModPrompt 在 YOLO-World 和 Grounding DINO 上实现了与全微调相当的性能，同时保持了模型的零-shot 能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00622v2",
      "published_date": "2024-12-01 00:19:59 UTC",
      "updated_date": "2025-03-14 20:32:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:49:16.038206"
    },
    {
      "arxiv_id": "2412.00621v1",
      "title": "Exposing LLM Vulnerabilities: Adversarial Scam Detection and Performance",
      "title_zh": "揭示 LLM 漏洞：对抗性诈骗检测与性能",
      "authors": [
        "Chen-Wei Chang",
        "Shailik Sarkar",
        "Shutonu Mitra",
        "Qi Zhang",
        "Hossein Salemi",
        "Hemant Purohit",
        "Fengxiu Zhang",
        "Michin Hong",
        "Jin-Hee Cho",
        "Chang-Tien Lu"
      ],
      "abstract": "Can we trust Large Language Models (LLMs) to accurately predict scam? This\npaper investigates the vulnerabilities of LLMs when facing adversarial scam\nmessages for the task of scam detection. We addressed this issue by creating a\ncomprehensive dataset with fine-grained labels of scam messages, including both\noriginal and adversarial scam messages. The dataset extended traditional binary\nclasses for the scam detection task into more nuanced scam types. Our analysis\nshowed how adversarial examples took advantage of vulnerabilities of a LLM,\nleading to high misclassification rate. We evaluated the performance of LLMs on\nthese adversarial scam messages and proposed strategies to improve their\nrobustness.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)在诈骗检测任务中的漏洞，特别是在面对对抗性(adversarial)诈骗消息时。研究者构建了一个全面数据集，包含原始和对抗性诈骗消息，并扩展了传统的二元分类到更细化的诈骗类型标签。分析结果显示，对抗性示例利用LLMs的弱点导致高误分类率，并通过性能评估提出了多种策略来提升模型的鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "4 pages, 2024 IEEE International Conference on Big Data workshop\n  BigEACPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.00621v1",
      "published_date": "2024-12-01 00:13:28 UTC",
      "updated_date": "2024-12-01 00:13:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T02:49:16.299855"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 59,
  "processed_papers_count": 59,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-16T02:49:39.040028"
}